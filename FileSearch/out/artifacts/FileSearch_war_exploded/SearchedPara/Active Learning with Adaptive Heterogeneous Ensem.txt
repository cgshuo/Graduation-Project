
Classification techniques need both input and label infor-mation to build classifiers. In many data mining applications, data points without labels are abundant, but obtaining the labels often involves the work of human experts and thus is a costly and time-consuming process. For example, a time-consuming, expensive and/or dangerous procedure might be required to obtain class labels for patients in a medical data set. The objective of active learning is to query labels only for a carefully chosen subset of data points to reduce the cost of labeling while minimizing impact on classification accuracy.

Active learning can be divided into two categories. One common approach is to choose one classifier and select data points that help the training of this classifier, which normally includes choosing data point s according to some confi-dence measure. This approach includes uncertainty sampling [2][3], in which data points that the current classifier is most uncertain about are considered informative. Many other methods have followed this approach [4][5][6], where uncertainty measures were developed for popular classifiers such as SVM [7] or k NN [8]. Recent work [9] introduced an uncertainty measure for C4 .5 [10] and performed active learning with constructed data points.

As is known, there is no one classifier universally suitable for all classification tasks. Different classifiers perform best with a particular distribution of data points. Therefore any method following the uncertainty sampling approach must first select the appropriate classifier type and then design an uncertainty measur e according to the choice of classifier type. Interesting recent work [ 11] exploits the clustering structures of data sets to suggest an active learning method that is independent of the choice of classifier type, but the effectiveness of the approach has only been tested with logistic regression.

The other approach to active learning is query by com-mittee (QBC)[1], in which the most informative data points are those that cause maximal disagreement among the pre-dictions of a committee of classifiers. QBC requires the committee of classifiers to be both accurate and diversified, which is also desired in ensemble learning [12]. Combining these two approaches is pr omising because it is known that ensemble learning methods [13][14][15][16] generalize better than a single learner [17] and there is no need to design different uncertainty m easures for different ensemble methods. In [18], bagging [13] and boosting [14] were combined with QBC, but both ensemble methods require a fair amount of data to start the learning process, which is not desirable considering the cost of labeling data points. In a different approach, the estimation-exploration algorithm [19][20][21](EEA) uses stochastic optimization algorithms to optimize models and find disagreement-causing data points. Informative sampling [22] adapted the EEA for data mining, but its performance is constrained by the limited power of stochastic optimization. For some classifiers, a satisfactory stochastic optimization algorithm may not exist, and existing classifiers such as C4.5 and Naive Bayes [23] can not currently be used in this framework.

In this paper, the random subspace method (RSM)[15] is combined with QBC. As a well-established ensemble method, RSM offers good generalization and only requires one data point to start training. Although previous methods have focused on building homogeneous ensembles with QBC, homogeneous ensembles face similar problems as when using one classifier: the bias of the chosen classifier type. For example, decision trees form decision boundaries that consist of lines or planes that are orthogonal to one of the input features, which makes them less suitable for a data set with continuous non-linear decision boundaries. In-tuitively, using different classifiers to construct an ensemble would be helpful because it pr ovides better diversity while each classifier remains partially accurate. Many methods have been suggested to construct heterogeneous ensembles. The ensembles were typically formed by including one instance of each classifier [24] or selecting members of an ensemble from a large library of classifiers trained with different parameters [25]. RSM is used here to construct the ensemble because it can create a large number of different instances of any classifier type.

This paper shows that active learning with heterogeneous ensembles outperforms homogeneous ensembles, and that different ratios of classifier types in an ensemble suit dif-ferent data sets. The question then arises as to how to find a suitable ratio of classifier types for a given data set. An algorithm is introduced in this paper to adapt the ratio of different classifier types in a heterogeneous ensemble during training, which we term adaptive heterogeneous ensembles (AHE). The algorithm starts with a heterogeneous ensemble and alternates between a query phase and a training phase. In the query phase, classifiers in the current ensemble predict labels for a set of unlabeled data points, and the data point that induces the maximum prediction variance across the ensemble is chosen for label querying and then added to the training set. In the training phase, the ratio of different classifier types is adapted toward the ratio with better performance and a new ensemble of classifiers is trained according to the adap ted ratio. In this work, we are especially interested in small ensemble sizes because it is computationally expensive to construct and evaluate a large new ensemble in each iteration. The major difference between our approach and [25] is that we are trying to find the best way to combine different classifier types for small ensembles rather than training a large library of classifiers and trying to find a more accurate subset of classifiers from it. To the best of our knowledge, our work is the first to adapt the ratio of classifiers in a heterogeneous ensemble for active learning.

In our current work, C4.5 and Naive Bayes were chosen as the classifier types, but AHE can work with any classification technique. Experiments on five data sets from the UCI Machine Learning Repository [26] show that the algorithm successfully finds the appropriate ratio for a data set and outperforms C4.5 with uncertainty sampling, Naive Bayes with uncertainty sampling, the same algorithm with random sampling, and bagging, boosting, and the random subspace method with random sampling.

The rest of the paper is organized as follows: Section 2 introduces the algorithm and the adaptive strategy, Section 3 provides the results and Section 4 concludes the paper.
The adaptive heterogeneous ensembles method alternates between two phases: the query phase chooses one data point for labeling and adds it to the training set, and the training phase adapts the ensemble towards a better combination of classifiers. One iteration of the AHE is defined as a query phase followed by a training phase. Table I outlines the general algorithm.

The algorithm starts by randomly selecting one data point from the current pool of training data, the label of this data point is queried, and the data point is added to the training set. In this work, the pool of training data is made available to the algorithm in a streaming manner similar to [27]. The whole potential training s et is partitioned into chunks of equal size, and in each iteration one chunk of data serves as the pool for choosing one new data point.

There are two reasons to run the algorithm in a streaming manner. The first is for the efficiency of the algorithm. Active learning is especially desirable when the potential unlabeled training set is large. Therefore, scanning the whole pool of potential training data in each iteration to choose one data point is prohibitively inefficient. The second reason is that in this way the algorithm works on both static and streaming data.

After the first data point has been chosen out of the first chunk of data, a heterogeneous ensemble of classifiers is generated according to a predefined initial ratio. In this work, the first ensemble consists of five C4.5 and five Naive Bayes classifiers, which provides the two classifier types with equal footing. Each classifier is trained on a 50% random subspace of the current training set.

During the first query phase, the second chunk of data is made available to the ensemble. Predictions are made by each classifier in the ensemble for each data point in the chunk. The predicted labels are recorded for each data point, and the data point with the largest variance of predicted labels is chosen. For a two class problem, the most informative data points are t hose that cause as close to half of the classifiers in the ensemble to predict the positive label as possible, and the remainder t o predict the negative label. Ties are broken by randomly choosing a data point from the chunk. The label of the chosen point is queried, and this data point is added to the training set as the second data point.

The first query phase is followed by the first training phase. In this phase, the ratio of classifier types in the ensemble is first adapted, and then a new ensemble of classifiers is generated according to the new ratio, each again on a 50% random subspace on the current training set. Several adaptation strategies could work with AHE. The adaptation strategy in thi s paper works as follows: training pool, the testing set and the adaptation set which is used for adapting the ratio of the ensemble. are obtained for the ensemble against the adaptation set: the voting accuracy of the entire e nsemble, the voting accuracy of the current ensemble with one randomly chosen C4.5 classifier removed, and the voting accuracy of the current ensemble with one randomly chosen Naive Bayes classifier removed. adapted according to the compar ison of the three accuracies. If the accuracy with one C4.5 classifier removed is better than the accuracy with one Naive Bayes classifier removed and better than or equal to the accuracy with the entire ensemble, then the number of C4.5 classifiers is decreased by one, and the number of Naive Bayes classifier is increased by one. The relative numbers are adjusted similarly in the opposite case. If the entire ensemble achieves the best accuracy, then the ratio remains the same.

The algorithm then starts the second query phase: a third data point is chosen from the third chunk of data and added to the training set. The second training phase then adapts the ratio of the ensemble again, and a new ensemble is trained according to the new ratio on the updated training set.
The algorithm executes a predefined number of iterations or until the accuracy of the ensemble meets some pre-defined criteria.

In this section the experimental settings and data sets are introduced, and then the experimental results are reported. Two sets of experiments were conducted. In the first set of experiments, evidence is provided that heterogeneous ensembles consistently outperform homogeneous ensembles, and different combinations of C4.5 and Naive Bayes clas-sifiers suit different data set s. Then the performance of the AHE was compared against the same algorithm with static ratios to show the effectiveness of the adaptation strategy. The second set of experiment s compares the accuracy of AHE with uncertainty sampling and random sampling. A. Data sets and experimental settings Experiments were conducted on five data sets from the UCI Machine Learning Repository [26]. All data sets were shuffled and partitioned into thr ee sets: the training pool, the testing set and the adaptation s et as mentioned before. For all data sets, parameters were chosen empirically as a trade-off between computation time and labeling cost, which are also criteria for choosing parameters in practice. The first data set was one of the feature sets from the Multiple Features Data Set called mfeat-pixel 1 .Thisdata set consists of features of handwritten digits extracted from a collection of Dutch utility maps. The mfeat-pixel data set contains 240 features and 10 possible output labels, with each representing a digit between  X 0 X  and  X 9 X . For each class label there are 200 data points, thus the whole data set contains 2000 data points. There are no missing values in the data set. 1200 data points were randomly chosen for the training pool, 500 were randomly chosen to serve as the testing set, and the remaining 300 data points serve as the adaptation set. The traini ng pool was partitioned into 400 chunks, with each containing th ree data points. One data point was drawn into the train ing set during each iteration of the algorithm. There totally 400 such iterations. Thus at the end of an execution of the algorithm, there were 400 data points in the training set.
 The second data set was the Page Blocks Classification Data Set 2 , henceforth referred to as  X  X age X . The data set recorded 10 features of segm ented blocks of a document, and the task is to classify a block as  X  X ext X ,  X  X orizontal line X ,  X  X icture X ,  X  X ertical line X  and  X  X raphic X . There are 5473 data points in this data set with no missing values, of which 4913(89.8%) data points belong to class  X  X ext X . 4000 data points were randomly chosen to serve as the training pool, 737 data points were randomly chosen to form the testing set, and the adaptation set contains the remaining 736 data points. Each chunk of the training pool contains five data points, and one data point was drawn into the training set during each of the 800 iterations. Thus the final training set contains 800 data points.

The third data set tested from UCI was the Statlog (Land-sat Satellite) Data Set 3 ,henceforth referred to as  X  X atImg X . The data set consists of the multi-spectral values of pixels in 3x3 neighborhoods in a satellite image, the objective of which is to predict the label asso ciated with the central pixel in each neighborhood. There ar e a total of 6435 data points in the data set without missing values. Each data point has 36 features and six possible class labels. The training pool of this data set contains 4435 randomly chosen data points, the testing set contains 1000 randomly chosen data points, and the remaining 1000 data points make up the adaptation set. The training pool was partitioned into chunks of 10 data points. One data point was chosen out of a chunk in each iteration of the algorithm, and there were 400 such iterations. Thus the final training set contains 400 data points. The fourth data set from UCI was the Spambase Data Set 4 , henceforth referred to as  X  X pam X . 57 features were collected from 4601 emails in this data set to predict if a given email is  X  X pam X  or  X  X on-spam X , and there are no missing values. 3601 data points were randomly chosen as the training pool, 500 were randomly chosen for the testing set and the remaining 500 data points serve as the adaptation set. Each chunk of the training pool contains five data points. One data point was chosen out of a chunk in each iteration, and 700 iterations were conducted during each run. Thus there were 700 data points in the final training set.
The last data set was the Waveform Data Set (Version 2) 5 This data set is an artificial data set generated with noise. Each data point in the data set has 40 features generated to predict three classes of waves. All of the features include noise, and the latter 19 features are all noise features with zero mean and unity variance. Of the 5000 data points, 4000 were randomly chosen as the training pool, 1000 were randomly chosen as the testing set, and the adaptation set contained the remaining 1000 data points. The training pool was partitioned into chunks of 10, one data point was chosen out of each chunk during each of the 400 iterations. Thus the final training set contained 400 data points.
All results report the means and standard deviations of 30 independent runs, and all accu racies were reported as error percentages. Each independent run works with a new seed to generate random numbers. The random numbers are used to randomly choose the first data point for AHE and uncertainty sampling, and to randomly choose all data points for random sampling. The C4.5 and Naive Bayes classifiers were the implementations by Weka [28] with default settings. The ensemble size was ten for each experiment. B. Comparison with Static Ensembles
In this section, evidence is provided that heterogeneous ensembles work consistently better than homogeneous en-sembles with active learning on the five tested data sets, and the effectiveness of AHE is reported in terms of both accuracy compared against AHE with static ratios and the algorithm X  X  ability to converge to suitable ratios. The experiments on AHE with static ratios consist of the test of AHE on all possible starting ratios of C4.5 and Naive Bayes classifiers without adaptation: we tested the AHE with static ratios from zero C4.5 classifiers and 10 Naive Bayes classifiers to 10 C4.5 classifiers and zero Naive Bayes classifiers.
 Figure 1 reports the results us ing data set  X  X feat-pixel X . In Figure 1(a), the error bars with circles represent AHE with static ratios (lines zero to 10 indicate the number of C4.5 classifiers in the ensembles), and the error bar with an asterisk represents AHE with adaptation. In Figure 1(b), ratio changes of AHE with adaptation are reported as the changes in the number of C4.5 classifiers in the ensemble over iterations. It can be seen that AHE with static ratios perform best when there are six C4.5 and four Naive Bayes classifiers in the ensemble. Although the accuracy of this ratio is only marginally better than employing equal numbers of C4.5 and Naive Bayes classifiers as well as seven C4.5 and three Naive Bayes classifiers, it is statistically significantly better than using homogeneous ensembles with all C4.5 classifiers or all Naive Bayes classifiers. AHE with adaptation converges suitably to close to six C4.5 classifiers in the ensemble as Figure 1(b) indicates, but is slightly though not significantly worse in accuracy than AHE with the best static ratio (compare line 11 to line 6 in Figure 1(a)). The reason is that the best combination of static ensembles aids AHE to choose the most informative data points for training as well as to make accurate predictions. Although AHE adapts the ensemble towards the best ratio, there might be costs associated with the quality of the chosen data during adaptation. This can clearly be seen before the 50th iteration in Figure 1(b) in which the ensemble first favors more Naive Bayes classifiers and then starts to increase the number of C4.5 classifiers. Fortunately, the cost appears marginal as the AHE with adaptation achieves statistically significantly better accuracy than homogeneous ensembles (compare line 11 to lines zero and 10 in Figure 1(a)).
 The results for the  X  X age X  data set are reported in Fig. 2. Fig. 2(a) shows that this data set favors more C4.5 classifiers in the ensemble such that the re are no significant accuracy differences when there are more than six C4.5 classifiers in the static ensembles. As is shown by Fig. 2(b), AHE with adaptation often converges appropriately to eight C4.5 classifiers, and the algorithm achieves equivalent accuracy compared to AHE with the optimal static ratios (compare line 11 to lines 6-10 in Fig. 2(a)).
Like the  X  X age X  data set, the  X  X atImg X  data set is more ac-curately modeled if more C4.5 than Naive Bayes classifiers are employed in the ensemble (Fig. 3(a)), though eight or nine C4.5 classifiers are optimal. AHE with adaptation con-verges correctly to close to a mean of eight C4.5 classifiers as seen in Fig. 3(b), and the algorithm achieves comparable accuracy compared to the best c lassifier ratios in the static ensembles (compare line 11 to lines 8 and 9 in Fig. 3(a)).
The  X  X pam X  data set is also modeled better when a majority of C4.5 classifiers are employed. Fig. 4(a) indicates that the AHE which employ ensembles with six or more C4.5 classifiers all perform equivalently, and better than those that employ five or fewer (compare line 11 to lines 6-10 in Fig. 4(a)). It is shown in Fig. 4(b) that AHE with adaptation converges to on average seven C4.5 classifiers.
Unlike the previous three data sets, the  X  X aveform X  data set is more amenable to modeling by Naive Bayes classifiers when incorporated in the AHE . As is shown in Fig. 5(a), AHE with three C4.5 classifiers performs significantly or marginally better than other ensemble ratios. AHE with adaptation achieves compara ble accuracy to the best static ratio of three C4.5 and seven Naive Bayes classifiers (com-pare line 11 to 3 in Fig. 5(a)), and Fig. 5(b) shows that AHE suitably converges to a mean of three C4.5 classifiers. C. Comparison against Random and Uncertainty Sampling
This section reports comparisons of AHE against random sampling and uncertainty samp ling. For each of the tested data sets, two sets of experiments were conducted: AHE was first compared against AHE with random sampling, C4.5 with uncertainty sampling, and Naive Bayes with uncertainty sampling over iterations, and then AHE was compared against bagging with random sampling, boosting with random sampling and the random subspace method with random sampling. AHE with random sampling was achieved by disabling the query phase: during each iteration, a single randomly-selected data point is drawn from each chunk. Adaptation of classifier ratio was retained in this algorithm variant. The first comparison is reported over iterations as the means and standard deviations of compared algorithms.
 The same settings employed in the AHE were used for C4.5 with uncertainty sampling, except that during each iteration the data point with the lowest confidence output by Weka [28] was drawn from the data chunk into the training set, and a C4.5 classifier was then trained on this enlarged training set. Naive Bayes with uncertainty sampling was implemented similarly to C4.5 with uncertainty sampling. Both uncertainty sampling methods start with a randomly chosen data point as AHE does, which explains the standard deviations in accuracies of th e two deterministic algorithms in the figures.

The second comparison was conducted because active learning methods with a poorly selected classifier might perform worse than a more accurate classifier with ran-dom sampling. Bagging, boosting and the random subspace method with C4.5 as base learners were therefore compared against AHE with random sampling. All methods were implemented by calling Weka with default settings. The three methods were trained on randomly-sampled training sets of the same size as those collected by the AHE. The results are reported in Fig. 6-10, and Table II. Each cell in Table II reports the mean and standard deviation of error percentage over 30 independent runs and the significance level p ( X * X  represents p&lt; 0 . 05 ,  X ** X  represents p&lt; 0 . 01 and  X *** X  represents p&lt; 0 . 001 ) between the accuracy of AHE and the corresponding method from the Mann-Whitney Utest[29].

Fig. 6 reports the results for t he  X  X feat-pixel X  data set. Fig. 6(a) indicates that AHE outperforms all competing methods, and Fig. 6(b) shows that AHE is statistically significantly more accurate than bagging, boosting and the random subspace method.

On the  X  X age X  data set, Fig. 7(a) indicates that AHE also outperforms all the other tested methods as well. It is interesting to note in Fig. 7(a) that the accuracy of Naive Bayes with uncertain sampling degenerates over the course of a run. This is because, as described in section III-A, the  X  X age X  data set is highly unbalanced. The Naive Bayes classifiers may achieve better accuracy early on in a run by simply predicting the majority label, but when more data points with minority labels are drawn into the training set because they tend to have low confidence, the Naive Bayes classifiers predict increasingly erroneous labels. Fig. 7(b) shows that AHE outperforms bagging, boosting and the random subspace method.

For the  X  X atImg X  data set, AHE is shown to be more accurate than all competing a lgorithms as shown by Figs. 8(a) and 8(b). For the  X  X pam X  data set, Fig. 9(a) shows that AHE significantly outperforms C4.5 and Naive Bayes with uncertainty sampling, and is slightly more accurate than AHE with random sampling. It can be seen in Fig. 9(b) that AHE is statistically significantly more accurate than bagging with random sampling, and performs slightly better than boosting and the random subspace method with random sampling. For the  X  X aveform X  data set, AHE is shown in Fig. 10(a) to significantly outperform C4.5 with uncertainty sampling, and slightly outperform AHE with random sampling and Naive Bayes with uncertainty sam-pling. It is shown in Fig. 10(b) that AHE again outperforms bagging, boosting and the random subspace method with random sampling.

In conclusion, AHE outperforms state-of-the-art classi-fiers that rely on uncertainty sampling as well as state-of-the-art ensemble methods that rely on random sampling, for all five of the tested data sets.

In this paper, the random subspace method is employed to create multiple instances of different classifier types within an ensemble. The resulting heterogeneous ensemble is then exploited to draw informative training data from the entire data corpus using query by committee, and the expanded training set is then used to alter the ratio of classifier types toward one that is appropriate for the given data set. This process is repeated until some termination criteria are met. We term this combined approach adaptive heterogeneous ensemble learning . We show that our approach outperforms homogeneous ensembles such as bagging, boosting and the random subspace method, as well as heterogeneous ensembles with fixed classifier ratios, in which all meth-ods use the same amount of training data. Our approach also outperforms C4.5 and Naive Bayes using uncertainty sampling (an active learning approach that does not require ensembles).

Although C4.5 and Naive Bayes were used as classifiers in this paper, AHE is a framework that can accommodate any classifier type. Future work will involve expanding AHE to include more classifiers in the ensemble and testing its per-formance against other classification methods on more data sets. We also plan to investigate more intelligent methods for adapting the ratio of classifier types and choosing subspaces for ensemble members. Finally, we plan to design methods that automatically determine p arameters such as the size of the ensemble and the rate at which training data is drawn from the data corpus in static and streaming domains. This research is supported in part by the US National Science Foundation (NSF) under grants EPS-0701410 and CCF-0905337, in part by the National Natural Science Foundation of China (NSFC) under award 60828005, and in part by the National Basic Research Program of China (973 Program) under award 2009CB326203.

