 Currently web image retrieval systems usually fall into two main categories: text-based and content-based. In text-based systems, images are first annotated with texts which are produced by people manually, or extracted from image surroundings automatically, then text retrieval techniques are used to performed image retrieval. However, annotating im-age manually is tedious, time-consuming and subjective, while annotating images auto-matically with surroundings often involves terms irrelevant to image semantics unavoid-ably. Content-based image retrieval (CBIR) systems automatically index and images by their low-level visual features. However, it is flawed in the following ways. Firstly, the user query must be provided in the form of a draft of the desired image. Secondly, the images with similar low-level features may have different contents [8]. Finally, there is so-called semantic gap between image low-l evel features and high-level concepts.
Recently, many approaches have been propos ed to automatically annotate images with keywords [7,2,6]. Automatic image annotation is a promising methodology for image retrieval. However it is still in its infancy and is not sophisticated enough to extract perfect semantic con cepts according to image low-level features, often produc-ing noisy keywords irrelevant to image semantics. Noisy concepts may be an obstacle to getting high-quality image retrieval. I n this paper we propose a novel approach to improve image retrieval, which utilizes coherence between coarse concepts and relat-edness between concepts and web textual information to refine image annotations. The refinement is based on two basic assumpti ons. One assumption is that concepts con-tained in an image should be semantically re lated to each other. Another assumption is intuitive that the observation of some specific terms in web documents should increase the belief of certain semantically similar concept. For example, if  X  X iger X  is included in web documents, the annotated concept  X  X iger X  should be more credible than before the observation. In this paper image is annotated as follows: first we use the classic anno-tation model CRM [6] to associate an image with a set of keywords (coarse concepts); then these coarse concepts are associated with weights which are calculated from co-herence and relatedness using ontological lexicon WordNet [3]. The model proposed by Jin et al. [5] also uses WordNet to improve image annotations, but there are two important differences comparing to our work. First, Jin et al. only take into account co-herence to remove noisy concepts, while we use extra text in web documents. Second, Jin et al. focused on eliminating  X  X oisy X  concepts, while we focus on applying these weighted concepts to improving the ranking of image retrieval results.

The remainder of this paper is organized as follows. Section 2 briefly introduces the classic image annotation model, CRM. S ection 3 describes how to refine coarse concepts produced by CRM in details, together with a brief introduction to an image retrieval prototype. Section 4 presents experimental results and some discussions. The last section concludes this paper plus some ideas for future work. Let V be the annotation vocabulary, T be the training set, J be an element of T . J is partitioned into a set of fixed-size small regions r J = { r 1 ,...,r n } , along with corre-sponding annotation w J = { w 1 ,...,w m } where w i  X  V . The Continuous Relevance Model [6] (CRM) assumes that generating J is based on three distinct probability dis-tributions. First, the set of annotation words w J is a result of | V | independent sam-ples from underlying multinomial distribution P V (  X | J ) . Second, each image region r is a sample of a real-valued feature vector g using a kernel-based probability density function P G (  X | J ) . Finally, the rectangular region r is produced according to some un-known distribution conditioned on g ,so r J are produced from a corresponding set of vectors g 1 ...g n according to a process P R ( r i | g i ) which is independent of J .Nowlet r ing set T . Similarly, let w B be some arbitrary subset of V ( | w B | = m ). Then we like to model P ( r A , w B ) , the joint probability of observing an image defined by r A together with annotation words w B . The observation of { r A , w B } can be supposed to come from the same process that generated one of the image J  X  in the training set T . Formally, the probability of a joint observation { r A , w B } is: So given a new image we can split it into regions r A , compute feature vectors g 1 ...g n for each region and then use formula 1 to determine what subset of vocabulary w  X  is the most likely to co-occur w ith the set of feature vectors: Here we only give a brief introduction to CRM, and for details please refer to [6]. In previous section, we have described how to use CRM to assign an a coarse con-cept sequence ( c 1 ,...,c T ) to image. However, some concepts are possible noisy or incorrect with respect to image semantics. In what follows we will describe how to distinguish these  X  X oisy X  concepts from others by using the notions of coherence and relatedness based on WordNet. The notion of coherence assumes that concepts in an-notations should be semantically similar each other, while relatedness refers to the se-mantic similarity between image annotations and terms in web documents. 3.1 Measuring Coherence and Relatedness The JCN algorithm [4] is adopted here to measure the similarity between words (con-cepts) due to its effectiveness, in which the similarity measure of two concepts  X  c 1  X  and  X  c 2  X  is based on the notations of Information Content ( IC ) and concept-distance, defined as: where IC ( c )=  X  logP ( c ) and P ( c ) is the probability of encountering an instance of concept  X  X  X  in WordNet; lcs ( c 1 ,c 2 ) is the lowest common sub-summer that subsumes both concepts  X  c 1  X  X nd X  c 2  X . Note that all measures are normalized so that they fall within a 0-1 range. For simplicity, normalization factor is omitted, simply assuming that 0  X  sim jcn ( c i ,c j )  X  1 .

Let C =( c 1 ,...,c T ) be the coarse concepts produced by CRM, D =( d 1 ,...,d n ) be the terms in page title and image su rroundings etc., then the measure of coherence of concept c i is defined as: where  X  1 is normalization factor. Similarly, the measure of increased belief for a con-cept c i according to the relatedness between concept c i and textual information D is defined as: Now we get two variables, a i and b i , as the measure of the importance of concept c i to the semantics of an image. Note that a i and b i are both 1, that is to say, a i and b i can be regarded as two independent probability distributions of the quantified importance of concept c i . We combine these two factors linearly as follows: important it is to the semantics of corresponding image. 3.2 Retrieval Prototype In the rest of this paper, we refer to C = { c 1 ,...,c T } as the annotation-set of image I , and Q = { q 1 ,...,q m } as the query. Let n be the number of index terms in the system, k be a generic index term. K = { k 1 ,...,k n } is the set of all index terms. A weight w term which does not appear in the annotation-set C , w i,c =0 . With the annotation-set C associated, an index term vector Similarly, let w i,q be the weight associated with the pair ( k i ,Q ) where w i,q  X  0 ,and  X   X  q =( w where the specifications of w i,c and w i,q are as follows:: where C is the annotation-set for image I produced by CRM, s ( k i ) is the scoring func-tion in formula 6, and  X  is a threshold for filtering noisy concepts whose scores are below it. In our experiment  X  was set be 0.1 empirically. For the sake of comparison, we implemented another probability based ranking strategy like in [6]: given a text query Q , we get a conditional probability P ( Q | J ) for image J according to formula 1, then retrieved images are ranked according to P ( Q | J ) . In short, we use two ranking strategies in this paper, one is the proposed vector-based ranking (VIR) and another is probability-based ranking (PIR). The training data set is the Corel Image Dataset, consisting of 5000 images from 50 Stock Photo CDs. Each image is partitioned into 4  X  6 regions. These images are an-notated with words drawn from a vocabulary of size 374, denoted by V . In addition, we have previously downloaded 10,000 we b pages from the WWW accompanied with im-ages. These images are used as test set in the experiments. We selected top 25 frequent terms in training set as test queries. The CRM was used to annotate each web image with up to 5 keywords. These keywords were u sed as image indexes for image retrieval.
We u s e d precision and recall metrics to evaluate the image retrieval results. Given a query Q and a set R of relevant images for a query Q , we obtained a set A of relevant and recall is | A  X  R | / | R | . To determine the set R of relevant images to each of the 25 test queries, we adopted a strategy in [1] : For each test query, we ran out two ranking strategies above. The 40 highest ranked images returned by each of the two ranking strategies were pooled into a set of unique images and then classified by volunteers as relevant or irrelevant with respect to the query term. At the same time, the byproduct of the construction of R is a small set of images which have been labeled be relevant or irrelevant to certain query term by human, denoted by D R . So we evaluated CRM over D R as the baseline. Note that all images in | D R | was associated with the query word, so the precision of CRM over D R is the number of images correctly annotated with a given word, divided by | D R | . Additionally, it is evident that the recall of CRM over D R is 100%. We calculated the mean precision for 25 queries and obtained 27%. It was used as baseline and depicted in figure 1.

Usually we want to evaluate average precision at given recall levels. The standard 11-point average precision curve is used for this purpose. It plots precisions at 0 percent, 10 percent,...,100percentofr ecalls. The mean average precision ( MAP ) is the arithmetic mean of average precision calculated over a ll queries (here 25) at some specific percent recall. Note that the results of CRM was a stra ight line, rather than a curve, because they were annotation accuracies rather than retr ieval accuracies. The results were depicted in figure 1. As indicated earlier, the baseline is the mean precision of CRM over D w for 25 query terms w . The curve for baseline in figure 1 reveals the weakness of automatic image annotation technique in image retrieval task without any ranking strategy, only 27 percent precision. In contrast, both of PIR and VIR ranking strategies improved im-age retrieval. Furthermore, the performance of the proposed approach (VIR) is overall superior to PIR owing to the removal of some noisy concepts and more reasonable weights associated with concepts. Because some noisy concepts were removed, the final precision at recall level 100 percent o f VIR is above that of PIR. Especially, in our experiment some retrieved images by u sing PIR ranking strategy would never be retrieved by using VIR ranking strategy b ecause the correct annotation keyword, i.e. query term, was accidentally removed as noise. For this situation, we simply removed this image from the image pool. This should not affect our final conclusions, since this happened seldom. Due to the limitations of current techniques, image annotations have a poor performance in image retrieval systems. To mitigate thi s problem, we propose an model which scores each annotated concept using semantic sim ilarity measure based on knowledge-based lexicon WordNet. The experimental results show that the precision is improved to some extent. Moreover, after re-ranking, most correctly annotated images are associated with higher rank. In real life it is reasonable since users often are interest to a first couple of retrieval results. However, some problems still need be further researched. The ex-perimental training data has a limited size of vocabulary, so the annotation results have a low coverage over total keyword space. In addition, the evaluation of image retrieval is conducted on a small set of retrieved results using only top 25 terms, since judging relevancy/irrelevancy to test queries requires substantive human endeavors. A wider evaluation on larger data set will be carried out in future work
