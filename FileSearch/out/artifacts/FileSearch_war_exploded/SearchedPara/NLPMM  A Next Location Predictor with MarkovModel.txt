 The prevalence of positioning technology has made it possible to track the movements of people and other objects, giving rise to a variety of location-based applications. For example, GPS tracking using positioning devices installed on the vehicles is becom-ing a preferred method of taxi cab fleet management. In many social network applica-tions (e.g., Foursquare), users are encouraged to share their locations with other users. Moreover, in an increasing number of cities, vehicles are photographed when they pass the surveillance cameras installed over highways and streets, and the vehicle passage records including the license plate numbers, the time, and the locations are transmitted to the data center for storage and further processing.

In many of these location-based applications, it is highly desirable to be able to ac-curately predict a moving object X  X  next location. Consider the following example in location-based advertising. Lily has just s hared her location with her friends on the so-cial network website. If the area she will pass by is known in advance, it is possible to push plenty of information to her, such as the most popular restaurant and the products on sale in that area. As another example, if we could predict the next locations of vehi-cles on the road, then we will be able to fo recast the traffic conditions and recommend more reasonable routes to drivers to avoid or alleviate traffic jams.

Several methods have been proposed to predict next locations, most of which fall into one of two categories: (1) methods that use only the historical trajectories of individual objects to discover individual movement patterns [7, 12], and (2) methods that use the historical trajectories of all objects to identify collective movement patterns [10, 11]. The majority of the existing methods train models based on frequent patterns and/or association rules to discover movement patterns for prediction.

However, there are a few major problems with the existing methods. First, those methods focus on either the individual patterns or the collective patterns, but very of-ten the movements of objects reflect both individual and collective properties. Second, in some circumstances (e.g., social check-in, and vehicle surveillance), the data points are very sparse; the trajectories of some objects may consist of only one record. One cannot construct meaningful frequent patterns with these trajectories. Finally, the ex-isting methods do not give proper consideration to the time factor. Different movement patterns exist in different time, for example, Bob is going to leave his house. If it is 8 a.m. on a weekday, he is most likely to go to work. But if it is 11:30 a.m., he is more likely to go to a restaurant, and he may go shopping if it is 3 p.m on weekends. Failing to take time factor into account would result in higher error rates in predicting the next locations.

To address those problems, we propose a Next Location Predictor with Markov Mod-eling ( NLPMM ) to predict the next locations of moving objects given past trajectory sequences. NLPMM builds upon two models: the Global Markov Model ( GMM )and the Personal Markov Model ( PMM ). GMM utilizes all available trajectories to discover global behaviours of the moving objects based on the assumption that they often share similar movement patterns (e.g., people driving from A to B often take the same route). PMM , on the other hand, focuses on modelin g the individual patterns of each mov-ing object using its own past trajectories. The two models are combined using linear regression to produce a more comp lete and accurate predictor.

Another distinct feature of NLPMM lies in its treatment of the time factor. The move-ment patterns of objects vary from one time period to another (e.g., weekdays vs. week-ends). Meanwhile, similarities also exist fo r different time periods (e.g., this Monday and next), and the movement patterns of moving objects tend to be cyclical. We thus propose to cluster the time periods based on the similarity in movement patterns and build a separate model for each cluster.

The performance of NLPMM is evaluated in a real dataset consisting of the vehicle passage records over a period of 31 days (1 /1/2013 -1/31/2013) in a metropolitan area. The experimental results confirm the superiority of the proposed methods over existing methods.

The contributions of this paper can be summarized as follows.  X  We propose a Next Location Predictor with Markov Modeling to predict the next  X  Based on the important observation that the movement patterns of moving objects  X  We conduct extensive experiments using a real dataset and the results demonstrate The remainder of this paper is organized as follows. Section 2 reviews related work. Section 3 gives the preliminaries of our work. Section 4 describes our approach of Markov modeling. Section 5 presents methods that take the time factor into consider-ation. The experimental results and perform ance analysis are presented in Section 6. Section 7 concludes this paper. There have appeared a considerable body of work on knowledge discovery from trajec-tories, where a trajectory is defined as a sequence of locations ordered by time-stamps. In what follows, we discuss three categories of studies that are most closely related to us.

Route planning: Several studies use GPS trajectories for route planning through con-structing a complete route [2, 3, 16]. Chen et al. search the k Best-Connected Trajecto-ries from a database [3] and discover the most popular route between two locations [2]. Yuan et al. find the practically fastest route to a destination at a given departure time using historical taxi trajectories [16].

Long-range prediction: Long-range prediction is studied in [5, 8], where they try to predict the whole future trajectory of a moving object. Krumm proposes a Simple Markov Model that uses previously traversed road segments to predict routes in the near future [8]. Froehlich and Krumm use previous GPS traces to make a long-range prediction of a vehicle X  X  trajectory [5].

Short-range prediction: Short-range prediction has been widely investigated [7, 10 X 12], which is concerned with the prediction of only the next location. Some of these methods make prediction with only the individual movements [7, 12], while others use the historical movements of all the moving objects [10, 11]. Xue et al. con-struct a Probabilistic Suffix Tree (PST) for each road using the taxi traces and propose a method based on Variable-order Markov Models (VMMs) for short-term route predic-tion [12]. Jeung et al. present a hybrid prediction model to predict the future locations of moving objects, which combi ne predefined motion functi ons using the object X  X  recent movements with the movement patterns of the object [7]. Monreale et al. use the previ-ous movements of all moving objects to build a T-pattern tree to make future location prediction [10]. Morzy uses a modified version of the PrefixSpan algorithm to discover frequent trajectories and movement rules with all the moving objects X  locations [11].
In addition to the three aforementioned categories of work, there has also appeared work on using social-media data for trajectory mining [9, 13]. Kurashima et al. recom-mend travel routes based on a large set of geo-tagged and time-stamped photographs [9]. Ye et al. utilize a mixed Hidden Markov Model to predict the category of a user X  X  next activity and then predict a location given the category [13]. In this section, we will explain a few terms that are required for the subsequent discus-sion, and define the problem addressed in this paper.
 Definition 1 (Sampling Location). For a given moving object o , it passes through a set of sampling locations , where each sampling location refers to a point or a region (in a two-dimensional area of interest) where the position of o is recorded.

For example, the positions of the cameras in the traffic surveillance system can be considered as the sampling locations.
 Definition 2 (Trajectory Unit). For a given moving object o ,a trajectory unit , denoted by u , is the basic component of its trajectory. Each trajectory unit u can be represented by ( u.l, u.t ) ,where u.l is the id of the sampling location of the moving object at time-stamp u.t .
 Definition 3 (Trajectory). For a moving object, its trajectory T is defined as a time-ordered sequence of trajectory units: &lt;u 1 ,u 2 ,...,u n &gt; .
 ( u n .l, u n .t ) &gt; where u i .t &lt; u i +1 .t ( 1 i n Definition 4 (Candidate Next Locations). For the sampling location u i .l , we define a sampling location u j .l as a candidate next location of u i .l if a moving object can reach u .l from u i .l directly.
 The set of candidate next locations can be obtained either by prior knowledge (e.g., locations of the surveillance cameras combined with the road network graph), or by induction from historical trajectories of moving objects.
 Definition 5 (Sampling Location Sequence). For a given trajectory &lt; ( u 1 .l, u 1 .t ) , ( u sampling locations appearing in the trajectory, denoted as &lt;u 1 .l, u 2 .l,...,u n .l &gt; . Definition 6 (Prefix Set). For a sampling location u i .l and a given set of trajectories T , its prefix set of size N , denoted by S N i , refers to the set of sequences such that each sequence is a length N subsequence that i mmediately precedes u i +1 .l in the sampling location sequence of some trajectory T  X  X  . We choose to use Markov models to solve the next location prediction problem. Specif-ically, a state in the Markov model corresponds to a sampling location, and state transi-tion corresponds to moving from one sampling location to the next.

In order to take into consideration both the collective and the individual movement patterns in making the prediction, we propose two models, a Global Markov Model ( GMM ) to model the collective patterns, and a Personal Markov Model ( PMM ) to model the individual patterns and solve the problem of data sparsity. They are combined using linear regression to generate a predictor. 4.1 Global Markov Model Using historical trajector ies, we can train an order-N GMM to give a probabilistic pre-diction over the next sampling locations for a moving object, where N is a user-chosen parameter. Let P ( l i ) represents a discrete probability of a moving object arriving at sampling location l i . The order-N GMM implies that the probability distribution P ( l ) for the next sampling location l of a given moving object o is independent of all but the immediately preceding N locations that o has arrived at:
For a given trajectory dataset, an order-N GMM for the sampling location l i can be trained in the following way. We first construct the prefix set S N i . Next, for every prefix in prefix in the dataset. These frequencies are then normalized to get a discrete probability distribution over the next sampling location.

Westartwithafirstorder GMM , followed by a second-order GMM , etc., until the order-N GMM has been obtained, to train a variab le-order GMM. In contrast to the order-N GMM , the variable-order GMM learns such conditional distributions with a varying N and provides the means of capturing different orders of Markov dependen-cies based on the observed data. There exist many ways to utilize the variable-order GMM for prediction. Here we adopt the principle of longest match. That is, for a given sampling location sequence ending with l i , we find its longest suffix match from the set of sequences in the prefix set of l i . 4.2 Personal Markov Model The majority of people X  X  movements are routine (e.g., commuting), and they often have their own individual movement patterns. In addition, about 73% of trajectories in our dataset contain only one point, but they also can reflect the characteristics of the moving objects X  activities. For example, someone who lives in the east part of the city is unlikely to travel to a supermarket 50 kilo-meters away from his home. Therefore, we propose a Personal Markov Model ( PMM ) for each moving object to predict next locations.
The training of PMM consists of two parts: training a variable-order Markov model for every moving object using its own trajectories of length than 1, and a zero-order Markov model for every moving object using the trajectory units.

For training the variable-order Markov model, we construct the prefix set for every moving object using its own trajectories, and then we compute the probability distri-bution of the next sampling locations. Specially, we iteratively train a variable-order Markov model with order i ranging from 1 to N using the trajectories of one moving object.

We train a zero-order Markov model using the trajectory units. For a moving object, let N ( l ) denotes the number of times a sampling location l appears in the training trajectories. Let L l be the set of distinct sampling locations appearing in the training trajectories. Then we have The zero-order Markov model can be seamlessly integrated with the variable-order Markov model to obtain the final PMM . 4.3 Integration of GMM and PMM There are many methods to combine the results from a set of predictors. For our prob-lem, we choose to use linear regression to integrate the two models we have proposed.
For the given i -th trajectory sequence, bot h GMM and PMM can get a vector of probabilities, p w i = p i 1 ,p i 2 ,  X  X  X  ,p i m ( w =1 for GMM and w =2 for PMM ), where m is the number of the sampling locations, and p i j is the probability of location j being for the i -th trajectory sequence, where y i j =1 if the actual next location is j and 0 otherwise. We can predict y i through a linear combination of the vectors generated by GMM and PMM : where 1 is a unit vector, and  X  0 ,  X  1 ,and  X  2 are the coefficients to be estimated.
Given a set of n training trajectories, we can compute the optimal values of  X  i Euclidean norm. The  X  i values thus obtained can then be used for prediction. For a par-ticular trajectory, we can predict the top k next sampling locations by identifying the k largest elements in the estimator  X  y . The movement of human beings demonstrates a great degree of temporal regularity [1, 6]. In this section, we will first discuss how the movement patterns are affected by time, and then show how to improve the predi ctor proposed in the preceding section by taking the time factor into consideration. 5.1 Observations and Discussions We illustrate how time could affect people X  X  movement patterns through Figure 1. In this case, for a sampling location l , there are seven candidate next locations, and the distributions over those locations do differ from one period to another. For instance, vehicles are most likely to arrive at the fifth location during the period from 9:00 to 10:00, whereas the most probable next location is the second for the period from 14:00 to 15:00.

Therefore, the prediction model should be made time-aware, and one way to do this is to train different models for different time periods. In what follows, we will explore a few methods to determine the suitable time periods. Here, we choose day as the whole time span, i.e., we study how to find movement patterns within a day. However, any other units of time, such as hour , week or month , could also be used depending on the scenario. 5.2 Time Binning A straight-forward approach is to partition the time span into a given number ( M )of equi-sized time bins, and all trajectorie s are mapped to those bins according to their time stamps. A trajectory spanning over more than one bin is split into smaller sub-trajectories such that the trajectory units in each sub-trajectory all fall in the same bin. We then train M independent models, each for a differen t time bin, using the trajectories falling in each bin. Prediction is done by choosing the right model based on the time-stamp. We call this approach Time Binning (TB) .

However, this approach has some limitations: the sizes of all time bins are equal, rendering it difficult to find the correct bin sizes that fit all movement patterns in the time span, as some patterns manifest themselves over longer periods whereas others shorter. One possible improvement to TB is to start with a small bin size, and gradually merge the time bins whose distributions are considered similar by some metric. 5.3 Distributions Clustering We propose a method called Distributions Clustering ( DC ) to perform clustering of the time bins based on the similarities of the probability distributions in each bin. Here, the probability distribution refers to the transition probability from one location to another. Compared with TB , the trajectories having similar probability distributions are expected to be put in one cluster, leading to clearer revelation of the moving patterns. Here, we use cosine similarity to measure the similarities between the distributions, but the same methodology still applies when other distan ce metrics such as the Kullback-Leibler divergence [4] are used.

For an object o appearing at a given sampling location l with a time point falling into the i th time bin, let p m i be an m -dimensional vector that represents the probabilities of o moving from l to another location, where m is the total number of sampling locations. We measure the similarity of two time bins i and j (with respect to o ) using the cosine can perform clustering for each sampling location l on the time bins. The algorithm is detailed in Algorithm 1. The results will b e a set of clusters, each containing a set of time bins, for the sampling location l .
 Algorithm 1. DC: Detecting Q clusters for the M time bins For a given location l i , we can get Q clusters, defined as C k i ,k =1 , 2 ,  X  X  X  ,Q . Combined with the order-N Markov model, the probability distribution P ( l ) for the next sampling location l of a given moving object o can be computed with the formula: We then train Q models with the trajectories in each cluster to form a new model NLPMM-DC (which stands for NLPMM with Distributions Clustering ). In the new model, the sequence of just-passed locations and the time factor are both utilized by combing distributions clustering and Markov model. We have conducted extensive experiments to evaluate the performance of the proposed NLPMM using a real vehicle passage dataset. In this section, we will first describe the dataset and experimental settings, followed by the evaluation metrics to measure the performance. We then show the experimental results. 6.1 Datasets and Settings The dataset used in the experiments consists of real vehicle passage records from the traffic surveillance system of a major met ropolitan area with a 6-million population. The dataset contains 10,344,058 records durin g a period of 31 days (from January 1, 2013 to January 31, 2013). Each record contains three attributes, the license plate number of the vehicle, the ID of the location of the surveillance camera, and the time of vehicle passing the location. There are about 300 camera locations on the main roads. The average distance between a neighboring pair of camera locations is approximately 3 kilometers. 6.2 Pre-processing We pre-process the dataset to form trajectories, resulting in a total of 6,521,841 tra-jectories. According to statistics, the trajectories containing only one point account for about 73% of all trajectories, which testifies to the sparsity of data sampling. We choose a total of 1,760,897 trajectories with the length greater than one to calculate the number of candidate next locations for every sampling location. Due to the sparsity of cam-era locations, about 86.3% of the sampling locations have more than 10 candidate next sampling locations, and the average numbe r of candidate next locations is about 43. We predict top-k next sampling locations in the experiments. 6.3 Evaluation Metrics Our evaluation uses the following metrics that are widely employed in multi-label clas-sification studies [14].

Prediction Coverage: It is defined as the percentage of trajectories for which the next location can be predicted based on the model. Let c ( l ) be 1 if it can be predicted and 0 otherwise. Then PreCov T = l  X  X  c ( l ) / |T | ,where |T | denotes the total number of trajectories in the testing dataset.

Accuracy: It is defined as the frequency of the true next location occurring in the list of predicted next locations. Let p ( l ) be 1 it does and 0 otherwise. Then accuracy T =
One-error: It is defined as the frequency of the top-1 predicted next location not being the same as the true next location. Let e ( l ) be 0 if the top-1 predicted sampling location is the same as the true next location and 1 otherwise. Then one  X  error T = |
Average Precision: Given a list of top-k predicted next locations, the average pre-cision is defined as AvePrec T = l  X  X  ( p ( i ) /i ) / |T | ,where i denotes the position in the predicted list, and p ( i ) takes the value of 1 if the predicted location at the i -th position in the list is the actual next location. 6.4 Evaluation of NLPMM We evaluate the performance of NLPMM and its components, PMM ,and GMM .For each experiment, we perform 50 runs and report the average of the results. First, we study the effect of the order of the Markov model by varying N from 1 to 6. Figure 2(a) shows that the accuracy has an apparent improvement when the order N increases from 1 to 2 for all models. The accura cy reaches the maximum when N is set to 3 and remains stable as N increases further. Therefore, we set N to 3 in the following experiments. Next, we evaluate the effect of top k on PMM , GMM ,and NLPMM . From Figure 2(b), we can observe that the accuracy of all three models improves as k increases. Further-more, the accuracy of GMM and NLPMM is significantly better than that of PMM , and the best results are given by NLPMM . Since the average number of candidate next locations is 43 (meaning there are 43 possibilities), the accuracy of 0.88 is surprisingly good when k is set to 10. 6.5 Effect of the Time Factor We evaluate the proposed methods that take into consideration of the time factor. Fig-ure 3(a) shows the effect of bin size on NLPMM-TB (which stands for NLPMM with Time Binning ). The performance of NLPMM-TB starts to deteriorate when the bin size becomes less than 8, because when the bins get smaller, the trajectories in them become too sparse to generate a meaningful collect ive pattern. Figure 3(b) shows the effect of the number of clusters on NLPMM-DC (which stands for NLPMM with Distributions Clustering ). When it is set to 1, the model is the same as NLPMM . The one-error rate declines and the average precision impr oves as the number increases from 1 to 5. When it continues to increase, the result starts t o get worse. This is because having too many or too few clusters with either hurt the cohesiveness or the separation of the clusters.
We evaluate the performance of NLPMM , NLPMM-TB and NLPMM-DC using one-error and average precision. The results are shown in Table 1. NLPMM-TB and NLPMM-DC perform better than NLPMM , which is because we can get a more refined model by adding the time factor and gener ate more accurate predictions. NLPMM-DC performs best, validating the effectiveness of the method of distributions clustering. It will be used in the following comparison with alternative methods. 6.6 Comparison with Existing Methods We compare the proposed NLPMM-DC with the start-of-the-art approaches VMM [12] and WhereNext [10]. VMM uses individual trajectories to predict the next locations, whereas WhereNext uses all available trajectories to discover collective patterns. In this experiment, we predict top-1 next sampling location. The parameters of VMM are set as follows: memory length N =3,  X  =0.3, and N min =1. For WhereNext , the support for constructing T-pattern tree is set as 20. For the NLPMM-DC , the setting is that the order N = 3 and the number of clusters is set at 5.
Figure 4 shows the performance comparison of NLPMM-DC , VMM and WhereNext in terms of prediction coverage and accuracy. As shown in Figure 4(a), NLPMM-DC performs the best, which can be attributed to the combination of individual and collec-tive patterns as well as the consideration of time factor. Figure 4(b) shows that the accu-racy of each model improves as the size of trai ning set increases. It is worth mentioning that NLPMM-DC performs better than VMM and WhereNext in terms of accuracy for any training set size. In this paper, we have proposed a Next Location Predictor with Markov Modeling to predict the next sampling location that a moving object will arrive at with a given trajec-tory sequence. The proposed NLPMM consists of two models: Global Markov Model and Personal Markov Model. Time factor is also added to the models and we propose two methods to partition the whole time span i nto periods of finer granularities, includ-ing Time Binning and Distributions Clustering. New time-aware models are trained ac-cordingly. We have evaluated the proposed models using a real vehicle passage record dataset. The experiments show that our predictor significantly outperforms the state-of-the-art methods ( VMM and WhereNext ).
 Acknowledgement. This work was supported in part by the National Natural Science Foundation of China Grant (No. 61272092), the Program for New Century Excellent Talents in University (NCET-10-0532), the Natural Science Foundation of Shandong Province of China Grant (No. ZR2012FZ004), the Independent Innovation Founda-tion of Shandong University (2012ZD012), the Taishan Scholars Program, and NSERC Discovery Grants. The authors would like to thank the anonymous reviewers, whose valuable comments helped improve this paper.

