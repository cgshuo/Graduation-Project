 University of Freiburg, Freiburg, GERMANY Holger Hoos HOOS @ CS . UBC . CA University of British Columbia, Vancouver, CANADA Kevin Leyton-Brown KEVINLB @ CS . UBC . CA University of British Columbia, Vancouver, CANADA Machine learning algorithms often behave very differently with different instantiations of their hyperparameters. This is true especially for complex model families, such as deep belief networks (Hinton et al. , 2006), convolutional networks (LeCun et al. , 2001), stacked denoising autoen-coders (Vincent et al. , 2010), and computer vision archi-tectures (Bergstra et al. , 2013), all of which have tens up to hundreds of hyperparameters. Since hyperparameter set-tings often make the difference between mediocre and state-of-the-art performance, and since naive hyperparameter op-timization methods, such as grid search, do not scale to many dimensions, there has been a recent surge of interest in more sophisticated hyperparameter optimization meth-ods (Hutter et al. , 2011; Bergstra and Bengio, 2012; Bergstra et al. , 2011; Snoek et al. , 2012; Bardenet et al. , 2013). In low-dimensional problems with numerical hyperparameters, the best available hyperparameter optimization methods use Bayesian optimization (Brochu et al. , 2009) based on Gaussian process models, whereas in high-dimensional and discrete spaces, tree-based models (Bergstra et al. , 2011), and in particular random forests (Hutter et al. , 2011; Thorn-ton et al. , 2013; Gramacy et al. , 2013), are more success-ful (Eggensperger et al. , 2013).
 Such modern hyperparameter optimization methods have achieved considerable recent success. For example, Bayesian optimization found a better instantiation of nine convolutional network hyperparameters than a domain ex-pert, thereby achieving the lowest error reported on the CIFAR-10 benchmark at the time (Snoek et al. , 2012). In high-dimensional hyperparameter optimization problems, recent success stories include (1) a new best result for the MNIST rotated background images dataset in 2011 using an automatically configured deep belief network with 32 hy-perparameters (Bergstra et al. , 2011); (2) a complex vision architecture with 238 hyperparameters that can be instanti-ated to yield state-of-the-art performance for such disparate tasks as face-matching verification, face identification, and object recognition (Bergstra et al. , 2013); and (3) Auto-WEKA, a framework enabling per-dataset optimization over a 768-dimensional space including all model classes and hyperparameters defined in WEKA (Thornton et al. , 2013). A similar development can be observed in the area of com-binatorial optimization, where automated hyperparameter optimization approaches have recently led to substantial improvements of high-performance heuristic algorithms for a wide range of problems, including propositional satis-fiability (Hutter et al. , 2007; KhudaBukhsh et al. , 2009), mixed integer programming (Hutter et al. , 2009), answer set programming (Silverthorn et al. , 2012) and the travel-ing salesman problem (Styles and Hoos, 2013). 1 While traditional hyperparameter optimization methods in that community are based on heuristic search (Adenso-Diaz and Laguna, 2006; Nannen and Eiben, 2007; Hutter et al. , 2009; Ansotegui et al. , 2009) and racing algorithms (Maron and Moore, 1994; Birattari et al. , 2010), recently, Bayesian opti-mization methods based on random forest models have been shown to compare favourably (Hutter et al. , 2011). The considerable success of Bayesian optimization for de-termining good hyperparameter settings in machine learning and combinatorial optimization has not yet been accompa-nied by much work on methods for providing scientists with answers to questions like the following: How important is each of the hyperparameters, and how do their values affect performance? Which hyperparameter interactions matter? How do the answers to these questions depend on the data set under consideration? The answer to such questions is the key to scientific dis-coveries, and consequently, recent Bayesian optimization workshops at NIPS have identified these topics as a core area in need of increased attention. Recent work on Bayesian optimization has targeted the case where most hyperparam-eters are truly unimportant (Chen et al. , 2012; Wang et al. , 2013), and several applications have yielded evidence that some hyperparameters indeed tend to be much more impor-tant than others (Bergstra and Bengio, 2012; Hutter et al. , 2013). However, not much work has been done on quantify-ing the relative importance of the hyperparameters that do matter.
 In this paper, we investigate the classic technique of func-tional analysis of variance (functional ANOVA) (Sobol, 1993; Huang, 1998; Jones et al. , 1998; Hooker, 2007) to decompose the variance V of a blackbox function f :  X  1  X  X  X  X  X   X  n  X  R into additive components V U associ-ated with each subset of hyperparameters U  X  X  1 ,...,n } . In our case, f is our algorithm X  X  performance with hyper-parameter settings  X  . As is standard, we learn a predictive model  X  f of f and partition the variance of  X  f . In order to do this tractably, we must be able to efficiently com-pute marginalizations of  X  f over arbitrary input dimensions T  X  { 1 ,...,n } . This has been shown to be possible for Gaussian process models  X  f with certain kernels (see, e.g., Jones et al. , 1998). However, here, we are most interested in random forest models, since these have been shown to achieve the best performance for model-based optimization in complex hyperparameter spaces, particularly in cases in-volving categorical and conditional hyperparameters (Thorn-ton et al. , 2013; Eggensperger et al. , 2013). To date, efficient marginalizations had not been available for random forest models, forcing researchers to revert to sampling-based techniques to compute approximate functional ANOVA de-compositions (Gramacy et al. , 2013). Here, we provide the first efficient and exact method for deriving functional ANOVA sensitivity indices for random forests.
 When applying this new method to quantify the importance of the hyperparameters of machine learning algorithms and combinatorial optimization procedures, following Hutter et al. (2011), we consider a setting slightly more general than blackbox function optimization: given an algorithm A with configuration space  X  , a set of training scenarios  X  ,..., X  k , and a performance metric m (  X  , X  ) capturing A  X  X  performance with hyperparameter configurations  X   X   X  on scenario  X  , find a configuration  X   X   X  that minimizes m over  X  1 ,..., X  k , i.e. , that minimizes the function In the case of hyperparameter optimization of machine learn-ing algorithms, the  X  i are typically cross-validation folds, and for combinatorial problem solving procedures, they are problem instances deemed representative for the kind of instances we aim to optimize performance for.
 In the following, we first introduce our new, linear-time algorithm for computing the marginals of random forest predictions (Section 2). We then show how this algorithm can be leveraged to tractably identify main and (low-order) interaction effects within the functional ANOVA framework (Section 3). Finally, we demonstrate the power of this ap-proach through an extensive experimental evaluation, using highly parametric machine learning frameworks and combi-natorial solvers for NP-hard problems (Section 4). Algorithm designers wanting to manually assess hyperpa-rameter importance often investigate the local neighbour-hood of a given hyperparameter configuration: vary one hyperparameter at a time and measure how performance changes. Note that the only information obtained with this analysis is how different hyperparameter values perform in the context of a single instantiation of the other hyperparam-eters. Optimally, algorithm designers would like to know how their hyperparameters affect performance in general, not just in the context of a single fixed instantiation of the remaining hyperparameters, but across all their instantia-tions. Unfortunately, performing algorithm runs for all these instantiations is infeasible in all but the easiest cases, since there are D k such instantiations of k discrete hyperparam-eters with domain size D . (Continuous hyperparameters are even worse, having infinitely many instantiations.) As it turns out, an approximate analysis based on predictive models can be used to solve this problem and quantify the performance of a hyperparameter instantiation in the con-text of all instantiations of the other hyperparameters . In this section, we will show that this marginal performance of a partial hyperparameter instantiation can be predicted by computing the required exponential (or infinite) sum of predictions in linear time. We first cover some notation and define the problem formally. Then, we introduce an algo-rithm to predict this marginal performance using random forests and prove its correctness and linear time complexity. 2.1. Problem Definition We begin with some basic definitions. Let A be an algorithm having n hyperparameters with domains  X  1 ,...,  X  n . We use integers to denote the hyperparameters, and N to refer to the set { 1 ,...,n } of all hyperparameters of A . Definition 1 (Configuration space  X  ) . A  X  X  configuration space is  X  =  X  1  X  X  X  X  X   X  n .
 Definition 2 (Hyperparameter Instantiation) . A complete instantiation of an algorithm X  X  n hyperparameters is a vec-tor  X  =  X   X  1 ,..., X  n  X  with  X  i  X   X  i . We also refer to complete hyperparameter instantiations as hyperparam-eter configurations . A partial instantiation of a subset U = { u 1 ,...,u m } X  N of A  X  X  hyperparameters is a vec-tor  X  U =  X   X  u 1 ,..., X  u m  X  with  X  u i  X   X  u i . The extension set of a partial hyperparameter instantiation is the set of hyperparameter configurations that are consistent with it.
 Definition 3 (Extension Set) . Let  X  U =  X   X  u 1 ,..., X  u be a partial instantiation of the hyperparameters U = { u 1 ,...,u m }  X  N . The extension set X (  X  U ) of  X  U is then the set of hyperparameter configurations  X  N | U  X   X  1 ,..., X  To handle sets of hyperparameter configurations with a mix of continuous and categorical hyperparameters, we define the range size of a set.
 Definition 4 (Range size) . The range size || S || of an empty set S is defined as 1; for other finite S , the range size equals the cardinality: || S || = | S | . For closed intervals S = [ l,u ]  X  R with l &lt; u , || S || = u  X  l . For cross-products S = S 1  X  X  X  X  X  S k , || S || = Q k i =1 || S i || . The probability density of a uniform distribution over X (  X  U ) is then simply 1 / || X (  X  U ) || = 1 / ||  X  T = { t 1 ,...,t k } = N \ U and  X  T =  X  t 1  X  X  X  X  X   X  t Now, we can define the marginal (predicted) performance of a partial instantiation  X  U as the expected (predicted) per-formance of A across X (  X  U ) .
 Definition 5 (Marginal performance) . Let A  X  X  (true) per-formance be y :  X  7 X  R , U  X  N , and T = N \ U . A X  X  marginal performance a U (  X  U ) is then defined as Similarly, A X  X  marginal predicted performance  X  a U (  X  U der a model  X  y :  X   X  R is Note that if the predictive model  X  y has low error on aver-age across the configuration space, the difference between predicted and true marginal performance will also be low. 2.2. Efficient Computation of Marginal Predictions in In this section, we show that when using random forest predictors  X  y , the marginal predicted performance  X  a U defined in Eq. 1 can be computed exactly in linear time. The fact that this can be done for random forests is important for our application setting, since random forests yield the best performance predictions for a broad range of highly parameterized algorithms (Hutter et al. , 2014).
 Random forests (Breiman, 2001) are ensembles of regres-sion trees. Each regression tree partitions the input space through sequences of branching decisions that lead to each of its leaves. We denote this partitioning as P . Each equiva-lence class P i  X  X  is associated with a leaf of the regression tree and with a constant c i . Let  X  ( i ) j  X   X  j denote the subset of domain values of hyperparameter j that is consistent with the branching decisions leading to the leaf associated with P . Then, for trees with axis-aligned splits, P i is simply the Cartesian product The predictor  X  y :  X   X  R encoded by the regression tree is where I ( x ) is the indicator function. Random forests simply predict the average of their individual regression trees. Our approach for computing marginal predictions  X  a U (  X  of a random forest works in two phases: a preprocessing phase that has to be carried out only once and a prediction phase that has to be carried out once per requested marginal prediction. Both phases require only linear time given a ran-dom forest model as input (constructing the random forest is a separate problem, but is also cheap: for T data points of dimensionality n , it is O ( n  X  T 2 log T ) in the worst case and O ( n  X  T log 2 T ) in the more realistic best case of balanced trees (Hutter et al. , 2014)).
 The key idea behind our algorithm is to exploit the fact that each of the regression trees in a given forest defines a partitioning P of the configuration space  X  , with piecewise constant predictions c i in each P i  X  X  , and that the problem of computing sums over an arbitrary number of configura-tions thus reduces to the problem of identifying the ratio of configurations that fall into each partition.
 We first show that, given a partitioning P , we can compute marginal predictions as a linear sum over entries in the leaves.
 Theorem 6. Given the partitioning P of a regression tree T that defines a predictor  X  y :  X  7 X  R , and a partial instantiation  X  U of  X   X  X  hyperparameters N , T  X  X  marginal prediction  X  a U (  X  U ) can be computed as All proofs are provided in the supplementary material. Us-ing Theorem 6, we can compute arbitrary marginals by a simple iteration over the partitions by counting the ratio of hyperparameter configurations falling into each parti-tion. However, regression trees do not normally provide explicit access to these partitions; they are defined implic-itly through the tree structure. Since we need to represent the partitioning explicitly, one may worry about space com-plexity. Indeed, if we stored the values  X  ( i ) j for each leaf i and categorical hyperparameter j (as well as lower and upper bounds for continuous hyperparameters), for a ran-dom forest with B trees of L leaves each, and n categorical hyperparameters with domain size at most D , we would end up with space complexity of  X ( B  X  L  X  n  X  D ) . In the largest of the practical scenarios we consider later in this work (random forests with B = 10 trees of up to L = 100 000 leaves, configuration spaces with up to n = 768 hyperpa-rameters and domain sizes up to D = 20 ) this would have been infeasible. Instead, we show that Algorithm 1 can compute the partitioning using a pointer-based data struc-ture, reducing the space complexity to O ( B  X  L  X  ( D + n )) . (Alternatively, when space is not a concern, the partitioning can be represented using a bit mask, replacing O (log( D )) member queries with O (1) operations and thus reducing the complexity of marginal predictions for single trees from O ( L  X  n log D ) to O ( L  X  n ) .) Theorem 7. Given a regression tree T with L leaves and a configuration space  X  with n hyperparameters and cate-gorical domain size at most D , Algorithm 1 computes T  X  X  partitioning of  X  in time and space O ( L  X  D + L  X  n ) . To compute marginal predictions of random forests, we av-erage over the marginal predictions of their individual trees. We use the variance across these individual tree predictions to express our model uncertainty.
 Corollary 8. Given a random forest with B trees of up to L leaves that defines a predictor  X  y :  X   X  R for a config-uration space  X  with n hyperparameters and categorical domain size at most D , the time and space complexity of computing a single marginal of  X  y is O ( B  X  L  X  max { D + n,n log D } ) . Each additional marginal can be computed in additional space O (1) and time O ( B  X  L  X  n log D ) . In this section, we review functional analysis of variance and demonstrate how we can use our efficient marginal predictions with this technique to quantify the importance of an algorithm X  X  individual hyperparameters and of low-order interactions between hyperparameters. 3.1. Functional Analysis of Variance (ANOVA) Functional analysis of variance (functional ANOVA) is a prominent data analysis method from the statistics litera-ture (Sobol, 1993; Huang, 1998; Jones et al. , 1998; Hooker, 2007). While this method has not received much attention in the machine learning community so far, we believe that it offers great value. In a nutshell, ANOVA partitions the observed variation of a response value (here: algorithm per-formance) into components due to each of its inputs (here: hyperparameters). Functional ANOVA decomposes a func-tion  X  y :  X  1  X  X  X  X  X   X  n  X  R into additive components that only depend on subsets of its inputs N : The components  X  f U (  X  U ) are defined as follows:  X  f
U (  X  U ) = The constant  X  f  X  is the function X  X  mean across its domain. The unary functions  X  f { j } (  X  { j } ) are called main effects and capture the effect of varying hyperparameter j , averaging across all instantiations of all other hyperparameters. The functions  X  f U (  X  U ) for | U | &gt; 1 capture exactly the interac-tion effects between all variables in U (excluding all lower-order main and interaction effects of W ( U ).
 By definition, the variance of  X  y across its domain  X  is and functional ANOVA decomposes this variance into con-tributions by all subsets of variables (see, e.g., Hooker, 2007, for a derivation): V = X The importance of each main and interaction effect  X  f U can thus be quantified by the fraction of variance it explains: F
U = V U / V . 3.2. Variance Decomposition in Random Forests In Algorithm 2, we use our efficient marginal predictions from Section 2.2 to quantify the importance of main and interaction effects of random forest predictors in the func-tional ANOVA framework of Eq. 6.
 Theorem 9. Given a configuration space  X  consisting of n categorical hyperparameters 2 of maximal domain size D and a regression tree T with L leaves that de-fines a predictor  X  y :  X   X  R , Algorithm 2 exactly com-putes the fractions of variance explained by all subsets U of  X   X  X  hyperparameters N of arity up to K , with space complexity O ( L  X  D + L  X  n ) and time complexity O L  X  D + P K k =1 n k  X  D k ( L  X  n log d + 2 k ) . To compute hyperparameter importance in random forests, we simply apply Algorithm 2 for each tree and compute means and standard deviations across the trees. 3.3. Practical Use for Identifying Hyperparameter Note that Theorem 9 assumes regression tree predictors  X  y :  X   X  R that predict the performance of hyperparameter configurations and do not mention training scenarios (such as cross-validation folds in machine learning, or problem instances when optimizing combinatorial solvers). How-ever, the performance data logged in modern Bayesian optimization methods consists of algorithm runs on sin-gle training scenarios:  X  X lgorithm run t used configuration  X  on fold/instance  X  t and achieved performance y t  X . We thus construct random forest predictors  X  y 0 :  X   X   X   X  R and use them to predict, for every unique  X  in our training data, the average performance  X  m  X  = 1 /m P k i =1 m (  X  , X  across training scenarios  X  1 ,..., X  k . Then, we learn ran-dom forests  X  y :  X   X  R using tuples (  X  ,  X  m  X  ) as training data. Our variance decomposition then applies directly to the trees in these forests.
 While some algorithm designers are interested in the global effect of a hyperparameter (subset) in the context of all other possible hyperparameter settings, others care more about effects in  X  X ood X  regions of the configuration space, where  X  X ood X  may have different meanings, such as  X  X t least as good as the (manually chosen) default X  or  X  X n the top X% of configurations X . We can easily support such alternative mea-sures of importance by capping the performance values of each training configuration at a given maximum y max and learning our random forest on tuples (  X  , min( m  X  ,y max instead of (  X  ,m  X  ) . The consequence is that no configura-tion is predicted to be worse than y max , and thus all remain-ing performance variation characterizes regions of the space with performance better than y max . We demonstrate our techniques by assessing the importance of hyperparameters of various machine learning algorithms and solvers for NP-hard problems. Additional details and results for all experiments are provided in the supplementary material. 4.1. Evaluation on Ground-Truth Data Our first experiment utilizes data gathered with an online variational Bayes algorithm for Latent Dirichlet Allocation (Online LDA) (Hoffman et al. , 2010) and made available as part of a previous study in Bayesian optimization for the hyperparameters of this algorithm (Snoek et al. , 2012). Complete ground truth data is available for a grid of three hyperparameters (  X  ,  X  0 , and S ), discretized to 6, 6, and 8 values, respectively;  X  controls how quickly information is forgotten,  X  0 &gt; 0 downweights early iterations, and S concerns the size of mini-batches. For each of the 6  X  8  X  6 = 288 grid points, we know the algorithm X  X  performance score (perplexity) and its training time.
 Let us assume we only have available a subset of 100 data points randomly sampled out of the 288. We can then fit a model (here, a random forest) and marginalize out the effects of all hyperparameters but one in the model. Us-ing our efficient marginalization techniques from Section 2, this approach also scales to high dimensions. Figure 1 shows the marginal perplexity that is achieved by setting each hyperparameter to a certain value (and averaging over all instantiations of the others). Clearly, the batch size hy-perparameter S is marginally most important, with large batch sizes consistently yielding lower perplexity. Indeed, functional ANOVA reveals that the batch size hyperparam-eter by itself is responsible for 65% of the variability of perplexity across the entire space. Another 18% are due to an interaction effect between S and  X  , which is visualized in Figure 2. From this figure, we note that the  X  hyperpa-rameter is much more important for small batch sizes than for large ones X  X n interaction effect that cannot be captured by single marginals. Figures 1 and 2 also compare true marginals (computed from the complete grid data) vs our predictions, verifying that the two are closely aligned. Since we also have data on the algorithm X  X  runtime with different hyperparameter settings, we can carry out exactly the same analysis to assess how runtime depends on hy-perparameter settings. As the results in Figure 3 show, hyperparameter  X  most influences runtime (causing 54% of the runtime variation), followed by the batch size hyperpa-rameter (causing 21% of the runtime variation). The batch size hyperparameter shows an interesting pattern, with high runtimes for very low or very high values and a sweet spot for intermediate values. Combining this finding with the results from Figure 1 shows that batch sizes around 1 000 yield a good compromise of speed and accuracy.
 Overall, in this experiment on ground truth data we observed that our predicted marginals and functional ANOVA analy-ses are accurate and can give interesting insights even for low-dimensional hyperparameter spaces. 4.2. Evaluation on WEKA X  X  Hyperparameter Space We now demonstrate how to use our framework for an exploratory analysis in a very high-dimensional hyperpa-rameter space, that of the machine learning framework WEKA (Hall et al. , 2009). We use the hyperparameter space defined by the recent Auto-WEKA framework (Thornton et al. , 2013), which includes not only numerical hyperpa-rameters of each of the models implemented in WEKA, but also discrete choices about which model to use, meta-classifiers, ensemble classifiers, and a multitude of feature selection mechanisms. With 768 hyperparameters, Auto-WEKA constitutes the largest configuration space of which we are aware, and it is precisely for this reason that we chose to study it: at this dimensionality, it is virtually impossible to manually keep track of each individual hyperparameter. Auto-WEKA uses tree-based Bayesian optimization meth-ods, such as TPE (Bergstra et al. , 2011) and SMAC (Hutter et al. , 2011), to search WEKA X  X  joint space of models and hyperparameters. Hyperparameter settings are evaluated by running the respective instantiations of WEKA on one or more cross-validation folds, and the resulting performance values are used to inform a sequential search process. Thorn-ton et al. (2013) made available the performance data for each of 21 datasets gathered by SMAC (the method yielding the best Auto-WEKA performance) during its optimization process.
 Table 1 lists a representative subset of those 21 data sets, along with the average number of data points gathered in a single SMAC run for the respective data set. With this data as input, we ran our functional ANOVA approach to identify the three most important hyperparameters for each dataset; we list these in Table 1 along with the fraction of variance explained by them. Not surprisingly, the most important hyperparameter was the model class used. Depending on the dataset, the second-most important hyperparameter con-cerned either feature selection or the base classifier to be used inside a meta classifier.
 Figure 4 shows the marginal performance of each model class in more detail for two representative datasets, revealing that different model classes perform very differently on different data sets. Note that performance, as shown in these plots, is marginalized over all possible instantiations of a given model, giving an advantage to methods that are robust with respect to their hyperparameter settings.
 These results demonstrate that our framework can be used out of the box for very high-dimensional hyperparameter spaces with mixed continuous/discrete hyperparameters. 4.3. Evaluation for Combinatorial Problem Solvers To demonstrate the versatility of our variance decomposition methods, we also applied them to assess hyperparameter im-portance of seven highly parametric state-of-the-art solvers for prominent combinatorial problems; in particular, we considered ten benchmarks from propositional satisfiabil-ity (SAT), mixed integer programming (MIP) and answer set programming (ASP), covering applications in formal verification, industrial process optimization, computational sustainability and database query optimization.
 As for hyperparameter optimization, we carried out an ex-periment with known ground-truth data for optimizing a combinatorial problem solver ( S PARROW , on its two bench-marks in Table 2). The results were qualitatively similar to those from Section 4.1 and are reported in the supplementary material.
 Next, we computed all main and interaction effects for all solver/benchmark combinations we considered; the left third of Table 2 summarizes the results. We note that in all cases, the main effects accounted for a substantial fraction of the overall performance variation (20 X 88%). Since single hy-perparameter effects are easier for humans to understand than complex interaction effects, this is an encouraging find-ing. These main effects were computed within seconds, meaning that algorithm designers could use our approach interactively. Finally, pairwise interaction effects were also important in several cases, explaining up to 45% of the overall performance variation.
 One particularly interesting case in Table 2 is SPEAR  X  X  performance on bounded model checking ( BMC ) instances. Here, 87% of the variance was explained by a single hy-perparameter, SPEAR  X  X  variable selection heuristic. Fig-ure 5 (left) shows that several standard activity heuristics (la-belled 0,2,3,4,5,6,7) performed well for this dataset, whereas other ad-hoc heuristics performed poorly. In contrast, for SPEAR  X  X  performance on software verification ( SWV ) in-stances (see Figure 5, right side), one of the heuristics ini-tially suspected to perform poorly turned out to be very effective. Before seeing these results, SPEAR  X  X  developer did not have any intuition about which variable selection heuristic would work well for SWV ; in particular, he did not know whether selecting variables in the order they were cre-ated (option 16, clearly the best choice) or in reverse order (option 17, one of the worst choices) would be preferable (personal communication). Our result helped him realize that the SAT-encoding used in these SWV instances creates important propositional variables first.
 Next, we evaluated hyperparameter importance in the  X  X ood X  parts of the space, as described in Section 3.3. We used two alternative notions of good configurations: (1) being in the top 25% in terms of performance, and (2) beating the algo-rithm default. Table 2 (middle and right) shows that single marginals still explain a sizable fraction of the variance in this good part of the space, but less than in the entire space. A closer inspection of extreme cases, such as C PLEX -CLS , showed that in these cases, most of the overall variance was explained by one or more hyperparameters that were best left at their defaults. However, for explaining improvements over the default, these hyperparameters were useless, since none of their values achieved such improvements. 4.4. Configuration in the Subspace of Important We can also use our variance decomposition techniques to identify a hyperparameter subspace that captures most of the potential for improvements over a good configuration (e.g., the default or the 25% quantile as in Section 4.3). To verify that, by doing so, we indeed capture the potential for improvement, we compare the results found by running the algorithm configuration SMAC in that subspace and in the full space. We performed this experiment for a neural net-work (8 hyperparameters; subspace: 3 hyperparameters) and for the mixed integer programming solver C PLEX (76 hyper-parameters; subspace: 10 hyperparameters). As the results in Figure 6 illustrate, configuration in the subspace yielded good results and often did so faster than in the full space. Note that these results do not imply that we have defined an improved configuration procedure for small configuration budgets: the models we used to identify the important sub-spaces were fit on data gathered by first running SMAC in the complete space. However, they do confirm that our func-tional ANOVA approach rates as important hyperparameters that capture the potential for improvement. In this work, we introduced an efficient approach for as-sessing the importance of the inputs to a blackbox function, and applied it to quantify the effect of algorithm hyperpa-rameters. We first derived a novel linear-time algorithm for computing marginal predictions over arbitrary input dimen-sions in random forests and then showed how this algorithm can be used to quantify the importance of main effects and interaction effects through a functional ANOVA framework. We empirically validated our approach on performance data from several well-known machine learning frameworks and from state-of-the-art solvers for several prominent combina-torial problems. We confirmed our predictions using ground truth data and showed how our approach can be used to gain insights into the relevance of hyperparameters. We also demonstrated that performance variability is often largely caused by few hyperparameters that define a subspace to which we can restrict configuration.
 The methods introduced in this work offer a principled, sci-entific way for algorithm designers and users to gain deeper insights into the way in which design choices controlled by hyperparameters affect the overall performance of a given algorithm. In future work, we plan to extend our approach to detect dominated hyperparameter values and interactions between instance characteristics and hyperparameter set-tings. We also plan to develop configuration procedures that determine important hyperparameters on the fly and exploit this information to speed up the optimization process. Our implementation, along with a quick start guide showing how to apply it to your own algorithms, is publicly available at www.automl.org/fanova .

