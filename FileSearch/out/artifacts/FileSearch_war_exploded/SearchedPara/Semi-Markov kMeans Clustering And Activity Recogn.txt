
Clustering of subsequences of time series is a widely studied and applied class of techniques [1], [2], [3], [4], [5], [6], [7] which aims at interpreting a time series as a shorter sequence of symbols, each of which represent a segment of the series with a pattern that is similar to other segments that have been assigned the same symbol. The authors of this paper are particularly motivated by recognition of human physical activities from body worn sensors such as accelerometers. In this case the goal is to translate the series of three dimensional acceleration observations into a series of primitive actions (activity atoms). Such a problem has recently been successfully addressed  X  X npublished X  [8] by using discriminative semi-Markov models trained using a max-margin criteria [9]. Discriminative semi-Markov mod-els have been applied with strong results in other areas, e.g. named entity recognition [10], automatic paragraph segmentation of text [11] and activity recognition from video [12]. This technique, however, has the disadvantage of requiring manually segmented and annotated training data, which often requires video and an expert user. Here we are instead addressing the problem of how to discover activity atoms in an unsupervised fashion.

The research field of subsequence clustering, which was already a widely applied and studied technique, took a dra-matic turn after Keogh, Lin and Truppel published a paper which claimed that clustering subsequences using sliding window approaches is meaningless [2]. They reported that techniques such as the standard k-means clustering, when applied to segments of a time series created by a sliding window of a fixed size, often result in outputs that look the same regardless of the input data. When using a fixed size sliding window one will often create segments that are in different phases with respect to the pattern in the data which makes it difficult to learn meaningful patterns.

In the supervised setting these problems have been dealt with by using a semi-Markov framework which allows for segments of varying length. This more powerful technique could learn to align the segments according to the patterns in the data. Here we are introducing the same idea into the unsupervised clustering setting by extending the objective function of k-means clustering from fixed window segments to segmentations of varying length. In this new clustering framework there is no need for using overlapping windows, a technique that for fixed window sizes make it more likely to create some similar looking segments but which has the drawback of also inserting segments of slightly varying phase into the set that will be clustered. The semi-Markov clustering does not cluster a fixed set of segments, but works with all possible segmentations to find one way of segmenting the sequence into consistent patterns. Unlike in the fixed window setting, the result is a segmentation into consistent patterns and centroids that look exactly like those patterns.

We evaluate the new method on:  X  one artificial dataset which displays the algorithm X  X   X  activity recognition tasks where acceleration data has
Accelerometers have been investigated extensively for the purpose of gait event detection [13], [14] which involves detection of the phases of gait during walking. This is of considerable interest in improving quality of life in children with cerebral palsy [14], and in functional electrical stimulation (FES) walking [13]. Of particular interest to the authors of this article is the use of accelerometers for the purpose of human activity recognition [13], and the use of machine learning techniques. Studies have been conducted into the optimal use of accelerometers in adults and children for activity assessment over different numbers of sensors with different placements on the body [15]. They have subsequently been used to measure activity levels to combat obesity [16]. The task of monitoring swimming training has been studied recently by  X  X npublished X  [8].

In section two we introduce the new machine learning method for subsequence clustering. We call it semi-Markov kMeans clustering. In section three we describe max-margin semi-Markov models. That is the method that we intend to train using the results from semi-Markov k-means clusterng. Section four contains experimental evaluation and section five contains conclusions.
 A. Data Model and Notation
We assume that we are given a sequence of observations x , i.e. a multi-dimensional time series, and that there exists an unknown associated sequence of labeled segments y describing the sequence of events which has generated x . We say that pairs ( x,y ) have been drawn from an unknown probability distribution. The data x , of length N , consists of a sequence of observations x i , i = 1 ,...,N . Associated with x are labels l i  X  L where L := { 1 ,...,L } denotes the set of possible labels. The labels, together with segment start positions, form what we call a labeled segmentation y . Since we want to design models which have a long-range persistence of labels, we typically express y in a compressed representation as a list of pairs y = { ( n 0 ,l 0 ) ,..., ( n of segment boundaries n i and associated labels l i . In other words, the segment [ x n i  X  1 +1 ,...,x n i ] bears the label l Moreover, n 0 = 0 and l 0 = start to indicate the left boundary, n m to denote the end of the whole sequence, and n i +1 &gt; n i for all i to denote consecutive labels. Note that the number of segments m itself is variable. We will restrain the durations of a segment by giving minimum and maximum allowed durations and in some special situations we will also consider incorporating prior knowledge about it through other restrictions which we here summarize by ( n segments that can immediately follow the segment that ends at n and and has label l . That the restriction is only based on adjacent segments is the definition of the semi-Markov prop-erty which is desirable since it is compatible with dynamic programming, see Section II-D. It can, however, be loosened to a higher order condition which allows dependence for a fixed number of consecutive segments. The set S ( n,l ) is defined by a combination of a grammar which says which label transitions are allowed, e.g. in a gym a person who is working out on the training bicycle cannot directly transition to working out on the rowing machine without having some state like standing or walking in between. We also restrict the possible durations of a segment by giving minimum and maximum allowed length and we can also for computational reasons introduce what we call a step size which regulates which segment lengths between the minimum and maximum lengths that we will consider.
 B. Formulation of the Semi-Markov kMeans Clustering Method
Semi-Markov kMeans attempts to find a labeled segmen-tation to minimize a loss function. This loss function is designed to obtain a segmentation where all segments which are given the same label are found to be  X  X imilar X , according to some notion of similarity with cluster centroids. The similarity is measured based on a square euclidean distance between a feature vector of a proposed segment and the centroid corresponding to the proposed cluster. These centroids are found in an unsupervised fashion, and are analogous to the cluster centroids in the traditional kMeans algorithm. We define a feature vector  X  ( x,n i  X  1 ,n i ) for every segment such that the feature vector X  X  dimension does not depend on the length of the segment. This enables us to compare segments of different lengths. These calculations can be distributed over the sensors, as long as the features for each sensor are calculated based only on the data from that sensor. In other words features and similarities can be calculated for each individual sensor, in parallel, and later combined.
 We define our feature vector in the following manner; Given a segment starting at index p of length l , we firstly take e.g. ten means for each channel in x , starting at indices p,p + l 10 ,...,p + l  X  l 10 , over a length of l 10 . This both resolves the problem of comparing segments of different lengths and performs a time-warping that enables us to recognize patterns even when dilated. The number of bins can be varied. Secondly we use derivative features which are defined as the difference between a given mean bin and the preceeding mean bin. Ten mean bins therefore result in nine derivative features. The third kind of features we use are variance bins. Variance can be binned like mean features, however, it has been found unnecessary for this study. For every experiment we will describe the exact choice of features used.

In our semi-Markov kMeans algorithm, we find a labeled segmentation y  X  of a given data sequence x by performing the following minimization: where we would ideally like to use which is the average square distance of the feature vector for the segments from y = { ( n 0 ,l 0 ) ,..., ( n m ,l m class centroids  X  ( l i ) . This objective is useful for picking class centroids given training data, however, (2) is ill suited for the optimization in (1). The problem here is that the essential dynamic programming requires the objective to be of a different form, on which (2) can not be rewritten due to the variablity in the number of predicted segments. Instead we take advantage of the fact that if all segments have the same length  X  and the total length of the sequence is M then in the objective that we use, namely where  X  i = n i  X  n i  X  1 , i.e. the segment length of the i :th segment, and we have removed the irrelevant factor all  X  i are equal, this formulation is equivalent to the ideal (2). Note that if we did not use either an average distance or the factors  X  i then we would have a bias towards having as few and long segments as possible since that would yield fewer terms in the sum.

Given a segmentation y , then for every label l we let where m l is the number of segments labeled l . This is minimizing (2).

Given centroids and a data sequence, the predicted seg-mentation is calculated by minimizing (3). This prediction step is actually in itself a novel supervised method that we call the semi-Markov nearest centroid method. It can be trained from labeled segmentations by simply calculating the centroid of each label through (4). Section II-D will deal with the problem of how to perform the minimization in a tractable manner through dynamic programming.
 Algorithm 1 : Subsequence clustering algorithms Sliding windows Kmeans clustering:
INPUTS:
OUTPUTS: Semi-Markov Kmeans clustering
INPUTS:
OUTPUTS: C. Gaussian Model
In this section we will introduce a slightly modified and generalized formulation of the objective function by interpreting the segment scores in the previous section as the logarithm of the probability density function of a Gaussian distribution. The previous section X  X  method corresponds to using Gaussians with the same covariance matrix for all classes. The covariance matrices were a positive constant times the identity matrix. We now generalize this model by using a not necessarily constant diagonal covariance matrix. Moreover, taking a probabilistic approach gives a better theoretical foundation, and allows us to model classes with variable widths, and shapes. When performing activity recognition we will have data where running has more variability than working out on a training bicycle. We note here that we take the case where each class has a diagonal covariance matrix (since we are only interested in the within cluster variance), but also acknowledge the further generalization to any covariance matrix. We now, using the multivariate Gaussian probability density function, formulate the new objective function as where for P ( x,y i |  X ,  X ) given by where  X ( l ) is the covariance matrix for our model of class l .

Given training data, the centroids and standard deviations are chosen through maximum likelihood estimation for each class using the gaussian density function. More formally, (  X  ( l ) ,  X ( l )) = argmax where m i is the number of segments with label l in the training data.

This optimization is achieved by setting: and  X ( l ) is set to where the centroid  X  l in (10) corresponds to that found in (9). We have decided to replace  X ( l ) with a matrix that agrees with it for the diagonal elements but whose non-diagonal elements are zero.
 D. Dynamic Programming
Our problem of optimizing (3) and (6) with the appropri-ate maximization and minimization function is a so called max-sum problem of the form
For this kind of problem [9], dynamic programming can be carried out in the following manner;
Denote by V ( n,l ) a score function and let U ( n,l ) be a tuple of a position and label. In this case we may carry out dynamic programming via
U ( n,l ) := argmax
V ( n,l ) := max Once the recursion reaches ( N,  X  ) (here  X - X  denotes the final null label) we may traverse U ( n,l ) backwards to obtain a full segmentation of the sequence.

The computational complexity of the dynamic program-ming is where M is the length of the data stream, k is the number of clusters,  X  max and  X  min are the maximum and minimum allowed segment lengths respectively, and s is the step size.
This section will describe max-margin semi-Markov mod-els as they were used in  X  X npublished X  [8]. This is a discrim-inative method which requires a full labeled segmentation as training data. In the next section we use the segmentations generated by semi-Markov kmeans clustering as training for the max-margin model.
 A. Linear discriminant functions in a semi-Markov setting
We use a discriminant function F ( x,y ) to define predic-tions We use linear functions of the form The problem of finding F becomes one of finding a suit-able vector w . Here  X ( x,y ) is a joint feature map which decomposes into a sum over features which only depend on a single segment. Formally  X  is of the form B. Training with a max-margin criteria
We use a max-margin criteria to define what it means for a set of parameters to be suitable, given the training data. This involves a loss function,  X ( y,  X  y ) , between a predicted labeled segmentation,  X  y over a sequence x and the corresponding true labeled segmentation for x , y . The loss function measures how incorrect  X  y is given y . We employ the loss function from  X  X npublished X  [8] which is the sum of the Hamming loss, which measures how many samples are incorrectly labeled, and what we call the boundary loss, which measures how far the predicted boundaries are from the true ones.

Given n pairs of sequences and annotations ( x i ,y i aim to minimize: 1 n Here  X  &gt; 0 is a regularization constant to trade off model complexity and empirical risk. We use stochastic sub-gradient descent to minimize this objective.

We perform semi-Markov subsequence clustering on two types of data, and provide results in the context of activity recognition.
 Firstly we test on a variation of the artificial Cylinder-Bell-Funnel dataset (CBF) [17], in which the parameters are drawn from the original distribution but with the time series X  concatenated. We secondly test on accelerometer data collected from swimmers performing the 4 standard strokes during training. We use the swimming data to show our ability to easily train an activity recognition system using only an ordered list of activity types as annotation. A. The Cylinder-Bell-Funnel dataset
The original CBF task was formulated by Saito [17] and consists of classifying a time series as one of three classes; Cylinder, Bell or Funnel. This publicly known problem, through high noise levels and variable onset and duration of states, tests our algorithm X  X  robustness. 128 samples long time series X  are sampled by first drawing h , letting it have a normal distribution with mean 0 and variance 1 . For every time sample t we draw  X  ( t ) from the same distribution N (0 , 1) . Furthermore an integer a is drawn uniformly from the interval [16 , 32] and then we let b = a + l where l is drawn uniformly from the interval [32 , 96] . The time series are defined from these numbers by letting if we have a cylinder and if we have bell, and if we have funnel we let where in all of these equations 1 [ a,b ] ( t ) is the indicator function which equals one if t  X  [ a,b ] and otherwise equals zero.

In this article we simply create one 12800 samples long time series by randomly, with equal probability, choosing one of the three patterns at a time, one hundred times and concatenate the one hundred time series after each other. For every one of the one hundred subsequences we label every time sample with the corresponding name, i.e. Cylinder, Bell or Funnel if t is in the drawn interval [ a,b ] and for the other samples we use the label flat.

The features used were 10 mean bins and 9 derivatives, scaled by a factor of 5 to help further emphasize slopes on the shapes. We used the traditional kmeans score function as in (3) with 10 restarts of 30 iterations, and k = 4 .
In Figure (3) one can see that the clustering has recovered the correct shapes. In that plot every segment has been replaced by a dilation of the centroid for the class the segment has been assigned to.
 B. Activity Recognition during Swimming
We have collected 3-D acceleration data recorded at 100Hz by a device placed on the rear of the swimsuit during swimming training sessions. We used this data to show our ability to train an activity recognition system by firstly obtaining a training segmentation in an unsupervised manner using Semi-Markov subsequence time series clustering. We then use this training segmentation with a user provided list of activity types to train two different activity classifiers, namely the Semi-Markov nearest centroid algorithm and max-margin SMM, to create training logs for subsequent sessions. For example, for a medley training session the user might provide the following list of activity types:
Butterfly  X  Backstroke  X  Breaststroke  X  Freestyle
Table (I) shows a list of data files used. Figures (4-6) further provide graphical examples of the data.

We compare the results of these segmentations to what we call the true segmentation. This true segmentation was obtained by hand segmenting the data with use of video -a very tedious task further motivating the full automation of training such algorithms.
 C. Training
In training we performed semi-Markov clustering on each of the 6 swimming data sets to obtain a labeled segmen-tation. We then matched a user specified list of activities to the labels to obtain a meaningfully labeled segmentation for training activity classifiers. For the purpose of activity classification we only care about the number of strokes, stroke times, and number of laps of each stroke undertaken. For this reason, in training we assign segments placed over turns to a common junk state. Such regions are found by being short sections of inconsistent labels in between longer sections of consistently labeled segments  X  which correspond to strokes. Below we show the error of the labeled training segmentation in the context of stroke count and rate given by the following measures where k is the number of classes, m is the number of segments in the true segmentation, M is the number of predicted segments and l  X  i is the label at segment i in the true segmentation. Further  X  i,j is the kronecker-delta function which is equal to 1 iff i = j .

To calculate the average length error we must do a slightly more complicated calculation. Since instantaneous stroke rate is directly calculable from the segment length, the average length error ideally aims to find the following where  X  i is the length of the i th predicted segment and  X  is the length of the i th segment in the true segmentation. This is not possible to calculate, however, since there is no guarantee that the predicted number of segments of a given label is the same as that in the true segmentation. Instead we must calculate this statistic from the largest possible subsets of the true and predicted segmentations in which there exists a 1-to-1 mapping between segments in each subset.

Semi-Markov k-means was run on each dataset with 10 restarts each with random initialization using both the standard and Gaussian score functions. The features used were 20 mean bins and 19 derivative bins.

In testing we trained both the semi-Markov nearest cen-troid and max-margin SMM activity classifiers with the training segmentations previously obtained. This was done using various train/test sets and the rate and count errors are reported for the predicted segmentations as before.
 D. Discussion of Activity Recognition Results
Semi-Markov k-means clustering was used to automati-cally generate training segmentations for sophisticated ac-tivity classifiers. Tables (II) and (III) show, in general, high accuracy in these training segmentations for both the standard and Gaussian score functions. We notice in Table (II) that the mFreeBack data set gets a significantly worse count error than the other data sets. This is due to a change in the data in the first freestyle lap. This resulted in different looking freestyle strokes and hence two different labels. This was immediately improved, however, using the Gaussian score function as seen in Figure (III). In Figure (7),(8),(9) and (10) we display reconstructions from the lists of labels and segment lengths which are created by dilating centroids to the given length and then concatenating all those segments. They show the high degree to which the clustering has captured the nature of the four strokes.

The training segmentations obtained with the standard semi-Markov k-means score function (without covariances) were then used to train the max-margin SMM, and the near-est centroid algorithms. We see that when the max-margin SMM is trained with these segmentations we typically (in 4 out of 6 cases) get errors comparable to those in training. In the other two cases, we still see a significantly better than random error. In fact the larger errors on the mMedley data sets were caused by a phenomenon in the data causing one person X  X  breaststrokes to look similar to another person X  X  butterfly strokes. This is illustrated in Figure (11).
Shown in Table (IV) are the errors obtained when training the max-margin SMM with both the automatically generated semi-Markov kmeans segmentation and the true segmen-tation. We can see that there is no significant difference between the two training methods and hence conclude that semi-Markov kmeans relieves the need for arduous hand segmentation without loss of accuracy.

In Table (V) we see that the nearest centroid method performs far worse than the max-margin SMM. This is a result of the poor generalizability accross swimmers of the nearest centroid method. This relatively poor generalizability compared to the max-margin SMM comes from the different objectives of the two algorithms. Namely that the max-margin attempts to pick the differences between the classes, whereas the nearest centroid method attempts to model how each class looks.

We introduce a novel subsequence clustering technique which does not rely on a sliding window approach. Sliding window approaches are particularly bad for data where the patterns have varying duration. Our new semi-Markov approach deals directly with this fundamental problem by considering varying length segments in the same segmenta-tion.

We demonstrate that our novel subsequence clustering technique enables simple training of systems that perform activity recognition from body-worn sensors. We only re-quire the user to provide a chronologically ordered list of the activity types that they were performing while recording sensor data. This is easy for a non-expert user to provide. In contrast, to train fully supervised methods requires detailed labeled segmentations where every boundary between units of activity is provided. We demonstrate performance that is comparable to the results obtained when using a full manually created labeled segmentation.
 NICTA is funded by the Australian Governments Backing Australias Ability and ICT Centre of Excellence programs.
