 Sebastian Nowozin sebastian.nowozin@tuebingen.mpg.de G  X okhan Bak X r ghb@google.com Google GmbH, Brandschenkestrasse 110, 8002 Zurich, Switzerland Methods for unsupervised learning aim at recovering underlying structure from data. In this paper, we are concerned with exemplar based models in which this structure is represented by a weighted set of points in input space. Depending on the used model, these points can be interpreted as clusters , codebook vectors or mixture components .
 Although the representation is done by a finite point set, the structure being represented  X  such as a den-sity  X  is defined on the entire input space by expanding a smoothing kernel function around each representing point. In this setting learning simply becomes decid-ing on the number of points and their weights, as well as their location in input space by means of a suitable objective . In EM-learning of mixture models and in k -means clustering one fixes the number of points and adjusts their position by performing descent steps on the objective function starting from a random initial-ization. This leads to well-behaved but usually non-convex learning problems. Recently, a number of con-vex approaches have been proposed. Our goal in this paper is to improve on these approaches.
 In section 2 we review convex formulations for unsu-pervised learning tasks and discuss two recent meth-ods. We show how convexity is achieved and derive a small experiment whose result suggests a way to improve on the established models. We describe our model in section 3 together with an algorithm and a theoretical justification. The model is validated exper-imentally in section 4 and we conclude in section 5. We now discuss two convex approaches to unsuper-vised learning from the literature. We will denote the training set as X = { x i } i =1 ,...,N , with x i  X  X and usually X = R d .
 Kernel Vector Quantization (Tipping &amp; Sch  X olkopf, 2001) learns a small set of codebook vectors such that the minimum distance from any training sample to its nearest codebook vector is bounded above by a given maximum distortion h . In (Tipping &amp; Sch  X olkopf, 2001), this is done by formulating a linear program-ming problem, of which the following problem is an equivalent reformulation. 1 Here K is a ( N,N ) matrix with K i,j = I ( k x i  X  x j k X  h ), where I (  X  ) evaluates to one if the predicate is true and to zero otherwise, therefore, K i,j is one if a ball of radius h centered on x j contains x i . In the solution of (1) the balls selected by q j &gt; 0 form a sparse cover-ing of the training set and the distance of each sample to its closest covering ball is bounded by h . Convex Clustering (Lashkari &amp; Golland, 2007) was re-cently proposed for clustering. In Lashkari and Gol-land X  X  model, a mixture model is fit to an observed training set, such that a candidate mixture compo-nent is centered around each training set exemplar. Using the framework of Bregman clustering (Baner-jee et al., 2005), their objective maximizes the log-likelihood subject to the constraint that the resulting model is a proper mixture model. In the optimum solution of the model, a sparse set of exemplars is se-lected, allowing the interpretation as clusters. Formally, Lashkari and Golland maximize parameters q j  X  0, j = 1 ,...,N with P N j =1 q j = 1. The model allows all exponential family distribu-tions with a corresponding Bregman divergence d  X  (Banerjee et al., 2005). For the maximization, a multiplicative update is used, which leads to slow convergence once elements of q approach zero. We re-formulate the above objective function by introducing a new set of variables  X  i , with i = 1 ,...,N as follows. Clearly, problem (2) is equivalent to the previous one because constraints (3) only serve to evaluate the like-lihood  X  i for each sample x i . 2.1. Where does Convexity come from? Models as proposed in (Tipping &amp; Sch  X olkopf, 2001) and (Lashkari &amp; Golland, 2007) achieve convexity by changing the problem parametrization. Instead of learning the coordinates of a fixed number of exem-plars z j , j = 1 ,...,M , there is now a larger set of possible candidate exemplars with fixed coordinates. Learning is performed by optimizing over indicator variables, selecting a sparse subset of the candidates. This reparametrization makes the problem convex but also changes the regularization: whereas usually the number of exemplars M is the main regularization pa-rameter, it is now an implicit guarantee on the quality of the solution. In (Tipping &amp; Sch  X olkopf, 2001) this is the maximum distortion h , whereas in (Lashkari &amp; Golland, 2007) the regularization parameter  X  controls the smoothness of the density. 2.2. Motivating Experiment: More Exemplars Restricting the set of possible prototype candidates to the training set might result in a suboptimal solution if there is no exemplar close to the true mean of a clus-ter. If the data is low-dimensional, normal-distributed within each cluster, has low noise and there are enough training examples, this effect is small and can be ig-nored. But in high dimensions the true mean might be far away from any exemplar.
 To demonstrate the effect of restricting the prototype candidate set we perform an experiment. A simple two-dimensional data set is created by sampling from an isotropic Gaussian and a ring of uniform density, forming two well-separated clusters, see Figure 1. We compare convex clustering (Lashkari &amp; Golland, 2007) with a modified model where the objective is changed samples x i and M cluster center candidates z j . This convex objective still represents the log-likelihood of the training samples under a mixture model. We gen-erate z j by densely discretizing the [  X  2; 7] 2 box on a regular grid. Our hope is that a fine discretization will increase the chance that { z j } j =1 ,...,M contains ex-emplars close to the true center of each cluster. For both models we use an isotropic multivariate normal distribution with covariance matrix  X  =  X  2 I ,  X  = 2 . 5. The clustering result is shown in Figure 1. For the cluster around the origin there is indeed a training set exemplar close to the mean of the generating Gaus-sian and the difference between the convex clustering and dense selection is small. However, for the ring-like structure, the training set exemplars cannot represent the cluster center adequately. This causes convex clus-tering to select two exemplars, while in the dense set a single good candidate is selected. A slight perturba-tion in the training data would lead to a different se-lection by the convex clustering method, as all samples bordering to the interior of the ring are roughly equally bad. For this data set, the solution produced by con-vex clustering is not only qualitatively disappointing but also unstable. The achieved objectives are shown in Figure 2, where the convex clustering objective is drawn as horizontal line and the dense exemplar model forms a curve as the discretization becomes finer and finer. At around eight discretizations per dimension our modified model surpasses the log-likelihood of the convex clustering model. At around 30 discretizations per dimension the log-likelihood levels out and adding more cluster candidates does not improve the solution. This experiment suggests that a larger set of candi-date clusters can lead to higher quality results which are also more robust. While dense discretization is only feasible in case the input space is low-dimensional, ideally we would like to use an infinitely fine discretiza-tion and thus use the set of all possible input points as candidates. This idea will be the basis for our method. We now introduce our model for unsupervised learning together with an efficient solution algorithm. Essential to the solution is the ability to solve a certain subprob-lem which we analyze in detail. 3.1. Model Our model for unsupervised learning generalizes con-vex clustering (Lashkari &amp; Golland, 2007) and kernel vector quantization (Tipping &amp; Sch  X olkopf, 2001). Let k (  X  ) be a non-negative smoothing kernel centered at z  X  Z , with Z  X  X . Let { x i } i =1 ,...,N , x i  X  X denote the training set. The following semi-infinite convex programming problem learns a convex combination of response functions such that an objective is minimized. where  X  ,  X  ,  X  and  X  are the Lagrange multipliers for the respective constraints. Before discussing the choice of objective function  X , let us discuss the purpose of the constraints.  X  Constraint (5) evaluates a convex combination of  X  Constraint (6) identifies  X  if  X   X   X (  X  , X  ) &lt; 0  X  the  X  Constraints (7) and (8) define the combination For the special case where Z is a finite set of points in X , we can replace the integrals and infinite constraints with a finite sum and finite set of constraints, respec-tively. Constraints (5) can then be compactly written as K q =  X  , where K is a ( N, |Z| ) matrix storing the kernel responses. The dual problem of (4) can be de-rived from the conjugate function  X   X  (  X  , X ,  X  ,  X  ) and its respective domain (Boyd &amp; Vandenberghe, 2004, result (5.11)). The dual problem is We propose the following choices of convex objective functions  X (  X  , X  ). 1.  X (  X  , X  ) =  X   X  2.  X (  X  , X  ) =  X  1 N P N i =1 log(  X  i ) 3.  X (  X  , X  ) =  X   X  + C N P N i =1 (  X  i  X   X  ) 2 In order to be able to compare our method with es-tablished methods from the literature we only use the first two objectives in the experiments. 3.1.1. Relation to existing methods.
 Most relevant for our approach is Boosting Density Es-timation (Rosset &amp; Segal, 2002). We note the follow-ing differences, i) our model includes different objec-tives, ii) in our solution algorithm, we will use totally-corrective weight updates 2 instead of a simple line-search procedure, and iii) we identify each weak learner uniquely with a point in input space. Also related is the hard-margin case of 1-class Boosting (R  X atsch et al., 2001). With exemplar-based weak learners it is a spe-cial case of our model with the first objective. Algorithm 1 Infinite Exemplar Column Generation ( Z, q ) = Infex ( X,,k,Z 0 ) Input: Output:
Algorithm:  X   X  X  X  1 N 1 , Z  X  Z 0 , R  X  X  Z 0 | + 1,  X   X  X  X  ,  X   X   X  0 loop end loop 3.2. Algorithm To solve problem (4), we propose Algorithm 1 (IN-FEX), a delayed column generation algorithm. The algorithm works with a finite and usually small set of candidate prototypes z j . This set is iteratively en-larged by adding good candidates. Selecting the can-didates to add in each iteration becomes a subproblem, which we define now.
 Problem 1 (Subproblem (SP)) Given a set of samples x i  X  X , i = 1 ,...,N , a corresponding non-positive sample weighting  X  i  X  0 , i = 1 ,...,N and a non-negative smoothing kernel k z ( x ) : Z  X X  X  R + , obtain z  X  as the solution of The solution to this subproblem provides a candidate z  X  that, when added to the set of considered candi-dates, will reduce the global objective. 3 We will now rigorously derive the subproblem from global optimal-ity conditions of problem (4). Theorem 1 Assume that the subproblem (SP) can be solved exactly in each iteration. Then Algorithm 1 solves problem (4) globally to the desired accuracy . Proof . Consider a slightly modified version of prob-lem (4), where a part of the constraints (7) is replaced by equality constraints. We replace (7) by the fol-lowing constraint set, parametrized by a finite set of points Z R = { z 1 ,..., z | Z where v z = 0 is constant for all z  X  X \ Z R . Together, constraints (11) and (12) restrict problem (4) such that only a finite subset of the variables q are used. For a given finite Z R , we can obtain an optimal primal ( q  X  ,  X   X  , X   X  ), and dual (  X   X  ,  X   X  ,  X   X  , X   X  ) solution to the modified problem by solving a finite problem in the restricted set of variables { q z : z  X  Z R } . Let the optimal function value of this solution be denoted by p ( v ). Because the optimal solution must be feasible, we have q  X  z = v z = 0 for all z  X  Z \ Z R . How would the objective function value p ( v ) change if we force a q  X  z to become non-zero? That is, if we increase v z by a very small amount can we improve the solution? The sensitivity theorem (Bertsekas, 1999, Proposition 3.3.3) provides a definite answer, namely we have for all z  X  X  \ Z R the following. If we have for all z  X  Z \ Z R that  X  v z p ( v )  X  0, then this implies that we can not decrease p ( v ) by making q z &gt; 0. Conversely, this observation provides us with a global optimality condition : if and only if Z R contains all relevant (positive q z ) exemplars, we have  X  z  X  X \ Z
R :  X   X  z  X  0. Given Z R and a primal-dual optimal solution we can find an alternative expression for  X   X  z . Consider the Lagrangian of the modified problem. Because of optimality of the solution, it must satisfy the Karush-Kuhn-Tucker necessary condi-tions (Bertsekas, 1999), therefore we must have a zero gradient with respect to the primal variables. Specifically, for all z  X  Z \ Z R we must have  X   X  z +  X  Therefore, if for all z  X  Z \ Z R we have dual feasible  X  z  X  0, then the current solution is optimal, despite the restrictions imposed by constraints (12). If we sat-isfy the optimality condition, then replacing (12) with constraints (11), does not change the solution, which remains optimal in the original problem (4).
 What remains to be shown is that Algorithm 1 makes progress in each iteration and thus in the limit will satisfy the optimality condition. Consider the case where the above optimality condition is violated for one or more z  X  Z \ Z R . Then, let z  X  = corresponding to the most negative partial derivative  X  v z  X  p ( v ) &lt; 0. Because of the sensitivity theorem, adding z  X  to Z R  X  making q z  X  a free variable  X  and re-solving (4) will reduce the objective value. There-fore, either no z  X  with  X  v convergence to the tolerance is established, or a strict decrease in the objective is obtained.
 Note that in practice, we can add multiple exemplars in each iteration. Suppose during solving the subprob-lem (SP) we obtain a number of good local maximizers. Then, we can add all these local maximizers in order to obtain a faster convergence. Adding redundant ex-emplars with  X  v z p ( v ) &gt; 0 does not have an effect as they will receive a zero weight q z = 0. 3.3. On the Nature of the Subproblem The subproblem (SP) is completely determined by the negative weighting of the training set and the shape of the smoothing kernel function. For further discussion let us define  X  i =  X   X  i and rewrite the subproblem as lows that all  X  i are non-negative. Clearly, this problem is non-concave whenever k is non-concave in z which is true for all smoothing functions we consider. However, for kernel functions of the form k z ( x ) = k ( k x  X  z k ), the optima of the subproblem, thus the new candidates, are located at the modes of the ex-exploited to efficiently solve the subproblem by stan-dard hill-climbing algorithms. Such algorithms start at a point z (0) in input space and generate iteratively P mean shift procedure which was introduced by (Fuku-naga &amp; Hostetler, 1975; Cheng, 1995) and gained pop-ularity due to (Comaniciu &amp; Meer, 2002). Given an initial starting point z (0) the iterates are produced by where g : R +  X  R + is the negative derivative of the so called kernel profile . If for a continuous kernel the function g is convex and non-increasing, then the mean shift procedure is guaranteed to converge to a local maxima (Comaniciu &amp; Meer, 2002). For each of the common continuous smoothing kernels, a unique func-tion g exists and some popular kernels and their profile derivatives are discussed in section 4. For the Gaus-sian kernel, g is a scaled version of the original kernel profile and thus particularly easy to maximize. 4 Mean shift is popular in computer vision, where specialized procedures have been developed to efficiently find glob-ally good modes, for example the annealed mean shift procedure (Shen et al., 2007).
 If the smoothing kernel function is a reproducing Hilbert kernel (Sch  X olkopf &amp; Smola, 2002), then prob-lem (SP) is known as the pre-image problem (Sch  X olkopf et al., 1999). An important difference which simplifies our subproblem considerably is that all our weights  X  are of the same sign. In the general pre-image problem the sign is not fixed and procedures such as the one of (Sch  X olkopf et al., 1999) can be unstable and do not have a convergence guarantee. 3.4. Optimality Bound The proof of global optimality of the solution obtained by Algorithm 1 was based on the assumption that the subproblem (SP) can be solved globally. We now show that even without this assumption, the method can be no worse than methods using a fixed exemplar set. Theorem 2 Given  X (  X  , X  ) , a set X = { x i } i =1 ,...,N x i  X  X and a finite set of exemplars Z F = { z j } j =1 ,...,M , the solution obtained by solving prob-lem (4) with Z = Z F can not achieve a better objective than the solution obtained by Algorithm 1 with Z = X , Z Proof . Let Algorithm 1 be called with Z 0 = Z F . In the first iteration of Algorithm 1, the solved problem is identical to problem (4) with Z = Z F . Therefore, after the first iteration, the objective of Algorithm 1 is equal to the one obtained by solving problem (4). In all later iterations, the objective can only improve. For the following experiments, we solve the restricted master problem (4) using IpOpt (W  X achter &amp; Biegler, 2006), a modern primal-dual interior point solver for non-linear programming available as open-source. For each master problem, we obtain accurate convergence in a few dozen solver iterations. We use tolerances 10  X  10 for the restricted master problem and 10  X  7 for the subproblems for all experiments. 5 As smoothing kernels we use the unnormalized Gaus-sian, the unnormalized Epanechnikov, and a simple uniform disc kernel. All are parametrized by a band-width parameter h . The following are the kernel func-tions k and profiles g used in the mean shift procedure. 1. Gaussian, bandwidth h 2. Epanechnikov, bandwidth h 3. Uniform disc, maximum distortion h The first two kernels are common in non-parametric density estimation, whereas the last one is used by (Tipping &amp; Sch  X olkopf, 2001) for vector quantiza-tion. We use the mean shift procedure (14) started from all training samples to solve the subproblem (SP) for the Gaussian and Epanechnikov kernels. We col-lect the result of each run and add the set of unique local maximizers to the restricted master problem. However, mean shift cannot be used to solve subprob-lem (SP) for the non-continuous uniform disc kernel. Instead, when using the uniform disc kernel, we find new codebook candidates by solving the subproblem with the Epanechnikov kernel instead. This is a rea-sonable approximation as the Epanechnikov kernel re-sponse lower bounds the uniform disc kernel response and its maximum lies in the center of the disc. 4.1. Comparison with KVQ In the first experiment we compare the original Ker-nel Vector Quantization formulation (1) with all train-ing exemplars as possible prototypes with our Algo-rithm 1, where the initial set is empty, Z R =  X  . We use the first objective  X (  X  , X  ) =  X   X  and the uniform disc kernel. As dataset we use a subset of 1100 exem-plars from the USPS digit machine learning dataset, with all labels removed and each class sampled equally such that there are 110 exemplars from each class. We evaluate by selecting the maximum allowed distortion h from { 800 , 1000 , 1200 , 1400 , 1600 , 1800 , 2000 } , where  X  2000 is the mean inter-class L 2 -distance in the dataset. We compare the achieved margin  X   X  KVQ ( h ) with  X   X  INFEX ( h ), and the number of codebook vectors k q as the maximum allowed distortion is varied.
 The proposed method outperforms KVQ, selecting a smaller number of codebook vectors and achieving a better objective value. Especially for larger allowed distortions, the benefit of selecting an arbitrary point in input space is substantial as due to the high dimen-sionality of the data set all input samples are relatively far away from each other. Because we use Z R =  X  to initialize our method, the results show that our sub-problem approximation using the Epanechnikov kernel is an effective way to find good codebook candidates. 4.2. Comparison with Gaussian Mixture EM In the second experiment we consider mixture model density estimation and compare our method with Con-vex Clustering and a homoscedastic Gaussian mix-ture ( X  =  X  2 I ) learned with Expectation Maximiza-tion (EM). 6 The log-likelihood objective and the same USPS dataset as before is used. The experimental pro-tocol is as follows. For a range of bandwidths our model and convex clustering are run once per band-width. For each run, the number of components of our model is used to fix the number of components in the Gaussian mixture model, which is trained by EM starting 20 times from random initial sample points. The results are shown in Table 1. Clearly, a single run of our model is consistently the best. The best EM run is always close to our result and Convex Clustering is always the worst. (Lashkari &amp; Golland, 2007) mention that their solution  X  X an be improved in practice with a few extra steps of the EM algorithm X . From Table 1, we conclude that the results of convex clustering are qualitatively inferior to plain EM and such refitting is actually essential for obtaining good results. 4.3. Subproblem Modes In the last experiment we show the qualitative behav-ior of our model with the Epanechnikov kernel with h = 1500 and the log-likelihood objective. Because the Epanechnikov kernel has finite support, if we start with Z 0 =  X  we could have some samples x i which have zero response because k z j ( x i ) = 0 for all j . Then, the restricted variables q j are too few and problem (4) would be infeasible. Thus, in order to ensure feasi-bility of the initial master problems, we use Z 0 = X . Some subproblem modes are shown in Figure 5. The modes approximate the  X  X atural X  clusters well except for classes such as 3, 8 and 9, which seem to be ex-plained by one joint region with many local modes in it, for example in the first and second row. We presented a unifying perspective on existing ex-emplar based methods that aim at density estimation, clustering and vector quantization. Existing methods were either non-convex or achieved convexity by se-vere restrictions. In contrast, our approach  X  although still non-convex as a whole  X  is provable better than all existing methods. This is achieved by isolating a non-convex but still efficient solvable subproblem. The non-convex subproblem is embedded into a convex master problem steering towards an optimal solution. One limitation of our model is that one cannot fix k q  X  k 0 , the number of components. For problems where guarantees such as maximum distortion or smoothness are more natural constraints, this is not an issue. There are open questions that result from our work: 1. Does there exists a response function k that is 2. What is the relation between objective  X , kernel 3. Can a decomposition similar to ours yield a train-Acknowledgments This work is funded in part by the EU CLASS project, IST 027978. This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778. This publication only reflects the authors X  views. Banerjee, A., Merugu, S., Dhillon, I. S., &amp; Ghosh, J. (2005). Clustering with bregman divergences. Jour-nal of Machine Learning Research , 6 , 1705 X 1749. Bengio, Y., Roux, N. L., Vincent, P., Delalleau, O., &amp; Marcotte, P. (2005). Convex neural networks. NIPS . Bertsekas, D. P. (1999). Nonlinear programming . Athena Scientific. 2nd edition.
 Boyd, S., &amp; Vandenberghe, L. (2004). Convex opti-mization . Cambridge University Press.
 Carreira-Perpi  X n  X an, M.  X  A. (2000). Mode-finding for mixtures of gaussian distributions. IEEE Trans. Pattern Anal. Mach. Intell , 22 , 1318 X 1323.
 Carreira-Perpi  X n  X an, M.  X  A., &amp; Williams, C. K. I. (2003).
An isotropic gaussian mixture can have more modes than components.
 Cheng, Y. (1995). Mean shift, mode seeking, and clus-tering. IEEE Trans. Pattern Analysis and Machine Intelligence , 17 , 790 X 799.
 Comaniciu, D., &amp; Meer, P. (2002). Mean shift: A ro-bust approach toward feature space analysis. IEEE Trans. Pattern Anal. Mach. Intell , 24 , 603 X 619. Cornuejols, G., &amp; T  X ut  X unc  X u, R. (2007). Optimization methods in finance . Mathematics, Finance and Risk. Fukunaga, K., &amp; Hostetler, L. D. (1975). The esti-mation of the gradient of a density function, with applications in pattern recognition. IEEE Trans. Information Theory , 21 , 32 X 40.
 Geoffrion, A. M. (1972). Generalized Benders decom-position. Journal of Optimization Theory and Ap-plications , 10 , 237 X 260.
 Lashkari, D., &amp; Golland, P. (2007). Convex clustering with exemplar-based models. NIPS .
 R  X atsch, G., Sch  X olkopf, B., &amp; Mika, S. (2001). SVM and boosting: One class.
 Rosset, S., &amp; Segal, E. (2002). Boosting density esti-mation. NIPS (pp. 641 X 648). MIT Press.
 R  X uckert, U., &amp; Kramer, S. (2006). A statistical ap-proach to rule learning. ICML (pp. 785 X 792).
 Sch  X olkopf, B., Mika, S., Burges, C. J. C., Knirsch, P., Mueller, K.-R., Raetsch, G., &amp; Smola, A. J. (1999). Input Space versus Feature Space in Kernel-Based Methods. IEEE-NN , 10 , 1000.
 Sch  X olkopf, B., &amp; Smola, A. J. (2002). Learning with kernels . MIT Press.
 Shen, C., Brooks, M. J., &amp; van den Hengel, A. (2007).
Fast global kernel density mode seeking: Applica-tions to localization and tracking. IEEE Transac-tions on Image Processing , 16 , 1457 X 1469.
 Tipping, M., &amp; Sch  X olkopf, B. (2001). A kernel ap-proach for vector quantization with guaranteed dis-tortion bounds. AISTATS .
 W  X achter, A., &amp; Biegler, L. T. (2006). On the imple-mentation of an interior-point filter line-search algo-rithm for large-scale nonlinear programming. Math-
