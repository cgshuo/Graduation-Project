 1. Introduction relevant documents are written in other languages than the one of the query. much higher rate compared to the percentage of English-speaking users boundaries of languages.
 guage. This requires the availability of resources meeting the following two requirements: 1.1. Topical coverage 1.2. Language independence versal, having the same meaning when interpreted in each language.
 are realized in different languages.
 that it is up-to date in contrast to many lexical resources which are not constantly developed further.
As a specific contribution of this paper we show the following: 1.3. Wikipedia as interlingual concept space on the basis of Wikipedia articles or rather categories. 1.4. Bag-of-concepts model
Words model, the dimensions of vectors differ between languages. 1.5. Vector-based retrieval
Concepts model. 1.6. Mate-retrieval experiments lingual and multilingual IR in respect to optimal design choices in the CL-ESA model. our experiments. In Section 6 we discuss related work. Last, Section 7 concludes the paper. 2. Background retrieval models evolved from the Boolean retrieval model name  X  bag  X  ).
 measure defined on vector spaces is zero in the general case. based retrieval models can be combined with such BoC models.
 cross-lingual retrieval.
 content on the cross-lingual retrieval task. 2.1. Cross-lingual vs. multilingual IR need to be retrieved to a query in any language. MLIR is thus inherently more difficult than CLIR. in monolingual IR, which are language-specific.
 comparable across languages in general, normalization and weighting are necessary steps. 2.2. Motivation for concept based vector space models concept space can be used for retrieval, the problem of score aggregation Translation  X  can be circumvented.
 2.3. Wikipedia database
For our retrieval framework we consider a fixed set of supported languages L ={ l multilingual retrieval model we use Wikipedia databases W
C W horizontal connections.
 Wikipedia used to define each of the different concept spaces.
 context. First, there is the problem of missing or false language links. As an example, the English article linked to its Spanish equivalent  X  Buque portacontenedores
An example in the Wikipedia dataset used in the experiments is the category sciences  X  . In the German Wikipedia however, the equivalent articles sub-category relation. 4 tions across a high number of articles, we assume that these on different snapshots of Wikipedia are in line with it. 3. Cross-lingual explicit semantic analysis
In this paper we are concerned with the question of how user-generated content from Web 2.0 sites sidering the category structure of Wikipedia to define concepts.
 concept spaces along with language specific textual descriptions for each concept. 3.1. Explicit semantic analysis guage links. However, ESA is a general model and has been applied to other resources, e.g. Wiktionary CL-ESA.
 ment from the Multext dataset 7 (introduced in Section 5 ), translated into English and German: English: The transport of bicycles on trains cepts used in the examples. 3.1.1. De fi nition of concepts
All concept models described below build on an explicitly defined finite set of concepts C ={ c documents will be indexed across languages.

Further, for all models we assume the existence of a text signature pedia W l in language l : 3.1.2. Original ESA model function  X  l : D  X  R m is defined as follows: is computed based on term distributions in d and in the text signature d : have at least one occurrence of this term: frequency of t , defined by the number of documents in D containing term t . dicate the term frequency vector defined by TF  X  3.2. Cross-lingual ESA IA W  X  X  and interlingual categories IC W  X  X  .
 articles across languages and define the relation LANGLINK languages based on  X  LL : different languages.
 articles to their Wikipedia articles in a specific language l :
To apply ESA in a cross-lingual setting, the concept space is based on interlingual articles: C sion to ESA as cross-lingual ESA (CL-ESA). In this case the text signature of concepts is defined as: gual articles using the same equivalence relation. The function CATS each language:
Applied to CL-ESA, the concept space is then defined by the interlingual categories: C Rail transport .
 articles (categories) in each interlingual article (category), will be presented in Section 5 . 3.3. CL-ESA applied to CLIR/MLIR efficient implementations with regard to memory and storage consumption.
The final ranking function for a document d in language l First, the query and the document are mapped to the concept space using the language-specific CL-ESA functions (compare Fig. 4 ). Both resulting concept vectors are then pruned using the function document.
 the next section, we present various alternatives for the above mentioned design choices. 4. Design choices for CL-ESA model, which can be summarized as follows:  X   X  based on the text signature  X  l of concept c and the text of document d .  X  natives inspired by different approaches to monolingual IR are possible here.  X  mental evaluation of particular instantiations applied to CLIR and MLIR scenarios. 4.1. Dimension projection systematically.
 defines an order on the indices of the dimensions according to descending values such that 10-th highest value of d  X 
Absolute : with  X  m abs d (as in [8] and [9] )  X 
Absolute Threshold : with  X  t thres d  X 
Relative Threshold : with  X  t rel d t  X  [0..1], thus restricting it to those values above a certain fraction of the highest-valued dimension.  X  the original ESA model [8] . Formally, the projected vector according to the order  X  d for which the following condition holds: d to 100 as in [2] . 4.2. Association strength text signature of c in language l .

Let | C | denote the number of concepts, |  X  l ( c )| the number of tokens in the text signature (TF  X  l ( c ) ( w )) denote the term frequency of w in document d (text signature then defined as ICF w  X  X  X  log C jj CF w  X  X  . We can use the following functions here:  X 
TFICF : The most widely used version of the TFIDF function applied to concepts:  X 
TFICF* : A modified TFICF version ignoring how often the terms occur in document d :  X 
TF : An association function only based on term frequencies (ignoring inverse document frequencies):  X  b =0.75: with  X 
The Cosine similarity between the TF and TFICF vectors:
Note that we have also experimented with versions of the above where the TF with the TF  X 
For MLIR settings we also performed experiments putting more weight on the ICF factor:  X 
TFICF 2 : TFICF as presented above with quadratic ICF factor:  X 
TFICF 3 : TFICF with cubic ICF factor: 4.3. Relevance function
The relevance function REL( q , d ) defines the score of a document d q :  X   X  X  l q q  X  X  of query q and d model. The  X  term frequency  X  of concept c in document d is defined as TF
Bag-of-Concepts model.  X  The Cosine similarity of query and document vectors (used by all ESA implementations known to us):  X  TFIDF: The TFIDF function transferred to the Bag-of-Concepts model:  X  follows: the following retrieval function:  X  4.4. Concept spaces Section 3 ). These text signatures are defined by articles.
 the structure of articles. Our results indeed support this conclusion. 4.4.1. Category explicit semantic analysis
Category ESA (Cat-ESA) is based on a set of categories  X  ={  X  to all articles assigned to the category.
 Instantiated for Wikipedia as in our case, the categories assign articles to categories in a specific language l : CATLINK categories may not be disjoint.
 In contrast to CL-ESA, theconcept spaceof Cat-ESA is then spanned by
When computing term statistics, this union is equivalent to the concatenation of the content of the articles. 4.4.2. Tree category explicit semantic analysis
The given tree structure on categories defines the sub-category relation SUB the function TREE:  X   X  2  X  that maps a category  X  to the set ofcategories that build the subtree rooted in
The association strength of document d to category  X  is then not only based on concepts in subcategories. This results in the following definition of the text signature function category structure. 4.4.3. Summary of concept spaces 5. Experiments choices:  X  parameter settings.  X  goal is to identify appropriate concept spaces for CLIR and for MLIR. ments were performed on two established parallel corpora: Multext 5.1. Datasets 5.1.1. Wikipedia
For the experiments analysing ESA implementation variants we used Wikipedia database dumps are linked across all three langua ges. Altogether, we used 166,48 4 articles in every language. knowledge resource and in particular to define universal (in our case called interlingual) concepts. tween interlingual articles and categories, defining the relationCATLINK egory link (  X  ,  X  )  X  CATLINK as follows:
We use a support threshold of 2 to select interlingual category links: ( lingual categories, resulting in 58,837 links.
 links and are therefore part of the category tree. 5.1.2. Parallel corpora
As datasets for the evaluation we us e two parallel corpora: Multext, preprocessing steps including elimination of stopwords, special characters and extremely short terms (length 5.2. Methodology of experiments and measures 5.2.1. Mate retrieval there exist thus four relevant documents for each query, i.e. the four equivalent articles or only the mate in the query language will give us a recall of 25%. 5.2.2. Evaluation measures setting, it measures how many of all translations have been found. of the mate in the ranked results list is. MRR is defined as follows: given query q . proportional to the position of the mate in the ranked retrieval list. MAP is defined as follows: with P@ k as Precision at cutoff rank k . 5.3. Evaluation of ESA model variants assess the combined impact of the best choices on the performance of the ESA model.
In summary the contributions of the experiments on ESA model variants are the following: 1. We identify best choices for the degrees of freedom of the ESA model on a CLIR scenario. 2. We show that the ESA model is sensitive to parameter settings, heavily influencing the retrieval results. tings, respectively. 5.3.1. Experimental settings dataset.
 To prove the significance of the improvement of our best settings (projection function at a confidence level of 0.01 are marked with  X  X  X  in Figs. 7 to 10 .
In the following we discuss the results of the different variations of the CL-ESA model: 5.3.2. Projection function We first used different values for the parameter m in the projection function is a good choice for both datasets.
 dimension projection function with 10,000 articles (  X  abs 5.3.3. Association Strength the normalization of the TFICF values (= cosine function) reduces the retrieval performance substantially. 5.3.4. Retrieval model 5.3.5. Discussion of the approach. For example, using TF c instead of RTF c number of dimensions taken into account).
 On the other hand, while we can confirm by our experiments that the settings in the original ESA model ( applied to.
 and Wikipedia articles. However, an examination of this remains for future work. 5.4. Concept spaces for multilingual scenarios focus on different concept spaces that are used by CL-ESA, Cat-ESA and Tree-ESA. (but the language differs from the language of the query).

In summary, the contributions of the experiments on multilingual scenarios are the following: 1. We present and analyze the performance of different CL-ESA models on a MLIR task.
ESA) to 39% (Multext dataset, Tree-ESA). we show how the performance in CLIR and MLIR tasks varies substantially for the same parametric choices. 5.4.1. Experimental settings evaluation measures we used Mean Average Precision (MAP) and Recall at cut-off level of 10 (R@10). 5.4.2. Document models 32% (JRC-Acquis) to the BoW model baseline and 39% (Multext) and 18% (JRC-Acquis) to CL-ESA. observe any significant difference between Cat-ESA and Tree-ESA.
 might be induced by the different distributions of these terms in each language. 5.4.3. Language Bias
MAP are mostly consistent when using the same query in English, German, French or Spanish. 5.4.4. CLIR vs. MLIR
MLIR. As these are different problems, good approaches to solve one setting might not be optimal on the other. 6. Related Work 6.1. Methods for CLIR/MLIR used to index new documents. Both LSI and LDA have been applied to cross-lingual retrieval tasks (see [18] ). 6.2. External vs. intrinsic concept de fi nitions the LSI and the LDA model. 6.3. Using background knowledge for IR improvements of up to 29% in retrieval performance for monolingual IR. disambiguation after translation. 6.4. Explicit semantic analysis
We have mentioned already different approaches for folding in between word pairs [26] . ESA has been also exploited in text classification approaches [25,22,27 plains some of the phenomena observed in previous work related to ESA.
Knoth et al. presented an approach to use cross-lingual ESA for cross-lingual link discovery [32] . 7. Conclusion settings even depend on whether we are considering a CLIR or MLIR task. open source. 13 Acknowledgements Commission under the Monnet Project (grant FP7-ICT-4-248458).

References
