 Joseph Mariani 1,2  X  Patrick Paroubek 1  X  Gil Francopoulo 2,3  X  Olivier Hamon 4 Abstract This paper analyzes the content of the proceedings of the Language Resources and Evaluation Conference (LREC) over the past 17 years (1998 X 2014), with the goal of gaining a picture of the LREC community and the topics that are most relevant to the field. We follow the methodology used in similar studies, including the survey of the IEEE ICASSP conference proceedings from 1976 to 1990, the survey of the Association of Computational Linguistics conference pro-ceedings over 50 years, and the survey of the proceedings of the conferences contained in the ISCA Archive over 25 years (1987 X 2012). We expand on results originally presented at LREC 2014, but include the proceedings of LREC 2014 itself in the study together with an analysis of various citation graphs. We show the evolution over time of the number of papers and authors, including their distribution by gender and affiliation, as well as collaborations and citation patterns among authors and papers, funding sources for reported research, and plagiarism and reuse in LREC papers; results for LREC are compared with similar results for major conferences in related fields. We also consider the evolution of research topics over time and identify the authors who introduced key terms. Finally, we propose and apply a measure of a researcher X  X  notability and provide the results for LREC authors. The study uses NLP methods that have been published in the corpus considered in the study. In addition to providing a revealing characterization of the LRE community, the study also demonstrates the need for establishing a system for unique identification of authors, papers and other sources to facilitate this type of analysis.
 Keywords ELRA Anthology Language resources Language processing systems evaluation Text analytics Social networks ISLRN Bibliometrics Scientometrics 1.1 Text analytics of scientific papers The application of text analytics to bodies of scientific papers has become an active area of research in recent years (see for example Li et al. 2006 ; Tang et al. 2008 ; Dunne et al. 2012 ; Osborne et al. 2013 ). For example, the Stanford Large Network Dataset Collection (SNAP) 1 is a recently launched effort to study research networks by providing social networks and collaboration and citation graphs for conferences in Astrophysics, High Energy Physics, General Relativity and Condensed Matter. Studies of research publication data mine conference and workshop proceedings to determine trends in publications within a given area or field, such as networks of collaboration and author and paper citation, author/topic pairings, topic shifts over time, and author and participant demographics, with the goal of better understanding research trends, collaborations, participation and publication data, etc. In the field of Speech and Natural Language Processing (SNLP), several studies of this type have recently been conducted, including the following:  X  ACL Anthology 2 (Radev et al. 2013 ) analysis, presented in several papers at the  X  Analysis of 25 years of research contained in the International Speech  X  Analysis of the proceedings of the TALN conference organized yearly by the  X  Results from the Saffron 6 project, which performs automatic analysis of Studies of this kind can reveal patterns and shifts that may otherwise go unnoticed, and which can ultimately affect perceptions and practice in a given field. For example, an analysis conducted on publications from the IEEE ICASSP conference series between 1976 and 1990 (Mariani 1990 ) showed that the percentage of papers on speech decreased over time, from about 50 % in 1976 to 30 % in 1990. Further analysis showed that the US produced most papers on speech (&gt;50 %), including in those years when the ICASSP conference took place outside the US; however at these conferences, the total participation increased, including a virtually undimin-ished level of US participation together with a dramatic increase in the number of European and Asian participants. As a result of this analysis, the speech community decided to begin organizing fully international conferences specifically devoted to spoken language processing, namely Eurospeech in Europe and ICSLP in Asia.
This paper reports on a comprehensive analysis of publications presented at the biennial Language Resources and Evaluation Conference (LREC) over the past 17 years, from its inception in 1998 through 2014. We first present an analysis of the number of papers and authors over time, including study of their gender, nationality and affiliation; collaboration among authors; citation among authors and papers; research funding sources; evolution of topics and those who introduced them; and reuse and plagiarism. We then propose a measure of a researcher X  X  notability in the Language Resources and Evaluation (LRE) scientific community based on this analysis. 1.2 The ELRA community and conference series analysis Activity in the area of LRE increased enormously over the past 30 years, due to the importance of language resources for research in the language sciences and the need for such resources in order to develop language processing systems based on statistical machine learning. ELRA, the European Language Resources Association (ELRA), was launched in 1995. In 1997, Joseph Mariani proposed adding a scientific dimension to ELRA X  X  language resource distribution activity by estab-lishing an international conference specifically devoted to the creation and use of language resources and their evaluation. The first LREC was held in 1998 in Granada (Spain), organized and chaired by Antonio Zampolli. Following its initial success, LREC has since been held biennially and is now chaired by Nicoletta Calzolari. At LREC 2014 in Reykjavik, Iceland, an analysis of the past 15 years of LREC conferences was presented (Mariani et al. 2014a ). The present article extends the previous analysis with the following additions: articles of the LREC 2014 conference itself, an analysis of citation graphs (authors and papers), and a proposal for measuring the notability of individual researchers based on various criteria. It also provides a comparison with similar analyses of major conferences in related research areas. 1.3 The NLP4NLP Speech and Natural Language Processing Analysis We produced a corpus containing research papers on spoken and written language processing, called the NLP4NLP corpus, a name chosen to reflect the fact that the study uses NLP methods that are the subject of the corpus content itself (Francopoulo et al. 2015a , b ). The NLP4NLP corpus contains papers from 32 conferences and journals on Natural Language Processing (NLP) and spoken language processing (SLP) published over 50 years (1965 X 2015) and including the LREC series (Table 1 ), thereby providing a good picture of research within the international SNLP community. We included material from conferences and journals only, as workshops may have widely varying ways of reviewing papers. The comparative analysis of the data contained in this corpus is presently ongoing and will be presented in a future paper. In the present paper, we used the entire corpus to study citations to and from LREC papers: it gives an analysis on how the LREC community globally considers and is being considered by its general scientific environment. We compared some of the results for LREC, which covers Language Resources and Evaluation for both spoken and written language processing, to those for two major conferences related to NLP (ACL conference) and SLP (ISCA conferences). A study of reuse and plagiarism in LREC papers was conducted using a subset of the NLP4NLP papers: those included in the ACL Anthology. As a convention, we refer to the conference publication as a document .A paper or article corresponds to a document that may have been published in one or several conference series when presented at a joint conference. We refer to individual authors and mention their authorships or contributions to a publication where they act as contributors . The same author may sign several papers at a given conference, as a single author or together with one or several co-authors. 2.1 The ELRA conferences Anthology This study covers the series of conferences contained in the LREC Anthology, initially assembled by Olivier Hamon, which contains the proceedings of all nine LREC conferences (see Table 2 ), covering a time span of 17 years (1998 X 2014). We have not included workshops organized as satellite events of LREC and other ELRA supported events in the current study. 2.2 Data and tools All LREC proceedings are freely available online on the ELRA website 7 as well as on the ACL Anthology website. Over the years, 4552 papers have been published in the nine LREC proceedings. The website includes metadata (list of authors and sessions, content of the sessions and, for each article, title, authors, affiliations, abstract, and bibliographic references) as well as the full content of the articles. All the documents are available in PDF, except the 1998 proceedings, which are available as scanned images.

As a first step, a preprocessing was applied to extract the textual content by means of PDFBox (Litchfield 2005 ) and when the document was a sequence of images, Tesseract-OCR 8 was called to produce a textual content. A benchmark to estimate the error rate of the extracted content was established based on a simple heuristics, which considers as  X  X  X ubbish X  X  character strings without match in any lexicon (unknown words). This estimation is computed as the number of unknown words divided by the number of words. The number of errors was computed from the result of the morphological module of TagParser (Francopoulo 2007 ), a deep industrial parser based on a broad English lexicon and Global Atlas (a knowledge base containing more than one million words from 18 Wikipedias) (Francopoulo et al. 2013 ). Variations in performance quality measures were used to control the parameterization of the content preprocessing tools.

Following this content extraction, another step in our preprocessing was dedicated to split the content into abstract, body and references sections. Initially, we attempted to use ParsCit (Councill et al. 2008 ), which had been used to extract citations from the ACL Anthology; however, the program was not suited for Slavic, German, extended Latin, and phonetic alphabets included in our data, and retraining the program would have required too much time. We therefore created a small set of rules in Java to extract the abstract and body of the papers and compute their quality, which yielded a 2.5 % higher performance than ParsCit.

The result of the preprocessing is summarized in Table 3 , and it can be noticed that the corpus contains about 17 million words, and that overall quality is good (the number of unknown words is higher for 1998 due to the fact that the textual data was obtained via OCR on that year).

After this preprocessing phase, the metadata and contents are ready to be processed by higher level tools based on the R statistical suite (The R Journal 2012 ), iGraph (Csa  X  rdi and Nepusz 2006 ), the search engine swish-e, 9 RankChart and a series of Java programs that we wrote. 2.3 Overall analysis: papers and authors The study of authors is problematic due to variations undergone by the same name across different publications (family name and given name, initials, middle initials, ordering, married name, etc.). It therefore required a tedious semi-automatic cleaning process (Mariani et al. 2014b ). On an initial total of more than 13,500 authors X  names, about 6000 family names or given names had to be corrected, resulting in a list of 7282 different authors (i.e., a 40 % reduction). This suggests a need to determine ways to uniquely identify researchers, see Joerg et al. ( 2012 ) for a proposal. Note that this last number of authors having published at the 9 LREC conferences is larger than the number of authors having published in the 36 ACL conferences (5012), but much lower than the number of authors having published at the 27 ISCA conferences (15,749).

The total number of papers published in the conference series is 4552 (Table 2 ), with a steady increase over time from 212 papers in 1998 to 517 at LREC 2004, followed by roughly the same number in 2006 (Genoa), and then by a steady increase to the maximum 744 at LREC 2014 (Fig. 1 ). This number of papers is close to the number of papers published at ACL (4262), but much lower than the number of papers published at ISCA (17,592). Accordingly, the number of authorships also rose steadily, up to 1709 at LREC 2004. It then slightly decreased to 1667 at LREC 2006 and has kept increasing since then to reach 2724 at LREC 2014 (Fig. 1 ). The rejection rate is stable, at about 40 % of the submitted papers.

The number of co-authors per paper is most often two or three (Fig. 2 ). The largest number of co-authors for a paper is 44, in a paper published at LREC 2014. The average number of co-authors per paper increased over time, from 2.9 in 1998 up to 3.7 in 2014 (i.e. 0.8 more authors on average) (Fig. 3 ). It is interesting to note that the number of papers with a single author was 32 % in 1998 and decreased to 10 % in 2014, while the number of papers with three authors or more was 40 % in 1998 and went up to 67 % in 2014. This clearly demonstrates the change in the way research is being conducted, going progressively from individual research investigations to large projects conducted within teams or in consortia from international projects and programs.

We studied the number of repeat authors at successive conferences (Table 4 ). For each conference, we identified the authors who did not publish at the previous conference ( new authors ). We also studied those who had not published at any previous LREC conference ( completely new authors ).

The ratio of the total number of papers (4552) to the overall number of different authors having published at LREC (7282) represents the global productivity of the community: each author published on average 0.6 papers over 17 years. The ratio of the total number of authorships (15,340) to the overall number of different authors (7282) represents the individual productivity of each author: each author contributed on average in 2.1 papers. The ratio of the number of different authors to the number of authorships at each conference reflects the variety of authors. This ratio would be 100 % if each author X  X  name appears on a single paper. We define author redundancy as 100 %-author variety . It appears that this redundancy slightly increased over time and has now stabilized at about 22 % (Fig. 4 ).

We then studied the authors X  renewal. It clearly showed (Fig. 5 a) that the number of different authors from one conference to the next conference has been high and increased over time, apart from a slight decrease in 2006 due to the smaller number of papers. The number of new authors from one conference to the next similarly increased over time until LREC 2008, where there were about 1200 new authors compared with LREC 2006. The number then remained steady, with a turnover of about 1200 different authors each year until 2014, where it still increased to 1502 new authors. The same trend applies to the number of completely new authors, which increased every year (excepting 2006) up to LREC 2010, with 951 new authors that year; the number then slightly decreased to 928 in 2012 but increased to nearly 1200 in 2014: 1200 authors who had never published at LREC before!
This same trend applies to percentages of different authors from one year to the next (Fig. 5 b), which decreased from 78 % in 2000 to 71 % in 2014, while the number of completely new authors decreased from 78 % in 2000 to about 56 % in 2014. This suggests a stabilization of the research community over time, but it also reflects a measure of the existence of  X  X  X ew blood X  X  in the field X  X his number being significantly higher than in other more established communities (40 % in the case of ACL or Interspeech 10 ).

Author gender An author gender study was performed with the help of a lexicon of 27,509 given names with gender information (66 % male, 31 % female, 3 % epicene 11 ). As noted above, variations due to different cultural habits for naming people (single vs. multiple given names, family vs. clan names, inclusion of honorific particles, ordering of the components etc.) (Fu et al. 2010 ), and changes in editorial practices and sharing of the same name by large groups of individuals contribute to make identification by name a difficult problem. In some cases, we only had an initial for the first name, which made gender guessing impossible unless the same person appears with his/her first name in full in another publication. Although the result of the automatic processing was hand-checked by an expert of the domain for the most frequent names, the results presented here should be considered with caution, allowing for an error margin.

The analysis over the nine conferences shows that 63 % of the authors are male, while 27 % of the authors are female, 1 % are of indeterminate gender, and 9 % are of unknown gender. If we assume that the authors of indeterminate and unknown gender have the same gender distribution as the ones that are categorized, male authors account for 70 % and female authors for 30 %, compared with 80/20 % for ACL and ISCA.

The analysis of the authors X  gender over time (Fig. 6 ) shows a relative stability of male authors around 70 % and of female authors around 30 %.

Author nationality We consider here that the country of his/her affiliation is the author X  X  nationality. When an author mentions several affiliations from different countries, he/she is counted as one contributor for the two first countries he/she mentions. Over the nine LREC conferences, authors from 78 different countries have published papers. The 12  X  X  X ost productive X  X  countries represent more than 80 % of the contributors: USA (15 %), Germany (12 %), France (11 %). Spain (8 %), Italy (8 %), UK (6 %), Japan (6 %), The Netherlands (4 %), Czech Republic, Portugal, Sweden, Greece, Belgium and Switzerland (2 %) (Fig. 7 ).
By continent, Europe has the largest share (70 %), followed by the Americas (17 %) and Asia (10 %). Africa, Oceania and the Middle East only represent 3 % (Fig. 8 ).

As for the evolution of the share of authors per country over time, for the eight countries totaling 4 % or more of the number of authors overall (Fig. 9 ), the share of the USA remained stable at about 15 %, the share of France initially ranked first and then strongly decreased, but is now back in the top 3 countries with about 12 %, together with Germany, which increased its share over time. A second set, currently at around 6 %, comprises Spain and Italy, both slightly decreasing their share over time, with a larger participation when the conference takes place in the country (Granada in 1998 and Las Palmas in 2002 for Spain, Genoa in 2006 for Italy). UK, Japan and The Netherlands are in a third set at around 4 %, with large variations over time for UK and Japan, and greater stability for The Netherlands.

The number of publishing countries (Fig. 10 ) and the share of emerging countries, especially India, PR China (including China Mainland, Hong Kong and Macau), and Brazil (Fig. 11 ), have increased over the years. Three new countries participated in LREC 2014: Saudi Arabia, Columbia, and Qatar, the last of which had a particularly strong increase in participation (from 0 to 8 papers).

If we cluster countries by continent (Fig. 12 ), we see that the share of Europe, around 70 %, slightly decreased over time until 2008, and has increased since then, while the Americas stayed very stable at around 20 % and Asia at around 10 %. The share of other countries is slowly and slightly increasing, but is still very low. Author affiliation The authors come from more than 1200 different institutions. Table 5 gives the list of the 20 institutions with the largest number of authorships, which comprises a total of 4014 authorships (representing about 25 % of the authorships) in 1636 papers [about 30 % of the papers). The mean number of authorships per paper varies from fewer than two (Universita  X  t Stuttgart, ELDA) to more than three (LDC, University of the Basque Country). At LREC 2014, the rank of LIMSI-CNRS, Charles University, Carnegie Mellon University and Universitat Pompeu Fabra increased, while the Universitat Politecnica de Catalunya decreased. Author production Nineteen authors published at all nine conferences (Nicoletta Calzolari, Nick Campbell, Khalid Choukri, Christopher Cieri, Thierry Declerck, Robert Gaizauskas, Eva Hajicova, Nancy Ide, Sadao Kurohashi, Mark Liberman, Bernardo Magnini, Simonetta Montemagni, Patrick Paroubek, Uwe Quasthoff, Bolette Sandford Pedersen, Takenobu Tokunaga, Dan Tufis, Hans Uszkoreit and Henk Van Den Heuvel). 5060 authors (70 % of the 7282 authors) published at a single conference (Fig. 13 ).

The most productive author published 48 papers, while 4643 authors (64 % of the 7282 authors) published only one paper (Fig. 14 ). Table 6 gives the list of the 10 most productive authors, accompanied by the number of papers they published as a single author. Table 7 gives the number of authors who published papers as single authors. 6806 authors (93 % of the authors) never published a paper as single author.
 2.4 Collaborations The most collaborating author published with 161 different co-authors, while 148 authors always published alone (Fig. 15 ). On average, an author collaborated with 6.8 other authors. Twelve authors published with 80 or more different co-authors (Table 8 ).
We may also consider the number of collaborations, possibly with the same co-authors. Table 9 gives the list of the 10 authors who have the largest number of collaborations.

A collaboration graph 12 (CollG) is a model of a social network where the nodes (or vertices) represent participants of that network (usually individual people) and where two distinct participants are joined by an edge whenever there is a collaborative relationship between them. As opposed to a citation graph, a CollG is undirected. It contains no loop -edge (an author does not collaborate with him/ herself) and no multiple edges (there is a single edge between two authors, whatever the number of papers they published together). The CollG needs not be fully connected, that is, isolated nodes represent people who never co-authored a joint paper. Those who are connected constitute a connected component . Cliques are fully connected components where all authors published with one another. The collaboration distance is the geodesic distance, or path-length, between two nodes in a CollG, which is equal to the smallest number of edges in an edge-path, or collaboration path , connecting them. The diameter of the CollG is the longest collaboration path in that graph. If no path connecting two nodes in a CollG exists, the collaboration distance between them is considered to be infinite. The degree of a node (number of edges attached to the node) reflects the number of co-authors associated with each author, as an absolute measure of his/her collaboration ability. The clustering coefficient of a node is a measure of the degree to which its neighboring nodes tend to cluster together: i.e. how close they are to form a clique. The density of a graph is the fraction of all possible edges that actually exists in the CollG, thus providing a measure of the density of collaboration: if all authors have published at least one paper with all the other authors, the density of collaboration of the graph would be equal to 1.

The LREC CollG contains 7282 nodes corresponding to the 7282 different authors who have published at LREC. There are 24,801 edges. The diameter is 15 [similar to the ACL conference (17) or ISCA (14)]. One hundred forty (140) pairs have this distance (192 for ISCA, 15 for ACL). The max degree (corresponding to the author who collaborated with the largest number of different co-authors) is 161 [close to ISCA (192), but much larger than ACL (73)] and the mean degree is 6.8, close to ISCA (6.2) but larger than ACL (4.1). This shows that there may be more collaboration within LREC and ISCA than in ACL. The density is 0.0009 [comparable to ACL (0.0008), and twice more dense than ISCA (0.0004), given that the number of authors is much larger at ISCA]. The average clustering coefficient is 0.7 [slightly larger than ACL or ISCA (0.6)].
 As shown in Table 10 , the LREC CollG contains 477 connected components. 13 The largest one groups 5886 authors, which means that 81 % of the 7282 LREC authors are connected through a collaboration path [similarly for ISCA (85 %), but only 67 % for ACL], reflecting the cohesion of the community. The authors of the largest connected component published 3815 papers (84 % of the total number of papers), and the average path length is 5.5 [similar to ISCA and shorter than ACL (6.2)]. The remaining connected components contain far fewer authors, each of whom has never published with any of the authors of the largest connected component; these components tend to represent small communities often related to the study of a specific language. As already mentioned, 2 % of the authors (148) have never published jointly with any other author. As it turned out, in our corpus the largest clique could be identified by simply looking at the paper with the largest number of authors (44 authors).

Measures of centrality We explored the role of each author in the CollG in order to assess his/her centrality. In graph theory, there exist several types of centrality measures (Freeman 1978 ). The Closeness distance has been introduced in Human Sciences to measure the efficiency of a Communication Network (Bavelas 1948 , 1950 ). It is based on the shortest geodesic distance between two authors regardless of the number of collaborations between the two authors. The Closeness centrality is computed as the average closeness distance of an author with all other authors belonging to the same connected component. More precisely, we use the harmonic centrality which is a refinement introduced recently by Rochat ( 2009 ) of the original formula to take into account the whole graph in one step instead of each connected component separately. The degree centrality is simply the number of different co-authors of each author, i.e. the number of edges attached to the corresponding node. The betweenness centrality is based on the number of paths crossing a node and reflects the importance of an author as a bridge across different sets of authors (or sub-communities).

Looking at Table 11 , we see that some authors who appear in the Top 10 according to the Closeness Centrality also appear in the other two types of centrality, eventually with a different ranking, while others do not. 2.5 Citations Unlike the CollG, a citation graph (CitG) is directed. In an authors citation graph (ACG), nodes (or vertices) represent individual authors. We may consider the citing authors graph ( CgAG ), in which a citing author is linked to all the authors of the papers that he/she cites by an edge directed towards those authors; and the cited authors graph ( CdAG ), where each cited author is linked to the authors who cite him/her by an edge directed towards this author. These graphs may have loop -edges , as an author may cite and be cited by him/herself, but they have no multiple edges : there is only one edge between two authors, whatever the number of times an author cites or is being cited by another author.

In a papers citation graph (PCG), nodes represent individual papers. Here also, we may consider the citing papers graph ( CgPG ), in which a paper is linked to all the papers it cites by an edge directed towards those papers; and the cited papers graph ( CdPG ), where each paper is linked to all the papers that cite it by an edge directed towards those papers. These graphs contain no loop -edge , as a paper does not cite itself, and no multiple edges : there is only one edge between two papers, whatever the number of times a paper cite or is being cited by another paper.
The citation graphs need not be connected, as an author may not cite any author and may not be cited by any author, not even him/herself, or a paper may not cite any paper and may not be cited by any other paper; in this cases, corresponding authors or papers appear as isolated nodes in the citation graphs. The nodes that are connected through a directed path (Author A cites Author B and Author B cites Author C and Author C cites Author A, for example), constitute a strongly connected component . The nodes that are connected in both directions constitute a symmetric strongly connected component ; they are common in ACGs (Author A cites Author B and Author B cites Author A, for example), but uncommon in PCGs (for example, if Paper M cites Paper N, it is very unlikely that Paper N will cite Paper M, as papers typically reference papers that have been already published. It may however happen in case of simultaneous publications).

The citation distance between two nodes is the smallest number of directed edges in an edge-path connecting them. The diameter of a citation graph is the longest path in the graph, which will be identical in both the citing and cited graphs. If no path connecting two nodes in a citation graph exists, the citation distance between them is said to be infinite. In a citing graph, the degree of a node (the number of directed edges issued from that node) reflects the absolute number of authors (or papers) cited by each author (or paper). In a cited graph, the degree of a node reflects the absolute number of authors (or papers) citing each author (or paper). As in the CollG, the clustering coefficient of a node is a measure of the degree to which its neighbors tend to cluster together. The density of a citation graph, which is the fraction of possible edges that exist in the graph, provides a measure of the density of citation: if all authors (or papers) cite at least once each other author (or paper), the density of citation of the graph would be equal to 1.

We studied citations in papers that are accessible in digital form, including both the OCR versions in the 1998 proceedings and the PDF versions from 2000 to 2014. Only 116 of the 4552 papers do not contain a list of references, and the number of missing references decreases over time (see Table 3 ). We studied the four Citing and Cited Authors and Papers Graphs, using the LREC conference series to represent the LREC community and the NLP4NLP corpus, 14 which also includes LREC, to represent the general Speech and Natural Language Processing scientific community (SNLP).

We studied:  X  the citation in LREC papers of other LREC papers ( Internal Papers Citations :  X  the citation in LREC papers of NLP4NLP papers ( Outgoing Global Papers  X  the citation in NLP4NLP papers of LREC papers ( Ingoing Global Papers Similarly, we also studied:  X  the citation by LREC authors of LREC authors ( Internal Authors Citations ),  X  the citation by LREC authors of SNLP authors ( Outgoing Global Authors Citations ),  X  the citation by SNLP authors of LREC authors ( Ingoing Global Authors We give some elements of comparison with ACL and ISCA conference series, keeping in mind that the time scale and the number of venues that are considered are different (9 venues over 17 years for LREC, 28 venues over 27 years for ISCA, and 36 venues over 35 years for ACL). We finally studied the citation in LREC papers of acknowledged Funding Agencies. 2.5.1 Authors citations We first consider internal authors citations : the citation by authors in their LREC papers of authors for their LREC papers.

Internal LREC authors citing activity ( CgAG ) What is the likelihood that LREC authors cite each other? As shown in Table 12 , 4181 (57 %) of the 7282 LREC authors cite at least one author having published at LREC (compared with 70 % for ACL, 61 % for ISCA). The mean degree (average number of LREC authors being cited by LREC authors) is 12 (18 for ISCA, 25 for ACL) and the max degree (corresponding to the author who cites the largest number of LREC authors) is 200. It seems therefore that LREC authors have a tendency to less cite their fellow LREC authors than ISCA, and furthermore ACL authors do. The density of the graph is 0.0008, and the diameter is 10. The 10 LREC authors who cite the largest number of LREC references are: Christopher Cieri (63 references), Khalid Choukri (61), Stephanie M Strassel (60), Patrick Paroubek (58), Nicoletta Calzolari (54), Monica Monachini (49), Anne Vilnat (48), Nu  X  ria Bel (48), Dan Tufis  X  (46) and Ulrich Heid (45).

Internal renown of LREC authors ( CdAG ): How often are LREC authors cited by other LREC authors? We see that 3140 (43 %) of the 7282 LREC authors are cited at least once by an author having published at LREC. This percentage is less than ISCA (49) and ACL (50), in agreement with the respective citing habits. The max degree is 514. The diameter, mean degree and density are the same as for the CgAG. Table 13 gives the list of the 10 most cited LREC authors in LREC papers, according to this measure of citation.

Strongly connected components The LREC ACGs have 3680 strongly connected components (Table 14 ). The largest strongly connected component has 3526 nodes among the 7282 LREC authors (48 %). This is comparable to ISCA (47 %), but less than ACL (60 %), which illustrates a less focused network of citations.

Symmetric strongly connected component The number of symmetric strongly connected components is 4866. The largest symmetric strongly connected component includes 43 authors who all cite each other, and mostly correspond to partners of the French Quaero project.

We now consider global authors citations citation by LREC authors of SNLP authors and by SNLP authors of LREC authors.

Global authors citing activity What is the propensity of the LREC authors to include citations of SNLP authors of NLP4NLP papers? As shown in Table 15 , 6232 (86 %) of the 7282 LREC authors cite at least one SNLP author having published in the NLP4NLP corpus. The max degree is 257 and the mean degree is 19 [close to ISCA (21), but less than ACL (31)]. We see here that LREC authors have similar general citing habits than ISCA authors, but that ACL authors have a larger propensity to citations. The 10 LREC authors who cite the most are: Nu  X  ria Bel (153 references), Martha Palmer (130), Dan Tufis (126), Massimo Poesio (113), Beno X   X  t Sagot (112), Alessandro Lenci (111), Robert J Gaizauskas (110), Patrick Paroubek (107), German Rigau (107) and Sophie Rosset (104).

Global renown of LREC authors How often are LREC authors cited by SNLP authors in the NLP4NLP papers? As shown in Table 16 , 5202 (71 %) of the 7282 LREC authors are cited at least once for a paper presented at LREC. The max degree is 667 and the mean degree is 13 [close to ISCA (16), but less than ACL (39)]. This shows that LREC and ISCA authors have about the same general audience, but less than ACL authors. We also noticed that 38 % of LREC authors are never cited in NLP4NLP papers. After checking Google Scholar, it appears that many of those never cited authors come from neighboring research domains (machine learning, medical engineering, phonetics, general linguistics), where they may be largely cited. Table 17 gives the list of the 10 most cited LREC authors in NLP4NLP papers.

Global renown of authors in LREC papers Who are the SNLP authors most cited in LREC papers ? Table 18 gives the list of the 10 most cited SNLP authors. 2.5.2 Papers citations Here also, we first consider internal papers citations : the citation in LREC papers of LREC papers.
 Internal papers citation activity ( CgPG ) How often do LREC papers cite other LREC papers? As shown in Table 19 , 2202 (48 %) of the 4552 LREC papers cite at least another paper having been published at LREC [close to ISCA (53 %), but less than ACL (67 %)]. The Max Degree is 19 (corresponding to the paper that cites the largest number of LREC papers) and the mean degree (average number of LREC papers being cited in other LREC papers) is 0.9 (2.3 for ACL, 1.2 for ISCA). This is in agreement with the results of the internal citing authors analysis. The Density of the graph is 0.00020 and the diameter is 11. The most citing paper is  X  X  X ultimodal Russian Corpus (MURCO): First Steps X  X  published by E. Grishina at LREC 2010, which contains 19 references to LREC papers.

Internal renown of LREC papers ( CdPG ) Which LREC papers are most cited by other LREC papers? 1606 (35 %) of the 4552 LREC papers are cited at least once by another paper published at LREC (48 % for ACL, 40 % for ISCA, but over a longer time period). The max degree is 40. The diameter, mean degree and density are the same as for the CgPG. Table 20 gives the list of the 10 most cited LREC papers in LREC papers.

Strongly connected components The LREC PCGs have 4461 strongly connected components (Table 21 ). The largest strongly connected component has only 4 nodes among the 4552 LREC papers (3 for ACL, 5 for ISCA). Those four papers were all published at LREC 2010 and cite each other as they use the same Dutch Reference Corpus .

We now consider global papers citations : citation in LREC papers of NLP4NLP papers and of LREC papers in NLP4NLP papers.
 Global LREC papers citation activity As shown in Table 22 , 3725 (82 %) of the LREC 4552 papers cite at least one other paper published in the NLP4NLP set of 32 conferences and journals [similar to ACL (86 %), but larger than ISCA (69 %)]. The max degree is 32: the paper containing the most citations is  X  X  X reating and using large monolingual parallel corpora for sentential paraphrase generation X  X  published by S. Wubben, A. Van Den Bosch and E. J. Krahmer at LREC 2014, which contains 32 references to NLP4NLP papers. The mean degree (average number of NLP4NLP papers cited in LREC papers) is 3.5 [larger than ISCA (2.8), but less than ACL (6.7)].
NLP4NLP papers citing LREC papers The NLP4NLP paper citing the largest number of LREC papers is actually the already mentioned LREC paper:  X  X  X ultimodal Russian Corpus (MURCO): First Steps X  X  published by E. Grishina at LREC 2010, which contains 19 references to LREC papers.

Global renown of LREC papers How many LREC papers are cited in NLP4NLP papers? As shown in Table 23 , 2279 (50 %) of the 4552 LREC papers are cited by at least another paper having been published in the NLP4NLP set of 32 conferences and journals [less than ACL (73 %), but similar to ISCA (49 %)]. The max degree is 262 (corresponding to the paper that is cited by the largest number of NLP4NLP papers: 1223 for ACL, 865 for ISCA, but over a longer time period). The mean degree (average number of LREC papers cited in NLP4NLP papers) is 2.4 [similar to ISCA (2.1), but less than ACL (9.1)]. We may thus conclude that LREC papers have about the same general audience than ISCA papers, but less audience than ACL papers, however over a much shorter time period than both of them. Table 24 gives the list of the 10 most cited LREC papers in NLP4NLP papers.

Global renown of NLP4NLP papers in LREC papers Table 25 gives the list of the 10 most cited NLP4NLP papers in LREC papers. None are LREC papers. 2.5.3 Impact factors and h-indexes The mean degree of the Internal CdPG expresses the degree to which the LREC community cites itself. An LREC paper is cited less than once on average (0.9) in LREC papers, which is less than ACL (2.3), but comparable to ISCA (1.2), while over a much shorter time period. The mean degree of the LREC Ingoing Global CdPG expresses the impact factor of LREC in the SNLP community, as represented by the NLP4NLP corpus. An LREC paper is cited 2.4 times on average, which is comparable to ISCA (2.1) but less than ACL (9.1). The h-index of LREC according to the NLP4NLP set of 32 conferences and journals is 35: i.e. 35 papers published at LREC are cited 35 times or more in NLP4NLP papers (71 for ACL, 45 for ISCA). The h-index of LREC according to LREC (internal citations) is 17: i.e. 17 papers published at LREC are cited 17 times or more in LREC papers (32 for ACL, 29 for ISCA). However, it should be stressed once again that both ACL and ISCA conferences cover a much longer time period than LREC.

As of October 2015, Google Scholar 15 places LREC 4th in the ranking of computational linguistics conferences with an h5 index of 38 within the last 5 years (therefore on the same citation time period) and an h5-median mean of 64, following ACL, EMNLP, and NAACL, and on a par with COLING (38 and 59) and ISCA Interspeech (39 and 70) conferences. Interspeech is in a different category ( X  X  X ignal Processing X  X ), and the new ISCA policy of opening the ISCA Archive to all, not only to members, has significantly increased the number of references to Interspeech papers. According to Google Scholar, the most cited LREC paper published in the past 5 years is  X  X  X witter as a Corpus for Sentiment Analysis and Opinion Mining. X  X  published at LREC 2010 by A. Pak and P. Paroubek, with 883 citations. 2.5.4 Most cited funding agencies We studied the funding agencies mentioned in the acknowledgment section of LREC papers (e.g.  X  X  X upported/funded/financed by ...  X  X ,  X  X  X upport/funding/grant/ fellowship from/of ...  X  X ), in order to estimate the support of public research funding in the different countries, to study later the way it is organized within those different countries, and analyze whether this funding has an influence on the research topics. We should stress that it may also reflect the requirements of the various agencies to acknowledge their support, or the acknowledgement habits in various countries.
If we consider the 12 most cited entities (Fig. 17 ), we see that the EU at the EC level ranks first (with more than 500 citations), followed by a set comprising Germany and the USA (about 270 citations). A third set comprises France (211) and Spain (181). It is followed by a set of 4 (UK, Czech Republic, The Netherlands and Japan) (around 90) and finally by a set of 3 (Portugal, Italy and Belgium) (40 X 70). There is a high correlation between the national production (see Fig. 7 ) and the national funding.

Among the 12 most cited agencies (Fig. 18 ), the European Commission (EC) is far out in front in first place, including all of its various programs [Framework Programs (FP), European Regional Development Fund (ERDF), Marie Curie grants, Structural Funds, etc.]. Next come the US National Science Foundation (NSF) and the German Research Foundation [ Deutsche Forschungsgemeinschaft (DFG)]. They are followed by the Spanish Ministry in charge of Science (the title of which varies over time), the French National Research Agency [ Agence Nationale de la Recherche (ANR)] created in 2005, the German Ministry of Education and Research [ Bundesministerium f X r Bildung und Forschung (BMBF)], the US Department of Defense DARPA and IARPA agencies, the UK Engineering and Physical Science Research Council (EPSRC), the French Ministry in charge of Industrial Affairs (MinEFi), the Netherlands Organization for Scientific Research (NWO), the Portuguese FCT ( Funda X  X o para a Ci X ncia e a Tecnologia ) and the Czech Ministry of Education [without including the grants of the Czech Science Foundation (GACR)]. 2.6 Topics Modeling the topics of a research field is a challenge in NLP (see for example Paul and Girju 2009 ; Hall et al. 2008 ). Here, our objectives were twofold: (1) to compute the most frequent terms used in the domain, (2) to study their variation over time. Like the study of citations, our initial input is the textual content of the papers available in a digital format apart from the proceedings of 1998, and a small set of papers from the other years that had been scanned. Over these 17 years, the archives contain a grand total of 17,604,367 words, mostly in English, as shown in Table 3 .
Because our aim is to study the terms of the NLP domain, it was necessary to avoid noise from phrases that are used in other senses in the English language. We therefore adopted a contrastive approach, using the same strategy implemented in TermoStat (Drouin 2004 ). For this purpose, as a first step, we processed a vast number of English texts that were not research papers in order to compute a statistical language profile. To accomplish this, we applied a deep syntactic parser called TagParser 16 to produce the noun phrases in each text. For each sentence, we kept only the noun phrases with a regular noun as a head, thus excluding the situations where a pronoun, date, or number is the head. We retained the various combinations of sequences of adjectives, prepositions and nouns excluding initial determiners using unigrams, bigrams and trigrams sequences and stored the resulting statistical language model. This process was applied on a corpus containing the British National Corpus (aka BNC), 17 the Open American National Corpus (aka OANC 18 ), the Suzanne corpus release-5, 19 the English EuroParl archives (years 1999 until 2009), 20 plus a small collection of newspapers in the domain of sports, politics and economy, comprising a total of 200 M words. It should be noted that, in selecting this corpus, we took care to avoid any texts dealing with Natural Language Processing.

In a second step, we parsed the LREC Anthology with the same filters and used our language model to distinguish LREC-specific terms from common ones. We worked from the hypothesis that when a sequence of words is inside the Anthology and not inside the general language profile, the term is specific to the field of language resources and evaluation. The 17,604,367 word content in 4549 documents reduces to 17,599,049 in 4539 documents when considering only the papers written in English. It includes 398,345 different terms (unigrams, bigrams and trigrams) and 1932,962 term occurrences, provided that this number counts all the occurrences of all sizes and does not restrict to the longest terms, thus counting a great number of overlapping situations between fragments of texts.

The twenty most frequent terms in the field of language resources and evaluation were computed over the period of 17 years, according to the following strategy. First, the most frequent terms were computed in a raw manner, and secondly the synonyms sets (aka synsets) for all most 50 frequent terms of each year (which are frequently the same from one year to another) were manually declared in the lexicon of TagParser. Around the term synset, we gathered the variation in upper/lower case, singular/plural number, US/UK difference, abbreviation/expanded form and absence/presence of a semantically neutral adjective, like  X  X  X rtificial X  X  in  X  X  X rtificial neural network X  X . Thirdly, the most frequent terms were recomputed with the amended lexicon. The 20 most frequent terms over time (1998 X 2014) are the following (Table 26 ). 2.6.1 Change in topics We studied the ranking among the 50 most popular terms (mixing unigrams, bigrams and trigrams) representing several topics of interest. The terms are followed by their ranking in 2 years: 1998 and 2014 (Rank 1998/Rank 2014). We first studied the following terms, which stayed in the top 20 over 17 years: Annotation (1/1), Parser (3/6), POS (2/2), Wordnet (7/11), NP (4/28), Tagger (5/14) and Lemma (14/ 5) (Fig. 19 ).

We also studied several terms that became more popular over time: Annotator (39/3), Synset (27/17), XML (Less than 100/26), Wikipedia (less than 100/15), Metadata (less than 100/16) and Treebank (80/13) (Fig. 20 ).

Among terms losing popularity, we find Encoding (9/less than 100) and SGML (8/less than 100) (see Fig. 21 ).

We also studied the rise in the use of the terms  X  X  bigram  X  X  ,  X  X  trigram  X  X  and  X  X  NGram  X  X , with  X  X  Ngram  X  X  increasing the most (Fig. 22 ).
 2.6.2 Specific study of the  X  X 17-year best friends X  X  of  X  X  X opular X  X  terms A selection of terms was studied with respect to their use over time and their semantic closeness, in order to detect trends and related properties among the terms characterizing the domain.
The previous diagrams were computed over whole texts in order to obtain a global estimation of the evolution of the various terms. However, in individual papers, the topics mentioned can be rather heterogeneous: for example, the paper may provide a survey of the state of the art, mentioning topics that are no longer major areas of activity, or, similarly, discuss future directions. We therefore decided to study the terms that appear in abstracts based on the hypothesis that the abstract is, in general, more targeted to the paper topic.

We implemented an algorithm that iterates on a term selected as a  X  X  X ocus X  X  and computes the  X  X  X est friends X  X  of this term, that is, the terms that appear the most frequently in the same abstract. This provides a selection of terms that can be analyzed using the general ranking algorithm used in the previous sections. In other words, we use a term X  X   X  X  X est friends X  X  as a filter. Studying the  X  X  X est friends X  X  may also help in identifying new trends in a particular area; for example (Fig. 23 ), annotation ranked first in 1998 and remained in that position through 2014, while its best friend annotator ranked 39th in 1998 but rose to 3rd in 2014, revealing a dramatically increased importance of the role of annotators (and/or terms like  X  X  X nnotator agreement X  X ) in the annotation process over this period. We also see that annotation of POS and treebanks for parsers was followed by the annotation of coreferences , while NLP and ontology were always mentioned, and while the XML format became usual and the notion of Datasets became popular.

As it appears in Fig. 24 , the need for Metrics or Evaluation Metrics was clearly identified very early for the evaluation of Machine Translation ( MT )or Parsers .It became especially popular starting in 2006 with the success of the Statistical Machine Translation ( SMT ) approach. BLEU is a very popular metric for MT , which was included in the evaluation campaigns regularly organized by NIST . Metrics is also somehow related to Annotation , Annotators and Ontology .

Figure 25 shows that Synset was typically coupled with the mention of Wordnet over the years, but also with ontology and semantics . Framenet came later, along with references to Eurowordnet and SUMO ( Suggested Upper Merged Ontology ). The relationship with disambiguation in general, and Word Sense Disambiguation ( WSD ) in particular is clear, wile the graph also reflects the recent interest in polarity through opinion mining. 2.6.3 Tag clouds for frequent terms The aim of this section is to provide a global estimation of the main terms used in specific years as well as an indication of the stability of the terms over the years. For this purpose, we use TagCrowd 21 to generate tag clouds. Figures 26 , 27 and 28 show the tag clouds for the first and last LRECs (1998 and 2014) and for the one at the midpoint between (2006).
 Globally, it appears that most frequent terms remained constant across the years. Annotation was already popular in 1998 and remained popular since. Treebank , wordnet , POS and MT are still present in 2014, while CLIR , WER and encoding disappeared and while classifier , dataset , tweet and wikipedia appeared more recently. 2.6.4 New terms introduced by the authors We studied when and who introduced new terms, as a mark of the innovative ability of various authors, which may also provide an estimate of their contribution to the advances of the scientific domain. We make the hypothesis that an innovation is induced by the introduction of a term which was previously unused in the community and then became popular. We consider the 4539 documents written in English and the 6970 authors who used the 398,345 terms contained in those documents. 28,877 of those terms are present in the first proceedings (1998), which we consider as the starting point for the introduction of new terms; of these, 88,833 occur in the 2014 proceedings. We then take into account the terms which are present in 2014 but not in 1998, and which are of scientific interest (excluding author X  X  names, unless they correspond to a specific algorithm or method, city names, laboratory names, etc.). This results in a set of 81,870 terms that therefore appeared at a certain time between 2000 and 2014. For each of these terms, starting from the second LREC (LREC 2000) proceedings, we determine the author(s) who introduced the term, referred to as the  X  X  X nventor(s) X  X  of the term. This may yield several names, as the papers could be co-authored or the term could be mentioned in more than one paper in a given year. We compute the overall impact factor (OIF) of a term as the ratio between the number of papers mentioning it (its  X  X  X resence X  X  in papers) in 2014 and the number of papers that mentioned it in the year when it first appeared.

Table 27 provides the ranked list of the 20 most popular terms based on the presence of the term in 2014. For example, the term SVM appeared first in the year 2000, when it was mentioned four times in two papers. In 2014, SVM was mentioned 491 times in 94 papers, yielding an OIF of 94/2 = 47. From its first mention in 2004 to 2014, the number of occurrences of  X  X  X ikipedia X  X  increased by a factor of 864, and the number of papers including that term by a factor of 132! Similarly, the study of  X  X  X weets X  X  became quickly very popular, and the number of mentions of the term goes from only once in the year of its introduction in 2010 to a close to thousand in 2014! Conditional Random Fields (CRF), Sentiment Analysis, DBpedia, Europarl, Crowd Sourcing or Open Data have also become very popular over time. The  X  X  X leu X  X  Machine Translation quality measure was presented by K. Papineni et al. in an ACL paper published in July 2002, but four LREC papers introduced it to the LREC community in May 2002 at LREC 2002 in Las Palmas (Spain), including the K. Papineni keynote paper itself.

From this analysis, we compute an innovation score for each author, illustrating his or her ability to introduce new terms that subsequently became popular, obtained as follows: for each term, we first compute the percentage of papers that contain the term at each conference ( X  X  X elative presence X  X  of the term). We only consider papers written by authors that are different from those who  X  X  X nvented X  X  the term, in order to avoid self citation, i.e. an excessive weight for the overuse of non-propagated terms, typically program or system names. Figure 29 gives the annual relative presence for the term  X  X  X ross validation X  X , which was encountered for the first time in 2 papers 22 at LREC 2000, when considering all papers or only those written by authors who are different from those who introduced the term ( X  X  X xternal papers X  X ).

The total innovation score of a term is the corresponding surface, taking into account the inventors X  papers in the year of introduction and the external papers in the subsequent years. The innovation score for  X  X  X ross validation X  X  (Fig. 30 ) is the sum of the yearly relative presences of the term and amounts to 0.17 (17 %). Some non-scientific terms may not have been filtered out, but their influence will be small as their presence is limited, while terms that became popular at some point in the past but lost popularity afterwards will still remain in consideration.
The innovation score for an author is the sum of the innovation scores of the terms he/she invented (Table 28 ). We see in this table that there is some overlap between the sets of terms introduced by Mark Liberman, Christopher Cieri and Stephanie Strassel, who have co-authored several papers as they are all from LDC. We find some languages that became recently more studied (Slovak, Urdu, Sanskrit, Levantine, Arabic). Some terms are surprising, such as the sequence  X  X  X oun noun X  X , the study of which is actually a challenging research topic. The number of terms introduced by each author may appear very large, but their innovation score may remain very low if they are sparsely mentioned over time. 2.7 Text reuse and plagiarism We studied the use across the conference series of parts of former papers written by the same authors (that we will call  X  X  X elf-reuse X  X , if the source paper is quoted and else  X  X  X elf-plagiarism X  X ) or by different authors (that we will call  X  X  X euse X  X  if the source paper is quoted and else  X  X  X lagiarism X  X ). For this, we considered an ngram of words (i.e. a window of words) and compared them with the ngrams obtained from papers published at all former conferences. We applied the deep parser TagParser to generate a robust morphological analysis, POS tagging, and named entity recognition and stored the result in an index. Each ngram is made up of a sequence of lemmas in the order they appear in the text. After several trials, we found that an ngram of seven words gave meaningful results, so we set seven as an empirical parameter. We examined papers with more than 4 % similarity as possible cases of reuse or plagiarism, and we checked whether the source was cited in the references (Francopoulo et al. 2016 ).
 Reuse of LREC papers in LREC papers We computed the similarity of each LREC paper with other LREC papers that were published earlier or in the same year. 23 We didn X  X  detect any case of possible plagiarism. Only 66 cases of self-reuse or self-plagiarism (less than 1.5 % of the LREC published papers) were detected, ranging from a similarity ratio of 4 % up to 46 %. Half of the papers (34) cite the source paper. In many cases, the similar parts are related to the presentation of a method, a program, a project, a problem or a resource shared by the two papers. In one case, the coverage is extensive and the difference is primarily in the name of the system being presented, while the description is almost the same! It appears that 20 % of those papers were published in the same LREC conference and that 68 % concern the reuse of a paper published at the previous conference. Only 12 % reuse material from a longer period prior, as shown in Table 29 .

Reuse of NLP4NLP conferences and journals papers in LREC papers We considered the reuse in LREC papers of material from papers that were published at conferences or journals in NLP4NLP, including LREC itself (Table 30 ). Only two cases of possible plagiarism were detected, but we found after checking manually that, in the two cases, both papers referred with the same wording to the content of a third previous paper which described the method they used and that they both properly acknowledged.

We found that 554 documents (about 12 % of the papers published at LREC) have been reused by their authors with or without citing the source paper. Only 37 % of the papers cite the source paper. The maximum degree of similarity is 89 %, corresponding to the description of the same research center in two different conferences. Twenty-one percent of the pairs (117) are from the ISCA conference series and 9 % from LREC itself (66), while the other main  X  X  X ompanion X  X  conferences are: COLING (11 %), ACL (11 %) and L&amp;TC (6 %), which is also a biennial conference held 6 months before LREC.

As it appears in Table 31 , 40 % of the document pairs (224 papers) involve papers published in the same year (similar papers that may have been simultane-ously submitted at different conferences, or additionally in a journal), 49 % fall within a window of the 2 previous years, while only 11 % span a longer timeframe. Reuse of LREC papers in NLP4NLP conferences and journals papers Conversely, we considered the reuse of LREC papers in papers that have were published in the NLP4NLP conferences or journals, including LREC itself (Table 32 ). Seven cases of possible plagiarism were detected, but here also it appeared that they correspond to the content of a third paper where a method, a corpus, a platform they used, an evaluation campaign or a project they participated in is described, and that they both properly acknowledge. In some cases, the authors are different but belong to the same laboratory.

We found that 445 documents (about 10 % of the LREC papers) have been reused by their authors with or without citing the source paper. Only 41 % of the papers cite the source paper. The maximum degree of similarity is 75 %. Fifteen percent of the pairs (66) are from LREC itself and 14 % (61) from the ISCA conference series, while the other main  X  X  X ompanion X  X  sources are: ACL (12 %), COLING (12 %) and, not surprisingly, the LRE Journal (11 %) as papers published at the LREC conference may be invited or may take the initiative to submit in the LRE journal. It is also interesting to notice a regular flow from LREC papers to other journals as well (Computational Linguistics, Computer Speech and Language) and increasingly to IEEE ICASSP. The flow towards the L&amp;TC conference is less important than in the other direction, given the chronological order of the two biennial conferences.

As it appears in Table 33 , 50 % of the pairs (the same 224 papers as above) include papers published in the same year, 35 % fall within a window of the 2 previous years, while only 15 % span a longer time scale.

We see in the previous results that self-reusing is much more important than reusing the content of others, with a comparable threshold of 0.04, when we consider the total of the two directions, there are 386 self-reuse and 613 self-plagiarism pairs, including the 224 duplicates, compared with 5 reuse and 9 plagiarism pairs. Within self-reuse and self-plagiarism, there are slightly more LREC papers borrowing (55 %) than being borrowed, and, globally, the source papers are quoted only in 39 % of the cases on average, a percentage which falls down from 49 to 25 % if the papers are published on the same year. 2.8 A composite hybrid measure of authors notability The study of social networks often uses the collaboration network centrality measures that we described above. As already mentioned, centrality measures reflect different roles of the authors and do not take into account several important criteria, in particular the productivity of the authors (number of published papers), their audience (number of citations), and their ability to introduce novelty in research (innovation score).
 We therefore propose (Table 34 ) as a measure of notability a Composite Hybrid Measure based on the arithmetic mean of the normalized ranking of authors according to those four criteria: Collaboration (see Table 11 ), Production (see Table 6 ), Citation (see Table 13 ) and Innovation (see Table 28 ).

This ranking is not intended to be a hit parade of the  X  X  X est X  X  LREC authors, but is rather intended to provide a picture of the LREC ecosystem and acknowledge the contributions of the members of its community, while stressing that those contributions may cover various aspects. Our next step is now to conduct a study of the whole NLP4NLP corpus, with a comparison across the various conferences and journals it contains over a 50-year time scale (1965 X 2015). We plan to investigate more deeply the structure of the corresponding research community through the graph of collaboration and the graph of citations among authors, as a social network. This process will help identifying factions of people who publish together or cite each other. We will also refine the study of the polarity of the citations, extend the bottom up term analysis already begun, and deepen the potential detection of weak signals and emerging trends. Establishing links among authors, citations and topics will allow us to study the changes in the topics of interest for authors or factions.

We will study the mutual influence of the conferences and journals, and their respective contribution in the advances of the research field, while identifying possible cultural differences among them. We plan to also consider the relationship among language resources, as registered in the LRE Map (Calzolari et al. 2012 ), and scientific papers. Researchers in other disciplines, e.g. biology (Bravo et al. 2015 ), face the same problems as in speech and NLP, such as identifying resources in a persistent and unique way, computing Resource Impact Factors, etc. Therefore different scientific communities could benefit from mutual experience and methodologies.

Finally, we plan to produce a RDF version of the corpus and make the results available over the web as Linked Open Data. The raw data that we gathered and the information we extracted after substantial cleaning could provide reference and test data for evaluation campaigns (such as automatic Name Extraction, or Multimedia Gender Detection). In this analysis, we benefited from the fact that all LREC proceedings data are freely available on-line. However, we faced some difficulty in the use of the available data. The information from LREC 1998 was not available in a text format and therefore had to be scanned, which introduced some errors. Additionally, we struggled with the lack of a consistent and uniform identification of entities (authors names, gender, affiliations, paper language, conference and journal titles, funding agencies, etc.). Establishing standards for such identification will demand an international effort in order to ensure that the identifiers are unique, which appears as a challenge for the scientific community.

Research in Natural Language Processing for spoken, written and signed languages has made major advances over the past 15 years through constant and steady scientific effort that was fostered thanks to the availability of a necessary infrastructure made up of publicly funded programs, largely available language resources, and regularly organized evaluation campaigns. Advancements in the field have also benefited substantially from the scientific social network connecting the community established by the LREC conference organized by the ELRA, which has provided a major venue in which researchers can share ideas on LRE.
 References
