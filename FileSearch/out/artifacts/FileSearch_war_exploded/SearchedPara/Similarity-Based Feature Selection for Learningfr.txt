 Feature selection is one of key problems in machine learning and data mining, which not only reduces training and infer ence time but also obtains better accu-racy of the predictor. In gen eral, feature selection including two key points: the proper evaluation criterion and the efficient search algorithm. According to the criterion, algorithms can be categorized into two classes: the filter model and the wrapper model [1,2]. In the wrapper mode l the selection method tries to directly optimize the performance of a specific predictor (algorithm). The main drawback of this method is its computational defici ency. In the filter model the selection is done as a preprocessing, without trying to optimize the performance of any specific predictor directly. This is usua lly achieved through an (ad-hoc) evalua-tion function using a search method in o rder to select a set that maximizes this function. Different methods apply a vari ety of search heuristics. For the search strategies, see [1,3] for a list of these te chniques. However, in this paper, we focus on learning from examples, and present the feature selection evaluation criterion and search strategy based on the model and theory of learning from examples. Finding the optimal set of features is intractable, and many problems related to feature selection have been shown to NP-hard [4]. As a resul t, we are forced to find heuristic methods that represent a compromise between solutions quality and time. The proposed method belongs to filter model. 2.1 Similarity Measure As we have known, if the similarity between classes with respect to a selected feature subset is low, then the distance b etween classes is large and the clas-sification performance of the selected feature subset is high, because we try to find the features that can separate the t wo classes as far as possible. There-fore, we first pay attention to the similarity measures between classes. Suppose the feature set is F =[ F 1 ,F 2 ,  X  X  X  ,F n ], then every example e is represented by continuous.
 Definition 1. Similarity between examples . According to the model of learning from examples, suppose the data set has two classes, we name them as Positive class P and Negative class N . Then the similarity between a positive example with respect to F is defined as: where sim ( v + i ,v  X  i ) denotes the similarity between a positive example and a neg-ative example with respect to feature F i , and one solution is defined as follows: max i and min i is the maximum and minimum values of i  X  X h feature respectively.
Noting that there exists many solutions to calculate the sim ( v + i ,v  X  i ), we can not guarantee that our selected similarity measure have the best performance for feature selection problem on specific data set. However, the following theoretic analysis will show our proposed feature selection method is not sensitive to the choice of similarity measure for sim ( v + i ,v  X  i ).
 Definition 2. Similarity between a positive example and N. with respect to F is defined as follows, which is the maximum value of similarity between a positive example and all negative examples.
 Definition 3. Similarly, the similarity between classes. can be defined as the maximum value of similarity between all positive examples and N .

For given examples e + , e  X  ,afeatureset F , and a similarity measure sim ,one can consider the similarity degree between e + and e  X  with respect to F ( e + , e  X  with respect to F are denoted by e + | F and e  X  | F ) is equal to 1 if and only if | F = e  X  | F , i.e., the similarity between e + and e  X  on every feature F i in F is equal to 1 and denoted by F i  X  F ( sim ( e + ,e  X  | F i )) = 1, and the similarity is consequent implies the overlapping degree between positive class P and negative class N (denoted by OV ( P, N | F )) is equal to 0 if and only if the intersection of P and N with respect to F is empty, i.e., ( P N | F )=  X  and OV ( P, N | F ) &gt; 0 if and only if ( P N | F ) =  X  where  X  denotes empty set, and OV ( P, N | F ) be considered as the similarity between P and N , and it shows whether the intersection of two classes is empty. It is clear that Sim ( e + ,e  X  | F ) taking value [0,1] with a larger value denoting a higher degree of similarity, and then min-max learning rule, i.e., = max and = min , is adopted [6]. So OV ( P, N | F ) is changed to Definition 3 above and regarded as maximal degree of similarity between two sets. 2.2 Evaluation Criterion The optimal feature selection generally attempts to select the minimally sized subset of features without sacrificing o r even get higher classification accuracy than the full feature set [7]. And just as pointed out in [3], generally, the objective function for feature selection consists of two terms that compete with each other: (1)goodness-of-fit (to be maximized), and (2) the number of features (to be minimized). Goodness-of-fit is generally denoted by classification performance and here it is inversely represented by the similarity between classes with respect to selected subset, i.e., the smaller similarity, the higher goodness-of-fit. Suppose the inter-class similarity between P and N with respect to feature set F is R = Sim ( P, N | F ). The optimal feature subset Fs should be satisfied:  X  Fs  X  F and Sim ( P, N | Fs )= min F  X  F Sim ( P, N | F )  X   X  ,  X   X  R , F  X  The number of features in Fs is minimum. 2.3 Search Strategy Extension matrix is an important theory for learning from examples and first developed by [8]. In the past, it was always used to deal with discrete feature values, such as in [5,9]. Here, we will extend it to handle continuous values and utilize it as the search strategy for feature selection.
 1 , 2 ,  X  X  X  ,l , l is the number of positive examples and v + pi denotes the value of feature F i ( i =1 , 2 ,  X  X  X  ,n )forthe p  X  X h positive example. Set negative class 1 , 2 ,  X  X  X  ,g , g is the number of negative examples and v  X  qi denotes the value of feature F i ( i =1 , 2 ,  X  X  X  ,n )forthe q  X  X h negative example. The extension matrix of P against N is defined as Eq.(5) Noting that each feature is corresponding to a column in extension matrix EM ( P, N ), and they have one-to-one relationship. In other words, feature F i is corresponding to the i  X  X h column in the extension matrix EM ( P, N ) 2.4 Algorithm Design Definition 4.  X  consistency , given a threshold  X  ,if Sim ( P, N | F )  X   X  ,then P and N is  X  consistent with respect to feature subset F ,and F is also called as a consistent feature subset with respect to P and N .
 Definition 5.  X  element , threshold  X  , if the element value in extension matrix is not more than  X  , then the element is called a  X  element.
 Definition 6. A path of an extension matrix refers to a connection of its  X  elements, which is obtained by selecting one  X  element from each row of the extension matrix EM ( P, N ).
 Lemma 1. Let F be a feature subset, P and N are  X  consistent with respect to F if and only if there exists at least one  X  element in the columns, which are corresponding to the features in F , of each row of extension matrix EM ( P, N ). Proof. According to Definition 4, if P and N are  X  consistent with respect to F ,then Sim ( P, N | F )  X   X  . According to Definition 2 and 3, the similarity between any positive example and any negative example is not more than  X  , i.e., Sim ( e + ,e  X  | F )  X   X  . From Definition 1, there e xists at least one feature F i  X  F such that sim ( v + i ,v  X  i )  X   X  , namely the element em i in the extension matrix EM ( e + ,e  X  ) is not more than  X  . Based on Definition 2 and 3, we know the searching involves all positive and negative examples, then all rows of extension matrix EM ( P, N ) are traversed according to the definition of extension matrix. So there exists at least one  X  element in the columns, which correspond to the features in F , of each row of EM ( P, N ).
 Conversely, if there exists a  X  element in each row of extension matrix EM ( P, N ), it implies that there exists at least a feature F i  X  F ( i  X  [1 ,n ]) such that sim ( v + i ,v  X  i )  X   X  for each row. According to Definition 1, the simi-larity between a positive example and a negative example with respect to F i is not more than  X  . All these features F i gotten from  X  element in each row are combined to obtain the feature subset F , which satisfies Sim ( P, N | F )  X   X  , then P and N is  X  consistent with respect to F . This completes the proof. Theorem 1. Let  X  be a given threshold and EM ( P, N ) be the extension matrix of P against N . Then finding an optimal feature subset is equivalent of searching for a path in the EM ( P, N ), which involves the minimum number of columns (features).
 Proof. According to evaluation criterion introduced in section 2.2 and Definition 4, one knows that Fs is an optimal feature subset if and only if: P and N is  X  consistent with respect to Fs , and The cardinality of Fs reaches a minimum.
According to Lemma 1, if th ere exists at least one  X  element in the columns, which are corresponding to feature in Fs , of each row of EM ( P, N ), then P and N is  X  consistent with respect to Fs . Therefore this path can be obtained by selecting one  X  element from each row of EM ( P, N ). Each column, which is involved in the process of selecting  X  element, corresponds to a feature. Hence, the number of involved columns is the number of selected features. Furthermore, if the path is made to involve minimum columns, then it satisfies the cardinality of feature subset reaching a minimum. This completes the proof.

According to Theorem 1, the optimal fe ature subset selection problem can be transformed into a search for a path that involves the minimum number of columns in EM ( P, N ). If we use the greedy algorithm, then it equals to select the columns with maximal number of  X  element.The propose d heuristic algorithm for feature selection is described in Algorithm 1. Let  X  be the threshold for the number of remaining  X  elements, F sup be the selected feature subset in one iteration, Fs be the final result, | P | denotes the number of native classes in Positive class P and F is the full feature set.
 Algorithm 1. Feature selection algorithm for learning from examples
Noting that, in the implementation of th e proposed heuristic feature selection algorithm for learning from examples, the extension matrix EM ( P, N )doesnot have to be really generated in memory. The time consumption exists in the number of  X  elements and removal rows need to be aggregated. Now, suppose the number of the classes is c , the number of training examples is S and the number of features is n , and if the number of examples in each native class is equal, then the total number of rows in extension matrix for all iterations is (( c  X  1) / 2 c ) S 2 , and the average time complexity of the algorithm is approximate  X  ( n (( c  X  1) / 2 c ) S 2 )  X   X  ( nS 2 ). In this section, we will compare the pr oposed method with other two classic feature selection methods on classification performance and dimensionality re-duction via benchmark data sets and facial image for gender classification. The selected methods are ReliefF [10,11] and the newly presented algorithm in [12] , we call it as Mitra X  X . These algorithms are well-known in machine learning.
Four bench-mark real-world data sets, DIAB, WDBC, Wine and Sonar, are downloaded from UCI Machine Learning Repository [13], and high-dimensional facial data sets for gender classification are used. The gallery sets used for train-ing include 100 male examples and 100 female examples, which have the same vector dimension of 1584 with the gabor filter. The probe sets used for testing include 11 kinds of facial data with various facial views and expressions, such as front, down 10 degrees, smiling, closed eyes, opened mouth, right 10 degrees, right 20 degrees, right 30 degrees, up 10 d egrees, up 20 degrees and up 30 degree. The number of testing samples in these probe sets are 1278, 820, 805, 815, 805, 814, 815, 805, 819, 816 and 816 respectively, and these probe sets are numbered as 1-11. Some examples are shown in Figure 1(a).

We will validate the performance of feature selection algorithm using K-NN classifiers with different K values and 5-fold cross-validation is used in the exper-iment. For gender classification, traditional Support Vector Machines (SVM)[14] is adopted, and the parameter C is set to 1. All the algorithms are implemented using Matlab compiler, the termination terms for the proposed algorithm is the number of remaining  X  elements less than the 1% and 1 / 100000 of original number of  X  elements over benchmark data sets and facial data sets respectively. The termination term is dependent on the characteristics of data set, which can be determined by cross validation.

The experimental results are shown in Table 1 for benchmark data sets, which are corresponding to K=1, 3 and 5 for K-NN classifier. For gender classification, the detailed accuracy rates on eleven p robe sets are displayed in Figure 1(b) and the dimensionality reduction rates for The proposed algorithm, Mitra X  X  and ReliefF are 96.65, 85.23 and 93.94, respectively. From the experimental results, the following four observations are obtained.
  X  The proposed evaluation criterion and search strategy is efficient for feature  X  The proposed algorithm always can get better trade-off between the classi- X  For gender classification, in most cases, the proposed method obtains highest  X  The proposed algorithm can obtain higher performance for different classi-The paper presents an algorithm to perform feature selection for learning from examples with continuous values, where the evaluation criterion is both the over-lapping (similarity) between classesand the cardinality of optimal feature subset are minimum. The proposed similarity measure is reasonable and has low time complexity. And extension matrix is extended to deal with continuous value and adopted as search strategy. The algorithm is proved by theory and it has been shown to get better trade-off between cla ssification accuracy an d dimensionality reduction for different classifiers.
 We gratefully thank OMRON Cooperation for supplying facial images. This research was partially supported by the Natural Science Fund for Colleges and Universities in JiangSu Province via the grants 08KJB520007, Scientific Research Foundation of Nanjing University of Posts and Telecommunications via the grants NY207137, National Natural S cience Foundation of Chin a via 40801149, JiangSu Province Bureau of Surveying and Mapping via JSCHKY200810.

