 University of Gothenburg University of Trento that demonstrate that relational features , mainly derived from dependency-syntactic and se-mantic role structures, can significantly improve the perfo rmance of automatic systems for a a sufficiently accurate and efficient approximation.
 ment in soft recall on the MPQA corpus over a conventional sequ ence labeler based on local opinion expressions lead to statistically significant impr ovements. 1. Introduction
Automatic methods for the analysis of opinions (textual expressions of emotions, be-liefs, and evaluations) have attracted considerable atten tion in the natural language processing community in recent years (Pang and Lee 2008). Ap art from their interest from a linguistic and psychological point of view, the techn ologies emerging from this research have obvious practical uses, either as stand-alon e applications or supporting other tools such as information retrieval or question answe ring systems.
 uments or passages expressing opinion, or classifying the p olarity of a given text, and these coarse-grained problem formulations naturally led t o the application of methods derived from standard retrieval or text categorization tec hniques. The models under-lying these approaches have used very simple feature repres entations such as purely lexical (Pang, Lee, and Vaithyanathan 2002; Yu and Hatzivass iloglou 2003) or low-level grammatical features such as part-of-speech tags and funct ional words (Wiebe, Bruce, and O X  X ara 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguist ic processing for tasks such as text categorization and search (Moschitti and Basili 200 4). There are a few exceptions, such as Karlgren et al. (2010), who showed that construction features added to a bag-of-words representation resulted in improved performance on a number of coarse-grained opinion analysis tasks. Similarly, Greene and Resnik (2009 ) argued that a speaker X  X  attitude can be predicted from syntactic features such as th e selection of a transitive or intransitive verb frame.
 problem formulations where the task is not only to find a piece of opinionated text, but also to extract a structured representation of the opini on. For instance, we may determine the person holding the opinion (the holder ) and towards which entity or fact of the opinion (the intensity ). The increasing complexity of representation leads us from retrieval and categorization deep into natural langua ge processing territory; the methods used here have been inspired by information extract ion and semantic role labeling, combinatorial optimization, and structured mac hine learning. For such tasks, deeper representations of linguistic structure have seen m ore use than in the coarse-grained case. Syntactic and shallow-semantic relations ha ve repeatedly proven useful for subtasks of opinion analysis that are relational in natu re, above all for determining the holder or topic of a given opinion, in which case there is c onsiderable similarity to tasks such as semantic role labeling (Kim and Hovy 2006; Ruppe nhofer, Somasundaran, and Wiebe 2008).
 structure in the relations between opinions expressed in text, despite the fact that the opinion expressions in a sentence are not independent but or ganized rhetorically to achieve a communicative effect intended by the speaker. We t herefore expect that the interplay between opinion expressions can be exploited to d erive information useful for propose several novel features derived from the interdepen dencies between opinion expressions on the syntactic and shallow-semantic levels.
 of opinion expressions, (2) joint expression extraction and holder extraction, and (3) joint expression extraction and polarity labeling. The models we re trained using a number of discriminative machine learning methods. Because the in terdependency features required us to consider more than one opinion expression at a time, the inference steps carried out at training and prediction time could not rely on commonly used opinion expression mark-up methods based on Viterbi search, but we s how that an approximate search method using reranking suffices for this purpose: In a first step a base system 474 using local features and efficient search generates a small s et of hypotheses, and in a second step a classifier using the complex features selects t he final output from the hypothesis set. This approach allows us to make use of arbitr ary features extracted from the complete set of opinion expressions in a sentence, witho ut having to impose any easy to implement as long as the underlying system is able to g enerate k -best output. from the MPQA corpus, and compared to strong baselines consi sting of stand-alone systems for opinion expression mark-up, opinion holder ext raction, and polarity classi-fication. Our evaluations showed that (1) the best opinion ex pression mark-up system we evaluated achieved a 10-point absolute improvement in so ft recall, and a 5-point im-provement in F-measure, over the baseline sequence labeler . Our system outper-formed previously described opinion expression mark-up to ols by six points in overlap
F-measure. (2) The recall was boosted by almost 10 points for the holder extraction task (over three points in F-measure) by modeling the interactio n of opinion expressions with respect to holders. (3) We saw an improvement for the ext raction of polarity-labeled expression of four F-measure points. Our result for opinion extraction and po-larity labeling is especially striking when compared with t he best previously published end-to-end system for this task: 10 X 15 points in F-measure i mprovement. In addition to the performance evaluations, we studied the impact of featu res on the subtasks, and the effect of the choice of the machine learning method for train ing the reranker. output in a number of applications. Although there have been several publications detailing the extraction of MPQA-style opinion expression s, as far as we are aware there has been no attempt to use them in an application. In contrast, we show that the opinion expressions as defined by the MPQA corpus may be used to derive machine learning features that are useful in two practical opinion mining tas ks; the addition of these features leads to statistically significant improvements i n all scenarios we evaluated.
First, we develop a system for the extraction of evaluations of product attributes from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etz ioni 2005; Titov and
McDonald 2008), and we show that the features derived from op inion expressions lead to significant improvement. Secondly, we show that fine-grai ned opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2004).
 overview of the related work; Section 3 describes the MPQA op inion corpus and its underlying representation; Section 4 illustrates the base line systems: a sequence labeler for the extraction of opinion expressions and classifiers fo r opinion holder extraction and polarity labeling; Section 5 reports on the main contrib ution: the description of the interaction models and their features; finally, Section 7 pr esents the experimental results and Section 8 derives the conclusions. 2. Motivation and Related Work
Intuitively, interdependency features could be useful in th e process of locating and disambiguating expressions of opinion. These expressions tend to occur in p atterns, and the presence of one opinionated piece of text may influence our interpretation of another as opinionated or not. Consider, for instance, the word said in sentences (a) and (b) in
Example (1), where the presence or non-presence of emotiona lly loaded words in the complement clause provides evidence for the interpretatio n as a subjective opinion or an objective speech event. (In the example, opinionated expre ssions are marked S for subjective and the non-opinionated speech event O for objecti ve.)
Example (1) (a)  X  X e will identify the [ culprits ] S of these clashes and [ punish ] (b) On Monday, 80 Libyan soldiers disembarked from an Antono v transport plane carrying military equipment, an African diplomat [ said ]
Moreover, opinions expressed in a sentence are interdepend ent when it comes to the resolution of their holders  X  X he person or entity having the attitude expressed in the opinion expression. Clearly, the structure of the sentence is also influential for this task because certain linguistic constructions provide evidenc e for opinion holder correlation.
In the most obvious case, shown in the two sentences in Example (2), pejorative words share the opinion holder with the communication and categor ization verbs dominating them. (Here, opinions are marked S and holders H.)
Example (2) (a) [ Domestic observers ] H [ tended to side with ] S the MDC, [ denouncing ] as [ fraud-tainted ] S and [ unfair ] S . (b) [ Bush ] H [ labeled ] S North Korea, Iran and Iraq an  X  X  axis of evil ]
In addition, interaction is important when determining opin ion polarity . Here, relations that influence polarity interpretation include coordinatio n, verb X  X omplement, as well as other types of discourse relations . In particular, the presence of a C relation, such as contrast or concession (Prasad et al. 2008 ), may allow us to infer that opinion expressions have different polarities. In Example ( 3), we see how contrastive discourse connectives (underlined) are used when there are contrasting polarities in the surrounding opinion expressions. (Positive opinions are t agged  X + X , negative  X - X .)
Example (3) (a)  X  X  This is no blind violence but rather targeted violence ]  X  However , the movement [ is more than that ] + . X  (b)  X  X  Trade alone will not save the world ] -, X  Neyts [ said ] [ important ] + factor for economic development.

The problems we focus on in this article X  X xtracting opinion e xpressions with holders and polarity labeling X  X ave naturally been studied previous ly, especially since the release of the MPQA corpus (Wiebe, Wilson, and Cardie 2005). For the first subtask, because the MPQA corpus uses span-based annotation to repre sent opinions, it is natural to apply straightforward sequence labeling techni ques to extract them. This idea has resulted in a number of publications (Choi, Breck, and Ca rdie 2006; Breck, Choi, and
Cardie 2007). Such systems do not use any features describin g the interaction between opinions, and it would not be possible to add interaction fea tures because a Viterbi-based sequence labeler by construction is restricted to usi ng local features in a small contextual window.
 ous (Bethard et al. 2005; Kobayashi, Inui, and Matsumoto 2007 ; Joshi and Penstein-Ros  X  e 476 2009; Wu et al. 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to deter mine holder and topic of opinions. Similarly, Choi, Breck, and Cardie (2006) succes sfully used a PropBank-based role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently ap-plied tree kernel learning methods on a combination of synta ctic and semantic role trees for extracting holders, but did not consider their relation s to the opinion expressions.
Ruppenhofer, Somasundaran, and Wiebe (2008) argued that se mantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. Choi, Br eck, and Cardie (2006) built a joint model of opinion expression extraction and hold er extraction and applied integer linear programming to carry out the optimization st ep.
 ion expressions (Wilson, Wiebe, and Hoffmann 2009) have most ly been studied in isolation, Choi and Cardie (2010) developed a sequence labe ler that simultaneously extracted opinion expressions and assigned them polarity v alues and this is so far the only published result on joint opinion segmentation and pola rity classification. Their experiment, however, lacked the obvious baseline: a standa rd pipeline consisting of an expression tagger followed by a polarity classifier. In addit ion, although their model is the first end-to-end system for opinion expression extrac tion and polarity classifica-tion, it is still based on sequence labeling and thus by const ruction limited in feature expressivity.

Mathieu 2009; Somasundaran et al. 2009; Zirn et al. 2011) app lying interaction features for polarity classification are arguably the most related be cause they are driven by a vision similar to ours: Individual opinion expressions int erplay in discourse and thus provide information about each other. On a practical le vel there are obvious differences, since our features are extracted from syntact ic and shallow-semantic linguistic representations, which we argue are reflections of discourse structure, while they extract features directly from a discourse representa tion. It is doubtful whether automatic discourse representation extraction in text is c urrently mature enough to provide informative features, whereas syntactic parsing a nd shallow-semantic analysis are today fairly reliable. Another related line of work is re presented by Choi and Cardie (2008), where interaction features based on compositional semantics were used in a joint model for the assignment of polarity values to pre-segmente d opinion expressions in a sentence. 3. The MPQA Corpus and its Annotation of Opinion Expressions
The most detailed linguistic resource useful for developin g automatic systems for opin-ion analysis is the MPQA corpus (Wiebe, Wilson, and Cardie 20 05). In this article, we use the word opinion in its broadest sense, equivalent to the word private state used by the
MPQA annotators (page 2):  X  X pinions, emotions, sentiments , speculations, evaluations, and other private states (Quirk et al. 1985), i.e., internal states that cannot be directly observed by others. X  (or subjective expression): A text piece that expresses a pri vate state, allowing us to draw the conclusion that someone has a particular emotion or belief about some topic.
Identifying these units allows us to carry out further analys is, such as the determination of opinion holder and the polarity of the opinion. The annota tion scheme defines two types of opinion expressions: direct subjective expressions (DSEs), which are explicit mentions of attitude or evaluation, and expressive subjective elements (ESEs), which signal the attitude of the speaker by the choice of words. The prototypical example of a DSE would be a verb of statement, feeling, or categorizatio n such as praise or disgust .
ESEs, on the other hand, are less easy to categorize syntacti cally; prototypical examples would include simple value-expressing adjectives such as beautiful and strongly charged words like appeasement or big government . In addition to DSEs and ESEs, the corpus also contains annotation for non-subjective statements, which a re referred to as objective speech events (OSEs). Some words such as say may appear as DSEs or OSEs depending on the context, so for an automatic system there is a need for d isambiguation. ESEs have been manually annotated.

Example (4) (a) He [ made such charges ] DSE [ despite the fact ] ESE cultural participation is [ not less than that ] ESE of men. (b) [ However ] ESE , it is becoming [ rather fashionable ] with each other [ like kids ] ESE . (c) For instance, he [ denounced ] DSE as a [ human rights violation ] seizure of satellite dishes in Iran. (d) This [ is viewed ] DSE as the [ main impediment ] ESE order in the country.

Every expression in the corpus is connected to an opinion holder , experiences the sentiment or utters the evaluation that app ears textually in the opinion expression. For DSEs, it is often fairly straightforward to find the opinion holders because they tend to be realized as direct semantic argument s filling semantic roles such as S PEAKER or E XPERIENCER  X  X nd the DSEs tend to be verbs or nominalizations.
For ESEs, the connection between the expression and the opin ion holder is typically less clear-cut than for DSEs; the holder is more frequently i mplicit or located outside the sentence for ESEs than for DSEs.
 particular mentions of a holder entity. Instead, the opinion s are connected to holder coreference chains that may span the whole text. Some opinion expressions are li nked text, it is called the writer , otherwise implicit . Example (5) shows sentences annotated with expressions and holders.

Example (5) (a) For instance, [ he ] H1 [ denounced ] DSE/H1 as a [ human rights violation ] banning and seizure of satellite dishes in Iran. (b) [ (writer) ] H1 : [ He ] H2 [ made such charges ] DSE/H2 women X  X  political, social, and cultural participation is [ not less than that ] (c) [ (implicit) ] H1 : This [ is viewed ] DSE/H1 as the [ main impediment ] lishment of political order in the country. 478
Finally, MPQA associates opinion expressions (DSEs and ESE s, but not OSEs) with a polarity feature taking the values P OSITIVE , N EGATIVE with an intensity feature taking the values L OW , M EDIUM two sentences in Example (6) from the MPQA corpus show opinio n expressions with polarities. Positive polarity is represented with a  X + X  and negative with a  X - X .
Example (6) (a) We foresaw electoral [ fraud ] -but not [ daylight robbery ] (b) Join in this [ wonderful ] + event and help Jameson Camp continue to provide the year-round support that gives kids a [ chance to create dreams ]
The corpus does not currently contain annotation of topics (evaluees) of opinions, although there have been efforts to add this separately (Sto yanov and Cardie 2008). 4. Baseline Systems for Fine-Grained Opinion Analysis
The assessment of our reranking-based systems requires us t o compare against strong baselines. We developed (1) a sequence labeler for opinion e xpression extraction similar to that by Breck, Choi, and Cardie (2007), (2) a set of classifi ers to determine the opinion holder, and (3) a multiclass classifier that assigns polarity to a given opinion expression similar to that described by Wilson, Wiebe, and Ho ffmann (2009). These tools were also used to generate the hypothesis sets for the r erankers described in
Section 5. 4.1 Sequence Labeler for Opinion Expression Mark-up
To extract opinion expressions, we implemented a standard s equence labeler for sub-jective expression mark-up similar to the approach by Breck, Choi, and Cardie (2007).
The sequence labeler extracted basic grammatical and lexic al features (word, lemma, and POS tag), as well as prior polarity and intensity feature s derived from the lexicon created by Wilson, Wiebe, and Hoffmann (2005), which we refer to as subjectivity clues . It is important to note that prior subjectivity does not alway s imply subjectivity in a particular context; this is why contextual features are essential for this task. The grammatical features and subjectivity clues were extracted in a window of size 3 around the word in focus. We encoded the opinionated expression bra ckets by means of the
IOB2 scheme (Tjong Kim Sang and Veenstra 1999). When using this r epresentation, we are unable to handle overlapping opinion expressions, but t hey are fortunately rare. by the sequence labeler. The ESE defenseless situation is encoded in IOB2 as two tags
B-ESE and I-ESE . There are four input columns (words, lemmas, POS tags, subje ctivity clues) and one output column (opinion expression tags in IOB2 encoding). The figure also shows the sliding window from which the feature extract ion function can extract features when predicting an output tag (at the arrow).
 and the online Passive X  X ggressive algorithm (Crammer et al . 2006) to estimate the model weights. The learning algorithm parameters were tune d on a development set.
When searching for the best value of the C parameter, we varied it along a log scale from
HRW has denounced the situation of these prisoners defenseless 0.001 to 100, and the best value was 0.1. We used the max-loss v ersion of the algorithm and ten iterations through the training set. 4.2 Classifiers for Opinion Holder Extraction
The problem of extracting opinion holders for a given opinio n expression is in many ways similar to argument detection in semantic role labelin g (Choi, Breck, and Cardie 2006; Ruppenhofer, Somasundaran, and Wiebe 2008). For inst ance, in the simplest case when the opinion expression is a verb of evaluation or ca tegorization, finding the holder would entail finding a semantic argument such as an E C
OMMUNICATOR . We therefore approached this problem using methods inspir ed by semantic role labeling: Given an opinion expression in a sen tence, we define binary classifiers that decide whether each noun phrase of the sente nce is its holder or not. Separate classifiers were trained to extract holders for DSE s, ESEs, and OSEs. example is given by the sentence in Figure 1. Some features ar e derived from the syntactic and shallow semantic analysis of the sentence, sh own in Figure 2 (Section 6.1 gives more details on this).
 S
YNTACTIC PATH . Similarly to the path feature widely used in semantic role l abeling S
HALLOW -SEMANTIC RELATION . If there is a direct shallow-semantic relation between E
XPRESSION HEAD WORD , POS, AND LEMMA . denounced , VBD , denounce for de-H D
OMINATING EXPRESSION TYPE . When locating the holder for the ESE defenseless 480 C E
XPRESSION V ERB V OICE . Similar to the common voice feature used in SRL. Takes E There are also differences compared with typical argument e xtraction in SRL, however.
First, as outlined in Section 3, it is important to note that t he MPQA corpus does not annotate direct links from opinions to holders, but from opi nions to holder coreference chains . To handle this issue, we used the following approach when tr aining the classi-fier: We created a positive training instance for each member of the coreference chain occurring in the same sentence as the opinion, and negative t raining instances for all other noun phrases in the sentence. We do not use coreference information at test time, in order for the system not to rely on automatic coreference r esolution.
 sentence, an opinion may be linked to an implicit holder; a special case of implicit holder is the writer of the text. To detect implicit and writer links, we trained t wo separate classifiers that did not use the features requiring a holder p hrase. We did not try to link opinion expressions to explicitly expressed holders outside the sentence; this is an interesting open problem with some connections to inter-sentential semantic role labeling, a problem whose study is in its infancy (Gerber and Chai 2010).

Guyon, and Vapnik 1992) using the L IBLINEAR software (Fan et al. 2008). To handle the restriction that every expression can have at most one ho lder, the classifier selects only the highest-scoring opinion holder candidate at test t ime. We tuned the learning parameters on a development set, and the best results were ob tained with an L2-regularized L2-loss SVM and a C value of 1. 4.3 Polarity Classifier Given an expression, we use a classifier to assign a polarity v alue: P or N EGATIVE . Following Choi and Cardie (2010), the polarity value B to N EUTRAL  X  X he expressions having this value were in any case very few. In the cases where the polarity value was empty or missing, we set the pola rity to N addition, the annotators of the MPQA corpus may use special u ncertainty labels in the case where the annotator was unsure of which polarity to assi gn, such as U POSITIVE . In these cases, we just removed the uncertainty label.
 sification has been studied in detail by Wilson, Wiebe, and Hof fmann (2009), who used a set of carefully devised linguistic features. Our classifi er is simpler and is based on fairly shallow features. Like the sequence labeler for opin ion expressions, this classifier made use of the lexicon of subjectivity clues.
 The examples come from the opinion expression defenseless situation in Figure 1. W ORDS IN EXPRESSION : defenseless , situation .
 S W W
To train the support vector classifiers, we again used L IBLINEAR eters. The three-class classification problem was binarize d using the one-versus-all method. 5. Fine-Grained Opinion Analysis with Interaction Feature s
Because there is a combinatorial number of ways to segment a s entence into opin-ion expressions, and label the opinion expressions with typ e labels (DSE, ESE, OSE) as well as polarities and opinion holders, the tractability of the opinion expression segmentation task will obviously depend on whether we impos e restrictions on the problem in a way that allows for efficient inference. Most pre vious work (Choi, Breck, and Cardie 2006; Breck, Choi, and Cardie 2007; Choi and Cardi e 2010) used Markov factorizations and could thus apply standard sequence labe ling techniques where the arg max step was carried out using the Viterbi algorithm (as d escribed in Section 4.1).
As we argued in the introduction, however, it makes linguist ic sense that opinion expression segmentation and other tasks could be improved i f relations between possible expressions were considered; these relations can be syntac tic or semantic in nature, for instance.
 break down and the problem of finding the best analysis to beco me computationally intractable. We thus have to turn to approximate inference m ethods based on reranking, which can be trained efficiently. 5.1 Formalization of the Model
We formulate the problem of extracting the opinion structur e (the set of opinion expres-sions, and possibly also their holders or polarities) from a given sentence as a structured prediction problem where w is a weight vector and  X  ( x , y ) a feature extraction function representing a sen-tence x and an opinion structure y as a high-dimensional feature vector. We now further decompose the feature representation  X  into a local part  X  where  X  L is a standard first-order Markov factorization, and  X  local interactions between pairs of opinion expressions: 482
The feature function  X  p represents a pair of opinion expressions e interaction in the sentence x , such as the syntactic and semantic relations connecting them. 5.2 Approximate Inference with Interaction Features
It is easy to see that the inference step arg max y w  X  ( x , y ) is NP-hard for unrestricted pairwise interaction feature representations  X  : This class of models includes simpler ones such as loopy Markov random fields, where inference is kn own to be NP-hard and require the use of approximate approaches such as belief pro pagation. Although it is possible that search algorithms for approximate inference in our model could be devised along similar lines, we sidestepped this issue by using a reranking decomposition of the problem:
The advantages of a reranking approach compared with more co mplex approaches re-quiring advanced search techniques are mainly simplicity a nd efficiency: This approach is conceptually simple and fairly easy to implement provide d that k -best output can be generated efficiently, which is certainly true for the Vit erbi-based sequence labeler described in Section 4.1. The features can then be arbitrari ly complex because we do not have to think about how the problem structure affects the algorithmic complexity of the inference step. Reranking has been used in a wide range of applications, starting in speech recognition (Schwartz and Austin 1991) and very comm only in syntactic parsing of natural language (Collins 2000).
 ity values and opinion holders of the opinion expressions en ter the picture. In that case, we not only need hypotheses generated by a sequence labeler, but also the outputs of a secondary classifier: the holder extractor (Section 4.2 ) or the polarity classifier (Section 4.3). The hypothesis set generation thus proceeds as follows: The hypothesis set size is thus at most k 1 k 2 .
 and the sentence The appeasement emboldened the terrorists . We first apply the opinion expression extractor to generate a set of k 1 possible segmentations of the sentence:
In the second step, we add polarity values, up to k 2 labelings for every segmentation candidate: 5.3 Training the Rerankers
In addition to the approximate inference method to carry out t he maximization of Equa-tion (1), we still need a machine learning method to assign we ights to the vector w by estimating on a training set. We investigated a number of mac hine learning approaches to train the rerankers. 5.3.1 Structured SVM Learning. We first applied the method of large-margin estimation for structured output spaces, also known as structured support vector machines . In this method, we use quadratic optimization to find the smalle st weight vector w that satisfies the constraint that the difference in output score between the correct output y on the degree of error in the output  X  y with respect to the gold standard y . This is a generalization of the well-known support vector machine fr om binary classification to prediction of structured objects.
 input x i is Y i , we state the learning problem as a constrained quadratic op timization program:
Because real-world data tend to be noisy, this optimization problem is usually also regularized to reduce overfitting, which leads to the introd uction of a parameter C as in regular support vector machines (see Taskar, Guestrin, and Koller [2004] inter alia for details).
 constraints makes a direct solution of the optimization pro gram intractable for most realistic types of problems. Instead, an approximation has t o be used in practice, and we used the SVM struct software package (Tsochantaridis et al. 2005; Joachims, Fi nley, and Yu 2009), which finds a solution to the quadratic program b y successively finding its most violated constraints and adding them to a working se t. We used the default values for the learning parameters, except for the paramete r C , which was determined by optimizing on a development set. This resulted in a C value of 500.

Section 7.1. When training rerankers for the complex extract ion tasks (expressions + holders or expressions + polarities), we used a weighted com bination of F-measures for the primary task (expressions) and the secondary task (h olders or polarities, see
Sections 7.1.1 and 7.1.2, respectively). 484 5.3.2 On-line Learning. In addition to the structured SVM learning method, we trained models using two variants of on-line learning. Such learnin g methods are a feasible alternative for performance reasons. We investigated two o n-line learning algorithms: the popular structured perceptron (Collins 2002) and the Passive X  X ggressive (PA) algorithm (Crammer et al. 2006). To increase robustness, we used an averaged imple-mentation (Freund and Schapire 1999) of both algorithms.
 mented in each step. In the perceptron, for a given input x , we compute an update to the current weight vector by computing the difference betwe en the correct output y and the predicted output  X  y . Pseudocode is given in Algorithm 1.

Algorithm 1 The structured perceptron algorithm. function P ERCEPTRON ( T , N ) input Training set T = { ( x i , y i ) } T i = 1 w 0  X  (0, . . . , 0) repeat N times margin learning similar to the structured SVM. Here we instea d base the update step on the  X  y that violates the margin constraints maximally, also takin g the loss function  X  into account. The update step length  X  is computed based on the margin; this update is bounded by a regularization constant C , which we set to 0.005 after tuning on a development set. The number N of iterations through the training set was 8 for both on-line algorithms.

Algorithm 2 The on-line passive X  X ggressive algorithm. function P ASSIVE  X  AGGRESSIVE ( T , N , C ) input Training set T = { ( x i , y i ) } T i = 1 w 0  X  (0, . . . , 0) repeat N times 6. Overview of the Interaction Features
The feature extraction function  X  NL extracts three groups of interaction features: (1) features considering the opinion expressions only; (2) fea tures considering opinion holders; and (3) features considering polarity values.
 the scores output by the base models (opinion expression seq uence labeler and sec-ondary classifiers); they did not directly use the local feat ures  X  scores over the k candidates so that their exponentials summed to 1. 6.1 Syntactic and Shallow Semantic Analysis
The features used by the rerankers, as well as the opinion hol der extractor in Section 4.2, are to a large extent derived from syntactic and semantic rol e structures. To extract them, we used the syntactic X  X emantic parser by Johansson an d Nugues (2008), which annotates the sentences with dependency syntax (Mel X   X  cuk 1988) and shallow semantics in the PropBank (Palmer, Gildea, and Kingsbury 2005) and Nom Bank (Meyers et al. 2004) frameworks, using the format of the CoNLL-2008 Shared Task (Surdeanu et al. 2008). The system includes a sense disambiguator that assig ns PropBank or NomBank senses to the predicate words.
 denounced the defenseless situation of these prisoners , where denounced is a DSE and de-fenseless situation is an ESE, has been annotated with dependency syntax (above t he text) and semantic role structure (below the text). The pred icate denounced , which is an instance of the PropBank frame denounce.01 , has two semantic arguments: the Speaker (A0, or Agent in VerbNet terminology) and the Subject (A1, or T heme), which are realized on the surface-syntactic level as a subject and a dir ect object, respectively. Sim-ilarly, situation has the NomBank frame situation.01 and an E argument (A0). 6.2 Opinion Expression Interaction Features
The rerankers use two types of structural features: syntact ic features extracted from the dependency tree, and semantic features extracted from t he predicate X  X rgument (semantic role) graph.
 a minor complication for multiword opinion expressions; we select the shortest possible path in such cases. For instance, in the sentence in Figure 2, the path will be computed between denounced and situation .
 486 S
YNTACTIC P ATH . Given a pair of opinion expressions, we use a feature repres enting L
EXICALIZED P ATH . Same as above, but with lexical information attached: DSE/ D
OMINANCE . In addition to the features based on syntactic paths, we crea ted a The features based on semantic roles were the following: P
REDICATE S ENSE L ABEL . For every predicate found inside an opinion expression, we P
REDICATE AND A RGUMENT L ABEL . For every argument of a predicate inside an C
ONNECTING A RGUMENT L ABEL . When a predicate inside some opinion expression 6.3 Opinion Holder Interaction Features
In addition, we modeled the interaction between different op inions with respect to their holders. We used the following two features to represent thi s interaction: S
HARED H OLDERS . A feature representing whether or not two opinion expressi ons H
OLDER TYPES + P ATH . A feature representing the types of the holders, combined w ith 6.4 Polarity Interaction Features
The model used the following features that take the polariti es of the expressions into account. These features are extracted from DSEs and ESEs onl y, because the OSEs have no polarity values. The examples of extracted features are g iven with respect to the two opinion expressions ( denounced and defenseless situation ) in Figure 2, both of which have a negative polarity value.
 P
OLARITY PAIR . For every pair of opinion expressions in the sentence, we cr eate a P P P P
OLARITY PAIR AND EXPRESSION TYPES . Adds the expression types (ESE or DSE) to P P P 7. Experiments
We trained and evaluated the rerankers on version 2.0 of the M PQA corpus, contains 692 documents. We discarded one document whose ann otation was garbled and we split the remaining 691 into a training set (541 docume nts) and a test set (150 documents). We also set aside a development set of 90 documen ts from the training set that we used when developing features and tuning learning al gorithm parameters; all experiments described in this article, however, used model s that were trained on the full training set. Table 1 shows some statistics about the traini ng and test sets: the number of documents and sentences; the number of DSEs, ESEs, and OSE s; and the number of expressions marked with the various polarity labels.
 (2) joint opinion expression and holder extraction; and (3) jo int opinion expression and polarity classification. Finally, the polarity-based opin ion extraction system was used in an extrinsic evaluation: document polarity classification of movie reviews. procedure: We split the training set into five pieces, traine d a sequence labeler and secondary classifiers on pieces 1 X 4, applied them to piece 5, and so on. 488 7.1 Evaluation Metrics
Because expression boundaries are hard to define rigorously (Wiebe, Wilson, and Cardie 2005), our evaluations mainly used intersection-based precision and recall measures to score the quality of the system output. The idea is to assign v alues between 0 and 1, as opposed to traditional precision and recall where a span is c ounted as either correctly or incorrectly detected. We thus define the span coverage c of a span s (a set of token indices) with respect to another span s  X  , which measures how well s token indices that two spans have in common. Because our eval uation takes span labels are different.
 respect to a set S  X  S  X  S  X  :
We now define the intersection-based precision P and recall R of a proposed set of spans  X 
S  X 
S  X  S with respect to a gold standard set S S S as follows: Note that in this formula, | S | means the number of spans in a set S .
 tion task, a predicted entity is counted as correct if it exac tly matches the boundaries of a corresponding entity in the gold standard; there is thus no reward for close matches.
Because the boundaries of the spans annotated in the MPQA cor pus are not strictly defined in the annotation guidelines (Wiebe, Wilson, and Car die 2005), however, mea-suring precision and recall using exact boundary scoring wi ll result in figures that are too low to be indicative of the usefulness of the system. Ther efore, most work using this corpus instead use overlap-based precision and recall measures, where a span is counted as correctly detected if it overlaps with a span in the gold standard (Choi,
Breck, and Cardie 2006; Breck, Choi, and Cardie 2007). As poi nted out by Breck, Choi, and Cardie (2007), this is problematic because it will tend t o reward long spans X  X or instance, a span covering the whole sentence will always be c ounted as correct if the gold standard contains any span for that sentence. Converse ly, the overlap metric does not give higher credit to a span that is perfectly detected th an to one that has a very low overlap with the gold standard.
 based measures: If the system proposes a span covering the who le sentence, the span coverage will be low and result in a low soft precision, and a l ow soft recall will be assigned if only a small part of a gold standard span is covere d. Note that our measures are bounded below by the exact measures and above by the overl ap-based measures. 7.1.1 Opinion Holders. To score the extraction of opinion holders, we started from t he same basic idea: Assign a score based on intersection. The ev aluation of this task is more complex, however, because (1) we only want to give cre dit for holders for correctly extracted opinion expressions; (2) the gold stan dard links opinion expressions to coreference chains rather than individual mentions of ho lders; and (3) the holder entity may be the writer or implicit (see Section 4.2).
 expression e and its holder h , we first located the expression e of e and e  X  . To assign a score to the proposed holder entity, we then sele cted the most closely corresponding gold standard holder entity h  X  in the coreference chain H to e c ( h , h ) and c ( h , h  X  ). We stress again that the gold standard coreference chains were used for evaluation purposes only, and that our system did not mak e use of them at test time. it as perfectly detected (coverage 1) if the coreference cha in H annotated in the gold standard contains the writer, and a full error (coverage 0) o therwise, and similar if h is implicit. 7.1.2 Polarity. In our experiments involving opinion expressions with polar ities, we report precision and recall values for polarity-labeled op inion expression segmentation:
In order to be assigned an intersection score above zero, a seg ment must be labeled with the correct polarity. In the gold standard, all polarity labe ls were changed as described in Section 4.3. In these evaluations, OSEs were ignored and DS Es and ESEs were not distinguished. 7.2 Experiments in Opinion Expression Extraction
The first task we considered was the extraction of opinion exp ression (labeled with expression types). We first studied the impact of the machine learning method and hypothesis set size on the reranker performance. Then, we ca rried out an analysis of the effectiveness of the features used by the reranker. We finall y compared the performance of the expression extraction system with previous work (Bre ck, Choi, and Cardie 2007). 490 7.2.1 Evaluation of Machine Learning Methods. We compared the machine learning meth-ods described in Section 5. In these experiments, we used a hyp othesis set size k of 8. All features from Section 6.2 were used. Table 2 shows the result s of the evaluations using the precision and recall measures described earlier. 3 The baseline is the result of taking the top-scoring labeling from the base sequence labeler.
 algorithm X  X utperform the perceptron soundly, which shows t he benefit of learning methods that make use of the cost function  X  . Comparing the two best-performing learning methods, we note that the reranker using the struct ured SVM is more recall-oriented whereas the PA-based reranker more precision-ori ented; the difference in trained using the PA learning algorithm (with the same param eters) because its training process is much faster than that of the structured SVM. 7.2.2 Candidate Set Size. In any method based on reranking, it is important to study the influence of the hypothesis set size on the quality of the reran ked output. In addition, an interesting question is what the upper bound on reranker p erformance is X  X he oracle performance. Table 3 shows the result of an experiment that i nvestigates these questions. potential improvement X  X he reduction of the F-measure error ranges between 10% and 15% of the oracle error reduction for all hypothesis set size s.
 does not seem to have an adverse effect on the precision, howe ver, until the candidate set size goes above eight X  X n fact, the precision actually imp roves over the baseline for small candidate set sizes. After the size goes above eight, t he recall (and the F-measure) a k value of 64, which we thought gave a good balance between proc essing time and performance. 7.2.3 Feature Analysis. We studied the impact of syntactic and semantic structural f ea-tures on the performance of the reranker. Table 4 shows the re sult of an investigation of the contribution of the syntactic features. Using all the syntactic features (and no semantic features) gives an F-measure roughly four points a bove the baseline. We then carried out an ablation test and measured the F-measure obta ined when each one of the three syntactic features has been removed. It is clear tha t the unlexicalized syntactic path is the most important syntactic feature; this feature c auses a two-point drop in
F-measure when removed, which is clearly statistically sig nificant (p &lt; 0 . 0001). The effect of the two lexicalized features is smaller, with only D nificant (p &lt; 0 . 05) drop when removed.
 moving the connecting label feature, which is unlexicalize d, has a greater effect than removing the other two semantic features, which are lexical ized. Only the connecting label causes a statistically significant drop when removed ( p &lt; 0 . 0001). labels with a tree fragment, it is interesting to study wheth er the expression labels alone would be enough. If this were the case, we could conclude that t he improvement is 492 caused not by the structural features, but just by learning wh ich combinations of labels are common in the training set, such as that DSE+ESE would be m ore common than
OSE+ESE. We thus carried out an experiment comparing a reran ker using label pair features against rerankers based on syntactic features onl y, semantic features only, and the full feature set. Table 6 shows the results. We see that th e reranker using label pairs indeed achieves a performance well above the baseline. Its pe rformance is below that of any reranker using structural features, however. In additio n, we see no improvement when adding label pair features to the structural feature se t; this is to be expected because the label pair information is subsumed by the struct ural features. 7.2.4 Analysis of the Performance Depending on Expression T ype. In order to better under-stand the performance details of the expression extraction , we analyzed how well it extracted the three different classes of expressions. Tabl e 7 shows the results of this evaluation. The DSE row in the table thus shows the results of the performance on DSEs, without taking ESEs or OSEs into account.
 the performance for a number of combined classes that we thin k may be interesting for applications: DSE &amp; ESE, finding all opinionated express ions and ignoring objective speech events; DSE &amp; OSE, finding opinionated and non-opinio nated speech and cat-egorization events and ignoring expressive elements; and u nlabeled evaluation of all types of MPQA expressions. The same extraction system was us ed in all experiments, and it was not retrained to maximize the different measures o f performance. into the details, we see that the reranker gives very large im provements for DSEs and OSEs, but a smaller improvement for the combined DSE &amp; OSE cla ss. This shows that one of the most clear benefits of the complex features is to hel p disambiguate these expressions. This also affects the performance for general opinionated expressions (DSE &amp; ESE). 7.2.5 Comparison with Breck, Choi, and Cardie (2007). Comparison of systems in opinion expression detection is often nontrivial because evaluati on settings have differed widely. Since our problem setting X  X arking up and labeling op inion expressions in the
MPQA corpus X  X s most similar to that described by Breck, Choi, and Cardie (2007), we carried out an evaluation using the setting from their exper iment.
 ones described in the previous sections in the following way s:
Again, our reranker uses the PA learning method with the full feature set (Section 6.2) and a hypothesis set size k of 64. Table 8 shows the performance of our baseline (Section 4.1) and reranked system, along with the best results report ed by Breck, Choi, and Cardie (2007).
 recall X  X han all results reported by Breck, Choi, and Cardie ( 2007). Note that our system was optimized for the intersection metric rather than the ov erlap metric and that we did not retrain it for this evaluation. 7.3 Opinion Holder Extraction
Table 9 shows the performance of our holder extraction syste ms, evaluated using the scoring method described in Section 7.1.1. We compared t he performance of the reranker using opinion holder interaction features (Secti on 6.3) to two baselines: The first of them consisted of the opinion expression sequence la beler (ES, Section 4.1) and the holder extraction classifier (HC, Section 4.2), without m odeling any interactions between opinions. The second and more challenging baseline was implemented by 494 adding the opinion expression reranker (ER) without holder interaction features to the pipeline. This results in a large performance boost simp ly as a consequence of improved expression detection, because a correct expressi on is required to get credit for a holder. However, both baselines are outperformed by the reranker using holder interaction features, which we refer to as the expression/h older reranker (EHR); the (p &lt; 0 . 0001).
 features; we see in Table 10 that both of them contribute to im proving the recall, and the effect on the precision is negligible. The statistical sign ificance for the recall improve-ment is highest for S HARED H OLDERS (p &lt; 0 . 0001) and lower for H P ATH (p &lt; 0 . 02).
 lation of the opinion holder extraction problem is differen t from those used in previous publications. Choi, Breck, and Cardie (2006) used the holde rs of a simplified set of opinion expressions, whereas Wiegand and Klakow (2010) ext racted every entity tagged as  X  X ource X  in MPQA regardless of whether it was connected to any opinion expression. Neither of them extracted implicit or writer holders.
 opinion expression type (DSE, OSE, and ESE), and whether the holder is internally or externally located; that is, whether or not the holder is tex tually realized in the same sentence as the opinion expression. In addition, Table 12 sho ws the performance for the two types of externally located holders.
 system and the reranker is that the recall and F-measure are i mproved; this is the case in every single evaluation. As previously, a large share of t he improvement is explained simply by improved expression detection, which can be seen b y comparing the reranked system to the strong baseline (ES+ER+HC). For the most import ant situations, however, we see improvement when using the reranker with holder inter action features. In those cases it outperforms the strong baseline significantl y: DSE internal: p &lt; 0 . 001,
The only common case where the improvement is not statistica lly significant is OSE internal.
 for the ESEs. Extracting the opinion holder for ESEs is often complex because the ex-pression and the holder are typically not directly connecte d on the syntactic or shallow-semantic level, as opposed to the typical situation for DSEs . When we use the reranker, however, the interaction features may help us make use of the holders of other opinion expressions in the same sentence; for instance, the interac tion features make it easier cases such as  X  I [ thought ] DSE the film was [ awful ] ESE connected to a DSE.
 496 7.4 Polarity Classification
To evaluate the effect of the polarity-based reranker, we ca rried out experiments to compare it with two baseline systems similarly to the evalua tions of holder extraction performance. Table 13 shows the precision, recall, and F-me asures. The evaluation used the polarity-based intersection metric (Section 7.1.2). T he first baseline consisted of an expression segmenter and a polarity classifier (ES+PC), and the second also included an expression reranker (ER). The reranker using polarity inte raction features is referred to as the expression/polarity reranker (EPR).
 which is in line with our previous results that also mainly im proved the recall. The precision shows a slight decrease from the ES+PC baseline bu t much lower than the recall improvement. The differences between the polarity r eranker and the strongest baseline are all statistically significant (precision p &lt; 0 . 02, recall and F-measure p &lt; 0 . 005).
 figures are shown in Table 14. We see that the differences in pe rformance when adding the polarity reranker are concentrated to the more frequent polarity values (N the figures are shown in Table 15. The table shows our baseline and integrated systems along with the figures 5 from Choi and Cardie. Instead of a single value for all polarit ies, we show the performance for every individual polarity value (P N
EGATIVE ). This evaluation uses the overlap metric instead of the int ersection-based one. As we have pointed out, we use the overlap metric for comp atibility although it is problematic.
 precision bias despite being optimized with respect to the r ecall-promoting overlap met-ric. In recall and F-measure, their system is significantly ou tperformed for all polarity values by our baseline consisting of a pipeline of opinion ex pression extraction and polarity classifier. In addition, our joint model clearly outp erforms the pipeline. The the Choi and Cardie system could be caused just by rebalancing precision and recall, we additionally trained a precision-biased reranker EPR p by changing the loss function  X  (see Section 5.3) from 1  X  F i to 1  X  1 3 F i  X  2 3 P o , where F
P the overlap precision. When we use this reranker, we achieve a lmost the same levels of precision as reported by Choi and Cardie, even outperform ing their precision for the 498 N
EUTRAL polarity value, while the recall values are still massively higher. The precision bias causes slight drops in F-measure for the P OSITIVE and N 7.5 First Extrinsic Evaluation: Extraction of Evaluations of Product Attributes
As an extrinsic evaluation of the opinion expression extrac tion system, we evaluated the impact of the expressions on a practical application: ex traction of evaluations of attributes from product reviews. We first describe the colle ction we used and then the implementation of the extractor.
 in extraction of attribute evaluations from product review s. The collection contains reviews of five products: one DVD player, two cameras, one MP3 player, and one cellular phone. In this data set, every sentence is associate d with a set of attribute eval-uations. An evaluation consists of an attribute name and an e valuation value between  X  3 and +3, where  X  3 means a strongly negative evaluation and +3 strongly posit ive. attribute evaluations size +2 , weight +2 , navigational system +2 , sound +2 . In this work, we do not make use of the exact value of the evaluation bu t only its sign. We removed the product attribute mentions in the form of anapho ric pronouns referring to entities mentioned in previous sentences; these cases ar e directly marked in the data set. 7.5.1 Implementation. We considered two problems: (1) extraction of attribute eva luations without taking the polarity into account, and (2) extractio n with polarity (positive or negative). The former is modeled as a binary classifier that t ags each word in the review (except the punctuation) as an evaluation or not, and the lat ter requires the definition of using simple features, a stronger baseline using a lexicon, and finally a system using features derived from opinion expressions.
 sifiers as SVMs that we trained using L IBLINEAR . For the extraction task without polarities, the best results were obtained using an L2-regu larized L2-loss SVM and a
C value of 0.1. For the polarity task, we used a multiclass SVM ( Crammer and Singer 2001) with the same parameters. To handle the precision/rec all tradeoff, we varied the class weighting for the null class.
 and lemma) in a window of size 3 around the word under consider ation (the focus word ). In addition, it had two features representing the overall s entence polarities. To compute the polarities, we trained bag-of-words classifier s following the implemen-tation by Pang, Lee, and Vaithyanathan (2002). Two separate classifiers were used: one for positive and one for negative polarity. Note that the se classifiers detect the presence of positive or negative polarity, which may thus occur in the same sentence. The classifiers were trained on the MPQA corpus, where we counted a sentence as positive if it contained a positive opinion expression with an intens ity of at least M conversely for the negative polarity. 7.5.2 Features Using a Sentiment Lexicon. Many previous implementations for several opinion-related tasks make use of sentiment lexicons , so the stronger baseline system used features based on the subjectivity lexicon by Wilson, Wi ebe, and Hoffmann (2005), which we previously used for opinion expression segmentati on in Section 4.1 and for polarity classification in Section 4.3. We created a classifi er using a number of features based on this lexicon.

In the following examples, we use the sentence The software itself was not so easy to use , presented in Figure 3. In this sentence, consider the focus wo rd software . One word is listed in the lexicon as associated with positive sentiment : easy . The system then extracts the following features: S
ENTIMENT LEXICON POLARITIES . For every word in the sentence that is listed in C S
YNTACTIC PATHS TO SENTIMENT WORDS . For every sentiment word in the sentence, S
EMANTIC LINKS TO SENTIMENT WORDS . When there is a direct semantic role link 7.5.3 Extended Feature Set Based on MPQA Opinion Expressions . We finally created an extended feature set incorporating the following features derived from MPQA-style opinion expressions, which we extracted automatically. Th e features are similar in construction to those extracted by means of the sentiment le xicon. The following list describes the new features exemplified with the same sentenc e above, which contains a negative opinion expression not so easy . 500 O
PINION EXPRESSION POLARITIES . For every opinion expression extracted by the C S S 7.5.4 Results. We evaluated the performance of the product attribute evalu ation extrac-tion using a 10-fold cross-validation procedure on the whol e data set. We evaluated three classifiers: a baseline that did not use the lexicon or t he opinion expressions, a classifier that adds the lexicon-based features, and finally the classifier that adds the
MPQA opinion expressions. The F-measures are shown in Table 16 for the extraction task, and Figure 4 shows the precision/recall plots. There a re clear improvements when adding the lexicon features, but the highest performing sys tem is the one that also used the opinion expression features. The difference betwe en the two top-performing consider the polarities, the difference is even greater: al most three F-measure points. 7.6 Second Extrinsic Evaluation: Document Polarity Classi fication Experiment
In a second extrinsic evaluation of the opinion expression ex tractor, we investigated how expression-based features affect the performance of a d ocument-level polarity classifier of reviews as positive or negative. We followed th e same evaluation protocol as in the first extrinsic evaluation, where we compare three c lassifiers of increasing complexity: (1) a baseline using a pure word-based represen tation, (2) a stronger base-line adding features derived from a sentiment lexicon, and ( 3) a classifier with features extracted from opinion expressions.
 as a document categorization task, and this has led to the app lication of standard text categorization techniques (Pang, Lee, and Vaithyanat han 2002). We followed this approach and implemented the document polarity classifier a s a binary linear SVM; this learning method has a long tradition of successful appl ication in text categorization (Joachims 2002).
 reviews written in English extracted from the Web by Pang and Lee (2004). set is an extension of a smaller set (Pang, Lee, and Vaithyana than 2002) that has been used in a large number of experiments. The remaining five sets consisted of product reviews gathered by Blitzer, Dredze, and Pereira (2007). 8 subsets: reviews of DVDs, software, books, music, and camer as. In all six collections, 1,000 documents were labeled by humans as positive and 1,000 as negative.
 feature vectors based on presence features for individual w ords. No weighting such as
IDF was used. The vectors were normalized to unit length. Agai n, we trained the SVMs using L IBLINEAR , and the best results were obtained using an L2-regularized L2-loss version of the SVM with a C value of 1. 7.6.1 Features Based on the Subjectivity Lexicon. We used features based on the subjectivity lexicon by Wilson, Wiebe, and Hoffmann (2005) that we used for opinion expression segmentation in Section 4.1 and for polarity classification in Section 4.3. For every word whose lemma is listed in the lexicon, we added a feature consi sting of the word and its prior polarity and intensity to the bag-of-words feature ve ctor.
 situation of these prisoners , where denounce is listed in the lexicon as strong/negative and prisoner as weak/negative .
 L L L
EXICON POLARITY AND WORD . denounced/negative , prisoners/negative . 502 7.6.2 Features Extracted from Opinion Expressions. Finally, we created a feature set based on the opinion expressions with polarities. We give example s from the same sentence; here, denounced is a negative DSE and defenseless situation is a negative ESE. E E
XPRESSION POLARITY AND WORD . negative/denounced , negative/defenseless , E
XPRESSION TYPE AND WORD . DSE/denounced , ESE/defenseless , ESE/situation . 7.6.3 Evaluation Results. To evaluate the performance of the document polarity classi fiers, we carried out a 10-fold cross-validation procedure for eve ry review collection. We evaluated three classifiers: one using only bag-of-words fe atures ( X  X aseline X ); one using features extracted from the subjectivity lexicon ( X  X exicon  X ); and finally one also using the expression-based features ( X  X xpressions X ).
 sured using AUC, the area under ROC curve. The AUC values are g iven in Table 17. ion expressions significantly outperforms the classifier us ing only a bag-of-words fea-ture representation and also that using the lexicon-based f eatures. This demonstrates that the extraction and disambiguation of opinion expressi ons in their context is useful for a coarse-grained task such as document polarity classifi cation. The differences in
AUC values between the two best configurations are statistic ally significant (p &lt; 0 . 005 for all six collections). In addition, we show the precision/ recall plots in Figure 5; we see that for all six collections, the expression-based set-up outperforms the other two near the precision/recall breakeven point.
 set. The main difference of this collection compared with th e other collections is that its documents are larger: The average size of a document here is a bout four times larger than in the other collections. In addition, its reviews often contain large sections that are purely factual in nature, mainly plot descriptions. The opi nion expression identification may be seen as a way to process the document to highlight the in teresting parts on which the classifier should focus. 8. Conclusion
We have shown that features derived from grammatical and sem antic role structure can be used to improve three fundamental tasks in fine-graine d opinion analysis: the detection of opinionated expressions, the extraction of op inion holders, and finally the assignment of polarity labels to opinion expressions. The m ain idea is to use relational features describing the interaction of opinion expression s through linguistic structures such as syntax and semantics. This is not only interesting fr om a practical point of view (improving performance) but also confirms our linguist ic intuitions that surface-linguistic structure phenomena such as syntax and shallow s emantics are used in the encoding of the rhetorical organization of the sentence, an d that we can thus extract useful information from those structures. 504 that can appear anywhere in a sentence, exact inference in th is model becomes in-tractable. To overcome this issue, we used an approximate se arch strategy based on reranking: In the first step, we used the baseline systems, whi ch use only simple local features, to generate a relatively small hypothesis set; we then applied a classifier using interaction features to pick the final result. A common object ion to reranking is that the candidate set may not be diverse enough to allow for much i mprovement unless it is very large; the candidates may be trivial variations th at are all very similar to the top-scoring candidate. Investigating inference method s that take a less brute-force approach than plain reranking is thus another possible futu re direction. Interesting examples of such inference methods include forest rerankin g (Huang 2008) and loopy belief propagation (Smith and Eisner 2008). Nevertheless, although the development of such algorithms is a fascinating research problem, it will n ot necessarily result in a more usable system: Rerankers impose very few restrictions on fe ature expressivity and make it easy to trade accuracy for efficiency.
 parameters such as the choice of machine learning method and the size of the hypothesis set. For the features, we analyzed the impact of u sing syntax and semantics and saw that the best models are those making use of both. The m ost effective features we have found are purely structural, based on tree fragments in a syntactic or semantic tree. Features involving words generally did not seem to hav e the same impact. Sparsity may certainly be an issue for features defined in terms of tree fragments. Possible future extensions in this area could include bootstrapping method s to mine for meaningful fragments unseen in the training set, or methods to group suc h features into clusters to reduce the sparsity.
 experiments demonstrating that features extracted from op inion expressions can be used to improve practical applications: extraction of eval uations of product attributes, and document polarity classification. Although for the first task it may be fairly obvious that it is useful to carry out a fine-grained analysis of the se ntence opinion structure, the second result is more unexpected because the document po larity classification task is a high-level and coarse-grained task. For both tasks, we s aw statistically significant increases in performance compared not only to simple baseli nes, but also compared to strong baselines using a lexicon of sentiment words. Alth ough the lexicon leads to clear improvements, the best classifiers also used the featu res extracted from the opinion expressions.
 useful for practical applications on reviews from several d omains, because this corpus mainly consists of news documents related to political topi cs; this shows that the expres-sion identifier has been able to generalize from the specific d omains. It would still be relevant, however, to apply domain adaptation techniques ( Blitzer, Dredze, and Pereira 2007). It could also be interesting to see how domain-specific opinion word lexicons could improve over the generic lexicon we used here; especia lly if such a lexicon were automatically constructed (Jijkoun, de Rijke, and Weerkamp 2 010).
 tant issue that we have left open is the coreference problem f or holder extraction, which has been studied by Stoyanov and Cardie (2006). Similarly, r ecent work has tried to incorporate complex, high-level linguistic structure suc h as discourse representations (Asher, Benamara, and Mathieu 2009; Somasundaran et al. 200 9; Zirn et al. 2011); it is clear that these structures are very relevant for explainin g the way humans organize their expressions of opinions rhetorically. Theoretical d epth does not necessarily guar-antee practical applicability, however, and the challenge is as usual to find a middle ground that balances our goals: explanatory power in theory , significant performance gains in practice, computational tractability, and robust ness in difficult circumstances. Acknowledgments References 506 508
