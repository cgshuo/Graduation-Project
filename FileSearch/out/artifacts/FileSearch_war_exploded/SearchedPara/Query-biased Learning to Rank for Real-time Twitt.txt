 By incorporating diverse sources of evidence of relevance, learning to rank has been widely applied to real-time Twitter search, where users are interested in fresh relevant messages. Such approaches usually rely on a set of training queries to learn a general ranking model, which we believe that the benefits brought by learning to rank may not have been fully exploited as the characteristics and aspects unique to the given target queries are ignored. In this paper, we pro-pose to further improve the retrieval performance of learning to rank for real-time Twitter search, by taking the differ-ence between queries into consideration. In particular, we learn a query-biased ranking model with a semi-supervised transductive learning algorithm so that the query-specific features, e.g. the unique expansion terms, are utilized to capture the characteristics of the target query. This query-biased ranking model is combined with the general ranking model to produce the final ranked list of tweets in response to the given target query. Extensive experiments on the standard TREC Tweets11 collection show that our proposed query-biased learning to rank approach outperforms strong baseline, namely the conventional application of the state-of-the-art learning to rank algorithms.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Experimentation, Performance, Algorithms Real-time Twitter search, query-biased learning to rank, semi-supervised learning
Recency is an important dimension of information need in real-time Twitter search, where users tend to be inter-ested in fresh news and events [16]. There are multiple in-trinsic features of tweets, such as user authority, mentions, retweets, hashtags, etc. Thus, the application of learning to rank by incorporating diverse sources of evidence of recency and relevance has shown beneficial in previous studies, espe-cially as demonstrated in the TREC 2011 Microblog track [24, 21, 22]. Besides, Duan et al. employ RankSVM to rank the tweets which output the matched tweets based on their relevance to the query in content [5].

The methods mentioned above attempt to learn a  X  X en-eral X  ranking model in the sense that the features used are common to different queries, such as the relevance score given by the content-based retrieval model, the user au-thority, and so on. Therefore, the ranking model learned from the training data is assumed to be able to generalize to the test data. On the other hand, since the user X  X  infor-mation need is a query-dependent notion, different queries have their own unique characteristics and aspects, hence the need for the query-biased modeling to capture the differences and boundaries between queries. Indeed, a few previous ap-proaches have been proposed to take query differences into consideration during the learning process to improve the ef-fectiveness of learning to rank, for instance [32, 30, 11, 28].
Inspired by the above mentioned studies on the query dif-ferences, in this paper, we construct a combined learning to rank framework by integrating a general ranking model with the query-biased model that ta kes the query differences into account. In our proposed combined framework, the gen-eral ranking model is learned from training queries by the conventional learning to rank approach. In addition, we pro-pose to learn a query-biased ranking model by a transductive learning algorithm [27] based on the pseudo relevance infor-mation. Finally, the query-biased model is combined with the general model to produce the final tweet ranking for the target queries.

The major contributions of the paper are two-fold. First, we propose a semi-supervised learning algorithm to build a query-biased ranking model. Instead of treating all queries equally, we train a query-biased model based on the queries X  intrinsic features through an iterative learning process. Sec-ond, we propose a learning to rank framework to combine the query-biased model with a general ranking model learned from the common features. Extensive experiments on the standard TREC Tweets11 collection [24] demonstrate the effectiveness of our proposed query-biased learning to rank approach.

The rest of the paper is organized as follows. In Section 2, we survey the existing learning to rank research on Twitter search. Section 3 gives a detail description about the method to build the learning to rank framework by combining the general learning to rank algorithms and the query-biased algorithm, which is evaluated in Section 4. Finally, we con-clude this research and suggest future research directions.
In real-time Twitter search, freshness of the retrieval re-sults is an important aspect of information need. The user experience can be negatively affected when failing to recog-nize the temporal aspect of the query [4]. Therefore, there have been quite a few previous research on real-time Twitter search to retrieve not only relevant but also fresh tweets [7, 17]. In microblogging services, there are many aspects and characteristics of the social features [16, 2, 5]. All these char-acteristics and aspects have their potential influence on the relevance of the tweets. By using the learning to rank ap-proach, it is easy to properly integrate the variety of features into the retrieval model [24, 21, 22]. Learning to rank is a family of algorithms that automatically construct a model or function to rank objects. The major advantage of learn-ing to rank is its flexibility in incorporating diverse sources of evidence into the process of retrieval [19].

In addition to the above described approaches, there are other methods proposed in the TREC 2011 Microblog track [13, 18, 20]. Besides, there are research applying semi-supervised learning algorithms to facilitate learning to rank with only limited training data available [33, 6].
However, most of learning to rank approaches treat all the queries equally during the learning and ranking processes. As a result, the unique aspects of different queries are ig-nored, which may potentially hurt the retrieval effectiveness. Recently, there have been efforts to take query differences into consideration during the learning process to improve the ranking functions. Zheng et al. put forward a mini-mum effort optimization method by considering all the en-tire training data within a query during each iteration [32]. Wu et al. propose a listwise query-level regression method, called ListReg, by using the neural network to model the ranking function and gradient descent for optimization [30]. Geng et al. put forward a K-Nearest Neighbour method to learn different ranking functions based on different proper-ties of queries [11]. Besides, Veloso et al. propose a novel method to uncover the patterns or rules in the training data by generating association rules on a demand-driven basis at the query time [28].

We argue that the improvement brought by learning to rank can be further improved if the differences that exist in the diverse queries are considered during the retrieval process. To address this problem, in this paper, we propose a combined framework that makes the use of both the common features and query-specific features for learning to rank, as introduced in the next section.
We propose a learning to rank framework that utilizes both the common features of Twitter messages, and the query-specific aspects that differentiate between queries. The proposed framework combines a general ranking model and a query-biased ranking model. More specifically, the gen-eral ranking model is learned from the training instances, represented by the features common to different queries. The query-biased model is learned from the query-specific features by a semi-supervised learning algorithm. The two models are integrated by a linear combination as follows:
Score final ( d, Q )= Score LT R ( d, Q )+  X   X  Score QLT R where Score final ( d, Q ) is the final score of tweet d for the given query Q ; Score LT R ( d, Q )isthescoregivenbythe general ranking model; Score QLT R ( d, Q )isthescoregiven by the query-biased model. The setting of the parameter  X  is obtained by training on different folds of cross-validation.
Our features are organized around the basic entities for each query-tweet tuple to distinguish between the relevant and irrelevant messages, some of which have been widely used in previous work on Twitter search [5, 22, 21, 33]. More specifically, in addition to the five types of features which were used in our previous work [33], we exploit the sentiment features in this paper.

Sentiment refers to those features that indicate the opin-ions embodied in the given tweet. In our model, we extract the proportion of negative sentiment words and positive sen-timent words, as defined in SentiWordNet [9], in the content of the tweet.
Many learning to rank approaches have been proposed in the literature [19], which can be applied for learning the gen-eral ranking model. Among them, in this paper, we choose to use two popular learning to rank algorithms, namely RankSVM [15, 29, 12] and LambdaMART [31, 10], which has shown to be a robust algorithm for solving real world ranking problems [3].

In the learning process, after the positive and negative examples are appended to the labeled set by making using of the relevance assessments information, we empirically as-sign preference values according to the temporal distance between the timestamps of the tweet and the query. The larger the preference value is, the higher the tweet is rele-vant to the given query. This labeling strategy is mainly due to the fact that recency is a crucial factor of relevance in real-time Twitter search. The fresh tweets are more likely to be relevant than those outdated.

The target values of RankSVM define the order of the examples of each query. We arbitrarily reassign the target values of the relevant tweets with an interval of 0.5, ranging from 0.7 to 3.2, according to the temporal distance in days between the timestamps of the tweet and the query [33]. Since the target values in LambdaMART is of the integer type, we round the double target values used in RankSVM for the listwise LambdaMART approach.
Since the purpose of the query-biased modeling is to uti-lize the query-specific characteristics to boost the retrieval performance, it is a challenging issue to select the appropri-ate features that are unique to the given queries to represent the tweets. In this paper, we choose to represent the tweets by the most informative terms in the pseudo relevance set, namely the top-ranked tweets in the initial retrieval. As queries are different to each other in their topical concepts, it is a natural choice to represent the query-specific aspects by the most weighted terms in the pseudo relevance set, which are usually assumed to be highly related to the query topics.

Figure 1 provides the algorithm used for extracting the term features for the query-specific tweet representation. In particular, all the unique terms in the top-30 tweets are taken as candidate terms, and the 10 terms with highest KL divergence weights are chosen as the query-specific features. Thus, the selected words and their corresponding KL diver-gence weights are used as attributes and values to represent the given tweets. Our arbitrary choice of selecting the top-10 terms from the top-30 tweets is mainly due to the fact that this setting was found to provide the best query expansion effectiveness in the TREC 2011 Microblog track, as reported in [1]. The KL divergence weight of a candidate term t in the top-k ranked tweets in the initial retrieval is computed as follows: where P ( t | R k ) is the probability of generating the candidate term t from the set of top-k ranked tweets R k ,and P ( t is the probability of generating t from the entire collection C .

Note that the relevance scores produced by query expan-sion have already been used in Section 3.1.1 as features to learn a general ranking model. In the query-biased model, instead of being wrapped up as an expanded query into the content-based model to produce a relevance score, the most weighted terms are treated as unique terms to represent the query-specific aspects of the tweets.
In this paper, a query-biased transductive learning algo-rithm is devised to boost the retrieval performance. Trans-ductive learning [27] is a semi-supervised method for clas-sification by utilizing limited data using transduction, it is not necessary to generate a model to predict the label of any unobserved point during the process of learning. The idea of learning from limited training data to improve re-trieval performance has been studied in a number of previ-ous publications [14, 26]. Different from the previous work, in this paper, the query-biased transduction learning algo-rithm and retrieval are integrated into a learning to rank paradigm. A general description of the proposed method is given in Figure 2. The underlying idea of our proposed method is similar to that of pseudo relevance feedback [25], which assumes a high degree of relevance of the top-ranked documents. However, we determine to adopt the transduc-tion algorithm to select the appropriate tweets, instead of just the top-ranked ones which are used in the conventional content-based query expansion models, due to the fact that the top documents do not necessarily represent an optimal feedback set. Before training, it is only required to predict the labels of a given test set examples [8]. After the initial retrieval using the content-based model, we label the top-ranked and bottom-ranked tweets are selected as the posi-tive and negative examples, respectively, on the hypothesis that the top-ranked tweets are highly relevant to the given query topic. In contrast, the bottom-ranked are believed to some extent off-topic in their contents. A model is learned to predict the ranking of the remaining unlabeled tweets, from which the very top-ranked and bottom-ranked tweets are appended to the labeled data set. Such a process repeats until the number of iterations exceed a predefined threshold. Finally, a ranking of all the retrieved tweets are produced by the query-biased model learning by the above transduction process. All the parameters in the proposed query-biased transductive learning algorithm, for example the number of iterations (i.e. in Figure 2), are obtained by tuning on train-ing queries.

Moreover, we define two variants of the query-biased learn-ing as follows:
Per-query learning: for each query, a query-biased model is learned with the unique term features. Thus, it is able to distinguish the special aspects of the individual target queries. This is equivalent to the algorithm in Figure 2 when there is only one test query. However, this method may suffer from the problem of sparseness since the learning is based only on the pseudo relevance set of single queries.
Batch learning: Instead of building the query-biased model for each target query, this method treats the selected highly weighted terms of different queries as the common features for a batch of target queries. As this method takes the pseudo relevance sets of a batch of queries as the training data, it is expected to be able to deal with the sparseness problem.

We only apply RankSVM for learning a query-biased model in this paper. This is mainly due to the small amount of pseudo training data available, where the advantage of the listwise approaches such as LambdaMART is negated. The query-biased model is an on-line learning process, which takes about 17 seconds for each query.
We experiment on the Tweets11 collection, which is used for evaluating the participating real-time Twitter search sys-tems over 50 official topics in the TREC 2011 Microblog track. Our crawl of the Tweets11 collection consists of 13,401, 964 successfully downloaded unique tweets. Standard stop-word removal and Porter X  X  stemmer are applied during in-dexing and retrieval. The official measure in the TREC 2011 Microblog Track, namely precision at 30 (P30), is used as the evaluation metric in our experiments. All of the index-ing and retrieval experiments are conducted on an in-house version of Terrier [23]. Figure 2: The query-biased transductive learning algorithm.
 Table 1: Values used in grid search for Lamb-daMART parameter tuning on the training queries.

The choice of the hyperparameter C, the regularization parameter for RankSVM, plays an important role in the retrieval performance. C can be selected by carrying out cross-validation experiments through a grid search from 1 to 40 on the training queries.

The LambdaMART algorithm has five parameters that need to be tuned to achieve the best results. We apply a greedy boosting algorithm for the parameter tuning. Firstly, we optimize each of the five parameters, while keep the other four parameters to the default values. Then, we set the parameter that has the highest optimized retrieval perfor-mance to its optimized value. This process repeats until all the parameters are optimized. We apply grid search to optimize each parameter. Values we scan for each of the parameters are listed in Table 1.
The aim of our experiments is to evaluate the effective-ness of the proposed learning to rank framework which com-bines the general learning to rank approach and transduc-tive query-biased method for real-time Twitter search. In particular, two state-of-the-art learning to rank approaches, namely Ranking SVM (RankSVM) [15], a classical pair-wise learning to rank algorithm and LambdaMART [31], a tree-based listwise learning to rank algorithm which has the best performance in the 2010 Yahoo! Learning to Rank challenge [3], are applied. We compare our proposed com-bined learning to rank framework, denoted as CombLTR , to the general ranking model (LTR) , which represents the conventional application of the state-of-the-art learning to rank algorithms. Extensive experiments are conducted us-ing RankSVM and LambdaMART, where the general rank-ing models are learned from the official relevance assess-ments from the Microblog track 2011. Using the LTR ap-proach, we conduct experiments in different granularities of the partitioning of the train-test topics, namely the 2-fold ( LTR 2 )5-fold( LTR 5 ),10-fold ( LTR 10 )andleave-one-out ( LTR L1 ) cross-validations, respectively. Cross-validation is a technique to estimate the performance of a predictive model. In K-fold cross-validation, the 50 topics is partitioned equally into K subsets. Among them, one subset is choosen to test the model, while the others are used for training.
The combined framework CombLTR is compared to the state-of-the-art LTR approaches, which learn the ranking models from the official relevance assessments. We exam-ine to which extent our proposed combined framework is able to improve the retrieval effectiveness by taking query differences into consideration. 2, 5, 10-fold, and leave-one-out cross-validations are conducted for the evaluation. Our proposed approaches, using QLTR Batch and QLTR PerQ respectively, are compared to the general LTR approaches which utilizing the relevance assessment information in Ta-ble 2. We can see that CombLTR Batch significantly out-performs RankSVM on 5 and 2-fold cross-validations, and outperform LambdaMART on the 10-fold cross-validation; while CombLTR PerQ outperforms LambdaMART on 10-fold and leave-one-out cross-validations. Comparing to Rank-SVM, CombLTR PerQ shows no notable improvement. In addition, a surprising observation is that, using RankSVM, CombLTR has better performance with fewer training data available, i.e. during the 2-fold and 5-fold cross-validations. The opposite is observed when using LambdaMART. We suggest that this is possibly due to the fact that RankSVM has a better ability in dealing with the sparseness prob-lem than LambdaMART in the learning process. Besides, the granularity of cross-validation is believed as a factor affecting the retrieval effectiveness, just as Table 2 shows that CombLTR Batchoutperform RankSVM on 5 and 2-fold cross-validations, while CombLTR PerQ outperforms Lamb-daMART on 10-fold and leave-one-out cross-validations.
In summary, our proposed method, namely the combined learning to rank framework, is able to significantly improve the retrieval effectiveness when comparing with the content-based retrieval models and the popular learing to rank ap-proaches. In addition, despite the noise during the learning of the query-biased ranking model, a series of experiments on Tweets11 demonstrate the benefit brought by the method we put forward in this paper, especially when there is only limited amount of training queries available.
In this paper, we have proposed a combined learning to rank framework that utilizes both the general and query-specific evidence of relevance for the real-time Twitter search. In particular, a query-biased ranking model is learned by a semi-supervised transductive learning algorithm in order to better capture the characteristics of the given queries. Such a query-biased ranking model is combined with a general ranking model given by the conventional learning to rank approach to produce the final ranking of the Twitter mes-sages, namely the tweets, in response to the user informa-tion need. Extensive experiments have been conducted on the standard Tweets11 dataset to evaluate the effectiveness of our proposed approach. Results show that the our pro-posed combined learning to rank approach is able to outper-form strong baseline, namely the state-of-the-art learning to rank algorithms. Moreover, our study in this paper suggests the possibility of simulating a training data without involv-ing human labels on given new queries, while achieving an effective retrieval performance by a combination with the conventional learning to rank approaches.

According to the experimental results, the number of iter-ations in the transductive learning process is a major factor that affects the effectiveness of the proposed approach. In the future, we plan to improve the robustness of the pro-posed approach by introducing an adaptive halting criterion of the iterative process.
 This work is supported in part by the National Natural Sci-ence Foundati on of China (61103131/F020511), the Presi-dent Fund of GUCAS (Y15101FY00/Y25102HN00), and the CAS R&amp;E Project (110700EA12).
