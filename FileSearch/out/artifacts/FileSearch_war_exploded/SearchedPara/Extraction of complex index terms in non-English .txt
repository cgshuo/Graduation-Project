 1. Introduction
Natural language processing (NLP) has frequently attracted the attention of the information retrieval (IR) community, since textual IR can be considered an NLP problem. This is because the task of deciding about the relevance of a given document with respect to a query basically consists in deciding whether the text of the document satisfies the information need expressed by the text of the query, the content of the document thus one of the major limitations of IR systems is the linguistic variation inherent to human language ( Arampatzis, efficient, robust and precise tools. These advances, together with the increasing power of new computers, facil-itate the application of NLP systems in real IR environments.

Morphological variation , for example, has usually been solved in IR systems through the employment of stemmers, which reduce the word to its supposed grammatical root, or stem, through suffix stripping based on a list of frequent suffixes. The effectiveness of stemming is dependent on the morphological complexity of the language ( Arampatzis et al., 2000 ). In the case of Romance languages, stemming does not seem to be an appropriate solution since many inflectional phenomena cannot be managed by such a simple tool.
Moreover, stemming can also cause problems in further processing because of the loss of information it involves ( Kowalski, 1997 ), a major problem when we intend to perform further processing.
In this context, NLP-based approaches seem to be a better solution. This is the case, for example, of the use contraction splitting, separation of enclitic pronouns from verbal stems, proper noun recognition, etc.) is applied to avoid erroneous behaviors during further processing ( Jurafsky &amp; Martin, 2000; Manning &amp;
Schu  X  tze, 1999 ); secondly, a state-of-the-art part-of-speech tagger and lemmatizer is applied and the lemmas of the content words ( Jacquemin &amp; Tzoukermann, 1999 ), namely nouns, verbs and adjectives, the grammatical categories containing the main semantics of the text, are extracted for indexing, where the lemma of a word stands for its canonic form  X  e.g. infinitive in the case of verbs or masculine singular in the case of nouns and adjectives.

Going one step further, another possibility consists, for example, in using morphological families for deal-ing with derivational morphology ( Vilares, Cabrero, &amp; Alonso, 2001 ). A morphological family is the set of words obtained from the same morphological root through derivation mechanisms ( Jacquemin &amp; Tzouker-mann, 1999 ), taking into account the derivational morphemes, their allomorphic variants and the phonolog-ical conditions they must satisfy. For each family, automatically generated in an off-line process, a unique identifier or representative is selected. In this way, during indexing, such a representative will be used for all words belonging to the same family.

An important fact is that the computational complexity of this NLP processing chain is linear with respect ares, Cabrero et al., 2001 ).

At a higher level of linguistic description, we can find syntactic variation , the modification of the syntactic structure of a sentence whilst keeping its underlying semantics for retrieval purposes. Syntactic variation has been traditionally managed through two different approaches: the use of syntactic structures, and the use of phrases as complex index terms. The goal pursued is to increase the precision of retrieval, trying to avoid the limitations of the bag-of-terms paradigm ( Tzoukermann, Klavans, &amp; Jacquemin, 1997 ). The use of complex representations based on syntactic structures such as trees ( Sheridan &amp; Smeaton, 1992; Smeaton, O X  X onnell, real environments because of its high processing cost. A more feasible approach consists in the use of phrases as index terms, since phrases denote more meaningful concepts or entities than single words, thus being more damaging recall, because the simple terms compounding the phrase would also have matched ( Mitra, Buckley, obtained through statistical techniques looking for sequences of contiguous words co-occurring with a signif-tic phrases, obtained through NLP techniques and formed by syntactically related sets of words ( Dillon &amp; Gray, 1983; Fagan, 1987; Hull et al., 1997; Jacquemin, 2001; Koster, 2004; Narita &amp; Ogawa, 2000;
Pohlmann, 1998; Mitra et al., 1997 ). Which phrase type has a better performance in IR tasks is still an unde-cided issue, although some results show syntactic phrases are the best choice when accurate parsing and syn-tactic disambiguation techniques are available ( Arampatzis et al., 2000 ). Another common approach consists in limiting the complexity of complex terms by only using pairs, thus decompounding those complex terms formed by more than two single terms into compounds of only two terms ( Arampatzis et al., 2000; Fagan, 1987; Koster, 2004; Perez-Carballo &amp; Strzalkowski, 2000 ).

Finally, since the use of phrases alone as index terms enables only a partial view of the document to be captured ( Mitra et al., 1997 ), complex index terms are frequently used in combination with simple index terms Smeaton, 1992; Smeaton et al., 1995; Strzalkowski &amp; Perez-Carballo, 1994 ).

In this context, the aim of the present article is to study the viability of the application of NLP for dealing with syntactic variation in European IR systems, taking Spanish as a case in point. The greater linguistic com-plexity of Romance languages in comparison with English, at both a syntactic and morphological level, does languages therefore demand their own experiments. Moreover, our approach introduces numerous features, which will be described in the following sections, that serve to differentiate our proposal from previous works.
Thus, in contrast to previous syntactic-based approaches, our proposal not only deals with strict syntactic variation, but also with morpho-syntactic variation, which combines both syntactic transformations and der-ivational phenomena. On the other hand, although previous approaches mainly work with noun phrases only, our work extends the range of phrases by also including verb phrases.

Another interesting aspect that differentiates our work from previous approaches is the fact that we have focused our study not only on the syntactic information obtained from queries, but also on the syntactic infor-mation offered by documents. Such syntactic information is obtained through shallow parsing, an approach often used in Information Extraction but not so commonly in IR.

Furthermore, we have to face one of the main problems in non-English NLP research, namely the lack of freely available linguistic resources: large tagged corpora, treebanks and advanced lexicons are not currently available for languages such as Spanish. The solution for minimizing this problem consists in restricting the complexity of the solutions proposed by focusing on the use of lexical information, which is easier to obtain. Limiting the complexity of the proposed approaches results in a general architecture which can be applied to
Romance languages in particular and to languages with similar characteristics and behavior in general. thermore, in order to minimize the computational cost of our approaches for their application in practical environments, finite-state technology has been widely used.

The structure of the rest of this article is as follows. Firstly, Section 2 explains how phrases can be used as complex index terms for dealing with syntactic and morpho-syntactic variation. Next, Section 3 describes the shallow parser developed for this task, while Section 4 introduces the indexing mechanism used. A detailed evaluation of our proposals is performed in Section 5 . Finally, Section 6 explains our conclusions and future work. 2. Complex terms as index terms
We can find noun and verb phrases in a text ( Arampatzis et al., 2000; Koster, 2004 ). In both cases, the degree of specificity of the phrase is greater than those for the individual simple terms it contains, phrases therefore becoming more precise and descriptive index terms than their individual components ( Arampatzis et al., 2000; Fagan, 1987; Strzalkowski &amp; Perez-Carballo, 1994 ). Thus, phrases are often used to obtain complex index terms , also called multi-word index terms , in order to complement the semantics of documents captured by representations based on simple index terms. Accordingly, phrases to be used as complex index terms should include, at the very least, noun phrases together with those relations also involving verb phrases (i.e., involving the main verb of the sentence and its subject, object or adjuncts).

Since the number of possible phrases is almost unlimited, the space of multi-word terms is much sparser than in the case of simple terms ( Arampatzis et al., 2000; Koster, 2004 ). So, it becomes necessary to develop a conflation mechanism able to project all the semantically equivalent forms of a phrase into the same index tion, half-way between an easy-to-compute plain representation, and a more complete, but also more complex, 2001, 2004 ) or graphs ( Montes-y-Go  X  mez et al., 2000 ).

Our approach relies on the extraction of the dependencies established between the different words contained in a sentence. When limiting the number of terms involved in a dependency to two words, we will obtain syn-coe, &amp; Sanfilippo, 1998 ):  X  Noun X  X djective, relating the head of a noun phrase with its modifying adjective.  X  Noun X  X djective prepositional phrase, 3 relating the head of a noun phrase with the head of the modifying prepositional phrase.  X  Subject X  X erb, relating the subject head with the main verb of the clause.  X  Verb X  X bject/adjunct, relating the main verb of the clause with the head of its object or adjunct.
Such dependency pairs will be used as complex index terms, called head-modifier pairs ( Arampatzis et al., modifier relation may suggest semantic dependence, what we obtain here is strictly syntactic, even though the &amp; Strzalkowski, 2000; Sheridan &amp; Smeaton, 1992; Smeaton et al., 1995 ).

Once the pairs have been identified and extracted, their component terms are themselves conflated, firstly by means of lemmatization, and then by morphological families (see Section 1 ). We are thus eliminating both the inflectional changes associated with syntactic and morpho-syntactic variants and the derivational transfor-mations of morpho-syntactic variants, which involve both syntactic variation and derivational transforma-tions ( Jacquemin, 2001; Jacquemin &amp; Tzoukermann, 1999 ). Some related approaches can be found in the literature. In Koster (2004) such components are lemmatized in order to eliminate inflection. In Strzalkowski and Perez-Carballo (1994), Mitra et al. (1997) and Kelledy and Smeaton (1997) , stemming techniques are used instead, also covering derivative phenomena. Finally, in Arampatzis et al. (2000) verb nominalization and noun verbalization are proposed for this task.
 In order to extract the dependencies, we must first analyze the syntactic structure of documents and queries. computational cost, which makes its application on a large scale impractical. Moreover, the lack of robustness of such approaches, which seek to obtain a complete parse of the whole sentence, greatly reduces their cov-of freely available resources such as grammars and treebanks. In this context, seeking to obtain a compromise between the quality of the syntactic information to be extracted and the ease of its extraction, the employment of shallow parsing techniques ( Abney, 1997 ) enables us both to reduce computational complexity and increase robustness. Shallow parsing has shown itself to be useful in several NLP application fields, particularly in information extraction ( Aone, Halverson, Hampton, &amp; Ramos-Santacruz, 1998; Grishman, 1995; Hobbs et al., 1997 ). Nevertheless, its application in IR has not yet been studied in depth, previous studies having mainly focused on languages other than Spanish and having often been limited to the obtaining of simple noun phrases ( Hull et al., 1997; Kraaij &amp; Pohlmann, 1998; Mitra et al., 1997 ).

There are also some approaches based on the use of existing terminological databases to extract a lexical-ized grammar. For instance, the work of Jacquemin ( Jacquemin, 2001; Jacquemin &amp; Tzoukermann, 1999 )is based on the term lists extracted from thesauri used for manual indexing at INIST/CNRS, a documentation center for scientific and technical information that produces two bibliographical databases, PASCAL and
FRANCIS, indexed with a controlled thesaurus. We have not followed this approach due to the fact that only scarce, small and often non-free resources of this kind are available for Spanish ( ACRoTermite, 2007; Buyse, 2003; Crespo Leo  X  n et al., 2005; Husson, Viscogliosi, Romary, Descotte, &amp; Campenhoudt, 2000; Reynoso et al., 2000; VERBA, 2007 ). 4 3. Shallow parsing through cascades of transducers: the cascade parser We have developed an advanced, modular, widely applicable and robust parser, named cascades of finite-state transducers. The theoretical basis for its design comes from Formal Language Theory first layer obtains the nodes labeled by non-terminals corresponding to left-hand sides of productions that only contain terminals on their right-hand side; the second layer obtains those nodes which only involve ter-minal symbols and those non-terminal symbols generated on the previous layer; and so on. 3.1. System architecture
The shallow parser is based on a five-layer architecture whose input is the output of a tagger-lemmatizer.
The rest of the section describes how each layer works. For this purpose, we will use as our notation context-free rules extended with classical regular expression operators. In the same way, uppercase identifiers denote a set of terms, which can either be pre-terminals, namely tags resulting from part-of-speech tagging, or elements of a given grammatical category. When the presence of a concrete lemma is required, this will be indicated by using the typewriter font. 3.1.1. Layer 0: preprocessing extension
This layer extends the linguistic preprocessing capability of the system, minimizing the noise generated dur-ing the subsequent parsing steps. It deals with:  X  Numerals in non-numerical format.  X  Quantity expressions. Expressions of the type algo ma  X  s de dos millones (a little more than two million), which denote a number but with a certain vagueness about its concrete value, are identified as numeral phrases ( NumP ).  X  Expressions with a verbal function. Some verbal expressions such as tener en cuenta (to take into account), must be considered as a unit  X  in this case synonym of the verb considerar (to consider)  X  to avoid errors in the upper layers such as identifying en cuenta as an object or adjunct of the verb. 3.1.2. Layer 1: adverbial phrases and first level verbal groups
This layer consists of rules only containing tags and/or lemmas in its right-hand side. To enable the next firstly, the lemma corresponding to the head of the phrase we are recognizing, and secondly, the tag with the appropriate morpho-syntactic features. The notation employed for this inheritance mechanism is inspired in the notation employed when specifying the set of restrictions in feature structure-based grammars ( Carpenter, 1992 ).

The first rule we describe here allows us to identify sequences of adverbs ( W ), called adverbial phrases ( AdvP ). The last adverb will be considered the phrase head, so its lemma and its tag will be the lemma and tag of the non-terminal AdvP :
The following set of rules allows us to identify first level verbal groups ( VG1 )  X  or non-periphrastic verbal groups  X  corresponding to passive forms, 6 whether simple tenses, e.g. soy observado (I am observed), or com-first rule manages compound forms: the tag is taken from the auxiliary verb haber (to have), whereas the lemma is taken from the main verb, which must be a participle, the same as the auxiliary verb ser (to be).
The second rule manages simple forms: the tag is obtained from the form of the auxiliary verb ser , whereas the lemma is taken from the main verb, again a participle.
 3.1.3. Layer 2: adjectival phrases and second level verbal groups
Adjectival phrases ( AdjP ) and second level verbal groups ( VG2 ) are processed here. An adjectival phrase ( AdjP )  X  e.g. azul (blue) or muy alto (very tall)  X  is formed by a head adjective ( A ) sometimes preceded by an adverbial phrase ( AdvP ) modifying it:
Second level verbal groups ( VG2 ) include periphrastic verbal forms such as tengo que ir (I have to go). Verbal periphrases are unions of two or more verbal forms working as a unit, giving to the semantics of the main verb attributive shades of meaning, such as obligation and degree of development of the action, which can not be expressed by means of the simple and compound forms of the verb. A periphrasis is generally formed by a giving the main meaning, and an optional element (preposition or conjunction) linking both verbs.
Infinitive periphrases are identified using the following rule, which takes into account the possibility of the existence of an enclitic pronoun (previously separated from the verb form by the tagger) when the auxiliary the main verb:
Gerund and participle periphrases are managed in a similar way, whereas first level verbal groups which do not take part in any periphrastic group are promoted to second level verbal groups. 3.1.4. Layer 3: noun phrases
Noun phrases ( NP ) are processed in this layer. 8 We have taken into account the possibility of their being preceded by a partitive complement ( PC ) such as alguno de (some of), ninguno de (none of), etc.:
Following the head of the noun phrase, there may appear an adjectival post-modifier consisting of two adjec-tival phrases coordinated by a conjunction ( Cc ), or consisting of a sequence of one, two or even three adjec-tival phrases:
The head of the noun phrase is formed by a common noun ( N ), an acronym or a proper noun; its tag and lemma will decide the tag and lemma of the whole phrase. In the case of several candidates for head appearing, we will take the last one. Optionally, we may find one or more determiners ( D ) and an adjectival phrase before the head. 10 The existence of adjectival post-modifiers is also optional, and thus we finally obtain the rule: 3.1.5. Layer 4: prepositional phrases
The function of the last layer is to identify prepositional phrases ( PP , PPof , PPby ), i.e. those formed by a noun phrase ( NP ) preceded by a preposition ( P ). To make the extraction of dependencies easier, we will dis-tinguish those phrases introduced by the prepositions de (of) and por (by) from the rest, producing the fol-lowing rules: 11 3.2. Identification of syntactic roles
The syntactic roles we are trying to identify, and the heuristics used for this purpose, are the following:  X  Subject. The closest noun phrase ( NP ) preceding a verbal group ( VG2 ) in personal form will be considered the subject of the sentence.  X  Object. This is the closest NP after an active non-copula VG2 .  X  Agentive BY-phrase. 12 This is the closest PPby following a passive non-copula VG2 .  X  Subject complement. For a copula verb, we will identify as the subject complement that non-attached AdjP or that head of a NP / PPof closest to the verbal group.  X  Adjunct. Due to the problem of prepositional phrase attachment, we have opted for a strict criterion when searching them in order to minimize the noise introduced by erroneous identifications. We will only con-sider as an adjunct that prepositional phrase following the verb which is closest to it, and previous to any subject complement, object or adjunct identified beforehand.  X  Adjective prepositional phrase. Due to the ambiguity in the attachment of prepositional phrases, we will only consider the prepositional PPof phrases due to their high reliability. So, when the system finds a PPof immediately after a noun or prepositional phrase, it is identified as an adjective prepositional phrase. 3.3. Extraction of dependencies
Once we have identified the syntactic roles of the phrases of the sentence, the syntactic dependencies exist-ing between them are extracted in the form of pairs that involve:  X  A noun and each of its modifying adjectives. In fact, whereas the rest of dependencies are extracted once the parsing process has finished, dependencies of this kind are extracted during the identification of noun phrases in layer 3. This is because such dependencies are internal to the noun phrase, and therefore, if they are not extracted at that point, the information would be lost once the phrase is reduced to its head.  X  A noun and the head of its modifying adjective prepositional phrase.  X  The head of the subject and the non-copula verb.  X  The head of the subject and the head of the subject complement, since from a semantic point of view cop-ula verbs act as mere links between them.  X  An active verb and the head of the object.  X  A passive verb and the head of the agentive BY-phrase.  X  A non-copula verb and the head of the adjunct.  X  The head of the subject and the head of the adjunct, but only when it is a copula sentence, due to the special behavior of copula verbs, as described above.

For each dependency extracted, their components are conflated through both lemmatization and morpho-logical families, as described in Section 2 . 3.4. Implementation of the shallow parser
Each of the rules involved in the different stages of the parsing process has been implemented through a mation of such pairs only involves the heads of the phrases, so we only need to retain the lemma of the head, together with its corresponding morpho-syntactic features. When extending such an approach to all layers, text will be formed by lemma tag non-terminal triplets. One of the main advantages of employing this representation is that it allows us to make references to any of the components (i.e. lemmas, tags and we will consider that every grammatical category is a valid non-terminal. This way, at the beginning, the out-put of the tagger is directly translated into the format required by the parser: lemma tag non-terminal .
Each time the parser finds a matching of the right-hand side of the rule with the input terms, the rule is reduced, and the matching terms are replaced by the left-hand side of the rule. Usually, the lemma and tag of the phrase are inherited from the lemma and the tag of its head. The output of the parser is a sequence of lemma tag non-terminal triplets corresponding to the phrases recognized during the parsing process. 3.5. A running example
Let us take as example the following sentence: Docenas de ni n  X  os muy alegres han estado apren-diendo hoy en el colegio una lecci o  X  n de historia (Dozens of very happy children have been learn-ing today in the school a lesson of history). The first step consists of tagging and lemmatizing the input sentence  X  you are reminded that, in this initial stage, the non-terminal is the grammatical category of the [ docena (dozen) NCFP N ][ de (of) P P ][ ni n  X  o (child) NCMP N ][ muy (very) WQ W ] [ alegre (happy) AQFP A ][ haber (to have) V3PRI V ][ estar (to be) VPMS V ] [ aprender (to learn) VRG V ][ hoy (today) WI W ][ en (in) P P ][ el (the) DAMS DA ] [ colegio (school) NCMS N ][ un (a) DAFS DA ] [lecci o  X  n (lesson) NCFS N ][ de (of) P P ] [ historia (history) NCFS N ]
A summary of the parsing process is shown in Fig. 1 . At the output of the cascade we obtain the sequence of heads corresponding to the phrases identified during this process: [ ni n  X  o (child) NCMP NP ] [aprender (to learn) V3PRI VG2 ][ hoy (today) WI AdvP ] [ colegio (school) NCMS PP ] [lecci o  X  n (lesson) NCFS N ][ historia (history) NCFS PPof ]
The dependencies contained in the parsed text are then extracted in order to be conflated into complex index terms. Firstly, according to the criteria established in Section 3.2 , the syntactic roles of the phrases are identified. In the case of our example, an active subject ( SUBJact ), an active non-copula verbal group identified: [ ni n  X  o (child) NCMP NP ] X  h SUBJact i [ aprender (to learn) V3PRI VG2 ] X  h Vact i [ hoy (today) WI AdvP ] X  hi [ colegio (school) NCMS PP ] X  h Adjunct i [ lecci o  X  n (lesson) NCFS NP ] X  h OBJ i [ historia (history) NCFS PPof ] X  h APP i Next, the dependencies indicated in Section 3.3 are extracted, obtaining as output: ADJ (ni n  X  o NCMP , alegre AQFP) APP (lecci o  X  n NCFS , historia NCFS) SUBJact (aprender V3PRI , ni n  X  o NCMP) OBJ (aprender V3PRI , lecci o  X  n NCFS)
Adjunct (aprender V3PRI , colegio NCMS)
The tests performed with this new parser have shown a reliable behavior when identifying the existence of syntactic dependencies between two words. Nevertheless, such dependencies are not always correctly classi-fied. This is the case, for example of subjects coming after verbs, which are identified as objects or adjuncts by the heuristics. 14 The existence of a dependency has however been correctly identified. Errors of this kind when identifying the syntactic role of a phrase are not a problem in our IR task, since such information is dismissed when conflating and adding the dependency to the index. The main point here is to detect the depen-dency correctly, a task that the parser performs reliably. 4. Indexing
Complex terms have to be considered as a complement of simple terms, since the exclusive use of complex view of the semantics of the text to be captured ( Mitra et al., 1997 ). Furthermore, when only complex terms are used, system recall is clearly reduced because of the high degree of sparseness of their term space. This is because the number of dependency pairs existing in a collection is much higher than the number of words it contains, since given a set of words, the number of phrases that can be built with them is much higher than the number of words forming such a set. As an example, we show in Table 1 the distribution of both word lemmas ( lem ) and dependency pairs ( qdp ) in the Spanish CLEF 2003 corpus ( CLEF, 2007 ), one of the test collections we have used. So, the probability of occurrence of the same phrase in two different documents is much lower than that of the words it contains and consequently the probability of a matching during the retrieval process is also much lower. This is why complex terms should be used in combination with simple terms ( Buckley 1992; Strzalkowski &amp; Perez-Carballo, 1994 ). In this work, dependency pairs are used as index terms together with the content word lemmas of the text. However, the combined use of simple and complex terms creates several problems: (i) The assumption of term independence is violated since words forming a pair also occur in the documents (ii) There is an over-balance of the weight of complex terms, which occur much less frequently than simple
This situation introduces an element of instability into the system, because when undesired matchings of complex terms with non-relevant documents occur, their relevances increase excessively. At the same time, and also due to the same reason, when correct matchings between complex terms and relevant documents occur, we obtain a clear improvement of the results with respect to the employment of simple terms only.
It can be argued that according to this we would expect similar results to those obtained only with simple terms. Nevertheless, complex term matchings are much less frequent than those for simple terms because of their high sparseness. Therefore, fortuitous matchings of complex terms are much more harmful than those for simple terms, whose effect tends to be weakened by the rest of the matchings. Thus, we can state that the noise introduced by erroneous complex term matchings is amplified. We therefore need to solve this over-balance of complex terms in order to minimize the negative effect of undesired matchings.
The solution to both problems consists in decreasing the extra initial relevance assigned to complex terms
Ogawa, 2000 ). 5. Evaluation
In order to evaluate our approach, it has been integrated into the well-known vector-based engine SMART ( Buckley, 1985 ), using an atn-ntc weighting scheme ( Savoy, 2003 ). Since our aim is to investigate whether syntactic processing can be used to improve the performance of classic IR systems, we have chosen as working environment a classic configuration which can be considered, to a certain extent, standard.
The system has been tested using the CLEF 2001 X 2003 Spanish monolingual test collection, become the standard evaluation corpus for Spanish IR tasks. An IR test collection is composed of three parts ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ): the documents, the example information requests  X  called topics in the literature  X , and a list of relevant documents for each of these topics.

In this case, the document collection is formed by news reports, formatted in SGML, provided by the Span-ish news agency EFE. The initial collection, used in CLEF 2001 X 2002, was formed by news reports from the year 1994 (subcollection EFE 1994) and was enlarged in CLEF 2003 with news reports from the year 1995 ( 1995). The composition of these subcollections is shown in Table 2 .
With respect to the topic set, 50 topics were used in CLEF 2001 (numbered 41 X 90), a further 50 in CLEF 2002 (91 X 140) and 60 in CLEF 2003 (141 X 200). The topics are formed by three fields, as shown in Fig. 2 :a
The resulting  X  X  X hort  X  queries are, to a certain extent, an acceptable approximation to those queries used in commercial engines, and have used for this purpose by the research community repeatedly ( Hull, 1996 ). More-over, experiments using these fields are those required by the CLEF organization for the official workshop ranking.

On the other hand, following Hull et al. (1997) , those topics with less than five relevant documents have been removed. This is because when the number of relevant documents is too small, any minor variation in the ranking of just one or two documents can involve noticeable changes in the results for that query, thus distorting the global results.

Finally, in order to maximize the homogeneity of the evaluation corpus, queries from CLEF 2001 and 2002 were combined. 17 Three corpora, whose statistics are shown in Table 3 , resulted from this process:
CLEF 2001 X 02 A: a corpus for training and parameter estimation formed by the CLEF 2001 X 2002 document
CLEF 2001-02 B: a corpus for evaluation employing the same news collection as the previous one, but using
We will now show the results obtained when using syntactic dependencies as complex index terms for com-plementing simple terms. Firstly, Section 5.1 discusses the results of the tests performed using the syntactic information extracted from queries. Next, Section 5.2 discusses the results obtained when the syntactic infor-mation extracted from documents was used. 18 5.1. Results using the syntactic information extracted from queries 5.1.1. Results for all dependencies
In this first set of experiments we combine lemmatized simple terms and complex terms obtained from que-ries. The terms extracted from documents, both simple and complex, are combined and added to the index as described in Section 4 . Query processing is similar.

The weight of simple terms is multiplied by a balance factor x in order to reduce the relative contribution of complex terms in a ratio 1 = x . This factor x has been calculated using the training corpus CLEF 2001-02 A.
The following values  X  and their corresponding ratios  X  were considered: After this tuning process, a factor x  X  8  X  ratio 1 = x  X  0 : 125  X  was chosen.
 Table 4 shows the results obtained applying the balance factor when using the pairs extracted with ( qdp ), compared with respect to our baseline, the stemming of simple terms ( stm ), have obtained positive improvement appearing in boldface. Column % D ment attained. In order to highlight the improvement strictly due to the use of dependencies, column % D shows the degree of improvement of qdp with respect to lemmatization only. Each row of the results table contains one of the parameters employed to measure performance ( Baeza-
Yates &amp; Ribeiro-Neto, 1999 ): number of queries submitted, number of documents retrieved, number of rele-vant documents expected, number of relevant documents retrieved, average precision (non-interpolated) for all relevant documents (averaged over queries), R-precision, precision at 11 standard recall levels, and preci-sion at N documents retrieved.
 Results are certainly positive since there is clear improvement at all levels, although the behavior of corpus
CLEF 2003 is more irregular. Our results are also qualitatively better than those of Mittendorfer and Winiw-arter (2002) for English or those of Kraaij and Pohlmann (1998) for Dutch, since there is not only an improve-ment in precision at the first documents retrieved  X  as in their case  X , but also a general improvement with regard to global precisions.

When taking stemming ( stm ) as the baseline, the Wilcoxon test ( a  X  0 : 05) shows a significant improvement in corpus CLEF 2001 X 02 A when comparing non-interpolated precisions, while no significant improvement is shown for the other two corpora. In the case of comparing precision at the top 10 documents, significant improvement is found in the two CLEF 2001 X 02 corpora. On the other hand, when taking lemmatization ( lem ) as the baseline, the Wilcoxon test obtains the same results, except in the case of corpus CLEF 2001 X  02 B, where pairs ( qdp ) perform significantly better regarding non-interpolated precision, but not in the case of precision at the top 10 documents. 21
We also show in column qdp of Table 1 the distribution, by document frequency ( df ), of the complex terms extracted from the CLEF 2003 corpus using CASCADE . These statistics include the number and percentage of pairs in each frequency rank, together with the total number of pairs contained in the index  X  those pairs occurring in more than four documents ( df &gt; 4). Figures show that only a small proportion of the pairs is added to the index (17%) because of the high sparseness of the term space. Moreover, the number of unique pairs generated by CASCADE is 43% lower than that of the number of pairs generated by other approaches ( Vil-due to the inherent conservatism of CASCADE when dealing with noun-adjective prepositional phrase dependen-cies, allowing the attachment of a prepositional phrase with the noun on its left only for de (of) prepositions, since it is unable to accurately disambiguate the dependency structure in the case of other prepositions  X  the classical prepositional phrase attachment problem. Other parsers are more permissive, allowing the attachment of any prepositional phrase to the noun on its left, thus boosting the number of dependencies generated, even when a high percentage of them are incorrect dependencies that introduce noise into the system. 5.1.2. Results for noun phrase dependencies
In order to facilitate the comparison of our work with those classical approaches based on the use of noun phrases as index terms ( Hearst et al., 1996 ), a new set of experiments was performed, this time restricted to those dependencies corresponding to noun phrases: those between a noun head and its modifying adjectives, and those between a noun head and its modifying adjective prepositional phrases.

Although the Wilcoxon test showed no significant difference with when all dependencies are used, except for precision at the top 10 documents for CLEF 2003, the results obtained through this approach downward trend with regard to the previous one due to the loss of the information contained in dependencies involving verb phrases. The document frequency distribution of the terms generated is shown in column qnp of
Table 1 . As can be seen, the distribution remains almost the same as with the complete set of dependencies  X  column qdp  X , but the number of unique terms in the index is 48% lower, clearly reducing the size of the index.
It should be noted that although in this way the costs associated with storage and management of the index can be reduced, term generation costs remain the same, since noun and prepositional phrases are identified in layers 3 and 4, respectively, while verb groups, which are necessary for the rest of the dependencies, are pro-cessed in the previous layers. 5.2. Results using the syntactic information extracted from documents
Previous experiments showed that the use of syntactic information enables us to obtain a consistent improvement with regard to the use of simple terms only. Nevertheless, we were of the opinion that these results could be improved by applying a more sophisticated mechanism for extracting syntactic dependencies.
To this end, we shifted our focus from the syntactic information of queries to the syntactic information of documents, employing a blind feedback-based approach in which the indexing process remains the same, but the querying process is performed in three stages: (i) The lemmatized query is submitted to the system. (ii) The n top documents retrieved by this initial query are used to select the most informative dependencies (iii) The expanded query is then submitted to the system in order to obtain the final set of documents
The work of Buckley et al. (1993) also refers to the expansion of the queries with the X best phrases of the top documents retrieved (apart from the Y best simple terms); nevertheless, there are clear differences with our approach: the former takes the X best phrases, whereas our approach takes the best dependencies among the X best terms; the former does not use linguistic phrases, but rather pairs of adjacent non-stopwords; Buckley et al. X  X  approach is applied on routing tasks, whereas our proposal is applied on ad hoc retrieval tasks; the relevance criteria they are using for feedback has been set manually instead of applying blind feedback, as in our case, where we are just assuming that the top n 0 1 selected documents are much more reliable than ours when making feedback; and finally, although both approaches use SMART, the weighting scheme they employed was much simpler and had a worse baseline performance, resulting in a greater margin for improvement. 5.2.1. Results for all dependencies
Before evaluation, we first needed to tune the parameters of the model, not only the weight balance factor x , as usual, but also the expansion parameters: the number of terms t uments n 0 1 to assume as relevant. This tuning process was performed with the training corpus CLEF 2001 X  02 A for the following ranges of parameters n 0 1 and t 0 where the resulting parameters were:
The results obtained with this new approach employing syntactic information from documents ( ddp ) are shown in Table 5 . As can be seen, there is a generalized improvement, not only with regard to stemming ( stm )  X  see column % D that syntactic information from documents is more useful than that from queries when it comes to increasing the precision of the documents retrieved.
 When using stemming ( stm ) as the baseline, the Wilcoxon test shows significant improvement for corpora
CLEF 2001 X 02 A and 2003 regarding non-interpolated precision, and also for precision at the top 10 docu-ments in the case of corpus CLEF 2001 X 02 A. When query dependency pairs ( qdp ) are used as the baseline, the same results are obtained for non-interpolated precision, but this time only significant improvement in pre-cision at the top 10 documents is found for corpus CLEF 2003. 5.2.2. Results for noun phrase dependencies
The positive results obtained with the new approach led us to investigate the possibility of the existence of some kind of relation between the index terms introduced by each approach. Table 6 shows the number of complex terms dismissed when using the syntactic information from documents  X  qdp n ddp  X  , the number of new terms introduced by documents  X  ddp n qdp  X  , and the number of terms common to both approaches proportion of common pairs remains nearly constant, which may suggest the existence of some kind of under-lying relation according to which only certain kinds of pairs are useful from the point of view of automatic extraction.

Thus, we analyzed the distribution of the different types of syntactic dependencies from which each complex index term has been obtained. The results obtained are shown in Table 7 : column qdp shows the number of pairs corresponding to each dependency type which were extracted from queries in our initial approach, whereas column  X  ddp \ qdp  X  shows the number of pairs common to those extracted from the documents.
Our aim was to discover whether there was any bias or preference and, as can be seen, dependencies corre-sponding to noun phrases  X  noun X  X djective and noun X  X djective prepositional phrase  X  seem to be preferred.
Since some kind of preference would appear to exist in the case of noun phrase dependencies, we decided to study the behavior of the system when using only complex terms obtained from noun dependencies, in a sim-ilar way as in the case of the pairs obtained from queries.

The results obtained for this approach, 24 whilst still positive since they continued to outperform stemming, were not as good as those obtained when the whole set of dependencies was used ( ddp ). Moreover, the differ-ence in the figures obtained was greater than that obtained when using the syntactic information from queries.
On the other hand, no significant difference between either approach was found when comparing both non-interpolated precision and precision at top 10 documents.

Khan and Khor (2004) also used relevance feedback for selecting relevant noun phrases. However, our approach is more complete since we deal with all kinds of dependency, and also because our evaluation is per-formed on a large set of standard queries instead of on an ad hoc set, as in their case. 6. Conclusions and future work
Throughout this article we have described the application of phrase-level analysis techniques in order to obtain, on the one hand, more precise and descriptive index terms, and on the other, to manage syntactic var-iation. For this purpose we have tested an approach based on the use of syntactic dependencies as complex index terms for complementing simple index terms.

We should point out that although some related works have been done for other languages, with English to the fore, Romance languages, and Spanish in particular, have stayed in the background. Furthermore, our proposal integrates numerous features which serve to differentiate it from previous works.
This is the case, for example, of the introduction of mechanisms based on morphological families for the management of derivational morphology, which enables the extension of the processing of strict syntactic var-iation to morpho-syntactic variation. Moreover, the use of different sources of syntactic information, namely queries and documents, has been also studied, the latter proving more effective, as has the restriction of the dependencies employed to those obtained from noun phrases in order to reduce costs.

In order to minimize the computational cost of the system and at the same time to increase its robustness, syntactic dependencies are obtained through shallow parsing by means of rapid processing of large collections and their application in practical environments.

Furthermore, we have had to face one of the main problems in non-English Natural Language Processing restricting the complexity of the solutions proposed by focusing on the employment of lexical information, which is easier to obtain.

With regard to our future work, the results presented in this work represent a starting point for the further development of NLP-based approaches to Romance language IR in general, and Spanish IR in particular. However, we still need to maintain our efforts in order to reduce the gap with IR in English as far as possible.
The work presented here opens the door to the possibility of using selection restrictions ( Gamallo, Agustini, the system, particularly with respect to the prepositional phrase attachment problem, the goal being to increase system recall without damaging precision.
 This work also provides a solid basis for the application of mechanisms for dealing with semantic variation. Current approaches for managing semantic variation mainly work at word level, using WordNet ( Miller, niques are very sensitive to word-sense ambiguity, requiring the employment of high-performance word-sense disambiguation techniques ( Stevenson, 2003; Stokoe, Oakes, &amp; Tait, 2003 ). Even so, real improvements are sists in working with phrase-level semantic variation ( Jacquemin, 1999 ), which reduces the problem of word-sense ambiguity because of the existence of a context in the complex term itself. Moreover, it would be possible to employ a fuzzy notion of synonymy which measures the degree of synonymy between two terms ( Sobrino, query expansion and term weighting during the matching of synonyms, in this way opening up new possibil-ities for study.

Finally, we are currently studying the possibility of adapting the local context analysis approach proposed by Xu and Croft (1996) . This technique proved to be less sensitive to the noise introduced by non-relevant documents than blind relevance feedback, which could be useful during the querying process when using the syntactic information extracted from documents.
 Acknowledgement This research has been partially funded by Ministerio de Educacio  X  n y Ciencia and FEDER (TIN2004-07246-C03 and HUM2007-66607-C04) and Xunta de Galicia (PGIDIT07SIN005206PR, PGIDIT05PXIC-10501PN, PGIDIT05PXIC30501PN and the Rede Galega de Procesamento da Linguaxe e Recuperacio  X  n de Informacio  X  n ). References
