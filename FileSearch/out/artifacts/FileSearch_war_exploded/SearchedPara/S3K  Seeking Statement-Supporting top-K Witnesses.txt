 Traditional information retrieval techniques based on keyword search help to identify a ranked set of relevant documents, which often contains many documents in the top ranks that do not meet the user X  X  intention. By considering the semantics of the keywords and their relationships, both precision and recall can be improved. Using an ontology and mapping keywords to entities/concepts and identifying the relationship between them that the user is interested in, allows for retrieving documents that actually meet the user X  X  intention. In this paper, we present a framework that enables semantic-aware document retrieval. User queries are mapped to semantic statements based on entities and their relationships. The framework searches for documents expressing these statements in different variations, e.g., synonymous names for entities or different textual expressions for relations between them. The size of potential result sets makes ranking documents according to their relevance to the user an essential component of such a system. The ranking model proposed in this paper is based on statistical language-models and considers aspects such as the authority of a document and the confidence in the textual pattern representing the queried information.
 H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Infor-mation Search and Retrieval Algorithms, Experimentation Information Retrieval, Ranking, Language Models, Information Extraction, Statement Search, Semantic Search, RDF 01IG09008E.

Information retrieval (IR) is traditionally based on keyword queries and aims at identifying documents containing a set of keywords, which provides a simple human understandable user interface. Semantically aware techniques, as for instance used in information extraction, can be used to enhance this basic paradigm [5, 6, 26, 27]. Query expansion, for instance, may consider the semantics of the given keywords [27]. Thus, the system is looking not only for exact matches of the given keywords but also for semantically similar keywords. Other approaches use information extraction techniques as a filter to rerank retrieved results [5] or to generate keyword queries using an IE framework [26].

Still, these systems have their shortcomings. Assume, for in-stance, a user is looking for documents that verify if Barack Obama was born in Kenya using  X  X bama X  and  X  X enya X  as keywords. Although keyword search might consider techniques such as query expansion and alternative keywords for entities, documents are still considered as bag-of-words so that the system might also output documents mentioning that Obama visited Kenya on a diplomatic mission. Although the search result might be improved by adding keywords such as  X  X irthplace X  or  X  X orn X , query results still poten-tially contain many false positives and documents using alternative formulations are not found. The search can be improved by making use of the full potential that information extraction offers, i.e., by also considering the relationships between entities and the patterns expressing these relationships.

In this paper, we propose a retrieval framework that applies information extraction to transform a user query into RDF state-ments and retrieves the most relevant witness documents sup-porting them. For instance, given the phrase query  X  X arack Obama was born in Kenya X  , information extraction tools can identify the contained statement in canonic form as the RDF triple ( Barack_Obama , bornIn , Kenya ). Having applied the same extraction tool on the document corpus beforehand allows us to retrieve documents that mention the given statement. Thus, we can find documents expressing the statement in various ways us-ing different representations of the entities and different textual expressions for the relationships between them. For example, the statement ( Barack_Obama , bornIn , Kenya ) can be textually expressed in many different forms:  X  X arack Obama was born in Kenya X  ,  X  X bama X  X  birthplace is Kenya X  ,  X  X enya, the birthplace of the first African-American president,. ..  X  , etc. In analogy to the term keyword search , we refer to this concept as statement search . The user input can either be a phrase (or a set of phrases) that is mapped to RDF statements or an arbitrary set of RDF statements that is selected by the user or corresponds to the result of a SPARQL query evaluated over an RDF knowledge base.
To make this work, our framework requires some information: entity and relation dictionaries. For each entity, an entity dictio-nary contains a set of possible textual expressions denoting the entity. For instance, for the entity Barack_Obama the following expressions are associated:  X  X arack Obama X  ,  X  X bama X  ,  X  X he first African-American president X  . Each of these expressions can be weighted to reflect how likely it represents the entity. This information can easily be obtained from large knowledge bases such as YAGO [33]. A relation dictionary provides a set of textual expressions or patterns for each relation. For instance, the relation bornIn can be expressed using the following patterns:  X  X  was born in Y X  ,  X  X , the birthplace of X X  . Each such pattern can be associated with a confidence weight reflecting how well it expresses the corresponding relation. Such patterns for relations can be extracted using an information extraction tool such as [21]. After having applied such information extraction techniques to a corpus of documents, we can derive indexes that relate factual statements, entities, and their relations to documents. And we can use this information to retrieve a set of documents that satisfy the user X  X  information need.

Given a user query, there are two important steps: (i) transform the user query into a set of statements and (ii) ranking matching documents according to their relevance. In this paper, we focus on the second problem and assume that the first one can be solved using information extraction tools [21, 33]. Consider, for example, again our example statement ( Barack_Obama , bornIn , Kenya ). Documents containing this statement multiple times should be ranked higher than documents that only casually mention it. This is similar in spirit to the term frequency in traditional IR models. In addition, an appropriate ranking model also needs to consider how well the statement is expressed in the document. For example, a document containing the sentence  X  X arack Obama was born in Kenya X  should be preferred over a document stating  X  X bama spent his childhood in Kenya X  . This can be achieved by exploiting confidences associated with the patterns. Another important aspect that an appropriate ranking model needs to con-sider is the on-topicness of a document. Biographical pages about Barack Obama, for instance, are more likely to be relevant than a news article that simply mentions the claim that he was born in Kenya as a side note. Furthermore, authoritative pages such as http://www.whitehouse.gov/ should be preferred over documents such as blog entries. This is similar to authority-ranking in traditional IR.

In summary, this paper introduces the notion of statement search and proposes a framework that enables semantic-aware document retrieval by mapping user queries to semantic statements and re-trieving witness documents expressing these statements in different textual forms. We propose an appropriate ranking model based on statistical language-models that considers the following criteria: 1) statement frequency, 2) pattern confidence, 3) entity occurrences, and 4) page authority.

The rest of the paper is organized as follows: we first discuss related work in Section 2 and provide background on information extraction in Section 3. Section 4 gives an overview of statement search. While Section 5 introduces the framework implementing statement search, Section 6 presents the ranking model that the framework relies on. Finally, Section 7 presents evaluation results and Section 8 concludes the paper with an outlook to future work.
Utilizing information extraction (IE) techniques to support key-word-based document retrieval has been studied in the litera-ture [5, 15, 27]. While applying such techniques can improve the performance of a keyword-based document search engine by providing some understanding of the underlying information need of a user, these approaches usually only use additional techniques extending existing information retrieval (IR) algorithms.
Recently published approaches try to integrate ontological knowl-edge databases with document retrieval techniques. Pound et al. [26] propose a framework using descriptive structured queries with keywords as basic operators. A query is assumed to describe a set of entities by a set of constraints. After resolving the entities that satisfy the query constraints using an ontological knowledge base, the framework retrieves documents containing references to those entities. However, the retrieved documents do not necessarily contain the relations given as constraints in the query. Hence, looking for a US President born in Hawaii, President Obama would probably satisfy the statement and the retrieved documents would include documents containing Obama and a reference to Hawaii, but the documents are not necessarily talking about the claim that he was born in Hawaii. In contrast, our system also ensures the presence of the relation in retrieved documents. While the proposed framework is similar to ours to some extent, the authors focused on the query language and used an ad hoc tf-idf approach for the ranking whereas our work focuses on the ranking of result documents. Still, the proposed query language could be combined with our ranking mechanism.

Bear et al [5] couple an NLP information extraction tool with an information retrieval engine by using it as a re-ranking filter for result lists obtained from the IR engine. They report only small gains by their technique compared to the results of the IR engine alone. A similar combination is proposed by Hearst [15] for the particular domain of topic based sentiment analysis.

Similarly, Blanco, and Zaragoza [6] examine ranking of support sentences for entities. Given a keyword query, they aim at finding entities in context answering the query, i.e., support sentences that explain why an entity is an answer to the query. The paper compares a number of different ranking approaches for the support sentences. In contrast to our approach, the sentences are ranked based on entities, not on statements.

Fagin et al. [13] investigate how to interpret keyword queries, given an auxiliary database providing semantic concepts for entities and relations. Given a keyword query, their technique can produce various interpretations that (partially) match the given keywords to semantic concepts or relations. For instance, it is not clear whether the keyword query  X  X uzz price X  refers to documents containing those words explicitly, documents about the Disney affiliated economist Harrison  X  X uzz X  Price, or documents stating the price of the  X  X uzz Lightyear X  toy. These parses of a keyword query  X  and documents containing expressions of such a parse  X  can be ranked by their specificity. Using this terminology, our approach is given a specific parse (or a set of parses) as input. Instead of considering different interpretations of the query, the problem we focus on is to rank the documents containing textual manifestations of the given (set of) parse(s).

Sereno et al. [30] as well as Uren et al. [34] propose a set of tools to annotate documents, especially research documents, with ontological expressions (claims) that form a claim network that can be browsed and searched. While claims can be seen as statements, their annotating approach is semi-automatic while our approach tries to automatically annotate documents with contained statements. This kind of search analysis focuses on the claim network, while we investigate document based rankings.

Our ranking model for witness documents is based on language models. Due to their good empirical performance and sound theoretical foundation, approaches based on language models have received much attention in the IR community. Many variations of the basic language model approach have been proposed and studied. Language models have been applied to various retrieval tasks such as cross-lingual retrieval [35], expert finding [2], XML retrieval [17]. In particular, recently Elbassuoni et al. [11] proposed a language-model-based ranking for structured queries over RDF graphs. Also, Nie et al. [22] used language models to retrieve and rank entities given a keyword query. However, our retrieval model is the first to adopt language models for statement search over documents.

An overview of semantic search for textual documents is given in [20], which surveyed 22 systems, none of which providing any document ranking; the authors actually point out the necessity of research in that direction. The Semantic Desktop [29] is a semantic search engine that combines fact retrieval with semantic document retrieval using a triple-based algorithm and graph traversal. Given a natural language query, the approach tries to infer a structured query and retrieve matching triples. If there is no perfect answer, the ontology that the queries are evaluated against is used to expand the user query, and the expanded query is evaluated on a document collection to retrieve matching documents. RankIE [7] is a system for entity-based document search that can exploit structural back-ground knowledge such as ontologies. It includes the ExplainIE tool [4], which was developed to debug IE results in business intelligence applications, mainly by visualizing how entities and documents are related.
 Semantic Web search engines try to find RDF content on the Web based on keywords or URIs. To rank the sources of relevant content, they often use variations of the the PageRank algorithm [9, 14] or the tf-idf measure [23]. Semantic Web search engines focus on locating RDF sources and do not consider provenance of RDF data. An important aspect in this context is how to present results to a user, the problem of snippet generation for facts has been studied a lot [1,24].
Information Extraction (IE) is the process of automatically ex-tracting structured information such as entities, relationships be-tween entities, and properties of entities from unstructured and semi-structured sources. The process of information extraction typically is a two-fold process. First, the system analyzes docu-ments and extracts statement candidates from the text. Second, it integrates these candidates by removing contradictory statements or deriving new ones (through inference for instance).

One common approach to extract statements from text is to use a set of phrase-patterns to match the possible linguistic realizations of the abstract statements. This is usually done by combining natural language processing and statistical inference in order to identify qualifying patterns that are used to extract facts of interest.
Once statements, also referred to as facts, have been extracted, they are stored in a fact repository or a knowledge base. The most common format to store extracted facts is the W3C-endorsed RDF, which is the model adopted in the Semantic Web commu-nity. RDF knowledge bases consist of statements in the form of subject-property-object triples. For example, the information that Barack Obama was born in Hawaii can be represented formally as the RDF triple ( Barack_Obama , bornIn , Hawaii ) with subject Barack_Obama , property/relation bornIn , and object Hawaii . An RDF knowledge base can conceptually be regarded as a large graph with nodes corresponding to entities (subjects and objects) and edges denoting relationships or properties, which we refer to as an RDF graph. Search on this kind of ER/RDF data is naturally expressed by means of structured graph-pattern queries using query languages such as the W3C-endorsed query language SPARQL [31]. A result to a query expressed in such a way is a subgraph of the underlying knowledge graph that consists of a set of triples matching the query graph.

In this paper, we assume that our framework is working with named (canonic) relations originating from information extraction tools working with predefined canonic relations [21, 33], i.e., the system is configured to look for patterns for a set of given relations, e.g., bornIn . We can easily extend the framework to work with anonymous relations defined by clusters of similar patterns. For a cluster the confidence of each pattern can be defined by the distance to the cluster center. Some systems [3, 12] extract  X  X nteresting X  patterns from a document corpus without naming them and try to cluster semantically similar patterns. Each obtained cluster represents an anonymous relation.
As explained in the introduction, statement search aims at re-trieving a set of relevant documents to a set of given statements. This type of search can be motivated by two slightly different information needs. First, a user might want to verify whether a certain statement is true. For instance, a user might wonder whether Barack Obama was born in Kenya and seek documents confirming this statement. Second, a user might want to investigate a certain statement, i.e., learn more details about it. For example, the user might know that Osama Bin Laden died on May 2011 but wants to find more details about his death. In both cases, the user is interested in documents referencing the statement in some way. However, with respect to these two user goals, a document is relevant to a statement query if it either verifies the statement (persuasiveness) or provides further information about the statement topic (on-topicness) . These two main properties might compete with each other in determining the relevance of a given witness document. There might be documents that very clearly and convincingly support the statement ( persuasive ) but only casually mention the statement and are mainly concerned with another topic (not on-topic ). For instance, the Wikipedia article about the Kapiolani Medical Center in Honolulu might clearly state that Obama was born there but then would provide no further information about Obama X  X  childhood. A blog post discussing Obama X  X  childhood on the other hand could provide more information about the president X  X  youth but might be a less convincing source for the verification of his birthplace. The quality of a witness for a given query therefore depends on the user X  X  preference on these two aspects. Our ranking model tries to incorporate different features estimating a document X  X  on-topicness and its persuasiveness in a way that allows to adapt the ranking according to a user X  X  preferences. Still, a perfect witness would satisfy both properties to the full, being informative on the topic and persuasive for the statement(s) in the query. Thus, at the core, our ranking model aims at finding the best documents based on the probability that they are a perfect witness for a given set of statements. In our model, the quality of a witness is therefore based on the following aspects: 1. Given a set of statements g , a witness can only be perfect if 2. A document is only informative to a user if she can learn 3. Statements can be expressed in various textual forms. Some 4. In general, we trust information obtained from some doc-Our ranking model, introduced in detail in Section 6, tries to take all these considerations into account. Before we dwell into the details of our ranking model and how it achieves the aforementioned goals, we describe our system framework in the next section.
As explained in the introduction, our system employs an infor-mation extraction tool to implement statement search . That is, it transforms a user query into RDF statements and retrieves the most relevant witness documents supporting them. Figure 1 gives an overview of our system. In a nutshell, our system works as follows: 1. The document corpus is processed by an information ex-2. Given a user query, our system utilizes a translation com-3. The system retrieves the documents that match these state-In the remainder of this section, we provide more details on the most important components of our framework.
The framework uses two types of dictionaries: an entity dic-tionary and a relation dictionary. With the help of an IE system, we can use these dictionaries to identify statements expressed in documents and to understand user queries and translate them into triple-based statement queries.
 Unique entities usually have several non-canonical names (Defini-tion 5.1). For instance, the unique entity Barack_Obama can be represented by the names  X  X bama X  ,  X  X arack Hussein Obama X  ,  X  X he US President X  , etc. The relationship between entities and their names is an m-to-n relation, i.e., every entity can have many names and every name can refer to different entities. The same entity name could even refer to different entities within the same document. For instance, in a biography about President Obama, the entity name  X  X bama X  might refer to President Barack_Obama or to his father Barack_Obama_Sr . The problem of identifying the right entity for a given name is known as entity disambiguation [19] . However, at this point we assume: 1. Each entity name is uniquely addressing exactly one entity 2. The disambiguation is solved externally and a mapping from tity name  X  s that occurs in a document d , the entity it refers to within d is given by: entity (  X  s ,d).

The framework makes use of the entity dictionary in three respects: first, when translating user queries into triple form (Sec-tion 5.5), second, when matching statements back to their textual occurrences in the document (Section 5.1), and third, when creating the entity occurrence index (Section 5.3).
 Similar to the case of entities, a relation can be expressed using various patterns . A pattern p is defined as a recurring textual representation of an abstract relation r . For instance, the textual phrase  X  X as born in X  could correspond to the pattern  X  X  was born in Y X  indicating the bornIn relation on the subject entity X and the object entity Y. While we assume that the patterns we deal with represent binary relations between two arguments, we make no assumptions on the way a pattern is defined. A pattern could, for example, be represented by a textual phrase, an n-gram, or a regular expression. We refer to the occurrence p (  X  s,  X  o ) of a pattern p with two entity names  X  s and  X  o as a pattern instance . If it occurs in document d , we write p (  X  s,  X  o )  X  d . For example, given the pattern above, the phrase  X  X bama was born in Kenya X  would be a pattern instance with X representing the entity name  X  X bama X  and Y being instantiated with  X  X enya X  .

Any pattern can indicate multiple relations with varying cer-tainty. For instance, the phrase  X  X bama X  X  home country Kenya X  could refer to the fact that Barack Obama has family roots in Kenya ( Barack_Obama , hasAncestorsFrom , Kenya ) or it could indicate that he was born ( bornIn ) in Kenya. Thus, for any relation r that can be expressed by pattern p a confidence value conf ( p, r ) (Definition 5.2) representing the likelihood that p expresses r is given.

D EFINITION 5.2 (P ATTERN C ONFIDENCE ). Assume two en-tity names  X  s and  X  o , a pattern p , and a relation r are given. The confidence conf ( p, r ) represents the probability that given that a pattern instance p (  X  s,  X  o ) appears in a document d , the author
Given a query statement (RDF triple), indicative pattern in-stances can easily be identified by matching the entities and the triple relation to patterns that indicate it. Formally, if D is the document corpus, all indications of a triple statement ( s , r , o ) are s  X  entity (  X  o, d ) = o  X  p (  X  s,  X  o )  X  d } .

For instance, given the statement triple t = ( Barack_Obama , bornIn , Kenya ) , a document where  X  X enya X  is mapped to Kenya ,  X  X bama X  is mapped to Barack_Obama , and given that indicates (  X  X  was born in Y X , bornIn ) holds, then the pattern instance  X  X bama was born in Kenya X  is an indicative pattern instance for t .
Based on the above introduced formalisms, we can now formally define the notion of a witness (document) for a (set of) statement(s).
Whenever a pattern instance p (  X  s,  X  o ) of a pattern indicating a relation r occurs in a document d , d is called a witness for the statement r ( s, o ) with s = entity (  X  s, d ) and o = entity (  X  o, d ) . Thus, given a set of statements g = { t 1 , ...t n } , where t ( s , r i , o i ) , then d is a witness for g if there is at least one t that a pair  X  s ,  X  o exists such that: 1. s i =entity (  X  s ,d), 2. o i =entity (  X  o ,d), 3. indicates ( p, r i ) , and 4. p (  X  s,  X  o ) occurs in d .
 Any witness for g is a candidate considered in our ranking al-gorithm described in Section 6. In order to efficiently identify documents in which pattern instances matching a given statement occur, we make use of a set of indices (Section 5.3).
We have two types of indices in our system. The first one deals with relation patterns and utilizes both the entity and relation dictionaries. For each document, it stores the number of times a certain pattern is instantiated with certain arguments in a docu-ment. For example, given the statement ( Barack_Obama , bornIn , Kenya ), and the text snippet  X  X bama was born in Kenya X  , the index would store that pattern  X  X  was born in Y X  appears with  X  X bama X  and  X  X enya X  . In fact, as we assume entity names are disambiguated, the index directly stores the disambiguated entities Barack_Obama and Kenya . Hence, to identify witness candidates given a statement triple, it suffices to match the pattern instances of all patterns indicating the relation against the arguments of the statement.

The second index deals with entities and stores for each docu-ment d the number of times an entity is mentioned in d . Since entities do not directly appear inside the documents but are referred to using different natural language names, this index utilizes the entity dictionary in order to look up the entity corresponding to an entity name (see Definition 5.1).
The information extraction tool is responsible for processing the documents in our corpus and providing the dictionaries. It is also responsible for generating the necessary indices that our retrieval engine operates on to retrieve and rank the witnesses. Generally speaking, we can use any information extraction tool, e.g., [21], if it provides us with some sort of binary patterns, preferably associated with confidence values, to extract entity occurrences. Since information extraction is not the focus of this paper, we omit the details of the information extraction tool and refer the user to related work [21]. The output of the IE tool that our system uses consists of the dictionaries and indices.
The query translator is responsible for processing a given user query and transforming it into a set of RDF statements that our system can process. In order to do so, it utilizes the dictionaries and the IE tool to perform the translation of the query into a set of RDF statements using the same vocabulary as our dictionaries and indices. Assume, for instance, a user wants to verify that Obama was born in Kenya, she might provide the phrase query  X  X bama was really born in Kenya? X  . It would then be translated into the statement triple ( Barack_Obama , bornIn , Kenya ) and the system would output witnesses containing the statement.
 Alternatively, queries might also be given directly as a set of RDF triples, such that our system could, for instance, be used as a verification tool for automatically extracted ontologies. Assume, for instance, a movie knowledge base is generated by an informa-tion extraction tool. Then, a user might consider it to be a mistake if she sees four different statements claiming that the role of the character Tony in the movie  X  X he Imaginarium of Dr. Parnassus X  was played by four different actors. Using our framework, she could issue the set of all four statements to see which of them is more likely to be true. This might lead her to a document containing expressions of all four statements, which explains that three actors replaced Heath Ledger as he died in the midst of the production.
The witness retrieval engine is responsible for retrieving any document that matches the given query (i.e. at least one statement of the query is indicated in the document) and rank this set of witnesses according to the ranking model described in Section 6. It utilizes the entity and relation dictionaries to identify the corresponding possible entity names and relation patterns for the given statements. It uses this information, along with the indices in order to rank the matching witnesses according to the ranking criteria described in Section 4.

Once witnesses are ranked, they are passed to the witness dis-player which is responsible for displaying the ranked witnesses to the user. The witness displayer can utilize a variety of techniques to display the witnesses to the user in response to its original query. For instance, the identified statements corresponding to the user query can be highlighted in the returned witnesses, as well as any other automatically extracted information that our IE tool managed to extract from the witnesses. However, this is not the focus of this work, and we thus omit further details about the witness displayer.
As explained in the previous section, our ranking model is concerned with ranking a set of witnesses that are relevant to a given set of RDF statements. Note that we decouple the problem of transforming the user query into RDF statements from the ranking of witnesses problem since they are basically independent problems. W e thus assume that the transformation of queries into RDF statements is deterministic. Our ranking model can easily be extended to handle the case where this transformation process involves inaccuracies. However, in this paper we focus on the problem of ranking the witnesses given a set of RDF statements.
Our ranking model is based on statistical language models (LMs) [16,25]. The witness documents are ranked based on the probabil-ity of being relevant to the query statements { t 1 , t 2 Bayes X  rule, we have: Since P ( t 1 , t 2 , ..., t n ) does not depend on the witnesses, we can ignore it during the ranking. This implies that P ( d ) is the prior probability that witness d is relevant to any statement. This probability can be estimated in various ways, and in our case we estimate it using the static authority of the page or pagerank [8]. We do this in order to take into consideration the trustworthiness of the witnesses (i.e., give higher weight to witnesses that are authoritative).

As for the probability P ( t 1 , t 2 , ..., t n | d ) , we assume indepen-dence between the query statements for computational tractability (in-line with most traditional keyword ranking models). Thus, To avoid overfitting and to ensure that the witnesses that do not contain all the query statements do not have a zero probability, we use smoothing:
P ( t 1 , t 2 , ..., t n | d ) = The first component is the the probability of the statement t being generated by document d and the second component is the probability of generating the statement t i by the whole collection Col .
 We rely on two different methods to estimate the probability P ( t i | X ) of generating statement t i using X , where X  X  { d, Col } : forms in witnesses, we need to first fold a statement into all corre-sponding indicative patterns instances that can be used to represent the statement in order to estimate the probability P ( t i is similar to translation models [36], when the query is expressed in one language, and the documents retrieved are in a different pattern instances. The probability P ( t i | X ) is then computed as follows: The first component P ( t i | y j ) in Equation 5 is the probability of representing the statement t i using pattern instance y is computed as a function of the confidence of representing the relation of t i using the pattern of y j , as defined in Definition 5.2. Assuming that the relation of t i is r i and the pattern of y have: P ( t i | y j ) = f ( conf ( p j , r i )) . Note that P ( t zero if r i cannot be expressed using pattern p j .

The second component P ( y j | X ) in Equation 5 is the probability of generating pattern instance y j given X where X  X  { d, Col } . This is estimated using a maximum-likelihood estimator as follows: Here, c ( y ; X ) denotes ho w often the pattern instance y occurs in X .
 into consideration the occurrence of the statements in the witnesses. It does not take into consideration what other statements occur in the witness and whether or not they are related to the query statements. For instance, assume the user query consisted of the statement t = (Barack_Obama, bornIn, Kenya) . Also assume that witnesses d 1 and d 2 both contain x statements, and that the query statement appears in each witness only once. Thus, P ( t | d P ( t | d 2 ) . However, assuming that d 2 contains more statements about Obama than d 1 which is a news directory for instance, we would like to rank d 2 higher than d 1 . To this end, let the statement t be of the form: (s, r, o) with subject s , predicate r and object o . The probability P ( t i | d ) is now defined as follows:
P ( t i | d ) =  X  s P e ( s | d ) +  X  o P e ( o | d ) + (1  X   X  subject s and object o using witness d respectively, and P the probability of generating the whole statement t using d . The parameters  X  s and  X  o control the influence of each component.
The probability P t ( t i | d ) is estimated using the unbiased estima-tor from Equation 5. The probabilities P e ( s | d ) and P turn estimated using the frequency of occurrence of s and o in d, respectively. For instance, P e ( s | d ) is computed as follows: where c ( e ; d ) is the number of times an entity e occurs in docu-ment d .
In this section, we discuss a three-fold evaluation of our ranking model. First, we outline the setup of our evaluation environment (Section 7.1). Then, we discuss what effect the parameters in gen-eral have (Section 7.2). Afterwards, we analyze the performance of our ranking model (given the set of witnesses is already deter-mined) based on some concrete settings tailored towards different use cases. We use a naive arbitrary ranking and a keyword-based entity-oriented ranking for comparison (Section 7.3). Finally, our retrieval approach is compared to a purely keyword-based retrieval approach, that is not aware of any statement indication found by the extraction engine (Section 7.4). of the ClueWeb 09 document corpus 1 , a standard IR benchmark collection containing about 500 million English language docu-ments harvested from the Web. As running our extraction system, SOFIE/PROSPERA [21,33], on the whole ClueWeb corpus would have been too expensive with our current setup, we constructed a document topic subset by selecting 43 well-known entities, i.e., politicians (e.g., Barack Obama), actors/directors (e.g., Clint Eastwood), soccer players and clubs (e.g., David Beckham) and scientists (e.g., Albert Einstein), to retrieve the top-1000 documents based on a BM25-based score [28] for different names and spelling variants of these entities taken from the general purpose knowledge base YAGO [32]. This resulted in a pool of 182,139 documents, on http://boston.lti.cs.cmu.edu/Data/clueweb09/ which a modified version of SOFIE/PROSPERA was run that keeps track of fact provenance information. As SOFIE/PROSPERA iteratively learns new patterns, the extraction process was repeated several times.
 For instance, one query with a single statement would be the set { ( Barack_Obama , isLeaderOf , USA ) } , while one with mul-tiple statements focussing on JFK X  X  assassination would be given by { ( JFK , diedOn , 1963-11-22 ) , ( JFK , diedIn , Dallas ) } . We only considered statements that were supported by at least 10 documents (usually more). For each query, a set of potentially relevant documents was generated by pooling the top-50 rankings using various settings of our ranking mechanism. Remember that this includes a filtering step such that only documents containing an indication for at least one statement of the query were considered for the ranking. This resulted in witness pools of 80 witnesses per query on average with a minimum of 11 documents and a maximum of 435 documents for a single query. For the system comparison (see Section 7.4), additional documents retrieved by the (statement indication unaware) runs based on keyword search were added to each pool.
 at two different use cases. First, a user might aim to learn more about specific statements; second, she might be interested in verify-ing the validity of some statements. Reflecting these two use-cases our system aims to support, all documents in the pool were assessed 1) as a whole document on their on-topicness (does it contain more information relevant to the query than the actual statement indication) and 2) separately for each statement on their persua-siveness (how strong is its support for this statement). For both assessment dimensions a graded relevance scale with three grades ( non-relevant, somewhat relevant, highly relevant ) was used. For binary measures such as MAP we project the graded assessments to a binary relevance scale, such that a statement or document is relevant in the binary scale if it is highly relevant in the graded scale. For the parameter and ranking (of prefiltered witnesses), evaluation we will mostly assume that documents can only be judged as being relevant for a statement on the persuasiveness scale if the extraction system recognized an appropriate indication of the statement in that document. From a usability point of view this makes sense since a user will usually skip a result for which the system cannot highlight a statement indication. However, if the user considers the document, she may be able to identify support for statements not recognized by the extraction engine. In a more aggressive assessment, we have also considered a document as relevant with regard to persuasiveness if the assessor could find the statement, but the system could not. Note that this only affects multi-statement queries since we require at least one indication of a statement to add a document to the pool. We denote the evaluation ignoring indications not recognized by the extraction system as machine-ignorant and the one that is more thorough as human-aware . For the system evaluation (Section 7.4), where we compare against purely keyword search approaches without any prior knowledge about statement indications, we solely use the human-aware method.
 loosely correspond to our two evaluation aspects. First, we com-pute the mean average precision (MAP) and the normalized dis-counted cumulative gain (nDCG, [18]) for the top 5,10, and 20 results, averaged over all queries. Second, we compute the mean rank (mr), i.e., the first rank at which at least one convincing (highly relevant) indication has been seen for each statement in the query on average, and related to this the mean reciprocal rank (mrr).
Our model provides several possibilities to adjust it to a user X  X  needs. The entity bias can be controlled by the  X  s ,  X  o Equation 7). Similarly the influence of the smoothing versus the main ranking function can be influenced by setting the  X  parameter (Equation 4). Additionally our implementation allows to switch influence of document trustworthiness (i.e., pagerank) on and off. Similarly, the influence of the pattern confidence for computing P ( t i | X ) in Equation 5 can be set to no influence, linear influence or quadratic influence.

We have investigated the effects of different settings of these parameters. The pagerank values, which we use as an approxima-tion of a document X  X  probability to contain accurate and extensive information, do not seem to have a significant impact. This might have several reasons: 1) Our query set did not contain many disputed statements where we saw  X  X ntrustworthy X  witnesses. 2) SPAM pages were already sorted out using the Waterloo Spam rankings 2 . 3) We use pagerank only as a page trust approximation. Pagerank might not always be a good indicator that the information provided is of high quality, in the end we aim at having user feedback provide the trust values. The other parameters, however, provide the expected effects. For instance, increasing the  X  values tends to improve results on the on-topicness aspect, but might decrease the precision for the persuasiveness (see Table 1), while a higher confidence influence tends to have the inverse effect. However, those parameters are quite interdependent, so that there is no definite behavior that applies to each setting, e.g., increasing one  X  value means decreasing either the pattern confidence influence or the other  X  parameter. If one of the  X  weights or both together reach 1 they completely dominate the ranking formula which might provoke a stronger negative (or sometimes positive) effect depending on the other settings. Similarly, the initial step from both  X  values being set to 0 towards having at least one of them at 0.1 has a much larger impact as similar increases on a setting where  X  s +  X  o &gt; 0 already holds. Table 1 shows one of the more extreme cases clearly outlining the influence of the  X  s parameter on the ranking performance. For changes of the  X  value we could not find a noteworthy overall impact on average. However, we did not look into extreme values (e.g.,  X  = 1 or 0). Still, a high value of  X  can lead to overfitting as the smoothing looses influence.
In this section, we analyze the performance of our ranking method, given a set of documents identified as witnesses for (part of) the query, in terms of on-topicness and persuasiveness. We compare the proposed ranking method to a naive ranking ( naive ), which ranks the witness set randomly, as well as to a keyword-based entity-oriented ranking method ( lucene ) implemented with Apache Lucene 3 using its default parameter setting. Both alter-native ranking methods were applied on the prefiltered witness pool generated for each query , as we only evaluate the ranking of witnesses, not the identification of potential witnesses.
For the Lucene-based rankings, we generated keyword queries based on the entities contained in the statement(s) of the query. The relations are therefore not represented in the Lucene keyword query translations. As keyword engines are specialized in finding documents for a certain topic given by keywords, we expected that this method will have good results in the on-topic evaluation. http://durum0.uw aterloo.ca/clue web09spam/ http://lucene.apache.org/ We were mainly interested in a on-topicness-oriented baseline and whether using the knowledge about relations in the ranking would have a noticeable impact in terms of persuasiveness.

For our own system, we make use of three main example con-figurations of our ranking model representing different use cases (for details see Table 2). One setting ( persuade ) aims at high per-suasiveness by ignoring any entity occurrences, but rather relying on the pattern confidence values. Another one ( topic ) mimics an entity search, aiming at on-topicness by focusing totally on the entity occurrences (  X  s =  X  o = 0.5, thus in a maximal combination) ignoring any other influence like page rank or pattern confidence. A third configuration tries to achieve a good balanced mix of both goals by applying some entity bias while still considering pattern confidences and applying pattern frequency smoothing ( mix ). 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 Figure 2: Comparison of on topic perf ormance based on machine-ignorant nDCG for all queries
Additionally, settings that mainly rely on pattern confidence ( cf ) and respectively pagerank ( pr ) are investigated as well as a setting without any entity bias and with linear confidence influence ( old ) introduced in an earlier demo paper [10].

Fig. 2 shows that lucene clearly meets the expectation of per-forming well in the on-topic evaluation in terms of nDCG, and only topic , our parameter setting optimized for achieving high on-topicness, can compete with it. Note, however, that the mixed configuration mix is not too far behind (  X  7%), while all other configurations are at the same level as the naive approach.
The picture changes when we look at persuasiveness of statement indications. Fig. 5(a) shows the mean rank for single-statement queries. Here, the statement-oriented rankings ( persuade , old ) are notably better than the entity-oriented approaches: using persuade a user would on average have to look only at the first result to be convinced that the statement she is interested in holds, while with 0 . 7 0 . 8 0 . 9 1 . 0 Figure 3: Comparison of persuasiveness performance based on machine-ignorant nDCG for all queries 0 . 6 0 . 7 0 . 8 0 . 9 Figure 4: Comparison of persuasiveness performance based on machine-ignorant MAP for all queries lucene she would on average need to look at a second document. Note that topic performs similar to lucene. The better performance of statement-oriented approaches is reinforced by the mrr values in Fig. 5(b), where persuade dominates while pagerank-only ranking is surprisingly good. The reason for the latter might be that those documents with a large pagerank in our corpus are often from Wikipedia. So for most single-statement queries the Wikipedia page of the main entity will be in the top results and would usually contain what we are looking for. The good performance of pr breaks down on multiple-statement queries or when we look at the nDCG values in Fig. 3, 6 and 7, as a single good document is not enough for a high nDCG value. For nDCG (Fig. 6) and MAP (Fig. 4), persuade is notably ahead of the other approaches.
The advantage of using relation information on persuasiveness becomes even clearer when we look at multi-statement queries (Fig. 5(a), 5(b) and 7). Note that the difference between persuade , cf and old and the topic-oriented approaches lucene,topic increases further. You may also note that cf , which sometimes outperforms persuade falls behind on multi-statement queries (Fig. 7). This is probably due to the higher  X  value, which may lead to over-fitting. Also, pagerank cannot help as a counterweight. It might also be surprising that the naive approach has obviously quite a convincingly small mr value, however, the nDCG is about half that of the better pattern aware settings.

So far we have only considered results from the machine-ignorant evaluation. However, results from a human-aware eval-uation recognizing also statement indications not found by the extraction system as support for the persuasiveness judgements are, in general, not too different as can be seen in Fig. 5(c). While the advantage of persuade and mix over lucene stays similar for single fact queries, lucene can gain on multi-statement queries.
Finally, in Fig. 8 we investigate a combined relevance measure that only counts documents as being relevant if they are highly 0 1 . 0 2 . 0 3 . 0 4 . 0 5 . 0 (a) Comparison based on machine-ignor ant mr for single-and multi-statement queries 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 Figure 6: Comparison of persuasiveness performance based on machine-ignorant nDCG for single-statement queries 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 Figure 7: Comparison of persuasiveness performance based on machine-ignorant nDCG for multi-statement queries 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 Figure 8: Comparison based on nDCG for multi-statement queries in a combined machine-ignorant evaluation setup relevant on exactly one aspect and as being highly relevant if they are highly relevant on both aspects. For instance, a document being relevant in the on-topicness aspect alone is non-relevant, a document being highly relevant in the on-topicness aspect alone is relevant and a document being highly relevant in the on-topicness and persuasiveness aspect is highly relevant. Here we can see that the statement-oriented and mixed settings perform quite well with mix being more adept for the long tail while the topic-oriented approach falls behind.

After all, by relying mainly on entity occurrences ( topic ), we can mimic the performance of a state-of-the-art keyword-based ranking system, and this gives good results in terms of on-topicness, while bringing in the relations ( persuade ) will increase the persuasiveness decreasing the on-topicness, but also a balancing is possible with our framework ( mix ). In the next step, we investigate how useful our framework for the overall retrieval task is compared to a keyword query system.
The general use case discussed in this paper is end-user state-ment search, so we also compare against a keyword search retrieval system without any prior knowledge about contained statements. Therefore, we ran Lucene (again with default settings) on the initial set of 182,139 documents retrieved from ClueWeb with three variations of keyword queries. First, each of our queries was translated into a keyword query based only on the involved entities (fluc:ee) . Second, each query was fully translated with the relations being translated by a manual mapping, e.g., translating the relation wasBornIn to  X  X as born in X  (fluc:ere) . Finally, each query was translated fully again but the relations were translated based on the best (highest confidence) pattern known to the extraction system for that particular relation (fluc:epe) . For each of these variations, we gathered the top-20 documents and assessed them. The results of the evaluation are shown in Fig. 9. As expected, the keyword-based search achieves good results for on-topicness that are competitive to our approach, albeit by comparing lucene with fluc:ee one can see that the indication-based filtering improves results notably. This becomes even more evident with respect to persuasiveness. This can be seen by comparing the keyword-based approaches to the naive ranking (with prefiltering) or to the prefilter-supported Lucene run (lucene) . Our persuasiveness tailored approach stands out prominently: similarly clear as the three measures shown in Fig. 9 are the mr values: using a keyword-based approach, a user would need to look on average at  X  4.5 results (fluc:ee) ,  X  3.8 results (fluc:ere) , and  X  3.7 results (fluc:epe) compared to  X  2.9 results with the prefilter supported lucene or  X  1.5 results with the persuade setting. Note that in general the effectiveness of our pattern index based ranking strongly depends on the quality of the extraction system. A keyword search approach has the general advantage that it is independent of the extraction system but vice versa does not gain from the extraction system X  X  knowledge about statement indications.
In this paper, we introduced the notion of statement search and presented a framework implementing it. Given a phrase query, the proposed framework maps it to a factual statement (or a set of such statements) and retrieves documents containing textual expressions supporting the queried statement(s). We proposed a configurable ranking model based on language models for ranking witnesses, which can be tuned for favoring either documents with strong statement support or further information related to the given statement(s). Our evaluation results show that our ranking model outperforms term-based document ranking in finding strongly sup-portive documents.

There are a number of possible directions for future work. An important extension of the ranking model would be to automati-cally derive optimal tuning parameters based on user preferences. Another aspect in this context is diversification, i.e., given multiple statements and a set of documents that only contain subsets of the statements, adapt the ranking of the results in a way that documents in the top ranks contain different subsets of the statements. In addition, considering user feedback might help to improve result ranking as well as the precision of the underlying information extraction process, e.g., by exploiting direct feedback on fact quality and counting user clicks to measure the importance of a witness document. [1] X. Bai, R. Delbru, and G. Tummarello. RDF snippets for [2] K. Balog, L. Azzopardi, and M. de Rijke. A language [3] M. Banko and O. Etzioni. The tradeoffs between open and [4] W. M. Barczynski, F. Brauer, and A. Mocan. ExplainIE -[5] J. Bear, D. J. Israel, J. Petit, and D. L. Martin. Using [6] R. Blanco and H. Zaragoza. Finding support sentences for [7] F. Brauer, W. M. Barczynski, G. Hackenbroich, [8] S. Brin and L. Page. The anatomy of a large-scale [9] L. Ding, T. Finin, A. Joshi, R. Pan, R. S. Cost, Y. Peng, [10] S. Elbassuoni, K. Hose, S. Metzger, and R. Schenkel. [11] S. Elbassuoni, M. Ramanath, R. Schenkel, M. Sydow, and [12] O. Etzioni, M. Banko, S. Soderland, and D. S. Weld. Open [13] R. Fagin, B. Kimelfeld, Y. Li, S. Raghavan, and [14] A. Harth, A. Hogan, R. Delbru, J. Umbrich, S. O X  X iain, and [15] M. Hearst. Direction-based text interpretation as an [16] D. Hiemstra. A probabilistic justification for using tf x idf [17] D. Hiemstra. Statistical language models for intelligent xml [18] K. J X rvelin and J. Kek X l X inen. Cumulated gain-based [19] S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti. [20] C. Mangold. A survey and classification of semantic search [21] N. Nakashole, M. Theobald, and G. Weikum. Scalable [22] Z. Nie, Y. Ma, S. Shi, J.-R. Wen, and W.-Y. Ma. Web object [23] E. Oren, R. Delbru, M. Catasta, R. Cyganiak, H. Stenzhorn, [24] T. Penin, H. Wang, T. Tran, and Y. Yu. Snippet Generation [25] J. M. Ponte and W. B. Croft. A language modeling approach [26] J. Pound, I. F. Ilyas, and G. Weddell. Expressive and flexible [27] Y. Qiu and H.-P. Frei. Concept based query expansion. In [28] S. E. Robertson and H. Zaragoza. The probabilistic relevance [29] K. Schumacher, M. Sintek, and L. Sauermann. Combining [30] B. Sereno, S. B. Shum, and E. Motta. Claimspotter: an [31] W3c: Sparql query language for rdf. [32] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A Core [33] F. M. Suchanek, M. Sozio, and G. Weikum. SOFIE: A [34] V. S. Uren, S. B. Shum, M. Bachler, and G. Li. Sensemaking [35] J. Xu, R. M. Weischedel, and C. Nguyen. Evaluating a [36] C. Zhai. Statistical language models for information retrieval
