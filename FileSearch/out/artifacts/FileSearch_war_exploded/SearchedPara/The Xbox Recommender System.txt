 A recent addition to Microsoft X  X  Xbox Live Marketplace is a recommender system which allows users to explore both movies and games in a personalized context. The system largely relies on implicit feedback, and runs on a large scale, serving tens of millions of daily users. We describe the system design, and review the core recommendation algo-rithm.
 H.2.8 [ Database Applications ]: Data Mining Algorithms Recommender Systems, Collaborative Filtering, Xbox
The distribution of items bought in a marketplace typ-ically follow a power-law distribution, with popular items exponentially overwhelming less popular ones. As a result, many items in the tail of the distribution, which would have been suited to a particular user, are never exposed. There are a number of reasons for tailoring a user X  X  experience in any marketplace, the most compelling of which are the need to increase user engagement, and the need to give exposure to tail items that would, under normal circumstances, be drowned by a few popular ones.

These can be addressed by a system that models and learns from user engagement. Recommender systems have been particularly effective at this task, to which the systems of MSN News, Amazon, Netflix, and the likes of others can testify. Each of these face unique challenges that are posed by the size and nature of the catalog, type of user feedback, rate of catalog change, scalability, and expected real-time system performance.
Our dataset comprise of usage information from Xbox con-soles. For every user we collect the interaction time with ev-ery item on our catalog (either a game or a movie). We have tens of millions of active users, almost all of which are play-ing Xbox games. Some of our users also enjoy our movies catalog and watch movies purchased using their Xbox con-soles. We provide personalized recommendations for both domains using two identical recommendation engines as de-scribed in this paper.

The number of Xbox users increase on a daily basis. Simi-larly, we regularly update our games and movies catalog with the latest products available on the market. With our very large user base, we hardly encounter a  X  X old start X  problem in the items domain, because within a few hours we already have sufficient information on every new item. Similarly, Xbox users generally purchase some games together with their consoles, so we rarely encounter  X  X old users X  in the games domain. Only in the movies domain, we encounter the cold start problem for users that do not watch movies on their consoles. We still hope to address this problem with cross domain learning, however this is out of the scope of this paper. In this paper, we deal only with users that have at least one item in their usage history.

Unlike the well-studied Netflix prize problem, where the user X  X  movie preferences are explicitly given by a five-star rating scale, our data largely comprises of implicit signals, like that of purchasing a game or watching a movie on the Xbox console. The absence of a negative signal poses a chal-lenge to conventional solutions based on the factorization of a  X  X atings X  matrix into two low rank matrices with one rep-resenting latent user features, and the other representing latent item features. We therefore augmented our implicit dataset with randomly generated negative signals to prevent trivial solutions as explained in Section 2.

In this paper we wish to bring a complete overview of a real world large scale recommender system. We draw a distinction between the tasks of modeling (how hidden pa-rameters are assumed to generate observed data), inference (finding the hidden parameters given the data), and utility (using the model X  X  predictions in order to optimize a reward function). Section 2 is devoted to an overview of the sys-tem X  X  architecture and some preprocessing steps to prepare the data. Section 3 explains the modeling and parameter inference, which happens offline , and gives an example of an online utility function. Finally, in Section 4 we evaluate the model and summarize in Section 5.

Figure 1: Xbox recommender system architecture
The architecture of the Xbox recommender system con-sists of an offline module and an online module (see fig-ure 1). Feedback from the the Xbox consoles ( X  X elemetry X ) is constantly sent back into the the offline module. In the offline storage we store information such as users X  play time and movies watched by users. This information is then pro-cessed into an ownership dataset to be used by the offline modeling component for learning the model parameters.
In general, the offline modeling component treats the own-ership information as an indication that the user liked the item. When processing the ownership information we em-ploy some filters to remove games that were almost never played or movies that were not fully watched, but these are rare events that hardly effect the final recommenda-tions. The ownership dataset comprises of implicitly gen-erated  X  X ositive X  only signals. Furthermore, it is hard to define credible ordering on those signals (e.g., the user liked a better than b ), thus ordering algorithms such as [4, 3] are not useful. We therefore resolve to randomly generating negative signals as we describe below.

On average, an Xbox user has about 18 games and watched about 7 movies. Therefore, we can randomly pick a small number of items from our large catalog and assume that the reason the user do not own them is because she doesn X  X  like them. For every user we pick the same number of  X  X egative X  items as the number of items she liked ( X  X ositive X  items). By doing so, we cancel out the user bias as explained in Section 3. We draw the  X  X egative X  items with probabilities proportional to their popularity (training-set frequencies). This approach was highly popular in many of the solutions submitted to the second track in the KDD-Cup X 11 [2] com-petition. Sampling according to popularity penalize popular items, which based on subjective internal evaluations found to increase user satisfaction.

The binary like -dislike matrix is then passed on to the offline modeling . A detailed description of the inference al-gorithm implemented in the offline modeling component is given in in 3.1.

Every few hours, the learned parameters are uploaded from the offline modeling component into the model pa-rameters database. The run-time recommender in the on-line module is a real-time service that utilize the learned parameters in order to generate user specific recommenda-tions. Upon a console-query for personalized recommenda-tions, the run-time recommender chooses items according to the utility algorithm described in Section 3.2. These recom-mendations are then sent back to the console and presented to the user under the title  X  X icks For Me X .
The interaction between users and items lends itself to bilinear models. In them, each user m is represented by u m  X  R K , each item n is represented by a similar vector v , and the magnitude of their inner product u T m v notes the user X  X  affinity to the item (personalization). The parameters U = { u m } M m =1 and V = { v n } N n =1 are unob-served, and should be inferred from data. This core set-up has been widely used in the Netflix prize competition, and we X  X e adopted the Matchbox library of [5] for parameter in-ference.

The bilinear model has the property that items which of-ten co-occur for a given user, have similar v -vectors. Figure 2 shows V embedded in R 2 for a number of games. In it, the similarity between vectors of sports games, for example, is clearly visible. Figure 2: Game feature vectors embedded in R 2 , tagged by genre.

In practice some items are more popular than others, for which the personalization term is offset with a bias b n for each item. We could equally add user-specific biases, but omit it in order to regress a users usage list which has the same number of positive and negative items. The probability that user m is going to like (or dislike) item n is where l mn = 1 for the user liking and  X  1 for the user dis-liking the item.  X ( z ) = R z  X  X  X  N ( x ; 0 , 1)d x is the Gaussian cumulative density function, and acts as a link function that maps its argument to a value in the (0 , 1) interval.
Our working assumption is that there is sufficient us-age data to infer v n . Alternatively, we can further regress against user or item-specific meta-data, or learn a K -dimen-sional representation for each meta-data atom if required [5].
Given the data D . = { l mn } and parameters  X  . = { U , V , b } , the likelihood for observing the data is Our interest lies in how well the data supports the parame-ters, i.e. the posterior probability of  X  given the data. This is given by Bayes X  theorem, which incorporates a prior distribution on the model param-Gaussian distributions over the features and biases.
Bayesian inference comes into its own right when the pos-terior density p (  X  |D ) is used for predictions. Instead of using a single parameter estimate  X   X  , which needs to be carefully regularized to avoid overfitting when data is sparse [1], we rather average the likelihood function over all parameters that plausibly explain the data. In other words, gives the predictive distribution that user i is going to like item j .

Both the posterior and predictive distributions, given in (2) and (3), require the computation of analytically intractable integrals. The integrals can be evaluated stochastically through Monte Carlo methods, or approximated deterministically through a relaxation into an optimization problem.
We approximate the posterior distribution with a one that factorizes with The choice is largely driven by practical convenience, as q (  X  ) is  X  X actorized enough X  to give rise to computationally tractable optimization problem and algorithm, and it allows us to tractably compute the (approximate) predictive distri-bution, p ( l ij |D )  X  where we X  X e replaced p (  X  |D ) with q (  X  ) in (3). The factor-ization also ensures that only the matching user and item X  X  approximations play a role, and other random variables are independently averaged away in (4).

We learn q ( v n ) for each item; this is chosen as a K -dimensional factorizing Gaussian distribution q ( v n ) = Q ).
 As a result, we not only have a mean estimate of each item X  X  (and user X  X ) feature vector, but also encode uncer-tainty about its location through the variances of q ( v The variance of q ( v n ) is typically small for very popular items, but larger for items that are less frequently played, watched, or bought. As a consequence, popular games (with more  X  X ell determined X  parameters) will have a stronger ef-fect on determining a user X  X  feature vector, than games for which the model is less certain.

The approximation q (  X  ) is found by an algorithm called  X  X xpectation Propagation X , which has its roots in old and well-studied approximations 1 in statistical physics; the reader is referred to [5] for further algorithmic details. Expectation Propagation solves for the saddle point of a Bethe free energy with weak consistency constraints.
After training the model and learning parameter approx-imations for { U , V , b } , we are free to choose a utility to optimize. As an example, the default task of retrieving rec-ommendation for a specific user m can be formulated as fol-over which we maximize arg max to find the items that a user will like with largest predictive probability.

In practice the above expression is analytic if either of the feature distributions is a point mass, or can otherwise be approximated with an analytic function as explained in [5].
We evaluate our system using a classification task inspired by the KDD-Cup X 11 [2] competition. In Section 4.2 we also give a more traditional mean rank metric.
Results here are based on 10,000 Xbox users with at least 2 games and at most 50 games. Here, we ignored Xbox con-soles with more than 50 games because these may belong to larger organizations rather than individuals. Since tradi-tional evaluation (e.g., root mean squared error) do not gen-eralize well to the case of implicit data, we choose a rather new evaluation metric that was recently introduced in the second track of the KDD-Cup X 11 [2] competition: For each test user we kept one item in the test set, and trained on the rest of the items. After training, we randomly picked an-other item for every user which is not present in the training or the test items of that user. The goal is to differentiate the item that the user owned from the item that was randomly picked. The randomly picked items were chosen in two meth-ods: uniformly from the items set, and non-uniformly with probabilities proportional to each item popularity (similar to the process we use to generate implicit negative signals in Section 2). Namely, we define a classification task for every user and measure the overall precision on all users: Precision = 1 | M | X where T is the test-set, p ( l mn = 1 |D ) is the probability that user m likes item n (3), p ( l mk = 1 |D ) is the probability for user m to like the randomly sampled item k , and 1 (  X  ) is an indicator function. Note that since each user has just one item in the test-set, there is no real meaning to the recall measurement in this setting.
 We had several objectives in choosing this evaluation task: First, it is closely related to the methodology we use in train-ing the model (Section 2). More importantly, we believe this evaluation better relate to real life scenarios: As explained in [2], the proposed metric is related to the common recommen-dation task of predicting the items that the user is likely to own (rather than predicting a rating value to items the user already owns). In fact, this metric extends the evaluation to the truly missing entries and measures the true generaliza-tion power of the algorithm. Finally, there is an advantage to using this metric with negative examples drawn according to their popularity (non-uniformly), because it discourages known trivial solutions where most popular items are always suggested regardless of the user taste. In this way we focus Table 1: Evaluation results for Xbox personal-ization recommender for movies and games. We present precision results against a baseline that al-ways chooses the more popular item. We used both uniformly sampled items (over the entire items set) and non-uniformly sampling according to items pop-ularity. our evaluation on the true personalization power of the al-gorithm, rather than learning biases.

We evaluate this classification task against a simple base-line that always predicts that the user prefers the more pop-ular item. Table 1 summarize the results. We see that the baseline algorithm performs relatively well when the nega-tive items are uniformly distributed (84.3% and 65.1% in the games and movies domains respectively). However, these re-sults are attributed only to learning popularity of items (bi-ases), without any personalization. When negative signals are drawn according to their popularity, we better observe the precision gained by our personalization recommender over the baseline: an improvement of about 15% and 19% in the games and movies domains respectively. The overall results of the games recommender are better than those of the movies recommender. This is the result of the higher sparsity in the movies dataset (many more items, much less implicit ratings).

Note that in general, if we can sample negative items ex-actly according to their popularity, we expect that the base precision on the non-uniform test-set will be about 50% (as it is in movies). However, in the games domain some items are so popular that it is impossible to generate negative exam-ples in exactly the same distribution as the positive items Because we do not assign negative items to users that also has those items in their positive items list, the resulting dis-tribution is still somewhat biased towards the popular items and the baseline algorithm still achieves 60.2% precision on this task.
We also present a more  X  X raditional X  evaluation using the mean rank metric. As before, for each user we keep one game in a test-set, and use the rest of the items to train the model. We then rank the item in the test set against 8 thousand other games from our catalog according to p ( l mk = 1 |D ) -the probability that the user m will like an item k . We measured the mean rank of the test item for every user in the test set.

Figure 3 presents the mean rank (on a log scale) vs. the number of items in users history. Here we used 100,000 users (here we also included users with history longer than 50 items). An interesting observation is that the mean rank values increase with the users history length. Most of our users seem to own several very popular games. As users buy more games, they are more likely to own long tail items that are harder to model, which explains the line trend in Figure 3.
Because the most popular Xbox games are owned by most of our users. Figure 3: Mean-rank values vs. items in users his-tory.
A recent addition to Microsoft X  X  Xbox Live Marketplace is a recommender system that allows users to explore personal-ized content tailored individually to their taste. This paper gives a complete overview of Xbox X  X  recommender system in terms of system design, architecture, modeling, inference and utilization. We hope this example of real world large scale recommender system will give insights that would ben-efit the RecSys community academia and industry alike.
The Xbox recommendation system was made possible through the hard work of everyone in the Recommendation Group in Microsoft Entertainment Services. The Matchbox library [5] was developed by the Machine Learning Group in Microsoft Research, Cambridge, UK. [1] G. Dror, N. Koenigstein, and Y. Koren. Yahoo! music [2] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer. [3] U. Paquet, B. Thomson, and O. Winther. A [4] S. Rendle, C. Freudenthaler, Z. Gantner, and [5] D. H. Stern, R. Herbrich, and T. Graepel. Matchbox:
