 A novel method for creating collection summaries is devel-oped, and a fully decentralized peer-selection algorithm is described. This algorithm finds the most promising peers for answering a given query. Specifically, peers publish per-term synopses of their documents. The synopses of a peer for a given term are divided into score intervals and for each interval, a KMV (K Minimal Values) synopsis of its docu-ments is created. The synopses are used to effectively rank peers by their relevance to a multi-term query
The proposed approach is verified by experiments on a large real-world dataset. In particular, two collections were created from this dataset, each with a different number of peers. Compared to the state-of-the-art approaches, the pro-posed method is effective and efficient even when documents are randomly distributed among peers.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search processes Algorithms, Performance, Experiments P2P search, KMV, top-k
Indexing and retrieval of large-scale data collections can be scaled up by creating distributed indices. Index sum-maries are created for each index and when processing a query, these summaries are used to select a small subset of the indices that hold the most relevant results for the query. The objective is, therefore, to be able to approximate the The work of these authors was partially supported by The Israeli Ministry of Science and Technology (Grant 3-6472). results of a centralized index, while using a small subset of the indices and keeping the communication cost low.
We propose a solution that is described in the context of a peer-to-peer (P2P) network, but can also be applied in other setups. Thus, each peer has one index. The basic idea is to use a peer-granularity global index [2, 7, 9] that is implemented (as commonly done in P2P systems) by means of a distributed hash table (DHT). In particular, for all terms t , each peer p i publishes some statistics on the relevance of its documents to t j . All the published statistics on term t are mapped by the DHT to some peer p j that becomes responsible for that term.

To maintain the size of the global index manageable, the statistics are only per term, and not on combinations of terms. The challenge is to effectively use the per-term statis-tics to answer conjunctive multi-term queries. For example, a peer may have many documents that are relevant to either  X  X hildren X  or  X  X ooks, X  but not to  X  X hildren books. X 
Our contribution is twofold. First, we propose a novel resource-description framework for distributed collections. This framework is a combination of histograms and KMV (K Minimal Values) synopses [1, 3], and it describes documents that are relevant to each term. Each peer creates KMV synopses of score intervals for every term. For multi-term queries, we use the KMV non-emptiness estimator to find documents that are relevant to all the query terms.
The second contribution is a peer-selection algorithm that uses the resource-description framework to rank peers by the scores of their documents for a multi-term query. We tested this algorithm on two collections created from a large real-world data set taken from TREC GOV2. Each collection has a different number of peers. And in both collections, data is partitioned randomly among the peers. Our exten-sive experiments show that the peer-selection algorithm is effective and efficient on both collections.

The rest of the paper is organized as follows. In Sec-tion 2, we give some preliminaries and, in particular, a back-ground on KMV synopses. In Section 3, we present the peer-selection strategy using the KMV non-emptiness estimator. Section 4 describes the experiments that verify the effective-ness of our algorithms, and includes a comparison with the state of the art. Finally, we conclude in Section 5.
We consider a P2P network of autonomous peers, each having its own collection, that work in concert to facilitate search in the union of all collections. The goal is to find an approximation of the top-k results that would have been obtained from a centralized index that contain all collec-tions, while minimizing the number of contacted peers and the communication cost.

We denote peers and terms by the letters p and t ,respec-tively, possibly with subscripts. A query is denoted by q and is just a set of terms { t 1 ,...,t n } . The query q is interpreted as the conjunction t 1  X  t 2  X  ...  X  t n ; that is, results must contain all the query terms.

The statistics of a peer p about a term t is denoted by  X  By a slight abuse of notation, we use  X  ij , instead of  X  to denote that statistics of peer p i for term t j .
For each term t , there is a peer, denoted by p t ,thatis responsible for t . Every peer sends its statistics about t to p . Thus, the statistics (of all the peers) about t can be obtained from p t by means of a DHT.
 Aquery q = { t 1 ,...,t n } can be initiated at any peer. Usually, we denote by p q the peer where q is initiated. Peer p q uses the DHT to locate the peers p t 1 ,...,p t n that are responsible for the query terms, and gets the statistics that they hold. Based on the statistics, p q estimates the peers that are most likely to have documents that are highly rel-evant to the query, and sends the query to them. Finally, peer p q merges the returned results and finds the top-k doc-uments. We use an estimator that is based on the work on KMV (K Minimal Values) synopses [1, 3]. Consider a set 1 A ,andlet D ( A ) be the domain of the values (i.e., elements) appearing in A . We use a hash function h : D ( A )  X  X  0 , 1 ,...,M such that M = O ( | D | 2 ) in order to avoid collisions. The KMV synopsis of A , denoted by L A ,isthesetofthe 2 l smallest values of { h ( v ) | v  X  D ( A ) } .

Let A and B be sets with KMV synopses L A and L B of sizes l A and l B , respectively. For our purpose, we only need to estimate if the intersection of A and B is nonempty. To do that, we can simply check if L A  X  L B is nonempty. This is the non-emptiness estimator .If L A  X  L B =  X  ,then A  X  B =  X  , because we use a collision-free hash function. This estimator can be extended to any number of sets.
We want to find a good approximation of the top-k doc-uments that match a conjunctive multi-term query, while keeping at a minimum both the number of peers that are contacted and the communication cost . The latter is de-fined as the size of the statistics that are sent during query processing.
As discussed earlier, given a query q = { t 1 ,...,t n } ,we would like to rank the peers according to the potential for having highly scored documents that contain all the query terms. The basic idea is to create for every peer p i and term t , a KMV synopsis of the (set of ids of the) documents in p that contain t j . We can rank peers by trying to estimate the number of their valid documents (i.e., those with all
In [3], they discuss multisets, but we only deal with sets.
It is customary to use k for the size of a synopsis, but we use l to avoid confusion with the notion of the top-k results. the query terms). This can be done by applying the KMV intersection-size estimator [3]. However, this is not quite enough, because we want to rank the peers by the scores of their valid documents. Thus, we use more detailed statistics, as described next.

We assume that the score of a document d for a query q , denoted by score q ( d ), is obtained by aggregating the scores of d for the individual query terms. Formally, The aggregate function g aggr could be, for example, sum.
Consider a document d in the collection of peer p i .Given the score t j ( d ) for all the query terms t j , we can compute score q ( d ) by applying equation (1). To be able to compare the scores assigned by different peers, we assume that all the peers use the same scoring function. Still, the scoring function may depend on some collection parameters, such as inverse domain frequencies and average document length, that are different between peers. Our method will work best if those parameters are shared by all peers (e.g., each peer can contact a random sample of peers and then estimates the global parameters of a  X  X irtual X  centralized system that contains all collections). We leave for future work the perfor-mance of our method with different levels of such parameter synchronization.

Ideally, the statistics of peer p i for term t j should include score t j ( d ) for every document d of p i . However, this is too large. So, for peer p i and term t j , we only keep a collection of intervals that covers the whole range of scores that term t may have in documents of p i . For each interval, the statistics include the KMV synopsis (of the set of ids) of all documents
Formally,  X  ij denotes the statistics of peer p i for term t and is built as follows. Let D i be the set of documents of p and D ij = { d  X  D i | score t j ( d ) &gt; 0 } . Next, we divide the range of scores score t j ( d ), where d  X  D ij ,into M ij size intervals. We denote an interval by b ij m and define where 1  X  m  X  M ij and S ij =max d  X  D ij score t j ( d ), namely, S ij is the maximal score of the query q = { t j } over all the documents of D ij . We also define that is, D ij m is the subset of D ij comprising all documents with a score that falls in interval b ij m (1  X  m  X  M ij ). In practice, D ij m is a set of document ids rather than the docu-ments themselves. Furthermore, we represent each D ij m by a KMV synopsis, denoted by L ij m . The statistics  X  ij consist of the three parameters M ij , S ij and | D i | ,aswellastheKMV synopses L ij m of the sets D ij m (1  X  m  X  M ij ).
Figure 1 depicts the statistics  X  i 1 and  X  i 2 of peer p terms. Note that distinct terms can have different values of M ij , and the sizes of the KMV synopses may vary.
Consider a query q = { t 1 ,...,t n } and a peer p i that has some valid documents for q .A q -partition of p i is a tuple of synopses ( L i 1 m 1 ,...,L in m n ), namely, there is a synopsis L (1  X  m j  X  M ij ) for each term t j of q .Recallthat L ij synopsis of the set D ij m j .The q -partition ( L i 1 m 1
Figure 1: Statistics of peer p i for terms t 1 and t 2 represents the intersection  X  n j =1 D ij m j , which consists of all the valid documents d of p i that satisfy the following. For all the query terms t j ,thescore score t j ( d ) is in the inter-val b ij m j . By applying the KMV non-emptiness estimator to L m 1 ,...,L in m n , we can check whether
To circumvent cumbersome notation, we typically denote a q -partition by ( h 1 ,...,h n ), where each h j is some L Figure 1, h is the q -partition ( L i 1 2 ,L i 2 4 ), where q =
Consider a q -partition h =( h 1 ,...,h n )where h j = L ij Thus, h j corresponds to the interval b ij m j (which contains the scores for term t j of the documents in the set described by the synopses h j ). We use mid ( h j ) to denote the middle
The score of h is defined by namely, as if ( h 1 ,...,h n ) represents a document that has the score mid ( h j )fortheterm t j (see equation (1)).
We use h  X  to denote the result of applying the KMV non-emptiness estimator to the synopses h 1 ,...,h n .Thatis, h  X  is true if the intersection is nonempty; otherwise, it is false .Let C be the set of all the q -partitions of peer p intersection score of p i with respect to the query q , denoted by score q ( p i ), is defined as follows. In other words, we take the highest score over all q -partitions h of C , such that the synopses of h have a nonempty intersec-tion. By definition, score q ( p i ) = 0 if there is no q -partition h  X  C , such that h  X  is true .Hence, score q ( p i )=0ifsome query terms do not appear at all in the documents of p i .
The peer-selection algorithm works as follows. For each peer p i , we use the statistics  X  ij (1  X  j  X  n ) to compute the score of p i with respect to q , denoted by score q ( p i on these scores, we contact each of the top-K ( K&gt;k ) peers and request its top-k results for q . The top-k results among all those obtained from the top-K peers are the answer to the query q .
In this section, we evaluate the peer-selection algorithm and compare it with state-of-the-art methods.
We carried out tests on a large real-world dataset con-sisting of 10 million web pages from TREC 3 GOV2. We tested two collections created from this dataset. In the first, http://trec.nist.gov/ we divided the data randomly among 1 , 000 peers, result-ing in 10 , 000 documents per peer. In the second collection, the number of peers was 10 , 000, and 1 , 000 documents were randomly assigned to each one. We refer to these collections as Trec-1K and Trec-10K , respectively, where the suffix de-notes the number of peers. The total size of the indices was about 53GB. We experimented with 15 queries, which were taken from the topic-distillation track of the TREC 2003 Web Track benchmark. The size of the queries was between two to five terms. Those queries were also used in [8].
The two Trec collections represent an extreme case where documents are randomly assigned to peers. Consequently, a specific topic may spread over many peers. Hence, it is harder to find the top-k results for a given query, because they could be located in many different peers.

For the KMV synopses, we used a hash function based on Knuth X  X  Golden-Ratio Multiplicative (GRM) [6], which is equal to 2 , 654 , 435 , 761. The hash function multiplies a document id by the GRM, divides the result by 2 32 and takes the fractional part. To avoid collisions, [3] recommends using O ( | D | 2 ) bits for a hash value, where | D | is equal to the number of documents per peer. Trec-10K and Trec-1K have 1 , 000 and 10 , 000 documents per peer, respectively. Thus, the number of bits should range from 20 bits for Trec-10K to 27 bits for Trec-1K. However, in our experiments, we found that using merely 10 bits had no effect on the performance. We simulated the peers on a 64 bit Windows 2003 Server, equipped with two Quad-Core 2.5GHz Intel processors and 16GB RAM. The code was written in Java 1.6, and we used Lucene 4 as the local index in each peer. We indexed the content at each peer using stemming and stop-words removal.
In the experiments, the scoring function is that of Lucene. We define the ground truth as the top-k results that are re-turned by a  X  X irtual X  system that uses a centralized index of all collections. We measure the effectiveness of our method using the normalized DCG (nDCG) [5], where the top result of the ground truth is assigned relevance level k , the second result is assigned relevance level k  X  1, and so on. As a nor-malization step, the DCG of the top-k results computed by our algorithm is divided by the DCG of the ground truth. Note that the nDCG measured in the experiments is aver-aged over all queries. We got similar results for different values of k , ranging from 10 to 50, so we only show those for k = 25. The communication cost (abbr. comm-cost) is defined as the total size of the statistics that are sent during query processing. We do not take into account the comm-cost of sending the top-k results, because it is proportional to the number K of selected peers.

For the Trec-10K collection, we used l =10(where l is the number of values in a KMV synopsis, see Section 2.1), and chose M =5asthenumberofintervals M ij (see Sec-tion 3.1). For the Trec-1K collection, where each peer has 10 , 000 documents, we used l =20and M = 10.

Now, we compare our algorithm with three state-of-the-art peer-selection methods: cdf-ctf [2], cori [4] and hist In the first two, each peer sends aggregated statistics about http://lucene.apache.org
Actually, we changed it slightly, but in way that does not affect the ranking order of documents.
We used the best variant of hist that is described in [7]. every term in its collection. In hist, similarly to our ap-proach, the statistics are more detailed and some parameters need to be set. We selected M =5and M =10forTrec-10K and Trec-1K, respectively (i.e., same as in our method), and used about 10 groups in each peer p i .Thesevaluesare about optimal for hist, and yield a comm-cost that is similar to that of our method.

To reduce the comm-cost, we used the two-phase heuristic of [7] whereby the peer p q that initiates the query operates in two phases. In the first phase, p q gets (for each query term) very compact statistics about all the peers, such as cdf-ctf [2]. In the second phase, p q uses the compact statis-tics to pick  X  K peers (for a sufficiently large  X  K ), and only for those peers, does it request the full KMV synopses. In our experiments, we used  X  K =2 , 000 and  X  K = 400 for Trec-10K and Trec-1K, respectively. We incorporated the two-phase heuristic in our method as well as in hist . (Note that it is redundant to apply this heuristic to either cdf-ctf or cori.)
Figure 2(a) shows, for Trec-10K, the nDCG as a function of the number K of selected peers. Our algorithm outper-forms the other methods quite significantly. For example, when selecting 10 peers (0 . 1% of all peers), cori, cdf-ctf, andhisthaveannDCGof0 . 02, 0 . 09 and 0 . 15, respectively, whereas the nDCG of our method is 0 . 61 (an improvement of about 400% over hist). After contacting 20 peers, cori, cdf-ctf and hist achieve an nDCG of 0 . 03, 0 . 12 and 0 . 19, re-spectively, compared with an nDCG of 0 . 66 for our method. Figure 2(b) shows the results for the Trec-1K collection. Our method is still better than the other ones, but its nDCG is lower than for Trec-10K. For example, when contacting 20 peers, cori, cdf-ctf and hist achieve an nDCG of 0 . 09, 0 . 12 and 0 . 14, respectively, compared with an nDCG of 0 . 44 for our method. The lower nDCG for Trec-1K (compared to Trec-10K) can be explained by the relatively large number of documents in each peer, which causes the KMV synopses to be less effective for ranking peers. We leave it for future work to improve our method when peers have large collections.
Table 1 shows the comm-costs of the different methods for the two collections. The comm-costs for Trec-10K are higher than those for Trec-1K, because statistics about more peers are sent. For Trec-1K, the costs of cdf-ctf and cori are relatively low compared with a moderate cost of our method and hist. This is quite expected, because the latter two use more detailed statistics than cdf-ctf and cori. However, there is not such a big difference when the number of peers is large and the collection of each peer is rather small.

We presented a fully decentralized peer-selection algorithm for approximating the results of a centralized search engine, while using only a small subset of the peers. The algorithm utilizes per-term KMV synopses, grouped by score intervals, that peers publish about their documents. Our method per-forms significantly better than the state of the art. The biggest improvement is for the Trec-10K collection, where the number of peers is large. In this case, our method out-Figure 2: Performance of different methods (Trec-10K: l =10 , M =5 , Trec-1K: l =20 , M =10 ) performs the state-of-the-art by more than 400%, while its comm-cost is at most double.

In future work, we will test our method on more collections with the goal of improving it and reducing the comm-cost.
