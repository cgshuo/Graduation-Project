 1. Introduction
Document clustering is to partition a collection of documents into several clusters, such that the documents in the same and tracking ( Allan, 2002 ).

Usually, document clustering is performed in unsupervised fashion, i.e, only unlabeled documents are handled. However, in real application scenarios, it is often the case that the users have some background knowledge about the dataset, which semi-supervised clustering .

Recently, semi-supervised clustering has attracted significant research effort in machine learning and data mining com-guide the clustering process. The commonly used constraints are the pairwise constraints, which specify that two data items incorporated ML and CL constraints into the COBWEB algorithm ( Fisher, 1987 ) and achieved performance improvement.
The pairwise constraints under semi-supervised clustering can be used in two ways: hard , i.e., the constraints cannot be category.
 where the authors showed that the objective function of traditional K -means could be formulated as the trace expression of the original data matrix and then could be optimized further by eigen-decomposition, we incorporate the ML constraints into zation procedure are adapted to the semi-supervised clustering framework. The derived new approach is then applied to document clustering. In our implementation, we use the vector space model (VSM) ( Salton &amp; McGill, 1983 ) to represent the documents. In summary, our contributions in this paper are as follows: proposed method. Section 4 presents the experimental settings and results. Section 5 concludes the paper and highlights future work. 2. Related work taken as input. Roughly, document clustering methods fall into three categories: partitioning, hierarchial, and graph-based sive (top-down) or agglomerative (down-up) way. Graph-based methods attempt to model documents as vertices of a responding documents. Then, the problem of document clustering is transformed to graph partitioning based on a certain 2000 ). Also, there exist some papers that model documents set and keywords set as two sets of component vertices in a bipartite graph ( Zha, Ding, &amp; Gu, 2001; Dhillon, 2001 ).
 semi-supervised clustering has been done by the machine learning community, and the developed methods can be applied on reviewing the related work of semi-supervised documents clustering.
 imposed constraints often come in the forms of ML and/or CL constraint pairs. As K -means algorithm ( Hartigan &amp; Wong, 1979 ) is a popular technique in data clustering for its simplicity and ease implementation/use, quite some research work and CL constraints were incorporated into K -means and were not allowed to be violated during the clustering process. Basu semi-supervised K -means clustering was proposed in Kulis et al. (2005) . It combines the sum of square Euclidean distance with the costs of violating ML and CL constraints. Though a powerful technique, the applicability of the kernel-based parameters ( Yan &amp; Domeniconi, 2006 ).
Inspired by the work of Ji et al. (2006) , where ML constraints were integrated into Normalized Cut criterion function, and a new semi-supervised documents clustering method S3  X  Kmeans that employs ML constraints into K -means . The distinctive not least, S3  X  Kmeans is based on a globally optimal solution by relaxing the cluster membership indicator to continuous domain. 3. Our method maximization, which can be solved by eigen-decomposition globally.

In the sequel, we use the following notation. Capital letters (such as A ) represent matrices; k A k F denotes the Frobenius norm of matrix A , and is computed by k A k F  X  as matrix M , which is a square matrix. 3.1. Trace formulation of K-means objective function
Given a document collection with N documents and m unique keywords, we can represent the document collection by a s show that According to Zha et al. (2002) , the traditional criterion function of K -means can be expressed as Above, like A and X , B and Y represent the data matrix and the corresponding cluster membership indicator matrix, respec-tively. Here, we use B and Y to denote the data matrix and the corresponding cluster membership indicator matrix, for they have slightly different forms from A and X .In Zha et al. (2002) , Y is in the form as follows: where e is a column vector of dimension s i for the i th cluster with all the elements equal to 1.
In what follows, we will show that A and X also satisfy Eq. 2 . We can introduce a permutation matrix P N N such that PX squares cost with regard to X and PX . That is Applying AP T and PX to Eq. 2 , i.e. replacing B and Y in Eq. 2 by AP T and PX , respectively, and using Eq. 4 , we have 3.2. Imposing the constraints
U consists of n constraint row vectors, which has the following property: Property 1. UX  X  0 can see F  X  i ; j  X  X  U  X  i ; :  X  X  X  : ; j  X  X  0. So UX  X  0 holds.
 In what follows, we give an example to demonstrate the property UX  X  0.
 example, as follows: So X can be expressed as follows: Obviously, UX  X  0 holds. 3 follows: where c P 0 is used to control the degree of enforcement of prior knowledge. Minimizing Eq. 8 is equivalent to maximizing globally optimal solution. According to the linear algebra theory ( Golub &amp; Van Loan, 1999 ), we can get X ; X 2 ; ... ; X k , then the maximum is achieved when X  X  X  X 1 ; X 2 ; ... ; X k .

Now, what we need to do is to derive the final cluster membership assignment based on X . There are mainly two ways to do this, which were suggested in Ng, Jordan, and Weiss (2002) and Yu and Shi (2003) , respectively. In our experiments, we adopted the method proposed in Yu and Shi (2003) , which applies orthogonal transformation to obtain the nearly optimal discrete solution. The reason is that the method proposed in Yu and Shi (2003) can produce a better and more stable result than the method proposed in Ng et al. (2002) according to our observation in experiments. 3.3. The algorithm and its complexity
The proposed S3  X  Kmeans algorithm can be summarized as follows: (i) Generating the matrix A 2 R m N , where each column corresponds to a data vector in R m . (ii) Creating the constraint matrix U 2 R n N , where each row vector encodes a particular ML constraint. (vi) Assigning data vector i to cluster j ,if P  X  i ; j  X  X  1.

Now, we analyze the time complexity of the proposed algorithm. The total time cost of S3  X  Kmeans consists of two major parts: eigenvectors computation and discretization. We apply the Lanczos method Saad (1992) to compute the eigenvectors, time cost of the proposed algorithm is O  X  k L nnz  X  H  X  X  N k 2  X  . 4. Experimental evaluation and the results of performance comparison with three existing methods. 4.1. Test datasets information retrieval fields. ( Zhao et al., 2001 ). They were derived from TREC-5, TREC-6 and TREC-7. 4
ReutersTop10 . The Reuters-21578 data collection 5 contains documents collected from the Reuters newswire in 1987. We of the 10 categories and combined these sampled documents as our experimental dataset, which is denoted as ReutersTop10 .
News20 . The Newsgroup-20 6 dataset contains about 20000 documents that were collected from 20 newsgroups in the pub-lic domain. The topics of these newsgroups are very diversified, ranging from computer graphics and automobiles to religions and politics, etc. We randomly picked up 100 documents from each of the 20 categories to derive the experimental dataset called News20 as in Basu, Banerjee, and Mooney (2004) .

Webace . The Webace dataset comes from WebACE project Boley (1998) . It contains 2340 documents consisting of news articles from Reuters news service via the Web in Oct. 1997.
 We first preprocessed ReutersTop10 , News20 , and Webace , including stop words removal, stemming, HTML tags skipping. We then selected 2000 words as features according to the mutual information between the words and the documents where w represents a word and d represents a document. Finally, We adopted the standard TF scheme for term weighting sTop10 and Webace is presented in Table 1 . 4.2. The evaluation metrics each dataset for all the clustering algorithms.
 information shared by two random variables representing cluster assignment and underlying class label. Corrected rand coef-ficient measures the agreement between two partitions. In order to compute both metrics, a confusion matrix is established mation and corrected rand coefficient are computed as follows: ses, respectively. 4.3. The results and discussions
We evaluated S3  X  Kmeans by using all the seven datasets mentioned above and compared its performance with that of the standard K -means , S  X  Kmeans ( Zha et al., 2002 ) and Cop  X  Kmeans ( Wagstaff et al., 2001 ). 50. So, without specific indication, we set c to 50 in the remaining experiments.
 S3  X  Kmeans . Their performance comparisons are presented in Tables 2 and 3 , which correspond to Normalized Mutual Informa-than K -means and S  X  Kmeans do in almost all cases except for datasets tr 11 (only when using the NMI metric) and tr 31.
We then compared S3  X  Kmeans with Cop  X  Kmeans . Cop  X  Kmeans was initialized according to Basu et al. (2004) . The exper-iments were conducted over the seven datasets with different numbers of ML constraint pairs. Constraint pairs were gener-ated by randomly sampling the test datasets. For each constraint number setting, 10 test runs were conducted on different randomly chosen constraints sets and the final performance was obtained by averaging the results of the 10 runs. In each run, we employed the same constraints set to the two algorithms: S3  X  Kmeans and Cop  X  Kmeans . The results are shown in Figs. 2 X 8 , respectively. may have larger numbers of constraint pairs. As we can see from Figs. 2 X 8 , the learning curves of both metrics NMI and CRC of S3  X  Kmeans show monotone improvement as more and more constraints are used. However, the learning curves of Cop  X  Kmeans take zigzag shape, which suggests that S3  X  Kmeans is more robust and more effective in exploiting constraints than robustness and effectiveness is very useful in practice, because prior knowledge available is not always so informative or coherent as required by some semi-supervised methods. And for most cases of the constraints settings, S3  X  Kmeans outper-forms Cop  X  Kmeans substantially.
 5. Conclusion and future work which is expressed as equivalent trace formulation Zha et al. (2002) and can be optimized by eigen-decomposition. We have et al. (2002) ) and Cop  X  Kmeans . The experimental results showed that in most cases our method outperforms all the three existing methods with an evident margin for both performance metrics NMI and CRC . As pointed by Davidson et al. (2006) , pairwise constraints are not always helpful in semi-supervised clustering, such as Cop  X  Kmeans . We argue that whether or not the constraints are helpful not only depends on their informativeness and coherence, but is also related clustering result and (2) comprehensive comparison study on different clustering algorithms with pairwise constraints. Acknowledgements This research was supported by the National Basic Research Program of China under Grant No. 2007CB310806, the National Natural Science Foundation of China under Grant Nos. 60573183, 90612007, and 60704044. Jihong Guan is also supported by Program for New Century Excellent Talents in University (No. NCET-06-0376).
 References
