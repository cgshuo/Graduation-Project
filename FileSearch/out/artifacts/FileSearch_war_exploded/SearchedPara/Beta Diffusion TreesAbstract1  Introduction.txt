 Creighton Heaukulani  X  CKH 28@ CAM . AC . UK Zoubin Ghahramani  X  ZOUBIN @ ENG . CAM . AC . UK
University of Cambridge, Department of Engineering, Cambridge, UK Stanford University, Department of Computer Science, Stanford, CA, USA Latent feature models assume that there are a set of non-overlapping subsets (called features ) of a collection of ob-jects underlying a data set. This is an appropriate assump-tion for a variety of statistical tasks, for example, in visual scene analyses, images could be assigned to the following features:  X  X mage contains a chair X ,  X  X mage contains a ta-ble X ,  X  X mage is of a kitchen X , etc. The Indian buffet pro-cess (IBP; Griffiths &amp; Ghahramani (2011)) defines a prior on such clusterings, called feature allocations .
 With the IBP, objects are assigned or not assigned to a fea-ture with a feature-specific probability that is independent of the other features. In the scene example, however, the features are structured into a hierarchy: tables and chairs are likely to appear in scenes together, and if the scene is in a kitchen, then possessing both tables and chairs are highly probable. In order to model hierarchically related feature allocations, we define the beta diffusion tree a random tree structure whose set of leaves define a feature allocation for a collection of objects. As with the IBP, the number of leaves (features) is random and unbounded, but will be al-most surely finite for a finite set of objects.
 Models for hierarchically structured partitions (non-overlapping subsets) of a collection of objects can be con-structed by the Dirichlet and Pitman X  X or diffusion trees (Neal, 2003b; Knowles, 2012; Knowles &amp; Ghahramani, 2014), in which a collection of particles (representing the objects) diffuse in some continuous space X (for example, as Brownian motion in Euclidean space) over some inter-val of time. Particles start at a fixed point and sequentially follow the paths of previous particles, potentially diverging from a path at random times. At the end of the time interval, the clusters of particles define a partition of the objects, and the paths taken by the particles define a tree structure over the partitions. The beta diffusion tree proceeds analogously to the Dirichlet diffusion tree, except that multiple copies of a particle (corresponding to multiple copies of an object) may be created (or removed) at random times. Therefore, at the end of the time interval, objects may correspond to particles in multiple clusters, and each cluster is interpreted as a feature.
 The article is organized as follows: In Section 2, we de-scribe a generative process for the beta diffusion tree and investigate its properties. In Section 3, we construct a hierarchically-clustered factor analysis model with the beta diffusion tree and review related work. In Section 4, we describe a Markov chain Monte Carlo procedure to inte-grate over the tree structures, which we apply in Section 5 to experiments on missing data problems.
 We describe a collection of particles, each labelled with one of N objects, diffusing in a continuous space X over a hypothetical time interval t  X  [0 , 1] . If a particle is la-beled with object n , then we call it an n -particle , multiple of which may exist at time t &gt; 0 . In this work, we take X = R D for some dimension D , and let the (random) dif-fusion paths be distributed as Brownian motion with vari-in
R D at time t , then it will reach position at time t + d t . Sequentially for every data point n = 1 ,...,N , we begin with one n -particle at the origin 0 , which follows the paths of previous particles. At random times t ? throughout the process, an n -particle travelling a path may perform one of two actions: 1. stop: The particle stops diffusing at time t ? . 2. replicate: A copy of the n -particle is created at time More precisely, let  X  s , X  r , X  s , and  X  r be positive, finite con-stants that parameterize the generative process, which pro-ceeds as follows:  X  n = 1 : A 1-particle starts at the origin and diffuses as  X  n  X  2 : For every n  X  2 , a single n -particle starts The process terminates at t = 1 , at which point all particles stop diffusing. The times until stopping or replicating on a path along which m particles have previously travelled are exponentially distributed with rates  X  s  X  s / (  X  s + m ) and  X   X  r / (  X  r + m ) , respectively, and it is therefore straightfor-ward to simulate a beta diffusion tree in practice. In Fig. 1, we show a beta diffusion tree with N = 3 objects in D = 1 dimension, along with its corresponding tree structure, in which the origin is the root node , stop points are stop nodes , replicate points are replicate nodes , and the points at t = 1 are leaf nodes . We call segments between nodes branches . Because multiple copies of a particle (all corresponding to the same object) can follow multiple branches to multi-ple leaves in the tree, the leaves define a feature allocation of the N objects. For example, adopting the notation of Broderick et al. (2013), the beta diffusion tree in Fig. 1 de-termines a feature allocation with two features { 1 , 3 } and { 2 , 3 } . The number of (non-empty) features is therefore the number of leaves in the tree structure, which is unbounded, however, in Section 2.2 we will see that this number is (al-most surely) finite for any finite number of objects. 2.1. The generative process is exchangeable Let T [ N ] denote the tree structure (i.e., the collection of nodes, associated node times, and branches) of the beta dif-fusion tree with ordered set of objects [ N ] : = (1 ,...,N ) , and let x T the generative process depends on the ordering of [ N ] , we will now show that the density p ( T [ N ] , x T pend on this ordering. Because the times until stopping or replicating on a branch are exponentially distributed, it fol-lows that the probability of neither replicating nor stopping between times t and t 0 (with t &lt; t 0 ) on a branch along which m previous particles have traversed is given by  X  m ( t,t 0 ) : = P { not replicating and not stopping in [ t,t = exp n  X  For example, consider the tree in Fig. 1, consisting of nodes t , t b , etc. From the tree structure, we can determine that there is one 1-particle, two 2-particles, and two 3-particles. The 1-particle contributes a factor of  X  0 (0 , 1) to the den-replicating) in t  X  (0 , 1) . The 2-particles contribute: 1.  X  1 (0 ,t a ) for no event in t  X  (0 ,t a ) , 3.  X  1 ( t a ,t b ) for no event in t  X  ( t a ,t b ) , 5.  X  0 ( t a , 1) for no event in t  X  ( t a , 1) . The 3-particles contribute: 1.  X  2 (0 ,t a ) for no event in t  X  (0 ,t a ) , 3.  X  2 ( t a ,t b ) for no event in t  X  ( t a ,t b ) , 4. 1  X  1  X  5.  X  1 ( t b , 1) for no event in t  X  ( t b , 1) , 6.  X  1 ( t a ,t c ) for no event in t  X  ( t a ,t c ) , 8.  X  1 ( t c , 1) for no event in t  X  ( t c , 1) , 9.  X  0 ( t c ,t d ) for no event in t  X  ( t c ,t d ) , and 10.  X  s for stopping at t = t d .
 Finally, the components of the density resulting from the node locations x a , x b , etc., are
N ( x a ; 0 , X  2 X t a ) N ( x b ; x a , X  2 X ( t b  X  t a  X N ( x c ; x a , X  2 X ( t c  X  t a )) N ( x d ; x c , X  2 X There is one Gaussian term for each branch. Because the behavior of objects in the generative process depends on previous objects, the terms above depend on their ordering. However, this dependence is superficial; if we were to mul-tiply these terms together, we would find that the resulting expression for p ( T [3] , x T ing of the objects. In Heaukulani et al. (2014), we general-ize this expression to an arbitrary number of objects N and find that the density function is given by = Y  X  Y  X  Y replicate nodes, stop nodes, and branches in T [ N ] , respec-tively; for every non-root node u in the tree structure, m ( u ) denotes the number of particles that have traversed the branch ending at u ; for every u  X  R ( T [ N ] ) , n r ( u ) denotes the number of particles that have replicated at u ; for every v  X  S ( T [ N ] ) , n s ( v ) denotes the number of particles that have stopped at v ; and finally H  X  n : = P n i =1  X / (  X  + i ) . Because this expression does not depend on the order-ing of the objects, the stochastic process ( T [ N ] exchangeable with respect to the ordering of the objects. Stated more formally: Theorem 1 (Exchangeability) . Let  X  ([ N ]) be any permu-tation of [ N ] . Then Because the ordering of the sequence [ N ] is irrelevant, we will henceforth simply write T N . By its sequential con-struction, the generative process is projective, and we may therefore define a stochastic process by a beta diffusion tree with set of objects N , the associated tree structure T N which is a tree structure over feature allocations of N . 2.2. A nested feature allocation scheme Let there be L levels in a nested feature allocation scheme of N objects. Associate each level `  X  L of the scheme with a discrete time t ` = ( `  X  1) /L  X  [0 , 1] , and let p and p ( ` ) 2 be independent random variables with At the first level, we allocate the N objects to two differ-ent features f (1) 1 and f (1) 2 independently with the level one-specific probabilities p (1) 1 and p (1) 2 , respectively. At the next level, we allocate the objects in f (1) 1 to two different fea-tures, f (2) 11 and f (2) 12 at level two, independently with prob-abilities p (2) 1 and p (2) 2 , respectively. The objects in f likewise allocated to two features f (2) 21 and f (2) 22 at level two. A figure depicting this scheme for L = 2 levels is shown in Fig. 2(a). Continue this scheme recursively for L levels, where we allocate the objects in every (non-empty) feature at level `  X  1 to two features in level ` , independently with the level ` -specific probabilities given by Eq. (11). Define a binary branching, discrete tree structure where every non-empty feature represents a node, as depicted in Fig. 2(b). Let segments between nodes be branches and let T N ,L de-note the collection of nodes and branches. In Heaukulani et al. (2014), we show that in the continuum limit L  X  X  X  , we obtain the tree structure of the beta diffusion tree: Theorem 2 (continuum limit) . Let T N be the tree structure of a beta diffusion tree with set of objects N . Then From the perspective of the nested feature allocation scheme and Theorem 2, it is clear that the de Finetti mea-sure for the beta diffusion tree with index set N is charac-terized by the countable collection of (tuples of) beta random variables, motivating our name for the stochastic process. In Heaukulani et al. (2014), we use this identification of F to provide yet another character-ization of the beta diffusion tree as a multitype continuous-time Markov branching process (Mode, 1971; Athreya &amp; Vidyashankar, 2001; Harris, 2002). Taking advantage of these well-studied stochastic processes, we show: Theorem 3. Let T N be the tree structure of a beta diffusion tree with a finite set of N objects. If  X  s , X  r , X  s , X  then the number of leaves in T N is almost surely finite. This is a reassuring property for any stochastic process em-ployed as a non-parametric latent variable model, the trans-lation in this case being that the number of latent features will be (almost surely) finite for any finite data set. Further-more, we also characterize the expected number of leaves in a beta diffusion tree. 2.3. Correlating features and related work We show another beta diffusion tree with N = 150 objects in Fig. 3(a). Let K denote the number of leaf nodes (fea-tures), which we have seen is unbounded yet almost surely finite. In this larger example, it is convenient to represent the feature allocation as a binary matrix, which we will de-note as Z , where the n -th row z n  X  { 0 , 1 } K indicates the features to which object n is allocated, i.e., z nk = 1 indi-cates object n is allocated to feature k . Then each column of Z represents a feature, and the tree structure defines a hi-erarchical clustering of the columns, depicted in Fig. 3(b). The Indian buffet process (IBP; (Griffiths &amp; Ghahramani, 2006; Ghahramani et al., 2007)) was originally described in terms of such binary matrices with an unbounded number of independent columns. A class of correlated IBP mod-els appeared in (Doshi-Velez &amp; Ghahramani, 2009), which cluster the columns of an IBP-distributed matrix in order to induce (sparse) dependencies between the features. For example, let Z (1) be an IBP-distributed matrix, and con-ditioned on Z (1) , let Z (2) be another IBP-distributed ma-trix whose rows corresponds to the columns of Z (1) . This scheme is extended to an arbitrary number of iterations by the cascading Indian buffet process (Adams et al., 2010b), in which the rows in an IBP-distributed matrix Z ( m ) at it-eration m correspond to the columns in the IBP-distributed matrix Z ( m  X  1) at iteration m  X  1 . While the beta diffusion tree generalizes the  X  X lat clustering X  of the correlated IBP to a hierarchical clustering, it does not obtain the general network structure obtained with the cascading IBP. These stochastic processes all model continuous tree struc-tures, which are most useful when modeling continuous variables associated with the hierarchy. We will see exam-ples using the beta diffusion tree in Section 3. Probabilis-tic models for non-parametric, discrete tree structures are also widespread (Blei et al., 2010; Rodriguez et al., 2008; Paisley et al., 2012; Adams et al., 2010a; Steinhardt &amp; Ghahramani, 2012), which would be appropriate for mod-eling only discrete variables associated with the tree struc-ture. Relevant examples of non-probabilistic models for non-parametric tree structures include (Heller &amp; Ghahra-mani, 2005; Blundell et al., 2010).
 Models based on the beta diffusion tree are not to be con-fused with the phylogenetic Indian buffet process (Miller et al., 2008), which hierarchically clusters the rows (ob-jects) in an IBP-distributed matrix. Alternatively, the distance-dependent IBP (Gershman et al., 2011) assumes that there is an observed distance metric between objects. If two objects are close, they tend to share the same fea-tures. Both of these models, unlike models based on the beta diffusion tree and the correlated IBP, assume that the features themselves are a priori independent. In applications, we typically associate the objects allocated to a feature with a set of feature-specific latent parameters. The objects can be observed data that depend on the latent parameters, or the objects can themselves be unobserved variables in the model. A convenient choice for a set of continuous-valued latent parameters associated with each feature (leaf node in the beta diffusion tree) are the loca-tions of the leaf nodes in X . Consider the following exam-ple: Let Z be the binary matrix representation of the fea-ture allocation corresponding to a beta diffusion tree with K leaf nodes. Recall that the k -th column of Z corre-sponds to a leaf node in the tree with diffusion location x k in X = R D at time t = 1 (c.f. Fig. 1). We model a collection of N data points y 1 ,..., y N in R D by where X is a K  X  D factor loading matrix whose k -th row is given by x k , and 1 ,..., n are i.i.d. Gaussian noise vectors with zero mean and covariance  X  2 Y I D . Here  X  is the noise variance and I D denotes the D  X  D identity matrix. Let Y be the N  X  D matrix with its n -th row given by y n . Then Y is matrix Gaussian and we may write E [ Y | T N , X ] = ZX . This is a type of factor analy-sis model that generalizes the linear Gaussian models uti-lized by Griffiths &amp; Ghahramani (2011) and Doshi-Velez &amp; Ghahramani (2009). In the former, the latent features (columns of Z ) are independent and in the latter, the fea-tures are correlated via a flat clustering. In both models, the factor loadings x 1 ,..., x K are mutually independent. With the beta diffusion tree, on the other hand, both the la-tent features and factor loadings are hierarchically related through the tree structure.
 Because the particles in the beta diffusion tree diffuse as Brownian motion, we may analytically integrate out the specific paths that were taken, along with the locations of the internal (non-leaf) nodes in the tree structure. Further-more, because Y and X are both Gaussian, we may follow the derivations by Griffiths &amp; Ghahramani (2011) to analyt-ically integrate out the factor loadings X from the model, giving the resulting likelihood function p ( Y |T N ) = with entries given by where t a ( `,k ) is the time of the most recent common ances-tor node to leaf nodes ` and k in the tree, and we recall that  X 
X is the variance of the Brownian motion (c.f. Eq. (1)). In Heaukulani et al. (2014), we describe a series of Markov Chain Monte Carlo steps to integrate over the random tree structures of the beta diffusion tree. These moves are sum-marized as the following proposals: Resample subtrees: Randomly select a subtree rooted at a non-leaf node in the tree, and resample the paths of one or more particles down the subtree according to the prior. Add and remove replicate and stop nodes: Randomly propose an internal (either replicate or stop) node in the tree structure to remove. If the removed node is a replicate node, then the entire subtree emerging from the divergent branch is removed. If the node is a stop node, then the par-ticles that stopped at the node need to be resample down the remaining tree according to the prior. Conversely, propose adding replicate and stop nodes to branches in the tree. Resample configurations at internal nodes: Randomly select an internal node in the tree and propose changing the decisions that particles take at the node (i.e., the decisions to either replicate at replicate nodes or stop at stop nodes). Heuristics to prune or thicken branches: Propose re-moving replicate (or stop) nodes at which a small propor-tion of the particles through the node have decided to repli-cate (or stop).
 Each proposal is accepted or rejected with a Metropolis X  Hastings step. In Heaukulani et al. (2014), we provide the results on joint distribution tests (Geweke, 2004) ensuring that the first three moves sample from the correct poste-rior distributions. The fourth move is a heuristic that does not leave the steady state distribution of the Markov chain invariant, though we found it critical for efficient mixing and good performance of the procedure. All hyperparame-ters were given broad prior distributions and integrated out with slice sampling (Neal, 2003a). We implement our MCMC procedure on the linear Gaus-sian model and evaluate the log-likelihood of the inferred model given test sets of held-out data on an E. Coli dataset of the expression levels of N = 100 genes measured at D = 24 time points (Kao et al., 2004), a UN dataset of human development statistics for N = 161 countries on D = 15 variables (UN Development Programme, 2013), and an India dataset of socioeconomic measurements for N = 400 Indian households on D = 15 variables (De-sai &amp; Vanneman, 2013). We compare this performance against baselines modeling Z with the two parameter In-dian buffet process (IBP; Ghahramani et al. (2007)) and two correlated latent feature models introduced by Doshi-Velez &amp; Ghahramani (2009). All three baselines model the factor loadings (independently from the factors) as mutu-ally independent Gaussian vectors x k  X  N ( 0 , X  2 X I D ) , k = 1 ,...,K , where K is the number of non-empty fea-tures. For each data set, we created 10 different test sets, each one holding out a different 10% of the data. In Fig. 4, we display the box-plots of the test log-likelihood scores over the 10 test sets, where the score for a single set is aver-aged over 3,000 samples (of the latent variables and param-eters of the model) collected following the burn-in period of each method. The beta diffusion tree achieved the high-est median score in every experiment, with the IBP-IBP achieving the second best performance in each instance. The difference between these two sets of scores is statis-tically significant in each case, based on a t-test at a 0.05 significance level. The p-values for the null hypothesis that the means of the two sets are the same were 5 . 3  X  10  X  3 1 . 5  X  10  X  4 , and 7 . 9  X  10  X  3 for the E. Coli , UN, and India data sets, respectively. In Fig. 5, we display box plots of the number of features inferred for each test set (averaged over the 3,000 samples following the burn-in). The supe-rior performance on the test log-likelihood metric therefore suggests that a hierarchical feature allocation is an appro-priate model for these data sets.
 We can extend the qualitative analysis by Doshi-Velez &amp; Ghahramani (2009) on the UN development statistics. Here we display the maximum a posteriori probability sample (among 2,000 samples collected after a burn-in period on the data set with no missing entries) of the feature matrix and tree structure over the features. For visualization, the rows (corresponding to different countries) are sorted from highest human development index (HDI  X  a score com-puted by the UN) to lowest. We also display the HDI scores for five ranges of equal sizes, along with the names of the top and bottom 10 countries in each range. We can see that a hierarchical structure is present; many highly devel-oped countries are assigned to the third feature, with a more refined set belonging to the fourth feature. An even finer subset belongs to the fifth feature. On the other hand, the less developed countries have high prevalence in the sec-ond feature, with a broader set belonging to the first. This subset is not strict; many countries belonging to the second feature do not belong to the first. We have also displayed the posterior mean of the factor loading matrix. The third feature places higher weight on the variables we expect to be positively correlated with the highly developed coun-tries, for example, GDP per capita, the number of broad-band subscribers, and life expectancy. On the other hand, these features place lower weight on the variables we ex-pect to be negatively correlated with the highly developed countries, notably, the rates for homicide and infant mor-tality. The first and second features are the reverse. Similarly, in Fig. 7 we display the maximum a posteri-ori probability feature matrix and corresponding hierarchy over the features for the E. Coli data set when no data is held out. We note that, in this figure, the features are not necessarily ordered with the divergent branches to the right like in the previous figures in the document. In this case, the individual genes are not as interpretable as the coun-tries in the UN data set, however, the hierarchical structure is reflected in the feature allocation matrix. The beta diffusion tree is an expressive new model class of tree-structured feature allocations of N , where the num-ber of features are unbounded. The superior performance of this model class in our experiments, compared to inde-pendent or flatly-clustered features, provides evidence that hierarchically-structured feature allocations are appropriate for a wide range of statistical applications, and that the beta diffusion tree can successfully capture this structure. There are many future directions to be explored. The fea-tures in the beta diffusion tree are not exchangeable (at a replicate point, particles are guaranteed to follow an origi-nal branch but not necessarily a divergent branch). This is reflected by the fact that the feature allocation probabilities p 1 and p exchangeable. In contrast, the exchangeability of the fea-ture allocation probabilities obtained from the beta process (Hjort, 1990) implies the exchangeability of the features in the IBP (Thibaux &amp; Jordan, 2007). One could investigate if there is a variant or generalization of the beta diffusion tree in which the features are exchangeable. This could be a desirable modeling assumption in some applications and may enable the development of new inference procedures. Teh et al. (2011); Elliott &amp; Teh (2012) showed that the Dirichlet diffusion tree (Neal, 2003b) is the fragementation-coagulation dual (Pitman, 2006; Bertoin, 2006) of the Kingman coalescent (Kingman, 1982; Teh et al., 2007). The stochastic process introduced therein can be viewed as combining the dual processes in order to model a time-evolving partition of objects. One could investigate if such a dual process exists for the beta diffu-sion tree or some variant thereof, from which a model for time evolving feature allocations could be obtained. We thank Daniel M. Roy, Christian Steinrueken, and anonymous reviewers for feedback on drafts. This work was funded in part by EPSRC grant EP/I036575/1. CH is funded by the Stephen Thomas studentship at Queens X  Col-lege, Cambridge.

