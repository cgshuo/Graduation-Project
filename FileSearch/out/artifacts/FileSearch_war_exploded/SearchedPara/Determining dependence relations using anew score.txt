 Department of Computation Science and Arti fi cial Intelligence, University of Granada, Granada, Spain 1. Introduction
The complexity that represents the management of data can be reduced discovering dependence/inde-pendence relations among them. Normally, the main criterion to determine the fi nal complexity is based on the independence relationships that can be obtained from the data sample. It is known that there is not a single criterion to be used when considering whether an independence relationship is supported by the data and none of the existing ones can be considered as superior to the others. All are based on some basic methodology and some additional assump tions or approxi mations. In this way, we have methods based on frequentist statistical tests of independence [22], on Bayesian scores [9], on the mini-mum description length principle [21], or on the theory of imprecise probabilities [17]. The quality of a measure will depend on the appropriateness of the assumptions, the error of the approximations, or the application.

In Moral [17] an empirical evaluation of the different criteria is presented, showing that none of them is superior to the others in any circumstance. Though the Bayesian scores showed a good performance, in general, their behavior is not satisfactory. Their main problem is that they can decide for dependence when the variables are independent from a sample of very small size. When applied to learning Bayesian networks, we can obtain links that are not supported by the data. As an example, we have considered a Bayesian network with 12 binary independent variables. The marginal distribution for all of them is 0.5 of probability fo r each one of its values. From it, we have obt ained by simulation a sample of size 4, and then we have learned a Bayesian network from that sample using the standard K2 algorithm [9]. The result is in Fig. 1. We can see a graph with a complex structure and this has been estimated from a very small sample coming from independent variables.

This problem is not shared by Chi-square independent tests, as for small samples they have a tendency to decide for independence. In fact when using PC learning algorithm [22], which is based on indepen-dence tests, we obtain a completely unconnected network from the same sample. But Chi-square tests show another characteristic which is not satisfactory from our point of view: with large samples they keep constant the error of deciding for dependence when there is independence. But, due to the sample size this error could be decreased without a great increase in the other error (assuming independence when there is dependence).

Independence can be studied for different aims: to know about the existence of a relation between two distribution is obtained by estimating the marginal distributions and then taking the product). Also the study of independence could be used as a previous step in classi fi cation, for example, if we are building a classi fi cation tree [5], we can determine a variable for branching between those that are dependent of the class variable. This paper mainly concentrates in determining the existence of independence, but in the experiments we will give some results relative to the classi fi cation problem. There is another related problem which is the measurement of the degree of dependence between two categorical variables [10], to which the imprecise Dirichlet model has been applied [7,8]. Deciding about independence and mea-suring the association between two variables are related issues: in most of the cases, the decision is made taken into account a measurement of the dependence degree of the variables, called a dependence score. However, in the paper this score will be always auxiliary to the main aim: determine the existence of independence.

The dependence problem is well known in the Statistical literature [10], and several association mea-sures have been proposed and statistical tests have been designed taking them as basis. However, in this paper we have concentrated in the procedures that have been used in the task of learning Bayesian networks [19] and some new procedures that we have proposed and that are based on the imprecise Dirichlet model.

In this paper we introduce a new imprecise score measure which has its starting point in the Heck-erman, Geiger, and Chickering X  X  score [11], but considering a bounded imprecise prior Dirichlet model instead of a precise one [18,25]. In this way we obtain three possible situations: dependence dominates, independence dominates, or none of them dominates the other. If we decide for independence in all the situations except when dependence dominates, then the behavior we obtain is intermediate between
This problem is clearly related with the study of the dependency among features (or attributes) when building classi fi ers, for example when constructing a Naive Bayes classi fi er [13]. There is an important difference: if we have a supervised classi fi cation problem, then the decisions could be based on the classi fi er performance following a wrapper approach. Here we have considered a more basic issue: how to measure and decide about dependence. Our procedures could be posteriorly applied to the problem of dependency among attributes.

In this paper we have carried out a set of experiments to determine the characteristics of the different methods to decide for independence. The setting is the following: two binary categorical variables and we have to decide whether they are dependent or independent. More complex scenarios are necessary (more than two values, conditional independence with more variables) but, at this stage we only want to discover the most basic behavior of the procedures and then we have tried to avoid other factors affecting it.

We have compared different classical scores, a sc ore based on imprecise pr obability proposed in Abel-l X n and Moral [5], a Bayesian score with a prior probability favoring independence, and the imprecise Dirichlet score introduced in this paper.

Though the experiments are with binary variables, our future objective is more important, but it needs of this previous study. Our fi nal aim is to determine a general procedure to determine the complexity of a model relating a set of variables, as for example a dependence graph [19] or a classi fi cation tree [20]. This construction will be based on the veri fi cation of conditional independence relationships between the problem variables, which can have more than two values. So will consider methods that are suitable for being generalized to general categorical variables and to conditional independence relationships. This is a reason for not considering Fisher exact test [10].

The paper is organized as follows: in Section 2, we consider the basic notation and give the basic procedures to decide about independence. In Section 3, we introduce the new interval-valued score. In Section 4, we describe the experiments in detail and the results obtained. The discussion of the results is in Section 5, while Section 6 is devoted to the conclusions and future work. 2. Score measures
Let X and Y be two variables taking values on set { 0 , 1 } . We assume that there is a probability distribution, p , with which these variables take jointly their values and that p X and p Y are its marginal all of which follow distribution p . N is the sample size. The basic problem is to determine from the sample whether there is independence in distribution p . Another important related problem is to obtain an estimation of the joint probability distribution.
 ( n
Y ( j ) ) the number of times that X = i ( Y = j ) in the sample. Let us consider that  X  p is the maximum the maximum likelihood estimators of the marginal distributions.

All the procedures to decide about dependence are based on a score of the degree of dependence. The fi rst measure of dependence ( CHI ) that we are going to consider is the p -value of the Chi-square test for independence. This measure is based on considering the estimated mutual information in the sample:
It is known that, under the independence hypothesis, 2  X  N  X  G asymptotically follows a Chi-square distribution with one degree of freedom (as the variables are binary). The p -value is the probability that dependence, CHI , will be 1 minus this p -value. It is a number between 0 and 1. Values very close to 1 imply a high level of dependence between the variables. To decide about dependence or independence, we need a signi fi cance level  X  .If 1  X  CHI &lt;  X  , then we assume that there is independence. The value of  X  is the probability of the error of deciding dependence when there is independence.

This measure is well justi fi ed from classical frequentist statistical theory. It is simple to compute, but independence relationships when we have more than two variables, and therefore there is not an obvious way of obtaining a global score for a general Bayesian network. We could think of considering something as 1 minus the p -value of a Chi-square test in which the null hypothesis is that a set of independence relationships is given, but then this measure will be larger if we have less independence relationships and we will not be penalizing the complexity of the model.

The second measure is based on the K2 score for Bayesian networks [9]. To score independence, it considers the probability distributions p X and p Y and then it assumes that there is independence between independent Beta distribution with parameters (1 , 1) [19]. Then the probability of the sample given these hypotheses is computed being equal to: where  X ( x ) is the gamma function (  X ( n )=( n  X  1)! when applied to an integer number as in this case). This value is obtained by integrating the likelihood function with respect to the prior probability for the parameters and it is also called the marginal likelihood [19].

To score the dependence case, we consider that the parameters for the marginal distribution ( p X (0) , p follow three independent prior Beta densities with parameters (1 , 1) , then the probability of the sample can be obtained as
K 2 D is also computed by integrating the likelihood with respect to the prior distribution of the pa-rameters.
 The fi nal score is K 2= K 2 D/K 2 I . The decision rule for dependence is when K 2 &gt; 1 .
This score only considers the pr obability of the data (th e sample) given each one of the two possible hypotheses. But, it can be seen as a Bayesian proce dure in which we use the pos terior probabilities of dependence-independencegiven the data, under a prio r probability of 0.5 for both of them, as in this case K 2 I and K 2 D are proportional to the posteri or probab ility of inde pendence and dependence, given the data.
 This measure can be used to score general Bayesian networks, as in fact it is a specialization of the K2 measure proposed by Cooper and Herskovits [9]. One of the main criticism to this rule is that its value can depend on the order of the variables. This is due to the fact that if we change the order of the variables the assumptions about the prior distributions about the parameters are not consistent. To avoid this problem, Heckerman et al. [11] proposed the so called Bayesian Dirichlet equivalent scores . The main question is about the parameters of the Dirichlet prior densities (Beta densities in our case as the variables have two possible values). Simple examples of these measures to score Bayesian networks can be obtained by assuming a global parameters S and then to assume that each conditional density has parameters that are equal to S divided by the product of the number of cases of the variable and the number of con fi gurations of parents variables. In our case with two binary variables, for a value of the parameter S , we obtain the following scores for independence and dependence:
The value BS = BSD/BSI is the measure of the degree of dependence. If it is greater than 1 we will consider than X and Y are dependent. In our experiments, we will consider three scores BS 0 . 02 ,BS 2 and BS 16 corresponding to values of S =0 . 02 ,S =2 and S =16 for the BS measure, respectively.
This score depends of the parameter S . The value of this parameter is very important to decide for dependence-independence. It has been shown [24] that for large values of S there is a tendency to decide for dependence and for small values of S a tendency to decide for independence. This behavior can be very extreme and, for example, if S is very large, then the result of the decision is almost always dependence and the information content of the data is neglected. For this reason we have chosen some variability in the values of the parameters.

Another important principle that has been used to determine the complexity of a model is the minimum description length principle [21]. This has been applied to score general probabilistic models [23] giving rise to the so called Bayesian information criterion. It has two components, one measures the fi tting of the model to the data and the other penalizes the complexity of the model. In our particular case, we can obtain a measure by considering the difference of the score of the full model and the score of the model with independence. The fi nal expression is:
This model can be also obtained as a Bayesian criterion by selecting some particular prior distribu-tion [15].

In [17] it has been introduced a score based on the theory of imprecise probability [25] and more concretely on the imprecise Dirichlet model. This model is used to compute the probabilities of a cate-Dirichlet model with parameter (equivalent sample size) S . With this, we assume as prior information expectation of p ( z i ) is a probability interval which is obtained by computing all the posterior expected which Z = z i in the sample, then the interval for p ( z i ) will be equal to:
In our case, we will use a value of S =1 . Reasons for it are given in [26], but any other positive values is also possible. One important thing is that intervals are wider if the sample size is smaller. So this method produces more precise intervals as N increases.

The entropy of this set of intervals will be measured as the maximum of the entropy of all probability p ( If l is the number of elements of A , then the distribution with maximum entropy is p  X  ,where p  X  ( z i )= information/uncertainty measure. A set of justi fi cations for its use in general credal sets and tools for its calculus can be found in [4 X 6,1,3]. As the intervals are wider with smaller sample sizes, then we will have a tendency to obtain greater values of maximum entropy with smaller sample sizes. It also increases with the number of possible values of variable Z (higher increasing of entropy with respect to the traditional point estimation) especially with very small samples as then 1 / ( N +1) of probability will be uniformly distributed between several cases.

If to determine the values of Z we only consider the part of the sample for which another variable, U takes a value u , then the value of upper entropy will be denoted by H  X  ( Z | U = u ) .
The basic intuition of the score is to consider the two cases: independence and dependence. In the case of independence we apply the imprecise probability model to X and Y obtaining the entropy of the global model as IPI = H  X  ( X )+ H  X  ( Y ) . In the case of dependence of Y from X , we apply the imprecise probability model to X and to each one of the cond itional proba bilitie s about Y ,given X =0 and given X =1 . In this stage we could compute the upper entropy of all the distributions that can be computational problem (at least for the general case of more than two variables) and we have considered as upper entropy of the dependence case the value:
Finally the imprecise probability score is the upper entropy of inde pendence minus the upper entropy of dependence: IMP = IPI  X  IDP = H  X  ( Y )  X  i  X  p ( i ) .H  X  ( Y | X = i ) . We decide for dependence if IMP &gt; 0 and for independence in other case. This score, resembles the mutual information degree of dependence, but it can be lower than 0. The basic idea is to measure the uncertainty under dependence and independence and then to chose the situation with lower uncertainty.

This measure can be extended to score general Bayesian networks. However, it is not symmetrical in the variables. The fi nal value of IMP depends of the order of the variables. This is due to the fact that assuming a global imprecise Dirichlet model for X and for all the conditional distributions of Y given X is not equivalent to assume it for Y and for the conditional distributions of X given Y . 3. The new imprecise score measure
The new score is based on Bayesian equivalent score and the imprecise Dirichlet model. Instead of an equal distributing of the S value among all the possible elements, it considers a family of parameter values for a S value and test whether independence dominates dependence for all of them, or vice versa. For the sake of simplicity, we introduce it for the particular case of two binary variables, but its generalization to more cases per variable is immediate.

We assume that we have a fi xed S value. Then we consider that the prior information about p ( i, j ) is The probability of the data (marginal likeli hood), under this prior information is given by: where  X  X ( i )= j  X  ( i, j ) .

Under independence, we assume that p (0 | i )= p (1 | i ) in the prior information and then the probability of the data under the resulting distribution is given by where  X  Y ( j )= i  X  ( i, j ) .

Considering a set  X   X  X   X  |  X  ( i, j ) &gt; 0 , i,j  X  ( i, j )= S } , the new interval-valued score can be de fi ned as:
We assume that dependence dominates if BS  X  &gt; 1 , independence dominates when BS  X  &lt; 1 and there is no dominance when 1  X  [ BS  X  ,BS  X  ] . This agrees with the usual dominance for imprecise probability [25] under strict preference taking as bas is the posterior probab ility of ha ving dependence or independence (considering that the prior probability is 0.5 for both). This criterion will be denoted as BSDOM .
 the imprecise Dirichlet model considered in [26]. The reason is that, with the exception of some trivial cases, we can make BSD  X  /BSI  X  as small as we want (by taking a very small  X  ( i, j ) , whereas  X  X ( i ) and  X  Y ( j ) are not so small, when at least in one observation we have X = i, Y = j ). Then the lower limit of the interval would always approach 0 and dependence would never dominate. In [18] the full imprecise Dirichlet model was criticized for being to uninformative in some situations. Then, it was proposed a modi fi cation of it with a more restrictive set  X  . This model was called the bounded imprecise Dirichlet model This model can be seen as splitting the S value in two parts S = S 1 + S 2 .Then S 1 is uniformly distributed and we consider all the possible parameters for S 2 value. Being more speci fi c, if  X  where the addition of vectors is pointwise additi on. All the experiments in this paper corresponds to S 1 = S 2 = S/ 2 and S =2 . We do not know any direct method to compute the upper and lower extremes of the interval, BS  X  and upper and lower values of the interval will change (we are optimizing a function for which we have not security that the optimum is obtained in an extreme point). However, we will keep only the Dirichlet prior distributions (we consider a non convex prior information). In this set, we have carried out an approximate computation. 2 To compute the lower extreme, we have selected the parameters  X  2 trying to favor independence as much as possible. We have considered the following two possible selections for  X  2 (  X  is computed as  X  1 +  X  2 ):
The fi rst strategy tries to make the conditional distributions of Y given X as uniform as possible. The second tries to makes the conditional distributions as similar as possible to the marginal one. They are similar, but they do not always assign parameters in the same way. For both cases, we compute the values BSD  X  /BSI  X  , and take the minimum of them.
 To compute the upper interval limit, we also consider two parameters but trying to favor dependence. We have also considered two vectors of parameters, computing the maximum of BSD  X  /BSI  X  for both vectors of them: 1. For each value i =0 , 1 ,if n ( i, j ) n ( i, j ) , then assign  X  2 ( i, j )= S 2 / 2 , X  2 ( i, j )=0 . The strategy is dual of the strategy for the lower limit.

One of the criteria we are going to test in the experiments, is to consider independence if BS  X  1 and dependence otherwise. This criterion always makes a decision and follows the intuitive idea of selecting independence except when we have evidence enough favoring dependence. It is similar to frequentist tests of hypothesis, where the null hypothesis is accepted except if we have evidence against it. In our case, if the data do not decide for independence or dependence, then we should prefer the simpler model that does not assume the existence of a relation between the two variables. However, as we will see it will have better asymptotic properties than classical statistical tests. 4. Experiments
We have carried out two series of experiments. In both of them, we have simulated 10000 joint proba-bility distributions for ( X,Y ) with dependence and 10000 in which X and Y are independent. To obtain the probabilities we have followed Dirichlet distributions. In the two cases, the procedure has been the same with the only difference of S value. We have considered the case of S =2 and S =8 . In the depen-dence case the probab ilities are randomly simula ted according to a Dirichlet d istribution of parameters (
S/ 4 ,S/ 4 ,S/ 4 ,S/ 4) . In the independence case, we follow a similar procedure to obtain the marginal distributions (simulated according to a Beta of parameters ( S/ 2 ,S/ 2) ) and then the joint probability is computed as product of the marginal distributions.

For each one of the distributions, we have simulated samples with sizes: 3, 5, 10, 20, 50, 100, 1000, 10000. We are used very small samples too cause we consider that it is a particularly important case. For example, in Bayesian networks learning, if we have a large sample and we are going to determine a model for the data, being its complexity dependent of the amount of data, some crucial decision are even generally done with small samples.

Then we have tried to determine from each sample the existence of dependence or independence of variables X and Y . For each one of the scores we have measured the number of errors in recover-ing dependence-independence relationships (for the dominance criterion, BSDOM ,wealsogivethe number of cases in which there is a decision). But, to evaluate their use in classi fi cation, we have also computed the average of the expected log-likelihood of the estimated probability of Y given X with respect to the true pr obability distribution, according to the following expressions: 1. If deciding for dependence i,j p ( i, j )log p  X  ( j | i ) 2. If deciding for independence j p ( j )log p  X  ( j ) ( j )= n Y ( j )+1 N +2 . What we do is to use the expected log-likelihood score to determine how good is the procedure in determining about dependence or independence in the following way: if the method decides for dependence, then the conditional distribution of Y given X is estimated, however if the method decides independence, only the marginal of Y is estimated without using the value of X ; then, the chosen distribution is scored with the expected log likelihood values of the distribution using for the expectation the true probability generating the data p .

As larger is this value the better is the method for deciding whether X is useful to estimate the proba-bility of Y .

We report fi rst and with more detail the results for S =2 . In this case, we follow exactly the same hypotheses to generate the distributions that are assumed by BS 2 score, so we are in the ideal situation for this score, and it should outperform the other ones. The tables contain the results for all the methods to decide about independence we have introduced and another score BS 2 MOD which is equal to BS 2 , but modifying the prior distribution for independence  X  dependence. In BS 2 it was 1/2 for each one of the two possibilities. In BS 2 MOD we have given more prior information to independence, to the point of obtaining exactly the same behavior of BS  X  for small samples (3 and 5). The number of errors can be seen in Table 1 for the case of independence between X and Y ; and in Table 2 in the case of distributions generated with true dependence between those variables. BSDOM criterion includes the number of decisions between parentheses.

Tables 3 and 4 show the average expected log-likelihood for the cases of dependence and indepen-dence, respectively (in the case of dominance this average is only for the cases in which there is a decision).

From Tables 1 X 4, we have obtained Tables 5 and 6 re spectively, where we g ive the add ition of the errors and the average log-likelihood for independence and dependence together.

The results for S =8 are given in Tables 7 and 8, but integrating the cases of dependent and indepen-dent distributions. samples generated with S =2 (their average are presented in Tables 5 and 6), for each sample size. Similarly, we can see in Table 10, the results about the t-test carried out on the results obtained from the samples generated with S =8 (their average are presented in Tables 7 and 8), for each sample size. In these tables, we compare the score BS 2 with each other via the number of errors and the log-likelihood measure. We note that we have not used the BSDOM score here because it produces some cases without a decision (the dominance criterion used does not decide for one alternative). 5. Discussion
We summarize our analysis of experiments results in the following points:  X  First we highlight an important property of the procedure based on BS  X  . With small sample sizes  X  There is no procedure which is better than the others ones in all the situations, though Bayesian score  X  In general, a smaller number of errors implies a bigger log-likelihood, but this is not always the case.  X  In order to test whether BS  X  behavior can be obtained with a pure Bayesian procedure, we have  X  Procedures IMP and CHI makes too many errors for large sample sizes, especially by no de- X  Dominance criterion makes a lower percentage of errors than methods that make decisions in all  X  If we compare Bayesian procedures (including K2 and BIC) with BS  X  we see that even with these  X  With larger S values in the Bayesian equivalent criteria, we have more tendency to favor depen- X  The results of the tests in Tables 9 and 10 support the comments above mentioned. We can see 6. Conclusions
In this paper we have carried out an empirical comparison of several criteria for deciding independence and introduced a new method based on the theory of imprecise probability. Perhaps the most important conclusion is that no single method outperforms the others. The fi nal criterion should be chosen as a function of the objective (deciding independence or classi fi cation) and the sample size.
But, the main conclusion of this paper is a new procedure to decide for dependence-independence that for small sample sizes always considers independence and that for large sample sizes is similar to Bayesian procedures. This method can be used when we want to determine the dependence relationships that can be found in a set of data, but we really only want relationships with real support, avoiding spurious relationships.

Another important aspect for future work is the integration of expert knowledge when deciding about independence. In [16] an interactive procedure has been proposed to learn a Bayesian network. This method is based on independence tests in which the expert is inquired when there is a doubt about the result, i.e. the data do not give a clear preference for one of the options. The procedure based on dominance can provide a basis for asking the experts: when there is no option dominating the other one.
Also, the new score can be used in procedures as the one of Abell X n et al. [2] where relations of dependency among features need to be discover to increase the accuracy of a classi fi er. Acknowledgements This work has been supported by the Spanish  X  X onsejer X a de Econom X a, Innovaci X n y Ciencia de la Junta de Andaluc X a X  and  X  X inisterio de Educaci X n y Ciencia X , under Projects TIC-06016 and TIN2010-20900-C04-01.
 This paper is an improved version of a paper presented in Isipta X 05 congress.
 References
