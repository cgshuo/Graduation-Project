 The Web originates for enabling users the information sharing power. As evolv-ing, it has been enriched to provide more fl exible ways to link different sources, one of which is to use web services, a set of related functionalities that can be programmatically accessed through the web [16], usually via XML-based pro-tocols. Web service composition, aimin g at composing distributed web services to carry out complex business processes, is considered to be a promising area to revolutionize the collaboration among heterogeneous applications[3]. Due to the constantly changing environment and the large scalability of the Web, the research focus on web service composition is moving from static to dynamic technology, known as Automatic Service Composition (ASC) [15]. The software programs that provide ASC functionalities are termed ASC agents. An ASC agent has to compose several web services to deliver results, which brings them a few particular characteristics that are different from single-functionality web services.

Researchers has realized the significan ce of trust in the web service environ-ment. Methods, models and frameworks ar e proposed to help service consumers identify trustworthy web services [17]. H owever, determining the trustworthiness for ASC agents is left as an open issue. We adopt the major consideration in [6] by defining the trustworthiness of ASC agents to be a two-layer structure, namely composition-layer and service-layer. This caters to the unique character-istics owned by ASC agents that their deliveries depend on not only their own activities, but also the single-functionality web services they compose.
The previous work did not give a complete investigation on how to use QoS factors to represent the trustworthiness of an ASC agent. The conducted ex-periment cannot reflect the effectivenes s of the proposed model. In this paper, we will thoroughly explore the relationships between trustworthiness and QoS factors. A proper set of indicators, based on QoS factors, will be constructed that can better present ASC X  X  trustworthiness. We will also use experiment to prove that our proposed metric shows better result than result of the forecast simply using historical data. The contributions of this paper are:  X  We explore the relationships between trustworthiness QoS factors for ASC  X  We design a metric of Trustworthiness Status Indicators (TSIs) that can be  X  We provide a mathematic method to transfer the multi-dimensional TSI The rest of the paper is structured as follows. In Section 2, we explore the rela-tionship between the trustworthiness of ASC agent and QoS factors. In Section 3, we present trustworthiness metric in detail. A method to integrate TSIs for trustworthiness comparison is introdu ced in Section 4. A simulation experiment is presented in Section 5. Section 6 give s the recent related works and Section 7 summarizes the paper and looks at the future work. In this section, we clarify the concep t of trust and trustworthiness. We will also discuss the commonly agreed dimensions of trustworthiness and what QoS factors can be used to represent tho se trustworthiness dimensions. 2.1 Trust and Trustworthiness Trust and trustworthiness are different. Trust is the willingness of a trustor to believe that the trustee will behave as expected. This willingness makes the trustor confident to be vulnerable to some trustee X  X  behaviors. Different from trust, trustworthiness is a trustee X  X  attribute to represent the extent that it can be trusted [5]. In other words, a trustor gives its trust to a trustee based on the trustworthiness of the trustee. Strictly sp eaking, trust is a mental activity. People are not necessarily to give their trust base d on trustworthiness. In reality, there are people that irrationally trust others without any reasons. However, rational people give their attitude based on gathered information on trustworthiness.
In this paper, the ASC requesters are trustors and the ASC agents are the trustees. Instead of directly modeling t rust, we design a metric to represent the trustworthiness of ASC agents, which has impact on the trust from requesters. 2.2 Trustworthiness Dimensions and QoS Factors In [11,9], three dimensions of trustworthiness were studied. They are ability, benevolence and integrity. This taxonomy is agreed by most researcher since proposed, for example in [4,5,12]. Since the process of automatic service com-position is entirely computer-controlled and a computer software does not have emotions to act altruistically or kindheartedly, it makes no senses to measure the dimension of benevolence in this paper. The concept of trustworthiness only covers the first two dimensions: ability and integrity.

As we mentioned, ASC agents own the speciality that their deliverables are based on their composition ability as well as the single-functionality web service they use. When we investigate the trustworthiness of an ASC agent, we have different concerns on these two sources. At composition layer, people implicitly make an assumption that they believe the ASC agent has the ability to complete the required tasks. What they concern is t hat whether their actual performance can be really as good as what they claim to be. In other words, people concerns the deviation between the actual action wit h the advertised one. At single service layer, people know that even the ASC agent cannot control the single services, because they are provided by third par ties. So people concern whether an ASC agent have enough qualified resources to compose. The selected QoS factor at service layer should be able to reflect th e general conditions of the resource pool maintained by the ASC agents.

Based on the above discussion, we design the following trustworthiness metric consisting of 4 aspect at each layer. Each aspect is represented by a Trustworthi-ness Status Indicator (TSI) to distinguish it from the basic corresponding QoS. At composition layer, we have: 1)Functi onal Effectiveness Deviation, 2)Execu-tion Efficiency Deviation, 3)Availability Deviation, and 4)Reputation. At service layer, another four TSIs are taken into account: 1)Average Execution Duration, 2)Average Availability, 3)Average Reliability, and 4)Service Monitoring. Please note that at composition layer, we concer n  X  X eviations X  between the promises and the actual performance. At service layer, we concern the general quality of the maintained resource pool. In the next section, we present all the TSIs in detail. 3.1 Composition-Layer TSIs Functional Effectiveness Deviation ( T fed ). This TSI represents the sta-tus of functional effectiveness. To define th e functional effectiveness deviation, we first need to define functional error rate. Functional error rate is the pro-portion of requests answered with functional errors over the total number of requests. Given an ASC agent c and a certain period du , it is calculated as  X  and N total is the total number of composition r equests. The functional effective-ness deviation T fed ( c, du ) represents the difference between the actual functional error rate  X  a ( c, du ) and the claimed one  X  c ( c, du ) within that duration. It is cal-be monitored by consumers or a third party and  X  c ( c, du ) is claimed by the ASC agent itself. The lower the T fed ( c, du ) is, the more trustworthy the ASC agent is. In general, this TSI reflects how tr ustworthy the ASC agent is in the aspect of functional capability.
 Execution Efficiency Deviation ( T eed ). This TSI indicates the status of the execution efficiency of an ASC agent. For an ASC agent c within duration du ,the execution efficiency deviation T eed ( c, du ) is the difference between the claimed on-time response rate  X  c ( c, du ) and the actual on-time response rate  X  a ( c, du ). On-time response rate  X  is the proportion of on-time response times over the to-tal times an ASC agent should response, i.e.,  X  = N ontime N efficiency deviation stands for how trus tworthy the ASC agent is in the aspect of on-time response.
 Availability Deviation ( T ad ). In [14,17], availability is defined to be the pro-portion of the available duration in the total duration. Based on those defini-tions, we define availability deviation T ad ( c, du ) to be the difference between the claimed availability and the actual availability for ASC agent c in duration du . It is difficult to make real-time availability monitoring of an ASC agent. One possible solution is to change this  X  X eal-time X  continuous monitoring to randomly discrete monitoring. For every time the ASC agent is called by con-sumers, its availability information is recorded. After a certain period of time du , the actual availability of an ASC agent c can be represented by the propor-tion of the available times in the total accessing times, i.e.,  X  = N available N we use the following formula to calculate availability deviation: T ad ( c, du )= max { 0 , X  c ( c, du )  X   X  a ( c, du ) } . Same as the above two TSIs, the lower the avail-ability deviation is, the more trustworthy the ASC agent is.
 Public Reputation ( T pr ). The public reputation of an ASC agent is a mea-sure of its trustworthiness from public opinions. There is no doubt that in an open environment, different people X  X  opinions will impact each other. Differ-ent consumers may have different opinions on the same ASC agent. Our public reputation is defined as the average ranking scored by consumers. This can be represented by the following formula: T pr ( c, du )= n i =1  X  i n ,where  X  i is the score of ranking given by consumer i in the last duration du and n is total number of consumers who give scores in that duration. To simplify the calculation in our framework, we define the range of ranking score is [0,1]. Therefore, the range of T pr ( c, du ) is also [0,1]. Different from the above three TSIs, the higher the public reputation is, the more trustworthy the ASC agent is.

The above four TSIs are composition-layer TSIs that reflect the trustwor-thiness of an ASC agent itself. In the next sub-section, we will introduce sev-eral service-layer TSIs which reflects the status of candidate elementary services maintained by an ASC agent. Their properties also impacts the final deliverables of ASC agents and therefore, im pact the trust consumers give. 3.2 Service-Layer TSIs Average Execution Duration ( T aed ). GivenanASCagent c , the average execution duration T aed ( c, du ) is the average processing time of the elementary services that ASC agent c calls during the last duration du , i.e., T aed ( c, du )= tary service and n is the total number of calls in duration du . To simplify the future calculation, we would like to normalize t i into range [0,1] by giving a value of maximum tolerable time t max , which is the largest execution time that consumers would like to tolerate, and a minimum satisfactory time t min ,which is the execution time that consumers are 100% satisfied with (consumers will not get more satisfaction even the web service is faster than that). These two reference values can be determined by consumer surveys. Here they are treated as environmental constants. If the average execution duration exceeds t max ,the score from this aspect deserves 1 point. If it is lower than t min ,thescoreis0.Ifit is in between, we use the following formula to calculate t i : Norm ( t i )= t i  X  t min t and the final value is ranging from [0,1]. Therefore, T aed will be in  X  X he-less-the better X  family. In general, this TSI repres ents how fast generally the elementary services can complete their tasks. The les s it is, the better the overall quality of elementary services is.
 Average Service Availability ( T asa ). Average service availability T asa ( c, du ) is the average value of the availability of each elementary service maintained in ASC agent c in the last duration du , i.e., T asa ( c, du )= n i =1 Avai i n ,where Avai i is the availability of the ith elementary service and n is the total number of ele-mentary service. The definition of the availability of a single elementary service can be referred to [14]. The higher the average service availability is, the better the quality of elementary services is. Generally, average service availability re-flects the overall quality of elementary services in the aspect of on-line available duration.
 Average Service Reliability ( T asr ). Reliability is defined as the probability that a request is correctly responded w ithin the maximum expected time frame indicated in the web service description. Therefore, the reliability of service s in duration du can be calculated as Rel ( s, du )= n resp n ber of correct responses and n total is the number in total. Our average service reliability T asr ( c, du ) is just the average value of all the maintained elementary number the elementary services in the service pool of ASC agent c .Sameasav-erage service availability, the higher the average service reliability is, the better the quality of elementary services is. Average service reliability stands for the overall quality in the aspect of correctly on-time responding.
 Monitoring Strength ( T ms ). Monitoring strength stands for the strength that elementary services are monitored by AS C agents or a third party. It is reason-able to think that if elementary services are monitored regula rly, their services will be of consistent quality. Two factors are considered to have impact on the monitoring strength: monitoring coverage mc and monitoring frequency mf . Monitoring coverage is the proportion of the monitored services over all the ser-vices. Its range is [0,1]. Monitoring frequency stands for how often the monitoring session is. To simplify the calculation, we normalize monitoring frequency by giv-ing satisfactory monitoring frequency smf as a reference so that the range of mf becomes [0,1]. The satisfactory monitoring frequency is a frequency in which if elementary services are m onitored, consumers will consider the monitoring to be satisfactory. This reference value can be decided by conducting consumer sur-vey. Once it is decided, it keeps constant and seldom changes. For a certain ASC agent c in duration du , we use the following formula to calculate Monitoring Strength: T ms ( c, du )= n i =1 mc i n  X  Min { mf smf , 1 } ,where mc i is the monitoring coverage of the ith monitoring process. The higher the monitoring strength is, the better the quality that it represents is.

In summary, we proposed several compos ition-layer and service-layer TSIs, which form a comprehensive metric, refl ecting the trustworthiness status of an ASC agent. As mentioned before, it is impossible to give complete metric that shows every aspects of a composer, since th e properties are countless and different contexts may concern different things. Ou r metric is extensibl esothatconsumers can add or remove TSIs as they preferred, as far as the new added ones can be scaled into the range [0,1]. In the next part, we will integrate all the TSIs together to form a single value through which the trustworthiness of ASC agents can be comprehensively distinguished. The trustworthiness of ASC agent c in the last duration du can be represented T asa ( c, du ) ,T asr ( c, du ) ,T ms ( c, du )). To simplify the representation, the eight TSIs are numbered from 1 to 8 in the order as shown. Then, the above vec-tor can be re-written as { T j ( c, du ) | j =1 , 2 , ..., 8 } . Given an ASC agent c i ,the jth TSI TSI i,j can be represented as TSI i,j = T j ( c i ,du ). We adopt simple addi-tive weighting technique to help transfer the multi-dimensional TSI metric into a single value. This technique has two phases: normalization and weighting. 4.1 Normalization Phase The eight TSIs can be grouped into two, as four of them (No. 4,6,7,8) are  X  X he bigger the better X  and the rest are entirely opposite. The normalization processes for these two types of TSIs are different. Given n ASC agents in total, the ith TSI of the jth ASC agent can be normalized by the following formula: In the above formula, TSI max i and TSI min i are respectively the highest and low-est values of the ith TSI of all the ASC agents, i.e., TSI max i = Max ( TSI i,j ) , 1  X  j  X  n and TSI min we obtain a 8  X  n matrix V N , where each column stands for one TSI dimension and each row represents an ASC agent. 4.2 Weighting Phase Even though we can normalize the TSI values of all the ASC agents into one matrix, we still cannot judge which ASC agent is more trustworthy, since one ASC agent may be better in some TSIs and the other may be better in other TSIs. This problem is related to how important each TSI is in the context that the comparison occurs. There are no fixed rules saying what TSI is the most important one and what is the second one. The significance of each TSI depends on user preference.
 In our trustworthiness comparison method, users are given an 1  X  8matrix W =( w j | 1  X  j  X  8 , 0  X  w j  X  1 , 8 j =1 w j = 1) to allocate weight values for each TSI, where element w j is the weight value for the jth TSI we described before. Givenan8  X  n normalized TSI matrix V N =( v N i,j | 1  X  i  X  n, 1  X  j  X  8), by applying the following formula, an 1  X  n matrix T will be achieved, in which each value T i stands for trustworthiness of an ASC agent.
 The above formula results in nT j , each of which is the final normalized trust-worthiness for the jth ASC agent. That value is ranging in [0,1]. The higher the value is, the more trustworthy the ASC agent is. Please note that if users only want to calculate an absolute trustworthiness value for an ASC agent (no comparison), they can directly multiple weight values with the result calculated from the formulas given in TSI description. In that case, normalization is not necessary.
 As we mentioned, it is very difficult to know which TSI is more important. However, there are some general rules that can be applied to any contexts. The first rule is effort-oriented division, which means that we put more weight to where the major computation jobs are done. If you know most of your work will be done by elementary services an d the composer is just for integrating the results, you should put more weight on service-layer TSIs, and vice versa. The second rule is difficulty-driven. We put more weight to the TSIs that are hard to achieve. For example, if you have a massive computation task in which computation logic is very simple but the amount is quite large, rather than putting too much weight on the TSIs reflecting the ability factors, you should give more attention on the quality fac tors such as execution efficiency. If the computation logic is very complex, you should focus on ability factors such as Functional Effectiveness Deviation. The third rule is concern-driven. If you are quite keen in certain aspects, e.g., in s ome time-crucial cases where even one microsecond determines everything, you should definitely put the highest weight to the corresponding factors. Again, our metric is open and it accepts any new TSIs as its member, as far as they can be scaled to [0,1]. Users are allowed to create their own TSIs ba sedontheircontexts.
 In this section, we present a simulation experiment to illustrate the effectiveness of our proposed TSI metric. We want to test that given several ASC agents with the same claimed capabilities status and different actual capabilities, the higher their calculated trustworthiness values are, the better the actual Eligible Return Rate (ERR) they give. Here ERR stands for the proportion of the eli-gible returned results over all the results. The eligible returns are the on-time return with valid composition results. An ASC agent giving an eligible return means it successfully delivered its job without any flaws. We use this indicator to represent the actual level of ASC agents. The whole experiment is conducted under Microsoft Office Excel 2007 Profession al with Visual Basic for Application (VBA) as the programming language.

Suppose that there is a business process that needs to composing three types of elementary web services. For each types o f function, there are 500 elementary web services available on the web. Their availability follows the normal distribution with mean value of 90% and variance of 10%. Same probability distributions are also followed by their reliability with mean value of 85% and variance of 10% and by actual execution duration with mean value of 20 milliseconds and variance of 5 milliseconds. ASC agents regard web services that can return results in 10ms as perfect and the web services that ex ceed 35ms to return as n on-tolerable.
There are 10 available ASC agents which claim to offer the corresponding composition service. They claim to pro vide results with 95% score for TSI 1, 2 and 3 and 100% for TSI 8. However, thei r actual capabilities are shown in the above table. Except the MC and MF data, the rest data in the table are just the average values. The performance of ASC agents follows a normal distribu-tion with the mean values given in the table and 1% variance for availability, 5 milliseconds variance for execution dur ation and 0.5% for error rate. 200ms is the longest duration consumers would like to wait for one composition job. Each ASC agent maintain their own elementary service pool that consists of 60 web services in total with 20 for each functi on. Those services are randomly selected from the web. The ideal monitoring strength in our experiment is suppose to check service pool every 10 compositions with coverage 100%.
 We simulate the composition processes for each ASC agents for 300 rounds. In each round, for a certain ASC agent, we first check whether it is available by comparing a even-distribution random number with its availability that follows normal distribution. Then we use same method to check whether the composition has errors. After that, the ASC agent has to select three elementary web service with different functionalities. If a sel ected web service cannot work properly (not available, not reliable or execution duration is too long), ASC agent has to select another elementary service with the same functionality. The final execution duration of ASC agent is calculated as D asc = n i =1 T ws i +( n  X  2)  X  T asc ,where n is the total times that the WSC interact with different web services, T ws i is the execution duration of ith web service and T asc is the duration. If the final total execution duration is greater than 200ms, the composition is regarded as not efficient. Until now, we finish calculation of the composition-layer TSIs. The calculation of the first three service-layer TSIs is done based on the current services maintained in each ASC agent. T hen based on the monitoring frequency, we decide whether a monitoring process must be started after the current round. If a monitoring process has to be conducted, we will randomly select several web services for each function type according to monitoring coverage and check their availability, reliability and execution duration. The web services that give ineligible results will be removed. In those cases, new services whose quality is better than the removed ones will be added in from the web. A web service that was once selected by a certain ASC agent is not allowed to be added again. After each round, a trustworthiness value (equal weight for TSIs) and a ERR value of each ASC agent are calculated based on the previous composition records. The results are shown in Fig.1.

Fig.1(a) and (b) give the general evolution curves of trustworthiness values and eligible returning rate (ERR) for each ASC agent. In general, as we ex-pected, the better actual capability an ASC agent has, the better its ERR and trustworthiness are. This shows that our calculated trustworthiness result effec-tively reflect the actual performance. We note that at the first several rounds, the trustworthiness curves and ERR curves fluctuate drastically and can not reflect the actual capability status. This is because the performance of the ASC agents and elementary services are of random nature (normal distributions in the settings). As the experiment goes, our calculated trustworthiness values be-come stable much faster than ERR. It te lls that our selected TSIs and trust-worthiness calculation method have better sensitivity of reflecting the actual performance than ERR which purely reflects capability by following previous historical records. This is more clear in Fig.1(c) and Fig.1(d). (c) give the trust-worthiness and ERR status after 50 rounds. At that time ERR was still fluc-tuating, while trustworthiness values had already correctly shown the actual capability comparison. In Fig.1(d) after 150 rounds when ERR gives general ca-pability status, our trustworthiness values are able to present clear and precise differences among the ASC agents. Research on trust in the online environment has been studied mainly in the information systems domain and computer science domain.

In the information systems domain, people concerns the fundamental concep-tualization and the implication of each concepts. The seminal work that clarify the subtilty includes [9,4,12,5]. There is a branch of trust research which focuses on analyzing the institutional trust elements and their functions [18,10,13]. Re-cently, some brain visualization technol ogies are used to analy ze trust reactions [8,2]. Basically, the trust research in inf ormation systems focuses on exploring the nature and roles of trust and use different methods to analyze the impact of people X  X  trustworthy and untrustworthy behaviors. Their research is valuable in guiding people and organizations to form trust relationships.

Researchers from computer science paid more attention on trust modeling and trust framework construction. There have been some very good survey papers in this field [17,7,1]. Details cannot be listed due to the limit of the required page size. In this paper, we clarified several trust-r elated concepts and e xplore the dimen-sions, manifestations and formation of trustworthiness. Based on the special characteristics of automatic web service composition, we proposed a combined composition-layer and service-layer trust status indicator metric to calculate the trustworthiness of ASC agents. For comparison purposes, we use simple additive weight method to transfer the multi-dimensional TSI metric into a single trust-worthiness value. We use experimental simulation to prove that our proposed metric and the comparison method can eff ectively differentiate the trustworthi-ness of ASC agents. They also present a better performance than the method that purely use historical data to judge trustworthiness.

Our next step will focus on implementing the proposed structure into a soft-ware framework and doing experiments with real data to determine the reason-able weights for TSIs under different situations. Exploring new service-layer and composition-layer trustworthiness factors to enrich the framework is also a job on our schedule.
