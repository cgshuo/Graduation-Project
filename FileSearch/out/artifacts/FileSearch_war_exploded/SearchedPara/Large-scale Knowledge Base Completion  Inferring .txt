 Constructing large-scale knowledge bases has attracted much attention in recent years, for which Knowledge Base Com-pletion (KBC) is a key technique. In general, inferring new facts in a large-scale knowledge base is not a trivial task. The large number of inferred candidate facts has resulted in the failure of the majority of previous approaches. Inference approaches can achieve high precision for formulas that are accurate, but they are required to infer candidate instances one by one, and extremely large candidate sets bog them down in expensive calculations. In contrast, the existing embedding-based methods can easily calculate similarity-based scores for each candidate instance as opposed to using inference, so they can handle large-scale data. However, this type of method does not consider explicit logical semantics and usually has unsatisfactory precision. To resolve the limi-tations of the above two types of methods, we propose an ap-proach through Inferring via Grounding Network Sampling over Selected Instances. We rst employ an embedding-based model to make the instance selection and generate much smaller candidate sets for subsequent fact inference, which not only narrows the candidate sets but also lters out part of the noise instances. Then, we only make infer-ences within these candidate sets by running a data-driven inference algorithm on the Markov Logic Network (MLN), which is called I n ferring via Grounding N et work S a mpling (INS). In this process, we especially incorporate the similar-ity priori generated by embedding-based models into INS to promote the inference precision. The experimental results show that our approach improved Hits@1 from 32.911% to 71.692% on the FB15K dataset and achieved much better AP@n evaluations than state-of-the-art methods.
 E.1 [ Data ]: DATA STRUCTURES| Graphs and networks Knowledge Base Completion; Embedding; Inference c  X 
Automatically extracting facts from texts and construct-ing large-scale knowledge bases (KB) have grown vigorously in recent years. As a result, several typical knowledge bases have been built, such as Freebase [2], Nell [6], YAGO [10], and Knowledge Vault [7]. However, these extracted reposi-tories are far from completion. To complete the constructed KBs, according to the conclusion in [7], using an existing knowledge base to complete itself is an important supple-ment for automatic knowledge extraction to increase the number of facts in KBs and cannot be substituted by other techniques. Therefore, this paper focuses on the large-scale knowledge base completion (KBC) and is committed to pre-dicting the missing links in the existing knowledge base.
In general, according to the process of KBC, there are two types of approaches: inference-based approaches and embedding-based approaches.

First, inference-based approaches [22, 16, 24] usually em-ploy logic formulas to infer the missing links among exist-ing entities in a KB. They manually or automatically con-struct various logic formulas and learn the weight of each formula by sampling or counting groundings from existing KBs. These weighted formulas are viewed as the long-range interactions across several relations. The biggest limitation of such approaches is the computation complexity. These methods need to infer knowledge one by one, which implies the computation complexity is linearly growing with the size of candidate sets. However, usually, there are extremely large candidate sets for some speci c relations in large-scale KBC, and in each candidate set, only one or a few are actu-ally correct. For example, Barack Obama's mother is miss-ing in a KB, and we need to nd out who Barack Obama's mother is. All persons or females in the KB are candidates, but only one is the correct selection. The huge candidate set brings inference-based approaches to an unacceptable run-ning time. Although some methods have avoided this issue through simple operations, such as only adding a small part of false facts to testing sets [13, 23], this strategy is too coarse to obtain precise inference results. On the other hand, there are some noise candidates, which may violate formulas and mislead inference algorithms. Therefore, existing inference methods that rely on formulas cannot remove the noises by themselves, and the noises may result in the decrease of the performance.

In contrast, embedding-based methods [21, 5, 12, 4, 3, 26, 18] are not a ected by huge candidate sets because they can easily calculate similarity-based scores for each candidate instance after learning representations of entities and rela-T able 1: Several embedding approach results on FB15K ti ons. Unfortunately, embedding-based approaches do not consider explicit logical semantics and cannot capture the interaction between di erent relations well enough. Most of them simply model the direct interaction between relations by entity embeddings, apart from the long-range interac-tion across several relations. Thus, their results are uni-versally unsatisfactory. Table 1 shows several typical em-bedding models' Hits@1 and Hits@10 evaluations 1 on the FB15K dataset [4]. Although many embedding-based meth-ods could obtain high ratings in Top N results (N &gt; 1), they usually have low performance in Top 1 results (the more useful metric than Top N in KBC).

To resolve the limitations of the above two types of meth-ods, we propose a novel approach by inferring via grounding network sampling over selected instances. We rst employ an embedding-based model to perform instances selection. In speci c, we employ TransE [4] to learn the representations of entities and relations in the KB. We calculate similarity scores between candidates and the input query, and we se-lect Top-N instances to constitute a new smaller candidate set for subsequent fact inference. In this way, we not only lter out a part of the noise but also improve the eciency of the inference algorithm. Meanwhile, the corresponding similarity scores are recorded and viewed as the prior to supervise the subsequent inference. We perform logical in-ference over selected instances by exploiting a data-driven inference algorithm on a Markov Logic Network (MLN), which is called Inferring via Grounding Network Sampling ( INS ). INS could consider the long-range interaction across several relations that were ignored in existing embedding-based methods. Moreover, we improve INS and propose a INS-ES algorithm (I nferri ng via ground N et work S a mpling Merging E m bedding S imi larity Priori), which could consider the probability of the transition between states when per-forming network sampling for inference. In this way, the recorded similarity scores can be incorporated naturally into our model and the inference process can be supervised in some way, which is bene cial for improving the inference precision. In general, our method is an comprehensive com-bination of two types of methods for knowledge inference, and it could not only avoid the disadvantage of inference-based methods (cannot handle large-scale knowledge bases) but also could improve the weakness of embedding-based methods (cannot obtain explicit logical semantics and can-not suciently capture the interaction between di erent re-lations). Experiments show that our approach has signi -cant improvement compared with state-of-the-art methods, increasing Hits@1 from 32.911% to 71.692% on the FB15K.
The main contributions of this work are summarized as follows.
Hit s@n means the proportion of correct entities ranked in the top N
Knowledge Base is a structural information storage sys-tem, e.g., Freebase, that usually contains entities, relations, and facts. A KB can be viewed as a directed graph G ( V; E; R ). The vertex set V contains all entities in KB, and the edge set E contains all facts. R is the set of relations that can be viewed as edge labels. We only consider binary relation in this paper. r ( h; t ) is an example of a fact, and there is an edge from h to t with label r .

Knowledge Base Completion is the prediction of addi-tional true facts using only an existing database[23]. Given a KB, noted as G ( V; E; R ), our goal is to predict a new fact set E  X  with high precision and recall, and V and R re-main unchanged. For example, 93.8% of persons included in Freebase have no place of birth, and 78.5% of them have no nationality[20]; KBC is committed to predicting them.
Embedding means representing each entity in KB as a low-dimension numeric vector, and di erent dimensions of the vector may implicitly represent di erent aspects of an en-tity. Relations in KB usually have relevant representations, such as vectors, matrixes and tensors. Entities interact un-der a speci c relation by performing arithmetical operations between entity embeddings and relation's representation.
The sketch of our approach's framework is shown in Fig-ure 1. In general, it contains two main parts: (a) selecting instances and (b) inferring instances.

Selecting instances ((a) in Figure 1) . First, we choose an embedding-based model to learn distributed representa-tions for both entities and relations in the existing knowledge base. For most of the existing embedding-based models, the objective is always to make r ( h; t ) in the KB more simi-lar than other r ( h; t  X  ) or r ( h  X  ; t ) generated randomly. For example, if BornIn(Barack Obama, Honolulu) exists in KB but BornIn(Barack Obama, Washington) does not, (Barack Obama, Honolulu) should be more similar than (Barack Obama, Washington) under the relation BornIn . Second, we employ learned representations of entities and relations to calculate similarity scores for each instance and sort them in descending order to pick the Top N to form a new smaller candidate set. To the query Mother(Barack Obama, ?) , there is an entity set X , and each x  X  X can replace ? in Mother(Barack Obama, ?) , where x can be any person or female. Mother(Barack Obama, X ) denotes the entire candidate set S , and we calculate all similarity scores for each Mother(Barack Obama, x ) to pick N instances with the highest similarity scores for subsequent fact inference.
Inferring instances ((b) in Figure 1) . In this com-ponent, we employ MLN to model KB and infer selected generated by embedding-based methods. instances on it. Before performing inference, we need to learn the structure and weights for MLN. Speci cally, we employ the method in [24] to learn structure and weights simultaneously by performing random walks. Then, we pro-pose an inferring algorithm, called Inferring via Ground Net-work Sampling or INS for short. INS performs random walks on the Partial Order Graph (POG) to generate paths and changes them to grounding formulas. For example, for Mother(Barack Obama, Ann Dunham) , we can obtain paths such as[ Barack Obama ]  X  [ Honolulu ]  X  [ Ann Dun-ham ] from random walks and turn it into a formula such as BornIn(Barack Obama,Honolulu)  X  LiveIn(Ann Dunham, Honolulu)  X  Mother(Barack Obama, Ann Dunham) . For each selected instance, we count formulas in which the in-stance occurs and treat counts of formulas as the instance features. Then, instance features are used to calculate prob-abilities for each instance. These probabilities can be viewed as nal predictions for outputting.

Furthermore, we replace the uniform transition probabil-ity in random walks with the function of similarities calcu-lated by the embedding model for the purpose of improv-ing precision ((c) in Figure 1). Originally, each state ad-jacent to the current state has an equal transition proba-bility, so the inference is completely independent from the previous instance selection. However, the similarity scores obtained from the embedding-based method should not only contribute in building candidate sets but should also be used in the following inferring process as a priori. The improved method employs a function from the similarity score to the transition probability, noted as p trans = f ( s sim ), where p trans means the probability of transition from one state to an adjacent state in random walks and s sim means the sim-ilarity score. For example, for Mother(Barack Obama, x ) , if an state contains a selected instance, e.g., Ann Dunham , we assign f ( s sim ( M other ( BarackObama; AnnDunham ))) to this state; otherwise, a small default probability is as-signed to the adjacent state.
To narrow candidate sets for subsequent fact inference, we make instance selection for each entire candidate set S , for example, Mother(Barack Obama, X ) . We propose employ-ing embedding-based methods to do selection and describe the process as follows.

We rst choose an existing embedding-based model to learn distributed representations for both entities and rela-tions in the KB. Almost all embedding-based models embed entities into a relatively low (e.g., 50) dimensional embed-ding vector space R k while representing relations in di er-ent ways. Therefore, for a speci c fact r ( h; t ), we repre-sent h and t as vectors E h and E t , respectively, and ab-stract the representation of r as E r . Meanwhile, di erent embedding-based models design di erent similarity-score-functions f s ( E h ; E t ; E r ) to measure the similarity of entities under a speci c representation of relations. For example, SE [5] represents a relation as two matrixes R lhs r  X  R k k R lowing the Gaussian kernel as the similarity-score-function; SME [3] represents a relation as a matrix, and its similarity-score-function is merged into a neural network; TransE [4] represents a relation as a vector with the same dimension as the entities and simply employs the negative 1-norm of the vector ( E h + E r  X  E t ).

Most of the embedding-based models design their objec-tives to make f s ( E h ; E t ; E r ) larger than other f or r ( h  X  ; t ) does not. Formally, we can de ne a uniform pair-wise loss function as follows: L = where [ x ] + denotes the positive part of x . KB  X  r ( h;t ) margin hyperparameter.

Speci cally, we employ TransE as the embedding-based model, which has been proved to be e ective for knowledge base embedding [4]. We mainly apply TransE to generate candidate sets for inference in our experiments. TransE [4] is a simple and practical model, and it embeds all entities and relations into the same k-dimensional vector space by viewing r in r ( h; t ) as a translation. TransE thinks E be the nearest neighbor of E h + E r in the same vector space and designs the similarity-score-function as f s ( E h ; E  X  X 
E h + E r  X  E t | . In essence, TransE replaces the complex matrix multiplications with a vector addition operation.
After the learning process, we obtain embeddings for en-tities and relations in KB that contain semantic similarities. For example, the embedding of Barack Obama is close to th e embedding of Ann Dunham under Mother relation. We calculate similarity scores for each candidate r ( h; t ) by em-ploying the similarity-score-function f s ( E h ; E t ; E is simple enough to be completed in a short running time. Then, we sort the candidate instances in descending order by their similarity scores and pick the Top N to form a new smaller candidate set. For example, for the query Who is Barack Obama's mother , we calculate all f s (Mother(Barack Obama, x)) and pick the Top 10 persons as a new candidate set. We only require that Ann Dunham (the true answer) be in the Top 10 regardless of its place, which can be han-dled suciently by the existing embedding-based methods. In this way, the subsequent logical inference would be con-strained by a smaller selected instance space, and the com-putational diculty of inference-based methods is avoided.
The network structure and logical interaction in KBs make it natural to apply MLN to model KBs. Performing infer-ence on MLN to predict missing links in KB is in accordance with KBC. In this section, we describe details of the infer-ence approaches based on MLN, which includes two parts: 1) using MLN to model the exiting knowledge base for the task of KBC; 2) inferring overselected instances obtained from embedding-based methods. In terms of inference-based methods based on MLN, the huge candidate set is not the only reason that may cause the expensive calculation. The vast facts and the large grounding space also cause the un-acceptable running time for both the learning and inferring process. In particular, learning structures for MLN su er from a severe scale issue [15, 14, 13, 19, 11], which leads to the result that MLN-based approaches are scarcely applied to the task of large-scale KBC. We propose an inferring al-gorithm via grounding network sampling, noted as INS. INS employs the learning method in [24] to learn structure and weights for MLN simultaneously and performs random walks on the Partial Order Graph (POG) [1] instead of the orig-inal KB, which treats paths as POG nodes. INS is able to take all Horn clauses with the query at the place of both head and body, which can capture more logical factors and cover more useful formulas. As the INS extension, INS-ES, short for I n ferring via ground N et work S am pling Merging E m bedding S imi larity Priori, takes advantage of similarity priori obtained from previous embedding-based models to achieve a further promotion.
To obtain new knowledge from the existing KB, we re-gard all facts in the existing KB as evidences and regard candidate facts as queries. First of all, we need to model the existing KB and candidate facts in a MLN, and then rank candidates by their likelihood.

Markov logic can be viewed as a probabilistic extension of rst-order logic by attaching weights to formulas. Each weight re ects the relative strength or importance of the corresponding formula. Higher weight indicates greater re-ward to a world that satis es the formula. More formally, let X be the set of all true facts in KBs, F be the set of all rst-order formulas in the MLN, w i be the weight associ-ated with formula f i  X  F , and G f i be the set of all possible groundings of formula f i ; then the probability of a possible world x is de ned as: where Z = constant, w represents a set of formula weights, g ( x ) equals 1 if g is satis ed and 0 otherwise, and n i ( x ) denotes the number of true groundings of f i in x .
 To model a knowledge base by MLN, we rst view the KB from the perspective of Markov Networks: Facts can be true or false, so we treat all facts in the existing KB as bi-nary random variables. We add them to a Markov Network as nodes and set their truth values as true. All facts not in the KB are not added to the Markov Network explicitly, and only a small part of them are generated when necessary. To represent interdependence between random variables, we add undirected edges for each two facts that share one com-mon entity. There are no hyperedges in Markov Networks. When more than two nodes share one common entity, we add edges for each two of them to represent the dependence between two variables and use cliques 2 to represent depen-dence between all of them. MLN simpli es the dependence between more than two facts by splitting cliques into logical formulas. MLN generalizes formulas by replacing entities with variables and replacing facts with relation types. Fi-nally, MLN learns weights for each conceptualized formula by counting groundings and treats weight formulas as fea-tures to predict missing links.

To obtain useful formulas and precise weights for one speci c query Q , we discriminatively learn structures and weights of the MLN for Q , which means only facts under Q are treated as queries. Therefore, we care about the proba-bility of the possible world only containing the speci c query facts and treat other facts as evidence. We note the query as Y and compute its conditional probability given evidence X as follows: Di erent from Equation (2), each f i in F Y must involve the query Y , and groundings of f i are counted when the Y is set as true and false, respectively. The normalizing constant Z y aggregates the probabilities for all possible y , and Z y =
For brevity, only non-recursive formulas are considered, and all queries become independent after evidence is given. The normalizing constant Z y can be simpli ed as the sum of two parts when Y = 1 and Y = 0. Therefore, for one speci c grounding query Y j , its conditional probability is simpli ed as follows: P ( Y j = y j | X = x ) = exp ( en .wikipedia.org/wiki/Clique (g raph th eory) ith formula when all the evidence facts in X and the query Y j are set to their truth values. Analogously, n i ( x; y [ Y and n i ( x; y [ Y j =1] ) are the numbers when Y j is set as 0 and 1, respectively.

For each MLN model under a speci c query relation Q , we employ the algorithm described in [24] to learn structure and weights discriminatively. Algorithm [24] performs ran-dom walks starting from a number of initial nodes sampled randomly and collects paths constituted by linking entities during random walks. In this process, we generate ground-ing formulas by replacing each two adjacent nodes in a path with facts containing them in the existing KB. Therefore, one path obtained from random walks can be changed to var-ious grounding formulas because there are possibly several facts under di erent relations between two adjacent entities.
After heuristic pruning, we only keep grounding formu-las containing a true fact under Q , and conceptualize them by replacing entities by variables. For example, the ground-ing formula BornIn(Barack Obama,Honolulu)  X  LiveIn(Ann Dunham,Honolulu)  X  Mother(Barack Obama, Ann Dun-ham) is conceptualized to BornIn( x 1 , x 2 )  X  LiveIn( x  X  Mother ( x 1 , x 3 ) . For one speci c fact under Q , all for-mulas attached to it are counted as its features, and the fact with counts of formulas are used to learn formula weights by Equation (4).
For a speci c query relation Q , we have obtained candi-date sets and an MLN learned under Q . We exploit an in-ferring algorithm to independently calculate the probability of each selected instance being true, called INS ( I nferri ng via ground N et work S amp ling). Additionally, we will de-scribe its extension, INS-ES (I n ferring via ground N et work S a mpling Merging E m bedding S im ilarity Priori).
INS is a data-driven algorithm and follows 4 steps: Per-forming random walks on POG; generating grounding for-mulas; counting formulas for instances; calculating probabil-ities for each instance. Algorithm 1 shows the detail of INS, where KB is the training set, C is the candidate fact set, Q is the query relation, F is the formula set with weights learned previously, S is the seed set, F p is the grounding formula set, q is one query in C , and QDI is a map from query to data instance.

First, INS performs random walks on the POG to gener-ate entity paths. We collect all entities occurring in selected instances under Q to form a set of initial nodes by employing each entity to build a single node path. Then, we perform random walks from each initial node to obtain paths of en-tities. There are formulas learned with a query at the place of body. For example, for BornIn( x 1 , x 2 )  X  LiveIn( x  X  Mother( x 1 , x 3 ) , when BornIn or LiveIn is treated as the query relation Q , we regard it as a formula with the query in body. To obtain this type of formula, we assume all can-didates are true and put them in the existing KB before performing random walks because the truth values of all candidates are unknown, and INS randomly walks to them only when they exist in the KB. The above process is pre-sented in Line 1 to 7 of Algorithm 1; Line 5 to 7 is the core strategy of random walks which is detailed in Section 5.2.2.
Second, INS generates grounding formula bodies from each sampled path: We rst transform adjacent nodes into ground-ing facts, which only occur in the existing KB. Then, to add heads to these formulas, we link the head and tail of the path with several possible relations, which can be true or false. After obtaining grounding formulas, all of them are conceptualized to generate a great deal of conceptualized formulas, which never have to be learned or pruned, and INS simply ignores them. Line 8 to 15 shows the generating and conceptualizing process.

Third, all formulas that remain are counted as features for a speci c selected instance r ( h; t ) that occurs in these formulas. INS travels all instances in the map of QDI and nally employs Equation (4) to calculate the probability for each candidate instance, which corresponds to Line 16 to 17. In particular, for some selected instance without any formulas, their scores are set to 0 directly.
 Alg orithm 1: INS (KB, C, Q , F).
 In put: KB , C , Q , F
Output: Probabilities for each instance in C . 1: Set q  X  C to true, add them to KB . 2: QDI  X  building pairs &lt; q , empty instance &gt; 3: S  X  entities in q , q  X  C . 4: Foreach s  X  S : 5: Initialize Path State p from s . 6: Repeat Until MaxIterator: 7: p  X  RandomToNextPathState( p ). 8: F p  X  GenerateFormulas( p ). 9: Foreach f p  X  F p . 10: If q in f p 11: Conceptualize f p . 12: If f p  X  F 13: Add f p truth counting to QDI(q) 14: Else Ignore f p . 15: Else Ignore f p . 16: Foreach q  X  C : 17: Calculate P(q j KB,F) by q and weights of
We apply grounding network sampling to obtain entity paths uniformly and count formulas for each selected in-stance. To sample grounding network, we perform random walks on a POG.
 POG is a directed graph and its nodes are subgraphs in the KB which is viewed as a graph. For KBC, we simplify it by only considering simple paths but not subgraphs. Edges in POG can be divided into three categories: super-edge, sub-edge and restart-edge. We take an example to explain them: There are two nodes, n 1 =[ Barack Obama ]  X  [ Honolulu ] and n 2 = [ Ann Dunham ]  X  [ Barack Obama ]  X  [ Honolulu ]. The edge from n 1 to n 2 is a super-edge, while the edge from n to n 1 is a sub-edge, and edges from n 1 or n 2 to the initial node are restart-edges, where the initial node can be [ Barack Obama ] or [ Honolulu ]. We make an further explanation for three types of edges, and they can be viewed as three opera-tions: (1) Add one node to the existing path, e.g., add [ Ann Dunham ] to n 1 at the far left; (2) Remove one node from the existing path, e.g., remove [ Ann Dunham ] from n 2 ; (3) Restart from the initial node which is determined before the random walk starting, e.g., both n 1 and n 2 can restart from [ Barack Obama ] which is in query Mother(Barack Obama, x ) .
T o achieve a uniform distribution for all the simple paths, we exploit ideas from Metropolis-Hastings (MH) algorithm to perform random walks on the graph [1, 17]. Note the sta-tionary distribution of node u as ( u ). At the current state X t = u , the next state X t +1 is proposed with a proposal probability Q(u, v)(u  X  = v). The proposed transition to v is accepted with an acceptance probability A ( u; v ), that is, and rejected with probability 1  X  A ( u; v ) in which case the state X t +1 remained unchanged.

The purpose of random walk on POG is to perform an unbiased graph sampling for MLN's structure and weight learning, so INS proposes that the target stationary dis-tribution should be set to the uniform distribution = (1 =n; 1 =n; :::; 1 =n ), where n = | N next | represents the num-ber of possible next states. Corresponding to super-edge and sub-edge, we also divide N next into N + and N , and denote d ( u ) = | N + ( u ) | and d ( u ) = | N ( u ) | . Additionally, INS sets a xed probability of restarting noted as . Exploring e ect of adjusting to result is beyond focus of this paper, so we directly set = 0 : 35 in our experiment. The proposal probability is de ned as follows: This process corresponds to the RandomToNextPathState( p ) in Algorithm 1 (Line 7).
To incorporate the similarity a priori of candidates ob-tained from the previous embedding method, we exploit another inferring algorithm, noted as INS-ES . INS-ES re-places the uniform transition probability with the function of similarity scores obtained from the embedding-based method. It bene ts from the framework of the MH algorithm, which employs a type of proposal probability Q ( u; v ) to obtain a desired stationary distribution for random walks. Q ( u; v ) can be viewed as a state transition probability of an arbi-trary irreducible Markov chain on the whole state space N with constraints: Q ( u; v ) &gt; 0 if and only if Q ( v; u ) &gt; 0, and Q ( u; v ) = 0 for all ( u; v ) =  X  E ( u  X  = v ), where the set of edges [17]. In INS, it assigns equal value to Q ( u; v ) ; v  X  N next ( u ), which can achieve an unbiased graph sampling. However, now, we expect to utilize similarity a priori obtained from embedding-based methods in random walk process. We assign distinguishing Q ( u; v ) to di erent adjacent nodes to break the unbiased graph sampling. This strategy can guide random walks to nodes containing se-lected instances with larger probabilities than others. More speci cally, the instance, being at the more frontal position in the candidate set, is more likely to be visited.
In the previous step of making the instance selection, each selected instance has been assigned a similarity score for ranking. The embedding-based methods treat the similarity scores as the nal results for KBC. Although Table 1 shows that these results are insucient for the task of KBC, we believe there are still some priori knowledge that imply that one instance with a higher score is more likely to be true.
To replace the uniform probabilities by similarity priori, we just need to reassign transition probabilities only for super-edges and keep probabilities unchanged for sub-edges and restart-edges. During random walks, we use entities added to paths to represent candidate states for simplic-ity. When the current state is u and we expect to travel to the next state, | N + ( u ) | includes all entities adjacent to two marginal entities in the current path. For example, when the current state contains a path, n 1 = [ BarackObama ]  X  [ Honolulu ], N + ( n 1 ) is composed of all entities adjacent to [Barack Obama] or [Honolulu] , and we assign diverse tran-sition probabilities to states in N + ( n 1 ). We split N into two parts: N + cand ( n 1 ) and N + usal ( n 1 ). N + represents the subset, in which nodes all come from selected instances, and other nodes form N + usal ( n 1 ). We assign dis-tinguishing Q ( u; v ) to entities in N + cand ( u ) and assign de-fault Q ( u; v ) for N + usal ( u ) as follows: where Z is a normalizing constant and Z = | N + usal ( u ) Q ( u; v ) and f ( score v ) is a function to map similarity to probability. We design a heuristic equation for f ( score To normalize score v , we rst divide it by | score | max is the maximum absolute value of the similarity score under Q . Then, multiply by a coecient , which can adjust the strength of the in uence from the embedding priori. With the increase in , an adjacent node that occurs in a selected instance has larger and larger transition probability from the current state, which means the strength of the embed-ding similarity priori is stronger and stronger. However, an overlarge means that other nodes that never occur in any selected instance are ignored easily and are never vis-ited when is in nite. We add 1 to the function as a base value, so that the candidate entity with score v = 0 has the same transition probability as ordinary ones. When = 1, f ( score v ) is always 1, and INS-ES degrades into INS.
We perform all experiments on an FB15K dataset, which is sampled from Freebase [4]. It contains 592,213 true facts with 14,951 entities and 1345 relation types. It has been split randomly into training, validation, and testing sets. These sets only contain true facts. For the learning process, both embedding-based methods and inference-based meth-ods have their own strategies to generate false facts. How-ever, for testing, false facts are uniformly generated for all methods as described in [4]. For each true fact r ( h; t ) in the testing set, the tail t is replaced by every entity in the existing KB, and these are mixed with the true one, forming the Left Testing Subset. Then, the head h is replaced in the same way, noted as the Right Testing Subset.

Two parts of experiments are performed:
All following experiments are performed by a single thread on the servers with 24 Intel Xeon E5-2620 clocked at 2.00GHZ, with 64GB RAM, running Linux 2.6.32.
We compare several embedding-based methods with a tra-ditional approach to selecting di erent sizes of candidate sets. Hits@n can be viewed as the true facts coverage in n-size candidate sets because we believe that the more the true facts are covered, the better the selection is.
BFS is a traditional approach based on an adjacent hy-pothesis, which means an entity t being adjacent or reach-able via one skip from h implies r ( h; t ) is more likely to be true. BFS is treated as a baseline.
 Embedding-based methods described previously, SE [5], SME [3](linear and bilinear), TransE [4], are as compared. As a baseline for TransE, Unstructured ( Uns , for short), which is a naive version of TransE that ignores the e ect of various relations, is treated as a baseline as well. We implement BFS in java by ourselves, but for Uns, SE, SME-lin, SME-bil and TransE, we use the codes on TransE's homepage 3 and keep the default setting for pa-rameters. Similar to evaluation methods in [4, 26, 18], we evaluate Left Testing Subsets and Right Testing Subsets for each true fact r ( h; t ) in testing KB. There may be other true facts in Left and Right Testing Subsets in addition to r ( h; t ), but previous evaluation methods ignore them. For example, in the Left Testing Subset of r ( h; t ), there is a true fact r ( h; t  X  ) also occurring in testing the KB, and we should treat it the same as r ( h; t ) when calculating Hits@n and AP@n. Our evaluation considers such r ( h; t  X  ) in testing the KB, so that the Hits@10 value is di erent from those reported in [4, 26, 18].
Figure 2 shows that, except for Uns, all embedding-based methods are better than BFS, the traditional approach. It indicates that most embedding-based methods have advan-tages on instance selection.

In Figure 2, the best Hits@1 is 32.911%, achieved by SME-bil, but after n increases to 10, TransE begins to outper-form the other methods. In addition, TransE is the simplest method, and its running time is the shortest. Therefore, TransE is the most suitable method for generating smaller candidate sets for the subsequent logical inference.
TransE's Hits@50 has exceeded 90%, and Hits@100 is 94.568%, which is sucient for subsequent fact inference. On the other hand, when n &gt; 100, all methods' Hits@n grad-ually approach 100%, and the performance of BFS starts to h ttps://everest.hds.utc.fr/doku.php?id=en:transe reach values that are close to the performance of embedding-based methods. The advantages of embedding-based meth-ods disappear with the increase in n, and too large candidate sets would slow down the subsequent inferring algorithm. Therefore, we set the size of candidate set to 100 for subse-quent experiments.
In this subsection, we compare our approaches with state-of-the-art methods including embedding-based methods and inference-based methods. Additionally, we explore the ef-fect of similarity obtained a priori by the embedding-based method by comparing INS-ES with INS.

We employ Hits@n and AP@n as evaluations for the fol-lowing experiments. Unlike instance selection, we employ Hits@1 as the most important metric because a good Hits@n with a large n cannot indicate that an approach has a good performance. AP@n is used as the other metric for the sup-plementary of Hits@n. AP@n can give a reasonable evalua-tion for Q ( h; ?), which has more than one true answer.
Embedding-based methods We employ Uns, SE, SME-lin, SME-bil and TransE to represent embedding-based ap-proaches in the same way as the methods in the experimen-tal subsection of instance selection. However, we treat the output from embedding-based models as the nal results.
Inference-based methods There are many methods that belong to this category. Most of them infer instances through logical formulas, but few of them can be used in KBC for their computation complexity. Here, we employ the INS al-gorithm to infer potential facts in the testing set and treat it as representative of inference-based methods.

Our approaches We apply the INS algorithm to infer overselected instances obtained from di erent embedding-based models. As Embedding-based methods , we choose Uns, SE, SME-lin, SME-bil and TransE to select instances and refer to the methods inferring over them as Uns-INS, SE-INS, SME-lin-INS, SME-bil-INS, TransE-INS, respectively. Our approach with a priori We continue to expand TransE-INS by incorporating the similarity generated a pri-ori by the embedding-based model into INS, noted as TransE-INS-ES. As an important super-parameter of TransE-INS-ES, in (in Equation 8) controls the strength of the e ect of the similarity priori. We use TransE-INS-ES-to denote our method with di erent , where n denotes di erent .
We keep the default parameter setting for all embedding-based methods as in the instance selection. The number of selected instances is set to 100. For the INS algorithm, we set the maximum formula length as l f = 5. The size of the initial node random walk starting from M random is 500. The random walk times from one starting point T is 50 for learning and 1000 for inference. The restarting probability in random walk P restart is 0.35. The maximum number of iterations of the weight learner M learner is 100, and the L2-norm parameter C is 1. We employ the TransE-INS with 1000-size candidate sets to approximate INS because it can-not run completely to infer each instance directly, which is also the main reason that we make instance selection before inference. The implement is available online. 4 h ttps://github.com/ZhuoyuWei/fpMLN
Table 2 shows the results of the comparison between our methods and state-of-the-art methods for KBC. From the results, we can obtain the following observations. 1) Our methods (except Uns-INS) in Table 2.c and 2.d achieve performance improvement over other methods in Ta-ble 2.a and 2.b. This indicates that our method based on Inferring via Grounding Network Sampling over Selected In-stances is e ective for the task of KBC. 2) Our methods, which perform inference after select in-stances by embedding-based methods in Table 2.c, all out-perform only the corresponding embedding-based models in Table 2.a. For example, TransE's Hits@1 is 29.401%, which was the state-of-the-art method for KBC, while TransE-INS promotes Hits@1 to 69.303%, and TransE-INS also achieves much higher Hits@10 and AP@n. This proves the e ective-ness of inferring instances following instance selection by embedding-based models, and the signi cant improvement is derived from the fact that our approaches consider explicit logical semantics and interaction between di erent relations, which is ignored in embedding-based models. 3) Our methods, which rst make instances selection and generate small-scale candidate sets for INS in Table 2.c, out-perform only INS in Table 2.b. The reason is that there are some noise instances, which may mislead the INS algorithm, but they are ltered by instance selection via embedding-based methods. It further indicates that embedding-based models can capture implicit factors that are ignored in in-ference methods, and both of these two methods cannot be substituted by each other. 4) Our extensional method, which incorporates the simi-larity generated a priori by the embedding-based model into INS in Table 2.d, outperforms our methods without the sim-ilarity priori in Table 2.c. This proves that the similarities obtained from embedding-based methods are helpful for pro-moting the inference precision as a priori.
In this part, we engage in a detailed discussion on our proposed approaches.

E ect of the candidate set size We explore the e ect of the number of instances for subsequent fact inference, and we employ TransE-INS as the experimental method. Figure 3 shows the results. The horizontal axis represents the number of selected instances, noted as N . The verti-cal axes of (a), (b) represent Hits@n, AP@n, respectively. Hits@1,Hits@10 and Hits@100 are presented in Figure 3.a. The best Hits@1 occurs at N =50, while the best Hits@10 and Hits@100 occur at N =200. When N &gt; 200, the perfor-mance keeps decreasing with an increasing N , and the AP@n in Figure 3.b has the similar appearance. This indicates that making instance selection before inferring to generate small-scale candidate sets is an e ective mechanism, which not only narrows the inferring space but also lters out part of the noise.

E ect to running time Figure 3.c shows the running time of performing inference, which indicates the compu-tation complexity of inference is linearly growing with the increase in the candidate set size. This proves that inference on large-scale KBs without rst reducing candidates would take an extremely long period of time. Therefore, selecting instances to narrow the inferring space in our approach is necessary and ensures that the inferring algorithm is com-pleted in a short time.

E ect of the similarity priori We are going to prove the e ectiveness of incorporating the similarity priori gener-ated by the embedding-based model into INS by comparing TransE-INS with several TransE-INS-ES with di erent . Figure 4 shows the results of INS and INS-ES over 100 se-lected instances generated by TransE. The horizontal axes represent , and the vertical axes in six sub-gures represent AP@1 (equal to Hits@1/100), AP@2, AP@3, AP@5, AP@7, AP@10. To reduce the randomness and highlight the e ect of similarity priori, this experiment is repeated 5 times, and we report the mean and variance of AP@n. Regardless of the , INS-ES outperforms INS on Hits@1 and AP@n, and the best performance of INS-ES is achieved when = 4 : 5. This indicates that incorporating the similarity priori into a random walk process can achieve better performance. The curves also show that the performance is becoming better with the e ect of similarity priori increase until = 4 : 5, and then the performance starts to become worse. The per-formance decrease may be caused by an excessively strong e ect of similarity priori easily ignoring the nodes, which never occurs in selected facts, but these nodes may be ac-companied by useful information.
Our work is related to two main categories of KBC meth-ods: embedding-based approaches [21, 5, 12, 23, 4, 3, 26, 18] and inference-based approaches [22, 16, 24].

Nearly all embedding-based approaches represent entities as low dimensional vectors, but the representation for rela-tions is multifarious. SE [5] represents each relation as two matrixes to capture the left and right interaction with one entity. RESCAL [21] represents relations as only one matrix and employs the product of three elements in a fact as sim-ilarity. SME[3] also represents relations as one matrix but employs an energy function to transform the interaction be-tween the relation and entity. In the simplest way, TransE [4] represents relations in the same vector space as entities. The extensions of TransE, TransH [26] and TransR [18] also represent relations as low dimensional vectors, but there are di erences between them: The former interprets a relation as a translating operation on a hyperplane to address one-to-many, many-to-one, and many-to-many relations, but the latter represents entities and relations in distinct spaces and projects entities from entity space to relation space. Tranc [9] and [12] take 2-way interactions between one entity and one relation into account. As a more complicated model, NTN [23] represents relations as tensors and matrixes si-multaneously. However, these embedding-based methods do not consider explicit logical semantics and cannot suciently capture the interaction between di erent relations. Our ap-proach applies a subsequent fact inference to overcome these shortages.

Inference-based methods usually view a KB as a graph, and seek, count or sample sub-structures (e.g., paths, sub-graphs) to judge or calculate probability for a new fact. There are some typical inferring methods for KBC: ARM (association rule mining) [8], ILP (inductive logic program-ming) [25], and MLN [22]. They are all limited by the large si ze of the candidate sets, but our approach speeds up MLN and employs it as a subsequent inference model for its high precision. MLN is an appropriate model for KBC because it applies weighted formulas to capture long-range interac-tion across several relations. The main issue of MLN is the computation complexity of the learning structure and infer-ring; thus, the methods based on MLN cannot be extended to large-scale knowledge bases. [15, 14, 13, 19, 11] are all proposed methods to speed up the learning and inferring process, but most of them still need grounding facts or for-mulas, so that the issue of computation complexity has still not been adequately addressed. PRA [16] employs random walks, but unlike our approach, it simpli es MLN by only keeping the horn clause with the query at the place of head. A promising algorithm is proposed by Sun [24], which is also a data-driven algorithm based on random walks that learns structure and weights simultaneously. Our approach takes advantage of this and employs a similar mechanism for in-ference and replaces the uniform transition probability for promotion.
We proposed employing embedding-based models to make instance selection for subsequent fact inference and exploited INS and INS-ES (incorporating embedding a priori) algo-rithms to infer facts on MLN, which improved Hits@1 from 32.911% to 71.692% for the FB15K dataset.
The authors are supported by the National High Tech-nology Research and Development Program of China (No. 2015AA015402) and the National Natural Science Founda-tion of China (No. 61272332, 61202329, 61303179 and 613031 72). [1] M. Al Hasan and M. J. Zaki. Output space sampling [2] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and [3] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. A [4] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, [5] A. Bordes, J. Weston, R. Collobert, Y. Bengio, et al. [6] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. [7] X. L. Dong, K. Murphy, E. Gabrilovich, G. Heitz, [8] L. A. Galarraga, C. Te ioudi, K. Hose, and [9] A. Garca-Duran, A. Bordes, and N. Usunier. E ective [10] J. Ho art, F. M. Suchanek, K. Berberich, and [11] T. N. Huynh and R. J. Mooney. Discriminative [12] R. Jenatton, N. L. Roux, A. Bordes, and G. R. [13] T. Khot, S. Natarajan, K. Kersting, and J. Shavlik. [14] S. Kok and P. Domingos. Learning the structure of [15] S. Kok and P. Domingos. Learning markov logic [16] N. Lao, T. Mitchell, and W. W. Cohen. Random walk [17] C.-H. Lee, X. Xu, and D. Y. Eun. Beyond random [18] Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu. Learning [19] L. Mihalkova and R. J. Mooney. Bottom-up learning [20] B. Min, R. Grishman, L. Wan, C. Wang, and [21] M. Nickel, V. Tresp, and H.-P. Kriegel. A three-way [22] M. Richardson and P. Domingos. Markov logic [23] R. Socher, D. Chen, C. D. Manning, and A. Ng. [24] Z. Sun, Z. Wei, J. Wang, and H. Hao. Scalable [25] W. Y. Wang, K. Mazaitis, and W. W. Cohen.
 [26] Z. Wang, J. Zhang, J. Feng, and Z. Chen. Knowledge
