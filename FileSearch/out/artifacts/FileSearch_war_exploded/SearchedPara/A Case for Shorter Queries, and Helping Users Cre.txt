 Query expansion has long been a focus of infor-mation retrieval research. Given an arbitrary short query, the goal was to find and include additional related and suitably-weighted terms to the original query to produce a more effective version. In this pa-per we focus on a complementary problem  X  query re-writing . Given a long query we explore whether there is utility in modifying it to a more concise ver-sion such that the original information need is still expressed.
 lect large portions of text from documents and issue them as queries. The search engine is designed to encourage users to submit long queries such as this example from the web site  X  X  need to know the gas mileage for my Audi A8 2004 model X  . The moti-vation for encouraging this type of querying is that longer queries would provide more information in the form of context (Kraft et al., 2006), and this ad-ditional information could be leveraged to provide a better search experience. However, handling such long queries is a challenge. The use of all the terms from the user X  X  input can rapidly narrow down the set of matching documents, especially if a boolean retrieval model is adopted. While one would ex-pect the underlying retrieval model to appropriately assign weights to different terms in the query and return only relevant content, it is widely acknowl-edged that models fail due to a variety of reasons (Harman and Buckley, 2004), and are not suited to tackle every possible query.

Recently, there has been great interest in personal-ized search (Teevan et al., 2005), where the query is modified based on a user X  X  profile. The profile usu-ally consists of documents previously viewed, web sites recently visited, e-mail correspondence and so on. Common procedures for using this large amount of information usually involve creating huge query vectors with some sort of term-weighting mecha-nism to favor different portions of the profile.
The queries used in the TREC ad-hoc tracks con-sist of title, description and narrative sections, of progressively increasing length. The title, of length ranging from a single term to four terms is consid-ered a concise query, while the description is consid-ered a longer version of the title expressing the same information need. Almost all research on the TREC ad-hoc retrieval track reports results using only the title portion as the query, and a combination of the title and description as a separate query. Most re-ported results show that the latter is more effective than the former, though in the case of some hard col-lections the opposite is true. However, as we shall show later, there is tremendous scope for improve-ment. Formulating a shorter query from the descrip-tion can lead to significant improvements in perfor-mance.

In the light of the above, we believe there is great utility in creating query-rewriting mechanisms for handling long queries. This paper is organized in the following way. We start with some examples and explore ways by which we can create concise high-quality reformulations of long queries in Sec-tion 2. We describe our baseline system in Section 3 and motivate our investigations with experiments in Section 4. Since automatic methods have shortfalls, we present a procedure in Section 5 to involve users in selecting a good shorter query from a small selec-tion of alternatives. We report and discuss the results of this approach in Section 6. Related work is pre-sented in Section 7. We wrap up with conclusions and future directions in Section 8. Consider the following query:
Define Argentine and British international rela-tions .

When this query was issued to a search engine, the average precision (AP, Section 3) of the results was 0.424. When we selected subsets of terms (sub-queries) from the query, and ran them as distinct queries, the performance was as shown in Table 1. It can be observed that there are seven different ways of re-writing the original query to attain better per-formance. The best query, also among the shortest, did not have a natural-language flavor to it. It how-ever had an effectiveness almost 50% more than the original query. This immense potential for improve-ment by query re-writing is the motivation for this paper.
 Table 1: The results of using all possible subsets (ex-cluding singletons) of the original query as queries. The query terms were stemmed and stopped.

Analysis of the terms in the sub-queries and the relationship of the sub-queries with the original query revealed a few interesting insights that had po-tential to be leveraged to aid sub-query selection. 1. Terms in the original query that a human would 2. The sub-query would often contain only terms 3. Good sub-queries were missing many of the 4. Sub-queries a human would consider as an in-
Given the above empirical observations, we ex-plored a variety of procedures to refine a long query into a shorter one that retained the key terms. We ex-pected the set of terms of a good sub-query to have the following properties.

A. Minimal Cardinality : Any set that contains more than the minimum number of terms to retrieve relevant documents could suffer from concept drift.
B. Coherency : The terms that constitute the sub-query should be coherent, i.e. they should buttress each other in representing the information need. If need be, terms that the user considered important but led to retrieval of non-relevant documents should be dropped.

Some of the sub-query selection methods we ex-plored with these properties in mind are reported be-low. 2.1 Mutual Information Let X and Y be two random variables, with joint distribution P ( x, y ) and marginal distributions P ( x ) and P ( y ) respectively. The mutual information is then defined as: Intuitively, mutual information measures the infor-mation about X that is shared by Y . If X and Y are independent, then X contains no information about Y and vice versa and hence their mutual information is zero. Mutual Information is attractive because it is not only easy to compute, but also takes into consid-eration corpus statistics and semantics. The mutual information between two terms (Church and Hanks, 1989) can be calculated using Equation 2.
 n ( x, y ) is the number of times terms x and y oc-curred within a term window of 100 terms across the corpus, while n ( x ) and n ( y ) are the frequencies of x and y in the collection of size N terms.

To tackle the situation where we have an arbi-trary number of variables (terms) we extend the two-variable case to the multivariate case. The extension, called multivariate mutual information (MVMI) can be generalized from Equation 1 to: The calculation of multivariate information using Equation 3 was very cumbersome, and we instead worked with the approximation (Kern et al., 2003) given below.

For the case involving multiple terms, we calcu-lated MVMI as the sum of the pair-wise mutual in-formation for all terms in the candidate sub-query. This can be also viewed as the creation of a com-pletely connected graph G = ( V, E ) , where the ver-tices V are the terms and the edges E are weighted using the mutual information between the vertices they connect.

To select a score representative of the quality of a sub-query we considered several options includ-ing the sum, average, median and minimum of the edge weights. We performed experiments on a set of candidate queries to determine how well each of these measures tracked AP, and found that the aver-age worked best. We refer to the sub-query selection procedure using the average score as Average . 2.2 Maximum Spanning Tree It is well-known that an average is easily skewed by outliers. In other words, the existence of one or more terms that have low mutual information with every other term could potentially distort results. This problem could be further compounded by the fact that mutual information measured using Equa-tion 2 could have a negative value. We attempted to tackle this problem by considering another mea-sure that involved creating a maximum spanning tree (MaxST) over the fully connected graph G , and us-ing the weight of the identified tree as a measure rep-resentative of the candidate query X  X  quality (Rijsber-gen, 1979). We used Kruskal X  X  minimum spanning tree (Cormen et al., 2001) algorithm after negating the edge weights to obtain a MaxST. We refer to the sub-query selection procedure using the weight of the maximum spanning tree as MaxST . 2.3 Named Entities Named entities (names of persons, places, organiza-tions, dates, etc.) are known to play an important anchor role in many information retrieval applica-tions. In our example from Section 2, sub-queries without Britain or Argentina will not be effective even though the mutual information score of the other two terms international and relations might indicate otherwise. We experimented with another version of sub-query selection that considered only sub-queries that retained at least one of the named entities from the original query. We refer to the vari-ants that retained named entities as NE Average and NE MasT . We used version 2.3.2 of the Indri search engine, de-inference network-based retrieval framework of In-dri permits the use of structured queries, the use of language modeling techniques provides better es-timates of probabilities for query evaluation. The pseudo-relevance feedback mechanism we used is based on relevance models (Lavrenko and Croft, 2001).

To extract named entities from the queries, we used BBN Identifinder (Bikel et al., 1999). The named entities identified were of type Person , Lo-cation , Organization , Date , and Time .

We used the TREC Robust 2004 and Robust 2005 (Voorhees, 2006) document collections for our ex-periments. The 2004 Robust collection contains around half a million documents from the Finan-cial Times, the Federal Register, the LA Times, and FBIS. The Robust 2005 collection is the one-million document AQUAINT collection. All the documents were from English newswire. We chose these col-lections because they and their associated queries are known to be hard , and hence present a chal-lenging environment. We stemmed the collections using the Krovetz stemmer provided as part of In-dri, and used a manually-created stoplist of twenty terms ( a, an, and, are, at, as, be, for, in, is, it, of, on, or, that, the, to, was, with and what ). To determine the best query selection procedure, we analyzed 163 queries from the Robust 2004 track, and used 30 and 50 queries from the 2004 and 2005 Robust tracks re-spectively for evaluation and user studies.

For all systems, we report mean average preci-sion (MAP) and geometric mean average precision (GMAP). MAP is the most widely used measure in Information Retrieval. While precision is the frac-tion of the retrieved documents that are relevant, av-erage precision (AP) is a single value obtained by averaging the precision values at each new relevant document observed. MAP is the arithmetic mean of the APs of a set of queries. Similarly, GMAP is the geometric mean of the APs of a set of queries. The GMAP measure is more indicative of performance across an entire set of queries. MAP can be skewed by the presence of a few well-performing queries, and hence is not as good a measure as GMAP from the perspective of measure comprehensive perfor-mance. We first ran two baseline experiments to record the quality of the available long query and the shorter version. As mentioned in Section 1, we used the description and title sections of each TREC query as surrogates for the long and short versions re-spectively of a query. The results are presented in the first two rows, Baseline and Pseudo-relevance Feedback (PRF), of Table 2. Measured in terms of MAP and GMAP (Section3), using just the title re-sults in better performance than using the descrip-tion. This clearly indicates the existence of terms in the description that while elaborating an information need hurt retrieval performance. The result of using pseudo-relevance feedback (PRF) on both the title and description show moderate gains -a known fact about this particular collection and associated train-Table 2: Results across 163 training queries on the Robust 2004 collection. Using the best sub-query results in almost 50% improvement over the baseline ing queries.

To show the potential and utility of query re-writing, we first present results that show the upper bound on performance that can obtained by doing so. We ran retrieval experiments with every combi-nation of query terms. For a query of length n , there are 2 n combinations. We limited our experiments to queries of length n  X  12 . Selecting the perfor-mance obtained by the best sub-query of each query revealed an upper bound in performance almost 50% better than the baseline (Table 2).

To evaluate the automatic sub-query selection procedures developed in Section 2, we performed retrieval experiments using the sub-queries selected using them. The results, which are presented in Ta-ble 3, show that the automatic sub-query selection process was a failure. The results of automatic se-lection were worse than even the baseline, and there was no significant difference between using any of the different sub-query selection procedures.
The failure of the automatic techniques could be attributed to the fact that we were working with the assumption that term co-occurrence could be used to model a user X  X  information need. To see if there was any general utility in using the procedures to select sub-queries, we selected the best-performing sub-query from the top 10 ranked by each selection procedure (Table 4). While the effectiveness in each case as measured by MAP is not close to the best possible MAP, 0.342, they are all significantly better than the baseline of 0.243. The final results we presented in the last section hinted at a potential for user interaction. We envi-Table 3: Score of the highest rank sub-query by var-ious measures.
 Table 4: Score of the best sub-query in the top 10 ranked by various measures sioned providing the user with a list of the top 10 sub-query candidates using a good ranking proce-dure, and asking her to select the sub-query she felt was most appropriate. This additional round of hu-man intervention could potentially compensate for the inability of the ranking measures to select the best sub-query automatically. 5.1 User interface design We displayed the description (the long query) and narrative portion of each TREC query in the inter-face. The narrative was provided to help the partic-ipant understand what information the user who is-sued the query was interested in. The title was kept hidden to avoid influencing the participant X  X  choice of the best sub-query. A list of candidate sub-queries was displayed along with links that could be clicked on to display a short section of text in a designated area. The intention was to provide an example of what would potentially be retrieved with a high rank if the candidate sub-query were used. The partici-pant used this information to make two decisions -the perceived quality of each sub-query, and the best sub-query from the list. A facility to indicate that none of the candidates were good was also included. Table 5: Number of candidates from top 10 that ex-ceeded the baseline 5.2 User interface content issues The two key issues we faced while determining the content of the user interface were:
A. Deciding which sub-query selection procedure to use to get the top 10 candidate sub-queries: To determine this in the absence of any significant dif-ference in performance due to the top-ranked can-didate selected by each procedure, we looked at the number of candidates each procedure brought into the top 10 that were better than the baseline query, as measured by MAP. This was guided by the belief that greater the number of better candidates in the top 10, the higher the probability that the user would select a better sub-query. Table 5 shows how each of the selection procedures compared. The NE MaxST ranking procedure had the most number of better sub-queries in the top 10, and hence was chosen.
B. Displaying context: Simply displaying a list of 10 candidates without any supportive information would make the task of the user difficult. This was in contrast to query expansion techniques (Anick and Tipirneni, 1999) where displaying a list of terms suf-ficed as the task of the user was to disambiguate or expand a short query. An experiment was per-formed in which a single user worked with a set of 30 queries from Robust 2004, and an accompanying set of 10 candidate sub-queries each, twice -once with passages providing context and one with snip-pets providing context. The top-ranked passage was generated by modifying the candidate query into one that retrieved passages of fixed length instead of documents. Snippets, like those seen along with links to top-ranked documents in the results from almost all popular search engines, were generated after a document-level query was used to query the collection. The order in which the two contexts were presented to the user was randomized to prevent the Table 6: Results showing the MAP over 19 of 30 queries that the user provided selections for using each context type. user from assuming a quality order. We see that pre-senting the snippet led to better MAP that presenting the passage (Table 6). The reason for this could be that the top-ranking passage we displayed was from a document ranked lower by the document-focussed version of the query. Since we finally measure MAP only with respect to document ranking, and the snip-pet was generated from the top-ranked document, we hypothesize that this led to the snippet being a better context to display. We conducted an exploratory study with five par-ticipants -four of them were graduate students in computer science while the fifth had a background in the social sciences and was reasonably proficient in the use of computers and internet search engines. The participants worked with 30 queries from Ro-values reported are automatic runs with the descrip-tion as the query.
 able to choose sub-queries that led to an improve-ment in performance over the baseline (TREC title query only). This improvement is not only on MAP but also on GMAP, indicating that user interaction helped improve a wide spectrum of queries. Most notable were the improvements in P@5 and P@10. This attested to the fact that the interaction tech-nique we explored was precision-enhancing. An-other interesting result, from # sub-queries selected was that participants were able to decide in a large number of cases that re-writing was either not useful for a query, or that none of the options presented to them were better. Showing context appears to have helped.
Baseline 0.203 0.159 0. 476 0.507 Upper Bound 0.336 0.282 0.784 0.719
Baseline 0.224 0.156 0.484 0.526 Upper Bound 0.359 0.293 0.810 0.742
Baseline 0.217 0.126 0.452 0.432 Upper Bound 0.354 0.263 0.762 0.654
Baseline 0.192 0.142 0.462 0.525 Upper Bound 0.344 0.310 0.862 0.800
Baseline 0.206 0.111 0.433 0.410 Upper Bound 0.341 0.245 0.738 0.640 measured using a paired t-test, with  X  set to 0.05. Our interest in finding a concise sub-query that ef-fectively captures the information need is reminis-cent of previous work in (Buckley et al., 2000). However, the focus was more on balancing the ef-fect of query expansion techniques such that differ-ent concepts in the query were equally benefited.
Mutual information has been used previously in (Church and Hanks, 1989) to identify collocations of terms for identifying semantic relationships in text. Experiments were confined to bigrams. The use of MaST over a graph of mutual information values to incorporate the most significant dependencies be-tween terms was first noted in (Rijsbergen, 1979). Extensions can be found in a different field -image processing (Kern et al., 2003) -where multivariate mutual information is frequently used.

Work done by (White et al., 2005) provided a ba-sis for our decision to show context for sub-query se-lection. The useful result that top-ranked sentences could be used to guide users towards relevant mate-rial helped us design an user interface that the par-ticipants found very convenient to use.
 A related problem addressed by (Cronen-Townsend et al., 2002) was determining query qual-ity. This is known to be a very hard problem, and various efforts (Carmel et al., 2006; Vinay et al., 2006) have been made towards formalizing and un-derstanding it.

Previous work (Shapiro and Taksa, 2003) in the web environment attempted to convert a user X  X  natu-ral language query into one suited for use with web search engines. However, the focus was on merg-ing the results from using different sub-queries, and not selection of a single sub-query. Our approach of re-writing queries could be compared to query re-formulation, wherein a user follows up a query with successive reformulations of the original. In the web environment, studies have shown that most users still enter only one or two queries, and conduct lim-ited query reformulation (Spink et al., 2002). We hy-pothesize that the techniques we have developed will be well-suited for search engines like Ask Jeeves where 50% of the queries are in question format (Spink and Ozmultu, 2002). More experimentation in the Web domain is required to substantiate this. Our results clearly show that shorter reformulations of long queries can greatly impact performance. We believe that our technique has great potential to be used in an adaptive information retrieval environ-ment, where the user starts off with a more general information need and a looser notion of relevance. The initial query can then be made longer to express a most focused information need.

As part of future work, we plan to conduct a more elaborate study with more interaction strategies in-cluded. Better techniques to select effective sub-queries are also in the pipeline. Since we used mu-tual information as the basis for most of our sub-query selection procedures, we could not consider sub-queries that comprised of a single term. We plan to address this issue too in future work. This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are the authors and do not necessarily reflect those of the sponsor. We also thank the anonymous reviewers for their valuable comments.

