 Knowledge graph techniques are widely used in several intelligent fields including in-telligent search [ 1 ] , question answering system s [ 2 ], expert system s [ 3 ], named entity recognition [ 4 ] and entity disambiguation [ 5 ] . These techniques represent the semantic network with a graph structure , express ing the knowledge entities as nodes and rela-tion s as edges in the graph. The knowledge graph can be formally described as G=(E, and S  X  E X R X E represents the constraint of the entit ies and relation s, that is, the triples the tail entity and r is the relation between h and t .

There is often a large number of corrupted triples in the knowledge graph ; problems in the triples include missing entities and incorrect relation s. It is difficult t o ensure the correctness and completeness of the knowledge graph. The main purpose of the com-pletion of the knowledge graph is to identify the missing entities and the incorrect rela-tion s by entity prediction or relation prediction. The main task of entity prediction is , ties ), to find all entities which can make the triple correct. Relation prediction is also known as link prediction, and the main task is to find the missing or incorrect relation s in the triple T=&lt;h, *, t&gt; ( * indicates the missing or incorrect relation s ) which make the triple correct.

Traditional knowledge representation has problems such as low computational effi-ciency, too -sparse data and the need to manually select features, which ma ke comple-tion tasks difficult in large -scale knowledge graph s . The deep learning technique based on representation learning provides a new perspective o n this problem. This technique expresses the knowledge entities and relation s as low -dimensional real -valued vectors (vector h represents head entity h , vector t represents tail entity t and vec tor r represents relation r ), then calculates the semantic info r mation by simple and fast mathematical methods to achieve the purpose of knowledge graph completion. Due to the high per-formance and the automatic learning of semantic features, this technique has developed rapidly. Researchers have proposed a lot of models such as Structured Embedding ( SE) [ 6 ] , Single Layer Model (SLM) [ 7 ] and Semantic Matching Energy (SME) [ 8 -9 ]. How-ever, most of these models only consider the simple relation s among entities , with poor prediction performance .

Translation models such as TransE [1 0 ] ha ve become a hotspot of research in recent years. Based on the translation invariance of the low -dimensional vector of the entities, translation models establish the loss function b y measuring the Ln distance between h + r and t , then train the final model. Based on the spatial projection and combinatorial operators, these models have provided a more useful perspective on the feature repre-sentation of complex relation s among entities, such as 1:N, N:1 and N:M. PTransE [1 1 ] and other models even consider the existence of information on paths among the entities in multiple -level relation s , modeling two to three indirect relation s. Compared with the previous models, these models have bett er performance i n knowledge graph completion tasks, but still fail to represent the indirect relation s among long -distance entities.
Based on the existing translation models, we propose a method to construct the entit y and relation paths for knowledge graph, and propose a multiple -layer deep learning model called TransP. TransP uses the LSTM to excavate the indirect relation s among the entities, and uses the method mentioned in ProjE [1 2 ] , which takes reduc ing the collective ranking loss as the optimiza tion goal to excavate the direct relation s, to ena-ble effective excavat ion of rich information about the relation s among the entities. By us ing the layer -wise training mechanism , TransP can more accurately learn low -dimen-sional embedding representation for entities and relation s. Experimental results show that TransP outperforms state -of -the -art models in knowledge graph completion tasks. As the first translation model, TransE [ 1 0 ] assumed that the tail entity t is translated by the head entity h along the relation r . TransE measures the Ln distance between h + r and t , lows :
In the above formula, &lt; h, r, t &gt; represents the existing triples in the knowledge graph, graph, called corrupted triple s , and  X  is the minimum margin between a gold en triple and a corrupted t riple . TransE has the fewest parameters and the lowest complexity of all translation models, but it is only suitable for learn ing the representation of 1 -to -1 relation s.
 variant models have been proposed. TransH [1 3 ] proposed to project the head entities and tail entities onto the hyperplane; TransR [1 4 ] proposed to project the entities into relation space; CTransR [1 4 ] proposed to cluster the relation s, then divide them into multiple sub -relation s ; KG2E [ 15 ] consider ed the uncertain t y, imbalance and heteroge-neity of entities and relation s, and propose d the use of Gaussian distribution modeling the relation s among entities, using the asymmetric energy function based on KL diver-gence and the symmetrical energy functio n based on expected probability; TransG [ 16 ] consider ed the multiple semantic properties of the relation s, us ing the Gaussian mixture model to describe the relation s among entities, and leverag ing the Bayesian nonpara-metric infinite mixture embedding model to discover the multiple relation s semantics; TransA [ 17 ] proposed the use of Markov distance measurement loss function to learn different weights for each dimension; TransD [ 18 ] proposed project ing the entities into relation space like TransR, but with fewer parameters; TransSparse [ 19 ] consider ed the heterogeneity and imbalance of the entities and relation s, usin g the sparse matrix in-stead of the dense matrix in TransR, and us ing different projection matrices for the head entities and tail entities; TransF [ 20 ] ha d flexibility, which ensure d that the sum of the head entities and the corresponding relation s ha d the same direction as the tail entities, but with different sizes.

Most of the models consider the knowledge completion problem as a pairwise rank-ing problem, but ProjE [ 12 ] views the completion problem as a collective scores rank-ing problem of candidate entities of the head entity h and corresponding relation r . ProjE establishes a three -layer neural network model that uses combinatorial operators to combine the input data into target vectors and improve performance by optimizing the collective rank loss of the candidate entities list (or relation s list). It is a self -con-tained model and does not depend on any preprocessing.

The above works have improved modeling of the direct relation s among entities , but do not take into account the indirect relation s among entities . In order to learn the rep-resentation of such relation s, PTransE [1 1 ] , RTransE [2 1 ] and other models take into account the translation model and the indirect relation path information among the en-tities. These models further improve the perf ormance of knowledge graph completion tasks. However, the long paths can result in higher computational complexity, and not all of the relation s are reliable, meaning these models usually only consider two or three level indirect relation s. To take into account the direct relation and the indirect relation among entities , TransP construct s the e ntity relation path s and a multiple -layer neural network model including LSTM unit, and use s the layer -wise training mechanism to optimize the network p a-rameters. In order to capture the rich and subtle relation s among entities in the knowledge graph, TransP reduces the overall collective loss as an optimization goal . 3.1 Term definitions This paper includes the following terms and definitions:
Fact T riple : For the knowledge graph G and any triples T=&lt;h, r, t&gt; , if the triple T exists in the knowledge graph G , that is, T  X  G , then T is the Fa ct T riple in the knowledge graph G , referred to as F act.
 a n Indirect Connection Exits between the two non -adjacent entities h and t .
Entity Relation Path and S ub -P ath : For entity pairs &lt;h, t&gt; , if there is a sequence of ordered fact triples inside the h and t arranged in a certain order, we call the set of en tities E={h=e 1 , e 2 ,...,t=e n } in the order as Entity Path, and the relation s set R={r 1 ,r 2 ,...,r n -1 } is called the Relation Path. The Entity Path and the Relation Path are collectively referred to as the Ent ity Relation Path. A subset of any length in the Entity Relation Path is called its Sub -Path. 3.2 Model Architecture As shown in Fig 1, TransP is a multiple -layer neural network model ; the overall archi-tecture includes an indirect path layer and a direct path layer. The indirect path layer consists of the input combinatorial sublayer and the LSTM sublayer (the LSTM sub-layer details are omitted), and the direct path layer includes the candidate entity com-bination sublayer and the score sublayer. This architectur e is somewhat similar to ProjE, but TransP provides a more comprehensive consideration of the indirect rela-tion s among entities in the knowledge graph , and provid es a more optimized initial value for the direct path analysis.

TransP uses a layer -wise train ing mechanism , which from bottom to top, train s the indirect layer and then the direct layer. The indirect layer is based on the pre -built entity relation path sequence ; it enters the entities and the relation s into the model, then com-bines with the combination operator, training through the LSTM network. The indirect layer can be used to capture long -distance and relatively weak correlational information among entities (a more accurate semantic representation can be found from the hypo-thetical space for the objective function). The direct layer uses the entities and relation s and their combination operator , trained by the indirect layer as the input , then combin es with the candidate entities and scoring respect ive ly . The direct layer further takes into account the direct relation s among the entities in the knowledge graph ( i ntuitively, the direct impact among entities is stronger than the indirect impact). 3.3 Entity relation path construction The basic method of entity relation path construction is to find all the indirect relation s relation path in sequence. The essence of finding an entity relation path is to explore all paths among all nodes in a directed graph with a time complexity of O(N 3 ) , where N is the total number of nodes in the directed graph, that is, the number of entities in the corr esponding knowledge graph .

In order to reduce the time complexity, we propose a batch entity relation path con-struction method. Assuming that the total number of triples in the knowledge graph G is Nt , the entities in each batch ( which have M triples) made into a directed sub -graph, will make a total of Nt/M sub -graphs ( M is the super -parameter) . For any of the triples T and append the entities in the path into the entity path set PE k ={PE k1 , PE k2 , ..., PE kn } } .

During the training process, all sub -entity paths and sub -relatio n paths are spliced into a long entity path and a long relation path , w here the sub -entity paths are separated by a special symbol and the sub -relation paths are separated by two special symbols (since the number of relation s in the sub -relation path is on e less than the number of entities in the corresponding sub -entity path). After the splicing is complete, the path is segmented according to the batch size B and the time step T to obtain the final indirect -layer training data set. 3.4 Candidate entity samplin g A c andidate entity is any entity that may make T become a fact. In a large -scale knowledge graph , if all the entities are treated as candidate entit ies for model training, it will bring great training costs. It is common practice to reduce the number of candi-date entities by using candidate sampling to improve training efficiency [2 2 -2 4 ]. As with ProjE, we use the negative sampling method used in word2ve c to sample candidate entities [2 3 ].
 is called a positive case ; otherwise it is called a negative case. In the candidate entity cho ice process, we include all positive case s into the candidate entity set E c . For the negative case s , we use a simplified 0 -1 distribution B (1, p y ) for sampling , where p y is the probability of negative cases being accepted and 1 -p y is the probability that negative cases are not accepted. The value range of p y is [0%, 100%], where 0% means that no negative cases are sampled and 100% means that all negative cases are sampled . 3.5 Indirect -layer training The indirect layer consists of the entity relation sub -layer, the combined sub -layer and the LSTM sub -layer. The LSTM sub -layer is a simple LSTM network, which consists of input layer, hidden layer and output layer. The LSTM sub -layer is responsible for processing the tensor by time step, and its unrolled structure is shown in Fig 2.
The input vec tor of the LSTM input layer is multiplied by the entity vector in the entity path and the matrix of the entity combination, plus the combination vector com-posed of the relation vector in the corresponding relation path multiplied by the relation combinatio n matrix. For the entity path matrix E and the corresponding relation path matrix R , the input tensor IT is defined as follows:
In the above equation, the W e and W r matrices are diagonal matrices. This means that in the pre -processing, we only consider the dimension weight s of the entity vector and the relation vector, regardless of the influence of dimensions. This approach, on the one hand, can simplify the calcula tion ; on the other hand, the influence among the di-mension s will be further considered in the hidden layer or directly related layer. In the entity and relation prediction task, the i ndirect layer input only considers h + r ; even only consider ing h can obtain better results .

The output of the LSTM layer is a tensor OT which has the same form as the input tensor IT , and a tensor contains B*T vectors with d dimensions ( B is the batch size, T is the time step). For each time step t (0&lt;t&lt;T) , the output is a matrix OM which contains B input vectors. Assuming that the symbol corresponding to the t -th time step in the l -th sequence is y l (t) , the total loss function is defined as: and the relation vector r t in the entity relation path, the target output vector is e t+1 . Based on this, the BPTT algorithm [ 25 -26 ], which is commonl y used in the recurrent neural network, is used to train the whole network parameters according to the time steps . After the indirect layer training is completed, the trained entity vectors, relation vector s and their corresponding combine d operators are t ransferred to the direct layer for fur-ther training. 3.6 Direct -layer training The direct layer is a three -layer simple feed -forward neural network, which contains three sub -layers of input layer, candidate entity combination layer and scores rank layer. The input layer data is transform ed by the tanh function after the combination o f the entity vector s and relation vector s and the corresponding combination operator trained by the indirect layer , defined as:
In the above equation, the t -function represents the result of training the original entities and relation s by the indirect layer. Similar ly to the ProjE model, we consider the knowledge graph completion problem as a ranking problem for candidate entities and use the list method [ 27 ] to handle the entity ranking task. According to this method, the candidate entity combination layer combines the input vector with all corresponding candidate entity vectors, and the loss function definition is also consistent with ProjE:
In the above equation , h d ( ID i ) is defined as:
In the above equation, b i is the corresponding bias. This is a multi -class problem, and all positive cases in the candidate entity set (i . e . , entities that are directly related to nu mber of positive cases in all candidate entities corresponding to e and r ), and the entities not directly related to the input entity e and the inpu t relation r are scored as 0 in the direct layer.

The training method of the direct layer is the same as the feedforward neural net-work, and the parameters of the direct path layer are trained in the reverse direction. During the training process, the derivative is directly derived from the loss value, until the parameters in the entire direct layer are trained. We used Python to implement the TransP model in the TensorFlow [ 28 ] framework and evaluated TransP  X  s entity prediction and relation prediction capabilities based on the commonly used data set FB15K [1 0 ]. FB15K contains 1,345 relation s and 14,9 51 entities, corresponding to 483,142 training triples, 50,000 validated triples and 59,071 test triples. 4.1 Parameter setting
The indirect layer is optimized using the GradientDes c ent optimizer. The super -pa-rameters that need to be set in the whole layer inc lude the number of sub -graph t ri ples M , time step t , hidden layer dimension h , hidden layer number l , mini -batch size bi and the maximum iteration period ei .

The direct layer is optimized by the Adam optimizer, and the L1 regularization is also use d to prevent overfitting. The super -parameter that needs to be set by the whole layer includes the negative sampling probability ps , the maximum number of training iterations ed , the regularization weight a , the mini -batch size bd , and the parameters  X 1 ,  X 2 and  X  that the optimizer needs to set.
In addition, we introduced dropout layers in both the indirect layer and the direct layer. The whole model needs to set the public hyperparameters including the entity and relation dimension k , the learning rate r an d the dropout probability d .
In our experiments, the parameters were initially set to M = 5,000, p y = 0.25, t = 5, h = 200, l = 1, bi = 20, ei = 4, ps = 0.5, ed = 100, a = 1e -5, bd = 200,  X 1 = 0.9,  X  2 = 0.999,  X  = 1e -8, k = 200, r = 0.01 and d = 0.5.

The initial value of the learning rate of the model can also be set larger, and then gradually decrease d with the increase in the number of training sessions. All parameters that need to be initialized in the model are initialized using TransE  X  s recommende d 4.2 E ntity prediction F or the entity prediction task, w e used the same evaluation criteria me n tioned in TransE, DKRL, TransH, TransR, PTransE, RTransE, TransA and ProjE. That is, for sorted in descending order according to the score s .
 Based on the above entity ranking, we consider the collective average ranking (Mean Rank) and the top 10 hit rate (HITS10) metri cs . T he Mean Rank refers to the average number of correct entities in the ranking . HITS10 refers to the proportion of the correct entities which appear in the top 10 .

In the original measurement results, there may be some misclassified triples T , T reflect the predictive performance objectively, we process the original results and re-move the triples from the list. In the experimental results, we mark the untreated origi-nal measure as Raw, and the result after processing is called Filter. All triples that are judged to be wrong in the filter do not exist in the knowledge graph .

We co mpare a number of models and the results are shown in Table 1. From the experimental results it can be seen that TransP outperform s state -of -the -art models . For MeanRank (Raw) and MeanRank (Filter), TransP improved by 12 (about 10%) and 12 (about 35%), respectively. And for HITS10 (Raw) and HITS10 (Filter), TransP im-proved by 7.5 ( a bout 14%) and 0.4 (about 0.0045%), respectively. The results indicat e that TransP has better prediction capabilities in entity prediction t asks .
 TransE 243 125 34.9 47.1 DKRL CNN 200 113 44.3 57.6
TransH 212 87 45.7 64.4 TransR 198 77 48.2 68.7 TransE+Rev 205 63 47.9 70.2 PTransE ADD -2 200 54 51.8 83.4 PTransE RNN -2 242 92 50.6 82.2 PTransE ADD -3 207 58 51.4 84.6 TransA 155 74 56.1 80.4 TransF 220 89 40.5 61.2 ProjE_pointwise 174 104 56.5 86.6 ProjE_listwise 146 76 54.6 71.2 ProjE_wlistwise 124 34 54.7 88.4
TransP 112 22 62.2 88.8 4.3 Relation prediction For the relation prediction task, we consider two metrics of Mean Rank and HITS1 ( t op 1 hit rate ). For the original measurement results (Raw), we used the same methods to process the results in the entity prediction task and all the triples which in the knowledge graph will be removed from the results list , recorded as Filter.

As shown in Table 2, TransP has the best effect on the Mean Rank (Raw and Filter) metrics (similar to the ProjE_wlistwise model), whereas for the HITS1 metric s , TransP is slightly inferior to ProjE, but has a significant advantage over other models . TransE 2.8 2.5 65.1 84.3 TransE + Rev 2.6 2.3 67.1 86.7 DKRL CNN 2.9 2.5 69.8 89.0 PTransE ADD -2 1.7 1.2 69.5 93.6 PTransE RNN -2 1.9 1.4 68.3 93.2 PTransE ADD -3 1.8 1.4 68.5 94.0 ProjE_pointwise 1.6 1.3 75.6 95.6 ProjE_listwise 1.5 1.2 75.8 95.7 ProjE_wlistwise 1.5 1.2 75.5 95.6
TransP 1.5 1.2 75.7 95.2 Based on ProjE and other translation models, this paper proposes a new knowledge representation learning model, called TransP. TransP is a deep neural network model using layer -wise training mechanism learning knowledge representations. TransP con-siders the rich entity relation path information in the knowledge graph. By constructing the entity relation path, the LSTM is used to model the relation among these entities, and the distant relation among the entities is excavated. Experiments show that this indirect relation considered by TransP has a significant effect on learning knowledge X  X  vector representations, which is of great significance in knowledge graph completio n tasks.

In addition to the methods mentioned in this paper, TransP also has a strong scala-bility, can be combined with other knowledge learning model s , and can even be ad-justed using these combination s to adapt to different tasks. After combining with oth er models, the indirect entity relation is extracted by the method proposed by TransP, and the model parameter space is optimized to further optimize the learning ability of the knowledge models.

In the future, we will consider two aspects: Firstly, we wil l consider the use of an a ttention mechanism, c oncerned with t he higher -frequency entities and relation s in the knowledge graph , especially for the entities and relation s that contribute greatly to the relation s in the process of the entity relation path c onstruction, to further enhance the model X  X  learning ability . Secondly, we will consider the excavation of the description information corresponding to the entities or relation s contained in the knowledge graph , to further enhance the model capacity.
 This study is supported by the Scientific Research Fund of Hunan Provincial Education Department (No. 15A007) and the National Natural Science Foundation of China (No. 61202116).

