 Billy Chang billy.chang@mail.utoronto.ca Uwe Kruger uwekruger@squ.edu.om Rafal Kustra r.kustra@utoronto.ca Junping Zhang jpzhang@fudan.edu.cn This section describes the derivation of (9-12) in the main article. First note that: By writing: Equation 9 in the main article is obtained by replacing K ij with (1) in (3).
 The cyclic property of the trace operator implies: Equation 10 in the main article is obtained by replac-ing K v ij with (2) in (4).
 To derive equation 11 in the main article, note that: The first term on the R.H.S. above is just Equation 9 in the main article divided by tr ( K u  X  K v ). The sec-ond term on the R.H.S. of (5) involves the derivative since P i  X  K u ij = 0 as  X  K u is column centered. Using similar arguments, along with the fact that  X  K u is row centered, (9) and (10) can also be shown to be 0. Com-bining (1) with the first term in (8), we arrive at an explicit expression for (6): All the partial gradients involved in (5) have now been evaluated explicitly, and equation 11 in the main arti-cle, along with its weights W u ij , can be obtain by simple factorizing and reordering.
 Finally, equation 12 in the main article and its associ-ated W v ij can be obtained by considering a derivation symmetrical to the derivation above. The time required to run hsicCCA and ktaCCA, be-sides the sample size and data dimension, will also depend on the number of iterations, the convergence threshold, the bandwidth parameter  X  x and  X  y (which affects the roughness and the amount of local minima of the cost surface), the starting-parameters, and the structure of the signals within the data. At the most basic level, the time required to evaluate the cost func-tion and the gradient for hsicCCA and ktaCCA, as a function of sample size and data dimension ( P for the x -variable set and Q for the y -variable set), are presented here. Each variable set is generated using the standard Gaussian distribution, and the computa-tions are performed using a laptop with an Intel Core i7-3517U processor and 8GB RAM.
 As seen in Figure 1, the time required for evaluating the cost function is roughly linear in sample size, and rather robust against the data dimension. However, the complexity for gradient evaluation is quadratic in both the sample size and data dimension.
 Although the gradient evaluation is computationally more intensive than cost function evaluation, the com-putational bottleneck for hsicCCA and ktaCCA lies in the step-sizes search step, where the Nelder-Mead al-gorithm may requires more than 30 evaluations of the cost function. For example, to compute one pair of canonical vectors using ktaCCA on a data set with sample size 1200 and dimension P = Q = 10, each iteration will require one gradient computation (about 1 second for each evaluation), but may require around 30 cost function evaluations (about 0.4 second for each evaluation) for step-sizes search using Nelder-Mead. For 100 iterations, this will take approximately 100  X  1 + 100  X  30  X  0 . 4 = 1300 seconds.
