 The performance of machine learning methods heavily depends on the volume of used training data. For the purpose of dataset en-largement, it is of interest to study the problem of unifying mul-tiple labeled datasets with different annotation standards. In this paper, we focus on the case of unifying datasets for sequence label-ing problems with natural language part-of-speech (POS) tagging as an examplar application. To this end, we propose a probabilistic approach to transforming the annotations of one dataset to the stan-dard specified by another dataset. The key component of the ap-proach, named as label correspondence learning , serves as a bridge of annotations from the datasets. Two methods designed from dis-tinct perspectives are proposed to attack this sub-problem. Exper-iments on two large-scale part-of-speech datasets demonstrate the efficacy of the transformation and label correspondence learning methods.
 I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Measurement, Experimentation natural language processing, sequence labeling, part-of-speech tag-ging, annotation transformation
Recent years have seen extensive applications of machine learn-ing methods to diverse research fields such as natural language pro-cessing (NLP) and speech processing [1]. The performance of ma-chine learning methods heavily depends on the volume of data set used for training. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of, say, learning-based NLP systems [2].
In general, there are three ways to enlarge training data. One way is to manually annotate new data in the paradigm of supervised learning. Human annotation, however, is an expensive process in terms of time and labor, and it generally requires domain expertises. A second way is to incorporate abundant and cheaply collectable unlabeled data in the paradigm of semi-supervised learning. Un-fortunately, semi-supervised methods tend to encounter difficulties when scaling up to large datasets. This is because, while semi-supervised methods succeed to gain information from unlabeled data, they also introduce numerous noises which will degrade the performance. A third way is to combine several readily available hand-coded datasets for the same task instead of annotating new data. Many tasks in the NLP field such as Chinese word segmen-tation [3], part-of-speech (POS) tagging [4], and syntactic parsing [5] do have more than one labeled dataset. Therefore, combining such datasets becomes an appealing approach in order to obtain an enlarged high-quality dataset. The third way may combine the best of datasets obtained from the first two approaches.

In practice, data from disparate datasets often cannot be com-bined directly because datasets annotated by different institutes gen-erally follow distinct annotation standards. In order to unify such datasets, automatic annotation transformation is required which is defined to be transforming the annotations in one dataset (referred to as source dataset ) to the annotations that fit the standard implicit in the other dataset (referred to as target dataset ) 1 . In this paper, we take a particular look at annotation transformation for sequence labeling problems. By sequence labeling we refer to any problem which assigns a label to each token in an input token sequence, where the label set is predetermined. For example, natural lan-guage POS tagging, as an example of sequence labeling problems, takes word sequences as input and returns POS tag sequence.
The degree of difficulty in annotation transformation mainly de-pends on the discrepancies of annotation standards. If most map-dataset are one-to-one or many-to-one mappings, the transforma-tion is a straightforward process. However, ambiguity carried by one-to-many and many-to-many mappings exists widely. Such am-biguity makes the transformation problem more challenging than it appears. We propose a probabilistic framework as a solution to this problem. The framework consists of two components, the most noteworthy component of which is the one that models the relationships between source and target tags. In this way, original annotations in the source dataset are used to guide the choices of appropriate target tags. Hereafter, this component is referred to as
Although the definition is limited to the case of one source dataset, the approach in this paper can be extended easily to the case of more than one source dataset. label correspondence learning . Two simple yet effective meth-ods are proposed for this sub-problem. Experiments on two large-scale POS datasets demonstrate very significant gains induced by label corresponding learning and satisfactory results achieved by the transformation framework.
For the convenience of discussion, we denote the source and tar-get datasets by D s and D t , respectively. The tag set of the source dataset is named as source tag set and correspondingly, that of the target dataset is named as target tag set . The source tag set and target tag set are denoted as T s and T t , respectively.
Note that we take the NLP task, POS tagging as a running ex-ample through the paper and often use POS examples to illustrate our ideas, the proposed method can be extended easily to other se-quence labeling problems.
In this work, we view automatic POS annotation transformation as a sequence inference problem. Given a word sequence o with a source tag sequence s , a generative transformation framework is formalized in order to search for the optimal target sequence t where G ( o ) represents the set of all possible target sequences of o .
It is interesting to notice that the first two terms P ( t ) and P ( o | t ) are the components of typical generative Hidden Markov Models (HMMs) which are defined in Equation 2. Inspired by this observation, we make the same independence as-sumptions for these two terms as in a state-of-the-art second-order HMM [6]. Parameters of the two terms are estimated from the tar-get dataset. For unknown words, we slightly modify the unknown-word model used in [6] by considering prefixes and suffixes of length up to two Chinese characters.

The third term in Equation 1 is noteworthy because it is the label correspondence learning component that functions as a bridge of two datasets. By assuming that the source tag at position i is only dependent on the word and target tag at the same position, term can be decomposed into a product of multiple word-specific terms, as follows: We back off the probability P ( s i | t i , o i ) to P ( s sparseness problem 3 which potentially causes unreliable estima-tion of P ( s i | t i , o i ) . Note that { P ( s i | t i { P ( t s | t t ) } introduced in the preceding subsection. Hereafter we use the terms mapping probability and label correspondence infor-mation interchangeably to refer to { P ( t s | t t ) } .
Whenever needed, the assumption can be relaxed by assuming that the current source tag is also dependent on preceding target tags.
This claim is supported by statistics from TCT which has more than 700K words; there are only about 12 . 2% word types that occur more than 10 times in this dataset.
 Figure 1: An illustrating example of common word-based cor-respondence learning
The overall transformation model is thus formalized in Equation 4. We can see that the model manages to integrate information from both source and target datasets. A slightly modified Viterbi algo-rithm can be used to search for the best output t  X  .
The most straightforward method to learn mapping probabili-ties { P ( t s | t t ) } is to manually annotate a dataset with both source and target tags, such that the probabilities can be estimated on the dataset by simply using maximum likelihood estimation. This so-lution, however, has to consider the trade-off between annotation cost and estimation reliability. In detail, manually annotating data is generally of high cost. On the other hand, the size of the newly annotated dataset should reach some minimum requirement in or-der to render the estimation of mapping probabilities reliable. To overcome this dilemma, we propose two fully automatic methods that do not incur additional annotation cost.
The basic idea of the supervised tagger-based method is to auto-matically annotate a dataset with POS taggers trained on the other dataset. For example, we can train a CTB-based tagger and then use it to assign CTB tags to the TCT data thus each word of which re-ceives a tag from T s and T t , respectively. On the newly generated dataset mapping probabilities can be calculated using the following formula: Clearly, the performance of the supervised tagger-based method is based on two factors: the accuracy of learned POS taggers and the size of the dataset on which mapping probabilities are estimated.
Figure 1 is a synthetic example used to formally describe the process of giving such weights. Suppose there is a common word w which has two tags in the source dataset and three tags in the target dataset. { N (  X  ) } refer to the numbers of co-occurrences of the word and its tags, as shown in the figure. The first step is to calculate the prior distribution using the counts of the target tags of the word w . Then this distribution is used to partition the counts on the source dataset side. In this way, we can get weights for each member in the Cartesian product { s w, 1 , s w, 2 } X { t w, 1 , t w, 2 steps amount to the calculation using the following formula: We can see that the weights { W ( s w,i , t w,j ) } are invariant relative to the size of target dataset if the prior distribution keeps stable.
In order to achieve accurate estimation, we favor to sum over all common words to get accumulative weights of tag pairs in T T . Algorithm 1 details the procedure of the common word-based method. In brief, the algorithm is modular with two components. The first two steps collect common words and the frequency counts of words paired with corresponding tags in source and target datasets, respectively. The third step is the aforementioned procedure which conducts calculation on each common word. The results of all common words are aggregated. The algorithm finally returns a | T s | X | T t | matrix mp whose elements correspond to the weights of the pairs ( t s  X  T s , t t  X  T t ). From the statistics contained in the matrix we estimate mapping probabilities using the formula: where i denotes the row index of t s and j denotes the column index of t t . We can see that the algorithm has linear-time complexity of O ( C 1  X  C 2  X | L | ) , where C 1 represents the average number of source tags for each common word and the C 2 represents that number of target tags.

Note that Algorithm 1 first calculates prior distributions on the target side and then uses them to partition counts from the source dataset. We can propose a variant algorithm by reversing the direc-tion, an algorithm that calculates prior distributions on the source side.
The most recent version of the CTB corpus [7], CTB6.0, is used in our experiments as the target dataset. Articles of CTB6.0 were collected from four newswire sources, including Xinhua news agency, Table 1: Feature templates for CRF-based Chinese POS tagger Information Services Department of HKSAR (Hongkong), Sino-rama magazine (Taiwan), and ACE broadcast news. Following the split utilized in [8], we divided the dataset into blocks of 10 files. For each block, the first file was added to the CTB development data, the second file was added to the CTB testing data, and the remaining 8 files were added to the CTB training data. The basic statistics on this dataset are given in Table
The TCT corpus [9] is used as the source dataset. It contains 31K sentences, 739K words, and 87 distinct tag types. Contrary to CTB, TCT consists of articles from diverse domains, including lit-erature, newswire, and scientific monograph domain. We randomly sampled 1K sentences for testing and use remaining sentences as the TCT training data. Annotators first manually corrected word boundaries in the sampled 1K sentences according to the CTB word segmentation standard. Thus we have two versions of TCT testing data. For the convenience of reference, we use the acronym TBWC to refer to the TCT testing data before word correction and refer to the TCT testing data after word correction as TAWC. Words in TBWC and TAWC were given CTB POS tags by two annotators. Note that we did not modify the word
For all the experiments, accuracy is used as the performance metric which is defined to be the ratio of correct decisions over all the decisions made by the system.
In this experiment, we use CRFs [10] 4 and HMMs (the imple-mentation of HMMs in this paper is inspired by the work in [6], with some modifications to the unknown-word model, as described in Section 2). For CRFs, feature templates are listed in Table 1. Table 2 shows the accuracy scores of HMM-based and CRF-based taggers on the CTB development set, CTB test set, and TCT test sets, where both taggers are trained on the CTB training data. As shown, the accuracy of the HMM-based tagger is lower than, but comparable with the accuracy of the CRF-based tagger.
 Table 2: Performance results of HMMs and CRFs using the CTB training data
In this paper, we use the implementation CRF++. http://crfpp.sourceforge.net/
In order to evaluate to what extent label correspondence informa-tion contributes to the improvement of the transformation accuracy, we conducted the following experiments on TBWC and TAWC. We always use the CTB training data to estimate the parameters of the first two terms in Equation 1. Thus the experiments actually give a comparative study on diverse label correspondence learning meth-ods. With respect to the supervised tagger-based method, we need to choose the machine learning method for the implementation of POS taggers and the dataset used as the training data. Thus we have four versions of the supervised tagger-based method: HMM-T, HMM-S, CRF-T, and CRF-S. Here the suffixes, T(target) and S(source), indicate where the training data for POS taggers comes from. For example, HMM-T refers to the one that trains a POS tagger on the training data from the target dataset (CTB) and cal-culates mapping probabilities on the training data from the other dataset (TCT). With respect to the common word-based method, CW-T refers to the one that calculates prior distributions on the tar-get dataset (CTB), and CW-S is used to denote the reverse case. CW-C is a combination method that uses the numerical averages of mapping probabilities from CW-T and CW-S. Table 3 contains the performance scores.

Annotation transformation is frequently utilized for the construc-tion of dependency-based corpora. The work in [11, 12] describes transformation methods to create English dependency-based tree-banks from readily available constituency-based treebanks. Simi-larly, [13] employed a transformation method for constructing dependency-based propbank from PropBank [14]. The transformation methods of this kind are efficient and effective, however they are generally heuristic and specific to annotation standards.

Annotation transformation is also used to refine original anno-tations in a treebank in order to increase parsing accuracy, such as the work in [15]. Instead of transforming annotations in a single dataset, the work in this paper focuses on the transformation be-tween two disparate datasets.

The most related work is done in [16] where a transfer learning algorithm is applied to transfer knowledge from the source to the target dataset. But in this paper, we focus on the annotation trans-formation but not on transferring knowledge. We have proposed a probabilistic framework for the problem of POS annotation transformation. The framework is able to integrate information from both source and target datasets. In order to model the relationships between source and target tags, two label corre-spondence learning methods: supervised tagger-based and com-mon word-based methods were proposed. The potential of these methods were evaluated by transforming TCT annotations to those fitting the CTB standard. Experimental results demonstrate that both methods can aid the transformation model to achieve satisfac-tory accuracy.
This work was supported in part by the National Science Foun-dation of China (60873091). [1] D. Jurafsky and J. H. Martin. Speech and Language [2] M. Banko and E. Brill. Scaling to very very large corpora for [3] J. K. Low, H. T. Ng, and W. Guo. A maximum entropy [4] A. Ratnaparkhi. A maximum entropy model for [5] M. Collins. Head-driven statistical models for natural [6] S. M. Thede and M. P. Harper. A second-order hidden [7] N. Xue, F. dong Chiou, and M. Palmer. Building a [8] Z. Huang, M. P. Harper, and W. Wang. Mandarin [9] Q. Zhou. Phrase bracketing and annotating on chinese [10] J. Lafferty, A. McCallum, and F. Pereira. Conditional random [11] J. Nivre. Inductive dependency parsing. In Springer. , 34. [12] R. Johansson and P. Nugues. Extended [13] S. Ekeklint and J. Nivre. A dependency-based conversion of [14] P. Kingsbury, M. Palmer, and M. Marcus. Adding semantic [15] M. Johnson. Pcfg models of linguistic tree representations. [16] W. Jiang, L. Huang, and Q. Liu. Automatic Adaptation of
