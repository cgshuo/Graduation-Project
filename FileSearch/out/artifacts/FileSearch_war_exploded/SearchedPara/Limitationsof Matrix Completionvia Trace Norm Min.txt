 In recent years, compressive sensing attracts intensive at-tentions in the field of statistics, automatic control, data mining and machine learning. It assumes the sparsity of the dataset and proposes that the whole dataset can be recon-structed by just observing a small set of samples. One of the important approaches of compressive sensing is trace norm minimization, which can minimize the rank of the data ma-trix under some conditions. For example, in collaborative filtering, we are given a small set of observed item ratings of some users and we want to predict the missing values in the rating matrix. It is assumed that the users X  ratings are af-fected by only a few factors and the resulting rating matrix should be of low rank. In this paper, we analyze the issues related to trace norm minimization and find an unexpected result that trace norm minimization often does not work as well as expected. Compressive sensing has been proposed to solve sparse ma-trix problem in many fields including machine learning, au-tomatic control, and image compression. It applies the spar-sity of the datasets and assumes that the whole datasets can be recovered by just observing a small set of the data. One of the important approaches is minimizing the rank of a ma-trix variable subject to certain constraints. For example, in collaborative filtering (CF), the objective is to predict the missing values of a rating matrix. It is assumed that the ratings are affected by only a few of factors and the rating matrix should be of low rank. Given that directly minimiz-ing the rank of the matrix is NP hard, a commonly-used convex relaxation of the rank function is the trace norm, defined as the sum of the singular values of the matrix. Recently, a number of work has shown that the low rank solution can be recovered exactly by minimizing the trace norm [12; 6]. Furthermore, trace norm minimization starts to be widely used in many scenarios, such as multi-task learning [1; 3], multivariate linear regression [20], matrix classification formulation [16; 4], multi-class classification [2], etc. Among all the applications, an important one is matrix completion [15; 13; 17; 11; 5; 9], which aims at recovering a low-rank matrix based on a small portion of observed entries. Recently in [10], the two-way matrix completion problem is further generalized into a tensor completion problem and trace norm minimization is adapted to solve the problem. However, in this paper, we present an unexpected result that despite its popularity, the trace norm minimization does not work well in general. We show the limitations of trace norm minimization in several situations and analyze the reasons behind the failures. We mainly focus on two issues related to trace norm minimization. First, only with a small portion of observed samples, it is difficult to determine whether or not the real matrix is of low rank. The results will deteriorate significantly for a high rank matrix. Second, we analyze that rank minimization or trace norm minimization can lead to multiple solutions, which may produce unstable results. The paper is organized as follows. A formal introduction of trace norm minimization is first given, followed by the argument of its limitations with examples. Three sets of experiments are then performed to analyze the empirical results in applications including collaborative filtering and image recovery. A conclusion is made with possible future works of trace norm minimization. We use capital-bold letters such as X , M as matrices, and X ij denotes the ( i, j )-th element of X . In the problem defi-nition, we further denote the observed matrix as M and the observed entries as M ij where ( i, j )  X   X . In other words, if ( m, n ) /  X   X , the value of M mn is missing.
 In compressive sensing [7], matrix completion can be usually stated as where M is the observed matrix with some missing values, and X is the resulting matrix with all missing values filled. The objective is to find a matrix X which has low rank after selectively filling the missing values. However, the op-timization function in Eq. 1 is not convex and may require intensive computation to obtain the result. As an alterna-tive, one usually solves the following optimization problem which is the tightest convex relaxation of Eq. 1. where k X k  X  is the nuclear norm, or trace norm of the matrix X , which is the sum of its singular values. A more general trace norm minimization problem can be stated as where  X  is a parameter to control the confidence that the matrix X is of low rank. And f ( X ) can be any evaluation function on the matrix X , and in matrix completion, f ( X ) can be defined as the difference between X ij and M ij for ( i, j )  X   X .
 There are various approaches proposed to solve Eq. 2 or Eq. 3. For example, they can be formulated as a semidef-inite program [8; 15]; or some approximation can be em-ployed to deal with the non-smooth trace norm term [13; 17; 18; 11]. Recently, [9] proposes a subgradient method to approach the optimal solution; and [5] proposes a fast and simple algorithm which performs a thresholding in the singular values to obtain the optimal result. In the experi-ment section, we will use these two algorithms as examples to study trace norm minimization. Trace norm minimization has strong mathematical founda-tions and has been used in various applications such as ma-trix completion. However, there are still issues related to this category of approaches.
 First, it is not clear what type of matrices can be well re-covered by trace norm minimization. In practice, we usually have to assume that the resulting matrix has low rank and we expect that trace norm minimization can lead to a good recovery. However, just observing a small set of samples (e.g., 10% of the matrix entries can be observed), it is diffi-cult to guarantee that it is of low rank. In [19], it is further assumed that those rating matrices 1 in collaborative filter-ing are of low rank because it is assumed to be affected by just a few factors. Later experiment in Section 3 shows that this assumption may not be valid in all cases. A more pre-cise criterion is needed to calibrate which type of matrices are suitable to be recovered by trace norm minimization. Second, trace norm minimization or rank minimization can lead to multiple alternative solutions, which make the result unstable. We illustrate an example in Eq. 4. It is an incom-plete matrix with one missing value (denoted as  X ? X ). Note that most of the traditional matrix completion approach will assign the missing value as 2 because it has more 2s than 4s in the second column. However, to complete the matrix with rank minimization, we can either fill in the missing value with 2 or 4. Both solutions lead to a rank-2 matrix. In other words, there may be multiple solutions that can induce the recovered matrix to be of the same rank. Intu-itively, with more missing values, it has higher probability that there are multiple solutions that can lead to the same matrix rank. Hence, the result can be very unstable because it can end up in any of the solutions. We further study this those matrices record users X  ratings on a set of items. Each row describes all the item ratings of one user; each column represents the ratings of one item among all users. problem in Section 3.1 and Section 3.3. In this section, three sets of experiments are devised to an-alyze the performance of trace norm minimization. We first design a simple matrix as in Eq. 5 and drop part of the entries with  X ? X  as in Eq. 6. We then apply one of the state-of-the-art trace norm minimization approaches, singular value thresholding (SVT [5]), to recover the matrix as in Eq. 7. It is clear that none of the entry has been recovered correctly. However, the resulting matrix is of the same rank as the original matrix. Hence, in the view of low rank approximation or trace norm minimization, the objective has been achieved. But in the view of ground truth, the result is far from satisfactory. This phenomenon is also analyzed in Section 2.1: trace norm minimization can lead to multiple unstable solutions. Similar phenomenon is also studied in tensor completion case in Section 3.3. In this section, we apply two state-of-the-art trace norm minimization approaches to perform matrix completion in collaborative filtering (CF). The two approaches are singular value thresholding (SVT [5]) and multivariate linear regres-sion described in [9] (MultiRegression). Both approaches solve matrix completion with the constraint to minimize the rank of the final matrix. SVT works well in a simulated matrix completion dataset [5] and MultiRegression works well in a multi-task learning application which minimizes the trace norm of the shared parameter matrix among tasks [9]. In this experiment, we apply the two approaches to tackle the matrix completion problem in CF. The dataset used in the experiment is the Jester Recommendation dataset 2 , which is known as one of the benchmarks in CF. The dataset contains three separate sub-datasets which are summarized in Table 1. The Jester dataset describes millions of con-tinuous ratings (-10.00 to +10.00) of 100 jokes from 73,421 users, which are collected between April 1999 to May 2003. As shown in Table 1, most of the users only rate a small por-tion of the items (sparsity); hence the final matrix should have good chance to be of low rank, and the two approaches should work well. In the experiment, 70% of the ratings are blocked, and we use trace norm minimization to recover the whole matrix by using the 30% of known ratings.
 The experiment results are evaluated by the Mean Absolute Error (MAE). If p ij is the prediction for how user i will rate item j, the MAE for user i is defined as where c is the number of items user i has rated and  X  r ij ground truth. MAE for a set of users is the average MAE over all members of that set. Hence, a small MAE value indicates a good prediction. Fig. 1 summarizes the results on 10 runs where in each run we randomly sample 70% of the matrix entries to be blocked. Since both approaches can be tuned to control the confidence that the final matrix is of low rank, we plot the result with the decreasing rank. Meanwhile, we plot the result of random guessing, which randomly assign values to the blocked entries.
 There are two conclusions can be drawn from Fig. 1. First, the results given by SVT and MultiRegression almost over-lap because both of them solve the same optimization func-tion and MultiRegression embeds SVT as a sub-function 3 . Second, the errors evaluated by MAE grow with the de-creasing ranks. The best results are given when the ranks are large. When the ranks are smaller than 80, the results are worse than random guessing. It is important to em-phasize that although this benchmark dataset should have good chance to be of low rank (sparse collaborative filtering matrix as analyzed in [19]), its best result is given with full h ttp://eigentaste.berkeley.edu/dataset/ http://www.public.asu.edu/ jye02/Software/SLEP/index.htm rank. Thus, a more precise criterion is needed to determine what type of matrices is suitable to be recovered by trace norm minimization. In [10], trace norm minimization is furthered extended to handle multidimensional matrix completion, or tensor com-pletion. More specifically, trace norm minimization is ap-plied to recover images which are expressed as 3-dimensional tensors. The experiment results in [10] show that the ap-proach can effectively recover some blurred images. In this experiment, we change to another set of images from Cal-tech 256 4 benchmark dataset. Fig. 2(a) and Fig. 3(a) show the two images used in the experiment. We further erase a small part of the images as shown in Fig. 2(b) and Fig. 3(b). The approach in [10] is applied to recover the images by filling pixels in the blanks, and the results are shown in Fig. 2(c) and Fig. 3(c) after 2000 iterations. We further plot Fig. 2(d) and Fig. 3(d) to show that the convergence of the optimization function of the approach, where x-axis denotes the number of iterations and y-axis is the value of the objective function. Hence, although the algorithm has reached the optimal or near optimal solution as shown in Fig. 2(d) and Fig. 3(d), the results of the recovered images are not encouraging. It is important to mention that both of the images are quite simple especially the one in Fig. 2(a), which has clear patterns which are similar as the matrix in Eq. 5. However, traditional trace norm minimization ap-proach fails to recover the images correctly. The reason of the poor performance of Fig. 2 is similar as the matrix in Eq. 5. Trace norm minimization can lead to multiple opti-mal solutions and end up in any of them; the result can be arbitrary. We study and analyze the limitations of trace norm mini-mization in matrix completion. First, it is difficult to guar-antee that the real matrix should be of low rank. Second, trace norm minimization can lead to arbitrarily satisfied ma-trix with low rank. Three sets of experiments are analyzed in the application of collaborative filtering and image recovery. For example, in collaborative filtering, the performance of trace norm minimization drops with decreasing matrix rank. In summary, the paper aims at studying the unexpected is-h ttp://www.vision.caltech.edu/Image-Datasets/Caltech256/ sues when intending to put trace norm minimization into practice: As a future work, a more precise criterion is needed to de-termine what type of matrices is suitable to be recovered via trace norm minimization. One of the possibility is to develop a lower bound to study the number of possible low rank solutions to a given incomplete matrix, and only use trace norm minimization when the value of the bound is small. Another extension is to bring in more constraints to lead to a better and unique low rank solution. For instance, the missing value can be filled in with the majority value, in addition of satisfying the low rank constraint. As an exam-ple, the missing value in Eq. 4 can be filled with  X 2 X  under the majority-value constraint. [1] J. Abernethy, F. Bach, T. Evgeniou, and J. P. Vert. [2] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Un-[3] A. Argyriou, T. Evgeniou, and M. Pontil. Convex [4] F. R. Bach. Consistency of trace norm minimization. J. [5] J. Cai, E. J. Cand`es, and Z. Shen. A singular value [6] E. J. Cand  X es and B. Recht. Exact matrix completion via [7] M. Fazel. Matrix Rank Minimization with Applications. [8] M. Fazel, H. Hindi, and S. P. Boyd. A rank mini-[9] S. Ji and J. Ye. An accelerated gradient method for [10] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor com-[11] S. Ma, D. Goldfarb, and L. Chen. Fixed point and [12] B. Recht, W. Xu, and B. Hassibi. Necessary and suffi-[13] J. D. M. Rennie and N. Srebro. Fast maximum mar-[14] N. Srebro. Learning with Matrix Factorizations. PhD [15] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. [16] R. Tomioka and K. Aihara. Classifying matrices with a [17] M. Weimer, A. Karatzoglou, Q. Le, and A. Smola. [18] M. Weimer, A. Karatzoglou, and A. Smola. Improving [19] K. Yu, S. Zhu, J. Lafferty, and Y. Gong. Fast Nonpara-[20] M. Yuan, A. Ekici, Z. Lu, and R. Monteiro. Dimension
