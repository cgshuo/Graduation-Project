 The problem of time series classification has attracted great interest in the last decade. However current research assumes the existence of large amounts of labeled training data. In reality, such data may be very difficult or expensive to obtain. For example, it may require the time and expertise of cardiologists, space launch technicians, or other domain specialists. As in many other domains, there are often c opious amounts of unlabeled data available. For example, the PhysioBank archive contains gigabytes of ECG data. In this work we propose a semi-supervised technique for building time series classifiers. While such algorithms are well known in text domains, we will show that special considerations must be made to make them both efficient and effective for the time series domain. We evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms, handwritten documents, and video datasets. The experiment al results demonstrate that our approach requires only a handful of labeled examples to construct accurate classifiers. H.2.8 [ Database Management ]: Database Applications  X  Data mining Semi-supervised Learning, Ti me Series, Classification Time series data are ubiquitous and are of interest to many communities. Such data can be found in virtually all avenues of human endeavor including medicine, aerospace, finance, business, meteorology, and entertainment [7][12][17][18]. The problem of time series classification has been the subject of active research for decades [3][4][6][9][11][14]. However current methods are limited by the need for large amounts of labeled training data. In reality, such data may be very difficult or expensive to collect. For example, it may require the time and expertise of cardiologists [7], space launch technicians [12], entomologists [18], or other domain experts to manually label the data. As in many other applications, copious amounts of unlabeled data are often readily available [7]. In this work we propose a semi-supervised technique for building time series classifiers that takes advantage of the large collections of unlabeled data. As we will demonstrate, our approach requires only a handful of labeled examples to construct accurate classifiers. Furthermore, we are able to leverage off recent advances in time series query filtering to use these classifiers very efficiently, particularly for streaming problems [18]. To enhance the readers X  apprecia tion of the diversity of domains which can benefit from a semi-supe rvised technique for building time series classifiers, we begin by considering an application that we will later address experimentally. Indexing of handwritten documents: There has been a recent explosion of interest in indexi ng handwritten documents [13]. It has recently been shown that simply treating the words as  X  X ime series X  (see Figure 1) is an extr emely competitive approach [13] for classifying (and thus inde xing) handwritten documents. The fundamental problem in creating highly accurate handwriting classifiers is that they must be trained on each individual X  X  particular handwriting; a classi fier built for George Washington will not generalize to Isaac Newton. However the cost of obtaining labeled data for each word, for every individual is very expensive as measured in human time. A semi-supervised approach where a user annotates just a few training examples would have great utility [13]. Figure 1: A) A sample of text written by George Washington. B) 
The word  X  X lexandria X  after having its slant removed. C) A time series created by tracing the upper profile of the word (Image courtesy of Raghavan Manmatha, used with permission) The rest of this paper is organi zed as follows. In Section 2 we review background material. We introduce our semi-supervised time series classification algorithm in Section 3. Section 4 sees a comprehensive empirical evaluation. Finally in Section 5 we offer some conclusions and directions for future work. In order to frame our contribution in the proper context, we begin with a review of the necessary background material. The idea of using unlabeled data to help classification may sound initially unintuitive. However, se veral studies in the literature have indicated the utility of unlabeled data for classification [5]. Although unlabeled data alone are generally insufficient to yield better-than-random-guess cl assification, they do contain information which can help classifi cation. In Figure 2, we have a dataset of three labeled instances and eight unlabeled instances (U). We need to classify the instance marked with  X  ?  X , which clearly belongs to the F (female) class. However this particular image happens to show the actor in a pose which is very similar to one of the M (male) instances, M 1 , and is thus misclassified
Figure 2: A simple example to motivate semi-supervised classification. The instance to be classified (marked with  X ? X ) is actually a F (female) but happens to be closer to a M (male) Note that while F 1 happens not to be a close match to the instance awaiting classification, it is a close match to the unlabeled instance U 4 . Because it is such a good match to this instance, we could simply change the label from U 4 to F 2 , and add it to our dataset of labeled instances. In fact, the basic tenet of semi-supervised learning is that we can do this repeatedly, and thus end up with the situation shown in Figure 3. Figure 3: The small dataset of labeled instances shown in 
Figure 2 has been augmented by incorporating the previously unlabeled examples. Now the instance to be classified (marked with  X ? X ) is closest to F 5 , and is correctly classified It is important to note that the usefulness of unlabeled data depends on the critical assumption that the underlying models/ features/kernels/similarity functi ons match well with the problem at hand [20]. Learning from both labeled and unl abeled data is called semi-supervised learning (SSL). There are many semi-supervised learning methods proposed in the literature, including SSL with generative models, SSL with low density separation, graph-based methods, co-training methods, and self-training methods [2][20]. Generative models assume that the data are drawn from a mixture distribution which can be identified by large amounts of unlabeled data. Low density separation approaches try to leverage off the assumption  X  the decision boundary should lie in a low density region  X  by pushing the decision boundary away from the unlabeled data. Graph-based met hods represent the data by nodes in a graph and use algorithms in graph theory afterwards. The idea of co-training was first proposed by Blum and Mitchell [1]. It trains two classifiers separate ly, and the predictions of one classifier are used to enlarge the training set of the other. A similar method is semi-supervis ed learning, which trains a classifier and use its own predictions to teach itself. Because of its generality and very few assumpti ons, we use self-training as a starting point for our work. For concreteness, we begin with a definition of our data type of interest, time series . 
Definition 1 . Time Series : A time series T = t ordered set of m real-valued variables. Time series data usually come in two formats: as a long time series or as a set of short time series. Data miners are typically not Therefore if we are given a long time series, we convert it into a set of short time series, where each time series in the set is a subsequence of the long time series. 
Definition 2 . Subsequence : Given a time series T of length m , a subsequence C p of T is a sampling of length w &lt; m of contiguous positions from T , that is, C p = t p ,...,t  X  m  X  w + 1. The extraction of subsequences from a time series is achieved by use of a sliding window . 
Definition 3 . Sliding Window : Given a time series T of length m , and a user-defined subsequence length of w , all possible subsequences can be extracted by sliding a window of size w across T and extracting each subsequence C p . The most common distance measure for time series is the Euclidean distance . 
Definition 4 . Euclidean Distance : Given two time series (or time series subsequences) Q and C both of length n , the 
Euclidean distance between them is: Before calling the distance function, each time series subsequence is normalized to have mean zero and a standard deviation of one, because it is well understood that in virtually all settings, it is meaningless to compare time series with different offsets and amplitudes [3][4][9][10][13][18]. 
Definition 5 . Time Series Classification: Given a set of unlabeled time series, the task of time series classification is to map each time series to one of the predefined classes. Note that in real world domains, it is typically not the case that we have two or more well defined cl asses. Usually there is only one (or some small number of) way(s) to be in the positive class, while there are an essentially infinite number of ways to be in the negative class. In addition, in most cases positive labeled examples are rare, but unlabeled data is abundant. Based on these two observations, we focus on building binary time series classifiers for extremely imbalanced class distributions, with only a small number of labeled examples from the positive class. In this section, we show the special considerations necessary to convert a one-nearest-neighbor cla ssifier to a semi-supervised framework. The problem of time series classification has attracted great interest recently. Although many algorithms have been proposed, it has been shown that one-near est-neighbor with Euclidean distance is very difficult to beat [9]. Therefore we have adopted one-nearest-neighbor with Euclidean distance as our base classifier. Note that it also needs a large labeled training set to work well. Below we show how to apply semi-supervised learning to make it feasible for the situation where only a small set of labeled data is available. The idea is formalized in Table 1. Given a set P of positively labeled examples and a set U of unlabeled examples, the algorithm iterates the following procedure. First, use P and U to train the one-nearest-neighbor classifier C (note that we regard instances in P as positive examples and instances in U as negative exampl es). Second, use classifier C to classify the unlabeled set U . Third, select one unlabeled example which is nearest to any instance in set P (breaking ties randomly), and add it to P . Table 1: Semi-supervised time s eries classification algorithm In this subsection, we will discuss the stopping criterion for training the classifier, an issue we have deliberately ignored to this point. Ideally we would like the training procedure to stop when the performance of the classifier begins to deteriorate. Since we are using a distance-based classifier, the distance statistics may give us some hint about how well the classifier is doing. To develop our intuition, we perform self-training classification on several datasets. For each iteration in the training procedure, we record the precision-recall breakeven point (explained in Section 4) and the distance between the closest pair in the labeled positive set. Figure 4 shows the results obtained on an ECG dataset. Note that the minimal nearest neighbor distance decreases dramatically in the first few iterations, stabilizes for a relatively long time, and drops agai n. Interestingly, the precision-recall breakeven point achieved by the classifier has a corresponding trend of increasing, stabilizing, and decreasing. Similar observations were made on other datasets. These indicate that, even though the question of when to stop the self-training procedure remains unsolved and is an open problem, we can use the change in the minimal nearest neighbor distance in the labeled positive set as a good heur istic in most cases. By the end of the training, much more data in the training set are labeled, and we can use the classifier to classify other datasets. At first glance, this is easy. For each instance to be classified, check whether its nearest neighbor in the training set is labeled or not, and assign it the corresponding cla ss label. However recall that the training set is huge (because of the enormous amount of negative examples). Comparing each instance in the testing set to each example in the training set is untenable in practice. To make the classification tractable, we modify the classification scheme of the one-nearest-neighbor classifier, using only the labeled positive examples in the training set. If an instance to be classified is within r distance to any of the labeled positive examples, it will be classified as positive. Otherwise it is negative. Recently we have successfully applied this scheme to the problem of monitoring streaming time series for a set of predefined patterns [18]. A natural value for r would be the average distance from a positive example to its n earest neighbor. The intuition is that if the positive examples we have seen before tended to be about r apart, then a future positive object will probably also be within r of one (or more) positive example(s) in the training set. Paradoxically, we may be victims of our own success. By greatly enlarging the size of the labeled positive set with our semi-supervised algorithm, it appears th at we will greatly increase the time taken to classify new instances. Fortunately this is not the case. We can leverage off an envelope-based lower-bounding technique [18] to speed up the classification procedure. For example, in [18] we applied this technique on an ECG dataset and the speedup achieved is more than 100 times. Because we are focusing on the effectiveness of the semi-supervised learning classifier in this paper, we w ill not discuss the speedup technique any more. We refer interested readers to [18] for more details. In this section, we test our se mi-supervised learning classifier with a comprehensive set of e xperiments on diverse domains. We compare the semi-supervised approach to a naive k -nearest-neighbor approach, where the k nearest neighbors of the labeled positive set are classified as positiv e and others as negative. As the reader may already appreciate, the setting of k is a non-trivial problem, since the classifier does not know in advance how many positive examples there are in th e testing set. To help the strawman achieve the best performance, we allow it to search over all possible values of k and only report the best result. The performance of the classifier at each iteration is reported using precision-recall breakeven point. Since the class distribution is highly skewed, accuracy is not a good performance metric. The classifier can simply classify everything as negative to ensure high accuracy. Note that precision-recall breakeven point is a standard information retrieval m easure for binary classification [8][15]. Precision and recall are defined as: The precision-recall breakeven point is the value at which precision and recall are equal [8]. Heartbeat classification has received a lot of attention because of the large amounts of freely available data and the potential applications in medical field. Ou r first experiment is on an ECG dataset obtained from the MIT-BI H Arrhythmia Database [7]. Each data record in the ECG dataset is a time series of the measurements recorded by one electrode during one heartbeat. The data has been annotated by cardiologists and a label of normal or abnormal is assigned to each data record. Of the 2,026 data records in the dataset, 520 were identified as abnormal and 1,506 were identified as normal. All the data records have been normalized and rescaled to have length 85 (recent results suggest that we lose nothing by rescaling [16]). We randomly split the data, using half for training and ha lf for testing, as summarized in Table 2. Because usually cardiologists are more interested in the occurrences of the abnormal heartbeats, here abnormal heartbeats are our target (positive class). 
Table 2: Number of positive and negative instances in the training set and the testing set for ECG Dataset For the semi-supervised approach, we randomly choose 10 positive examples in the training set as the initial labeled positive set P . In each iteration, the semi-supervised algorithm adds one example to the positive set P and uses the adjusted training set to classify the testing set. The precision-recall breakeven point achieved is recorded. Note that the initial labeled set P has an effect on the performance of the classifier (a good initial set P may give the classifier a high precision-recall in the beginning while a bad initial set P may take the classifier more iterations to achieve good performance). To avoi d the bias introduced by the initial set, we ran the experiments 200 times and report the results in Figure 5. The bold line is th e average performance over the 200 runs. The gray lines bounding it from above and below are one standard deviation intervals. Note that the precision-recall breakeven value increases dramatically in the beginning and stabilizes after about ten iterations. On average, the maximal precision-recall breakeven value achieved by the semi-supervised approach is 94.97%. The shaded area in Figure 5 is where the performance of the semi-supervised classifier deteriorates because it begins to ingest negative examples. We then ran another 200 experiments (each time with the same initial labeled set P as used in the semi-supervised experiment) for the naive k -nearest-neighbor approach (with k = 312). However, even with the optimal k value, the k -nearest-neighbor approach only achieves an aver age precision-recall breakeven value of 81.29%, which is much lower than that of the semi-supervised approach. This show s that with the help of the unlabeled data, the semi-supervised approach can greatly increase the performance of the classifier. In second experiment, we consid er classification of handwritten documents. We test on the Word Spotting dataset, which was created by Rath and Manmatha for word image matching [17]. It contains 2,381 word images from 10 handwritten pages. We take the images of 50 common words such as  X  X he X ,  X  X nd X , etc. and obtain 905 instances in total. Each word image is represented by a four dimensional time series whic h describes the profile of the image. For example, in Figure 1, we have shown the upper profile of the word  X  X lexandria X . For simplicity we only consider the first dimension of each image, which is of an average length of 270. Here we focus on the two-cl ass problem of differentiating the word  X  X he X  from others. In total, there are 109 images for word  X  X he X  and 796 images for othe r words. In this experiment, we use the same 905 images both for training and testing, as summarized in Table 3. 
Table 3: Number of positive and negative instances in the training set and the testing set for Word Spotting Dataset As before, for the semi-supervised approach, each time we randomly choose 10 positive examples in the training set as the initial labeled positive set P and record the precision-recall breakeven point for each iteration. We repeated the experiment 25 times and the results are shown in Figure 6. The bold line is the average performance over the 25 runs, and the gray lines are one standard deviation intervals. We can see that the performance increases steadily at the beginning, reaches its maximal value 86.2% at about fifty iterations, and then begins to decrease (the shaded area in Figure 6). We then ran the same 25 experiments for the naive k -nearest-neighbor approach (with k = 109). On average, the precision-recall breakeven value obtained by the k -nearest-neighbor approach is only 79.52%. Figure 6: Classification performance on Word Spotting Dataset Handwritten text is an intuitive domain so we spend more time analyzing its results. For example, it is instructive to take a closer look at what happened during the training procedure. Figure 7 shows the changes of the rankings of two instances during the training process, where Image 19 is a positive example and Image 585 is a negative example. Th e ranking of an instance is determined by its distance to the labeled positive set  X  the larger the distance, the higher the ranking. So an instance with higher ranking has lower probability to eventually be classified as positive. In Figure 7, as training begins, Image 19 has a relatively high ranking, while Image 595 has a relatively low ranking. This represents a bad initial labeled set, where Image 19 happened to be similar to none of the examples in the initial labeled positive set, while Image 595 is simila r to one or more of them. Fortunately, even with a bad start, the semi-supervised learning classifier is able to correctly label more positive examples, which in turn helps it model the positive ex amples better. As a result, the ranking of Image 19 decreases and the ranking of Image 585 increases after several iterations. 
Figure 7: Ranking changes of two instances in Word Spotting dataset during semi-supervised training For our last experiment, we revi sit the classification problem in Figure 2 on a realistic dataset. The dataset was obtained by capturing two actors transiting betw een yoga poses in front of a green screen. It has been shown recently that in many domains it can be useful to convert images into pseudo time series. Therefore we have converted the motion capture data into time series by a well-known technique as in Figure 8. 
Figure 8: Shapes can be converted to time series. The distance from every point on the profile to the center is measured and treated as the Y-axis of a time series In total, we have 316 time series with an average length of 426. Among them, 156 time seri es came from the female actor and 150 time series came from the male actor. We use the same dataset both for training and testing, as shown in Table 4. 
Table 4: Number of positive and negative instances in the training set and the testing set for Yoga Dataset We ran the experiment 10 times, each time randomly choosing one positive example as labeled. The results are shown in Figure 9. As we can see, the precision-recall breakeven point increases steadily with the number of iterations, and gets to a maximum of 89.04% on average. While the sa me experiments on the naive k -nearest-neighbor approach (with k = 156) only achieves an average precision-recall of 82.95%. It is well known that building accu rate classifiers requires large quantities of labeled data and such labeled data is often difficult to obtain. To mitigate this discrepancy, we propose a semi-supervised learning framework to build accurate time series classifiers when only a small set of labeled examples is available. While there are many semi-supervised algorithms in other domains, their underlying assumptions rarely hold for time series data. Special considerations have been taken to make the semi-supervised classification both efficient and effective for the time series domain. The experimental re sults show that the reduction in the number of labeled examples n eeded can be dramatic: our self-training classifiers require only a handful of labeled examples to achieve high precision-recall. This suggests that the self-training method of using unlabeled data has a potential for significant benefits in time series classification. There are many directions in which this work may be extended. We intend to perform a thorough investigation on the stopping criterion for the training process. In addition, we plan to extend our framework to other distance measures which have been shown to be effective, for ex ample, Dynamic Time Warping (DTW) [16]. Finally, we are conducting a field study of insect classification using the semi-supervised approach. We gratefully acknowledge th e datasets donors. We also acknowledge insightful comments from Dr. Christian Shelton. Thanks also to Helga Van Herl e M.D. for her expertise in cardiology, Dr. Raghavan Manmatha for help with the Word Spotting dataset, and Xiaopeng Xi fo r help with the Yoga dataset. Reproducible Research Statement: In the interests of competitive scientific inquiry, all da tasets used in this work are freely available at the following URL [19]. This research was partly funded by the National Science Foundation under grant IIS-0237918. [1] Blum, A. &amp; Mitchell, T. (1998). Combining labeled and [2] Chapelle, O., Scholkopf, B., &amp; Zien, A. (2006). Semi-Supervised [3] Chen, L. &amp; Kamel, M. S. (2005). Design of Multiple Classifier [4] Chen, L.,  X zsu, M. T., &amp; Oria, V. (2005). Using Multi-Scale [5] Cohen, I., Cozman, F. G., Sebe, N., Cirelo, M. C., &amp; Huang, T. [6] Cohen, W. (1993). Efficient pruning methods for separate-and-[7] Goldberger, A., Amaral , L., Glass, L., Hausdorff, J., Ivanov, P., [8] Joachims T. (1998). Text categor ization with support vector [9] Keogh, E. &amp; Kasetty, S. (2002). On the need for time series data [10] Keogh, E., Lin, J., &amp; Fu, A. ( 2005). HOT SAX: Efficient finding [12] Landford, J. P. &amp; Quan, A. (2002). Evolution of knowledge-[13] Manmatha, R. &amp; Rath, T. M. (2003). Indexing of Handwritten [14] Nanopoulos, A., Alcock, R., &amp; Manolopoulos, Y. (2001). [15] Nigam, K., Mccallum, A. K., Th run, S., &amp; Mitchell, T. (2000). [16] Ratanamahatana, C. A. &amp; Keogh. E. (2004). Everything you [17] Rath, T. &amp; Manmatha, R. (2003). Word image matching using [18] Wei, L., Keogh, E., Van Herle, H., &amp; Mafra-Neto, A. (2005). [19] Wei, L. (2006). http://www.cs.u cr.edu/~wli/selfTraining/ [20] Zhu, X. (2005). Semi-supervised learning literature survey. 
