 aspects of image clustering. Some researchers work on dividing the content of a single sensing images. For this kind of image, we can assign every pixel to some class such as mountain, river, cloud, and so on [1]. On the other hand, some researchers focus on the networks or probabilistic method as classifier [4]. 
Recently there has been some research interest in applying central clustering manner, a structural characterization is adopted. For instance, both Lozano and sample can be obtained from the super-graph using edit operations. However, the way in which the super-graph is learned or estimated is not statistical in nature. Bagdanov and Worring [7] have overcome some of the computational difficulties associated with the above method by using continuous Gaussian distributions. Qiu and Hancock problem of graph clustering in a hierarchi cal manner. Luo et al. [9] have shown how components analysis (PCA) on the vectorized adjacency matrix. They commence by pattern-space. eigenvectors of graph corresponding to the obj ect structure. In a different manner, this Euclidean distance between points in the same image. Secondly, the normalized feature vector formed from eigenvalues can be projected to eigenspace alternatively to compute the distance between pairwise images. Thirdly, with the distances available, we can cluster the images easily. Finally, the experiments on both synthetic data and real-world images demonstrate the feasibility of our method. clustering and a theoretical analysis is given to support our approach. In Section 3, we conduct some experiments to validate out method. Finally, Section 4 summarizes some conclusions. 2.1 Spectral Representati on of Feature of Image 
GVE = where Kkk EVV  X  X  is the edge-set. For each k G , we compute the indexed i and column indexed j is defined as: 
With the matrix k A available, we can calculate the eigenvalues k  X   X  by solving the eigenvalues in descending order, i.e. 12 k V kk k  X  X   X   X  X  X  X  L . Furthermore, we can image. In other words, we can use spectrum to represent the object structure. 2.2 Spectral Edit Distance compute the mean feature vector and the covariance matrix. The feature vector is: 
And the covariance matrix is column t u is an eigenvector of  X  associated with eigenvalue t  X  . Then we obtain an orthogonal vector set T U . 
The graph spectrum can be projected onto referenced set T U of the covariance 
ZU  X  . For each pair of graphs k G and l G , we compute the Euclidean with 2.3 Theoretical Analysis ,, , m II I L and m images 12 ,,, m JJ J L as well as containing n points. points of these 2 m images, and let 11 () I G AA = , 11 () J G = BB . Then 
For some permutation matrices k P , k Q with order n , we can obtain 
With eigenvalues , kk  X  X  available, we form these eigenvalues to a vector and normalize it, then and the mean of this vector is 
So we have the covariance matrix as follows: where 2 {,,,} n normalized and orthogonal to each other, and 2 diagonal matrix. Then we have 
Consequently we can obtain a distance matrix as follows: 
By means of the above methods, we can easily cluster the 2 m images ,, , m
II I L and 12 ,,, m JJ J L into two groups with the former m images in one group and the latter m images in the other. following matrix: and those 2 m images can also be clustered via the method discussed earlier. real-world data are used to explain variatio ns of graph structure associated with graph spectrum. 3.1 Synthetic Data Analysis shown in Fig.1 are generated by performing a series of similar transformations on the letters  X  X  X  and  X  X  X . And we obtain 20 synthetic images respectively. Then we acquire the feature vectors formed from those eigenvalues by performing SVD on the weighted adjacency matrix and normalizing them. Based on the above discussion, we two different feature vectors are obtained. Next we calculate the mean of them and the covariance matrix. And then we derive a distance matrix. Fig. 2 gives the distribution of the distance in two-dimension for these two groups of synthetic images. Moreover, another group of synthetic data is generated by performing a series of affine transformations on a house shown in Fig. 3, and the projection of the distance matrix in the two-dimensional space is shown in Fig. 4. areas denote that the value of Euclidean dist ance is 0. When the color approaches dark indicates that the structural variation of these two images is extremely large. 3.2 Real-World Image Experiments Now we use the real-world data with unknown correspondences to study the relationship between the graph structure and spectral distance. The first example is the We compute the covariance matrix and the distance matrix. Fig 6(a) demonstrates the distribution of the distance matrix in the two-dimensional space. 
The second example is the CMU image sequence shown in Fig. 5 (b). 40 images each image respectively. We compute the covariance matrix and the distance matrix. Fig. 6(b) shows the projection of distance matrix in two-dimension. (a) (b)
From the experiments of the real-world images, we can see that the Euclidean distance between two images increases when the viewing angle changes. Fig. 6 shows the tendency of the Euclidean distance. When two graph indices are nearer, the value Whereas, the color approaches light blue, the Euclidean distance is larger. That is to graph structure is related to the spectral distance. matrix of the relational graph constructed on an image and obtain a covariance matrix pairwise images. Experiments of synthetic data and real image demonstrate the feasibility of our method. 
Our future plans involve studying in more detail the object structure resulting from studied here can be used for the purpose of organizing image databases Acknowledgments. This work is supported by National Science Foundation of China (Grant No. 10601001), Anhui Provincial Natural Science Foundation of China (Grant No. 050460102 070412065), Natural Science Foundation of Anhui Provincial Education department (Grant No. 2006KJ030B) and Innovative research team of 211 project in Anhui University. 
