 Combining multiple documents to represent an information object is well-known as an effective approach for many Infor-mation Retrieval tasks. For example, passages can be com-bined to represent a document for retrieval, document clus-ters are represented using combinations of the documents they contain, and feedback documents can be combined to represent a query model. Various techniques for combina-tion have been introduced, and among them, representation techniques based on concatenation and the arithmetic mean are frequently used. Some recent work has shown the poten-tial of a new representation technique using the geometric mean. However, these studies lack a theoretical foundation explaining why the geometric mean should have advantages for representing multiple documents. In this paper, we show that the arithmetic mean and the geometric mean are ap-proximations to the center of mass in certain geometries, and show empirically that the geometric mean is closer to the center. Through experiments with two IR tasks, we show the potential benefits for geometric representations, in-cluding a geometry-based pseudo-relevance feedback method that outperforms state-of-the-art techniques.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models Algorithms, Measurement, Experimentation multiple documents, information geometry, geometric mean
A typical goal in Information Retrieval (IR) is to find relevant documents, where we rank the documents using a representation for a single document. Often, however, a representation for multiple documents is needed. For exam-ple, tasks such as relevance feedback, passage retrieval and resource selection in distributed information retrieval or in aggregated search, use representations for sets of multiple documents.

One of standard approaches for relevance feedback is to estimate an underlying relevance model from given feedback documents and sample likely terms from the model for query expansion. That is, the estimated underlying model can be considered as a representation of the feedback documents. In passage retrieval, representations of text passages can be used to rank passages or documents. In the latter case, we represent a document using a combination of some or all of its passages. In resource selection tasks, the resource or col-lection is represented using the documents in the collection.
As many tasks require representations for multiple docu-ments, various approaches have been introduced. Among them, representation techniques based on the arithmetic mean and concatenation are frequently used. Representa-tion techniques based on the arithmetic mean literally com-pute the arithmetic mean of multiple language models or vector representations. Representation techniques based on concatenation make a large document by concatenating mul-tiple documents and use a language model or vector to rep-resent the large document.

In addition to traditional group representation techniques, some recent studies show the potential of a new representa-tion technique, the geometric mean representation of lan-guage models [26, 30, 11, 31]. Liu and Croft [26] com-pared various representation techniques for cluster retrieval and demonstrated that representations using the geomet-ric mean outperformed others via empirical evaluation. Seo and Croft [30] applied a resource selection technique based on the geometric mean to blog site search. Moreover, Elsas and Carbonell [11] and Seo et al. [31] showed that a thread representation using the geometric mean of postings in the thread can be a good choice for online forum search.
The previous work which uses the geometric mean to rep-resent a group of documents, however, did not theoretically analyze the geometric mean in the language modeling frame-work. In other words, although they have demonstrated the performance of representation techniques based on the geo-metric mean empirically, theoretical evidence or the assump-tions behind the geometric mean have not been sufficiently addressed to justify its use in IR.

Therefore, in this paper, we give a theoretically grounded explanation for geometric mean-based techniques for rep-resenting multiple documents. To do this, we consider In-formation Geometry as a tool and discuss how the arith-metic mean as well as the geometric mean can be inter-preted in certain geometries. More specifically, we show that the arithmetic mean and the geometric mean relate to the Fr  X  echet sample mean which minimizes the Fr  X  echet sample function. Furthermore, we empirically show that the geo-metric mean is closer to the Fr  X  echet mean.

In addition, we address two applications considering the geometric interpretation: cluste r retrieval and pseudo-relevance feedback. Particularly, for pseudo-relevance feedback, we in-troduce a variation of the relevance model [21], the geometric relevance model, and show that this new approach performs better than the relevance model.

The remainder of this paper is organized as follows. Sec-tion 2 reviews previous work. In Section 3, we introduce the Fr  X  echet mean and geometric representations correspond to the Fr  X  echet mean in two different metric spaces using Information Geometry. In Section 4, we provide empirical evidence for the geometric representations through experi-ments for two IR tasks. Section 5 discusses other evidence for the geometric representations. Section 6 concludes this paper.
Combining multiple evidence is one of the most frequently addressed topics in Information Retrieval. Belkin et al. [2] showed that different representations of the same infor-mation object leads to different results and combinations of such representations can improve retrieval performance. Various combination heuristics suggested by Fox and Shaw [12] and analyzed by Lee [23] are still used in many IR tasks such as passage retrieval and resource selection. Using passage-level evidence [7, 25, 3] for document retrieval nec-essarily requires combination techniques. Resource selection where a collection is represented by its own documents [6, 32] actively uses combination techniques as well.
Relevance feedback (and pseudo-relevance feedback) is an-other task using combination-based representation techniques. To estimate a query model for query expansion, the top ranked documents are combined. Rocchio [29] introduced a feedback technique to combine positive or negative feedback documents in vector spaces. Lavrenko and Croft [21] in-troduced a technique that estimates a underlying relevance model in the language modeling framework. In fact, these standard relevance feedback approaches implicitly use the arithmetic mean. Recently, Collins-Thompson and Callan [9] used a parametric approach using re-sampling to esti-mate a posterior Dirichlet distribution for the documents. That is, they use the mean and the variance of the Dirichlet distribution to get a feedback model.

The geometric mean-based representation technique was relatively recently introduced. Liu and Croft [26] demon-strated that representation by the geometric mean works well for cluster retrieval via comparisons with vairous rep-resentation techniques. Seo and Croft [30] suggested a re-source selection technique by the geometric mean for blog site retrieval. Furthermore, the technique was shown to work well for thread search in online forums [11, 31]. The geomet-ric mean is often used in other fields. For example, Kogan et al. [18] used the geometric mean for k -means cluster-ing. Veldhuis [34] showed that a centroid of the symmetri-cal Kullback-Leibler divergence is related to the arithmetic mean and the normalized geometric mean.

In this paper, to justify the use of the geometric mean in IR, we find evidence from Information Geometry. Rao [28] and Jeffreys [14] are the first people who considered the Fisher information metric as a Riemannian metric. Later, Efron [10] focused on differential geometry in statistics con-sidering the curvature of statistical models. Recently, Lebanon [22] applied the theory to many machine learning tasks. See Amari and Nagaoka [1] and Kass and Vos [16] for compre-hensive introduction to Information Geometry.
We introduce the Fr  X  echet mean and derive the mean in two different metric spaces, i.e., the Euclidean metric space and the Riemannian manifold defined by the Fisher information metric.
Let us consider a Riemannian manifold M with a distance measure dist ( x , y )where x and y are points on the manifold. Assume that we have a distribution Q on a convex set U X  M . Now we define a function F : M X  R as follows:
This function is known as the Fr  X  echet function. A set of points which minimize the function is called the Fr  X  echet mean set of Q . If there is only a point in the set, the point is called the Fr  X  echet mean. This general notation for a center or centroid associated with a probability distribution was introduced by Fr  X  echet [13] and Karcher [15]. This mean is called by various names, e.g., the center of mass, barycenter, Karcher mean and Fr  X  echet mean. In this work, we refer to this mean as the Fr  X  echet mean 1 . The concept of the Fr  X  echet mean is general and not limited to any specific metric; ac-cordingly, this can be applied to any metric space. Indeed, as we will see soon, it also generalizes the ordinary Euclidean mean.

Kendall [17] proved that if the support of Q is in a geodesic ball of sufficiently small radius r , then one Fr  X  echet mean uniquely exists. As we see later, we consider a statistical manifold for multinomial distributions, and the distributions are mapped onto a simplex or a positive sphere. Since the mapped area is sufficiently small, a unique Fr  X  echet mean exists. For example, in case of a sphere, the radius of the geodesic ball is  X / 4 and the positive sphere is contained in the ball.

If we have n unique points p 1 , p 2 ,  X  X  X  , p n in m i.i.d. sam-ples from distribution Q , then we consider the sample Fr  X  echet mean which minimizes the Fr  X  echet sample function given by where  X  Q is an empirical distribution estimated from the sam-ples.

Bhattacharya and Patrangenaru [5] showed that every measurable choice from the Fr  X  echet sample mean set of  X  a strongly consistent estimator of the Fr  X  echet mean of Q .In this paper, we consider multiple documents to represent as samples and the Fr  X  echet sample mean as a representation.
Strictly speaking, this is the intrinsic Fr  X  echet mean in that we use a geodesic distance. However, since we address only the intrinsic Fr  X  echet means in this paper, we omit term  X  X n-trinsic X . Therefore, we address how to compute the sample Fr  X  echet mean from the multiple documents in the following sections.
Let X  X  begin with the Euclidean metric space. We assume that terms observed in a document are samples from a multi-nomial distribution and each document has a distinct distri-bution. Assuming a conjugate Dirichlet prior, we estimate the multinomial distribution, i.e. a language model, using Dirichlet smoothing [35] as follows: where tf w,D istheoccurrenceofterm w in document D , cf w is the occurrence of w in a set of observations C considered for the prior distribution (typically, a corpus), | D | is the number of observations, i.e. the length of D , | C | is the length of C ,and  X  is the Dirichlet smoothing parameter. Note that P ( w | D ) is a parameter which corresponds to outcome w in the multinomial distribution.

The size of vocabulary of a language model is defined as the number of terms observed in C , which also determines the number of dimensions of the Euclidean metric space for a multinomial distributions. When the number of dimensions is n + 1, a multinomial distribution corresponds to a point in n -simplex P n which is defined as follows: An example of 2-simplex embedded in 3-dimensional Eu-clidean space is shown in Figure 1.

Since a geodesic linking two points in n -simplex is a straight line, the distance between two multinomial distributions is calculated by the Euclidean distance as follows:
Consider multinomial distributions of k given documents, p , p 2 ,  X  X  X  , p k as samples from distribution Q over the n -simplex. Then, the Fr  X  echet sample function is given by
Therefore, we have the following optimization problem to obtain the Fr  X  echet sample mean. It is trivial to solve this problem using the method of Lagrange multipliers. Finally, we have a solution as follows: This is the Fr  X  echet sample mean in the Euclidean metric the same as the ordinary Euclidean mean or the arithmetic mean. Therefore, the Fr  X  echet sample mean in the Euclidean metric space generalizes the arithmetic mean.

We use the Fr  X  echet sample mean as a representative multi-nomial distribution for the given group of multiple docu-ments. Many IR approaches assume that data is embedded in the Euclidean geometry. However, assumptions of non-Euclidean geometries may lead to a better understanding of data. We here consider a Riemannian space where a Riemannian met-ric is the Fisher information metric. This metric space is used for investigating the geometric structures of statistical models in most of the Information Geometry literature [28, 1, 16]. Furthermore, a number of approaches assume this metric space for statistical inference and machine learning [20, 22, 1]. Particularly, for text classification, Lafferty and Lebanon [20] showed that techniques based on this metric space perform better than techniques based on the Euclidean metric.

The Fisher information metric is defined as follows: where  X  is a point in a differential manifold and corresponds to a statistical model in a parametric familty p ( x ;  X  ), i and j are indices for a coordinate system. In this work, it is easy to think that  X  is a multinomial model for a document while i and j are indices for unique terms in vocabulary.
This metric has some nice properties. By Cram  X  er-Rao in-equality [28], the variance of unbiased estimators is bounded by the inverse of the metric. Particularly, an unbiased es-timator achieving the bound is called an efficient estimator which is the best unbiased estimator because it minimizes the variance. Furthermore, by Chentsov X  X  theorem [8], the Fisher information metric is the only Riemannian metric which is invariant under basic pr obabilistic transformations. We now look into the Riemannian geometry with the Fisher information metric as a Riemannian metric. First of all, let us consider the positive n -sphere of radius 2, instead of n -simplex P n . Figure 1 shows an example of the positive 2-sphere of radius 2.

We can define transformation  X  : P n  X   X  S + n by The inverse transformation  X   X  1 is well known to pull back the Fisher information metric on P n to the Euclidean metric on  X  S + n [16, 22]. Therefore, the transformation is an isometry, and we can compute the distance between two statistical models by the Fisher information metric using the geodesic distance between two corresponding points on the sphere. In other words, the distance is the length of the shortest curve linking two corresponding points on the sphere and is given by This is called the information distance.

With this distance, we have the following Fr  X  echet sample function. Unfortunately, there is no closed form solution for the Fr  X  echet sample mean which minimizes this function. Although we can use some convex optimization techniques, such approaches may be impractical in case that n is large. Indeed, in many IR tasks, n + 1 is the size of vocabulary and can be very large.

Therefore, to find the Fr  X  echet sample mean, we try an ap-proximation approach using the Kullback-Leibler (KL) di-vergence which is defined as follows: As y  X  x , approximately by the Taylor expansion, From this, D ( x || y )+ D ( y || x ) = = 1
Since y approaches x along geodesic c linking them, we can parameterize the path by arclength s so that c ( s 0 )= x , c ( s 1 )= y and s 1  X  s 0 = dist ( x , y ). The difference between two points is expressed by a product of the geodesic length and the tangent vector to the curve as follows: Then, the first term in Equation (5) can be rewritten as follows: 1 2 = 1 where I ( s ) is the Fisher information for s . By definition of the length of the curve, Hence, I ( s ) = 1, and we finally have the following:
Similarly,thesecondterminEquation(5)canbealso written as Equation (6). Therefore, we have an approxima-tion of Equation (5) as follows: Similar relationships between divergences and distances can be founded in various texts [1, 16].

From this approximation, we can express the Fr  X  echet sam-ple mean with the KL divergence as follows: This means that finding the Fr  X  echet sample mean is reduced to finding the symmetrized Bregman centroid c F [27] which is defined as follows: c F =argmin c where D F ( x || y ) is the Bregman divergence defined by F ( x ) F ( y )  X  x  X  y ,  X  F ( y ) and F is a generator function. For ex-ample, if F is the negative Shannon entropy, i.e. j x ( j ) then the Bregman divergence is the same as the KL diver-gence. That is, the Bregman divergence is a generalized di-vergence. In addition, right-sided centroid c F R and left-sided centroid c F L are defined as follows:
Nielsen and Nock [27] show that symmetrized Bregman centroid c F lies on a geodesic linking c F R and c F L via the Bregman Pythagoras X  theorem. We can apply the result to the KL divergence.

We can easily compute c F R using the method of Lagrange multipliers with the same constraints as Equation (3), and the solution coincides with the arithmetic mean as follows:
Similarly, using the method of Lagrange multipliers, we compute c F L as follows: If  X 
Q =1 /k , then this is the ordinary normalized geometric mean.

Therefore, the symmetrized Bregman centroid when F is the negative Shannon entropy, or the approximated Fr  X  echet sample mean lies on the geodesic linking the arithmetic mean and the normalized geometric mean.

We consider the two means as approximations to the Fr  X  echet sample mean and take the following approach to decide a representation among them: 1. Compute the arithmetic mean c A and the normalized 2. Compute  X   X ( c A )and  X   X ( c G ) by Equation (1) 3. As a representation, choose c G if  X   X ( c A ) &gt;  X  That is, we choose a point which is closer to the Fr  X  echet sample mean as a representation. We call this approach  X  X eometric selection X .
To evaluate representation techniques derived in the previ-ous section, we conduct experiments for two different tasks: cluster retrieval and pseudo-relevance feedback.
 For the experiments, we use 3 standard collections from TREC. Table 1 shows the statistics of the collections. To estimate a language model from each document, we use the Dirichlet smoothing. For each task, the initial results are ob-tained by query-likelihood scores which are computed under an independence assumption as follows: where P ( q | D ) is estimated by Equation (2).

For index building, we used the Indri system [33]. Each document was stemmed by the Krovetz stemmer and stopped by a standard stopword set. To test the significance of re-sults, we performed a randomization test.
Cluster retrieval involves finding the best document clus-ter [24, 26]. We first retrieve the top 100 documents for each query according to query-likelihood scores. Next, we perform k NN clustering [19]. That is, assuming that each returned document is a cluster centroid, a cluster is formed by its k  X  1nearestneighbors( k is set to 5). We use co-sine similarity as a similarity measure. In fact, since cosine similarity assumes the Euclidean metric space, other simi-larity measures may perform better for our representation technique which assumes a different metric. However, since arbitrary clusters are assumed in cluster retrieval, we use the same similarity measure as used in previous work [26].
Once we have clusters, we represent each cluster by the arithmetic mean of language models of documents in a clus-ter assuming the Euclidean metric. On the other hand, assuming the Fisher information metric, we can determine a representation via geometric selection between the arith-metic mean and the normalized geometric mean of the doc-uments.

Evaluation of various representation techniques such as concatenation or CombMax [12] for cluster retrieval has been already done by Liu and Croft [26]. They concluded that the geometric mean representation outperforms other tech-niques. Therefore, we do not intend to repeat the same work. Instead, we focus on geometric interpretations for experimental results.

For a fair comparison, the same clusters are given to each representation technique. The only parameter to be tuned is the smoothing parameter for the initial results. We set the parameter so that Mean Average Precision (MAP) for the initial results by the query-likelihood P ( Q | D ) is maximized. Evaluation is performed using all topics. Since our goal is to find the best cluster, we use Precision at 5 (P@5) in order to evaluate the cluster first ranked by each representation technique, i.e. how many relevant documents the cluster has. Table 2 shows the results. In addition to the arithmetic mean and geometric selection, we present results using the geometric mean as well.

For all collections, representations by the geometric mean and geometric selection show better performance than rep-resentations by the arithmetic mean. Except for GOV2, The improvements are statistically significant. These exper-iments indicate some interesting points. First, in geometric selection, the normalized geometric means were selected as representations which minimize the Fr  X  echet sample function for all queries across all collections. In other words, the nor-malized geometric means are better approximations to the Fr  X  echet sample mean. Second, since the normalized geomet-ric means selected by geometric selection lead to consistently better retrieval results, we may say that the goodness of a representation for this task is related to how close the rep-Table 2: Results for cluster retrieval. A-MEAN, G-MEAN and SELECT mean representations by the arithmetic mean, by the geometric mean, and by geometric selection, respectively. The numbers are P@5 scores. A * indicates a statistically significant improvement over A-MEAN ( p&lt; 0 . 05) . resentation is to the center of mass, i.e. the Fr  X  echet sample mean. Moreover, this justifies the assumption of the geom-etry defined by the Fisher information metric. Lastly, since geometric selection does not consider the geometric mean but the normalized geometric mean, the results in the  X  X E-LECT X  row are exactly the same as those by the normalized geometric means. Therefore, the differences between the  X  X -MEAN X  row and the  X  X ELE CT X  row are caused by the normalization. As you see, since the differences are small, we suggest that the geometric mean without normalization canbeabetterchoiceinpractice.
Lavrenko and Croft X  X  relevance model [21] is one of the standard language modeling approaches for pseudo-relevance feedback. The model assumes that the top k retrieved docu-ments for query q are sampled from an underlying relevance model for q . That is, a hidden multinomial model relevant to a user information need exists, and we estimate the model from the top k documents. Then, we sample terms which describe the information need better than the original query and use the terms for query expansion.

Estimation of the relevance model is done by the following formula: where q is a user query, w is a candidate for expansion terms, and D i is a document in the top k initial results, respectively.
Although this is derived from a Bayesian model, we can see this as a representation for the top k documents by the arithmetic mean rewriting Equation (8) as follows: This has the same form as the weighted arithmetic mean of Equation (4). In other words, P ( w | D i ) is a multinomial pa-rameter and P ( D i | q ) represents a distribution over a sample space limited by q , i.e,  X  Q . In the standard implementation of the relevance model by the Indri system [33], P ( D )is assumed to be uniform. Hence, That is, the weight  X  Q = P ( D i | q ) is the normalized query-likelihood scores obtained in the initial retrieval phase. There-fore, we can say that the relevance model represents a group of the top k documents combining the language models by the arithmetic mean weighted by the initial search results. Table 3: Results for pseudo-relevance feedback. RM and GRM mean the relevance model and the geo-metric relevance model, respectively. The numbers are MAP scores. A * indicates a statistically signif-icant improvement over RM ( p&lt; 0 . 01 ).
 In this sense, we can say that the relevance model implicitly assumes the Euclidean metric space.

We can replace the arithmetic mean by the normalized geometric mean to develop a new representation as follows:
P ( w | q )= We can consider the original relevance model and this model as two approximated representations in the Riemannian man-ifold defined by the Fisher information metric. To determine a representation, we use geometric selection and call the se-lected model the  X  X eometric relevance model X .

We compare the geometric relevance model with the rele-vance model. For each query, we first retrieve the top k docu-ments by query-likelihood scores and build a relevance model or geometric relevance model for the documents. Then, we choose the top M terms according to probabilities of the terms in the models. Finally, we expand the original query combining the expansion terms using an interpolation weight  X  in the Indri query language. The paremeters k , M and  X  are tuned so that MAP scores by the relevance model are maximized. The same parameters are used for the geometric relevance model. Topic 51-150 for AP and WSJ and topic 701-750 for GOV2 are used as training topics to learn the parameters. Topic 151-200 for AP and WSJ and topic 751-800 for GOV2 are used as test topics. We retrieve up to 1000 results for each expanded query and use MAP as the evaluation metric.

Table 3 shows the results. The geometric relevance model significantly outperforms the relevance model for all three collections. Similar to cluster retrieval, geometric selection selected models by Equation (9) rather than the original relevance model as representations for all queries except for three queries of GOV2. That is, the geometric mean is a better approximation to the center of mass for this task. This provides more empirical evidence that the geometric mean can be an appropriate choice for representation.
To show how multiple documents, the arithmetic mean and the normalized geometric mean are distributed in each geometry, we use the following visualization. First, we con-struct a weighted complete graph, where each node is a doc-ument or the mean and a weight is determined by a kernel reflecting each geometry.
 For the Euclidean metric, we use the following heat kernel: Figure 2: Geometric visualization of the top 20 doc-uments for Topic 770 (GOV2), the arithmetic mean (AM) and the normalized geometric mean (GM) for different metrics, i.e. the Euclidean metric (left) and the Fisher information metric (right). Figure 3: Determinination of a middle point m on a geodesic linking x and y where t is a time parameter.

For the Fisher information metric, we use the following information diffusion kernel [20]:
K ( x 1 , x 2 )=exp  X  arccos 2
We visualize each geometry using CCVisu [4] which is a tool implementing energy models so that the higher weight between two points results in the smaller Euclidean distance between them. A visualization example is shown in Figure 2. As you see, the arithmetic mean appears closer to the center in the Euclidean metric space while the normalized geometric mean appears closer in the Riemannian manifold defined by the Fisher information metric. Since the visual-ization tool uses random seeds to initialize the layout, the results vary every time. However, the trend for the locations of the means was consistent.
Geometric selection is a somewhat simple approach to de-termine the approximated Fr  X  echet sample mean. That is, we choose one among only two options: the normalized ge-ometric mean and the arithmetic mean. We now consider a more accurate estimation technique for the Fr  X  echet sample mean.

A point which minimizes the approximated Fr  X  echet sam-ple function of Equation (7) lies on a geodesic linking the arithmetic mean and the normalized geometric mean. Let M , x , y and c be the statistical manifold defined by the Fisher information metric, the arithmetic mean, the normal-ized geometric mean and a geodesic linking the two points, respectively. First, we get vector V on tangent space T x via log map log x : M  X  T x M . In case of a sphere, the log Figure 4: Relative locations of the more accurately estimated Fr  X  echet sample means. The x -axis corre-sponds to the relative locations, and the y -axis cor-responds to queries for each collection. As a relative location is closer to 1.0, the estimated mean for the topic is located near the normalized geometric mean. Table 4: Pseudo-relevance feedback results of the more accurately estimated Fr  X  echet sample mean in the Riemannian manifold defined by the Fisher in-formation metric. map is given by:
V Then, V links x to y on T x M corresponding to y on M . m denotes a middle point between x and y on T x M , reached by  X V (0  X   X   X  1). We now get a middle point m on c via exponential map exp x : T x M  X  M . The exponen-tial map of a sphare is: m ( j ) =exp x (  X V ) ( j ) =cos(  X  || V || )+ sin (  X  Figure 3 illustrates this procedure. Note that the arithmetic mean x and the geometric mean y are interchangeable in the above formulation because a sphere is symmetric.

We apply this result to pseudo-relevance feedback exper-iments. We perform grid search on the geodesic varying  X  in [0,1] by step-size 0.1, and a point which minimizes the Fr  X  echet sample function of Equation (1) is selected as a representation. Figure 4 shows  X   X  X  selected for test queries for each collection. For all test topics except for three top-ics of GOV2, the selected  X   X  X  are equal to or greater than 0.5. That is, the more accurately estimated Fr  X  echet sam-ple means are also closer to the normalized geometric mean than the arithmetic mean. Table 4 shows the results when the representations are used for pseudo-relevance feedback. All results are equal to or a little bit better than the results of the GRM in the Table 3, but not significantly. There-fore, we can say that the geometric relevance model is a reasonable approximation to the Fr  X  echet sample mean for this task.
We have addressed so far theoretical and empirical reasons explaining why the geometric mean should have advantages for many IR tasks. There can be many other explanations. One of them is the log-linearity of the geometric mean. As more documents contain a specific term, the geometric mean for the term increases exponentially while the arithmetic mean increases linearly. Accordingly, the arithmetic mean can be sensitive to a few dominant terms in a small number of documents. On the other hand, the geometric mean favors the common terms across a whole set of documents and is relatively insensitive to such a few dominant terms. This shows the robustness of the geometric mean which can lead to a good representation for multiple documents.
Previous work which uses the geometric mean as a rep-resentation technique does not provide enough theoretical evidence explaining why the geometric mean should have advantages as a representation for IR. There are various ex-planations. In this work, we showed that using Information Geometry, the arithmetic mean and the normalized geomet-ric mean are approximation points to the center of mass in the Euclidean space or in a statistical manifold. In par-ticular, through empirical evidence, we demonstrated that the normalized geometric mean is closer to the center in the statistical manifold. In addition to this discovery, we in-troduced a new approach to pseudo-relevance feedback that outperformed the relevance model. For future work, we will investigate how geometric interpretations can be applied to other IR tasks. We expect that this effort will lead to not only the discovery of novel IR theories but also development of effective algorithms.
This work was supported in part by the Center for In-telligent Information Retrieval, in part by NSF grant #IIS-0711348, and in part by NSF grant #IIS-0534383. Any opin-ions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily re-flect those of the sponsor.
