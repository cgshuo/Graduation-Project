 With the dramatic increase in computing power, the field of machine learning has undergone tremendous changes in recent years. This change can be attributed to a deeper understanding of machine learning algorithms, a series of new inventions such as multi-core computers and distributed computers, as well as an ever increasing availability of real-world data. The consequence of the change has resulted in many new developments in machine learning that are truly characterized by large scale.
This special issue collects five diverse examples of large-scale machine learning algorithms and applications. By highlighting these examples, we hope to take stock of what we have achieved as well as foresee what will come in the design of learning machines in the area of intelligent systems and technology.

The first article, titled  X  X LDA+: Parallel Latent Dirichlet Allocation with Data Place-ment and Pipeline Processing X , authored by Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang and Maosong Sun, proposes four strategies for improving the scalability of dis-tributed Gibbs sampling for Latent Dirichlet Allocation (LDA). The LDA algorithms are important in large-scale information retrieval applications. Previous methods ran into problems characterized by either memory or communication bottlenecks. Large-scale experiments show that the four strategies significantly reduce the unparallelizable communication bottleneck and achieve good load balancing.

The second article, titled  X  X IBSVM: a Library for Support Vector Machines X , is au-thored by Chih-Chung Chang and Chih-Jen Lin. This article describes the LIBSVM system in detail. LIBSVM is a machine learning library for supporting large-scale sup-port vector machines (SVM). Since its inception in 2000, this SVM library has had a huge impact on machine learning applications across a wide spectrum of applications in science and engineering. The authors explain implementation details behind LIBSVM, shedding light on valuable experience for other AI system and application developers. The third article, titled  X  X atch and Online Learning Algorithms for Nonconvex Neyman-Pearson classification X , authored by Gilles Gasso, Aristidis Pappaioannou, Marina Spivak and L X  X on Bottou, presents an approach to Neyman-Pearson classifi-cation, which can be used in many applications such as fraud detection where it is more important to avoid one kind of error than another. The approach often taken is a nonconvex problem formulation involving a constraint on the false negative rate. This article presents two highly scalable solutions for solving this problem, which were extended to solving the q-value optimization problem. These approaches are tested in several large scale datasets, including one collected from a mass spectrometry protein screening task.
 The fourth article, titled  X  X earning to Recommend with Explicit and Implicit Social Relations X  by Hao Ma, Irwin King and Michael R. Lyu integrates both recommendation systems technology and users X  trust information. They propose a novel probabilistic factor-analysis framework, which naturally fuses the users X  tastes and their trusted friends X  favors together. The proposed framework is quite general, and it can also be applied to a pure user-item rating matrix even if we do not have explicit social trust information among users. They define a term  X  X ocial trust ensemble X  to represent the formulation of the social trust restrictions on the recommender systems. Analysis show that they can handle very large datasets since it scales linearly with the number of observations. The fifth article, titled  X  X earning to Detect Malicious URLs X , is authored by Justin Ma, Lawrence K. Saul, Stefan Savage and Geoffrey M. Voelker. This article explores how to detect malicious Web sites from the lexical and host-based features of their URLs. They exploit a novel adaptation of online learning algorithms that can be both scalable and quickly adaptable to new features in future malicious URLs. They inno-vatively combine a real-time system for gathering URL features and a real-time feed of labeled URLs from a large Web mail provider and demonstrate the scalability of their system on real-world datasets.

The five articles in this special issue present several perspectives. The articles cover very different approaches on different types of data, ranging from text documents, learning problems with asymmetric loss, recommendation systems, and malicious URL detection. They also formulate interesting approaches to large-scale system design, from online learning to distributed learning algorithms. With these articles, we hope to highlight the important developments in large-scale machine learning as an exciting research area.

