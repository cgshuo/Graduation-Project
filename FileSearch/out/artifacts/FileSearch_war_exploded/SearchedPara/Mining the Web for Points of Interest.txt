 A point of interest (POI) is a focused geographic entity such as a landmark, a school, an historical building, or a business. Points of interest are the basis for most of the data support-ing location-based applications. In this paper we propose to curate POIs from online sources by bootstrapping train-ing data from Web snippets, seeded by POIs gathered from social media. This large corpus is used to train a sequen-tial tagger to recognize mentions of POIs in text. Using Wikipedia data as the training data, we can identify POIs in free text with an accuracy that is 116% better than the state of the art POI identifier in terms of precision, and 50% better in terms of recall. We show that using Foursquare and Gowalla checkins as seeds to bootstrap training data from Web snippets, we can improve precision between 16% and 52%, and recall between 48% and 187% over the state-of-the-art. The name of a POI is not sufficient, as the POI must also be associated with a set of geographic coordinates. Our method increases the number of POIs that can be localized nearly three-fold, from 134 to 395 in a sample of 400, with a median localization accuracy of less than one kilometer. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing geographic information extraction, geo-localisation, location-based applications, points of interest Work performed while the author was a visitor at Yahoo! Research
A point of interest (POI) is a focused geographic entity such as a landmark, a school, an historical building, or a business. Many POIs are permanent structures such as statues, or buildings. Others are semi-permanent, such as restaurants which may open one year and close the next. Still others are temporal or periodic, such as the location of an annual festival.

Points of interest are the basis for most of the data sup-porting location-based applications. They are displayed on online maps, and are provided to users of location-based mo-bile applications such as Foursquare 1 and Gowalla. 2 POIs are often displayed in the local search results or in sponsored listings related to a user X  X  search, or are presented to users of recommender systems such travel booking websites.
In this paper we address the question of whether it is possible to automatically identify POIs without manual in-tervention, given their broad definition, and the lack of con-sistency in their naming. Further, as knowing the name of a POI is not useful without knowing its location, we address the question of how accurately POIs can be localized using automatic methods.

To curate POI data, companies such as Navteq 3 and TeleAt-las 4 send a surveyor to a location to identify, verify and record POIs. This process yields high-quality and accurate data, although the process is very expensive, and it is dif-ficult to scale to large geographic areas, or hard-to-reach locations. Furthermore, the frequency with which the loca-tions can be surveyed is limited by the expense of gathering the data, so the process focuses on POIs that are more likely to be permanent, in areas of high commercial value. This creates a bias in the data towards POIs such as landmarks, schools, hospitals and other stable, stationary items that are unlikely to change significantly within a short time frame. Other types of POIs, such as restaurants, may change more quickly than human geographers can be sent out to update http://www.foursquare.com visited June 2012 http://www.gowalla.com visited January 2012. Gowalla has been purchased by Facebook, and the site is no longer available. http://www.navteq.com/ visited June 2012 http://www.teleatlas.com/ visited June 2012 their records, and hence would become stale. For this reason they are not typically covered in this type of curated data.
A second method for curating POIs is to create a direc-tory of sponsored listings. Directories of this type are used in local search engines and mapping products such as those that display restaurants and businesses. This second type of data is dependent on businesses that have the budget and in-clination to pay to be included in the directory. Very small, independent businesses may be less likely to be listed, and in countries where Internet usage is low, the sponsored listings may be non-existent or sparse, or dominated by large busi-nesses and national chains. As a result, the representation of the place in terms of local businesses will be far from the user X  X  perception in real life.
 A third technique is crowd-sourcing, such as the Open-StreetMap Project 5 , in which users themselves contribute to the representation of the place by uploading location data. This helps address the bias toward large businesses and per-manent POIs, but crowd-sourcing introduces other issues such as the provenance of the data, and its reliability. For example, the New York Times reported that Google erro-neously labeled businesses as  X  X losed X  on their maps, as a result of spamming by competing businesses [18].

In this paper we propose to curate POIs from online sources by bootstrapping training data from Web snippets, seeded by POIs gathered from social media. This large corpus is used to train a sequential tagger to recognize mentions of POIs in text. The features of the sequential tagger are generic, and as such are designed to recognize points of interest from their context. This allows the tagger to tag previously unseen POIs. Furthermore, the evaluation data is taken from a completely different sample of data (news articles as opposed to Web snippets or mentions in social media), and many of the POIs annotated by assessors are not present in gazetteers.

Using Wikipedia data as the training data, 6 we can iden-tify POIs in free text with an accuracy that is 120% better than the state of the art POI identifier in terms of preci-sion, and 48% better in terms of recall. The points of inter-est represented in Wikipedia resemble curated data found in gazetteers, and focus on official names for permanent structures. Businesses, restaurants and venues are under-represented in Wikipedia, but are well-covered in online checkin services such as Foursquare and Gowalla. We show that using Foursquare and Gowalla checkins as seeds to bootstrap training data from Web snippets, we can improve precision between 16% and 52%, and recall between 48% and 187% over the state-of-the-art.

The name of a POI is not sufficient, as the POI must also be associated with a set of geographic coordinates. We lo-calize the POI mentions using location models inferred from Flickr data. The result is a method for discovering POIs pre-viously not found in sponsored listings or online gazetteers. Our method increases the number of POIs that can be lo-calized nearly three-fold, from 134 to 395 in a sample of 400, with a median localization accuracy of less than one kilometer.

To summarize, our contributions are as follows: http://www.openstreetmap.org visited June 2012 http://www.wikipedia.org visited June 2012
We present an overview of the related work in Section 2, followed by a description of the model for identifying POIs in text in Section 3. While the models are trained on data that is automatically generated, the evaluation is conducted on data labeled by human assessors. Section 4 presents the creation of manually annotated evaluation data, as well as training data gathered from Wikipedia. We present the bootstrapping system in Section 5 to extract POI mentions from social media data, and use those as seeds to create training data from Web snippets. The localization compo-nent is presented in Section 6, and the work as a whole is discussed in Section 7. We lay out our conclusions and re-flect on our findings in Section 8.
To the best of our knowledge there is no other work that aims to discover points of interest from unstructured text on the web. This section discusses related work that lever-ages the location of a user and his interaction with a map to improve the metadata associated with a point of inter-est. Although the literature on Named Entity Recognition is certainly relevant here, we included the related work on NER in the section describing the models. The localization component is based on the work of O X  X are and Murdock [16] and Serdyukov et al. [19], but there are other similar works that are also relevant to the localization of place mentions.
Mummidi and Krumm discover points of interest from pushpins placed on maps by users [15], mining the anno-tations of the pushpins for terms with a high TF  X  IDF value. The authors propose the map data as a reliable source of data from users, because the users have explicitly indicated a point of interest on the map, and after processing the data yields a textual characterisation of the point, plus its ge-ographic coordinates. Evaluating a data discovery system is always a difficult task and the authors address this by conducting a user study in which 100 users assess points of interest shown on a map in their neighbourhood and are asked to indicate whether the POI is identified correctly or not.

In related, more recent work, Zheng et al. [27] propose a method to mine GPS data to recommend locations to users wanting to do an activity and to recommend activi-ties to users at a particular location. Their data is obtained from an interactive mapping application, with 162 users, who have generated roughly 12,000 trajectories in Beijing over the past 2.5 years. In their data, the points of inter-est from a database of POIs are associated with geographic coordinates.

Both of these works rely on users X  interactions with a map and are based on a small-scale user study. This is a key dif-ference between our work and theirs as they rely on users to annotate maps with POI data. Our work discovers the mentions of POIs in Web snippets and does not rely on users interacting with any particular application. This allows us to potentially gather vast amounts of training data, indepen-dent of any given application. Although the POIs discovered in Web data are less structured and consistent than those entered by users on a map application, we expect that the vast amount of data will compensate for the noise.

In the case of Zheng et al., their system is restricted to the city of Beijing, whereas we consider any place in the world. Finally, Zheng et al. populate their list of points of interest with a categorised POI database. Thus users can comment on existing POIs, but no previously unseen POIs are introduced into the system. The focus of their work is to recommend known POIs to users who are in a given place, which is distinctly different than discovering previously unknown POIs.

Yin et al. [26] do not extract points of interest, but rather model the topics in a given location. In their system a topic is a  X  X patially coherent meaningful theme X . They create a data set based on seven concepts: Landscape, Activity, Manhattan, National Park, Festival, Car and Food, using the topics as keywords to crawl the Flickr API for images associated with those concepts. They propose latent geo-graphical topic analysis to discover sub-topics related to the seven parent concepts. In a second task they identify the regions associated with a given topic. Their system relates to ours in the sense that a POI could be considered a sub-topic of a region and a system that is designed to find topic mentions in Flickr data may discover POIs, along with other topics. However, they have constrained their system to re-gions in the U.S. and allow for the discovery of a wide range of topics.

Our localization component is based on the work of O X  X are and Murdock, described in [16]. We discuss this work in more detail in Section 6. In similar work, Crandall et al. [6] propose a system to predict among ten landmarks in a given city, within 100 meters, in Flickr images. Their experiments are limited to a specific set of landmarks in a fixed set of cities, as there are no images in their test or training sets that represent places outside of this set of locations. This differs significantly from our task, as we are trying to predict the location of a landmarks and other POIs, anywhere in the world. Furthermore, their work focuses primarily on images, and leverages image features, whereas we work entirely with text.

Yi et al. [25] use language modeling to determine the lo-cations implicit in queries. They use Placemaker to identify location mentions in queries, which they then remove. The resulting queries are intended to contain implicit locations. However, the way in which they use Placemaker is likely to leave mentions of neighborhoods and POIs, as they remove only the primary locations in the queries, in the case that there is more than one location mentioned. This represents an explicit mention of a location, rather than an implicit one. Furthermore, their evaluation is limited to predicting locations that exist in Placemaker. They do not create an independent ground truth.

The work of Hollenstein and Purves [9] seeks to identify vernacular regions in Flickr 7 data. Vernacular regions in-clude mentions such as  X  X owntown X  or  X  X BD X , which are not as granular as POIs, but are significantly smaller than cities, and represent non-official locations that are not typ-ically included in gazetteers. They present a case study of six cities in the U.S. and Europe. This is one of the few studies that attempts to identify regions smaller than a city.
Other work that does not seek to localize geographic en-tities, but rather to assign a geographic scope to a doc-ument includes work to build location topic models from blog data [13, 24], and finding the geographic focus of web pages [7, 4, 28].
Named-entity recognition has been well-studied for a num-ber of years. The Message Understanding Conference [1] (MUC) ran from 1987 to 1999. More recently the Automatic Content Extraction Program (ACE) [2] ran from 2000 X 2008. The CoNLL Shared Tasks for 2002 [20] and 2003 [21] pro-vides a reasonable overview of the standard data and bench-marking tasks for NER.
 Conditional Random Fields (CRF) were introduced by Lafferty et al. [11], for text classification and sequence la-belling. The CRF was proposed for NER by McCallum and Li [12], and we borrow this approach for POI detection. They report identifying location mentions in the CoNLL En-glish data set with 87% precision and recall. The locations in the CoNLL data refer only to cities, states and coun-tries and the entire data set is composed of news articles. It represents a much simpler problem, in part because of the nature of the location mentions, and the presence of towns, states and countries in gazetteers and because the news data is more semantically rich and cleaner than social media data and Web data.

The conditional random field computes the probability of a label sequence, y , given an observation sequence x , accord-ing to: where Z ( X ) is a normalising factor, and F ( Y,X ) is the set of feature functions computed over the observations and the label transitions. The learning process selects the set of feature weights  X  which maximise the label sequence prob-ability P ( Y | X ): More information about CRFs are given in Wallach [23]. We use Okazaki X  X  implementation of the CRF [17] from the CRFsuite project 8 as it has been shown to outperform other implementations in terms of training speed for sparse data and implements a number of training methods capable of handling large amounts of data -an important consideration for a system designed to learn over large-scale web snippet http://www.flickr.com visited June 2012 http://www.chokkan.org/software/crfsuite/ visited June 2012 data. We use the Averaged Perceptron training method [5] with a maximum number of iterations of 10. We limited the iterations in training because it made training over large amounts of data tractable. Our data is labelled in BI0 nota-tion, as is standard for NER tasks; that is, a token is labeled as the beginning of a POI mention (B), the continuation of a POI mention (I), or not part of a POI mention (0).
Each example sentence in our data is treated as a sequence of tokens, represented by a vector of binary features (de-scribed in Table 1). The observation features fall into one of four classes: lexical, geographic, grammatical and statis-tical. Lexical features are computed over the surface text of the token stream. They represent standard NER lexical features, and include the word identity, word shape, position in the sentence, the prefix and suffix of the token.
Geographic features were computed using Yahoo! Place-maker 9 , a geographic parsing service, to provide data for tokens that match a POI name. For a token that matches, Placemaker provides information that includes a list of can-didate places to which the token may refer and for each, contextual information like name variants in different lan-guages, and colloquial names. Characterising statistics are computed over this list.

To encode the grammatical function of each token, part-of-speech tagging was done for each token within a sentence using the Apache OpenNLP 10 implementation of a max-ent POS tagger, using the Penn English Treebank POS tag dic-tionary 11 that comprises of 36 tags.

Normalized pointwise mutual information ( npmi ) was com-puted over token bigrams appearing in a random sample from the Yahoo! 12 mobile search query logs. For each bi-gram, the normalised point-wise mutual information of a token x and its subsequent token y was computed as:
To convert the ( npmi ) into a binary feature, the output values were discretized by applying a greater-than threshold test at each 0.1 interval between -1 and +1, resulting in 20 binary features per bigram.

For the state transition features, we consider the previous state and the next state for all features, except for the word identity and word shape features, which are computed over the previous two, and the next two states (this helps in the common case of longer formulaic POI names such as  X  X hurch of Saint Martin X  of  X  X he Museum of Natural History X ).
For the extraction task, we created a manually annotated data set composed primarily of news articles from the U.S. http://developer.yahoo.com/geo/placemaker/ visited June 2012 http://incubator.apache.org/opennlp/ visited June 2012 http://www.ling.upenn.edu/courses/Fall_2003/ ling001/penn_treebank_pos.html visited June 2012 http://www.yahoo.com visited June 2012 Ground Truth Assessor 1 0.749 0.792 0.770 2 0.814 0.716 0.762 Table 2: The result of identifying location bound-aries, if one assessor is used as the ground truth labelling and the other assessor is used as the test labelling. and the U.K., but also included a small number of examples from Yahoo! Answers 13 and a small number of queries sub-mitted to a search engine. The POIs represented in this data include businesses, services, landmarks and public buildings such as schools, hospitals, airports and prisons.
 Our data was annotated by two assessors, both native English speakers (annotating data in English), one from the U.S. and one from the U.K. They were shown random exam-ples from multiple sources, and were instructed to highlight all locations in the text. The inter-assessor agreement was 73.9%. In total 1,337 of the examples they annotated con-tained POIs, which yielded 1,066 unique POIs.

In addition to measuring the inter-assessor agreement, we measured the precision, recall, and F-measure of one asses-sor, using the other assessor X  X  data as the ground truth. The results of this is shown in Table 2. This provides a reason-able upper bound on the performance of the POI detection. The results shown in Table 2 are much lower than those re-ported on the named-entity recognition benchmarking tasks, where the location mentions are entirely composed of cities, states, and countries, and all of the data is news data. The results indicate that identifying POIs in data is not straight-forward, even for human assessors. We trained a model us-ing this data with 10-fold cross validation and a 90/10 split. The results are shown in Table 3 in the row labeled  X  X anual Annotations X . The performance of the model is almost as good as that of the human assessors in terms of precision, on the same task.
 To create the Wikipedia data set, we selected pages in Wikipedia whose topic was a POI. We determined which Wikipedia pages relate to POIs as follows. The Geonames 14 database encodes geographic entities with a feature code that classifies entities according to an entity taxonomy. These codes are grouped into 9 classes, labelled with a class code letter. The Yago2 ontology gives a concordance between Wikipedia articles and Geonames geographic entities [8]. We selected English language Wikipedia articles which have been identified with the Geonames  X  X  X  class, which corre-sponds most closely to the definition of POIs used in this paper, containing entities such as airports, buildings, facil-ities and historical and industrial sites. From this set of articles, the title text is used as a surrogate for the name of the POI. The abstract of the article is segmented into sentences and filtered for those that contain the title text. This process gave us 2,896 unique POIs with a total of 5,186 examples of their use in context.

The results of training a model on the Wikipedia data (with 10-fold cross-validation) are better than the results for the manually annotated data. This reflects the lack of noise in the data, and the use of official names for the POIs. The http://answers.yahoo.com/ visited June 2012 http://www.geonames.org/ visited June 2012 which is the state-of-the-art. results are shown in Table 3 in the row labeled  X  X ikipedia Article Sentences X .

The baseline results (in Table 3 , the rows labeled  X  X lace-maker Baseline X ) were obtained by processing the two data sets with the Placemaker service to extract points of interest mentioned in the text. We expanded the valid place types returned by Placemaker to include airports, and land fea-tures which are not POIs in the Placemaker classification, but are in the Geonames classification.

It is important to note that the Wikipedia data is labeled according to whether it is a POI or some other type of entity, but it has not been labeled with sequence labeling (such as the BI0 notation described above) that would be needed for NER. For the purpose of detecting mentions of POIs with a sequential tagger, the data is unlabeled. Thus, this re-sult represents a first system to recognize geographic points of interest, entirely from unlabeled data. Both the sys-tem trained on manual annotations, and the system trained on Wikipedia data performed significantly better than the Placemaker baseline.
Both the Wikipedia and the manual annotation data sets are very small, which means geographic coverage, as well as their coverage in terms of the types of POIs mentioned is limited. In addition, the mentions of POIs vary greatly from one data source to another. For example, the University of Buffalo might be referred to as #UBuffalo , or University of Buffalo depending on the data source and the context of the mention. Creating enough manually annotated data to learn patterns from this amount of variation would be a major undertaking. In this section we show that increasing the amount of training data by bootstrapping from the Web yields a signficant improvement in the learned POI extrac-tion. The Wikipedia title text was used as seed queries to the Bing search engine via their web-based API 15 to retrieve snippets or web page abstracts relevant to those queries. We retrieved up to 10 web snippets per Wikipedia title. The snippets provide a small amount of text to contextu-alize the POI. The idea is to provide a context in which a POI is used, to enable the model to learn a more gen-eral representation of the POI. The resultant list of POIs from Wikipedia is relatively clean, but there is no guaran-tee that the POI will be mentioned in the proper context in the search engine snippets. For example in this scenario the POI  X  X he White House X  might retrieve a web snippet about white houses. This process gave data for 2,896 total unique POIs and 21,228 examples of their use in context.

As stated earlier, the POIs mentioned in Wikipedia are largely mentions of permanent structures such as landmarks and government buildings, usually represented by their offi-cial name. For Web applications, the definition of POI also includes more ephemeral places such as restaurants and lo-cal businesses. Location check-in services such as Foursquare and Gowalla generate a large number of such POIs. An ad-http://www.bing.com/toolbox/bingdeveloper/ visited June 2012 Table 5: Results of evaluating each web snippet-based data set using Yahoo! Placemaker vantage of this data is that it has high coverage of places the users of these applications actually visit.

Both Foursquare and Gowalla provide public APIs that allow their data to be crawled, within a rate limit. The POIs in this data consist largely of mentions of businesses, but also include landmarks and public buildings such as li-braries. Users may select from lists of known POIs (mostly sponsored listings, or licensed data), or they may create their own POI. The majority of check-ins in our data are to pre-existing POIs. They are relatively clean, because the formu-laic way in which they appear in the data allows them to be extracted reliably. Although the POIs in this data could be used to create a lexicon of POIs, the POI check-ins cannot be used directly to train or evaluate a sequential model because they contain no textual context. Furthermore, mining this data directly for the POIs would simply be reconstructing an existing database, which is not the aim of this work. Rather we seek to model the way in which POIs are mentioned, to allow discovery of previously unseen POIs.

Once the elements that represent POI names are extracted from the checkin, they are used as seed queries to the Bing search API. These snippets contain sentences where the POI has been used in context (as opposed to the terse, formulaic mentions in the checkins). It should be noted however that this mechanism does not guarantee the snippets X  relevance to the seed POI query. Nor does it ensure that all possible POIs present in web snippets are correctly labelled.
This process was carried out for all check-ins, with 10 snippets being retrieved from each seed POI query. This generated millions of sample sentences that could be used for training. For each check-in service, we randomly sam-pled a subset of 50,000 examples, giving the data set char-acteristics shown in Table 4. These samples were selected with the criteria that they contained the POI as an exact substring and that the sample contained only unextended ASCII characters. Table 5 shows the bootstrapped snip-pets tagged for POIs using Placemaker, which represents the state-of-the-art in POI detection. The baseline for the manual annotations was shown in Table 3.

The results of training and testing on this data using 10-fold cross validation, as well as training on the checkin data and evaluating on the manual annotations are shown in Ta-ble 6. Note that bootstrapping the data improved the results for the checkin data, whereas the bootstrapped Wikipedia results are much lower than simply training on the original Wikipedia POI mentions.
In order for the discovery of a point of interest to be useful in an application, it must be associated with a location. We employ the location modeling approach proposed by O X  X are and Murdock [16] to predict the location of the POIs in a sample of the manually annotated data set. We evaluate our approach in terms of the distance in kilometers from the ground truth location of each POI. The Placemaker service is our baseline, as it is the current state-of-the-art. Finally, we use our model and Placemaker in a cascade architecture to improve the results over either system alone.

The location models described in Serdyukov et al. [19] and in O X  X are and Murdock [16] are built by quantizing the coordinate system into one kilometer, 10 km or 100 km cells. For the work in this paper, we choose one kilometer grid cells because we are predicting geographic entities that are mostly smaller than one square kilometer. As in [19] and [16], each cell is associated with the geo-tagged Flickr images that were taken within the cell boundaries. We es-timated the models from the raw tags associated with ten million geotagged Flickr images, uploaded to Flickr before October 2010. The cell is represented by the distribution of tags associated with its images. The problem of predicting a location can then be reduced to a standard information retrieval ranking problem where cells are  X  X ocuments X  and the image tags are the terms in the document. In [19] the terms in the cell-documents are weighted by their term fre-quency. This produces a model that might be biased toward the tags applied to a set of images by a single user. To remediate this, in our models we weight the terms in the cell-documents according to the user frequency, as in [16], the number of users who have applied that tag to that cell, rather than the number of times the tag has been applied in that cell. More specifically, where c user ( t,L ) is the number of unique users who use the term in the location (cell). | L | is calculated as the sum of the user frequency of all terms in the location:
Weighting the terms according to the user frequency, rather than the term frequency, reduces the effects of bulk upload-ing, or of applying near-duplicate tag sets to multiple im-ages. It has been proposed before for choosing representative and using these as seed data to generate web snippets. 0.001 according to McNemar X  X   X  2 test. tags for a given location [3, 10], and for suggesting tags for photos [14].

To evaluate the system, we created a ground truth data set. Although the coordinates given by Placemaker for POIs are derived from curated data sources, and are as accurate as reasonably can be expected, Placemaker does not iden-tify every POI in the data, and when comparing Placemaker to other approaches, both must be compared to a common ground truth. We sampled 400 examples from the man-ually annotated data described in Section 4. The ground truth locations of the POIs were determined by identify-ing the address (or geographic coordinates when they were available) of the POI from its official web page. We located the address on a map, and the POI was verified visually by zooming in on the satellite view of the map. POIs were discarded in cases where the POI could not be visually veri-fied on the map, or where the exact address of the POI was not available on its official web page. In our data, out of 400 examples of POIs, 291 represented unique locations ac-cording to the geographic coordinates, and 327 were unique mentions of POIs. All of the POIs in our example were men-tioned in unique contexts, and the context was used to limit the set of candidate locations.

We measured the Vincenty Distance [22] in kilometers from the predicted location of the POI to its true loca-tion. Our localization method predicts the centroid of a one-kilometer grid cell, so our system can be at most accu-rate within 500 meters at the equator, and more accurate toward the poles. We use the median distance as the met-ric instead of the average, because a single location that is predicted incorrectly to be on the opposite side of the globe will skew the average distance in such a way as to make the results difficult to interpret.

Our evaluation considers three subsets of the data. The first consists of the subset of examples which are identified by Placemaker as points of interest. We call these  X  X nown X  locations. The second is the set of examples that Placemaker identifies as some other type of location (such as a city, or a country -the  X  X ther X  locations). The third consists of loca-tions identified by Placemaker as not containing a location. We call these  X  X ew X  locations, although they are only new in the sense that they are not in the curated data used by Placemaker. Table 7 shows that placemaker is able to iden-tify 265 locations out of 400, of which 134 are identified as POIs, and 131 are identified as other types of locations. This represents a recall of roughly 33%. Our system localizes 395 out of 400 POIs, a recall of roughly 98%.

When Placemaker identifies a point of interest in text, it localizes it with very high accuracy (roughly 300 meters), thus it makes sense to use Placemaker to find as many POIs as it can. In the cascade model, Placemaker is used first to identify any POIs in the data. For examples that Place-maker does not find a POI, if it finds a location of another type, the bounding box of that location is used to constrain the search for the location in the Location Model, by taking the first result in the ranked list of results returned by the Location model that falls within the bounding box of the location returned by Placemaker. Table 7: The distance, in kilometers, from the true location to the location predicted by the experimen-tal systems. The 265 locations included in this table were identified by Placemaker, out of 400 total ex-amples. Of the locations found by Placemaker, 134 were identified by Placemaker as points of interest, as opposed to other types of lcoations such as cities, or states.
 The Location Model performance in localizing POIs that Placemaker identified as other types of locations is an im-provement over the Placemaker result. Using the location information returned by Placemaker, even when Placemaker fails to identify the POI, improves the result further.
For the 130 POIs localized by our system that were de-termined to contain no place mention by Placemaker, the median distance from the true location was a somewhat depressing 439 kilometers. Clearly, having location infor-mation to constrain the search improves performance. The Placemaker system was designed to provide information about the mentions of each individual location in free text, but also to provide information about the text as a whole, such as its geographic scope (the minimum bounding box enclosing most of the locations mentioned in the text). In our data, for the 130 POIs that were deemed non-locations by Place-maker, we took the geographic scope of the textual context from which the POI was originally extracted from (the news article, or query or question-answer pair), and used it to con-strain the search in our system. Table 8 shows the results over all examples of the median distance from the predicted location to the true location, when the search in the Cascade model is constrained by the geographic scope.

For locations that are identified by Placemaker, it is clear that other disambiguators in the POI mention improves per-formance. For example, in the POI  X  X GM Grand Garden Arena, Las Vegas, NV X  Placemaker is unable to identify the arena as the POI, and instead returns the city of Las Vegas. The Location Model ranks the locations associated with the terms mentioned in the POI. In the Cascade Model, after ranking the locations by their score, the list is scanned for the locations that are contained in the bounding box for Las Vegas, NV, and the first location within the bounding box is returned as the correct location of the POI. In the Geo Scope Model, rather than return a bounding box for  X  X as Vegas, NV X , Placemaker returns the geographic scope of the article the POI mention originally appeared in to constrain the search. It is important to note that there is no guarantee that a given POI in an article will be within the geographic scope of the article.
This work has identified the significant difference between traditional location NER and the task of POI recognition. Such recognition is non-trivial, as demonstrated in Section 4, in which we showed that even with POI labelling undertaken in a strictly controlled environment, consensus was difficult among human assessors.

The subjectivity of POIs also highlights their diverse na-ture and how their definition is closely dependent on the application in which they are used. It also makes it hard to generate sufficient quantities of manually annotated data for training robust models.

To tackle these problems, we first used Wikipedia as a source of POI mentions and their usage in context. This produced models that performed well at recognizing POIs in Wikipedia articles. However, the POIs found in such text do not reflect the range and variety of those that are used in location-based social media and mobile applications, such as businesses and restaurants.

Social media data, such as location checkins, are an ex-tensive and easily accessible source of POIs, but they lack the textual context required for training sequential models. Extending POIs mentioned in social media with web search engine snippets improves performance over the state of the art. The data itself is not manually labeled, so there is the potential to train on vast amounts of this data cheaply. Furthermore, by being an entirely automatic process run on dynamic data from the Web, models can be continually trained and updated, to capture ephemeral and temporary POIs.

We see that amplifying the Wikipedia data with web snip-pet data degrades performance. Wikipedia is a very clean data source, and the POIs mentioned in Wikipedia are usu-ally in their canonical form, and in the proper context. Al-though the data set is small, it is sufficient to predict the entity boundaries for these POIs. It is important to note that such a system can be trained from unlabeled data. Adding web snippet data, however, only makes the data noisier, without adding information.

By contrast, the checkin data benefits greatly from boot-strapping with web snippets. This type of data is an excel-lent source of POI mentions, but even when there is textual context surrounding the POI mention in the checkin, it is not sufficiently informative to estimate a model from it. Other types of social media data, such as Twitter, may be a good source of information about places that people visit, but it is extremely noisy, and contains abbreviations and textual shortcuts inherent to the constraints of the application it-self. Bootstrapping from the web allows the POIs to be placed in a natural language context, which, while noisy, is considerably less noisy than the Twitter data itself.
One reason for lower results on large amounts of data com-pared to the smaller data sets from Section 4 is the variation in the POIs themselves. They represent different classes of entities and it may be necessary to learn them as distinct classes. It is possible that a mention of a local business in text is not sufficiently similar to a mention of a landmark or library or public school. Since we did not canonicalize the POI mentions or aggregate them according to their ge-ographic coordinates, we do not have equivalences among the POIs. Since the evaluation relies on an exact match of the POI entities, the system is penalized for correct men-tions that do not match exactly. For example, if the ground truth assessor marked the POI  X  X he New School X  and the system instead tagged the POI  X  X ew School, X  the tagging is completely wrong. Partial credit is not given for correctly matching one boundary but not the other.

With regard to localization, we have shown that we can determine the location of the POI with high accuracy, less than one kilometer for most POIs. In the evaluation, we results are repeated for the sake of comparison did not evaluate the end-to-end performance of the entire system, because if a set of tokens is erroneously tagged as a POI, it is not meaningful to try to localize it. The system may correctly guess the location, but if the input to the system is not valid, the evaluation will not be meaningful. The evaluation on the ground-truth tagging gives an upper-bound on the localization performance, as errors generated in the tagging phase will propagate to the localization phase. It may be possible to leverage the localization, either as a filter step, or as a feature in the tagger, to improve the performance of the tagging, but this is left to future work. The localizer is built on Flickr data, specifically tag sets. The tag sets in Flickr are unique in that they contain mostly nouns, and a few adjectives, and very little noise. Many of the images that are geotagged and uploaded to Flickr por-tray POIs and tourist destinations. This makes Flickr ideal for localizing POIs in the places where Flickr has coverage. For locations with no Flickr coverage, it is not possible to predict the location. This is a limiting factor that affects all systems built on social media.

The Placemaker system localizes POIs within a median distance of 1.1 kilometers for the locations it identifies (which is just over half of the examples). For the POIs that Place-maker identifies as such, the median distance from the ground truth is around 300 meters. This gives a reasonable bound on the performance of any localization system because the Placemaker system is built upon surveyed, curated data. The differences in the distance from the ground truth can partially be accounted for by the fact that some POIs (such as airports, and university campuses) are larger than one kilometer, and it is not clear how to express the location with a latitude/longitude point.
Points of interest form the basis of content for a grow-ing number of mobile and social media applications. Lo-cal search and recommender systems rely on knowing the points of interest in a city in order to understand a user X  X  geographic context, to better serve relevant results. Auto-matically detecting POIs allows us to develop systems that are dynamic and that reflect the services and places people visit in a city in the course of their daily lives. In this paper we presented a system for detecting POIs in unstructured text from labelled and unlabelled data. We showed system performance on manually annotated data composed mostly of news articles, on Wikipedia articles and on mentions of POIs bootstrapped from social media. We can achieve a precision of up to 87% training and testing on unlabelled data. For half of the POIs, we can identify their location within one kilometer of the true location.

For future work we would like to introduce a more le-nient evaluation metric that allows for small variations in the name of a POI. We also intend to refine the learning algorithm to leverage social media data more effectively and to incorporate physical information about the POI such as its relationship to the geography in which it is situated. Fi-nally, as the research community builds more systems on social media data, the credibility of this data must be bet-ter understood to determine which data examples are most suitable for training and evaluation.
This work was supported in part by the European Com-mission FP7 programme: (FP7/2011-14) under grant agree-ment no. 287615 (PARLANCE). [1] http://www.itl.nist.gov/iaui/894.02/related_ [2] http://www.itl.nist.gov/iad/mig//tests/ace/ [3] S. Ahern, M. Naaman, R. Nair, and J. Yang. World [4] E. Amitay, N. Har X  X l, R. Sivan, and A. Soffer. [5] M. Collins. Ranking algorithms for named-entity [6] D. Crandall, L. Backstrom, D. Huttenlocher, and [7] J. Ding, L. Gravano, and N. Shivakumar. Computing [8] J. Hoffart, F. Suchanek, K. Berberich, E. Kelham, [9] L. Hollenstein and R. Purves. Exploring place through [10] L. Kennedy, M. Naaman, S. Ahern, R. Nair, and [11] J. Lafferty, A. McCallum, and F. Pereira. Conditional [12] A. McCallum and W. Li. Early results for named [13] Q. Mei, C. Liu, H. Su, and C. Zhai. A probabilistic [14] E. Moxley, J. Kleban, and B. S. Manjunath.
 [15] L. Mummidi and J. Krumm. Discovering points of [16] N. O X  X are and V. Murdock. Modeling locations with [17] N. Okazaki. Crfsuite: a fast implementation of [18] D. Segal.  X  X losed, Says Google, but Shops X  Signs Say [19] P. Serdyukov, V. Murdock, and R. van Zwol. Placing [20] E. F. Tjong and K. Sang. Introduction to the [21] E. F. Tjong, K. Sang, and F. de Meulder. Introduction [22] T. Vincenty. Direct and inverse solutions of geodesics [23] H. Wallach. Conditional random fields: An [24] C. Wang, J. Wang, X. Xie, and W.-Y. Ma. Mining [25] X. Yi, H. Raghavan, and C. Leggetter. Discovering [26] Z. Yin, L. Cao, J. Han, C. Zhai, and T. Huang. [27] V. W. Zheng, Y. Zheng, X. Xie, and Q. Yan.
 [28] W. Zong, D. Wu, A. Sun, E.-P. Lim, and D. H.-L.
