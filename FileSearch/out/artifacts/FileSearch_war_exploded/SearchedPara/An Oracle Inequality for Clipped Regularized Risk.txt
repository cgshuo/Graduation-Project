 the regularized risk our results also apply to actual SVM algorithms. prior knowledge on the noise exponents.
 be found in Section 4. closed unit ball B of continuous functions C ( X ) equipped with the usual maximum-norm k . k constants we always assume that this embedding has norm 1, i.e. k . k that the associated L -risk of a measurable function f : X  X  R is defined by Note that the assumption L ( y, 0)  X  1 immediately gives R L -risk is denoted by R  X  attaining this infimum is denoted by f  X  where  X &gt; 0 . Note that if we identify a training set T = (( x its empirical measure, then f of these learning schemes is the approximation error function are defined by where B covering numbers in the Hilbert space L f : X  X  Y  X  R and which is equipped with the norm In other words, L Learning schemes of the form (1) typically produce functions f clipping the function f we now define its clipped version  X  f : X  X  [  X  1 , 1] by R inequality for clipped versions of f (4). Let H be a RKHS of continuous functions on X . For a fixed element f In addition, we assume that we have a variance bound of the form for some constants p  X  (0 , 1) and a  X  1 . For fixed  X &gt; 0 let f R We begin with an example that deals with a fixed kernel: where Recall that this learning rate has already been obtained in [11]. the sample size: loss and the Gaussian RKHSs H [9] that there exists a function f (7) for all p  X  (0 , 1) with where  X &gt; 0 can be arbitrarily chosen and c for  X  := q yields that the corresponding SVM can learn with rate n  X   X  +  X  , where [9, Theorem 2.8].
 T cardinality m (8) then shows that with probability not less than 1  X  3 m second half T where C is a constant only depending on d , q ,  X  , and  X  , and (  X   X  minimizes the empirical risk R Now assume that  X  we can choose  X  without knowing the noise exponents  X  and q a-priori. with respect to the measure  X  and a function h : Z  X  R we define R n  X  1  X  1 h ( z 1 ) +  X  X  X  +  X  n h ( z n ) . Now the general oracle inequality is: L be a loss function which satisfies (3) and (4). For a fixed pair ( p Moreover, let us assume that the quantity B ( f f : X  X  R . Furthermore, suppose that there exists a subroot  X  n with Finally, let ( p all  X   X  1 and all r satisfying we have with probability not less than 1  X  3 e  X   X  that Proof: We write B for B ( f Let us first estimate the term in line (12). To this end we write h assumption on L guarantees h B . In addition, we obviously have E Now using  X  ab  X  a Let us now estimate the term in line (13). To this end we write h k h E Since elementary calculations show that 2  X   X   X   X  1 2  X   X   X  1 we obtain Therefore we have with probability not less than 1  X  e  X   X  that Let us finally estimate the term in line (11). To this end we write h Moreover, for r&gt; 0 we define Then for g Now define Standard symmetrization then yields and hence Lemma 3.2 proved below together with (9) shows  X ( r )  X  10  X  applying Talagrand X  X  inequality in the version of [3] to the class G Let us define  X  not less than 1  X  e  X   X  that for all ( p,f )  X  X   X F we have and consequently we have with probability not less than 1  X  e  X   X  that Now observe that for the functions h have obtain that with probability not less than 1  X  3 e  X   X  we have  X  30  X  n ( r )+ However we also have 120  X  2(2  X   X  ) r 4  X  r , and hence we find the assertion.
 For the proof of Theorem 3.1 it remains to show the following lemma: [0 ,  X  ) . Define and suppose that there exists a subroot  X  such that Then we have  X ( r )  X  5 by setting x := 4 . Proposition 4.1 Let F := H be a RKHS, P := { p is satisfied then there exists a constant c plies we have the bound (9) with  X  Theorem 3.1 implies that Condition (10) becomes and solving with respect to r yields the conclusion.
 for model selection: (4), and the variance bound (6). Furthermore, let F := { f mapping X into [  X  1 , 1] . For T  X  ( X  X  Y ) n we define than 1  X  3 e  X   X  that Proof: Since all functions f ator. For r&gt; 0 we now define F F where c is a universal constant. Applying Theorem 3.1 then yields the assertion.
