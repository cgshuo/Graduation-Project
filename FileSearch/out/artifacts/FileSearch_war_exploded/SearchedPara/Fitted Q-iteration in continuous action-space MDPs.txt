 We will build on the results from [1, 2, 3] and for this reason we use the same notation as these papers. The unattributed results cited in this section can be found in the book [4]. a measurable space and M ( X ) denotes the set of all probability measures over X . The Lebesgue-measure shall be denoted by  X  . We start with the following mild assumption on the MDP: Assumption A1 (MDP Regularity) X is a compact subset of the d X -dimensional Euclidean space, and that the expected immediate reward function, r ( x, a ) = reward that is encountered while the policy is executed: V  X  ( x ) = E  X  [ R to denote the action-value function of policy  X  : Q  X  ( x, a ) = E  X  [ is optimal. For  X  : X  X  A we define its evaluation operator , T  X  : B ( X  X A )  X  B ( X  X A ) , by ( T  X  Q )( x, a ) = r ( x, a ) +  X   X  R R sup a  X  X  Q ( x, a ) .
 Throughout the paper F  X  { f : X  X A  X  R } will denote a subset of real-valued functions over the state-action space X  X A and  X   X  X  X will be a set of policies. For  X   X  M ( X ) and f : X  X  R measurable, we let (for p  X  1 ) k f k p p, X  = Further, we extend k X k  X  to F by k f k 2  X  = distribution over A . We shall use the shorthand notation  X f to denote the integral denote the space of bounded measurable functions with domain X by B ( X ) . Further, the space of the supremum norm. The generic recipe for fitted Q-iteration (FQI) [5] is problem in the form of a list of data-point pairs:  X 
R function in FQI becomes random. Furthermore, this function depends on the same data that is used example shows that if the complexity of the random functions defining the regression problem is uncontrolled then successful estimation might be impossible.
 Amongst the many regression methods in this paper we have chosen to work with least-squares methods. In this case Equation (1) takes the form We call this method the least-squares fitted Q-iteration (LSFQI) method. Here we introduced the by the behavior policy.
 should be chosen carefully, to keep a balance between the representation power and the number of samples. As a specific example for F consider neural networks with some fixed architecture. In Then the above minimization becomes the problem of tuning the weights. Another example is to use linearly parameterized function approximation methods with appropriately selected basis functions. training procedure becomes similar to LS-SVM training [7].
 As indicated above, the analysis of this algorithm is complicated by the fact that the new dataset procedure, as documented in a number of cases in, e.g., [8].
 that under appropriate conditions these problems can be overcome if the function set is chosen in a judicious way. However, the results of these works would become essentially useless in the case of an infinite number of actions since these previous bounds grow to infinity with the number of suggested by the counterexample that involved random targets. The following result elaborates this point further: Proposition 2.1. Let F  X  B ( X  X A ) . Then even if the pseudo-dimension of F is finite, the fat-shattering function of can be infinite over (0 , 1 / 2) . 2 imposing further special conditions on F , the learning problem may become infeasible. the action space has a really high dimensionality, this approach becomes unfeasible (even enumer-this way it might be difficult to get a fine control of the capacity of the resulting set. In the approach explored here we modify the fitted Q-iteration algorithm by introducing a policy set  X  and a search over this set for an approximately greedy policy in a sense that will be made ( Q k ,  X   X  k ) , k = 0 , . . . , K , defined by the following equations: are similar across similar states.
 the above mentioned complexity control problem provided that the complexity of  X  is controlled and the proof will rely on controlling the complexity of F  X   X  by selecting F and  X  appropriately. 3.1 Outline of the analysis define the t th TD-error as follows: Further, we define the empirical loss function by where the normalization with  X  ( A ) is introduced for mathematical convenience. Then (3) can be written compactly as Q k +1 = argmin f  X  X   X  L N ( f ; Q k ,  X   X  k ) .
 unbiased estimate of random samples: This result is stated formally by E Since the variance term in (5) is independent of f , argmin f  X  X  L ( f ; Q,  X  ) = over the space of action-value functions, projecting TQ k using empirical risk minimization on the will have to deal with both the error coming from the approximate projection and the error coming from the choice of  X   X  k . To make this clear, we write the iteration in the form controlled, too. Since we are ultimately interested in the performance of the policy obtained, we For these we need a number of assumptions that concern either the training data, the MDP, or the function sets used for learning. 3.2 Assumptions 3.2.1 Assumptions on the training data mixing means that future depends weakly on the past.
 Note that the mixing coefficients do not need to be known. In the case when no mixing condition is X
N . Thus, in this case the learner has many copies of the same random variable and successful of a more involved proof) the result would still hold. 3.2.2 Assumptions on the MDP we shall need some assumptions on the MDP. A convenient assumption is the following one [11]: Assumption A3 (Uniformly stochastic transitions) For all x  X  X and a  X  A , assume that C results under the following, weaker assumption: Assumption A4 (Discounted-average concentrability of future-state distributions) Given  X  , sup  X  max bility coefficient of the future-state distributions.
 The number c ( m ) measures how much  X  can get amplified in m steps as compared to the reference class of systems (see the discussion in [11]).
 Assumption A5 (The random policy  X  X akes no peak-states X ) Consider the distribution  X  = (  X   X   X  Note that under Assumption A3 we have  X   X   X  C  X  . This (very mild) assumption means that after a set is upper bounded by  X   X  -times the probability of the starting state being in the same set.  X  ( a 0 , v )  X  R d A +1 : k a  X  a 0 k 1  X   X , 0  X  v/h  X  1  X  X  a  X  a 0 k 1 / X  h and base given by the ` 1 -ball B ( a,  X  ) def = all a  X  X  , for all  X  &gt; 0 , [12, 13]). Here we employ the following extra condition: Assumption A7 (Lipschitzness of the MDP in the actions) Assume that the transition probabilities ( x, a, a 0 )  X  X   X A X A and measurable set B of X , sistent planning algorithms. 3.2.3 Assumptions on the function sets used by the algorithm as we shall see it from the bounds. The first assumption concerns the class F : Assumption A8 (Lipschitzness of candidate action-value functions) Assume F  X  B ( X  X A ) Q ( x, a 0 ) | X  L A k a  X  a 0 k 1 holds for any x  X  X  , a , a 0  X  X  , and Q  X  X  . with the concept of VC -dimension. 7 Here we use the pseudo-dimension of function sets that builds upon the concept of VC -dimension: Definition 3.1 (Pseudo-dimension) . The pseudo-dimension V F + of F is defined as the VC -F ).
 Since A is multidimensional, we define V  X  + to be the sum of the pseudo-dimensions of the coordi-nate projection spaces,  X  k of  X  : Now we are ready to state our assumptions on our function sets: Assumption A9 (Capacity of the function and policy sets) Assume that F  X  B ( X  X A ; Q max ) for Q max &gt; 0 and V F + &lt; +  X  . Also, A X  [  X  A  X  , A  X  ] d A and V  X  + &lt; +  X  . Let us first consider the policy set  X  . Introduce Note that inf  X   X   X   X  ( EQ  X  E  X  Q ) measures the quality of approximating  X EQ by  X E  X  Q . Hence, be made small by choosing  X  large.
 a fixed policy  X  , the one-step Bellman-error of F w.r.t. T  X  is defined as Taking again a pessimistic approach, the one-step Bellman-error of F is defined as (pseudo-dimensions) which leads to an increase of the estimation errors. Hence, F and  X  must be selected to balance the approximation and estimation errors, just like in supervised learning. 3.3 The main result Assumption A3 (respectively A4), k V  X   X  V  X  K k  X  (resp. k V  X   X  V  X  K k 1 , X  ), is bounded by where C depends on d A , V F + , ( V  X  + Q plays the role of the  X  X ombined effective X  dimension of F and  X  . We have presented what we believe is the first finite-time bounds for continuous-state and action-that has proved to be useful in a number of cases, even when used with non-averagers for which no there is a systematic way of making these algorithms work and to point at possible problem sources the same time. We discussed why it can be difficult to make these algorithms work in practice. We critic algorithms. The bound in this paper is similar in many respects to a previous bound of a Bellman-residual minimization algorithm [2]. It looks that the techniques developed here can be used to obtain results for that algorithm when it is applied to continuous action spaces. Finally, although we have not explored them here, consistency results for FQI can be obtained from our results using standard methods, like the methods of sieves. We believe that the methods developed here will eventually lead to algorithms where the function approximation methods are chosen based on the data (similar to adaptive regression methods) so as to optimize performance, which in our opinion is one of the biggest open questions in RL. Currently we are exploring this possibility. Acknowledgments
