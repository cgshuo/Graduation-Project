 Human beings are quite familiar with grouping text documents into categories. However, with the rapid growth of the Internet and the wide availablitity of electronic documents, it is infeasible for a user to browse all the documents and determine the underlying clusters. Therefore, it is very useful if a system can simulate human learning and discover the clusters automatically. Text clustering technique deals with finding a structure in a collection of unlabeled text docu-ments. Often, text clustering is conducted in a completely automated manner. However, a small amount of user provided information often offers good hints on guiding the clustering to a better partition. Our goal is to incorporate limited user provided information into the automatic clustering process which in return improves the clustering performance.

Although browsing the whole document collection is not feasible, it is much easier for users to read a small amount of documents and grasp a preliminary understanding on the document set. Users can provide a small amount of sample documents on some, not necessarily all, clusters. These sample documents are helpful to guide the clustering process. This problem can be viewed and handled as a semi-supervised clustering problem. After the clustering process, some un-labeled documents are grouped into existing clusters with samples. Besides, new clusters are also generated.

After the clusters are learned, users may examine the clustering results and give a small amount of feedback information. The user feedback is useful since it can provide the learning bias in the subsequent clustering task. Another round of clustering process is then invoked taking into consideration of the newly user feedback information. The user feedback can be provided in such an incremental manner until a satisfactory partition of the document collection is obtained. Some natural user feedbacks are:  X  Document d should belong to cluster S .  X  Document d should not belong to cluster S .  X  Document d should be switched from cluster S i to S j .  X  Two documents should be in the same cluster.  X  Two documents should be in different clusters.

For the modeling of each cluster, it is common to find that each cluster em-phasizes on different features. In our model, a weight metric is associated with each cluster. The weight metric captures a set of weights for the features of each cluster. The more important a feature, the higher weight is assigned to the feature. This local weight metric contributes to the distance calculation and it is learned using both constraints and the unlabeled data via a separate opti-mization procedure. The quality of metric is improved by incorporating more precise constraints automatically generated from user feedbacks. After the qual-ity of local metric for every cluster is improved, the clustering results will also be improved in the subsequent learning process.

We have conducted extensive experiments comparing the two variants of our proposed approach. One of them is to incrementally consider the constraints generated from user feedbacks. We also compared with the approach without local metric learning. From the experimental results, our approach of learning local metric from user feedbacks is more effective than the approach without local metric learning. User feedbacks can dramatically improve the clustering performance especially when there are a small amount of constraints. Semi-supervised clustering is of great interest in recent years. Wagstaff and Cardie proposed instance-level hard constraints to improve the performance of clustering algorithms [9]. Demiriz et al. [4] proposed an unsupervised clustering method which labels each cluster with class membership, and simultaneously optimizes the misclassification error of the resulting clusters. Basu et al. [1] ex-plored the use of labeled data to generate initial seed clusters, as well as the use of constraints generated from labeled data to guide the clustering process. All of the approaches use only user provided labeled documents or constraints to guide the clustering algorithm. However, the distance measure does not consider the user provided information.

Bilenko et al. [3] described an approach that unifies the constraint-based and metric-based semi-supervised clustering methods. It restricts that constraints are needed for every cluster. Sugota et al. [2] investigated a theoretically mo-tivated framework for semi-supervised clustering that employs Hidden Random Markov Fields. It learns a single distance metric for all clusters, forcing them to have similar shapes. A common shortcoming of all of the above semi-supervised clustering algorithms is that they do not consider the user feedback information in an incremental manner.

There are also some recent research works on learning better metric over the input space. Frigui and Nasraoui [6] proposed an approach that performs clus-tering and feature weighting simultaneously in an unsupervised manner. Jing et al. [7] presented a modification to the original FW-KMeans to solve the sparsity problem that occurs in text data where different sets of words appear in different clusters. All of these approaches do not consider user feedback to guide the met-ric learning. Only the unlabeled data ar e considered. Xing et al. [10] presented an algorithm that, given examples of similar pairs of points, can learn a distance metric that respects these relationships. The limitation of this approach is that it only uses the constraints to learn the metric. We propose an approach con-sidering both the unlabeled documents and constraints to learn the local weight metric for each cluster. 3.1 Constraint Generation from User Feedback We formulate the problem as a semi-supervised clustering problem. The sam-ple documents and user feedbacks are handled by a set of constraints. Two kinds of constraints namely, must-link constraints and cannot-link constraints are adopted. If a must-link relationship is specified for two documents d i and d j , they must be grouped into the same cluster. In some situations, there is a fur-ther specification that they should be in the same cluster as a particular sample document. If a cannot-link relationship is specified for two documents d i and d j , they must be grouped into different clusters.

After the clusters are learned, users can examine the clustering result and give feedbacks on the quality of the clusters. The user feedback information is used to guide the clustering process to a better partition in the next round. The user feedback information is transformed automatically into a set of constraints. Suppose cluster S contains some previously provided sample documents. If a user suggests document d should belong to cluster S , d will form must-link constraints with all the sample documents of S . d will also be regarded as a sample document of S . Similarly if a user suggests d should not belong to S , it will form cannot-link constraints with all the sample documents of S . If a user suggests document d should be switched from cluster S i to S j , it will form must-link constraints with all the sample documents of S j and cannot-link constraints with all the sample documents of S i . A user can also specify that two documents should belong to the same cluster. In this case, a must-link constraint is generated. Similarly, if a user specifies that two documents should belong to different clusters, the two documents form a cannot-link constraint. 3.2 Metric-Based Distance Measure We make use of cosine similarity to measure the similarity between two docu-ments or between a document and a cluster centroid. Standard cosine similarity fails to estimate similarity accurately if different features contribute to differ-ent extent to the similarity value. We design a local metric A for each cluster. Each feature is assigned a weight to indicate its contribution to the similarity of two documents in that particular cluster. Precisely, the local weight metric A = { a i ,i =1 ...m } is a set of weights assigned to each feature of the cluster. The metric-based distance is calculated as follows: where X and Y are the vector representation of two documents; m is the number of features in A ; x i , a i ,and y i are the values of feature i in the vector represen-tation of X ,metric A ,and Y respectively; || X || A denotes the weighted L 2 norm: || 3.3 Objective Function Clusters are learned by minimizing an objective function. The objective func-tion is composed of two parts, namely, the document distance function and the penalty function, which considers the documents and constraints respectively. An optimal partition is obtained when the overall distance of the documents from the cluster centroids is minimized while a minimum number of constraints are violated.

The document distance function  X  measures the distance of the documents from the cluster centroids. The distance is calculated with the help of the local weight metric A S for cluster S . This function is formulated as follows: where K is the number of clusters; D S is the set of documents that belong to the cluster S ; d and  X  S are the vector representation of a document and the centroid of the cluster S respectively; A S is the local metric associated with cluster S ;  X  A s ( d,  X  S ) measures a metric-based distance between document d to cluster centroid  X  S . Since the amount of labeled documents is small compared with unlabeled documents, the document distance function  X  is mainly related to the unlabeled documents.

The penalty function  X  is related to the constraints. It measures the cost of violation of the constraints. We denote the set of must-link constraints as M and the set of cannot-link constraints as C . A traditional must-link constraint is violated when two documents are grouped into different clusters. When a must-link constraint M is specified for two documents and the documents are regarded as sample documents for a particular cluster, the following situation is regarded as violating a must-link constraint. A must-link constraint is specified for two documents. One of the documents is grouped into the correct cluster as a particular sample document. The other document is assigned to a wrong cluster. When a cannot-link constraint C is specified for two documents and they are grouped into the same cluster, we regard this situation as a violation of cannon-link constraints. We make use of the distance of the two documents in the constraints as the penalty cost for violating the constraints in M and C . The penalty function  X  is formulated as follows:  X  = where S d is the cluster that d belongs to; A S is local weight metric for cluster S ; L d is the actual cluster to which document d should belong;  X  is the indicator function. For the traditional must-link constraint, L d i is not known from the user feedback and sample documents.  X  will be replaced by the indicator function. Therefore, the penalty function  X  is formulated as follows:  X  = Consequently, the objective function  X  is formulated as follows: where  X  is a parameter for balancing the contribution of the document distance and constraints in the objective function. Therefore the semi-supervised cluster-ing task is to minimize the following objective function: 3.4 EM Process The optimization for finding the clusters minimizing the objective function ex-pressed in Equation 6 is achieved by an iterative EM process. The outline of the algorithm is presented in Figure 1.
 The user provided samples are used as seeds to initialize the cluster centroids. Let  X  be the number of clusters that the user has provided sample documents. We assume that  X   X  K . In addition to  X  sample document centroids, we select K  X   X  centroids using the farthest-first algorithm [2].
 In the E-step, the assignment of the documents to the clusters are updated. Each unlabeled document is assigned to the cluster that minimizes the objective function.

The M-step consists of two parts. Every cluster centroid  X  S is re-estimated using the current document assignment as follows: where D S is the set of documents that belong to the cluster S ; A S is the local metric associated with cluster S ;
The second part of the M-step performs local weight metric learning for each cluster. The local weight metric is learned by a separated optimization process presented in Section 3.5.

Two different methods are investigated for incorporating the new constraints to the semi-supervised clustering process. The first method is denoted by Method-B. It conducts the next round of clustering process with the updated constraints without considering the current document partition. After the new constraints are obtained from the user feedbacks, the semi-supervised clustering is invoked from Step 1 of the algorithm shown in Figure 1. The clusters are initialized with updated sample documents and farthest-first algorithm. Clustering process is conducted considering the whole set of the updated constraints.

The second method is denoted by Method-I. It conducts the semi-supervised learning with feedback in an incremental manner. The current clustering result is a local optimal partition. User feedback information provides some sugges-tions on the direction where the learning process should go in the next round. Illustrated by Figure 1, we invoke the clustering process starting from Step 2 with the current clustering result. The semi-supervised clustering is conducted by considering the whole set of the updated constraints. 3.5 Method for Local Metric Learning Given the current document assignment, the local metric is learned to obtain different weights on the features used by each cluster. We cast the metric learning as a separate optimization problem for each cluster. The objective function is composed of the objective function components  X  S of each cluster as follows: where  X  S is the document function for cluster S ;  X  S is the penalty function for cluster S .  X  S and  X  S are formulated as follows: where  X  S is the set of must-link constraints involved in cluster S ; D S is the set of documents that belong to the cluster S ; d and  X  S are the vector representation of a document and the centroid of the cluster S respectively; A S is the local metric associated with cluster S ;where S d is the cluster that d belongs to; L d is the actual cluster to which document d should belong;  X  A S (  X  ,  X  )measuresthemetric-based distance between two documents or between a document and a cluster centroid;  X  is the indicator function. For the traditional must-link constraint, the  X  S is formulated as follows:
The metric is learned by minimizing the objective function  X  S .Weemploythe steepest descent method to conduct the optimization for the metric A S for each cluster using the objective function component for each cluster. Every weight a S in the local metric A S is updated using the formula: the gradient direction for the the  X  -th iteration which is calculated as follows: 4.1 Datasets Two real-world data sets were used for conducting our experiments. The first dataset was derived from the TDT3 document corpus 1 . We selected the native English newswire news documents with human assigned topics. The human as-signed topics are also grouped into 12 general topic categories such as  X  X lection X ,  X  X ports X , etc. This dataset contains 2,450 news stories organized in 12 classes corresponding to those general topic categories. The second dataset is derived from Reuters RCV1 corpus [8]. The RCV1 corpus is an archive of 800,000 man-ually categorized newswire stories. The news stories are organized in four hier-archical groups. We made use of part of the RCV1 corpus to form a dataset for our experiments. We did not select those small clusters which contain a small number of news stories. Stories belonging to at most one of clusters in the first level and the second level were randomly selected. There were 10,429 news sto-ries, in total, organized in four classes were chosen. In particular, there are 6037, 1653, 2239, and 500 news stories in each class respectively.

We pre-processed the TDT3 dataset by stop-word removal and TF-IDF weighting. The RCV1 dataset has been pre-processed 2 . High-frequency and low-frequency words removal were conducted for the two datasets, following the methodology presented in [5]. The thresholds for removing high-frequency and low-frequency words for TDT3 dataset were set to 30 and 2 respectively. The thresholds for removing high-frequency and low-frequency words for RCV1 dataset were set to 1000 and 1 respectively. 4.2 Evaluation Methodology We make use of FScore measure to evaluate the quality of a clustering solu-tion [11]. Each true class L is mapped to an appropriate system output cluster S to which it matches the best. The suitability of the cluster to the class is mea-sured using the F value. The F value combines the standard precision measure P and recall measure R used in information retrieval as follows: where n ij is the number of documents in cluster S j belonging to L i ; n s j is the number of documents in cluster S j ; n l i is the number of documents belonging to L i ; The FScore for the class L i is the maximum F value for each system generated cluster.
 where T is the set of system generated clusters. The FScore of the entire system generated clusters is the summation of FScore for each class L i weighted by the size of the class.
 where n is the total number of documents in the dataset. 4.3 Experimental Setup We make use of the set of true classes as a reference to simulate the user feed-backs. The clustering solution is compared with the true classes. For each cluster with sample documents, some wrongly assigned documents are selected. The cor-rect cluster of the document is provided if the correct cluster has some sample documents. For the clusters without sample documents, we select document pairs to simulate the user feedbacks. One document in such pairs belongs to a cluster with sample documents while the other does not belong to the clusters with sample documents.

We conducted experiments for our proposed two methods, namely, Method-B and Method-I. For comparative investigation, we also conducted experiments for our proposed method but without local metric learning. The number of clusters, K , for TDT3 dataset and RCV1 dataset was set to 12 and 4 respectively. In each experiment, the number of clusters with user provided sample is less than K . For TDT3 dataset, Figure 2, Figure 3, and Figure 4 depict the clustering performance of 9, 10, and 11 clusters with user provided samples respectively. In each experiment, the initial sample documents for each cluster was set to 10. The feedback information was provided incrementally. We repeat such experiment setup for different combinations of clusters with user provided samples. The performance was measured by the average of the FScore of 3 combinations of clusters.

The experiments for RCV1 dataset were conducted in a similar way. The number of clusters with user provided sample was 2 and 3 as shown in Figure 5 and Figure 6. The performance was measured by the average of the FScore of 4 combinations of clusters with sample documents. The initial sample documents for each cluster was 40. In each round, 20 user feedbacks were provided for each cluster. 4.4 Analysis of the Results From the experimental results, it shows that the Method-B and Method-I out-perform the approach without local metric learning in all of the experiments. The local metric learning is effective in improving the clustering performance.
Incorporating user feedbacks is useful especially when there are a few number of constraints. After some iterations, when the number of constraints for each cluster reach to 25 for TDT3 dataset and 100 for RCV1 dataset, the performance of the clustering becomes stable. Providing more user feedbacks does not affect the clustering performance dramatically. Therefore, only a small number of user feedback is needed to guide the clustering process to a better document partition.
From all the experimental results, it shows that the performance of Method-B and Method-I are close to each other. However, the execution time of Method-I is much faster than Method-B. In TDT3 corpus, Method-I uses 28%, 27%, and 30% less time than Method-B for the clustering performance of 9, 10, and 11 clusters with user provided samples respectively. In RCV1 corpus, it takes 62% and 47% less time than Method-B. Incremental semi-supervised clustering is computationally more efficient than conducting the clustering process in batch 3 . We have presented an approach that incorporates incremental user feedback into text clustering. User feedbacks and a small amount of sample documents are provided for some clusters and are transformed into a set of constraints. A local weight metric is associated with each cluster reflecting the importance of each feature of the cluster. The local weight metric is learned with the help of unlabeled data and constraints. The quality of local weight metric can be improved with newly updated constraints learned from user feedbacks. After the quality of local weight metric for every cluster is improved, a better document partition can be obtained in the subsequent clustering process. Experimental results show that our proposed approach on user feedback incorporation and local metric learning is effective for improving the clustering performance.
There are some directions for further study. Incorporating more natural types of user feedbacks especially for the newly generated clusters can be one direction. Currently, the user feedbacks in our approach capture the relationship between documents. Another useful user feedback can be the term feedback reflecting the relationship between features and clusters. Also, currently we only generate cannot-link constraints from the newly generated clusters. The clustering per-formance can be further improved if we can consider more user feedbacks and generate more kinds of constraints.
 Another direction is to allow users provide feedbacks with a confidence value. Since a user only has a preliminary understanding on the dataset, he/she may provide some feedbacks with uncertainty. In this situation, we can assign a con-fidence value to the user feedbacks. The confidence value can help to judge how much the clustering process will rely on the user feedbacks.

