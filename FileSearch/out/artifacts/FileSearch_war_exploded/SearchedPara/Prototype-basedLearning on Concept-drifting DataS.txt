 Data stream mining has gained growing attentions due to its wide emerging applications such as target marketing, email filtering and network intrusion detection.In this paper, we propose a prototype-based classification model for evolving data streams, called SyncStream, which dynamically mod-els time-changing concepts and makes predictions in a lo-cal fashion. Instead of learning a single model on a sliding window or ensemble learning, SyncStream captures evolv-ing concepts by dynamically maintaining a set of prototypes in a new data structure called the P-tree. The prototypes are obtained by error-driven representativeness learning and synchronization-inspired constrained clustering. To identify abrupt concept drift in data streams, PCA and statistics based heuristic approaches are employed. SyncStream has several attractive benefits: (a) It is capable of dynamically modeling evolving concepts from even a small set of pro-totypes and is robust against noisy examples. (b) Owing to synchronization-based constrained clustering and the P-Tree, it supports an efficient and effective data representa-tion and maintenance. (c) Gradual and abrupt concept drift can be effectively detected. Empirical results shows that our method achieves good predictive performance compared to state-of-the-art algorithms and that it requires much less time than another instance-based stream mining algorithm. H.2.8 [ Database Management ]: Database Applications -Data Mining Data stream; Concept drift; Classification; Synchronization
Data stream classification is a challenging data mining task because of two important properties: potentially infi-Figu re 1: How to select suitable training examples for learning a classification model on an evolving data stream? nite length and evolving nature. Currently, there are two main strategies to mine evolving data streams: single model learning and ensemble learning. Single model learning tries to learn and update a classification model by training on a fixed or adaptive window of recent incoming examples [9], [12]. The prediction performance of this type of learning usually suffers in the presence of concept drift. By contrast, ensemble learning partitions continuous data streams into smaller chunks and uses them to train a number of base classifiers to capture evolving concepts [16], [2]. Ensembles can scale up to large volumes of streaming data and adapt quickly to new concepts. However, two existing limitations of ensemble learning are considered only little. First, the performance of ensemble learning depends on the accuracy of each base classifier capturing the concept, where each base classifier is actually used in a black-box fashion. For illus-tration, let us take Fig. 1 as an example. If the size of data chunk is small (data arrived during [ t 0 , t 1)), the con-cept of the data stream for each base classifier cannot be sufficiently learned. On the other hand, if the size of a data chunk is too large (data arrived during [ t 0 , t 2)), base classi-fiers may learn more than one concept. Second, it is difficult to decide whether more recent or older examples are more relevant (e.g., the relatively old example P 1 is important for predicting the class label of P 2 in Fig. 1 (a)). Learning from recent examples, as in most established algorithms, is not always optimal. A more natural way is to dynamically se-lect short-term and/or long-term representative examples to capture the trend of time-changing concepts, and use them to predict the test data (cf. Fig. 1 (b)).

In this paper, we propose a new prototype-based learn-ing method to mine evolving data streams. To dynamically identify prototypical examples, error-driven representative-ness learning is used to compute the representativeness of examples over time. Noisy examples with low represen-tativeness are discarded, and representative examples can be further summarized by a smaller set of prototypes using synchronization-inspired constrained clustering. To dynam-ically represent the online training data, a new data struc-ture, the P-Tree, is proposed to update the set of prototypes, to capture time-changing concepts and to support accurate predictions.
 Building upon the data representation of synchronization-inspired constrained clustering, SyncStream has several at-tractive benefits, most importantly:
The remainder of this paper is organized as follows: In the following section, we briefly survey related work. Section 3 presents our algorithm in detail. Section 4 contains an extensive experimental evaluation. We finally conclude the paper in Section 5.
Data stream classification has received much attention in recent decades. Early algorithms focused on learning a sin-gle model from evolving data (e.g., [9]). But very soon they were replaced by ensemble models in order to obtain more accurate predictions. Ensemble models (e.g., [16, 2]) are of great interest, as each model can represent one possible dis-tribution over the data, therefore current data can be gener-ated from a mixture of distributions with changing weights over time. Also, if one of the previous concepts reappears, they can handle this case more effectively [8]. However, as explained in Section 1, ensemble models have some short-comings in determining a suitable window size and useful instances. One efficient solution to address these shortcom-ings is the use of instance-based learning, which has been considered in some papers in recent years [10, 17, 13]. In instance-based learning, instead of taking time to build a global model, the target function is approximated locally by means of selected instances. The inherent incremental nature of instance-based learning algorithms and their sim-ple and flexible adaptation mechanism makes this type of algorithm suitable for learning in complex dynamic envi-ronments. Nevertheless, instance-based learning algorithms have to face their own challenges, such as their space re-quirements and their sensitivity to noise. To manage mem-ory consumption and accelerate nearest neighbor lookup, researchers usually use a tree based data structure such as the M-Tree [17, 13]. In this tree, instances of small neigh-borhoods are kept in the leaves; inner nodes combine their subnodes to form bigger spheres, and the root encompasses all the data. The number of subnodes is determined heuris-tically. In contrast, we provide an intuitive and effective strategy to summarize data, and dynamically maintain a small set of prototypes in the proposed P-Tree to support effective and efficient online data maintenance.

To handle concept drift in an evolving data stream, one strategy is to dynamically monitor the change of a distribu-tion over time and set a flag whenever drift is detected. This type of method is referred to as explicit concept drift detec-tion. In contrast to this strategy, implicit methods adapt to new concepts by updating a model continuously. The bene-fit of using explicit methods is the ability to react to sudden drift more quickly, while implicit methods are more suit-able for data streams with gradual concept drift. Generally, methods for concept drift detection are categorised into sta-tistical techniques (e.g., [5]) and those based on performance monitoring between two consecutive data chunks (e.g., [1]). If concept drift is detected, the model may be reconstructed from scratch (e.g., [7]) or only the affected parts of the model are updated (e.g., [9]). Locally updating a model has the benefit of maintaining useful knowledge from previous data and is especially useful in recognizing recurring concepts. In this paper, error-driven representativeness learning is intro-duced to identify the most important prototypes to capture the trend of time-changing concepts in an implicit and lo-cal way, while sudden concept drift is handled based on two heuristic strategies.
In this section, we present the SyncStream algorithm for mining evolving data streams. Before that, we provide an overview of the algorithm and the intuition behind it.
As introduced in Section 1, the basic idea of our algo-rithm is to dynamically keep track of short-term and/or long-term representative examples to capture the trend of time-changing concepts, and use them for prediction in a local instance-based fashion. Specifically, a small set of data examples is first regarded as initial prototypes and in-serted into a proposed hierarchical tree structure called the P-Tree, which is used to maintain prototypes and drifting concepts on two levels, respectively. For each new exam-ple, the nearest prototype in the P-Tree is used to predict its label. Meanwhile, during the classification process, an error-driven learning approach is employed to determine the representativeness of prototypes in the P-Tree. If the near-est prototype correctly predicts the new example, this in-dicates that the prototype is in line with the trend of the current concept of the new example. The representative-ness of a prototype is thus increased (and decreased in the opposite case). However, due to restricted memory, we can-not store all prototypes in the P-Tree. To fit the P-Tree in an any-space memory framework and support efficient pre-diction, inappropriate or noisy examples with low represen-tativeness are discarded. The representative examples are further summarized as a smaller set of prototypes based on synchronization-inspired constrained clustering, where each prototype solely belongs to one specific class (cf. Section 3.2). Relying on constrained clustering, historical data is sum marized and, more importantly, the data structure of the original examples is well preserved. To further han-dle the evolving nature of data streams, PCA and statis-tics based methods are introduced to detect sudden con-cept drift, so that prototypes in the P-Tree can be updated quickly to capture a new concept. Once a new concept is emerging, the previous concept is modeled as a set of proto-types and stored on the concept level of the P-Tree. In the following, we will first elaborate on how to summarize his-torical data based on synchronization-inspired constrained clustering, before we move on to drift detection and the P-tree.
In the context of an evolving data stream, it is infea-sible to keep track of the entire history of the data. To support effective and efficient online data maintenance, we first introduce a data summarization technique by adapting synchronization-inspired clustering [14], [3]. The key philos-ophy of synchronization-based clustering approaches (e.g., Sync [3] or pSync [14]) is to regard each example as a phase oscillator and simulate the dynamical behavior of the exam-ples over time. Based on the non-linear local interactions among examples, similar examples will finally synchronize together and end up in the same common phase. The syn-chronized common phase represents the set of similar exam-ples, and thus provides an intuitive way to summarize the data.

Typically, a synchronization-based clustering algorithm needs three definitions to simulate a dynamic clustering pro-cess. In the following, we give a short summary of all nec-essary definitions.
 Definition 1 (  X  -Range Neighborhood )
Given a  X  2R and x 2D , the  X  -range neighborhood of an example x , denoted by N  X  ( x ), is defined as: where dist ( y, x ) is a metric distance function, and the Eu-clidean distance is used in this study.
 Definition 2 ( Interaction Model )
Let x 2R d be an example in the data set D and x i be the i -th dimension of the example x , respectively. Each example x is viewed as a phase oscillator. With an  X  -range neighborhood interaction, the dynamics of each dimension x of the example x is defined by: x i ( t + 1) = x i ( t ) + 1 j wh ere sin ( ) is the coupling function. x i ( t +1) describes the renewal phase value of the i -th dimension of example x at t = (0 , . . . , T ) during the dynamic clustering.
To characterize the level of synchronization between os-cillators during the synchronization process, a cluster order parameter R c is defined to measure the coherence of the local oscillator population.
 Definition 3 ( Cluster Order Parameter )
The cluster order parameter R c is used to terminate the dynamic clustering by investigating the degree of local syn-chronization, which is defined as:
P1 Figu re 2: Illustration of synchronization-inspired constrained clustering.

Th e dynamic clustering terminates when R c ( t ) converges, which indicates local phase coherence. At this moment, clus-ter examples have the same phase (common location in the feature vector space).

One of the most salient features of synchronization-based clustering is its dynamic property. During the process of clustering, each object moves in a non-linear way driven by the local data structure, and finally a set of examples with similar local structure will group together. The synchro-nized phase thus provides a natural way to summarize the local data. In contrast to computing the center of cluster examples or other summarized statistics, the synchronized phases represent the original data without losing the local structure of the data.

However, for data stream classification, clustering-based data summarization needs to be conducted in a supervised manner, i.e. it needs to take into account class informa-tion. Therefore, the interaction model needs to be extended to make sure the examples in a cluster solely belong to the same class. To achieve this goal, there are two strategies: active or passive. The active strategy is to exert the oppo-site interaction on two examples with different class labels. Although this strategy can push the examples with different class labels away from each other, the local structure cannot be maintained. The passive strategy is to impose a cannot-link constraint [15], where two examples with different class labels cannot interact with each other. Without interaction, the examples with distinct class labels cannot synchronize together. Thereby, we reformulate the interaction model us-ing the cannot-link constraint as follows.
 Definition 4 ( Interaction Model with Constraint )
Let x 2R d be an example in the data set D and x i be the i -th dimension of the example x , respectively. l x 2 is the class label of example x . With an  X  -range neighbor-hood interaction, the dynamics of each dimension x i of the example x is defined by: where l x and l y indicate the class labels of examples x and y , respectively. eq ( , ) is the function to check whether two class labels are equal.
Based on the extended interaction model, Fig. 2 displays two snapshots of the simulated dynamical example move-ment for data synchronization. Specifically, there are two classes of examples with different shapes. Given an inter-action range (Fig. 2 (a)), examples of the same class will interact together, while examples of different classes will not (e.g., example P 1). Finally, due to the non-linear interac-tion driven by local structure, similar examples of the same label synchronize together (Fig. 2 (b)). In this paper, we define the synchronized phases as prototypes. Due to the benefits of synchronization-based constrained clustering, we obtain two desirable properties of a prototype-based data representation:
In Section 3.4, we will demonstrate that the two properties are useful for online data maintenance.
After the prototype-based data representation, we intro-duce two heuristic strategies to identify abruptly drifting concepts, so that the classification model allows quickly learn-ing and adapting to emerging new concepts. Instead of only investigating the change of the data distribution (for de-tails, see the review paper by Gama et al. [6]), we exactly examine the change of the target concepts (i.e., the class distributions).

Formally, let D t and D t +1 be two given sequential data chunks in a data stream D , and L = f l 1 , l 2 , , l | L the set of labels. The objective is to analyze whether or not the statistical properties of each target concept (class distribution) between D t and D t +1 have changed over time.
To identify concept drift, the first proposal is to analyze the change of each class distribution using principal compo-nent analysis (PCA):
The second strategy for concept drift detection is based on statistical analysis. The basic idea is to compute a suit-able statistic, which is sensitive to class distribution changes between two sets of examples. The degree of concept drift is measured by the resulting p -value. Although statistical analysis is a straightforward solution to concept drift de-tection, computing statistics on multivariate data is usually complicated and time consuming. Here, we extend Brunner and Munzel X  X  generalized Wilcoxon test statistic [4], [11] to compare the differences of class distributions of two data sets. In contrast to most existing statistics (e.g., Welch test, Mann-Whitney U test), this non-parametric statisti-cal test allows for relaxation of the assumption of equality of variance and normal distributions simultaneously. More importantly, it allows an efficient computation. Formally, Brunner and Munzel X  X  generalized Wilcoxon test statistic is calculated as follows.

Given each data pair ( D l t and D l t +1 ), we investigate whether the sets of examples corresponding to class label l in the data sets D t and D t +1 are significantly different. For this purpose, we define the intra-and inter-rank on both data sets as follows. Let  X  l = D l t are defined as the rank of examples x 2 D l t , y 2 D l t +1 z 2  X  l on the j th dimension, respectively. In addition, we further calculate R j sional mean ranks of examples from D l t and D l t +1 in the data set  X  l . Formally, we compute the estimated variance  X  2 as follows: wh ere  X   X  ) . Fin ally, the adapted Brunner and Munzel X  X  generalized Wilcoxon test statistic, W l BF , is defined as:
If both numbers of examples are large enough ( &gt; 20), then the statistic W l BF is asymptotically standard normal. The test statistic is compared with a normal distribution of zero mean and unit standard deviation to obtain a p -value. The minimal value of all resulting p -values on the data pairs is used to measure the degree of concept drift between the two data chunks D t and D t +1 .
In this section we will first demonstrate how to learn the representativeness of prototypes over time. Subsequently, a hierarchical tree structure, called P-Tree, is proposed to dy-namically maintain the set of prototypes and evolving con-cepts building upon the prototype-based data representation and concept drift detection as introduced above.
In an evolving data stream, either short-term or long-term historical examples may be important for prediction. Tradi-tional single model learning or ensemble learning approaches usually capture the current data concept by operating on a sliding window of data or a set of data chunks. However, how to decide the size of a window or horizon of the training data is a non-trivial task. In addition, for both traditional single model learning or ensemble learning, usually, not all examples in the window or in a data chunk are relevant for classification. Therefore, in this paper, we propose an error-driven approach to automatically learn the represen-tativeness of data and dynamically identify the short-term or long-term prototypical examples to capture the data con-cepts.

The basic idea is to leverage the prediction performance of incoming examples to infer the representativeness of ex-amples in the training data. Formally, given a new example x , let C be the current training data set (a set of proto-types), and y 2C the one prototype nearest to x . x l and x are the true label and the predicted label of x by y , respec-tively. The representativeness of the prototype y is updated as follows: where the initial value of Rep ( y ) is one and Sign ( x, y ) is the sign function, and 1 if x equals y , -1 otherwise.
To support efficient and effective online data maintenance, we propose a tree structure called the P-Tree. The P-Tree is a hierarchical tree structure consisting of two levels, where the first level stores a set of time-changing prototypes cap-turing the current concept, and the second level stores drift-ing concepts over time (see Fig. 4). Except for the inclusion of new, incoming examples continuously, the P-Tree is also
Fig ure 4: An illustration of the P-Tree structure. required to be updated when the following two scenarios occur: (a) maximum boundary , or (b) concept drift .
Maximum boundary limits the number of prototypes and the number of concepts that can be stored in the P-Tree. This restriction allows us to perform data stream classifi-cation on any computer at hand in an any-space memory framework. The updating procedure is as follows: For each incoming example, if the size of the prototypes in the P-Tree does not exceed the maximum boundary (i.e., maxP ), it is directly inserted into the entry on the first level of the P-Tree, and meanwhile the representativeness of existing pro-totypes in the P-Tree is computed based on error-driven learning (cf. Section 3.4.1). Once the size exceeds maxP , prototypes with different representativeness are handled sep-arately. (a) Prototypes with low representativeness (e.g., a negative value) in the P-Tree are directly removed. (b) Pro-totypes with high representativeness (e.g., a positive value) in the P-Tree are kept, which indicates these prototypes can capture the current concept well. (c) Prototypes with unchanged representativeness are further summarized by a new and smaller set of prototypes based on synchronization-inspired constrained clustering (cf. Section 3.2). Due to the two desirable properties of prototype-based data representa-tion (cf. Section 3.2), the newly generated prototypes pre-serve the original data structure well and can be summarized further on a higher level of abstraction. Moreover, to main-tain the time-changing concepts on the second level, once the number of concepts is greater than maxC , the oldest concept is removed.

The second scenario occurs when a new concept is emerg-ing, which can be detected by PCA or statistical analysis (cf. Section 3.3). In this case, we draw sample data from all prototypes representing a previous concept, and perform synchronization-inspired constrained clustering on it. The resulting set of new prototypes is inserted into one entry of the concept level of the P-Tree. Meanwhile, all proto-types representing the previous concept are removed from the first level of the P-Tree. Moreover, if there is no signifi-cant concept change over time (i.e., gradual concept drift or no concept drift), we also extract the concept based on the prototypes in the P-Tree by clustering after a specified time T ; yet the prototypes are not removed. This strategy helps to give a summarization of data over time on the concept level.
Building upon the proposed online data maintenance, fi-nally instance-based learning in the form of the nearest neigh-bor classifier is used for data stream classification. In con-trast to eager learning approaches, SyncStream only needs Alg orithm 1 SyncStream to mai ntain the online training data (represented by a small set of time-changing prototypes) in the P-Tree. Finally, the pseudocode of SyncStream is shown in Algorithm 1. In this section, we evaluate our proposed algorithm Sync-Stream on both synthetic and real-world data streams. Data sets. We evaluate the proposed method on synthetic data and four different types of real-world data: Spam, Elec-tricity, Covtype, and Sensor. Table 1 lists statistics of the four real-world data streams.

Synthetic Data was generated based on a moving hyper-plane [16]. The hyperplane in 2-dimensional space was used to simulate different time-changing concepts by altering its orientation and position in a smooth or sudden manner. In this study, two synthetic data consisting of one million ex-amples each were created to characterize the data stream with gradual and sudden concept drift, respectively (Fig. 5 and Fig. 6).

Spam Filtering Data is a collection of 9324 email mes-sages derived from the Spam Assassin collection 1 . Each email is represented by 500 attributes using the Boolean bag-of-words approach.

Electricity Data contains 45,312 instances, which was col-lected from the Australian New South Wales Electricity Mar-ket for every five minutes. In this market, prices are not fixed and affected by demand and supply on the market.

Covtype Data containing 581,012 instances describes seven forest cover types on a 30 30 meter grid with 54 different geographic measurements.

Sensor Data collects information (temperature, humid-ity, light, and sensor voltage) from 54 sensors deployed in the Intel Berkeley Research Lab over a two months period (one reading per 1-3 minutes). It totally contains 2,219,803 instances belonging to 54 classes.
 Evaluation Metrics. To quantitatively evaluate Sync-Stream, we consider the following performance metrics: Selection of comparison methods. To extensively eval-uate the proposed algorithm SyncStream, we compare its performance to several representatives of data stream clas-sification paradigms.

Adaptive Hoeffding Tree [9]: is an incremental, anytime decision tree induction algorithm capable of learning from massive evolving data streams, using ADWIN to monitor the performance of branches in the tree and to replace them by new branches if they turn out to be more accurate. for classification on data streams. A prediction for a new ex-ample is achieved by combining the outputs of the neighbors of this example in the training data.
 ing concept-drifting data streams using weighted ensemble classifiers.
 and Russell with the addition of the ADWIN algorithm as a change detector and an estimator for the weights of the boosting method. When a change is detected, the worst classifier of the ensemble of classifiers is removed and a new classifier is added to the ensemble.
 exploit the existence of recurring concepts in the learning process and improve the classification of data streams.
We have implemented SyncStream in Java. The source code of PASC was obtained from the authors, and the IBL-Streams algorithm is available at: http://www.uni-marburg. de/fb12/kebi/research/software/iblstreams . Adaptive Hoeffding Tree, Weighted Ensemble and OzaBagAdwin have ht tp://spamassassin.apache.org/ 20 (d) T 100 (e) Data Dynamics 20 ) (i) P-Tree ( T 100 ) (j) P-Tree (Concepts) 51 (d) T 76 (e) Data Dynamics 51 ) (i) P-Tree ( T 76 ) (j) P-Tree (Concepts) been implemented in the MOA framework available at http: //moa.cms.waikato.ac.nz/ . All experiments have been per-formed on a workstation with 3.0 GHz CPU and 32 GB RAM.
We start the evaluation with two-dimensional synthetic data streams to facilitate presentation and demonstrate some properties of SyncStream.
 Concept Modeling: We first evaluate whether or not SyncStream can capture the data concept with the proposed P-Tree. Fig. 5 (a) -(d) and Fig. 6 (a) -(d) show two syn-thetic data streams consisting of one million examples with gradual and sudden concept drift, respectively. The evolv-ing mean values of the data for each class are plotted in Fig. 5 (e) and Fig. 6 (e), where each time unit (T) is composed of one thousand examples. It is interesting to note that the derived prototypes in the P-Tree at any time interval en-able to capture the changing data concept regardless of the type of data stream (Fig. 5 (f) -(i) and Fig. 6 (f) -(i)). Instead of learning a recent window of historical data or learning a set of base classifiers as a black box, SyncStream well captures the trend of time-changing data concepts by maintaining a set of short-term or long-term prototypes dy-namically. Fig. 6 (j) displays the four changing concepts, which are preserved in the concept level of the P-Tree. For the data stream with gradual concept drift, the concepts are also learned after a specified time window (e.g., 25,000 examples in this experiment) and inserted into the concept level of the P-Tree (see Fig. 5 (j)).

Concept Drift Detection: To investigate the perfor-mance of sudden concept drift detection, Fig. 7 shows the results based on the PCA and statistical analysis, respec-tively. For the data stream with gradual concept drift, we can observe that the PCA-resulting angles over time are relatively stable and lower than 10  X  , and similarly, the de-rived p -values are high and no significant concept drift is detected (see Fig. 7 (a) and (c)). In contrast, for the sud-den concept-drifting data stream, both the PCA and the statistical approach are capable of identifying the abruptly changing concept effectively (Fig. 7 (b) and (d)).
Prototype-based Data Representation: Based on the synchronization-inspired constrained clustering, important examples are further summarized as a set of prototypes. From Fig. 5 and Fig. 6, it is interesting to observe that the derived prototypes stored in the P-Tree preserve the data structure as well as the class distribution. Moreover, concept drift, respectively. inappropriate examples can be handled even in the presence of noise by error-driven representativeness learning (Fig. 8). The proposed prototype-based data representation provides an effective and efficient strategy to handle a large amount of online training data. Figure 8: Effective prototype-based data represen-tation in a data stream with noise. Fig ure 9: Performance of different data stream clas-sification algorithms on spam data. Fig ure 10: Performance of different data stream classification algorithms on electricity data. Figu re 11: Performance of different data stream classification algorithms on covtype data. Figu re 12: Performance of different data stream classification algorithms on sensor data.
In this section, we compare the performance of different stream classification algorithms on the mentioned four real data streams. For SyncStream, unless specified otherwise, the parameters were set to maxP = 1000, maxC = 10 and chunkSize = 1000. For all other algorithms, the default parameters suggested by the authors were used.

Fig. 9 -Fig. 12 plot the prediction performance against the number of processed instances. Generally, with an in-creasing number of processed examples, the prediction error of SyncStream is quite small and relatively stable. This is the result of modeling concepts by sets of dynamically ad-justed prototypes. Instead of focusing on recent data or a set of data chunks, SyncStream can select the prototypes to capture the data concept on the instance level. Although IBLStream is similar to SyncStream in capturing local data characteristics by instance-based learning, it cannot adapt well to evolving concepts, and thus prediction performance fluctuates over time. HoeffdingAdaTree is a typical single model learner: due to the insufficient handling of time-changing concepts, it achieves the worst prediction perfor-mance in most cases. Although the ensemble learning ap-Elec tricity 45, 312 8 2 Covt ype 581 ,012 54 7 pro aches, Weighted Ensemble, OzaBagAdwin and PASC, al-low adapting to changing concepts quickly according to pre-vious studies, the classification accuracies are not promising, which may caused by the two limitations stated in Section 1. Table 1 further summarizes the performance of the algo-rithms in terms of different evaluation measures. Regarding the computation time, HoeffdingAdaTree is the fastest al-gorithm due to the Hoeffding bound, yet the classification accuracy suffers. SyncStream, Weighted Ensemble, OzaBa-gAdwin and PASC are comparable, while IBLStream is the most time-consuming algorithm. From the experiments, we can see that SyncStream not only allows making accurate classifications, but also working efficiently in terms of com-putation time (Table 1). In this section, we perform a sensitivity analysis of Sync-Stream on the cover type data.

Maximum Boundary: As stated in Section 3.4.2, the maximum number of prototypes maxP needs to be speci-fied to indicate how many prototypes can be dynamically maintained in the P-Tree according to the available com-putational resources. The higher the value of maxP , the
Fig ure 13: The sensitivity analysis with maxP . more prototypes can be managed to model the concepts. In this experiment, we varied maxP from 500 to 3000. Fig. 13 (a) plots the classification accuracies dependent on the num-bers of prototypes, the corresponding computation time is shown in Fig. 13 (b). We observe that the classification per-formance is quite stable, while the computation time is grad-ually growing due to the increased effort needed for nearest neighbor search.

Chunk Size: The chunk size determines how many ex-amples have been used to examine whether the concept has changed across two consecutive data chunks. Fig. 14 (a) shows the classification performance with respect to differ-ent chunk sizes ranging from 500 to 3000. Similarly, the cor-respon ding number of time-changing concepts is reported in Fig. 14 (b). With increasing chunk size, the number of de-tected concepts is gradually decreasing, as more data make the transition from one concept to another appear smoother. However, thanks to error-driven representativeness learning, the set of dynamical prototypes in the P-Tree has already learned the evolving concepts implicitly. By combining rep-resentativeness learning and concept drift detection, Sync-Stream allows adapting to both gradual and sudden concept drift quickly, which results in high classification accuracies in the experiments (Fig. 14 (a)).
In this paper, we introduce a new prototype-based classi-fication algorithm, SyncStream, to learn from evolving data streams. While existing approaches focus on learning a single model on a window of recent data or a set of base classifiers on recent data chunks, time-changing concepts may not be learned well in this way due to two factors: the difficulty of selecting a suitable horizon of the train-ing data and the loss of relevant historical data. In the light of these problems, this paper proposes error-driven representativeness learning to determine the importance of training examples. Only representative examples, allowing to model the current concept, are kept and further sum-marized as a smaller set of prototypes by synchronization-inspired constrained clustering. With this strategy, Sync-Stream allows dynamically learning a set of prototypes to capture evolving concepts implicitly on the level of instances. Moreover, in order to adapt to abrupt concept drift quickly, two heuristic concept drift detection approaches are intro-duced. Equipped with both implicit and explicit concept drift handling, SyncStream allows modeling time-changing concepts (either gradual or sudden) effectively. One other attractive property of SyncStream is its any-memory prop-erty. Owing to the prototype-based data representation, a multi-scale representation of the data (a set of prototypes) is possible. Although SyncStream is an instance-based learn-ing scheme, it largely differs from traditional instance-based learning such as IBLStreams. One main difference is that IBLStreams keeps track of recent instances, while Sync-Stream dynamically learns a set of prototypes and supports efficient data maintenance. In comprehensive experiments, we have shown that SyncStream outperforms several other state-of-the-art data stream classification methods. [1] A. Bifet and R. Gavalda. Learning from time-changing [2] A. Bifet, G. Holmes, B. Pfahringer, R. Kirkby, and [3] C. B  X  ohm, C. Plant, J. Shao, and Q. Yang. Clustering [4] E. Brunner and U. Munzel. The nonparametric [5] A. Dries and U. R  X  uckert. Adaptive concept drift [6] J. Gama, A. Bifet, M. Pechenizkiy, and [7] J. Gama, P. Medas, G. Castillo, and P. Rodrigues. [8] M. J. Hosseini, Z. Ahmadi, and H. Beigy. Using a [9] G. Hulten, L. Spencer, and P. Domingos. Mining [10] L. I. Kuncheva and J. S. S  X anchez. Nearest neighbour [11] M. Neuh  X  auser and G. D. Ruxton. Distribution-free [12] S. Papadimitriou, A. Brockwell, and C. Faloutsos. [13] A. Shaker and E. H  X  ullermeier. IBLStreams: a system [14] J. Shao, X. He, C. B  X  ohm, Q. Yang, and C. Plant. [15] K. Wagstaff and C. Cardie. Clustering with [16] H. Wang, W. Fan, P. S. Yu, and J. Han. Mining [17] P. Zhang, B. J. Gao, X. Zhu, and L. Guo. Enabling
