 Log-linear models have become popular in text mining because of its capability to incorporate a vast number of possibly correlated features, which is common in text mining tasks. Given an instance x and its corresponding output y  X  X  ,we present a feature vector F ( x , y ) to a learning system for training. In log-linear models, we aim to learn the conditional distribution: where  X  =(  X  1 , X  2 ,..., X  m ) is the training model parameter, and Z ( x ;  X  )=
To help our discussion, we assume x is a sentence and y is a sequence of tags for each word in x . To keep the size of the feature vector F ( x , y )tractable, usually we assume first order independence on the output y and write F ( x , y ) as the sum of features at all positions of x .
 where | x | is the length of sentence x ,and f =( f 1 ,f 2 ,...,f m ). Each f k is a feature function defined at position i of sentence x . More precisely, it is a binary function of the whole sentence x , the previous label y i  X  1 ,thecurrent(i.e.the i -th) label, and the position i .

To see the importance of conjunction of features in text mining, let us consider the following example. Suppose we have instances p =  X  X organ X , q = X  X tanley X , and r =  X  X organ Stanley X . Usually we tend to think that p and q are two people. If we blindly extend the inference without considering the two words together as  X  X organ Stanley X , we will conclude that r is also a person (but it is an investment bank). This shows that we need to consider  X  X organ Stanley X  as one object in our learning algorithm. How ever, features are us ually constructed manually and there is little research done on the construction of conjunctions of features for text mining tasks. For example, in a noun-phrase chunking task, we observe a sentence x where each word x i has a label y i  X  X  B, I, O } to indicate that a word is the  X  X eginning X ,  X  X nside X , or  X  X ut X  of a noun-phrase respectively. Thus a feature ( x i =boy ,y i = B) may be defined as Manually writing all these features is not feasible. A possible solution to this problem is to define a set of feature templates and let the computer generate the features automatically. For exampl e, consider two feature templates: A 1:( x i ,y i ), and A 2:( x i  X  1 ,x i ,y i ) When these two feature templates work on the following labeled sentence, the features generated by each template are: where START denotes the start of a sentence. In this way, the problem of exactly defining a set of features is reduced to that of defining a set of feature templates. In many cases, users may want to define a threshold  X  to remove a feature f that occurs rarely, i.e. Count( f ) &lt; X  . An alternative to using a threshold is using a statistical test on the features as in [1]. This feature template technique is used in programs such as [2], [3], and [4]. However, researchers often resort to intuition or a trial-and-error method to determine whether a feature should be used.
As long as computation power is not a concern, it is expected that incor-porating more features in a mining sys tem increases the performance. In [5], a noun-phrase chunking system is developed by using the above feature tem-plate technique. On a complete feature set, the system achieve 94.38% F1. On a reduced feature set where the features are supported in the training set, the system performance is 94.19% F1. However, it is not true in all cases. In [1], the model with a complete feature set (where a semantic lexicon is added) gives a lower accuracy. Using orth ographic features only, th e biomedical named entity recognition performance is 69.8% F1 on the evaluation set (and 88.9% F1 on the training set). The performance decreases to 69.5% F1 on the evaluation set (and 86.7% F1 on the training set) when both orthographic features and semantic lexicons are used. A more interesting study is presented in [6]. The performance of a named entity recognition system wit h hand-engineered features is 73% F1. However, without the tedious and elusive manual search in the feature space, a simple model with only word features can achieve 80% F1. The above results suggest that adding more features does no t necessarily increa se the performance.
To investigate the above problem, w e need an efficient method to generate and store the enormous number of featu res. We also need to efficiently get all the features activated in our feature sp ace, given an instance. In this paper, we propose a new algorithmic framework called F-tree for automatically generating and storing conjunctions of features in text mining tasks. Instead of a complete set of feature templates, our approach requires only a set of atomic feature templates. These atomic feature templates generate the atomic features on which high-level molecular features are based. An instance may activate many high-level molecular features and more features can be considered than that of a hand-engineered approach. It is infeasible to find all the molecular features activated by an instance by using a one-vs-one test, i.e. test if an instance can activate a feature and then the next feature. To efficie ntly retrieve these activated features, we develop a prefix tree algorithm to store all features and do a one-to-all test. We further improve our framework by proposing a systematic method for removing redundant features based on a hierarchical data structure, which is helpful in representing the relationships between th e low-level features and the high-level features. In practice, the hierarchy can b eintegratedwiththeprefixtreetosave storage space. In many text mining tasks, a set of training data is given and the problem can be cast as a supervised learning problem . We follow [7] to describe the general setting of a learning problem. Assume that x is an observation, y is the answer available, and P ( f ( x , y )) is the unknown probability distribution of f ( x , y ). Definition. The goal of the general learning problem is to minimize the following risk function R ( w ).
 Our goal is to find the parameter w that can minimize the risk R ( w ), i.e. to minimize the number of prediction mistakes on f ( x , y ) from the distribution { f ( x i , y i ) } is given. Usually w parametrizes and enables a model to predict the answer for x , thus the loss function Q (  X  ) can be calculated by comparing the pre-dicted answer with the true answer y .Thatiswhy Q (  X  ) is also parametrized by w . However, in many real-world applicatio ns, there are uncertainties in choosing f ( x , y ). In general, the feature selection p roblem affects the learning perfor-mance in two ways: linear combination of features and model selection. Linear combinations of features are widely used in many learning algorithms, e.g. sup-port vector machines (SVMs), maximum entropy models, and their variants. In these algorithms, part of the models involves a linear combination of features w f 1 + w 2 f 2 +  X  X  X  + w n f n where f i represents the presence of feature i .Model selection is closely related to the over-fitting problem [7,8]. In general, a family of models M can be specified by its struct ure with a scale parameter k .Our goal is to select the model with the best k  X  so as to minimize the generaliza-tion error, given that we can observe only the fitting error. There are many approaches to choosing k  X  , including regularization, cross-validation, minimum description length (MDL) principle, Akaike X  X  information criterion, structural risk minimization (SRM), etc..

The above issues become more complicat ed in text mining problems because of the rich feature sets available for text. These features may or may not be fully derived from the given text documents, e.g. part-of-speech (POS) information, prefix, suffix, lemma form, base type, and domain lexicons. 2.1 Discriminative Training and Log-Linear Models Log-linear models are widely used in maximum entropy modelling. The param-pectation par ametrized by w and [  X  ] is the empirical expect ation. The log-linear models can be shown to have maximum entropy over the training instances.
In discriminative training, a conditional distribution P w ( y | x ) is modelled di-rectly. This gives more freedom to a m odel to fit the training instances { ( x , y ) } by adjusting its parameter w . By comparing the following models where (5) is used in generative training and (6) is used in discriminative training, it is easy to see that there are fewer constraints for discriminative training as the parameter w does not need to account for the marginal distribution P d ( x ; w ), which can be parametrized by another parameter w .

Another advantage of modelling the conditional distribution directly is the possibility of adding a large number of possibly correlated and diversified features without degrading the performance as that of generative training. In this paper, we use conditional random fields (CRFs) to evaluate our automatic method because it has been shown to perform better on text mining tasks than hidden Markov models (HMMs) which is its generative counterpart. 3.1 Definitions Definition. Afeature a is an atomic feature if there exists no other features f ,f 2 ,...,f n such that a = f 1  X  f 2  X  X  X  X  X  f n , i.e. it is not a conjunction of other features. A feature f is a molecular feature if it can be expressed as a conjunction of atomic features, i.e. there exists atomic features a 1 ,a 2 ,...,a n such that f = a  X  a resentation for x is a set of molecular features, denoted by x { f 1 ,f 2 ,...,f j } . Definition. Afeature f = { b 1 ,b 2 ,...,b m } is said to be activated by an instance x = { a 1 ,a 2 ,...,a n } if f  X  x . 3.2 Finding All Activated Features In text mining, each x is usually a word with its associated information such as POS, lemma form, prefix, etc.. We can cast the feature generation problem as a frequent itemset mining problem. Then an instance is matched against the generated frequent itemset (the final feature space).

To avoid manual construction of conjunction of features, a frequent itemset mining algorithm can be applied on the atomic features of each instance by treating each atomic feature a j as an item and each instance x i as a transaction. The frequent itemsets Z will be served as our feature space.
 Example 1 (Finding all activated features). Suppose we have an instance x that possesses the following atomic features: x = { e, c, a, b } ,andfromthe whole training set we have the following set of features Z :
The activated features for x are f 1 ,f 2 ,f 3 and f 6 . In general, suppose each feature f  X  X  has a length of | f | , and each instance x has a length of | x | .When to get all activated features for all instances. Naively matching each instance x to
Z is infeasible. To efficiently retrieve all activated features from Z , we develop a prefix tree data st ructure, called feature tree ( F-tree ), to store Z . An auxiliary array of linked list is used to store all nodes of the same atomic feature a j to allow fast access. Algorithm 1 describes the details. We sort the atomic features a j in each instance x in Example 1 according to lexicographic order. All the features are then further sorted according to their feature size | f | . Algorithm 1.
 In Algorithm 1, we only need to mark nodes that have their parents marked. Ifanodeisnotmarkedbutitsparentismar ked, then all the molecular features represented by this node (if its fid is non-null) and downward will not be acti-vated. This can be easily proved by contradiction. Suppose a molecular feature represented by a downward node is activated. This implies that we will need to mark the current node later. However, because the atomic features to be marked is sorted, this node will never be marked.

Figure 1 shows the complete F-Tree T for Z in Example 1. We can use T to check which molecular features can be activated by x . Algorithm 1 accomplishes this by making use of an array of linked list L for fast access to the list of atomic feature nodes. When marking nodes, we need to check if the current node is marked and has a non-null feature ID ( fid ). If we find such a node, the corresponding fid will be stored and returned. We first mark the nodes representing the first atomic feature a of x as true .Wedothesameforeach of the sorted atomic features in x . After marking the third atomic feature c , the resulting F-tree is shown in Figure 1. At this point, we have retrieved f 1 and f 2 , i.e.  X  = { 1 , 2 } . The complete marked tree is shown in Figure 3.2. The middlenodeof e is not marked because its parent is not marked. Finally,  X  = { 1 , 2 , 3 , 6 } . Our experimental results (described in Section 4.1) show that the memory requirement can be reduced by half and the performance is comparable to that of the state-of-the-art systems that use manual construction of features. 3.3 Removing Redundant Features In order to further reduce memory usage, we propose the following principle to remove redundant features.
 Abstraction Principle. If a feature f is activated by x but all of its immediate supersets in the set of features are also activated by x , i.e. G : { g  X  X  ,f  X  g and | g | = | f | +1 } and every g  X  G is activated by x ,then f is abstracted away from the representation for x .
 To illustrate this principle, we consider the following example that shows an Apriori lattice.
 Example 2 (Abstract away redundant features). Suppose from the whole training set we have the following list of features: These features can be represented structurally with an Apriori lattice as in Figure 3. Suppose further that we have x = { a, b, c } . The activated features are shown in Figure 4. At this point, we need to choose among the activated features to represent x . Suppose we choose abc as one of the features to represent x .For the remaining features, we can see clearly their relationships from the lattice . For feature bc , from the training set we know that it can be used to generate the feature bcd . However, bcd is not activated because cd and bd is not activated. Therefore, bc may contain some information that may not be covered by abc so we also choose it to represent x . The same argument also applies for feature b and c .

For feature ab , it can only be used to generate higher-level feature abc but abc is already chosen to represent x . From this lattice struct ure (which is generated by the training set), we can expect that the information covered by ab should also be covered by abc . Therefore we do not choose ab to represent x . ac and a are not chosen because of the same argument. Finally, instance x is represented by features abc, bc, b and c , i.e. x { abc,bc,b,c } .

To make a contrast, we also consider u = { a } whose representation is also a , i.e. u { a } . u and x share common atomic features a ,andthat u has a as its representation, but the representation for x does not necessarily contain a . To further illustrate our idea, let us consider a different set of features where feature d and its associated features are removed, as shown in Figure 5. In this case, x { abc } . This also shows the interactive characteristics between an instance and the whole feature space in our approach. To implement the above feature abstraction, we need an Ap riori lattice to show the relationships between features. We add a procedure F-tree-Apriori , which is presented in Algorithm 2, to F-tree-Construct (in Algorithm 1) to create the Apriori lattice.

Figure 6 shows the complete F-tree with the Apriori lattice incorporated. The solid lines point to the original parents in F-tree while the dotted curved lines point to the parents in the Apriori lattice. Starting from Figure 4, we call the procedures F-tree-Apriori-counts and F-tree-Apriori-remove defined in Algorithm 2 when we retrieve the features from F-tree by procedure F-tree-retrieve (Algorithm 1). These two procedures check the number of activated children for each activated node. If all children are activated, then according to our abstraction principle it is not included in the representation. the instance being considered. Figure 7 shows the activated features and the features chosen to represent x , i.e. x { b, c, bc, abc } . For example, in Figure 4, the node a has two children in the Apriori lattice. They are indicated by the two dotted curved arrows pointing to ( a : 1) in Figure 7. Each of these arrows comes from an activated node. Therefore feature a is not chosen to represent x .Asanother example, the node ( c : 3) has one of its children ( d : 8) not activated. Thus feature c is chosen to represent x . Algorithm 2.
 Since the conditional random field (CRF) model [9] has shown promising perfor-mance in many text mining tasks, we use it to conduct large scale experiments on three publicly-available datasets and compare with other published text mining results on them. In all experiments, we u sed our automatic method to generate and store features. Then we find the representation for each instance. 4.1 Memory Usage We compare the memory usage of our F-tree with that of a dictionary which is used in storing the feature space Z and for matching each x .Theexperiment was done on a public dataset CoNLL-2000. The details of this dataset will be described in Section 4.2. We tested the memory requirement using different sizes (see Figure 8) and found that F-tree required only approximately half of the memory as that of a dictionary on this dataset. 4.2 Accuracy Noun-phrase Chunking. We use the WSJ corpus from the CoNLL-2000 shared task to conduct noun-phrase (NP) chunking experiments. Our atomic features and performance, along with o ther published results, are shown in Tables 1 and 2. Sha and Pereira [5] used a manual template with a second-order CRF. Their results are on par with the best results (Kudo and Matsumoto [10] : 94.39% F1). McCallum [6] used a feature induction method on a first-order CRF to induce features to be used in the CRF. Their feature induction method is specific to CRFs. In both of the work, they did not report on the precision ( P ) and recall ( R ), so we can list only the F 1-measures in the table. Our model is a first-order CRF (same as McCallum). Our precision is 94.65% and recall is 94.37%. Our automatic method gets a better F 1 than the above methods. It also outperforms Kudo and Matsumoto [10] (94.39% F1) that has the best result in the CoNLL-2000 shared task. We believe that although Sha and Pereira [5] used a second-order CRF, there may not be enough training data to train it. The feature induction method [6] uses a pseudo-likelihood to approximate the effect of adding a new feature. Therefore the new feature is not necessarily the best feature to be induced.
 General Named Entity Recognition. In this experiment, we used the dataset in the CoNLL-2003 shared task, which was used in the feature induction experiments in [6]. McCallum conducted an experiment with manually-prepared templates with lexicons, and another one with single word features only. Surpris-ingly, the manually-prepared templates with lexicons got only 73.3% F1 while in the experiment using simple single word features, the performance was 80% F1. Our automatic method with all features got 82.04% F1. After abstraction, the performance increased slightly to 82.31% F1. The results are listed in Table 3. Both are better than those of the experiment using manually-prepared templates with lexicons and the experiment using the single word features.
 Biomedical Named Entity Recognition. In this experiment, we use the GENIA corpus from the COLING workshop (BioNLP/NLPBA 2004). We tried to reproduce the features described in [1] and used them only as atomic features. The POS features are added by the GENIA tagger. Two lexicons for cell lines and genes are drawn from two online public databases: Cell Line Database and BBID. They are listed in Tables 4 and 5.

There are many different types of features involved in this experiments. Thus we used fewer number of first-order features to reduce the computation cost. The final results are listed in Table 6. Settles used two conditional random fields with different sets of features and applied a  X  2 test to select the words for their keyword lexicons. One CRF only used a set of orthographic features and the other used orthographic features and lexicons. The performance of the orthographic features is better than that of complete features. We used our automatic method on the orthographic and lexicon features, resulting in a higher F1 of 71.39 and 71.20. The performance also beats a CRF model used in the NLPBA-2004 shared task.
 We propose an algorithmic framework F-tree for automatically generating and storing conjunction of features for text mining tasks. It only requires the user to define a set of atomic feature templates. An instan ce X  X  activated features in a feature space can be found by a one-vs-all matching process. Our experimental results show that our automatic method reduces the memory usage by half and achieves state-of-the-art performance which is achieved by manual construction of features. To further improve the performance, we propose an abstraction prin-ciple to remove redundant features, which leads to slight improvements on the top performance. We believe that our abstraction principle can be more aggres-sive and further improvements can be made by employing more refined criteria.
