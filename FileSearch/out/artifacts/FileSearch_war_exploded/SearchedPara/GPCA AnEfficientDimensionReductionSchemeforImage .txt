 Recen t years have witnessed a dramatic increase in the quan-tity of image data collected, due to adv ances in elds suc h as medical imaging, reconnaissance, surv eillance, astronom y, multimedia etc. With this increase has come the need to be able to store, transmit, and query large volumes of image data ecien tly. A common operation on image databases is the retriev al of all images that are similar to a query image. For this, the images in the database are often represen ted as vectors in a high-dimensional space and a query is an-swered by retrieving all image vectors that are pro ximal to the query image in this space, under a suitable similarit y metric. To overcome problems asso ciated with high dimen-sionalit y, suc h as high storage and retriev al times, a dimen-sion reduction step is usually applied to the vectors to con-cen trate relev ant information in a small num ber of dimen-sions. Principal Comp onen t Analysis (PCA) is a well-kno wn dimension reduction scheme. However, since it works with vectorized represen tations of images, PCA does not tak e into accoun t the spatial localit y of pixels in images. In this pa-per, a new dimension reduction scheme, called Generalized Principal Comp onen t Analysis (GPCA), is presen ted. This scheme works directly with images in their nativ e state, as two-dimensional matrices, by pro jecting the images to a vec-tor space that is the tensor pro duct of two lower-dimensional vector spaces. Exp erimen ts on databases of face images sho w that, for the same amoun t of storage, GPCA is su-perior to PCA in terms of qualit y of the compressed images, query precision, and computational cost.
 Categories and Sub ject Descriptors: H.3.3 [Informa-tion Storage and Retriev al]: Information Searc h and Re-triev al General Terms: Algorithms Keyw ords: Dimension reduction, image compression, Prin-cipal Comp onen t Analysis, Singular Value Decomp osition, tensor pro duct, vector space
Recen t years have witnessed an explosion in the the quan-tity and qualit y of image data (both still and video) gen-erated by div erse applications, suc h as medical imaging, surv eillance, reconnaissance, astronom y, multimedia, etc. A-long with this increase has come the need to be able to store, transmit, and query large volumes of image data ecien tly. While images are inheren tly 2D in nature, i.e., matrices of pixels, the mec hanisms emplo yed to operate on them often entail working with data in very high dimensions. The fo-cus of this pap er is on the design of a dimension reduction scheme for ecien t image compression and retriev al, under limited storage.

A common type of query on image data is the retriev al of all images that are similar to a query image. Suc h a query is useful in con ten t-based image retriev al and has receiv ed considerable atten tion in the database comm unit y [4, 5, 8]. To supp ort suc h queries, the 2D images in the database are con verted to a vector-based represen tation and a similarit y query is answ ered by retrieving all vectors that are pro xi-mal to the query vector, under a suitably de ned similar-ity metric. (The hop e here is that pro ximit y in the vector space is correlated to similarit y in the image space.) Unfor-tunately , this image-to-v ector transformation often results in very high-dimensional data. It is not uncommon for a 2D image of size 100 100 pixels to generate vectors whose dimensions run into sev eral thousands. High dimensional-ity has a negativ e impact on virtually all asp ects of image managemen t, including image compression, storage, trans-mission, and retriev al.

A natural approac h for achieving better performance is to reduce the dimensionalit y of the data. The idea is to apply a prepro cessing step to the data so that most information is concen trated in a small num ber of dimensions. Besides reducing storage requiremen ts and impro ving query perfor-mance, dimension reduction has the added bene t of often remo ving noise from the data, as suc h noise is usually con-cen trated in the excluded dimensions [2].

Principal Comp onen t Analysis (PCA) is a well-kno wn sch-eme for dimension reduction [12]. This approac h condenses most of the information in a dataset into a small num ber, p , of dimensions by pro jecting the data (subtracted by the mean) onto a p -dimensional axis system f i g p i =1 ( p is usu-ally pre-sp eci ed or determined by heuristics). The optimal axis system can be computed by the Singular Value Decom-position (SVD) [10]. The reduced dimensions are chosen in a way that captures essen tial features of the data with very little loss of information. PCA is popular because of its use of multidimensional represen tations for the compressed for-mat. Suc h represen tations allo w database applications suc h as indexing to operate directly on the reduced represen ta-tions without a rst phase of reconstruction [3]. The work in [8, 13] adopts this approac h for similarit y-based image retriev al. However, traditional PCA is applicable only for data in vectorized represen tation. When raw data (say an image) is represen ted in matrix form, it has to be con verted to a vector. A typical way to do this so-called \matrix-to-vector alignmen t" is to concatenate all the rows in the ma-trix together to get a single vector. There are two dra wbac ks to this matrix-to-v ector alignmen t: First, spatial localit y in-formation may be lost, and, second, the alignmen t leads to higher time and space costs.

Example 1.1. Figur e 1 shows an example of matrix-to-vector alignment. The 3 3 matrix on the left is aligne d to get the single vector on the right. The numb ers 1{9 de-note corresponding positions in the matrix and vectorize d representations. Positions 1 and 4 are neighb ors in the ma-trix representation, while they are quite far away from each other in the vectorize d representation. The same observa-tions hold for positions 3 and 6, etc. Also, the size of the so-c alled covarianc e matrix in PCA is 9 9 in this case, as comp ared to the original 3 3 matrix. 1 2 3 4 5 6 7 8 9 Figure 1: Matrix-to-v ector alignmen t. 1{9 denote the positions in the matrix and vector formats.

Note that the SVD computation requires the whole data matrix to reside in main memory , whic h limits the appli-cabilit y of PCA to relativ ely small image databases. Work has been done to address the high space complexit y of PCA from the persp ectiv e of incremen tal SVD [14], where new data points are added to the database sequen tially . But it has been found that most incremen tal algorithms tend to degrade in performance noticeably and frequen t update is required [14]. Some recen t work [1, 7, 9] applies random sampling to speed up the SVD computation. However, the e ectiv eness of these approac hes is dep enden t on the spec-tral structure of the data matrix. There is little work in addressing the issue of spatial information loss due to the matrix-to-v ector alignmen t in traditional PCA. The above two dra wbac ks are the motiv ation behind this pap er.
In this pap er, we prop ose gener alize d PCA (GPCA) for di-mension reduction whic h aims to overcome the dra wbac ks in traditional PCA. As stated earlier, traditional PCA works on data in vectorized represen tation and it computes the p -dimensional axis system suc h that the pro jection of the data points (subtracted by the mean) onto the vector space spanned by this axis system has the largest variance among all p -dimensional axes systems. In con trast, the prop osed GPCA algorithm deals with data in its nativ e matrix repre-sen tation and considers the pro jection onto a space, whic h is the tensor pro duct of two vector spaces. More specif-ically , for given integers ` 1 and ` 2 , GPCA computes the ( ` ; ` 2 )-dimensional axis system u i v j , for i = 1 ; ; ` and j = 1 ; ; ` 2 , where denotes the tensor pro duct, suc h that the pro jection of the data points (subtracted by the mean) onto this axis system has the largest variance among all ( ` 1 ; ` 2 )-dimensional axes systems. We form ulate GPCA as an optimization problem in Section 4. Unlik e traditional PCA, there is no closed form solution to GPCA. We thus deriv e an iterativ e pro cedure for GPCA. Our empirical re-sults sho w that the algorithm usually con verges to a (local) maxim um within two iterations. We perform extensiv e ex-perimen ts on four well-kno wn face datasets to evaluate the e ectiv eness of GPCA and compare it with traditional PCA. The four image datasets consist of gra y scale images with sizes varying from 88 101 to 220 175. All images repre-sen t human faces in fron tal view, roughly aligned with the image boundaries.
 Our exp erimen ts sho w that: Furthermore, given the same amoun t of space to store the transformation matrices and the compressed images:
The rest of this pap er is organized as follo ws. Section 2 il-lustrates the e ect of dimension reduction on retriev al. Sec-tion 3 gives a brief introduction to traditional PCA. Sec-tion 4 introduces generalized PCA. Exp erimen tal results are presen ted in Section 5. Conclusions are o ered in Section 6. In this section, we presen t the bac kground on the K-Nearest-Neigh bor (K-NN) query , distance measures, the ef-fect of dimension reduction on a K-NN query due to loss of information, and query precision (a common measure for the e ectiv eness of dimension reduction schemes).
One of the most common queries on a con ten t-based re-triev al system is of the form: Suc h a request can be form ulated as a K-Nearest-Neigh bor query (K-NN query): Giv en a dataset S , a query object q 2 S , and an integer K 1, the K-NN query , selects a subset, R ( q; S ), of K elemen ts from S that have the smallest distance from q . That is, (i) R ( q; S ) S ; (ii) j R ( q; S ) j = K ; where d ( ; ) is a suitable distance measure (See Section 2.2).
Example 2.1. Consider a dataset, S , of six points A, B, metric, a 2-NN query with point C returns B and D. Figure 2: Illustrating a K-NN query on a 2D dataset.
An image is represen ted naturally as a matrix X 2 IR r c where r is the num ber of rows and c is the num ber of columns. By a matrix-to-v ector alignmen t, an image can be embedded in an N -dimensional space, IR N , for N = r c . One way to measure the distance between two images is to compute the Euclidean distance between the corresp onding vectors in IR N . The Euclidean distance between two vectors is the 2-norm of the di erence of the two vectors. The 2-norm of a vector is de ned as follo ws:
Definition 2.1. The 2 -norm of a vector x 2 IR N is de-ne d as jj x jj 2 = i -th coordinate of x .

The distance between two vectors in a reduced k -dimensional space, IR k , can be measured similarly .

The notion of distance between two vectors can be ex-tended to matrices, where the distanc e betwe en two matric es is the Frob enius norm of the di erence of the two matrices. The Frob enius norm of a matrix is de ned as follo ws:
Definition 2.2. The Frobenius norm of a matrix M = ( m ij ) 2 IR r c is de ne d as
Reducing data dimensionalit y may result in sucien t loss of information to a ect the output of K-NN queries. Con-sider the example in Figure 2. If we retain only the X -coordinates of the points, then the 2-nearest neigh bors of point C are A and D. Lik ewise, if we retain only the Y -coordinates of the points, then the 2-nearest neigh bors of point C are B and E. However, the 2-nearest neigh bors of point C in the original space are B and D. In either case, there is a loss of distance information from 2D to 1D that a ects the output of the 2-NN query .

A common technique to minimize the loss of information is to apply a linear transformation on the data so that most of the information gets concen trated in a few dimensions. A well-kno wn solution for this is the Principal Comp onen t Analysis (PCA), discussed in more detail in Section 3. Fig-ure 3 sho ws the result of applying PCA on the six data points from Figure 2. First, a new set of coordinate axes ( X 1 ; Y 1) is computed. Then information about the input points is retained along one of the two axes, in this case X 1. If we retain only the X 1-co ordinates of the points, then the 2-nearest neigh bors of point C are B and D. Thus, the 2-NN information for C is preserv ed.
Figure 3: Change of axes from ( X; Y ) to ( X 1 ; Y 1) .
We can measure the e ectiv eness of a dimension reduction algorithm using the notion of query precision to compare the result of a query in the original N -dimensional space with the result in the reduced k -dimensional space. This approac h was previously used for evaluating SVD-based di-mension reduction algorithms in [14, 11].

Let S N denote the set of data points (images) in the orig-inal N -dimensional space and let S k denote the set of data points in the reduced k -dimensional space. Let R ( q; S N the subset of points in S N returned by a K-NN query with point q in S N and let R ( q; S k ) be the subset of points in S returned by a K-NN query with the reduced-dimensional version of q in S k . The query precision can be de ned as follo ws: Note that for a K-NN query , with query point q , j R ( q; S = j R ( q; S k ) j = K . For the example in Figure 2, the preci-sion of the 2-NN query with point C is 0.5 using either the X -dimension or Y -dimension.
 Key notations used in the rest of this pap er are listed in Table 1.
Principal Comp onen t Analysis (PCA) is one of the best-kno wn metho ds for dimension reduction [12]. PCA was rst applied to reconstruct human faces in [15], in the con text of image compression. The Eigenface technique was dev elop ed in [16].

Assume that the n images in the dataset are originally represen ted in matrix form as A i 2 IR r c , for i = 1 ; ; n , where r and c are the num ber of rows and columns in the image, resp ectiv ely. In vectorized represen tation, eac h im-age A i is represen ted by a single vector a i by concatenating all the rows in A i together in order (a so-called matrix-to-vector alignmen t). The dimension of the image vector a i thus N = r c . The adv antage of using a vectorized repre-sen tation is that the n images can be represen ted compactly by a single data matrix A 2 IR n N , where eac h row of A corresp onds to a single image vector a i 2 IR N . Later com-putations, including the covariance matrix de ned below, come directly from the data matrix A .

Definition 3.1. The covariance matrix of the dataset f a i g n i =1 2 IR N is = Z T Z , wher e the i -th row of the matrix Z 2 IR N is a i m and m = 1 n n i =1 a i is the mean of the dataset.

PCA determine the N eigen vectors of the N N covari-ance matrix , in whic h the ( i; j )-th entry consists of the covariance between the dimensions i and j . The covariance matrix can be diagonalized by computing the SVD of the matrix Z as Z = U D T . By diagonalizing the covari-ance matrix as = ZZ T = ( U D T ) T ( U D T ) = T , where = D T D , we can obtain the eigen values from and the orthonormal eigen vectors from . The eigen values in are ordered in non-increasing order along the diagonal. Let f g N i =1 be the set of eigen vectors corresp onding to the columns in .

The rst step for performing the dimension reduction is to transform the data onto the new axis system f 1 ; ; N g . Hence, for a given data point x = ( x 1 ; ; x N ), the coordi-nates in the new axis system are ( x 1 ; ; x N ). In the ac-tual dimension reduction step, down to dimension p , we re-tain the p eigen vectors corresp onding to the largest p eigen-values of . These p eigen vectors are f 1 ; ; p g . When the data point x is pro jected onto the subspace spanned by the above p eigen vectors, the corresp onding coordinates are ( x 1 ; ; x p ).

Definition 3.2. Let S = f a 1 ; ; a n g be a set of vec-tors in IR N . Then the variance of S is de ne d as var ( S ) = S , and jjjj 2 denotes the 2 -norm of a vector as de ne d in Section 2.

The intuition behind PCA is that it maximizes the vari-ance of the pro jections of the data points (subtracted by the mean) onto the p -dimensional axis system f 1 ; ; p g , as stated below:
Proposition 3.1. Let G = [ 1 ; ; p ] be the matrix consisting of the eigenve ctors corresponding to the p largest eigenvalues from . Then G solves the following maximiza-tion problem: wher e a i is the i -th row of data matrix A , m = 1 n n i =1 the mean, and I p is the p p identity matrix. That is, the projections of the data points (subtr acte d by the mean) onto the p -dimensional axis system f 1 ; ; p g have the largest varianc e among all p -dimensional axes systems.
The key di erence between PCA and the generalized PCA (GPCA) metho d that we prop ose in this pap er is in the represen tation of image data. While PCA uses a vectorized represen tation of the 2D image matrix, GPCA works with a represen tation that is closer to the 2D matrix represen tation (as illustrated schematically in Figure 4) and attempts to preserv e spatial localit y of the pixels.

We will see later in this section that the matrix repre-sen tation in GPCA leads to SVD computations on matrices with much smaller sizes. More speci cally , GPCA involves the SVD computation on matrices with sizes r r and c c , whic h are much smaller than the matrices in PCA (where the dimension is n ( r c )). This reduces dramatically the time and space complexities of GPCA as compared to PCA. Fur-thermore, exp erimen tal results presen ted in Section 5 sho w sup erior performance of GPCA over PCA in terms of im-age compression and query precision. This is partly due to the fact that images are two-dimensional signals and there are spatial localit y prop erties intrinsic to images that the represen tation used by GPCA seems to tak e adv antage of. Figure 4: Schematic view of the key di erence be-tween GPCA and PCA. GPCA works on the original matrix represen tation of images directly , while PCA applies matrix-to-v ector alignmen t rst and works on the vectorized represen tation of images, whic h may lead to loss of spatial localit y information.
In GPCA, we consider images as two dimensional signals and we consider the follo wing ( ` 1 ; ` 2 )-dimensional axis sys-tem: u i v j , for i = 1 ; ; ` 1 and j = 1 ; ; ` 2 , where denotes the tensor pro duct. (W e sho w how to compute the vectors u i 2 IR r 1 and v j 2 IR c 1 later.) For a given matrix X 2 IR r c , its pro jection onto the ( i; j )-th coordi-nate, i.e., u i v j is u i X v j . From Prop osition 3.1, the p -dimensional axis system f 1 ; ; p g in PCA is computed suc h that the pro jections of the data points (subtracted by the mean) onto this axis system have the maxim um vari-ance among all p -dimensional axes systems. Similarly , in GPCA, we compute an optimal ( ` 1 ; ` 2 )-dimensional axis sys-tem u i v j , for i = 1 ; ; ` 1 and j = 1 ; ; ` 2 , suc h that the pro jections of the data points (subtracted by the mean) onto this axis system have the maxim um variance among all ( ` ; ` 2 )-dimensional axes systems. Unlik e PCA, however, the pro jections of the data points onto the ( ` 1 ; ` 2 )-dimensional axis system in GPCA are matrices, instead of vectors. Sim-ilar to De nition 3.2, the mean and variance of a set of matrices can be de ned as follo ws:
Definition 4.1. Let S = f X 1 ; X n g be a set of matri-ces in IR r c . Then the variance of S is de ne d as var ( S ) = of S , and jjjj F denotes the Frobenius norm of a matrix.
Example 4.1. To illustr ate this, let's consider the projec-tion of X onto the (2 ; 2) -dimensional axis system u i v i = 1 ; 2 and j = 1 ; 2 , wher e X = onto u 1 v 1 is u 1 X v 1 = (1 ; 0 ; 0) Similarly, the projections of X onto u 1 v 2 , u 2 v 1 and u 2 v 2 are 2 , 4 , 3 , respectively. Henc e, the the projec-tion of X onto the above (2 ; 2) -dimensional axis system is 1 2 4 3 .

A simple way to compute the projection above is to form can then be compute d by L T XR 2 IR 2 2 . Henc e to compute the optimal axis system is equivalent to computing optimal matric es L and R with orthonormal columns.

Let A i 2 IR r c , for i = 1 ; ; n be the n images in the dataset and M = 1 n n i =1 A i be their mean. Let ~ A i = A M , for all i . Then the variance of the pro jections of f onto the ( ` 1 ; ` 2 )-dimensional axis system u i v j , for i = 1 ; ; ` 1 and j = 1 ; ; ` 2 , can be computed as where L = [ u 1 ; ; u ` 1 ] 2 IR r ` 1 and R = [ v 1 ; ; v IR Form ulation of GPCA GPCA aims to compute two ma-suc h that the variance var( L; R ) is maxim um.

Unfortunately , there is no closed form solution to the max-imization problem [17]. The main observ ation, whic h leads to an iterativ e algorithm for GPCA, is stated in the follo w-ing theorem:
Theorem 4.1. Let L , R be the matric es maximizing the
Proof. n i =1 jj L T ~ A i R jj 2 F can be written as where the trace of a matrix is the sum of the diagonal en-tries in the matrix. Hence, for xed R , the maxim um of is also maximized.

Let the eigen-decomp osition of M L be M L = U U T , where U has orthonormal columns, = diag( 1 ; ; r ), and 1 r 0. It can be sho wn that trace L T M L L sists of the ` 1 eigen vectors of the matrix M L corresp onding to the largest ` 1 eigen values. (W e omit the pro of due to space limitation.)
Similarly , by the comm utativ e prop erty of the trace of ma-trices, that is, trace ( AB ) = trace( BA ), for any two matrices A and B , n i =1 jj L T ~ A i R jj 2 F can also be written as Hence, for xed L , the maxim um of n i =1 jj L T ~ A i R jj tained, only if R 2 IR c ` 2 consists of the ` 2 eigen vectors of the matrix M R , corresp onding to the largest ` 2 eigen val-ues.

The maximization of the variance var( L; R ) bears some resem blance to the one in [6], where the co-clustering of the gene expression data is considered. However, the two ma-trices L and R in GPCA have orthonormal columns, while they satisfy a speci c discrete structure in [6].
Remark 4.1. In typic al images, the numb er of rows and the numb er of columns are comp arable. Ther efor e, when re-ducing the dimension using GPCA, we take ` 1 = ` 2 = d . This is only for simplicity in the exposition; our metho d can be extende d easily to the mor e gener al case.

Theorem 4.1 pro vides us an iterativ e pro cedure for com-puting L and R . More speci cally , for a xed L , we can compute R by computing the eigen vectors of the matrix M R With the computed R , we can then update L by computing the eigen vectors of the matrix M L . The pro cedure can be rep eated until the result con verges (we pro ve con vergence in Section 4.2). Theoretically , the solution from this iterativ e pro cedure is a local solution. The solution dep ends on the initial choice, L 0 , for L . Exp erimen ts sho w that choosing L 0 = ( I d ; 0) T , where I d is the iden tity matrix, pro duces ex-cellen t results. We use this initial L 0 in all the exp erimen ts. Giv en L and R , the pro jection of ~ A i onto the axis system by L and R can be computed by D i = L T ~ A i R .
Con versely , given L; R and f D i g n i =1 , we can reconstruct f ~
A 1 ; ; n , where M is the mean. The reconstruction er-ror for ~ A i can be computed as E i = jj ~ A i LD i R T jj jj ~
A i LL T ~ A i RR T jj F . We de ne the root mean squar e er-ror (RMSE), to measure the average reconstruction error as follo ws:
We will sho w in the next section that the iterativ e pro ce-dure for computing L and R con verges, as measured by the RMSE value. More speci cally , we will sho w that the iter-ativ e pro cedure monotonically decreases the RMSE value, hence it con verges, since the RMSE value is bounded from below by 0. A user-de ned threshold is used to chec k the con vergence (more details are given in the next section). The pseudo-co de for the GPCA algorithm is given in Algo-rithm 1 .
 Algorithm 1 GPCA( A 1 ; ; A n , d ) Input: A 1 ; ; A n , d
Output: L , R , D 1 ; ; D n begin 0. M = 1 n n j =1 A j // Compute the mean 1. for j from 1 to n do begin 2. ~ A j = A j M 3. end 4. L 0 ( I d ; 0) T 5. i 0 6. RMSE( i ) 1 7. Do 8. form the matrix M R = n j =1 ~ A j T L i L T i ~ A j 9. compute the d eigen vectors f R j g d j =1 of M R 10. i i + 1 12. form the matrix M L = n j =1 ~ A j R i R T i ~ A j T 13. compute the d eigen vectors f L j g d j =1 of M L 16 Until (RMSE( i 1) RMSE( i ) ) 17. L L i 18. R R i 19. for j from 1 to n do begin 20. D j L T ~ A j R 21. end 22. return( L; R; D 1 ; ; D n ) end
In this section, we pro ve the con vergence of the iterativ e pro cedure (Lines 7{16 in the GPCA algorithm). The result is stated in follo wing theorem:
Theorem 4.2. The Do-Until Loop in the Algorithm 1 (Lines 7{16) monotonic ally decreases the RMSE value as de ne d in (2), henc e the GPCA algorithm conver ges.
Proof. Theorem 4.1 implies that the update of the ma-trices R and L in Lines 11 and 14 of Algorithm 1 in-
It is easy to chec k by Linear Algebra that It follo ws that the RMSE value decreases monotonically . The Do-Un til Loop in Algorithm 1 thus con verges, since the RMSE value is bounded from below by 0.
 Theorem 4.2 sho ws that we can chec k the con vergence of Algorithm 1 using the RMSE value. More speci cally , let RMSE( i ) and RMSE( i 1) be the RMSE values at the i -th and ( i 1)-th iterations from Algorithm 1 . Then, the con vergence of the GPCA algorithm can be determined by chec king whether RMSE( i 1) RMSE ( i ) &lt; , for some small threshold &gt; 0. In all our exp erimen ts, we choose = 0 : 05.
To illustrate the GPCA algorithm, we generate three ma-trices with integer entries (for simplicit y). The dimensions of the matrices are 3 3. That is, the num ber of points n = 3, the num ber of rows r = 3, and the num ber of columns c = 3. We also set d = 2. The three matrices f A i g 3 i =1 are:
By subtracting their mean, we obtain f ~ A i g 3 i =1 as: ~ A
In Line 4 of the GPCA algorithm, we set L 0 = The matrix M R in Line 8 can be computed as
By computing the two eigen vectors of M R corresp onding to the largest two eigen values ( d = 2) as in Line 9, we obtain whic h corresp onds to Line 11 of the GPCA algorithm. With the computed R 1 , the matrix M L in Line 12 can be com-puted as M
By computing the two eigen vectors of M R corresp onding to the largest two eigen values as in Line 13, we obtain
With the computed R 1 and L 1 , we can compute the root mean square error (RMSE) in Line 15 as
RMSE(1) = 1
The above pro cedure can be rep eated and at the end of the second iteration (details omitted), we obtain R : and the new RMSE value can be computed by
RMSE(2) = 1 Since RMSE(1) RMSE(2) = 0 : 0026 &lt; = 0 : 05, the al-gorithm exits the Do-Un til Loop in Lines 7{16 after the second iteration. We obtain L L 2 and R R 2 in Lines 17 and 18 resp ectiv ely. The pro jections of f ~ A i g 3 (2 ; 2)-dimensional axis system, whic h is the tensor pro duct of the space spanned by the two columns of L and the other space spanned by the two columns of R , can be computed in Line 20 as
In summary , GPCA computes the optimal L; R 2 IR 3 2
We note that the most time-consuming steps of the GPCA algorithm are the formation of the matrices M R and M L (Lines 8 and 12 of Algorithm 1 ), the eigen-decomp ositions in Lines 9 and 13, and the computation of the reduced rep-resen tation D i in Line 20.

It tak es O ( n d r ( r + c )) for computing M R . The computation of the d eigen vectors on M R 2 IR c c tak es O ( c 3 ). Similarly , it tak es O ( n d c ( r + c )) for computing M L and O ( r 3 ) for computing the d eigen vectors on M L 2 IR r r [10]. The computation of D i = ( L T ( A i R )) using the given order is O ( r c d + r d 2 ) = O ( r d ( c + d )). Hence, the total complexit y of GPCA can be simpli ed as where I is the num ber of iterations involved in the Do-Un til Loop in Lines 7{16 of the GPCA algorithm and the time O ( c 3 + r 3 ) for computing the eigen vectors in Lines 9 and 13 of the GPCA algorithm is omitted, since it is usually negligible compared to the time for computing M R and M L
Since n r d ( c + d ) n ( r + c ) 2 d , the time complexit y of GPCA is bounded above by O (( I + 1) n ( r + c ) 2 d ). Assuming r c time complexit y of GPCA can be simpli ed as O ( InN d ). Exp erimen ts in Section 5 sho w that the GPCA algorithm con verges within two iterations, i.e., I 2.
The memory requiremen t of the GPCA algorithm can be analyzed through Lines 0, 8{9, 12{13 and 20 in Algo-rithm 1 .

Lines 9, 13 and 20 in the algorithm only involves a single matrix with dimension r c . The most space-consuming steps are Lines 0, 8 and 12, where all A i 's are involved. The key observ ation is that the three matrices M , M R and M these three steps can be computed incremen tally by reading A i sequen tially without losing any information. Hence the required memory for the GPCA algorithm can be as low as O ( r c ).

The fact that GPCA computes the optimal solution with-out requiring all data points in the memory is a ma jor ad-vantage of GPCA over tradition PCA, esp ecially for large databases. As men tioned in the Introduction, even though the memory requiremen t of PCA can be reduced using an incremen tal SVD algorithm or random sampling techniques, good performance is not guaran teed.
Both PCA and GPCA can be applied for image compres-sion. The qualit y of compression is dep enden t on the space available for storing the transformation matrices and the reduced represen tations. In general, large storage leads to high qualit y of the compressed image and small storage leads to low qualit y. The exp erimen tal comparison of PCA and GPCA in the next section is based on the assumption that they both use the same amoun t of storage. Hence it is im-portan t to understand how to choose the reduced dimension for PCA and GPCA for a speci c storage requiremen t.
The original dataset con tains n images f A i g n i =1 2 IR hence it requires nN scalars to store the original n images, where N = r c .

GPCA compresses eac h of the n images of size r c to size d d , while it needs to keep the two matrices L 2 IR and R 2 IR c d to reconstruct the original images. Hence it requires ( nd + r + c ) d scalars in the reduced space by GPCA.
Next, consider PCA with p principal comp onen ts. To store the p principal comp onen ts, eac h in IR N , it requires pN scalars. Eac h image is transformed to a reduced rep-resen tation in IR p , hence it requires np scalars to store the n reduced represen tations. In total, it requires p ( N + n ) scalars in the reduced space by PCA.

In our exp erimen ts, we choose d for GPCA and p for PCA suc h that their storage requiremen ts are (nearly) equal.
In this section, we exp erimen tally evaluate the perfor-mance of the GPCA algorithm and compare it with PCA. Note that all the comparisons between GPCA and PCA are based on the assumption that they have the same storage re-quiremen t for the transformation matrices and the reduced represen tations. All of our exp erimen ts are performed on a P4 1.80GHz Lin ux mac hine with 1GB memory . We divide this section into six parts. The four face image datasets used in the exp erimen ts are describ ed in Section 5.1. Section 5.2 discusses the e ect of the value of the reduced dimensional-ity, d , on GPCA. More speci cally , we look at the e ect of d on the query precision. The con vergence prop erty of GPCA is studied in Section 5.3. A detailed comparativ e study be-tween GPCA and PCA is given in Section 5.4, where the comparison is based on the visual qualit y of the compressed images. Section 5.5 compares the e ectiv eness of GPCA and PCA in terms of query precision. In Section 5.6, we compare the eciency of GPCA and PCA. Exp erimen ts sho w GPCA has distinctly smaller cost in time than traditional PCA.
For all the exp erimen ts, we use 10-fold cross validation to rep ort the mean precision of a K-NN query with K = 10. In 10-fold cross validation, the entire dataset is split into ten subsets. We use all the data points in eac h of the subsets to query the set consisting of the union of the other nine subsets. The nal precision rep orted is the mean precision of all queries.
The follo wing are the four face datasets in our study; all are publicly available.
 1 http://p eipa.essex.ac.uk/ipa/pix/faces/manc hester/test-hard/ 2 http://www.uk.researc h.att.com/facedatabase.h tml 3 http://rvl1.ecn.purdue.edu/ aleix/aleix face DB.h tml 4 http://www.ri.cm u.edu/pro jects/pro ject 418.h tml Figure 5: The e ect of the value of d on GPCA, as measured by query precision
The value of d determines the dimensionalit y in the trans-formed space by GPCA. We did extensiv e exp erimen ts using di eren t values of d on the four datasets. The results are summarized in Figure 5, where the x -axis denotes the value of d (between 2 and 20) and the y -axis denotes the query precision. As sho wn in Figure 5, the precision curv es on all datasets have big jumps from d = 2 to 4 and 6.

As is clear from Figure 5, a larger value of d leads to a higher query precision, but a higher storage requiremen t. There is a clear trade-o between query precision and stor-age required. As discussed in last section, ( nd + r + c ) d scalars are required in the reduced space by GPCA. In practice, we can determine the maxim um value of d , for a given amoun t of available space.
We study the con vergence of the GPCA algorithm in this exp erimen t. The results for the four datasets: PIX, ORL, AR and PIE are sho wn in Figure 6, where the x -axis denotes the num ber of iterations, and the y -axis denotes the Root Mean Square Error (RMSE). We set d = 20 for the four datasets. Figure 6 sho ws that the RMSE drops dramatically during the rst and second iterations and it stabilizes after two iterations.
Storage and transmission of large image data commonly apply image compression (co ding) as a pre-pro cessing step. Figure 7: First row: raw images. Second row: im-ages compressed by PCA. Third row: images com-pressed by GPCA.
 Both PCA and GPCA can be applied for compression. We compare GPCA and PCA in terms of image compression, when using the same amoun t of storage.

We apply PCA and GPCA on the 400 images in the ORL dataset. We use p = 15 principal comp onen ts in PCA and set d = 20 for GPCA corresp ondingly . The storage require-men ts for GPCA and PCA are, resp ectiv ely, 164080 and 160560 scalars, and the transformation matrices have size 4080 and 154560, resp ectiv ely. To illustrate the di eren t performance of GPCA and PCA, we sho w the result of 10 images from 10 di eren t persons in Figure 7. The 10 images in the rst row are the raw images from the dataset. The 10 images in the second and third rows are the ones compressed by PCA ( p = 15) and GPCA ( d = 20) resp ectiv ely. As can be seen, the images under GPCA are (visually) closer to the raw images.

As a second exp erimen t, we apply PCA and GPCA on the 1638 images from the AR dataset, with p = 62 and d = 20. The storage requiremen ts for GPCA and PCA are, resp ec-tively, 658980 and 652612 scalars, and the transformation matrices have size 3780 and 551056, resp ectiv ely. We sho w the result on a single person (eac h person has 13 images) in Figure 8. The 13 images in the rst row are the raw images from the dataset. The 13 images in the second row are the ones compressed by PCA ( d = 62), and the 13 images in the third row are the ones compressed by GPCA ( d = 20). Again, the qualit y of the GPCA images app ears higher.
Figure 7 and Figure 8 sho w that GPCA yields visually higher qualit y images than PCA, when using the same amoun t of storage. This con rms our initial intuition that by work-ing on the images in the matrix represen tation, GPCA is able to capture the spatial localit y information intrinsic to the images much better than traditional PCA. The result in the next section further con rms this by sho wing the su-perior performance of GPCA over PCA in terms of query precision.
 Figure 8: First row: raw images (same person). Sec-ond row: images compressed by PCA. Third row: images compressed by GPCA.
In this exp erimen t, we evaluate the e ectiv eness of GPCA and compare it with PCA in terms of query precision as de ned in Section 2. Figure 9 sho ws the precision curv es of PCA and GPCA on three face image datasets: PIX, ORL, and AR. The x -axis denotes the value of d , and the y -axis denotes the query precision.

Note that the precision curv e of PCA on the PIE dataset is not sho wn, since the corresp onding data matrix is simply too large to reside in main memory . For all precision curv es, the reduced dimension p in PCA is chosen suc h that both PCA and GPCA use the same amoun t of storage for the transfor-mation matrices and the reduced represen tations. For exam-ple, on the AR dataset, p = 3 ; 10 ; 22 ; 40 ; 62 principal comp o-nen ts are used in PCA, corresp onding to d = 4 ; 8 ; 12 ; 16 ; 20 used in GPCA.
 The main observ ations are:
The above exp erimen ts sho w that GPCA is a more e ec-tive dimension reduction scheme as it has signi can tly lower loss of information, i.e., high precision, when using the same amoun t of storage.
In this exp erimen t, we examine the eciency of GPCA and compare with PCA. As sho wn in Figure 10, GPCA has distinctly less computational time than PCA. As the size of the dataset gets larger from PIX to ORL to AR, the speedup of GPCA over PCA increases. For the AR dataset, GPCA is almost two orders of magnitude faster compared to PCA.
Note that the CPU time of PCA on the PIE dataset is not sho wn, due to the same reason men tioned above.
A new dimension reduction algorithm, GPCA, is presen ted for face image databases. GPCA is a signi can t extension of PCA. The key di erence between GPCA and PCA is that GPCA works on the matrix represen tation of images directly , while PCA uses a vector represen tation. GPCA has asymptotically minim um memory requiremen ts, and lower time complexit y than PCA, whic h is desirable for large face databases. GPCA also uses transformation matrices that are much smaller than PCA. This signi can tly reduces the space to store the transformation matrices and reduces the Figure 10: Comparison of running time (in log-scale) using the four datasets. Note that PCA is not ap-plicable for the PIE dataset, due to its large size. computational time in computing the reduced represen ta-tion for a query image. Exp erimen ts sho w sup erior perfor-mance of GPCA over PCA, in terms of qualit y of compressed images and query precision, when using the same amoun t of storage.
 Ackno wledgemen t Researc h of J. Ye and R. Janardan is sponsored, in part, by the Arm y High Performance Computing Researc h Cen ter under the auspices of the Departmen t of the Arm y, Arm y Researc h Lab oratory cooperativ e agreemen t num ber DAAD 19-01-2-0014, the con ten t of whic h does not necessarily re-ect the position or the policy of the governmen t, and no ocial endorsemen t should be inferred. [1] D. Achlioptas and F. McSherry . Fast computation of [2] C. C. Aggarw al. On the e ects of dimensionalit y [3] C. C. Aggarw al. Hierarc hical subspace sampling: A [4] P. Aigrain, H.J. Zhang, and D. Petk ovic.
 [5] V. Castelli, A. Thomasian, and C.S. Li. CSVD: [6] H. Cho, I.S. Dhillon, Y. Guan, and S. Sra. Minim um [7] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and [8] C. Faloutsos and K.I. Lin. FastMap: A fast algorithm [9] A. Frieze, R. Kannan, and S. Vempala. Fast [10] G. H. Golub and C. F. Van Loan. Matrix [11] H. Jin, B. C. Ooi, H. T. Shen, C. Yu, and A.Y. Zhou. [12] I. T. Jolli e. Princip al Comp onent Analysis . [13] R. Ng and A. Sedighian. Evaluating multi-dimensional [14] K. V. Ravi Kan th, D. Agra wal, A. E. Abbadi, and [15] L. Siro vich and M. Kirb y. Low-dimensional pro cedure [16] M. Turk and A. Pentland. Eigenfaces for recognition. [17] J. Ye. Generalized low rank appro ximations of
