 Huge amounts of search log data have been accumulated in various search engines. Currently, a commercial search engine receives billions of queries and collects terabytes of log data every single day. Other than search log data, browse logs have also been collected by client-side browser plugins, which record user browse information if users X  permissions are granted. Such massive amounts of search/browse log data, on the one hand, provide great opportunities to mine the wisdom of crowds and improve Web search results. On the other hand, designing effective and efficient methods to clean, model, and process large-scale log data also presents great challenges.

The objective of this survey is threefold.  X  X irst, we provide researchers working on search/browse log mining or related prob-lems with a good summary and analysis of the state-of-the-art methods and a stim-ulating discussion on the core challenges and promising directions. Particularly, for researchers planning to start investigations in this direction, the survey can serve as a short introduction course leading them to the frontier quickly.  X  X econd, we provide the general data mining audience with an informative survey.
They can get a global picture of the state-of-the-art research on search and browse log data mining. Moreover, researchers in other fields who need to tackle problems of a similar nature can quickly understand the on-the-shelf techniques that they can borrow to solve their problems.  X  X hird, we provide industrial search engine practitioners with a comprehensive and in-depth reference to the advanced log mining techniques. We try to bridge the research frontier and the industrial practice. The ideas and solutions introduced in this survey may motivate the search engine developers to turn research fruits into product reality.

In this section, we first describe the major content recorded in search and browse logs, respectively. Since raw log data is usually noisy and of extremely large size, preprocessing is often conducted before mining algorithms are applied. In the second part of this section, we will introduce four data summarizations which are created in preprocessing and widely used in log mining. After that, we will present an overview of log mining technologies and applications in Web search in the last part of this section. To better understand what search and browse logs record, let us first consider how people access information on the Web through browsers. Users X  behavior in a Web browser can be categorized into two states, namely, the search state and the browse state (see Figure 1).

In the search state, a user raises a query to a search engine and selectively clicks on the search results returned by the search engine. After that, the user may further refine the query and continue the interaction with the search engine. When the user visits a webpage other than a search result page, she is considered to be in the browse state. The major difference between the search state and the browse state lies in the type of server which the user interacts with: if a user is interacting with a search engine, she is in the search state; otherwise, she is in the browse state.
Users often make transitions between the search state and the browse state. When a user browses a webpage, she may want to go to a search engine and search contents re-lated to the page. In this case, the user transits from the browse state to the search state. On the other way, a user may also transit from the search state to the browse state. To be specific, after the user clicks on a webpage in the search result, she may further follow the hyperlinks of the webpage and leave the interaction with the search engine.
There exist various types of log data, and each targets some specific user actions and can be collected at either the server side or the client side. In this survey, we focus on two types of log data, namely, search logs and browse logs .

Search logs are collected by search engines and record almost all interaction details between search engines and users, including queries submitted to search engines, search results returned to users, and clicks made by users. In general, there are four categories of information in search log data: (1) user information, such as IP addresses and machine-generated user IDs, (2) query information, such as text and timestamps of queries, (3) click-through information, such as URLs and timestamps of clicks as well as positions of clicked URLs, and (4) search results provided by search engines, such as document ranking results and advertisement results.

Browse logs are usually collected by client-side browser plugins or proxies of internet service providers. They record all URLs visited by users, no matter whether from search engines themselves or other Web servers. Therefore, we may extract from browse logs not only users X  browsing behaviors but also users X  search behaviors. To that extent, browse logs provide a more comprehensive picture of user behaviors than search logs do. In addition, browse logs also contain URLs and timestamps of webpages browsed by the users. Browse logs, however, usually do not contain search results returned from search engines. To connect search results and click information, certain data integration processing is necessary.

Importantly, collecting and using browse log data must strictly follow some wellde-fined privacy policy meeting the proper regulation. Browse log data may be collected only under users X  permissions. Moreover, users should be able to easily opt out from browse log data collection. Although search and browse log data provide great opportunities for enhancing Web search, there are several challenges before such data can be used in various applica-tions. First, the size of log data is usually very large. In practice, the size of search and browse log data at a search engine is often at the magnitude of tens of terabytes each day. Second, log data are quite noisy. For example, queries may be issued by machines for experiments; user input in URL boxes may be redirected to search engines by Web browsers; and clicks on search result pages may be randomly made by users.
To overcome noise and volume, one can aggregate raw log data in preprocessing. By summarizing common patterns in raw data, the size of data can be greatly reduced. Moreover, after aggregation, we may prune patterns with low frequencies to reduce noise.

One question is how to summarize raw log data for various log mining tasks. In fact, search and browse log data have very complex data structures with various types of data objects and relationships, as illustrated in Figure 2. The data objects may include users, sessions, queries, search result pages, clicks on search results, and follow-up clicks. These different types of data objects form a hierarchy. At the top level, each user has a series of sessions, where each session contains a sequence of queries. In a query, a user may open several webpages. Finally, a user may further follow the hyperlinks in the webpages of search results and browse more webpages. In addition to the hierarchical relationship between different types of data objects, the data objects at the same level often form a sequential relationship.

Here, we introduce four types of data summarization that are widely used in log mining, namely, query histograms , click-through bipartites , click patterns ,and session patterns . Among the literature reviewed in this survey, 90% of the papers on log mining utilized at least one of the four types of data summarization. 1.2.1. Query Histogram. A query histogram represents the number of times each query is submitted to a search engine. As shown in Figure 3, query histogram contains query strings and their frequencies. As a simple statistics, query histogram can be used in a wide variety of applications, such as query auto completion and query suggestion. 1.2.2. Click-through Bipartite. A click-through bipartite graph, such as Figure 4, sum-marizes click relations between queries and URLs in searches. The bipartite graph consists of a set of query nodes and a set of URL nodes. A query and a URL are con-nected by an edge if the URL is clicked by a user when it is returned as an answer to the query. A weight c ij may be associated with an edge e ij , indicating the total number of times URL u j is clicked with respect to query q i . Click-through bipartite is probably the most widely used data structure in log mining. As we will see in the following sections, it can be used for query transformation, query classification, document annotation, and many other tasks. 1.2.3. Click Patterns. Click patterns summarize positions of clicked URLs in search results of queries. To be specific, each search result (also known as search impression) I q with regard to query q can be represented by ( u , p , c ), where u is the URL of a page, p is the position of the page, and c indicates whether the page is clicked. The identical search results are further aggregated to one click pattern P q = ( q ; L ; cc ), where cc is the number of search results. Figure 5 illustrates examples of click patterns. In practice, a list L only includes the top N URLs. Compared with a click-through bipartite, click patterns contain richer information. A click-through bipartite only represents aggregated clicks of URLs, while click patterns further represent the positions of the clicked URLs as well as unclicked URLs. As will be seen in the later sections, click patterns can facilitate many tasks in search, such as classifying navigational and informational queries, learning pairwise document preference, building sequential click models, and predicting user satisfaction. 1.2.4. Session Patterns. Session patterns summarize transitions among queries, clicks, and browses within search sessions. In fact, session patterns can be defined in different ways depending on specific applications. For example, Cao et al. [2008] and Boldi et al. [2008] take sequences of queries as sessions and extract frequent query sequences as session patterns. In other cases, session patterns may involve not only queries but also clicked URLs. For example, Cao et al. [2009] defined session patterns based on sequences of queries and their clicked URLs. Since session patterns represent users X  search behaviors in a more precise way, it has been used extensively. As will be seen later, session patterns have been widely used in tasks such as query transformation, document ranking, and user satisfaction prediction.

One critical issue with regard to session patterns is to determine the boundaries of sessions in a query stream from the same user. A widely used simple method is the so-called 30-minute rule . That is, any time interval longer than 30 minutes can be regarded as a boundary [Boldi et al. 2008]. Jones and Klinkner [2008] formalized the problem of session boundary detection as a classification problem. That is, given two adjacent queries in a query stream, decide whether they belong to two sessions or not. Their classifier makes use of features like the length of time between the two queries, the word and character similarities between the two queries, the statistical co-occurrences of the two queries, and the similarities of search results between the two queries. Jones and Klinkner [2008] showed that they can significantly enhance the precision from 70% to 92% using solely temporal features in the classification approach. See He et al. [2002] and Lucchese et al. [2011] for other approaches to session segmentation. With the abundance of search and browse log data, numerous log mining technologies have been developed. Silvestri [2010] divided log mining technologies into two major categories. The first category focuses on enhancing the efficiency of a search system, while the second category focuses on enhancing the effectiveness of a system. In this survey, we mainly introduce the technologies in the latter category, because most of the technologies are about effectiveness. There are also some other surveys on the topic (e.g., [Baeza-Yates 2004; Agichtein 2010]). Those surveys mainly focus on query understanding, while this survey covers five aspects of how log mining enhances Web search, namely, query understanding, document understanding, document ranking, user understanding, and monitoring and feedback. 1.3.1. Overview of Web Search System. From the viewpoint of effectiveness, a Web search system usually consists of three basic components, namely, query understanding , docu-ment understanding ,and document ranking , and one optional component, namely user understanding , as shown in Figure 6.  X  X ocument understanding transforms webpages into some representations that cover both content and importance. This task is usually carried out offline.  X  X uery understanding is performed online. Specifically, it receives queries and trans-forms them into some representations.  X  X ocument ranking is conducted to retrieve and rank documents with respect to a query based on the query and document representations. A ranking model typically has the form P ( M d | M q ), where M q is the query representation of a query q and M d is the document representation of a document d .

These three components constitute a search system. In recent years, the importance of users is emphasized, and the component of user understanding is also considered. The task of user understanding is to analyze user behavior and create either user profiles or context profiles in order to rank documents better. In general, a user profile summarizes from a user X  X  long search and browse history and characterizes the user X  X  preferences, while a context profile usually focuses on the environment, such as time, location, and search device, of a specific search activity. User profiles and context profiles will be described in detail in Sections 5.1 and 5.2, respectively. Given a user or P ( M d | M q , P c ).

In addition to these components, monitoring a search system and collecting user feedback are also important for the effectiveness of a system. 1.3.2. Enhancing Query Understanding. Understanding the search intent of a query has been recognized as a crucial part of effective information retrieval. Despite this, search systems, in general, have not focused on explicitly representing query intents. Query processing has been limited to simple transformations, such as stemming or spelling correction.

In Web search, with large amounts of search and browse log data available, it be-comes possible to conduct deeper query understanding and represent intents of queries with richer representations. Query understanding includes methods for various tasks, including (1) query substitution, (2) query classification, (3) query transformation, (4) query segmentation, and (5) named entity recognition in queries. Section 2 will re-view the existing work on using search and browse log mining to enhance those tasks. 1.3.3. Enhancing Document Understanding. Webpages are hypertext documents contain-ing HTML tags and links. One can create rich document representations for webpages and leverage them in Web search. Actually, use of the tagging and linking information of webpages has been proved to be very effective. The anchor texts of webpages can be utilized as external tags to pages. The HTML formatting information of webpages is useful for identifying titles as well as key phrases of pages. Furthermore, the link graph of the webpages can be used to judge page importance by applying the PageRank or HITS algorithms.

Search and browse log data provide new opportunities to enhance the representations of documents in both content and importance. First, user clicks can be considered as implicit feedback on the relevance of documents. Therefore, they can be used as external tags of webpages as well. In addition, the browse logs can be leveraged for calculating the importance of webpages. Intuitively, if a webpage is visited by many users and users stay long on the page, the page is likely important. Section 3 will survey how search and browse log mining can improve Web search through query annotation and page importance calculation. 1.3.4. Enhancing Document Ranking. The key functionality of a Web search engine is to rank documents according to their relevance to a given query. How to leverage users X  explicit feedback to improve the quality of ranking has been well studied in traditional information retrieval. Search logs provide users X  implicit feedback on ranking. Intu-itively, if users often skip a search result ranked at a higher position but click on a result at a lower position, then the lower ranked result is likely to be more relevant than the higher ranked one.

In Section 4, we will review two approaches to mining click-through data. The first approach derives information on the relative relevance of document pairs. The second approach builds a sequential click model to characterize user X  X  click behavior on the list of search result. 1.3.5. Enhancing User Understanding. Search consists of three major factors: queries, documents, and users. Traditionally, the modeling of users was based on the contents created by the users. The availability of search and browse log data makes it possible to conduct better user modeling and in sequel improve search quality. For example, with better understanding of users, one can conduct disambiguation of queries more effectively and thus enhance the performance of document ranking.

There are two popular approaches to addressing search using user modeling, namely, personalization and contextualization . The personalization approach often mines search and/or browse log data and creates a user profile for each individual user to characterize her preference, as will be described in Section 5.1. The contextu-alization approach creates a context profile from log data to capture users X  preference in different contexts, as will be explained in Section 5.2. 1.3.6. Monitoring and Predicting User Satisfaction. Obviously, search and browse log data are useful for evaluation of a search system. Numerous metrics, such as numbers of queries, numbers of users, and average click-through rates can be calculated from log data with respect to different markets and time periods. In the meantime, various models can be applied to predict user satisfaction from search and browse log data. The applications of log data for monitoring and predicting user satisfaction will be discussed in Section 6. 1.3.7. Enhancing Efficiency of Web Search. Mining search and browse log data can en-hance the efficiency of Web search [Silvestri 2010]. For example, search logs can be exploited to build a caching mechanism to reduce the workload of a search engine. The data can also be leveraged to make a balanced partition of documents and terms within the distributed system of search engine.

The remaining part of this survey is organized as follows. From Sections 2 to 6, we will elaborate how search and browse log data can enhance each of the five major com-ponents of a search system, namely, query understanding, document understanding, document ranking, user understanding, and monitoring and feedback. Section 7 gives a summary of the survey. Query understanding aims at understanding intent of queries, performing better doc-ument ranking, and providing better search result presentation. It is also useful in tasks such as query completion (e.g., [Shokouhi and Radinsky 2012]) and query sug-gestion (e.g., [Ozertem et al. 2012]). Obtaining statistics about queries often serves as the first step. In addition, most of the existing work addresses query understanding as query classification, query segmentation, query transformation, and named entity recognition in queries. We describe them in detail. Let us start with some basic statistics of queries. This will be useful for understanding of existing work.

Numerous studies were conducted to analyze queries in search and browse log data from different perspectives, including (1) how users conduct search, such as query length, query frequency, query term frequency, number of viewed search result pages, session length; and (2) what users search for, such as topic distribution and function. Detailed analysis on these two aspects, is available [H  X  olscher and Strube 2000; Jansen et al. 2007; Jansen and Pooch 2001; Jansen and Spink 2006; Spink et al. 2001, 2002; Silverstein et al. 1999; Silvestri 2010; Wolfram et al. 2001]. The major conclusion from the statistics is that Web search is very different from traditional information retrieval. Other statistics on queries, are available [Beitzel et al. 2004, 2007; Backstrom et al. 2008; Mei and Church 2008; Weber and Jaimes 2011; Weerkamp et al. 2011].

The average length of search queries is about 1 . 66 X 2 . 6 words, which is much shorter than in traditional IR (6 X 9 words) (cf., [Jansen et al. 2007]). The average length is becoming longer but in general remains constant in a relatively short time period.
Query frequencies and query term frequencies follow power law distributions. That is, head (high frequency) queries and terms account for the majority of search traffic. On the other hand, tail (low frequency) queries and terms also consist of large percentages of the distributions.

It is also observed that users browse on average less than two webpages in search results and over half of the users do not access results beyond the first pages. This observation implies that the relevance of the top ten results is critically important for a Web search engine. Average session length is around two to three queries and more than half of the sessions consist of only one query.

The major topical categories of queries are  X  X eople and place X ,  X  X ommerce X ,  X  X ealth X ,  X  X ntertainment X ,  X  X nternet and computer X , and  X  X ornography X . The relative order among them can be slightly different over time and in various regions.

There are four major types of queries from the viewpoint of linguistic structure [Bendersky and Croft 2009], namely, (1) noun phrases, (2) composition of noun phrases, (3) titles (titles of books, products, music, etc.), and (4) natural language questions. The majority of queries fall into the first two types. Queries can be classified along multiple dimensions, including search goals, topics, time sensitivity, and location sensitivity. The classes of a query can be used to represent the intent of the query.

All types of search and browse log data can be used for query classification in addition to Web data. In fact, click-through data, session data, and search result data are widely used for the query classification tasks.

Query classification, however, is a challenging task, due to the following reasons: (1) queries are short, (2) queries are usually ambiguous, (3) the meanings of queries may change over time and location, (4) queries are also noisy, for example, about 10% of queries contain typos. 2.2.1. Search Goals. Broder [2002] and Rose and Levinson [2004] pointed out that from the viewpoint of the goals of search in the searchers X  minds, search intents can be categorized into navigational intents, informational intents, and transactional intents. A navigational search is to reach a particular site, an informational search is to acquire information assumed to be present on one or more webpages, and a transactional search is to perform some Web-mediated activity. In a specific search, the intent of the user in terms of search goal should be very clear, either navigational, informational, or transactional.

Queries are often ambiguous. The same query can be used in different searches to represent different search goals. However, it is often the case that for a specific query there is a dominating intent. Then, given a query, is it possible to identify its dominating search goal? This is the problem of query classification by search goal.

Lee et al. [2005] proposed using distribution of clicks and that of anchor text to identify whether the dominating intent of a query is navigational or informational, assuming that transactional is included in navigational. Their method mainly takes advantage of the following heuristic in click-through data and Web data to perform the task. If a query is navigational, then the main search intent represented by the query is to find a specific webpage. Thus, the majority of clicks with respect to the query should be on a single URL in the click-through data. For example, for the query  X  X icrosoft X , the majority of clicks are on the URL microsoft.com . If a query is navigational and also exists in the anchor texts in a Web collection, then the majority of the anchor texts should be linked to the same URL. For example, the majority of the anchor texts  X  X icrosoft X  should point to the URL microsoft.com . In other words, the click distribution of a navigational query is skewed, and so is the anchor distribution of a navigational query.

They used the preceding information as features to train a classifier to identify whether a given query is more likely to be navigational or informational. The infor-mation is specifically represented as mean, median, skewness, etc. They asked human subjects to manually label queries as navigational or informational and took majority votes on the labeled results. The accuracy for the classification was about 90% on the labeled data. The challenge for the approach is that it is difficult to apply to tail queries because much less click data and Web data are available for tail queries.

Queries can also be classified by search tasks [Ji et al. 2011; Liao et al. 2012]. 2.2.2. Semantic Class. Query classification by their semantic classes or topics is useful in search result presentation. The classified semantic classes of a query can also be included in the query representation.

The classes in query classification can be coarse (e.g.,  X  X nternet X ) or fine-grained (e.g.,  X  X BA X ,  X  X hoes X ). Shen et al. [2006] designed a method for the former case, while Fuxman Fuxman et al. [2008] and Li et al. [2008] tackled the latter case.

One simple approach to query classification is to view a query as a short text and formalize query classification as text classification. Since queries are short and am-biguous, this approach does not work well. To enhance the accuracy of classification, Shen et al. [2006] proposed using the search results of a query at Web search engines to enrich the original query and use the enrichment of the query in classification. Specifically, in their method, each query is submitted to several Web search engines. In the search result of each search engine, the semantic categories of the webpages, as well as the titles and snippets of the webpages, are collected. The former is viewed as synonyms, and the latter is viewed as pseudotexts of the query. Two classifiers are then applied to the synonyms and pseudotexts, respectively, for query classifica-tion. The classifiers from different search engines are then linearly combined to make the final classification decisions. Shen et al. [2006] used the techniques and won the championship at the query classification task of the KDD Cup 2005.

Fuxman et al. [2008] proposed a method for classifying queries using a click-through bipartite graph. Their method views a click-through bipartite as an undirected graph. In the graph, there is a special node called the null node. Every other node is linked to the null node. A random walk model on the graph is then defined. In the model, the probability of an edge represents the transition probability, which is calculated based on click counts, the probability of a node represents the probability of belong-ing to a class. In binary classification, some nodes (query nodes, URL nodes, or both) are labeled as positive examples at the beginning. That is, their probabilities of being in the class are all set to one. The learning process of the model then becomes proroga-tion of the class labels to the rest of nodes on the graph through random walk. Fuxman et al. [2008] adopted an efficient iterative algorithm to conduct the label propagation. The algorithm keeps updating the probabilities on the nodes and is guaranteed to be converged. Finally, the unlabeled nodes are assigned probabilities of belonging to the class. The model is an analogy to an electrical network. The probabilities on the edge correspond to conductance, the probabilities of nodes correspond to voltage, and the null node corresponds to the ground. At the beginning, the labeled nodes have a unit of voltage, and after some iterations, the voltages of all the nodes become stable.
Li et al. [2008] proposed a method of classifying queries into a class (topic) using click-through bipartite and some labeled queries. The labeled queries include both positive and negative examples. They employed two classifiers for the task. One is based on content and the other one based on propagation on the click-through bipartite graph. In learning, their method iteratively trains the two classifiers. First, it trains a classifier based on the content of the labeled queries and then propagates the class labels on the click-through bipartite. Finally, the content-based classifier is used for query classification. In the label propagation classification, their method views the click-through bipartite as an undirected graph and formalizes the learning problem as propagation of the labels from the labeled nodes to the whole graph, assuming that the similarities between nodes are calculated from the click counts. The intuitive explanation is that the class memberships of unlabeled nodes can be inferred from those of the labeled nodes according to the proximities of the nodes in the graph. Their method is equivalent to increasing the amount of training data by semisupervised learning with click-through bipartite graph. They showed that with their method it is possible to significantly increase the training data size and thus significantly improve the accuracy of content-based query classification.

Other methods of query classification have also been proposed [Baeza-Yates et al. 2006; Beitzel et al. 2007; Hu et al. 2009, 2012; Kang et al. 2011]. 2.2.3. Location Sensitivity. Queries may also have location intents, which means that the searchers want to get information close to certain locations, such as the search locations. For example, when a user submits a query  X  X izza hut X , she may want to find the information about the Pizza Hut restaurants nearby. According to Welch and Cho [2008], about 30% of queries are location sensitive, which are called localizable queries. Welch and Cho [2008] also proposed a classifier for identifying whether a query is a location-sensitive query. Their method makes use of features derived from search log data and a dictionary of locations. The method takes advantage of the fact that a location-sensitive query is likely to co-occur with location names in other queries. For example, the location sensitive query  X  X ar rental X  may co-occur with many location names, such as  X  X alifornia X  and  X  X ew york X . The features used by the classifier include whether the query frequently co-occurs with location names in the log data and whether the distribution of co-occurrence between the query and the location names is close to uniform. For example,  X  X ar rental X  is a location-sensitive query, since it occurs with many different location names. In contrast, although  X  X eclaration X  frequently co-occurs with the location name  X  X ndependence X , a city in Missouri, in the queries  X  X eclaration of independence X , it is not a location-sensitive query. This is because the distribution of the query with location names is skewed. For query classification based on location, please see also Yi et al. [2009]. 2.2.4. Time Sensitivity. Queries may have time intents as well. When submitting a time-sensitive query, a searcher often intends to find information in a time period, typically very recent. In the context of recency ranking in Web search, Dong et al. [2010] proposed a method for identifying whether a query is time sensitive, specifically, recency sensitive. The key idea is to calculate the difference between the likelihood of a query in the current time slot and that in the past time slots, both in query logs and news articles. If there is a significant difference, then it is likely that the query is recency sensitive. More specifically, the method creates language models from query logs and news articles (contents) in the current time slot and the historical time slots of past day, past week, and past month, referred to as M Q , t , M C , t , M Q , t  X  r respectively, where i denotes the nested past time slots. Given a new query in the current time slot, the method estimates the likelihoods of the query being generated from each of the models. It calculates the buzziness of the query from the query logs as The buzziness of the query from the news articles buzz ( q , t , C ) is calculated similarly. The final score is obtained by linearly combining the two buzziness scores. Other related work, is also available [Vlachos et al. 2004; Chien and Immorlica 2005; Kulkarni et al. 2011]. Query transformation changes an original query to a query or some queries similar to it, such as changing  X  X y times X  to  X  X ew york times X . Query transformation is also referred to as similar query finding, query rewriting, and query alteration [Croft et al. 2010]. A query is considered similar to another query if they share similar search intents. Query transformation can be exploited for enhancing search relevance as means of query expansion and query reduction. It can also be used in query suggestion. Query transformation can be performed by using click-through data and session data. It can also be performed by models trained with click-through data and session data. 2.3.1. Query Transformation Using Click-Through Bipartites. In a click-through bipartite graph, similar queries may have similar clicked URLs associated. The co-click in-formation can be used in finding similar queries. Likewise, the information can also be used in finding similar URLs. We consider two types of query transformation or similar query finding using click-through bipartite graphs. One is to calculate similarity scores and the other is to conduct query clustering.
 Xu and Xu [2011] calculated query similarity from click-through bipartite using the Pearson correlation coefficient. They found that when the Pearson coefficient is larger than 0.8, more than 82.1% of query pairs are indeed similar query pairs.

Beeferman and Berger [2000] proposed an agglomerative clustering algorithm for clustering similar queries using a click-through bipartite graph. Their algorithm is completely content-ignorant in the sense that it makes no use of the actual contents of the queries and the URLs but is based on only how they co-occur within the click-through bipartite. Although the algorithm is simple, it can discover high-quality query clusters. For other methods of query clustering using click-through, refer to Cui et al. [2002] and Wen et al. [2001].

A click-through bipartite graph in practice can be extremely large. How to efficiently perform clustering on click-through bi-partite then becomes a critical issue. Cao et al. [2008] developed an efficient algorithm for the task of context aware query suggestion. To significantly enhance the efficiency of clustering, they leveraged the fact that, on average, each query only has about three associated URLs, and each URL only has about three associated queries. The algorithm needs only one scan of the data, and the average case time cost is linear to the number of instances. Specifically, their algorithm adopts an agglomerative approach to clustering. It linearly scans the data and incrementally creates clusters. Given the current instance, the algorithm compares it with the centroids of all the existing clusters. If the instance is close enough to one of the clusters, then it is added into the cluster; otherwise, a new cluster is created for the instance. The algorithm takes advantages of the fact that, when calculating the similarity between two instances (also between an instance and a centroid) in Euclidian distance or dot product, one only needs to make calculation on the nonzero elements shared by the two instances. The algorithm creates and maintains an inverted index about all the nonzero elements of the instances in the existing clusters. Once a new instance comes, it only takes its nonzero elements, looks up the index, and makes similarity comparison with the clusters that also have nonzero elements in the same positions. Later, Liao et al. [2011] further improved the method such that postprocessing is conducted to remerge and resplit clusters to reduce the low quality clusters due to the order of input.

Another state-of-the-art method for finding similar queries from click-through bipartite graph was proposed by Craswell and Szummer [2007]. From click-through bipartite, one can find not only query-query similarity but also document-document similarity and query-document similarity. Craswell and Szummer X  X  method takes the click-through bipartite graph as a directed graph and defines a backward random walk model on the graph. The transition probabilities P ( k | j ) from node j to node k is calculated by normalizing the click counts out of node j ,thatis, where C jk denotes the number of clicks on the edge between the nodes j and k ,and s is the self-transition probability. The random walk usually takes several steps of walk and stops. It is observed that the role of self-transition probability is very important. Craswell and Szuemmer X  X  model is actually a model of similarity weight propagation on the click-through bipartite graph. For other work on finding similar queries from a click bipartite graph, see Mei et al. [2008]. 2.3.2. Query Transformation Using Session Data. Searchers sometimes issue similar queries in the same search sessions. We can also find pairs of successive queries from search sessions. Jones et al. [2006] developed a method for discovering similar query pairs from session data. Given two queries q 1 and q 2 , they conducted likelihood ratio testing to check whether their co-occurrence in search sessions is statistically signifi-cant.
 The likelihood ratio is The query pairs whose likelihood ratios are above a threshold are viewed as similar queries or substitutable queries. The similar queries discovered by Jones et al. X  X  method are based on typical substitutions web searchers make. Other methods of finding sim-ilar queries from search session data are available [Huang et al. 2003; Fonseca et al. 2005; Boldi et al. 2008; Szpektor et al. 2011]. 2.3.3. Model-Based Transformation. The aforementioned methods can automatically mine similar queries from click-through data and session data. These methods usually work very well for head queries, but not for tail queries. We need to consider using the data from head queries to train models applicable to tail queries. An essential idea is that the linguistic knowledge learned from heads can be applied to tails. For example, if we can learn from head queries  X  X ign on hotmail X  and  X  X ign up hotmail X  that phrases  X  X ign on X  and  X  X ign up X  are similar, then we can judge that the tail queries  X  X ign on x-forum X  and  X  X ign up x-forum X  should also be similar.

Guo et al. [2008] proposed a method based on this rationale. They viewed query transformation as a mapping from the space of original queries X to the space of refined queries Y . Obviously, directly exploiting the model P ( y | x ) is not practical, because both X and Y are extremely large, where y and x are random variables taking values from Y and X . They proposed to add another random variable o and employ the model P ( y , o | x ) to solve the problem, where o takes values from a set of operations. An operation can be insertion, deletion, and substitution of letters in a word, splitting of a word into multiple words, merging of multiple words into a single word, word stemming, or some others. To be specific, the number of mappings from any x in X to any y in Y can be very large. However, the number of mappings from x to y under operation o will be drastically reduced. They defined P ( y , o | x ) as a conditional random field (CRF) model on query word sequences. They developed methods for learning the model and making prediction using dynamic programming. Given a sequence of query words, the CFR model predicts a sequence of refined query words as well as corresponding refinement operations. One merit of this approach is that different types of transformations, such as spelling error correction, merging, splitting, and stemming, can be performed simultaneously, and thus the accuracy of transformation can be enhanced, because sometimes the transformations are interdependent. The data for training the CFR model can be mined from session data using a method developed by Jones et al. [2006].

Spelling error correction in query can be viewed as a specific task of query transfor-mation. Normally, about 10% of queries contain spelling error, and thus spelling error correction is a very important component for Web search [Guo et al. 2008]. Guo et al. [2008] developed a discriminative approach. Duan and Hsu [2011] proposed generative approaches to spelling error correction. See also Li et al. [2012]. A query q can be viewed as a sequence of words ( w 1 ,w 2 ,...,w k ). A segmentation of query q is a sequence of phrases that can compose the query. For a query of k words, there are 2 k  X  1 possible segmentations. Query segmentation is a difficult task, because queries are short and ambiguous. For example, the query  X  X ew york times square X  may have different segments,  X (new york) (times square) X  and  X (new york times) (square) X . Both supervised and unsupervised approaches are proposed for query segmentation.
Bergsma and Wang [2007] proposed viewing the query segmentation task as a problem of making a segmentation decision at each adjacent word pair. In the classification framework, the input is a query and a position in the query, and the output is a segmentation decision at the position (yes/no). In segmentation, k  X  1 decisions are made for a k word query. A binary classifier can be trained for the problem. Features like whether the left word is  X  X he X  and part of speech of the left word are used. Bergsma and Wang [2007] verified that a local classification approach works better than a global tagging approach, such as one using a hidden Markov model, the reason being that query segmentation is not a sequential modeling problem. Hagen et al. [2011] proposed an unsupervised approach to query segmentation. The advantage of the unsupervised approach is that no training is needed. Although the method proposed by them is very simple, it works very well in experimentations. The method, called na  X   X ve normalization, calculates a score for each segmentation of a query, ranks the segmentations based on their scores, and takes the segmentation with the highest score as output. That is, where S denotes one segmentation and s is a segment (an n -gram) within S , freq ( s ) is the frequency of s calculated from a large Web corpus. The summation is only taken from the segments of more than one word. Furthermore, | s | | s | is a weight favoring long segments ( n -grams), because longer segments are more preferable. For example,  X  X oronto blue jays X  should not be further segmented to  X  X lue jays X , though the latter has a larger frequency than the former.

For query segmentation, see also Li et al. [2011] and Hagen et al. [2012]. Many queries contain named entities in different types, such as personal names, loca-tions, organizations, and product names. Named entity recognition in queries is helpful for search result presentation.

Pas  X ca [2007] and Pas  X ca and Alfonseca [2009] conducted a series of research on the problem and proposed several methods for the task. Their basic idea is to use the patterns of attributes of entities in a class to identify new entities pertaining to the class. Their approach employs weakly supervised learning by assuming that there is a small number of labeled instances available, that is, seed entities belonging to a class. It starts the mining process with the seed entities. It matches the seed entities to the query log, discovers context patterns of the entities, and mines new entities with the patterns. Finally, it calculates the context similarities between the mined entities and the seed entities and ranks the new entities based on their context similarity scores. For example,  X  X ioxx X  is a seed entity of the class Drug. From the query log, its context patterns can be found, such as,  X  X ong term * use X ,  X  X ide effect of * X , where * is a wildcard. With the patterns, new entities of the class, such as  X  X iagra X  and  X  X hentermine X , can be mined. Their methods all assume that if A is a prominent attribute of class C and I is an instance of class C , then a fraction of queries about I should be about both I and A .
One challenge for the preceding deterministic approaches is that it is hard to deal with ambiguities in named entities. For example,  X  X arry potter X  can belong to multi-ple classes including Book, Game, and Movie. The mined attributes can be those for different classes, and thus it is easy to include noises in the mining process. Guo et al. [2009] proposed a probabilistic approach for tackling the disambiguation problem. The approach is based on Weakly Supervised Latent Dirichlet Allocation (WS-LDA), an ex-tension of conventional LDA. Their method creates a pseudo document for each labeled named entity. It views the contexts of a named entity in search log as words of the document with regard to the entity, and the classes of the named entity are regarded as possible topics of the document. Thus, the possible topics for each document are reduced to a small number, though they are still ambiguous. This is a different setting from the conventional LDA. Their method learns the topic model given the labeled data using variational EM algorithm. In learning, the supervision information is in-corporated into the objective function as constraints. As a result, the probabilities of words given topics, that is, probabilities of contexts given classes, are learned from the documents, and they can be utilized as patterns for entity mining. Since the framework for mining is probabilistic instead of deterministic, more accurate context patterns can be learned by the approach. Document understanding is to represent and measure documents (webpages) in an effective way so that documents that are relevant to a query can be retrieved and ranked high in search. Web search takes into account two important aspects of webpages: representation and importance. In this section, we explain how log data may help to improve document understanding in creation of webpage representation as well as calculation of webpage importance. A webpage is actually a text document and can be represented as a vector of TF-IDF scores of the words in it. The vector can then be indexed in a search system and used in the vector space model, the BM25 model, or language model for IR for relevance ranking. This is a conventional way and has been well explored in traditional information retrieval [Salton et al. 1975; Baeza-Yates and Ribeiro-Neto 1999].
Webpages contain hypertexts and thus are more than just words. There is rich information on the Web which can help to enhance the representations of webpages. For example, the anchor text of a webpage, pointed from another webpage, often gives a compact and precise description of the page. The anchor texts of a webpage actually reflect how the authors of the other pages on the Web think about the webpage. It has been widely verified that anchor texts are useful information for representing webpages in order to make them better searched.

While anchor texts may help represent webpages, they only represent the views from Web content creators, not necessarily Web users. Furthermore, the distribution of anchor texts also follows the zipf X  X  distribution, and the tail webpages (unpopular) usually do not have enough anchor texts. Thus, queries as annotations are explored. 3.1.1. Queries as Annotations. Since search log data, particularly, click-through data, record user queries and the corresponding clicked webpages, a natural idea for using log data to enhance webpage representations is to using queries as annotations of the webpages. If a user asks a query q and clicks a webpage p in the result list, then it is likely that p is relevant to q in one way or another. Consequently, q can be used as an annotation of page p . In other words, the log data can be regarded as users X  annotations of webpages. Although click-through data contain noise, it has been verified that the use of click-through data as annotations of webpages can help improve relevance significantly [Agichtein et al. 2006a, 2006b]. Obviously, data cleaning, for example, using frequency cut-off, is necessary.

One advantage of using log data to represent webpages is that such data reflect the views on webpages aggregated from many users, which might be more useful for search. Moreover, as log data are accumulated, the annotations from the users will be dynamically and continuously updated.

Poblete and Baeza-Yates [2008] developed two query-based webpage representation models using click-through data. The first one is called query-document model. The major idea is to use the terms in queries associated with a webpage as annotations of the page. The query-document model uses a vector of query terms weighted by TF-IDF. The frequency of a term is calculated as the frequency of the queries containing the term associated with the page.

A drawback of the query-document model is that it assumes terms are independent from each other. In practice, some query terms frequently occur together in the same query, expressing a concept. For example, queries  X  apple computer  X  X nd X  apple juice  X  carry completely different meanings for the word  X  apple  X .

To deal with the problem, the authors proposed the second model, the query-set model. The model uses frequent query term sets as annotations. Given a set of queries associated with a webpage, where a query may contain multiple terms, we can find frequent one-, two-, and three-term combinations in the set of queries based on certain supports (frequency thresholds). Each frequent term combination is called a relevant set. Then, the webpage is represented by a vector of relevant sets weighted by TF-IDF. The term frequency of a relevant set is the number of times that the relevant set appears in the queries associated with the webpage. 3.1.2. Coping with Data Sparseness. While using queries to annotate webpages is an intuitive idea, there are several technical challenges. Click-through data are sparse. Many webpages may have very few or even no clicks. For example, Gao et al. [2009] reported that in a real dataset of 2 . 62 million query-document pairs, 75% of them do not have any clicks. How can we effectively annotate those webpages? Several studies addressed the challenge using various techniques.
 Xue et al. [2004] proposed two methods for dealing with the sparseness challenge. The first method makes use of not only the queries associated with a webpage as the representations of the web page, but also the queries associated with its similar webpages. Here, two webpages are considered similar if they are clicked in the searches of the same set of queries. For example, consider the queries and the webpages in Figure 7. A query and a webpage are linked by an edge if the webpage is clicked when the query is searched. Pages p 2 and p 4 are clicked by the same set of queries, namely, { q 1 , q 3 , q 4 } . Thus, p 2 and p 4 are considered similar because they may satisfy the similar information needs. The term frequency of query q j with respect to webpage d is calculated by W ( d i , q j ) = d webpages similar to webpage d i , S ( d i , d k ) is the similarity between webpages d i and d k calculated based on co-click relations, and W ( d k , q j ) is the frequency of query q j with respect to webpage d k .

The second method is an iterative approach that mutually reinforces query and doc-ument similarities on the click-through bipartite graph. The method performs random walks on the click-through graph. It is assumed that two webpages are similar if they can be searched by similar queries and two queries are similar if they can search similar webpages.

Technically, let S Q [ q 1 , q 2 ]  X  [0 , 1] be the similarity between two queries q 1 , q 2 in question, and Let S P [ p 1 , p 2 ]  X  [0 , 1] be the similarity between two webpages p 1 , p 2 in question. The following equations are used to implement the previous mutual reinforc-ing rules.
 where C is a decaying factor that is set to 0 . 7 empirically, O ( q ) is the set of webpages associated with query q , O i ( q )isthe i th webpage in the set O ( q ), I ( p )isthesetof queries associated with webpage p ,and I i ( p )isthe i th query in the set I ( p ).
Since Equations (1) and (2) are recursive, we can propagate the similarities through an iterative process. We start with and compute S i + 1 from S i . It is easy to see that the values S k are nondecreasing and have an upper bound of 1. Thus, the iterative process converges.

The similarity propagation can overcome the sparsity in log data. Even two webpages p 1 and p 2 do not share any queries, that is, I ( p 1 ) similar if many pages associated with the queries in I ( p 1 )and I ( p 2 ) are similar.
To overcome the sparseness problem, Gao et al. [2009] proposed a random walk method to smooth the frequencies (weights) of queries. Technically, we can construct To make each query or document similar to itself, a self-transition is added to each query or document. Then, we can conduct random walk on the click-through graph by multiplying the matrices. Take the click-through bipartite graph in Figure 7 as an example. Without the expansion, webpage p 1 is only associated with query q 2 . After a random walk step, the association between webpage p 1 and query q 3 is augmented, because query q 3 has a similar click pattern as query q 2 . Calculating the importance of webpages is an essential task in Web search. Important webpages should be ranked high in the answer list of webpages to a query. Tradition-ally, the importance of webpages is calculated by link analysis. A link from one page p 1 to another page p 2 is regarded as an endorsement of p 2 from p 1 . The more links pointed to a page, the more likely the page is important. The importance of pages can be propagated in the Web graph, where vertices are pages and edges are hyperlinks. For example, in PageRank [Page et al. 1999], a famous link analysis method, page impor-tance is modeled as a stationary distribution of a Markov chain (random walk) model defined on the Web graph. The PageRank scores of webpages can be calculated itera-tively. In other words, PageRank is a discrete-time Markov process on the Web graph.
HITS [Kleinberg 1999] is another link analysis method for modeling the importance of webpages. Two scores are calculated for each page: a hub score and an authority score. A page has a high hub score if it links to many pages. A page has a high authority score if it is pointed by many pages. Using the heuristic that good hubs tend to link to good authorities, and vice versa, the hub scores and the authority scores can be updated in an iterative way until they are stabilized for all pages.

While link analysis, such as PageRank and HITS, has been shown effective, one drawback is that those methods only model the importance of webpages from the webpage authors X  points of view, instead of from the webpage users X  points of view. Mining user feedback from log data can help to improve the modeling of webpage importance.

Liu et al. [2008] proposed an algorithm called BrowseRank, which computes page importance from a user browsing graph built from users X  browsing history. In the user browsing graph, webpages are represented as vertices, and the transitions between pages are represented as edges. The staying time on webpages is also considered. A continuous-time Markov process is defined on the user browsing graph, and its stationary probability distribution is computed as the page importance. Liu et al. [2008] reported through their empirical study that BrowseRank achieves good performance in several tasks. For example, in Web spam fighting, BrowseRank can push many spam websites to the tail buckets. This is because users more frequently browse and spend more time on high-quality pages than spam pages. This user behavior is effectively utilized by the algorithm.

Session data representing user browsing trajectories can also help the calculation of page importance. Zhu and Mishne [2009] viewed a session as a sequence of hops through the Web graph by a user and computed ClickRank as the importance of each webpage in the session. The ClickRank of a page in a session is defined as a function of the order of the page in the session and the dwell time on the page by the user. Intuitively, the lower the rank order and the longer the dwell time, the more important the page is in the session. The importance score of a page is then calculated as the sum of its scores over all the sessions containing the page. In Web search, given a query, a ranking list of webpages is returned, where the web-pages are ranked in the descending order of the degrees of relevance to the query (i.e., degrees of matching to the query). A critical issue here is to properly rank the webpages with respect to the query on the basis of relevance [Salton et al. 1975; Baeza-Yates and Ribeiro-Neto 1999; Croft et al. 2009]. Log data mining can help substantially improve this document ranking process. First, we can derive preference on webpages with respect to queries from click-through data. Second, we can use the preference information to improve search relevance, for example, using it as training data or fea-ture in learning to rank [Li 2011]. In this section, we review representative methods for mining preference information from click-through data, in particular, preference pairs and click models. In Web search, when a ranking list of webpages is presented to a user, some additional information about the webpages is also provided, such as the titles and snippets of webpages. Then, the user clicks on a webpage in the result list if she thinks the page looks interesting and relevant to the query, possibly hinted by the snippet. After the user clicks on a webpage in the result list, she may click on another webpage. What may such a click tell us? Possibly, the webpage clicked previously does not completely satisfy the user X  X  information need. Therefore, the user is looking for more information. The webpage she clicks later may be more relevant.

As we can see, users X  click-through activities can be regarded as users X  implicit feedback about the search results. That is, when a user clicks a webpage, the user does not explicitly tell whether the webpage satisfies her information need or to what extent the page satisfies. The click-through activities, however, provide hints about the users X  preference on the webpages with respect to queries.

Now, the question is how we can use the users X  click-through data to derive preference information between queries and documents and how to use the preference information to improve search relevance. Under the assumption that a click of a webpage implicitly suggests that the webpage is relevant, a na  X   X ve method is to promote those webpages clicked in the searches of a query and demote those webpages un-clicked. However, such a na  X   X ve method has some fatal drawbacks. First, there exists position bias from the ranking list of webpages. Users usually scan the webpages from the top. Those webpages ranked at higher positions may have a better chance to be clicked. A webpage hidden at a late position, say the 1,000th position in the ranked list, would unlikely be viewed by the users, even if it is perfectly relevant to the query. Second, due to many reasons, relevant documents may not be included in the ranking list to the query. The na  X   X ve method does not work in such a situation.

We need to employ more appropriate methods to learn users X  preference on webpages from click-through data. There are two ways to model user preferences. We can capture the pairwise preferences. That is, given webpages a and b , we try to learn from the click-through data which one is more preferable. Alternatively, we can learn the user preference order on a set of webpages. Correspondingly, the methods of using log data to improve ranking relevance can be divided into two groups, namely the preference pair methods and the click models. For a query q , suppose a search engine returns a ranked list of webpages d 1 ,..., d n , and a user clicks some webpages in the list. A brute-force method to learn preferences is to assume that the clicked webpages are more preferable to those not clicked. That is, we can extract a preference relation d i  X  d j for 1  X  j &lt; i ,when d i is clicked, and d j is not clicked, meaning d i is more preferable to d j . Such a brute-force method, however, leaves much information unused. Importantly, no preference is derived between any two clicked webpages. Similarly, no preference is derived between any two non-clicked webpages, either. We need a systematic way to model preference pairs.

Joachims et al. [2005; Joachims 2002] examined individual users X  implicit feedback in some click-through data. In a ranked list of webpages d 1 ,..., d n with respect to a query q ,let C be the set of clicked webpages. They suggested and verified using real data that the following types of preferences can be extracted.  X  X  clicked page is more preferable to the pages skipped above, that is, d i  X  d j for all pairs 1  X  j &lt; i with d i  X  C and d j  X  C . As a variant, the last clicked page is more preferable than all the pages skipped above. Their experimental result shows that the variant is slightly more accurate than its general form.  X  X  clicked page is more preferable to the pages clicked earlier, that is, d i  X  d j for all this way, we can derive preferences among clicked webpages.  X  X  clicked page is more preferable to the next page in the list if it is not clicked, that is, d i  X  d i + 1 for all d i  X  C and d i + 1  X  C .

Joachims [2002] proposed a method for enhancing relevance ranking using the pref-erences learned from click-through data. More specifically, he trained a ranking-SVM model using the preference pairs as training data.

Dou et al. [2008] compared the effectiveness of using preferences derived from click-through data and using human labeled data to train a learning-to-rank model. Let click ( q , d ) be the aggregated click frequency of webpage d with respect to query q .For a query q , to derive the preference between a pair of webpages d 1 and d 2 , one can then d 1 is regarded more preferable to d 2 for query q . They found that the preferences derived from click-through data can be used as training data for learning-to-rank with the advantage of low cost.

In practice, a user may not represent her information need perfectly at the first place, and she may reformulate her query and conduct search again with the new query. Therefore, we can use a sequence of queries, called a query chain, and the corresponding clicks in a search session by a user as an instance in learning of preferences. Radlinski and Joachims [2005] proposed several rules for extracting user preferences from query chains and the corresponding click-through data. Those rules are extensions of the methods by Joachims et al. [2005]. The essential idea is that a user may likely look for the same information using two queries in the same query chain.

Joachims et al. [2005] reported that the probability of a webpage in a ranked list being clicked is heavily biased toward higher positions in the ranked list, known as position bias. Position bias may strongly affect the effectiveness of pairwise preference learning. Thus, it is important to develop position bias free methods for the learning task. Radlinski and Joachims [2006] gave a simple FairPairs algorithm as follows.
Let R = d 1 ,..., d n be the ranked list of webpages for a query. The FairPairs algo-rithm randomly draws a value k  X  X  0 , 1 } with equal probability. If k = 0, then, for all odd numbers i (1  X  i  X  n ), swap d i and d i + 1 in R with probability 0 . 5. Similarly, if k = 1, then, for all even numbers i (1  X  i  X  n ), swap d i and d i + 1 in R with probability 0 . 5. Then, the list R with the preceding changes is presented to the users, and the user clicks are recorded. When the pairwise preferences are extracted, for a swapped pair d and d i + 1 in which d i is clicked, d i is regarded more preferable to d i + 1 . The authors theoretically proved that the preferences extracted in this way are unbiased toward higher positions.

In addition to position bias, one issue in learning preferences is that very often users only consider the top ranked webpages and seldom evaluate the webpages at low positions, such as those outside the first page. Consequently, the click-through data recorded by a search engine in a passive way are strongly biased toward webpages that are already ranked high. Those webpages highly relevant to a query but initially ranked low may never be viewed or evaluated by any users. To overcome this prob-lem, Radlinski and Joachims [2007] proposed an active exploration approach. A na  X   X ve method may intentionally put unevaluated webpages in the top positions. Those un-evaluated webpages, however, may be irrelevant to the query, and thus may seriously hurt user satisfaction. The authors developed a principled approach to overcome the problem using a Bayesian method. The central idea is to present to users a ranked list of webpages that is optimized to obtain user feedback. Pairwise preferences are relatively easy to learn. Such preferences, however, may not generate a ranked list in general. For example, suppose that for webpages a , b ,and c , pairwise preferences a  X  b , b  X  c ,and c  X  a are learned from click-through data. The preferences cannot lead to a ranked list among the three webpages.

To overcome the problem, click models are learned and exploited, which can produce a ranking of webpages for a given query on the basis of the click-through data of the query. In other words, using a click model one can predict the preference of webpages with respect to the query. We review several click models called sequential click models here.

To learn sequential click models, one critical problem is coping with position bias. As we discussed, the probabilities of clicks are affected by the positions of webpages in the ranking list. Thus, the probability of a webpage being clicked in a sequential click model webpage u presented at position r is clicked by a user who issues query q .
Craswell et al. [2008] examined several sequential click models. They used as the where P ( a | u , q ) is the attractiveness of webpage u given query q .

They considered the examination hypothesis: users examine the webpages before they click and the examinations only depend on the positions of webpages. In this hypothesis, every position is associated with a probability P ( e | r ) of being examined. is a generalization of the baseline, since the latter can be obtained from the former by setting P ( e | r ) = 1.

Another hypothesis they considered is the cascade model, which assumes that users view search results in a top-down manner. Users make a decision on whether they click a webpage before they move to the next webpage. Under such an assumption, each webpage is either clicked with a probability P ( a | u , q ) or skipped with a probability 1  X  P ( a | u , q ). Moreover, a user who clicks never comes back, and a user who skips always continues until she clicks. Thus, we have Essentially, the cascade model captures the user behavior that sequentially examines all webpages in the result until a relevant webpage is found. After the first click, the search is regarded done, and the rest of the search result is abandoned.

Craswell et al. [2008] tested those hypotheses on real data and reported that the cascade model performs significantly better than the other models for prediction on clicks at higher positions, but slightly worse for prediction on clicks at lower positions.
There are several factors that these models do not consider. First, the relation be-tween a webpage and the webpages clicked so far is an indicator on whether the user is likely to click the webpage. Second, there is no distinction between navigational queries and informational queries. For navigational queries users tend to stop at the most relevant webpages, while for the informational queries, users tend to click multi-ple webpages.

To address those issues, Dupret and Piwowarski [2008] examined several user brows-ing models. In the single browsing model, the probability that a user examines a web-page depends on the distance from that page to the position of the last clicked webpage. This is based on the heuristic that a user tends to abandon a search after she sees less attractive snippets. Mathematically, they modeled both the attractiveness and exami-nation as Bernoulli variables.

Navigational queries and informational queries can be regarded as two extremes in a wide spectrum of queries. Queries may stay at different positions between the two extremes. To address the general varieties of queries, in the multiple browsing model, a mixture of single browsing models is built, and a latent variable is used to indicate which specific single browsing model is used for a particular query. Their experimental results on real data show that the single browsing model has clearly better performance.

Chapelle and Zhang [2009] proposed a dynamic Bayesian network to learn the preferences of webpages with respect to queries from click-through data. Figure 8 gives an illustration of the model. C i denotes whether a user clicked on a webpage at position i . Variables E i  X  1 , E i , E i + 1 denote whether the user examined the webpages at the three positions i  X  1 , i , i + 1, respectively. A i denotes whether the user feels that the snippet is attractive, and S i denotes whether the user is satisfied by the webpage. Variables a u and s u represent attractiveness and relevance, respectively. C i is the only observed variable. The variables in the box are repeated from positions 1 to n . The following equations hold in the model. The dynamic Bayesian network model can closely mimic the user X  X  search behavior. It is assumed that a user clicks a webpage if and only if she looks at the snippet and thinks that it is attractive (Equation 3(a)). The probability of being attracted depends only on the snippet of web page (Equation 3(b)). The user is assumed to sequentially scan the list of webpages from the top to the bottom until she decides to stop. If the user clicks and visits the webpage, then there is a probability that she will be satisfied with the webpage (Equation 3(c)). On the other hand, if she does not click, then she will not be satisfied (Equation 3(d)). Once the user is satisfied, she will stop the search (Equation 3(e)). If the user is not satisfied with the webpage, then there is a probability that the user will abandon the search (Equation 3(f)), and there is another probability that the use will examine the next snippet. If the user does not examine the snippet at position i , then she will not examine the subsequent positions (Equation 3(g)).
Some recent studies further improve the click models [Liu et al. 2009; Guo et al. 2009; Wang et al. 2010; Hu et al. 2011]. We describe two tasks in user understanding, namely, personalization and contextual-ization . Personalization typically builds a user profile to describe each individual user X  X  preference, while contextualization creates a context profile to capture the environment of a search activity. In both cases, log mining plays an important role.

Some researchers consider contextualization as a special case of personalization (e.g., [Dou et al. 2007]), because contextualization only takes into account users X  short search history, while others consider personalization as a special case of contextualiza-tion (e.g., [Pitkow et al. 2002; Wedig and Madani 2006]), because a user can be viewed as a part of the context in contextualization. A personalization method usually consists of two steps. First, it constructs a profile for each user using the content data as well as log data. Next, when a user u issues a query, it extracts the profile of the user and applies it into the ranking function. Various methods proposed for creating user profiles can be divided into three major categories, namely, click-based profiles , term-based profiles ,and topic-based profiles . 5.1.1. Click-Based Methods. Teevan et al. [2007] showed that users often repeatedly visit the same webpages by conducting searches at search engines. In other words, users submit the same queries and click on the same search results. Click-based per-sonalization methods take into account the fact and promote the rank of a page p with respect to query q for user u if evidence from search log data shows that page p is often searched by user u with query q . See also Bennett et al. [2012].
 For example, Dou et al. [2007] defined the following click-based ranking score, where S ( q , p , u ) is the personalized relevance score of document p with respect to query q and user u , click ( q , p , u ) is the number of times user u clicks on document p with respect to query q in the log data, click ( q ,  X  , u ) is the total number of times user u clicks on documents with respect to query q ,and  X  is a smoothing factor. The more times document p is clicked by user u with respect to query q , the higher personalized relevance score p receives. In practice, this ranking model does not work for new queries and suffers from data sparsity.

To address the problem of data sparsity, Dou et al. [2007] proposed borrowing the idea of collaborative filtering and using the information from other users to conduct smoothing on the relevance scores. If user u who is similar to user u searches with query q before, then the click information from user u can be leveraged to estimate the personalized relevance score in Equation (4) can be redefined as It means that the more similar user u s is to user u , the more likely the clicked pages by u s are also clicked by u , and thus the higher relevance score document p has. The similarity function sim ( u s , u ) in Equation (5) can be defined in different ways. Dou et al. [2007] classified webpages into predefined topics and learned each user X  X  preference on the topics using the pages visited by the user. The similarity between users is then determined by the similarity between their topic preferences.

Sun et al. [2005] proposed the CubeSVD algorithm, which conducts three-order singular value decomposition on the query-document-user cube. We can also use the algorithm to calculate the relevance score of a document with respect to a query and a user, which turns out to be another click-based method. CubeSVD employs the more general higher-order singular value decomposition (HOSVD) [Lathauwer et al. 2000] and is an extension of Latent Semantic Analysis [Deerwester et al. 1990]. Specifically, the CubeSVD method builds a three-mode tensor C  X  R l  X  m  X  n from the log data, where l , m ,and n are the numbers of users, queries, and documents, respectively, and each element C uqp (1  X  u  X  l ,1  X  q  X  m ,and1  X  p  X  n ) denotes the number of times document p is clicked by user u with respect to query q . The method then calculates the core tensor S from C using HOSVD. The core tensor S can capture the major latent factors among the users, queries, and documents in C . Finally, the CubeSVD method derives a new tensor  X  C from the core tensor S .Anelement  X  C uqp in the new tensor  X  A represents the personalized relevance score of document p with respect to user u and query q . Since the correlation among users is encoded in the core tensor S , even if user u never raises query q , her preference on page p with respect to query q can still be estimated. For other related work see also Jin et al. [2004]. 5.1.2. Term-Based Methods. Compared with click-based methods, term-based personal-ization methods are more robust to sparse data. They typically create a profile for each user through documents visited or queries issued by the user and integrate it into the ranking model BM25 [Jones et al. 1998] or language model [Lafferty and Zhai 2001].
Teevan et al. [2005] created a profile for each user u , consisting of tuples ( t i ,w u i ), where t i is a term and w u i is the weight of term t i with respect to user u . This profile is then applied into the BM25 model to rerank search result. The BM25 score of document d with respect to query q and user u is defined as where t i is a term in query q , tf i is the term frequency of t i in document d , k 1 is a constant, and w u i is the term weight with respect to user u , calculated in the same way as in relevance feedback where N is the total number of documents in the corpus, n i is the number of documents containing w i , D u is the set of documents browsed by user u ,and D u i is the subset of documents in D u that contain term w i . Their method assumes that the pages browsed by user u are relevant to u , either explicitly or implicitly judged by the user.
Tan et al. [2006] built personalized language models. Suppose that query q i is sub-mitted by user u . The method finds in the user X  X  search history H u all queries q j that user u asked before. For each query q j , the method constructs a language model  X  j from both the clicked and unclicked search results of q j . It then uses the personalized language models in search.
 The retrieval framework based on language models is formally defined as where  X  i and  X  d , respectively, are the language models for query q i and document d , and D (  X  i ||  X  d ) is the Kullback-Leibler divergence between  X  i and  X  d . The major idea of their method is to replace the query language model  X  i in Equation (6) by the personalized language model  X  u i , which includes user X  X  history information H u . The probability distribution of  X  u i is specifically defined as where  X  i is a parameter between 0 and 1, and  X  h i is the language model constructed from user u  X  X  search history H u .Let H u ={ q 1 ,..., q k } ,  X  h i is defined as the weighted sum of the language models of the queries in H u , normalized by the sum of the weights. Then, where the language model  X  j for each query q j can be estimated from both the clicked and unclicked documents of q j , and the weight  X  j for model  X  j depends on the similarity between q i and q j . The more similar q j is to q i , the more influence  X  j has on the personalized model  X  u i . 5.1.3. Topic-Based Methods. The term-based methods may not be applicable to a query and a user if none of the terms in the query occurs in the user X  X  search history. In such cases, we may consider employing topic-based methods.

A topic-based personalization method creates a topic profile for each user. In gen-eral, a topic profile  X  u for user u is represented by a vector, where each element  X  u [ c i ] indicates the probability that the user is interested in a particular topic c i . The prob-abilities are estimated from the user X  X  search and/or browse history. For example, we may collect the terms in the user X  X  queries, clicked documents, and browsed documents, and then apply conventional text classification techniques (e.g., Berry [2003]) to infer the user interests on a set of topics. Once the user profiles are created, different topic-based methods mainly differ in how they integrate the topic profiles into the ranking function.

Pretschner and Gauch [1999] adopted the Magellan hierarchy as the set of topics. The hierarchy consists of 4,400 nodes (i.e., topics), and each node is associated with a set of documents. Their method builds a TF-IDF vector V ( c k ) for each node c k using its associ-ated documents. To construct the topic profile  X  u for user u , the method first collects all the documents d visited by the user and then estimates the probability P d [ c k ] of each document d belonging to each topic c k . Finally, the user X  X  preference on topic c k is de-rived by aggregating the probabilities P d ( c ) over all the visited documents. Given query q , the personalized ranking score for document d with respect to user u is defined as where S ( q , d ) is the nonpersonalized ranking score, such as that generated by BM25,  X  ( c k ) is the value for topic c k in the topic profile of user u , c 1 ,..., c K are the top K topics with the largest values  X  u ( c k ), V ( d )and V ( c k ) are the vector space models of document d and topic c k , respectively, and  X  (  X  ,  X  ) is the cosine similarity between two vector space models. Obviously, the more the topic distribution of document d matches with the topic profile of user u , the higher personalized score document d receives. See also Speretta and Gauch [2005] for other related work.

Qiu and Cho [2006] exploited the first level of ODP and constructed a topic profile  X  u for each user u from the documents clicked by the user. They then used the profile in calculating the personalized importance score of page d with respect to user u , given by where TSPR k ( d ) is the topic-sensitive page rank [Haveliwala 2002] of page d with more the user u is interested in topic c k and the higher topic-sensitive page rank score is for page p with respect to topic c k , the more likely page p is important to the user. Note that S ( d , u ) represents the importance of web page d and does not depend on a query.
On the one hand, topic-based methods may be applicable to more queries than click-based and term-based methods. The larger coverage is due to the abstraction of user preference at the topic level. On the other hand, topics may be too coarse to represent users X  search needs. Consequently, the accuracy of topic-based methods may not be as high as that of click-based and term-based methods [Dou et al. 2007]. For other topic-based methods, please see Liu et al. [2002]. Contextualization is a complementary task to personalization. Instead of building a user profile for each individual user, contextualization creates a context profile to cap-ture the environment of each search activity. Personalization methods take into account an individual user X  X  preference over a long search history. In contrast, contextualization methods take into consideration difference users X  preferences in similar short search histories.

Figure 9 shows the process of the contextualization approach. Suppose that a user raises query q t . A contextualization method characterizes the search context and leverages the context information in search. Different contextualization methods may consider different aspects of the search context, such as the time of the search, the loca-tion of the user, the search device from which the query is issued, and the user behavior before raising the current query. In this section, we review several studies that focus on user behavior. User behavior is the most intensively studied context information in the state-of-the-art literature. More specifically, given a query q t , the contextualization methods consider the immediately previous queries, clicked documents, and/or browsed documents that are within the same session of q t as the context .
Given the context information, we may follow two strategies. The first strategy cre-ates a profile directly for the context and integrates the profile into the ranking function. In this case, contextualization is very similar to personalization. The major difference is that contextualization considers a user X  X  short history within a session, while per-sonalization considers a user X  X  long history over several days or even longer time. Therefore, all the personalized techniques discussed in Section 5.1, including the click-based methods, term-based methods, and topic-based methods, are applicable here.
For example, [Shen et al. 2005a, 2005b] proposed two contextualization methods using language models. The basic idea is to incorporate the context information of the current query, including the previous queries and the clicked documents, into the query language model. The methods rank the documents high that are not only similar to the current query but also similar to the previous queries and the clicked documents in the same session. The authors evaluated their context-aware model with a small TREC dataset ( http://trec.nist.gov ) and confirmed the effectiveness of their approaches.
Xiang et al. [2010] proposed several context-aware ranking principles for Web search, including one based on clicks, two based on terms, and one based on topics. The authors evaluated the ranking principles on large-scale search log data and made two observa-tions. First, at the click level and term level, users are more likely to favor the search results that are different from the contexts. Second, at the topic level, users tend to click on the search results that are consistent with the contexts.

One challenge with this contextualization strategy is that the context information is usually very sparse. For example, a user typically raises only two or three queries in a session. Moreover, for each query, a user may click on only one or two URLs. Therefore, the context information from a single user may not be sufficient to create an effective context profile. To address this challenge, the second strategy for contextualization creates context profiles by summarizing the common behavior of many users in log data.
Cao et al. [2009] characterized users X  search behavior in a session with a variable length hidden Markov model (or vlHMM for short) and then conducted contextualiza-tion with the vlHMM model. In the vlHMM model, the sequence of queries and clicked URLs in a search session is assumed to be a sequence of observations, and a hidden sequence of states representing topics is assumed to exist behind the sequence of ob-servations. The generative process of the two sequences is determined by the vlHMM model. To learn the vlHMM model, the method first discovers states (topics) by clus-tering queries and URLs in the click-through bipartite graph using a method by Cao et al. [2008], and then takes the clusters as states. The method then employs the EM algorithm to estimate the parameters of the vlHMM model under the Map-Reduce programming mode [Dean and Ghemawat 2004, 2008]. In contextualization, given a query q t and its context O 1 ,..., O t  X  1 , the user X  X  search topic can be inferred using the conditional probability distribution of s t given the query and the context. Once s t is in-ferred, the search result can be reranked accordingly. Furthermore, query suggestions and URL recommendations based on the context information can also be carried out.
Other studies related to personalization and contextualization are available [Wedig and Madani 2006; Teevan et al. 2008; White et al. 2009; Matthijs and Radlinski 2011; Tyler and Teevan 2010]. Another important way of using log data is to evaluate the effectiveness of the search system. In traditional IR, such an evaluation is usually performed on the basis of human labeled data, for example, by taking the Cranfield approach [Cleverdon 1967]. The traditional IR approach faces several challenges when it is applied to Web search. First, the cost of data labeling is high, and the coverage tends to be small. Furthermore, since human judges are not the searchers who submitted the queries, it is often difficult for them to make judgments on the relevance of search results. In recent years, people proposed taking search log data as users X  implicit feedback and evaluating the effectiveness of a search system using search log data. By mining and analyzing log data, we can assess how users think about that their information needs are satisfied. In this Section, we review three representative methods that use search log data to predict users X  satisfaction. All the three methods make use of log data in search sessions but with different models, including a sequential pattern model [Fox et al. 2005] and Markov chain [Hassan et al. 2010]. See also Piwowarski et al. [2009] for other related work. 6.1.1. Sequential Pattern Model. Fox et al. [2005] defined a vocabulary of five letters to represent various user activities in search sessions, as shown in Table I. With the vo-cabulary, a search session can be represented by a sequence of letters. For example, the sequence SqLrZ represents a simple session in which a user starts the session, raises a query, receives a search result page, makes a click on a URL, and leaves the session. The authors then represented all user sessions as sequences of letters and mined frequent patterns from the sequences. To summarize similar patterns, the au-thors allowed a wild card symbol  X * X  to match any letter or letter sequence. Finally, the authors investigated the relations between the mined frequent patterns and user sat-isfaction. The user satisfaction data was collected after the users conducted searches. Some interesting patterns are listed in Table II.

The first behavioral pattern SqLrZ in Table II indicates that a user clicks on a result in the search result page and exits the session immediately. Five hundred and nine sessions in the data follow this pattern. Among the sessions, 81% of the sessions were explicitly labeled by the users as satisfied, 10% as partially satisfied, and only 7% as dissatisfied. Therefore, pattern SqlrZ has a strong positive correlation with user satisfaction. In other words, a session that follows this pattern likely may make the user satisfied. In contrast, the last behavioral pattern SqLr Lr Lr Lr  X  indicates that a user keeps browsing search result pages. For this pattern, only 13% of the sessions were labeled as satisfied, 35% as partially satisfied, and 51% as dissatisfied. The pattern has a strong negative correlation with user satisfaction. 6.1.2. Markov Chain Models. Hassan et al. [2010; Hssan 2012] extended the vocabu-lary of search activities defined by Fox et al. [2005] and constructed a Markov chain to model user behavior in search sessions. They used a vocabulary V ={ Q , SR , AD , RL , SP , SC , OT H } , where Q represents submission of a query by a user, SR represents a click on a search result, AD represents a click on an advertisement, RL represents a click on a query suggestion, SP represents a click on a spelling suggestion, SC represents a click on a deep link, and OT H represents any other click. They then took search sessions as sequences of activities and constructed a Markov chain on the sequences. The Markov chain is defined as a tuple ( V , E ,w ), where V is the vocabulary of letters denoting user activities, E is the set of transition edges between activities, and w is the set of transition probabilities associated with the edges. In training, the transition probability from activity s i to actitivity s j is estimated from log data using maximum likelihood estimation P ( s i , s j ) = N s i , s j N transition from s i to s j is observed in the log data, and N s s is observed.

They assumed that user satisfactions on search sessions can be labeled by human judges. Specifically, a judge can go through a user session and infer whether the user X  X  information need is fulfilled. The judge can then label the session as successful or not. The authors then learned two Markov chains, denoted by M s and M f , respectively, from all the sessions labeled as successful and unsuccessful. Given a user session S = s Markov chain LL M model M f can be calculated. Finally, the session is predicted as successful if the ratio of LL M
The authors further incorporated time features into the Markov model. The basic idea is that the distribution of transition time between activities can be very different in successful and unsuccessful sessions. For example, Figure 10 shows the distribution of time for the transition from activity SR to activity Q for successful sessions and unsuccessful sessions. The two curves have very different shapes and thus can be leveraged to differentiate successful and unsuccessful sessions. The authors assumed that the transition time follows the Gamma distribution and estimated the parameters from successful and unsuccessful sessions, respectively. Finally, the time distribution can be either used independently or integrated into the transition probabilities to make predictions on the success of sessions. 6.1.3. Discussion. The studies discussed in this section have three major differences from those in Section 4. (1) The purpose of the models in Section 4 is to predict the relevance of documents (2) Most of the models in Section 4 focus on the user behavior with respect to a single (3) The models in Section 4 mainly consider users X  click-through information. In con-In this article, we presented a survey on search and browse log mining for Web search, with the focus on improving the effectiveness of Web search by query understanding, document understanding, document ranking, user understanding, and monitoring and feedback. As reviewed, many advanced techniques were developed. Those techniques were applied to huge amounts of search and browse log data available at Web search engines and were powerful in enhancing the quality of the search engines.

There are still many challenging and interesting problems for future work. We list three of them here as examples.

First, it is challenging to deal with the long tail in search and browsing log effec-tively. Search and browse log data are user behavior data and follow the power law distributions in many aspects. Usually it is easy to mine useful knowledge from the head part of a power law distribution (e.g., [Spink et al. 2002]). How to propagate the mined knowledge from the head part to the tail part is still a challenge for most log mining tasks.

Second, it is important to leverage other information or knowledge in mining. Log mining mainly focuses on the use of log data. It would be helpful to leverage information or knowledge in other data sources during the mining process, such as Wikipedia. It is necessary to conduct more research on log mining in such a setting.

Last, privacy preserving log mining remains a grand challenge. In 2006, AOL re-leased a search log dataset, but unfortunately, a privacy issue arose in the data release. The identity of a user could be detected from the data, although certain data processing had been done in advance [Barbaro and Zeller 2006]. How to preserve privacy in log data and not sacrifice the utility of the log data in the meantime is a critical research issue.
