
This work addresses the task of recommending high qual-ity tags by exploiting not only previously assigned tags, but also terms extracted from other textual features (e.g., title and description) associated with the target object.To esti-mate the quality of a candidate tag recommendation, we use several metrics related to both tag co-occurrence and information quality. We also propose a heuristic function to combine the metrics to produce a final ranking of the recommended tags. We evaluate our heuristic function in various scenarios, for three popular Web 2.0 applications. Our experimental results indicate that our heuristic func-tion significantly outperforms two state-of-the-art tag rec-ommendation algorithms.
 H.3.5 [ Information Storage and Retrieval ]: Online Information Services Algorithms, Experimentation
Tag Recommendation, Information Quality
Tagging has become a popular activity on the Web, par-ticularly with the advent of Web 2.0 applications, which fa-cilitate and stimulate the user participation in content cre-ation. This content, typically multimedia, brings challenges to current multimedia Information Retrieval (IR) methods, not only due to the scale of collections and upload rate, but  X  also due to the (usually) poor quality of user-generated ma-terial [2]. Tags, among other textual features such as title and description, commonly associated with objects in Web 2.0 applications, offer a good alternative for organizing, dis-seminating and retrieving this content [3].

In this context, tag recommendation aims at improving the quality of the available content by suggesting terms which describe the associated object more correctly and com-pletely. Most existing tag recommendation strategies focus on term co-occurrences in previous tag assignments in the collection, expanding an initial tag set I o of an object o with other tags that frequently co-occur with the terms in I o [5, 8, 4]. Other methods exploit terms extracted from other textual features [7, 9]. However, these features pro-vide a large but inaccurate term candidate set [7]. Thus, it is necessary to filter and consider only the terms with higher quality, that is, more appropriate for recommendation.
In this paper, we model tag recommendation as a multi-ple term candidate ranking problem. In other words, we de-velop a ranking function that ranks candidate terms that are more appropriate to the target content in higher positions than the lower quality terms. The assumption is that higher quality terms are more relevant for the tag recommendation task. Thus, the solution of our target problem is a function that estimates the relevance of a candidate term as a tag recommendation for the target object, thus enabling us to recommend the most relevant terms as tags for a content.
Our ranking function extends a previously proposed strat-egy, called Sum + [8], which expands an initial pre-assigned tag set exploring only tag co-occurrences and tag frequency. In addition to considering previously assigned tags, our ap-proach also explores terms extracted from other textual fea-tures, namely, title and description, as candidates for recom-mendation. It also uses the content of these textual features to estimate the descriptive power of each candidate term, i.e., how related it is to the associated content, by means of a heuristic metric called Term Spread [3]. Thus, our heuristic, called Sum + TS , builds on the original approach by applying a metric related to quality of information and by considering other textual features.

We compare Sum + TS with two state-of-the-art tag rec-ommendation strategies, namely, the original Sum + ,and the winner of the 2009 ECML Discovery Challenge [7], re-ferred here as Co-occurrence and Text-based Tag Recom-mender (CTTR). Unlike Sum + , CTTR exploits only the content of textual features associated with the target ob-ject, and thus does not consider pre-assigned tags.
We evaluated the three approaches using datasets col-lected from three popular Web 2.0 applications, namely, the video sharing sites YouTube 1 and YahooVideo 2 ,and the online radio station LastFM 3 . We find that Sum + TS outperforms Sum + and CTTR by up to 74% and 137% in precision, respectively. These results indicate that jointly exploiting both tag co-occurrence patterns and terms ex-tracted from other textual features, filtered by metrics that capture their quality, can significantly improve tag recom-mendation effectiveness over state-of-the-art strategies that consider only one of the two sources of content.
Most previous text based tag recommendation strategies exploit terms extracted from the object metadata and tag co-occurrence patterns in order to suggest tags for the user. Heymann et al . [5] use association rules to extract tag co-occurrence patterns, exploiting them to expand an initial tag set I o associated with object o . Although the authors restrict the rules used by a confidence threshold, they do not provide a ranking of the recommended tags. Sigurb-jornsson and Zwol [8] propose to use global metrics of tag co-occurrence (some of which are related to tag frequency) such as confidence and Jaccard coefficient to produce a final ranking of recommended tags. In comparison, we here con-sider tag frequency as well as other quality-related metrics. Garg and Weber [4] study the problem of making personal-ized tag recommendations using the history of all tags a user has applied in the past, whereas Krestel et al . [6] use Latent Dirichlet Allocation to expand tag sets of objects annotated by only a few users.

Other efforts do not exploit previously assigned tags, fo-cusing, rather, on other data sources [7, 9]. For instance, Lipczak et al . [7] extract terms from the title and from other textual features, expanding them through association rules. A similar study is presented in [9], but they use the traditional TF  X  IDF metric to extract and rank the most important terms from the object textual content.

In comparison with the literature, the present work is, to the best of our knowledge, the first to jointly exploit an initial tag set and other textual features of the target object, applying metrics related to information quality to mitigate the impact of low quality user generated content.
The tag recommendation problem is stated as follows: given a tag set I o previously assigned to an object o ,andthe set of textual features (other than tags) F o = {F 1 o , F 2 where each element F i o is the set of terms of the textual fea-ture i associated with the object o , generate a candidate set C , and recommend the k most relevant terms of this set.
Many tag recommendation strategies exploit co-occurrence patterns by mining relations between tags assigned to the same object in an object collection. The process of learn-ing such co-occurrence patterns is defined as follows. There is a training data set D = {I d , F d } ,where I d ( I d = contains all tags assigned to object d ,and F d contains the sets of terms of the other textual features associated with d . There is also a test set O , which is a collection of objects {I o , F o , Y o } ,whereboth I o and Y o are sets of tags associ-ated with object o . However, while tags in I o are previously known (and given as input to the recommender), tags in Y o are assumed to be unknown and are taken as the relevant recommendations to the target object.

Thus, we define associative and text based tag recommen-dation methods as algorithms that estimate the relevance of a tag candidate set relying on the elements described above. Next, we present three metrics -Sum , Stability [8], and Term Spread , which can be used to estimate the relevance of a term candidate for recommendation. The latter was proposed in [3] for assessing the quality of textual features, although its use for tag recommendation is novel.
Tag recommendation approaches based on co-occurrence usually exploit association rules, that is, implications of type x  X  y , where the antecedent x is a tag and the consequent y is a candidate tag for recommendation. The importance of an association rule is estimated based on: (1) support (  X  ), which is the number of co-occurrences of x and y in the training data set, and (2) confidence (  X  ), which is the conditional probability that y is assigned as a tag to an object d  X  X  given that x is also a tag associated with d .
Given that the total number of rules mined from D can be very large, and that some of them may not be helpful to the recommendation, support and confidence thresholds (  X  min and  X  min , respectively) are used as lower bounds to select only the most frequent and/or reliable rules. We here assume that the Apriori [1], a classical data mining algo-rithm, is used to efficiently generate a set of rules R given thresholds  X  min and  X  min . This step is performed offline, over the training data set D .

At recommendation time, we take the available tags from the target object o ( I o =  X  ), and select the rules whose an-tecedents are included in I o .Foreachterm t which appears as consequent of any of the selected rules, we estimate the relevance of t as a possible tag for object o as the sum of the confidence values of all rules containing t :
Terms that are very common, such as  X  X ideo X  X n a YouTube object collection, are too general, thus representing low qual-ity recommendations. On the other hand, very rare terms may be too specific or may represent noise, i.e., misspellings, neologisms and unknown words, with little use for recom-mendation purposes. Given a candidate tag t , Sigurbjorns-son et al. [8] propose the metric Stability, which gives more importance to tags with intermediate frequency: where f t,tag is the number of objects in D containing t as a tag, and k s is a parameter that represents the  X  X deal fre-quency X  of a term and must be adjusted to the data collec-tion. Weherealsousethe Stab metric to assess the quality of a candidate tag. However, unlike [8], we apply this metric not only to tags but also to terms extracted from all consid-ered textual features F o associated with the target object o . We note, however, that even though term t may be extracted from other textual features, the frequency f t,tag refers to the number of objects that contain t as an associated tag.
Whenever other textual features are available to describe a content, we can exploit a heuristic metric called Term Spread [3] to estimate the descriptive power of a term t that appears in (at least) one of the textual features of the target object. The Term Spread of a term t in an object o , TS ( t, o ), is given by the number of textual features (except tags, in the tag recommendation context) of o which contain t .
The assumption behind TS ( t, o )isthatthelargerthe number of textual features of an object o that contain a term t , the more related t is to o  X  X  content. For instance, if the term  X  X ting X  appears in all features of a video, there is a high probability that the video is related to the famous singer. The maximum TS value is given by the number of considered textual features, other than tags. We do not consider the tags feature because it does not make sense to use terms already assigned as tags (i.e., terms in I o )ascan-didates for recommendation and because our strategy does not use any knowledge about Y o . In our evaluation, the maximum TS is 2, as we consider title and description.
In this section, we introduce the three tag recommenda-tion strategies considered in our work. The first method, originally proposed in [8], exploits only tag co-occurrence patterns to expand an initial tag set I o . The second method, CTTR, which is not described here due to space limita-tions, is an adaptation of the proposal in [7], which considers terms extracted from other textual features associated with the target object, although it does not consider pre-assigned tags. Our implementation of CTTR is an adaptation of the original proposal as we disregard user information, since our focus is not on personalized recommendations.
The first baseline we consider is the best function proposed in [8], called Sum + . Sum + extends the Sum metric by weighting the confidence values by the Stability of the terms in the antecedent and in the consequent of the corresponding association rules. Let c be the candidate term for object o . Sum + function is defined as follows: where k x , k c and k r are tuning parameters. Rank ( c, o, k is employed to make confidence values decay smoother. It is the position of c in the ranking of candidates according to the confidence of the corresponding association rule. Our new ranking function combines the weighted sum of Eq. 3 and the Spread of a candidate term. Let c be a candidate term to the object o . Sum + TS is defined as: The  X  parameter weights the two factors, Sum + and TS . We note that the Sum + factor is computed only over can-didates generated from the tag co-occurrence rules, whereas TS is computed for terms extracted both from these rules and from other textual features.
To evaluate the recommendation methods, we collected four textual features, namely title , tags ,and description , associated with objects from LastFM, YouTube and Ya-hooVideo. Our crawling strategy follows a snowball sam-pling , that is, from a selected set of objects used as seeds, the crawler recursively follows the links to related objects, collecting them. We used as seeds: the artists related to the most popular tags in LastFM and the all time most popu-lar videos in YouTube and YahooVideo. Our collected data contain the textual features associated with 99,161 LastFM artists, 180,778 YouTube videos and 160,228 YahooVideo videos.

For each collected feature, we used the Porter Stemming algorithm to remove the affixes of each word, thus avoiding trivial recommendations such as plurals and other simple variations of the same word. Moreover, we removed stop-words, as well as terms that are either too frequent (with more than 10,000 occurrences in the collected sample) or too rare (with less than 30 occurrences in the collected sam-ple), since such terms are hard ly good recommendations [8].
Similarly to [4], we used a fully-automated methodology to evaluate the considered recommendation strategies. The idea is to use a subset of the object X  X  assigned tags as an expected answer for the recommendation task, that is, as the relevant tags for that object. As such, these previously assigned tags should be disregarded by the recommenda-tion functions when computing the metrics presented in Sec-tion 3. This methodology was adopted because the man-ual evaluation of the relevance of the recommended tags is a very expensive process that may be affected by the subjectivity of human judgments. We note that the re-sults obtained according to the proposed methodology repre-sent lower bounds for the relevance of the recommendations, since it is possible that some of the returned tags, which are not in the expected answer, are indeed relevant for an object.
The relevance of a recommendation is evaluated in terms of precision in the top k positions of the rank ( P @ k ). Let Y o be the set of relevant tags for an object o , C o be the ranked recommendations and C k,o the top k elements of C o Precision is defined as P @ k ( C o , Y o )= |C k,o  X  X  o |
Following the proposed methodology, we evenly divided the tags associated with each object o = I o , F o , Y o into the groups I o and Y o . Half of the tags (randomly chosen) were included in I o and the other half were included in Y as expected answer. In addition to tags, we used title and description as textual feature in F o .

In order to evaluate the considered methods for different amounts of training data, we divided each collection into three object subsets based on the number of tags available in the object. This division reflects real scenarios, allow-ing the assessment of the effectiveness of the recommenders under different amounts of previously available tags in the application data. We chose the intervals so that each sub-set contains approximately the same number of objects, as shown in Table 1. Next, we randomly sampled 20,000 ob-jects from each subset. Each sample was divided into five parts, each with 4,000 objects, which were used in a five-fold cross validation. In other words, three parts are used as training set ( D ). A third part is used as validation set, used for parameter tuning. The last part is used for testing.
In this section, we show the most relevant results of the comparison of Sum + , Sum + TS and CTTR . All results are averaged over 5 folds (either test set or validation set). For all experiments, we computed 95% confidence intervals, which indicated a maximum error of only 2% of the mean. Table 2: P@5 and 95% confidence intervals. Best results in bold.

Our evaluation started with a series of experiments with the validation set to find the best parameters of each method. The results of such experiments are ommitted due to space constraints. Nevertheless, we found that, for both Sum + and Sum + TS , the best parameters were k r =1, k x =5, k Best results for  X  varied between 0 . 8and1 . 0. In most cases, precision decreases as the values of  X  min and  X  min increase, particularly when fewer tags are provided as input, since the number of generated candidates is already small in this case. However, when the number of input tags is larger, the choice of thresholds can be more aggressive without signif-icantly affecting precision. Thus, for all three methods, we set  X  min =2 for all data collections,  X  min =0 . 01 for objects with the smallest range of number of tags,  X  min =0 . 1forthe medium range, and  X  min =0 . 3 for the largest range.
Table 2 shows results (in terms of precision in the top 5 ranked candidate tags) for the three methods applied to the test set, configured with best parameter values. Com-paring Sum + and Sum + TS , the inclusion of the TS met-ric led to great improvements in effectiveness, particularly in YahooVideo and YouTube, in which precision increased up to 72-74%. In LastFM, the contribution of TS is not very significant. This is due to the large diversity between LastFM textual features, in relation to YahooVideo and YouTube. That is, there is little overlap between title, de-scription and tags in LastFM [3]. This fact leads to a highly concentrated distribution of TS around small values (0 and 1), which makes it difficult to distinguish  X  X ood X  from  X  X ad X  terms based solely on this metric in that application.
Comparing Sum + TS and CTTR , the benefits of Sum + TS are even greater (gains up to 137%). The main reason (be-sides the use of the TS quality metric) for those gains is the use of pre-assigned tags in Sum + TS , which leads to the se-lection of the (possible more) reliable tag co-occurrence rules for the target object. As terms used by CTTR are extracted from other features, and thus may be noisier, their use as antecedent may produce rules that are not as good as those generated from pre-assigned tags (as done by Sum + TS ).
Table 3 shows an example of the top recommended tags by each strategy, given a set of input tags. The example corre-sponds to a YouTube video in which an American voter talks about his favorite candidate in the last presidential election. Although all strategies were able to correctly predict part of the expected answer, Sum + function could only suggest two tags,  X  X arack X  and  X  X bama X , since these are the only tags that co-occur frequently with the input tags  X  X oubama X  and  X  X ww.youbama.com X . Sum + TS could suggest more relevant and specific terms, such as  X  X ote X  and  X  X ight X , extracted from other textual features and ranked also according to the de-scriptive power of a term. CTTR recommended not only relevant tags in the top positions, but also unrelated tags such as  X  X atur X  and  X  X nim X  (omitted in Table 3 due to space limitations), which were obtained from co-occurrences be-tween low quality terms of the target object and tags. Table 3: Top stemmed tag recommendations. Rec-ommendations in the expected answer are in bold.
We have proposed and evaluated a novel tag recommen-dation heuristic strategy that exploits, in addition to tag co-occurrence statistics, information quality metrics applied to terms extracted from other textual features. Our exper-iments, performed over data collected from 3 popular Web 2.0 application, shows that, our strategy can lead to im-provements, in terms of precision, of up to 74% and 137%, over the Sum + and CTTR baselines. As these results rep-resent lower bounds, we intend, in the future, to perform a manual assessment of the methods. We also plan to exploit other quality-related metrics as well as metrics that capture the users X  interests for personalized recommendations.
