 Optim ising the paramet ers of ranking functi ons with respect to standard IR rank-dep enden t cost functi ons has eluded satisfact ory analyti cal treatm ent. We build on recent ad-vances in alternat ive di eren tiable pairwise cost funct ions, and show that these techniques can be successfull y applied to tuning the param eters of an existing family of IR scor-ing funct ions (BM25), in the sense that we cannot do better using sensibl e searc h heuri stics that direct ly optim ize the rank-base d cost functi on NDCG. We also demonstrate how the size of training set a ects the number of parameters we can hope to tune this way.
 H.3.3 [ Informat ion Systems ]: Inform ation Storage and Retrieval| infor matio n search and retriev al Experimentation evaluati on, optimisation, e ect iveness measures, ranking, scoring
Traditional retriev al functi ons have very few free param -eters , but nevertheless these paramet ers need to be set to Cop yright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. some value, and this choice a ects the resulti ng performanc e. In order to determine the optimal value of the free param -eters , experiments are typically run testing very many pa-rameter values on some traini ng set and selecting those that lead to the best performanc e with respect to the measure of choice. Such an exhaustiv e searc h for the optimal parame-ter values is both simple and extremely e ective: it can be applied to any combination of retrieval funct ion and perfor-mance measure . However, the number of values that need testing grows exponen tially with the number of parameters. For this reason, exhaus tive searches can only be carried out for very few param eters. Beyond that, exhausti ve search is impossibl e and we need to use heuristi cs or a learning algorit hm to direct the search for optimal parameter values. This results in a dicult situation well known to most IR researchers: we add features to our retriev al functi ons to impro ve their perform ance, but in doing so we increas e the number of unkno wn param eters, which makes it harder to nd good param eter values, which in turns hurts perfor-mance. In parti cular we consi der that negati ve results (e.g. adding this feature did not help ...) make sense only if we be-lieve that we have reached the global maxim um of the new ranking functi on with respect to the perform ance meas ure of choice.

For these reasons the lack of ecient learni ng algori thms for retrieval functi ons (and retrieval perform ance measure s) seems to us one of the bottlenec ks of IR research today. In this paper we do not propose a solution to this problem, but rather, we discuss and test two approac hes available to us today: tradi tional greedy searc hes, and an extension of the gradi ent descen t approac h recen tly prop osed in [4]. In this paper, we apply these to training sets of up to 2048 queries. What is a typical traini ng set size? This depends very much on the environm ent of the experiment. Our evaluation data is taken from a commerci al Web search engine; it is reason-able to suppose that all such searc h engines typically collect and use very large training sets, considerably larger than those used here. On the other hand, a comm on TREC sit-uation is to have 50 queries to train on, at the lower end of our range.

Our guess is that there are relatively few environm ents in which very large traini ng sets are feasibl e { outside of the Web searc h engines, and possibly a few other publi c search services and the intranets of a handful of large companies. In many environm ents, the questio n of whether one can rea-sonably train on less than (say) 100 queries may be critical to any traini ng exercise. The choice of training set sizes in this paper is inform ed by such considerations.
 First we summ arise some fundam ental points about param -eters and optim isation, including a brief review of some of the methods used in search engine tuning. Next we discuss the measure of e ect iveness we use: a prim ary metri c which we really wish to optimise, and a substitute metric used in the tuning process. Then we describe the tuning metho ds and procedures , for which we require gradi ents (derivatives) of the metric with respect to the param eters of the model. The BM25 F ranking function, which is a componen t of the search engine, has non-linear paramet ers; the gradi ents for this comp onen t are established in the Appendix. In sections 3 and 4, we present experiments and results, with di erent size models (number of param eters ) and di eren t size tran-ing sets.

The prim ary contributions of the paper are as follows: In the course of this work, we compare two alternative meth-ods of optimisation, and show that a relatively fast machine learni ng method can perform just as well as a na X ve search of paramet er space. We also show that signi can t bene t can be gained from using a large number of weak features, over and above the usual range of features used in typical TREC web track work.
A model, in this case a ranki ng model for informat ion retrieval, will have some number of free parameters. If the model has too few free paramet ers, then its e ect iveness is limited, because even given a large amount of extra training data it cannot improve. It has too few degrees of freedom . If the model has too many free parameters , the danger is over tting. Over tti ng gives very good perform ance on the training data, but the system does not generali se to new examples, so will perform poorly on a test set of queries. [1]
A typical machine learni ng approac h would be to err on the side of having a powerful model with many parameters, to take advantage of the available training data, and to then empl oy additional techniques to avoid over tting. One such technique is early stoppi ng, to which subject we will return in section 2.5.

A fundam ental question for informa tion retrieval is whether these machine learning approac hes, using large numbers of param eters and large amounts of training data, can give better retriev al e ectiveness than approac hes with few pa-rameters. A signi can t barri er to study in this area has been nding ecient metho ds for exploring large parameter spaces , because rank-base d e ecti veness metrics do not lend them selves to ecient tuning. This is discuss ed in detai l in the remainder of the paper.

There are two general approac hes used by the IR commu-nity to handle such tuning. One is to try many parameter settings and evaluat e the objectiv e functi on (such as mean average preci sion) at each parameter setting: potentially a very expensive procedure for many param eters . Thes e ap-proac hes can be further sub-di vided into a) approac hes that involve full explorat ion of a low-dimensional grid of param -eters , for exampl e a full 3-D grid [7], and b) heuristi cs that allow incompl ete explorat ion of more dimensions [15] [2]. Thes e latter methods are used as a baseline in this paper, and further detai ls are presen ted in section 2.2.
The other approac h is to choose a metri c that does not depend on calculat ing an overall ranking [6, 5], but that can be more easily optim ised, for exampl e by gradient descent [4]. Such approac hes are potentially much faster, but have the potential disadv antage of mismat ch between the met-ric being optimised and the rank-based metric that better re ects user satisfacti on [12]. This is one of the questi ons addres sed in the present paper.

In the work discus sed below, we use as our prim ary metric only the rank-based NDCG , de ned below. However, we make no assumptions about the primary measure, and our methods could be used with any single-point or rank-based measure, such as for example Mean Average Precisi on. A number of other approac hes to this problem have also been used. Metho ds based on SVMs , for exampl e [10], typically have a time-com plexit y which depends on the size of the training set rather than the number of param eters . Such methods also interact with the choice of metri c: for example Joachims [10] uses metri cs relating to a singe point in the ranked output such as Precisio n at rank k . He extends his method to one rank-base d measure , the area under the ROC curve { his argumen t depends on being able to de ne this area in terms of the probabil ity of pairwise error.
Wanting to consider cases with a large amoun t of training data, we do not pursue the SVM option here.
 Optim ising the parameters of a model for a parti cular set of conditi ons raises questions of general isabil ity, beyond the over tti ng question already raised. Either or both of the set of documents or of topics may be special in some way, or systematically di eren t from the target real environm ent in which the system will be deployed. A similar probl em is frequently observed in a TREC environm ent { some choice of methods or param eters which worked well on last year's TREC data will often fail on this year's task.

This is a general probl em, not addres sed in the prese nt paper (we assume some sampl ing process in generating the training and test topics, and a xed documen t collection) . We expect, for purel y statistical reasons, to over t becaus e our traini ng set is too small, but not because of systemat ic di erences . To some extent our assumptions mirror the sit-uation from which our data is taken, a general web search engine. We might expect the overall character of the doc-umen ts and topics to change over time, but probably grad-ually. However, we recognise the existence of the general probl em.
The corpus used in this paper has 5 levels of relevance judgemen t: four relev ant levels and one irrelev ant. A suit-able ranking e ect iveness measure that makes use of the multi-level relev ance judge ments is NDCG (norm alised dis-coun ted cumulative gain) [9]. The four positive relevance grades are assigned rather arbitrary gains of 31, 15, 7 and 3 respectively. The rank-based discoun t functi on is 1 log (1+ j ) for rank j ; however, the calcul ation is truncat ed at rank 10 (in e ect the discoun t functi on goes to zero after rank 10). As usual for NDCG , we calculate DCG for each query , by steppi ng down the ranks accum ulating the discounted gain at each rank; we also calculate the best possible DCG for this query, deriv ed from an ideal ranking in which a more-relev ant document always precedes a less-relev ant doc-umen t. DCG is norm alised per query by this maximum value, and the resulti ng per-query NDCG is averaged over queries.

Anot her feature of the data is the relative sparsity of rel-evance judgemen ts: only shall ow pools have been judged. This raises questio ns about how to evaluate ranking mea-sures such as NDCG, given that there may be many unrated documen ts early in the ranki ng result ing from any changed ranking functi on. We might consider either treat ing unrated documen ts as non-rel evant (this is the usual IR convention) or evaluating the measure only on rated documents. In the experiments presen ted here, we adopt the former usual con-vention when reporting NDCG on a test set. Most IR performa nce measures of interest, including NDCG, are de ned in term s of the rank of the documents and not their score. Computi ng the rank involves a sorting operation on the scores , which is not continuous, and for which no suitable gradien ts have been prop osed. For this reason, it is clearly dicult to use gradien t descent opti-mization techniques.
 Given this dicul ty, it is natural to use techniques that do not depend on gradi ent, but instead only on the actual computati on of NDCG. An obvious candidat e is the general class of optimisiation metho ds someti mes referred to as line search [13].

We use the following procedure in the experiments in this paper [8]. From an initial point in param eter space, a searc h along each individual parameter co-ordi nate axis is perform ed (all other param eters are taken as xed). The value of the training set NDCG is comput ed for each of N sampl e points along the line, centred on an initial point in parameter space. The location of the best NDCG value is recorded. The distances of these optima from the initial point de ne a \prom ising" directi on in param eter space. For example, if a distant optimum was found for a partic ular dimension, then there would be a large comp onen t of this prom ising direction in that dimension. Finally, we sample NDCG along this promising direct ion, generally involving changes now in all paramet ers. If we nd an NDC G value that is better than where we started, we move the current estimate of the best parameter vector.

We de ne an epoch as one cycle through all parameters, plus the nal search along the prom ising direction. Thus if there are P param eters then we perform P + 1 line search operations per epoch.

At the end of each epoch we reduce the scale over which the sampl es are taken by a factor of 0.85. The starting point for the next epoch uses the chosen optim um of the previous epoch.

We stop training if either we have reached a (rather arbi-trary) limit of 24 epochs or we have seen 3 success ive epochs with no impro vemen t.

Whil e this procedure is unques tionabl y comput ationall y expensive, this partic ular kind of line search is at least par-allelizable, as each co-ordina te axis can be searched inde-penden tly. Another common technique we could use in the future to speed up the line search is called search by golden section [13], where the optimum is bracketed by ever reduc-ing intervals.
Some appro aches to optimisatio n formulate the probl em with an explicit or implicit objective funct ion (that which the method tries to optimise) that may be di erent from any tradi tional measure of retrieval e ect iveness. For exampl e, approac hes based on linear regres sion typically use a least-squares error functi on in the predicted variabl e. Logistic regres sion attem pts to predi ct a binary categorisati on, us-ing its own error funct ion. Other categori sation approac hes similarly make use of parti cular error funct ions which may be very di eren t from any recognisabl e IR measure .
It may be observed that the general recomm endation in the machine learning literat ure (see for exampl e [12]) is that the objective funct ion shoul d indeed be the functi on one re-ally wants to optimise. As against this, it may be that the measures we really care about are not susceptibl e to suit-able methods. Clearly , if we pursue other measures purel y because we have suitable optimisation metho ds to go with them , then we need to pay great attention to the question of whether good paramet ers according to the substi tute mea-sures are also good for the measures we care about.
Logisti c regress ion is an example where the objective of the optimiser is seen as optimising the predict ion of rele-vance itself, rather than a result ing e ectiveness meas ure such as MAP or NDCG . The assumption is that if we can predict the relevance of each document well, we can also do well on the regula r IR measures . An alternativ e approa ch is to consider pairs of documents rather than single docu-ments. That is, what we want to optimise is a ranking, or a measure based on rankings, and a simple test of a ranking is whether a given pair of documents is in the right order. This approac h has some supp ort in the IR literature (with the bpref measure of [3]) and in the machine learni ng lit-erature (with the learner of [6]). The bpref work indica tes that the choice of pairs is critical for the IR case; this relates to the fact that measures such as MAP or NDCG are very heavily weighted to the top end of the ranking.

In this paper, we use a di erentiable cost measure based on the scores of pairs of documen ts. We discuss this meas ure in the next section.
In RankNet [4], a cost functi on (referred to here as RNC) is de ned on the di erence in scores of a pair of documents. The training data is de ne d as some set of pairs of docu-ments on which the relevance judge ments give us a prefe rred order for a particular query. Thus a pair consisti ng of one relev ant and one non-relevant, or one highl y relev ant and one minimally relev ant, might be a candidat e. The choice of pairs used in RankNet is discuss ed further in section 3.1.
The basic idea behind RankNet is as follows. The algo-rithm assumes that it is provided with a traini ng set con-sisting of pairs of documen ts d 2 ; d 1 toget her with a target probabi lity P 12 that documen t d 1 is to be ranked higher than document d 2 . They de ne a ranking function (model) of the form g : R D 7! R , in other words, mapping a vector of D feature values to the reals. The rank order of a set of documen ts is therefore de ned by the values taken by g . In parti cular, it is assumed that g ( d 1 ) &gt; g ( d 2 ) means that the model ranks d 1 higher than d 2 .

The map from the outputs g to probabi lities is modelled using a logistic functi on P 12 e Y = (1 + e Y ) where Y g ( d 2 ) g ( d 1 ), and P 12 is the probabi lity that d 1 higher than d 2 . Burges et. al. then invoke the cross -entropy error funct ion to penali ze pair ordering errors : This is a very general cost funct ion allowing us to use any available uncerta inty we may have concerning the pairwise ratings. In this paper, we take the pair ordering data as certai n, and so for us, P 12 are always one. With this simpli-cation, the RankNet cost becomes: where Y is the di erence in the scores of the two documen ts in the pair, positive if the documents are in the wrong order.
The RankNet cost funct ion assigns a cost to a pair in the wrong order, increas ing as the score di erence increases in that direction. It does not go immedia tely to zero, but asymptoti cally approac hes zero, as the score di erence in-crease s in the other direct ion. Thus the gradient of RNC not only encourages the pair to be in the right order, but encourages them to have at least some separati on in their scores .

Given a scoring/ ranking funct ion g that is di erentiable with respect to the parameters, RNC will also be di eren-tiable with respect to the parameters. Thus we will be able to discover the gradien t of RNC, and a gradien t descent procedure for discovering an optimum is feasible.
The full RankNet metho d is based on a neural net. The only aspect we have taken from this work is RNC itself, toget her with the idea of gradient descent.
 Our approac h to this is a special case of a textbook imple-mentation of the back-propagati on algorithm [11]. From a starti ng point (set of parameter values), the chosen pairs are evaluated in a random order. An epoch is a cycle through all pairs. For each pair, its contribution to the gradien t vec-tor for RNC is evaluated. A small step (determined by a training rate) is taken in parameter space, in the direct ion of reduci ng cost. The initial traini ng rate is 0.001, and this is reduced every time an epoch results in an increase in the total RNC. As with the line search optimization procedure described in section 2.2, we do a maxim um of 24 epochs. The various ranking funct ions used are de ne d in more de-tail in section 3.2. In general , they involve a single strong content feature, namely BM25F, combined with a number of addit ional weak features (both static and dynamic) pro-vided by a commerci al search engine. The parameters to be optimised are (a) the linear weights of the combina-tion of BM25F with the other features, and (b) the highl y non-l inear param eters of BM25F itself. That is, BM25F is treated as a single feature in a single-layer neural net with the other features, but the back-propagati on is extended to the internal parameters of BM25F. 1
The derivatives of RNC with respect to the linear weights are simple, but those with respect to the BM25F parame-ters are more compl ex { they are prese nted in the Appendix. The back-propagati on procedure extends to these parame-ters. This aspect is an original contribut ion of the prese nt paper.
Given a xed set of queries with associated relev ance eval-uations, we will frequen tly want to hold out some set of queries from the training process in order to use them for testing { we wish to avoid over tti ng the traini ng set, and such a held-out test set will help us establish the general is-ability of our trained model. However, a comm on procedure in machine learning training methods is to do a 3-way divi-sion of the evaluated materi al, into traini ng, validation and testing . The validatio n step has several possible funct ions. Conventionall y, one use is to avoid over tt ing, as follows. We go through an iterat ive training process on the train-ing set, and at each iteratio n evaluate on the validation set. When we reach some stopping point, we would norm ally expect the best value of the objective functi on in the train-ing set to be that achieved in the last iterati on. However, we choose the model on the basis of the validatio n set re-sults, which often means choosing an earlier iterat ion. This heuristi c is found to work well.
 In [4] the validation stage has a dual purp ose. The training process is driven by gradi ent descent on the RNC. How-ever, instead of validati ng on RNC, the authors validate by choosing the best model (over traini ng epochs) accordi ng to validati on set NDCG , the e ect iveness measure we are really interested in. So in addition to preventing us from over t-ting, it also serves to reject models that were only good according to RNC by checking them on the true metric of interest.
Given a paramet erised ranking funct ion, training set and a measure that we wish to optimise, we have to choose a training (opti misation) method. A perfect traini ng method would nd the true global optimum of the desired measure in the full parameter space; however, any realistic method is likely to fail in this task, and result in a set of param eters which is not exactly optimal. We are concerne d with the extent to which we may fall below optim al performance. In
We are currently also working on 2-layer nets, which need more data but may perform better than linear models [4] the case of a heuristi c line-se arch, such sub-optimal perfor-mance will be purely because the line search fails to nd the true global optimum. In the case of gradi ent descent based on RNC, we have the additional facto r that optim al cost is not neces sarily the same as optimal e ectiveness in terms of our real objective.

As we have seen, the choice of training method may de-pend on the number of param eters in the model. Another variabl e that may be importan t is the size of the training set. We might expect that we need a larger training set if we have (want to make e ective use of) more parame-ters. Also, while all training methods might be expected to do better with larger training sets, the relationship between perform ance of the metho d and training set size might vary between trainers.

The variables that we wished to consider in the experi-ments described below are as follows: Our experiments involve training sets ranging from 16 to 2048 rated queries 2 (secti on 3.1) with rankers with 2 or 9 or 375 tunabl e parameters (section 3.2).
Because we wished to explore these variables (speci call y the rst two) in regions which are not comm only available in public data sets, we have conduct ed our experiments on data sets taken from a commerci al searc h engine. Thes e data sets provide us with a large number of queries with relev ance judgem ents, and also allow us to use many dis-tinct features which may be combined in weighted fashion. Thus the feature weights provide us with most of our many param eters.

The data sets are based on a set of English queries (num-bers given below) sampl ed from query logs on a large web search engine. For each query, we select a candi date docu-ment collection consisti ng of appro ximately 2500 top ranked documen ts, of which approximately 30 are rated by judges on a 5 point relevance scale (including non-relevant). The remai nder are labelled as not relevant. In addition, there are a random sample of a few hundred unretri eved documen ts from the set of documents containing all query terms, which repres ent documents that are poor matches with high prob-ability. However, idf values for the term weights are taken from the original collection.

To facilitate the investigation of the e ect of training set size, we vary the size of the training set, starti ng at 16 queries, and increasing by steps of training set, there is an associated validation set of one-quarter the size. The test set is a xed set of 512 queries. For validation and test, NDCG is evaluated from the full ranking; unrated documents are treated as non-relev ant.
The remai nder of this section descri bes how the choice of optimisation method a ects the training data that we use in our experiments.
We are curren tly working on experiments on training sets upto 10 times larger.
 Line searc h traini ng can use all the documen ts that we have for each query, both rated and un-rat ed, giving appro xi-mately 2500 documents per query. We assume that unrated documen ts get zero gain in the NDCG computat ion. If each unlab eled documen t were used to comput e the set of all pairs for the computat ion of RNC, there would be an extremely large number of pairs per query . Also the evidence from the bpref study [3] suggests that we need to be more selective about the pairs we use. To reduce the number of pairs to a more manageable size, we adopted a similar sampl ing strat egy to that described in [4] as follows. We random ly subsam pled from the unrat ed documents to give a subset of the same size as the average number of rated documen ts per query. These documents are treated as irrelevant. We do not use tied pairs.

As argued in [4], it is possibl e that this is not just good for comput ationa l reasons. In perform ing this subsampl ing process, we are substan tially increas ing the proba bility that any unrat ed documen t that we use in our training process really is irrelev ant, and therefore should get zero gain in NDCG.
This section describes the three ranking models we shall investiga te.
 The 2-parameter model is simpl e BM25, the param eters be-ing the usual BM25 param eters of k controlling term fre-quency saturati on rate and b controlling document length norm alisation. Note that k 0 and 0 b 1. The -nal document score is the sum of BM25 with a single xed weight (parameter-free) query independen t feature. This query indep enden t feature captures the quality of the docu-ment according to the link struct ure of the web, independe nt of query and content.
 The 9-parameter model is BM25F [14, 15] (see appendix), this time in a trainabl e combinati on with the singl e query indep ende nt feature used in the 2-param eter model. The BM25F ranking functi on is an extension of the BM25 func-tion that admit s extra degree s of freedom to capture the relat ive semantic signi cance of any elds that may be con-tained in the document.

In our curren t corpus, there are four elds: incoming an-chor text, URL, title and body. Each eld has its own b and its own term-frequency weight w . Again, 0 b 1 and w 0. Since there is redundancy between varying the weights and varying the k value [14], we choose to x the singl e k value and vary the four weights. The nal scori ng functi on is BM25F linearly combined with the static feature (one more weight). The formula for this ranking funct ion and the deriv atives that may be used in back-propagat ion are given in the appendix.

We note that the 9-param eter model is very similar to the model used in our success ful subm ission to the TREC Web Track in 2004 [15]. It thus repres ents something close to the state of the art of web search at TREC. The 375-parameter model uses a large number of weak fea-tures. These features are not described in detail here; the object is to illustrate the process. As with the 9-parameter model, the scoring funct ion rst calcul ates BM25F and then combines the BM25F score in a weighted linear fashion with the remaining features.
We investigate the relev ance performanc e of the three rankers described in the last section over a range of training set sizes.
With two parameters, we can plot an appro ximate e ec-tiveness surface by runni ng a full grid over the parameter space. In Figure 1 we see three surfaces, based on the 64-query traini ng set and the 512-query test set, in the form of contours. The three are: RNC on the selected pairs from the training set; NDCG as calculat ed on the training set; and NDCG as calcul ated on the test set.

The striking thing about this gure is the similarity be-tween Figure 1a, the RNC comput ed from rated and random sub-sampl ed documen ts from only 64 queries, and Figure 1c, the target test set NDCG comput ed from all available docu-ments from 512 queries. Both these surfaces appear smooth (good for search-and gradient-based optimisation alike) and seem to have very similar struct ure. In contrast, the train-ing set NDC G surfac e in Figure 1b appears bump y. This gives us at least an intuitive feel for why direct optimisation of the training set NDCG may give unpredi ctabl e results for smal l traini ng sets.

Armed with the deriv atives of the RNC with respect to k; b (see appendix), we ran line search on NDCG verses gradi -ent descent optimisation on RNC over the range of training query set sizes. We initialised both at k = 1 : 0 and b = 0 : 5. Our principle result here is that that gradien t descent and line searc h perform ed almost identically over all query set sizes. In all cases , the trained ranker gave a test set NDCG of about 0 : 30 0 : 02 3
In addit ion, we observed that the test set NDC G did not improve signi can tly from the initial starting point. In other words, we would have got similar test set NDCG perfor-mance with the 2-parameter ranker set to its initial point, with no optimization of BM25 paramet ers.
As we increase the number of param eters , the `ground truth' repres ented by the grid search is not access ible to us. We use the line search procedure describ ed in section 2.2 as a substitute baseline. Because it directl y optimizes NDCG, there is reason to believe that it should perform well against the gradien t descen t approac h.

With this ranker, we again compared the performanc e of line search against gradi ent descen t on the RNC, using deriv atives of RNC with respect to BM25 F paramet ers pre-sented in the appendix. We set identical initial BM25F weights as w s = 1 : 0, b s = 0 : 5 (see appendix) and the initial
This con dence limit is 95% on true population NDCG value given the 512 sample size, and unless otherwise stated, applies to all our results on the 512 query test set. Figure 1: Objective function contours for training and test sets for the 2-parameter model: (a) RNC in the training set; (b) NDCG in the trainin g set; (c) NDCG in the test set. (Trainin g set of 64 queries. RNC in the training set based on chosen pairs only; Both NDCG plots, train and test, are based on all documen ts.) weight of the query independen t feature to be the same as it was for the 2-param eter model.

The results are shown in Figure 2 as \Line search 9" and \Gradien t descent 9". We can see that the two optimization techniques are compa rable in perform ance, with gradi ent descent being slightly more consistent.

We also report a signi c ant impro vemen t in performanc e over the initial model (epoch zero), thus proving the value of BM25F tuning in this ranker. The initial model had a test set NDCG of 0.27. As you can see from Figure 2, both tuning mechanisms beat this by a very signi ca nt margin for all but the smallest traini ng set sizes. We concl ude that the extra degrees of freedom in a tuned BM25F model are a valuable additi on to the parameter space.
In this scenario the line searc h baseli ne is not feasibl e us-ing commonly available computer hardware. Therefore we set out to compare the use of gradi ent descent on all the param eters (\Gradien t descent 375" in Figure 2) with the use of gradien t descent on the BM25F param eters alone and line searc h on the rest (\Line search 375").

As you would expect, such a large model needs a certain amount of training data before it starts performi ng well. Paired t-tests indicat e that the 375-pa rameter model sig-ni cantly outp erforms the 9-para meter model for training sets of 256 queries or more. It is also signi c antly worse for traini ng sets of 45 queries or fewer. All these di erences were signi c ant at p &lt; 0 : 01 (and usuall y at p &lt; 0 : 001). We also measured changes as the collection size grows, showing signi can t improvements in the 375-parameter model, but no signi can t chang es in the 9-param eter model (Table 1).
We note that this result indicates that many weak features can add signi can tly to a state-of-the-art TREC Web Track ranker. Figure 2: NDCG on the test set for di erent train-ing set sizes, di eren t numbers of paramet ers per model, and the two optimisatio n methods.
 Assuming that we wish to adopt the gradi ent descent ap-proac h to tuning BM25F param eters, it is natural to con-sider how much improvement the tuning process makes over the initialisation point. In Figure 3 we repeat the \Gradi ent descent 375" run, now labelled \Tuned BM25F". We show three further runs: the rst is the test NDCG for the model witho ut BM2 5F (with 367 parameters, \No BM25 F"), the second is a run with the BM25F model with its initial set-tings (\Un tuned BM25 F") and the third is the line-se arch tuned BM25F param eters from the corresponding \Line search 9" runs as xed values in the 375-para meter ranker (\Preco oked BM25F") .

We see that the ranker without BM25F is consistently beaten for traini ng sets above about 100 queries. Adding even an untuned BM25F gives a statistically signi can t im-provemen t for training set sizes greater than or equal to 512 queries. However, the di erence between the remai ning three models is not statistically signi can t. It is surprising that tuning BM25 F in this context makes so little di erence. We are currently working on larger query sets (traini ng and test) to see under what conditi ons, if any, this di erence becomes signi c ant. Figure 3: The e ect of tuning BM25F within a large ranking model. How NDC G improves by adding an untuned BM25F featu re, and how it does not change much by tuning it in the presence of hundreds of other features. We may summari se our results as follows: We nd the relative success of gradien t descent, using RNC, very interesting. This is a far more ecient method than line searc h when we work with subsampled pairs as discuss ed, and seems to do just as well, despite the fact that it uses the `wrong' objective funct ion. The RankNet pro-cedure seems to be a prett y robust process. We do not of course know how close we are to any `true' optim um, but it appears at least dicult to do better.

It remai ns the case that optimising these ranking func-tions is an intuitive process involving many heuristi cs. Very many decisions were taken in setting up both these of ex-periments, any one of which could be a ect ing the result s we have obtai ned. One exampl e is the train-v alidation split { for situations where the potential number of rated queries is restri cted, holding out queries for validatio n (and/o r test) limits the traini ng set in ways that might be bad for traini ng. Similarly the use of judge e ort for deep pool evaluat ion on smal ler numbers of queries or shallow pool on larger num-bers is a signi can t issue. A few of these variabl es have been explored in a limited way during the preparatio n of this pa-per, but there remain many unexplored areas. Neverthel ess, the appare nt robus tness of gradient descent is encouragi ng. GD375 0.355 +0.018 (p=0.013) +0.043 (p &lt; 0.001) Table 1: Adding more training queries does not give signi cant improvements for the 9 param eter mod-els, but does for the 375 paramet er model.
We would like to thank Andy Lauci us and Timo Burk ard for both providing the data and many useful discussions. Also thanks to Greg Hullender and Erik Selberg for helpful feedbac k on an earlier draft . [1] C. M. Bishop. Neur al networks for pattern recognition . [2] C. Buckley and G. Salton. Optim ization of relevance [3] C. Buckley and E. Voorhees . Retri eval evaluation with [4] C. J. C. Burges, T. Shak ed, E. Renshaw, et al. [5] J. Gao, H. Qi, X. Xia, and J.-Y. Nie. Linear [6] R. Herbric h, T. Graepel, and K. Obermayer. Large [7] D. A. Hull, J. O. Pedersen, and H. Sch X  utze. Metho d [8] G. Hullende r. Personal communicat ion, 2004. [9] K. J X arvelin and J. Kek X  al X  ainen. IR evaluat ion metho ds [10] T. Joachims. A support vector method for [11] Y. LeCun, L. Bottou, G. B. Orr, and K. R. M X  uller. [12] D. D. Lewis. Evaluating and optimizing autonom ous [13] D. G. Luen berger. Linear and nonlinear programming . [14] S. Robertson, H. Zaragoza, and M. Taylor. Simple [15] H. Zarago za, N. Cras well, M. Taylor, S. Saria, and
In this appendix, we derive the deriv atives of the RNC with respect to the BM2 5F parameters. We de ne the BM25F ranking function as: where I t is the Robertson-Spark-Jones term weight (idf), and is some sort of saturat ion functi on and f t is the eld aggregat ed term frequency . We will use the following usual saturati on funct ion: and the de nit ion of f t is: where w s is the eld weight, and the documen t length nor-malisation model is de ned in the usual way, per eld, as: We rst deriv e some general results, and then go on to use these for the required deriv atives with respect to k; w s giving:
This section uses the partial deriv atives of BM25F to get the required deriv atives of the RNC, here denoted C . Let be a generic BM2 5F param eter, y (1) ; y (2) be a pair of scores wher e d 1 has a better rating than d 2 and Y y (2) y (1)
We assume that the scoring functi on combines N features linearly , and that the N th feature is BM25F. We have and so: wher e F ( k ) is BM25F. Now the gradi ent we require is The required partial derivatives of the RankNet cost with respect to k; w s and b s can be obtained by substit uting (7), (8) and (10) respectively for @F ( k ) =@ in (16), and @C=@Y is given by (13).
