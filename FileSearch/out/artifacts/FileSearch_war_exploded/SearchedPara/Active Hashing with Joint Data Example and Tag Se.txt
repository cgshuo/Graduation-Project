 Similarity search is an important problem in many large scale applications such as image and text retrieval. Hashing method has become popular for similarity search due to its fast search speed and low storage cost. Recent research has shown that hashing quality can be dramatically improved by incorporating supervised information, e.g. semantic tags/labels, into hashing function learning. However, most existing supervised hashing methods can be regarded as passive methods, which assume that the labeled data are provided in advance. But in many real world applications, such supervised information may not be available.
 This paper proposes a novel active hashing approach, Active Hashing with Joint Data Example and Tag Selection (AH-JDETS), which actively selects the most informative data examples and tags in a joint manner for hashing function learning. In particular, it first identifies a set of informative data examples and tags for users to label based on the selection criteria that both the data examples and tags should be most uncertain and dissimilar with each other. Then this labeled information is combined with the unlabeled data to generate an effective hashing function. An iterative procedure is proposed for learning the optimal hashing function and selecting the most informative data examples and tags. Extensive experiments on four different datasets demonstrate that AH-JDETS achieves good performance compared with state-of-the-art supervised hashing methods but requires much less labeling cost, which overcomes the limitation of passive hashing methods. Furthermore, experimental results also indicate that the joint active selection approach outperforms a random (non-active) selection method and active selection methods only focusing on either data examples or tags.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance, Experimentation Hashing, Active Learning, Similarity Search, Data Selection
Similarity search is a key problem in many information retrieval applications including image and text retrieval, content reuse detection and collaborative filtering. The purpose of similarity search is to identify similar data examples given a query example. With the explosive growth of the internet, a huge amount of data such as texts, images and video clips have been generated, which indicates that efficient similarity search with large scale data becomes more important. Traditional similarity search methods are difficult to be used directly for large scale data since computing the similarity using the original features (i.e., often in high dimensional space) exhaustively between the query example and every candidate example is impractical for large applications. There are two major challenges for using similarity search in large scale data: storing the large data and retrieving desired data efficiently.

Hashing methods [27, 40, 42, 43] have been proposed for addressing these two challenges and have achieved promising results. These hashing methods design compact binary code in a low-dimensional space for each data example so that similar data examples are mapped to similar binary codes. In the retrieval process, these hashing methods first transform each query example into its corresponding binary code. Then similarity search can be simply conducted by calculating the Hamming distances between the codes of available data examples and the query and selecting data examples within small Hamming distances. In this way, the two major challenges for large scale similarity search can be addressed as: data examples are encoded and highly compressed within a low-dimensional binary space, which can usually be loaded in main memory and stored efficiently. The retrieval process is also very efficient, since the Hamming distance between two codes is simply the number of bits that differ, which can be calculated using bitwise XOR.

Recently, some supervised hashing methods [23, 34, 37] have incorporated labeled data/information, e.g. semantic tags, for learning more effective hashing function than unsupervised hashing methods. It has been shown that hashing quality could be dramatically improved by leveraging supervised information. For example, in text retrieval applications, semantic tags (e.g. documents labeled with the same tag) reflect the semantic relationship between documents and thus can be very important and helpful for learning hashing function. However, most existing supervised hashing methods can be regarded as passive methods, which assume that the labeled data are provided beforehand. But in many real world applications, such supervised information may not be available and it is often expensive to acquire for a large dataset. Therefore, it is important to design effective methods to actively identify only the most informative data examples for users to label. On the other hand, the labeling cost will also depend on the total number of tags that the users label to the selected data examples. In many large scale applications, there are often hundreds or thousands of tags for users to label. Moreover, similar tags usually carry similar semantic meanings. For instance,  X  X ar X  and  X  X utomobile X  have similar meanings and choosing both of them may not gain substantial new information over just selecting one. Therefore, selecting a small set of most informative tags is also important to lower the efforts of user labeling. This paper proposes a novel active hashing approach, Active Hashing with Joint Data Example and Tag Selection (AH-JDETS), to achieve the goal of learning accurate hashing function with a limited amount of labeling efforts. AH-JDETS actively selects the most informative data examples and tags in a joint manner for hashing function learning. Specifically, it first identifies a set of informative data examples and tags for users to label based on the selection criteria that both the data examples and tags should be most uncertain and dissimilar with each other. Then the supervised information is combined with the unlabeled data to generate an effective hashing function. An iterative procedure is proposed for learning the optimal hashing function and selecting the most informative data examples and tags. Extensive experiments on four different datasets have been conducted to demonstrate that AH-JDETS can achieve good performance with much less labeling cost when compared to state-of-the-art supervised hashing methods, which overcomes the limitations of passive hashing methods. Moreover, the experiments have clearly shown the advantages of the proposed AH-JDETS approach against several other selection methods for obtaining training data.
This section reviews the related work in two research areas: hashing function learning and active learning.
Hashing methods [6, 18, 44, 36, 38] are proposed to address the similarity search problem within large scale data. These hashing methods try to encode each data example by using a small fixed number of binary bits while at the same time preserve the similarity between data examples as much as possible. In this way, data examples are transformed from a high-dimensional space into a low-dimensional binary space and therefore, the similarity search can be done very fast [30] by only computing the Hamming distance between binary codes. Hashing methods generate binary codes for efficient search, which is different from traditional dimensionality reduction methods such as Principal Component Analysis (PCA) and Latent Semantic Indexing (LSI) [8, 14]. Existing hashing methods can be divided into two groups: unsupervised and supervised hashing methods.
 Among the unsupervised hashing methods, Locality-Sensitive Hashing (LSH) [7] is one of the most popular methods, which simply uses random linear projections to map data examples from a high dimensional Euclidean space to a low-dimensional binary space. This method has been extended to Kernelized and Multi-Kernel Locality-Sensitive Hashing [19, 41] by exploiting kernel similarity for better retrieval efficacy. The Principle Component Analysis (PCA) Hashing [22] method represents each example by coefficients from the top k principal components of the training set, and the coefficients are further binarized using the median value. A Restricted Boltzman Machine (RBM) [13] is used in [27] to generate compact binary hashing codes. Recently, Spectral Hashing (SH) [40] is proposed to design compact binary codes with balanced and uncorrelated constraints. Self-Taught Hashing (STH) [43] combines an unsupervised learning step with a supervised learning step to learn hashing codes. More recently, the work in [42] proposes a Composite Hashing with Multiple Information Sources (CHMIS) method to integrate information from different sources. In work [24], a hyperplane hashing method is proposed for efficient active learning, which can find nearest points to a query hyperplane in sublinear time.

For the supervised hashing methods, a Canonical Cor-relation Analysis with Iterative Quantization (CCA-ITQ) method has been proposed in [10] which treats the content features and tags as two different views. The hashing codes are then learned by extracting a common space from these two views. Recently, several pairwise hashing methods have been proposed. The semi-supervised hashing (SSH) method in [34] utilizes pairwise knowledge between data examples besides their content features for learning more effective hashing codes. A kernelized supervised hashing (KSH) framework proposed in [23] imposes the pairwise relationship between data examples to obtain good hashing codes. More recently, a ranking-based supervised hashing (RSH) [35] method is proposed to leverage listwise ranking information to improve the search accuracy. Most recently, the work in [37] proposes a Semantic Hashing method which combines Tag information with Topic Modeling (SHTTM) by extracting topics from texts and exploiting the correlation between tags and hashing codes for document retrieval. It has been shown that supervised hashing methods achieve better performance than unsupervised methods. However, as aforementioned, most existing supervised hashing methods can be regarded as passive methods which assume the labeled data are provided beforehand but in practice, such supervised information may not be available or can be expensive to obtain. Therefore, it is important to design effective methods to actively identify only the most informative data for users to label for generating accurate hashing codes with low cost.
The purpose of active learning [11, 15, 32] is to select data examples from an unlabeled pool which will be very beneficial in training the model, thereby reducing the labeling cost since noninformative instances are not selected. Many strategies have been proposed to measure the informativeness of unlabeled data examples. Uncertainty sampling [20] selects the data examples whose predicted labels are the most uncertain. A batch mode active learning method is proposed in [15] that applies the Fisher information matrix to select a number of informative examples simultaneously based on the criteria that the selected set of unlabeled examples should result in the largest reduction in the Fisher information. The work in [11] proposes a discriminative batch mode active learning strategy that exploits information from an unlabeled set to learn a good classifier directly, which obtains high likelihood on the labeled training instances and low uncertainty on labels of the unlabeled instances. A comprehensive survey of active learning can be found in [28].

Recently, active learning has been extended to various tasks including image classification [15], learning to rank [9, 25], query selection [4, 39] and collaborative filtering [12, 17]. For example, the work in [9] addresses active rank learning based on expected hinge rank loss minimization. Inspired by the expected loss reduction strategy, [25] recently introduces an expected loss optimization framework for ranking, where the selection of query and documents is integrated in a principled manner. The work in [39] generalizes the empirical risk minimization principle to active learning which identifies the most uncertain and representative queries by preserving the source distribution as much as possible. A Bayesian selection approach is proposed in [17] for collaborative filtering, which identifies the most informative items such that the updated user model will be close to the expected user model.

The only work we found using active learning in hashing is [45], which directly chooses the most uncertain data examples based on the hashing function. A batch mode algorithm is also proposed in this work to speed up their active selection. However, the method in [45] only considers identifying the most informative data examples and tries to label all possible tags to these selected examples, which requires a great amount of labeling efforts for those datasets associated with a huge number of tags. Therefore, in this paper, we develop a novel active hashing approach that jointly selects the most informative data examples and tags such that the hashing function can be learned efficiently with only a small number of labeled data, which greatly reduces the labeling cost.
The proposed AH-JDETS approach mainly consists of two components as shown in Figure 1: (1) Supervised hashing, which incorporates the labeled information into hashing function learning. In this work, we modify the recent supervised hashing method in [37] to learn effective hashing function and present the details in next section. (2) Joint data example and tag selection, which actively selects a set of most informative data examples and tags for users to label. These newly labeled data are added to existing labeled information for learning a more accurate and effective hashing function.
Our active hashing method is related to a recent supervised hashing method, Sematic Hashing using Tags and Topic Modeling (SHTTM) [37], which utilizes tags and topic modeling together to learn effective hashing function. One main advantage of this method is that it not only learns high quality hashing codes but extracts a set of tag correlation variables, which reflect the correlation between tags and learned hashing codes. Therefore, the relationship among different tags is also represented in the tag correlation variables (more details will be given later).

We first introduce some notation. Assume there are n training examples total, denoted as: X X X = { x 1 ,x 2 ,...,x R R R m  X  n , where m is the dimensionality of the content feature. Denote the labeled tags as: T T T  X  { 0 , 1 } l  X  n , where l is the total number of possible tags associated with each data example. A label 1 in T T T means an example is associated with a certain tag, while a label 0 means the example is not associated with that tag. The goal is to obtain optimal binary hashing codes Y Y Y = { y 1 ,y 2 ,...,y n } X  X  X  1 , 1 } the training examples, and a hashing function f : R R R { X  1 , 1 } k , which maps each example to its hashing code (i.e., y = f ( x i )). Here k is the code length. A linear hashing function is utilized: where W W W is a k  X  m parameter matrix representing the hashing function and sgn is the sign function.

There are two key problems that need to be addressed in supervised hashing methods: (1) how to incorporate the labeled tag information into learning effective hashing codes, and (2) how to preserve the similarity between data examples in the learned hashing codes. The SHTTM method solves the first problem by ensuring the learned hashing codes to be consistent with labeled tag information through a tag consistency component. In particular, a latent variable u j for each tag t j is first introduced, where u k  X  1 vector indicating the correlation between tag t j and hashing codes. Then a tag consistency component can be formulated as follows: where T T T ij is the binary label of the j -th tag on the i -th data example. y T i u j can be viewed as a weighted sum that indicates how the j -th tag is related to the i -th example, and this weighted sum should be consistent with the label T T T ij as much as possible. The second regularization term, P  X  is trade-off parameter and kk F is the matrix Frobenius norm. By minimizing this component, the consistency between tags and the learned hashing codes can be ensured. Note that semantically similar tags will have similar latent variables, since these tags are often associated with common data examples, and thus the learned corresponding latent variables will be similar by ensuring the tag consistency term. In the extreme case, if two tags are assigned in exactly the same set of examples, their latent variables will be identical.

The second problem in supervised hashing methods is similarity preserving, which indicates that semantically similar examples should be mapped to similar hashing codes within a short Hamming distance. In SHTTM, it points out that the similarity calculated using the original feature vector may not reflect the semantic similarity between data examples. Therefore, it proposes to utilize features extracted from topic modeling [2] to measure the semantic similarity between data examples instead of original features, since topic modeling provides an interpretable low-dimensional representation of the data examples associated with a set of topics. SHTTM exploits the Latent Dirichlet Allocation (LDA) [3] approach of topic modeling to extract k latent topics from the data examples. Each example x i corresponds to a distribution  X  i over the topics where two semantically similar examples have similar topic distributions. In this way, semantic similarity between data examples is preserved in the extracted topic distributions  X   X   X  and the similarity preservation component in SHTTM is defined as follows: By minimizing this component, the similarity between different examples is preserved in the learned hashing codes.
Combining the tag consistency and similarity preservation components from Eqns.2 and 3, and substituting Eqn.1, the overall objective function for learning the hashing function and tag correlation can be formulated as: where  X  and  X  are trade-off parameters to balance the weight between the components. Note that the sgn operator in Eqn.1 is relaxed to make the above optimization problem tractable. Since the objective function in Eqn.4 is convex with respect to either one of the two sets of parameters ( W W W and U U U ) when the other one is fixed, the above problem can be solved iteratively by coordinate descent optimization with guaranteed convergence similar to that in SHTTM.
The main research problem in our AH-JDETS approach is to actively select a small set of informative data examples and tags for users to label, which can be utilized for learning a better hashing function. The goal of joint data example and tag selection is that we hope to identify a set of L data examples, A d , together with a set of M tags, A t , such that the labeling information of the selected data can best boost the performance of supervised hashing. During the labeling process, for a given data example and a tag, users label 1 or 0 to denote whether this specific tag is assigned to this data example or not. The labeling cost will depend on the total number of selected tags that the users need assign to the selected data examples, which can be measured as LM . The reason is that users need to label either 1 or 0 for each data example and tag pair, and there are total LM such pairs. We will provide more details and possibilities of measuring the labeling cost with respect to the sizes of L and M for learning hashing function later in the experiments. As mentioned, for large scale datasets, there might be millions of data examples associated with thousands of different tags, which makes it impractical for labeling all tags to every data example. Therefore, it is important to select only a small set of most informative data examples and tags for users to label to save user labeling efforts.

An important question to ask is how to measure the informativeness of data examples and tags? In this work, we propose to combine two measuring criteria, data uncertainty and data dissimilarity, which are widely used in active learning literature [4, 25, 39]. In particular, the selected data (both data examples and tags) are more informative if they are more uncertain. For example, the hashing codes of some potential data examples are not certain/well-learned or the predicted labeling results for some potential tags are not certain. The intuitive idea is that we would gain more knowledge by labeling on uncertain data than on certain ones. On the other hand, the selected data are more informative if they are more dissimilar to each other, since similar data may provide redundant information which is not helpful to the learning process. In the following sections, we first describe the data certainty and similarity modeling based on the selection criteria respectively. Then the final objective together with the optimization algorithm will be elaborated. Finally, we discuss the computational cost of the learning algorithm.
Recall that in our supervised hashing model, the binary hashing code y i , is obtained by thresholding a linear projection of a data example x i , i.e., y i = sgn ( W W Wx and the linear hashing function W W W can be viewed as k decision/classification hyperplanes, each of which is used to generate one bit of a code. More precisely, if a data example sits on the positive side of a decision hyperplane, then its corresponding hashing bit is 1, otherwise -1. The data example certainty with respect to coding can be measured by its distance to the hyperplane, which is | wx | . Intuitively speaking, the smaller the distance of a data example to a hyperplane, the more uncertain the data example is. Considering an extreme case where a data example lies exactly on a decision hyperplane, then it is highly uncertain to decide whether its corresponding bit should be 1 or -1. Since there are total k decision hyperplanes, we use the l norm 1 to calculate the certainty, c d i , of a data example as: It can be seen that the data example certainty is inversely related to the informativeness, i.e., the smaller the certainty value is, the more informative it is. Then the total certainty of the selected data examples can be written as P i  X  X  where A d is the active set of data examples.

Besides exploiting data example certainty, we also need to model the tag certainty. In our supervised hashing method based on SHTTM, the correlation between tags and hashing codes are learned in the latent variables U U U . Inspired by the data example selection, we use the magnitude of u j to represent the tag certainty as: where small value indicates an uncertain tag. u = 0 is an extreme which means this tag contributes no information in the hashing function learning. Therefore, we can compute the total certainty of the selected tags as P j  X  X  Combining the data example certainty and tag certainty terms, the joint data certainty can be modeled as: where  X  is a trade-off parameter to balance the weight between two data certainty terms. By minimizing the above objective function, we can select a set of data examples and tags that are most uncertain.
Similar data may contain similar knowledge, which provides redundant information in the learning process and is not desirable. For instance, the tags of  X  X ar X  and  X  X utomobile X  have similar meanings and choosing both of them may not gain substantial new information over just selecting one. Therefore, selecting a set of dissimilar data to label is very important for acquiring more information, which can make the learning process more effective. The pairwise similarity S S S ij between data example x i and x be pre-calculated as: where  X  2 is the bandwidth parameter. Note that we use the Gaussian function/kernel to calculate the similarity in this work due to its popularity in many hashing methods [42, 43], but other similarity criteria may also be used, such as cosine similarity or inner product similarity. Then the total sum of similarity values among the selected data examples
For the tag similarity between t i and t j , we directly calculate it as the inner product of their corresponding tag correlation vectors, u T i u j . Then the sum of similarity values between selected tags can be written as P ( i,j )  X  X  Combining the above two similarity terms, the joint data similarity can be modeled as:
Other norms such as l 1 norm may also be used. where  X  is also a trade-off parameter to balance two data similarity terms. By minimizing the above objective function, we can select a set of data examples and tags that are most dissimilar to each other.
The entire objective function of joint data example and tag selection integrates two components such as data certainty component in Eqn.7 and data similarity component given in Eqn.9 as: min To formalize the above objective, we introduce an indicating vector  X  d  X  X  0 , 1 } n whose entries specify whether or not the corresponding data examples are selected, i.e.,  X  d i = 1 when x is selected and  X  d i = 0 when x i is not selected. Similarly, an indicating vector  X  t  X  X  0 , 1 } l for tags is also used. Then the above formulation can be rewritten as: s.t.  X  d  X  X  0 , 1 } n ,  X  t  X  X  0 , 1 } l ,  X  d T 1 1 1 = L,  X  where C d = [ c d 1 ,c d 2 ,...,c d n ] T is the data example certainty vector and C t = [ c t 1 ,c t 2 ,...,c t l ] T is the tag certainty vector.  X  ,  X  and  X  are trade-off parameters. 1 1 1 is a vector of all ones and the constraints  X  d T 1 1 1 = L and  X  t T 1 1 1 = M mean we wish to select exact L data examples together with M tags. The first two terms in the objective function are the sum of certainty values of the selected data, and the last two terms are the sum of similarity values between them. By minimizing the above objective function, we can jointly select a set of data examples and tags that are most uncertain and dissimilar with each other.

Directly minimizing the objective function in Eqn.11 is intractable due to the integer constraints, which makes the problem NP-hard to solve. Therefore, we propose to relax these constraints to the continuous constraints 0 0 0  X   X  and 0 0 0  X   X  t  X  1 1 1 and further decompose the optimization problem into two sub-problems as: and where 0 0 0 is a vector of all zeros. These two relaxed sub-problems are standard constrained quadratic programs (QP) which can be solved efficiently using convex optimization methods, such as successive linear programming (SLP) [1] and the bundle method [31]. After obtaining the relaxed solution from Eqn.12, we select L data examples with the largest  X  d values to form the active data example set A Similarly, we choose M tags with the largest  X  t values based on the relaxed solution from Eqn.13 to form the active tag set A t . Finally, we will request the users to label all tags from the selected tag set A t to every data example in A , and update the labeled information T T T to retrain the Algorithm 1 Active Hashing with Joint Data Example and Tag Selection (AH-JDETS) Input: Training examples X X X , initial labeled data T T T and Output: Hashing function W W W , tag correlation U U U and 1: Compute S S S and  X   X   X  , initialize L and M . 2: Supervised Hashing 3: Solve the optimization problem in Eqn.4 to obtain 4: W W W and U U U . 5: Joint Data Example and Tag Selection 6: Calculate data certainty C d and C t using Eqns.5 7: and 6 based on W W W and U U U . 8: Calculate tag similarity U U U T U U U . 9: Solve the optimization problem in Eqn.12 to select 10: a set of data examples A d . 11: Solve the optimization problem in Eqn.13 to 12: select a set of tags A t . 13: Labeling tags from A t to data examples in A d and 14: Repeat step 2 to 13 for several iterations. 15: Generate the hashing codes Y Y Y using Eqn.1. supervised hashing model (see Figure 1). The alternative process of learning hashing function and actively selecting data can be repeated for several iterations. We will discuss more in the experiments. The full AH-JDETS algorithm including supervised hashing and joint data example and tag selection is summarized in Algorithm 1.
The learning algorithm of AH-JDETS consists of two main parts: supervised hashing and joint selection of data examples and tags. For the supervised hashing method, we iteratively solve the optimization problem in Eqn.4 to obtain the hashing function and tag correlation, where the time complexity is bounded by O ( nlk + nk 2 ). The second part involves selecting data examples and tags by solving the two relaxed optimization problem in Eqns.12 and 13. Since these are standard quadratic program problems, the time complexity for obtaining  X  d and  X  t is bounded by O ( n 2 Thus, the total time complexity of the learning algorithm is bounded by O ( nlk + nk 2 + n 2 + l 2 ). For large scale dataset, O ( n 2 ) cost for data example selection might not be feasible. In practice, we reduce the computational cost in the experiments by only considering the top 10% data examples corresponding to the smallest certainty values without much loss in accuracy. However, the training process is always conducted off-line and our focus of efficiency is on the retrieval process. This process of generating hashing code for a query example only involves some dot products and comparisons between two binary vectors, which can be done in O ( mk + k ) time.
This section presents an extensive set of experiments to demonstrate the advantages of the proposed approach.
We conduct experiments on four datasets, including two image datasets and two text collections as follows: 1. Flickr 1 m [16] is collected from Flicker images for 2. NUS -WIDE [5] is created by NUS lab for evaluating 3. ReutersV 1 (Reuters-Volume I): This dataset contains 4. Reuters (Reuters21578) 2 is a collection of text 512-dimensional GIST descriptors [26] are used as image features and tf -idf features are used to represent the documents.
 We implement our method using Matlab on a PC with an Intel Duo Core i5-2400 CPU 3.1GHz and 8GB RAM. The parameters  X  ,  X  ,  X  and  X  are tuned by 3-fold cross validation on the training set through the grid { 0 . 01 , 0 . 1 , 1 , 10 , 100 } and we will discuss more details how they affect the performance of our approach later. We randomly select 2 k labeled data examples in the training set to form the initial tag matrix T T T . We repeat each experiment 10 times and report the result based on the average over the 10 runs. Each run adopts a random split of the dataset.
To conduct similarity search, each example in the testing set is used as a query example to search for similar examples in the corresponding training set based on the Hamming distance of their hashing codes. We follow two evaluation criteria that are commonly used in the literature [10, 37, 42]: Hamming Ranking and Hash Lookup. Hamming Ranking ranks all the data examples in the training set according to their Hamming distance from the query and the top k examples are returned as the desired neighbors. Hash Lookup returns all the data examples within a small Hamming radius r of the query. The performance is measured with standard metrics of information retrieval: precision as the ratio of the number of retrieved relevant http://daviddlewis.com/resources/textcollections/reuters2 1578/. with 32 hashing bits.
 on four datasets with 32 hashing bits.
 Figure 2: Results of Precision-Recall curve with 32 hashing bits on Flickr 1 m and ReutersV 1 datasets. examples to the number of all returned examples and recall as the ratio of the number of retrieved relevant examples to the number of all relevant examples. The performance is averaged over all test queries in the datasets.
In this set of experiments, we compare our joint selection method against three other selection methods: 1. Actively select data examples and randomly select tags ( A d R t ). 2. Randomly select data examples and actively select tags ( R d A t ). 3. Randomly select data examples and Randomly select tags ( R d R t ). Clearly, our method can be regarded as A
A t . The size of data examples, L , is set to be 1000 for all datasets and the size of tags, M , is set to be 30 for the two image datasets and 10 for the other two text datasets. Note that for fair comparison, we adopt a modified version of [45], the A d R t selection method, which has the same labeling cost as our method by randomly selecting a set of tags instead of all tags.

We report the precision and recall for the top 200 retrieved examples with 32 hashing bits in Table 1. The precision and recall for the retrieved examples within Hamming radius 2 are shown in Table 2. It can be seen that AH-JDETS gives overall the best performance among all four selection methods on all datasets. From these comparison results, we can see that R d R t does not perform well in terms of both precision and recall. The reason is that the randomly selected data examples and tags may be noninformative. For example, these selected data might carry much redundant information if they are very similar to each other. It is also possible that the selected data examples have high certainties. In other words, the hashing codes of these data examples are already well-learned with high quality, and thus cannot contribute much for learning more effective hashing codes and function. We can also observe from the results that our AH-JDETS method outperforms the two methods A d R t and R d A t which either actively select data examples or select tags. This can be attributed to the joint selection strategy in our method, since it not only identifies the most informative data examples but at the same time selects the most informative tags. In this way, the learner could gain most information from labeling these selected data. Moreover, a two-sided paired t-test [29] is used to determine the statistical significance improvements in Tables 1 and 2. T-test shows that AH-JDETS significantly outperforms R d R t ( p &lt; 0.01), R &lt; 0.03) and A d R t ( p &lt; 0.03) on all datasets. The precision-recall curves of different selection methods with 32 hashing bits on Flickr 1 m and ReutersV 1 are reported in Fig.2. It can be seen that among all of these comparison methods, AH-JDETS shows the best performance, which is consistent with the results in Tables 1 and 2. We have also observed similar results on the other two datasets. But due to the limit of space, they are not presented here.

To evaluate the effect of different code lengths, we conduct another experiment by varying the number of hashing bits in the range of { 8 , 16 , 32 , 64 , 128 } . The precisions for the top 200 retrieved examples with different numbers of hashing bits on four datasets are shown in Fig.3. From this figure, we observe that larger code length gives better precision results on all datasets. It also can be seen that our method consistently outperforms the other three methods under different code lengths. top 200 retrieved examples with 32 hashing bits.
 Figure 3: Results of precision of top 200 examples on four datasets with different hashing bits.
We compare our AH-JDETS with two state-of-the-art hashing methods, Semantic Hashing using Tags and Topic Modeling (SHTTM) [37] and Self-Taught Hashing (STH) [43], on all datasets to demonstrate the advantages of our active hashing approach. SHTTM is a passive supervised hashing method which is briefly discussed in section 4. STH is an unsupervised hashing method that does not utilize any of the label information.

Precisions for the top 200 retrieved examples and the labeling costs of all three methods using 32 hashing bits are reported in Table 3. Note that there is no labeling cost for STH since it does not require any label knowledge. SHTTM is a supervised hashing method that incorporates all labels into hashing function learning. Its labeling cost is simply the number of total labels associated with the training examples. For our AH-JDETS, the labeling cost is LM since we have to label each data example and tag pair and there are total LM such pairs. We evaluate AH-JDETS with two different labeling costs, ( L , M ) and (5 L ,2 M ), where L =1 k and M =30 for image datasets and L =1 k and M =10 for text datasets. From these comparison results we can see that AH-JDETS achieves much better performance than the unsupervised method STH, while it also obtains comparable results with SHTTM (especially on (5 L, 2 M ) setting) but requiring much less labeling cost. For example, the labeling cost for SHTTM is about 1100 times more than our AH-JDETS with 1 k data examples and 30 tags on Flickr 1 m , which is impractical. Therefore, our AH-JDETS approach can be viewed as a good trade-off between unsupervised hashing and passive supervised hashing methods, which achieves good performance but saving much labeling efforts and thus overcomes the limitation of passive hashing methods. In this set of experiments, we evaluate the performance of AH-JDETS and A d R t methods by varying the set size L and M . It is obvious that we would gain more information by selecting more data examples and tags simultaneously. An interesting question would be: given the same labeling effort, how should we choose L and M to achieve best performance? Therefore, to answer this question, we fix the labeling cost LM = 30 k for image datasets and LM = 10 k for text datasets, and vary both L and M in the experiments. The code length is set to be 32 in all experiments.

The precision results of top 200 retrieved examples with respect to data example size on all datasets are shown in Fig.4. We can observe that the performances of both two methods are not satisfactory when selecting either very few data examples or tags (correspond to two end points in the figure). For example, in Flickr 1 m dataset, the right most red point corresponds to the combination of selecting 15000 data examples with 2 tags, while the left most red point represents the choice of 10 data examples with all 3000 tags. Although these two configurations have the same labeling cost 30 k , the performance is the worst among all possible combinations. It can be seen that the optimal combination is around 1200 data examples with 25 tags for our method on Flickr 1 m , which is roughly proportional to the total number of data examples and tags. Therefore, it is important to choose a good combination of L and M to achieve the best performance.

Consider a more realistic labeling cost measure scenario, where users are assigning M possible tags to a document or an image. Usually people first read through the content of the document or view the content of an image, which takes a reading cost r (may vary for documents and images), and then assign M labels to it. In this case, the labeling cost for a data example is r + M and the total labeling cost for L examples is ( r + M ) L . Using this cost measure, we conduct another experiment on Flickr 1 m by fixing r =5 and have found similar pattern as in Fig.4(a). However, we observe that the optimal combination drifts toward smaller number of data examples with more tags (around 940 data examples Figure 4: Precision results of varying batch sizes while fixing the labeling cost. Hashing bits are set to be 32 for all datasets. with 27 tags), which is consistent with our expectation due to the reading cost associated with each data example.
In our AH-JDETS approach, after obtaining the label information on the active set, we will update the labeled tags T T T to retrain the supervised hashing model as shown in Figure 1. The alternative process of learning hashing function and actively selecting data can be repeated for several iterations. In this set of experiments, we evaluate the performance of AH-JDETS by varying the number of iterations from 1 to 15 on all datasets. Note that the labeling cost grows linearly with the number of iterations since during each iteration the labeling cost is identical, i.e., LM .

Precisions for the top 200 examples with 32 hashing bits are reported in Fig.5. Not surprisingly, we can observe that the precision increases with the increasing number of iterations. However, we have found that the performance of AH-JDETS does not increase much after the first few iterations. Our hypothesis is that we have gained sufficient knowledge to learn a good hashing function within the first few iterations. In other words, the labeled data from the later iterations contains more and more redundant information, which does not contribute much for retraining a better hashing function. Therefore, the proposed AH-JDETS approach can obtain good performance in a few iterations, which also saves the labeling cost.
There are four trade-off parameters in AH-JDETS,  X  and  X  in supervised hashing component, and  X  and  X  in the joint data example and tag selection method. To prove the robustness of the proposed joint selection method, we conduct parameter sensitivity experiments of  X  and  X  on all datasets. In each experiment, we tune the parameter Figure 5: Precision results of increasing number of iterations on four datasets with 32 hashing bits. Flickr 1 m and ReutersV 1 in Fig.6. It is clear from these experimental results that the performance of AH-JDETS is relatively stable with respect to  X  and  X  . We have also observed similar results of the proposed method in the other two datasets. On the other side, it has already been shown in SHTTM [37] that the supervised hashing method is robust with respect to a wide range of  X  and  X  . This paper proposes a novel active hashing approach, Active Hashing with Joint Data Example and Tag Selection (AH-JDETS), which actively selects the most informative data examples and tags in a joint manner for learning hashing function. The selection principle is to identify data examples and tags that are both uncertain and dissimilar with each other. The labeled information is utilized together with unlabeled data to generate an effective hashing function. Extensive experiments on four different datasets demonstrate the advantage of the proposed AH-JDETS approach for learning a more effective hashing function with small labeling costs than the baseline passive supervised learning and some other active learning methods. In future, we plan to conduct new research for automatically selecting the optimal L and M . We also plan to model the information gain from the labels that the users input for better data selection. This work is partially supported by NSF research grants IIS-0746830, CNS-1012208, IIS-1017837 and CNS-1314688. This work is also partially supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370. Figure 6: Parameter Sensitivity for  X  and  X  . Results of precision of the top 200 examples with 32 hashing bits.
