 Investigations into brain connectivity aim to recover net-works of brain regions connected by anatomical tracts or by functional associations. The inference of brain networks has recently attracted much interest due to the increasing avail-ability of high-resolution brain imaging data. Sparse inverse covariance estimation with lasso and group lasso penalty has been demonstrated to be a powerful approach to discover brain networks. Motivated by the hierarchical structure of the brain networks, we consider the problem of estimating a graphical model with tree-structural regularization in this paper. The regularization encourages the graphical model to exhibit a brain-like structure. Speci cally, in this hier-archical structure, hundreds of thousands of voxels serve as the leaf nodes of the tree. A node in the intermediate layer represents a region formed by voxels in the subtree rooted at that node. The whole brain is considered as the root of the tree. We propose to apply the tree-structural regular-ized graphical model to estimate the mouse brain network. However, the dimensionality of whole-brain data, usually on the order of hundreds of thousands, poses signi cant compu-tational challenges. Efficient algorithms that are capable of estimating networks from high-dimensional data are highly desired. To address the computational challenge, we de-velop a screening rule which can quickly identify many zero blocks in the estimated graphical model, thereby dramat-ically reducing the computational cost of solving the pro-posed model. It is based on a novel insight on the relation-ship between screening and the so-called proximal operator that we rst establish in this paper. We perform experi-ments on both synthetic data and real data from the Allen Developing Mouse Brain Atlas; results demonstrate the ef-fectiveness and efficiency of the proposed approach. c  X  H.2.8 [ Database Management ]: Database Applications| Data Mining Algorithms Graphical lasso, tree-structural regularization, screening, second-order method, proximal operator, brain networks
The rich behavior of numerous complex systems is rooted in the underlying networks governing element interactions. For example, cells are best described as networks of molecules connected by chemical reactions; brains are commonly rep-resented as networks comprising a set of neurons intercon-nected by their communication pathways; our society is char-acterized by a network of individuals connected by social relationships. In reality, it is usually the behaviors of in-dividual elements, rather than their interactions, that are directly measurable. This gives rise to the central problem of how to identify system interaction structures by reason-ing backwards from the observed behaviors of the individual elements, a process known as network modeling.

Undirected graphical models explore relationships among a set of random variables through their joint distribution. The estimation of undirected graphical models has applica-tions in many domains, such as biology and medicine [10, 12, 33, 27]. One instance is the analysis of gene expression data. As shown in many biological studies, genes tend to work in groups based on their biological functions, and there exist some regulatory relationships between genes [6]. Such biological knowledge can be represented as a graph, where nodes are the genes, and edges describe the regulatory rela-tionships. Graphical models provide a useful tool for mod-eling these relationships, and can be used to explore gene interactions. One of the most widely used graphical models is the Gaussian graphical model (GGM). In the GGM, the variables are assumed to follow a Gaussian distribution [4, 35]. Then the problem of learning a graphical model is equiv-alent to estimating the inverse of the covariance matrix (pre-cisio n matrix), since the nonzero off-diagonal elements of the precision matrix represent edges in the graph [4, 35].
The main challenge of estimating a sparse precision ma-trix for problems with a large number of nodes (variables) is its intensive computation. Witten et al. [30] and Mazumder and Hastie [21] independently derived a necessary and suffi-cient condition for the solution of a single graphical lasso to be block diagonal (subject to some rearrangement of vari-ables). This can be used as a simple screening test to identify the associated blocks, and the original problem can thus be decomposed into a group of smaller sized but independent problems corresponding to these blocks. When the num-ber of blocks is large, it can achieve massive computational speedup. However, these formulations only assume that the graph is sparse. In many applications, domain structural knowledge exists and can potentially be exploited to im-prove the learning performance; in this case, structural reg-ularization can be used to improve the estimation of graph-ical model. However, due to the complexity of structural regularization, it is challenging to derive screening rules for general structural regularization.

To attack the above central challenge, we derive a screen-ing rule for structural Graphical Lasso in this paper. Speci -cally, we show that the derivation of the screening rule is crit-ically dependent on the so-called \proximal operator" asso-ciated with the structural regularization, e.g. , Lasso, group Lasso, or tree Lasso penalty. In recent years, tremendous efforts have been devoted to the efficient computation of the proximal operators, which plays a central role in structured sparse learning [3, 34]. In many cases, the proximal oper-ators, such as Lasso penalty, group lasso penalty, and tree group penalty, have closed form solutions, thereby leading to very efficient computation [17]. One of our major contribu-tions in this work is to establish a bridge between the com-putation of the proximal operator associated with a struc-tural regularization and the derivation of a screening rule for structural Graphical Lasso. To the best of our knowledge, our work represents the rst attempt to construct screen-ing rules for the Structural Graphical Lasso based on general structural regularization.

The major contributions of this work are summarized as follows: Notation: In this paper,  X  stands for the set of all real numbers,  X  n denotes the n -dimensional Euclidean space, and the set of all m n matrices with real entries is denoted by  X  m n . All matrices are presented in bold format. The space of symmetric matrices is denoted by S n . If X 2 S n positive semide nite (resp. de nite), we write X  X  0 (resp. X  X  0). The cone of positive semide nite matrices in S n is denoted by S n + . Given matrices X and Y in  X  m n , the standard inner product is de ned by  X  X ; Y  X  := tr( XY T where tr( ) denotes the trace of a matrix. The determinant of a real symmetric matrix X is denoted by det( X ). Given a matrix X 2  X  n n , diag( X ) denotes the vector formed by the diagonal of X , i.e. , diag( X ) i = X ii for i = 1 ; : : : ; n . Organization: The rest of this paper is organized as fol-lows. We introduce the structural graphical lasso formula-tion as well as the screening rule in Section 2. The exper-imental results for both synthetic data and real data are shown in Section 3. Related work is discussed in Section 4. We conclude the paper in Section 5.
Suppose we are given a data set X 2  X  n p with n samples, and p features (or variables). The n samples are indepen-dently and identically distributed with a p -variate Gaussian distribution with zero mean and positive de nite covariance matrix . Even if all features are correlated, there are usually many conditional independences among these fea-tures. In other words, a sparse precision matrix = 1 is of interest in most cases. This Gaussian graphical model (GMM) is also referred to as Gaussian Markov Random Field (GMRF). The negative log likelihood for the data X takes the form of where S is the sample covariance matrix given by S = 1 n X Minim izing (1) leads to the maximum likelihood estimation (MLE) = S 1 . However, there are some issues with MLE. In particular, it fails in the high-dimensional setting ( n &lt; p ), where MLE does not exist due to the singu-larity of S . To handle this issue, regularization is usually employed, resulting in penalized maximum likelihood esti-mation. For applications with prior domain knowledge, dif-ferent regularization terms can be employed to encourage the estimated model to satisfy the desired structural property. For example, a common assumption is that the graphical model is sparse. In this case, the  X  1 regularization has been employed to encourage sparsity [8]. In this paper, we con-sider the general structural graphical lasso, which integrates the structural regularization as follows: where  X  ( ) is the convex structural regularization. We refer to problem (2) as structural graphical lasso (SGL). Exam-ples include but are not limited to Figure 1: Illustration of two precision matrices (bot-tom) whose nodes are in different order correspond to the same graph with two connected components (top). The white color in the precision matrices rep-resents a zero entry.
 Figure 2: Illustration of the brain 2 . Yellow: frontal lobe; green: parietal lobe; red: temporal lobe; blue: occipital lobe. Number represents brain regions within lobes.

The penalized log likelihood function with a convex reg-ularizer, i.e. , problem (2), is strictly convex, however, the minimum of problem (2) may not be achievable. This is usually dependent on the property of the sample covariance matrix S . For example, diag( S ) &gt; 0 is a sufficient condition for problem (2) to have a unique solution [32] when the  X  regularization exists. For simplicity of presentation, we as-sume throughout the paper that the minimum of problem (2) can be achieved, i.e. , problem (2) has a unique solution.
The remainder of this section is organized as follows. We introduce a Tree-Guided Graphical Lasso formulation in Sec-tion 2.1. In Section 2.2, we present a second-order method to efficiently solve the proposed model. In addition, we de-rive a sufficient condition for estimating many zero blocks in the graph in Section 2.3. Based on this property, we pro-pose a simple screening rule which signi cantly reduces the complexity of the optimization problem, thus improving the computational efficiency. The proposed screening only relies htt p://www.umich.edu/~cogneuro/jpg/Brodmann.html on the data and the parameters, thus it can be combined with any existing algorithms to reduce the computational cost. We discuss two special cases in Section 2.4.
In this subsection, we present a hierarchical graphical model framework where the features exhibit a hierarchical struc-ture. A motivating example is the estimation of brain net-works. The brain is a multi-level system, and the brain network has a native hierarchical structure as shown in Fig-ure 2: hundreds of thousands of voxels form regions, and regions form systems.

We employ the tree-structural group regularization to en-courage the estimated graph to have a hierarchical structure. Speci cally, in this hierarchical structure, hundreds of thou-sands of voxels serve as the leaf nodes of the tree. A node in the intermediate layer represents a region formed by vox-els in the subtree rooted at that node. The whole brain is considered as the root of the tree. Mathematically, we solve the following formulation: where  X  ( ) = G i is the i -th group at depth j (the groups of a tree are de-ned in De nition 1 below; see Figure 3 for an illustration),
G j i ;G j i  X  denotes the submatrix of consisting of features :;:;off represents the matrix :;: excluding the diagonal elements. We do not penalize the diagonal elements of since is required to be positive de nite. For simplicity of notation, we use j ii  X  to represent G j resent G j estimated precision matrix to be tree-structural (see Figure 4 for an example). We formally de ne a tree structure as follows:
Definition 1. [18] For an index tree T of depth U , we let T u = f G 1 ; : : : ; G n i g contain all the nodes corresponding to depth u , where n 0 = 1 ; G 0 1 = f 1 ; : : : ; K g and n 1 ; : : : ; U . The nodes satisfy the following conditions:
We propose to employ the second-order method to solve the tree-guided graphical lasso problem in (3) as it has been shown to be quite efficient for solving the Graphical Lasso formulation with  X  1 regularization [11]. Let f ( ) be the smooth function in (3) such that Figure 3: A sample index tree. Root: G 0 1 = f 1 ; 2 ; 3 ; 4 ; 5 ; 6 ; 7 ; 8 g . Depth 1: G 1 1 = f 1 ; 2 f 3 ; 4 ; 5 ; 6 g , G 1 3 = f 7 ; 8 g . Depth 2: G 2 1 = f 1 G Figure 4: Illustration of a hierarchical graphical model. The features exhibit a hierarchical struc-ture speci ed by tree groups f G j i g : The blue blocks represent the nonzero blocks in the precision matrix. (3) can be rewritten as In the second-order method, we solve a \quadratic" model of (3) at each iteration de ned by where W t = 1 t and D = t , and t represents the t -th Newton iteration.

The subproblem (5) can be solved by non-monotone spec-tral projected gradient (NSPG) method [31]. When applied to (5), NSPG needs to solve the proximal subproblem in the form of wh ere Algo rithm 1: Tree-Guided Graphical Lasso (TGL) Input : S ; f G j i g ; f w j ii  X  g
Output :
Initialization: 0 = (Diag( S )) 1 ; while Not Converged do 3 Compu te the Newton direction D by solving (5) 4 Choose t +1 by performing the Armijo end and r den otes the r -th inner iteration in NSPG. Denote and then is given by where [ min ; max ] is a prede ned safeguard.

After obtaining the optimal solution of (5) , the New-ton direction D can be computed as
Once the Newton direction is obtained, we need to nd an appropriate step size 2 (0 ; 1] to ensure a sufficient reduc-tion in the objective function in (4). Because of the positive de nite constraint in (4), we need to ensure the next iterate t +1 = t + D to be positive de nite. It is not hard to show that such step size satisfying the above requirements always exits [11]. Thus, we can adopt the Amrmijo's back-tracking line search rule to select a step length 2 (0 ; 1]. We use the Cholesky decomposition to check the positive de -niteness of t +1 = t + D . In addition, the log det( t +1 and 1 t +1 can be efficiently computed as a byproduct of the Cholesky decomposition of t +1 . The algorithm is summa-rized in Algorithm 1.

Under the assumption that the subproblem (5) is solved exactly, the convergence rate of the second-order method is locally quadratic when the exact Hessian is used [11, 14, 28]. If the subproblem (5) is solved inexactly, the conver-gence rate of the second method is locally superlinear by adopting an adaptive stopping criterion in NSPG [14]. Due to the use of Cholesky decomposition and the need of com-puting tr( W t DW t D ) in (5), the complexity of Algorithm 1 is O ( p 3 ).
Due to the existence of the log determination, it is com-putationally expensive to solve the penalized log likelihood model (3) by applying Algorithm 1 directly. The screening strategy has commonly been employed to reduce the size of optimization problems so that a massive computational gain can be achieved. In this section, we derive a sufficient con-dition for the solution of SGL to be block diagonal (subject to some rearrangement of features; see Figure 1 for illus-tration), thus signi cantly reducing the complexity of the problem.
Let C 1 ; : : : ; C L be a partition of the p features into L non-overlapping sets such that We say that the solution b of SGL (2) is block diagonal (subject to some rearrangement of features) with L known blocks C l ; l = 1 ; : : : ; L if b ij = b ji = 0 for i 2 C  X  ; l  X  = l  X  . Without loss of generality, we assume that a block diagonal solution b with L blocks C l ; l = 1 ; : : : ; L takes the form of where b l is the j C l j j C l j symmetric submatrix of b sisting of features in C l .

Since the elements in off diagonal blocks are zero, the original optimization problem can thus be reduced to a much smaller problem restricted to the elements in the diagonal blocks, resulting in a great computational gain. Our main result is summarized in the following theorem:
Theorem 1. Suppose U d +1 = S , where d is the depth of the tree structure. For different groups G j i at the depth j , de ne U j recursively as follows: A sufficient condition for the solution of SGL to be block diagonal with blocks C 1 ; : : : ; C L is that U j at some layer j has the same block diagonal structure such that and there is no group G j i across two blocks, that is, there do not exist x 1 2 C l , and x 2 2 C l  X  , such that f x 1 ; x
Proof. By the rst-order optimality condition, optimal solution of problem (2) if and only if it satis es
Suppose that U j at layer j has the block diagonal struc-ture C 1 ; : : : ; C L such that and there is no group G j i across two blocks. According to [18], it is not hard to show that U j is the solution of the following problem: wh ere and U 0 is the solution of Accord ing to Theorem 1 in [18], we have U k 0 ; : : : ; j 1 if U j ii  X  = 0. Thus, we have As U k that 0 2  X  ( ) k of  X   X  F is achieved at 0. Then, we have since 0 2  X  ( ) k rst-order optimality condition holds for the elements in off diagonal blocks.

Next we show how to construct a b which satis es the rst optimality condition. Let b be a block diagonal matrix with blocks C l ; l = 1 : : : ; L . It is clear that the optimality con-dition of (2) for off diagonal elements are satis ed. We can let the elements in the diagonal blocks of b be the solution of the following problem: Since U k optimality condition (10) holds for b , thus b is the optimal solution of (2). This completes the proof of the theorem.
Theorem 1 can be used as a screening rule to determine the elements in the identi ed off-diagonal blocks to be zero in advance. Assume that there are L blocks of the same size identi ed by the screening rule, p 2 (1 1 L ) elemen ts do not need to be computed as the optimal values for these elements are determined as 0 by the screening. Recall that the com-plexity of the proposed second-order method is O ( p 3 ) due to Cholesky decomposition and computation of tr( W t DW t D ). The complexity of solving the proximal operator (11) for the tree group structural regularization is O ( p 2 ) [18]. By applying the screening rule, the complexity of Cholesky de-composition and computation of tr( W t DW t D ) are reduced to O ( p 3 =L 2 ), and the complexity of solving (11) is reduced to O ( p 2 =L ). Therefore, the complexity of the second-order method with screening is O ( p 3 =L 2 ) since L p . When L is large, application of the screening rule can achieve a great computational gain.
We want to emphasize that Theorem 1 provides a screen-ing rule for a large family of graphical model problems. Several examples in the literature can be reformulated into problem (2) with speci c constraints. In the following, we provide several examples as follows.  X  regularization: When the  X  1 regularization is used, SGL degenerates to standard graphical lasso [4, 8] given by: The proximal operator in (11) can be written as Acco rding to Theorem 1, the sufficient condition for the op-timal solution ( i.e. , the solution of graphical lasso based on the  X  1 regularization) to have a block structure C 1 ; : : : ; C is that the optimal solution b X of (12) has the same block di-the following rst order optimality condition is satis ed which is exactly the same as the screening condition for graphical lasso proposed in [21, 30]: Thus, the screening rule in [21, 30] is a special case of the proposed rule.
 Group regularization: The graphical lasso with group regularization has been studied in [13]. The formulation of group graphical lasso is given by where G i ;G j is a submatrix of , and G i is the i -th group of features. Note that [ G i = f 1 ; : : : ; p g and different groups do not overlap. In [13], Kolar et al. proposed a sufficient condition for the solution of group graphical lasso to be block diagonal, which is given by It is clear that condition (13) is the rst-order optimality condition for the solution of (11) to have the block diagonal solution C 1 ; : : : ; C L . Thus, the screening rule in [21, 30] is also a special case of the proposed rule.
 Figure 5: The ontology hierarchy of the Allen De-veloping Mouse Brain Atlas from level 0 to level 5. Each brain region is colored using the color code of the Allen Developing Mouse Brain Reference Atlas.
In this section, we conduct experiments to demonstrate the effectiveness and efficiency of the proposed screening rule and the proposed tree-guided graphical lasso (TGL). We used both synthetic and real mouse brain gene expres-sion data to evaluate our methods. The experiments are performed on a PC with quad-core Intel i7 3.4GHz CPU with 16GB of RAM. The TGL formulation is implemented in MATLAB, while the sub-routine for solving the subproblem (6) is implemented in C. We compare TGL with standard graphical lasso (GLasso) in the experiments.
We rst evaluate our method using synthetic data. We follow [32] in generating the synthetic covariance matrix. Speci cally, we rst generate the ground truth precision ma-trix with random block nonzero patterns. Each nonzero block has a random sparse structure. Given the precision matrix , we sample from a Gaussian distribution to com-pute the sample covariance matrix. The weights for tree-structural group regularization take the form of where is a given positive parameter and j j ii  X  j is the num-ber of elements in j ii  X  . To make a fair comparison between different methods, we control the regularization parameters of TGL and GLasso to ensure the numbers of edges obtained from both estimations to be the same.

Figure 6 shows the comparison between TGL and GLasso in terms of edge detection. The rst column of Figure 6 shows the nonzero patterns ( i.e. , edges) of two ground truth precision matrices. In both cases, the same index tree is used, which is given by We can observe from Figure 6 that the nonzero patterns of the precision matrices estimated by TGL are more similar to the ground truth than GLasso. These results demonstrate that TGL outperforms GLasso in terms of detecting true edges in the precision matrices.

We conduct experiments to demonstrate the effectiveness of the proposed screening rule. We terminate NSPG using the following stopping criterion: Addit ionally, TGL is terminated when the relative error of the objective value is smaller than 1 e -5. The used index tree is given by of nonzeros in the solution.
 screenin g TGLs where L is th e number of blocks. The time comparison re-sults are given in Table 1. We can observe that the compu-tational time of screening is negligible compared with that of solving the TGL. Since the complexity of identifying the connected components is O(  X   X  0 ), the computational time of screening is almost linear with respect to  X   X  0 . Results in Table 1 demonstrate that the screening rule can achieve very signi cant computational gain. The larger the L is, the higher the speedup is. These results demonstrate the potential of our method for identifying structured networks for large-scale data.
We also evaluate our methods using the Allen Develop-ing Mouse Brain Atlas data. The Allen Developing Mouse Brain Atlas contains spatiotemporal in situ hybridization (ISH) gene expression data across multiple stages of mouse brain development [26, 1]. The primary data consist of 3-D, cellular resolution ISH expression patterns of approximately 2000 genes in sagittal plane across four embryonic (E11.5, E13.5, E15.5, and E18.5) and three early postnatal ages (P4, P14, and P28). The ISH image series are passed through an informatics data processing pipeline by which they are converted to grid-level expression summaries in the same coordinate space [2].

After the ISH image series are mapped to the reference space, a gridding module is applied to divide the 3-D refer-ence space into regular grids, creating a low resolution 3-D summary of the gene expression. The resolution of the data grids varies with age. For each grid voxel, expression density is the number of expressing pixels divided by the number of image pixels in the voxel; expression intensity is the averaged inverted ISH gray-scale value at expressing pixels within the span of the grid voxel; expression energy is de ned as the product of expression intensity with expression density. Our analysis in this work is also based on the grid-level expres-sion energy. In this work, we use data from the rst three developmental ages with 7796, 9963, and 8258 structural voxels, respectively. We use a data set of 1724 genes.
We apply the proposed TGL method to the voxel-level gene expression data to demonstrate the effectiveness of TGL and the proposed screening rule. In the Allen Develop-ing Mouse Brain Atlas, the brain regions are organized into a tree-structural hierarchy as shown in Figure 5. This provides an ideal setting for evaluating our proposed tree-structural graphical Lasso formulation. We use such hierarchical struc-ture as the input prior knowledge to our algorithm TGL. We compare TGL with the standard GLasso on this data. Fig-ure 7 shows the comparison between the precision matrices estimated by TGL and GLasso. We can observe that, al-though the data inherently exhibits certain tree structures, the results obtained by GLasso do not recover these struc-tures clearly. In contrast, our proposed TGL method suc-cessfully recovers the hierarchical structures. Nevertheless, GLasso recovers some overall structures that are largely con-sistent with the hierarchy with the corruption of some noises.
To demonstrate the power of the proposed screening, we report the running time of the TGL with and without screen-ing. We use the data from the rst stage for our evaluation. We stop the computation of the algorithm after we obtain a solution with precision 1 e -6. The computational time of TGL without screening is 57189.6 seconds. With the screen-ing, the total computational time of TGL-S including the time for screening is reduced to 2781.5 seconds, demonstrat-ing the superiority of the proposed screening rule.
Brain connectivity describes how the brain regions are connected, thereby providing information pathways in the brain. Graphical modeling is a statistical tool to capture the connectivity between multiple random variables. Thus graphical models are natural tools for brain connectivity analysis. However, the dimensionality is usually very large for brain data, and this prohibits the direct application of many existing methods. Therefore, large-scale brain net-work estimation is considered as a big data problem and has raised several challenges and opportunities [20].
The task of estimating the whole brain connectivity is im-portant, but also very challenging. There are two major types of connectivity analysis; namely functional connectiv-ity and effective connectivity. There are a few simple ap-proaches for estimating functional connectivity, i.e. , these based on pair-wise correlations, clustering, and independent component analysis (ICA) [9]. Effective connectivity aims to nd directional relationships between brain regions. Pop-ular approaches for effective connectivity include dynamic causal modeling, structural equation models, and Granger causality. These tools are complex in computation and mod-eling, thus they are usually applicable for a small number (e.g. &lt; 100) of preselected voxels or regions. Recently, voxel correlations are used to provide more accurate selection of voxels for a certain region of the brain. However, the results may be sensitive to the selection of regions, and the network inference can be biased if the in uence from other omitted regions is large [20]. To date, several challenges remain in inferring large-scale direct connectivity.

Sparse Gaussian graphical models (sGGM) [4, 8, 21, 22, 35] are proposed to estimate large-scale brain connectivity. This type of models has a solid probabilistic foundation for distinguishing direct connections from indirect connections. Suppose we have a multivariate variable X following a p -variate normal distribution N ( ; ), and we are given n i.i.d observations. sGGM represents the relationships be-tween the p variables by a network of p nodes, where each node represents a variable and there are connections between nodes. Formally, inference of the connections between the p nodes is reduced to estimating a sparse inverse covariance cates that the corresponding row and column variables are connected. Similarly, a zero entry indicates the absence of connection. The sGGM approach performs well on a simu-lation study using a small number of regions [20].
In recent years, considerable research efforts have been de-voted to estimating the precision matrix and the correspond-ing sGGM [11, 12, 15, 16, 19, 23, 24]. Numerous methods have been developed for solving this model. For example, Banerjee et al. [4] and Friedman et al. [8] proposed block co-ordinate ascent methods for solving the dual problem. The latter method [8] is widely referred to as Graphical lasso (GLasso). Yuan [36] and Scheinberg et al. [25] applied the alternating direction method of multipliers (ADMM) [5] to this problem. Wang et al. [29], Hsieh et al. [11], Olsen et al. [24], and Dinh et al. [7] applied the Newton method for solving this model.

The brain network system is complex and structured. For example, brain regions are usually organized into a hierarchy in which a large region includes multiple sub-regions. We propose a tree-structural graphical model to represent the multi-level brain network in this paper. Speci cally, voxels are represented as the leaf nodes of the tree. The nodes in the intermediate layer represents the regions. This way, the entire brain is considered as the root of the tree. Our model is different from the model in [20] in multiple ways, and our proposed model is more general. Speci cally, the nodes in [20] can only connect with each other via the hub nodes, while the nodes can connect in arbitrary ways in our model. In [20] an alternating update algorithm is proposed to solve the model, and much computational efforts have been devoted to computing the determinant of . This prohibits the direct application of graphical models from large-scale brain datasets. The contributions of this paper lie in two folds: (1) we propose a tree-structural graphical model to incorporate the multi-level brain structure; and (2) we develop a sufficient screening rule to dramatically reduce the computational cost for computing the determinant of in general structural graphical model. [4] O. Banerjee, L. El Ghaoui, and A. d'Aspremont. [5] S. Boyd, N. Parikh, E. Chu, B. Peleato, and [6] H.Y. Chuang, E. Lee, Y.T. Liu, D. Lee, and T. Ideker. [7] Q. Dinh, A. Kyrillidis, and V. Cevher. A proximal [8] J. Friedman, T. Hastie, and R. Tibshirani. Sparse [9] Karl J Friston. Functional and effective connectivity: [10] J. Guo, E. Levina, G. Michailidis, and J. Zhu. Joint [11] C.J. Hsieh, M. A. Sustik, I. S. Dhillon, and [12] S. Huang, J. Li, L. Sun, J. Liu, T. Wu, K. Chen, [13] M. Kolar, H. Liu, and E. Xing. Markov network [14] J. D. Lee, Y. Sun, and M. A. Saunders. Proximal [15] L. Li and K.C. Toh. An inexact interior point method [16] H. Liu, K. Roeder, and L. Wasserman. Stability [17] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with [18] J. Liu and J. Ye. Moreau-yosida regularization for [19] Z. Lu. Smooth optimization approach for sparse [20] X. Luo. A hierarchical graphical model for big inverse [21] R. Mazumder and T. Hastie. Exact covariance [22] R. Mazumder and T. Hastie. The graphical lasso: New [23] N. Meinshausen and P. B  X  uhlmann. High-dimensional [24] P. A. Olsen, F. Oztoprak, J. Nocedal, and S. J. [25] K. Scheinberg, S. Ma, and D. Goldfarb. Sparse inverse [26] Carol L Thompson, Lydia Ng, Vilas Menon, Salvador [27] R. Tibshirani. Regression shrinkage and selection via [28] P. Tseng and S. Yun. A coordinate gradient descent [29] C. Wang, D. Sun, and K.C. Toh. Solving [30] D. M. Witten, J. H. Friedman, and N. Simon. New [31] S. J. Wright, R. D. Nowak, and M.A.T. Figueiredo. [32] S. Yang, Z. Lu, X. Shen, P. Wonka, and J. Ye. Fused [33] S. Yang, L. Yuan, Y.C. Lai, X. Shen, P. Wonka, and [34] J. Ye and J. Liu. Sparse methods for biomedical data. [35] M. Yuan and Y. Lin. Model selection and estimation [36] X. Yuan. Alternating direction method for covariance
