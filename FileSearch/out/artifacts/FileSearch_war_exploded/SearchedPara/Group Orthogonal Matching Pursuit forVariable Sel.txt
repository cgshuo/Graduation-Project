 may involve interpreting the models and making decisions based on them. proposed the Group Lasso, which solves arg min  X  1 2 where X G Composite Absolute Penalties family T (  X  ) = investigating to what extent they carry over to the new scenario.
 suit X  algorithm ( Group-OMP ), which extends the OMP algorithm to leverage variable groupings, which generalizes the  X  X xact Recovery Condition X  of [9](Theorem 3.1) stated for OMP under the pare the performance of Group-OMP with existing methods, on simulated and real world datasets. Our results indicate that Group-OMP favorably compares to the Group Lasso, OMP and Lasso al-include [10, 3] using OMP for simultaneous sparse approximation, [11] showing that standard MP selects features from correct groups, and [4] that consider a more general setting than ours. presented in Section 4. We conclude the paper with some discussions in Section 5. [ f variables of X consisting of J groups X G i 6 = j and X G the groups, i.e. y = assume that each X G Group-OMP procedure we propose is described in Figure 1, which extends the OMP procedure to boosting-like procedures.  X  Initialization: G (0) =  X  ,  X  (0) = 0 .
 For k = 1 , 2 , . . .
 End 3.1 Notation G let g good and g bad denote the set of  X  X ood incides X  and  X  X ad indices X , i.e. g good = The same holds for G bad and g bad . In this notation supp(  X   X  )  X  g good . We denote by  X  X ( G good ) the smallest eigenvalue of X  X  g v = { v 1 , . . . , v | g Then we define  X  X ( G good ) = k X + g 3.2 The Noiseless Case G m +1 , . . . , G J . Let r  X  Span ( X g good ) . Then the following holds { G m +1 , . . . , G J } . Let  X   X  : R n  X  R d 1  X  R d 2  X  . . .  X  R d m be defined as can be rephrased as Lemma 1. The map  X   X  restricted to Span spaces spanned by X G ( X G following Proof of Lemma 2. We prove for V  X  , the proof for V  X  is identical.
 k v  X  k = sup The last equality follows from sup isomorphism. Thus of Theorem 1.
 Exact Recovery condition for the standard OMP algorithm, namely that k X + g model (since the notion of groups does not pertain to OMP in its original form). 3.3 The Noisy Case sketch of the proof is provided at the end of this section.
 then when the algorithm stops all of the following hold: We thus obtain the following theorem which states the main consistency result for Group-OMP. Theorem 2 and Theorem 3 are similar to those required for the standard OMP algorithm [13], the the standard OMP case. The following lemma gives a lower bound on the correlation between the to a set of good groups.
 f = X X  and f 0 = X X  0 . Then max G set of good groups has been correctly identified, to the true parameter  X   X . Lemma 4. For all  X   X  (0 , 1) , with probability at least 1  X   X , we have from the prediction by OLS given that the set of good groups has been correctly identified. Lemma 5. Let  X  0 =  X   X  X ( G good , y ) and f 0 = X X  0 . We have have Lemma 3 together with the definition of  X  X ( G good ) implies We then have to deal with the following cases.
 where the last inequality follows from Eq. 7. Then Eq. 6 implies that max G group is selected, i.e., G i ( k )  X  X  good and Eq. 9 implies that the algorithm does not stop. Case 2.1: G i ( k )  X  X  good and the procedure does not stop.
 Case 2.3: G i ( k ) 6 X  G good in which case we have max G max G max G max G max G where the last inequality follows by Eq. 7. Hence the algorithm stops.
 G k  X   X  constraints. This leads to Theorem 2. 4.1 Simulation Results We empirically evaluate the performance of the proposed Group-OMP method, against comparison methods OMP, Group Lasso, Lasso and OLS (Ordinary Least Square). Comparison with OMP will arg min  X  penalty parameter  X . For Group-OMP and OMP rather than parameterizing the models according to experimental setup.
 The responses in the data are generated using the true model: each with 50 observations for training and 25 for validation.
 Experiment 2: We use an additive model with continuous variables taken from [12](model III), defined as W i = ( Z i + Z 17 ) /  X  ran 100 runs, each with 100 observations for training and 50 for validation.  X  i.i.d.  X  X  (0 ,  X  = 0 . 1 1 / 2 ) . The true model is We ran 100 runs, each with 500 observations for training and 50 for validation.  X  19 . 22) , and 300 observations for training and 50 for validation.
 output by Ordinary Least Squares, Lasso, OMP, Group Lasso, and Group-OMP. output by OLS, Lasso, OMP, Group Lasso, and Group-OMP on the X  Boston Housing X  dataset. 4.2 Experiment on a real dataset We use the  X  X oston Housing X  dataset (UCI Machine Learning Repository). The continuous vari-confirm that Group-OMP has the highest prediction accuracy among the comparison methods, and also leads to the sparsest model. forward/backward extension that allows correcting for mistakes (similarly to [14]). [3] C HEN J., H UO X. , Sparse representations for multiple measurement vectors (MMV) in an [4] H UANG J., Z HANG T., M ETAXAS D. , Learning with Structured Sparsity, in ICML X 09, 2009. [5] M ALLAT S., Z HANG Z. , Matching pursuits with time-frequency dictionaries , IEEE Transac-[10] T ROPP J.A., G ILBERT A.C. , S TRAUSS M.J. , Algorithms for simultaneous sparse approxi-[14] Z HANG , T. , Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear
