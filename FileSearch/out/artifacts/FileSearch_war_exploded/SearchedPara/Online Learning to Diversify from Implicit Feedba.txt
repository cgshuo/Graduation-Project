 In order to minimize redundancy and optimize coverage of multiple user interests, search engines and recommender sys-tems aim to diversify their set of results. To date, these di-versification mechanisms are largely hand-coded or relied on expensive training data provided by experts. To overcome this problem, we propose an online learning model and al-gorithms for learning diversified recommendations and re-trieval functions from implicit feedback. In our model, the learning algorithm presents a ranking to the user at each step, and uses the set of documents from the presented rank-ing, which the user reads, as feedback. Even for imperfect and noisy feedback, we show that the algorithms admit the-oretical guarantees for maximizing any submodular utility measure under approximately rational user behavior. In ad-dition to the theoretical results, we find that the algorithm learns quickly, accurately, and robustly in empirical evalua-tions on two datasets.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Theory Online Learning, Diversified Retrieval, Submodularity
Modeling the dependencies between items in a ranking of results is one of the most promising directions for improving the quality of retrieval and recommendation systems. First, consider the example of a search engine and a query such as  X  X aguar X  or  X  X pple X . For such queries, it is important to present a diverse set of results since diversity hedges against uncertainty about the users intent. Such hedging against uncertainty about the user X  X  information need is called ex-trinsic diversity [10]. A second reason for diversity is called intrinsic diversity [10] where it is important to avoid redun-dancy and provide a set of results that cover multiple aspects of an information need. For example, of all the articles in the NY Times on a given day, a user only has time to read a small subset. Therefore, even if the user is interested in the European Debt Crisis, he may not want to read exclusively about this one topic, but rather read one article and also cover other topics. In the following, we focus on problems where such intrinsic diversity is important.

In this paper, we extend the recently proposed coactive learning model [14] to learn diversified results from implicit user feedback. In particular, we develop two algorithms for learning both relevance and the desired amount of diversity from set-valued preference data that can be derived from implicit feedback. The algorithms proposed in this paper are easy to implement and allow theoretical analysis. Fur-thermore, the ability to learn the desired amount of diversity based on user feedback makes the algorithms attractive for a wide range of applications where the required amount of diversity is not determined apriori. A crucial extension over the methods in [14] is that we now consider models with submodular structure, whose diminishing returns property makes it possible to avoid redundancy and increase novelty.
Coactive learning proceeds in the following online fash-ion. In each step, a ranking is presented to the user that (approximately) maximizes the current estimate of the sub-modular utility function. As feedback, the algorithm ob-serves the (possibly diverse) set of documents the user reads in the presented ranking. After receiving this feedback, the algorithm updates its model. Even though we allow user feedback to be imperfect, noisy, and only  X  X eakly informa-tive X  (in a specific sense), we are able to prove guarantees on the performance of the algorithm. Unlike the theorems in [14], our guarantees apply even though submodular models only allow for approximate inference. Finally, experiments demonstrate the empirical effectiveness of the proposed ap-proach in learning both relevance and diversity.
Presenting a diverse set of results is an important goal in both web-search ranking as well as recommender systems research. While much prior work on diversity has focused on non-learning approaches (e.g. [2, 19, 3, 16, 4]), recently developed supervised learning methods for diversity have shown a lot of promise (e.g. [13, 18, 12, 8]). Unfortunately, supervised learning relies on manually judged training data with multi-topic annotations, which are expensive and dif-ficult to obtain. El-Arini and Guestrin [6] proposed an ap-proach to discover relevant scientific literature based on a set of scientific papers. They retrieve a set of papers based on both diversity and relevance. While their approach also makes use of submodular influence measures, they assume noise-free feedback, which is unrealistic for our problem.
While some online learning methods exist that can exploit click data, those methods either cannot generalize across queries [11] and/or have a hard-coded notion of diversity that cannot be adjusted through learning [15]. Recently, Yue and Guestrin [17] proposed online learning algorithms to maximize submodular utilities and applied them to di-versified retrieval. However, their model relies on observing cardinal utilities whereas in our model we only rely on im-plicit preference feedback. User studies for web search [7] have shown that such preference feedback can be extracted reliably from observable user behavior (e.g. clicks), while at-tempt to interpret click-data as cardinal utility statements were found to be biased and unreliable.
To illustrate our learning model, consider the example of a personalized news reader that users visit on a daily basis. On day t , the news reader suggests a list of articles y t ( d 1 ,d 2 ,d 3 ,d 4 ,d 5 , ... ) and observes which of these articles are actually read by the user. We assume that the decision to read an article is influenced by two factors. First, the article must be relevant to the user X  X  interest. Second, the decision may have dependencies with other articles in y .For example, the user may be interested in the European debt crisis. But the user may only want to read one article related to this issue, even if y contains 5 relevant articles.
In this paper, we design an online learning algorithm that can model both relevance as well as interdependencies be-tween documents. The training data we exploit are the sets of documents read by the user each day. Continu-ing the example from above, the system may observe that the user read articles d 3 and d 5 . Obviously, we cannot conclude that { d 3 ,d 5 } was the optimal set of articles the user wanted to read on day t , since there may have been other articles far down the list that the user never saw. However, we can conclude that the user would have pre-to  X y t as the user feedback ranking .

We now define the learning problem and the user-interaction model more generally. At each round t , our algorithm presents aranking y t from a corpus x t  X  X  of candidate documents 1 We assume that the user acts (approximately) rational ac-cording to an unknown utility function U ( x t , y t ) that models both relevance of the documents as well as their dependen-cies (e.g. redundancy). In the context of such a utility function, we can interpret the user feedback as a preference between rankings. This type of preference feedback over multiple rounds t is the input for our learning model. Given the set of candidate documents x t ,the optimal ranking is denoted by In general, x t can also represent a query/context. Since the user X  X  utility function U ( x t , y ) is unknown, this optimal ranking y  X  t cannot be computed. The goal of the learning algorithm is to predict rankings with utility close to that of y  X  t . Note, however, that the user feedback does not even give the optimal y  X  t to the algorithm (as in traditional supervised learning), but only the user feedback ranking  X y is observed. To analyze the learning algorithms in the sub-sequent sections, we refer to any feedback that satisfies the following inequality as strictly  X  -informative feedback : The above inequality states that the utility of the user feed-back ranking  X y t must be slightly better than the utility of the ranking y t that was presented as a fraction of the dif-ference between the utility of y  X  t and the utility of y demonstrated in the example above, such a slightly improved rankings  X y t can be constructed as a reordering of y t based on user clicks. The amount of improvement is quantified by  X   X  (0 , 1], which is (an unknown) parameter in the above inequality that controls by what fraction the utility of the feedback ranking  X y t is higher than that of the predicted ranking y t as compared to the maximum possible utility gain. To allow noisy feedback, we introduce slack variables  X  which allow violations of the above condition. This gives the following user feedback, referred to as  X  -informative feedback :
U ( x t ,  X y t )  X  U ( x t , y t )=  X  ( U ( x t , y  X  t ) The above feedback model can be further relaxed, requiring that it merely holds in expectation over feedback. We refer to this as expected  X  -informative feedback
E [ U ( x t ,  X y t )  X  U ( x t , y t )]=  X  ( U ( x t , y and many of our results can be generalized to his weaker form of feedback as well. Note that the above expectation is over user X  X  choice of  X y t given y t for corpus x t (i.e., distri-slack variable.

To measure the performance of our method we define a no-tion of regret based on the utility of the ranking we present with respect to the utility of the best possible ranking y that could have been presented in each step: Note that regret is measured with respect to the user X  X  true utility function U ( x t , y t ) and the optimal ranking y though neither is ever explicitly revealed to the algorithm. Thus a decreasing regret indicates the utility of the predicted ranking improves over time.
A key step in designing a learning algorithm that models both relevance and diversity lies in the design of an appro-priate hypothesis space for modeling U ( x , y ). In short, the learning algorithm needs to learn an accurate model of how the user values a ranking y for a given x . Since this relates to metrics for evaluating retrieval systems, we start our design of U ( x , y ) based on existing retrieval measures. While traditional IR metrics are oblivious to diversity (e.g. NDCG, Precision), more recent additions account for diver-sity in some form (e.g. [16, 11, 18, 1, 5]). We define our hy-pothesis space based on the family of performance measures proposed in [12], since it subsumes many existing measures. These measures exhibit a diminishing returns property (i.e. submodularity), which means that the marginal utility of a document is lower if the intents the document is relevant to are already represented in the ranking.

While [12] focuses on the case of extrinsic diversity, the same model structure also applies to problems with need for intrinsic diversity. In particular, we model U ( x , y )as a function that is linear in its parameters w  X  R m with w  X  0, 2 but submodular (and non-linear) in a feature map  X  ( x , y )  X  R m with  X  ( x , y )  X  0: The parameters w will be learned by the learning algorithm. The feature vector  X  ( x , y ) describes the ranking, but for simplicity of exposition we will consider y to be the set con-sisting of the top k results that were viewed by the user, not the full ranking 3 . The function  X  ( x , y ) generates a fea-ture vector describing the set y = { d i 1 ,d i 2 , ..., d context x = { d 1 ,d 2 , ..., d | x | } in the following manner: We assume that each document d itself is described by a feature vector  X  ( d ). These feature vectors are aggregated into the feature vector  X  ( x , y )of y using an aggregation function F . Let  X  j ( x , y )bethe j -th feature of  X  ( x , y )and  X  j feature of  X  ( d ), then Examples of the per-feature aggregation function F are:
The MAX variant, but not LIN, encourages diversity in the following way. As example, consider a boolean bag-of-words representation of documents  X  ( d ). The first document to contain a term t will increase the feature value of t in  X  ( x , y ) by 1. The second document to contain t , however, will not cause any increase. This models the redundancy of multiple occurrences of t ,asitdoesnotgiveanybenefitto all but the first occurrence of t . Note that multiple aggrega-tion functions F can be stacked into  X  ( x , y ), which allows the linear model to select a desired diminishing-returns pro-file. Note also that our model is not restricted to the F listed above, but rather any F can be used that is mono-tone and submodular [12], including less stringent functions which allows for some redundancy (like square root).
To compute the ranking that maximizes a utility function, i.e. y := arg max y  X  X  w  X  ( x , y ) , one can use the simple and efficient Greedy Algorithm 1. At each step, the algorithm greedily chooses the document with the highest marginal utility to be added to the ranking. Note that y  X  d is used to refer to the operator that appends document d to ranking y . Also note that Algorithm 1 computes the exact utility
Denotes component-wise non-negativity.
A ranking can be viewed as a nested structure of top-k sets, and the greedy algorithm we will later use to compute rankings uniformly optimizes the utility of the sets at any cutoff in the ranking.
 Algorithm 1 GreedyRanking( w , x ) y  X  0 for i =1 to k do return y optimizer y t for the modular measure LIN, whereas it finds a1  X  1 /e approximate y t for any submodular and monotone function F .
In this section, we present our coactive learning algo-rithms. In section 5.1, we present a perceptron style al-gorithm and then a clipped version of it. In section 5.2 we present an exponentiated gradient algorithm. We prove re-gret bounds for all the proposed algorithms.
We now describe our first learning algorithm for mini-mizing regret (5) for utility functions of the form (6). Al-gorithm 2, which we call the Diversifying Perceptron (DP) , maintains a weight vector w t which is initialized to 0 .Ateachtimestep t , DP presents a ranking y t from the corpus x t using Algorithm 1 with the current estimate w t DP then uses the user feedback ranking  X y t (obtained as outlined in Section 3) to update the weight vector w t in the modeling user feedback (in Eqns. (2) and (3)) is unknown to the algorithm; it only plays a role in the analysis.
The following theorem describes the generalization per-formance of the Diversified Perceptron. Note that bound on the worst-case regret is independent of the dimensionality of the feature space, that the regret converges to its asymp-tote at the rate of 1 / examples), and that the informativeness  X  of the feedback enters the bound only linearly. The first term of the bound captures the noise in the feedback.

Theorem 1. The average regret of the diversified percep-tron algorithm can be upper bounded, for any w  X  R m with w  X  0 that defines the utility in Eq. (6), as follows:
REG T  X  1 Here 1  X  +1 is the approximation factor of the greedy algorithm with  X   X  2 and  X  ( x , y ) 2  X  R .

Proof. Consider the 2 norm of w T : The first line comes from the update rule in Algorithm 2. The second line is from the fact: w T  X  1  X  ( x T  X  1 ,  X y Algorithm 2 Diversifying Perceptron.

Initialize w 0  X  0 for t =0 to T  X  1 do 1) w T  X  1  X  ( x T  X  1 , y T  X  1 ) since the greedy algorithm produces an 1  X  +1 approximation and that  X  (  X  ,  X  )  X  R . The third line comes by using the Cauchy-Schwarz inequality.
Let us inductively assume that w t  X  c 1 R ( t + c 2 )for t = { 0 , ...T  X  1 } where the values c 1 ,c 2  X  0 will be determined later. The base case is trivially shown as w 0 =0. Thus to complete the induction step, we have:
We now choose c 1 and c 2 such that the induction step holds. This is done by ensuring that the coefficients of T and T in the above expression are smaller than the corre-sponding terms in c 2 1 T 2 +2 c 2 1 c 2 T + c 2 1 c 2 2 .First,set c which will ensure the inequality for T 2 . Next, we can ensure  X  therefore have w T  X  ( +  X  ) TR + (4  X   X  2 ) R 2  X   X R 2 .Mini-mizing the above bound over ,weget = 4  X   X  2 2 T . Substi-tuting this in the upper bound for w T ,weget w T  X  (  X T + 4  X   X  2
Thus using the update rule of Algorithm 2, we have, We now use the fact that w T w  X  w w T (Cauchy-Schwarz inequality) which implies, The above inequality, along with the condition of  X  -informative feedback gives: from which the claimed result follows.

For the case of modular utility (LIN),  X  = 0 and the above bound resembles the bound in [14]. For submodular utilities,  X  =1 / ( e +1) in the worst case, but is typically much smaller in practice. When users provide  X  X lean X  feedback according to (2), the first term in the bound (8) vanishes. We can also show a result similar to the one above in the case of expected Algorithm 3 Clipped Diversifying Perceptron.

Initialize w 0  X  0 for t =0 to T  X  1 do  X  -informative feedback (4). We do not provide a proof for this case due to space limitations.

While the above theorem holds whenever there is a 1 1+  X  -approximation for finding y t , there is a caveat. In the case of submodular utility, to ensure that the approximation guar-antee holds, all the weights in w t must be positive. This can be done by an additional clipping step that modifies each weight of w t by clipping it at zero if it is negative. The clipped version of the algorithm is shown in Algorithm 3.
For Algorithm 3, assuming that the utility is also defined using a vector w which has only non-negative components, we can still give a regret bound similar to Theorem 1. Start by observing that, for any t , The first inequality holds because the product of any clipped value with itself is positive. Since all the components of w are positive and since only negative values in  X w T are set to zero in the clipping step, the second inequality holds. With these two steps, the remaining steps in the proof of Theorem 1 follow and we have the following corollary.

Corollary 2. The average regret of the diversified per-ceptron algorithm can be upper bounded, for any w  X  R m with w  X  0 that defines the utility, as follows:
REG T  X  1 where 1  X  +1 is the approximation factor of the greedy algo-rithm with  X   X  2 and  X  ( x , y )  X  R .

We obtained the clipped version of the algorithm to avoid non-negative weights. In the next sub-section, we provide an elegant exponentiated algorithm that naturally maintains non-negative weights.
Our exponentiated algorithm for learning to diversify from implicit feedback is shown in Algorithm 4. In this algorithm, the weights are initialized uniformly at the start. There is a rate  X  associated with each step. The rate depends on the maximum  X  norm of the feature vectors (i.e.,  X  (  X  ,  X  )  X  S ) and time horizon T .

At each step, a context x t is observed and an object y t presented just like in the earlier algorithms. However, once the feedback  X y t is obtained, the update rules are multiplica-tive as shown in Algorithm 4. The weights are normalized to one and the steps of the algorithm repeat. Since the updates are multiplicative and the weights are initially positive, w is guaranteed to remain positive in this algorithm.
We now prove the regret bound for Algorithm 4. While the regret bounds for Algorithms 2 and 3 depended on the norm of the features, and the 2 norm of w , the bound Algorithm 4 Exponentiated Diversifying Algorithm.
Initialize w i 0  X  1 m  X  1  X  i  X  m. for t =0 to T  X  1 do for the exponentiated algorithm depends on the  X  norm of the feature vectors and the 1 norm of w .

Theorem 3. For any w  X  R m such that w w  X  0 , the average regret of the exponentiated algorithm can be upper bounded as follows:
REG T  X  1 where 1  X  +1 is the approximation factor of the greedy algo-rithm with  X   X  2 and  X  ( x , y )  X   X  S .

Proof. We look at how the KL divergence between w and w t evolves, On the second line, we pulled out log( Z t ) from the sum since m i =1 w i = 1. Now, consider the last term in the above equation. Denoting  X  i ( x t ,  X y t )  X   X  i ( x t , y brevity, we have, by definition, Onthesecondlineweusedthefactthatexp( x )  X  1+ x + x 2 for x  X  1. The rate  X  ensures that  X  ( X  i  X  )  X  1. On the last line, we used the fact that log(1+ x )  X  x. Combing (13) and (14), we get, ( w  X  w t )  X   X  t  X  KL ( w Adding the above inequalities, we get:
Rearranging the above inequality, and substituting the value of  X  from Algorithm 4, we get:  X   X   X   X ST +2log( m ) S In the above, we also used the fact that KL ( w || w 0 )  X  log( m )since w 0 is initialized uniformly. On line three, we used the fact that the greedy algorithm finds a 1 1+  X  approx-imation. Moreover, from a generalized version of Cauchy-Schwarz inequality, we obtained The above inequality along with  X  -informative feedback gives the claimed result.

Like the result in Theorem 1, Theorem 3 also bounds the regret in terms of the noise in the feedback (first term), the approximation factor of the inference algorithm (second term), and additional terms which converge to zero at the rate O (1 / regret bound of the exponentiated algorithm scales logarith-mically with the number of features, and with the 1 -norm of w , which can be advantageous if the optimal w is sparse.
In this section we empirically study different aspects of our proposed algorithms. In particular, we show how using the submodular utility helps achieve diversity. Furthermore, we explore the robustness of our learning method under de-graded feedback quality and noise. We also explore learning the amount of diversity a user wants and also compare our method against a supervised method. Finally, we compare the three algorithms that we proposed in this paper against each other.
Since there is no large publicly available real-world corpus containing intrinsic diversity judgments 4 , we created two ar-tificial datasets from the RCV-1 [9] text corpus and from the 20 newsgroups dataset (abbreviated 20NG).

The RCV-1 corpus contains over 800k documents, each of which is annotated as belonging to one or more of 100+ top-ics . While the original RCV-1 topics are arranged hierarchi-cally, to make the problem non-trivial, we considered only topics from the second level. The 20NG dataset contains about 19k documents (with duplicates removed) with a sin-gle class label for each document. We simulate users with multiple different interests, by forming super-users with 5 different interests corresponding to 5 different topics/classes. Thus, if a document is relevant to any of these topics it is relevant to that super-user, else it is not. We assume that
Corpora like the TREC WEB corpus are small and contain relevance judgments only for extrinsic diversity. purely seeking diversity; top: RCV-1, bottom: 20NG. all topics are equally important unless otherwise mentioned. In addition, for a given super-user we removed documents relevant to multiple interests. In this manner, producing a diverse set of results would require being able to truly learn each of the interests separately.

We ran the Diversifying Perceptron algorithm with a fresh set of 1000 documents for RCV1 (100 for 20NG )ineach step as the corpus x and presented a ranking y from the current corpus. In particular we focus on the top 5 results for all evaluation measures for brevity, though the trends reported in the following hold true for other ranking lengths as well. All results we report are averaged over 50 runs of the algorithm, each for a different super-user .Documents are represented as TFIDF vectors. The joint feature map  X  ( x , y ) is an aggregation of the document vectors using one (or multiple) of the aggregation functions F described in Section 4.
We first evaluate if the proposed DP algorithm is really able to learn a function that combines relevance and diver-sity. In particular, we generated users with 5 different and disjoint interests, and each user wants to read exactly one document relevant to each interest in every iteration. Note that users of this type are seeking maximum diversity in their rankings. To illustrate the performance of the algo-rithm, we report two quantities. First, we computed how many interests are covered in the top 5 documents of the presented ranking in each iteration. Second, we considered the median depth the user needs to search down the ranking to find one document for each of his interests.

We ran the DP algorithm with the MAX feature map as defined in Section 4. This is compared against another instance of our algorithm that uses the conventional model LIN , which focuses purely on relevance but cannot model diversity directly. For simplicity, we assume  X  = 1 informa-tive feedback. We also compare against a Random baseline, which is the performance of a random ranking.

Figure 1 shows the average and standard error of the re-sults for this experiment on the two datasets. The left col-umn shows the number of intents covered in the top 5 po-sitions over time. While the LIN method is far better than the Random method and continues to improve over time, it is outperformed by the MAX method, which is able to learn better.

The right pane further illustrates this result, as it shows how the median search length (required to find at least one document for each intent) starts at high values, but quickly drops to small values after a few iterations. Both learning methods clearly outperform the Random baseline, the value of which is too large to show. In all the plots, the standard errors are small implying statistical significance.
It can be observed that the difference between the MAX and the LIN is much higher in the case of RCV-1 compared to 20NG dataset. This is due to the fact that 20NG has only 20 categories, whereas RCV-1 has more than 100 and is thus much harder to learn for LIN.
We next study the effect of the quality of feedback (as described by  X  ) on the performance our method. As real-world users are unlikely to provide perfect feedback, we would like our algorithm to learn even in scenarios where the user-feedback is far from ideal. We varied the quality of the feedback by changing the value of  X  . A change in  X  is achieved through the following mechanism: for any intent not covered in the presented ranking, but covered in the op-timal ranking, with probability 1  X   X  , documents covering that intent are absent in the feedback ranking. This leads to having  X  -informative feedback in expectation.
Figure 2 shows the results for this experiment. Most no-tably, the performance is nearly unchanged for larger values of  X  . In particular, we find that for  X   X  0 . 6 the perfor-Figure 2: Effect of  X  on performance of the algo-rithm for users that are purely seeking diversity; top: RCV-1, bottom: 20NG. mance is very close to that with perfect feedback (  X  =1 . 0). At low values of  X  such as 0 . 2or0 . 1, the method still makes reasonable progress over time, albeit at a slower rate. We see that for  X  =0 . 2 within 100 iterations the number of intents covered more than doubles. These results indicate that the proposed method is still able to learn even when the informativeness of the user feedback is poor.
While the experiments in the previous section showed ro-bustness to imperfect feedback, we now test the robustness of our algorithm to noisy feedback. One key difference be-tween the two is that with noisy feedback, the user may return a feedback ranking that is worse than the one he was presented. Such a degradation in the quality of the ranking will be captured by the slack variable seen in Eq. (3). We would particularly like the noise introduced to be reflective of that expected in the real-world, where users may some-times be unsure of the relevance of some documents. Thus we modify the user clicking mechanism that produces the feedback in the following manner:
Like  X  ,  X  affects only the quality of the user feedback and not the learning algorithm itself. Figure 3 shows the effect of varying the noise factor  X  . As seen in the figure, the algorithm is quite robust to noise. For high values of  X  , such as 0.2, we find that the algorithm is still able to learn quite well. The figures also indicate the expected  X  of the feedback received after adding noise. However, note that in this scenario, unlike the experiments varying  X  , the feed-back ranking can be significantly worse than the predicted ranking. Thus we see that for  X  =0 . 2, although  X   X  0 . 4 in expectation, the performance is noticeably worse than for thecaseof  X  =0 . 4. Figure 3: Effect of  X  on performance of the algorithm for users that are purely seeking diversity (number in bracket indicates the effective  X  of the feedback); top: RCV-1, bottom: 20NG.
 Algo-Util Table 1: Average Regret for different user utilities and algorithm utility functions.
We next explore whether the algorithm can learn how much diversity the user wants. Furthermore, it is interest-ing to know how the algorithm performs in settings where the utility that the user optimizes (to provide feedback) is different from the one the algorithm uses.

To study this question, we experimented with the MAX and LIN utility functions mentioned earlier. We varied the user X  X  inherent utility as well as the algorithm X  X  utility to either of these two values. We also experimented with a combination method for the DP algorithm, which simply takes the joint feature vector representations used in the MAX and LIN functions and appends them to form a single vector. We refer to this method as MAX + LIN . To ensure difference in feedback between the two user utility functions, we weight the different intents (as done in [18]), which results in the utility being higher if a more popular topic is covered instead of a less popular one. We ran the DP algorithm for 100 iterations, where at each iteration the feedback provided by the user is as per the utility they optimize. We report performance in terms of the the average regret over these 100 iterations of the user X  X  utility measure (since that is what the true w captures), thus lower the better .

Table 1 shows the results for RCV1 5 . First, consider the cases where the algorithm is given the user X  X  true diversity
We observe similar results for 20NG but omitted it due to space limitations profile . As expected, the algorithm performs very well, as seen in the case of the LIN-maximizing algorithm perform-ing best for purely-relevance seeking users (and similarly for the MAX-maximizing algorithm and diversity-seeking users). However, an important result of the experiment is that even when the amount of diversity the user requires is unknown, the combination algorithm is able to learn the amount of diversity the user wants. It performs nearly as well as the case where the user X  X  diversity needs are known, as can be seen in the last row of the table. This shows that the combination algorithm is able to learn the trade-off between relevance and diversity that the user is looking for. This is very encouraging as it allows for the method to be used in scenarios where there is no a priori information about the desired amount of diversity. While related to re-cent work on extrinsic diversity [13], our method is an online learning technique and utilizes much weaker feedback than methods in [13] do.
Compared to the other two algorithms, the exponentiated algorithm has a rate  X  associated with it. This rate needs to be set appropriately. In practice, we observed that the performance of the exponentiated algorithm is sensitive to the value of the rate. In particular, we multiplied the rate  X  by a numerical value and studied how the algorithm be-haved. Note that this effectively changes the radius of the data, but seemed to significantly affect the behavior of the exponentiated algorithm. The results of this experiment is shown in Figure 4. The performance of the algorithm first improves and then deteriorates as the rate factor increases. Figure 4: Exponentiated algorithm with different rates; top: RCV-1, bottom: 20NG.
We proposed three algorithms to learn diversity from im-plicit feedback. In this section, we study whether there is a difference in performance of these three algorithms. The clipped DP (Algorithm 3) was proposed mainly due to theo-retical considerations. To compare the three algorithms, we followed the same setup as in Section 6.2. For the exponenti-ated algorithm, we considered the best rate parameter from the previous experiment. The results for this experiment are showninFigure5. Itcanbeseenthatthereisnotmuch of a difference between the clipped and the non-clipped al-gorithms in the case of RCV-1. In the case of 20NG, there is hardly any difference between the three algorithms. Even though restricting weights to positive values is required for theoretical purposes, in practice it does not seem to make much of a difference on these two datasets. Figure 5: Comparison of the three algorithms; top: RCV-1, bottom: 20NG.
Figure 6: Comparison with supervised learning.
To the best of our knowledge, ours is the first online learn-ing method that can provide a ranking, of required diver-sity, from a different corpus (i.e. context) in every itera-tion. Hence there is no suitable online learning baseline to compare against. We thus compare our method against a batch learning method. In particular, we compare against the one-level versionofthemethodproposedin[12],which is a generalization of [18].

In this setup, for each maximum diversity-seeking user we obtain the complete document-intent relevance labels for the first 50 (20) iterations for RCV1 (20NG), which is then used in training the SVM-struct based supervised learning method of [12] to obtain the w t . We train models using the labels from 40 (16) iterations, while utilizing the remaining 10 (4) to select the best value of the C parameter, which is varied from 10  X  2 to 10. The best model is then used to make predictions over the next 100 (50) iterations. We also run the online algorithm over these 150 (70) iterations to com-pare the two methods. Note that both the online method and the supervised learning method use exactly the same MAX model of user utility and exactly the same document features.

Since the supervised method does not predict rankings for the first 50 (20) iterations, to ensure a fair evaluation, we report the average regret for the next 100 (50) iterations i.e. :
We also run both methods with noise introduced using the technique mentioned in subsection 6.4.

As seen in Figure 6, the DP algorithm performs signifi-cantly better than the supervised learning method, achiev-ing nearly 25% lower regret when there is no noise for RCV1. This is particularly encouraging given that the amount of feedback the supervised algorithm receives is vastly supe-rior in informativeness to that of the online learning method: While the supervised algorithm receives the relevance labels of each document for each of the user X  X  intent, the DP al-gorithm only receives a single preference (which has atmost 5 documents) in each iteration. Even for the  X  =0 . 2case, the DP algorithm is able to achieve lower regret eventually, indicating that the trend holds even under noisy conditions.
Finally, note that the (per-iteration) training times of the supervised batch method are vastly larger than those of the DP algorithm (  X  1000s vs. 0.1s). This is because the supervised method solves a more complex optimization problem (the structural SVM objective), while training the Diversifying Perceptron involves just a single update step. Consequently, this makes the DP algorithm especially use-ful in problem settings where we would like to continu-ously improve the learned model over time, something that would be prohibitively expensive with the supervised learn-ing method.
We proposed online-learning algorithms for learning diver-sity in rankings. The proposed algorithms balance diversity and relevance by modeling the utility of the ranking as a submodular function. Using plausible user feedback in the form of preferences between rankings, the algorithms are able to learn rankings that optimize the user X  X  utility. In addition to theoretically characterizing the performance of the algorithms and their robustness to noise, we showed that the algorithms perform well in empirical studies. Future re-search directions are the deployment of the algorithm in a real system and validation of the feedback model in user studies.

This research was funded in part by NSF Award IIS-0905467.
 [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [2] J. Carbonell and J. Goldstein. The use of mmr, [3] H. Chen and D. R. Karger. Less is more: probabilis-[4] C. Clarke, M. Kolla, and O. Vechtomova. An effective-[5] C. L. Clarke, N. Craswell, and I. Soboroff. Overview of [6] K. El-Arini and C. Guestrin. Beyond keyword search: [7] T. Joachims, L. Granka, B. Pan, H. Hembrooke, [8] A. Kulesza and B. Taskar. Learning determinantal [9] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [10] F. Radlinski, P. N. Bennett, B. Carterette, and [11] F. Radlinski, R. Kleinberg, and T. Joachims. Learning [12] K. Raman, T. Joachims, and P. Shivaswamy. Struc-[13] R. L. Santos, C. Macdonald, and I. Ounis. Selectively [14] P. Shivaswamy and T. Joachims. Online structured pre-[15] A. Slivkins, F. Radlinski, and S. Gollapudi. Learning [16] A. Swaminthan, C. Metthew, and D. Kirovski. Essen-[17] Y. Yue and C. Guestrin. Linear submodular ban-[18] Y. Yue and T. Joachims. Predicting diverse subsets [19] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond
