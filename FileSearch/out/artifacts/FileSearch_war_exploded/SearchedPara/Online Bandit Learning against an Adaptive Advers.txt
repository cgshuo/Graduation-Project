 Raman Arora arora@ttic.edu Toyota Technological Institute at Chicago, Chicago, IL 60637, USA Ofer Dekel oferd@microsoft.com Microsoft Research, 1 Microsoft Way, Redmond, WA 98052, USA Ambuj Tewari ambuj@cs.utexas.edu Online learning with bandit feedback is commonly de-scribed as a repeated game between a player and an adversary. On each round of the game, the player chooses an action X t from an action set X , the ad-versary chooses a loss function f t , and the player suf-fers a loss of f t ( X t ). We often assume that f t ( X bounded in [0 , 1]. The player observes the loss value f ( X t ) and uses it to update its strategy for subsequent rounds. Unlike the full-information player, the bandit player does not observe the entire loss function f t . The player X  X  goal is to accumulate the smallest possible loss over T rounds of play.
 While this presentation is intuitively appealing, it hides the details on what information the adversary may use when choosing f t . Since this aspect of the problem is the main focus of our paper, we opt for a less common, yet entirely equivalent, definition of the online bandit problem.
 We think of online prediction with bandit feedback as an iterative process where only the player makes active choices on each round. The adversary, on the other hand, prepares his entire sequence of loss func-tions in advance. To ensure that this assumption does not weaken the adversary, we make the additional as-sumption that the loss function f t takes, as input, the player X  X  entire sequence of past and current actions ( X 1 ,...,X t ), which we abbreviate by X 1 , ... ,t formally, for each t , F t is a class of loss functions from X t to the unit interval [0 , 1], and the adversary chooses each f t from the respective class F t .
 We model the player as a randomized algorithm that defines a distribution over X on each round and sam-ples X t from this distribution. Therefore, even though f is a deterministic function fixed in advance, the loss f ( X 1 , ... ,t ) is a bounded random variable. The player observes the value of this random variable and nothing else, and uses this value to define a new distribution over the action space.
 Interesting special cases of online learning with ban-dit feedback are the k-armed bandit (Robbins, 1952; Auer et al., 2002), bandit convex optimization (Klein-berg, 2004; Flaxman et al., 2005; Abernethy et al., 2008), and bandit submodular minimization (Hazan &amp; Kale, 2009). In the k -armed bandit problem, the ac-tion space X is the discrete set { 1 ,...,k } and each F t contains all functions from X t to [0 , 1]. In ban-dit convex optimization, X is a predefined convex set and each F t is the set of functions that are convex in their last argument. A special case of bandit convex optimization is bandit linear optimization (Awerbuch &amp; Kleinberg, 2004; Bartlett et al., 2008), where the functions in F t are linear in their last argument. In bandit submodular minimization, X is the power-set of { 1 ,...,k } and each F t contains all of the functions that are submodular in their last argument.
 Various different adversary types have been proposed in the literature (Borodin &amp; El-Yaniv, 1998; Cesa-Bianchi &amp; Lugosi, 2006). All adversary types are strategic and possibly malicious, have unlimited com-putational power, and are free to use random bits when choosing their loss functions. If the adversary is not restricted beyond the setting described above, he is called an adaptive adversary. Other adversary types are restricted in various ways. For example, an oblivi-ous adversary is restricted to choose a sequence of loss functions such that each f t is oblivious to the first t  X  1 arguments in its input. In other words, f t can only be a function of the current action. Formally, for all x 1 ,...,x t and x 0 1 ,...,x 0 t  X  1 in X . The expected cumulative loss suffered by the player af-ter T rounds (which we abbreviate simply as loss ) is is, we compare it to a baseline. To this end, we choose a competitor class C T , which is simply a set of deter-ministic action sequences of length T . Intuitively, we would like to compare the player X  X  loss with the cumu-lative loss of the best action sequence in C T . In prac-tice, the most common way to evaluate the player X  X  performance is to measure his external pseudo-regret compared to C T (Auer et al., 2002) (which we abbrevi-ate as regret ), defined as Most of the theoretical work on online learning uses this definition, both in the bandit setting (e.g., (Auer et al., 2002; Awerbuch &amp; Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Aber-nethy et al., 2008; Hazan &amp; Kale, 2009)) and in the full information setting (e.g., (Zinkevich, 2003; Cesa-Bianchi &amp; Lugosi, 2006; Hazan et al., 2006; Blum &amp; Mansour, 2007; Hazan &amp; Kale, 2009)).
 If the adversary is oblivious, regret has a simple and intuitive meaning. In this special case, we can slightly overload our notation and rewrite f t ( x 1 ,...,x t ) as f ( x t ). With this simplified notation, the regret de-fined in Eq. (1) becomes The above is the difference between the player X  X  loss and the loss of the best sequence in the competitor class C T . Intuitively, this difference measures how much the player regrets choosing his action-sequence over the best sequence in C T .
 However, if the adversary is adaptive, this simple intuition no longer applies, and the standard no-tion of regret losses much of its meaning. To ob-serve the problem, note that if the player would have chosen a sequence from the competitor class, say ( y 1 ,...,y T ), then his loss would have been P t =1 f t ( y 1 ,...,y t ). However, the definition of regret in Eq. (1) instead compares the player X  X  loss to the articulate the meaning of this term: it is the loss in the peculiar situation where the adversary reacts to the player X  X  original sequence ( X 1 ,...,X T ), but the player somehow manages to secretly play the sequence ( y i ,...,y T ). This is not a feasible situation and it is unclear why this quantity is an interesting baseline for comparison.
 As designers of online learning algorithms, we actu-ally have two different ways to obtain a small regret: we can either design an algorithm that attempts to by designing an algorithm that attempts to maximize P gorithm that identifies an action to which the adver-sary always responds (on the next round) with a loss function that constantly equals 1 (here we use our as-sumption that the adversary may play any strategy, not necessarily the most malicious one). Repeatedly playing that action would cause regret to asymptote to a constant (the best possible outcome), since the player X  X  loss would grow at an identical rate to the loss of all of its competitors. While this algorithm min-imizes regret, it certainly isn X  X  learning how to choose good actions. It is merely learning how to make its competitors look bad.
 The problem described above seems to be largely over-looked in the online learning literature, with the ex-ception of two important yet isolated papers (Merhav et al., 2002; de Farias &amp; Megiddo, 2006). To overcome this problem, we define the policy regret of the player as the difference between his loss after T rounds and the loss that he would have suffered had he played the best sequence from a competitor class. Policy regret captures the idea that the adversary may react differ-ently to different action sequences. We focus on the bandit setting and start with the class of constant-action competitors. We first prove a negative result: no online bandit algorithm can guarantee a sublinear policy regret. However, if the adversary has a bounded memory, we show how a simple mini-batching tech-nique converts an online bandit algorithm with a re-gret bound of O ( T q ) into an algorithm with a policy regret bound of O ( T 1 / (2  X  q ) ). We use this technique to derive a policy-regret bound of O ( T 2 / 3 ) for the k -armed bandit problem, O ( T 4 / 5 ) for bandit convex op-timization, O ( T 3 / 4 ) for bandit linear optimization (or O ( T 2 / 3 ) if the player knows the adversary X  X  memory size), and O ( T 3 / 4 ) for bandit submodular optimiza-tion. We then extend our technique to other notions of regret, namely, switching regret, internal regret, and swap regret. 1.1. Related Work The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k -armed bandit prob-lem) and presents a concrete full-information algo-rithm with a policy regret of O ( T 2 / 3 ) against memory-bounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition. Sec-ond, note that a bandit algorithm can always be run in the full-information setting (by ignoring the extra feedback) so all of our results also apply to the full-information setting and can be compared to those of Merhav et al. (2002). While Merhav et al. (2002) present one concrete algorithm with a policy regret bound, we show a general technique that endows any existing bandit algorithm with a policy regret bound. Despite the wider scope of our result, our proofs are simpler and shorter than those in Merhav et al. (2002) and our bound is just as good. Our extensions to switching regret, internal regret, and swap regret are also entirely novel.
 The work of de Farias &amp; Megiddo (2006) is even more closely related to ours as it presents a family of al-gorithms that deal with adaptive adversaries in the bandit setting. However, it is difficult to compare the results in de Farias &amp; Megiddo (2006) with our results. While we stick with the widely accepted notion of on-line regret, de Farias &amp; Megiddo (2006) forsake the notion of regret and instead analyze their algorithms using a non-standard formalization. Moreover, their analysis makes the assumption that the true value of any constant action can be estimated by repeating that action for a sufficiently large number of rounds, at any point in the game.
 The reinforcement learning (RL) literature is also re-lated to our work, at least in spirit. Specifically, the PAC-MDP framework (Szepesv  X ari, 2010, section 2.4.2) models the player X  X  state on each round; typically, there is a finite number S of states and the player X  X  actions both incur a loss and cause him to transition from one state to another. The PAC-MDP bounds typically hold when the comparison is with all k S poli-cies (mappings from states to actions), not just the k constant-action policies. Our work is still substantially different from RL. The state transitions in RL are of-ten assumed to be stochastic, whereas our setting is adversarial. An adversarial variant of the MDP set-ting was studied in Even-Dar et al. (2009), however, it assumes that all loss functions across all states are observed by the player. There are recent extensions (Yu et al., 2009; Neu et al., 2010) to the partial feed-back or bandit setting but they either give asymptotic rates or make even more stringent assumptions on the underlying state transition dynamics. Also, the de-pendence on the number of states, S , tends to be of the form  X ( S  X  ) for some  X  &gt; 0. In the case of an m -memory bounded adaptive adversary, associating a state with each possible m -length history results in an exponential number of states.
 In other related work, Ryabko &amp; Hutter (2008) ad-dress the question of learnability in a general adap-tive stochastic environment. They prove that envi-ronments that allow a rapid recovery from mistakes are asymptotically learnable. At a high level, the assumption that the adaptive adversary is memory bounded serves the same purpose. Our results differ from theirs in various ways: we consider adversarial environments rather than stochastic ones, we present a concrete tractable algorithm whereas their algorithm is intractable, and we prove finite-horizon convergence rates while their analysis is asymptotic.
 More recently, Maillard &amp; Munos (2010) considered adaptive adversaries in the k -armed bandit setting. They define a framework where the set of all action-histories is partitioned into equivalence classes. For example, assuming that the adversary is m -memory-bounded is the same as assuming that two action-histories with a common suffix of length m are equiv-alent. Within this framework, they study adversaries whose losses are functions of the equivalence classes and competitors whose actions are functions of the equivalence classes. However, they still use the stan-dard notion of regret and do not address its intuitive problems. As mentioned above, this makes their regret bounds difficult to interpret. Moreover, when faced with an m -memory-bounded adversary, their bounds and running time both grow exponentially with m . Define the player X  X  policy regret compared to a com-petitor class C T as
E This coincides with Eq. (1) for oblivious adversaries. First, we show a negative result. Let C T be the set of constant action sequences, namely, sequences of the form ( y,...,y ) for y  X  X  . We prove that it is impossi-ble to obtain a non-trivial (sublinear) upper-bound on policy regret that holds for all adaptive adversaries. Theorem 1. For any player there exists an adaptive adversary such that the player X  X  policy regret compared to the best constant action sequence is  X ( T ) . Proof. Let y  X  X  and p  X  (0 , 1] be such that Pr( X 1 = y ) = p . Define an adaptive adversary that chooses the loss functions f 1 ( x 1 ) = 0 and All of the loss functions defined above are constant functions of the current action. From round two and on, the value of the loss function depends entirely on whether the player X  X  first action was y or not. The player X  X  expected cumulative loss against this adver-sary equals pT , since the probability that X 1 = y equals p . On the other hand, if the player were to play any constant sequence other than ( y,...,y ), it would accumulate a loss of zero. Therefore, the player X  X  pol-icy regret is at least pT . For comparison, note that the player X  X  (standard) regret is zero.
 Other adversarial strategies can cause specific algo-rithms to suffer a linear regret. For example, the popu-lar EXP3 algorithm (Auer et al., 2002) for the k -armed bandit problem maintains a distribution ( p 1 ,t ,...,p k,t over the k arms on each round. This distribution is a deterministic function of the algorithm X  X  past observa-tions. If the adversary mimics EXP3 X  X  computations and sets the loss to be f t ( j ) = p j,t we can prove that this distribution converges to the uniform distribution and EXP3 suffers a linear loss. In contrast, playing any constant arm against this adversary results in a sublinear loss, which implies a linear policy regret. Given that no algorithm can guarantee a small pol-icy regret against all adaptive adversaries, we must restrict the set of possible adversaries. We consider an adversary that lies between oblivious and adaptive: An m -memory-bounded adaptive adversary is an ad-versary that is constrained to choose loss functions that depend only on the m + 1 most recent actions. Formally, f for all x 1 ,...,x t and x 0 1 ,...,x 0 t  X  m  X  1 in X . An obliv-ious adversary is 0-memory-bounded, while a general adaptive adversary is  X  -memory-bounded. We note that m -memory-bounded adversaries arrise in many natural scenarios. For example, the friction cost asso-ciated with switching from one action to another can be modeled using a 1-memory-bounded adversary.
 For m -memory-bounded adaptive adversaries we prove a positive result, in the form of a reduction. Again, let the competitor class C T be the set of all constant action sequences of length T . We show how an al-gorithm A with a sublinear (standard) regret bound against an adaptive adversary can be transformed into another algorithm with a (slightly-inferior) policy re-gret bound against an m -memory-bounded adaptive adversary. We note that this new algorithm does not need to know m , but m does appear as a constant in our analysis.
 We define a new algorithm by wrapping A with a mini-batching loop (e.g., Dekel et al. (2011)). We spec-ify a batch size  X  and name the new algorithm A  X  . The algorithm A  X  groups the online rounds 1 ,...,T into consecutive and disjoint mini-batches of size  X  : The j  X  X h mini-batch begins on round ( j  X  1)  X  + 1 and ends on round j X  . At the beginning of mini-batch j , A  X  invokes A and receives an action Z j drawn from A  X  X  internal distribution over the action space. Then, A  X  plays this action for  X  rounds, namely, X A does not observe any feedback, does not update its internal state, and is generally unaware that  X  rounds are going by. At the end of the mini-batch, A  X  feeds A with a single loss value, the average loss suffered From A  X  X  point of view, every mini-batch feels like a single round: it chooses a single action Z j , receives a single loss value as feedback, and updates its internal state once. Put more formally, A is performing stan-dard online learning with bandit feedback against the loss sequence  X  f 1 ,...,  X  f J , where J = b T/ X  c ,  X  f j ( z 1 ,...,z j ) = and z i denotes i repetitions of the action z . By assumption, A  X  X  regret against  X  f 1 ,...,  X  f J is upper bounded by a sublinear function of J . The following theorem transforms this bound into a bound on the policy regret of A  X  .
 Theorem 2. Let A be an algorithm whose (standard) regret, compared to constant actions, against any se-quence of J loss functions generated by an adaptive adversary, is upper bounded by a monotonic function R ( J ) . Let  X  &gt; 0 be a mini-batch size and let A  X  be the mini-batched version of A . Let ( f t ) T t =1 be a sequence of loss functions generated by an m -memory-bounded adaptive adversary, let X 1 ,...,X T be the sequence of actions played by A  X  against this sequence, and let y be any action in X . If  X  &gt; m , the policy regret of A compared to the constant action y , is bounded by
E Specifically, if R ( J ) = CJ q + o ( J q ) for some C &gt; 0
E Proof. Assume that  X  &gt; m , otherwise the theorem makes no claim. Let J = b T/ X  c and let  X  f j J the sequence of loss functions defined in Eq. (2). Let Z ,...,Z J +1 be the sequence of actions played by A against the loss sequence  X  f j J A implies that From the definitions of A  X  and  X  f j , Introducing the notation t j = ( j  X  1)  X  , we rewrite
X For any j  X  J , the bound on the loss implies
X and our assumption that the adversary is m -memory-bounded implies Combining Eqs.(5-7) gives the bound Together with Eq. (3) and Eq. (4), we have We can bound the regret on rounds J X  + 1 ,...,T by  X  . Plugging in J  X  T/ X  gives an overall policy regret bound of  X R ( T/ X  )+ Tm/ X  +  X  . Focusing on the special case where R ( J ) = CJ q + o ( J q ), the bound becomes With Thm. 2 in hand, we prove that the policy regret of existing online bandit algorithms grows sublinearly with T . We begin with the EXP3 algorithm (Auer et al., 2002) in the classic k -armed bandit setting, with its regret bound of J loss functions generated by an adaptive adversary. Applying Thm. 2 with C = proves the following result.
 Corollary 1 ( k -armed bandit) . Let X = { 1 ,...,k } and let F t consist of all functions from X t to [0 , 1] . The policy regret of the mini-batched version of the EXP3 algorithm (Auer et al., 2002), with batch size  X  = (7 k log k )  X  1 / 3 T 1 / 3 , against an m -memory bounded adaptive adversary, is upper bounded by We move on to the bandit convex optimization prob-lem. The algorithm and analysis in Flaxman et al. (2005) guarantees a regret bound of 18 d ( against any sequence of J loss functions generated by an adaptive adversary, where d is the dimension, D is the diameter of X , and L is the Lipschitz coef-ficient of the loss functions. Applying Thm. 2 with C = 18 d ( result.
 Corollary 2 (Bandit convex optimization) . Let X  X  R d be a closed bounded convex set with diameter D and let F t be the class of functions from X t to [0 , 1] that are convex and L -Lipschitz in their last argument. The policy regret of the mini-batched version of Flaxman, Kalai, and McMahan X  X  algorithm (Flaxman et al., 2005), with batch size  X  = (18 d ( against an m -memory bounded adaptive adversary is upper bounded by An important special case of bandit convex optimiza-tion is bandit linear optimization. The analysis in Dani &amp; Hayes (2006) proves a regret bound of 15 dJ 2 / 3 for the algorithm of McMahan &amp; Blum (2004) against any sequence of J loss functions generated by an adap-tive adversary. Applying Thm. 2 with C = 15 d and q = 2 / 3 proves the following result.
 Corollary 3 (Bandit linear optimization) . Let X  X  [  X  2 , 2] d be a polytope (or, more generally, let X  X  R be a convex set over which linear optimization can be done efficiently). Let F t be the class of functions from X t to [0 , 1] that are linear in their last argument. The policy regret of the mini-batched version of McMahan and Blum X  X  algorithm (McMahan &amp; Blum, 2004), with batch size  X  = (15 d )  X  3 / 4 T 1 / 4 , against an m -memory bounded adaptive adversary is upper bounded by Finally, we apply our result to bandit submodular min-imization over a ground set { 1 ,...,k } . Recall that a set function f is submodular if for any two subsets of the ground set A,B it holds that f ( A  X  B )+ f ( A  X  B )  X  f ( A ) + f ( B ). The algorithm in Hazan &amp; Kale (2009) has a regret bound of 12 kJ 2 / 3 against any sequence of J loss functions generated by an adaptive adversary. Applying Thm. 2 with C = 12 k and q = 2 / 3 proves the following result.
 Corollary 4 (Bandit submodular minimization) . Let X be the power set of { 1 ,...,k } and let F t be the class of functions from X t to [0 , 1] that are submodular in their last argument. The policy regret of the mini-batched version of Hazan and Kale X  X  algorithm (Hazan &amp; Kale, 2009), with batch size  X  = (12 k )  X  3 / 4 against an m -memory bounded adaptive adversary is Theorem 2 is presented in its simplest form, and we can extend it in various interesting ways. 4.1. Relaxing the Adaptive Assumption Recall that we assumed that A has a (standard) re-gret bound that holds for any loss sequence generated by an adaptive adversary. A closer look at the proof of Thm. 2 reveals that it suffices to assume that A  X  X  regret bound holds against any loss sequence gener-ated by a 1-memory-bounded adaptive adversary. To see why, note that the assumption that each f t is m -memory-bounded, the assumption that  X  &gt; m , and the definition of  X  f j in Eq. (2) together imply that each  X  f j is 1-memory-bounded. 4.2. When m is Known We can strengthen Thm. 2 in two ways if the mem-ory bound m is given to A  X  . First, we redefine  X  f ( z 1 ,...,z j ) as Note that the first m rounds in the mini-batch are omitted. This makes the sequence (  X  f j ) J j =1 a 0-memory-bounded sequence. In other words, we only need A  X  X  regret bound to hold for oblivious adversaries. In addition to relaxing the assumption on the regret of the original algorithm A , we use m to further op-timize the value of  X  . This reduces the linear depen-dence on m in our policy regret bounds, as seen in the following example. We focus, once again, on the bandit linear optimization setting. We use the algo-rithm of Abernethy, Hazan, and Rakhlin (Abernethy et al., 2008), whose regret bound is 16 n J &gt; 8  X  log J , for any sequence of J loss functions gen-erated by an oblivious adversary. The constant  X  is associated with a self-concordant barrier on X . In the current context, understanding the nature of this constant is unimportant, and it suffices to know that  X  = O ( n ) when X is a closed convex set (Nesterov &amp; Nemirovsky, 1994).
 Theorem 3 (Bandit linear optimization, known m ) . Let X be a convex set and let F t be the class of func-tions from X t to [0 , 1] that are linear in their last ar-gument. In this setting, The policy regret of the mini-batched version of Abernethy, Hazan, and Rakhlin X  X  algorithm (Abernethy et al., 2008) where the first m loss values in each mini-batch are ignored, with batch size  X  = m 2 / 3 (16 n memory-bounded adaptive adversary, is upper bounded for all T &gt; 8  X  log T by 4.3. Switching Competitors So far, we defined C T to be the simplest competitor class possible, the class of constant action sequences. We now redefine C T to include all piece-wise constant sequences with at most s switches (Auer et al., 2002). Namely, a sequence in C T is a concatenation of at most s shorter constant sequences, whose total length is T . In this case, we assume that A  X  X  regret bound holds compared to sequences with s switches and we obtain a policy regret bound that holds compared to sequences with s switches.
 Theorem 4. Repeat the assumptions of Thm. 2, ex-cept that C T is the set of action sequences with at most s switches (where s is fixed and independent of T ) and A  X  X  regret bound of R ( J ) holds compared to action-sequences in C J . Then, the policy regret of A  X  , com-pared to action-sequences in C T , against the loss se-quence ( f t ) T t =1 , is upper bounded by The main observation required to prove this lemma is that our proof of Thm. 2 bounds the regret batch-by-batch. The s switches of the competitor X  X  sequence may affect at most s batches. We can trivially upper bound the regret on these batches using the fact that the loss is bounded, adding s X  to the overall bound. In the k -armed bandit setting, (Auer et al., 2002) de-fines an algorithm named EXP3.S and proves a regret bound compared to sequences with s switches. Com-bining the guarantee of EXP3.S with the lemma above gives the following result.
 Theorem 5 ( k -armed bandit with switches) . Let X = { 1 ,...,k } and let F t consist of all functions from X to [0 , 1] . The policy regret of the mini-batched ver-sion of the EXP3.S algorithm, with batch size  X  = (7 ks log( kT ))  X  1 / 3 T 1 / 3 , compared to action sequences with at most s switches, against an m -memory bounded adaptive adversary, is upper bounded by It is possible to give similar guarantees for settings such as bandit convex optimization, provided that re-gret guarantees under action switches are available. For instance, Flaxman et al. (Flaxman et al., 2005, Section 4) talk about (but do not explicitly derive) extensions of their bandit convex optimization regret guarantees that incorporate switches. 4.4. Internal Regret, Swap Regret,  X  -Regret We have so far considered the standard notion of exter-nal pseudo-regret, where the player X  X  action-sequence is compared to action sequences in a class C T , where C
T is commonly chosen to be the set of constant se-quences. Other standard (yet less common) ways to analyze the performance of the player use the notions of internal regret (Blum &amp; Mansour, 2007) and swap regret (Blum &amp; Mansour, 2007). To define these no-tions, let  X  be a set of action transformations, namely each  X   X   X  is a function of the form  X  : X  X  X  . The player X  X   X -regret is then defined as: In words, we compare the player X  X  loss to the loss that would have been attained if the player had replaced his current action according to one of the transformations in  X . We recover external regret compared to constant action sequences by letting  X  be the set of constant functions, that map all actions to a constant action y . Internal regret is defined by setting  X  = {  X  y  X  y 0 y,y 0  X  X } , where In other words,  X  y  X  y 0 replaces all occurrences of ac-tion y with action y 0 , but leaves all other actions un-modified. To define swap regret, we specialize to the k -armed bandit case, where X = { 1 ,...,k } . Swap re-gret is defined by setting  X  = {  X  y 1 ,...,y replaces every action with a different action.
  X -regret suffers from the same intuitive difficulty as external regret, when the adversary is adaptive. Define the policy  X  -regret as We repeat our technique to prove the following. Theorem 6. Repeat the assumptions of Thm. 2, ex-cept that now let  X  be any set of action transforma-tions and assume that A  X  X   X  -regret against any se-quence of J loss functions generated by an adaptive adversary is upper bounded by R ( J ) . Then, the pol-icy  X  -regret of A  X  against ( f t ) T t =1 generated by an m -memory-bounded adaptive adversary is bounded by  X R ( T/ X  ) + Tm/ X  +  X  .
 The proof is omitted due to space constraints.
 Blum &amp; Mansour (2007) presents a technique of con-verting any online learning algorithm with an external regret bound into an algorithm with an internal regret bound. Combining that technique with ours endows any of the online learning algorithms mentioned in this paper with a bound on internal policy regret. We highlighted a problem with the standard defini-tion of regret when facing an adaptive adversary. We defined the notion of policy regret and argued that it captures the intuitive semantics of the word  X  X egret X  better than the standard definition. We then went ahead to prove non-trivial upper bounds on the policy regret of various bandit algorithms.
 The main gap in our current understanding of policy regret is the absence of lower bounds (in both the ban-dit and the full-information settings). In other words, we do not know how tight our upper bounds are. It is conceivable that bandit algorithms that are specifi-cally designed to minimize policy regret will have su-perior bounds, but we are yet unable to show this. On a related issue, we do not know if our mini-batching technique is really necessary: perhaps one could prove a non-trivial policy regret bound for the original (un-modified) EXP3 algorithm. We leave these questions as open problems for future research.

