 Top-K query processing is one of the most important prob-lems in large-scale Information Retrieval systems. Since query processing time varies for different queries, an accu-rate run-time performance prediction is critical for online query scheduling and load balancing, which could eventually reduce the query waiting time and improve the throughput. Previous studies estimated the query processing time based on the combination of term-level features. Unfortunately, these features were often selected arbitrarily, and the linear combination of these features might not be able to accurately capture the complexity in the query processing.

In this paper, we propose a novel analytical performance modeling framework for top-K query processing. Our goal is to provide a systematic way of identifying important fea-tures for the efficiency prediction and then develop a general framework for estimating the query processing time. Specif-ically, we divide the query processing into three stages, iden-tify useful features and discuss how to use them to model the query processing time for each stage. After that, we propose to fit the model using a step-by-step strategy and compute the approximated feature values based on easily ob-tained statistics. Experimental results on TREC collections show that the developed performance model can predict the query processing time more accurately than the state of the art efficiency predictor, in particular for the dynamic prun-ing methods.
 Categories and Subject Descriptors: H.3.4 [Systems and Software]: Performance evaluation (efficiency and effec-tiveness) General Terms: Performance, Experimentation Keywords: performance modeling; efficiency prediction; top-K query processing Large-scale Information Retrieval (IR) systems run under strict performance constraints, since their efficiency could directly affect search users X  satisfaction as well as search en-gine revenues [4, 23]. Web search engines need to process tens of thousands of queries every second over billions of documents, but search users still expect the response time to be under one hundred milliseconds [26]. Even though it is important to return high-quality search results, the search quality might have to be sacrificed when necessary to meet the time constraint [9]. Thus, it is important to know, or more precisely to predict, the processing time for each query before executing the query.
 Run-time performance prediction can benefit large-scale IR systems in many aspects. First, it facilitates online query scheduling [19]. To improve the throughput, Web search en-gines often replicate the indexes so that independent queries can be processed in the same time [4,5]. Since the execution time varies for different queries, an accurate prediction of query processing time can lead to better load balancing and minimize the wait time. Second, knowing the query process-ing time in advance enables us to select retrieval strategies or dynamic pruning methods accordingly [6, 27]. For exam-ple, Tonellotto et al. proposed a selective pruning framework and showed that, when combined with a query efficiency pre-dictor, the average response time can be reduced by 36%. Finally, run-time performance prediction could also be use-ful in automatically detecting problems in a large-scaled IR system. For example, if a node X  X  actual query processing time is much longer than the predicted value, it could indi-cate that some faults have happened and the query scheduler might need to assign the task to other nodes.

Query processing in large-scale IR systems often involves multiple retrieval stages [2,17] and a distributed architecture with hundreds of thousands of machines [8, 12]. Despite its complexity, the query processing always starts with retriev-ing top-K ranked results for the given query on a single node. After that, the top-K results from one node can then be merged with those from other nodes and the merged results can be re-ranked with more complicated retrieval methods. Clearly, top-K query processing is the one of the most impor-tant steps in the entire query processing pipeline, and it is important to study how to model its run-time performance.
The most basic top-K query processing technique is to ex-haustively compute the relevance score for each document and return the top-K ranked documents. Previous studies have suggested that the query processing time is related to the number of query terms and the number of postings in the query terms X  inverted lists [19, 20]. However, such an ex-haustive evaluation is extremely time consuming and often unnecessary since most of the evaluated documents will not make to the top-K results. To improve the efficiency, vari-ous dynamic pruning (or early termination) techniques have been proposed to reduce the number of documents that need to be fully evaluated by skipping the documents that can not make to the top-K results [7, 10, 13, 14, 22, 24, 28]. Although the pruning methods improve the query processing time, they make the performance prediction problem more chal-lenging since the processing time is determined by many fea-tures that are difficult to obtain before executing the queries, e.g., how the top-K results of a query are located in the post-ing lists. Macdonald et al. [19] recently proposed to aggre-gate various term-level features into query-level features and then predict the processing time based on a learned linear regression model. Unfortunately, the features were selected and aggregated in an arbitrary way without a deep analysis on what would really happen. Thus, it is necessary to study how to systematically identify important features that can used to more accurately predict the run-time performance for various query processing techniques.

In this paper, we propose a novel analytical performance modeling framework for top-K query processing methods, in-cluding both the exhaustive evaluation and dynamic pruning methods. The basic idea is to first divide query processing methods into three stages, analyze each stage to identify fea-tures that can be used to predict the query processing time, and model the run-time performance based on the identified features. With the developed model, we propose to fit the parameters using a step-by-step method, and then describe how to approximate the feature values based on a small set of easily obtained statistics about each query term such as the number of postings, the maximum score of the postings and the minimum score of the postings.

Experimental results over TREC collections show that the proposed analytical prediction model can generate more ac-curate run-time performance prediction than the state of the art efficiency predictor [19, 20] for the dynamic pruning methods such as maxScore [28] and WAND [7]. In particular, the proposed analytical prediction model is more effective in capturing the relation between the number of retrieved doc-uments (i.e., K ) and query processing time. Furthermore, the proposed step-by-step model fitting method has been shown to be more robust for cross-collection prediction.
Our main contribution is to develop an analytical perfor-mance model that can more accurately model and predict the query processing time. Such an analytical study enables the discovery of a set of fine-grained features that are es-sential for accurate run-time performance prediction. These features can be used either using the proposed analytical model or combined with additional useful features through machine learning algorithms. It is well known that selecting important fine-grained features (e.g., Okapi BM25) in learn-ing to rank methods [17] can lead to more effective perfor-mance than those using only coarse-grained features (e.g., raw TF-IDF values) [18]. Similarly, our study shows the im-portance of identifying fine-grained features in the run-time performance prediction.
Large-scale IR systems have leveraged various performance optimization techniques, such as caching [3, 15], index com-pression [1] and dynamic pruning [7, 28], to achieve high throughput and low latency. We now provide some back-ground information about the DAAT-based query process-ing techniques, which are used in this paper.

Indexes and Document-At-A-Time (DAAT): Inverted indexes consist of inverted lists of all terms in the collection, and an inverted list contains a list of postings. A posting in the inverted list of a term corresponds to a document containing the term, and it contains the information about the document such as the document ID and the count of the term in the document. The postings are often sorted based on the corresponding document IDs. Query process-ing requires the traversal of the postings in the inverted lists of all query terms. A commonly used strategy is called Document-at-A-Time (DAAT). It traverses the inverted lists of all query terms in parallel. A document needs to be fully evaluated based on the contribution of all query terms before moving to the next one. DAAT is often preferred because its ability to handle more complex queries [7, 25].
Dynamic pruning methods for DAAT: To reduce the query processing time, a few dynamic pruning methods, in-cluding maxScore [28] and WAND [7], have been proposed for DAAT-based processing. All of these methods aim to skip the scoring of documents that can not make to the final top-K results. To ensure the results of the pruning methods are the same as those of the exhaustive evaluation method, we need to maintain two types of information: (1) a prun-ing threshold, which stores the smallest document score that can make to the top-K results; and (2) the maxscore of each query term, which is the highest relevance score that the term can contribute to any of its postings. The maxScore method [28] skips documents that contain only non-essential terms by evaluating only the postings of essential-terms. It first ranks query terms based on their maxscores, and iden-tifies a group of non-essential terms so that they have lower maxscores than the remaining essential terms while the sum of their maxscores is smaller than the pruning threshold. With the increasing of the threshold, more terms would be identified as non-essential ones. The WAND method [7] takes a pivot-based approach. In each iteration, terms are first sorted based on their current document IDs, and a pivot term is then selected so that the sum of its maxscore to-gether with the maxscores of all the terms ranked before it is greater or equal to the pruning threshold. After select-ing the pivot term, the current document pointer of each non-pivot term is then moved to the first document in its posting whose ID is equal or larger than the document ID of the pivot term. If the current document IDs of all the terms are either equal to or larger than the pivot document ID, we can then evaluate the document, update the pruning threshold when necessary and move the current document pointer to the next posting.
 Query efficiency prediction and its applications: Moffat et al. found that the query processing time is related to the number of postings of all query terms [20]. Macdon-ald et al. [19] proposed to learn the prediction time through linear regression based on a set of aggregated term-level fea-tures such as the total number of postings, the variance of the length of the posting lists, etc. This efficiency predic-tor has been shown to be useful in selective pruning [6, 27] and online query scheduling [19]. More recently, Jeon et al. proposed a predictive parallelism framework to predict long-running queries and selectively parallelize them [16].
Instead of arbitrarily selecting and aggregating a set of coarse-grained features, we focus on develop an analytical Figure 1: Multiple stages in the query processing model and use this model to identify a set of fine-grained fea-tures that can more accurately capture the run-time query processing time.
We describe how to develop a general model to estimate top-K query processing time in this section. In particular, we first divide the query processing pipeline into multiple stages, identify the most useful features for cost estimation at each important stage, and then develop the model based on these features accordingly.
Despite the differences among various top-K query pro-cessing methods, all of them can be divided into three stages: initialization , retrieval and result generation , as shown in 1.
In the initialization stage, a query is pre-processed and parsed into one or multiple query terms. We need to locate the inverted list for each query term in the index and initial-ize the variables, which include the pointer to the current document IDs in each inverted list for all methods, and the maxscore of each list for the pruning methods.

In the retrieval stage, we first traverse the inverted indexes to find documents that need to be evaluated, and then com-pute the relevance score of each document and return the top-K ranked results. This stage is the most important one in the query processing pipeline.

In the result generation stage, the top-K ranked results could be either displayed to search users or passed to other more complex retrieval methods as input for re-ranking [2]. Since the processing time of this stage is much smaller than the other two stages and it depends on the architecture de-sign of the IR systems (e.g., how to render the search results etc.), we focus on estimating the query processing time for only the first two stages in this paper.
The initialization stage consists of two tasks: (1) for each query term, we need to locate its inverted list in the index and load the list to the memory if the posting list data are stored on the disk; (2) for each query term, we need to initialize the current document ID pointer in its inverted list and fetch its maxscore when necessary. Clearly, the time complexity of the both tasks is determined by the number of query terms.

Thus, we propose to model the processing time of query q in the initialization stage as follows: where L ( q ) is the number of unique terms in query q and  X  is a parameter measuring the average initialization cost of a query term. Our preliminary results show the query processing time of this stage is much smaller than that of the retrieval stage.
The retrieval stage consists of three steps: document lookup, document evaluation and heap update.

Document lookup: In this step, we need to traverse the indexes and locate candidates documents that need to be evaluated. Since indexes are often compressed to reduce the space and transferring time [1,30], the first thing we need to do is to decompress the postings in the inverted lists. We denote the time spent on this decompression process as T d It is clear that T d is determined by the amount of data that need to be accessed and how the postings are compressed. In our implementation, we compressed the inverted lists in blocks of 64 postings so that each block can be accessed and decompressed individually [30]. Thus, we can estimate the decompression time as follows: where B ( q ) is the number of blocks that need to be accessed and decompressed in the query processing for q , and  X  1 is a parameter measuring how fast the system can read from the disk and execute the decompression algorithm.

After decompressing the postings, we need to traverse the indexes to locate documents that need to be evaluated. Since we focus on the DAAT-based methods, the pointers to the current document IDs in all inverted indexes need to be syn-chronized so that a document can be fully evaluated before moving to the next one. This process suggests that the query processing time of this step is related to the number of query terms as well as the number of document IDs that need to be synchronized. So, the time spent on locating the documents can be estimated as: where L ( q ) is the number of unique terms in query q , D is the number of document IDs that need to be synchronized, and  X  2 is a parameter that reflects the machine speed and the complexity of the retrieval function used for the query evaluation.

In summary, the processing time of query q in the docu-ment lookup step can then be estimated as T d ( q ) + T l
Document evaluation: The computational cost of this step is related to the number of documents that need to be evaluated as well as the cost of computing the relevance score of a document for the query. Thus, we can model the processing time of query q in the document evaluation step using: where EPL ( q ) is the number of postings that need to be evaluated, and  X  3 is a parameter related to the complexity of the retrieval function.

Heap update: For top-K query processing, a minimum heap with the size of K can be used to store the information about the top-K documents. With the minimum heap, the update cost can be greatly reduced. In our preliminary ex-periments, we find that the time spent on the heap update is usually much smaller compared with other steps unless K is extremely large (e.g., more than 100,000). Thus, in this paper, we ignore the time spent on heap update and leave the study for the extreme situations as our future work.
We now summarize the developed model for estimating the query processing time of q . As described earlier, the model is related to four important features: The processing time of q can be estimated as: where  X , X  1 , X  2 , X  3 are parameters whose value depend on various factors such as the machine speed and the retrieval method used for ranking.

We will discuss how to train the parameter values in Sec-tion 4, and how to approximate the feature values based on easily obtained statistics in Section 5.
As shown in Equation (5), the analytical model has four parameters that need to be estimated on a training set. One simple solution is to follow the commonly used practice that trains all the parameters together to fit the model [19]. Specifically, for each training query, we can compute the val-ues of the four features and get the actual processing time of the query. With this information, we can fit a linear regres-sion model to get the learned parameter values. Although this solution would work, the learned model might not be able to generalize well due to its inability to capture the rela-tion between each parameter and its corresponding portion of the processing time.

To overcome this limitation, we propose to estimate the parameter values using a step-by-step method so that the parameters can be estimated individually based on Equa-tions (1)-(4). When constructing the training set, instead of recording the total query processing time for each query, we record the time spent on each step, i.e., T i , T d , T T . With such a training set, we can perform a step-by-step model fitting, in which we will learn the value of each parameter based on the time of the corresponding step.
Let us take the exhaustive query processing method as an example. We first record the time spent on initialization (i.e., T i ) and then learn the value of  X  using the linear re-gression based on Equation (1). We then record the time spent on depression and learn the value of  X  1 using the lin-ear regression based on Equation (2). After that, we record the time spent on going through all documents without com-puting the relevance scores as T l and learn the value of  X  based on Equation (3). Finally, we record the time spent on document evaluation by subtracting the sum of T i , T d and T from the total query processing time, and learn the value of  X  3 accordingly.

For the dynamic pruning methods such as maxScore [28] and WAND [7], we can not apply the same strategy as the exhaustive evaluation method because the stages are not in-dependent. For example, it is impossible to separate the document lookup step from the evaluation since the prun-ing threshold used in the lookup stage needs to be adjusted
Figure 2: Document groups for query q= { A,B,C } by the relevance scores computed in the evaluation stage. Fortunately, the operations involved in the posting list de-compression and document evaluation are identical to the exhaustive evaluation method. Thus, we assume that  X  and  X  3 are the same as those learned in the exhaustive eval-uation method, and only need to estimate the value of  X  and  X  2 using the method described earlier.
We have developed the performance model based on four features and discussed how to estimate the parameters in the previous sections. In order to predict a query X  X  process-ing time before executing the query, we need to know the values of these four features without traversing the indexes. Among the four features, L ( q ) is easier to obtain while the other three features are impossible to directly obtain from the indexes. To tackle this challenge, we propose to approx-imate the real feature values based on easily obtained statis-tics information and then predict the run-time performance based on the developed model as shown in Equation (5).
The statistics information used for the approximation is summarized as follows:
All these statistics here are either accessible from the in-dexes or can be computed with the minimal cost. N , PL i ( q ) and M i ( q ) are available in the indexes supporting the dy-namical pruning methods, while m i ( q ) can be pre-computed and stored in the indexes in the same way as M i ( q ). It is certainly possible to use other statistics information such as the distribution of the postings. However, the more statis-tics we use, the more space and time will be spent on the prediction. This paper focuses on predicting the run-time performance with minimum cost, so we leave the problem of exploring more statistics as our future work.

We first introduce some concepts and notations that will be used in the feature approximation. Given a query with L ( q ) unique terms, all the documents containing at least one query term can be divided into 2 L ( q )  X  1 groups based on the matched query terms. Figure 2 shows an example of 7 document groups for a query with three unique terms, i.e., A , B and C . Each circle represents all the documents con-taining the corresponding query term. We use the following notations related to the document groups:
The exhaustive evaluation method is essentially to tra-verse the postings of all the query terms and compute the relevance score for each document in the postings. Thus, we can easily compute the values of EPL ( q ) and B ( q ) as follows: where Bsize is the number of postings in each compressed block, and it is set to 64 in our implementation.
 The remaining challenge is to approximate the value of D ( q ), which is equal to the number evaluated documents in the exhaustive evaluation method. In other words, D s is the number of documents containing at least one query term, i.e., P A j ( q ). We now discuss how to estimate A
Let us start with a simple case when a query has only two terms, i.e., q = { q 1 ,q 2 } . There would be three docu-ment groups: G 1 ( q ) (documents containing q 1 but not q G ( q ) (documents containing q 2 but not q 1 ), and G 3 ( q ) (doc-uments containing both terms). Since we know A 1 ( q ) = question is how to estimate A 3 ( q ), i.e., the number of docu-ments covering two terms. It is impractical to store such in-formation in the indexes due to the large number of unique terms, so we now discuss how to approximate this value. Assuming query terms follow the binomial distribution, the probability of observing a query term in a document can be estimated as PL i ( q ) N . If we assume that the two query terms are independent, the expected number of documents con-taining both terms can be estimated as PL 1 ( q ) N  X  PL 2 To make it more generalized, when the two terms are not independent and PL 1 ( q )  X  PL 2 ( q ), we propose to estimate the number of documents containing both terms as follows: where  X  is a parameter which controls how the two terms are related. When  X  is equal to 1, the two terms are in-dependent. And when  X  is equal to 0, the two terms are closely related (i.e., PL 1 ( q )  X  PL 2 ( q )). In this paper, we set  X  = 0 . 5 arbitrarily and leave the study of providing em-pirical justification of the parameter setting as our future work.
 Algorithm 1 The approximation of features for the exhaus-tive evaluation method FUNCTION getExhaustiveFeatures { q }
EPL ( q ) = 0, B ( q ) = 0, D s ( q ) = 0 for i = 1 to L ( q ) do end for for j = 1 to 2 L ( q )  X  1 do end for return B ( q ), D s ( q ), EPL ( q )
Algorithm 1 summarizes how to approximate feature val-ues for the exhaustive query processing. Note that we can re-cursively compute A j ( q ) based on the approximation method mentioned in the previous paragraph.
The feature approximation for pruning methods are much more complicated since the query processing is controlled by a pruning threshold,  X  , whose value would keep changing in the process. Indeed,  X  plays a critical role in the query processing method since it determines when the pruning be-gins and how much pruning is needed for each inverted list. Thus, we will start with the estimation of  X  and then move on to discuss how to approximate the feature values in the maxScore and WAND methods.
The basic idea of dynamic pruning methods is to skip the scoring of documents that can not make to the final top-K results. The selection of these documents is controlled by  X  , the smallest relevance score in the intermediate top-K results. The value of  X  will monotonically increase during the query processing. To model the pruning behavior, we have to trace the change of  X  . Not every possible value of  X  could change the pruning behavior, i.e., how much to prune for each inverted list. Instead, the pruning behavior would only change at a few critical values .

Let us consider the maxScore method first. The pruning would start when  X  is equal to or greater than the small-est maxScore value of all the inverted lists. The pruning behavior will change (i.e., prune more postings), when  X  is equal or greater than the sum of the two smallest maxScore values. Given a query q , the number of critical values for  X  in maxScore is L ( q ). The pruning process in the WAND method is similar but with different number of critical val-ues, i.e., 2 L ( q )  X  1, since it considers the combinations of the inverted lists in the pruning process.

The key challenge is to understand the pruning behavior at each critical value in a pruning method. We propose to estimate the percentage of the postings that will be pruned in each inverted list at critical value C based on D ( C ), i.e., the number of documents whose score is higher than C . If D ( C ) smaller than K , there is no pruning is necessary and 0 is returned. Otherwise, we know around K out of D ( C ) documents are not pruned, so the pruning percentage can be computed as 1  X  K D ( C ) .

To estimate D ( C ), we can go through each document group of q , estimate the number of documents whose value is larger than C , and take the summation. Within docu-ment groups covering a single query term, we assume that the document scores follow a normal distribution N (  X  g , X  and the parameters can be estimated based on the statistics of the term. We can also prove that the document scores from other groups also follow the normal distribution and can be estimated based on the distributions of the covered terms.

Algorithm 2 explains how to estimate the pruning per-centage at critical value C when retrieving top-K documents for query q . Note that F j () is the cumulative distribution function of the j -th document group, and F j ( C ) is the per-centage of documents in G j whose value is less than or equal to C . Thus, A j ( q )  X  (1  X  F j ( C )) essentially calculates the number of documents scored higher than C in G j . Algorithm 2 The estimation of pruning percentage of a critical value C when retrieving top K documents for q Function { getPrunePercentage }{ C , K , q }
D ( C ) = 0 for j = 1 to 2 L ( q )  X  1 do end for if D ( C ) &lt; K then else end if
EndFunction
Algorithm 3 describes how to estimate the features in the maxScore method. For the simplicity, we assume that M 1 ( q )  X  M 2 ( q )  X  ...  X  M L ( q ) ( q ) for any query q . Algorithm 3 The approximation of features for maxScore
FUNCTION getmaxScoreFeatures { K , q } sort ( query )
B ( q ) = 0, D s ( q ) = 0, EPL ( q ) = 0 for i = L ( q ) to 1 do end for for i = 1 to L ( q ) do end for return B ( q ), D s ( q ), EPL ( q ) ENDFUNCTION
We first compute all the critical values, i.e., C i ( q ), for the maxScore method. Note that each critical value corresponds to a query term. After that, we go through each term to calculate its contributions to all the features.

For each term, we first try to estimate the number of documents that contain both the current term and the terms we have already evaluated, i.e., Con . Con is important since these documents will always be accessed no matter whether pruning is applied or not. B i ( q ) is the number of blocks in the inverted list of the current term and the block size used in our implementation is 64. After that, we get the pruning percentage based on the corresponding critical value and figure out the percentage of the pruned postings in the inverted list of current term.

The contribution to each feature is calculated as two parts: before  X  reaches the critical value and after. For exam-ple, before reaching the critical value, the contribution to EPL ( q ) is PL i ( q )  X  (1  X  Per ) since every non-pruned post-ings will be exhaustively evaluated. After reaching the criti-cal value, the contribution to EPL ( q ) is Con  X  Per since we only access the postings that contain other terms (i.e., Con ) after pruning. Moreover, before reaching the critical value, the contribution to D s ( q ) is related to the number of post-ings that do not contain previously evaluated terms and can be computed as ( PL i ( q )  X  Con )  X  (1  X  Per ), but after reach-ing the critical value, the corresponding query term will not be eligible to contribute new documents, so its contribution to D s ( q ) is 0.

RB i ( q ) is the number of remaining blocks after reaching the critical value. We can then estimate the number of de-compressed blocks within them using binomial model. If the pruning happened, not every posting in the posting list will be accessed. And if none of the 64 postings within a block is accessed, the block will be skipped and it will not be decompressed. If we have RB blocks and we accessed p postings from them, for each block, the probability that none of the p postings is from the block can be estimated blocks, the expected number of decompressed blocks can be estimated as RB  X  (1  X  ( RB  X  1 RB ) p ).
 Algorithm 4 The approximation of features for WAND FUNCTION getWANDFeatures { K , q }
B ( q ) = 0, D s ( q ) = 0, EPL ( q ) = 0 for i = 1 to L ( q ) do end for for j = 1 to 2 L ( q )  X  1 do end for for i = 1 to L ( q ) do end for return B ( q ), D s ( q ), EPL ( q )
ENDFUNCTION
Different from maxScore, WAND applies a pivot-based pruning strategy to prune more documents. As described earlier, given a query, we can divide documents into differ-ent groups based on the query term occurrences. We denote Max j ( q ) as the sum of maxscores of all query terms corre-sponding to the j -th group. In fact, each Max j ( q ) can be considered as a critical point for the WAND method. If  X  is higher than Max j ( q ), documents from the j -th will be pruned. For example, if we know the pruning percentage at the critical value Max j ( q ) is equal to 0 . 7, we expect the first 30% documents from j -th group will be evaluated while the remaining 70% will be pruned.

Algorithm 4 shows the method used to compute the ap-proximated features values in WAND . The basic idea is to estimate the number of the evaluated postings for each query term (i.e., E i ( q )) and then use it to estimate D B ( q ). Since the pruning of WAND is based on each doc-ument group G j ( q ), for each document group, we can get the pruning percentage ( Per ) and estimate the number of evaluated documents in each group using A j ( q )  X  (1  X  Per ). After iterating through all the document groups, we can get the value E i ( q ) for each query term. Finally, we can then compute the value of EPL ( q ) by summing all the value of E ( q ) together.

The remaining question is how to estimate D i s ( q ). We know that its upper bound is the posting list length (i.e., PL i ( q )) and its lower bound is the number of evaluated postings for the term (i.e., E i ( q )). In this paper, we use a heuristic way to estimate a value between these two bounds, i.e., D i s ( q ) = ( E i ( q )  X   X  PL i ( q )) 1 / (1+  X  )  X  to control which bound to approach more. We set  X  to 2 in our experiments. After we get the number of access postings for each term (i.e., D i s ( q )), we can estimate D by summing them up for all query terms, and estimate B ( q ) based on the binomial model described in Algorithm 3.
We evaluate how well the proposed performance model can predict the query processing time for the exhaustive evaluation method using DAAT, denoted as Exhaustive , as well as two dynamic pruning methods, i.e., maxScore [28] and WAND [7].

Regarding the data collection, we use the TREC Gov2 collection, which consists of 25.2 million web pages crawled from the .gov domain. For queries, we randomly select 1,000 queries from the efficiency queries used in the TREC 2005 Terabyte track as the training set to fit the proposed model. This data set is denoted as Train . Moreover, we randomly select a different set of 4,000 queries from the same track as the first test collection, denoted as Test05 , and randomly select 5,000 queries from the efficiency queries used in the TREC 2006 Terabyte track as the second test collection, denoted as Test06 .

All experiments were conducted on a single machine with dual AMD Lisbon Opteron 4122 2.2GHz processors and 32GB DDR3-1333 memory. And we use Okapi BM25 [21] as the retrieval function to rank documents. To evaluate the per-formance of the prediction, we use the RMSE (Root Mean Square Error) as the evaluation measure. A smaller RMSE means the prediction is more accurate.

We denote the developed performance model with the real feature values as Analytical(Real) , and denote the model with the approximated feature values from Section 5 as An-alytical . The models are compared with two baseline meth-ods: (1) BL-PL , which uses the sum of the postings of each query term for the prediction; and (2) BL-ML , which uses the learned model to combine aggregated term-level fea-tures [19]. The parameters of all methods are learned us-ing the Train data set. In many plots, we will also show the results of ideal prediction (denoted as Ideal ), where the predicted query processing time is equal to the actual time. Due to the space limit, we show the results on only one data set or for one method, but other results are similar.
All the prediction models are trained on the Train collec-tion when the number of retrieved documents (i.e., K ) is set to 10. When evaluating the learned model on the testing collections, K is also set to 10 unless stated otherwise.
The first set of experiments is to validate the proposed model using the real feature values. In particular, we learn the values of the four parameters based on the Train data set, compute the real feature values for each query in the testing set, and then compute the predicted processing time based on Equation (5).

Figure 3 compares the predicted processing time with the actual processing time for each query using the three query processing methods on Test05 collection. The strong cor-relation between the predicted and actual query processing time indicates the developed model is able to capture the run-time performance very well when the real feature values are available.
Since it is impossible to get the real feature values with-out traversing the indexes, we would need to estimate the features values based on some easily obtained statistics as described in Section 5. We conduct experiments to evalu-ate how well the Analytical method can predict the query processing time.

Table 1 compares the prediction results of the proposed model with the two baseline methods on the two test col-lections. It is clear that the Analytical method can make a more accurate prediction than BL-PL for all the query pro-cessing methods over both collections. Compared with the state of the art method, i.e., BL-ML , Analytical can capture the query processing time of the complicated dynamic prun-ing methods more accurately. Figure 4 shows more details about the per-query prediction time comparison for WAND (right) on Test05 . These plots confirm that the Analytical model can generate more accurate predicted processing time for dynamic pruning methods than the two baselines.

It is quite encouraging to see that our model can make much more accurate prediction for the dynamic pruning methods using only a few easily obtained statistics. In a way, the developed Analytical framework is complementary to the previous machine-learning based predictor, i.e., BL-ML , since it identifies four additional features as described in Section 3.4. We conduct experiments to see whether com-bining the 42 features used in BL-ML with the 4 new fea-tures in Analytical could further improve the performance of Analytical . The results show that such a combination with 46 features is unable to lead to significant improvement over the developed Analytical model with only four features. For example, the RMSE of the combination method for WAND on Test05 is 31.07. which is only slightly better than 31.76 as shown in Table 1. This observation suggests that the four features identified in our paper are very powerful.
Finally, we conduct experiments to study how the query length affects the prediction accuracy. Figure 5 shows the comparison between the Analytical method and the stronger baseline, i.e., BL-ML . It is clear that Analytical can consis-tently generate more accurate run time performance predic-tion than BL-ML .
To examine the robustness of our model, we use the pa-rameter values trained for K = 10, but predict the query processing performance for a different set of values for K , i.e., 100, 1,000 and 10,000 on the test collections. Since all the documents in the posting lists need to be evaluated no matter how many documents need to be retrieved, the value of K would not affect the performance of the Exhaus-tive method. However, the value of K could significantly affect the performance of the dynamic pruning methods be-cause the pruning threshold is determined by the smallest relevance score in the intermediate top-K results.

Figure 6 shows the prediction errors of BL-ML and Ana-lytical when testing on different values of K . It is clear that Analytical is more robust for both dynamic pruning meth-Table 2: Cross collection prediction results: RMSE (ms) Analytical 1.86 2.39 72.91 63.86 Table 3: Comparison of two model fitting methods: RMSE(ms)
Step-by-Step 50.24 29.75 1.86 72.91 ods, since its performance does not change a lot for different values of K . On the other hand, we see the prediction er-ror of BL-ML increases as the value of K becomes larger, indicating the model might be over-fitted to the training set where K is set to 10.
To further evaluate the robustness of our model, we apply the models trained on Gov2 collection to predict the process-ing time on the other two TREC collections: (1) Robust04 : the collection used in the TREC 2004 Robust track [29] with 528,000 documents and 249 queries; and (2) ClueWeb09 : the category B collection used in the TREC 2009-2012 Web track [11] with 50 million Web pages and 200 queries. Table 2 compares the prediction errors of Analytical and BL-ML for the two dynamic pruning methods. It is sur-prising to see that the prediction errors of BL-ML are much larger than the Analytical method. It indicates that the trained parameters of BL-ML are collection dependent and can not be generalized to collections with different charac-teristics. Note that if we train BL-ML on the Robust04 , the prediction error for maxScore on the same collection would be 1.49, which is much smaller than 902.82 when training on a different collection.

It is clear that the Analytical model is more robust to such cross-collection testing. The parameters learned from one collection can be applied to accurately predict the per-formance of the other collections. (ms) K =10): RMSE (ms)
As discussed in Section 4, there are two possible model fitting methods: (1) Together , which applies linear regression to learn all the parameters in the same time (as described in the first paragraph). (2) Step-by-Step , which uses the method described in the later part of Section 4. We conduct experiments to compare these two strategies.

Table 3 shows the prediction errors of Analytical when using two model fitting methods to learn the parameters. We can see that the model learned by using the Step-by-Step model fitting method can consistently generate more accurate prediction over all collections.
The space overhead of the proposed model is small. Most features in our model can be directly accessed from the in-dexes that support dynamical pruning methods (e.g. the posting list length and the maximum score of each query term). The only additional feature we need to compute and store is the minimum score of each query term, which does not take much space. For example, the size of the indexes for Gov2 is around 10.8 GB, but the additional space needed by our model is only 0.2 GB.
 The computational cost of our model is O (2 L ( q ) ), where L ( q ) is the number of unique query terms. Since most search queries contain less than 10 terms, the cost of the prediction is ignorable compared with the query processing cost, which may involve millions or even billions of operations. We con-duct experiments to examine the efficiency of the prediction model over the Test05 and Test06 collections.
 Processing only 42.73 39.05 59.89 44.53
Table 4 compares the time spent on only processing queries with that on making the prediction and processing the queries. We can see that the cost of prediction is much smaller (less than 0.6%) than the cost of query processing, indicating that the proposed Analytical model is efficient enough to be used in practice. Note that the cost of our prediction method is independent to the collection size, so the percentage of the computational cost introduced by our model would become even smaller for larger collections.

One potential application of the performance prediction is to leverage the prediction results to select the most effi-cient query processing method for each query. We design a set of experiments to evaluate the efficiency of the Analyti-cal model through this application. For each query, we first predict the query processing time for maxScore and that for WAND using the Analytical model, and then use the pre-diction results to select the more efficient query processing method to process the queries. The total processing time of such a hybrid method is 38.26ms for Test05 and 43.09ms for Test06 , which is smaller than the processing time of the non-hybrid methods. Recall that the total processing time of the hybrid method includes the time spent on predicting the performance of maxScore , the time spent on predicting the performance of WAND and the actual query processing time of the selected method. Clearly, these results further confirm that the developed Analytical model is efficient.
Note that the improvement of using the hybrid method over using only WAND is small, because there are only a few queries, in which maxScore is more efficient than WAND . For example, the optimal performance of the hybrid method (i.e., when we know the actual processing time of both meth-ods and select the more efficient one for each query) is only 42.32 ms on Test06 . We expect larger improvement when combining WAND with other more efficient query processing methods (e.g., BMW [14]).
Top-K query processing is one of the most important prob-lems in large-scale IR systems. Knowing the processing time of a query before executing it can lead to better online query scheduling, which could reduce the query latency and im-prove the throughput. Although it is relatively easy to pre-dict the performance of the exhaustive evaluation method, it is rather challenging to accurately model and predict the run-time performance for the dynamic pruning methods. Previous studies have attempted to address the problem by either using one feature to approximate the performance or applying machine learning methods to combine more than 40 loosely related features without conducting deep analysis on the query processing pipeline.

One of our contributions is to identify a set of fine-grained features that can accurately model the efficiency of the top-K query processing. To the best of our knowledge, this work is the first one that tackle the challenge through an analytic performance model. In particular, we analyze each query processing step, identify important features related to the query processing time and propose to use a small set of easily obtained statistic information to estimate the features. Ex-perimental results show that the proposed analytical model can predict the query processing more accurately than the state of the art methods, in particular for the dynamic prun-ing method. Moreover, the learned model is more robust and can be applied to accurately predict the processing time for other collections and for different values of K .

This paper has demonstrated that the proposed analytical model allows us to identify more accurate features related to the run-time performance. There are many interesting directions for the future work. First, we plan to revisit as-sumptions made in our study and conduct more thorough experiments to provide empirical justifications for these as-sumptions. Second, we plan to refine the developed model and extend it to other block-based index methods such as BMW [14]. Third, we focus on only the DAAT methods in this paper and plan to study other index traversing method such as TAAT. Finally, it would be interesting to study how to develop a general model to predict the performance in the distributed environment.
This material is based upon work supported by the Na-tional Science Foundation under Grant Number IIS-1423002. We thank the anonymous reviewers for their useful com-ments.
