 Session search retrieves documents for an entire session. Dur-ing a session, users often change queries to explore and in-vestigate the information needs. In this paper, we propose to use query change as a new form of relevance feedback for better session search. Evaluation conducted over the TREC 2012 Session Track shows that query change is a highly ef-fective form of feedback as compared with existing relevance feedback methods. The proposed method outperforms the state-of-the-art relevance feedback methods for the TREC 2012 Session Track by a significant improvement of &gt; 25%. H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Relevance Feedback; Session Search; Query Change
Session search retrieves documents for an entire session of queries. [3, 4]. It allows the user to constantly modify queries in order to find relevant documents. Session search involves many interactions between the search engine and the user. The challenge for session search is how to make use of these interactions and the user feedback to effectively improve search accuracy. In TREC (Text REtrieval Con-ference) 2012 Session tracks [6], the users (NIST assessors) clicked retrieved documents and interacted with a search engine to produce the queries and sessions. For each inter-mediate query, a retrieved document set containing the top 10 retrieval results ranked in decreasing relevance for the query are kept. The clicked data contains the documents clicked by users, their clicking orders, and dwell time. Fig-ure 1 illustrates the interactions among the user and the search engine.
 ences between two adjacent queries q i  X  1 and q i . Considering the directions of editing, query change  X  q i can be further decomposed into two parts: positive  X  q and negative  X  q . They are written as  X + X  q  X  and  X   X   X  q  X  respectively. The positive  X  q are new terms that the user adds to the previ-ous query; that is, they appear in q i , but did not appear in q i  X  1 . The negative  X  q are terms that the user deletes from the previous query; that is, they appeared in qi  X  1, but not appear in q i .

We thus decompose an adjacent query pair into: where + X  q i and  X   X  q i represent added terms and removed terms respectively, q theme is the theme terms, and the nota-tion of r represents set-theoretic difference. Table 1 demon-strates a few example TREC 2012 Session queries and their query changes.
 The theme terms ( q theme ) appear in both q i  X  1 and q i . Generally it implies a strong preference for those terms from the user. For example, in Table 1 session 32, q 1 =  X  X ollywood legislation X , q 2 =  X  X ollywood law X . q theme =  X  X ollywood X .
The added terms (+ X  q ) may indicate a specification or drifting between q i  X  1 and q i . In session 32, (+ X  q 2 ) =  X  X aw X .
The removed terms (  X   X  q ) may indicate a generalization or a drifting. In session 32, (  X   X  q 2 ) =  X  X egislation X .
Besides queries, a TREC session also contains retrieved document sets D (set of D i ) for each query q i , and clicked information C (set of C i ) for each query q i .

Based on observation of session search and user inten-sion, we propose an important assumption that the previous search result D i  X  1 influences the current query change  X  q i : In fact, this influence can be in quite a complex way. Figure 1 shows session 85 as an example, illustrating how the previ-ous retrieved documents D i  X  1 influence the query changes.
Based on our definition of query change, we utilize dif-ferent cases of query change in the calculation of relevance score between the current query q i and a document d .
Suppose P ( t | d ) is the original term weight for the re-trieve model in our utilization, we increase and decrease term weights on top of it. In the following formulas, P ( t | d ) is calculated by the multinomial query generation language model with Dirichlet smoothing [9] while P ( t | d ) is calculated based on Maximum-Likelihood Estimation (MLE): where d is the document under evaluation, Length ( d ) is the length of the document, TF ( t,d ) is the term frequency of t in document d , P ( t | C ) calculates the probability that t appears in corpus C based on MLE.  X  is set to 5000 in experiments.
We adjust the term weights for the three types of query changes as the following:  X  Theme terms are the repeated common parts nearly ap-pearing in the entire session. It implies their importance
Table 2: Dataset statistics for TREC 2012 Session Track. Table 3: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2012. A statistical significant improvement on nDCG@10 over the baseline is indicated with a  X  at p &lt; 0.05.
By considering all cases above, the relevance score be-tween the current query q i and a document d can be rep-resented as a linear combination of various term weight ad-justments: where d is the document under evaluation, log P ( q i | d ) is the original query-document relevance scoring function in log form,  X  ,  X  , , and  X  are coefficients for each type of query changes. Empirically, we set the coefficients as  X  = 2 . 2,  X  = 1 . 8, = 0 . 07, and  X  = 0 . 4. We evaluate our algorithm on the TREC 2012 Session Track [6]. According to how much prior information is used, the Track is divided into four phases: RL1 (using only the last query), RL2 (using all queries in the session), RL3 (us-ing all session queries and ranked lists of URLs and the cor-responding web pages), RL4 (using all session queries, the ranked lists of URLs and the corresponding web pages, the clicked URLs, and the time that the user spent on the cor-responding web pages). Table 2 shows the statistics about the TREC 2012 Session Track.
 The corpus used in our evaluation is ClueWeb09 CatB. 1 CatB consists of 50 million English pages from the Web col-lected during two months in 2009. We removed documents whose Waterloo X  X   X  X roupX X  spam raining score [2] are less than 70.

We compare the following algorithms in this paper:  X  Baseline (Lemur without relevance feedback) Using the original Lemur system (language modeling + Dirichlet smoothing) to retrieve for the last query q n .  X  PRF (Pseudo Relevance Feedback) . We utilize pseudo rel-evance feedback algorithm that developed in Lemur. We use the top 20 documents as pseudo relevant documents.
The retrieval is for the last query q n .  X  RF D i  X  1 . Rocchio using the previously retrieved top doc-uments proved by TREC. This method uses q n , q n  X  1 , and http://lemurproject.org/clueweb09/ observe that all systems improve their search accuracy when using query aggregation. The proposed QueryChg SAT run achieves an nDCG@10 of 0.3350, which is a 3.81% improve-ment over Lemur after uniform query aggregation, and a 27.76% improvement over Lemur without query aggrega-tion. The Lemur run after query aggregation performs well (nDCG@10=0.32) as compared with without query aggre-gation (nDCG@10=0.26 in Table 3). However, the proposed query change runs (QueryChg Click, QueryChg SAT) do not benefit much from query aggregation. This may be because that uniform aggregation equally weights each query, which assumes query independence among the queries in a session; whereas the query change relevance feedback runs assume that previous query and current query are dependent. The difference in the assumptions between query change rele-vance feedback model and the uniform aggregation may be the reason that the former does not benefit much from the latter. Other aggregation methods may be able to improve the situation.
TREC 2012 sessions were created by considering two dif-ferent dimensions: product type and goal quality. For prod-uct type, a session can be classified as searching for either factual or intellectual target. For search goal, a session can be classified as either specific or amorphous.

Both Table 5 and Table 6 show that the proposed method demonstrate difference effects on different session types. It achieves more improvement on Intellectual sessions (37.20%) and Amorphous sessions (31.48%) than on Factual sessions (19.09%) and Specific sessions (21.08%). This suggests that for more exploratory-style sessions, i.e., more difficult ses-sions, such as Intellectual and Amorphous sessions, our method is able to generate more performance gain. We believe that our method effectively captures query changes and well rep-resents the dynamics in a search session.
Based on the idea that query change is an important form of feedback, this paper presents a novel relevance feedback model by utilizing query change. Experiments show that our approach is highly effective and outperforms other feedback models for the TREC 2012 Session Track. Moreover, the
