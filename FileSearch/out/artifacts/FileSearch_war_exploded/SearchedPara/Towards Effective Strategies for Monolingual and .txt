 YAN QU, DAVID A. HULL, GREGOR Y GREFENSTETTE, DAVID A. EVANS Clairvoyance Corporation MOTOKO ISHIKAWA, SETSUKO NARA , TOSHIYA UEDA, DAISUKE NODA, KOUSAKU ARITA, YUKI FUNAKOSHI, HIROSHI MATSUDA Justsystem Corporation ________________________________________________________________________ ________________________________________________________________________ 1. INTRODUCTION World Stats, last updated on May 31, 2004 (www.InternetWorldStats.com), although North America and Europe (e.g., 6.0% penetration rate in China vs. 70.4% in the United States). The population size and projected gr owth of China, Japan, and Korea highlight the potential of the information market for such language communities. example, Chinese and Japanese character s are sequentially concatenated  X  written without spaces to mark word boundaries. Natural language processing of these languages European languages like Eng lish, French, and Spanish. Asian language communities. (BLIR) subtracks. Our major goal was to evaluate the performance and robustness of our recently developed commercial-grade CLIR systems for English, Japanese, and system. Such evaluation and analysis provides a basis for identifying possible improvements in our processing strategies. which text strings are parsed into overlapping character n-grams. Based on the word or n-grams tokens, researchers have experimented with indexing Japanese or Chinese documents using words [Chen et al. 1997; Kwok 1997; Fujita 1999; Fuji and Croft 1993], types alone [Kwok 1999]. Korean, researchers have explored general stra tegies such as query expansion via pseudo-thesaurus [Yang et al. 2004], and multi-word term down-weighting [Fujita 1999]. [Littman et al. 1998]; or using a pivot language when direct translation resources are not available for a language pair [Kishida and Ka ndo 2003]. Translation can be done either for using one translation over another [Oard 1998; McCarley 1999], even though a hybrid improving retrieval performance [McCarley 1999]. Most systems adopt the query requirements. choosing just a few good word s as a translation and keeping all the possible translations eliminating some words and keeping others, e. g., with parallel corpora [Davis and Ogden 1997] and co-occurrence statis tics from non-parallel corpora [Ballesteros and Croft 1998; Qu et al. 2002]. For instance, Qu et al. [200 2] addressed the problem of word translation performance in CLIR. models), more weight is given to the source words with more translations than those with addressed this problem by introducing balanced translation, in that the weight associated with each translation of a query term can be averaged in some way to compute a weight those weights. Pirkola and colleagues make use of structured queries, in that query keys are grouped together to form some structure and query operators are used in such a way same source query key together by means of the Syn-operator (which treats its operands [Oard and Wang 2001; Sperer and Oard 2000]. NTCIR-4 evaluation. Monolingu al IR system performance has been heavily analyzed in In this article we concentrate on English and Japanese. Our goal in monolingual analysis retrieval strategies such as pseudo-rel evance feedback and multi-word term down-strategies that are effective when used alone and in combination with other strategies. In this article we focus on Japanese-to-English retrieval. the strategies we employed to improve retrieval performance. For language-independent bilingual retrieval tasks, we examine techniques specifically targeting translation-related issues such as translation disambiguation and translation weight-balancing. In Section 3, we briefly describe the experimental setup, including parameter settings, data collections, We conclude this article in Section 6. 2. SYSTEM DESCRIPTION Justsystem and Clairvoyance corporatio ns share a common system framework for information retrieval and management, which serves as the foundation of Clairvoyance X  X  commercial CLARIT APIs for the English language [Evans and Lefferts 1995] and language text retrieval (CLIR) capability to the framework. Both monolingual and CLIR systems are highly parameterized to allow for system experimentation and optimization. two different retrieval systems in the experiments: ConceptBase Java (CBJ) and CLARIT Justsystem, served as the inde xing system for English, Japanese, and Chinese runs as well as the Japanese-to-English runs. Commercial versions of CBJ are currently available for English and Japanese; the Chinese version is under development. CLARIT Java (CLJ), a but rather takes indices from CBJ. first present components and features shared by both monolingual and bilingual retrieval systems: indexing, retrieval, pseudo-relevance feedback, and multi-word term down-structuring, query translation disambiguation, and translation term-filtering. 2.1 Indexing and Retrieval To identify indexing terms, we used CBJ to process documents and queries. CBJ employs tagging and rules for phrase identification. Built upon statistical bigram models, JPOT is as Japanese, Chinese, English, French, and Spanish. It uses the hidden Markov model these probabilities are learned from a corpus of each language. morphological analyzer aims to find the maximum of P(W), based on bigram statistics Yamashita and Matsumoto [2000]. normalization, dictionary lookup, and computation of maximum probability. JPOT also contains language-dependent processing components. For Chinese, JPOT forms. For example, the verb form  X   X  X  X  X   X  (made a business trip) is constructed by inserting a past tense particle  X   X   X  to the base verb form  X   X  X  X   X  (make a business trip). maintains a list of particles like  X   X   X  and morphological rules for such normalization. For Japanese and Chinese , kanji (Chinese character) numerals are also handled during this process. to concatenate some morphemes into phras es. The grammar can produce noun phrases, even though only noun phrases are usually used for indexing and query analysis during retrieval. Each language has its own grammar for constructing phrases. appears independently as a full noun phrase elsewhere in the document collection [Evans of an inverted index, with each index entry specifying the index word and a list of texts. experiments. Subdocuments range in size from 8 to 20 sentences and average about 12 sentences in length. are supported in the model. For CBJ in NTCIR-4, we used the dot product function for computing similarities between a query and a document: associated with the term t in the document D . The two weights are computed as follows: where IDF and TF are standard inverse document frequency and term frequency the user or automatically by the syst em (e.g., updated during feedback). term weights W D (t) as shown in Equation (2) to compute the similarity score between the again with the coefficient C(t) for assigning differential weights to terms. document length in the collection. 2.2 Query Expansion and Term Weighting used pseudo-relevance feedback based on high-scoring subdocuments to augment the queries. After retrieving some subdocuments for a given topic from the target corpus, we extracted terms from these subdocuments. We use two formulas, Prob2 and Rocchio, for extracting and ranking terms for expansion. where N is the number of subdocuments in the reference corpus; N t is the number of sub-documents that contain the term t in the corpus; R is the number of subdocuments in the and merged with the original query to create the final expanded query. from a reference corpus to provide a TF-IDF weighting of terms in the subdocuments and then applied the Rocchio formul a to compute the centroid vect or for the given set of sub-documents. The coordinates of the centroid vector are taken as term weights and used to rank and select terms: NumDoc , the number of subdocuments in the given set of subdocuments; and TF D (t) the term frequency score for term t in subdocument D . original terms in the query to form an expanded query. extracted for expansion, respec tively. Weighting options for Q exp include CLJ as an example: pairs. The settings are reported below in sections with the corresponding evaluation runs. 2.3 Multi-word Term Down-Weighting Fujita [1999] observed that the down-weighting of phrasal terms helped retrieval performance for the NTCIR-1 tasks [Kando et al. 1999]. While phrase terms are precise For NTCIR-4, we applied a weight of 0.8 or 1.0 to all single-word terms and 0.1 or 0.2 to all multi-word terms. 2.4 Bilingual Retrieval Task For bilingual CLIR, we adopted query translation as the means for bridging the language gap between the query language (Japanese) and the document language (English). First, the Japanese topics were parsed into words and phrases with the Japanese NLP module in CBJ. The terms were then translated into English. EDCIT and ENAMDICT dictionaries, a commercial lexicon (ATOK) developed by Justsystem Corporation, a lexicon extracted fro m a parallel corpus via a translation pair extraction tool, and a list of the names of fa mous Chinese collected from the Internet. Table I gives the details of these bilingual resources. corpus based on the Yomiur i (Japanese) / the Daily Yomiuri (English) newspaper texts, which contains 150,000 aligned sentences [Uchiyama and Isahara 2002]. Extraction of translation pairs works as follows: term t e . In extracting translation pairs from the parallel corpus, we focused on extracting proper with a weighted dice coefficient of more than 0.0 and co-occurrence frequency of 1 or correct proper noun translation pairs. keyboard input of Romanized Japanese wo rds into normal Japanese text, contains Kanji hiragana representation can be converted to romaji in Latin script. Atok also contains lexicon operates effectively as a translation lexicon. famous Chinese such as politicians, writers, and Hong Kong movie stars. uchikawa/ list/name.html, and http://bluesky.osaka-gaidai.ac.jp/~aono/zjcidian/zjall.htm. Dictionaries Sources No. of Entries EDR J-E Dictionary http://www.iijnet.or.jp/edr/E05EBIL.txt 170,875 EDR J-E Technical Terminology Dictionary ENAMDICT (historical persons) Translation pairs from Yomiuri parallel corpora Chinese names Made manually, based on resources from 2.4.2 Cross-Lingual Strategies. The English document collection for BLIR was indexed multi-word term down-weighting (Section 2.3). structuring. We describe these in detail below. incorrect segmentation or the terms are too ambiguous. Multiple translations of a source term are di sambiguated through the parallel corpora, as follows: translation pairs, described earlier in Sec tion 2.4.1, for the disambiguation process. are multiplied by 0.2; weights for translations for parts of phrases are multiplied by 0.8. frequency. The intuition is that if a translation has very low document frequency, then it is unlikely to be a good translation. The following formula is applied for terms with IDF higher than a threshold: term translation, some terms may have only one translation, while others may have many. Without proper balancing, the terms that have many translations can become over-weighted in the target language, compared to the terms with fewer translations. To deal language and multiple translations of a source term: Total no. terms 321 532 274 432 Total no. words 232 382 208 303 Total no. phrases 80 150 66 129 Avg. no. terms/topic 5.4 9.2 5.0 7.8 Avg. no. words/topic 4.0 6.6 3.8 5.5 
Avg. no. phrase/topic 1.4 2.6 1.2 2.3 and each target language term t translated from s. 3. EVALUATION SETUP To evaluate the different feat ures of our system, we used NTCIR-3 document collections The NTCIR-3 Japanese and English collections have 220,078 documents and 22,927 respectively, 596,058 documents and 347,550 documents. (CONC). In our experiments, we focused on the TITLE and DESC fields, as we believe documents associated with them. For English, the number of terms per topic ranges from comparable across English and Japanese topics. average uninterpolated precision and recall fo r comparing different parameter settings in assigned 4 degrees of relevance: S (highly relevant); A (relevant); B (partially relevant); and C (irrelevant). Two measurements of relevance granularity are defined subsequently: both evaluation measures and ex plored whether system parameter settings are influenced by the relevance granularity sp ecified by user requirements. For more information about the NTCIR-3 and NTCIR-4 CLIR tracks, the reader is referred to the conference overview reports by Oyama et al. [2 003] and Kishida et al. [2004]. 4. MONOLINGUAL RETRIEVAL two special properties of NTCIR experimental design and explore how they interact with relevance model . 4.1 System Calibration The Japanese or English documents were first parsed into linguistically meaningful units, The topics and documents were parsed similarly. We used CLJ for retrieval. has quite a few parameters. This is not a seri ous drawback as long as an appropriate set the parameter settings. In order to measure the robustness of the system, we performed a relevance judgments. For each of the sy stem components, we computed average precision over a range of parameter values. To conserve space, we report only the total shows the pre-expansion performance; all other results reflect post-query expansion. encouraging, we find that optimizing over the NTCIR-3 collection was extremely effective, putting us at or near the top of the range in every case. The only way we could have improved our official performance result of 0.389 would have been to expand with 30 terms instead of 35, giving us a meager (and statistically insignificant) gain of 0.002. 4.2 Monolingual Analysis experiments with two query types: Title only and Description only. The first experiment considered the weight assigned to multi-word terms and the interaction of this parameter expansion algorithms and three strategies for merging the set of expansion terms with the original query. (English or Japanese), query type (Title or De scription), query expansion (Yes [Prob2] or indexed in addition to the standard word features. All single words have a base weight of report summary statistics for only 32 of these runs directly. balanced experimental design, so the three English topics with no relevance judgments in uncontrolled sample from some underlying population). ANOVA models with a mixture computing significance tests [Lindman 1974]. effects, 10 two-way interactions, 10 three-wa y interactions, as well as four-and five-way significant impact on retrieval performance. more effective with the relaxed relevance judgments. Second, giving phrases full weight overwhelm the impact of the primary key words. Third, giving phrases full weight hurts weight is a bad idea, so neither (E) nor (F) is of particular long-term concern. or Description). It adds two factors: query expansion method (Rocchio or Prob2) and merging strategy (expansion terms only, constant term weights, and scaled term documents and 30 terms. Once again, English topics T2, T22, and T25 are dropped to relevance judgments and the title topic field. Fig. 3 shows the pruned ANOVA table. In this case, we isolate three significant va riables (G) to (I), whose coefficients appear performance gains only 0.010. There is no significant difference between Rocchio and Prob2 expansion. Rocchio works better when using the expansion terms only, while Prob2 is better with between relevance judgments, merging strategy, and language. This effect is small and difficult to interpret. Loosely speaking, scaled expansion is slightly better overall for be safely ignored. as follows. There is clear evidence that if mu lti-word phrases are indexed, they should be given a lower weight; any value less than or equal to half the weight of single-word terms terms is the most robust, although the overall gain is relatively small. 5. JAPANESE-ENGLISH RETRIEVAL TASK In this section we first examine the different errors our system produced in our NTCIR-4 and the interactions among these strategies. Lastly, we compare our system against MT-based CLIR systems and human translation. 5.1 Error Analysis classified the errors in our NTCIR-4 Japanese -English retrieval submission into types, as low retrieval performance scores, as compared with those of their respective monolingual documents not retrieved. The errors were then classified into major-and subtypes as in and D-run (Description). disambiguation module currently implemented in our system simply checks whether a their translations. As a result, the disambiguation process was not always effective in system used normalized merging strategies (Section 2.2) and lacked effective tuning of number of expansion terms, the bigger their effect on document scoring. As we have system. 5.1.2 Additional Experiments with Manual Modification. Given the error types presented above, we examined whether manually fixing the problems or changing the settings could help improve system performance. We have used the topics in the T-run for this analysis. Error Type Subtypes Description Failure to obtain correct translations (E1) Suboptimal tuning of system (E2) Term weighting JSCCC-J-E-T-01 
JSCCC-J-E-D-02 missing translations (E1.1) and incomplete phrasal types (E1.5)  X  account for the major Table VII result from missing translations of proper nouns. For example, in Query-10  X   X  X  X  X  ,  X  X  ,  X  X  X  ,  X  X  X   X  (Hu Jintao, Visit, Japan, Korea), the translation of precision from 0.0892 to 0.4794 (Table VIII). In Query-22  X   X  X  X  X  X  X  ,  X  X  X  X  X  X  X ,  X  X   X  (Legal Management, Kia Motors, Opinion), the translation of  X   X  X  X   X  is missing in our bilingual dictionary. Adding  X  X ia X  as a translation of  X   X  X  X   X  improved average precision from 0.0001 to 0.2504. submission, some translations analyzed as part-of-speech phrases other than NPs were not used for retrieval. For example, in Query-43  X   X  X  X  X  X  X  X  X  X  ,  X  X  X   X  (Derivative, Adj, and Adv were all used for retrieval, then the results became much better. example  X   X  X  X  X  X  X  ,  X  X  X  X  X  X  X  X  X  X   X  (Daily Life, Environmentally Friendly), where obtained from our bilingual dictionary included  X  X imple, X   X  X oft, X  and  X  X ight, X  which are performance for this query. Even though there might be cases like Query-57, since T-translation proved to be essential for good retrieval performance. Since parts of speech specific parts of speech. weight is important. For Query-25  X   X  X  X  X  ,  X  X  X  X  X  X  X  ,  X  X  X   X  (Report, Unemployment Rate Depreciation, South Korea), the document frequency of  X  X orea X  as a translation of  X   X  X  X   X  is very high (36,819), and many of the retrieved documents  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  ,  X  X  X  X  X  X  X  X  X   X  (Indonesia X  X  Political Crisis, America's Stand). Though  X  X ndonesia X  is a proper noun and expresses an important concept in the query, the retrieved documents did not contain  X  X ndonesia X  at all. We put more weight on this word and the results improved: the higher the weight, the better. PRF. To the extent that the translations are good, we cannot blame the PRF algorithm retrieval model for monolingual IR needs to be refined. Further investigation by us is required to explore the problem of tuning term weights and PRF. 5.2 Effectiveness of System Components implemented in our CLIR system as previously described in Section 2.4. For reference, frequency are CLIR-specific strategies. We compare the results against both the English monolingual baseline runs (Table X, designated as Mono.) and the bilingual baseline runs (designated as Baseline) in Tables XI to XIV. relaxed evaluations. Compared to their respective baselines, the T-run with combined strategies achieved 62.5% of the monolingual baseline; 141.2% of the bilingual baseline based on rigid evaluation; and 66.4% of monolingual baseline and 144.8% of rigid evaluation; 85.1% of monolingual baseline; and173.1% of bilingual baseline based on a relaxed evaluation. While all the strategies improved performance over the structuring X  caused performance degradation for the T-run. We examine the two strategies in detail below. down-weighting vs. the bilingual baseline for the T-run with relaxed evaluation. While or maintained the baseline performance (2 topics), 20 queries suffered from the use of multi-word term down-weighting. Queries that degraded most included Query-48  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  ,  X  X  X   X  (International Space Station, Construction); Query-55  X   X  X  X  X  ,  X  X  X  X  X  X  ,  X  X  X  X  ,  X  X  X   X  (North Korea, Daepodong, Asia, Response); Query-58  X   X  X  X  X  X  X  X  X  X  X  X  X  X   X  (Contactless SMART Card); and Query-60  X   X  X  X  X  X  X  X  X  X  ,  X  X  X   X  (Theme Park, Plan). Many of the multi-word terms in these phrasal terms. For example,  X   X  X  X  X   X  was considered a single morpheme as a result of phrasal term by English NLP; subsequently , multi-word term down-weighting reduced which resulted in ambiguity and poor performance. Condition Explanation Baseline Bilingual baseline + multi-word term down-weighting + Pseudo-relevance feedback (PRF) Pseudo-relevance feedback with Rocchio + disambiguation + query structuring + suppression of terms with low document frequency + all components Setting of NTCIR-4 submission. All the components above are used. Baseline 0.1509  X  44.2% 3108/5866  X  +disambiguation 0.1715 +13.7% 50.3% 3280/5866 +172 +multi-word term down-weighting 0.1468  X  2.7% 43.0% 2918/5866  X  190 +PRF 0.1708 +13.2% 50.1% 3246/5866 +138 +query structuring 0.1328  X  12.0% 38.9% 2975/5866  X  133 +suppress terms with low DF 0.1673 +10.9% 49.0% 3184/5866 +76 +all components ( JSCCC-J-E-T-01 ) Baseline 0.2005  X  45.9% 5419/11056  X  +disambiguation 0.2293 +14.4% 52.5% 5823/11056 +404 +multi-word term down-weighting 0.1940  X  3.2% 44.4% 5188/11056  X  231 +PRF 0.2394 +19.4% 54.8% 5732/11056 +313 +query structuring 0.1802  X  10.1% 41.2% 5188/11056  X  231 +suppress terms with low DF 0.2237 +11.6% 51.2% 5588/11056 +169 +all components ( JSCCC-J-E-T-01 ) Baseline 0.1503  X  44.4% 3096/5866  X  +disambiguation 0.1644 +9.4% 48.6% 3184/5866 +88 +multi-word term down-weighting 0.1557 +3.6% 46.0% 3052/5866  X  44 +PRF 0.2136 +42.1% 63.2% 3517/5866 +421 +query structuring 0.1712 +13.9% 50.6% 3275/5866 +179 +suppress terms with low DF 0.1608 +7.0% 47.5% 3125/5866 +29 +all components ( JSCCC-J-E-D-02 ) Baseline 0.2111  X  49.2% 5324/11056  X  +disambiguation 0.2320 +1.0% 54.1% 5547/11056 +223 +multi-word term down-weighting 0.2137 +0.3% 49.8% 5326/11056 +2 +PRF 0.3083 +46.0% 71.8% 6170/11056 +846 +query structuring 0.2324 +10.1% 54.1% 5609/11056 +285 +suppress terms with low DF 0.2230 +5.6% 52.0% 5381/11056 +57 +all components ( JSCCC-J-E-D-02 ) (e.g.,  X  X ot dog, X   X  X orth Korea X ) from among general multi-word terms as lexical atoms down-weighting. with longer queries, additional query terms help moderate any negative effect caused by down-weighting. bilingual baseline. Note that the use of query structuring caused great variation in both their subterms and (2) to connect multiple translations from one query term. Based on multi-word terms. in which  X   X  X  X   X  (Director) has 37 possible translations and  X   X  X  X   X  (Kurosawa) has one translation, translation structuring improved average precision dramatically, from 0.1686 to 0.4994.  X  X  X  X  X  X  X  X  X  X  ,  X  X  X   X , which had a big performance degradation due to query of both  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  (International Space Station) and its subterms. When query structuring is used, the query is represented as in Fig. 7. suggests that we should re-examine other operators such as OR for connecting a multi-work. Table XVI. Multi-word Term Down-Weighting with Other Strategies, T-Run, Rigid +multi-word term down-weighting 0.1468  X  2.7% 43.0% 2918/5866  X  190 +multi-word term down-weighting +disambiguation (vs. baseline+disambiguation) +multi-word term down-weighting +PRF (vs. baseline+PRF) +multi-word term down-weighting +query structuring (vs. baseline+query structuring) +multi-word term down-weighting +suppression of terms with low DF (vs. baseline+suppression of low dist terms) +all components(JSCCC-J-E-T-01) 0.2 131 +41.2% 62.5% 3688/5866 +580 word terms and their subterms, yielding an overall positive effect. 5.3 Interaction Effects Between Strategies system performance achieved by the integrated combination of strategies performed better than simply summing up the contributions of the individual strategies (Table XV). For instance, summing up the contribution of each component in Table XI for the T-run ( rigid ) had a cumulative improvement of 0.0347, while the integrated system produced an improvement of 0.0622. In addition, as we have observed earlier, multi-word term down-overcoming any negative effects of the individual strategies. Table XVII. Multi-word Term Down-Weight ing with Other Strategies, T-Run, Relaxed +multi-word term down-weighting 0.1940  X  3.2% 44.4% 5188/11056  X  231 +multi-word term down-weighting +disambiguation (vs. baseline+disambiguation) +multi-word term down-weighting +PRF (vs. baseline+PRF) +multi-word term down-weighting +query structuring (vs. baseline+query structuring) +multi-word term down-weighting +suppression of terms with low DF (vs. baseline+suppression of low dist terms) +all components(JSCCC-J-E-T-01) 0.2904 +44.8% 66.4% 6 509/11056 +1090 Baseline 0.1509  X  44.2% 3108/5866  X  +query structuring 0.1328  X  12.0% 38.9% 2975/5866  X  133 +query structuring + disambiguation (vs. baseline+disambiguation) +query structuring + multi-word term down-weighting (vs. baseline+multi-word term down-weighting) +query structuring + PRF (vs. baseline+PRF) +query structuring + suppress terms with low DF (vs. baseline+suppression of low dist terms) +all components ( JSCCC-J-E-T-01 ) 0.2131 +41.2% 62.5% 3688/5866 +580 combination with other strategies. weighting combined with another system strategy. Combining multi-word term down-Baseline 0.2005  X  45.9% 5419/11056  X  +query structuring 0.1802  X  10.1% 41.2% 5188/11056  X  231 +query structuring + disambiguation (vs. baseline+disambiguation) +query structuring + multi-word term down-weighting (vs. baseline+multi-word term down-weighting) +query structuring + PRF (vs. baseline+PRF) +query structuring + suppress terms with low DF (vs. baseline+suppression of low dist. terms) +all components ( JSCCC-J-E-T-01 ) 0.2904 +44.8% 66.4% 6509/11056 +1090 suppression of terms with low document frequency. Multi-word term down-weighting ( relaxed ), which is better than baseline. but worse than suppression of terms with low term down-weighting and query structuring was an improvement over query structuring However, the performance differences between Table XI and Table XVI for rigid different. disambiguation, X  all other combinations of query structuring with other strategies led to actual differences are not significantly different statistically. performance. Query structuring, however, causes complications when combined with other strategies, generally reducing effective performance. In our future work, we plan improve its performance. 5.4 Machine Translation vs. Bilingual Dictionaries for Query Translation The MT systems are Toshiba MT (Toshiba Co rporation) and YakushiteNet (Oki Electric query translation to the bilingual dictionaries in our system. The MT system we used is translated by the machine translation system; all other system settings remained the same for the machine translation experiment. queries (32 queries), performance decreased for other queries. with machine translations contained proper nouns that were correctly translated by the MT system, but, due to missing or incorrect translations, not by our bilingual dictionaries. a more correct and compact translation as a whole. Disambiguation is an integral part retrieval, reduced ambiguity generally contributes more to good performance; the lack feedback. Therefore, when good MT systems are available, they should be considered an important source of translation fo r cross-language information retrieval. 5.5 Evaluation Against Human Translation the Japanese-English bilingual retrieval re sults from Japanese topics to English monolingual retrieval results from English monolingual topics. Even though such human-translated English topics. has high proficiency in English. The translator had never seen either the English topics or the English document collection. The time for translating the titles and descriptions dictionaries 2 and the Google search engine to locate some translations. differences in the translations, other system settings are the same. generally performs as well as, or outperforms, the run based on bilingual dictionaries. But even for these queries, human translations are acceptable: Query-3 ES X  X  X  ES cell Embryonic Stem Cells 
Query-54  X  X  X  X  X  X  X  X ,  X  X  X  X ,  X  X  X 
Query-59  X  X  X  X  X  X  X ,  X  X  X  X  X  X  X  rather than the poor quality of human translation. performance for some queries. On the other hand, different word choices produced improvements in some other disambiguation. For example, for Query-12  X   X  X  X  ,  X  X  X  X   X , machine translation which is the right choice for the movies. translation is as good as human translation, yielding similar system performance. 6. SUMMARY influence the adoption of strategies. In the first monolingual experiment, we explored the (Japanese or English), query type (Title or De sc), query expansion (yes, no), and phrase the choice of language (English or Japanese ) nor the topic field (TITLE or DESC) has a terms. In the second monolingual experiment, we examined the interaction between the query expansion method and weight-merging strategy, again in a five-factor experimental or Desc), query expansion method (Rocchio or Prob2), and merging strategy (expansion terms is the most robust weight-merging method, but the overall gain is relatively small. weighting, PRF, query structuring, suppression of terms with low document frequency  X  performance enhancer, but query structurin g and multi-word term down-weighting were shown to affect retrieval performance ne gatively. Examining multi-word term down-and their subterms, and how multiple translati ons are weighted. Given the positive effect structuring in future work. based translation did not produced results as good as those achieved by MT translation coverage and quality of bilingual dictionaries and refining disambiguation functionality will remain a major research issue in our future work. REFERENCES 
