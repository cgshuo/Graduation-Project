 Ali Jalali and Sujay Sanghavi alij &amp; sanghavi@mail.utexas.edu Linear stochastic dynamical systems are classic pro-cesses that are widely used due to their simplicity and effectiveness in practice to model time series data in a huge number of fields: financial data (Cochrane, 2005), biological networks of species (Lawrence et al., 2010) or genes (Bar-Joseph, 2004), chemical reactions (Gille-spie, 2007; Higham, 2008), control systems with noise (Young, 1984), etc. An important task in several of these domains is learning the model from data which is often the first step in both data interpretation, pre-diction of future values or perturbation analysis. Often one is interested in learning the dependency structure ; i.e., identifying, for each variable, which set of other variables it directly interacts with.
 This paper considers the problem of structure learning in linear stochastic dynamical systems, in a setting where only a subset of the time series are observed, and others are unobserved/latent. In particular, we consider a system with state vectors x ( t )  X  R p and u ( t )  X  R r , for t  X  R + and dynamics described by where, w ( t )  X  R p + r is an independent standard Brow-rameters. We observe the process x ( t ) for some time horizon 0  X  t  X  T , but not the process u (  X  ). We are interested in learning the matrix A  X  (both for the con-tinuous and discrete time systems), which captures the interactions between the observed variables. However, the presence of latent time series u (  X  ), if not properly accounted for by the model learning procedure, will re-sult in the appearance of spurious interactions between observed variables especially for classic max-likelihood estimators even over infinite horizon.
 Suppose, for illustration, that we are interested in learning the dependency structure between the prices of a set of stocks x (  X  ) via model (1). Clearly, stock prices depend not only on each other, but are also jointly influenced by several variables u (  X  ) that may not be observed, for example, currency markets, com-modity prices, etc. The presence of u (  X  ) means that a naive learning algorithm (say LASSO) will report sev-eral spurious interactions; say, e.g. between all stocks that fluctuate with the price of oil.
 Clearly there are several issues with regards to funda-mental identifiability, and sample and computational complexity, that need to be defined and resolved. We do so below in the specific context of our model set-ting and provide both theoretical guarantees on the problem, as well as numerical illustrations for both synthetic and real data extracted from stock market. We organize the most directly related work as follows (recognizing of course that these descriptions overlap). Sparse Recovery and Gaussian Graphical Model Selection: It is now well recognized (Tib-shirani, 1996; Wainwright, 2009; Meinshausen &amp; Buhlmann, 2006) that a sparse vector can be tractably recovered from a small number of linear measurements; and also that these techniques can be applied to do model selection (i.e. inferring the Markov graph struc-ture and parameters) in Gaussian graphical models (Meinshausen &amp; Buhlmann, 2006; Ravikumar et al., 2008; d X  X spremont et al., 2007; Friedman et al., 2007; Yuan &amp; Lin, 2007). Two differences between our set-ting and these papers are that they do not have any latent factors, and theoretical guarantees typically re-quire independent (over time) samples. In particular, latent factors imply that these techniques will in ef-fect attempt to find models that are dense, and hence not be able to have a high-dimensional scaling. Cor-relation among samples means we cannot directly use standard concentration results, and also brings in the interesting issue of the effect of sampling frequency; in our setting, one can get more samples by finer sam-pling, but increased correlation means these do not result in better consistency.
 Sparse plus Low-Rank Matrix Decomposition: Our results are based on the possibility of separating a low-rank matrix from a sparse one, given their sum (either the entire matrix, or randomly sub-sampled el-ements thereof)  X  see (Chandrasekaran et al., 2011; Candes et al., 2009; Chen et al., 2011; Zhou et al., 2010; Candes &amp; Plan, 2010) for some recent results, as well as its applications in graph clustering (Jalali et al., 2011; Jalali &amp; Srebro, 2012), collaborative filter-ing (Srebro &amp; Jaakkola, 2003), image coding (Hazan et al., 2005), etc. Our setting is different because we observe correlated linear functions of the sum matrix, and furthermore these linear functions are generated by the stochastic linear dynamical system described by the matrix itself. Another difference is that sev-eral of these papers focus on recovery of the low-rank component, while we focus on the sparse one. These two objectives have a very different high-dimensional behavior.
 Inference with Latent Factors: In real applica-tions of data driven inference, it is always a concern that whether or not there exist influential factors that have never been observed (Loehlin, 1984; West, 2003). Several approaches to this problem are based on Ex-pectation Maximization (EM) (Dempster et al., 1977; Redner &amp; Walker, 1984); while this provides a natural and potentially general method, it suffers from the fact that it can get stuck in local optima (and hence is sen-sitive to initialization), and that it comes with weak theoretical guarantees. The paper (Chandrasekaran et al., 2010) takes an alternative, convex optimiza-tion approach to the latent factor problem in Gaus-sian graphical models, and is of direct relevance to our paper. In (Chandrasekaran et al., 2010), the objective is to find the number of latent factors in a Gaussian graphical model, given iid samples from the distribu-tion of observed variables; they also use sparse and low-rank matrix decomposition. Differences between our paper and theirs is that we focus on recovering the support of the  X  X parse part X , i.e. the interactions between the observed variables exactly, while they fo-cus on recovery the rank of the low-rank part (i.e. the number of latent variables). Our objective requires O (log p ) samples, theirs requires  X ( p ). Another major difference is that our observations are correlated, and hence sample complexity itself needs a different defi-nition (viz. it is no more the number of samples, but rather the overall time horizon over which the linear system is observed).
 System Identification: Linear dynamical system identification is a central problem in Control Theory (Ljung, 1999). There is a long line of work on this problem in that field including expectation maximiza-tion (EM) methods (Martens, 2010), Subspace Identi-fication (4SID) methods (Van Overschee &amp; De Moor, 1993), Prediction Error Method (PEM) (Ljung, 2002; Peeters et al.; Fazel et al., 2011). Our problem can be considered as a special case of system identification  X  X = AX + BU + W with output Y = CX + DU , when X = [ x ; u ], U = 0 and C is a matrix with identity ma-trix of size p  X  p on its diagonal and zero elsewhere. However, the results in the literature do not provide high-dimensional guarantees for system identification and perhaps our paper is an initial step in that direc-tion. Recently, (Bento et al., 2010) considered a prob-lem similar to ours, without any latent variables, i.e., the matrix C is identity. They implement the LASSO; the main contribution is characterizing sample com-plexity in the presence of sample dependence. In our setting, with latent variables, their method returns several spurious graph edges caused by marginaliza-tion of latent variables.
 Time-series Forecasting: Motivated by finance ap-plications, time-series forecasting has got a lot of attention during the past three decades (Chatfield, 2000). In the model based approaches, it is assumed that the time-series evolves according to some statis-tical model such as linear regression model (Bower-man &amp; O X  X onnell, 1993), transfer function model (Box et al., 1990), vector autoregressive model (Wei, 1994), etc. In each case, researchers have developed differ-ent methods to learn the parameters of the model for the purpose of forecasting. In this paper, we focus on linear stochastic dynamical systems that are an in-stance of vector autoregressive models. Previous work toward estimating this model parameters include ad-hoc use of neural network (Azoff, 1994) or support vector machine method (Kim, 2003), all without pro-viding theoretical guarantees on the performance of the algorithm. Our work is different from these results because although our method provides better predic-tion, our main focus is sparse model selection not pre-diction. Perhaps, once a sparse model is selected, one can study the prediction as a separate subject. Other than the continuous time model (1), we are in-terested in a similar objective for an analogous dis-crete time system with parameter 0 &lt;  X  &lt; 2  X  for  X  max (  X  ) being the maximum singular value: for all n  X  N 0 . Here, w ( n ) is a zero-mean Gaussian parameter  X  can be thought of as the sampling step; in particular notice that as  X   X  0, we recover model (1) from model (2). The upper bound on  X  ensures the stability of the discrete time system as required by our theorem. Intuitively,  X  max ( A  X  ) corresponds to the fastest convergence rate (Nyquist sampling rate). (A1) Stable Overall System : We only consider stable systems. In fact, we impose an assumption slightly stronger than the stability on the overall sys-tem. For the continuous system (1), we require D := eigenvalue. With slightly abuse of notation in the dis-crete system (2), we require D := 1  X   X  2 max  X  &gt; 0, where,  X  As a consequence of this assumption, by Lyapunov the-ory, the continuous system (1) has a unique stationary measure which is a zero-mean Gaussian distribution with positive definite (otherwise, it is not unique) co-Q steady-state covariance matrices of the observed and latent variables, respectively, and R  X  is the steady-state cross-covariance between observed and latent variables. By stability, we have C min :=  X  min ( Q  X  ) &gt; 0 and D max :=  X  max ( Q  X  ) &lt;  X  , where,  X  min (  X  ) is the minimum eigenvalue.
 Identifiability: Clearly, the above objective of iden-tifying A  X  is in general impossible without some ad-ditional assumptions on the model; in particular, sev-eral different choices of the overall model (including different choices of A  X  ) can result in the same effec-tive model for the x (  X  ) process. x (  X  ) would then be statistically identical under both models, and correct identification would not be possible even over an in-finite time horizon. Additionally, it would in general be impossible to achieve identification if the number of latent variables is comparable to or exceeds the num-ber of observed variables. Thus, to make the problem well-defined, we need to restrict (via appropriate as-sumptions) the set of models of interest. 3.1. Main Idea Consider the discrete-time system (2) in steady state and suppose, for a moment, that we ignored the fact that there may be latent time series; in this case, we would be back in the classical setting, for which the (population version of) the likelihood is Lemma 1. For x (  X  ) generated by (2) , the the optimum  X  A := max A L ( A ) is given by  X  A = A  X  + B  X  R  X  ( Q  X  Thus, the optimal  X  A is a sum of the original A  X  (which captures the spurious interactions obtained due to the has the rank at most equal to number r of latent time series. We will assume that the number of latent time series is smaller than the number of observed ones  X  i.e. r &lt; p  X  and hence B  X  R  X  ( Q  X  )  X  1 is a low-rank matrix . 3.2. Identifiability Besides identifying the effect of the latent time series, we would need the true model to be such that A  X  is study models that have a local-global structure where (a) each of the observed time series x i ( t ) interacts with only a few other observed series, while (b) each of the latent series interacts with a (relatively) large number of observed series. In the stock market example, this would model the case where the latent series corre-sponds to macro-economic factors, like currencies or the price of oil, that affect a lot of stock prices. In particular, let s be the maximum number of non-zero entries in any row or column of A  X  ; it is the maxi-mum number of other observed variables any given ob-served variable directly interacts with. Note that this means A  X  is a sparse matrix. Let L  X  := B  X  R  X  ( Q  X  )  X  1 that its rank is r . Then, following (Chen et al., 2011), L  X  is said to be  X  -incoherent if  X  &gt; 0 is the smallest real number satisfying where, e i  X  X  are standard basis vectors and k X k is vector 2-norm. Smaller values of  X  mean the row/column spaces make larger angles with the standard bases, and hence the resulting matrix is more dense. (A2) Identifiability : We require that the s of the sparse matrix A  X  and the  X  of the low-rank L  X  , which has rank r , satisfy  X  := 3 3.3. Algorithm Recall that our task is to recover the matrix A  X  given observations of the x (  X  ) process. We saw that the max-likelihood estimate (in the population case) was the sum of A  X  and a low-rank matrix; we subsequently assumed that A  X  is sparse. It is natural to use the max-likelihood as the loss function for the sum of a sparse and low-rank matrix, and separate appropri-ate regularizers for each of the components. Thus, for the continuous-time system observed up to time T , we propose solving and for the discrete-time system given n samples, we propose solving Here k X k 1 is the  X  1 norm (a convex surrogate for spar-sity), and k X k  X  is the nuclear norm (i.e. sum of sin-gular values, a convex surrogate for low-rank). The optimum b A of (4) or (3) is our estimate of A  X  , and our main result provides conditions under which we recover the support of A  X  , as well as a bound on the error in values k b A  X  A  X  k  X  (maximum absolute value). We provide a bound on the error k b L  X  L  X  k 2 (spectral norm) for the low-rank part. 3.4. High-dimensional setting We are interested in recovering A  X  with a number of samples n that is potentially much smaller than p (for small s ). In the special case when we are in steady state and L = 0 (i.e.  X  L large) the recovery of each row of A  X  is akin to a LASSO (Tibshirani, 1996) problem with Q  X  being the covariance of the design matrix. We thus require Q  X  to satisfy incoherence conditions that are akin to those in LASSO (see e.g. (Wainwright, 2009) for the necessity of such conditions). (A3) Incoherence : To control the effect of the irrel-evant (not latent) variables on the set of relevant vari-0 , where, S k is the support of the k th row of A  X  and k is the complement of that. The norm k X k  X  , 1 is the maximum of the  X  1 -norm of the rows. In this section, we present our main result for both Continuous and Discrete time systems. We start by imposing some assumptions on the regularizers and the sample complexity. (A4) Regularizers : Let m be the maximum of effect of initial condition and latent variables through matrix B  X  . We impose the following assumptions on the regularizers: (A5) Sample Complexity : In our setting, the smaller the  X  is, the more dependent two subsequent samples are. Sample complexity is thus governed by the total time horizon  X n = T over which we observe the system, and not simply n ; indeed finer sampling (i.e. smaller  X  ) requires a larger number of samples. For a probability of failure  X  , we require
Here K is a constant independent of any other system parameter; for example, K  X  3  X  10 6 suffices. theorem states our main result for both discrete and continuous time systems.
 Theorem 1. If assumptions (A1)-(A5) are satisfied, then with probability 1  X   X  , our algorithm outputs a pair ( b A, b L ) satisfying (a) Sub Support Recovery: Supp ( b A )  X  Supp ( A  X  ) . (b) Error Bounds: k b
A  X  A  X  k  X   X   X  X  A and k b L  X  L  X  k 2  X  (c) Exact Signed Support Recovery: If addition-ally the smallest magnitude A min of a non-zero ele-ment of A  X  satisfies A min &gt;  X  X  A , then we obtain full signed-support recovery Sign ( b A ) = Sign ( A  X  ) . Note: Note that  X  A , as defined in (A4-1) , depends on the sample complexity T , and goes to 0 as T becomes large. Thus it is possible to get exact signed support recovery by making T large.
 Remark 1: Our result shows that, in sparse and low-rank decomposition for latent variable modeling, re-covery of only the sparse component seems to be pos-sible with much fewer samples  X  O ( s 3 log p )  X  as com-pared to, for example, the recovery of the exact rank of the low-rank part; the latter was show to require  X ( p ) samples in (Chandrasekaran et al., 2010). Remark 2: The above theorem shows that, even in the presence of latent variables, our algorithm requires a similar number of samples (i.e. upto universal con-stants) as previous work (Bento et al., 2010) required in the absence of hidden variables. Of course, this is true as long as identifiability (A2) holds. Note that the absence of such identifiability conditions makes even simple sparse and low-rank matrix decomposition ill-posed (Chandrasekaran et al., 2011).
 Remark 3: Although our theoretical result shows a scaling of s 3 for the sample complexity, the empirical result suggests that the correct scaling factor is s 2 . We suspect our result as well as Bento et al. (2010) can be tightened.
 Illustrative Example: Consider a simple idealized example that helps give intuition about the above the-orem. Suppose that we are in the continuous time set-ting, where each latent variable j depends only on its own past, updating according to dx j dt =  X  x j ( t ) + dw and for each observed variable i depends only on its own past and a unique latent variable j ( i ), i.e., ables, and assume that each latent variable affects ex-actly p r observed variables in this way.
 For this idealized setting, we can exactly evaluate all the quantities we need. It is not hard to show that the steady-state covariance matrices are Q  X  = 0 . 5( I + r . Hence, we need r &lt; Moreover, we can show that  X  = 1 2 for this example and hence the assumption (A3) is also satisfied. Finally by evaluating other parameters in the theorem, we get the error bounds k A  X   X   X  A k  X   X  (3 r/ (4 and k L  X   X   X  L k 2  X  3 r X  A / (32 calculations can be found in the appendix available online. In this section, we first introduce some notations and definitions and then, provide a three step proof tech-nique to prove the main theorem for the discrete time system. The proof of the continuous time system is done via a coupling argument in the appendix. There are two key novel ingredients in the proof en-abling us to get the low sample complexity result in our theorem. The first ingredient comes from our new set of optimality conditions inspired by (Candes et al., 2009). This optimality conditions enable us to cer-tify an approximation of L  X  while certifying the exact sign support of A  X  . The second ingredient comes from the bounds on the Schur complement of the perturba-tion of positive semi-definite matrices (Stewart, 1995). This result enables us to get a bound on the Schur com-plement of a perturbation of a positive semi-definite matrix of size p with only log( p ) samples. Given a matrix A  X  , let  X  be the subspace of matrices whose their support is a subset of the matrix A  X  . The orthogonal projection of a matrix M to  X  is denoted by P  X  ( M ). Denote the orthogonal complement space with  X  c with orthogonal projection P  X  c ( M ). For any matrix L  X  R p  X  p , if the SVD is L = U  X  V T , then let T ( L ) := { M = U X T + Y V T for some X, Y } denote the subspace spanned by all matrices that have the same column space or row space as L . The orthogonal projection of a matrix N to T is denoted by P T ( N ). Denote the orthogonal com-plement space with T c with orthogonal projection P
T c . We define a metric to measure the close-ness of two subspaces T 1 and T 2 as  X  ( T 1 , T 2 ) = value decomposition.
 We outline the proof in three steps as follows: STEP 1: Constructing a candidate primal optimal solution ( e A, e L ) with the desired sparsity pattern using the restricted support optimization problem, called or-acle problem : This oracle is similar to the one used in (Chan-drasekaran et al., 2010). It ensures that the right spar-sity pattern is chosen for e A and the tangent spaces e L and L  X  come from are close with parameter  X  . STEP 2: Writing down a set of sufficient (stationary) optimality conditions for ( e A, e L ) to be the unique solu-tion of the (unrestricted) optimization problem (4): Lemma 2. If  X   X  X  = { 0 } , then ( e A, e L ) , the solution to the oracle problem (5) , is the unique solution of the problem (4) if there exists a matrix e Z  X  R p  X  p s.t. STEP 3: Constructing a dual variable e Z that satisfies the sufficient optimality conditions stated in Lemma 2. For matrices M  X   X  and N  X  X  , let It has been shown in (Chen et al., 2011) that if  X  &lt; 1 then both infinite sums converge. Suppose we have the SVD decomposition e L = e U e  X  e V T . Let where,  X  is a matrix such that (C5) is satisfied. As a result of this construction, we have P  X  ( X ) = P T ( X ) = 0. Now, we can establish P  X  ( e Z ) =  X  A Sign( e A ) and P tions (C1) and (C3) in Lemma 2 are satisfied. It suf-fices to show that (C2) and (C4) are satisfied with high probability. This has been shown in Lemma 6. 6.1. Synthetic Data Motivated by the illustrative example discussed in sec-tion 4, we simulate a similar (but different) dynamic system for the purpose of our experiments. Consider the system where each latent variable only evolves by itself, i.e., C  X  = 0 and D  X  is a diagonal matrix. More-over, assume that each latent variable affects 2 p/r ob-served variable and each observed variable is affected by exactly two latent variable. We randomly select a support of size s per row for A  X  and draw all the values of A  X  and B  X  i.i.d. standard Gaussian. To make the matrix A  X  stable, by Ger X sgorin disk theo-rem (Ger X sgorin, 1931), we put a large-enough negative value on the diagonals of A  X  and D  X  .
 We generate the data according to the continuous time model sub-sampled at points t i =  X i for i = 1 , 2 , . . . , n , that is The stochastic integral can be estimated by binning the interval and assuming the Brownian motion is con-stant over the bin and hence, can be estimated by a standard Gaussian. See Chapter 4 in Shreve (2004). Using this data, we solve (4) using accelerated proxi-mal gradient method (Lin et al., 2009). Motivated by our Theorem, we plot our result with respect to the Figure 5 shows the phase transition of the probability of success in recovering the exact sign support of the matrix A  X  . We ran three different experiments, each investigating the effect of one of the three key param-eters of the system  X  (sampling frequency), r (num-ber of latent variables) and s (sparsity of the model). These three figures show that the probability of suc-cess curves line up if they are plotted versus the correct control parameter. The first two curves for  X  and r line up versus  X , indicating that our theorem suggests the correct scaling law for the sample complexity. How-ever, from this experiment, it seems that the phase transition probability lines up with respect to  X  s sug-gesting the scaling of s 2 instead of s 3 . 6.2. Stock Market Data We take the end-of-the-day closing stock prices for 50 different companies in the period of May 17, 2010 -May 13, 2011 (255 business days). These compa-nies (among them, Amazon, eBay, Pepsi, etc) are con-sumer goods companies traded either at NASDAQ or NYSE in USD. The data is collected from Google Fi-nance website. Applying our method and pure LASSO (Bento et al., 2010) to the data, we recover the struc-ture of the dependencies among stocks. We present the result as a graph in Fig 6.2; where each company is a node in this graph and there is an edge between company i and j if  X  A ij 6 = 0. This result shows that the recovered dependency structure by our algorithm is order of magnitude sparser than the one recovered by pure LASSO.
 To show the usefulness of our algorithm for prediction purposes, we apply our algorithm to this data and try to learn the model using the data for random n (con-secutive) days. Then, we compute the mean squared error in the prediction of the following month (25 busi-ness days). The ratio n 25 is the training/testing ratio in our experiment.
 Figure 3(b) shows the prediction error for both our and pure LASSO (Bento et al., 2010) methods as the train/test ratio increases. It can be seen that our method not only have better prediction, but also is more robust. Our algorithm requires only three months of the past data to give a robust estimation of the next month; in contrast with almost 6 months requirement of LASSO while the error of our algorithm is much smaller (by a factor of 6) than LASSO even in the steady state. Figure 3(a) illustrates that our estimated b A is order of magnitude sparser than the one estimated by LASSO. The number of latent vari-ables our model finds varies from 8  X  12 for different train/test ratios.

