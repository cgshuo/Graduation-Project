 How do we find patterns in author-keyword associations, evolving over time? Or in DataCubes, with product-branch-customer sales information? Matrix decompositions, like principal component analysis (PCA) and variants, are in-valuable tools for mining, dimensionality reduction, feat ure selection, rule identification in numerous settings like st ream-ing data, text, graphs, social networks and many more. However, they have only two orders, like author and key-word, in the above example.

We propose to envision such higher order data as tensors , and tap the vast literature on the topic. However, these methods do not necessarily scale up, let alone operate on semi-infinite streams. Thus, we introduce the dynamic ten-sor analysis (DTA) method, and its variants. DTA provides a compact summary for high-order and high-dimensional data, and it also reveals the hidden correlations. Algorith -mically, we designed DTA very carefully so that it is (a) scalable , (b) space efficient (it does not need to store the past) and (c) fully automatic with no need for user defined parameters. Moreover, we propose STA, a streaming tensor analysis method, which provides a fast, streaming approxi-mation to DTA.

We implemented all our methods, and applied them in two real settings, namely, anomaly detection and multi-way latent semantic indexing. We used two real, large datasets, one on network flow data (100GB over 1 month) and one from DBLP (200MB over 25 years). Our experiments show that our methods are fast, accurate and that they find in-teresting patterns and outliers on the real datasets. H.2.8 [ Database applications ]: Data mining Algorithms
Given a keyword-author-timestamp-conference bibliogra-phy, how can we find patterns and latent concepts? Given Internet traffic data (who sends packets to whom, on what port, and when), how can we find anomalies, patterns and summaries? Anomalies could be, e.g., port-scanners, pat-terns could be of the form  X  X orkstations are down on week-ends, while servers spike at Fridays for backups X . Sum-maries like the one above are useful to give us an idea what is the past (which is probably the norm), so that we can spot deviations from it in the future.

Matrices and matrix operations like SVD/PCA have played a vital role in finding patterns when the dataset is  X 2D X , and can thus be represented as a matrix. Important applications of this view point include numerous settings like: 1) information retrieval , where the data can be turned into a document-term matrix, and then apply LSI [9, 24]; 2) market basket analysis , with customer-products ma-trices, where we can apply association rules [2] or  X  X atio Rules X  [21]; 3) the web , where both rows and columns are pages, and links correspond to edges between them; then we can apply HITS [19] or pageRank [3] to find hubs, au-thorities and influential nodes; all of them are identical or closely related to eigen analysis or derivatives; 4) social networks , and in fact, any graph (with un-labelled edges): people are rows and columns; edges again correspond to non-zero entries in the adjacency matrix. The network value of a customer [13] has close ties to the first eigenvector; graph partitioning [18] is often done through matrix alge-bra (e.g. spectral clustering [16]); 5) streams and co-evolving sequences can also be envisioned as matrices: each data source (sensor) corresponds to a row, and each time-tick to a column. Then we can do multivariate anal-ysis or SVD [25], X  X ketches X  and random projections [14] to find patterns and outliers.
 The need for tensors: Powerful as they may be, matrix-based tools can handle neither of the two problems we stated in the beginning. The crux is that matrices have only two  X  X imensions X  (e.g.,  X  X ustomers X  and  X  X roducts X ), while we may often need more, like  X  X uthors X ,  X  X eywords X ,  X  X imes-tamps X ,  X  X onferences X . This is exactly what a tensor is, and of course, a tensor is a generalization of a matrix (and of a vector, and of a scalar). We propose to envision all such problems as tensor problems, to use the vast literature of tensors to our benefit, and to introduce new tensor analysis tools, tailored for streaming applications. Using tensors , we can attack an even wider range of problems, that matrices can not even touch. For example, 1) Rich, time-evolving net-work traffic data, as mentioned earlier: we have tensors of order M = 3, with modes  X  X ource-IP X ,  X  X estination-IP X  and  X  X ort X  over time. 2) Labeled graphs and social networks: suppose that we have different types of edges in a social net-work (eg., who-called-whom, who-likes-whom, who-emailed -whom, who-borrowed-money-from-whom). In that case, we have a 3rd order tensor, with edge-type being the 3rd mode. Over time, we have multiple 3rd order tensors, which are still within the reach of our upcoming tools. 3) Microarray data, with gene expressions evolving over time [32]. Here we have genes-proteins over time, and we record the expression level: a series of 2nd order tensors. 4) All OLAP and Dat-aCube applications: customer-product-branch sales data i s a 3rd order tensor; and so is patient-diagnoses-drug treat-ment data.
 Motivating example: Let us consider the network mon-itoring example. Here we have network flows arriving very fast and continuously through routers, where each flow con-sists of source IP, destination IP, port number and the num-ber of packets. How to monitor the dynamic traffic behav-ior? How to identify anomalies which can signify a potential intrusion, or worm propagation, or an attack? What are the correlations across the various sources, destinations and ports?
Therefore, from the data model aspect , we propose to use a more general and expressive model tensor sequences . For the network flow example, the 3rd order tensor for a given time period has three modes: source, destination and port, which can be viewed as a 3D data cube (see Figure 1(a)). An entry ( i , j , k ) in that tensor (like the small blue cube in Figure 1(a)) has the number of packets from the correspond-ing source ( i ) to the destination j through port k , during the given time period. The dynamic aspect comes from the fact that new tensors are arriving continuously over time.
From the algorithmic aspect , we propose the dynamic and the streaming tensor analysis (DTA and STA) to sum-marize the original tensors into smaller  X  core  X  tensors and the  X  projection matrices  X  (one for each mode), as shown in Figure 1(b). The projection matrices can be updated incre-mentally over time when a new tensor arrives, and contain valuable information about which, eg., source IP addresses are correlated with which destination IP addresses and des-tination ports, over time.

Finally, from the application aspect , we illustrate the anomaly detection and multi-way LSI through DTA/STA. For the former, our method found suspicious Internet activ-ity in a real trace, and it also found the vast majority of injected anomalies (see Section 6). For the latter, we found natural groups of authors, keywords and time intervals, in
Symbol Description v a vector (lower-case bold) v ( i ) the i -element of vector v A a matrix (upper-case bold) A T the transpose of A
A i | n i =1 a sequence of N matrices A 1 , . . . , A n A ( i, j ) the entry ( i, j ) of A A ( i, :) or A (: , i ) i -th row or column of A A a tensor (calligraphic style)
A ( i 1 , . . . , i M ) the element of X with index ( i 1 M the order of the tensor
N i the dimensionality of the i th mode (1  X  i  X  M ) the DBLP bibliography subset.
 Our contributions:
The rest of the paper is organized as the following: Section 2 introduces the necessary background. Then Section 3 for-mally defines the problem and gives an overview of the methods. Section 4 presents the key methods, dynamic ten-sor analysis (DTA) and streaming tensor analysis (STA). Using DTA or STA, Section 5 illustrates two important ap-plications: anomaly detection and multi-way latent seman-tics indexing. In Section 6 and Section 7 we extensively evaluate all the proposed methods using two real datasets. Section 8 discusses the related work. Finally we conclude in Section 9.
Here we briefly give the main concepts and terms from principal component analysis (PCA), and from tensor alge-bra, also known as multilinear analysis.
PCA, as shown in Figure 2, finds the best linear projec-tions of a set of high dimensional points to minimize least-squares cost. More formally, given n points represented as row vectors x i | n i =1  X  R N in an N dimensional space, PCA computes n points y i | n i =1  X  R R ( R  X  N ) in a lower dimen-sional space and the projection matrix U  X  R N  X  R such that the least-squares cost e = P n i =1 k x i  X  y i U T k 2 2 Figure 2: PCA projects the N -D vector x i s into R -D
The solution of PCA can be computed efficiently by di-agonalizing the covariance matrix of x i | n i =1 . Alternatively, if the rows are zero mean, then PCA is computed by the Both x and y are row vectors.
 Singular Value Decomposition (SVD): if the SVD of X is
The intuition behind PCA is the following: if X is the author-keyword matrix of the DBLP dataset, then matrix Y is roughly the author-topic, and matrix U is the keyword-topic matrix. As mentioned, a tensor of order M closely resembles a Data Cube with M dimensions. Formally, we write an M th order tensor X  X  R N 1  X  X  N M as X [ N N i (1  X  i  X  M ) is the dimensionality of the i th mode ( X  X i-mension X  in OLAP terminology). For brevity, we often omit the subscript [ N 1 , . . . , N M ].

We will also follow the typical conventions, and denote matrices with upper case bold letters (e.g., U ) row vectors with lower-case bold letters (e.g., x , scalars with lower-case normal font (e.g., n ), and tensors with calligraphic font (e.g., X ). From the tensor literature we need the following defini-tions: Definition 1 (Matricizing or Matrix Unfolding).
 The mode-d matricizing or matrix unfolding of an M th or-keeping index d fixed and varying the other indices. There-fore, the mode-d matricizing X ( d ) is in R ( Q i 6 = d The mode-d matricizing X is denoted as unfold( X , d )= X Similarly, the inverse operation is denoted as fold( X ( d ) particular, we have X = fold(unfold( X , d )). Figure 3 shows an example of mode-1 matricizing of a 3rd order tensor X  X  R the shaded area of X (1) in Figure 3 the slice of the 3rd mode along the 2nd dimension.
 Definition 2 (Mode Product). The mode product X X  d for all index values.

Figure 4 shows an example of 3rd order tensor X mode-1 multiplies a matrix U . The process is equivalent to first matricize X along mode-1, then to do matrix multiplication of X 1 and U , finally to fold the result back as a tensor. U clarity. Furthermore, the notation for X  X  1 U 1  X  i  X  1 U except the i -th) is simplified as X Q Definition 3 (Rank-( R 1 , . . . , R M ) approximation). with rank  X   X  X ( d )  X  = R d for 1  X  d  X  M , that minimizes the least-squares cost  X   X  approximation of X . 2 The best rank approximation  X  X = Y U
In this section, we first formally define the notations of tensor sequence and tensor stream . Then we overview the operations on them.

Definition 4 (tensor sequence). A sequence of M th order tensor X 1 . . . X n , where each X i  X  R N 1  X  X  N M ( 1  X  i  X  n ), is called tensor sequence if n is a fixed natural num-ber. And n is the cardinality of the tensor sequence. In fact, we can view an M th order tensor sequence X 1 . . . X as a single ( M +1)th order tensor with the dimensionality n on the additional mode.

Definition 5 (tensor stream). A sequence of M th order tensor X 1 . . . X n , where each X i  X  R N 1  X  X  N M ( 1  X  i  X  n ), is called a tensor stream if n is an integer that increases with time.

Intuitively, we can consider a tensor stream is coming in-crementally over time. And X n is the latest tensor in the stream. In network monitoring example, a new 3rd order tensor (as the one in Figure 1(a)) comes every hour.
After defining the data models, the main operation is to represent the original tensors in other basis such that un-derlying patterns are easily revealed.

Definition 6 (Tensor analysis). Given a sequence of tensors X 1 . . . X n , where each X i  X  R N 1  X  X  N M ( 1  X  i  X  n ), find the orthogonal matrices U i  X  R N i  X  R i | M i =1 , one for each mode, such that the reconstruction error e is minimized: The square Frobenius norm is defined as kXk 2 F = X Note that X t the space spanned by U i | M i =1 . And it can be rewritten as Y X
More specifically, we propose three variants under tensor analysis : offline tensor analysis (OTA) for a tensor sequence (see Section 4.1); and dynamic tensor analysis (DTA) and streaming tensor analysis (STA) for a tensor stream (see Section 4.2 and Section 4.3).
First, Section 4.1 introduces offline tensor analysis (OTA) for a tensor sequence. Second, we present the core com-ponent, dynamic tensor analysis (DTA) (Section 4.2) and its variants. Third, Section 4.3 introduces streaming tensor analysis (STA), which approximates DTA to further reduce the time complexity.
We now introduce the offline tensor analysis (OTA) which is a generalization of PCA for higher order tensors 3 .
Unlike PCA, which requires the input to be vectors (1st-order tensors), OTA can accept general M th order tensors for dimensionality reduction. The formal definition is ex-actly the same as Definition 6. Figure 5 shows an example of OTA over n 2nd order tensors. Figure 5: OTA projects n large 2nd order tensors X i
Unfortunately, the closed form solution for OTA does not exist. An alternating projection is used to approach to the optimal projection matrices U l | M l =1 . The iterative al-gorithm converges in finite number of steps because each sub-optimization problem is convex. The detailed algorith m is given in Figure 6. Intuitively, it projects and matricize s along each mode; and it performs PCA to find the projec-tion matrix for that mode. This process potentially needs performing more than one iteration.

In practice, this algorithm converges in a small number of iterations. In fact, Ding and Ye [7] shows the near-optimality of a similar algorithm for 2nd order tensors. The re-fore, for time-critical applications, single iteration is usu-ally sufficient to obtain the near-optimal projection matric es U d | M d =1 . In that case, the algorithm essentially unfolds the tensor along each mode and applies PCA on those unfolded matrices separately.

Note that OTA requires all tensors available up front, which is not possible for dynamic environments (i.e., new
Similar ideas have been proposed for 2nd-or 3rd order tensors [22, 30, 12].
 Input : The dimensionality of the output tensors Y i  X  R R 1  X  ...  X  R Output : Algorithm : 1. Set U l to be N l  X  R l truncated identity matrix. 2. Conduct 3 -8 iteratively. 3. For d = 1 to M 4. For 1  X  i  X  n 5. construct X d i = X i ( Q 7. construct variance matrix C d = P n i =1 X T i ( d ) 8. compute U d by diagonalizing C d 9. Check convergence: T r ( || U T l U l | X  I | )  X   X  for 1  X  l  X  M . 10.Calculate the core tensor Y i = X i tensors keep coming). Even if we want to apply OTA every time that a new tensor arrives, it is prohibitively expensiv e or merely impossible since the computation and space re-quirement are unbounded. Next we present an algorithm for the dynamic setting.
Here we present the dynamic tensor analysis (DTA), an incremental algorithm for tensor dimensionality reductio n. Intuition: The idea of incremental algorithm is to exploit two facts: 1. In general OTA can be computed relatively quickly 2. Variance matrices can be incrementally updated with-
The algorithm processes each mode of the tensor at a time. In particular, the variance matrix of the d th mode is updated as: the tensor X . The updated projection matrices can be com-puted by diagonalization: C d = U d S d U T d , where U d orthogonal matrix and S d is a diagonal matrix. The pseudo-code is listed in Figure 7. The process is also visualized in Figure 8.
 Forgetting factor: Dealing with time dependent models, we usually do not treat all the timestamps equally. Often the recent timestamps are more important than the ones far away from the past 5 . More specifically, we introduce a for-getting factor into the model. In terms of implementation, the only change to the algorithm in Figure 7:
Unless there is a seasonality in the data, which is not dis-cussed in the paper.
 Input : Previous projection matrices U i | M i =1  X  R N i  X  R i Previous energy matrices S i | M i =1  X  R R i  X  R i Output : Algorithm : // update every mode 1. For d = 1 to M 3. Reconstruct variance matrix C d  X  U d S d U T d 4. Update variance matrix C d  X  C d + X T ( d ) X ( d ) 5. Diagonalization C d = U d S d U T d 6. Compute new rank R d and truncate U d and S d 7. Calculate the core tensor Y = X
Figure 7: Algorithm: Dynamic Tensor Analysis Figure 8: New tensor X is matricized along the d th where  X  is the forgetting factor between 0 and 1. Herein  X  = 0 when no historical tensors are considered, while  X  = 1 when historical tensors have the same weights as the new tensor X . Note that forgetting factor is a well-known trick in signal processing and adaptive filtering [11]. Update Rank: One thing missing from Figure 7 is how to update the rank R i . In general the rank can be different on each mode or can change over time. The idea is to keep the smallest rank R i such that the energy ratio k S i k F  X  k is above the threshold [15]. In all experiments, we set the threshold as 0.9 unless mentioned otherwise.
 Complexity: The space consumption for incremental algo-factor is from the first term O( Q M i =1 N i ). However, standard OTA requires O( n Q M i =1 N i ) for storing all tensors up to time n , which is unbounded.
 The computation cost is P M i =1 R i N 2 i + P M i =1 N i + P M i =1 R  X  i N 2 i . Note that for a medium or low mode tensor (i.e., order M  X  5), the diagonalization is the main cost. Section 4.3 introduces a faster approximation of DTA that avoids diagonalization.

While for high order tensors (i.e., order M &gt; 5), the dom-inant cost becomes O( P M i =1 N i Q M j =1 N j ) from updating the variance matrix (line 4). Nevertheless, the improvement of DTA is still tremendous compared to OTA O( n Q M i =1 N i where n is the number of all tensors up to current time.
In this section, we present the streaming tensor analysis (STA), a fast algorithm to approximate DTA without diago-nalization. We first introduce the key component of tracking a projection matrix. Then a complete algorithm is presented for STA.
 Tracking a projection matrix : For most of time-critical applications, the diagonalization process (in Figure 7) fo r every new tensor can be expensive. Especially, when the change of the variance matrix is small, it is not worthy di-agonalizing that matrix. The idea is to continuously track the changes of projection matrices using the online PCA technique [25].

The main idea behind the tracking algorithm (see Figure 9) is to read in a new vector (one row of a tensor matrix) and perform three steps: 1. Compute the projection y , based on the current pro-2. Estimate the reconstruction error ( e ) and the energy, 3. Update the estimates of U .

Intuitively, the goal is to adaptively update U quickly based on the new tensor. The larger the error e , the more U is updated. However, the magnitude of this update should also take into account the past data currently  X  X aptured X  by U . For this reason, the update is inversely proportional to the current energy s ( i ).
 Input : projection matrix U  X  R n  X  r energy vector s  X  R n input vector x  X  R n Output : updated projection matrix U updated energy vector s 1. As each point x t +1 arrives, initialize  X  x := x t +1 2. For 1  X  i  X  r Streaming tensor analysis : The goal of STA is to adjust projection matrices smoothly as the new tensor comes in. Note that the tracking process has to be run on all modes of the new tensor. For a given mode, we first matricize the tensor X into a matrix X ( d ) (line 2 of Figure 10), then adjust the projection matrix U d by applying tracking a pro-jection matrix over the rows of X ( d ) . The full algorithm is in Figure 10. And the process is visualized in Figure 11.
To further reduce the time complexity, we can select only a subset of the vectors in X ( d ) . For example, we can sam-ple vectors with high norms, because those potentially give higher impact to the projection matrix.
 Complexity : The space complexity of STA is the same as DTA, which is only the size of the new tensor. The com-putational complexity is O(( P i R i ) Q i N i ) which is smaller than DTA (when R i  X  N i ). The STA can be further im-proved with random sampling technique, i.e., use only subse t of rows of X ( d ) for update.
 Input : Old projection matrices U i | M i =1  X  R N i  X  R i Output : New projection matrices U i | M i =1  X  R N i  X  R i Core tensor Y  X  R R 1  X  X  R M Algorithm : 1. For d = 1 to M // update every mode 3. For each column vector x in X T ( d ) 4. Track a projection matrix ( U d , diag( S i ), x ) 5. Calculate the core tensor Y = X
Figure 10: Algorithm: Streaming Tensor Analysis Figure 11: New tensor X is matricized along the d th
After presenting the core technique of tensor analysis, we now illustrate two practical applications of DTA or STA: 1) Anomaly detection , which tries to identify abnormal be-havior across different tensors as well as within a tensor; 2) Multi-way latent semantic indexing (LSI) , which finds the correlated dimensions in the same mode and across differ-ent modes.
We envision the abnormal detection as a multi-level screen-ing process, where we try to find the anomaly from the broadest level and gradually narrow down to the specifics. In particular, it can be considered as a three-level process for tensor stream: 1) given a sequence of tensors, identify the abnormal ones; 2) on those suspicious tensors, we locate the abnormal modes; 3) and then find the abnormal dimensions of the given mode. In the network monitoring example, the system first tries to determine when the anomaly occurs; then it tries to find why it occurs by looking at the traffic patterns from sources, destinations and ports, respective ly; finally, it narrows down the problem on specific hosts or ports.

For level one (tensor level), we model the abnormality of a tensor X by its reconstruction error: For level two (mode level), the reconstruction error of the l th mode only involves one projection matrix U l for a given tensor X : For level three (dimension level), the error of dimension d on the l th mode is just the reconstruction of the tensor slice of dimension d along the l th mode.

How much error is too much? We answer this question in the typical way: If the error at time T is enough (say, 2) standard deviations away from the mean error so far, we declare the tensor X T as abnormal. Formally, the condition is as follows:
The goal of the multi-way LSI is to find high correlated di-mensions within the same mode and across different modes, and monitor them over time. Consider the DBLP example, author-keyword over time, Figure 12 shows that initially (i n X ) there is only one group, DB, in which all authors and keywords are related to databases; later on (in X n ) two groups appear, namely, databases (DB) and data mining (DM).
 Correlation within a mode: A projection matrix gives the correlation information among dimensions for a given mode. More specifically, the dimensions of the l th mode can be grouped based on their values in the columns of U l The entries with high absolute values in a column of U l cor-respond to the important dimensions in the same  X  X oncept X .
In the DBLP example shown in Figure 12, U K corre-sponds to the keyword concepts. First and second columns are the DB and DM concepts, respectively. The circles of U
A and U K indicate the influential authors and keywords in DM concept, respectively. Similarly, the stars are for DB concept. An example of actual keywords and authors is in Table 3.
 Correlations across modes: The interesting aspect of DTA is that the core tensor Y provides indications on the correlations of different dimensions across different modes . More specifically, a large entry in Y means a high correla-tion between the corresponding columns in all modes. For example in Figure 12, the large values of Y i (in the shaded region) activate the corresponding concepts of the tensor X . For simplicity, we described a non-overlapping example, however, groups may overlap which actually happens often in real datasets.
 Correlations across time: And the core tensor Y i s also capture the temporal evolution of concepts. In particular, Y 1 only activates the DB concept; while Y n activates both DB and DM concepts.

Note that DTA monitors the projection matrices over time. In this case, the concept space captured by projec-tion matrix U i s are also changing over time.

In this Section, we evaluate the proposed methods on real datasets. Section 6.1 introduces the datasets. Section 6.2 studies the computation efficiency of different methods. name description dimension timestamps IP2D Network 2D 500-by-500 1200 IP3D Network 3D 500-by-500-by-100 1200 DBLP DBLP data 4584-by-3741 11 The Network Datasets: IP2D , IP3D
The traffic trace consists of TCP flow records collected at the backbone router of a class-B university network. Each record in the trace corresponds to a directional TCP flow between two hosts through a server port with timestamps indicating when the flow started and finished.

With this traffic trace, we study how the communication patterns between hosts and ports evolve over time, by read-ing traffic records from the trace, simulating network flows arriving in real time. We use a window size of an hour to construct a source-destination 2nd order tensors and sourc e-destination-port 3rd order tensor. For each 2nd order ten-sor, the modes correspond to source and destination IP ad-dresses, respectively, with the value of each entry ( i, j ) rep-resenting the total number of TCP flows (packets) sent from the i -th source to the j -th destination during an hour. Sim-ilarly, the modes for each 3rd order tensor corresponds to the source-IP address, the destination-IP address and the server port number, respectively.

Because the tensors are very sparse and the traffic flows are skewed towards a small number of dimensions on each mode, we select only N 1 = N 2 =500 sources and destinations and N 3 =100 port numbers with high traffic. Moreover, since the values are very skewed, we scaled them by taking the logarithm (and actually, log( x + 1), to account for x = 0), so that our tensor analysis is not dominated by a few very large entries. All figures are constructed over a time interv al of 1,200 timestamps(hours).
 DBLP Bibliographic Data Set:
Based on DBLP data [1], we generate author-keyword 2nd order tensors of KDD and VLDB conferences from year 1994 to 2004 (one tensor per year). The entry ( a, k ) in such a tensor is the number of papers that author a has published using keyword k during that year. In total, there are 4,584 authors and 3,741 keywords. Note that the keywords are generated from the paper title after proper stemming and stop-word removal.

All the experiments are performed on the same dedicated server with four 2.4GHz Xeon CPUs and 12GB memory. For each experiment, we repeat it 10 times, and report the mean. Computational cost: We first compare three different methods, namely, offline tensor analysis (OTA), dynamic tensor analysis (DTA), stre am-ing tensor analysis (STA), in terms of computation time for different datasets. Figure 14 shows the CPU time in loga-rithm as a function of elapse time. Since the new tensors keep coming, the cost of OTA increases linearly 6 ; while DTA and STA remains more or less constant. Note that DBLP in Figure 14(c) shows lightly increasing trend on DTA and STA because the tensors become denser over time (i.e., the num-ber of published paper per year are increasing over time), which affects the computation cost slightly.

We show that STA provides an efficient way to approxi-mate DTA over time, especially with sampling. More specif-ically, after matricizing, we sample the vectors with high norms to update the projection matrices. Figure 15 shows the CPU time vs. sampling rate, where STA runs much faster compared to DTA.
 Accuracy comparison: Now we evaluate the approximation accuracy of DTA and STA compared to OTA.
 Performance metric: Intuitively, the goal is to be able to compare how accurate each tensor decomposition is to the original tensors. Therefore, reconstruction error provid es a natural way to quantify the accuracy. Recall the reconstruc -tion error is defined in Definition 6. Error can always be re-duced when more eigenvectors are included (more columns in the projection matrices). Therefore, we fix the number of eigenvectors in the projection matrices for all three meth-ods such that the reconstruction error for OTA is 20%. And we use the error ratios between DTA/STA to OTA as the performance indices.
 Evaluation results: Overall the reconstruction error of DTA and STA are close to the expensive OTA method (see Figure 16(d)). Note that the cost for doing OTA is very expensive in both space and time complexity. That is why only a few timestamps are shown in Figure 16 since after that point OTA runs out of the memory.

In more details, Figure 16(a)-(c) plot the error ratios over time for three datasets. There we also plot the one that
We estimate CPU time by extrapolation because OTA runs out of the memory after a few timestamps.
 never updates the original projection matrices as a lower-bound baseline.

DTA performs very close to OTA, which suggests a cheap incremental methods over the expensive OTA. And an even cheaper method, STA, usually gives good approximation to DTA (see Figure 16(a) and (b) for IP2D and IP3D). But note that STA performs considerably worse in DBLP in Figure 16(c) because the adaptive subspace tracking tech-nique as STA cannot keep up to the big changes of DBLP tensors over consecutive timestamps. Therefore, STA is onl y recommended for the fast incoming with significant time-dependency (i.e., the changes over consecutive timestamps should not be too big).
After we evaluated the efficiency of the proposed methods, we now present two data mining applications using both our proposed methods, the dynamic and the streaming tensor analysis . The two applications are 1) anomaly detection for network traffic analysis and multi-way LSI on DBLP data.
Here we focus on the IP3D dataset, that is, source-destination-port tensors, and we use the algorithm described in Section 5 .1 to detect the following three types of anomalies: Abnormal source hosts : For example, port-scanners that send traffic to a large number of different hosts in the system. Abnormal destination hosts : For example, the victim of a distributed denial of service attack (DDoS), which receiv es high volume of traffic from a large number of source hosts. Abnormal ports : Ports that receive abnormal volume of traffic, and/or traffic from too many hosts.
 Experiment Setup: We randomly pick one tensor from normal periods with no known attacks. Due to the lack of detailed anomaly information, we manually inject anomalie s into the selected timestamps using the following method. For a given mode (i.e., source, destination or port) we ran-Table 4: Network anomaly detection: precision is very domly select a dimension and then set 50% of the corre-sponding slice to 1 simulating an attack in the network.
There are one additional input parameter: forgetting fac-tor  X  which is varied from .2 to 1. The result is obtained using DTA, and STA achieved very similar result so we ig-nore it for clarity.
 Performance Metrics: We use detection precision as our metric. We sort dimensions based on their reconstruction error, and extract the smallest number of top ranked hosts (say k hosts) that we need to select as suspicious hosts, in order to detect all injected abnormal host (i.e., recall = 10 0% with no false negatives). Precision thus equals 1 /k , and the false positive rate equals 1  X  precision. We inject only one abnormal host each time. And we repeat each experiment 100 times and take the mean.
 Results: Table 4 shows the precision vs. forgetting fac-tor for detecting three different anomalies. Although the precision remains high for both types of anomaly detection, we achieve a higher precision when forgetting factor is low meaning it forgets faster. This is because the attacks we in-jected take place immediately which do not depend on the past; in this case, fast forgetting is good. For more elabora te attacks, the forgetting factor may need to be set differently . Identify real anomaly: For network traffic, normal host communication patterns in a network should roughly be sim-ilar to each other over time. A sudden change of approxi-mation accuracy suggests structural changes of communica-tion patterns since the same approximation procedure can no longer keep track of the overall patterns. Figure 17(a) shows relative reconstruction error 7 over time using DTA. The anomaly points are the ones above the red line, which is the threshold based on 3 standard deviation above the mean error percentage. The overall accuracy remains high. But a few unusual error bursts occurs at hour 140 and 160 (cir-cle in Figure 17(a). We manually investigate into the trace further, and indeed find the onset of worm-like hierarchical scanning activities. Figure 17 (b) and (c) show the normal (green dash circle in (a)) and abnormal (red solid circle in (a)) communication patterns. The dot in Figure 17 means there are packet flows between the corresponding source and destination. The prominent difference between these two is that there are more scanning sources (more columns with many dots). We can successfully identify this real anomaly in the data using reconstruction error from DTA.
After showing the use of reconstruction error for anomaly detection, we now illustrate another important applicatio n of DTA/STA, i.e., multi-way LSI . The idea is presented in Section 5.2. Due to the lack of ground truth, we provide manual inspection on the clusters. And we now present some of the observations and examples.
 Clustering network data: We perform 3-way LSI on Net-work 3D data. The cluster is a tuple with three sets for source, destination and port, respectively. Several inter est-ing clusters are verified by domain experts. For example, the algorithm successfully groups web server ports into one cluster while mail server ports into another. And the desti-nation hosts in those clusters are confirmed to be web server and mail servers.
 Clustering DBLP data: Similarly, we perform 2-way LSI on DBLP datasets over time. Two example clusters are listed in Table 3. The algorithm correctly separate the two groups (data mining, databases) of people and concepts (2nd and 3rd clusters). And it identifies the focus change
Relative error is the reconstruction error by the input ten-sor norm. over time as well, e.g., 1st and 2nd groups are both about databases, but the focus has changed from object-oriented (1995) to stream (2004). Tensor and Multilinear Analysis: Tensor algebra and multilinear analysis have been applied successfully in man y domains [5, 17, 29]. Powerful tools have been proposed, in-cluding Tucker decomposition [27], parallel factor analys is [10] or canonical decomposition [4], and bilinear PCA. Ten-sors have been recently used in machine vision research, for example by Shashua and Levin [26] for linear image cod-ing, by Vasilescu and Terzopoulos [28] for face recognition . Ye [31] presented the generalized low rank approximation which extends PCA from the vectors (1st-order tensors) into matrices (2nd order tensors). Ding and Ye [7] proposed an approximation of [31]. Similar approach is also proposed in [12]. Xu et al. [30] formally presented the tensor represen-tation for PCA and applied it for face recognition. Drineas and Mahoney [8] showed how to approximate the tensor SVD using biased sampling.

These methods do one or more of the following assump-tions: the dataset is dense, or static. We are interested in sparse, streams of tensors, like IP traffic matrices over time . Stream and Graph Mining: Data streams have been ex-tensively studied in recent years. A recent surveys [23] hav e discussed many data streams algorithms. Among them, Pa-padimitriou et al. [25] proposes an online algorithm that summarizes the multiple streams incrementally. Our STA algorithm has a similar flavor but for general tensors instea d of just vectors.

A 2nd order tensor is a matrix and can be seen as a graph. Analysis and mining of static graphs has attracted a lot of interest, with many graph partitioning methods, in-cluding METIS [18], spectral partitioning [16], informati on-theoretic methods [6] and several variations. All these wor ks focus on static graphs (2nd order tensors), while our empha-sis is on time-evolving sequences of graphs and in general, sequences of tensors of potentially even higher order.
Numerous mining applications can be handled by matri-ces, the powerful SVD/PCA, and its variants and exten-sions. However, they all fall short when we want to study multiple modes, for example, time-evolving traffic matri-ces, time-evolving dataCubes, social networks with labele d edges, to name a few.

Next we show how to solve these higher order problems, by introducing the concept and vast machinery of tensors . Our contributions are the following:
We introduce tensors and specifically tensor stream to solve even more general streaming problems than those in the past. Our approach is applicable to all time-evolving se t-tings, including as co-evolving time series, data streams a nd sensor networks, time-evolving graphs (even labeled ones) , time-evolving social networks.

We propose two new tools, the dynamic and the streaming tensor analysis (DTA and STA) which incrementally mine and summarize large tensors, saving space and detecting patterns. DTA and STA are fast , nimble (since they avoid storing old tensors), and fully automatic , without requiring any user-defined parameters.

We provide experiments on two real, large datasets: net-work flow data and DBLP data. With respect to efficiency, our DTA and STA methods gives several orders of magni-tude speed-up over the offline tensor analysis, with a small loss of accuracy. With respect to effectiveness, we applied our methods to anomaly detection and  X  X ulti-way LSI X ; in both settings our tools found interesting and explainable patterns.
