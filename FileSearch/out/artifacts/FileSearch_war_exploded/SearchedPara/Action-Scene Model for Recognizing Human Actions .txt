 applications such as on-line video management, video retrieval, robotics, and surveil-lance. Although a large amount of impressive results have been achieved on datasets recorded in controlled environments, such as KTH [20], WEIZMEN [17], much less progress has been made on realistic videos. This is caused by clutter backgrounds, lots videos are usually collected from real-world sources, such as movies [12], TV series, wearable camera videos or websites. Figure 1 depicts some sample frames from these background settings. Furthermore, the human bodies are occluded or seen from varied gions for human action recognition. 
In real-world videos, actions usually happen under particular environments. The sets. Besides, the performance of detectors is prone to affect the recognition precision. between actions and scenes without scene detectors. We propose an Action-Scene Model to automatically model the relationship between actions, scenes and back-ground features. The approach only needs to set the number of scenes without know-each action class. For example, basketball shooting mostly happens in basketball recognize walking. To solve this problem, in this paper, we propose a factor to weight the contextual cues automatically when attempting to integrate the contextual cues with other methods. 
In detail, we first segment videos into person regions and background regions, and the features of the background regions are extracted. Then the contextual cues of ac-tions, scenes are modeled using a graphical model called Action-Scene Model. Final-ly, the proposed model gives the probability of action happening in videos according to the background features. The contextual cues can either infer actions from videos directly or integrate with existing methods as a complement. Context weights are computed when contextual cues are integrated with other methods. Figure 2 demon-strates the entire framework of our proposed method. 
The main contributions of our work are: 1) Modeling the contextual cues between actions and scenes with less prior knowledge about the scene. Most previous work the context information is added in recognition stage with an empirical weight, which ignores the distinction of action-scene dependency. 
The rest of this paper is organized as follows. Section 2 reviews related work. Sec-Model in Section 4 and explain the integration with other methods in Section 5. Sec-tion 6 presents experimental results, followed by conclusion in Section 7. work was focus on videos with fixed background [1,20]. However, this scenario is too approaches [9,12,13,16] tried to deal with  X  X ild X  videos. 
Laptev [12] addressed the problem of automatically annotating actions in movies to recognize actions. Features integrating additional non-local cues were used in [21]. instance learning framework. However, th ese efforts formulated object, scene and action as multiple feature channels without modeling the relationship between them. 
Modeling object and scene context has also been explored in recent work. Marsza-lek [15] exploited the context using movie scripts and developed a joint scene-action detectors. Han [9] proposed a MKGPC method to select and weight multiple features automatically. Jiang [11] modeled the action context from both labeled and unlabeled [16], which mined context from text corpus. 
Most of the previous work considering object and scene context [9,10,11,15,16] re-three limitations: 1) Hard to select a general category of scene and object suitable for all datasets. 2) A lack of training data for every object and scene. It is time-consuming text [16]) leads to a semantic gap with visual features. 
Unlike previous literatures, the proposed approach in this paper distinguishes from the detector-based method in learning context cues without providing scene types and static images [22] and unsupervised learning method in [17], we use a graphical mod-el to represent action and scene in videos. The action-scene method can capture the underlying action-scene context directly from visual features. Learning context between action and scene in videos requires information of scenes. For simplicity, we segment videos into person regions and non-person regions. The non-person regions are viewed as background regions, and these regions provide fea-tures of scenes. In this section, we will present the feature extraction from background regions (Section 3.2) and bag-of-feature video representation (Section 3.3). 3.1 Person Detection The person detection is implemented based on Felzenszwalb's object detector [7] and mean shift tracking [3]. We first use the person detector to find candidate person re-the PASCAL Visual Object Classes Challenge 2009 [6]. Since there are many false alarms, a sliding window based method is used to discard the false alarms following mean shift algorithm. 3.2 Background Region Features Extraction In order to capture the color, shape and local features of scene, we extract color histo-every 20 frames in each video. Background Color Feature. The background region is divided into 30 X 30 (pixels) blocks. Then the color histogram for each block is computed. The RGB color space is discretized into a 512-bins color histogram (3 channel and 8 bins per channel). Background Shape Feature. The dominant spatial structure is also helpful to original parameter settings in [18]. Background Static Features. Static interest point extracts discriminative points in an image, and it is a useful local feature in object recognition. We first apply 2D Harris detector to obtain interest points. Then 128-dimensional SIFT descriptor [14] for each point is computed. 3.3 Bag-of-Features Video Representation Bag-of-feature model based video representation has proved to be effective for action recognition [12,13,16,21]. The bag-of-feature model represents a video clip as a vec-tor over a  X  X isual codebook X . The  X  X isual codebook X  is constructed by clustering visual features detected from videos. The cen ter of a cluster is defined to be a visual signed to the unique visual word having the nearest distance. The video representation the number of features assigned to the i th visual word in the video. We constructed three codebooks for color, shape and static features respectively. Then these vectors are concatenated to form a visual codebook with 1200 visual words. Finally, a video clip can be represented with a 1200-dimensional vector based on the codebooks. scenes in videos using the Action-Scene Model. video clip is directly associated with actions, and each action has relation with several scenes. Each scene is associated with visual words in the visual codebook (introduced in Section 3.3). 
According to the relation between videos, actions, scenes and visual words, we na-them. The graphical model corresponding to the generative process of Action-Scene Model is shown in Figure 3. In the figure, nodes are random variables. Shaded node is conditional dependency between variables. Plates indicate repeated sampling with the number of repetitions given by the variable in the bottom. 
In the Action-Scene Model, each video is seen as a mixture of actions, and each ac-with a distribution over visual words. Our model is similar with LDA model [2] in the generating process. The LDA model views each video clip as a distribution of actions, and each action is a distribution of visual word [17]. The generating process does not model the relationship between actions and scenes. On the contrary, the Action-Scene associated with the action. 
Suppose we have a collection of M video clips, each video v is represented as a set of categories, S scene categories, we can describe the process generating each video v as: 2) For each visual word vi of the clip v : a) Choose an action category z vi according to distribution Multinomial (  X  v ). with  X  z ~ Dirichlet (  X  ). with  X  s ~ Dirichlet (  X  ) . 
Under this generative process, the probability of video corpus having visual word set w , conditioned on  X  ,  X  and  X  is: 
The Action-Scene Model includes the following unknown parameters: the action and the assignments of individual words to action z and scene s . In this paper, we use Gibbs sampling [19] to evaluate the posterior distribution on z and scene s . Then  X  ,  X  word, the equation needed for the Gibbs sampler is: the number of times the visual words from video v assigned to action x not including of times scenes are assigned to action z excluding current instance. estimate  X  ,  X  and  X  with: where C WS , C SZ and C ZV are counted from the assignment of s and z . 
Algorithm 1 describes the process of recognizing action for a new video. In the training stage, the Gibbs sampling is applied over all the visual words in the training are saved. When classifying a new video clip, we run the Gibbs sampling only on the matrices are updated after the assignment. Then the action distribution of this video  X  v is estimated based on the count matrix C ZV using Equation (3). The video clip is classified with a category index having the largest probability in  X  . The index is assigned with actual action class label using ground truth labels in the training dataset. Each index corresponds to the most popular action class label within videos belonging to this index. nent easily. We explain how to combine the results from Action-Scene Model with introduce a factor measuring the contributions of contextual cues in subsection 5.2. 5.1 Linear Combination with Other Components nent and Action-Scene Model respectively. The classification score integrating the contextual cues is defined as: where f c is a vector of context weight. The first component is any classifier providing the prediction score of assigning label c to v . The second one is the prediction score of assigning label c to the video using Action-Scene Model. 5.2 Action Oriented Context Since not every action class occurs in the special condition, it is necessary to leverage the importance of context to different action classes. Therefore, we compute a factor to estimate the strength of relationship between actions and scenes. Suppose variables Then the information about AC captured by SE can be measured by mutual informa-tion I ( AC ; SE ): definition of f c is: Larger f c means the action is more closely related with scenes. lected from YouTube website, which makes them  X  X ealistic X  and challenging. Besides, each action was recorded under several scenes. The dataset is suitable for studying the effects of scene to action recognition. Following the evaluation methodology of [10,13], we apply leave-one-out cross validation over the 25 subsets of YouTube data-set in the experiments. 11 action classes: basketball shooting, biking , diving, golf swing, horse riding, soccer juggling, swing, tennis swing, trampoline jumping, volleyball spiking, and walking vided into 25 subsets. 6.1 Parameter Setting number of actions K =11 for all models, we test the effect of S on recognition accuracy in the Action-Scene Model, as illustrated in Figure 4 (a). The actual number of scene categories in dataset is about 17. It shows that the recognition accuracy is the highest when S closes to the actual value. We also look for insights on the effect of  X  on rec-ognition accuracy in Figure 4 (b). The experiment achieves the best result when  X  =50/ K ,  X  = 0.3 and  X  = 0.01. 6.2 Performance Evaluation In order to validate whether the added variable in Action-Scene Model benefits action identification, we compare our Action-Scene Model with LDA model using same features, evaluation methodology and iteration times. In LDA model, the parameters is set as  X  =50/ K ,  X  = 0.01. 
Table 2 shows the recognition precision of different methods. The rows "backAS" and "backLDA" refer to the result from Action-Scene Model and LDA model respec-tively. It shows that the proposed method achieves an improvement over all the action categories. The average improvement in precision is 9.85%. The improvement is es-(17.73%), horse ride (18.19%) and trampoline jump (10.92%). Figure 5 is a more detail graph of improvement on each action category. It demonstrates that the contex-realistic videos. 
In Table 2, we also compare our results with other methods based on scene features on the YouTube dataset. The Gist and color features of scene are used as single fea-ture channels for recognition in [10], which corresponds to ''Gist" and "Color" in Ta-ble 2. The result of "backAS" achieves a higher average precision than both of them. Further, "o+s" is the results of combine object feature (e.g. basketball, bike, etc.) and scene feature together [10]. Although lower in average precision, the result of "back-AS" is comparable to "o+s" in several actio n classes (basketball, soccer juggle, swing and tennis swing), which demonstrates the effectiveness of background context mod-eling. Although "o+s" [10] achieved higher performance when object has explicit meaning for actions, it needs complex pr e-processing for object extraction. 6.3 Context Weighting Measurement We also test whether the contextual cues could be a complement to existing compo-nents. The combination of SVM classifier [19] and person region features is the most widely used component in action recognition. We integrate the learned contextual weight linear combination use the f c computed as the formula (6), and the weight are normalized into the range [1, 2]. 
The gain in average precision of these two strategies is given in Fig. 6. After using global linear combination, recognition accuracy of most action classes has been improved besides "bike". Furthermore, co ntext weight linear combination achieves captures the contribution of context. realistic videos. An Action-Scene Model is proposed to model and learn the relation-ship between actions and scenes without scene detectors. To automatically measure the dependency of action on scene, we compute a context weight based on the model. with the context weight. Experimental results validate that Action-Scene Model effec-tual cues indeed improve the recognition precision. 
