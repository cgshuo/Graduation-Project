 A number of relevant information retrieval classification problems are one-class classification problems at heart. I.e., labeled data is only available for one class, the so-called target class, and common discrimination-based classification approaches, be them binary or multiclass, are not applica-ble. Achieving a high effectiveness when solving one-class problems is difficult anyway and it becomes even more chal-lenging when the target class data is multimodal, which is often the case. To address these concerns we propose a cluster-based one-class ensemble that consists of four steps: (1) applying a clustering algorithm to the target class data, (2) training an individual one-class classifier for each of the identified clusters, (3) aggregating the decisions of the indi-vidual classifiers, and (4) selecting the best fitting clustering model. We evaluate our approach with four datasets: an ar-tificially generated dataset, a dataset compiled from a known multiclass text corpus, and two datasets related to one-class problems that received much attention recently, namely au-thorship verification and quality flaw prediction. Our ap-proach outperforms a one-class SVM on all four datasets. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval X  clustering, information filtering Keywords: One-class Classification, Ensemble, Clustering
In a one-class problem one is given information of the target class only. The task is to define a boundary that encloses as many target objects as possible while minimizing the chance of accepting objects from outside the target class, so-called outliers [8]. An example for a one-class problem is authorship verification [4], where we are given writing examples for a single author T , and we are asked whether a text of unknown authorship was written by T as well. Notice that, despite the fact that a sheer endless number of outliers are at our disposal, we are not able to define a closed outlier class with texts from other authors. Specialized one-class classifiers shall cope with this setting; however, there is no cure-all for one-class problems, and additional constraints may render the classification task even more challenging. Two of which are pretty common in information retrieval: a highly diverse target class, i.e., a target class with a complex, support vectors. The decision function c j is of the following form: Step 3: Aggregation. The ensemble classifier e k combines the decisions of the k single classifiers for a vector x as follows: Step 4: Model Selection. The clustering parameter k runs from 1 to l , and altogether l ( l +1) / 2 one-class SVMs are con-structed. We choose the classifier e k that performs best on holdout validation data in terms of the classification error.
We use a random subset D train  X  D T for the training phase; the test set D test is comprised of a balanced num-ber of outliers and examples from D T \ D train . Each exper-iment is repeated 15 times. The effectiveness of the classi-fier is reported as averaged F -measure and for varying val-ues of the cluster number k . In all experiments a one-class SVM with a non-linear RBF kernel is used. The parame-ters of classifier c j are optimized on the respective training set C j  X  D train ; clusters with less than five elements are discarded. Figure 1 illustrates the results on four different datasets: artificially created objects with three clusters (plot in Figure 2), documents from the 20 Newsgroups dataset with category  X  X omputer X  in the role of the target class, books from different authors for which the authorship is to be verified [4], and Wikipedia articles tagged with certain quality flaws that are to be detected [1]. All documents are represented under a vector space model with a tf-idf weight-ing except for Wikipedia articles where quality-specific fea-tures [1] are employed. On all four datasets our approach outperforms a one-class SVM that has been trained with all target objects, or equivalently, where |C| = 1. Table 1 summarizes the achieved improvements (significant) over the baseline. Note that our approach is a meta-classifier. Thus, arbitrary clustering and one-class classification technology can be applied. Both k-means and density estimation can be computed in parallel, so our approach is applicable on huge data as well.
 Table 1: Percentage of improvement over the base-line for each dataset and for the optimum cluster number k .

