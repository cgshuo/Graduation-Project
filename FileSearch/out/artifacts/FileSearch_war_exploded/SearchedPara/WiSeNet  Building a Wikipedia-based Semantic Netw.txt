 In this paper we present an approach for building a Wiki-pedia-based semantic network by integrating Open Infor-mation Extraction with Knowledge Acquisition techniques. Our algorithm extracts relation instances from Wikipedia page bodies and ontologizes them by, first, creating sets of synonymous relational phrases, called relation synsets , sec-ond, assigning semantic classes to the arguments of these relation synsets and, third, disambiguating the initial re-lation instances with relation synsets. As a result we ob-tain WiSeNet, a Wikipedia-based Semantic Network with Wikipedia pages as concepts and labeled, ontologized rela-tions between them.
 I.2.4 [ Artificial Intelligence ]: Knowledge Representation Formalisms and Methods  X  Semantic networks ; I.2.6 [ Artifi-cial Intelligence ]: Learning  X  Knowledge acquisition ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Experimentation Information Extraction, Knowledge Acquisition, Relation Ontologization, Semantic Network
In recent years we have witnessed an increasing popularity and availability of wide-coverage knowledge resources. Such resources, like Wikipedia and Wiktionary, are collabora-tively created by exploiting the so-called  X  X isdom of crowds X . As such, they provide a great wealth of semi-structured information in the form of hyperlinked Web pages, which has been shown to be reliable and up-to-date [7]. However, much of this knowledge is only implicitly available in textual form for human consumption and therefore cannot be im-mediately exploited by machines. This implicit knowledge can be automatically harvested and transformed in machine-readable format by means of automatic tasks such as Infor-mation Extraction and Knowledge Acquisition. These tasks aim at enabling one of the long standing goals of Artificial Intelligence, i.e., Machine Reading [16], which is the un-supervised understanding of knowledge extracted from un-structured text.

Information Extraction (IE) is concerned with harvesting relations between entities represented in textual form. Tra-ditional IE techniques focus on extracting relation instances using a fixed set of pre-defined relations [1, 13]. In order to extract relations without pre-defining them, a new IE paradigm, called Open Information Extraction (OIE), has been introduced [3, 18]. The state-of-the-art OIE system is ReVerb [4], which relies only on two simple constraints: i) the lexical aspect of the relational phrase is enforced by means of a manually-defined part-of-speech-based regular expression; ii) an informative relational phrase must appear with several different arguments. However OIE techniques do not provide a formal semantic representation for the ar-guments and the labels of the harvested relations, which can denote different meanings due to the ambiguous nature of text. As a consequence, redundant relation instances are often produced by OIE systems. For instance, ReVerb ex-tracts the following two synonymous relation instances: In order to reduce this kind of redundancy, it is possible to cluster synonymous relational phrases [8, 19] and then consider clusters as relations. However, assuming that a re-lational phrase can have only one meaning limits the number of distinct relational phrases associated with a relation [19]. An alternative solution is that of ontologizing semantic re-lations [14]. However, the use of WordNet [9] to perform this task makes the ontologization step difficult for many domains, because of the inherent lack of coverage of special-ized concepts and named entities.

Knowledge acquisition aims at building large knowledge bases containing semantic relations. Moreover it enables one to overcome the issues of manually-created knowledge bases, like WordNet, which need continuous human maintenance and have very few semantic relations. Recent automatic ap-proaches to building ontologies and semantic networks lever-age Wikipedia pages as the main source of semi-structured information from which concepts and relations can be ex-tracted [2, 6, 11, 12, 15, 17]. One of the most widespread ontologies is YAGO2 [6], which exploits a set of relation-specific heuristics to extract knowledge from Wikipedia, Geo-Names and WordNet. However this ontology considers only about 100 different semantic relations between its concepts. Nastase and Strube presented an automatically created con-cept network called WikiNet [11] that differs from YAGO2 in that it uses more general heuristics. This resource has around 500 different semantic relations, however, it heavily relies on Wikipedia categories and Infoboxes. As a conse-quence, WikiNet does not easily scale with the number of different relations. Finally, Babelnet [12], while providing wide coverage of lexicographic and encyclopedic senses, does not provide labels for the relations obtained from Wikipedia.
The approach presented in this paper aims at address-ing the above-mentioned issues in OIE, relation ontologiza-tion and knowledge acquisition by taking the best of each technique. Large-scale shallow relation extraction is coupled with a Wikipedia category-based semantic representation of the extracted ambiguous relations, which is used to create a full-fledged Wikipedia-based semantic network.
Our approach consists of two phases, i.e., relation extrac-tion and relation ontologization, which will be illustrated in the following two subsections.
The first phase of our approach consists of extracting re-lation instances from Wikipedia pages. We use a hyperlink-based heuristic to harvest relational phrases together with their arguments.

Definition 1. A relational phrase is a sequence of words that comprises at least one verb.

Definition 2. A relation instance is a triple ( p 1 , X ,p 2 where p 1 ,p 2 are Wikipedia pages and  X  is a relational phrase. For each Wikipedia page p we mark each occurrence of the title in the body of the page p as a link to p itself. Then, for each sentence s in p , we consider each pair of hyper-links to the respective Wikipedia pages p 1 and p 2 , and if the text between the two links satisfies the definition of re-lational phrase, we keep the corresponding relation instance ( p 1 , relational phrase, p 2 ) .

For example from the following excerpt of a Wikipedia page:  X  X [Natural Language Processing]] is a field of [[com-puter science]] X  we extract the following relation instance: (Natural Language Processing, is a field of, Computer sci-ence) .

As a result of this extraction phase on the entire Wikipedia dump, we obtain the set T := { ( p 1 , X  1 ,q 1 ) , ..., ( p of all the extracted relation instances. We further denote with P the set of all the relational phrases in T , P := {  X  :  X  ( p 1 , X ,p 2 )  X  T } .

In contrast to ReVerb our relation extraction step extracts relation instances between Wikipedia pages with a less re-strictive constraint on the relational phrase.
In the second phase we automatically provide explicit se-mantics for our relation instances, i.e. we ontologize them. Starting from the shallow semantic network obtained in the previous section (Figure 1a), we obtain WiSeNet, an ontol-ogized, Wikipedia-based Semantic Network (Figure 1b).
To cluster synonymous relational phrases in P we build vectors whose components count the occurrences of the most frequent words which occur to the left and to the right of a target relational phrase in a large corpus.

We denote with w l j (and w r j ) the j -th most occurring word on the left (right) of all the extracted relational phrases. Given a relational phrase  X  , following [10], we define the j -th component l j (  X  ) of our left vector ~ l (  X  ) as the conditional probability of w l j given  X  divided by the prior probability of w :
We define the right vector ~r (  X  ) in a similar way, using the respective frequencies w r j (the dimension of these vec-tors is established in the experimental setup, cf. Section 3). Finally we define a measure of similarity between two rela-tional phrases  X  and  X  0 by calculating the harmonic mean between the cosine similarity of these vectors: where H ( a,b ) = 2 ab a + b is the harmonic mean of a and b . Then for each relational phrase  X   X  P we use this similarity mea-sure to aggregate all the relational phrases that have a simi-larity with  X  greater than a given threshold  X  (see Algorithm 1 for details and Section 3 for the parameter settings). As a result, we obtain a set S of relation synsets for each  X   X  P .
Definition 3. A relation synset is a set of synonymous relational phrases.

For example for the relational phrase is a field of we ob-tain the following relation synset { is a field of, is an area of, is studied in } . Algorithm 1 Building relation synsets input: P , the set of relational phrases output: S , the set of relation synsets function RSS ( P )
Now that we have our relation synsets, we can introduce semantic classes to describe their arguments. We model se-mantic classes with Wikipedia categories, which have been shown to provide an adequate semantic representation for several domains [11, 15]. For instance, considering the fol-lowing relation synset { is a field of, is an area of, is studied in } we can use Subfields by academic discipline as one of the semantic classes for its left argument and Scientific Disci-plines as one of the semantic classes for its right argument.
We use multisets to carry out a depth-first-search explo-ration of the Wikipedia category hierarchy. Given a Wiki-pedia category, the algorithm recursively searches, up to a fixed depth  X  , the category hierarchy and counts the num-ber of times each category is visited. This counting is con-sidered as a relevance ranking of the super-categories of a given category. For instance, given the category Computa-tional Linguistics as input and  X  = 2, the algorithm outputs: { (Linguistics, 3), (Language, 2), ..., (Computing, 1) } . We named this algorithm WSC (for Wikipedia Super Categories).
We next define the categories of the left and right argu-ments of a relational phrase  X   X  P in the following way:
L  X  = { c :  X  p 1 ,p 2 , c  X  wikiCat ( p 1 )  X  ( p 1 , X ,p 2
R  X  = { c :  X  p 1 ,p 2 , c  X  wikiCat ( p 2 )  X  ( p 1 , X ,p 2 where wikiCat ( p ) denotes the set of categories of a Wiki-pedia page p and T is the full set of extracted relation in-stances (cf. Section 2.1).

Next we define Left  X  and Right  X  to represent the ex-tended semantics of the left and right arguments of  X  : Compared to the sets L  X  and R  X  , the extension consists of more varied (i.e. generalized) and consistent multisets of semantic classes for the relational phrase  X  .

In order to describe the left and right arguments of a rela-tion synset  X   X  S , we merge the extended category multisets of each relational phrase in the given relation synset  X  : As a result of this step we obtain ontologized relation synsets, i.e. synsets of relational phrases whose arguments are iden-tified by one or more Wikipedia categories. We show some examples of this ontologization step in Table 1.
At this point, on one hand we have a large set T of shal-low relation instances (cf. Section 2.1), on the other hand we have a wide range of ontologized relation synsets. Our final objective is to use the latter synsets to ontologize the former, possibly ambiguous, relation instances. To do this, for each extracted relation instance t = ( p 1 , X ,p 2 )  X  T , we disam-biguate  X  with the most suitable relation synset  X  , among those which contain  X  . As a result, we obtain a semantically labeled relation instance between two Wikipedia pages.
The algorithm takes as input a set of shallow relation in-stances T and the set of all the relation synsets S , and out-puts a set I of ontologized relation instances.

For each relation instance t = ( p 1 , X ,p 2 ) we define the set of candidate synsets S  X  = {  X   X  S :  X   X   X  } . For example, for: t = (Natural language processing, is a field of, Computer science) , the set S  X  contains the following relation synsets: { is a field of, is the battlefield of, was the site of the }} . The core of the algorithm is the computation of the intersec-tions between the argument categories of the relation synset candidates and the Wikipedia categories of pages p 1 ,p 2 normalize the cardinality of the intersections to obtain the most suitable relation synset among the candidates. The following function computes the score for each candidate  X  : where C p denotes the extended categories of a Wikipedia page p , C p := S c  X  wikiCat ( p ) WSC ( c ). We use the harmonic mean to guarantee a higher value for those synsets  X   X  S  X  that have a large intersection for both the left and right argument categories of  X  and the categories of p 1 , p 2 .
The relational phrase  X  is disambiguated by selecting the synset that maximizes the function q ( p 1 , X ,p 2 ) over  X   X  S . Following the above example, the relation instance t is disambiguated with the second synset, i.e. (Natural lan-guage processing, { is a field of, is an area of, is studied in } , Computer science) , because the semantic classes of the rela-tion synset and the considered Wikipedia pages share super-categories like Science and Subfields by academic disciplines , among others, that are not shared with the other candidates.
Recall that from the relation extraction phase (Section 2.1) we obtain a shallow semantic network with Wikipedia pages as nodes and relational phrases between them (Figure 1a). Now, thanks to our relation ontologization process, we can move to a full-fledged Wikipedia-based Semantic Net-work, that we call WiSeNet. In this network relations have well-defined semantics and edges are explicitly associated with the most suitable relation synset (Figure 1b).
We use the 2 July 2012 dump of Wikipedia for our relation extraction phase. To determine the optimal value of the maximum relational phrase length we created a tuning set of 400 sentences containing 783 hyperlinked pairs overall. A judge evaluated each of the extracted relation instances from these sentences and we set the maximum relational phrase length to the value that maximizes the F1-score, that is 16. We extracted 16 , 344 , 622 relation instances with 10 , 863 , 122 distinct relational phrases which we evaluate hereafter.
Following [4], we performed two manual evaluations re-garding the precision of the extracted relational phrases and relation instances. We first built a random sample of 2 , 000 distinct relational phrases. A judge was asked to label a relational phrase as correct if the phrase could be used in a sentence with a valid subject and object. For instance, is a scientific paper by was marked as correct while is a sci-entific paper was not. We obtained a precision of 79 . 8%. An error analysis has identified the following main classes of errors: i) phrases containing lists of objects; ii) phrases that do not represent a relation. We then built a random sample of 2 , 000 relation instances. The judge was asked to label a relation instance as correct if it makes sense as a sentence. As a result of this evaluation we calculated a precision of 82 . 8%. An error analysis has identified the following classes of errors (other than those found for relational phrases): i) hyperlinks labeling modifier words, instead of the syntactic head; ii) subject and object are ordered lists.

In order to study the ability of our relation extraction ap-proach at harvesting fresh relation instances, we calculated the degree of coverage and novelty against well-known ex-isting resources such as ReVerb, YAGO2 and WikiNet. To this end we used the following measures: Coverage ( A,B ) = | B | , Novelty ( A,B ) = | B | , where A is either our set of relational phrases P or the Wikipedia page pairs in T , the former for B as ReVerb and the latter for B as YAGO2 or WikiNet, as detailed hereafter.
We ran ReVerb 1 on our Wikipedia dump and we consid-ered only the relation instances output by ReVerb with a confidence score greater than 0 . 1, selected as a result of our tuning phase. Moreover, as ReVerb does not use Wikipedia pages as arguments of its relation instances, we restricted our comparison to relational phrases. Automatic inspection revealed that our set of relational phrases shares only 2 . 6% of its elements with ReVerb, while contributing 159 . 0% new relational phrases that ReVerb did not extract, obtaining a novelty score of 94 . 4%, as shown in Table 2.

As regards YAGO2 2 and WikiNet 3 , we compared only the arguments of the extracted relation instances, as the re-sources do not share relational phrases. Moreover we filtered out the instances that did not consider Wikipedia pages as their arguments (that might happen with WikiNet when it uses substrings of category names and with YAGO2 when it uses terms from WordNet or GeoNames). Finally we dis-carded all the self-loops and multiple edges. We cover 2 . 5% of the pairs extracted by YAGO2, but, on the other hand, we contribute 136 . 0% new pairs that YAGO2 did not ex-tract, obtaining a novelty score of 98 . 3%. As for WikiNet, we cover only 0 . 2% of the pairs, but we contribute 45 . 7% new pairs that WikiNet did not extract, obtaining a novelty score of 99 . 5% (see Table 2).

Our evaluation of the extracted relation instances shows that the nature of our relations is complementary to that of alternative resources in the literature.

In the second phase, we start from the set P of rela-tional phrases extracted in the first phase. For each rela-tional phrase  X   X  P , similarly to [10] we create two 2 , 000-dimensional vectors, one for the top 2 , 000 words occurring to the left and another one for the words occurring to the right of  X  in a 5-word window (see Section 2.2.1). We es-timated such frequencies from a large corpus, that is, Gi-gaword [5]. We found context words for 314 , 210 of our re-lational phrases. To set up the threshold  X  = 0 . 64, used to build relation synsets (see Algorithm 1), we built a tun-ing set by manually selecting 100 held-out relational phrases and aggregating them in relation synsets. We then chose the http://reverb.cs.washington.edu/reverb-latest.jar http://www.mpi-inf.mpg.de/yago-naga/yago/download/ yago2/yago2core 20120109.7z http://www.h-its.org/downloads/nlp/wikinet.tar.gz threshold value that maximizes the number of correctly clas-sified relational phrase pairs.

To evaluate the precision of the 39 , 577 relation synsets that we automatically built, one judge manually evaluated a random sample of 2 , 000 pairs of relational phrases occurring in a same relation synset. An element was marked as correct if the two relational phrases can be used to suitably represent the same semantic relation. We calculated a precision of 82 . 1%. An error analysis has identified the following main classes of errors (other than error classes found for relational phrases/instances): (i) relational phrases that negate each other (ii) relational phrases that share the same arguments but are not synonyms.

In this section we evaluate the semantic classes associ-ated with the arguments of our relation synsets. One judge evaluated a random sample of 2 , 000 instances composed of three parts: a random relational phrase of a random rela-tion synset and the top-5 semantic classes extracted for its left and right arguments. A correct assignment of semantic classes to the relation synset arguments was marked as cor-rect if the classes correctly describe a subset of the concepts that the relational phrase can assume on its left and right. We obtained a precision of 68 . 7%. Note that this is a par-ticularly difficult task, as it involves the quality of both the relational phrases (used for building relation synsets) and relation instances (used to assign the semantic classes to the relation synsets). We show some of the evaluated instances in Table 1.

Finally we evaluated our disambiguation procedure of our relation instances, as described in Section 2.2.3. A judge evaluated a random sample of 2 , 000 disambiguated rela-tion instances ( p 1 , X ,p 2 ), where, instead of the whole rela-tion synset  X  , a randomly-chosen  X   X   X  was presented to the annotator such that ( p 1 , X ,p 2 ) 6 X  T (this can happen as the relational phrases in  X  can relate different pairs of Wikipedia pages in T ). We required the judge to mark an element as correct if p 1  X  p 2 makes sense as a sentence. In this way we calculated a precision of 76 . 7%. This evaluation indi-cates that, despite the difficulty of the disambiguation task and the degree of ambiguity of relational phrases (almost 5), the initial precision of our relational phrases, i.e. 82 . 8%, decreases by around 6% when moving from shallow to on-tologized relation instances. Moreover this is an evaluation of the whole system as it takes into account all the previ-ous steps and assesses novel relation instances that were not extracted from the first step.
We presented an automatic approach to the construction of a full-fledged semantic network by combining Open Infor-mation Extraction with knowledge acquisition techniques. Our algorithm extracts relation instances from Wikipedia pages and ontologizes them by, first, creating relation syn-sets, second, assigning semantic classes to the arguments of these synsets and, third, disambiguating the initial relation instances with the most suitable relation synsets.

To our knowledge, this is the first time that large-scale information extraction and relation ontologization are in-tegrated to produce a full-fledged semantic network with Wikipedia pages as concepts and labeled, ontologized rela-tions between them. Our evaluation shows that our resource, WiSeNet, is complementary in nature and content with ex-isting wide-coverage resources like YAGO2 and WikiNet.
As future work, we aim at exploiting the syntactic struc-ture of sentences to further improve the precision of our ap-proach. The shallow semantic network, as well as WiSeNet, are available at http://lcl.uniroma1.it/wisenet.
 The authors gratefully acknowledge the support of the ERC Starting Grant MultiJEDI No. 259234.

