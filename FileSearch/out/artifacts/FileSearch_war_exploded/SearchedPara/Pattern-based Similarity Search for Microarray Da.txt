 One fundamental task in near-neighbor search as well as other sim-ilarity matching efforts is to find a distance function that can effi-ciently quantify the similarity between two objects in a meaningful way. In DNA microarray analysis, the expression levels of two closely related genes may rise and fall synchronously in response to a set of experimental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very similar. Unfortunately, none of the conventional distance met-rics such as the L p norm can model this similarity effectively. In this paper, we study the near-neighbor search problem based on this new type of similarity. We propose to measure the distance between two genes by subspace pattern similarity, i.e., whether they exhibit a synchronous pattern of rise and fall on a subset of dimensions. We then present an efficient algorithm for subspace near-neighbor search based on pattern similarity distance, and we perform tests on various data sets to show its effectiveness.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing ; I.5.2 [ Pattern Recognition ]: Design Methodology X  Pattern analysis pattern recognition, near neighbor, distance function Algorithms
Given a distance function dist (  X  ,  X  ) that measures the similarity between two objects, a query object q  X  X  near-neighbors within a given tolerance radius r in a database D is defined as: Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00.
 The distance function dist (  X  ,  X  ) has a direct impact on the efficiency of the search of near-neighbors [3]. More importantly, it also de-termines the meaning of similarity and the meaning of the near-neighbor search. Applications. In this paper, we address a new type of similarity that cannot be effectively captured by conventional distance metric such as the L p norm. As a motivating example, in Figure 1, we show the expression levels of two Yeast genes under 17 different external conditions. It is clear that the two genes manifest similarity under a subset of conditions (linked in thick lines).

In many scientific experiments, we measure objects in differ-ent environments. Figure 2(a) is a dataset that contain the mea-surements of 3 objects in 8 different environments. Now, given a new object X whose measurements are shown in Figure 2(b), we (d) show potential near-neighbors of object X . In Figure 2(c), the values of object X and A rise and fall coherently under conditions { a, b, d, e, g } . Figure 2(d) reveals, in much the same way, the sim-ilarity of X and C under { a, b, c, e, h } .

Finding near neighbors based on subspace pattern similarity is important to many applications including DNA microarray analy-sis [1, 8, 7]. A DNA microarray is a two dimensional matrix where entry d ij represents the expression level of gene i in sample vestigations show that more often than not, several genes contribute to a disease, which motivates researchers to identify genes whose expression levels rise and fall synchronously under a subset of con-ditions, that is, whether they exhibit fluctuation of a similar shape when conditions change.
 Problems. Assume we are given a new gene for which we do not know in which conditions it might manifest coherent patterns with other genes. This new gene might be related to any gene in the database as long as both of them exhibit a pattern in some sub-space. The dimensionality of the subspace is often an indicator of the degree of their closeness, that is, the more columns the pattern spans, the closer the relationship between the two genes.
Given a gene q , and a dimensionality threshold r , find all genes whose expression levels manifest coherent patterns with those of in any subspace S , where | S | X  r .
 Our Contributions. We introduce a new measure to capture pattern-based similarity exhibited by objects in subspaces. With the new distance measure, we extend the concept of near-neighbor to the realm of pattern-based similarity, which often carries signif-icant meanings. We also propose a novel method to perform near-neighbor search by pattern similarity. Experiments show that our method is effective and efficient, and it outperforms alternative al-gorithms (based on an adaptation of the R-Tree index) by an order of magnitude.
Let u , v be two objects in dataset D . How can we measure their pattern-based similarity in a given subspace, say S = { a, b, c, d, e } A straightforward way is to normalize both objects in subspace S (Figure 3) by shifting u and v by an amount of u s and v s spectively, where u s ( v s ) is the average coordinate value of in subspace S . After normalization, we can check whether exhibit a pattern of good quality in subspace S : exhibit a coherent pattern in subspace S  X  X  if where u s = 1 nate values of u and v in subspace S , and  X   X  0 .

The above definition, although intuitive, may not be applicable or effective for near-neighbor search in arbitrary subspaces. Near-neighbor search queries often rely on index structures to speed up the search process. The definition of the coherent pattern (Eq 2) uses not only coordinate values (i.e., u i , v i ) but also average coor-to index average values for each of the 2 |A| subsets.

To avoid the curse of dimensionality, we relax Definition 1 by eliminating the need of computing average values in Eq 2. Instead, we use the coordinate values of any column k  X  S as the base for comparison. Given a subspace S and any column k  X  S , we define:
However, the choice of column k may be questionable: does an arbitrary k affect our ability in capturing pattern similarity? The following property relieves this concern.

T HEOREM 1. If there exists k  X  S such that d k, S ( u, v )  X   X  then we have:
P ROOF . (sketch) Note:
Not only the difference among base columns is limited, Theo-rem 1 also shows that, the difference between using Eq 2 and Eq 3 is bounded by a factor of 2 in terms of the pattern quality. In the same light, we can show that if u and v exhibit an coherent pattern in subspace S , then  X  k  X  S , we have d k, S ( u, v )  X  2  X  order to find all coherent pattern, we can use d k, S ( u, v )  X  2  X  criteria and then prune the results, since Eq 3 is much less costly to compute.

In order to find patterns defined by a consistent measure, we fix the base column k for any subspace S  X  A . We assume there is a total order among the dimensions in A . Given a subspace S its least dimension in terms of the total order as the base column. Now we arrive at the definition of  X  -pattern that induces an efficient implementation.

D EFINITION 2 (  X  -PATTERN ). Objects u , v  X  D exhibit an pattern in subspace S  X  X  if where k is the least dimension in S and  X   X  0 .
The  X  -pattern definition focuses on pattern similarity in a given subspace. How to measure similarity between two objects when no subspace is specified? Usually, we do not care over which subspace two objects exhibit a similar pattern, but rather, how many dimen-sions the pattern spans. Thus, the dimensionality of the subspace can be used as an indicator of the degree of the similarity. Given two objects u , v  X  D and some  X   X  0 , we say that the similarity between u and v is r , or if r is the maximum dimensionality of all subspaces S  X  A where u and v exhibit an  X  -pattern.
 Thus, two objects that exhibit an  X  -pattern in the entire space will have the largest similarity of |A| . The distance between the two objects is reversely proportional to their similarity. For instance, we can define dist ( u, v ) = 1 /s ( u, v ) . Note that the distance de-fined above is non-metric in that it does not satisfy the triangular inequality. One object can share  X  -patterns with two other objects in different subspaces, and the sum of the distances to the two ob-jects might be smaller than the distance between the two objects, which may not share synchronous patterns in any subspace. Problem Statement. We define the following problem of near neighbor search. Given an object q , a tolerance radius NN ( q, r ) in dataset D :
In this section, we propose a new index structure called PS-Index (pattern similarity index) to support near-neighbor search in pattern distance space.
Given a dataset D in space A = { c 1 , c 2 , ..., c n } , where c 2  X   X  X  X   X  c n is a total order on attributes, we represent each object u  X  X  as a sequence of (column, value) pairs, that is:
An aligned suffix of u is defined as:
We use an example to demonstrate the data sequentializing pro-cess.

E XAMPLE 2. Let database D be composed of the following ob-ject defined in space A = { c 1 , c 2 , c 3 , c 4 , c 5 }
We derive all aligned suffices of length  X  2 of the object, and insert them into a trie. Figure 4 demonstrates the insertion of the following sequence: Each leaf node n in the trie maintains an object list , L ing the insertion of f (#1 , 1) leads to node x , which is under arc ( e,  X  3) , we append 1 (object #1), to object list L x .
The trie enables us to find near-neighbors of a query object ( c , v 1 ) , ..., ( c n , v n ) in a given subspace S , provided by a set of consecutive columns, i.e., S = { c i , c i +1 The PS-index, described below, allows us to  X  X ump X  directly from a column c j to any column c k , where k &gt; j .

We use the following two steps to build the PS-index on top of a trie. First, after all sequences are inserted, we assign to each node x a pair of labels,  X  n x , s x  X  , where n x is the prefix-order of node in the trie (starting from 0, which is assigned to the root node), and s is the number of x  X  X  descendent nodes.

Next, we store nodes into buffers. For each unique edge ( col, dist ) during a depth-first walk of the trie. When we encounter a node under edge ( col, dist ) , we append x  X  X  label  X  n x , s of ( col, dist ) . From the definition of base-column aligned suffixes, it is clear that a buffer is composed of nodes that have the same distance from their base columns (root node).

The labeling scheme and the node buffers have the following property.
 1. If node x and y are labeled  X  n x , s x  X  and  X  n y , s 2. nodes in any buffer are ordered by their prefix-order number; 3. if a buffer contains nodes u . . . v . . . w (in that order), and
P ROOF . 1) and 2) are due to the labeling scheme which is based on depth-first traversal. For 3), note that if nodes u, ..., v, ..., w in a buffer (in that order), and u , v are descendents of n x &lt; n u &lt; n v &lt; n w  X  n x + s x , which means v descendent of x .

The above properties enable us to use range queries to find de-scendents of a given node in a given buffer.

Algorithm 1 summarizes the index construction procedure. The time complexity of building the PS-index is O ( |D||A| ) . The Ukko-nen algorithm [6] builds suffix tree in linear time. The construction of the trie for pattern-similarity indexing is less time consuming be-cause the length of the indexed subsequences is constrained by Thus, it can be constructed by a brute-force algorithm [4] in linear time. The space taken by the PS-Index is linearly proportional to 1 For each column col , there are at most 2  X   X  1 unique edges, where  X  is the number of unique values of that column. Input : D : objects in multi-dimensional space A
Output : PS-Index of D for each u  X  X  do for each node x encountered in a depth-first traversal of the trie do the data size. Since each node appears once and only once in the buffer, the total number of entries equals the total number of nodes in the trie, or O ( |D||A| 2 ) in the worst case (if none of the nodes are shared by any subsequences). On the other hand, there are ex-actly |D| ( |A| X  1) object ids stored. Thus, the space is linearly proportional to the data size |D| .
In this section, we provide an efficient solution to the 2nd prob-lem defined in Section 2.
 The Coverage Property. Each node x in the trie represents a coverage, which we denote as a range c ( x ) = [ n x , n x ing x is labeled  X  n x , s x  X  ). Finding near-neighbors with similarity  X  r boils down to finding leaf nodes whose preorder number is inside at least r ranges associated with the query object.
Let q be a query object, and p  X  D be a near-neighbor of (with similarity above threshold r , or s ( p, q )  X  r ). Hence, there exists a subspace S , | S | = r , in which p and q share a pattern. contains a set of nodes. Let P ( q, i ) denote the set of all nodes that appear in the buffers of the elements in f ( q, i ) , and let
S
T HEOREM 3. (The Coverage Property) For any object p that shares a pattern with query object q in subspace S , there exists a set of | S | nodes { x 1 , ..., x | S | }  X  P ( q ) , and a leaf node contains p ( p  X  L y ), such that n y  X  c ( x 1 )  X   X  X  X   X  c ( x where n y is the prefix-order of node y .

P ROOF . (Sketch) Let c i be the first column of S (that is, there does not exist any c j  X  S such that j &lt; i ). Assume the insertion of f ( p, i ) follows the path consisting of nodes x i , x i +1 which leads to c ( x |A| )  X   X  X  X   X  c ( x i +1 )  X  c ( x i x is in the list of ( c j , p j  X  p i ) . Since p and q share pattern in ( c , p j  X  p i ) = ( c j , p j  X  q i ) holds for at least | S | which means | S | of the nodes in x i , x i +1 ,  X  X  X  , x |A| P ( q, i )  X  P ( q ) .

The proof also shows that, to find objects that share patterns with q in subspace S, of which c i is the first column, we only need to consider ranges of the objects in P ( q, i ) , instead of in the entire object set P ( q ) .

The reverse of the coverage property is also true, and can be proved under the same spirit: for any { x 1 ,  X  X  X  , x n } X  P ( q ) fying c ( x 1 )  X   X  X  X   X  c ( x n ) , any object  X  L x of q with similarity r  X  n .
 The Algorithm. Based on the coverage property, to find NN ( q, r ) we need to find those leaf nodes whose preorder number is inside at least r nested ranges. We perform near-neighbor search iteratively. At the i th step, we find objects that share patterns with space S , of which c i is the first column. During that step, we only need to consider ranges of objects in P ( q, i ) .
 We demonstrate the search process with an example.

E XAMPLE 3. Given a query object, find NN ( q, 3) in D (Table 1).
 By definition,  X  p  X  NN ( q, 3) , p and q must share a pattern in 3-or higher-dimensional space.
In Figure 5, we show a labeled trie built on D , and the object lists associated with each leaf node of the trie. For presentation simplic-ity, we did not include suffixes of length less than 3 in Figure 5. It does not affect the result of Example 3, which looks for patterns in 3-or higher-dimensional space.

We start with f ( q, 1) , that is, we look for patterns in subspaces that contain column a (the 1st column of A ). For each element in f ( q, 1) , we consult the corresponding buffer and record the labels of the nodes in the buffer. For instance, finds one node, which is labeled  X  1 , 9  X  . We record it in Figure 6. For the remaining elements of f ( q, 1) , our search is confined within that range, since we are looking for subspaces where column present. We consult the buffers of elements in f ( q, 1) After we consult ( b, 0) , ( c, 1) , and ( d,  X  1) and record the results, we find region [4 , 6] inside three brackets (Figure 6). It means ob-jects in the leaf nodes whose prefix-order are in range [4 , 6] match the query object in a 3-dimension space. To find what those objects are, we perform a range query [4 , 6] in the object list table shown in Figure 5, which returns object 1 and 2, and they belong to leaf node 5 and 6 respectively. The two objects share a pattern with q in 3-dimension space { a, c, d } . We repeat this process for f ( q, 2) , and so on. Optimization. In essence, the searching process maintains a set of embedded ranges represented by brackets (Figure 6), and the goal is to find regions within r brackets. The performance of the search can be greatly improved by immediately dropping those re-gions from further consideration if i) all nodes inside the region already satisfy the query, or ii) no node inside the region can possi-bly satisfy the query. More specifically, 1. A region inside less than r  X  X A| + i brackets after the 2. If a region is already inside r brackets, we output the objects
For instance, in Figure 6, after the range of [4 , 6] is returned, only region [3 , 4] shall remain before ( e, 2) is checked.

Input : q = ( c 1 , v 1 ) ,  X  X  X  , ( c n , v n ) : a query object
Output : NN ( q, r ) for i = 1 , ..., r + 1 do end
We tested PS-Index with both synthetic and real life data setson a Linux machine with a 700 MHz CPU and 256 MB main memory. The yeast micro-array is a 2 , 884  X  17 matrix (2,884 genes under 17 conditions) [5]. The mouse cDNA array is a 10 , 934  X  49 (10,934 genes under 49 conditions) [2] and it is pre-processed in the same way. We also generate synthetic data, which are random integers from a uniform distribution in the range of 1 to |D| be the number of objects in the dataset and |A| the number of dimensions. The total data size is 4 |D||A| bytes. neighbor search over the yeast microarray data, where genes X  ex-pression levels (of range 0 to 600 [1]) have been discretized into  X  = 30 bins. Assume we are interested in genes related to gene YAL046C. Let  X  = 20 (or 1 after discretization). We found one gene, YGL106W, within pattern distance 3 of gene YAL046C, i.e., YAL046C and YGL106W exhibit an  X  -pattern in a subspace of di-mensionality 14. This is illustrated by Figure 7(a), where except under conditions 1, 3, and 9 ( CH1B , CH2I , and RAT2 ), the expres-sion levels of the two genes rise and fall in sync.

Figure 7(b) shows 11 near-neighbors of YAL046C found with distance radius of 4. That is, except for 4 columns, each of the 11 genes shares an  X  -pattern with YAL046C. It turns out that none of any two genes share  X  -patterns with YAL046C in the same sub-space. Naturally, these genes do not show up together in any sub-space cluster discovered by algorithms such as bicluster [1]. Thus, subspace near-neighbor search may provide insights to understand-ing their interrelationship overlooked by previous techniques. Space Analysis. The space requirement of the pattern-similarity index is linearly proportional to the data size. In Figure 8(a), we fix the dimensionality of the data at 20 and change  X  , the discretization granularity, from 5 to 80. It shows that  X  has little impact on the index size when the data size is small. When the data size increases, the growth of the trie slows down as each trie node is shared by more objects (this is more obvious for smaller  X  in Figure 8(a)).
In Figure 8(b) and 8(c), the discretization granularity  X  at 20, while the dimensionality of the dataset varies. The dimen-sionality affects the index size. With a dataset of dimensionality |A| , the lowest similarity between two objects is 2, i.e., they do not share patterns in any subspace of dimensionality 2 or larger. How-ever, given a query object q , our interest is in finding near-neighbors of q , that is, finding NN ( q, r ) where the similarity threshold high. Thus, instead of inserting each suffix of an object sequence into the trie, we insert only those suffixes of length larger than a threshold t . This enables us to find NN ( q, r ) where r  X  t a 40 Mbytes dataset of dimensionality |A| = 80, restricting near-neighbor search within r  X  72 reduces the index size by 71%. Time Analysis. We compare our algorithm with two alternative approaches, i) brute force linear scan, and ii) R-Tree family indices. The linear scan approach for near-neighbor search is straightfor-ward to implement. The R-Tree, however, indexes values not pat-terns. To support queries based on pattern similarity, we create an extra dimension c ij = c i  X  c j for every two dimensions Still, R-Tree index supports only queries in given subspaces and does not support finding near-neighbors that manifest patterns in any subspace of dimensionality above a given threshold.

The query time presented in Figure 9(a) indicates that PS-Index scales much better than the two alternative approaches for pattern matching in given subspaces. The comparisons are carried out on synthetic datasets of dimensionality |A| = 40 and discretization level  X  = 20 . Each time, a subspace is designated by randomly selecting 4 dimensions, and random query objects are generated in the subspace. We find that the R-Tree approach is slower than brute force linear-scan for two reasons: i) the R-Tree approach de-grades to linear-scan under high-dimensionality, and ii) the fact that it indexes on a much larger dataset (with |A| 2 / 2 extra dimensions) means that it scans a much larger index file. In Figure 9(b), we show the results of near-neighbor search with different tolerance sponse time of PS-Index increases rapidly when the radius expands, as a lot more branches have to be traversed in order to find all ob-jects satisfying the criteria. Figure 9(c) also confirms that dimen-sionality is a major concern in query performance.
We identify the need of finding near-neighbors under subspace pattern similarity, a new type of similarity not captured by Eu-cations, including DNA microarray analysis and e-commerce tar-pattern in a subspace of dimensionality beyond a given threshold is at least O ( n log( n )) , where n = |A| . get marketing. Two objects are similar if they manifest a coherent pattern of rise and fall in an arbitrary subspace, and their degree of similarity is measured by the dimensionality of the subspace. A non-metric distance function is defined to model near-neighbor search in subspaces. We propose PS-Index, which maps objects to sequences and index them using a tree structure. Experimental results show that PS-Index achieves orders of magnitude speedup over alternative algorithms based on naive indexing and linear scan. [1] Y. Cheng and G. Church. Biclustering of expression data. In [2] R. Miki et al. Delineating developmental and metabolic [3] Piotr Indyk. On approximate nearest neighbors in [4] E. M. McCreight. A space-economical suffix tree construction [5] S. Tavazoie, J. Hughes, M. Campbell, R. Cho, and G. Church. [6] E. Ukkonen. Constructing suffix-trees on-line in linear time. [7] Haixun Wang, Chang-Shing Perng, Wei Fan, Sanghyun Park, [8] Haixun Wang, Wei Wang, Jiong Yang, and Philip S. Yu.
