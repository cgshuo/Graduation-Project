 I.2.m [ Computing Methodologies ]: Artificial Intelligence Algorithms, Experimentation, Performance Justification, OWL, TMS, MapReduce
The Web Ontology Language (OWL) is a standardized language for the Semantic Web. With the development of the Semantic We-b, more and more OWL ontologies are published on-line, and are linked to each other in the Linked Open Data (LOD) project these ontologies evolve or new links are created, it is important to ensure that logical inconsistency does not occur in the updated on-tologies and in the linked ontologies. Finding all justifications of an entailment is an important reasoning service to help the users or domain experts to resolve logical inconsistencies when they oc-cur. More generally, finding justifications can be useful to explain unwanted entailments of an OWL ontology.

In recent years, many approaches have been proposed for find-ing all justifications of an OWL entailment. These approaches can be roughly classified into two classes: one consists of approach-es that are based on modification of a description logic reasoner, and the other consists of approaches that take a description logic reasoner as oracle to check satisfiability of an ontology. Existing approaches cannot handle large ontologies, for example, ontolo-gies with more than one million triples. To improve the scalability of these approaches, some optimization techniques are developed by extracting a small module of an ontology that covers all justi-fications of an entailment. However, there is no evidence showing that the approaches optimized by module extraction techniques can scale to billions of triples. To provide explanation services for rea-soning with large scale semantic data sets, like those in LOD, some novel optimization techniques are expected.

In this paper, we focus on a fragment of OWL, called OWL pD fragment [15]. The pD  X  semantics is different from other OWL semantics based on description logics in that it provides a complete set of entailment rules represented as if-then rules. This semantics is important as large amounts of Web data can be expressed with OWL pD  X  . Moreover, pD  X  is the basis of OWL 2 RL [11], but it is much more concise and hence convenient for explaining our ideas. Scalable OWL reasoning is possible for OWL pD  X  fragment as it has been implemented in MapReduce framework [16]. Therefore, it is one of the most important fragments of OWL that can be used to express semantic data. The rule-based feature of pD  X  semantics inspires us to consider using Truth Maintenance Systems (TMS) [3] to find all justifications of an OWL entailment in pD  X  semantics. TMS, also known as Reason Maintenance Systems, has been used to reason about and adapt to changes in a knowledge bases [13]. It has the ability of dealing with large search spaces efficiently [13].
In order to break the limitation of I/O and main memory, dis-tributed computing techniques are widely exploited in data inten-http://esw.w3.org/SweoIG/TaskForces/ CommunityProjects/LinkingOpenData sive computing tasks involving Semantic Web applications. Al-though MapReduce and other distributed or parallel computing tech-niques have already emerged in the implementation of several in-ference engines, there is no work exploiting them to find all jus-tifications of OWL entailments. Our second contribution is to use MapReduce technology to improve the scalability of our algorithm for finding all justifications using TMS. Our approach finds justifi-cations in a MapReduce manner on a simplified Justification-based TMS (JTMS) which interacts with the first and only MapReduce-based OWL pD  X  inference engine, WebPIE [16]. The experimental results on synthetic data and real world data show that our approach has the ability to scale to more than one billion triples.
An RDF graph is defined as a subset of the set UB  X  UB  X  UBL where U , B and L represent the set of URI references , Blank nodes , and Literals respectively. Furthermore, L is composed of the set of base element of an RDF graph is the triple which contains three components, i.e. the subject, the predicate, and the object, usually in the form of ( s, p, o ) with s  X  UB , p  X  UB ,and o  X  UBL .Given an RDF graph G , T ( G ) is the set of elements in UBL that occur bl ( G )= T ( G )  X  B . The vocabulary of G is denoted as V ( T ( G )  X  UL .If bl ( G )=  X  ,wesay G is ground .

In [15], a subset of Ontology Web Language (OWL) vocabulary (e.g., owl:sameAs ) is introduced, which extends the RDFS seman-tics and includes a subset of OWL vocabulary to so-call pD mantics. An ontology in the OWL pD  X  fragment is a generalized RDF graph , which extends an RDF graph by allowing blank nodes in the predicate position and using a subset of OWL vocabulary. It guarantees a sound and complete entailment rule set. The set of rules, shown in Table 1, is introduced to define partial and full pD closures. The partial (or full) pD  X  closure of a generalized RDF graph is computed by iteratively applying the rules on input triples until no new data derived, i.e., a fixpoint is reached [15, 16]. The entailment relation | = between two generalized RDF graphs can then be equivalently defined by the partial pD  X  closure.
A simple interpretation I over a vocabulary V is a tuple I R 1. R I is a non-empty set, called the set of resources or the uni-2. P I is the set of properties, which is not necessarily disjoint 3. LV I  X  R I is the set of literal values, which contains at least 4. E I maps every property p  X  P I to a subset of R I  X  R 5. S I maps each t  X  V  X  U into R I  X  P I that defines interpre-
We assume U , B and L fixed, and we concatenate the names to denote the union of them for ease. As in [15], we allow predicate to be blank node.
 6. L I maps each t  X  V  X  L t into R I that defines interpretation
If I is a simple interpretation of a vocabulary V ,then I can also be treated as a function with domain V in the following way Given a simple interpretation I and a partial function A : R , we can define a function I A that extends the domain of I to the domain of A : Let G be an arbitrary RDF graph, we say I satisfies G if there is some function A : bl ( G )  X  R I , such that for each triple G ,wehave I A ( p )  X  P I and ( I A ( s ) ,I A ( o ))  X  E I ,if p is interpreted as a property name, s and o are interpreted as resources, and the pair of resources assigned to ( s, o ) the extension of the interpreted property of p . Furthermore, blank nodes are treated as variables, e.g. the triple ( x, p, o is satisfied by I if there is a function A maps x to I ( s such that ( s, p, o ) is satisfied by I .

A datatype d is composed by three parts: the lexical space L the value space V ( d ) and the lexical-to-value mapping function L 2
V ( d ): L ( d )  X  V ( d ) .Both L ( d ) and V ( d ) are non-empty sets. A datatype map is a partial function D from U to the set of datatypes. Given a datatype map D ,t he D -v ocabulary is the domain dom ( D ) of D . The set of all well-typed D -literals with type in D is defined by L + D = { ( s, a )  X  L t : a  X  dom L ( D ( a )) } .

If a simple interpretation I over the vocabulary rdfV  X  rdfsV dom ( D )  X  pV is said to be a pD  X  -interpretation if I satisfies all D -constraints and P -constraints mentioned in [15]. Because of the space limitation, we do not list these constraints here. An RDF graph S is said to pD  X  entail an RDF graph G , denoted as S if every pD  X  interpretation that satisfies S also satisfies G .
The pD  X  semantics can result in two kinds of logical inconsis-tencies: (1) two triples of the form v owl:differentFrom w and v owl:sameAs w ; (2) three triples of the form v owl:disjointWith w , u rdf:type v , u rdf:type w . In [8], the notion of justification is introduced to explain an OWL OWL DL ontology is a minimal set of axioms in the ontology that is responsible for the entailment. We adapt this notion as follows. eralized RDF graph G and a triple  X  , suppose G | =  X  . A sub-graph J of G is an ontology justification for  X  ,if J | = all J  X  J , J | =  X  . The set of all ontology justifications of  X  is denoted as J  X  .
 E XAMPLE 1. Suppose there are two triples in the ontology. T1: &lt;telephone, rdfs:domain, Person&gt; T2: &lt;GraduateStudent83, telephone, "xxx-xxx-xxxx"&gt;
According to OWL pD  X  Rule D 2 ,i.e. ( p rdfs:domain x, s p o s rdf:type x ), a new triple &lt;GraduateStudent83, rdf:type, Person&gt; (denoted as T3) can be inferred. In other words, it means that one and the only one of all justifications for T 3 is i.e. J T 3 = {{ T 1 ,T 2 }} . There have been variants of Truth Maintenance Systems (TM-S) algorithms, such as JTMS, LTMS, ATMS, HTMS, and ITMS other variants, which uses  X  X ustifications X  to record dependencies between propositions. Though the same term is used, the meaning of justification in TMS is different from that of ontology justifica-tion. Here, justification is used to represent an entailment (or called a rule instance for rule inference engines) that links between a de-rived fact and its antecedents [3, 13]. To distinguish these two terms with the same name, we call a justification in TMS an entailmen-t justification . Formally, an entailment justification is a derivation of the form A  X   X  where  X  is a proposition (also called a con-clusion or consequent) and A is a set of propositions (also called antecedents) [2]. An entailment justification can be also written as an implication of the form A  X   X  .

A TMS is built by calling specific add-rule functions from the inference engine [1]. Hence, all rule instances (entailments) pro-cessed by inference engine are recorded in TMS as entailment jus-tifications. Usually, entailment justifications are organized as a de-pendency graph [5].

D EFINITION 2 (TMS D EPENDENCY G RAPH ). For a TMS T , let P be the set of all propositions appearing in it and R = {  X  it, where  X   X  X  and A  X  P . A TMS dependency graph is a hypergraph G T =( V, E ) ,where V = { v p : p  X  X } and E = { ( V A ,v  X  ): A  X   X   X  X } .  X  p  X  X  , v p  X  V is called the node of the dependency graph corresponding to proposition p , and V A = { v p : p  X  A } . For any r  X  X  , e r  X  X  is called the edge of the dependency graph corresponding to rule instance r ,which may consist of more than two nodes, thus is a hyperedge. According to [10], the TMS dependency graph can be used to gen-erate explanations for a proposition entailed by a set of proposition-s.
In this section, we first propose to exploit TMS to find all jus-tifications of OWL pD  X  entailments. On this basis, a compres-sion technique is designed to simplify the traditional TMS labeling scheme.
Given a generalized RDF graph G , when calculating the closure of G , we can store a TMS dependency graph G by considering triples in G as propositions and pD  X  inference rule instances as entailment justifications. The generation of G is complete when the closure of G is calculated. We have Theorem 1 if the TMS dependency graph G is acyclic.

T HEOREM 1. An acyclic JTMS dependency graph can be used to generate explanations for a triple in the closure of G by recur-sively traversing backwards from the node representing the triple through hyperedges.

P ROOF . Let the ontology justification for  X  to be J  X  .Let v be the node corresponding to  X  on the TMS dependency graph If v  X  has no parent node, obviously J  X  = {  X  } .

Otherwise, there must be at least one edge linking backwards to a set of nodes. Let the edge be e r and the node set V be V To facilitate the discussion, only the simplest type of TMS, Monotonic-JTMS, is considered in this paper since OWL seman-tics is monotonic. { v A  X   X  ,i.e.
Recursively, taking each node v  X  i  X  V A , traverse backwards un-til no nodes can be explored. As G T is acyclic, the recursive back-wards traversal will definitely stop in finite steps. In other words, isensuredtobe well-founded [5]. Hence, in terms of propositional calculus, Equation 3 can be further expanded as shown in Equation 4[5]. where  X  i is a premise.

As all the in-edges of a node are traversed, multiple explanations will be found. All explanations E for will be found if the traversal covers all possible branches of the search space on the dependency graph, which is ensured by the construction process of TMS
According to Definition 1 and 2, J are the subset of dependency graph edge set E where  X J i , J j  X  X  , J i  X  X  j and J j  X  X 
Based on Theorem 1, we present an approach for finding all jus-tifications of an OWL pD  X  entailment.

Algorithm 1: FindOWLJustifications input :Triple  X  ; Dependency graph G T output :Set J containing all ontology justifications for  X  1begin 2 Put ( J , {  X  } ); // Init J with {  X  } . 3 J  X  X  X  ; 4 while J = J do // Find until no new found 5 J  X  X  ; 6for J i  X  X  do 7f or a j  X  J i do 8i f InEdges ( Node ( a j )) =  X  then 9 E X  InEdges ( Node ( a j )); 10 for e  X  X  do 11 if e is not marked then 12 Remove ( J i , a j ); 13 J i,e  X  J i ; // Make a copy. 14 V  X  Antecedent ( e ); 15 for v  X  V do 16 Put ( J i,e , Triple( v ) ); 17 Put ( J , J i,e ); 18 if a j is not in the original ontology then 19 Remove ( J , J i ); 20 J  X  X  ; // Make a copy. 22 for J j  X  X  do 23 if J i  X  J j then 24 Remove ( J , J j ); 25 else if J j  X  J i then 26 Remove ( J , J i ); 27 J X  X  ;
In practical JTMS, specific data structures are used to support the traversing algorithm on the dependency graph. For our purpose, data structures on dependency graph nodes and edges are defined below. First, we uniquely label a node v  X  with the correspond-ing triple  X  , and hence define two functions Node (  X  )= Triple ( v  X  )=  X  to get the other representation given one kind of representation. Secondly, we uniquely label an edge e the form of ( V A ,v  X  ) where entailment justification r satisfies that A  X   X  . We use another two functions Antecedent ( e r )= V and Consequent ( e r )= v  X  to return the set of antecedent nodes and the consequent node of r respectively. Furthermore, we use function InEdges ( v  X  ) to get all the edges point to v  X  1 thus describes the algorithm to find all ontology justifications for OWL pD  X  semantics based on JTMS.

There may exist cycles in the dependency graph, which unsat-isfies the well-foundedness required by JTMS algorithms. There-fore, all cyclic dependencies, i.e. edges, should be marked before backwards traversal. Let V be the node set of G T . The routine of marking cyclic dependencies can be implemented based on the depth-first search algorithm and executed once before Algorithm 1. The complexity of cyclic dependencies marking is O ( | V | 2 ) sidering that the marking needs to visit all the nodes and the links between adjacent nodes, and the number of links between adjacent nodes is up to | V | 2 .
 Before performing finding all explanations, a set data structure J is initialized with {  X  } to store all explanations (and future on-tology justifications) as shown in Line 2.

From Line 4 to Line 19, the algorithm recursively traverses the dependency graph reversely following incoming edges of all in-volved nodes that are not previously marked to cause cycles. Once a node is explored following certain incoming edge during the traver-sal, the triple corresponding to the node is replaced by a set of an-tecedent triples according to the label of the incoming edges. The replacement becomes a start point of a possible explanation for fur-ther expansion during the recursive traversal, and is put into The recursion stops when no new explanations are put into worst-case of such a traversal is visiting all the nodes and the links between adjacent nodes of the dependency graph if implemented in dynamic programming, hence the complexity is O ( | V | 2 )
There is a more complex situation that a triple may have a d-ual role of derivation and original in the ontology. For example, &lt;A, rdfs:subClassOf, B&gt; , &lt;B, rdfs:subClassOf, C&gt; ,and &lt;A, rdfs:subClassOf, C&gt; . Obviously, the justifications of &lt;A, rdfs:subClassOf, C&gt; include both &lt;A, rdfs:subClassOf, C&gt; itself and {&lt;A, rdfs:subClassOf, B&gt; , &lt;B, rdfs:subClassOf, cation as the original ones in the ontology, rather than directly re-placed by other triples as a derivation. This is shown in Line 18 and Line 19.

From Line 20 to Line 26, the algorithm computes all ontology justifications for  X  out of the explanations obtained formerly by removing all the supersets of each explanation that also appeared in J . Obviously, the complexity is O ( X   X  J i  X  X  | J i | ) .
To sum up, the computation complexity of Algorithm 1 is O all ontology justifications and the size of each individual ontology justification are relatively small compared with the size of nodes of the dependency graph. Therefore, we omit the last portion, and have O ( | V | 2 ) . Data manipulated in Algorithm 1 are mainly in triples. Take Example 2 for instance.

E XAMPLE 2. For Example 1, triple T3 is obtained by substitut-ing RDF terms for corresponding variable items of the rule as in Table 2. In other words, it means that one of the ontology justifica-tions for T 3 is { T 1 ,T 2 } as depicted in Figure 1.

In the example, the entailment justification is labeled with T 2 } ,T 3) . When expanded to triples, it turns to be ({ &lt;telephone, Rule spox Figure 1: Dependency Graph Fragment: A black triangle is used to represent an entailment justification that links between the antecedents and the consequent as a hyperedge on the de-pendency graph. Nodes without incoming edges are premises for the entailment, e.g. T 1 and T 2 . rdfs:domain, Person&gt;, &lt;GraduateStudent83, telephone,  X  xxx-xxx-xxxx  X  &gt; }, &lt;GraduateStudent83, rdf:type, Person &gt; ). RDF terms telephone , Person ,and GraduateStudent83 each appears twice in the label. Moreover, rdfs:domain and rdf: type are unchanged from any instance of Rule D 2 , thus they are not necessary to be recorded explicitly given the rule name. Ap-parently, there is a redundancy of 5 RDF terms compared with the RDF terms bound in Table 2. We will have a more compact data structure if it is possible to utilize such binding information to label an entailment justification.

D EFINITION 3(R ULE B INDING ). Given a generalized RDF graph G and a rule r with variables x 1 , ..., x n ,asetofinstances v , ..., v n to the variables is called a Rule Binding of r iff for rule instance r obtained from r by replacing x i by v i , triples in the antecedent of r canbeentailedby G .

T HEOREM 2. A rule binding can be used uniquely to determine an entailment justification, and vice versa.
 P ROOF . Straightforward.

Definition 3 and Theorem 2 enable us to label an entailment jus-tification with a rule binding. It can be inferred that there will be binding to label an entailment justification compared with the tra-ditional JTMS label.

To reduce the storage space consumption while improve the ac-cess speed, we propose a fixed length labeling scheme to represent the rule binding. A rule binding label is a 7-tuple ( Con-Sub, Con-Pre, Con-Obj, RuleName, Binding1, Binding2, Binding3 )where Con-Sub , Con-Pre ,and Con-Obj are the subject, predicate, and ob-ject RDF terms of the consequent triple. RuleName indicates the rule schema in OWL pD  X  rule set. Binding1 , Binding2 ,and Bind-ing3 are the RDF terms appeared in the antecedents. In order to identify an original triple that has no antecedents in the ontology, the values of  X  X uleName X ,  X  X inding1 X ,  X  X inding2 X , and  X  X ind-ing3 X  in the rule binding label is set to be some invalid values, e.g.  X  1 .

Six RDF terms are enough for recording a rule binding by inves-tigating OWL pD  X  rule set in Table 1. Table 1 summarizes most of the rules in OWL pD  X  rule set. In the fourth column, it gives the compressed entailment justification label based on the above rule
Some trivial rules are omitted similar to those in [16]. binding labeling scheme. In order to rebuild the entailment justifi-cations, it simply expends the binding labels according to Theorem For some rules, e.g. p 15 and p 16 , the compression rate reaches up to
As discussed in the previous section, JTMS provides us a mean-s of finding all ontology justifications on inferred data given cor-responding inference engine. Although Algorithm 1 has a theo-retically feasible complexity equal to that of DFS on dependency graphs, the scalability is limited by the capability of main memory and I/O of standalone systems. Distributed computing technolo-gies will help on this point. WebPIE [16] is an open source Hadoop MapReduce framework based OWL pD  X  inference engine that s-cales well even when dealing with large scale data sets. Thus, for OWL pD  X  semantics, WebPIE is a possible appropriate choice of shows the architecture of our MapReduce-based ontology justifica-tion finding system for OWL pD  X  . For a generalized RDF graph, the ontology justification finding process can be divided into two phases, i.e. building phase and finding phase.

For the first phase, the simplified JTMS is built along with the forward inference process of WebPIE. Suppose WebPIE derives a triple &lt; s, p, o &gt; by applying rule  X  on antecedents &lt; s fied JTMS records a binding label &lt; s, p, o,  X , b 1 ,b b ,and b 3 are RDF terms referred in the antecedents according to Definition 3 and Table 1. Note that the same triple derived by ap-plying different rules or from different antecedents may result in different binding labels. Therefore, the count of labels stored in the simplified JTMS is equal to the count of edges in the dependency graph. The simplified JTMS is obtained once the forward inference of WebPIE comes to the iteration fixpoint. As no additional opera-tions performed other than storing 7-tuples, the labeling operations does not affect the inference operation much.

Entering the second phase, users can initiate a MapReduce ver-sion of Algorithm 1 based on data from the simplified JTMS ob-tained during the previous phase. Analysis on Algorithm 1 shows that most of the time is spent on recursively traversing the depen-dency graph reversely and replacing triples with their antecedents. Hence, we design a MapReduce job, named TracingJob in Algo-rithm 2, to decompose and parallelize codes from Line 4 to Line 18 of Algorithm 1. Moreover, in order to return an explanation result as soon as possible, we design another job, CollectingJob ,as well. The algorithm flow can be illustrated by Figure 3.
Algorithm 2: MapReduceFindOWLJustifications input :Triple  X  ; Dependency Graph G output :Set J containing all ontology justifications for  X  1begin 2 Put ( J ,{  X  }); 3 repeat 4 Set J and G as the inputs of TracingJob ; 5 Set J as the inputs of CollectingJob ; 6 J  X  X  ; 7 TracingJob . waitForCompletion (); 8 CollectingJob . waitForCompletion (); 9 until J == J ; 10 Minimize J ; // See Algorithm 1 Line 19 25.

According to MapReduce programming specification, a job usu-ally consists of two steps (functions). The map step decomposes the original problem into independent sub-problems, and then dis-tributes them across machines. The reduce step collects and com-bines the outputs from the map step. The map step of TracingJob deals with two data sources, i.e. dependency graph G and explana-tions (future candidate ontology justifications) J .For G tion node by taking triple &lt; s, p, o &gt; as decomposition key. For an explanation { &lt; s 1 ,p 1 ,o 1 &gt; ,..., &lt; s n ,p this way, binding labels and explanations with the same triple value are grouped to the same computation node, which facilitates future expansion operations in TracingJob  X  X  reduce step.

Algorithm 3: TracingJobReduce input :Triple key ; Justifications or BindingLabels values output :Integer oKey ; Justification oV alue 1begin 2for value in values do 3if value is a BindingLabel then 4 Put (bindings, value); 5 else // value is a Justification 6 Put (justifications,value); 7if justifications.size() == 0 then 8 return ; // The binding is not referenced. 9if bindings.size() == 0 then 10 for just in justifications do 11 Write (1, just) ; // Find a premise. 12 else 13 for binding in bingdings do 14 for just in justifications do 15 for triple in just do 16 if triple == key then 17 Remove (just, triple); 18 PutAll (just, Antecedent (binding)); 19 Write (0, just);
As shown in Algorithm 3, distributed expansion to explanations is implemented as joining binding labels and explanations on each binding labels ( bindings ) and explanations ( justifications ) respectively as the operands of join operation. Then, we perform join operations accordingly. If bindings is empty, the triple key is a premise. If justifications is empty, the binding label is not relevant. Otherwise, a nested loop join is performed from Line 13 to 19.

During the map step of CollectingJob , it simply distributes the output of TracingJob by taking each explanation as decompo-sition key and integer 0 or 1 as value. And during the reduce step of CollectingJob , Algorithm 4, it sums the values being 1 for each explanation. An explanation is a candidate ontology justification iff the sum is equal to the count of triples in the explanation, otherwise the explanation needs further expansion.

Algorithm 4: CollectingJobReduce input : Justification key ;IntegerList values (value 0 or 1) output : Justification oKey ;Null oV alue 1begin 2 total  X  0; 3for value in values do 4 total  X  total + value; 5if total != key.size() then 6if total == 0 then 7 Write (Null, key); 8 else 9 Put ( J ,key);
A comprehensive example as described in Example 3 may help to better understand the process of above algorithm flow.
E XAMPLE 3. Given an RDF graph consisting of 7 triples as follows, T1: &lt;Lecturer1, teacherOf, GraduateCourse46&gt; T2: &lt;teacherOf, rdfs:range, Course&gt; T3: &lt;_:genid13, owl:someValuesFrom, Course&gt; T4: &lt;_:genid13, owl:onProperty, takesCourse&gt; T5: &lt;GraduateStudent114, takesCourse, T6: &lt;_:genid13, owl:someValuesFrom, GraduateCourse&gt; T7: &lt;GraduateCourse46, rdf:type, GraduateCourse&gt; it will derive two more triples by calculating the closure of the graph.
 T8: &lt;GraduateCourse46, rdf:type, Course&gt; T9: &lt;GraduateStudent114, rdf:type, _:genid13&gt;
Hence, we can build a dependency graph as shown in Figure 4, and get a list of corresponding compressed rule binding labels as shown in Table 3.

Suppose two map tasks and two reduce tasks are configured in our MapReduce framework. The process of finding all justification-sofT9: &lt;GraduateStudent114, rdf:type, _:genid13&gt; can be illustrated with Figure 5
After three runs, the algorithm will output totally two justifica-tions, J3={T4, T5, T6, T7} and J4={T1, T2, T3, T4, T5}. At the first run, T9 is used to initialize explanation J1={T9}. During the re-duce step of tracing job, binding labels and explanations are joined on the key T9 with the values L9 and L10, which results in two re-placements of J1, i.e. J2={T8, T3, T4, T5} and J3={T4, T5, T6, T7} correspondingly. As it cannot determine whether there exist labels to support tracing back from T3, T4, T5, T6, T7, and T8 currently, emitted to the next collecting job. During the collecting job at this run, explanations J2 and J3 are collected and counted, while no justification is output because of their unequal between the count and the size respectively. At the second run, J2 is further expand to be J4 with binding label L8 on T8, and output as pair &lt;0, J4&gt;. At the same time, J3 is determined to be one of the justification-s of T9 because T4, T5, T6, and T7 are original triples from the RDF graph. Finally, at the last run, J4 is also output as another justifications of T9 because all elements in it are original triples.
In order to show the feasibility of our proposal, we implemented a prototype system based on the WebPIE inference engine which is built on top of Hadoop release 0.20.2. We set up a Hadoop cluster to conduct all experiments. The cluster consists of 9 commodity com-puters, one as the master (Intel Q8400 Core 2 Quad 2.66GHz pro-cessor, 4GB main memory, and 320GB SATA 7200rpm hard disk), and the others as the slaves (Intel E5400 Core 2 Duo 2.7GHz, 2GB main memory, and 320GB SATA 7200rpm hard disk). All compu-tation nodes are connected via Gigabit Ethernet interconnect. The operating system is 32-bit Ubuntu 10.04. Hadoop heap size is set to be 2GB.

We did experiments on two groups of data sets. One of them consists of the synthetic data sets of LUBM [6], i.e., 1-university, 10-university, 100-university, 1000-university, 8000-university da-ta set (denoted by LUBM-1, LUBM-10, LUBM-100, LUBM-1000, and LUBM-8000) 5 . The other consists of the DBpedia data sets We first investigated the impact of constructing the simplified JTMS on WebPIE inference engine on time cost and space con-sumption. In this experiment, all 8 slave nodes are involved. We assigned 16 map tasks and 16 reduce task for each job, i.e. 2 map tasks and 2 reduce tasks per slave node. In Figures 6 and 7, compar-isons were made between inference processes with and without the
Total count of triples after derivation is approximately 82K, 1M, 11M, 110M, and 0.88 billion respectively. http://downloads.dbpedia.org/3.6/en/ , accessed in Dec. 2010 (data sets images , labels , long / short abstracts ,and page links are not included). Total count of triples after derivation is approxi-mately 1M. simplified JTMS on time cost and space consumption respective-ly. As shown in Figure 6, using JTMS has almost no influence on the import time. The overhead of constructing a simplified JTMS mainly occurs at the inference phase of WebPIE. For any data set, although time cost of inference with simplified JTMS is higher than that without JTMS, they are of the same magnitude. The ratio of inference with JTMS to inference without JTMS is between 1.24 and 3.11 for this group of data sets. Similarly, according to Figure 7, the count of triples with and without JTMS are of the same mag-nitude too, and the ratio of sizes with and without JTMS is 2.72 for LUBM and is 8.74 for DBpedia. As a result of gzip compression, the ratio of space consumption with and without JTMS is 2.0 for LUBM, and is 1.15 for DBpedia.
Another evaluation objective is to show the scalability of our al-gorithm for finding justifications based on MapReduce. We take speedup as the evaluation measure, which is defined to be justification time, we randomly selected 10 derived triples to per-form our algorithm on each of them, and then computed the aver-age value as the result. For LUBM-1000 data set, we increased the number of slave nodes from 1 to 8 every 2 times. When performing our algorithm on the data set using only 1 slave node, it failed with  X  X ut of heap size X  error. Therefore, results on LUBM1000-1 are not available. Taking LUBM1000-2 as the baseline, the speedup has a growth trend. Table 4 gives the experimental results. Theoretically, the ideal speedup values are linear, i.e. speed scales linearly with the number of slave nodes. The proposed algorithm does not scale linearly for the data sets. The reason is that there exists basic plat-form startup overhead which prevents the linear characteristics of the system. The impact decays along with the increase of data set size and complexity. Experimental results on real world DBpedia data set are shown in Table 5.

We also challenged the task of finding all justifications on billion triples utilizing Amazon Elastic Compute Cloud 7 (Amazon EC2) web service that provides configurable computational resources. Four Large Type Amazon EC2 instances were employed, each has 7.5 GB of memory, 4 EC2 Compute Units, 850 GB of local in-stance storage, and 64-bit platform. It took 10.85 hours to import all triples, and 58.6 hours to do inference on the triples and to con-struct the simplified JTMS. The average time cost to find all justi-fications is 1.5 hours.
Given an ontology and number of slaves, we observed that the execution time of finding justifications is only closely related to the number of iterations, but irrelevant to the number of justifications and the number of triples in each justification. For instance, Table 6 shows the experimental results for 10 different justification find-ing tasks executed on LUBM-1000 data set with 8 slave nodes. We investigated the execution time, number of justifications found, av-erage number of triples in each justification, and count of iterations. The results essentially reflect our observations. As MapReduce ex-ploits process-based parallel processing, memory objects are not easy to share between iterations. The objects have to be serialized on the disks, which becomes the bottleneck. Therefore, execution time is closely relevant to the number of iterations. An interest-ing observation is that the count of justifications does not affect the time consumption much. For instance, comparing the first line and the fourth line, there are 4 justifications for the first triple and 238 justifications for the fourth triple. However, it is slower to find all justifications for the former triple than that for the latter one. When looking into the log of this justification finding process, we found that all the justifications were output at the second run of the al-gorithm. The execution time was saved because no intermediate result need be written to the disks, and the expansion of justifica-tions happened in parallel.
The first work on finding all justifications of an OWL ontology is proposed in [12]. Their approach is limited to ALC and is then extended to more expressive description logics in [9]. These ap-proaches are based on the reasoning algorithm of a description log-ic, thus are often called glass-box approaches. Alternatively, there are some other approaches that treats a description logic reasoner as a  X  X lack-box" or an  X  X racle" and uses it to check satisfiability of an ontology (see [8] for example). These approaches are often called black-box approaches. Neither glass-box approaches nor black-box approaches can handle large scale OWL ontologies. There-fore, some optimization techniques and heuristic approaches are proposed. The main optimization techniques are modularization-based ones, which compute all justifications in a syntactic locality-based module [14] or a justification-preserving module [4] rather than in the whole ontology.

Our approach for finding all justifications of OWL pD  X  entail-ments falls into the glass-box approaches. Since OWL pD mantics is different from other OWL semantics based on descrip-tion logics, our approach is fundamentally different from existing glass-box approaches for finding all justifications in fragments of OWL DL. As for existing optimization techniques, although the modularization-based techniques can be applied to optimize find-ing all justifications of OWL pD  X  entailments, it is still impractical in some cases where the extracted module is large, and it requires a mass of resources, including time and space, to extract the target module in a large scale OWL ontology. http://aws.amazon.com/ec2/ tent ontology and providing explanations of unwanted entailments. In this paper, we proposed an approach to find all justifications of OWL entailments using TMS and MapReduce. With the help of TMS, the problem of finding justifications was converted to traver-sals on the dependency graph of TMS. Scalability was achieved by developing appropriate MapReduce algorithms. We analyzed the feasibility of the approach theoretically and empirically. Eval-uation results show that our approach can scale to more than one billion triples. One of the applications of our work is to improve the quality of Web of Data produced in the LOD project, which can contain different errors such as logical inconsistencies [7]. One direction of our future work lies in adapting more complex TMS systems to deal with other OWL sub-languages. Furthermore, the evaluation is very preliminary due to our limited experimental conditions. To provide more convincing results, we will try larger or commercial Hadoop clusters (e.g. Amazon EC2), and provide extensional experimental results on data sets consisting of the large size of terminologies (T-Box) as well as those consisting of large number of instance data (A-Box). We also plan to study iterative MapReduce techniques to further improve our algorithm.
Gang Wu is partially supported by the National Natural Sci-ence Foundation of China under grants 60903010, 61025007 and 60933001, the National Basic Research Program of China under grant 2011CB302206, the Natural Science Foundation of Jiangsu Province under grant BK2009268, and the Key Laboratory of Ad-vanced Information Science and Network Technology of Beijing under grant XDXX1011. Guilin Qi is partially supported by Excel-lent Youth Scholars Program of Southeast University under grant 4009001011, the National Natural Science Foundation of China under grant 61003157, the Jiangsu Science Foundation under grant BK2010412, and the Key Laboratory of Computer Network and In-formation Integration (Southeast University). Jianfeng Du is partly supported by the National Natural Science Foundation of China un-der grant 61005043. [1] F. Bry and J. Kotowski. Reason Maintenance -State of the [2] J. de Kleer. An assumption-based tms. Artif. Intell. , [3] J. Doyle. A truth maintenance system. Artif. Intell. , [4] J. Du, G. Qi, and Q. Ji. Goal-directed module extraction for [5] K. D. Forbus and J. de Kleer. Building Problem Solvers .The [6] Y. Guo, Z. Pan, and J. Heflin. LUBM: A benchmark for [7] A. Hogan, A. Harth, A. Passant, S. Decker, and A. Polleres. [8] A. Kalyanpur, B. Parsia, M. Horridge, and E. Sirin. Finding [9] A. Kalyanpur, B. Parsia, E. Sirin, and J. Hendler. Debugging [10] D. Mattox, K. Smith, and S. C. Y. Lu. Tracking Causal [11] B. Motik, B. C. Grau, I. Horrocks, Z. Wu, A. Fokoue, and [12] S. Schlobach and R. Cornet. Non-standard reasoning [13] M. Stanojevic, S. Vranes, and D. Velasevic. Using Truth [14] B. Suntisrivaraporn, G. Qi, Q. Ji, and P. Haase. A [15] H. J. ter Horst. Completeness, decidability and complexity of [16] J. Urbani, S. Kotoulas, J. Maassen, F. van Harmelen, and
