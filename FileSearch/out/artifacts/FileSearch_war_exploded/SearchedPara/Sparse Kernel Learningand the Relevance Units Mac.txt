 Statistical Learning plays a key role in many areas of science, finance and indus-try. Kernel methods [5] have demonstrat ed great successes in solving many ma-chine learning and pattern recognition problems. Many kernel methods produce a model function dependent only on a subset of kernel basis functions associated with some of the training samples. Those samples are called the support vec-tors (SVs) in support vector machine (SVM) methods[6]. SVM has an excellent generalization properties with a sparse model representation [7]. The SVM and kernel machine models (KMM) [5,8] have a ttracted considerable interests. Gen-erally speaking, an SVM method often learns a parsimonious model that ensure the simplest possible model that explains the data well. Apart from the obvious computational advantage, practices have demonstrated that simple models often generalize better for the unseen data. Additionally, learning a sparse model has
However, it has been shown that the standard SVM technique is not always able to construct parsimonious models, for example, in system identification [13]. This inadequacy motivates the exploration of new methods for parsimo-nious models under the framework of both SVM and KMM. Tipping [3] first introduced the relevance vector machine (RVM) method which can be viewed from a Bayesian learning framework of ke rnel machine and produces an identical functional form to SVM/KMM. The results given by Tipping [3] have demon-strated that the RVM has a comparable generalization performance to the SVM but requires dramatically fewer kernel basis functions or model terms than the SVM. A drawback of the RVM algorithm is a significant increase in computa-tional complexity, compared with the SVM method.

Recently, two separate works [14,15] c onsidered applying the L1 penalty into model regularization. L1 penalty originates from the least absolute selection and shrinkage operator (LASSO) first introduced by Tibshirani [16]. In the classical LASSO, the L1 penalty is applied on the weights of each predictor in a linear model. An earlier attempt, called  X  X eneralized LASSO X , can be found in [17].
One features shared by all the mentioned approaches is that the sparse model is obtained from a full kernel model defined on the whole dataset and the ap-proach employs an algorithm procedure to trim off unnecessary kernel basis functions associated with some input vectors. The retained input vectors used in the resulting spare model are called s uch as SVs, relevance vectors (RVs) and critical vectors (CVs) etc. Obviously it is not necessary for these CVs to have to be chosen from the training input vectors. In this paper, we propose a new sparse kernel model called Relevance Units Machine (RUM) in which the CVs will be learnt from data. The idea is not new. It has been around for many years, for example, the direct method for sparse model learning [18], the reduced set (RS) method [5,19], and the sparse pseudo-input Gaussian processes [20]. However in our approach a Bayesian inference framework is adopted so that all the param-eters including kernel parameters are to be learnt from the Bayesian inference.
In Section 2 of this paper, the concepts of the RUM are given and the algo-rithm associated with RUM is presented in Section 3. The experiment results are presented in section 4, followed by our conclusions in Section 5. In supervised learning we are given a set of examples of input vectors set X = { x are independent and identically distributed. Many kernel learning algorithms result in a kernel machine (KM) (such as a kernel classifier), whose output can function, effectively defining one basis function for each example in the training Most criteria like those used by SVM a nd RVM lead to zero values for a large number of w n so that a sparse model is established.

Inspired by the idea used in the direct method for sparse model learning [18], where u m  X  R q ,m =1 , 2 , ..., M are unknown units for the model and is additive noise assumed to be a Gaussian of 0 mean and an unknown variance  X   X  1 , denoted by N ( | 0 , X   X  1 ). M controls the sparsity of the model and we assume that M is known in the modeling process. In our experimental setting, we set the minimum of M =0 . 04 N where N is the total number of training data.

Without loss of generality, we assume w 0 = 0 in the sequel. The learning [ u 1 , ..., To develop an approach for learning all the parameters, we propose to apply k ( x , u the likelihood of the complete training data can be written as where  X  is the set of kernel hyperparameters.

To make a generative Bayesian model , we further specify a Gaussian prior over the weights as done in the RVM approach[3], that is, sion of the Gaussian over the weights. Similarly the prior over U is
These hyperparameters  X  , r and the precision  X  of the Gaussian noise in the target are empowered with hyperp riors given by Gamma distributions p (  X  )= We fixed parameters a = b = c = d = e = f =10  X  4 in our setting.

Combining (1) -(3), we have,
Let A =diag(  X  1 , X  2 , ...,  X  M ), to integrate (4) over w ,wecansee p ( t , U ,  X  , X  | X,  X  )= exp
To cope with the outliers in the targets t , a L1 Laplacian noise model [21] can replace the Gaussian one, in whic harobustRUMcanbeformulatedwith the variational Bayesian inference procedure. We leave this for another work. As the probabilistic model for the relevance units machine only involves Gaussian and Gamma distributions, see (5), the i nference can be eas ily established by using normal maximum likelihood (ML) procedure. The algorithm deduction  X  X  K T u t . By a tedious calculation process we can obtain that where  X  mm is the m -th diagonal element of  X  . Similarly we have
However, once the analytical expression of the kernel function is given, it is for maximizing L can be constructed.

Once all the parameters have been dete rmined, to solve a normal regularized least squares problem, we have w =  X  X  K T u t . For a new unseen input x  X  ,let K prediction outcome t  X  = K  X  u w can be given by Note 1 : The above algorithm can be easily extended to the case of multivariate outputs, i.e., t n is a D -dimensional vector. In some application problems, the dimension D is much greater than the dimension d of the input vectors. Note 2 : The algorithm can be generalized to the unsupervised learning setting in which the input X is unknown. When D d , to seek the projection images x of the latent variable model. The closest approach in literature is the so-called Gaussian Process latent variable model [22]. This is our next work. In this section, we will test the RUM method for regression problems on both synthetic and real world datasets. The experiments will show the ability of RUM in modeling dataset and learning kernel hyperparameters and also compare RUM to RVM [3]. The experiment of the RVM part was conducted by using Tipping X  X  Matlab code ( http://www.miketipping.com/index.php?page=rvm ).
 Example 1: In this example synthetic data were generated from the scalar func-from the uniform distribution over [  X  10 , 10] and target Gaussian noise within t n and tt i were given with zero mean and variance 0.113. The targets is quite noisy compared to the maximal target values 1.

The RBF kernel function used in this experiment takes the following form, k ( x, x ;  X  w )=exp  X  1 2  X  2 kernel. A full kernel model is defined for the RVM algorithm by all the RBF regressors with centers at each input training datum. In the experiment, the width of the RBF kernel function is set to 2.1213 for RVM algorithm where 9.0 was the best value for the kernel variance as chosen for this example in Tipping X  X  Matlab code. For RUM algorithm, the width of the RBF kernel function is treated as a unknown parameter which is automatically estimated by the learning procedure. All the two algorithms produce sparse models. Their predictions on unseen data are presented in Figure 1.

Table 1 compares the mean square erro r (MSE) values over the training and test sets for the models constructed by the RUM and RVM. The number of iterative loops and the RBF kernel widths are also listed. The numbers of chosen regressors are, respectively, 7 (RVM), 5 X 8 (RUM).
In summary, the results given by the RUM are comparable to the result gen-erated by the RVM algorithm however the performance of the RVM algorithm depends on the choice for the value of the RBF width. In experiment we also find that the RUM has less computational cost than the RVM algorithm. Example 2: The second example is a practi cal modeling problem [8] 1 .Inthisex-ample, we are about to construct a model representing the relationship between the fuel rack position (input) and engine speed (output) for a Leyland TL11 tur-bocharged, direct inject diesel engine operated at low engine speed. A detailed system description and experimental setup can be found in [23]. The data set consists of 410 samples. We use the first 1/3 of the total data points, 140 data points as training data in modeling and all the points in model validation with one-step prediction and i terative prediction.
 also considered as outputs in modeling, and v means the fuel input at the last time step. That is, the output t n at time n depends on the output t n  X  1 at time n  X  1. Actually this is a time series modeling problem. The kernel to be used is an RBF kernel defined on three dimensional vectors, k ( x where  X  w are positive width which is set to 0.9192 for RVM algorithm which is equivalent to 1.69 the best value for the kernel variance as reported in [8] and  X  w is learned in the RUM algorithm. We shall note that, in [8,14], this example fails the RVM algorithm due to numerical instability. After carefully tracking the procedure for the RVM algorithm, we found out what causes the numerical instability. The revised program works well for this example.

To demonstrate the capability of model generalization on unseen data, we use the constructed RBF models by the RU M and RVM algorithms to generate the iterative prediction results are presented in Figures 2.

The one-step prediction and iterative prediction results of 8 X 11 term RUM model are better than the ones given by 22-term RVM model, see Table 2. The RUM algorithm has been proposed for solving kernel regression model-ing problems under the Bayesian inference framework. The overall performance offered by the RUM algorithm is comparable to the results given by RVM al-gorithm but RUM has superior sparsity, which has been demonstrated on two modeling problems.

