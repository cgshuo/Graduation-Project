 We studied an alternative choice-based interface for preference elicitation during the cold start phase and compared it directly with a standard rating-based interface. In this alternative interface users start ed from a diverse set covering all movies and iteratively narrowed down through a matrix factorization latent feature space to smaller sets of items based on their choices. The results show that compared to a rating-based interface, the choice-based interface requires less effort and results in more satisfying recommendations, showing that it might be a promising candidate for alleviating the cold start problem of new users. H.1.2 [ Information Systems ]: User/Machine Systems  X  Human information processing ; H.5.2 [ Information interfaces and presentation ]: User Interfaces  X  Evaluation / methodology Algorithms, Measurement, Performance, Design, Reliability, Experimentation, Human Factors, Theory. User-Centric Evaluation, Diversification, Preference Elication, Matrix Factorisation, Choice, Rating Recommender systems face a cold start problem when insufficient data is available to make personalized recommendations. This can occur when a new system is starting, when a new user enters the system or when a new item is entered into the system. For new users in particular the cold-start problem can be an issue, as they are often asked to submit a number of ratings to overcome the cold-start problem, which is a tedious task. In addition users typically do not get any feedback from the recommender system during such a training task. For these reasons user dropout can occur as the task takes too much effort. Adaptive elicitation strategies have been developed to reduce the effort during the cold start phase by collect ratings as efficiently as possible , requesting ratings for items that are predicted to provide the most information. This can be done from both a system-perspective [3], minimizing the duration of the cold-start for the sy stem, and a user perspective [4], minimizing the duration of the cold-start for users. Ratings are the main data used as input for recommender systems. They are expressions of how a user evaluates an item along a n absolute scale (like a star rating) . Research in marketing and decision making psychology demonstrates the challenges and drawbacks of these separate evaluations instead of relative, joint evaluations. Jameson et al. [5] argue that preferences are by nature rela tive expressions and that absolute ratings thus might not might not measure the preference of the user in the best way possible. Because ratings are collected separately they have more noise due to inconsistencies in the data, while the scales lead to limited granularity; in addition users of recommender systems have indicated that choices (joint evaluations) instead of ratings (separate evaluations) are easier and faster [6]. Nguyen et al. investigated how these problems can be reduced by providing rating support by showing exemplars (previously rated movies) as anchor values on the rating scale [11]. However, rather than improving the rating task, perhaps a better approach would be to use a choice task that would allow for relative preference expressions. Decision trees can provide a way to leverage the advantages of choices compared to ratings in recommender systems [4]. By repetiti vely asking a user of a system to pick her favorite from a set of items extracted from a decision tree, a system can quickly remove irrelevant subtrees and narrow down the recommendations to a set that is likely to contain the best items. We argue that such a decision tree could be improved by making it dynamic rather than static, using the latent features underlying Matrix factorization algorithms. Matrix factorization is a family of collaborative filtering algorithms that is used to predict unobserved ratings. It does so by performing a dimensionality reduction similar to singular value decomposition over the matrix R , where R i,u corresponds to the rating given by user u for item i . The matrix is decomposed into two submatrices of dimensionality k describing users and items on these k latent features [8] . These latent features are said to describe attributes of movies that relate to the preferences. Previous research has demonstrated that these features can be used for reducing choice overload in recommender systems by providing lists with more diverse items in terms of latent feature scores [12] or to increase control in rec ommendations by allowing users to fine-tune their recommendations through choosing between candidate sets based on these latent feature scores [9]. We propose a way to alleviate the cold start problem by combining adaptive preference elicitation based on the latent feature space with the advantages of choices over ratings . In this solution a user is assigned a null-vector as user vector when entering the system. In order to establish the actual user vector, the user is asked to make choices from a number of sets of 10 items. The initial choice set is randomly selected from all available items . After which in an iterative fashion, for every choice 1) the user vector is moved in the direction of the item vector corresponding to the chosen item, 2) rating predictions are calculated based on the new user vector, 3) a proportion of items with the lowest predicted ratings is discarded, and 4) a list of the previously chosen item together with maximally diversified (in terms of latent features) candidate items is calculated following the algorithm described by Minack[ 10] and presented as the next choice set . In this way, the user vector slowly traverses the latent feature space in the direction of the evolving user vector , narrowing down the set of movies to that part of the space where the ideal movie(s) might be located. Maximizing diversity ensures that a user has the full set of choices and that each choice step can maximally learn from the choice made . The goal of the alternative choice interface is to improve the user experience with a recommender system (and in particular the cold start), while ensuring that the quality of recommender output does not deteriorate. This leads to the following research question: RQ: Can choice-based preference elicitation improve the training An online experiment was implemented to compare the proposed interface with the conventional way of preference elicitation during the training phase (i.e. asking users to rate the movies they know from lists of randomly selected movies until 15 ratings are collected) . The experiment used a within-subjects design, so each participant evaluated both preference elicitation tasks. The recommendations were predicted through a matrix factorization model trained on ratings for the 2500 most rated movies in the 10M MovieLens dataset. The final dataset consisted of 69k users, 2500 items and 8.82M ratings. The performance metrics of the used model were up to standards (MAE: 0.61358, RMSE: 0.79643, measured through 5-fold cross-validation). The study started after participants agreed to the informed consent and read the basic instructions, when they were provided with either the choice-based interface or the conventional rating based interface (the order was counter-balanced over users) . Once sufficien t information was provided (i.e. 10 choices or 15 ratings), the participant was asked to complete a questionnaire measuring the usability of the interface. Subsequently the same would be done for the other interface. Both interfaces provided users additional information for the movies consisting of a picture of the movie cover, a synopsis and the names of cast and director for all titles to allow them to make better choices. Two recommendation lists were generated, each using the input data of one of the two preference elicitation tasks . To measure differences in the perception of these lists, participants were asked differences in diversity, novelty and satisfaction (c.f. [2]). After th is list comparison was made, participants were again presented with the recommendations based on one task (in the same order as the interfaces were originally presented) and asked to pick their favorite from the list. After choosing they were asked to complete a final questionnaire for list accuracy, choice difficulty and choice satisfaction (c.f. [7]) . Finally participants would choose from and eva luate the other list of recommendations. Note that in questionnaires we did not reveal from which interface the recommendation list was generated. Participants were gathered through an online panel and were rewarded with  X 5 compensation for their participation. Some participants were excluded from our analysis for unreliability from being either exceptionally fast or slow (e.g. more than an hour on an elicitation task) or from providing contradictory responses. In total data from 103 participants was used in the analysis. Average age was 27.22 years ( SD = 12. 68 ), with 54 female and 49 male participants . The three questionnaires were based on Knijnenburg et al. [7] and Ekstrand et al. [2]. The first  X  X nterface usability X  questionnaire was aimed to measure interaction usability in terms of ease of use (e.g.  X  X t was easy to let the system know my preferences X ) and effort (e.g.  X  X sing the interface was effortful.  X ). The second  X  X ist comparison X  questionnaire was aimed to compare the two recommendation lists in terms of diversity, satisfaction and novelty. The final  X  X ist perception X  questionnaire was aimed to measure recommendation output in terms of accuracy (e.g.  X  X ach of the recom mended movies in the list was relevant. X  ), satisfaction (e.g.  X  X  like the movie I have chosen. X ) and choice difficulty (e.g.  X  X t was easy to select a movie. X ). Items from the  X  X ist perception X  and  X  X nterface usability X  questionnaires were formulated as statements to which the user was asked to express to what extent they agreed to them on a 5-point Likert scale (from  X  X ompletely Disagree X  to  X  X ompletely Agree X ) . Per questionnaire all items were submitted to a confirmatory factor analysis (CFA). The CFA used ordinal dependent variables and a weighted least squares estimator. Items with low factor loadings, high cross-loadings, or high residual correlations were removed from the analysis. For the  X  X ist comparison X  questionnaire the scales ranged from ( X  X ist A much more than B X  to  X  X ist B much more than A X ) and these questions were averaged and directionally coded, rather than submitted to a CFA. The subjective experience can be checked against objective properties of the recommendation sets [2]. Relevant features of the output recommendations are the novelty and the diversity of the set. Ekstrand et al.[2 ] also calculated retrospective accuracy but this was not possible as we had no prior ratings from our participants. A number of metrics were gathered or calculated from the data to provide insight in how participants completed the study and the type of recommendations they received. First we inspect (like Ekstrand et al. [2]) the algorithmic measures of novelty and diversity of the recommendations sets calculated from input from the two tasks as depicted below in Fig 1. It shows that recom mendation sets for the choice task are more popular and less diverse tha n recommendations based on the rating task. Figure 1. Boxplots for Popularity (top) and Diversity (bottom) When looking at the choice task, interesting aspects are the moment where users encounter their favorite movie and stop picking other movies and the number of unique items that is chosen throughout the task. If the first list already contains the perfect item we would expect a user to choose that item and stick with it, whereas if the lists are not perfect we expect more unique choices. We see that (upper left chart of Fig. 2), and that they inspect quite some unique items along the way (upper right chart of Fig. 2), suggesting our parameterization of the choice task was adequate. When looking at the rating task, the main metric of interest is the number of lists (of 10 movies each) a user sees before enough ratings (15) are provided to continue. Users need to inspect a large number of these lists (median = 13, see bottom graph in Fig. 2) suggesting h igh effort in the rating task. This effort is consistent with the large differences in the duration of completing the two tasks (rating 15 items versus choosing 10 movies). Participants needed more time on average for the rating task (388sec) than for the choice task (280 sec). The log of the duration (correcting for skewness) was compared in a paired t-test and found to be significantly higher for the rating task ( D=0.533 ) with t(102) = 6.024, p &lt; .001. After each preference elicitation interface, participants were asked to answer 12 questions regarding the usability (ease of use) of the interface and the effort required to perform the task. This gives two sets of responses per user, one for each task. These were submitted to a CFA (clustered per participant), resulting in 5 items loading on the construct  X  X sability X  and 3 items on  X  X ffort X . Using structural equation modeling we related the two factors to each other and the independent variables (task and order factors). We also related these subjective perceptions to the durations for both tasks. The resulting model shows that effort and usability are highly related (correlation of 0.62) and that only effort is affected by the independent variables. This indicates that we cannot really sufficiently distinguish effort from usability and consequently we fitted a model for effort only, as it was affected more by the independent variables. The resulting model shows a reasonable fit (  X  2 (4) = 11.8 , p = .01, CF I = .97, TLI = .93, RMSEA = 0.098, 90% CI: [.036, .165]) in which subjective effort is perceived to be higher in rating than in choice (  X  = -.242, SE = .141, p &lt; .1) and perceived effort increases with the log-transformed duration of the task (  X  = .401, SE = .113 , p &lt; .001). The path model produced from the questionnaires in which participants were asked to compare the lists from both tasks side by side is shown in Fig. 3. Participants are more satisfied with the choice lists than the rating lists in general (as can be seen from the statistically significant positive intercept). Satisfaction is further positively affected when the choice list is more diverse and less novel than the rating list. The advantage of the choice task on satisfaction is reduced if the rating list is presented left. This reflects a well-known phenomenon in decision psychology for comparison tasks [1], by which the first (left) item in a comparison receives the focus in the comparison and therefore gets an advantage. We also find that differences in perceived novelty between lists increases with differences in the (log) ratio of popularity rank (similar to Ekstrand et al.) and the similarity ratio (which interestingly was related to novelty more than diversity as in Ekstrand et al [2]: however, they found this measure to have small differences and to be strongly dependent on the size of the item set). As a final task, participants made a choice from each recommendation list and rated the lists in terms of perceived accuracy, choice satisfaction and choice difficulty. The items from the questionnaires were submitted to a CFA, clustered per participant. Accuracy was discarded from the remainder of the analysis as it correlated very strongly with choice satisfaction. The resulting constructs of choice difficulty and choice satisfaction were combined with the independent measures (order/task) and the algorithmic measures for novelty and diversity in a SEM model. The model shows a good fit (  X  2 (41) = 44.26 , p = .34, CFI = 1.00, TLI = 0.999, RMSEA =.02, 90% CI: [0, .053]). The resulting model (Fig. 4) shows that choice satisfaction dec reases with choice difficulty and less popular items. As a result choices from the recommendations based on the choice-based interface are perceived to be more satisfactory than those by the rating task, as these lists are less difficult to choose from and contain more popular items. Using choices for preference elicitation reduced the effort for participants in our study compared to rating-based elic itation. The recommendation sets generated from both interfaces were perceived to be quite different , with mainly the higher popularity of the items from the choice task leading to participants being more satisfied with their choice-based recommendations than their rating-based recommendations. Moreover, the recommendations calculated from the choice interface resulted in a higher choice satisfaction, again mostly because these were less difficult to choose from and had a higher populari ty. These results indicate that a choice-based elicitation might indeed be a way to alleviate the cold start problem of new users of systems. This new interface seems promising given that recommendations calculated from it were perceived as better than those from a conventional interface. Further research is needed. For example, the parameterization of the choice task, as well as the strong effect of choice on the popularity of the resulting list requires additional research. Furthermore, choice and rating required different instructions. For example, we instructed participants to choose the movie they would like to watch in the choice based interface, but in the rating interface we asked to express to what extent they liked the movies presented. In addition a choice based interface reduces the number of ratings a system receives and recommender performance in terms of prediction accuracy may be harmed over time compared to an interface that uses ratings only. The extent of these effects will have to be investigated. There are some limitations to our study. The number of participants, while enough for the study as performed, did not allow for optimizing the parameters used in the choice-based interface. Furthermore, novelty effects can play a role especially when investigating alternative interfaces. Recommender systems and ratings can be found anywhere, leading to assumptions against which a new interface is compared. Especially the effects in terms of interface usability can diminish over time as people get used to the new way of interacting . We would like to thank Johannes Sanders for programming the study and collecting the data. [1] Dhar, R. and Simonson I. 1992, The Effect of the focus of [2] Ekstrand, M.D., Harper, F.M., Willemsen, M.C. and [3] Elahi, M., Ricci, F. and Rubens, N. 2013. Active learning [4] Golbandi, N., Koren, Y. and Lempel, R. 2011. Adaptive [5] Jameson, A., Willemsen, M.C., Felfernig, A., Gemmis, M. [6] Jones, N., Brun, A. and Boyer, A. 2011. Comparisons instead [7] Knijnenburg, B. , Willemsen, M., Gantner, Z., Soncu, H. and [8] Koren, Y., Bell, R. and Volinsky, C. 2009. Matrix [9] Loepp, B., Hussein, T., &amp; Ziegler, J. (2014). Choice -based [10] Minack, E., Siberski, W. and Nejdl, W. 2011. Incremental [11] Nguyen, T.T., Kluver, D., Wang, T.-Y., Hui, P.-M., [12] Willemsen, M.C., Knijnenburg, B.P., Graus, M.P., Velter-
