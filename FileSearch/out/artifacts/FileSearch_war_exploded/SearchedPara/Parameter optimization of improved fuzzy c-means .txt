 1. Introduction
Magnetic resonance imaging (MRI) is a technique that uses a magnetic field and radio waves to create cross-sectional images of organs, soft tissues, bone, and v irtually all other internal body structures ( Haacke et al., 1999 ). MRI possesses good contrast resolution for different tissues and has advantages over computer-ized tomography (CT) for brain studies due to its superior contrast properties. In this context, brain MRI segmentation has become an increasingly important image processing step in many applications, including: (i) automatic or semiautomatic delineation of areas to be treated prior to radiosurgery, (ii) delineation of tumors before and after surgical or radiosurgical inte rvention for response assessment, and (iii) tissue classification ( Bondareff et al., 1990 ).
Several techniques have been developed for brain MR image segmentation among which thresholding ( Suzuki and Toriwaki, 1991), edge detection ( Canny, 1986), region growing ( Pohle and methods of clustering. A main drawback of the EM algorithm is that it is based on a Gaussian distribution model for the intensity distribution of brain images, which is not true, especially for noisy images. Since Zadeh (1965) first introduced fuzzy set theory which gave rise to the concept of partial membership, fuzziness has received increasing attention. Fuzzy clustering algorithms have been widely studied and applied in various areas. Among fuzzy clustering techniques, FCM is the best known and most powerful method used in image segmentation. FCM was first conceived in 1973 by Dunn (1973) and further generalized by
Bezdek (1981). It is based on minimization of an objective function and is frequently used in pattern recognition. Unfortu-nately, FCM does not consider the spatial information in the image space and is highly sensitive to noise and imaging artifacts.
Since medical images contain significant amount of noise caused by operator, equipment, and the environment, there is an essential need for development of less noise-sensitive algorithms. to alleviate the effects of noise, such as noisy clustering (NC) (Dave, 1991), possibilistic c-means (PCM) ( Krishnapuram and
Keller, 1993), robust fuzzy c-means (RFCM) algorithm ( Pham, 2001), and so on. These methods generally modify most equations along with modification of the objective function. Therefore, they lose the continuity from FCM, which inevitably introduce computation issues.
 model to unify some variations of FCM and then studied its optimality test with parameter selection. However, the variations of the FCM in this method may not have two kinds of optimality test, i.e., one based on the cluster prototypes and another one based on membership functions. It was shown in Yu and Yang (2005) that the GFCM has only the optimality test with the cluster prototype. In Yu and Yang (2007) , an alternative model of GFCM, called a generalized fuzzy clustering regularization (GFCR) was proposed that can have the optimality test with membership functions. Recently, Shen et al. (2005) introduced a new extension of FCM algorithm, called improved FCM (IFCM). They introduced two influential factors in segmentation that address the neighbor-hood attraction. The first parameter is the feature difference between neighboring pixels in the image and the second one is the relative location of the neighboring pixels. Therefore, segmentation is decided not only by the pixel X  X  intensity but also by neighboring pixel X  X  intensities and their locations. However, the problem of determining optimum parameters constitutes an important part of implementing the IFCM algorithm for real applications. The implementation performance of IFCM may be significantly degraded if the attraction parameters are not properly selected. It is therefore important to select suitable parameters such that the IFCM algorithm achieves superior partition performance compared to the FCM. In Shen et al. (2005) , an artificial neural network (ANN) was employed for computation of these two parameters. However, designing the neural network architecture and setting its parameters are always complicated which slow down the algorithm and may also lead to inappropriate attraction parameters and consequently degrade the partitioning performance.
 mentioned drawbacks in segmentation of the intensity MR images. Same as in Shen et al., (2005) , a neighborhood attraction is considered to exist between neighboring pixels of the intensity image. The degree of attraction depends on pixel intensities and the spatial position of the neighbors. Two parameters l (0 o l o 1) and x (0 o x o 1) will adjust the degree of the neighborhood attractions. We will then investigate the potential of genetic algorithms (GAs) and particle swarm optimization (PSO) to determine the optimum values of the neighborhood attraction (Dave, 1991). This method is not proper for image segmentation since noisy pixels are separated from other pixels while they should be assigned to the most appropriate cluster. Pham (2001) modified the FCM objective function by including a spatial penalty on the membership functions. The penalty term leads to an iterative algorithm called as RFCM that allows the estimation of spatially smooth membership functions. However, this method is only slightly different from the original FCM and the new objective function results in the complex variation of the membership function. Krishnapuram and Keller (1993) consid-ered clustering as a possibilistic partition and called their new approach PCM. The drawback of PCM is that it limits the clustering to only one or two classes ( Shen et al., 2005 ).
To overcome these drawbacks, Shen et al., 2005 presented an improved algorithm. They found that the similarity function d ( x j , v i ) is the key to segmentation success. In their approach, a kind of relationship named neighborhood attraction is considered to exist between neighboring pixels. During clustering, each pixel attempts to attract its neighboring pixels toward its own cluster. This neighborhood attraction depends on two factors; the pixel intensities or feature attraction l (0 o l o 1), and the spatial position of the neighbors or distance attraction x (0 o x o 1), which also depends on the neighborhood structure. Considering this neighborhood attraction, they defined the similarity function as below d  X  x j ; v j  X  X  99 x j v i 99 2  X  1 l H ij x F ij  X  X  4  X  where H ij represents the feature attraction and F ij represents the distance attraction. The parameters l and z adjust the degree of the two neighborhood attractions. H ij and F ij computed in a neighborhood containing S pixels as follows: H  X  F  X  with g  X  9 x j x k 9 ; q jk  X  X  a j a k  X  2  X  X  b j b k  X  2 : respectively. It should be noted that a higher value of l results in a stronger feature attraction and a higher value of x results in a stronger distance attraction. Optimized values of these para-meters lead to the best segmentation results while inappropriate values degrade the results. Therefore, parameter optimization is an important issue in IFCM algorithm that can significantly affect the segmentation results. 2. Parameter optimization of IFCM algorithm
As mentioned earlier, the problem of determining optimum attraction parameters constitutes an important part of imple-menting the IFCM algorithm. Shen et al. computed these two parameters using an ANN through an optimization problem ( Shen et al., 2005 ). However, designing the ANN architecture and setting its parameters are always complicated tasks that slow down the algorithm and may lead to inappropriate attraction parameters. This consequently degrades the partitioning performance. In this Section, we introduce three new algorithms, namely GAs, PSO, and BS for optimum determination of the attraction parameters. The performance evaluation of the proposed algorithms is carried out in the next Section. retains the knowledge of where in search space it performed the best, while in GAs if an individual is not selected for crossover or mutation, the information contained by that individual is lost. Comparisons between PSO and GAs are done analytically in Eberhart and Shi (1998) and also with regards to performance in
Angeline (1998). In PSO, a swarm consists of individuals, called particles, which change their position x i  X  t  X  with time t . Each particle represents a potential solution to the problem and flies around in a multidimensional search space. During flight each particle adjusts its position according to its own experience, and according to the experience of neighboring particles, making use of the best position encountered by itself and its neighbors. The algorithm is associated with this topology ( Eberhart et al., 1996 ; Corne et al., 1999): 1. [Start] Generate a random swarm of P particles in D -dimensional space, where D represents the number of variables (here D =2). 2. [Fitness] Evaluate the fitness f  X  x i  X  t  X  X  of each particle with respect to the cost function J . 3. [Update] particles or moved toward the best solution by repeating the following steps: pbest i is the current best fitness achieved by the i th particle and x pbest i is the corresponding coordinate. 3.2. Change the velocity v i of each particle: 3.3. Fly each particle to its new position x i  X  t  X  X  v i  X  t  X  . 4. [Loop] Go to step 2 until convergence.

The above procedures are iterated until a certain criterion is met. At this point, the most fitted particle represents the corresponding optimum values. The specific parameters of the introduced structure are described in Section 4. 2.3. Structure of breeding swarm (BS) optimization
Both GAs and PSO have strengths and weaknesses. The PSO algorithm is theoretically simple and can be implemented in a few lines of code. A PSO individual also keeps the knowledge of where in the search space it performed the best (a memory of the past experience). In GAs, if an individual is not selected for crossover or mutation, the information by that individual is lost. However, without a selection operator PSO may waste resources on poor individuals. A PSO X  X  group interaction enhances the search for an optimal solution, whereas GAs have trouble finding an exact solution ( Settles and Soule, 2005 ).

In this context, our goal is to introduce a hybrid GAs/PSO method, combining the strengths of PSO with GAs, simulta-neously. The hybrid algorithm combines the standard velocity and position update rules of PSO with the ideas of selection, crossover and mutation from GAs. The algorithm is designed so that the GAs facilitate a global search and the PSO performs the local search. The structure of BS algorithms can be summarized as follows: 1. [Start] Generate a random population of size P . 2. [Fitness] Evaluate the fitness of each particle with respect to the cost function J . 3. [Selection] Select P best particles using the roulette wheel algorithm (in the first iteration this step is needless). 4. [New population] Perform step 3 of the GAs and PSO in parallel and create a new population gathering the output of both GAs and PSO. 5. [Loop] Go to step 2 until convergence.
 Although the BS algorithm seems to be more complicated than GAs and PSO, it is able to locate an optimal, or near optimal, solution significantly faster than either GAs or PSO. This is the result of combining the strengths of PSO with GAs, simulta-neously. The GAs facilitate a global search to reach a near optimal solution and the PSO X  X  group interactions enhances the search for noise-free image and the noisy one are illustrated in Fig. 1 (a) and (b), respectively.

In order to evaluate the segmentation performance quantita-tively, some metrics are defined as follows: (1) Under segmentation (Un s ), representing the percentage of (2) Over segmentation (OvS), representing the percentage of (3) Incorrect segmentation (InC), representing the total percen-
Table 1 lists the above metrics calculated for the seven tested methods. It is clear that FCM, PCM, and RFCM cannot overcome the degradation caused by noise and their segmentation performance is very poor compared to IFCM-based algorithms. Among IFCM-based algorithms, GAs-and PSO-based methods are superior to the ANN-based method. However, unprecedented improvement in segmentation results is achieved by the BS-based method. This is the result of combining advantages of GAs and PSO, simultaneously. For better comparison, the segmentation results of IFCM-based methods are illustrated in Fig. 1 (c) X (f); where the segmented classes are demonstrated in red, green, blue, and black colors.

Since the segmentation results of IFCM-based algorithms are too closed to each other, another metric is defined for better comparison of these methods. The new metric is the similarity index (SI) used for comparing the similarity of two samples defined as follows: SI  X  2 A \ B A  X  B 100  X  11  X  where A and B are the reference and the segmented images, respectively. We compute this metric on the squared segmented which included four classes (CSF, gray matter, white matter, and others), the cluster number was set to 4. The segmentation results of FCM algorithm is shown in Fig. 4 (c), while segmentation of parametrically, and completely eliminated the effect of noise. These results nominate the BS-IFCM algorithm as a good technique for segmentation of noisy brain MR images in real application. 4. Conclusion
In this paper, we proposed new algorithms, namely GAs and PSO, to estimate the optimized valu es of neighborhood attraction parameters in IFCM clustering algorithm. GAs are best at reaching a near optimal solution but have trouble finding an exact solution, while PSO X  X  group interactions enhances the search for an optimal solution. Therefore, a combined GAs/PSO, the BS, algorithm was employed for further improvements. Although the BS algorithm seems to be more complicated than GAs and PSO, it is able to locate an optimal solution significantly faster than either GAs or PSO. This is the result of combining the strengths of PSO with GAs, simultaneously. The GAs facilitate a global search to reach a near optimal solution and the PSO X  X  group interactions enhances the search for the optimal local solution. We tested the proposed methods on three kinds of images; a square image, simulated brain MR images, and real brain MR images. Both quantitative and quantitative comparisons at different noise levels demonstrated that both GAs and PSO are superior to the previously proposed ANN method in optimizing the attraction parameters. However, signifi-cant improvements in segmentation results were achieved using the BS algorithm. These results nominate the BS-IFCM algorithm as a good technique for segmentation of noisy brain MR images. Acknowledgment
The authors would like to thank Mr. Youness Aliyari-Ghassabeh for the constructive discussions and useful suggestions. References
