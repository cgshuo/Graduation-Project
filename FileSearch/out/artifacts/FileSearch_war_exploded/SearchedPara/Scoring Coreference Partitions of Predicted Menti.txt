 Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions ( men-tions ) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora X  MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from us-ing perfect ( gold , or key ) mentions as input for purely testing the quality of the entity linking al-gorithm, to an end-to-end evaluation where pre-dicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this sub-field of NLP can be gauged by: (i) the contin-ual addition of corpora manually annotated for coreference X  X he OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013).
The CoNLL-2011/2012 shared tasks on corefer-ence resolution using the OntoNotes corpus (Prad-han et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would al-low future systems to compare against them. Fol-lowing the timely emphasis on end-to-end evalu-ation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), CEAF m (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task X  X  final score.

An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics X  B 3 and CEAF  X  used by the community to handle predicted men-tions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of the BLANC metric for partitions of predicted men-tions. Different interpretations as to how to com-pute B 3 and CEAF scores for coreference systems when predicted mentions do not perfectly align with key mentions X  X hich is usually the case X  led to variations of these metrics that manipulate the gold standard and system output in order to get a one-to-one mention mapping (Stoyanov et al., 2009; Cai and Strube, 2010). Some of these variations arguably produce rather unintuitive re-sults, while others are not faithful to the original measures.

In this paper, we address the issues in scor-ing coreference partitions of predicted mentions. Specifically, we justify our decision to go back to the original scoring algorithms by arguing that manipulation of key or predicted mentions is un-necessary and could in fact produce unintuitive re-sults. We demonstrate the use of our recent ex-tension of BLANC that can seamlessly handle pre-dicted mentions (Luo et al., 2014). We make avail-able an open-source, thoroughly-tested reference implementation of the main coreference evalua-tion measures that do not involve mention manip-ulation and is faithful to the original intentions of the proposers of these metrics. We republish the CoNLL-2011/2012 results based on this scorer, so that future systems can use it for evaluation and have the CoNLL results available for comparison. The rest of the paper is organized as follows. Section 2 provides an overview of the variations of the existing measures. We present our newly updated coreference scoring package in Section 3 together with the rescored CoNLL-2011/2012 out-puts. Section 4 walks through a scoring example for all the measures, and we conclude in Section 5. Two commonly used coreference scoring metrics  X 
B 3 and CEAF  X  X re underspecified in their ap-plication for scoring predicted , as opposed to key mentions. The examples in the papers describing these metrics assume perfect mentions where pre-dicted mentions are the same set of mentions as key mentions. The lack of accompanying refer-ence implementation for these metrics by its pro-posers made it harder to fill the gaps in the speci-fication. Subsequently, different interpretations of how one can evaluate coreference systems when predicted mentions do not perfectly align with key mentions led to variations of these metrics that ma-nipulate the gold and/or predicted mentions (Stoy-anov et al., 2009; Cai and Strube, 2010). All these variations attempted to generate a one-to-one map-ping between the key and predicted mentions, as-suming that the original measures cannot be ap-plied to predicted mentions. Below we first pro-vide an overview of these variations and then dis-cuss the unnecessity of this assumption.

Coining the term twinless mentions for those mentions that are either spurious or missing from the predicted mention set, Stoyanov et al. (2009) proposed two variations to B 3  X  B 3 handle them. In the first variation, all predicted twinless mentions are retained, whereas the lat-ter discards them and penalizes recall for twin-less predicted mentions. Rahman and Ng (2009) proposed another variation by removing  X  X ll and only those twinless system mentions that are sin-gletons before applying B 3 and CEAF . X  Follow-ing upon this line of research, Cai and Strube (2010) proposed a unified solution for both B 3 and CEAF m , leaving the question of handling CEAF e as future work because  X  X t produces unintuitive results. X  The essence of their solution involves manipulating twinless key and predicted mentions by adding them either from the predicted parti-tion to the key partition or vice versa, depend-ing on whether one is computing precision or re-call. The Cai and Strube (2010) variation was used by the CoNLL-2011/2012 shared tasks on corefer-ence resolution using the OntoNotes corpus, and by the i2b2 2011 shared task on coreference res-olution using an assortment of clinical notes cor-by Recasens et al. (2013) that there was a bug in the implementation of this variation in the scorer used for the CoNLL-2011/2012 tasks. We have not tested the correctness of this variation in the scoring package used for the i2b2 shared task.
However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with (2011) correctly state that  X  CEAF can compare par-titions with twinless mentions without any modifi-cation. X  We will look at this further in Section 4.3.
We argue that manipulations of key and re-sponse mentions/entities, as is done in the exist-ation process, but are also subject to abuse and can seriously jeopardize the fidelity of the evalu-ation. Given space constraints we use an exam-ple worked out in Cai and Strube (2010). Let the key contain an entity with mentions { a,b,c } and the prediction contain an entity with mentions { a,b,d } . As detailed in Cai and Strube (2010, p. 29-30, Tables 1 X 3), B 3 0 assigns a perfect pre-cision of 1.00 which is unintuitive as the system has wrongly predicted a mention d as belonging to the entity. For the same prediction, B 3 precision of 0.556. But, if the prediction contains two entities { a,b,d } and { c } (i.e., the mention c is added as a spurious singleton), then B 3 sion increases to 0.667 which is counter-intuitive as it does not penalize the fact that c is erroneously placed in its own entity. The version illustrated in Section 4.2, which is devoid of any mention ma-nipulations, gives a precision of 0.444 in the first scenario and the precision drops to 0.333 in the second scenario with the addition of a spurious singleton entity { c } . This is a more intuitive be-havior.

Contrary to both B 3 and CEAF , the BLANC mea-sure (Recasens and Hovy, 2011) was never de-signed to handle predicted mentions. However, the implementation used for the SemEval-2010 shared task as well as the one for the CoNLL-2011/2012 shared tasks accepted predicted mentions as input, producing undefined results. In Luo et al. (2014) we have extended the BLANC metric to deal with predicted mentions Given the potential unintuitive outcomes of men-tion manipulation and the misunderstanding that the original measures could not handle twinless predicted mentions (Section 2), we redesigned the CoNLL scorer. The new implementation:  X  is faithful to the original measures;  X  removes any prior mention manipulation,  X  has been thoroughly tested to ensure that it  X  is free of the reported bugs that the CoNLL  X  includes the extension of BLANC to handle we present as a reference implementation for the Table 1: Performance on the official, closed track in percentages using all predicted information for the CoNLL-2011 and 2012 shared tasks. community to use. It is written in perl and stems from the scorer that was initially used for the SemEval-2010 shared task (Recasens et al., 2010) and later modified for the CoNLL-2011/2012
Partitioning detected mentions into entities (or equivalence classes) typically comprises two dis-tinct tasks: (i) mention detection; and (ii) coref-erence resolution. A typical two-step coreference algorithm uses mentions generated by the best Figure 1: Example key and response entities along with the partitions for computing the MUC score. possible mention detection algorithm as input to the coreference algorithm. Therefore, ideally one would want to score the two steps independently of each other. A peculiarity of the OntoNotes corpus is that singleton referential mentions are not annotated, thereby preventing the computation of a mention detection score independently of the coreference resolution score. In corpora where all referential mentions (including singletons) are an-notated, the mention detection score generated by this implementation is independent of the corefer-ence resolution score.

We used this reference implementation to rescore the CoNLL-2011/2012 system outputs for the official task to enable future comparisons with these benchmarks. The new CoNLL-2011/2012 results are in Table 1. We found that the over-all system ranking remained largely unchanged for both shared tasks, except for some of the lower ranking systems that changed one or two places. However, there was a considerable drop in the nation of two things: (i) mention manipulation, as proposed by Cai and Strube (2010), adds single-tons to account for twinless mentions; and (ii) the once as pointed out by Luo (2005). This resulted in a drop in the CoNLL averages ( B 3 is one of the three measures that make the average). This section walks through the process of com-puting each of the commonly used metrics for an example where the set of predicted mentions has some missing key mentions and some spu-rious mentions. While the mathematical formu-lae for these metrics can be found in the original papers (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005), many misunderstandings dis-cussed in Section 2 are due to the fact that these papers lack an example showing how a metric is computed on predicted mentions. A concrete ex-ample goes a long way to prevent similar misun-derstandings in the future. The example is adapted from Vilain et al. (1995) with some slight modifi-cations so that the total number of mentions in the key is different from the number of mentions in the prediction. The key ( K ) contains two entities with mentions { a,b,c } and { d,e,f,g } and the re-sponse ( R ) contains three entities with mentions { a,b } ; { c,d } and { f,g,h,i } :
Mention e is missing from the response, and men-tions h and i are spurious in the response. The fol-lowing sections use R to denote recall and P for precision. The main step in the MUC scoring is creating the partitions with respect to the key and response re-spectively, as shown in Figure 1. Once we have the partitions, then we compute the MUC score by: where K i is the i th key entity and p ( K i ) is the set of partitions created by intersecting K i with response entities (cf. the middle sub-figure in Fig-ure 1); R i is the i th response entity and p 0 ( R i ) is the set of partitions created by intersecting R i with key entities (cf. the right-most sub-figure in Fig-ure 1); and N k and N r are the number of key and response entities, respectively.

The MUC F 1 score in this case is 0.40. signed a credit equal to the ratio of the number of correct mentions in the predicted entity contain-ing the key mention to the size of the key entity to which the mention belongs, and the recall is just the sum of credits over all key mentions normal-ized over the number of key mentions. B 3 preci-sion is computed similarly, except switching the role of key and response. Applied to the example: Note that terms with 0 value are omitted. The B 3 F 1 score is 0.46. The first step in the CEAF computation is getting the best scoring alignment between the key and response entities. In this case the alignment is straightforward. Entity R 1 aligns with K 1 and R 3 aligns with K 2 . R 2 remains unaligned.
 CEAF m recall is the number of aligned mentions divided by the number of key mentions, and preci-sion is the number of aligned mentions divided by the number of response mentions: The CEAF m F 1 score is 0.53.
 We use the same notation as in Luo (2005):  X  ( K i ,R j ) to denote the similarity between a key entity K i and a response entity R j .  X  4 ( K i ,R j ) is defined as:
CEAF e recall and precision, when applied to this example, are:
The CEAF e F 1 score is 0.52. The BLANC metric illustrated here is the one in our implementation which extends the original BLANC (Recasens and Hovy, 2011) to predicted mentions (Luo et al., 2014).

Let C k and C r be the set of coreference links in the key and response respectively, and N k and N r be the set of non-coreference links in the key and response respectively. A link between a men-tion pair m and n is denoted by mn ; then for the example in Figure 1, we have Recall and precision for coreference links are: and the coreference F-measure, F c  X  0 . 23 . Sim-ilarly, recall and precision for non-coreference links are: and the non-coreference F-measure, F n = 0 . 50 . We have cleared several misunderstandings about coreference evaluation metrics, especially when a response contains imperfect predicted mentions, and have argued against mention manipulations during coreference evaluation. These misunder-standings are caused partially by the lack of il-lustrative examples to show how a metric is com-puted on predicted mentions not aligned perfectly with key mentions. Therefore, we provide detailed steps for computing all four metrics on a represen-tative example. Furthermore, we have a reference implementation of these metrics that has been rig-orously tested and has been made available to the public as open source software. We reported new scores on the CoNLL 2011 and 2012 data sets, which can serve as the benchmarks for future re-search work.
 This work was partially supported by grants R01LM10090 from the National Library of Medicine and IIS-1219142 from the National Sci-ence Foundation.
