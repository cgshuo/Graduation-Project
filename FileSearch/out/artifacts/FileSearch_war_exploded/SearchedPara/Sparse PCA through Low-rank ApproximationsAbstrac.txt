 Dimitris S. Papailiopoulos dimitris@utexas.edu Alexandros G. Dimakis dimakis@austin.utexas.edu Stavros Korokythakis stavros@stochastictechnologies.com Stochastic Technologies Principal component analysis (PCA) reduces the di-mensionality of a data set by projecting it onto prin-cipal subspaces spanned by the leading eigenvectors of the sample covariance matrix. The statistical sig-nificance of PCA partially lies in the fact that the principal components capture the largest possible data variance. The first principal component (i.e., the first eigenvector) of an n  X  n matrix A is the solution to where A = SS T and S is the n  X  m data set matrix consisting of m data-points, or entries, each evaluated on n features, and k x k 2 is the ` 2 -norm of x . PCA can be efficiently computed using the singular value decomposition (SVD). The statistical properties and computational tractability of PCA renders it one of the most used tools in data analysis and clustering applications.
 A drawback of PCA is that the generated vectors typ-ically have very few zero entries, i.e., they are not sparse . Sparsity is desirable when we aim for inter-pretability in the analysis of principal components. An example where sparsity implies interpretability is doc-ument analysis, where principal components can be used to cluster documents and detect trends. When the principal components are sparse, they can be eas-ily mapped to topics (e.g., newspaper article classifi-cation into politics, sports, etc.) using the few key-words in their support (Gawalt et al., 2010; Zhang &amp; El Ghaoui, 2011). For that reason it is desirable to find sparse eigenvectors.
 Sparse PCA. Sparsity can be directly enforced in the principal components. The sparse principal com-ponent x  X  is defined as The ` 0 cardinality constraint limits the optimization over vectors with k non-zero entries. As expected, sparsity comes at a cost since the optimization in (1) is NP-hard (Moghaddam et al., 2006a) and hence com-putationally intractable in general.
 Our Contribution. We introduce a novel algo-rithm for sparse PCA that has a provable approxima-tion guarantee. Our algorithm generates a k -sparse, unit length vector x d that gives an objective provably within a 1  X  d factor from the optimal: with where  X  i is the i th largest eigenvalue of A and  X  (1) is the maximum diagonal element of A . For any de-sired value of the parameter d , our algorithm runs in time O ( n d +1 log n ). Our approximation guarantee is directly related to the spectrum of A : the greater the eigenvalue decay, the better the approximation. Equa-tion (2) contains two bounds: one that uses the largest eigenvalue  X  1 and one that uses the largest diagonal element of A ,  X  (1) 1 . Either bound can be tighter, de-pending on the structure of the A matrix.
 We subsequently rely on our approximation result to establish guarantees for considerably general families of matrices.
 Constant-factor approximation. If we only as-sume that there is an arbitrary decay in the eigenvalues of A , i.e. , there exists a constant d = O (1) such that  X  1 &gt;  X  d +1 , then we can obtain a constant-factor ap-proximation guarantee for the linear sparsity regime. Specifically, we find a constant  X  0 such that for all sparsity levels k &gt;  X  0 n we obtain a constant approxi-mation ratio for sparse PCA, partially solving the open problem discussed in (Zhang et al., 2012; d X  X spremont et al., 2012). This result easily follows from our main theorem.
 Eigenvalue Power-law Decay. When the data ma-trix spectrum exhibits a power-law decay, we can ob-tain a much stronger performance guarantee: we can solve sparse PCA for any desired accuracy in time polynomial in n,k (but not in 1 ). This is some-times called a polynomial-time approximation scheme (PTAS). Further, the power-law decay is not neces-sary: the spectrum does not have to follow exactly that decay, but only exhibit a substantial spectral drop after a few eigenvalues.
 Our algorithm operates by scanning a low-dimensional subspace of A . There, it examines a polynomial num-ber of special vectors, that lead to a sparse principal component which admits provable performance. A key conceptual innovation that we employ is a hyperspher-ical transformation on our problem space to reduce its dimensionality. Another important component of our scheme is a safe feature elimination step that allows the scalability of our algorithm for data sets with millions of entries. We introduce a test that discards features that are provably not in the support of the sparse PC, in a similar manner as (Zhang &amp; El Ghaoui, 2011), but using a different combinatorial criterion.
 Experimental Evaluation. We evaluate and com-pare our algorithm against state of the art sparse PCA approaches on synthetic and real data sets. Our real data set is a large Twitter collection of more than 10 million tweets spanning approximately six months. We executed several experiments on various subsets of our data set: collections of tweets during a specific time-window, tweets that contained a specific word, etc. Our implementation executes in less than one second for 50 k  X  100 k documents and in a few minutes for millions of documents. Our scheme typically comes closer than 90% of the optimal performance, even for d  X  3, and empirically outperforms previously pro-posed sparse PCA algorithms. 1.1. Related Work There has been a substantial volume of prior work on sparse PCA. Initial heuristic approaches used factor rotation techniques and thresholding of eigenvectors to obtain sparsity (Kaiser, 1958; Jolliffe, 1995; Cadima &amp; Jolliffe, 1995). Then, a modified PCA technique based on the LASSO (SCoTLASS) was introduced in (Jolliffe et al., 2003). In (Zou et al., 2006), a non-convex regression-type approximation, penalized `a la LASSO was used to produce sparse PCs. A noncon-vex technique was presented in (Sriperumbudur et al., 2007). In (Moghaddam et al., 2006b), the authors used spectral arguments to motivate a greedy branch-and-bound approach, further explored in (Moghaddam et al., 2007). In (Shen &amp; Huang, 2008), a similar tech-nique to SVD was used employing sparsity penalties on each round of projections. A significant body of work based on semidefinite programming (SDP) ap-proaches was established in (d X  X spremont et al., 2007a; Zhang et al., 2012; d X  X spremont et al., 2008). A vari-ation of the power method was used in (Journ  X ee et al., 2010). When computing multiple PCs, the issue of deflation arises as discussed in (Mackey, 2009). In (Amini &amp; Wainwright, 2008), the first theoretical op-timality guarantees were established for thresholding and the SDP relaxation of (d X  X spremont et al., 2007a), in the high-dimensional setting of a generative model where the covariance has one sparse eigenvector. In (Yuan &amp; Zhang, 2011), the authors introduced a very efficient sparse PCA approximation based on truncat-ing the well-known power method to obtain the exact level of sparsity desired, which came along with pefor-mance guarantees for a specific data model. In (As-teris et al., 2011), the authors present an algorithm that solves sparse PCA exactly and in polynomial time for matrices of constant rank. The main algorithmic differences from (Asteris et al., 2011) are i) our solver speeds up calculations for matrices with nonnegative entries by a 2 d  X  1 factor in running complexity and ii) a safe feature elimination step is introduced that is fundamental in implementing the algorithm for large data sets. Despite this extensive literature, to the best of our knowledge, there are very few provable approxi-mation guarantees for sparse PCA algorithms and usu-ally under limited data models (Amini &amp; Wainwright, 2008; Yuan &amp; Zhang, 2011; d X  X spremont et al., 2012; Ma, 2011). 2.1. Proposed Algorithm Our algorithm is technically involved and for that rea-son we start with a high-level informal description. For any given accuracy parameter d we follow the following steps: Step 1: Obtain A d , a rank-d approximation of A . We obtain A d , the best-fit rank-d approximation of A , by keeping the first d terms in its eigen-decomposition: where  X  i is the i -th largest eigenvalue of A and v i the corresponding eigenvector.
 Step 2: Use A d to obtain O ( n d ) candidate supports. For any matrix A , we can exhaustively search for the optimal x  X  by checking all n k possible k  X  k sub-matrices of A : x  X  is the k -sparse vector with the same support as the sub-matrix of A with the maximum largest eigenvalue. However, we show how sparse PCA can be efficiently solved on A d if the rank d is constant with respect to n . The key technical fact that we prove is that there are only O ( n d ) candidate supports that need to be examined. Specifically, we show that a set of candidate supports S d = {I 1 ,..., I T } , where I a subset of k indices from { 1 ,...,n } , contains the op-timal support. We prove that the number of these supports is 1 The above set S d is efficiently created by our Spanno-gram algorithm described in the next subsection. Step 3: Check each candidate support from S d on A . Algorithm 1 Sparse PCA via a rank-d approximation 1: Input: k , d , A 2: p  X  1 if A has nonnegative entries, 0 if mixed 4:  X  A d  X  feature elimination( A d ) 5: S d  X  Spannogram k,p,  X  A d 6: for each I  X  X  d do 7: Calculate  X  1 ( A I ) 8: end for 11: x opt d  X  the principal eigenvector of A I opt For a given support I it is easy to find the best vec-tor supported on I : it is the leading eigenvector of the principal sub-matrix of A , with rows and columns indexed by I . In this step, we check all the supports in S d on the original matrix A and output the best. Specifically, define A I to be the zeroed-out version of A , except on the support I . That is, A I is an n  X  n matrix with zeros everywhere except for the principal sub-matrix indexed by I . If i  X  I and j  X  I , then A I = A ij , else it is 0. Then, for any A I matrix, with I  X  S d , we compute its largest eigenvalue and corre-sponding eigenvector.
 Output: Finally, we output the k -sparse vector x d that is the principal eigenvector of the A I matrix, I  X  S d , with the largest maximum eigenvalue. We refer to this ap-proximate sparse PC solution as the rank-d optimal solution .
 The exact steps of our algorithm are given in the pseudo-code tables denoted as Algorithm 1 and 2. The spannogram subroutine, i.e., Algorithm 2, computes the T candidate supports in S d , and is presented and explained in Section 3. The complexity of our algo-rithm is equal to calculating d leading eigenvectors of A ( O ( dn 2 )), running our spannogram algorithm ( O ( n d +1 log n )), and finding the leading eigenvector of total complexity is O ( n d +1 log n + n d k 2 + dn 2 ). Elimination Step: By using a feature elimination subroutine we can iden-tify that certain variables provably cannot be in the support of x d , the rank-d optimal sparse PC. We have a test which is related to the norms of the rows of V d that identifies which of the n rows cannot be in the op-timal support. We use this step to further reduce the number of candidate supports |S d | . The elimination algorithm is very important when it comes to large-scale data sets. For example, for some of our Twit-ter experiments, the elimination was reducing n from 100 , 000 down to only 100, or fewer candidate features. This subroutine is presented in detail in the extended version of the manuscript (Papailiopoulos et al., 2013). 2.2. Approximation Guarantees The desired sparse PC is x  X  = arg max We instead obtain the k -sparse, unit length vector x d which gives an objective We measure the quality of our approximation using the standard approximation factor: where  X  ( k ) 1 = x T  X  Ax  X  is the k -sparse largest eigenvalue of A . 2 Clearly,  X  d  X  1 and as it approaches 1, the ap-proximation becomes tighter. Our main result follows: Theorem 1. For any d , our algorithm outputs x d , where || x d || 0 =k, || x d || 2 =1 and Proof. The proof can be found in (Papailiopoulos et al., 2013). The main idea is that we obtain i ) an upper bound on the performance loss using A d instead of A and ii) a lower bound for  X  ( k ) 1 .
 We now use our main theorem to provide the following model specific approximation results.
 Corollary 1. Assume that for some constant value d , there is an eigenvalue decay  X  1 &gt;  X  d +1 in A . Then there exists a constant  X  0 such that for all sparsity lev-els k &gt;  X  0 n we obtain a constant approximation ratio. Corollary 2. Assume that the first d + 1 eigenvalues of A follow a power-law decay, i.e.,  X  i = Ci  X   X  , for some C, X  &gt; 0 . Then, for any k =  X n and any &gt; 0 we can get a (1  X  ) -approximate solution x d in time The above corollaries can be established by plugging in the values for  X  i in the error bound. We find the above families of matrices interesting, because in prac-tical data sets (like the ones we tested), we observe a significant decay in the first eigenvalues of A which in many cases follows a power law. The main point of the above approximability result is that any matrix with decent decay in the spectrum endows a good sparse PCA approximation. In this section, we describe how to construct the can-didate supports in S d and explain why this set has tractable size. We build up to the general algorithm by explaining special cases that are easier to under-stand. 3.1. Rank-1 case Let us start with the rank 1 case, i.e., when d = 1. For this case Assume, for now, that all the eigenvector entries are unique. This simplifies tie-breaking issues that are for-mally addressed by a perturbation lemma in (Papail-iopoulos et al., 2013). For the rank-1 matrix A 1 , a sim-ple thresholding procedure solves sparse PCA: Simply keep the k largest entries of the eigenvector v 1 . Hence, in this simple case S 1 consists of only 1 set. To show this, we can rewrite (1) as where S k is the set of all vectors x  X  R n with || x || 2 1 and || x || 0 = k . Thus, we are trying to find a k -sparse vector x that maximizes the inner product with a given vector v 1 . This problem is solved by sorting the absolute elements of the eigenvector v 1 and keeping the support of the k entries in v 1 with the largest absolute value.
 Definition 1. Let I k ( v ) denote the set of indices of the top k largest absolute entries of a vector v . We can conclude that for the rank-1 case, the optimal k -sparse PC for A 1 will simply be the k -sparse vector that is co-linear to the k -sparse vector induced on this single candidate support: S 1 = {I k ( v 1 ) } . 3.2. Rank-2 case Now we describe how to compute S 2 . This is the first nontrivial d which exhibits the details of the Spannogram algorithm. Here, we have the rank 2  X  In the rank-1 case we could write the quadratic form maximization as a simple maximization of a dot prod-will prove that in the rank-2 case we can write for some specific vector v c in the span of the eigen-vectors v 1 ,v 2 ; this will be very helpful in solving the problem efficiently.
 To see this, let c be a 2  X  1 unit length vector, i.e., k c k 2 = 1. Using the Cauchy-Schwartz inequal-ity for the inner product of c and V T 2 x we obtain c only if, c is co-linear to V T 2 x . By the previous fact, we have a variational characterization of the ` 2 -norm: We can use (5) to rewrite (4) as where v c = V 2 c . We would like to note two impor-tant facts here. The first is that for all unit vectors c , v c = V 2 c generates all vectors in the span of V 2 (up to scaling factors). The second fact is that if we fix c , then the maximization max x  X  S k v T c x 2 is a rank-1 instance, similar to (3). Therefore, for each fixed unit vector c there will be one candidate support (denote it by I k ( V 2 c )) to be added in S 2 .
 If we could collect all possible candidate supports I ( V 2 c ) in then we could solve exactly the sparse PCA problem on A 2 : we would simply need to test all locally optimal solutions obtained from each support in S 2 and keep the one with the maximum metric. The issue is that there are infinitely many v c vectors to check. Naively, one could think that all possible k -supports could ap-pear for some v c vector. The key combinatorial fact is that if a vector v c lives in a two dimensional sub-space, there are tremendously fewer possible supports : |S 2 | X  4 n 2 . Spherical variables. Here we use a transformation of our problem space into a 2-dimensional space. The transformation is performed through spherical vari-ables that enable us to visualize the 2-dimensional span of V 2 . For the rank-2 case, we have a single phase variable  X   X   X  =  X   X  2 ,  X  2 and use it to rewrite c , without loss of generality, as which is again unit norm and for all  X  it scans all 2  X  1 unit vectors. Under this characterization, we can express v c in terms of  X  as Observe that each element of v (  X  ) is a contin-uous curve in  X  : [ v (  X  )] i =  X  support set of the k largest absolute elements of v (  X  ) (i.e., I k ( v (  X  ))) is itself a function of  X  . The Spannogram. In Fig. 1, we draw an example randomly generated matrix V 2 . We call this a spanno-gram , because at each  X  , the values of curves corre-spond to the absolute values of the elements in the column span of V 2 . Computing [ v (  X  )] i for all i, X  is equivalent to computing the span of V 2 . From the spannogram in Fig. 1, we can see that the continu-ity of the curves implies a local invariance property of the support sets I ( v (  X  )), around a given  X  . That is, ciently small &gt; 0. As a matter of fact, a support set I ( v (  X  )) changes, if and only if , the respective sorting of two absolute elements | [ v (  X  )] i | and | [ v (  X  )] Finding these intersection points | [ v (  X  )] i | = | [ v (  X  )] the key to find all possible support sets.
 There are n curves and each pair intersects on ex-actly two points. 5 Therefore, there are exactly 2 n intersection points. The intersection of two absolute curves are exactly two points  X  that are a solution to [ v (  X  )] i = [ v (  X  )] j and [ v (  X  )] i =  X  [ v (  X  )] the only points where local support sets might change. These 2 n 2 intersection points partition  X  in 2 n 2 + 1 regions within which the top k support sets remain invariant.
 Building S 2 . To build S 2 , we need to i) determine all c intersection vectors that are defined at intersec-tion points on the  X  -axis and ii) compute all distinct locally optimal support sets I k ( v c ). To determine an intersection vector we need to solve all 2 n 2 equations [ v (  X  )] i =  X  [ v (  X  )] j  X  e T i V c =  X  e T j V c , that is Since c needs to be unit norm, we simply need to nor-malize the solution c . We will refer to the intersec-tion vector calculated on the  X  of the intersection of two curves i and j as c + i,j and c  X  i,j , depending on the corresponding sign in (9). For the intersection vec-tors c + i,j and c  X  i,j we compute I k ( V 2 c + i,j ) and I Observe that since the i and j curves are equal on the intersection points, there is no prevailing sorting among the two corresponding elements i and j of V 2 c + i,j or V 2 c  X  i,j . Hence, for each intersection vector c + c i,j , we create two candidate support sets, one where element i is larger than j , and vice versa. This is done to secure that both support sets, left and right of the  X  of the intersection, are included in S 2 . With the above methodology, we can compute all possible I ( V 2 c ) rank-2 optimal candidate sets and we obtain The time complexity to build S 2 is then equal to sort-ing n 2 vectors and solving 2 n 2 equations in the 2 unknowns of c + i,j and c + i,j . That is, the total complex-ity is equal to n 2 n log n + n 2 2 3 = O n 3 log n . Algorithm 2 Spannogram Algorithm for S d . 1: Input: k , p , V d = 2: Initialize S d  X  X  X  , B  X  X  b 1 ,...,b d  X  1 } X  X  X  1 } d  X  1 3: if p = 1 then 4: B  X  X  1 ,..., 1 } , V d  X  [ V T d 0 T d  X  1 ] T , n  X  n + 1 5: end if 6: for all n d subsets ( i 1 ,...,i d ) from { 1 ,...,n } do 7: for all sequences ( b 1 ,...,b d  X  1 )  X  X  do 8: c  X  nullspace 9: if p = 1 then 10: I  X  X  indices of the k -top elements of V c } X  11: else 12: I  X  indices of the k -top elements of abs( V c ) 13: end if 14: l  X  1 16: r  X  X J 1  X  ( i 1 ,...,i d ) | 17: if r &lt; d then 18: for all r -subsets M from ( i 1 ,...,i d ) do 19: l  X  l + 1 21: end for 22: end if 23: S d  X  X  d  X  X  1 ...  X  X  l . 24: end for 25: end for 26: Output: S d .
 Remark 1. The spannogram algorithm operates by simply solving systems of equations and sorting vec-tors. It is not iterative nor does it attempt to solve a convex optimization problem. Further, it computes solutions that are exactly k -sparse, where the desired sparsity can be set a-priori.
 The spannogram algorithm presented here is a sub-routine that can be used to find the leading sparse PC of A d in polynomial time. The general rank-d case is given as Algorithm 2. The details of our algorithm, the elimination step, and tune-ups for matrices with non-negative entries can be found in (Papailiopoulos et al., 2013). We now empirically evaluate the performance of our algorithm and compare it to the full regularization path greedy approach (FullPath) of (d X  X spremont et al., 2007b), the generalized power method (GPower) of (Journ  X ee et al., 2010), and the truncated power method (TPower) of (Yuan &amp; Zhang, 2011). We omit the DSPCA semidefinite approximation of (d X  X spremont et al., 2007a), since the FullPath algo-rithm is experimentally shown to have similar or bet-ter performance (d X  X spremont et al., 2008). We begin with a synthetic experiment: we seek to estimate the support of the first two sparse eigenvectors of a covari-ance matrix from sample vectors. We continue with testing our algorithm on gene expression data sets. Fi-nally, we run experiments on a large-scale document-term data set, comprising of millions of Twitter posts. 4.1. Spiked Covariance Recovery We first test our approximation algorithm on an artifi-cial data set generated in the same manner as in (Shen &amp; Huang, 2008; Yuan &amp; Zhang, 2011). We consider a covariance matrix  X , which has two sparse eigenvec-tors with large eigenvalues; the remaining eigenvec-tors correspond to small eigenvalues. Here, we con-sider  X  = P n i =1  X  i v i v T i with  X  1 = 400 , X  2 = 300 , X  1 ,..., X  500 = 1, where v 1 , v 2 are sparse and each has 10 nonzero entries and non-overlapping supports. We have two sets of experiments, one for few samples and one for extremely few. First, we generate m = 50 samples of length n = 500, distributed as zero mean Gaussian with covariance matrix  X  and repeat the ex-periment 5000 times. We repeat the same experiment for m = 5. We compare our rank-1 and rank-2 algo-rithms against FullPath, GPower with ` 1 penalization and ` 0 penalization, and TPower. After estimating the first eigenvector with  X  v 1 , we deflate A to obtain A . We use the projection deflation method (Mackey, 2009) to obtain A 0 = ( I  X   X  v 1  X  v T 1 ) A ( I  X   X  v on it to obtain  X  v 2 , the second estimated eigenvector of  X . In the following table, we report the probability of correctly recovering the supports of v 1 and v 2 : if both estimates  X  v 1 and  X  v 2 have matching supports with the true eigenvectors, then the recovery is considered suc-cessful. In our experiments for m = 50, all algorithms were comparable and performed near-optimally, apart from the rank-1 approximation (PCA+thresholding). For m = 5 samples we observe that the performance of the rank-1 and GPower methods decay and Full-Path, TPower, and rank-2 find the correct support with probability approximately equal to 96%. This overall decay in performance of all schemes is due to the fact that 5 samples are not sufficient for a perfect estimate. 4.2. Gene Expression Data Set In the same manner as in the relevant sparse PCA lit-erature, we evaluate our approximation on two gene expression data sets used in (d X  X spremont et al., 2007b; 2008; Yuan &amp; Zhang, 2011). We plot the ratio of the explained variance coming from the first sparse PC to the explained variance of the first eigenvector (which is equal to the first eigenvalue). We also plot the performance outer bound derived in (d X  X spremont et al., 2008). We observe that our approximation fol-lows the same optimality pattern as most previous methods, for many values of sparsity k . In these exper-iments we did not test the GPower method since the output sparsity cannot be explicitly predetermined. However, previous literature indicates that GPower is also near-optimal in this scenario. 4.3. Large-scale Twitter data set Here, we evaluate our algorithm on a large-scale data set. Our data set comprises of millions of tweets com-ing from Greek Twitter users. Each tweet corresponds to a list of words and has a character limit of 140 per tweet. Although each tweet was associated with meta-data, such us hyperlinks, user id, hash tags etc, we strip these features out and just use the word list. We use a simple Python script to normalize each Tweet. Words that are not contextual are discarded in an ad-hoc way. We also discard all words that are less than three characters, or words that appear once in the cor-pus. We represent each tweet as a long vector consist-ing of n words, with a 1 whenever a word appears. 6 In the following tests, we compare against TPower and FullPath. TPower is run for 10 k iterations, and is initialized with a vector having 1s on the k words of highest variance. For FullPath we restrict the covari-ance to its first 5k words of highest variance. 7 In our experiments, we use a simpler deflation method: once k words appear in the first k -sparse PC, we strip them from the data set, recompute the new convariance, and then run all algorithms. The performance metric here is again the explained variance over its maximum pos-sible value.
 In Table 2, we show our results for all tweets that con-tain the word Japan , for a 5-day and then a month-length time window. In all these tests, our rank-3 ap-proximation consistently captured more variance than all other compared methods. In Table 1, we show a day-length experiment and report the first 4 sparse PCs of all methods. The average computation times for this time-window where less than 1 second for the rank-1 approximation, less than 5 seconds for rank-2, and less than 2 minutes for the rank-3 approximation on a Macbook Pro 5.1 running MATLAB 7. The main reason for these tractable running times is the use of our elimination scheme which left only around 40  X  80 rows of the initial matrix of 64k rows. In terms of running speed, we empirically observed that our algo-rithm is slower than Tpower but faster than FullPath for the values of d tested. In Table 1, words with strike-through are what we consider non-matching to the  X  X ain topic X  of that PC. Words marked with G are translated from Greek. From the PCs we see that the main topics are about Skype X  X  acquisition by Mi-crosoft, the European Music Contest  X  X urovision X , a crime that occurred in the downtown of Athens. We conclude that our algorithm can efficiently provide interpretable sparse PCs and matches or outperforms the accuracy of previous methods. In terms of run-ning speed, our algorithm is slower compared to the Tpower method and faster than FullPath for d  X  3. A parallel implementation in the MapReduce framework and larger data studies are exciting future directions. This work was supported by NSF Awards 1055099, 1218235 and research gifts by Google, Intel, and Mi-crosoft. Amini, A.A. and Wainwright, M.J. High-dimensional analysis of semidefinite relaxations for sparse princi-pal components. In Information Theory, 2008. ISIT 2008. IEEE International Symposium on , pp. 2454 X  2458. IEEE, 2008.
 Asteris, M., Papailiopoulos, D.S., and Karystinos,
G.N. Sparse principal component of a rank-deficient matrix. In Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on , pp. 673 X  677. IEEE, 2011.
 Cadima, J. and Jolliffe, I.T. Loading and correlations in the interpretation of principle compenents. Jour-nal of Applied Statistics , 22(2):203 X 214, 1995. d X  X spremont, A., El Ghaoui, L., Jordan, M.I., and
Lanckriet, G.R.G. A direct formulation for sparse pca using semidefinite programming. SIAM review , 49(3):434 X 448, 2007a. d X  X spremont, A., Bach, F., and Ghaoui, L.E. Optimal solutions for sparse principal component analysis.
The Journal of Machine Learning Research , 9:1269 X  1294, 2008. d X  X spremont, A., Bach, F., and Ghaoui, L.E. Ap-proximation bounds for sparse principal component analysis. arXiv preprint arXiv:1205.0121 , 2012. d X  X spremont, Alexandre, Bach, Francis R., and
Ghaoui, Laurent El. Full regularization path for sparse principal component analysis. In Proceed-ings of the 24th international conference on Machine learning , ICML  X 07, pp. 177 X 184, 2007b.
 Gawalt, B., Zhang, Y., and El Ghaoui, L. Sparse pca for text corpus summarization and exploration.
NIPS 2010 Workshop on Low-Rank Matrix Approx-imation , 2010.
 Jolliffe, I.T. Rotation of principal components: choice of normalization constraints. Journal of Applied Statistics , 22(1):29 X 35, 1995.
 Jolliffe, I.T., Trendafilov, N.T., and Uddin, M. A modified principal component technique based on the lasso. Journal of Computational and Graphical Statistics , 12(3):531 X 547, 2003.
 Journ  X ee, M., Nesterov, Y., Richt  X arik, P., and Sepul-chre, R. Generalized power method for sparse prin-cipal component analysis. The Journal of Machine Learning Research , 11:517 X 553, 2010.
 Kaiser, H.F. The varimax criterion for analytic rota-tion in factor analysis. Psychometrika , 23(3):187 X  200, 1958.
 Ma, Zongming. Sparse principal component anal-ysis and iterative thresholding. arXiv preprint arXiv:1112.2432 , 2011.
 Mackey, L. Deflation methods for sparse pca. Advances in neural information processing systems , 21:1017 X  1024, 2009.
 Moghaddam, B., Weiss, Y., and Avidan, S. General-ized spectral bounds for sparse lda. In Proceedings of the 23rd international conference on Machine learn-ing , pp. 641 X 648. ACM, 2006a.
 Moghaddam, B., Weiss, Y., and Avidan, S. Spectral bounds for sparse pca: Exact and greedy algorithms.
Advances in neural information processing systems , 18:915, 2006b.
 Moghaddam, B., Weiss, Y., and Avidan, S. Fast pixel/part selection with sparse eigenvectors. In
Computer Vision, 2007. ICCV 2007. IEEE 11th In-ternational Conference on , pp. 1 X 8. IEEE, 2007. Papailiopoulos, D. S., Dimakis, A. G., and Ko-rokythakis, S. Sparse pca through low-rank approx-imations. arXiv preprint arXiv:1303.0551 , 2013. Shen, H. and Huang, J.Z. Sparse principal component analysis via regularized low rank matrix approxima-tion. Journal of multivariate analysis , 99(6):1015 X  1034, 2008.
 Sriperumbudur, B.K., Torres, D.A., and Lanckriet, G.R.G. Sparse eigen methods by dc programming.
In Proceedings of the 24th international conference on Machine learning , pp. 831 X 838. ACM, 2007.
 Yuan, X.T. and Zhang, T. Truncated power method for sparse eigenvalue problems. arXiv preprint arXiv:1112.2679 , 2011.
 Zhang, Y. and El Ghaoui, L. Large-scale sparse prin-cipal component analysis with application to text data. Advances in Neural Information Processing Systems , 2011.
 Zhang, Y., d X  X spremont, A., and Ghaoui, L.E. Sparse pca: Convex relaxations, algorithms and applica-tions. Handbook on Semidefinite, Conic and Poly-nomial Optimization , pp. 915 X 940, 2012.
 Zou, H., Hastie, T., and Tibshirani, R. Sparse prin-cipal component analysis. Journal of computational
