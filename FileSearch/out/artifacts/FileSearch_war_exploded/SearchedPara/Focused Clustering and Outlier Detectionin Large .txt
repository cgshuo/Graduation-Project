 Graph clustering and graph outlier detection have been stud-ied extensively on plain graphs, with various applications. Recently, algorithms have been extended to graphs with at-tributes as often observed in the real-world. However, all of these techniques fail to incorporate the user preference into graph mining, and thus, lack the ability to steer algorithms to more interesting parts of the attributed graph.

In this work, we overcome this limitation and introduce a novel user-oriented approach for mining attributed graphs. The key aspect of our approach is to infer user preference by the so-called focus attributes through a set of user-provided exemplar nodes. In this new problem setting, clusters and outliers are then simultaneously mined according to this user preference. Specifically, our FocusCO algorithm iden-tifies the focus, extracts focused clusters and detects outliers. Moreover, FocusCO scales well with graph size, since we perform a local clustering of interest to the user rather than global partitioning of the entire graph. We show the effec-tiveness and scalability of our method on synthetic and real-world graphs, as compared to both existing graph clustering and outlier detection approaches.
 H.2.8 [ Database Applications ]: Data mining; I.5.3 [ Pattern Recognition ]: Clustering focused graph mining; infer user preference; attributed graphs; clustering; outlier mining; distance metric learning
Many real-world graphs have attributes associated with the nodes, in addition to their connectivity information. For example, social networks contain both the friendship relations as well as user attributes such as interests and de-mographics. A protein-protein interaction network may not Figure 1: Example graph with two focused clusters and one focused outlier. only have the interaction relations but the gene expressions associated with the proteins. Both types of information can be described by a graph in which nodes represent the objects, edges represent the relations between them, and feature vec-tors associated with the nodes represent the attributes. Such graph data is often referred to as an attributed graph .
For attributed graphs, we see major challenges that re-main unsolved by traditional graph mining techniques [4, 10, 19, 24, 31], which consider plain graphs (without attributes). Recent methods have been proposed for attributed graphs, however, they either use all the given attributes [2, 14, 20, 34] or they perform an unsupervised feature selection [16, 18, 22, 26]. In contrast to all of these graph mining paradigms (cf. Table 1), we consider a user-oriented setting where the users can control the relevance of attributes and as a conse-quence, the graph mining results.

In particular, we consider cluster and outlier detection based on user preference. This focused clustering is of par-ticular interest in attributed graphs, where users might not be concerned with all but a few available attributes. As dif-ferent attributes induce different clusterings of the graph, the user should be able to steer the clustering accordingly. As such, the user controls the clustering by providing a set of exemplar nodes (perceived similar by the user) from which we infer attribute weights of relevance that capture the user-perceived similarity. The essence of user preference is cap-tured by those attributes with large weights. We call these the focus attributes , which form the basis of our approach for discovering focused clusters and outliers.

To elaborate on this new terminology, we give a toy exam-ple in Figure 1. The graph represents the friendship relations and the node attributes denote degree, location, mother tongue , and work . There are two focused clusters: On the M ETIS [19], Spectral [24], Co-clustering [10] X X le ft, people know each other due to their degrees and loca-tions . On the right, a similarity in work induces the second cluster. As such, different user interest in subsets of the attributes may induce different clusters. In case a user is in-terested in degree and location , focused clustering should only find the left cluster and not the right one. Analogously, the example outlier is deviating with a college degree among all others having PhDs, where degree is a focus attribute.
While our example is on a toy graph, our problem set-ting has several practical applications in the real-world. For instance, a marketing manager interested in selling cosmet-ics could aim to find communities in a large social network with its members being of a certain age, gender, and income-level. S/he could then offer deals to a few members from each community, and expect the news to propagate by the word-of-mouth. A scientist could aim to identify clusters of sky-objects that are all in close-distance to one another (assuming a graph is constructed among sky-objects by dis-tance in space) and share certain characteristics of interest (e.g., helium level, temperature, light, etc.).

Such user-preferred clusters are likely a handful in the graph, and thus an algorithm should be able to ( i ) effectively identify user preference, ( ii ) efficiently  X  X hop out X  relevant clusters locally without necessarily partitioning the whole graph, and additionally ( iii ) spot outliers if any. In this paper, we offer the following contributions:
In our evaluation we demonstrate the effectiveness and scalability of our method on synthetic and real-world graphs, compared to existing graph clustering and outlier detection methods. Our experiments show that existing approaches are not suitable for the new focused graph mining setting.
We show the highlights of related work in Table 1. The two key differences of our work are summarized as follows: (1) we introduce a new user-oriented problem setting for attributed graphs, in which we aim to find focused clusters and outliers based on user preference , and (2) we propose an algorithm that simultaneously extracts relevant clusters and outliers from large graphs. In the following, we discuss related work in three areas; traditional plain graph mining, attributed graph mining, and semi-supervised data mining. Graph partitioning has been well studied in the literature. Widely used methods include METIS [19] and spectral clus-tering [11, 24], which aim to find a k -way partitioning of the graph. Different from partitioning, community detec-tion methods [12] cluster the graph into variable size com-munities. Autopart, cross-associations [7], and information-theoretic co-clustering [10] are parameter-free examples to graph clustering methods. Several methods [3, 30, 33] also allow clusters to overlap as observed in real-world social and communication networks. Works that aim to spot structural outliers in plain graphs include [1, 28]. However, all of these methods are limited to plain graphs (without attributes).
Compared to the wide range of work on plain graph min-ing, there has been much less work on attributed graphs. The representative methods [2, 14, 17, 20, 27, 34] aim to par-tition the given graph into structurally dense and attribute-wise homogeneous clusters, detect deviations from frequent subgraphs [25], or search for community outliers in attributed graphs [14]. These methods, however, enforce attribute ho-mogeneity in all attributes. Recently some methods loosen this constraint by unsupervised feature selection [26], sub-space clustering [16, 22] and subspace outlier detection [18, 23] and extract cohesive subgraphs with homogeneity in a subset of attributes. However, all of these methods either do not perform a selection of attributes or do not allow for user preference to steer the algorithm.

A broad variety of methods for semi-supervised clustering consider user-given constraints like  X  X ust-link X  and  X  X annot-link X  referred to as constraint-based clustering [5]. There also exist methods for semi-supervised outlier mining [13]. However, these methods are based on vector data and fur-ther, not applicable to graphs with attributes.

Methods on seeded community mining [3, 8, 15, 30] find communities around (user-given) seed nodes. However, those methods find structural communities on plain graphs and neither apply to attributed graphs, nor enable user prefer-ence on attributes. Moreover, they do not provide outlier detection. In contrast, we use user-given exemplar nodes to automatically infer user preference on attributes. To the best of our knowledge this problem setting is new and we propose the first focused graph mining approach for simulta-neous clustering and outlier detection in attributed graphs.
In this section we first introduce the notation and pose the focused clustering and outlier detection problem formally. Next, we discuss the main components of our approach and walk through the details of our algorithm. Lastly, we analyze the computational complexity of FocusCO .
In this paper we introduce the novel problem of focused clustering and outlier detection in attributed graphs, defined as follows: Given a large attributed graph G ( V,E,F ) with | V | = n nodes and | E | = m edges, where each node is as-sociated with | F | = d attributes (features), extract from G only the (type of) clusters pertaining to a user u  X  X  interest (rather than partitioning the whole graph). To do so, the user provides a small set C ex of exemplar nodes that s/he considers to be similar to the type of nodes the clusters of his/her interest should contain. Assuming that the nodes in a cluster  X  X nite X  or  X  X nit up X  around a few defining at-tributes, we then aim to infer the implicit weights  X  u (i.e., relevance) of attributes that  X  X efine X  the nodes in C ex , i.e., the weights of attributes that make them as similar as pos-sible. Thus,  X  u is expected to be a sparse vector with large weights for only a few attributes (e.g., degree and location in Figure 1), which we call the focus attributes .

Having inferred the attribute weights  X  u from user u , our first goal is to extract focused clusters C from G that are (1) structurally dense and well separated from the rest of the graph, as well as (2) consistent on the focus attributes with large weights. The focused clusters can be overlapping, sharing several of their nodes, as observed in real-world so-cial and communication networks. Moreover, the set C is a subset of all the clusters in G since different sets of clus-ters are expected to unite around different attributes and we aim to extract only those that are specifically similar to the type of clusters user u is interested in. Besides focused clustering, our second goal is to also perform outlier detec-tion . Outliers O are those nodes that structurally belong to a focused cluster (i.e., have many cluster neighbors), but deviate from its members in some focus attributes. In sum-mary, the focused clustering and outlier detection problem in attributed graphs is given as follows:
Given a large graph G ( V,E,F ) with node attributes, and Infer attribute weights  X  u of relevance/importance,
Extract focused clusters C that are (1) dense in graph
Detect focused outliers O , i.e. nodes that deviate from
Next we present the details of the three main components of
FocusCO : (1) inferring attribute weights, (2) extracting focused clusters, and (3) outlier detection.
Our focused clustering setting is a user-oriented one, where each user is interested in extracting certain kind of clusters from a given graph. The user steers the clustering by pro-viding a small set of exemplar nodes that are similar to one another as well as similar to the type of nodes the clus-ters of his/her interest should contain. Our first goal then is to identify the relevance weights of node attributes that make the exemplar nodes similar to each other. This kind of weighted similarity is often captured by the (inverse) Ma-halanobis distance: the distance between two nodes with feature vectors f i and f j is ( f i  X  f j ) T A ( f i  X  f A as the identity matrix yields Euclidean distance, other-wise the features/dimensions are weighted accordingly.
Given the exemplar nodes, how can we learn an A such that they end up having small distance to each other? This is known as the distance metric learning problem [29]. We adopt the optimization objective by [32]: which is convex and enables efficient, local-minima-free al-gorithms to solve it, especially for a diagonal solution.
We give the details of inferring attribute weights in Pro-cedure P1. P S and P D are two sets of similar and dissimilar pairs of nodes, respectively (P1 Line 1). In our setting, all pairs of exemplar nodes constitute P S (P1 Line 2). We cre-ate P D by randomly drawing pairs of nodes that do not belong to the exemplar set (P1 Lines 3-7).

We remark that in creating P D , we may also obtain sam-ples similar to those in P S since these draws are random. To alleviate the affect of such draws, we keep the size of P sufficiently large. This assumes that the number of dissim-ilar pairs is substantially larger than the number of similar pairs in the original distribution. This a suitable assump-tion, given that the number of  X  X ocused X  clusters for any particular user-preference is likely small. Thus, we make the size of P D be | F | times larger than that of P S (P1 Line 7). This also ensures that the data size exceeds dimension size, and that the learning task is feasible.

Moreover, in inferring attribute weights we learn a diago-nal A matrix (P1 Line 9). The reason for this choice is two-fold. First, individual weights for attributes provide ease of interpretation. Second, learning a diagonal A is computa-tionally much more tractable (especially in high dimensions) than learning a full one, since the latter requires solving a program with a semi-definite constraint. Of course if desired, one can instead learn a full matrix (in low dimensions).
Having determined attribute weights  X  , we extract the focused clusters of interest. The main idea in  X  X hopping out X  focused clusters from G is to first identify good candidate nodes that potentially belong to such clusters, and then to expand around those candidate nodes to find the clusters. Details are given in Algorithm A1, which we describe below.
The process of finding good candidate sets to expand is de-tailed in Procedure P2. Intuitively, nodes in focused clusters have high weighted similarity to their neighbors. Therefore, we first re-weigh the edges E by the weighted similarity of Procedure P1 InferAttributeWeights Input: exemplar set of nodes C ex Output: attribute weights vector  X  1: Similar pairs P S =  X  , Dissimilar pairs P D =  X  2: for u  X  C ex ,v  X  C ex do P S = P S  X  ( u,v ) end for 3: repeat 4: Random sample u from set V \ C ex 5: Random sample v from set V \ C ex 6: P D = P D  X  ( u,v ) 7: until d | P S | dissimilar pairs are generated, d = | F | 8: Oversample from P S such that P S = P D 9: Solve objective function in Equ. (1) for diagonal A 10: return  X  = diag ( A ) their end nodes (P2 Lines 2-4), induce G on the edges with notably large weights 1 (P2 Line 5), and consider the nodes in the resulting connected components as our candidate nodes (P2 Lines 6-7). We call each such component a core set.
Next, we expand around each core by carefully choosing new nodes to include in the cluster and continue expanding until there exist no more nodes that increase the quality of the cluster. There exist several measures of cluster quality including modularity and cut size [24]. In this work, we use conductance [3] as it accounts for both the cut size as well as the total volume/density retained within the cluster. The weighted conductance  X  ( w ) ( C,G ) of a set of nodes C  X  V in graph G ( V,E,F ) is defined as where WV ol ( C ) is the total weighted degree of nodes in C . The lower the conductance of a cluster, the better its quality is with few cross-cut edges and large within-density. The expansion operation is presented in Procedure P3. First, we enlist all their non-member neighbors as the can-didate set (P3 Line 4). For each candidate node n , we com-pute the difference  X   X  ( w ) n in cluster conductance if n was to be added to C (P3 Lines 6-16). If there exist any node with negative  X  (i.e., node improves conductance), we pick the best n with the minimum (i.e., largest absolute drop in conductance) (P3 Lines 17-23). We continue iterating until no candidate node yields negative  X   X  ( w ) .

The node additions are based on a best-improving search strategy. While being cautious in which node to add (the best one at every step), our decisions are greedy. Thus, in the following step of our algorithm we adopt a retrospective strategy and check if there exist any nodes in C whose re-moval would drop conductance, presented in Procedure P4. We repeat the node addition and removal iterations until convergence, that is, when the conductance stops changing (A1 Lines 7-12).

We remark that our algorithm is guaranteed to converge; as the (weighted) conductance of a cluster is lower-bounded Algorithm A1 FocusCO : Focused Clusters&amp;Outliers Input: attributed graph G ( V,E,F ), exemplar nodes C ex Output: focused clusters C and outliers O 1: Cores  X  FindCoreSets ( G ( V,E,F ) ,C ex ) 2: C =  X  , O =  X  3: for each core i  X  Cores do 4: C  X  Seeds ( i ) .getNodes () 5: BSN =  X  // holds all Best Structural Nodes to add 7: repeat 11: BSN  X  BSN \ C 13: C  X  X  X  C , O  X  X  X  BSN 14: end for Procedure P2 FindCoreSets Input: attributed graph G ( V,E,F ), exemplar nodes C ex Output: seed sets to expand as focused clusters 1:  X   X  InferAttributeWeights ( C ex ) 2: for each ( i,j )  X  E do 3: w ( i,j ) = 1 / (1 + p ( f i  X  f j ) T diag (  X  )( f i 4: end for 5: w 0  X  max w 0 w 0 /  X  distribution f ( { w | w  X  w 0 } ) 6: Build induced subgraph g ( V 0 ,E 0 ,F ) s.t. 7: return ConnectedComponents( g ( V 0 ,E 0 ,F )) by 0 and we improve (i.e., decrease) the weighted conduc-tance in every iteration (P3 Line 8, P4 Line 5). 2
We omit the details of the  X  conductance computation for brevity, but remark that it is an efficient operation. Specif-ically, the operation of a node u to be added to or to be removed from a cluster S has complexity proportional to the degree of u , i.e. O ( d ( u )). In addition, the total volume of S is simply increased/decreased by the weighted degree w ( u ) of u , when it is added/removed, which takes O (1).
Our algorithm also identifies outlier nodes in each focused cluster along with finding the clusters in a unified fashion. Our definition of a focused cluster outlier is quite intuitive: a node that belongs to a focused cluster structurally (hav-ing many edges to its members), but that deviates from its members in some focus attributes significantly is an outlier. To quantify this definition, the main idea is to identify the best structural nodes BSN s (best in terms of un weighted conductance) during the course of expansion (P3 Lines 3, 14, 22) and later check if there exist any BSN s which were not included in the resulting focused cluster (A1 Line 11).
In order to identify the best structural node for a cluster in each iteration, we need to also track its unweighted con-ductance. An advantage of our proposed approach is that the overhead of computing the unweighted  X   X  of a node, in addition to its weighted  X   X  ( w ) , is negligible. The reason Procedure P3 Expand Input: attributed graph G ( V,E,F ), focused cluster C , set Output: a focused cluster C , its best structural nodes 1: repeat 2: bestNode = NULL , 3: bestStructureNode = NULL 4: candidateNodes  X  neighbors ( C ) 6: for each node n in candidateNodes do 7:  X   X  ( w ) n ,  X   X  n  X  Get  X  Conductance( G,C,n, add) 10: bestNode  X  n 11: end if 12: if  X   X  n &lt;  X   X  best then 13:  X   X  best =  X   X  n 14: bestStructureNode  X  n 15: end if 16: end for 17: if bestNode 6 = NULL then 18: C  X  C  X  bestNode 20: end if 21: if bestStructureNode 6 = NULL then 22: BSN  X  BSN  X  bestStructureNode 23: end if 24: until bestNode = NULL is that, to compute  X   X  , we simply count the total number, instead of the total weight, of those same edges that are involved in the computation of  X   X  ( w ) . As such, both con-ductances can be computed efficiently at the same time and the best structural node and the best focused cluster node can be identified simultaneously.
Given an attributed graph G ( V,E,F ) and the exemplar nodes C ex , we first create similar and dissimilar node pairs which we use to infer the attribute weights. As the opti-mization objective we adopt is convex and as we aim for a diagonal solution, local-optima-free gradient descent tech-niques will take O ( d 2 ) for an -approximate answer [6].
To determine good core sets to expand clusters around, we re-weigh the graph edges by the weight vector  X  with com-plexity O ( dm ). Assuming  X  is sparse with only a few non-zero entries for focus attributes, the multiplicative factor becomes effectively constant yielding a complexity of O ( m ). Next, we identify the top-k edges with largest weights on which we induce G to find the core sets ( k m ). To do so, we use a min-heap to maintain this top set while making a single pass over the edges. This requires O ( m log k ) in the worst case, assuming each edge triggers an insertion into the heap. Using these top-k edges we estimate the parameters of a Normal distribution, which takes O ( k ). Next we make another pass over the edge set and subject each to a mem-bership test against the Normal model in O ( m ). We induce the graph on all the edges that pass the test, the connected components of which yield the core sets. Overall complexity for finding the core sets is thus O ( m log k ).
 Procedure P4 Contract Input: attributed graph G ( V,E,F ), focused cluster C , cur-Output: a focused cluster C and its conductance  X  ( w ) curr 1: repeat 2: removed  X  false 3: for each node n in C do 4:  X   X  ( w ) n  X  Get  X  Conductance( G,C,n, remove) 5: if  X   X  ( w ) n  X  0 then 6: C  X  C \ n 8: removed  X  true 9: end if 10: end for 11: until removed = false
For expanding a focused cluster, we enlist all the non-member neighbors as the candidate set C and evaluate their weighted  X  conductance. As discussed in  X  3.2.2, the com-plexity is P n  X  C d ( n ). Since C  X  V , it is equivalently O ( m ). As we add one node at each iteration, the total complexity becomes O ( | S | m ) where | S | is the size of the focused clus-ter, and | S | n . Also note that focused clusters can be extracted around each core set in parallel.

We remark that scanning candidate set C for picking the best node takes O ( m ) in the worst case. Assuming small rounded focused clusters, one can expect that not all edges of G are  X  X ouched X  by C  X  X  neighbors. This implies sub-linear performance for expanding a single cluster in practice. 3
We conclude this section by briefly discussing a couple of variants of our problem setting and how we can adapt our proposed algorithm to handle these variants.

In one variant of the problem, the user might explicitly ask for the exemplar nodes s/he provided to be included in the focused clusters. While it is highly likely that most of the exemplar nodes will indeed be part of the focused clus-ters found in Algorithm 1, inclusion of all is not guaranteed. To handle this setting, we can include the connected com-ponents induced on the exemplar nodes as additional core sets (in P2 Line 7) and later never allow the removal of any exemplar node in extracting the clusters (in P4 Lines 5-9).
Another variant involves the user asking for a sparser rep-resentation of the focus attributes. In other words, it may be practical to define the similarity among the exemplar nodes using as few attributes as possible, especially in high dimensions. In such a case, we can tune the regularization constant  X  that we introduced in Equation (1) for learning the weight vector  X  . Specifically, a large  X  drives the second term in the objective to become large. To make the pairs in P D as dissimilar as possible, a dense  X  is learned. The smaller the  X  gets, the less the emphasis on the dissimilar pairs becomes, and a sparser weight vector is learned. 4
In other settings, the user may choose to explicitly provide the set of dissimilar nodes or the attribute relevances (i.e., the  X  vector) directly, which can be incorporated trivially.
In this section we thoroughly evaluate our method 5 clustering quality, outlier detection performance, and run-time on synthetic and real-world networks. None of the ex-isting methods address the focused clustering and outlier detection problem we pose in this paper. Nevertheless, we compare to two representative techniques, CODA [14] and METIS [19]. CODA is a graph clustering and outlier detec-tion algorithm on attributed graphs, and treats all attributes equally. The clustering is not steered by user-preference, as such, it clusters the whole graph. METIS is a graph par-titioning algorithm and does not provide outlier detection. Both methods expect the number of clusters as input.
To evaluate focused clustering quality, we use the Nor-malized Mutual Information (NMI), a widely used metric for computing clustering accuracy of a method against the desired ground truth [21]. The ground truth in our case is the true focused clusters that are relevant to a particular user X  X  interest. The best NMI score is 1. We evaluate outlier detection performance by the F1-score; the harmonic mean of precision and recall for a known set of outliers.
To study the behavior of our algorithm compared to other approaches on graphs with ground truth (focused) clusters and outliers, we generated synthetic graphs with various number of clusters focusing on different subsets of the at-tribute space, containing various number of clusters, and with varying size ranges. Our generative algorithm is based on the planted partitions model [9].

Simply put, given the desired number of nodes in each cluster we split the adjacency matrix into blocks defined by the partitioning. For each block B ij , we choose a probabil-ity p ij . Using a random draw process we assign a 1, i.e. an edge, for each possible entry in the block, and 0 otherwise. In other words, p ij specifies the density of each block. The di-agonal blocks constitute the actual clusters and off-diagonal entries yield the cross edges. Unless otherwise noted, we set p ii = 0 . 35 and 0 . 10  X  p ij  X  0 . 25, i 6 = j .

We assign the graph clusters generated, either to one of two focus attribute sets (i.e. focus-1 or focus-2) or as un-focused. Please note that in real-world graphs, we expect to see more than two focuses on a variety of attribute sub-spaces. For each focused cluster, one of the two subsets (focus-1 or focus-2) is chosen as focus attributes. For each attribute i in this subset the attribute values are drawn from a Normal distribution N (  X  i , X  ) with uniform random mean  X   X  [0 , 1] and a variance  X  = 0 . 001. The variance is specifi-cally chosen to be small such that the clustered nodes  X  X gree X  on their focus attributes. The rest of the attributes, on the other hand, are drawn from a Normal distribution with much larger variance; N (0 , 1). In contrast, all of the at-tribute values of nodes in unfocused clusters are drawn from large-variance Normals.

Focused outliers are generated by randomly choosing mem-bers from each focused cluster and  X  X eflating X  (depending on the setting) one or more of their focus attributes i ; by re-placing them by a value drawn from N (  X  i , X  = 1). Figure 2: NMI vs. attribute size | F | . Results aver-aged over 100 runs, bars depict 25-75%.
To study the clustering performance, we generated graphs with 3 focused clusters that are coherent in focus-1 attributes, 3 focused clusters that are coherent in focus-2 attributes, and 3 unfocused clusters, for a total of 9 clusters. The task is to extract the focus-1 clusters with the respective user preference.

For comparison we use CODA and METIS, which do not perform focused cluster extraction. They both partition the entire graph, and thus, we explicitly need to select the 3 best clusters that these methods returned, by measuring the overlap among the clusters they produced to the ground-truth clusters. Since both methods require the number of clusters to be provided as input, we asked for the correct number of (9) clusters, i.e. best performance, although in practice this number is hard to choose as the number of hidden clusters is unknown, especially for large graphs.
METIS is a graph partitioning algorithm that does not handle attributes. In one version of METIS, we ignore the attributes and use only the graph structure. In a second version, we incorporate attribute information by weighing the edges of the graph (using  X  ) by the attribute similarity of their end nodes. We call these two versions as weighted METIS (w) and unweighted METIS (uw). While CODA can perform clustering for attributed graphs, a main challenge with it is to carefully choose a  X  parameter that controls the trade-off between structural and attribute similarity of the nodes. In our experiments we report results using 3 different  X  settings, 0 . 05, 0 . 1, and 0 . 5, for CODA.

Figure 2 shows the clustering performance (mean NMI over 100 independent runs) of the methods when we increase the number of attributes while retaining the same number of (5) focus attributes for the focused clusters. We observe that FocusCO remains superior to all the other approaches in the face of irrelevant attributes for the clustering task.
To illustrate the importance of weight learning, we also study the performance of a variant of our FocusCO , in which we use a uniform attribute weight vector  X  , i.e., we bypass weight learning and directly perform cluster extrac-tion. We observe that the performance of this version of our algorithm drops quickly with increasing attribute size. Our analysis suggests that this occurs due to the edge weights having a more and more uniform distribution when weighted by using a uniform  X  , which yields an inferior collection of Symbols depict the mean over 100 runs, bars depict 25-75%. core sets around which we find clusters. Thus, we proceed with studying the performance of our original FocusCO .
Next in Figure 3 we show the clustering performance of the methods under various other settings. In (a), we increase the cluster size where we create clusters of the same size in the graph. In (b), we allow the graph to contain variable size clusters and increase the variance of the cluster sizes, by randomly drawing them from increasing ranges. Finally in (c), we increase the number of unfocused clusters in the graph, while keeping the number of focused clusters fixed. Notice that recovering a few focused clusters of interest in the existence of more unfocused clusters is an increasingly challenging problem.

From all these setups, we observe that FocusCO out-performs the competing methods and their variants in all scenarios. Weighted METIS seems to achieve slightly better performance than the unweighted version, although the dif-ferences are not significant. CODA X  X  accuracy is the lowest, as the homophily assumption it is making does not hold in the full attribute space. We note that in (c), one param-eterization of CODA (  X  = 0 . 5) achieves as high accuracy as METIS for small number of clusters. Its accuracy, how-ever, quickly drops when many more unfocused clusters than focused ones are introduced. Other two parameterizations give low accuracy, pointing out the sensitivity of CODA to the choice of its parameter.

Finally, we study the clustering performance when focus-1 and focus-2 clusters share common focus attributes. We create 20 node attributes out of which the first 10 are as-signed as focus-1 attributes and the next 10 are assigned as focus-2 attributes to the respective clusters. Then, we grad-ually overlap the focus attributes until they are the same set of 10 for all the focused clusters. Figure 4 shows that the performance of FocusCO remains stable and high across all overlaps. The accuracy of METIS (w) is also quite stable and stays around 0 . 6 (METIS (uw) is not expected to be affected in this setup). We also notice that CODA X  X  per-formance starts increasing after more than 50% of the focus attributes overlap. At 100%, there is essentially only a single focus in the graph, where CODA X  X  performance peaks. This suggests that CODA is more suitable for attributed graphs with uniform graph clusters and would suffer when the graph contains many heterogeneous focused clusters (with multiple focuses) as we would expect to see in real networks. F igure 4: Clustering performance by increasing overlap on focus attributes of different focused clus-ters. (mean NMI over 100 runs, bars: 25-75%).

These results show the robustness of FocusCO , where its performance remains quite stable across different settings. They also illustrate that the general graph clustering meth-ods are not suitable for our focused clustering problem, as their performance is ( i ) sensitive to the (parameter) setting, and ( ii ) lower than that of FocusCO at all settings. Next we evaluate outlier detection performance. Since METIS does not detect outliers, we compare to CODA. In addition to its  X  parameter, CODA expects a parameter r that controls the top percentage of nodes to be returned as outliers. We report experiments for the cross-product of  X  = { 0 . 05 , 0 . 1 , 0 . 5 } and r = { 1% , 5% } .
In the first setup, we study the performance with respect to the severity of outliers. If an outlier deviates in a larger number of focus attributes from its cluster members, it be-comes more severe in outlierness, but easier to detect. To create outlier nodes with higher outlierness, we gradually in-crease their number of focus attributes that we deflate. Fig-ure 5 shows the F1-score and precision (averaged over 100 runs). We observe that FocusCO achieves superior perfor-mance to CODA in both metrics. We can also notice the increase in detection accuracy of FocusCO with increas-ing number of deflated focus attributes (i.e., increasing ease in spotting outliers), as we expected, while the same trend is not apparent for CODA potentially because it does not calibrate to attribute subspaces. Figure 5: Outlier detection performance by increas-ing number of deflated focus attributes.

We further analyze the outlier detection accuracy with re-spect to the attribute space size. In Figure 6 we observe that CODA X  X  performance starts dropping after a certain number of attributes, where identifying descriptive focus attributes becomes more and more crucial to accurately spot the out-liers that are hidden in different community structures. The performance of FocusCO on the other hand remains quite stable and superior to CODA. Figure 6: Outlier detection performance by increas-ing number of attributes | F | .

Finally, we note that the precision of FocusCO is often higher than its recall, while both being superior to CODA X  X .
Finally we study the scalability of the methods. Figure 7 shows the running times with increasing number of edges and attributes. CODA X  X  inference techniques are compu-tationally demanding, and thus, its running time is much higher than other methods. METIS on the other hand uses an extremely efficient heuristic and achieves low running times. We report the average cluster extraction time for FocusCO in addition to total running time, as each cluster extraction can be performed in parallel. In fact, we notice that while its total running time increases by graph size, av-erage time to extract a single cluster remains stable and low. In such a case, FocusCO is also comparable to METIS. Figure 7: Running time scalability w.r.t. (a) num-ber of edges | E | and (b) number of attributes | F | .
We use several attributed networks obtained from real-world data to evaluate our approach. Disney is an Ama-zon co-purchase graph of Disney movies. 7 Each movie has 28 attributes such as price, rating, number of reviews, etc. PolBlogs is the citation network among a collection of on-line blogs that discuss political issues. Attributes are the keywords in their text. DBLP and 4Area are two different co-authorship networks of computer science authors. The attributes reflect the conferences which an author has pub-lished in broadly ( DBLP ), or just in databases, data min-ing, information retrieval, and machine learning ( 4Area Finally, we have the friendship relations of YouTube users and the attributes depict their group memberships. Dataset statistics are given in Table 2.
 Table 2: Real-world datasets used in this work. Av-erage running time in seconds per cluster  X  std (avg. number of clusters extracted).

Our real datasets come from various domains and have different node, edge, and attribute counts. Here we report running time experiments to demonstrate the efficiency of our method on these real graphs.

We setup 10 runs of our method on each graph, each with a randomly sampled 1% of the attributes as the focus at-tributes. Each run returns a different number of clusters, thus we report the running time per cluster averaged over the 10 runs and their standard deviations in Table 2. Notice that similar to Figure 7, the running times are quite low. In particular, the average time to extract a cluster in our largest graph YouTube takes around 3 seconds.
The first case study we consider is finding two types of focused clusters in Disney . In one instance, a user wants to understand how the popularity of a movie influences its and illustrated. See text for discussion. (best viewed in color) community in a co-purchase network. The user decides that the product X  X  popularity is related to the features Num-ber_of_reviews and Sales_rank , and so chooses a few prod-ucts which have similar values in those attributes. cusCO then uses this exemplar set C ex to learn an attribute weighting  X  u . This  X  u reflects the user X  X  intent, and has also captured another dimension correlated with those at-tributes; Number_of_different_authors .

Several extracted focused clusters for this task are shown on the left in Figure 8. In general, the discovered clusters consist of movies of similar age and acclaim. The first fo-cused cluster (blue) reflects traditional Disney classics such as Robinhood . Its outlier is a sequel ( An Extremely Goofy Movie ) that is much less popular than the other classics in the cluster. The second community (green) focuses on pop-ular older Disney movies, and has outliers such as American Legends and again the Goofy sequel, that are much less popular. The third cluster (orange) overlaps with the first focused cluster. It is a subset of the classic Disney movies of the larger cluster that were predominantly starred by an-imals (e.g., The Rescuers ). Its cluster outlier is The Black Caldron , which although of similar vintage, starred a human protagonist and was much less popular.

In the next instance, a user wants to examine how the differences in the distribution of consumer ratings affect the Disney clustering. In this case, the focused clusters repre-sent collections of movies that are similarly rated (e.g., Pixar films or animated Disney classics). The outliers represent movies which are rated differently from the movies they are purchased with, and reflect consumer opinion. The results are shown in the right of Figure 8. The first focused clus-ter (purple) represents traditional Disney classics. Its outlier (which was included in the previous focused popularity clus-ter) is the movie A Goofy Movie which although reasonably popular, is not as high rated. The second cluster (green) is the Pixar community, featuring high-rated movies like Toy Story . Its focused outlier is the live action version of 101 Dalmatians , a movie which is rated quite differently than most Pixar films. The third cluster consists of renowned Disney films such as Fantasia . It also contains an outlier, the Spanish version of Beauty and the Beast .

We consider a second case study on PolBlogs , where a user seeks to understand the difference between blog content Figure 9: A focused cluster of liberal blogs in Pol-Blogs with a focus on Iraq war debate. Outlier David Sirota does not mention Waas in his posts. written by different liberal bloggers during the height of the Iraq war controversy in 2005. Using several liberal blogs as an example set, the user learns a  X  u which represents the focus of the bloggers (as shown in the top of Figure 9, text size of features proportional to weight). It contains words such as eriposte , an active liberal blogger, peacenik , a term for an anti-war activist, and most strongly Waas . Murray Waas was an independent journalist praised for his investigative journalism of the Bush administration. 8 The focused community outlier for this group is David Sirota, a well-connected liberal blogger who did not explicitly mention Waas in the dataset.

The third case study we consider is on the 4Area dataset. Here a user wants to understand who the out-liers are in the data mining co-authorship network. S/he learns a  X  u using {Christos Faloutsos, Jiawei Han, Jon M. Kleinberg, Jure Leskovec, Andrew Tomkins} as in-put, who focus primarily on data mining. One of the focused clusters found is around data mining researchers mostly in industry, as shown in Figure 10. The outlier is Cameron Marlow , the former head of Facebook X  X  data science team, who collaborated with the researchers in the data mining community but published on information retrieval. Figure 10: A focused cluster of data mining re-searchers in 4Area. Outlier Cameron Marlow closely publishes with them, but on information retrieval .
In this work we introduce a new problem of finding focused clusters and outliers in large attributed graphs.

Given a set of exemplar nodes that capture the user in-terest, our goal is two-fold: 1)  X  X hop out X  clusters of similar nodes that are densely connected and exhibit coherence in a subset of their attributes, called the focus attributes, and 2) identify focused outliers, i.e. nodes that belong to a focused cluster in network structure but show deviance in some focus attribute(s). We propose an efficient algorithm that infers the focus attributes of interest to the user, and that both extracts focused clusters and spots outliers simultaneously. Experiments on synthetic and real-world graphs show the effectiveness and scalability of our approach and that the existing graph clustering and outlier detection techniques are not suitable to handle the newly posed problem. The authors thank the anonymous reviewers for their help-ful comments. This material is based upon work supported by the ARO Young Investigator Program grant with Con-tract No. W911NF-14-1-0029, an ONR SBIR grant under Contract No. N00014-14-P-1155, NSF Grant IIS-1017181, the Stony Brook University Office of Vice President for Re-search, the Young Investigator Group program of KIT as part of the German Excellence Initiative, and by a post-doctoral fellowship by the Flanders research foundation (FWO). Any findings and conclusions expressed in this ma-terial are those of the author(s) and do not necessarily reflect the views of the funding parties.
