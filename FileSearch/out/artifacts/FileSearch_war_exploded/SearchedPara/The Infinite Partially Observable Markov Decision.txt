 The Partially Observable Markov Decision Process (POMDP) m odel has proven attractive in do-agents to compare the values of actions that gather informat ion and actions that provide immedi-ate reward. Unfortunately, modelling real-world problems as POMDPs typically requires a domain expert to specify both the structure of the problem and a larg e number of associated parameters, and both of which are often difficult tasks. Current methods i n reinforcement learning (RL) focus on learning the parameters online, that is, while the agent i s acting in its environment. Bayesian RL [1, 2, 3] has recently received attention because it allow s the agent to reason both about uncer-tainty in its model of the environment and uncertainty withi n environment itself. However, these methods also tend to focus on learning parameters of an envir onment rather than the structure. In the context of POMDP learning, several algorithms [4, 5, 6 , 7] have applied Bayesian methods to reason about the unknown model parameters. All of these ap proaches provide the agent with the each state. Even when the size of the state space is known, how ever, just making the agent reason about a large number of unknown parameters at the beginning o f the learning process is fraught with much of the model will be highly uncertain. Trying to plan und er vast model uncertainty often requires significant computational resources; moreover, t he computations are often wasted effort when the agent has very little data. Using a point estimate of the model instead X  X hat is, ignoring the model uncertainty X  X an be highly inaccurate if the expert  X  X  prior assumptions are a poor match for the true model. We propose a nonparametric approach to modelling the struct ure of the underlying space X  specifically, the number of states in the agent X  X  world X  X hich allows the agent to start with a simple model and grow it with experience. Building on the infinite hi dden Markov model (iHMM) [8], the infinite POMDP (iPOMDP) model posits that the environment co ntains of an unbounded number of corresponding to its limited experience (also conducive to fast planning). It will dynamically add structure as it accumulates evidence for more complex model s. Finally, a data-driven approach to structure discovery allows the agent to agglomerate states with identical dynamics (see section 4 for a toy example). A POMDP consists of the n-tuple { S , A , O , T ,  X  , R ,  X  } . S , A , and O are sets of states, actions, and observations. The transition f unction T ( s  X  | s, a ) defines the distribution over next-states s  X  to which the agent may transi-tion after taking action a from state s . The observation function  X ( o | s  X  , a ) is a distribution over observations o that may occur in state s  X  after taking action a . The reward function R ( s, a ) specifies the immediate reward for each state-action pair (see figure 1 for a slice of the graphic al model). The factor  X   X  [0 , 1) weighs the importance of current and future rewards.
 We focus on discrete state and observation spaces (generali sing to contin-uous observations is straightforward) and finite action spa ces. The size of the state space is unknown and potentially unbounded. The tr ansitions, observations, and rewards are modelled with an iHMM.
 The Infinite Hidden Markov Model A standard hidden Markov model (HMM) consists of the n-hierarchical Dirichlet Process (HDP) to define a prior over H MMs where the number of underlying where  X  is the DP concentration parameter and H is a prior over observation distributions. For example, if the observations are discrete, then H could be a Dirichlet distribution. butions. More formally, the first two steps involve a draw G stick-length,  X  T is unbounded,  X  T construction of  X  T elements of  X  T and  X  .
 The second step of the iHMM construction involves defining th e transition distributions T (  X | s )  X  (see lower rows of figure 2). Thus, the generating process enc odes a notion that the agent will spend most of its time in some local region. However, the longer the agent acts in this infinite space, the more likely it is to transition to somewhere new. Infinite POMDPs To extend the iHMM framework to  X  1  X  2  X  3  X  4 iPOMDPs, we must incorporate actions and rewards into the generative model. To incorporate actions, we draw an ob-servation distribution  X (  X | s, a )  X  H for each action a and each state s . Similarly, during the second step of the gener-ative process, we draw a transition distribution T ( s  X  | s, a )  X  HMMs have one output X  X bservations X  X hile POMDPs also output rewards. We treat rewards as a secondary set of observations. For this work, we assume that the set of pos-sible reward values is given, and we use a multinomial dis-tribution to describe the probability R ( r | s, a ) of observing reward r after taking action a in state s . As with the obser-vations, the reward distributions R are drawn from Dirichlet distribution H venience; however, other reward distributions (such as Gau s-sians) are easily incorporate in this framework.
 In summary, the iPOMDP prior requires that we specify To sample a model from the iPOMDP prior, we first sample the mea n transition distribution  X  T  X  Stick (  X  ) . Next, for each state s and action a , we sample Samples from the iPOMDP prior have an infinite number of state s, but fortunately all of these states do not need to be explicitly represented. During a finite life time the agent can only visit a finite number of states, and thus the agent can only make inferences about a finite number of states. The remaining (infinite) states are equivalent from agent X  X  per spective, as, in expectation, these states will exhibit the mean dynamics of the prior. Thus, the only pa rts of the infinite model that need to we discuss joint inference over the unknown state history an d the model in section 3.1. As in the standard Bayesian RL framework, we recast the probl em of POMDP learning as planning in a larger  X  X odel-uncertainty X  POMDP in which both the true model and the true state are unknown. We outline below our procedure for planning in this joint spa ce of POMDP models and unknown states and the detail each step X  X elief monitoring and action -selection X  X n sections 3.1 and 3.2. Because the true state is hidden, the agent must choose its ac tions based only on past actions and observations. Normally the best action to take at time t depends on the entire history of actions and spaces, the belief at time t + 1 can be computed from the previous belief, b observation o , by the following application of Bayes rule: where P r ( o | b, a )= joint belief b over models and states with a closed-form expression. We app roximate the belief b with a set of sampled models m = { T,  X  , R } , each with weight w ( m ) . Each model sample m maintains a belief over states b equation 1. Details for sampling the models m are described in section 3.1.
 Given the belief, the agent must choose what action to choose next. One approach is to solve the maximize the expected discounted reward, then the optimal p olicy is given by: where the value function V ( b ) is the expected discounted reward that an agent will receive if its equation 3 is only tractable for tiny problems, but many appr oximation methods [12, 13, 14] have been developed to solve POMDPs offline.
 While we might hope to solve equation 3 over the state space of a single model, it is intractable to solve over the joint space of states and infinite models X  X he mo del space is so large that standard point-based approximations will generally fail. Moreover , it makes little sense to find the optimal policy for all models when only a few models are likely. There fore, instead of solving 3 offline, we build a forward-looking search tree at each time step (see [15] for a review of forward search in POMDPs). The tree computes the value of action by investigat ing a number of steps into the future. The details of the action selection are discussed in section 3.2. 3.1 Belief Monitoring As outlined in section 3, we approximate the joint belief ove r states and models through a set of samples. In this section, we describe a procedure for sampli ng a set of models m = { T,  X  , R } from integrations over models that occur during planning; in the limit of infinite samples, the approxima-tions will be guaranteed to converge to their true values. To simplify matters, we assume that given a model m , it is tractable to maintain a closed-form belief b models need to be sampled, but beliefs do not.
 Suppose we have a set of models m that have been drawn from the belief at time t . To get a set of models drawn from the belief at time t +1 , we can either draw the models directly from the new belief or adjust the weights on the model set at time t so that they now provide an accurate representation of the belief at time t + 1 . Adjusting the weights is computationally most straightfo rward: directly following belief update equation 1, the importance weight w ( m ) on model m is given by: where  X ( o | m, a )= true model does not change.
 The advantage of simply reweighting the samples is that the b elief update is extremely fast. How-ever, new experience may quickly render all of the current mo del samples unlikely. Therefore, we must periodically resample a new set of models directly from the current belief. The beam-sampling approach of [16] is an efficient method for drawing samples fr om an iHMM posterior. We adapt this approach to allow for observations with different temporal shifts (since the reward r the state s by both the current state and the most recent action. The corr ectness of our sampler follows directly from the correctness of the beam sampler [16].
 The beam-sampler is an auxiliary variable method that draws samples from the iPOMDP posterior. A detailed description of beam sampling is beyond the scope o f this paper; however, we outline the general procedure below. The inference alternates between three phases: As with all MCMC methods, initial samples (from the burn-in period) are biased by sampler X  X  start position; only after the sampler has mixed will the samples b e representative of the true posterior. Finally, we emphasize that the approach outline above is a sa mpling approach and not a maximum likelihood estimator; thus the samples, drawn from the agen t X  X  belief, capture the variation over been made. Specifically, we are not filtering: each run of the b eam sampler produces samples from the current belief. Because they are drawn from the true post erior, all samples have equal weight. 3.2 Action Selection Given a set of models, we apply a stochastic forward search in the model-space to choose an action. The general idea behind forward search [15] is to use a forwar d-looking tree to compute action-values. Starting from the agent X  X  current belief, the tree b ranches on each action the agent might take and each observation the agent might see. At each action node, the agent computes its expected immediate reward R ( a ) = E From equation 3, the value of taking action a in belief b is action selection must be completed online, we use equation 4 to update the belief over models via small problems. We approximate the true value stochastical ly by sampling only a few observations from the distribution P ( o | a ) = where N Once we reach a prespecified depth in the tree, we must approxi mate the value of the leaves. For each model m in the leaves, we can compute the value Q ( a, b solving offline the POMDP model that m represents. We approximate the value of action a as This approximation is always an overestimate of the value, a s it assumes that the uncertainty over models X  X ut not the uncertainty over states X  X ill be resolved i n the following time step (similar to the QMDP [19] assumption). 7 As the iPOMDP posterior becomes peaked and the uncertainty o ver models decreases, the approximation becomes more exact.
 The quality of the action selection largely follows from the bounds presented in [20] for planning through forward search. The key difference is that now our be lief representation is particle-based; during the forward search we approximate an expected reward s over all possible models with re-wards from the particles in our set. Because we can guarantee that our models are drawn from the true posterior over models, this approach is a standard Mont e Carlo approximation of the expecta-tion. Thus, we can apply the central limit theorem to state th at the estimated expected rewards will be distributed around the true expectation with approximat ely normal noise N (0 ,  X  2 the number of POMDP samples and  X  2 is a problem-specific variance. We begin with a series of illustrative examples demonstrati ng the properties of the iPOMDP. In all experiments, the observations were given vague hyperpa rameters (1.0 Dirichlet counts per ele-ment), and rewards were given hyperparameters that encoura ged peaked distributions (0.1 Dirichlet counts per element). The small counts on the reward hyperpar ameters encoded the prior belief that Beliefs were approximated with sample set of 10 models. Mode ls were resampled between episodes and reweighted during episodes. A burn-in of 500 iterations was used for the beam sampler when drawing these models directly from the belief. The forward-search was expanded to a depth of 3. necessary states, ignoring the more complex (but irrelevan t) structure.
 Avoiding unnecessary structure: Lineworld and Loopworld. We designed a pair of simple envi-ronments to show how the iPOMDP infers states only as it can di stinguish them. The first, lineworld was a length-six corridor in which the agent could either tra vel left or right. Loopworld consisted lower branches. In both environments, only the two ends of th e corridors had unique observations. Actions produced the desired effect with probability 0.95, observations were correct with probability end of the corridor and received a reward of -1 until it reache d the opposite end (reward 10). never infers separate states for the identical upper and low er branches. By inferring states as they needed to explain its observations X  X nstead of relying on a pr especified number of states X  X he agent (unsurprisingly) learns optimal performance in both envir onments.
 Adapting to new situations: Tiger-3. The iPOMDP X  X  flexibility also lets it adapt to new situations . In the tiger-3 domain, a variant of the tiger problem [19] the agent had to choose one of three doors to open. Two doors had tigers behind them ( r =  X  100 ) and one door had a small reward ( r = 10 ). At each time step, the agent could either open a door or listen for the  X  X uiet X  door. It heard the correct door correctly with probability 0.85.
 The reward was unlikely to be behind the third door ( p = . 2 ), but during the first 100 episodes, we artificially ensured tha t the reward was always behind doors 1 or 2. The improving rewards in figure 4 show the agent steadily learning the dy-namics of its world; it learned never to open door 3. The dip in 4 following episode 100 occurs when we next allowed the reward to be behind all three doors, but the agent quickly adapts to the new possible state of its environment. The iPOMDP enabled the agent to first adapt quickly to its sim-plified environment but add complexity when it was needed.
 Broader Evaluation. We next completed a set of experi-ments on POMDP problems from the literature. Tests had 200 episodes of learning, which interleaved acting and re-sampling models, and 100 episodes of testing with the mod-els fixed. During learning, actions were chosen stochasti-cally based on its value with probability 0.05 and completel y randomly with probability 0.01. Oth-erwise, they were chosen greedily (we found this small amoun t of randomness was needed for ex-ploration to overcome our very small sample set and search de pths). We compared accrued rewards and running times for the iPOMDP agent against (1) an agent th at knew the state count and used backward-sampling (FFBS) algorithm used in the beam sampli ng inner loop to sample models, and (3) an agent that used FFBS with ten times the true number of st ates. For situations where the number of states is not known, the last case is particularly interes ting X  X e show that simply overestimating the number of states is not necessarily the most efficient sol ution.
 Table 1 summarises the results. We see that the iPOMDP often i nfers a smaller number of states than the true count, ignoring distinctions that the history does not support. The middle three columns show the speeds of the three controls relative the iPOMDP. Be cause the iPOMDP generally uses smaller state spaces, we see that most of these values are gre ater than 1, indicating the iPOMDP is faster. (In the largest problem, dialog, the oversized FFBS model did not complete running in several days.) The latter four columns show accumulated rewards; we see that the iPOMDP is generally on learning curve for one of problems, shuttle. Recent work in learning POMDP models include[23], which use s a set of Gaussian approximations to allow for analytic value function updates in the POMDP spa ce, and [5], which jointly reasons over the space Dirichlet parameter and states when planning in discrete POMDPs. Sampling based approaches include Medusa [4], which learns using state-qu eries, and [7], which learns using policy Figure 5: Evolution of reward for shuttle. During training ( left), we see that the agent makes fewer mistakes toward the end of the period. The boxplots on the rig ht show rewards for 100 trials after learning has stopped; we see the iPOMDP-agent X  X  reward dist ribution over these 100 trials is almost identical an agent who had access to the correct model.
 Table 1: Inferred states and performance for various proble ms. The iPOMDP agent (FFBS-Inf) often performs nearly as well as the agents who had knowledge of the true number of states (EM-true, FFBS-true), learning the necessary number of states m uch faster than an agent for which we overestimate the number of states (FFBS-big).
 Metric States Relative Training Time Performance Problem True FFBS-Tiger[19] 2 2.1 0.41 0.70 1.50 -277 0.49 4.24 4.06 Shuttle[21] 8 2.1 1.82 1.02 3.56 10 10 10 10 Network[19] 7 4.36 1.56 1.09 4.82 1857 7267 6843 6508
Gridworld[19] (adapted)
Dialog[22] (adapted) queries. All of these approaches assume that the number of un derlying states is known; all but [7] focus on learning only the transition and observation model s.
 In many problems, however, the underlying number of states m ay not be known X  X r may require significant prior knowledge to model X  X nd, from the perspecti ve of performance, is irrelevant. The knowledge is incorporated into the prior: for example, the D irichlet counts on observation param-eters can be used to give preference to certain observations as well as encode whether we expect observations to have low or high noise. As seen in the results , the iPOMDP allows the complex-ity of the model to scale gracefully with the agent X  X  experie nce. Future work remains to tailor the planning to unbounded spaces and refine the inference for POM DP resampling.
 Past work has attempted to take advantage of structure in POM DPs [24, 25], but learning that struc-ture has remained an open problem. By giving the agent an unbo unded state space X  X ut strong locality priors X  X he iPOMDP provides one principled framewo rk to learning POMDP structure. Moreover, the hierarchical Dirichlet process constructio n described in section 2 can be extended to include more structure and deeper hierarchies in the transi tions. We presented the infinite POMDP, a new model for Bayesian RL in partially observable domains. The iPOMDP provides a principled framework for an agent to po sit more complex models of its world as it gains more experience. By linking the complexity of the model to the agent X  X  experience, the agent is not forced to consider large uncertainties X  X hic h can be computationally prohibitive X  near the beginning of the planning process, but it can later c ome up with accurate models of the world when it requires them. An interesting question may als o to apply these methods to learning large MDP models within the Bayes-Adaptive MDP framework [2 6]. [1] R. Dearden, N. Friedman, and D. Andre,  X  X odel based Bayes ian exploration, X  pp. 150 X 159, [2] M. Strens,  X  X  Bayesian framework for reinforcement lear ning, X  in ICML , 2000. [4] R. Jaulmes, J. Pineau, and D. Precup,  X  X earning in non-st ationary partially observable Markov [5] S. Ross, B. Chaib-draa, and J. Pineau,  X  X ayes-adaptive P OMDPs, X  in Neural Information Pro-[6] S. Ross, B. Chaib-draa, and J. Pineau,  X  X ayesian reinfor cement learning in continuous [7] F. Doshi, J. Pineau, and N. Roy,  X  X einforcement learning with limited reinforcement: Using [8] M. J. Beal, Z. Ghahramani, and C. E. Rasmussen,  X  X he infini te hidden Markov model, X  in [9] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei,  X  X ierarc hical Dirichlet processes, X  Journal [10] J. V. Gael and Z. Ghahramani, Inference and Learning in Dynamic Models , ch. Nonparametric [11] Y. W. Teh,  X  X irichlet processes. X  Submitted to Encyclo pedia of Machine Learning, 2007. [12] J. Pineau, G. Gordon, and S. Thrun,  X  X oint-based value i teration: An anytime algorithm for [14] T. Smith and R. Simmons,  X  X euristic search value iterat ion for POMDPs, X  in Proc. of UAI [15] S. Ross, J. Pineau, S. Paquet, and B. Chaib-Draa,  X  X nlin e planning algorithms for POMDPs, X  [16] J. van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani,  X  X ea m sampling for the infinite hidden [17] R. Neal,  X  X lice sampling, X  Annals of Statistics , vol. 31, pp. 705 X 767, 2000. [18] C. K. Carter and R. Kohn,  X  X n Gibbs sampling for state spa ce models, X  Biometrika , vol. 81, [20] D. McAllester and S. Singh,  X  X pproximate planning for f actored POMDPs using belief state [21] L. Chrisman,  X  X einforcement learning with perceptual aliasing: The perceptual distinctions [22] F. Doshi and N. Roy,  X  X fficient model learning for dialog management, X  in Proceedings of [23] P. Poupart and N. Vlassis,  X  X odel-based Bayesian reinf orcement learning in partially observ-[24] J. H. Robert, R. St-aubin, A. Hu, and C. Boutilier,  X  X PUD D: Stochastic planning using decision [25] A. P. Wolfe,  X  X OMDP homomorphisms, X  in NIPS RL Workshop , 2006. [26] M. O. Duff, Optimal learning: computational procedures for Bayes-ada ptive markov decision
