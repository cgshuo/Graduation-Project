 Several Text Categorization applications require a represen-tation beyond the standard bag-of-words paradigm. Kernel-based learning has approached this problem by (i) consid-ering information about syntactic structure or by (ii) incor-porating knowledge about the semantic similarity of term features. We propose a generalized framework consisting of a family of kernels that jointly incorporate syntactic and semantic similarity and demonstrate the power of this ap-proach in a series of experiments.
 Categories and Subject Descriptors: I.2.6 Artificial Intelligence: Learning, H.3.1 Information Storage and Re-trieval: Content Analysis and Indexing.
 General Terms: Algorithms, Theory, Experimentation. Keywords: Machine Learning, Text Classification, Kernel Methods
The prevailing paradigm for representing documents for text mining tasks is the bag-of-words model. Here, input documents are encoded as vectors whose dimensions corre-spond to the terms in the training corpus. Inner product or cosine between two vectors are used as kernel, hence mak-ing the similarity of two documents dependant only on the amount of terms they share. This simple approach has pro-duced good results in cases of sufficient training data. The shortcomings of this kind of representation are that (i) it does not encode the syntactic structure of the input text and (ii) builds on a very rough approximation of lexical se-mantics. As a consequence, the resulting models cannot take advantage of complex patterns, e.g. a verbal phrase built on identical (or similar) verbs, and are not sufficiently robust with respect to variations in terminology. Recently, there has been increased interest in incorporating a-priori
Parts of this work were funded by the European Commis-sion as part of the X-Media project under EC grant number IST-FP6-026978.
 knowledge about syntactic structure or semantic similar-ity within kernel-based learning algorithms such as Support Vector Machines by means of a specific choice of the em-ployed kernel function [ 9 ]. For the case of syntactic struc-ture, Tree Kernels [ 3 , 8 ] have been proposed as a framework for exploiting the parse trees of the input texts. For the case of lexical semantics, Semantic Kernels exploit background information from semantic networks such as WordNet [ 10 , 7 , 1 ] or from statistical models of term co-occurrence [ 4 ] to make different, though semantically similar, terms con-tribute to the overall similarity of the input tokens.
While both approaches seem intuitive and powerful, nat-ural language draws from both, syntax and semantics and finding principled techniques for approaching both aspects within a unified framework appears to be a promising re-search line. We present a framework for Semantic Syntactic Tree Kernels (SSTKs) which built on linguistic structure and semantic similarity at the same time. Based on our own earlier results in [ 2 ], we introduce two new components, namely an embedded semantic term kernel and a leaf weight-ing component, to improve the matching of tree fragments containing terminal nodes. We propose two models for the design of semantic term kernels, one based on taxonomic background knowledge and one based on latent semantics. We demonstrate the power of the new class of kernels in a set of experiments dealing with the problem of classifying natural language questions from a TREC question answer-ing dataset. Our experiments show that the new models outperform both, bag-of-words kernels and the original tree kernels.
Support Vector Machines (SVMs) are state-of-the-art learning methods based on the principle of linear classifi-cation. SVMs and other kernel-based learning techniques can naturally incorporate domain-specific notions of item similarity by means of a corresponding kernel function . Formally, any function  X  that for all x, z  X  X satisfies put domain under consideration and  X  is a suitable mapping from X to a feature (Hilbert-) space F . In the remainder of this section we only focus on tree kernels, for further infor-mation on SVMs and kernel methods the reader may refer to [ 9 ].
We define a tree as a connected directed graph with no cy-cles. Trees are denoted as T 1 , T 2 , . . . ; tree nodes are denoted Figure 1: A simple parse tree with some fragments. as n 1 , n 2 , . . . ; and the set of nodes in tree T i are denoted as N T i . We denote the set of all substructures (fragments) that occur in a given set of trees as { f 1 , f 2 , . . . } = F . As the structures we will work with are parse trees, each node with its children is associated with a grammar production rule. The labels of the leaf nodes of the parse trees correspond to terms, i.e. terminal symbols , whereas the preterminal sym-bols are the parents of leaves. Figure 1 shows an example of a parse tree with some of its substructures. Tree Kernels have been designed based on the idea of efficiently counting the number of tree substructures that are common to both argument trees.

Definition 1. Given two trees T 1 and T 2 , we define the (Subset-) Tree Kernel as: whereby  X ( n 1 , n 2 ) = an indicator function which determines whether fragment f is rooted in node n .

The value of  X  is equal to the number of common frag-ments rooted at nodes n 1 and n 2 . A naive enumeration of all tree fragments is computationally problematic as the number of substructures grows exponentially in the num-ber of nodes of the input trees. A recursive computation introduced in [ 3 ] allows to compute  X  efficiently: where nc ( n 1 ) is the number of children of n 1 , ch j n child of node n and  X  is the decay parameter, i.e.  X  = 1 yields the number of common substructures whereas  X  &lt; 1 penalizes larger tree structures by giving them less weight in the summation.
Plain tree kernels rely on the intuition of counting com-mon substructures of two trees. If two trees have similar substructures that employ different (though related) termi-nology, they will not contribute to the kernel result. From a semantic point of view, this is an evident drawback as re-lated terminology should still allow for a (possibly smaller) contribution.
Recent research work in the direction of Semantic Ker-nels has looked at ways to allow for partial matches between bag-of-words vector components [ 10 , 7 , 1 ]. Similarly, we are now interested in also counting partial matches between tree fragments. A partial match occurs when two fragments dif-fer only by their terminal symbols, e.g. [N [pneumonia]] and [N [infection]] . In this case the match should give a contribution smaller than 1 , depending on the semantic similarity of the respective terminal nodes. To ensure the validity of the kernel, this similarity needs to be a valid ker-nel on the terms itself and will be denoted by  X  S .
Definition 2. For two tree fragments f 1 , f 2  X  X  , we define the Tree Fragment Similarity Kernel as: where the compatibility function, comp ( f 1 , f 2 ) , is 1 if f f 2 are identical except for the terminal nodes and 0 other-wise; nt ( f i ) is the number of f i  X  X  leaves; and f i ( t ) indicates the t-th leave of f i (from left). It should be noted that, (a) as the tree fragments need to be compatible, they have the same number of terminal symbols; (b) the fragment simi-larity is 0 if the fragments are incompatible otherwise it is the product of the semantic similarities of all corresponding terminal nodes (i.e. sitting at identical positions). There-fore, it is maximal if corresponding leaves have a maximum similarity.

The Semantic Syntactic Tree Kernel is the sum of  X  F over all pairs of tree fragments derivable from two input trees as shown by the following Definition 3. Given two trees T 1 and T 2 , the Semantic Syntactic Tree Kernel is evaluated by: where  X ( n 1 , n 2 ) =
The naive computation of this kernel is exponential as all compatible pairs of tree fragments would need to be con-sidered in the summation. However, it can be efficiently evaluated by changing step 1 and 2 of  X  computation in Section 2.1 as follows: where label ( n i ) denotes the label of node n i .
Up to now, we have regarded the term similarity kernel as a black-box. We now look at a way, proposed in [ 1 ] to encode such kernels by means of taxonomic background knowledge structures such as WordNet. We denote terms as t i  X  T and concepts as c i  X  C . According to an idea investigated in [ 1 ], we model the similarity of two terms as a dot product of their respective superconcept vectors.
Since n 1 and n 2 are pre-terminals they have only one child (a term).
Definition 4. The Superconcept Kernel  X  S for two con-cepts c i , c j  X  C is given by  X  S ( c i , c j ) =  X  SC ( c whereby SC (  X  ) is a function C  X  R |C| that maps each con-cept to a real vector whose dimensions correspond to super-concepts present in the employed semantic network and the respective entries are determined by a particular weighting scheme.

In our work, we have used the following measures of term similarity as weighting schemes: (i) Wu &amp; Palmer, i.e. sim WUP ( c 1 , c 2 ) = 2 dep ( lso ( c 1 , c 2 )) / d ( c i.e. sim LIN ( c 1 , c 2 ) = 2 log P ( lso ( c 1 , c 2 )) / log P ( c log P ( c 2 ) ; (iv) no weighting (full superconcept vectors), where d denotes the number of superconcept edges between c and c 2 (distance), dep is the distance of the concept to the unique root node (depth) and lso is the lowest super ordinate (lso) of two concepts. The probability P ( c ) of en-countering a concept c is estimated from corpus statistics.
The taxonomy kernel appears to be an accurate indicator of similarity of terms but requires the existence of a tax-onomy. As an alternative, work in the direction of latent semantic indexing (LSI) [ 5 , 4 ] investigated means for calcu-lating term similarities using co-occurrence analysis of terms in documents and vice versa. Given a term-by-document matrix M , the singular value decomposition of M is given by M = U  X  V 0 where  X  is a diagonal matrix with the same dimensionality as D containing the singular values in de-creasing arrangement and U,V are orthogonal matrices. The columns of U are the singular vectors of the feature space corresponding to the respective singular value. A projection onto the first k dimensions is given by U k = I k U , where I the identity matrix, i.e. all but the first k diagonal elements are zero.

Definition 5. The latent semantic similarity kernel of terms t i and t j is given as  X  LSI S =  X  U i k ( U j k ) is the i-th (row) vector of the truncated matrix U k .
In these experiments, we aim at showing that our ap-proach is effective and at investigating how particular de-sign/parameter choices affect performance.
We implemented the kernels introduced in Section 3.1 within the SVM-Light-TK software 2 which encodes tree ker-nel functions in SVM-Light. We either used the noun hier-archy of WordNet as the underlying semantic network for calculating topological term similarity kernels or LSI-based term similarity kernels 3 .

The parameter  X  downweights the contribution of larger tree structures, whereas, to increase the contribution given by similar leaves, we introduce an additional  X  parameter in the  X  computation as follows: http://ai-nlp.info.uniroma2.it/moschitti/
For word sense disambiguation, we used a simplifying as-sumption in mapping each term to its most frequent noun sense (if it exists). Undefined similarities (e.g. because of a missing mapping to a noun synset) were assumed to take the default values (i.e. o for distinct and 1 identical terms).
Question Classification [ 6 ] aims at detecting the type of a question, e.g. whether it asks for a person or for an or-ganization which is critical to locate and extract the right answers in question answering systems. A major challenge of Question Classification compared to standard Text Classifi-cation settings is that questions typically contain extremely few words which make this setting a typical victim of data sparseness. We consider the same dataset and classification problem as introduced in [ 6 , 11 ]. The dataset consists of free text questions from TREC and is freely available 4 . It is di-vided into 5,500 questions for training and 500 questions for testing. Each of these questions is labeled with exactly one class of the coarse grained classification scheme consisting of 6 classes.
We used the same experimental setup used in [ 11 ] and compared the linear kernel based on bag-of-words 5 , the orig-inal tree kernel, and a set of semantic syntactic tree ker-nel configurations with different (normalized) term similari-ties. The 6 binary classifiers 6 (one for each class) were com-bined in a multiclassification scheme, which selects the single class for which the test instance produces the highest mar-gin score of the binary SVMs. For all cases, we report the micro-averaged F 1 , i.e. the harmonic mean between micro-averaged precision and recall, and the multi-classifier accu-racy (since only one class should be assigned to a question).
Table 1 reports the results of our experiments The first column indicates the value of  X  . The second column shows the type of term similarity kernel used together with the tree kernel function, where string matching means that the original tree kernel is used. The remaining columns report the micro-averaged F 1 and the multiclassification accuracy.
We note that: http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/
For a fair comparison, we did not remove any stopwords for bag-of-words as function words like  X  X hat X  or  X  X ho X  are highly informative for the QC task.
For each of them, we preliminary selected the best cost-factor (parameter j ) on the validation set and then exper-imented with different  X  values. We always used c = 1 as soft-margin parameter.
This last result is relevant as it improves (i) previous work on question classification using tree kernels, i.e. 90% in [ 11 ], (ii) our own preliminary results [ 2 ] and (iii) the results ob-tained in [ 6 ], i.e. 92.5%, using many features and semantic resources manually annotated for such question dataset. Fi-nally, we have noted that other values of  X  parameter do not improve the results any further.
We have investigated how the syntactic structures of nat-ural language texts can be exploited simultaneously with se-mantic background knowledge on term similarity. We have proposed Semantic Syntactic Tree Kernels (SSTKs) based on Trees and Semantic Smoothing Kernels. To our knowl-edge, no other work has so far combined the syntactic and semantic properties of natural language in such a principled way. Beside the novelty of taking into account tree frag-ments that are not identical, it should be noted that the lexical semantic similarity is now constrained by syntactic structures, which decrease the word sense ambiguity. This is a remarkable improvement on the usual application of lexi-cal semantic similarities based on term vectors and semantic smoothing kernels.

The experiments that we conducted on the TREC ques-tion classification data show that our new Syntactic Se-mantic Tree Kernel (SSTK) improves the state-of-the-art in Question Classification. This suggests that SSTK may constitute a prototype for a full-fledged natural language kernel. [1] S. Bloehdorn, R. Basili, M. Cammisa, and [2] S. Bloehdorn and A. Moschitti. Combined syntactic [3] M. Collins and N. Duffy. Convolution kernels for [4] N. Cristianini, J. Shawe-Taylor, and H. Lodhi. Latent [5] S. Deerwester, S. T. Dumais, T. K. Landauer, G. W. [6] X. Li and D. Roth. Learning question classifiers. In [7] D. Mavroeidis, G. Tsatsaronis, M. Vazirgiannis, [8] A. Moschitti. Efficient convolution kernels for [9] J. Shawe-Taylor and N. Cristianini. Kernel Methods [10] G. Siolas and F. d X  X lche Buc. Support vector [11] D. Zhang and W. S. Lee. Question classification using
