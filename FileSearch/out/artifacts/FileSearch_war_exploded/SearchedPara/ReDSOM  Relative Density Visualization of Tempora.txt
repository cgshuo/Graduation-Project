
We introduce a Self-Organizing Map (SOM) based visu-alization method that compares cluster structures in tem-poral datasets using Relative Density SOM (ReDSOM) visualization. Our method, combined with a distance matrix-based visualization, is capable of visually identi-fying emerging clusters, disappearing clusters, enlarging clusters, contracting clusters, the shifting of cluster cen-troids, and changes in cluster density. For example, when a region in a SOM becomes significantly more dense com-pared to an earlier SOM, and well separated from other regions, then the new region can be said to represent a new cluster. The capabilities of ReDSOM are demonstrated us-ing synthetic datasets, as well as real-life datasets from the World Bank and the Australian Taxation Office. The results on the real-life datasets demonstrate that changes identified interactively can be related to actual changes. The identi-fication of such cluster changes is important in many con-texts, including the exploration of changes in population be-havior in the context of compliance and fraud in taxation.
Businesses and government organizations need knowl-edge of change in order to adapt their strategies to ever-changing environments. Knowing what has changed can be a major competitive advantage for an organization. To un-derstand what has changed, analysts have to be able to relate new knowledge or models acquired from a newer dataset to those acquired from an earlier dataset. Without this con-text, it can be difficult to revise existing strategies. This is particularly problematic if an organization has already im-plemented a strategy based on an earlier model.

In supervised learning, classifier performance often de-grades over time, an issue known as concept drift [19, 23]. In many real-life domains, a concept of interest may de-pend on some hidden context , which is not given explicitly in the form of predictive features (i.e. some variables are invisible to the learner). For example, such hidden concepts can be changes in economic policy, disasters, life events, or changes in marketing strategies. Changes in the hidden context can induce more or less radical changes in a target concept. Most research in concept drift only addresses con-cept drift in a supervised learning context X  X ittle has been researched in the context of unsupervised learning.
In data mining of conceptual changes, a number of tem-poral data mining algorithms have focused on detecting the point in time when something has changed (change detec-tion), rather than understanding or exploring the causes that have made the changes (change analysis). For example, by gradually eliminating the effects of past data, an on-line discounting learning algorithm can detect outliers and the change points in time in a changing data source [25].
To discover changes between two datasets, the resulting data mining models can be compared, given that a data min-ing model is designed to capture specific characteristics of a dataset. A theoretical framework has been introduced in [7] that allows measuring changes between two models. In this framework, when the structural components of the models are different, both structures are  X  X xtended X  to a greatest common refinement. The deviation between two models is then calculated by aggregating the differences in the mea-surement components of the models.

This paper focuses on visualizing, identifying, and ana-lyzing changes in cluster structures in a SOM, trained with a newer dataset, compared to a SOM trained with an older dataset. Temporal cluster analysis can be useful to under-stand changes in datasets, or to review the effectiveness of deployed strategies. For example, if an organization has de-vised a marketing strategy based on a clustering of the past year X  X  customer data, it is important to know if the current year X  X  clustering differs, in order to understand changes in customer behavior, or to review the effectiveness of the im-plemented marketing strategy. New strategies can then be devised to encourage or deter the development of new clus-ters or to slow the demise of clusters, as suits the require-ments of the business.

ReDSOM visualization allows users to explore the dis-tinctive features of changes interactively using the hot-spot methodology [6]. Involving the user in the data exploration process is important in ensuring effective data analysis [12].
We propose the use of SOMs for effectively visualizing changes. SOMs have several advantages in temporal clus-ter analysis. They are able to relate clustering results by linking multiple visualizations, and they can detect various types of cluster changes, including emerging and disappear-ing clusters [5]. Furthermore, SOMs create a smaller but representative dataset, and they have topology preservation properties [15]. Importantly, SOMs can be used to explore high-dimensional data spaces through a non-linear projec-tion onto a two-dimensional (2-D) plane using visualiza-tions that are easy to understand even by non-analysts [15]. A mathematical analysis of SOM properties can be found in [17]. Applications of SOM for data mining are found in engineering, speech analysis and recognition, finance, and information retrieval [4, 15].

We contrast our ReDSOM visualization methodology to the goals of time series clustering [13]. Time-series clus-tering aims to cluster entities that have similar time-series patterns, whereas our method clusters entities at points in time (snapshots), and compares the clustering structures of such snapshots.

Research in data stream mining has also provided some insights into the problem described here. Aggarwal et.al. [2] presented a framework for clustering data streams where the aim is to discover changes in the evolving data streams. The basic idea of the approach is to divide clustering into an on-line and off-line component, with the online component pe-riodically storing summary information of the data stream clusters (so called micro-clusters), and the off-line compo-nent generating aggregated clusters according to the needs of an analyst. Our work does not consider data streams but investigates snapshot datasets that were collected at differ-ent points in time. While data streams are becoming com-mon in many application areas, static snapshot datasets are still the most commonly used type of data in many orga-nizations. There still remains a need to analyze changes between such snapshots.

The main contribution of this paper is the development of Relative Density SOM (ReDSOM) visualization that can compare and contrast changes in cluster structures in tem-poral datasets. ReDSOM allows analysts to explore and understand changes interactively. We evaluate our method on synthetic datasets and on real-life datasets published by the World Bank [24]. We have also evaluated our method on large real-life datasets from the Australian Taxation Of-fice. The results on the real-life datasets demonstrate that changes identified interactively can be related to actual real-life changes.

The remainder of the paper is organized as follows. The next section discusses related work in temporal cluster anal-ysis. An overview of Self-Organizing Maps is provided in Section 3. Section 4 introduces our relative density defini-tion and ReDSOM visualization. The results of our experi-ments are then discussed in Section 5, and conclusions and future work are provided in Section 6.
Temporal data can be grouped into four broad categories: static, sequences, time-stamped, and fully temporal [18]. In static datasets, temporal context is not included and cannot be inferred. Sequences are basically ordered lists of events, but not time-stamped. Examples of time-stamped datasets are census data, web-based activity, or sales transaction. In fully temporal databases, each tuple in a time-varying rela-tion in the database may have one or more dimensions of time, such as age and treatment time. Our method analyzes changes in two or more static temporal datasets.

Chakrabarti, Kumar, and Tomkins [3] defined evolution-ary clustering as the problem of processing time-stamped data to produce a sequence of clusterings; that is, a clus-tering for each time step. This framework tries to opti-mize two potentially conflicting criteria: remaining faith-ful to the current data, and not shifting dramatically from the previous clustering results. Therefore, the user has to define a snapshot quality function sq ( C t ,D t ) which mea-sures the quality of a clustering result C t for dataset D at time t , and a history cost function hc ( C t  X  1 ,C t measures how much a latter clustering result C t differs from the previous one, C t  X  1 . The optimal cluster sequence can, therefore, be found by determining at each time step t a clustering C t that optimizes the incremental quality sq ( C t ,D t )  X  cp  X  hc ( C t  X  1 ,C t ) , where cp is a non-negative change parameter. As cp is increased, more weight is placed on matching the historical clusters. Based on this, the au-thors derived an agglomerative hierarchical and a k -means clustering algorithm. When calculating clustering result C using k -means, for example, the previous cluster centroids of C t  X  1 are used as the starting seeds. The new centroids are then calculated based on the closest match of cluster centroids in the previous clustering result, and the cluster centroids of the non-evolutionary (conventional) k -means.
This framework is able to find a balance between remain-ing faithful and not shifting dramatically in training the sub-sequent clustering results, in order to smooth the clustering sequence. However, the aim of this framework is not to an-alyze the changes of the clustering results. It is not easy to understand the actual cluster changes using plots of snap-shot quality and historical cost over time. It is not clear how to relate and understand changes in terms of the earlier clus-tering results. Furthermore, the capability of the framework to detect any rate of changes (abrupt or gradual changes) is questioned as the change parameter cp is constant over time and has to be defined beforehand.

The k -means version of the framework also has some is-sues related to the discovery of new or lost clusters. With the proposed k -means variant, the previous cluster centroids C t  X  1 are used as starting seeds. A problem arises when there is a larger number of cluster ( k ) selected in finding the latter clustering result C t . It is not clear how to initial-ize the additional cluster centroids in this case. A similar problem occurs when a smaller k is selected. In this case, one or more cluster centroids from the previous clustering result C t  X  1 have to be removed in finding the latter clus-tering result C t . Our method, on the other hand, is able to show emerging clusters and lost clusters without having to determine the number of clusters beforehand.

Hido et.al. [9] proposed an approach to explain changes between two datasets by using a decision tree, and label-ing one dataset as negative and the other as positive. The trained model is then investigated to understand differences between the datasets. As decision trees divide the data space into hypercubes and try to separate the entities based on their labels, this approach can detect when there is a new hypercube that was more sparsely occupied by the other dataset. However, this method cannot show the separa-tion between the hypercubes and the density of these hyper-cubes. Therefore, this method cannot differentiate between emerging clusters and cluster enlargements. In our method, on the other hand, separation between prototype vectors can be shown using distance matrix visualization [10]. Further-more, when correlated attributes exist in the dataset, only one of them will be used to describe new or lost hypercubes, reducing the description of the hypercubes.

Recently, Adomavicius and Bockstedt [1] introduced a graph visualization technique for exploring trends in multi-attribute temporal datasets using a temporal cluster graph. In this technique, transactional dataset D is partitioned into data subsets D t according to time periods. Each data sub-set is then clustered. Only clusters that have at least  X  | D number of entities are shown as nodes, where  X   X  [0 , 1] is a node filter parameter. Nodes between two adjacent time periods are connected with an edge, if the distance be-tween the nodes is less than a threshold  X  , which is calcu-lated based on an edge filter parameter  X  and the average between-cluster distances. This graph is visualized interac-tively, where users have to experiment with the number of clusters for each partition D t , node filter parameter  X  , and edge filter parameter  X  . In our method, users do not have to experiment to find the optimal number of clusters. For detecting changes between two SOMs, Kaski and Lagus [11] proposed a dissimilarity measure for two maps, which is calculated based on the expected value of distances between pairs of representative data points on both maps. This approach can determine how much two SOMs differ, but it cannot analyze the changes.

Lingras et.al. [16] studied the temporal cluster character-istics of supermarket customers using an interval set based-SOM. The capability of the proposed method to detect new clusters is questioned as the number of clusters was constant for all time periods in their experiments.

Denny and Squire [5] proposed a SOM training method and SOM-based visualization techniques that are capable of explaining the clustering results of a second dataset in terms of the clustering result of a first dataset by using color and position linking. These visualization techniques can relate two clustering results which can show the following struc-tural changes: changes in cluster size, centroid movements, new clusters, cluster splitting, missing clusters, and clus-ter merging. Such changes have to be analyzed visually by selecting a number of clusters for both datasets. How-ever, these techniques cannot differentiate between cluster enlargement (occupying new space), and increase in cluster density (more entities in the same space).
A SOM is an artificial neural network that performs un-supervised competitive learning [14]. Artificial neurons are arranged on a low-dimensional grid, commonly a 2-D plane with n r rows, n c columns, and a total number of n unit = n r  X  n c units. Each neuron j has an d -dimensional prototype vector, m j , where d is the dimensionality of dataset D . Each neuron is connected to neighboring neu-rons, with distances to its neighbours being equidistant in the map space. The lattice structure can be a hexagonal grid, where each neuron is connected to six neighbours. Larger maps generally have higher accuracy and generalization ca-pability [22], but they also have higher computation costs.
Before training a map, the prototype vectors should be initialized using random initial values, or ordered values (linear initialization) [15]. Linear initialization uses ordered values of component vectors based on the first two largest principal components. When using random initialization, the radius of the neighbourhood function should be large enough; otherwise the map will not be globally ordered. However, if linear initialization is used for the initial map, then a smaller radius and a shorter training length could be used [15]. Therefore, linear initialization is preferred over random initialization, because it can speed up the learning process by orders of magnitude [15].

At each training step t , the best matching unit b i (BMU) for training data vector x i , i.e. the prototype vector m est to the training data vector x i , is selected from the map according to Equation 1:
In the batch training algorithm [15], the values of new prototype vectors m j ( t + 1) are weighted averages of the training data vectors x i , where the weight is the neighbour-hood kernel value h b i j centered on the best matching unit b . Since the neighbourhood function h b i ,j value is the same for all data vectors mapped to the same unit, the sum S V of the Voronoi set V j of each prototype vector m j at train-ing step t . So, each training data vector belonging to the Voronoi set of its closest prototype vector (BMU), can be calculated first using Equation 2: Then, the prototype vectors are updated with: where h ji is the neighbourhood kernel function centered on unit j (commonly Gaussian) [15], and n V j is the number of training data vectors in Voronoi set V j . To handle missing values, S V j and n V j only perform summation and count-ing of non-missing components, respectively. This calcula-tion of new prototype vectors takes into account neighbor-ing prototype vectors which preserve the topological order.
The map is usually trained in two phases: a rough train-ing phase and a fine tuning phase. The rough training phase usually has shorter training length and larger initial radius compared to fine tuning phase [15].

SOMs are popularly used in cluster analysis because they perform vector quantization and preserve topological or-der. Furthermore, the trained SOMs can be visualized using various methods that allow non-technical users to explore a dataset. Component plane visualizations can be used to show the spread of values of a certain component of all pro-totype vectors in a SOM [21]. Distance-matrix based vi-sualizations, such as u-matrix visualization [10], show dis-tances between neighboring nodes using a color scale rep-resentation on a map grid. This visualization can be used to identify borders between clusters, where long distances show highly dissimilar features between neighboring nodes that divide clusters, i.e. the dense parts of a map with similar features [10].

In [22], the prototype vectors of a trained SOM can be treated as  X  X roto-clusters X  serving as an abstraction of the dataset. The prototype vectors are then clustered using a traditional clustering technique, such as k -means, to form the final clusters. In this two-level clustering, adding an extra layer simplifies the clustering task and reduces noise, but may yield higher distortion [22].
Let D (  X  1 ) be a dataset at time  X  1 , and D (  X  2 ) be a dataset at time  X  2 , where  X  1 &lt;  X  2 . In order to be able to compare two maps that are trained using datasets D (  X  1 ) and D (  X  the orientation of map M (  X  1 ) and M (  X  2 ) must be the same. Therefore, the following map training procedure, as pro-posed in [5], is used. 1. Normalize both datasets D (  X  1 ) and D (  X  2 ) using the 2. Initialize map M (  X  1 ) using ordered values. 3. Train map M (  X  1 ) using dataset D (  X  1 ) . 4. Initialize map M (  X  2 ) using the prototype vectors of the 5. Train map M (  X  2 ) using dataset D (  X  2 ) .
As a SOM follows the distribution of a dataset it is trained on, more prototype vectors are allocated for dense regions, as shown in Figure 1. Therefore, area density at m j (  X  1 ) on its own map, M (  X  1 ) , might be different com-pared to area density at the same location on map M (  X  2 When the area density at the location of the prototype vec-tor m j (  X  1 ) in D (  X  2 ) is more sparse, the area density on map M (  X  2 ) is lower, compared to the area density at the same lo-cation on map M (  X  1 ) , and vice-versa. In Figure 1, the area density at the marked area (the center of the Gaussian ker-nel contour) on the left plot is higher, compared to the area density at the same location on the right plot. vectors is shown on both plots.

We define area density  X  M (  X  ) ( v ) at the location of a vec-tor v on map M (  X  ) as the weighted sum of similar proto-type vectors m j (  X  ) on M (  X  ) centered on vector v , where the weight is calculated based on a Gaussian kernel func-tion centered on vector v , as shown in Equation 4: As the radius r can be different for different maps (e.g. datasets with large attribute value ranges will need to have a larger radius), the radius should be determined based on  X  X etween neighbour distances X  on the map. To adapt the radius for different maps, the quartile (e.g. third quartile) of these distances is used as the radius. As the area den-sity is normalized into a relative density (Equation 5), this relative density is not sensitive to the radius. However, the radius should not be too small or too large. If the radius is too small, the relative density will be too sensitive to noise. On the other hand, if the radius is too large, then relative density cannot capture the details of changes.

We define relative density RD M (  X  of the area density at location of vector v on map M (  X  2 the area density at the same location on the reference map M (  X  1 ) , as shown in Equation 5: Without using a logarithm function in Equation 5, values between 0 and 1 are interpreted as becoming more sparse, the value of 1 is interpreted as no change, and values above 1 are interpreted as becoming more dense. Therefore, a base two logarithm is used to make it easier to interpret the ra-tio where positive values are interpreted as becoming more dense, negative values as becoming more sparse, and 0 is interpreted as no change. For example, a value of +2 is in-terpreted as the area centered at the location of vector v in the dataset D (  X  2 ) being four times more dense compared to the same area in the reference dataset D (  X  1 ) . Based on our observations, when the value of cation of vector v is no longer occupied in the next map M (  X  2 ) (it is lost). Similarly, when RD M (  X  greater than +3 , then the space at location of vector v was not occupied on the reference map M (  X  1 ) . In other words, the space is only occupied on map M (  X  2 ) .

Both Equations 4 and 5 are performed on all the proto-type vectors of both maps, but not on the actual data vectors. Therefore, the running time of the calculation for a map is quadratic in the number of map units n u , not in the number
As a shorthand, let rd 1  X  RD M (  X  rd
To visualize the relative density of the locations of all prototype vectors m j (  X  1 ) , the values of rd 1 are visualized on a map M (  X  1 ) in a gradation of blue for positive values and in a gradation of red for negative values 1 , as shown in the left map in Figure 2. Values over +3 are represented as dark blue, and values under  X  3 are represented as dark red. A value of 0 is represented as white, as it indicates no change in the density. For example, visualizations of the datasets and prototype vectors from Figure 1 can be seen in Figure 2.

However, rd 1 rarely returns values greater than +3, be-cause all prototype vectors m j (  X  1 ) are always present on the reference map M (  X  1 ) (prototype vector m j (  X  1 ) is part of map M (  X  1 ) ). Therefore,  X  M (  X  turns 1 , which makes the denomination part of Equation 5 larger compared to  X  M (  X  rd 2 should be visualized on map M (  X  2 ) to detect emerging clusters, as shown in the right map in Figure 2. To detect new clusters, rd 2 is used because the area at the location of prototype vector m j (  X  2 ) might be empty on map M (  X  Similarly, rd 2 cannot be used to detect lost clusters on map M (  X  2 ) , because the empty space at the location of the pro-totype vector m j (  X  1 ) is not represented on map M (  X  M (  X  2 ) follows the distribution of dataset D (  X  2 ) .
Because of SOM X  X  vector quantization property and our definition of relative density, prototype vectors on map M (  X  1 ) that have negative rd 1 values will be less repre-sented on map M (  X  2 ) . For example, there are less prototype vectors in cluster  X  X  X  in the right map in Figure 2. On the other hand, prototype vectors on map M (  X  1 ) that have pos-itive rd 1 values will be more represented on map M (  X  2 shown in cluster  X  X  X  in the right map in Figure 2.
There are several types of possible structural changes be-tween two datasets: new clusters, lost clusters, cluster en-largements, cluster contractions, shifting of centroids, and changes in cluster density. All these structural changes can be identified using ReDSOM and distance matrix visualiza-tions. As mentioned before, distance-matrix based visual-izations show distances between neighboring nodes using a color scale representation on the map, which can be used to identify cluster borders [10]. For example, in Figure 3, there are four clusters in both datasets (light yellow regions) separated by long distances. Identifying new clusters. New clusters in dataset D (  X  2 can be identified by dark blue regions (values above +3) on relative density visualization rd 2 of map M (  X  2 they have long distances at the border of the regions on map M (  X  2 ) , for example cluster  X  X  X , as shown in the right maps in Figures 2 and 3. When a new cluster appears inside the distribution of dataset D (  X  1 ) , in other words between disjoint clusters, the values of rd 1 are close to +3, and the cluster is positioned in a sparse area (long distances in the distance matrix visualization between the clusters). This is due to interpolative units, which appear when the data clus-ters are disjoint [22], as can be seen in both plots in Figure 1. Therefore, the area density at this gap is higher, compared to the area density at the empty space outside the data dis-tribution.
 Identifying cluster enlargements, cluster contractions, movement of cluster centroids. When a new dense area emerges in dataset D (  X  2 ) , but it does not have a good sep-aration to its neighbour, the changes can be interpreted as cluster enlargements. This can be identified by dark blue regions on the relative density visualization rd 2 , and the region has short distances at the border of the regions on map M (  X  2 ) , as shown in the bottom-right corner of the right maps in Figure 4. When this kind of new region appears at the border of map M (  X  2 ) , it can be said that the changes move towards the tail of the data distribution. Similarly, cluster contraction can be identified by a lost region, but it does not have a good separation to its neighbours. Move-ment of cluster centroids can be identified by simultaneous cluster enlargements and cluster contractions.
 Identifying lost clusters. Lost clusters in dataset D (  X  can be identified by dark red regions (value below -3) on the relative density visualization rd 1 of map M (  X  1 long distances at the border of the regions on map M (  X  1 An example is cluster  X  X  X  in the left maps in Figures 2 and 3. Identifying change of cluster density. An increase of cluster density in dataset D (  X  2 ) can be identified in the rel-ative density visualization as light blue, for example cluster  X  X  X  as shown in the left map in Figure 2. On the other hand, a decrease of cluster density in dataset D (  X  2 ) can be iden-tified in the relative density visualization as light red, for example cluster  X  X  X  as shown in the left map in Figure 2. Analyzing interesting changes. Once a region of interest is selected interactively by a user, our hot spot methodol-ogy [6] can be used to understand distinctive features of these changing regions. In this methodology, the compo-nent planes are sorted by the importance of the attributes that distinguish the region from the rest of the population
Figure 2. Relative density visualizations rd 1 (left) and rd (dark gray for component  X  X  X  and light gray for component  X  X  X ).
Voronoi region in the data space, and similarly in the right two maps. using an attribute selection measure [8], such as informa-tion gain or gain ratio, as shown in Figures 5 and 7. As a SOM produces a smaller but representative dataset, the pro-totype vectors can be used as an approximation of the whole dataset. Efficient computation allows an analyst to explore distinctive features of any region of the map interactively.
Our method has been tested using our Java SOM Tool-box (JSOM) on both synthetic and real-life datasets. Syn-thetic datasets were used to evaluate the ability of the proposed method to visualize individual known cluster changes, such as the introduction of new clusters and dis-appearing clusters. Due to space limitations only one com-bined scenario has been presented in Section 4. The syn-thetic dataset D (  X  1 ) has been generated using Gaussian dis-tributions, while the dataset D (  X  2 ) has been generated using a transition matrix P = { p ij } [8], which contains the prob-ability of an entity moving from cluster i in dataset D (  X  to cluster j in dataset D (  X  2 ) .
We evaluated our method using selected indicators from the World Development Indicator (WDI) dataset [24], which is a multi-variate temporal dataset covering 205 countries [5]. Yearly values were grouped for 10-year pe-riods, and the latest available values are used. The experi-ments compare cluster structures based on the selected indi-cators that reflect different aspects of welfare, such as pop-ulation, life expectancy, mortality rate, immunization, illit-eracy rate, education, television, and inflation.

The visualizations in Figure 4 reveal several interest-ing changes. First, the cluster at the bottom-left of the 1980s map is missing in the 1990s map. This cluster con-sists of four South American countries: Brazil, Argentina, Nicaragua, and Peru. These countries were suffering eco-nomic difficulties (e.g. high inflation) due to a debt crisis in the 1980s, which is known as the  X  X ost decade X  [26]. How-ever, South American countries performed rapid reforms in the late 1980s and early 1990s [26]. The welfare of these countries therefore became more similar to other countries, which explains the missing cluster in the 1990s.
Another interesting finding is that there is a cluster en-largement towards the tail of the distribution at the bottom-right corner of the 1990s map. This new region consists of OECD (Organization for Economic Co-operation and De-velopment) and other developed countries who achieved a higher standard of living in the 1990s, that have not been achieved in the 1980s. However, this region cannot be con-sidered as a new cluster, as it does not have a good separa-tion from its left neighbours, as shown in the distance ma-trix visualization of the 1990s map (the bottom-right map in Figure 4).

Finally, the top-right region of the 1980s map experi-enced a decrease in density compared to the 1990s map. This region consists of several African countries. Generated by applying hot-spot analysis [6], Figure 5 can be used to understand distinctive features of this shrinking region. Af-ter selecting this region, the sorted component planes show that this region is characterized by high illiteracy, high mor-tality rate, high percentage of children in the labor force, low ratio of physicians, and low school enrolment.
Our method has been used to explore changes in cluster structures in very large anonymized taxpayer datasets from 2003 to 2007 for the Australian Taxation Office (ATO). Here, we provide aggregate indicative results that demon-strate the effectiveness of our method, without breaching the confidentiality of the data or the specifics of the discov-eries made.

The datasets consist of nearly 2.8 million entities, each with 83 numeric attributes, such as income from various sources, work-related expenses, and tax deductions, from 2003 to 2007. The datasets were pre-processed [5] and im-ported into the embedded database. The map size chosen was 15x20, and a map for each year was trained in around 6 hours on a Debian GNU/Linux machine running on a 64-bit, 2.6 GHz AMD Opteron, dual-core quad processor server, with 32 GB main memory. During the map training, the Java SOM Toolbox only used approximately 4 GB of mem-ory (our JSOM Toolbox avoids loading the whole datasets into memory).

The top map in Figure 6 shows the emergence of a new large region (the dark blue region at the top of the map) in the 2007 dataset, compared to the 2006 dataset. This change is identified as a cluster enlargement as it does not have a good separation with its neighbours, as shown in the distance matrix visualization (the bottom map in Figure 6). This kind of massive change in cluster structure was not found to exist in previous time periods (from 2003 to 2006). Noting that a SOM performs vector quantization, the size of this region reflects the magnitude of the population affected by the change. Thus, this is a sizable change in the behavior of the population.
Figure 6. Relative density visualization rd 2 of the 2007 to the 2006 ATO dataset (top) and distance matrix visualization (bottom) of the 2007 ATO dataset.

Further analysis of the discriminating characteristics of this new region lead to insights that are important to the taxation analysts. The top distinctive feature was found to relate to low income rebate amounts, as shown in Figure 7. It was noted, in comparing the values to the 2006 map, that the maximum value of the low income rebate amount had doubled. Without any other knowledge, a change in behav-ior was identified through the deployment of ReDSOM.
An investigation, conducted after discovering this be-havioral change, found that it was caused by a change in government policy. In 2006, the Australian Government in-creased the Low Income Tax Offset from $235 to $600 per year for the financial year 2006/2007 [20]. This also ex-plains why low values of taxable income was also a distinc-tive feature, as seen in Figure 7.
We have introduced a relative density SOM (ReDSOM) visualization that is able to show various changes in clus-ter structures, such as emerging clusters, disappearing clus-ters, cluster enlargements, cluster contractions, movement of cluster centroids, and changes in cluster density. ReD-SOM has been tested with real-life datasets, including large datasets from the Australian Taxation Office.
 Experiments using real-life datasets have shown that ReDSOM is capable of indicating actual changes, such as the change in economic fortunes of South American coun-tries between the 1980s and 1990s, or the change in tax pol-icy for low income earners.

These structural changes can be analyzed further by looking into the migration patterns of the entities. Future work incorporating migration analysis is underway.
This research has been supported by the Australian Taxa-tion Office. The authors express their gratitude to Elea Gud-geon for providing data and domain expertise, to AusAID for providing a PhD scholarship to the first author, and to the reviewers for providing useful feedbacks. Map colors are based on www.ColorBrewer.org .

