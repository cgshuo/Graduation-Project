 unfortunate that, this increasing growth of information is still restricted to the English language as this is the most dominant language used. Attempts and efforts devoted to improve the area of information search and retrieval in Arabic language are still limited and simple compared to the efforts done in the same area in other languages. As indicated by Khoja [1] the Arabic language is different from other indo-European languages in terms of its syntax, morphology and semantics. Since the morphological nature of the Arabic language is characterised by being complex, a wide of body of research has been conducted in this area, and more specifically, focusing on the impact of Arabic morphol ogy on Arabic Document Clustering. It is indicated that the Arabic morphology in Do cument Clustering aims to conflate words effectiveness over the use of words or stems is increasingly and significantly enhanced by indexing Arabic text using roots [2]. 
In defining Document Clustering, it is cat egorised as an unsupervised learning task, which does not demand pre-defined categories and labelled documents. It aims to group or put text documents with high intra-group similarities and low inter-group similarities in a group [3]. tokenisation and normalisation, the second part which has method for stemming the data named ISRI, the third part provides a discussion of the representation process of The third section of the study is evaluation which is divided into two parts, which are; explaining and discussing cluster data by using K-mean algorithm, describing of Arabic data set and viewing of performance measure. The forth section is experiments and result. Finally, a conclusion and future work is provided. The two most effective Arabic stemmers are Khoja [4] based on root-extraction stemmer and Larkey X  X  light stemmer [5], [6]. Moreover, Duwairi [7], El-Kourd et al [8] and Mustafa et al. [9] discovered that N-gram stemming technique is not efficient for Arabic Text processing. 
In a study by Larkey X  X  [5] the researcher proposed several light stemmers based on heuristics and a statistical stemmer based on co-occurrence for Arabic retrieval. It was determined that the effectiveness of the best light stemmer was better for cross-root for each word. Darwish [10] examined the effect of improved morphological analysis, in particular, the focus was on the effect of the context sensitive morphology comparison of the effect of context sensitive morphology with that of non-context sensitive morphology, it was revealed that better coverage and improved correctness dramatically impacted the IR effectiveness and that context sensitive morphology further enhanced the retrieval effectiveness. 
Taghva et al [11] indicated that by using a root-extraction stemmer for Arabic with Khoja stemmer, the performance of the stemmer was equivalent to the Khoja monolingual document retrieval tasks. Froud et al. [12] assessed the effect of stemming on Arabic Text Document Clustering by using similarity/distance measures. It was found that the Euclidean Distance, the Cosine Similarity and the without stemming and without stemming better than Khoja and light stemmers. On the other hand, the Pearson Correlation and averaged KL Divergence obtained quite similar results when using Khoja and light stemmers. In this paper we evaluated same five similarities/distances measure for Document Clustering, with morphology-based ISRI stemming and without stemming, where ISRI stemmer does not need a root dictionary in order to make clustering faster. This section comprises four parts: Arabic text pre-processing, stemming proposed, term representation and similarity/distance measures. 3.1 Arabic Text Pre-processing pre-processing methods which are applied as the followings: Step 1: Conversion of the text to Unicode. Step 3: Filtering the non Arabic words. Step 4: Splitting the text into tokens which usually consist only of letters. root. 3.2 Arabic Stemming Algorithm According to the report of The Informa tion Science Research Institute X  X  (ISRI) generated by Taghva et al [11] the Arabic stemmer is proved to have common features with the Khoja stemmer. However, the main difference between the two the following table: Input: Arabic documents.
 Phase one: remove stop word . Phase two: Remove length three and length two prefixes in that order. Phase three: Remove connector  X  if it precedes a word beginning with  X  . 
Phase four: Return stem if less than or equal to three. Attempting to shorten stems further results in ambiguous stems. relevant stem and return. Otherwise, attempt to remove length-one suffixes and prefixes in that order provided the word is not less than length three. 
Phase six: Length = 5: Extract stems with three characters for words that match patterns 5. If none are matched, attempt to remove suffixes and prefixes, otherwise the relevant length-three stem is returned. If the word is still five characters in length, relevant stem is returned if found. 
Phase seven: Length = 6. Extract stems of length three if the word matches a pattern 6. Otherwise, attempt to remove suffixes. If a suffix is removed and a resulting term of length five results, send the word back through Phase six. Otherwise, attempt to remove one character prefixes, and if successful, send the resulting length-five term to Phase six. 
Phase eight: Length = 7. Attempt to remove one-character suffixes and prefixes. If successful, send the resulting length-six term Phase seven. 
Output: stemmed documents 3.3 Term Representation We used the bag-of-words (BOW) methodology which is commonly used in the language-independent method since they are independent of the meaning of the weighting method based on Boolean weighting was used, this was considered as the matrix to which the weight of a term is assigned in the case when the term appears in the document and 0 is the matrix to which the term the weight of a term is assigned in case when the term does not appear in the document. Weighting each term will be conducted by using the Term Frequency  X  Inverse Document Frequency (TF X IDF) weighting. In this approach, proportional assigning of the weight of term i in corpus in which the term occurs. 3.4 Similarity Measures For the current study, the similarity/dista nce measures are Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coeffici ent, Euclidean Distance and Averaged Kullback-Leibler Divergence, where not every distance measure is a metric. To qualify as a metric, a measure d must satisfy the following four conditions. 
Let x and y be any two objects in a set and d(x, y) be the distance between x and y. 1. The distance between any two points must be nonnegative, that is, d(x, y)  X  0. 2. The distance between two objects must be zero if and only if the two objects are A. Cosine Similarity Cosine similarity is one of the most well-known similarity measures which is applied which is non-negative. Therefore, the cosine similarity is non-negative and bounded between [0, 1]. The cosine similarity is independent of document length, which is an in measuring the cosine similarity between two identical copies of a document d which are combined to get a new pseudo document d 0 , the cosine similarity between d and d 0 is 1, thus, implying that these two documents are regarded to be identical. B. Jaccard Coefficient similarity in the intersection divided by the union of the objects. For text documents, condition that they are not the shared terms. The formal definition is as follows: According to the Jaccard coefficient, the similarity measure ranges between 0 and 1. measure is D J =1-SIM J and D J is used instead in subsequent experiments. C. Pearson Correlation Coefficient forms. Given the term set T=  X  t  X  ...t  X   X  a popularly used form is: experiments, the corresponding distance measure, which is D p =1-SIM p is used when SIM p  X  0 and D p =| SIM p | when SIM p &lt; 0. D. Euclidean Distance including Document Clustering. It is also identified as a measure for determining the default distance used with the K-means algorithm. In measuring the distance between text documents, given two documents d a and d b are represented by their term vectors t  X   X   X   X  follows: value is used as term weights, that is w t, a = tfidf (d a , t). E. Averaged Kullback-Leibler Divergence In Document Clustering which is based on information theory, a document is the two corresponding probability distributions is known as the similarity of two documents which has to be measured. The Kullback-Leibler divergence (KL divergence). Given two distributions P and Q, the KL divergence from distribution P to distribution Q is defined as: In such document scenario, the divergence between two distributions of words is: In contrast to the previously discussed measures, the KL divergence is not symmetric, divergence is used instead in the current study, and which is defined as: formula provides or shows computation of averaged KL divergence: that there is a symmetry ensured by the average weighting between two vectors. In other words, the divergence from document i to document j is symmetrical to the divergence from document j to document i. For the testing dataset, we experimented with three similarities and two distance measures using two methods: ISRI using the morphological analyser from Taghva et al [11] and without stemming. 4.1 Clustering Cluster analysis [16] refers to the process of classifying objects into groups of similar objects based on a similarity/distance measure. It has been applied in a wide number K-means is a method which has been widely used for partitional clustering with a with distance measures which principally aim at minimising the within-cluster distances. 
The Euclidean distance and the averaged KL divergence are defined as measures used for distance whereas the cosine similarity, Jaccard coefficient and Pearson transformation was used for the purpose of converting the similarity measure to (0, 1) and monotonic, D = 1-SIM is taken as value of the corresponding distance. As far as Pearson coefficient, which ranges fr om  X 1 to +1 is concerned, D=1-SIM is taken when SIM  X  0 and D = |SIM| when SIM &lt; 0. 4.2 Data Description and Performance Measure The testing dataset consisted of 4 categories namely art, economics, politics and sport articles, and each contains documents taken from Al-salemi and Aziz [18] 1680 documents where used in testing the dataset. other measures called overall F-measure, which is the most well-known and used measure in Document Clustering, was used for this purpose [17] where the higher overall purity and F-measure means the best cluster. Three different sets of experiments were created. Moreover, each experiment was run 5 times and the results are the averaged value over 5 runs. Each run had different initial seed sets. 
The first experiment aimed at Clustering Documents which are clustered under two different groups, based on Table 2 the distance measure better than similarity measure as a general with or without stemming, In contrast the similarity with ISRI stemming is better than without, where the best value was (0.75%) with D Avg KL as a distance measure and (0.7%) with cosine similarity measure. 
The second experiment was conducted to Cluster Documents which belonged to three groups, based on Table 3 the results are equivalent between cosine, Jaccard and D
Avg KL In addition, the ISRI method was better than without stemming with similarity/distance measure. 
The last experiment was carried out to examine the effect on clustering of the four effect on highly relevant documents, based on Table 4 the cosine and Jaccard was better than Pearson as similarity measure and Euclidean better than D Avg KL measure, on the another hand ISRI stemming was better than without stemming. We concluded based on our experiments which are detailed above, that the similarity/distance measure is more effective on ISRI stemming as morphological words than without stemming; especially when they group documents on similarity measures more than distance measures. 
The main reason for determining the effectiveness of the three similarities and two distances with the ISRI method, is that without a stemming method have under-stemming error in which some terms that should be stemmed to one root are not, which leads to create similarities among the unrelated documents containing the same roots for different words, as shown in Table 5. The ISRI method have over-stemming error that means there are two words with different stems which are stemmed to the same root as displayed in Table 5. These errors of stemming decrease the effectiveness on the similarities and distances measure. The similarities and distances calculate the term frequency for each word, and within the previous similarities and distances measure can be done as shown in our results, and also a lot of terms have under-stemming more than over-stemming so that the ISRI method has on the result better than without stemming. The cosine as specific and jaccard as general similarity have proved to be better than the Pearson in our experiments. 
There are some suggestions are mentioned for future research. Firstly, a combination of different models is planned as to make a representation of the documents. So that, we propose another similarities/distances measure such as Probabilistic Models as BM25 and Language models. Secondly, using another stemming method or generating new one which is based on the morphological and syntactic structure same as generated by [19]. These are due to the idea of stemming and stop word to decrease the words whic h are not important to the documents. 
