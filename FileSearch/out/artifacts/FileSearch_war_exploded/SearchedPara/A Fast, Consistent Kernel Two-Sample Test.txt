 Learning algorithms based on kernel methods have enjoyed co nsiderable success in a wide range of which linear algorithms can be brought to bear. While classi cal kernel methods have addressed the mapping of individual points to feature space, more recent d evelopments [14, 29, 28] have focused a metric on distributions. This distance is known as the maxi mum mean discrepancy (MMD). One well-defined application of the MMD is in testing whether two samples are drawn from two whether DNA microarrays obtained on the same tissue type by d ifferent labs are distributed iden-(and cannot be aggregated) [8]. Other applications include schema matching in databases, where verification, where MMD can be used to identify whether a spee ch sample corresponds to a person for whom previously recorded speech is available [18].
 A major challenge when using the MMD in two-sample testing is in obtaining a significance thresh-old, which the MMD should exceed with small probability when the null hypothesis (that the sam-identical to the sequence of normalized eigenvalues obtain ed in kernel PCA [26, 27, 7]. Thus, we In experiments, our new estimate of the test threshold has a s maller computational cost than that of resampling-based approaches such as the bootstrap, whil e providing performance as good as the alternatives for larger sample sizes.
 in an RKHS. We also review the maximum mean discrepancy as our chosen distance measure on we present both moment-based approximations to the null dis tribution of the MMD (which have spectrum of the kernel matrix over the aggregate sample. Our experiments in Section 4 compare the of text are on the same topic, or on different topics. an expression for the asymptotic distribution of this dista nce measure, from which a significance threshold may be obtained.
 Let F be an RKHS on the separable metric space X , with a continuous feature mapping  X  ( x )  X  F function k ( x,x  X  ) := h  X  ( x ) , X  ( x  X  ) i P be the set of Borel probability measures on X . Following [4, 10, 14], we define the mapping to F of
P  X  P as the expectation of  X  ( x ) with respect to P , or The maximum mean discrepancy (MMD) [14, Lemma 7] is defined as the distance between two such mappings, where x and x  X  are independent random variables drawn according to P , y and y  X  are independent many common kernels, such as the Gaussian kernel (both on com pact domains and on R d ) and the B We now consider two possible empirical estimates of the MMD, based on i.i.d. samples ( x 1 ,...,x m ) plicity). An unbiased estimate of MMD is the one-sample U-statistic where z biased estimate MMD 2 terms i = j ).
 Our goal is to determine whether P and Q differ, based on m samples from each. To this end, we require a measure of whether MMD 2 we conduct a hypothesis test with null hypothesis H H appropriate threshold is the 1  X   X  quantile of the asymptotic distribution of the empirical MM D assuming P = Q . According to [14, Theorem 8], this distribution takes the f orm where  X  value equation Z and  X  k ( x shown in [14].
 The eigenvalue problem (3) has been studied extensively in t he context of kernel PCA [26, 27, 7]: and we summarize certain important results. Following [3, 1 0], we define the covariance operator C : F  X  F as The eigenvalues  X  Following e.g. [27, p.2511], empirical estimates of these e igenvalues are where  X  K m MMD 2 b , we observe that these differ by a quantity with expectation tr( C ) = We first present the Pearson curve and Gamma-based approxima tions, which consist of parametrized approximations can be accurate in practice, although they r emain heuristics with no consistency counterpart in the large sample limit. 3.1 Moment-based null distribution estimates The Pearson curves and the Gamma approximation are both base d on the low order moments of the empirical MMD. The second and third moments for MMD are obtai ned in [14]: Pearson curves take as arguments the variance, skewness and kurtosis As in [14], we replace the kurtosis with a lower bound due to [31], kurt MMD 2 more computationally efficient approach is to use a two-para meter Gamma approximation [20, p. 343, p. 359], m MMD b ( Z )  X  and we use the biased statistic MMD 2 the Gamma approximation, as opposed to O ( m 3 ) for Pearson). Moreover, we will observe in our 3.2 Null distribution estimates using Gram matrix spectrum approaches an infinite weighted sum of independent  X  2 strategy is proposed for the KFDA test with fixed regularizat ion).
 computed from quantiles of the null distribution estimate, is asymptotically consistent in level . Theorem 1 Let z Assume P  X  Furthermore, as m  X  X  X  Proof (sketch) We begin with a proof of conditions under which the sum P  X  that this sum converges a.s. if the assumption P  X  and b C be the covariance operator and its empirical estimator. Let  X  eigenvalues of C and b C , respectively, in descending order. We want to prove in probability as n  X   X  , where Z p  X  2
X We now establish P inequality. To prove the latter, we use that since  X   X  have the desired result by Chebyshev X  X  inequality. The proo f is complete if we show From we have It is known as an extension of the Hoffmann-Wielandt inequal ity that where kk gives k b C  X  C k second statement follows immediately from the Polya theore m [21], as in [18]. 3.3 Discussion We now have several ways to calibrate the MMD test statistic, ranked in order of increasing com-the resampling procedures (subsampling or bootstrap with r eplacement). We include the final two approaches in the same cost category since even though the Pe arson approach scales worse with m of the covariance operator, whereas the Gamma approximatio n calculations are straightforward and remain possible for any spectrum decay behaviour. The Gamma approximation remains a heuristic, the approximation error computed with respect to simulatio ns from the true null, and when used in homogeneity testing. Our approaches are denoted Gamma (the two-parameter Gamma approx-imation), Pears (the Pearson curves based on the first three moments, using a l ower bound for the kurtosis), Spec (our new approximation to the null distribution, using the G ram matrix eigenspec-trum), and Boot (the bootstrap approach).
 Artificial data: We first provide an example of a distribution P for which the heuristics Gamma P to replications. The eigenvalues of the Gram matrix were estim ated in this experiment using [13], which is slower but more accurate than standard Matlab routi nes. The true quantiles of the MMD max t t samples from each of P and Q . We also use this performance measure for the Gamma and Pears approximations. This focuses the performance comparison o n the quantiles corresponding to the combination of distribution and kernel, Spec performs almost uniformly better than both Gamma and Pears . We emphasize that the performance advantage of Spec is greatest when we restrict ourselves to higher quantiles, which are of most interest in testing. Benchmark data: We next demonstrate the performance of the MMD tests on a numb er of mul-tivariate datasets, taken from [14, Table 1]. We compared mi croarray data from normal and tumor potential (LFP) electrode recordings from the Macaque prim ary visual cortex (V1) with and with-accurate density estimates as an intermediate step).
 quantiles. For the Spec case, we computed the eigenspectrum on the gram matrix of the aggregate data from P and Q , retaining in all circumstances the maximum number 2 m  X  1 of nonzero eigen-values of the empirical Gram matrix. This is a conservative a pproach, given that the Gram matrix smallest eigenvalues. For the bootstrap approach Boot , we aggregated points from the two samples, then assigned these randomly without replacement to P and Q . In our experiments, we performed 500 such iterations, and used the resulting histogram of MMD val ues as our null distribution. We the aggregation of samples from P and Q .
 We applied our tests to the benchmark data as follows: Given d atasets A and B, we either drew one sample with replacement from A and the other from B (in which c ase a Type II error was made when the null hypothesis H single pool consisting of A and B combined (in which case a Typ e I error was made when H was rejected: this should happen a fraction 1  X   X  of the time). This procedure was repeated 1000 times to obtain average performance figures. We summarize ou r results in Table 1. Note that an extensive benchmark of the MMD Boot and Pears tests against other nonparametric approaches to test [17]. See [14] for details.
 The computational cost shows the expected trend, with Gamma being least costly, followed by Spec , larger sample sizes, however, we expect the cost of Pears to exceed that of the remaining methods, due to its O ( m 3 ) cost requirement (vs O ( m 2 ) for the other approaches). Finally, we demonstrate the performance of the test on struc tured (text) data. Our data are taken from the Canadian Hansard cor pus ( and immigration were used. Transcripts were in English and F rench, however we confine ourselves and Q from transcripts on immigration (in the null case, both samp les were from the same topic). The data were processed following the same procedures as in [ 15]. We investigated two different both cases, we computed kernels between five-line extracts, ignoring lines shorter than five words long. Results are presented in Figure 2, and represent an ave rage over all three combinations of each topic pairing, results are averaged over 300 repetitio ns.
 We now investigate the fact that for sample sizes below m = 30 on the Hansard data, the Spec diagonally dominant: thus for small sample sizes, the empir ical estimate of the kernel spectrum 2). This effect vanishes on the Hansard benchmark once the nu mber of samples reaches 25-30. By contrast, for the Neural data using a Gaussian kernel, thi s small sample bias is not observed, negligible. We emphasize that the speed advantage of the Spec test becomes important only for larger sample sizes (and the consistency guarantee is only m eaningful in this regime). We have presented a novel method for estimating the null dist ribution of the RKHS distance be-tween probability distribution embeddings, for use in a non parametric test of homogeneity. Unlike previous parametric heuristics based on moment matching, o ur new distribution estimate is consis-consistent approach. We have demonstrated in experiments t hat our method performs well on high show inaccuracies. We anticipate that our approach may also be generalized to kernel independence tests [15], and to homogeneity tests based on the kernel Fish er discriminant [18].
