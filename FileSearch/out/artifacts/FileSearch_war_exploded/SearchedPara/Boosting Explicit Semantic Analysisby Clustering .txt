 With the boom of Internet and social media, a vast volume of data has been generating and cumulating incessantly. Text content accounted for the majority among the User-generated Content [1] data in web. Automatically estimating the semantic relatedness of two text fragments is fundamental for many text mining and IR applications. In this work, we develop a SR computing frame-work exploiting inter-entries dista nce to reduce the redundancy and utilize the correlations between Wikipedia article s of text representation by ESA. Our ex-periments show its superiority against original ESA.

Existing approaches measuring text semantic relatedness could be classified to three categories. lexical overlap is a kind of straight-forward but weak method, for mainly text relatedness is not based on common terms of given text pair. The other kind of approaches are based on knowledge bases such as WordNet [2] or Cyc [3]. Texts are initially mapped to taxonomy nodes in knowledge bases, then the SR of text pairs could be computed by exploiting links and paths be-tween nodes [4][5][6][7]. Besides, LDA [8], LSA [9] and ESA [10] constitute the 3 rd category of schemes that generate feat ure vector representation for text by exploring the words co-occurrence relationships or word occurrence statistics in a corpus [11]. There is also much effort [16][14] are devoted to combine a vari-aty of individual models together to gain b etter performance. Normally cosine similarities are then utilized to measure the SR.

It is known that when reading human can refer the content in text to things in real world and relations between them. Analogously, ESA served as an interpreter that refer given text to semantically related articles of Wikipedia. Hence, a semantic relatedness of a pair of texts sharing little common words could be captured by ESA. Prior work [12] shows that ESA achieve the state of the art performance within the individual methods on SR tasks. different from works devoted to combine a variaty of methods, mostly including ESA, to train a integrated model that mostly outperform the methods seperatedly, our work focuses on improvement of the ESA method, which is also able to be incorperated with other methods.
For a given text fragment,ESA treat it as a collection of words disregarding the order. Those words, after stemming process, are mapping to a list of arti-cle entries and corresponding scores via a pre-built inverted indices base that link to the articles in Wikipedia. Normally the scores in the list were calculated by TFIDF scheme. By this means, ESA p rovide an more effective and elegant model to represent a text comparing to Bag of Words model, and then Using this model to computing semantic relatedness among text pair. However, the semantic relatedness between entries, especially for the redundancy issue, is not considered in ESA. As is shown in Figure 1, the scale of Wikipeida has expe-rienced a huge growth during the last several years. That more and more very specific entries being established in Wikipedia may results in redundant entries in the text representation by ESA. There are two shortcomings for this situation in semantic relatedness computing. First, when text pair share a considerable amount of entries which is highly similar to each other in their representations, ESA scores may be unreasonably enhance due to the bias in computing. On the other hand, when both text representations in a pair contain some entries respectively about a specific topic, if they share no common entries, the resulting score will be underestimated. We believed that the system performance could be enhanced by overcoming those disadvantages. Hence, we devise a framework exploiting entries similarity to reduce the redundancy problems.

In the paper, we propose a SR computing framework that employ density based clustering to improve ESA accuracy. The framework incorporates Paragraph Vec-tor, a newly devised text embedding technique that remarkably superior to Bag of Words methods, to represent Wikipedia entries. In the framework we present 1) a density based clustering method to group similar Wikipedia entries to form new text representation that bridge the gap between similar entries, and 2) a scoring mechanism by using the maximum score in each group and disregarding other en-tries to avoid redundancy. We conduct experiments on four dataset, and the result indicate that our system gains better performance comparing original ESA.
The structure of this paper is organized as follow. Related works is stated in Section 2. Then we outline the main idea of ESA, Section 3 is devoted to a detailed description of our framework. In which we persent two key algorithm of our framework. Preprocessing and experimental evaluation will be elaborated in Section 4. Finally, we provide concluding remarks and suggestions for future work in Section 5. We firstly outline the ESA X  X  principal fundamentals. ESA initially maintain a database stores the mapping between terms a nd their relevant Wikipedia articles. Similar to ordinary IR tasks, all the Wikipedia articles with meaningful content are indexed by terms (except the stop words) they contain. Hence each non-stop term is mapped to a list of Wikipedia articles. Namely Where e i = { e i .title, e i .score, e i .fulltext } .

TF-IDF measure are chosen as a score of the mapping in original ESA imple-mentation. So we have
So far, Vec ( t ), which contains a list of Wikipedia articles and their corre-sponding scores related to t measured by TF-IDF scheme, could be served as the semantic representation of the single term t . For longer documents, the rep-resentation is the centroid of the v ectors representing the each terms.
Then we present the key idea of the PV [18] technique that is recently de-veloped. PV is the extended version of the Word2Vec [17] 1 that served to learn vector representation of words from a corpus. As a neural language model (Fig-ure 2), Word2Vec is originally designed to predict word based on given context. However, what make it unique from other language model is the compact vector representation (100 dimensions by default) for every words. In the training pro-cess, both the weights of the model and the representation vectors of the words are updated simultaneously. After training is finished, every words in the corpus get a fixed-length vector re presentation. with the sa me architecture, Paragrah Vector are gained by treat the sentences or paragraphs as the a single word in the beginning of the text. Experiments [18] on sentiment analysis and informantion retrieval show that PV could ourperform other model including Bag of Words, which is also why we adopt it to mearsure the similarities of the Wikipedia ar-ticles. It is worth noting that although PV itself could be adopted to measure the text similarity for corpora, it does not apply to the scenarios that only small amount of text are given. In those cases ESA is more suitable. 3.1 Overview The proposed framework (Figure 3) is comprised of three components. 1) Ordi-nary ESA that generate Wikipedia articl es based representation for given text pairs; 2) Wikipedia articles embedding, in which we running the PV program on all the Wikipedia articles to generate and store on database 100-dimensional vectors for each article; and 3) A clustering based algorithm for redundancy re-duction (Algorithm 1). Based on the similarity between the precomputed PVs of different articles, original ESA representations of the text pairs is reconstructed to relatively more compact representations with little redundancy. 3.2 Algorithm In this section, we present an algorithm called Semantic Relatedness via Para-graph Vector based Clustering (SRPVC) to compute SR for a given text pair.
Comparing to existing clustering algorithms, the key point of SRPVC is that it guarantees the maximum distance between points within cluster, i.e., the Wikipedia articles of corresponding points in the same cluster are considerably related to each other. In this way, highly related or similar articles, treated as redundant entries, were merged together. The effect of our algorithm comparing to Kmeans [20] and DBSCAN [21] is illustrated in Figure 4. Kmeans assigns data points to the nearest means iteratively, without regard for distances of points within clusters. Relatively distant po ints may be assigned together by Kmeans. DBSCAN clusters together points that are closely packed together, without con-sider the diameters (i.e. maximum distances) of clusters also.
 Algorithm 1. SRPVC
We explain the Algorithm 1 as follows: Firstly, Both text X  X  ESA representa-tions, as a whole ( S  X  E 1  X  E 2 ), are treated as the input of clustering. Every data point, which represent a single Wikipedia article, is initially assigned to a specific group. Then any two groups are iteratively merge to a larger group if their locality property satisfies certain condition. In the algorithm we set the condition that 1) the minimun distance of data points of two group respectively must not larger that  X  , and 2) distances of all the entries within the new cluster must be not larger than  X  . By this way algorithm make sure entries within the same group are related enough to eath other. In other words, for any data point c in a group, the distances of nearest point and farthest point from c in the same group should not be higher than  X  and  X  respectively. After clustering, the final groups (clusters from clustering) served as the vector space for new representation vectors of t he text pair. Unrelated in previous representation but actually similar Wikipedia entries may be related to each other by being as-signed to same groups. For each text and each group, the maximum ESA scores are adopted as the corresponding scores of the groups in new representations, as shown in the final part of the algorithm. By ignoring other scores in the same group, redundancy issue will be mitigated.

To make the idea of the algorithm more intuitive, we present an intermediate result (Figure 5 and the Representations Reconstruction component of Figure 3) that depict a segment of ESA representation of a text pair, before and after the clustering. As depicted on the left, although only one Wikipedia entry (UBS) is shared by the text pair, there are still a lot of Wikipedia entries are considerably related. similar entries are clustered together as the dimen sions of new vector space after clustering, as shown in the middle of the figure. For each groups, Picking the maximum score for both text, disregarding other entries to avoid redundancy, forming the final representation vectors on the right of the figure. 4.1 Preprocessing We acquire the snapshot of Wikipedia dump (static documents) of December 2nd, 2013, then extract and transform data into relational database. Similar to the preprocessing in [12][15], the following phrases are involved.  X  Extracting data, mainly including full text of articles, from original Wikipedia  X  Stemming and indexing the content field of pages.  X  Using index function to calculate IDF value for each term-document pair, As for of the PV of each Wikipedia articles, the program running on a ordinary PC took approximately 50 hours to get the output. We also store the output results in database for the convenience of future queries. 4.2 Experimental Setup and Results Experiments on four datasets are employ ed to demonstrate the effectiveness of our method. Lee.50 [13] comprise 50 documents from the Australian Broad-casting Corporation X  X  news mail servi ce. Those documents, covering a variety of topics, are paired and then judged by volunteer students on their relatedness to each other. MSRpar , MSRvid and SMTeuroparl 2 are stand for Microsoft Re-search Paraphrase Corpus , Microsoft Research Video Description and WMT2008 Develoment Dataset respectively. They each consist of approximately 750 text pairs with human-judged scores on their relatedness. Consistent with the prior works on this dataset, we employ Pearson correlation to measure the relatedness between human-judged scores and resulting score output by our system. Higher correlation indicates that the system output results are more likely in accord with gold standard scores. The Pearson correlation is defined as As is shown in Table 1, we apply our method to top 400 entries in ESA on Lee.50 dataset to explore the effect of parame ters. System gain best performance when  X  =0 . 25 or 0 . 3and  X  =0 . 4, hence those two configurations are adopted in the experiments on all the datasets.

Experimental results are presented in Figure 6. The parameter  X  indicated 1 minus the cosine similarity threshold that entries being clustered. We explore the System performance on the configurations against the number of the entries in ESA phrase. Results show that SRPVC system can significantly enhance the ESA performance on th e datasets except MSRpar . It is noted that MSRpar is abbreviation for Microsoft Research Paraphrase Corpus , which indicate that MSRpar dataset is scored for measuring whether two texts are paraphrase to each other rather than SR. For instance,  X  X mgen shares gained 93 cents,or 1.45 percent, to $ 65.05 in afternoon trading on Nasdaq. X  and  X  X hares of Allergan were up 14 cents at $ 78.40 in late trading on the New York Stock Exchange. X  , two sentences in MSRpar that are about different stocks on different markets topic, although were considerably related in semantics, were scored only 1.333 (range from 0 to 5) for the extent of being paraphrases. Since ESA is prerequisite of our method, its outcome is highly related to our method X  X . As it is shown, ESA gains relatively low accracy in MSRpar , indicating that the generated entries by ESA do not capture enough semantics of the original texts. Therefore clustering on those en tries also does not make sense.
In a word, system gains notable improvement comparing to ESA in the datasets except MSRpar . Informally, those results reveal that clustering the representation entries to reduce their redendancy could effectively enhance the semantic relatedness performance for the cases that ESA show relatively good outcomes. In the paper we propose a clustering scheme to reduce redundancy in ESA repre-sentations of texts for SR tasks. It reconstructs the ESA representations of text based on the groups from clustering. By clustering redundant entries in the same text are merged together. And also, the semantic relatedness beween simlilar en-tries from two text is captured by the maximum scores of the Wikipedia entries of each groups. By this means, surplus entries in the representations of the same text are eliminated, and similar entries in representations of different texts are considered. Experiments show the effectiveness of this method against ESA, which indicated that the new form of representations for texts after clustering capture the text semantics more accurate.

On the other side, parameters configuration of the model for a wider range of datasets still remains an o pen question. Moreover, frequent access to database to get PV for texts and online clustering both are time-consuming processes. Clustering the Wikipedia articles offline in advance will be a promising direction to explore. We will focus on those issues to improve our method in the future work.
 Acknowledgments. This research is supported by the 863 project of China (2013AA013300), National Natural Science Foundation of China (Grant No. 61375054 and 61402045), Tsinghua University Initiative Scientific Research Pro-gram Grant No.20131089256, and Cross fund of Graduate School at Shenzhen, Tsinghua University (Grant No. JC20140001)
