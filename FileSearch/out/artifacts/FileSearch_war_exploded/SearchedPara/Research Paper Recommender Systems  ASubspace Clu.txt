 The explosive growth of the world-wide web and the emerging popularity of e-commerce has caused the collection of d ata to outpace the analysis necessary to extract useful information. Recommender systems were developed to help close the gap between information collection and analysis by filtering all of the available information to present what is most valuable to the user [20]. cation of research papers. The number of research papers published continues to increase, and new technology has allowed many older papers to be rapidly digitized. A typical researcher must sift through a large quantity of articles man-ually, relying on keyword-based searches or paper citations to guide them. The search results of researchers with simila r interests can help direct a more effective search, but the process of sharing search results is often too cumbersome and time consuming to be feasible. A recommender system can help by automatically recommending papers based on the preferences of other researchers with similar interests.
 ing and collaborative filtering. Content based filtering (CBF) approaches create relationships between items by analyzing inherent characteristics of the items. Collaborative filtering (CF) systems do not analyze an items properties, but instead take advantage of information about users X  habits to recommend poten-tially interesting items. The analysis of user behavior patterns, allows collabo-rative filtering systems to consider cha racteristics that would be very difficult for content based systems to determine such as the reputation of the author, conference, or journal. CF approaches are also well suited to handle semantic heterogeneity , when different research fields use the same word to mean different things.
 the common challenges in the collaborative filtering process followed by a formal definition of the problem we propose to solve. In Section 3 we give a detailed description of our proposed algorithm. In Section 4 experimental results are presented, including description of the dataset used, the evaluation metrics and discussion. In Section 5 we discuss related work in the area of subspace clustering and recommender systems. Finally Section 6 contains concluding remarks and directions for future research. In spite of the success achieved by CF al gorithms, there are limitations to such systems [10] that arise in the research paper domain. In many domains, there is an ever increasing number of users while number of items remains relatively sta-ble. However, in research paper recommendation domain, the number of users (researchers) is much less than the numb er of items (articl es). Collaborative filtering systems face two major challeng es in the research paper domain: scal-ability to high dimensional data and data sparsity. In a typical recommender system there are many items. For exa mple, Amazon.co m recommends specific books of interest from a large library of available books. Item-based approaches that determine similarity measures between items do not perform well since the item space is extremely large. A user based approach allows us to lever-age the relatively small number of users to create an efficient algorithm that scales well with the huge number of res earch papers published each year. An intuitive solution used by early collaborative filtering algorithms is to find users with similar preferences to the current user and recommend other items that group of users rated highly. Even with a r elatively small number of users, how-ever, this approach is computationally complex. The use of clustering algorithms to pre-determine groups of similar users has been used to sign ificantly increase performance [16].
 often rely on nearest neighbor schemes to map a new user to the existing user groups. It has been demonstrated that the accuracy of nearest neighbor al-gorithms is very poor for sparse data [4]. Subspace clustering is a branch of clustering algorithm that is able to find low dimensional clusters in very high-dimensional datasets. This approach to clustering allows our system to find groups of users who share a common interest in a particular field or sub-field regardless of differences in other fields. S earching for similar users across all of the items leads to finding users who share many common interests. We address the issue of high-dimensionality and sparsity of the data space by proposing a new approach to collaborative filtering utilizing subspace clustering principles. Furthermore, we propose a novel subspace clustering algorithm suited to sparse, binary data.
 such that there are m researchers R = { r 1 ,r 2 , ..., r m } and n articles A = { a ,a 2 , ..., a n } .Therow r i represents the interests of researcher i and consists of a list of articles A r i which indicates the user X  X  interest in those articles. In the research paper domain, this could indicate that the user has read or accessed a certain article. For a given session there is an active researcher ( r active  X  R )for which the collaborative filtering algorithm would like to recommend new articles that may be of interest to r active based on the researcher X  X  interest in the current session and the opinion of other like-minded researchers.
 els of like-minded researchers. Given the m  X  n matrix, we can find like-minded researchers by finding groups of researchers r  X  R who have expressed interest in similar articles a  X  A . The problem of finding such groups can be transformed intoaproblemof subspace clustering using the previously described binary ma-trix as input. The result of subspace clustering would be clusters, of researchers in corresponding subspaces of articles. Here, the underlying assumption is if a group of researchers have similar inter ests then they usually access similar sets of articles or vice-versa.
 Problem Statement. Given a binary m  X  n matrix, where rows represent m researchers ( R = { r 1 ,r 2 , ..., r m } ) and columns represent n articles ( A = { a ,a 2 , ..., a n } ), find subspace clusters of researchers, r  X  R , defined in subspaces of articles, a  X  A . The access patterns of the researchers c an be maintained by tracking the log of research papers they access. From th ese access patterns, we can generate a researcher/article data space as defined in the previous section. We infer that people accessing a similar set of articles ar e interested in the topics represented by the articles. Such a group is represented by a subspace cluster in the re-searcher/article data space. Finding these experts will ultimately help us achieve our goal of finding the groups of articles that form fields of interest. finding subspaces that form clusters, the n post-processing to remove redundancy. The output of these steps are sub-matrices of the original data matrix. Before discussing the above steps in detail, we will show how the proposed algorithm addresses the challenges associated with sparse, high-dimensional, binary-valued data in the research paper domain. 3.1 Challenges of the Domain -Addressed Sparsity and Binary Data. High sparsity means that for a given r i ,whichis a vector in n dimensional space, most of the ent ries in the vector are zeros. Since we are only interested in the values whic h are not zeroes, the original vector is transformed into a string containing positions of non-zero values. An example is shown in Figure-1. The result of the transformation is compact representation resulting in reduced memory requirem ents and less processing overhead. High-Dimensional Data. In high dimensional data, the number of possible subspaces is huge. For example, if there are N dimensions in the data, the num-ber of possible subspaces is 2 N . Hence, subspace clustering algorithms must devise an efficient subspace search strategy [17]. Most existing algorithms will work well when the number of dimensions i s relatively small. In the research pa-per domain there are thousands of dimensions and such approaches may become impractical. For example, based on our work in [17], we chose an efficient repre-sentative subspace clustering algorithm, MAFIA [7], to run on a data set with 1000 dimensions. The program ran out of memory because the algorithm was designed to be very efficient for datase ts with many rows but comparatively few dimensions. In the research paper domain, we have a unique property that the number of rows is significantly less than the number of dimensions. We overcome this challenge of high-dimensional dat a by devising an algorithm that exploits the row-enumeration space rather than the larger dimension space. representation of binary sparse data, we convert the challenges into advantages in our subspace clustering algorithm. The next two sections discuss the major steps of our algorithm in more detail. 3.2 Find Subspaces That Form Clusters Most straightforward way to find subspaces is to project the instance in all pos-sible subspaces and find the ones in which clusters are formed. Such an approach is not feasible for data with a large number of dimensions since number of pos-sible subspaces is exponential, O (2 N ). Therefore, we propose a search strategy which explores the smaller row-enumeration space.
 Subspace Search: The result of the transformation (as shown in Figure-1) of the high-dimensional data is a list of strings representing rows in the original data. We call the new, transformed dataset D . An example is shown in Figure-2. we first find the intersection between row 1 and row 2 . The result of the intersec-tion is 123 which represents a subspace in dimension 1, 2 and 3 with row 1 and row 2 as cluster members. 123 is stored as a key in a temporary hash table and the number of cluster members is stored as the value in the table. In addition, we also store the row-id as the values in the table in order to keep track of the cluster members for a given subspace. Next, row 1 is compared with row 3 and the intersection 23 is placed in the hash table. The intersection of row 1 and row 4 , 123 , is already present in the table, sothecountvalueisupdated.At the end of the pass for row 1 , the hash table will have two entries 123 and 23 . At this point, the entries in the temporary hash table are put in a global hash table which only accepts entries which ar e not already present in the global table and the temporary hash table is flushed. The rationale for having this rule is the following. When the search commences at row 2 , it will find the intersection 123 and eventually it will update the global hash table with its local one. Notice here that the subspace 123 has already been found during the search commencing from row 1 . Therefore, there is no requirement to update the global table. At the end of the search, the global hash table will have five entries, 5678 , 123 , 2 3 , 234 and 67 . Notice that the subspace 23 is subsumed by the subspace 1 23 or 234 . This redundant subspace is removed in the next step. The formal description of the algorithm is shown in Figure-3.
 Memory Usage: The main sources of memory consumption are the temporary and global hash tables, and the transformed dataset. The hash table memory requirement grows slowly since the temporary hash table is flushed every time a new search commences and the global hash table only contains previously un-found entries. Although use of a hash table may lead to some overhead of space due to unmapped entries, the advantage of constant time lookup greatly out-weighs such overhead. Generally, it was noticed that the memory requirements grew linearly and were stable during our experiments.
 Time Complexity: The algorithm takes advantage of the fact that the number of rows, m , is relatively small. As a result, th e subspace search is performed on the row enumeration space which is O( m 2 ). It should be noted that in our case, it is actually less than m 2 because if we are at row i ,weonlylookat row i +1 ,row i +2 , ..., row m . The algorithm also requires finding intersection of two strings which is performed in O(k) time where the k is the length of the strings. Notice that k is usually very small due to the high sparsity of the data. In summary, the total complexity is O( m 2 k ) .
 3.3 Post-processing to Remove Redundancy A larger subspace which contains sever al smaller subspaces covers more arti-cles more articles within the same field o f interest. Removing smaller subspaces subsumed by larger ones helps in making recommendation process faster in the absence of redundant subspace clusters.
 of such a collection is shown in Figure -4. The subspaces connected with arrows indicate two subspaces, one of which subsumes another. We must remove the fourth subspace which is 67 since it is subsumed by subspace 5678 .In general, to remove the redundant/subsumed subspaces in S ,weperformthe following steps: 1. Sort the set S according to the size of each subspace in descending order. 2. Take an element s i from the set, S , and pass through s i +1 ,s i +2 , ..., s | S | re-in S . Then performing step 2, starting with the first element of the set, we will remove all subsets of s 1 , in the first pass. Since s 1 is the largest subspace, without loss of generality, we can assume that there will be a large number of subsets of s 1 in S .Asaresult, | S | will shrink considerably and the remaining passes through S will be shorter, resulting in reduced running time.
 computing subsumption between two subspaces. Notice that p is quite small due to sparsity and | S | is shrinking with each iteration. The time complexity for sorting is O( | S | lg | S | ) , so the overall complexity is still O( | S | 2 p ) . 3.4 Finding Overlapping Subspaces In real world data or data which is highly sparse, it might be possible that subspace clusters in the form of sub-matrices will not be significant in number and size. In that case, we relax our subs pace search so that we can find clusters of irregular shape as opposed to strict sub-matrices. These irregularly shaped clusters are larger in size and cover more of the data. Overlapping of these subspace clusters can represent extende d or implicit relationships between both items and users which might be more interesting to the user.
 because they share a number of common subspaces. We apply a simple clustering algorithm to cluster the subspaces. For each element in the list of subspaces, the overlap between the given element and the other subspace clusters are found. The degree of overlap is controlled by threshold parameter indicating the percentage of dimensions that match. If the overlap is above the given threshold, the original subspace cluster is selected as a membe r of the cluster and is considered the seed of the cluster. For example in Figure-5, the seed of the new cluster found is the subspace ABCD . The other members of this cluster have some degree of overlap with ABCD . In this example, subspaces with at least one item in common with the seed, are put into the new cluste r. This process is repeated for each element from the list of subspaces, resulting in large clusters of subspaces. The user can control the threshold parameter depending on the domain and the data characteristics. A feature of this clustering process is that we allow a subspace to be member of several clusters as oppos ed to forming discrete partitions. This allows us to find all of the relationships that a subspace may participate in, instead of restricting it to one cluster membership. 3.5 Generating Recommendations The process of generating recommendations illustrated in Figure-6 involves map-ping a user X  X  query to the subspaces and making recommendations based on the subspaces. The query represents the curre nt selection of the active user. All of the subspaces containing the query item are collected and the matching sub-spaces are ranked based on the coverage of the query. Coverage is defined as the number of query items present in the subspace. The subspace with the highest coverage is ranked first and the ranked order of the subspaces determines the ranking of the recommendations. The recommendations are the elements in the subspace that are not part of the query. For example, in Figure-6 for the query ABC , the subspace ABCD has the highest coverage so the recommendation from that subspace, D , is ranked first. In the case of a tie while ranking, the subspace with the higher strength is picked first where the strength is defined as the number of cluster members present in the subspace. In Figure-6, subspaces BCDEF and XYAB have equal coverage. In this case, their ranking is determined by their cluster strengths. 3.6 Fall-Back Model Since the process of generating recommendations is dependent upon the success-ful mapping of a query to the subspace clustering model, we consider the scenario where a query is not covered by the subspace clustering model. Although this is a rare case (see Section 4.4), there must be a mechanism to handle such a scenario if it arises.
 query, the articles in the query are indexed in the matrix and the correspond-ing rows(researchers) are found. The articles in the rows are ranked according to their global frequency and the reco mmendations are made in the ranked or-der. This approach is similar to the user-based approach where the items in the nearest user-vector are recommended to the active user. Notice that the compu-tational requirements of the fall-back approach are minimal, consisting mainly of indexing the query articles and ranki ng according to the article frequency. Article frequency information can be maintained in a table enabling inexpensive look up cost.
 queried item exists in the user/item matrix. An interesting scenario is, when a researcher selects an article that does not exist in the researcher/article ma-trix. The consequence will be that both the fall-back model and the subspace clustering model will not be able to cover the query. In this case, the top-N frequent (or popular) articles are returned to the researcher. Here, the quality of the recommendation will be poorer than the fall-back model but the goal of covering the query will be satisfied. We first evaluate our subspace clustering algorithm using synthetic data and then evaluate the recommendation approach with the benchmark data. With synthetic data, we can embed clusters in specified subspaces. Since we know these locations we can check whether the clustering algorithm is able to recover the clusters. For evaluating the quality of recommendations, MovieLens [10] benchmark dataset has been used. We compare our approach using SCuBA with the baseline approach defined in Section 4.2. We present the experimental details in the following subsections. 4.1 Clustering Evaluation on Synthetic Data We have developed a synthetic data generator that allows us to embed clusters in subspaces. We can control the number and the size of such clusters and also size of the dataset. Apart from the clusters, the data space is filled with noise. For each row we place x noise values at random positions where x =  X   X  the number of dimensions. We set  X  = 1%. An example of a synthetic data with three clusters is shown in Figure-7.
 dimensions in increments of 1000. The number of rows is kept fixed at 1000. We embed 5 subspace clusters. Each subs pace cluster has 20 dimensions and 10 instances. Figure-8 shows the scalability of our algorithm as we increase the dimensions in the data. Notice that the curve is linear since our subspace search strategy is not dependent on the number of dimensions but rather it searches in the row-enumeration space.
 of subspaces with the number of dimensions in the data. In this case, we set the size of the subspace to be 1 percent of the number of dimensions. For example, when the number of dimensions is 5000, size of the subspace is 50. Here the running time is negligibly higher than the previous case but the curve is still linear. Higher running time is due to the computation of intersection, k , between two strings to check for redundancy as mentioned in Section 3.3. As discussed previously, the size of k is generally very small due to high sparsity of data. havior of SCuBA as the density of the dataset increases. Dimensionality of the dataset is fixed to 2000 and the number of instances to 1000. We embed 5 sub-space clusters each with 10 instances. T he subspace size is increased from 20 to 200 in steps of 20. It can be observed that running time increases with the density of the dataset but the curve remains linear.
 Here, accuracy is defined by the number of true positives and true negatives w.r.t. the recovered clusters. In all cases , the embedded clusters were completely recovered as shown in Table-1. No extr a clusters were rep orted even though  X  = 1% of noise is present in the data. 4.2 Recommendations from Benchmark Data Here we evaluate the quality of the recommendations made by the SCuBA ap-proach on the MovieLens dataset. In Section 1, we reviewed two approaches in CF and pointed out that memory-based approach produce high quality recom-mendations by finding the nearest neighbors of a target user. We use this as our baseline approach. It is quite practical to assume that users view or rate very few articles of the thousands of available. Since we want to make recommendations based on the few articles a user looks at, we show that when the number of selected terms are few, our approach pro duces higher quality recommendations than the baseline approach.
 mation retrieval systems. In our experiments, we define quality using precision which is the ratio between number of re levant results returned and the total number of returned results. We choose this measure for the following reasons. The goal of a recommender system is to present a small amount of relevant in-formation from a vast source of information. Therefore, it is more important to return a small number of recommendation s that contains relevant items rather than giving the user a large number of recommendations that may contain more relevant recommendations but also requires the user to sift through many ir-relevant results. The ratio between the n umber of relevant results returned and the number of true relevant results is defined as recall. Notice it is possible to have very high recall by making a lot of recommendations. In the research paper recommendation domain, a user will be more interested in reading papers that really qualify for his interests rather than going through a huge list of recom-mended papers and then selecting those which are of interest. Precision more accurately measures our abilit y to reach our goal than recall. Experimental Setup: We divide the data into training and testing sets with 80% used for training and 20% for testing. For our subspace clustering approach, we build subspace clustering models from the training data and for the baseline approach we use the training data to find similar users and make recommenda-tions based on those similar users. During the testing phase, for each user from the test set we take a certain percentage of the items from the test row. We call these query items . The rest of the items in the test row are called hidden items . We make recommendations (using both approaches) for the user, based on the query items. The list of recommended items are compared with the hid-den items and the intersection gives th e number of relevant recommendations (results) returned. This forms the basis of our precision measure.
 Results and Discussion: The precision curve in Figure-11 shows that we per-form better than the baseline approach as we reduce the percentage of query items. As the query items decrease, bot h relevant recommendations and total recommendations also decrea se. In SCuBA, the decrease in relevant recommen-dations is less than the decrease in total recommendations which is not the case with the baseline approach. Therefore an increase in the precision value is observed. The results validates the d iscussion presented in Section 1 where it was pointed out that although the user comparison approach produces very high quality recommendations, it will not perform well in our domain where we would like to make recommendations based on very few query terms. Moreover, user comparison approach does not scale linearly with the number of dimensions as shown in Figure-14. These results also verify the fact that more focussed relations are captured using our approach.
 ilarity between two vectors. The similarity calculations will be poor when there are only a few terms in the test row. In other words, this approach requires large user profiles (similar to e-commerce applications) to generate high quality rec-ommendations which in turn warrants user-tracking and raises privacy issues. In our case we do not require user-profiles that saves the overhead of user-tracking and preserves privacy as well. At a given instant, a researcher may be inter-ested in a new research topic(s), and if w e use the researcher X  X  previous profile or likings, we will not find relevant articles matching his/her current interest(s). With SCuBA approach we can overcome this challenge as shown in the precision curve. 4.3 Model Building Times In model-based approaches a model is created using item similarity [5]. Since, the complexity of building the similarity table is dependent on the number of items, this approach would be unnecessarily computationally expensive in the research paper domain where we have large number of articles but much smaller number of users. Our proposed solution takes advantage of the small number of users and avoids dependence on the number of items. Hence, we would expect that the time required to build models following the subspace clustering approach would be much less than the above approach in [5].
 we measure the time taken to build models from the two data sets used in the experiments. The subspace clustering approach clearly outperforms the item-item similarity approach. 4.4 Coverage Results Here we present statistics on query coverage of the subspace clustering model. As was discussed earlier, we anticipated the subspace clustering model will not be able to provide complete coverage of queries and hence we proposed a fall-back model. Results shown in Table-2 validate our hypothesis but more importantly they show that percentage of queries not covered by subspace clustering model is very low. This means fall-back model is rarely used and hence the overall quality of recommendation will not suffer too much.
 length denotes the number of terms considered in the test row. The total num-ber of test rows considered is 188 which is 20% of the complete dataset, 80% of which was used to construct the subspace cluster model. For queries of length 1, there were 16 test rows out of 188 for w hich no recommendation was made, or the miss percentage is 8.5%. For query l engths greater than 4, miss percentage become zero or some recommendation was made. 4.5 Comparing Subspace Cluster Model with Fall-Back Model In Section 3.6, fall-back model was introduced which is used when there is no mapping of query terms to the subspace cluster model. In these experiments we try to show that the fall-back model is not a replacement for subspace clustering model. The fall-back model helps to avoid those situations when subspace cluster model fails to make recommendations and it should be used for that purpose only. The experimental setup is kept the same as in Section 4.2. Again, we compare Precision for both the models, SCuBA as well as the fall-back. The precision curve in Figure-12 clearly shows SCuBA performs far better than the fall-back model for different values of quer y terms considered. These results also support the discussion in Section 3.6, the goal of the fall-back model is to provide coverage for query items even if the quality of recommendations is compromised. Research paper recommendation systems a re mainly content based systems, uti-lizing a combination of text based analysis as well as the citations of each paper to generate recommendations. Collaborative filtering algorithms have been very successful in other domains, however, and their application to research paper rec-ommendation has not been fully explored. Most collaborative filtering systems generate models in order to scale up massive numbers of users and items. We developed SCuBA, based on the principles of subspace clustering, to generate the collaborative filtering models to recommend research papers. This section explores related work in both recommender systems and subspace clustering. 5.1 Recommender Systems Content Based recommender s ystems attempt to determine relationships be-tween items by comparing inherent charac teristics of items. In research paper domain, the CiteSeer system utilizes the citation network between papers to find related papers [8]. CiteSeer also utilizes text-based analysis, but as a separate list of recommendations. Quickstep and FoxTrot both utilize ontologies of re-search topics to assist in recommendati on [15]. McNee et. al. propose a method to combined the citation network with various existing CF algorithms [14]. based approaches. The nearest neighbor, user-based approaches make recommen-dations by examine the preferences of similar users. Model-based approaches at-tempt to improve performance by building models of user and item relationships and using those models to make recommendations. Early CF systems compare the active user to all of the other users and found the k most similar users [22]. Weights are then assigned to items bas ed on the preferences of the neighbor-hood of k users, using a cosine or correlation function to determine the sim-ilarity of users. Recommendations are made using the weighted list of items. The recommendations produced are high quality and the systems are easily able to incorporate new or updated information, but the approaches do not scale well.
 These systems pre-build a user or item based model that is used to make rec-ommendations for an active user. There are two major categories of models, user based and item based. Aggarwal et. al. introduced a graph-based approach where the nodes were users and the edges their similarity. Bayesian probabil-ity networks also proved to be useful in building models [2]. Performance was further improved by using dependency networks [9]. Using a bipartite graph, Huang et. al. were able to find transitive associations and alleviate the sparsity problem found in many recommender system datasets [12]. Hofmann was able to discover user communities and prototypica l interest profiles using latent seman-tic analysis to create compact and accura te reduced-dimensionality model of a community preference space [11]. Sarwar et. al. used correlations between items to build models [21]. Recently, Demiriz b orrows from clustering approaches and uses a similarity measure to find rules, instead of an exact match [4]. 5.2 Subspace Clustering Algorithms Subspace clustering algorithms can be broadly categorized based on their search method, top-down or bottom-up [17]. Top down approaches search in the full dimensional space and refine the search through multiple iterations. Searching in all of the dimensions first means they are not well suited for sparse data such as that found with recommender systems. Bottom-up approaches first search for interesting areas in one dimension and build subspaces. This approach is much more suited to sparse datasets where clu sters are likely to be found using fewer than 1% of the dimensions.
 [1]. Adaptations to the basic method include ENCLUS which uses entropy in-stead of measuring density directly [3] and MAFIA which uses a data-driven adaptive method to form bins [7]. CLTree uses a decision tree algorithm to determine the boundaries of the bins [13]. Each of these algorithms focus on continuous valued data and do not perform well on categorical or binary data. Recently there have been subspace cluste ring algorithms developed for binary [18] and categorical data [19]. The few algorithms designed for both sparse and binary high-dimensional data do not cluster in subspaces of the dataset [6]. In this paper, we proposed a subspace clustering approach for recommender sys-tems aimed at the research paper domain. A useful source of information when recommending research papers is the reading habits of other researchers who are interested in similar concepts. Thus, we adopted a collaborative filtering ap-proach which allows us to use data collected from other researchers browsing patterns, and avoids issues with the interpretation of content. Such data con-sists of a small number of users (researc hers) and a very large number of items (research papers). Our proposed approach takes advantage of the unique charac-teristics of the data in this domain and provides a solution which is fast, scalable and produces high quality recommendations.
 tions, a more sophisticated ranking scheme could be developed as an extension to the algorithm. The algorithm could also be extended to include the subjective user ratings rather than treating them as binary values and categorize recom-mendations as strong, mediocre and weak. A lot of work has been done in mixing the two models, content based filtering and collaborative filtering, to generate a hybrid model which tries to enhance the recommendation quality.

