 Abstract Given the contemporary trend to modular NLP architectures and mul-tiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everyday X  X  NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences from a corpus-linguistic perspective.
 Keywords Linguistic annotation Multi-layer annotation Conflicting tokenizations Tokenization alignment Corpus linguistics 1 Motivation 1.1 Challenges of multi-layer annotations The field of multi-layer annotations has faced a rapid development in the last years: On the one hand, the range of tools for different NLP tasks and for manual annotation is continuously growing. This can be seen in the increased popularity of modular annotation and NLP architectures like GATE (Cunningham et al. 2002 )or UiMA (Ferrucci and Lally 2004 ).

On the other hand, there is a tendency to reuse existing corpora for new annotation tasks. As an example, the Penn Treebank (PTB) has originally been created as a resource annotated for morphology and syntax by Marcus et al. ( 1993 ), but later, subcorpora of the PTB were augmented with, e.g., different semantic annotations (Kingsbury and Palmer 2002 ; Pustejovsky et al. 2003 ; Meyers et al. 2004 ), various annotations for named entities and coreference (Vilain et al. 1995 ; Hovy et al. 2006 ; Poesio and Artstein 2008 ), and different types of discourse structure annotation (Carlson et al. 2003 ; Wolf and Gibson 2005 ; Prasad et al. 2008 ), just to name a few.
For both automatic and manual annotations, however, every new layer of annotations might very well come with its own requirements on tokenization . NLP tools may require a particular, task-specific tokenization to work properly; similarly, during manual annotations of previously tokenized data, annotation tools may perform a re-tokenization, or annotation guidelines may X  X n conjunction with a syntactic annotation, for example X  X equire a revision of earlier tokenization decisions.
The handling of multiple layers of annotations thus poses the problem that different automatic or manual annotation tools produce or require different tokenizations, thereby leading to the question how to deal with conflicting tokenizations. As an illustration for the extent of such conflicts, see Table 1 , where a number of taggers and parsers are compared on the OANC (Ide 2008 ) letter section. For this example, we find that different tokenizers disagree in their tokenization decisions for up to 10% of tokens. Consequently, only 35 X 75% of sentences receive identical tokenization.
Particularly remarkable in this table is the divergency between the OANC tokenization, the OpenNLP tokenization and the Brill (GPosTTL) tokenization, as all are assumed to follow the tokenization guidelines of the PennTreeBank (Marcus et al. 1993 ). This indicates that the question of divergent tokenizations does not only emerge in multi-layer annotated corpora, but also in evaluation scenarios, where the output of one or several NLP tools, say, the OpenNLP tagger and the (GPosTTL implementation of the) Brill tagger, are to be compared against a gold standard, say, the PennTreeBank-style annotations of the OANC.
 1.2 Tokenization Tokenization is the process of mapping sequences of characters to sequences of words . However, as it has often been discussed in the literature, the definition of the approximate words  X  X ut that is another non-trivial concept, cf. Manning-Schu  X  etze ( 1999 , 4.2.2). Word or sentence boundaries may be indicated by whitespaces or special characters, but neither does each such marker correspond to a boundary, nor is every boundary marked in such a way (examples will be discussed below). Across languages, different orthographic conventions have emerged, e.g. with respect to the separate or compound spelling of clitics, or to the use of whitespaces (whitespaces generally being optional, e.g. in Chinese, cf. Wu 1998 ). Accordingly, there is not a simple and universal solution to the task of tokenization.

Instead, we argue that the definition of the term  X  X  X ord X  X  depends on the research questions or applications of interest. Consider the examples in (2): For a shallow morphosyntactic analysis (part of speech tagging), a  X  X  X imple X  X  tokenization using whitespace and special characters as delimiters seems acceptable. A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each. (1) a. department store Similarly, examples (2a) and (2b) can be argued to represent a single token for (morpho)syntactic analyses . Despite intervening whitespace and special characters, they are complex instances of the  X  X  X lassical X  X  part of speech adjective .For semantic analyses (such as Information Extraction), however, it may be useful to split these compounds up, so that the embedded elements can be accessed ( E 605, No. 22 ). (2) a. E 605-intoxicated Finally, (3) illustrates a morphology-based tokenization strategy (token boundaries represented by square brackets): the principle of splitting at morpheme boundaries. Morphological tokenization may help distributional (cooccurrence-based) semantics and/or parsing; however, the resulting tokens might be argued to be less intuitive to users of a corpus search tool: While the morpheme-based tokens in (3.a) can be seen as standard lexemes and may thus be relatively  X  X atural X  tokens, the lexical status of tokens wo, ca and ai in (3.b) is less transparent and may therefore be unexpected for guidelines. (3) a. [ Mitchell ][  X  X  ], [ they ][  X  X e ], [ do ][ n X  X  ] These examples show that different applications (tagging, parsing, information extraction) and the focus on different levels of description (morphology, syntax, semantics) require specialized tokenization strategies. When working with multiple tools for standard NLP tasks, thus, it is the norm rather than the exception that they disagree in their tokenization, as shown in ex. (4) and (5). (4) doesn X  X  (5) the attorney general X  X  office
When creating a corpus that is annotated at multiple levels and/or using several tools, different tokenizations are not always avoidable, as some tools (automatic NLP tools, but also tools for manual annotation) have integrated tokenizers. Hence, the problem of handling conflicting tokenizations arises. Before we turn to our approach toward this problem, in the following subsection we summarize common applications. 1.3 Functions of tokens For most NLP tasks and linguistic annotations, especially those concerned with syntax (part-of-speech tagging, chunking, parsing) and the interpretation of syntactic structures (esp., the extraction of semantic information), tokens represent the minimal unit of analysis : words (lexemes, semantic units, partly morphemes) on the one hand and certain orthographic markers on the other hand. From a corpus-linguistic perspective, tokens also represent the minimal unit of investigation , the minimal character sequence that can be addressed in a corpus query (e.g. using search tools like TIGERSearch (Ko  X  nig and Lezius 2000 ) or CWB (Christ 1994 )). Tokens also constitute the basis for  X  X ord X  distance measurements. In many annotation tools and their corresponding formats, the order of tokens provides a timeline for the sequential order of structural elements, e.g., in TIGER XML (Ko  X  nig and Lezius 2000 ), MMAX (Mu  X  ller and Strube 2006 ), LAF/GrAF (Ide and Suderman 2007 ) or GENAU (Rehm et al. 2008 ). In several multi-layer formats, tokens also define the absolute position of annotation elements, and only by reference to a common token layer, annotations from different layers can be aligned with each other, e.g., in NITE (Carletta et al. 2003 ), or GENAU (Rehm et al. 2008 ).
Thus, by their function, tokens have the following characteristics: i. tokens are totally ordered, ii. tokens cover the full (annotated portion of the) primary data, iii. tokens are the smallest unit of annotation, and iv. there is only one single privileged token layer. Aspect (iv) is especially relevant for the study of richly annotated data, as an integration and serialization of annotations produced by different tools can be established only by referring to the token layer.

From a corpus-linguistic perspective, i.e., with a focus on querying annotated corpora, tokens need to be well-defined, and all annotations of a particular text are to be preserved without any corruption. In the following, we argue that for this purpose, characteristic (iii) is to be abandoned. We will describe a data format and an algorithm for merging different tokenizations and their respective annotations. 1.4 Goals and overview Our goal is a fully automated merging of annotations that refer to different tokenizations (henceforth tok 1 and tok 2 ) of the same text. With respect to general properties of tokens, we regard the following criteria as crucial: Information preservation . All annotations assigned to the original tokenizations should be preserved.
 Theoretically well-defined notion of token . It should be possible to give a plausible list of positive criteria that define character sequences as tokens. (From a corpus-linguistic perspective, knowledge about the token definition is essential for formulating queries for words, e.g. in a corpus interface.) Integrative representation . All annotations that are consistent with the merged tokenization should refer to the merged tokenization. (For corpus linguistics, this is necessary in order to query across multiple annotations originating from different annotation layers or tools.) Unsupervised merging . The integration of conflicting tokenizations should not require manual interference.
 Another aspect addressed in this paper is the question how token boundaries should be physically represented. Commonly applied solutions are, for example, to mark token boundaries by the space character ( X   X ) or a line break ( X  X n X ). This means that the primary data is to be manipulated: When the space character is used as token boundary, token-internal whitespaces have to be represented by replacement characters, e.g., an underscore ( X  X  X ). Line breaks may be regarded as seemingly better-suited token boundaries, because token-internal line breaks occur seldom, and are explicitly marked by a hyphen. Nevertheless, we lose information about the use of line breaks in the original data, i.e., paragraph boundaries may be lost. Such corruptions of the original data, however, makes reconciling/merging the data a difficult enterprise.

Given this background, we postulate another goal regarding the representation of token boundaries: XML markup . Token boundaries should preferably be marked without affecting the original primary data, according to modern standards by means of an XML-based annotation.
 In a straightforward XML model, tokens are represented by the introduction of XML markup in the file that represents the primary data, e.g., by elements that enclose primary text slices (cf. the BNC encoding scheme, Burnard 2007 ). For such an approach that imposes an XML hierarchy over the primary data directly, however, the existence of conflicting tokenizations is particularly challenging: The lossless representation of conflicting tokenizations by means of the XML hierarchy is impossible for tokenization conflicts as illustrated for the tokenizations in Fig. 1 (cf. Example (4.a) vs. (4.b) above).

In case of such conflicting tokenizations, the  X  X traightforward X  approach requires harmonization, i.e., a single tokenization has to be established. The next section describes a number of simple, unsupervised approaches that perform this harmoni-zation. To compensate for the deficits of these  X  X traightforward X  approaches, Sect. 3 then proposes to adopt an existing XML standoff format and to extend it by introducing structures smaller than tokens, called  X  X erminal nodes X  here. With these extensions, Sect. 4 describes a merging algorithm for annotations with conflicting tokenization and discusses its properties. Section 5 briefly enumerates a number of extensions to our scenario: With minimal modifications, the algorithm itself can be applied to merge more than one conflicting tokenization and to integrate not only alternative tokenizations, but also other structural elements that carry linguistic annotations. Further, the introduction of terminal nodes besides classical tokens can be exploited to represent zero-extension tokens and time-line information. Finally, Sect. 6 provides a summary and discussion of our proposal. 2 Conflicting tokenizations: straightforward strategies By  X  X traightforward X  strategies, we mean approaches that aim to preserve the definition of tokens as atomic, minimal, unambiguous units of annotation when unifying different tokenizations tok 1 and tok 2 of the same text. We call such strategies  X  X nsupervised X  when they operate on the primary data only, without consulting external resources such as dictionaries or human expertise. Unsupervised straightforward strategies to the task include: 1. No merging. In a conservative approach, we could create independent 2. Normalization. Adopt one of the source tokenizations, say tok 1 , as the 3. Maximal tokens. For every token boundary in tok 1 that is also found in tok 2 , 4. Maximal common substrings. For every token boundary in tok 1 or tok 2 , As shown in Table 2 , none of the strategies sketched above fulfills all criteria identified in Sect. 1.4 . Avoiding a merging process counteracts data integration; token normalization and maximal tokens violate information preservation, and maximal common substrings violate the requirement to specify a theoretically well-defined notion of token.

As an alternative, we propose a formalism for the lossless integration and representation of conflicting tokenizations by abandoning the assumption that tokens are an atomic, primitive concept that represents the minimal unit of annotation. Rather, we introduce annotation elements smaller than the actual token  X  so-called terminals (or term s for short)  X  that are defined according to the maximal common substring strategy described above.

Then, tokens are defined as nodes that span over a certain range of terms, similar to phrase nodes that dominate other nodes in syntax annotations. The representation of conflicting tokenizations, then, requires a format that is capable of expressing conflicting hierarchies. 3 Conflicting tokenizations in the PAULA format In this section, we introduce the PAULA format, an XML standoff format designated to deal with heterogeneous annotations, and especially with conflicting hierarchies. We describe the concept of terminal nodes as a novel extension to PAULA. For reasons of clarity, we distinguish between the original PAULA format  X  PAULA 1.0  X  , as described by Dipper ( 2005 ) and Dipper and Go  X  tze ( 2005 ), and the PAULA format in general, which also includes the extensions described below.

It should be noted that the algorithm developed on this basis in the next section is not restricted to PAULA XML. Instead, every standoff format with comparable characteristics (e.g., LAF/GrAF (Ide and Suderman 2007 )) can be processed using the same algorithm, given the necessary extensions as described for PAULA below. 3.1 Annotation structures in PAULA 1.0 The PAULA format (Dipper 2005 ; Dipper and Go  X  tze 2005 ) is a generic XML format, used as a pivot format for NLP pipelines (Stede et al. 2006 ) and the web-based corpus interface ANNIS (Chiarcos et al. 2008 ; Zeldes et al. 2009 ). It uses standoff XML representations, and is conceptually closely related to the formats NITE XML (Carletta et al. 2003 ) and GraF (Ide and Suderman 2007 ). 3
PAULA was specifically designed to support the lossless representation of different types of text-oriented annotations (layer-based/timeline annotations, hierarchical annotations, pointing relations), optimized for the annotation of multiple layers, including conflicting hierarchies and simple addition/deletion routines for annotations layers.

For these reasons, the primary data is stored in a separate file, and likewise, multiple annotations are also stored in separate files in order to avoid interference between concurrent annotations. Annotations refer either to the primary data or to other annotations by means of XLinks and XPointers.

With respect to linguistic annotations, the underlying object model distinguishes nodes (token, markable, struct), edges (dominance and pointing relations) and labels (annotations), as summarized in Table 3 . Since each type of annotation is stored in a separate file, competing or ambiguous annotations can be represented in an encapsulated way.
PAULA 1.0 is already sufficiently expressive for capturing the data heterogeneity sketched above, including the representation of overlapping segments, intersecting hierarchies, and alternative annotations (e.g., for ambiguous annotations), but only for annotations above the token level.

Regarding tokens, PAULA 1.0 relies on the existence of a unique layer of non-overlapping, atomic tokens as minimal units of annotation: 4 For all nodes, their position and sequential order is defined with respect to the absolute position of tokens that they cover; and for the special case of markables, these are defined solely in terms of their token range. Furthermore, PAULA 1.0 tokens are totally ordered , they cover the (annotated) primary data completely , and they are non-overlapping . Only on this basis, the extension and (token-)distance of annotated elements can be addressed; and only by means of unambiguous reference, information from different layers of annotations can be combined and evaluated. 3.2 Introducing terminal nodes In our extension of the PAULA format, we introduce the new concept of term nodes, atomic terminals that directly point to spans of primary data. Term s are subject to the same constraints as tokens in PAULA 1.0 (total order, full coverage, non-overlapping). Thus, terms can be used in place of PAULA 1.0 token s to define the extension and position of super-token level and sub-token level annotation elements.

Markables are then defined with respect to (spans of) terminal nodes rather than tokens, such that alternative tokenizations can be expressed as markables in different layers that differ in their extensions.

Although terms adopt several functions formerly associated with tokens, a privileged token layer is still required: In many query languages, tokens define the application domain of regular expressions on the primary data. More importantly, tokens constitute the basis for conventional ( X  X  X ord X  X ) distance measurements and ( X  X  X ord X  X ) coverage queries. Consequently, the constraints on tokens (total order, full coverage and absence of overlap) remain in the extended PAULA format. Differently from PAULA 1.0, we represent tokens as a subclass of structs rather than markables. So, it is possible to represent tokens as complex, hierarchically structured elements that may be composed of certain smaller entities.

Annotations that are incompatible with the desired tokenization can now be represented as structures that exist independently from tokens as structures over terminal nodes. Such annotations are thus preserved, although not with reference to the privileged token layer.
 The resulting specifications for structural units of annotation are summarized in Table 4 . Distinguishing terminal elements and re-defining the token layer as a privileged layer of structs allows us to disentangle the technical concept of  X  X tomic element X  and the linguistic concept of  X  X oken X  as the conventionally assumed minimal unit of analysis. 4 Merging annotation layers with conflicting tokenizations vBased on the extensions of the PAULA format proposed in the last subsection, we now turn to the description of a merging algorithm that makes use of these structures. 4.1 A merging algorithm In order to integrate annotations on tokens, it is not enough to represent two tokenizations side by side with reference to the same layer of terminal nodes. Instead, a privileged token layer is to be established and it has to be made sure that annotations can be queried with reference to the token layer . Then, all annotations whose segmentation is consistent with the privileged token layer are directly linked with tokens.

Algorithm 1 describes our merging algorithm, and its application to the four main following section that describes the main characteristics of the algorithm and the consequences for querying, we use the following notation: prim primary data tok , term annotation layers t [ Lt is a node (struct, markable, tok) on a layer L a .. b continuous span from tok/term a to tok/term b a , b list of tok/term/markable nodes a , b t = [ a ] t is a node that points to a node, span or list a Algorithm 1 produces a PAULA project with one single tokenization. The algorithm is unsupervised, the token concept of the output tokenization is well-defined and consistent (if one of the input tokenizations is adopted as target tokenization), and, as shown in the following subsections, the algorithm is efficient (running in polynomial time), it is integrative (allowing for queries across different tokeniza-tions), and it is information-preserving (reversible). 4.2 Time complexity After a PAULA project has been created, the time complexity of the algorithm is quadratic with respect to the number of characters in the primary data n . This is due to the total order of tokens: Step 2 and 3.a are applied once to all original tokens from left to right. Step 5 can be reformulated such that for every terminal node , the relationship between the directly dominating tok 1 and tok 2 is checked. Then, Step 5 is also in O ( n ). In terms of the number of markables m , the time complexity in Step 3.b is in O ( nm ): for every markable, the corresponding term element is to be found, taking at most n repositioning operations on the term layer. Assuming that markables within one layer are non-overlapping 5 and that the number of layers is scenarios, the algorithm is thus quadratic. 4.3 Reversibility The merging algorithm is reversible  X  and, thus, lossless  X  using the splitting algorithm given in Algorithm 2 . A correctness proof for this algorithm can be constructed on the basis of the following insight: Every node that corresponds to an original token of the  X  X ther X  tokenization is removed, plus every node that points to it, so that only annotations remain that are directly applied to the target tokenization. 4.4 Querying merged tokenizations In this paper, we focus on the merging of analyses with different tokenizations primarily for the scenario of corpus linguists issuing queries across multiple annotation layers. Although the merging algorithm produces annotation projects that allow for queries that integrate annotations from analyses with different tokeniza-tions, the structure of the annotations is altered, such that the behaviour of merged and unmerged PAULA projects may be different: Obviously, token-level queries must refer to the new privileged tokenization, which may be different from the original tokenization.

We distinguish three classes of query operators: (i) precedence and extension operators, (ii) distance operators, and (iii) dominance operators. 7 (i) In terms of PAULA 1.0 data structures, an extension operator checks whether (ii) In terms of PAULA 1.0 data structures, (token-)distance operators restrict (iii) Also for dominance operators , query results from the merged annotation Accordingly, queries applicable to PAULA projects before the merging have behave differently on merged PAULA projects  X  although equivalent queries can always be found, as all information is preserved. 10 Users are to be instructed to keep this in mind and to read the specifications for the merged tokenization and its derivation. 5 Extensions In the following, we describe some extensions of the merging algorithm, and some additional ways of using the newly introduced term layer to represent non-token annotations. 5.1 Merging more than two tokenizations In the current formulation, Algorithm 1 is applied to PAULA 1.0 projects and generates extended PAULA annotation projects with a term layer. The algorithm, however, may be applied iteratively, if step 3 is slightly revised, see Algorithm 3 . 5.2 Annotation integration The merging algorithm creates a struct node for every original token. Although this guarantees reversibility, one may consider to remove such redundant structs. Algorithm 4 proposes an optional postprocessing step for the merging algorithm. This step is optional because these operations are destructive: We lose the information about the origin ( analysis 1 vs. analysis 2 )of stok (super-token) elements and their annotations. 5.3 Tokens with zero extension Another extension of our approach applies to information that can be expressed with the help of our format extensions, but which previously required tampering with the original primary data.

One example is the representation of zero extension elements, e.g., traces or dropped pronouns. Traditionally, these are represented by special marks in the primary data, that are then annotated just like ordinary tokens. For example, the PTB-style syntactic annotation of the OntoNotes corpus (Hovy et al. 2006 ) makes use of marks like *T*-1 for some trace t 1 with index 1.

We already pointed out that modifications to the primary data impedes a merging with other annotation layers where such modifications have not been performed, e.g., the OntoNotes named-entity annotation. Such a merging requires the development and the application of scripts that copy these special tokens to another annotation layer. But even if this labor-intensive merging succeeds, then word distance is affected by zero extension elements. So, in example (14) below, the token distance between persuade and to would be 3, although both tokens are actually adjacent and a token distance of 1 would be expected.

In a token-focused standoff format such as LAF/GrAF or PAULA 1.0, another problem arises. In principle, it would be possible to model zero-extension tokens as elements that point to a particular position in the primary data, e.g., a whitespace character. For zero extension elements between  X  X ormal X  tokens, this would work fine, but when multiple subsequent zero extension tokens follow one another and only one whitespace character is available to provide an anchor in the primary data, we can no longer represent their relative order.

Examples for multiple subsequent zero extension elements can be found in the linguistic literature. Multiple subsequent traces occur in multiple NP-fronting in German (9), multiple Wh-phrase fronting in Russian (10), small clause scrambling in Russian (11), in analyses for subject postposing in Russian (12), but also in English small clause fronting (13). Similarly, traces can occur before or after a PRO element, as in the English example (14). (10) Kogda j kogo i Ivan videl t i t j ? (11) P X  X anym j my Ivana i nikogda ne videli t i t j . (12) Segodnja prigotovit zavtrak j Mario i t i t j . (13) [ VP Criticize John i ] j , I said he i should not [ VP t i t j ]. Heycock ( 1995 , (10)) (14) Who i did you persuade t i PRO to buy what j ? Using the concept of terminal nodes as introduced here, we can just create one terminal node for every zero-extension element that covers no primary data. Terminal nodes are totally ordered, so their relative order is firmly established without reference to the primary data. Their absolute position can be contextually determined from the nearest  X  X rdinary X  terminal node that has a reference to the primary data.
On the token level, then, these zero extension terminal nodes are not being referred to. In this way, zero extension elements can be queried by precedence, extension and dominance operators as any other annotation that is defined over terminal nodes. Zero extension elements do not, however, interfere with word distance queries that are defined with respect to the token layer alone. 5.4 Other forms of non-textual information Similar to zero-extension elements, other forms of non-textual information can be represented as terminal nodes. As one example, transcriptions of spoken language are traditionally composed of both linguistic information and metadata including time-stamps, background noise, interruptions, etc. An excerpt from the CHRISTINE corpus (Sampson 1999 ), a classical resource of spoken British English, is given in Table 5 .
As Table 5 shows, the primary data column of the CHRISTINE corpus includes pauses, event references and other forms of meta-data. In a conventional sense, such information does not constitute a token proper, although they are represented in the primary data (i.e., the transcript). In our approach, such elements can be represented as terminal nodes that are not dominated by a token, so that, again, they can be accessed by precedence and extension operators, but they do not interfere with word distance queries.
 5.5 Terminal nodes as a timeline In a somewhat more radical way, terminal nodes can establish a timeline for linguistic annotations that do not have an unambiguous layer of primary data. In corpora with both phonemic and orthographic transcription (e.g., Kohler 1996 ), neither one nor the other constitutes necessarily a single level of maximum granularity. Even the character level does not fulfill this function: See for example, many-to-many relationships between graphemes and phonemes as shown in Fig. 3 for English, and in Examples (15) and (16) for Ukrainian. (15) orthography: cc (duplication indicates voicelessness between vowels) (16) orthography: o order that will be relevant for querying the data. For this purpose, we suggest the explicit definition of a timeline which establishes a total order between individual events on any level. Like other forms of non-textual information, timestamps can be represented as elements of the terminal layers, thus leading to a natural alignment between textual and temporal precedence. In this way, the layer of terminal nodes actually constitutes such a timeline. 6 Summary and conclusions In this paper, we argued that conflicting tokenizations are a pervasive problem both in scenarios of manual and automatic annotation of linguistic data, as soon as multiple levels of annotation are being produced. After identifying the characteristics of the  X  X oken X  concept, we proposed to abandon the idea that tokens constitute the atomic unit of description, and we introduced  X  X erminal elements X  as a supplement. On this basis, we modeled tokens in a way comparable to ordinary structured units that dominate one or more terminal nodes.

We showed that standard, hierarchical XML models are insufficient to deal with tokenization conflicts and thus argued that a standoff approach is necessary. Taking one specific such standoff format as a representative, we proposed to extend the PAULA format with terminal nodes. On this basis, we developed an algorithm for merging alternative X  X ossibly conflicting X  X okenizations, and we demonstrated that this algorithm is efficient (running in quadratic time), lossless (reversible), and integrative (allows for queries over annotations of different origin).

We conclude the paper with a number of observations, including pointers to some related work.

First, notice that terminals are atomic units only within the annotation project at hand (there is no unit addressed that is smaller than a terminal). By iterative applications of the merging algorithm, however, complex terms may be split up into smaller units, so that they are not atomic in an absolute sense. Alternatively, terms could be identified a priori if minimal addressable units are available, e.g., characters (as in the formalization of tokens as charspan s and charseq s in the ACE information extraction annotations, Henderson 2000 ). It is not clear, however, how a character-based term definition would deal with sub-character and zero extension terms: A character-based definition of terms that represent traces is possible only by corrupting the primary data. 11 Consequently, a character-based term definition is insufficient unless we restrict ourselves to a particular class of languages, texts and phenomena.

The role of terminals can be compared to timestamps: With reference to a numerical time-line, it is always possible to define a new event between two existing timestamps. Formats specifically designed for time-aligned annotations, e.g., EXMARaLDA (Schmidt 2004 ), however, typically lack a privileged token layer and a formal concept of tokens. Instead, tokens, as well as longer or shorter sequences, are represented as markables for the annotation process, defined by their extension on the timeline.
 Similarly, GrAF (Ide and Suderman 2007 ), although being historically related to PAULA, does not have a formal concept of a privileged token layer in the sense of PAULA. 12 We do, however, assume that terminal nodes in GrAF can be compared to PAULA 1.0 tokens.

For conflicting tokenizations, Ide and Suderman ( 2007 ) suggest to define  X  X ummy X  elements, which cover necessary tokenizations for controversially tokenized stretches of primary data. Such dummy elements combine the possible tokenizations for strategies 1 (no merging) and 3 (maximal tokens), so that the information preservation deficit of strategy 3 is compensated by strategy 1, and the integrativity deficit of strategy 1 is compensated by strategy 3 (cf. Table 2 ). However, tokens, if defined in this way, are overlapping and thus only partially ordered, so that distance operators are no longer applicable. 13
Another problem that arises from the introduction of dummy nodes is their theoretical status as compared to nodes that represent annotation elements, i.e., it is not clear how both dummy nodes can be distinguished from annotation structured on a conceptual level. In the PAULA formalization, dummy nodes are not necessary, so that this ambiguity is already resolved in the representation. References
