 Different Information Retrieval (IR) tasks require different evaluation metrics. For example, a patent survey task may require a recall-oriented metric, while a known-item search task [16] may require a precision-oriented metric. When we search the Web, for example, we often stop going through the ranked list after finding one good Web page even though the list may contain some more relevant pages, either knowing or assuming that the rest of the retrieved pages lack novelty , or additional information that may be of use to him. Thus, finding exactly one relevant document with high precision is an important IR task.
Reciprocal Rank (RR) [16] is commonly used for the task of finding one rel-evant document: RR = 0 if the ranked output does not contain a relevant doc-ument; otherwise, RR =1 /r 1 ,where r 1 is the rank of the retrieved relevant document that is nearest to the top of the list. However, RR is based on binary relevance and therefore cannot distinguish between a retrieved highly relevant document and a retrieved partially relevant document. Thus, as long as RR is used for evaluation, it is difficult for researchers to develop a system that can rank a highly relevant document above partially relevant ones. In light of this, Sakai [10] proposed a metric called O-measure for the task of finding one highly relevant document. O-measure is a variant of Q-measure which is very highly correlated with Average Precision (AveP) but can handle graded relevance [8,14]. O-measure can also be regarded as a generalisation of RR (See Section 3).
Eguchi et al. [4], the organisers of the NTCIR Web track, have also proposed a metric for the task of finding one highly relevant document, namely, Weighted Reciprocal Rank (WRR). WRR assumes that ranking a partially relevant docu-ment at (say) Rank 1 is more important than ranking a highly relevant document at Rank 2. It has never actually been used for ranking the systems at NTCIR (See Section 3) and its reliability is unknown. We point out in Section 3 that, if WRR must be used, then it should be normalised before averaging across topics: We call the normalised version Normalised Weighted Reciprocal Rank (NWRR).

Just like RR, both O-measure and NWRR rely on r 1 , the rank of the first relevant document in the list. This means that all of these metrics assume that the user stops examining the ranked list as soon as he finds one relevant docu-ment, even if it is only partially relevant . This assumption may be valid in some retrieval situations, but not always, as we shall discuss in Section 3. In contrast, P-measure , recently proposed at the SIGIR 2006 poster session [12], assumes that the user looks for a highly relevant document even if it is ranked below partially relevant documents . We shall also discuss its variant called P + (pee-plus)-measure in Section 3.

Thus we have at least five metrics for the task of finding one relevant docu-ment: P + -measure, P-measure, O-measure, NWRR and RR. All of them except for RR can handle graded relevance, as illustrated in Figure 1. (Section 2 will touch upon all of the metrics shown in this figure.) This paper compares the sen-sitivity of these five metrics using a method recently proposed at SIGIR 2006 [11]. This method derives the sensitivity and the performance difference required to guarantee a given significance level directly from Bootstrap Hypothesis Tests, and therefore has a theoretical foundation unlike the ad hoc (but nevertheless useful)  X  X wap X  method proposed by Voorhees and Buckley [17]. We use four data sets (i.e., test collections and submitted runs) from NTCIR [6] and conduct extensive experiments. Our results show that  X  X ( + )-measure  X  O-measure  X  NWRR  X  RR X  generally holds, where  X   X   X  means  X  X s at least as sensitive as X . These results generalise and reinforce previously reported ones which were based only on ad hoc methods such as the swap method. We therefore recommend the use of P( + )-measure and O-measure for practical tasks such as known-item search where recall is either unimportant or immeasurable.

The remainder of this paper is organised as follows. Section 2 discusses previ-ous work on evaluating the sensitivity of IR metrics to clarify the contribution of this study. Section 3 formally defines and characterises the metrics we examine. Section 4 describes the Bootstrap-based method for assessing the sensitivity of metrics [11]. Section 5 describes our experiments for comparing the sensitivity of P( + )-measure, O-measure, NWRR and RR. Section 6 concludes this paper. Table 1 provides an overview of representative studies on the stability and sensi-tivity of IR evaluation metrics or evaluation environments. Metrics for the task of finding one relevant document (as opposed to finding as many relevant doc-uments as possible) are shown in bold, as we are focussing on these metrics. Below, we shall briefly review these studies and clarify the contribution of the present one.

At SIGIR 2000, Buckley and Voorhees [1] proposed the stability method which can be used for assessing the stability of IR metrics with respect to change in the topic set. The primary input to the stability method are a test collection, a set of runs submitted to the task defined by the test collection and an IR metric. The essence of the method is to compare systems X and Y in terms of an IR metric using different topic sets and count how often X outperforms Y , how often Y outperforms X and how often the two are regarded as equivalent. To this end, the stability method generates B new topic sets (where typically B = 1000) by sampling without replacement from the test collection topics. Using this method, Buckley and Voorhees examined binary IR metrics such as Average Precision (AveP), R-Precision (R-Prec), Precision at a given document cut-off (PDoc) and Recall at document cut-off 1000 (Recall@1000).

At SIGIR 2002, Voorhees and Buckley [17] proposed the swap method for assessing the sensitivity of IR metrics. The essence of the swap method is to estimate the swap rate , which represents the probability of the event that two experiments (using two different topic sets) are contradictory given an overall performance difference. Just like the stability method, the swap method samples topics without replacement, but generates B disjoint pairs of new topic sets to establish a relationship between the swap rate and the overall performance difference between a system pair.

Several studies followed that used the stability and swap methods for assess-ing different IR metrics: Buckley and Voorhees [2] assessed their bpref (binary preference) metric to deal with incomplete relevance judgments; Voorhees [18] assessed the area measure and Geometric Mean AveP (G AveP) to emphasise the effect of worst-performing topics. Soboroff [16] assessed RR for the TREC Web known-item search task. Moreover, Sanderson and Zobel [15] and Sakai [9] explored some variants of the swap method, including one using topic sampling with replacement which allows duplicate topics within each set.

All of the aforementioned studies (except for Sakai [9]) used the TREC data and considered binary IR metrics only. Among them, only Soboroff [16] consid-ered the task of finding one relevant document, by examining RR.

At AIRS 2005, Sakai [8,14] reported on stability and swap experiments that included IR metrics based on graded relevance, namely, Q-measure , R-measure and normalised (Discounted) Cumulative Gain (n(D)CG) [7]. But this study was limited to IR metrics for the task of finding as many relevant documents as possible. As for the task of finding one highly relevant document, two previous studies based on the stability and the swap methods reported that P-measure [12] and O-measure [10] may be more stable and sensitive than RR. As Table 1 shows, all of these studies involving graded-relevance metrics [8,10,12,14] used either the NTCIR-3 or the NTCIR-5 data, but not both.

All of the aforementioned studies used the stability and the swap methods, which have proven to be very useful, but are rather ad hoc. In light of this, Sakai [11] proposed a new method for assessing the sensitivity of IR metrics at SIGIR 2006, which relies on nonparametric Bootstrap Hypothesis Tests [3]. This method obtains B bootstrap samples by sampling with replacement from the original topic set, conducts a Bootstrap Hypothesis Test for every system pair, and estimates an absolute difference req uired to guarantee a given significance level based on Achieved Significance Levels. However, the above SIGIR paper only dealt with IR metrics for finding as many relevant documents as possible, including AveP, Q-measure and Geometric Mean Q-measure (G Q-measure).
As it is clear from the bottom of Table 1, this paper complements a SIGIR 2006 poster [12] by assessing IR metrics for the task of finding one relevant document based on the aforementioned Bootstrap-based method instead of the ad hoc stability and swap methods. This paper also extends the poster by us-ing four data sets, namely, NTCIR-3 and NTCIR-5 Chinese/Japanese data, and also by examining two additional metrics (P + -measureandNWRR).Inaddi-tion, it studies how changing gain values (See Section 3) affects the sensitivity of P( + )-measure and O-measure. This paper can also be regarded as an extension of the Bootstrap paper [11], where the IR tasks being considered are differ-ent: finding as many relevant documents as possible versus finding one relevant document. This section formally defines and characterises P( + )-measure, O-measure and NWRR. (We have already defined RR in Section 1.) Prior to this, we also define AveP and Q-measure since we include them in our experiments just for compar-ison. (The AveP and Q-measure results have been copied from [11], and are not part of our contribution.) 3.1 AveP and Q-measure Let R denote the number of relevant documents for a topic, and count ( r )denote the number of relevant documents within top r of a system output of size L (  X  1000). Clearly, Precision at Rank r canbeexpressedas P ( r )= count ( r ) /r . Let isrel ( r )be1ifthedocumentatRank r is relevant and 0 otherwise. Then, AveP can be defined as:
Next, we define Q-measure [8,14], which is very highly correlated with AveP but can handle graded relevance. Let R ( L ) denote the number of L -relevant documents so that L R ( L )= R ,andlet gain ( L )denotethe gain value for retrieving an L -relevant document. In the case of NTCIR, L = S (highly rele-vant), L = A (relevant) or L = B (partially relevant), and we use gain ( S )= 3 ,gain ( A )=2 ,gain ( B ) = 1 by default. Let cg ( r )= 1  X  i  X  r g ( i )denotethe cumulative gain at Rank r for a system output [7], where g ( i )= gain ( L )ifthe document at Rank i is L -relevant and g ( i ) = 0 otherwise. Similarly, let cg I ( r ) denote the cumulative gain at Rank r for an ideal ranked output: For NTCIR, an ideal ranked output lists up all S-, A-and B-relevant documents in this order. Then, Q-measure is defined as: where BR ( r ) is called the blended ratio , which measures how a system output devi-ates from the ideal ranked output and penalises  X  X ate arrival X  of relevant docu-ments. (Unlike the blended ratio, it is known that weighted precision WP ( r )= cg ( r ) /cg I ( r ) cannot properly penalise late arrival of relevant documents and is therefore not suitable for IR evaluation [8,10,14].) 3.2 O-measure and NWRR Traditional IR assumes that recall is important: Systems are expected to return as many relevant documents as possible. AveP and Q-measure, both of which are recall-oriented, are suitable for such tasks. (Note that the number of relevant documents R appear in their definitions.) However, as was discussed in Section 1, some IR situations do not necessarily require recall. More specifically, some IR situations require one relevant document only. Although RR is commonly used in such a case, it cannot reflect the fact that users prefer highly relevant docu-ments to partially relevant ones. Below, we describe O-measure and Normalised Weighted Reciprocal Rank (NWRR), both of which can be regarded as graded-relevance versions of RR.

O-measure [10] is defined to be zero if the ranked output does not contain a relevant document. Otherwise: That is, O-measure is the blended ratio at Rank r 1 . (Since the document at r 1 is the first relevant one, cg ( r 1 )= g ( r 1 )and count ( r 1 ) = 1 hold.) In a binary relevance environment, O -measure = RR holds iff r 1  X  R ,and O -measure &gt; RR holds otherwise. Moreover, if small gain values are used with O-measure, then it behaves like RR [10].

Next, we define Weighted Reciprocal Rank (WRR) proposed by Eguchi et al. [4]. Our definition looks different from their original one, but it is easy to show that the two are equivalent [13]. In contrast to cumulative-gain-based metrics (including Q-measure and O-measure) which require the gain values ( gain ( L )) as parameters, WRR requires  X  X enalty X  values  X  ( L )( &gt; 1) for each relevance level L .Welet  X  ( S )=2 , X  ( A )=3 , X  ( B ) = 4 throughout this paper: note that the smallest penalty value must be assigned to highly relevant documents. WRR is defined to be zero if the ranked output does not contain a relevant document. Otherwise: where L 1 denotes the relevance level of the relevant document at Rank r 1 .
WRR was designed for the NTCIR Web track, but the track organisers always used  X  ( L )=  X  for all L , so that WRR is reduced to binary RR. That is, the graded relevance capability of WRR has never actually been used.

WRR is not bounded by one: If the highest relevance level for a given topic is denoted by M , WRR is bounded above by 1 / (1  X  1 / X  ( M )). This is unde-sirable for two reasons: Firstly, a different set of penalty values yields a differ-ent range of WRR values, which is inconvenient for comparisons; Secondly, the highest relevance level M may not necessarily be the same across topics, so the upperbound of WRR may differ across topics. This means that WRR is not suitable for averaging across topics if M differs across the topic set of the test collection.

This paper therefore considers Normalised WRR (NWRR) instead. NWRR is defined to be zero if the ranked output does not contain a relevant document. Otherwise: The upperbound of NWRR is one for any topic and is therefore averageable.
There are two important differences between NWRR and O-measure. (a) Just like RR, NWRR disregards whether there are many relevant documents (b) NWRR assumes that the rank of the first retrieved document is more impor-
We first discuss (a). From Eq. (5), it is clear that NWRR depends only on the rank and the relevance level of the first retrieved relevant document. For example, consider a system output shown in the middle of Figure 2, which has an S-relevant document at Rank 3. The NWRR for this system is (1  X  1 / X  ( S )) / (3  X  1 / X  ( S )) = (1  X  1 / 2) / (3  X  1 / 2) = 1 / 5for any topic. Whereas, the value of O-measure for this system depends on how many L -relevant documents there are. For example, if the system output was produced in response to Topic 1 which has only one S-relevant document (and no other relevant documents), then, as shown on the left hand side of Figure 2, O -measure =( g (3) + 1) / ( cg I (3) + 3) = (3 + 1) / (3 + 3) = 2 / 3. On the other hand, if the system output was produced in response to Topic 3 which has at least three S-relevant documents, then, as shown in the right hand side of the figure, O -measure =(3+1) / (9 + 3) = 1 / 3. Thus, O-measure assumes that it is relatively easy to retrieve an L -relevant document if there are many L -relevant documents in the database . If the user has no idea as to whether a document relevant to his request exists or not, then one could argue that NWRR may be a better model. On the other hand, if the user has some idea about the number of relevant documents he might find, then O-measure may be more suitable. Put another way, O-measure is more system-oriented than NWRR.

Next, we discuss (b) using Topic 3 shown in Figure 3, which has one S-relevant, one A-relevant and one B-relevant document. System X has a B-relevant docu-ment at Rank 1, while System Y has an S-relevant document at Rank 2. Regard-less of the choice of penalty values (  X  ( L )), X always outperforms Y according to NWRR. Thus, NWRR is unsuitable for IR situations in which retrieving a highly relevant document is more important than retrieving any relevant doc-ument in the top ranks . In contrast, O-measure is free from the assumption underlying NWRR: Figure 3 shows that, with default gain values, Y outper-forms X .Butif X should be preferred, then a different gain value assignment (e.g. gain ( S )=2 ,gain ( A )=1 . 5 ,gain ( B ) = 1) can be used. In this respect, O-measure is more flexible than NWRR. 3.3 P-measure and P + -measure Despite the abovementioned differences, both NWRR and O-measure rely on r , the rank of the first retrieved relevant document. Thus, both NWRR and O-measure assume that the user stops examining the ranked list as soon as he finds one relevant document, even if it is only a partially relevant one . This assumption may be counterintuitive in some cases: Consider System Z in Figure 3, which has a B-relevant document at Rank 1 and an S-relevant document at Rank 2. According to both NWRR and O-measure, System Z and System X are always equal in performance regardless of the parameter values, because only the B-relevant document at Rank r 1 = 1 is taken into account for Z .Inshort,both NWRR and O-measure ignore the fact that there is a better document at Rank 2.
This is not necessarily a flaw. NWRR and O-measure may be acceptable models for IR situations in which it is difficult for the user to spot a highly relevant document in the ranked list. For example, the user may be looking at a plain list of document IDs, or a list of vague titles and poor-quality text snippets of the retrieved documents. Or perhaps, he may be examining the content of each document one-by-one without ever looking at a ranked list, so that he has no idea what the next document will be like. However, if the system can show a high-quality ranked output that contain informative titles and abstracts, then perhaps it is fair to assess System Z by considering the fact that it has an S-relevant document at Rank 2, since a real-world user can probably spot this document. Similarly, in known-item search [16], the user probably knows that there exists a highly relevant document, so he may continue to examine the ranked list even after finding some partially relevant documents. (A  X  X arrow X  definition of known-item search would involve only one relevant document per topic. That is, the target document is defined to be the one that the user has seen before, and it is always highly relevant. However, we adopt a broader definition: In addition to the known highly relevant document, there may be unvisited documents which are in fact relevant to the topic. It is possible to treat these documents as partially relevant in evaluation. Moreover, there is a related task called suspected-item search [5], which does not require that the user has actually seen a relevant document. It is clear that more than one relevant document may exist in such cases too, possibly with different relevance levels.)
We now define P-measure [12] for the task of finding one highly relevant doc-ument, under the assumption that the user continues to examine the ranked list until he finds a document with a satisfactory relevance level . P-measure is defined to be zero if the system output does not contain a relevant document. Other-wise, let the preferred rank r p be the rank of the first record obtained by sorting the system output, using the relevance level as the primary sort key (preferring higher relevance levels) and the rank as the secondary sort key (preferring the top ranks). Then: That is, P-measure is simply the blended ratio at Rank r p .ForSystem Z in Figure 3, r p = 2. Therefore, P -measure = BR (2) = ( cg (2) + 2) / ( cg I (2) + 2) = (4 + 2) / (5 + 2) = 0 . 86. Whereas, since r p = r 1 holds for systems X and Y , P -measure = O -measure =0 . 50 for X and P -measure = O -measure =0 . 57 for Y .Thus,only Z is handsomely rewarded, for retrieving both B-and S-relevant documents.

Because P-measure looks for a most highly relevant document in the ranked output and then evaluates by considering all (partially) relevant documents ranked above it, it is possible that P-measure may be more stable and sensi-tive than O-measure, as we shall see later. Moreover, it is clear that P-measure inherits some properties of O-measure: It is a system-oriented metric, and is free from the assumption underlying NWRR.

However, just like R-measure [8,14], P-measure is  X  X orgiving X , in that it can be one for a suboptimal ranked output. For example, in Figure 3, supppose that there is a fourth system output, which is a perfect inverse of the ideal output. For this system output, r p = 3 and therefore P -measure = BR (3) = (6 + 3) / (6 + 3) = 1. One could argue that this is counterintuitive. We therefore examine P + (pee-plus)-measure in addition, which does not have this problem: For example, for the above perfect inverse output, BR (1) = (1 + 1) / (3 + 1), BR (2) = (3 + 2) / (5 + 2) and BR (3) = P -measure =1.Thus P + -measure = (2 / 4+5 / 7+1) / 3=0 . 74. Note also that P + -measure = P -measure = O -measure holds if there is no relevant document above Rank r p , i.e., if r p = r 1 .
In practice, a document cut-off may be used with P( + )-measure, since these metrics assume that the user is willing to examine an  X  X nlimited X  number of documents. That is, in theory, r p can be arbitrarily large. However, a small cut-off makes IR evaluation unstable, and requires a larger topic set [1,8,14]. This section briefly describes Sakai X  X  Bootstrap-based method for assessing the sensitivity of IR metrics [11].

First, we describe the paired Bootstrap Hypothesis Test, which, unlike tradi-tional significance tests, is free from the normality and symmetry assumptions and yet has high power [3]. The strength of the Bootstrap lies in its reliance on the computer for directly estimating any data distribution through resampling from observed data. Let Q be the set of topics provided in the test collection, and let | Q | = n .Let x =( x 1 ,...,x n )and y =( y 1 ,...,y n ) denote the per-topic performance values of systems X and Y as measured by some performance met-ric M . A standard method for comparing X and Y is to measure the difference between sample means  X  x = i x i /n and  X  y = i y i /n such as Mean Average Pre-cision values. But what we really want to know is whether the population means for X and Y (  X  X and  X  Y ), computed based on the population P of topics, are any different. Since we can regard x and y as paired data, we let z =( z 1 ,...,z n ) where z i = x i  X  y i ,let  X  =  X  X  X   X  Y and set up the following hypotheses for a two-tailed test: Thus the problem has been reduced to a one-sample problem [3]. As with stan-dard significance tests, we assume that z is an independent and identically dis-tributed sample drawn from an unknown distribution.
In order to conduct a Hypothesis Test, we need a test statistic t and a null hypothesis distribution . Here, let us consider a Studentised statistic: where  X   X  is the standard deviation of z ,givenby Moreover, let w =( w 1 ,...,w n )where w i = z i  X   X  z , in order to create bootstrap samples of per-topic performance differences w  X  b that obey H 0 .Figure4shows the algorithm for obtaining B bootstrap samples of topics ( Q  X  b ) and the corre-sponding values of w  X  b .(Welet B = 1000 throughout this paper.) For example, let us assume that we only have five topics Q = (001, 002, 003, 004, 005) and that
For each b ,let  X  w  X  b and  X   X   X  b denote the mean and the standard deviation of w  X  b . Figure 5 shows how to compute the Achieved Significance Level (ASL) using w  X  b . In essence, we examine how rare the observed difference would be under H 0 .If ASL &lt;  X  , where typically  X  =0 . 01 (very strong evidence against H 0 )or  X  =0 . 05 (reasonably strong evidence against H 0 ), then we reject H 0 .Thatis, we have enough evidence to state that  X  X and  X  Y are probably different. We now describe Sakai X  X  method for assessing the sensitivity of IR metrics. Let C denote the set of all possible combinations of two systems. First, per-form a Bootstrap Hypothesis Test for every system pair in C and count how many of the pairs satisfy ASL &lt;  X  : The result represents the sensitivity of a given IR metric. We thereby obtain the values of  X  w  X  b and t ( w  X  b )foreach |
Q | = n topics, we can use the algorithm shown in Figure 6 to obtain a nat-ural estimate of the minimum performance difference required for guarantee-ing ASL &lt;  X  , given the topic set size n . For example, if  X  =0 . 05 is chosen, the algorithm looks for the B X  = 1000  X  0 . 05 = 50-th largest value among | the | C | values thus obtained, the algorithm takes the maximum value just to be conservative.

Note that the estimated differences themselves are not necessarily suitable for comparing metrics, since some metrics tend to take small values while others tend to take large values. The sensitivity of metrics should primarily be compared in terms of how many system pairs satisfy ASL &lt;  X  , that is, how many pairs show a statistically significant difference. 5.1 Data Table 2 shows some statistics of the NTCIR CLIR data (i.e., test collections and submitted runs) we used, which were kindly provided by National Institute of Informatics (NII), Japan. Currently, the NTCIR-3 and NTCIR-5 CLIR data are the only data publicly available from NII. For each data set, we selected the top 30 runs as measured by  X  X elaxed X  Mean AveP [8,14]. Thus we conducted four sets of experiments, each with | C | =30  X  29 / 2 = 435 system pairs. 5.2 Results and Discussions Table 3 shows the results of our Bootstrap-based experiments. It shows, for ex-ample, that if P-measure is used for assessing the  X  X op X  30 systems (as measured by AveP) that were submitted to the NTCIR-3 Chinese document retrieval sub-task, it can detect a statistically significant difference at  X  =0 . 05 for 39% of the system pairs; The estimated overall perfo rmance difference required for showing a statistical significance is 0.18.

The absolute sensitivity values depend heavily on the set of runs: It can be observed, for example, that P-measure can detect a significant difference for 62% of the system pairs for the NTCIR-3 Japanese data, but for only 17% for the NTCIR-5 Japanese data. That is, the NTCIR-5 Japanese runs are much harder to distinguish from each other because a larger number of teams performed well at NTCIR-5 than at NTCIR-3. (For both NTCIR-3 and NTCIR-5, the top 30 Japanese runs we used came from 10 different teams.)
On the other hand, the relative sensitivity, which is the focus of this study, is quite consistent across the four data sets: We can observe that  X  X ( + )-measure  X 
O-measure  X  NWRR  X  RR X  generally holds, where  X   X   X  means  X  X s at least as sensitive as X . (O-measure outperforms P-measure by two system pairs for the NTCIR-5 Chinese data, but this difference is probably not substantial.) The difference in sensitivity between P( + )-measure and O-measure arises from the fact that P( + )-measure consider all relevant documents ranked above r p ;That between O-measure and NWRR arises from the fact that the former considers an ideal ranked ouput (i.e., the number of relevant documents); That between NWRR and RR arises from the use of graded relevance. Our findings generalise and reinforce previously-reported results which suggested the superiority of P-measure and O-measure over RR [10,12]. However, these previous studies were based only on the ad hoc stability and swap methods, and examined neither P + -measure nor NWRR.

It is also clear that the five metrics for the task of finding one relevant doc-uments are not as sensitive as Q-measure and AveP. This is because, while Q-measure and AveP considers all relevant documents, the four metrics do not: They generally examine the very top of a ranked output only. That is, because they are based only on a small number of observations, they are inherently less stable than Q-measure and AveP. Therefore, one should prepare a larger set of topics if any of the four metrics is to be used instead of more stable metrics such as Q-measure and AveP.
 As for the estimated difference required for a given significance level, the NTCIR-5 results suggest that, with aro und 50 topics, we need an absolute dif-ference of around 0.14-0.16 in terms of P( + )-measure or O-measure in order to detect a significant difference at  X  =0 . 05. 5.3 Changing the Gain Values for P( + )-measure and O-measure We finally focus on P( + )-measure and O-measure which we have shown to be the three most sensitive metrics for the task of finding one relevant document, and study the effect of chaning gain values (See Section 3) using the NTCIR-5 data.

Table 4 summarises the results, where, for example,  X  X 10:5:1 X  represents P-measure with gain ( S )=10 ,gain ( A )=5 ,gain ( B ) = 1. (Hence the default gain value results, labelled with  X 3:2:1 X , have been copied from Table 3.) Recall that: (a) Using small gain values makes O-measure resemble RR (See Eq. (3)); (b) Given that the gain values are all one, O -measure = RR holds iff r 1  X  R ; (c) P ( + )-measure = O -measure holds if there is no relevant document above Rank r p . Thus, we can expect  X  X lat X  and small gain values to reduce the sen-sitivity of these metrics. Indeed, Tab le 4 shows that the gain value assign-ments gain ( S )=1 ,gain ( A )=1 ,gain ( B )=1and gain ( S )=0 . 3 ,gain ( A )= 0 . 2 ,gain ( B )=0 . 1 tend to hurt sensitivity, especially the former. On the other hand, using large gain values (which implies less penalty on late arrival of rel-evant documents) and using  X  X teeper X  gain values (which emphasises the rele-vance levels) generally do not seem to have a substantial impact on sensitivity. In summary, P( + )-measure and O-measure are fairly robust to the choice of gain values as long as graded relevance is properly utilised. This paper compared the sensitivity of five evaluation metrics designed for the task of finding one relevant document, using Sakai X  X  method based on Bootstrap Hypothesis Tests. Using four data sets from NTCIR, we showed that  X  X ( + )-measure  X  O-measure  X  NWRR  X  RR X  generally holds, where  X   X   X  X eans  X  X s at least as sensitive as X . These results generalise and reinforce previously reported ones [10,12] based on the ad hoc stability and swap methods, which suggested the superiority of P-measure and O-measure over RR. Moreover, in terms of sensitivity, P( + )-measure and O-measure are fairly robust to the choice of gain values. Thus we recommend the use of P( + )-measure and O-measure for practical tasks such as known-item search where recall is either unimportant or immeasurable. But it should be remembered that while O-measure represents a user who is satisfied with finding one partially relevant document, P( + )-measure approximate one who looks for one  X  X ood X  relevant document.

As for how the above metrics resemble each other in terms of system ranking, we already have results that generalise Sakai X  X  previous analysis [10] which did not include NWRR and P + -measure: Metrics for finding one relevant document (P( + )-measure, O-measure, NWRR and RR) produce system rankings that are substantially different from those produced by Q-measure and AveP; Rankings by P( + )-measure, O-measure and NWRR generally resemble each other. More-over, system rankings by P( + )-measure are fairly robust to the choice of gain values. But due to lack of space, we will discuss these additional results else-where.

