 Web content quality assessment is a typical static ranking problem. Heuristic content and TFIDF features based sta-tistical systems have proven effective for Web content quality assessment. But they are all language dependent features, which are not suitable for cross-language ranking. In this pa-per, we fuse a series of language-independent features includ-ing hostname features, domain registration features, two-layer hyperlink analysis features and third-party Web service features to assess the Web content quality. The experiments on ECML/PKDD 2010 Discovery Challenge cross-language datasets show that the assessment is effective.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Measurement, Experimentation, Algorithms Quality Assessment, Feature Extraction, Machine Learning
Web content quality assessment is a typical static ranking (query-independent ranking) problem [1], which is crucial to search engine, Web archiving and Internet directory, etc. Web quality assessment can also be applied to domain abuse detection to reduce false positive rate of the detection model.
In 2010, Andr  X  a s A. et al [2] released a dataset for Web quality assessment; based on the dataset, ECML/PKDD 2010 Discovery Challenge(DC2010) was held. As multilin-gualism is imperative on the Web, the DC2010 involves a multilingual task and expects language-independent features to be used to classify the French and German language test sets. How to extract effective features to train a model for multi-language Web content quality prediction is a challenge and interesting topic. In DC2010, NLP, hyperlink, content and TFIDF features were used, among them heuristical con-tent features and TFIDF features have proven the most ef-as a quality measure due to the growing prevalence of link spam [4] [6], it should still be a useful measure for the Web content quality.

For a host , the third-party features include: PageRank and Alexa ranking of the host  X  X  corresponded domain, num-ber of inlinks gathered by Yahoo and Alexa, and number of pages indexed by Google and Yahoo.
The page level hyperlink features used here are provided by the ECML/PKDD 2010 Discovery Challenge organiza-tion committee, i.e. link-based features [2]. Given that the transformed feature, such as TrustRank/PageRank, is meaningless for nonlinear classification algorithms, we do not use them in this paper.
Based on the common sense that benign sites tend to link to other high quality sites and malicious nodes are mainly linked by low quality nodes, we will extract a series of host level link analysis features to mine the quality relations from the Web topology dependency.
 Truncated PageRank ( t =1 , 2 , 3 , 4) [4] (spam detection), HostRank (static ranking) and Estimation of supporters ( d = 1 , 2 , 3 , 4) [4] (spam detection) are extracted. Considering the topological dependencies of low and high quality nodes, we also compute their neighbors X  weighted mean values just as our previous work [3]. The neighbor relation include inlinks and outlinks, etc. We treat the domainP R used in [3] as a third-party feature.
In DC2010, modeling via weighted probabilities of samples has proven one of the most effective assessment methods [3]. We adopt this strategy in the following experiments and use the same learning algorithm, i.e. bagging with C4.5. So we can focus on the effectiveness of the extracted features and compare with the DC2010 best results. The iterations of bagging is 90 in our experiments.

NDCG is used as our evaluation criterion, which is also used to evaluate the submissions of DC2010 [2].
DC2010 data set consists of 23,808,829 pages, 600 mil-lion hyperlinks and 191,388 different hosts [2]. We use the labeled English training data for multilingual Web content quality assessment task (German and French).

We extract 11 HostName features ( N ), 3 Registration features ( R ), 6 Third -Party features ( T ), 45 Host Level Link features ( H )and79 P age Level Link features ( P ) 1 .Con-ducting feature selection with information gain ( IG ) algo-rithm on the aforementioned features, we get the Fusion features ( F ). That is, F = IG ( N R T H P ). In our experiments, the dimension of the Fusion features is 35.
We queried the WHOIS and third-party services 2 months after DC2010 release, when some domains had expired. Given that domains with high reputation usually renew in advance, it should not quite affect the assessment results. The registration features include create date, update date and number of name servers.
