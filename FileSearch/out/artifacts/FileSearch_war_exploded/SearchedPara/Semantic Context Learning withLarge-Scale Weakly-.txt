 There are a large number of images available on the web; mean-while, only a subset of web images can be labeled by professionals because manual annotation is time-consuming and labor-intensive. Although we can now use the collaborative image tagging system, e.g., Flickr, to get a lot of tagged images provided by Internet users, these labels may be incorrect or incomplete. Furthermore, seman-tics richness requires more than one label to describe one image in real applications, and multiple labels usually interact with each other in semantic space. It is of significance to learn semantic con-text with large-scale weakly-labeled image set in the task of multi-label annotation. In this paper, we develop a novel method to learn semantic context and predict the labels of web images in a semi-supervised framework. To address the scalability issue, a small number of exemplar images are first obtained to cover the whole data cloud; then the label vector of each image is estimated as a local combination of the exemplar label vectors. Visual context, semantic context, and neighborhood consistency in both visual and semantic spaces are sufficiently leveraged in the proposed frame-work. Finally, the semantic context and the label confidence vec-tors for exemplar images are both learned in an iterative way. Ex-perimental results on the real-world image dataset demonstrate the effectiveness of our method.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Experimentation, Performance Image Annotation, Semantic Context, Large Scale, Weakly La-beled Corresponding Author: weizh@fudan.edu.cn
Automatic image annotation is an efficient way for retrieving and managing numerous images on the web. In the task of image an-notation, machine learning techniques are often used to learn clas-sifiers from the labeled training images. Since annotating training samples by professionals is time-consuming and labor-intensive, only a subset of large-scale image dataset can be labeled manually. Although we can now use the collaborative image tagging system, e.g., Flickr, to get a lot of tagged images provided by Internet users, the tags of collaboratively-tagged images may not have exact se-mantics because the Internet users may tag the images according to their personal perceptions or social backgrounds.

In recent years, many algorithms on image annotation have been proposed. [13] introduced a technique for image annotation which used low-level visual features and a combination of basic distances called JEC to find the nearest neighbors of any given image; [7] proposed a method to annotate and retrieve images by learning one relevance model from a set of labeled samples; [9] proposed a weighted nearest-neighbor model based on neighbor rank or dis-tance metric by maximizing the log-likelihood of the label predic-tions on training images. However, above methods did not allow for label-label correlation which is important to the performance in multi-label learning task. In real-world applications, seman-tics richness requires more than one label to sufficiently describe an image, and multiple labels usually interact with each other in semantic space. To capture the inter-label correlation for multi-label annotation application, [18] presented a measurement of the relationship between semantic concepts using the square root of Jensen-Shannon divergence between the corresponding visual lan-guage models; [4] proposed a hierarchical context model that cap-tured object co-occurrences and spatial relationships among more than a hundred categories by a tree structure; [6] constructed a topic network to precisely characterize inter-topic (inter-label) con-texts; [19] proposed a joint multi-label multi-instance learning model which captured the correlations between labels based on hid-den conditional random fields. However, it is still unclear how to efficiently leverage unlabeled or weakly-labeled samples for multi-label learning in above methods.

To exploit weakly-labeled images available on the web for multi-label learning, [11] proposed a formulation to implement various tag analysis tasks in a unified framework, but the inter-label cor-relation was omitted in [11]. [10] introduced an image retagging method to improve the quality of the tags associated with social im-ages in terms of content relevance; [15] proposed a bipartite graph reinforcement model for web image annotation, where a reinforce-ment algorithm was performed on the bipartite graph to re-rank the candidates; [17] defined the candidate annotations as the states of a Markov chain and formulated the annotation refinement process as a Markov process; [2] proposed a framework to improve the re-trieval performance by refining noisy tags of a group of Flickr pho-tos; [22] proposed a method to refine image labels by considering label correlation, content consistency, low-rank, and error sparsity. Above methods should be given a known semantic context as input, and how to learn semantic context from large-scale weakly-labeled image dataset is still not mentioned.

Semi-supervised learning is one way to learn from labeled and unlabeled samples [1]. In [23], a semi-supervised learning method was proposed to label data via a Gaussian random field model where the label of each datum was computed as the average of its neigh-bors; [8] introduced a semi-supervised learning scheme to propa-gate labels through images by constructing approximations to the eigenvectors of the graph Laplacian; [12] proposed a technique to make semi-supervised learning practical on large-scale dataset by seeking anchor points to construct a large adjacent graph. The frameworks in [1, 23, 8, 12] were not designed for multi-label learning, thus semantic context was not considered in above works. Recently, [16] proposed a sparse graph-based semi-supervised learn-ing method to boost the performance of each concept detector in semantic space; [20] also proposed a graph-based learning frame-work in the setting of semi-supervised learning with multiple la-bels. Although [16, 20] allowed for multi-label learning, the se-mantic context should be provided as an input, instead of being learned from the image set automatically.

In this paper a novel method is developed to predict the labels for images by learning semantic context in a semi-supervised frame-work based on a large-scale web image dataset. By investigating the label confidence matrix for image exemplars from different per-spectives, our method sufficiently leverages visual context, seman-tic context, and neighborhood consistency in both visual and se-mantic spaces. To address the scalability issue, a small number of exemplar images are first obtained to cover the whole data cloud, then the label vector of each image is estimated as a local combi-nation of the label vectors of these exemplars. The semantic con-text and the label confidence vectors for exemplar images are both learned in an iterative way.

The rest of this paper is organized as follows: Section 2 gives the overview of semantic context learning and image annotation frame-work with large-scale weakly-labeled dataset. In Section 3, we for-mulate the proposed model. Experimental results on the real-world web image dataset are shown in Section 4. Finally, we conclude this paper in Section 5.
Fig.1 gives the framework of our model. Firstly, a small number of exemplar images are selected to cover the whole data cloud, then the neighborhood contexts between samples and exemplars are pre-served when mapping images from visual feature space to semantic label space. We investigate the label confidence matrix for image exemplars from column and row views, which should be consis-tent with visual context and semantic context, respectively. Visual context can be derived from the set of image exemplars and seman-tic context can be learned by our algorithm on large-scale weakly-labeled image dataset. We predict the label vectors for images by leveraging semantic context, visual context, and neighborhood con-text simultaneously.
Let { ( x 1 , y 1 ) , ..., ( x l , y l ) } and { x l +1 , ..., x beled and u unlabeled images respectively, C = { c 1 , ..., c the semantic lexicon of m concepts, and y i = [ y i 1 , . . . , y { 0 , 1 } m . If the concept c s is associated with x i , then y 1 , . . . , m ) ; otherwise, y i s = 0 . Furthermore, let h note the label confidence vector for the image x i , and the s  X  th element of h i measures the probability that the image x i concept c s . Our goal is to predict the label vectors for images by learning semantic context with the large-scale web image dataset.
We employ K-means clustering algorithm to seek a small num-ber of exemplars covering the total data cloud. Suppose that n clusters are obtained, and the sample closest to the center for each cluster is regarded as the corresponding exemplar, then we get n exemplars {  X  x 1 , ...,  X  x n } . Like [14, 12, 21, 3], each image is ap-proximately reconstructed as a local combination of the exemplars: where  X  x i  X  is the index set of k nearest exemplars for x can be estimated as follows: If we preserve the neighborhood contexts when images are mapped from visual space to semantic space, then the label confidence vec-tor for each image x i can be approximated as a local combination of the labels of exemplars as well: where  X  H = [  X  h 1 , ...,  X  h n ]  X  [0 , 1] m  X  n , and label confidence vector of the exemplar  X  x q , ( q = 1 , ..., n ) ; ~ X  [  X  i 1 , ...,  X  in ] &gt; , and  X  iq = 0 if q /  X   X  x i  X  .
Thus the label confidence vector of all samples can be obtained as H = [ h 1 , ..., h l + u ] =  X  HA , where A = [ ~ X  1 , ..., ~ X  R n  X  ( l + u ) . For those labeled samples, the corresponding label con-fidence vector should be consistent with the given labels to some extent. Thus, we minimize the following loss function: where Y = [ y 1 , ..., y l ]  X  { 0 , 1 } m  X  l ,  X  H = [ and A l  X  R n  X  l is the sub-matrix according to the labeled subset. To capture the visual context, we define a similarity matrix S  X  R ( l + u )  X  ( l + u ) measuring the visual similarities between image pairs. The matrix S is large-scale. Inspired by [12], we can approximate the weight matrix S as follows: where the diagonal matrix  X   X  R n  X  n is defined as  X  ii = P The visual and semantic consistency can be achieved by: where col ( H, i ) = P n q =1 A qi col (  X  H, q ) denotes the i  X  th column of H , i.e., the label confidence vector for the i  X  th sample, which is just the linear combination of all columns of  X  H . Note that L normalized graph Laplacian of the visual context L s = I n  X  n we can compute AL s A &gt;  X  R n  X  n efficiently in Eq. (6), and thus avoid computing the large matrix L s directly.

At the same time, to capture the semantic context, we also define the weight matrix W to measure the inter-label correlations. W is an m  X  m symmetric matrix and its entry can be defined as the harmonic mean of the empirical conditional probabilities: where the empirical conditional probabilities p ( t | s ) = semantic relation is; so, the weight W st measures the correlations between concepts c s and c t . Let f W = D  X  1 2 w W D  X  1 an m  X  m diagonal matrix with D w ( t, t ) = P m s =1 W st row of  X  H can be viewed as the feature vector of a certain concept, we can achieve the goal that strongly correlated concepts should have similar feature vector by: where row (  X  H, s ) denotes the s  X  th row of  X  H , L w graph Laplacian of the semantic context L w = I m  X  m  X  f
It should be pointed out that the correlations between concepts derived by the empirical conditional probabilities in Eq. (7) is sim-ple and effective if the labeled samples are sufficient. If the avail-able training images are weakly labeled, there is lack of sufficient training samples and the empirical conditional probabilities might not be estimated correctly, thus the semantic graph with the edge weights Eq. (7) is not expected to capture the semantic context re-lationship well. To address this problem, we should learn the se-mantic context and the label confidence vectors, simultaneously. By incorporating various information ( including visual context and semantic context ) in a single framework, the proposed model takes the formulation as follows: where  X  1 and  X  2 are the trade-off parameters.

The cost function (9) can be minimized by updating  X  H and L alternatively. We derive the gradients of the above cost function with respect to  X  H and L w , respectively: Now the optimal  X  H and L w can be obtained via the iterative alter-nating updating procedure as follows: where  X  and  X  ( 0 &lt;  X ,  X  &lt; 1 ) are both the step sizes for gradient search.
 To get the initial label confidence matrix  X  H 0 , we can employ SVM or other existing image annotation algorithms like TagProp [9] by using the labeled images as training samples. Based on Eq. (7), we also initialize the normalized graph Laplacian of the semantic context L 0 w .
Once the optimal graph Laplacian L opt w is learned, the optimal semantic context f W opt is computed straightforward: f W L w . Based on the learned optimal  X  H , the label confidence vector for each image can be obtained according to Eq. (3). By choosing a threshold for each component of the label confidence vector, we can predict the label vectors for each image easily. We evaluate our method on the real-world image dataset NUS-WIDE [5] which is a challenging collection of web images from Flickr comprising 269,648 images with over 5,000 user-provided tags. Since the ground-truth of 81 concepts for the entire dataset can be used for evaluation, we focus on the 81 concepts in experi-ments. As in [3], two image pools are constructed from the entire dataset: the pool of labeled images is comprised of 161,789 im-ages while the rest are used for the pool of unlabeled ones. For each image, we first extract two types of visual features: 512-Dim GIST and 1024-Dim SIFT. 3000 exemplar images are selected in experiments.
 Figure 2: Semantic context learned from NUS-WIDE dataset. Each concept is linked with relevant concepts with larger weights.

Fig.2 shows the semantic context learned from NUS-WIDE im-age dataset, where each concept is linked with its relevant concepts with larger weights. For some concepts, their inter-concept con-texts could be very weak (i.e., having smaller weights), thus it is not necessary for each concept to be linked with all the other con-cepts. As an example in Fig. 2, the concept mountain is more relevant to those concepts water , clouds , tree , lake , and valley in the NUS-WIDE dataset.

Fig.3 shows the results of our method (Ours) in comparison with the baselines SVM and TagProp [9] in terms of F score for individ-ual concepts on the unlabeled pool of NUS-WIDE images. F score is defined as the harmonic mean of precision and recall: As to the baseline SVM, we train one binary SVM for each concept; then 81 SVMs are learned. These SVMs for different concepts are independent because there are no correlations between concepts are leveraged. As observed from the results, our method outperforms the others for most concepts, which demonstrates that our method can effectively learn semantic context from image dataset, and suf-ficiently leverage neighborhood context, visual context, and seman-tic context to improve the annotation performance.
In this paper a novel method is developed to predict label vec-tors for web images by learning semantic context with large-scale weakly-labeled image dataset in a semi-supervised framework. To address the scalability issue, clustering technique is employed to obtain a small number of exemplar images which cover the whole data cloud. The neighborhood contexts are preserved when map-ping images from the visual space to the semantic space. The label vector for each image is estimated as a local combination of the exemplar label vectors. By investigating the label confidence ma-trix for image exemplars from column and row views, our method sufficiently leverages visual context, semantic context, and neigh-borhood consistency in both visual and semantic spaces, which is important to multi-label image annotation task.
We would like to thank the anonymous reviewers for their help-ful comments. This work was supported in part by the STCSM X  X  Programs (No. 10511500703 and No. 12XD1400900), the NSF of China (No.60903077), and the 973 Program (No.2010CB327906). [1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [2] L. Chen, D. Xu, I. W. Tsang, and J. Luo. Tag-based web [3] X. Chen, Y. Mu, S. Yan, and T.-S. Chua. Efficient large-scale [4] M. J. Choi, J. J. Lim, A. Torralba, and A. S. Willsky. [5] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y.-T. Zheng. [6] J. Fan, Y. Shen, N. Zhou, and Y. Gao. Harvesting large-scale [7] S. L. Feng, R. Manmatha, and V. Lavrenko. Multiple [8] R. Fergus, Y. Weiss, and A. Torralba. Semi-supervised [9] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. on NUS-WIDE. [10] D. Liu, X.-S. Hua, M. Wang, and H.-J. Zhang. Image [11] D. Liu, S. Yan, Y. Rui, and H.-J. Zhang. Unified tag analysis [12] W. Liu, J. He, and S.-F. Chang. Large graph construction for [13] A. Makadia, V. Pavlovic, and S. Kumar. A new baseline for [14] S. T. Roweis and L. K. Saul. Nonlinear dimensionality [15] X. Rui, M. Li, Z. Li, W.-Y. Ma, and N. Yu. Bipartite graph [16] J. Tang, S. Yan, R. Hong, G.-J. Qi, and T.-S. Chua. Inferring [17] C. Wang, F. Jing, L. Zhang, and H.-J. Zhang. Content-based [18] L. Wu, X.-S. Hua, N. Yu, W.-Y. Ma, and S. Li. Flickr [19] Z.-J. Zha, X.-S. Hua, T. Mei, J. Wang, G.-J. Qi, and Z. Wang. [20] Z.-J. Zha, T. Mei, J. Wang, Z. Wang, and X.-S. Hua. [21] W. Zhang, Y. Lu, X. Xue, and J. Fan. Automatic image [22] G. Zhu, S. Yan, and Y. Ma. Image tag refinement towards [23] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised
