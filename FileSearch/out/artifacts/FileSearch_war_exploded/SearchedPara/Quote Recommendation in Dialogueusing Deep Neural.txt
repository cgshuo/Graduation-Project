 Quotes, or quotations, are well known phrases or sentences that we use for various purposes such as emphasis, elabo-ration, and humor. In this paper, we introduce a task of recommending quotes which are suitable for given dialogue context and we present a deep learning recommender system which combines recurrent neural network and convolutional neural network in order to learn semantic representation of each utterance and construct a sequence model for the di-alog thread. We collected a large set of twitter dialogues with quote occurrences in order to evaluate proposed rec-ommender system. Experimental results show that our ap-proach outperforms not only the other state-of-the-art algo-rithms in quote recommendation task, but also other neural network based methods built for similar tasks.
 I.2.7.7 [ Artificial Intelligence ]: Natural Language Pro-cessing X  Text Analysis Quote recommendation, Dialogue model, Deep neural net-work
Quotes, or quotations, are well known phrases or sentences that we use in our writings and conversations for various pur-poses such as emphasis, elaboration, and humor. However, it is not always easy to promptly come up with an adequate quote that fits the situation at hand. We can try to find one on search engines, but it is also hard to figure out the right search keywords because in most cases the quotes are metaphorical so the words in those quotes do not always match the words expressing our intentions. A quote recom-mender can be quite useful in these situations.

Meanwhile, as we rely more and more on mobile devices in written communications including text messages, tweets, not only the other state-of-the-art algorithms for quote rec-ommendation task, but also other neural network based methods built for similar tasks.
Quote recommendation can be viewed as a task of search-ing quote that fits in the given query texts. Tan et al., 2015 [10] apply learning-to-rank framework using cosine similarity-based features between query and quote context to recom-mend ranked list of quotes relevant to query. He et al., 2010 [2] also exploit similarity features between candidate paper and current writing context. Huang et al., 2012 [4] proposes to use translation model to connect the context of writing and the cited paper. These approaches use the traditional bag-of-words model which have a clear drawback: the loss of semantics. The bag-of-words model performs especially worse when it comes to dialogues because utterances in di-alogues are in short and irregular colloquial forms.
With recent success of distributional word representation which overcomes such drawback, there are several approaches trying to learn distributional representation of sentences or paragraphs. One of the most prominent and recently emerg-ing method is convolutional neural network. [5] Convolu-tional network based approaches are resulting in state-of-the-art performance for the tasks like sentence classification [6] and relevant short text matching [9]. We also take ad-vantages of convolutional network to learn intermediate rep-resentationofeachtweet.

Our work is related to dialogue models which have been studied for the purpose of building automatic dialogue sys-tems. [13, 1] Recently with the significant advances in train-ing of neural network, deep neural networks based dialogue models are actively being proposed. Most of them take re-current neural network based approaches to model the utter-ance sequence. [12, 8] Their purpose is to generate natural language sentences, but our task focuses on recommending a ranked list of quotes among candidate quotes.
In this section we describe our deep neural network model for quote recommendation. Our architecture is a combina-tion of the convolutional neural network ( ConvNetwork )and the recurrent neural network ( RNN ) as shown in Figure 2. ConvNetwork maps tweets in the thread to their distribu-tional vectors. And then the sequence of tweet distributional vectors are fed to RNN so as to compute the relevance of target quotes to the given tweet dialogue. In the following, we briefly explain main components of our network.
Since use of a quote in a dialogue highly depends on tem-poral history of previous utterances, it is important to model a dialogue as a sequence of utterances. Recurrent neural net-work is a neural network which is widely used in modeling sequence, because it effectively learns significant patterns of sequential data. So we exploit recurrent neural network to recommend quotes suitable for given context of dialogue.
We use long short-term memory unit (LSTM) [3] which is a recurrent neural network with several gates which al-low networks to learn long-term dependencies without loss map c  X  R m  X  h +1 is produced by convolution where each component is computed as follows: where  X  is element-wise multiplication, t [: ,i : i + h  X  1] is matrix slice of size h , b i is the bias value, and f is non-linear acti-vation function. Generally, more than one filter is applied to convolution layer to yield richer information, so we use multiple filters w (1) , w (2) ,..., w ( p ) where p is another hyper parameter which indicates the number of filters. With mul-
After obtaining feature maps, we apply max-pooling op-eration which take the maximum value  X  c ( i ) from each of feature map c ( i ) and form a p-length feature vector: Eventually m tweet matrices in tweet thread are mapped to sequence of m tweet representation vectors.
We initialize the word vectors in our network with those trained from unsupervised neural language model and keep them static. It is a popular method to improve perfor-mance in case supervised training data is not sufficient. We use publicly available pre-trained word vectors, which were trained on 27 billion words of 2 billion tweets from Twitter. [7]
The entire networks are trained to minimize the cross-entropy of the predicted and true distributions. The ob-jective function includes an L 2 regularization term over the parameters to prevent overfitting. RMSProp [11] is used to optimize stochastic gradient decent with a learning rate of 10  X  3 and a decay rate of 0.9. Dropout is adapted on max-pool layer in ConvNetwork and fully-connected layer in RNN . We set other hyper-parameters as follows: 3 for fil-ter width (h), 500 for the number of feature maps (p), and dropout rate as 0.5. And we use ReLU activation function.
We collected the quotes to be recommended from two sources of quotes: one is the Wikiquote 1 website, and the other is Oxford Concise Dictionary of Proverbs 2 .Foreach quote, we collected tweet dialogues that contain at least one occurrence of the quote.

From each tweet dialogue thread, we only use the tweets up to the quote occurrence as context . For the tweet that contains the quote, only the part from the first word to the last word before the quote is used as context .

Westartedoutwithtop100quotesinorderofquantity of relevant tweet threads and then expanded to top 200, 300, and 400. Table 1 shows the statistics of datasets. q is total number of quotes, N is total number of tweet threads, t is average number of collected tweet threads per quote, http://en.wikiquote.org Oxford University Press, 1998 Table 2: Recommendation performance of different methods for top100 dataset Figure 3: Results of recall@5 of different methods when increasing # of target quotes which search for quotes based on similarity features between query and quote contexts, do not result in successful per-formance. That is because they cannot find out contexts which are semantically similar to the query but expressed in different words. CTM which use translation model per-forms slightly better. Deep neural network based approaches achieve higher scores than the non-neural methods, which shows that the distributed word representation helps seman-tic word matching and the deep neural network automati-cally extract meaningful representation of tweet context in respect to the quote recommendation. CNN captures signif-icant local semantic features, i.e., n-grams, while RNN cap-tures the overall ordering of words in context. Our approach outperforms both separate CNN and RNN models showing that per-tweet feature extraction captures structural con-text of dialogue while simple concatenation of tweets can-not. Figure 3 shows the results of recall@5 when increas-ing the number of target quotes. Although overall accuracy decreases as the number of target quotes increases, our ap-proach consistently outperforms other methods.

Hit@5 evaluation of our model reaches almost 0.7 for top100 dataset which means that our recommendation system rec-ommends at least one relevant quote in top 5 list in seven out of ten times. This proves that our recommendation model is practically useful. Figure 4 shows the example of recom-mendation results for a query context in the test set. As we can see, our system recommends quotes comforting the user named  X  X onvolution X  and it is quite appropriate to the situation.
In this work, we present a task of quote recommendation in a dialogue. We propose a deep neural network model which efficiently extracts meaningful local semantic features from each tweet using convolutional network and learns each tweet features in sequence. We evaluate the proposed model
