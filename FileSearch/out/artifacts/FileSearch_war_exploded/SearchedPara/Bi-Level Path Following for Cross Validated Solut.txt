 Saharon Rosset saharon@post.tau.ac.il Sciences, Tel Aviv University, Israel In the standard predictive modeling setting, we are given a training sample of n examples { x 1 , y 1 } , ..., { x n , y n } drawn i.i.d from a joint distribu-tion P ( X, Y ), with x i  X  R p and y i  X  R for regression, y i  X  { 0 , 1 } for two-class classification. We aim to utilize these data to build a model  X  Y =  X  f ( X ) to describe the relationship between X and Y , and later use it to predict the value of Y given new X values. This is often done by defining a family of models F and finding (exactly or approximately) the model f  X  F which minimizes an empirical loss function : P i =1 L ( y i , f ( x i )). Examples of such algorithms include linear and logistic regression, empirical risk minimization in classification and others.
 If F is complex, it is often desirable to add regular-ization to control model complexity and overfitting. The generic regularized optimization problem can be written as: where J ( f ) is an appropriate model complexity penalty and  X  is the regularization parameter. Given a loss and a penalty, selection of a good value of  X  is a model selection problem. Popular approaches that can be formulated as regularized optimization prob-lems include all versions of support vector machines, ridge regression, the Lasso and many others. For an overview of predictive modeling, regularized optimiza-tion and the algorithms mentioned above, see for ex-ample Hastie et al. (2001).
 In this paper we are interested in a specific setup where we have a family of regularized optimization prob-lems, defined by a parameterized loss function and a regularization term. A major motivating example for this setting is regularized quantile regression (Koenker, 2005): where L  X  , the parameterized quantile loss function, has the form: and is termed  X  -quantile loss because its population optimizer is the appropriate quantile (Koenker, 2005): arg min Because quantile loss has this optimizer, the solution of the quantile regression problems for the whole range 0 &lt;  X  &lt; 1 has often been advocated as an approach to estimating the full conditional probability of P ( Y | X ) (Koenker, 2005; Perlich et al., 2007). Much of the in-teresting information about the behavior of Y | X may lie in the details of this conditional distribution, and if it is not nicely behaved (i.i.d Gaussian noise being the most commonly used concept of nice behavior), just estimating a conditional mean or median is often not sufficient to properly understand and model the mechanisms generating Y . The importance of estimat-ing a complete conditional distribution, and not just a central quantity like the conditional mean, has long been noted and addressed in various communities, like Econometrics, Education and Finance (Koenker, 2005; Buchinsky, 1994; Eide &amp; Showalter, 1998). There has also been a surge of interest in the Machine Learning community in conditional quantile estimation in re-cent years (Meinshausen, 2006; Takeuchi et al., 2006). Figure 1 shows a graphical representation of L  X  for several values of  X  , and a demonstration of the con-ditional quantile curves in a univariate regression set-ting, where the linear model is correct for the median, but the noise has a non-homoscedastic distribution. On the penalty side, we typically use the ` q norm of the parameters with q  X  { 1 , 2 } . Adding a penalty can be thought of as shrinkage, complexity control or putting a prior to express our expectation that the  X   X  X  should be small.
 As has been noted in the literature (Rosset &amp; Zhu, 2007; Hastie et al., 2004; Li et al., 2007) if q  X  { 1 , 2 } and if we fix  X  =  X  0 , we can devise path following (AKA parametric programming) algorithms to effi-ciently generate the 1-dimensional curve of solutions {  X   X  (  X  0 ,  X  ) : 0  X   X  &lt;  X  X  . Although it has not been explicitly noted by most of these authors, it naturally follows that similar algorithms exist for the case that we fix  X  =  X  0 and are interested in generating the curve {  X   X  (  X ,  X  0 ) : 0 &lt;  X  &lt; 1 } .
 In addition to parameterized quantile regression, there are other modeling problems in the literature which combine a parameterized loss function problem with the existence of efficient path following algorithms. These include Support vector regression (SVR, Smola and Sch  X olkopf (2004), see Gunther and Zhu (2005) for path following algorithm) with ` 1 or ` 2 regularization, where the parameter  X  determines the width of the don X  X  care region around 0.
 An important extension of the ` 2 -regularized optimiza-tion problem is to non-linear fitting through kernel embedding (Sch  X olkopf &amp; Smola, 2002). The kernel-ized version of problem (1) is:  X  f (  X ,  X  ) = arg min where k  X  k H definite kernel K in the Reproducing Kernel Hilbert Space (RKHS) it generates. The well known repre-senter theorem (Kimeldorf &amp; Wahba, 1971) implies that the solution of problem (3) lies in a low dimen-sional subspace spanned by the representer functions et al. (2004) for the support vector machine, Li et al. (2007) have shown that  X  -path of solutions to problem (3) when  X  is fixed can also be efficiently generated. It is important to note the difference in the roles of the two parameters  X ,  X  . The former defines a family of loss functions, in our case leading to estimation of different quantiles. Thus we would typically want to build and use a model for every value of  X  . The latter is a regularization parameter, controlling model com-plexity with the aim of generating a better model and avoiding overfitting, and is not part of the prediction objective (at least as long as we avoid the Bayesian view). We would therefore typically want to generate a set of models  X   X  (  X  ) (or f  X  (  X  ) in the kernel case), by selecting a good regularization parameter  X   X  (  X  ) for ev-ery value of  X  , thus obtaining a family of good models for estimating the range of conditional quantiles, and consequently the whole conditional distribution. This problem, of model selection to find a good regularization parameter, is often handled through cross-validation . In its simplest form, cross-validation entails having a second, independent set of data {  X  x is used to evaluate the performance of the models and select a good regularization parameter. For a fixed  X  , we can write our model selection problem as a Bi-level programming extension of problems (1, 3), where f  X  (  X  ) =  X  f (  X ,  X   X  ) and  X   X  solves: The objective of this minimization problem is not con-vex as a function of  X  . A similar non-convex opti-mization problem has been tackled by Kunapuli et al. (2007). The fundamental difference between their set-ting and ours is that they had a single bi-level op-timization problem, while we have a family of such problems, parameterized by  X  . This allows us to take advantage of internal structure to solve the bi-level problem for all values of  X  simultaneously (or more accurately, in one run of our algorithm).
 The concept of a parameterized family of bi-level reg-ularized quantile regression problems is demonstrated in Figure 2, where we see the cross-validation curves of the objective of (4) as a function of  X  for several values of  X  on the same dataset. As we can see, the optimal level of regularization varies with the quantile, and correct choice of the regularization parameter can have a significant effect on the success of our quantile prediction model.
 The main goal of this paper is to devise algorithms for following the bi-level optimal solution path f  X  (  X  ) as a function of  X  , and demonstrate their practicality. We show that this non-convex family of bi-level programs can be solved exactly, as the optimum among the so-lutions of O ( n + N ) standard (convex) path-following problems, with some additional twists. This result is based on a characterization of the evolution of the so-lution path  X  f (  X ,  X  ) as  X  varies, and on an understand-ing of the properties of optimal solutions of the bi-level problem, which can only occur at a limited set of points. We combine these insights to formulate an actual algorithm for solving this family of bi-level pro-grams via path-following.
 The rest of this paper is organized as follows. In Sec-tion 2 we discuss the properties of the quantile regres-sion solution paths  X  f (  X ,  X  ) and their evolution as  X  changes. We then discuss in Section 3 the properties of the bi-level optimization problem (4) and demonstrate that the solutions change predictably with  X  . Bring-ing together all our insights leads us to formulate an algorithm in Section 4, which allows us to follow the following a large but manageable number of solution paths of problem (3) simultaneously. We demonstrate our methods with a simulated data study in Section 5, where we demonstrate the computational efficiency of our approach and the ability of KQR to capture non-standard conditional distributions P ( Y | X ). This paper is a short version of Rosset (2008), and we defer the proofs, some of the technical details and much of the discussion to that version. We concentrate our discussion on the kernel quantile regression (KQR) formulation in (3), with the un-derstanding that it subsumes the linear formulation (1) with ` 2 regularization by using the linear kernel K ( x ,  X  x ) = x T  X  x .
 We briefly survey the results of Li et al. (2007) regard-ing the properties of  X  f (  X ,  X  ), the optimal solution path of (3), with  X  fixed. The representer theorem (Kimel-dorf &amp; Wahba, 1971) implies that the solution can be written as: For a proposed solution f ( x ) define:  X  E = { i : y i  X  f ( x i ) = 0 } (points on elbow of L  X   X  L = { i : y i  X  f ( x i ) &lt; 0 } (left of elbow)  X  R = { i : y i  X  f ( x i ) &gt; 0 } (right of elbow) Then Li et al. (2007) show that the Karush-Kuhn-Tucker (KKT) conditions for optimality of a solution  X  f (  X ,  X  ) of problem (3) can be phrased as: with some additional algebra they show that for a fixed  X  , there is a series of knots , 0 =  X  0 &lt;  X  1 &lt; ... &lt;  X   X  such that for  X   X   X  m we have  X  f (  X ,  X  ) = constant and for  X  k  X  1 &lt;  X   X   X  k we have:  X  f (  X ,  X  )( x ) = where h k ( x ) = b k 0 + of as the direction in which the solution is moving for the region  X  k  X  1 &lt;  X   X   X  k . The knots  X  k are points on the path where an observation passes between E and either L or R , that is  X  i  X  X  such that exactly  X  i =  X  or  X  i =  X  (1  X   X  ). These insights lead Li et al. (2007) to an algorithm for incrementally generating  X  f (  X ,  X  ) as a function of  X  for fixed  X  , starting from  X  =  X  (where the solution only contains the intercept  X  0 ).
Although Li et al. (2007) suggest it is a topic for fur-ther study, it is in fact a reasonably straight forward extension of their results to show that a similar sce-nario holds when we fix  X  and allow  X  only to change, and also when both  X ,  X  are changing together along a straight line , i.e., a 1-dimensional subspace of the (  X ,  X  ) space (this has been observed by Wang et al. (2006) for SVR, which is very similar from an opti-mization perspective). The explicit result is given in (Rosset, 2008), but we omit it here for brevity, given its marginal novelty.

Armed with this result, we next show the main re-sult of this section: that the knots themselves move in a (piecewise) straight line as  X  changes, and can therefore be tracked as  X  and the regularization path change. Fix a quantile  X  0 and assume that  X  k is a knot in the  X  -solution path for quantile  X  0 . Fur-ther, let i k be the observation that is passing in or out of the elbow at knot  X  k . Assume WLOG that  X   X  (  X  0 ,  X  k ) =  X  0 , i.e. it is on the boundary between R k and E k . Let column removed, and  X  b k = b k with index i k removed. Let s i = be the vector of all these values.
 Theorem 1 Any knot  X  k moves linearly as  X  changes. That is, there exists a constant c k such that for quan-tile  X  0 +  X  there is a knot in the  X  -solution path at  X  + c k  X  , for  X   X  [  X   X  k ,  X  k ] , a non-empty neighborhood of 0. c k is determined through the solution of another set of |E k | +1 linear equations with |E k | +1 unknowns: with And the fit at this knot progresses as: This theorem tells us that we can in fact track the knots in the solution efficiently as  X  changes. We still have to account for various types of events that can change the direction the knot is moving in. The value  X  for a point in E k  X  X  i k } can reach  X  or  X  (1  X   X  ), or a point in L X  X  may reach the elbow E . These events correspond to knots crossings , i.e., the knot  X  k is en-countering another knot (which is tracking the other event). There are also knot birth events, and knots merge events, which are possible but rare, and some-what counter-intuitive. The details of how these are identified and handled can be found in the detailed algorithm description (Rosset, 2008). When any of these events occurs, the set of knots has to be up-dated and their directions have to be re-calculated us-ing Theorem 1 and the new identity of the sets E , L , R and the observation i k . This in essence allows us to map the whole 2-dimensional solution surface  X  f (  X ,  X  ). Our next task is to show how our ability to track the knots as  X  changes allows us to track the solution of the bi-level optimization problem (4), as  X  changes. The key to this step is the following result. Theorem 2 Any minimizer 1 of (4) is always either at a knot in the  X  -path for this  X  or a point where a validation observation crosses the elbow. In other words, one of the two following statements must hold:  X   X   X  is a knot:  X  i  X  { 1 ...n } s.t.  X  f (  X ,  X   X  (  X  ))( x  X   X   X  is a validation crossing : Corollary 1 Given the complete solution path for  X  =  X  , the solutions of the bi-level problem (4) for a range of quantiles around  X  0 can be obtained by following the paths of the knots and the validation crossings only, as  X  changes.
 To implement this corollary in practice, we have two main issues to resolve: 1. How do we follow the paths of the validation crossings? 2. How do we determine which one of the knots and validation crossings is going to be optimal for every value of  X  ? The first question is easy to answer when we consider the similarity between the knot following problem we solve in Theorem 1 and the validation crossing follow-ing problem. In each case we have a set of elbow obser-vations whose fit must remain fixed as  X  changes, but whose  X   X  values may vary; sets L , R whose  X   X  are chang-ing in a pre-determined manner with  X  , but whose fit may vary freely; and one special observation which characterizes the knot or validation crossing. The only difference is that in a knot this is a border observation from the training set, so both its fit and its  X   X  are pre-determined, while in the case of validation crossing it is a validation observation, whose fit must remain fixed (at the elbow ), but which does not even have a  X   X  value. Taking all of this into account, it is easy to show a re-sult similar to Theorem 1 for the validation crossings. We refer the reader to Rosset (2008) for the details. The second question we have posed requires us to ex-plicitly express the validation loss (i.e., L  X  on the val-idation set) at every knot and validation crossing in terms of  X  , so we can compare them and identify the optimum at every value of  X  . Using the representation in (7) we can write the validation loss for a knot k : see Rosset (2008) for details, and note that similar ex-pressions can naturally be derived for validation cross-ings. These are rational functions of  X  with quadratic expressions in the numerator and linear expressions in the denominator. Our cross-validation task can be re-formulated as the identification of the minimum of these rational functions among all knots and validation crossings, for every value of  X  in the current segment , where the directions h k , h v of all knots and validation crossings are fixed (and therefore so are the coefficients in the rational functions). This is a lower-envelope tracking problem, which has been extensively studied in the literature (Sharir and Agarwal (1995) and ref-erences therein).
 To calculate the meeting point of two elements with neighboring scores we find the zeros of the cubic equa-tion obtained by requiring equality for the two rational functions of the form (9) corresponding to the two ele-ments. The smallest non-negative solution for  X  is the one we are interested in. Bringing together all the elements from the previ-ous sections, we now give (Algorithm 1) a succinct overview of the resulting algorithm. Since there is a multitude of details, we defer a detailed pseudo-code description of our algorithm to Rosset (2008). The algorithm follows the knots of the  X  -solution path as  X  changes using the results of Section 2, and keeps track of the cross-validated solution using the results of Section 3. Every time an event happens (like a knot crossing), the direction in which two of the knots are moving has to be changed, or knots have to be added or deleted. Between these events, the evolution of the cross-validation objective at all knots and validation crossings has to be sorted and followed. Their order is maintained, and updated whenever crossings occur between them. 4.1. Approximate Computational Complexity Looking at Algorithm 1, we should consider the num-ber of steps of the two loops and the complexity of the operations inside the loops. Even for a  X  X tandard X   X  -path following problem for fixed  X  , it is in fact im-possible to rigorously bound the number of steps in the general case, but it has been argued and empiri-cally demonstrated by several authors that the num-ber of knots in the path behaves as O ( n ), the number of samples (Rosset &amp; Zhu, 2007; Hastie et al., 2004; Li et al., 2007). In our case the outer loop of Al-gorithm 1 implements a 2-dimensional path following problem, that can be thought of as following O ( n ) 1-dimensional paths traversed by the knots of the path. It therefore stands to reason (and we confirm it empir-ically below) that the outer loop typically has O ( n 2 ) steps where events happen. The events in the inner loop, in turn, have to do with N validation observa-tions meeting the O ( n ) knots. So a similar logic would lead us to assume that the number of meeting events (counted by the inner loop) should be at most O ( nN ) total for the whole running of the algorithm (i.e., many iterations of the outer loop may have no events hap-pening in the inner loop). Each iteration of either loop requires a re-calculation of up to three directions (of knots or validation crossings), using Theorem 1. These calculations involve updating and inversion of matrices that are roughly |E| X |E| in size (where |E| is the number of observations in the elbow). However note that only one row and column are involved in the updating, leading to a complexity of O ( n + |E| 2 ) for the whole direction calculation operation, using the Sherman-Morrison formula for updating the inverse. In principle, |E| can be equal to n , although it is typ-ically much smaller for most of the steps of the algo-rithm, on the order of that the loop cost is between O ( n ) and O ( n 2 ). Putting all of these facts and assumptions together, we can estimate the algorithm X  X  complexity X  X  typ-ical dependence on the number of observations in the training and validation set as ranging between O ( n 2  X  max( n, N )) and O ( n 3  X  max( n, N )). Clearly, this estimation procedure falls well short of a formal  X  X orst case X  complexity calculation, but we offer it as an in-tuitive guide to support our experiments below and get an idea of the dependence of running time on the amount of data used.
 We have not considered the complexity of the lower envelope tracking problem in our analysis, because it is expected to have a much lower complexity (number of order changes O (max( n, N ) log(max( n, N ))) and each order change involves O (1) work). We demonstrate two main aspects of our methodol-ogy: The computational behavior as a function of the amount of training data used; and the ability of KQR to capture the form of the conditional distribu-tion P ( Y | X ), both in standard (i.i.d error) situations and when the error is not homoscedastic and asym-metric. We limit the experiments to simple simulated data, where we know the truth and can understand the behavior of KQR. This is due to shortage of space, and since our main contribution is not a new method that should be compared to competitors on real data, but rather a new algorithmic approach for an existing method.
 Our simulation setup starts from univariate data x  X  [0 , 1] and a  X  X enerating X  function f ( x ) = 2  X   X  exp(  X  30  X  ( x  X  0 . 25) 2 ) + sin(  X   X  x 2 ) We then let Y = f ( x ) +  X  , where the errors  X  are inde-pendent, with a distribution that can be either: 1.  X   X  N (0 , 1), i.e., i.i.d standard normal errors 2.  X  +( x +1) 2  X  exp(1 / ( x +1) 2 ), which gives us errors Figure 3 demonstrates the results of the algorithm with i.i.d normal errors, 200 training samples and 200 validation samples and Gaussian kernel with param-eter  X  = 0 . 2. We see that the quantile estimates all capture the general shape of the true curve, with some  X  X moothing X  due to regularization.
 Next we consider the computational complexity of the algorithm, and its dependence on the number of train-ing samples (with 200 validation samples). We com-pare it to the 1-dimensional KQR algorithm of Li et al. (2007), who have already demonstrated that their algorithm is significantly more efficient than grid-based approaches for generating 1-dimensional paths for fixed  X  . Table 1 shows the number of steps of the main (outer) loop of Algorithm 1 and the total run time of our algorithm for generating the complete set of cross-validated solutions for  X   X  [0 . 1 , 0 . 9] as a func-tion of the number of training samples (with validation sample fixed at 200). Also shown is the run time for the algorithm of Li et al. (2007), when we use it on a Algorithm 1 Main steps of our bi-level path following algorithm grid of 8000 evenly spaced  X  values in [0 . 1 , 0 . 9] and find the best cross validated solution by enumerating the candidates as identified in Section 3. Our conjecture that the number of knots in the 2-dimensional path behaves like O ( n 2 ) seems to be consistent with these results, as is the hypothesized overall time complexity dependence of O ( n 3 ).
 Finally, we demonstrate the ability of KQR to cap-ture the quantiles with  X  X trange X  errors from model 2. Figure 4 shows a data sample generated from this model and the (0 . 25 , 0 . 5 , 0 . 75) quantiles of the conditional distribution P ( Y | X ) (solid), compared to their cross-validated KQR estimates (dashed), using 500 samples for learning and 200 for validation (more data is needed for learning because of the very large variance at values of x close to 1). As expected, we can see that estimation is easier of the lower quantiles and at smaller values of x , because the distribution P ( Y | X = x ) has long right tails everywhere and has much larger variance when x is big. In this paper we have demonstrated that the family of bi-level optimization problems (4) defined by the family of loss functions L  X  can be solved via a path following approach which essentially maps the whole surface of solutions  X  f (  X ,  X  ) as a function of both  X  and  X  and uses insights about the possible locations of the bi-level optima to efficiently find them. This leads to a closed-form algorithm for finding f  X  (  X  ) for all quan-tiles. We see two main contributions in this work: a. Identification and solution of a family of non-convex optimization problem of great practical interest which can be solved using solely convex optimization tech-niques; and b. Formulation of a practical algorithm for generating the full set of cross-validated solutions for the family of kernel quantile regression problems. Our algorithm as presented here can easily be adapted to bi-level path following of cross validated solutions of SVR, as the size  X  of the don X  X -care region changes (see Rosset (2008) for details). However, it should be noted that the statistical motivation for solving quan-tile regression for multiple quantiles does not really carry through to  X  -SVR, as the parameter  X  and the loss function it parameterizes do not have a natural interpretation in the spirit of  X  .
 There are many other interesting aspects of our work, which we have not touched on here, including: de-velopment of further optimization shortcuts to im-prove algorithmic efficiency; investigation of the range of applicability of our algorithmic approach beyond KQR and SVR; analysis of the use of various kernels for KQR and how the kernel parameters and kernel properties interact with the solutions; implementation of in-sample model selection approaches such as SIC (Koenker, 2005; Li et al., 2007) instead of cross valida-tion in our framework (which should require minimal changes).

