 Lawr ence Carin LCARIN @ EE . DUKE . EDU Ov er the last two decades researchers have considered sparse signal representations in terms of orthonormal basis functions ( e.g. , the wavelet transform). For example, con-sider an m -dimensional real-v alued signal u and assume an m  X  m orthonormal basis matrix  X  ; we may then express u =  X   X  , where  X  is an m -dimensional column vector of weighting coef  X cients. For most natural signals there ex-ists an orthonormal basis  X  such that  X  is sparse. Consider now an approximation to u , ^ u =  X  ^  X  , where ^  X  approxi-mates  X  by retaining the lar gest N coef  X cients and setting the remaining m  X  N coef  X cients to zero; due to the afore-mentioned sparseness properties, jj u  X  ^ u jj 2 is typically very small even for N  X  m . Con ventional techniques require one to measure the m -dimensional signal u but  X nally dis-card m  X  N coef  X cients (Charilaos, 1999). This sample-then-compress frame work is often wasteful since the sig-nal acquisition is potentially expensi ve, and only a small amount of data N is eventually required for the accurate approximation ^ u . One may therefore consider the follo w-ing fundamental question: Is it possible to directly measure the informati ve part of the signal? Recent research in the  X eld of compressi ve sensing sho ws that this is indeed pos-sible (Candes, 2006)(Donoho, 2006).
 Exploiting the same sparseness properties of u emplo yed in transform coding ( u =  X   X  with  X  sparse), in com-pressi ve sensing one measures v =  X   X  , where v is an n -dimensional vector with n &lt; m , and  X  is the n  X  m sensing matrix. There are several ways in which  X  may be constituted, with the reader referred to (Donoho, 2006) for details. In most cases  X  is represented as  X  = T X  , where T is an n  X  m matrix with components constituted randomly (Tsaig &amp; Donoho, 2006); hence, the CS mea-surements correspond to projections of u with the rows of T : v = T u = T X   X  =  X   X  , which is an under -determined problem. Assuming the signal u is N -sparse in  X  , implying that the coef  X cients  X  only have N nonzero values (Candes, 2006) (Donoho, 2006), Cand  X  e s, Romber g and Tao in (Candes et al., 2006) sho w that, with over-whelming probability ,  X  (and hence u ) is reco vered via if the number of CS measurements n &gt; C  X  N  X  log m ( C small constant); if N is small ( i.e. , if u is highly compress-ible in the basis  X  ) then n  X  m . In practice the signal u is not exactly sparse, but a lar ge number of coef  X cients in the basis  X  may be discarded with minimal error in re-constructing u ; in this practical case the CS frame work has also been sho wn to operate effecti vely .
 The problem in (1) may be solv ed by linear program-ming (S. Chen &amp; Saunders, 1999) and greedy algo-rithms (Tropp &amp; Gilbert, 2005) (Donoho et al., 2006). A Bayesian compressi ve sensing (BCS) methodology is pro-posed in (Ji et al., 2007b), by posing the CS inversion prob-lem as a linear -re gression problem with a sparseness prior on the regression weights  X  . One adv antage of BCS is that this frame work may be extended to multi-task compressi ve sensing (Ji et al., 2007a), in which each CS measurement v i =  X  i  X  i represents a sensing  X task X  and the objecti ve is to jointly invert for all f  X  i g i =1 ;M , through an appropriate sharing of information between the M data collections. In multi-task CS, one may potentially reduce the number of measurements required for each task by exploiting the sta-tistical relationships among the tasks, for example,  X Dis-trib uted Compressed Sensing X  (DCS) (Baron et al., 2005), an empirical Bayesian strate gy  X Simultaneous Sparse Ap-proximation X  in (W ipf &amp; Rao, 2007), and a hierarchical Bayesian model for multi-task CS (Ji et al., 2007a). Ho w-ever, these multi-task algorithms assume all tasks are ap-propriate for sharing, which may not be true in man y prac-tical applications. In this paper we introduce a Dirichlet process (DP) prior (W est et al., 1994) to the hierarchical BCS model, which can simultaneously perform the inver-sion of the underlying signals and infer the appropriate sharing/clustering structure across the M tasks.
 As detailed belo w, an important property of DP for the work presented here is that it pro vides a tool for semi-parametric clustering ( i.e. , the number of clusters need not be set in adv ance). The DP-based hierarchical model is emplo yed to realize the desired property of simultane-ously clustering and CS inversion of the M measurements f v i g i =1 ;M . A variational Bayes (Blei &amp; Jordan, 2004) in-ference algorithm is considered, yielding a full posterior over the model parameters  X  i . 2.1. Multi-T ask CS Formulation for Global Sharing Let v i represent the CS measurements associated with task i , and assume a total of M tasks. The i -th CS measurement may be represented as where the CS measurements v i are characterized by an n i dimensional real vector , the sensing matrix  X  i correspond-ing to task i is of size n i  X  m , and  X  i is the set of (sparse) transform coef  X cients associated with task i . The j th co-ef X cient of  X  i is denoted  X  i;j . The residual error vector  X  2 R n i is modeled as n i i:i:d: dra ws from a zero-mean Gaussian distrib ution with an unkno wn precision  X  0 (vari-ance 1 = X  0 ); the residual corresponds to the error imposed by setting the small transform coef  X cients exactly to zero when performing the CS inversion.
 We impose a hierarchical sparseness prior on the parame-ters  X  i , the lower level of which is where  X  i;j is the j th component of the vector  X  i . To im-pose sparseness, on a layer abo ve a Gamma hyperprior is emplo yed independently on the precisions  X  i;j . The lik eli-hood function for the parameters  X  i and  X  0 , given the CS measurements v i , may be expressed as p ( v i j  X  i ;  X  0 ) = ( Concerning the aforementioned hyperprior , for the multi-task CS model proposed in (Ji et al., 2007a), the parame-ters  X  i =  X  , for i = 1 ;  X  X  X  ; M , and  X   X  Q m In this frame work the CS data from all M tasks are used to jointly infer the hyper -parameters  X  (global processing). Ho we ver, the assumption in such a setting is that it is ap-propriate to emplo y all of the M tasks jointly to infer the hyper -parameters. One may envision problems for which the M tasks may be clustered into several sets of tasks (with the union of these sets constituting the M tasks), and data sharing may only be appropriate within each clus-ter. Through use of the Dirichlet process (DP) (Escobar &amp; West, 1995) emplo yed as the prior over  X  i , we simultane-ously cluster the multi-task CS data, and within each cluster the CS inversion is performed jointly . Consequently , we no longer need assume that all CS data from the M tasks are appropriate for sharing. 2.2. Dirichlet Pr ocess for Cluster ed Sharing The Dirichlet process, denoted as DP (  X ; G 0 ) , is a measure on measures, and is parameterized by a positi ve scaling pa-rameter  X  and the base distrib ution G 0 . Assume we have f  X  i g i =1 ;M and each  X  i is dra wn identically from G itself is a random measure dra wn from a Dirichlet process, where G 0 is a non-atomic base measure.
 Sethuraman (Sethuraman, 1994) pro vides an explicit char -acterization of G in terms of a stick-breaking construction. Consider two in X nite collections of independent random variables  X  k and  X   X  dra wn i.i.d. from a Beta distrib ution, denoted Beta (1 ;  X  ) and the  X   X  The stick-breaking representation of G is then de X ned as where  X  k j  X  iid  X  Beta (1 ;  X  ) and  X   X  sentation mak es explicit that the random measure G is dis-crete with probability one and the support of G consists of an in X nite set of atoms located at  X   X  from G 0 . The mixing weights w k for atom  X   X  successi vely breaking a unit length  X stick X  into an in X nite number of pieces, with 0  X  w k  X  1 and P 1 2.3. Multi-T ask CS with DP Priors We emplo y a DP prior with stick-breaking representation for  X  i in the model in (3), which assumes that  X  i j G  X  G and G = P 1 responds to the sparseness promoting representation dis-cussed in Sec 2.1. To facilitate posterior computation we introduce an indicator variable z i with z i = k indicating  X  i =  X   X  k . Therefore the DP multi-task CS model is ex-pressed as  X  where i = 1 ;  X  X  X  ; M , j = 1 ;  X  X  X  ; m , k = 1 ;  X  X  X  ; K 1  X  K  X  1 , and  X  i;j is the j -th element of  X  i . For con venience, we denote the model in (8) as DP-MT CS. In practice K is chosen as a relati vely lar ge inte ger ( e.g. , K = M if M is relati vely lar ge) which yields a negligi-ble dif ference compared to the true DP (Ishw aran &amp; James, 2001), while making the computation practical.
 The choice of G 0 here is consistent with the sparseness-promoting hierarchical prior discussed in Section II-A. Consider task i and assume  X  i tak es value  X   X  distrib ution over  X  i is then p (  X  i j c; d ) = Equation (9) is a type of automatic rele vance determination (ARD) prior which enforces the sparsity over  X  i (Tipping, 2001). We usually set c and d very close to zero ( e.g. , to mak e a broad prior over  X   X  on man y of the elements of  X   X  values, consequently the posteriors on the associated ele-ments of  X  i concentrate at zero, and therefore the sparse-ness of  X  i is achie ved (MacKay , 1994) (Neal, 1996). Since these posteriors have  X hea vy tails X  compared to a Gaussian distrib ution, the y allo w for more rob ust shrinkage and bor -rowing of information. Similarly , hyper -parameters a , b and f are all set to a small value to have a non-informati ve prior over  X  0 and  X  respecti vely . One may perform inference via MCMC (Gilks et al., 1996), howe ver this requires vast computational resources and MCMC con vergence is often dif  X cult to diagnose (Gilks et al., 1996). Variational Bayes inference is therefore in-troduced as a relati vely ef X cient method for approximating the posterior . From Bayes' rule, we have where V = f v i g i =1 ;M are CS measurements from M CS tasks, H = f  X  0 ,  X ;  X  , f z i g i =1 ;M , f  X  i g i =1 ;M f  X   X  k g k =1 ;K g are hidden variables (with  X  = f  X  k g and  X  = f a; b; c; d; e; f g are kno wn hyper -parameters. The inte gration in the denominator of (10), called the mar ginal likelihood , or  X evidence X  (Beal, 2003), is gener -ally intractable to compute analytically . Instead of directly estimating p ( H j V ;  X ) , variational methods seek a distri-bution q ( H ) to approximate the true posterior distrib ution p ( H j V ;  X ) . Consider the log mar ginal lik elihood log p ( V j  X ) = F ( q ( H )) + D KL ( q ( H ) jj p ( H j V ;  X )) ; where F ( q ( H )) = and D KL ( q ( H ) jj p ( H j V ;  X )) is the KL divergence be-tween q ( H ) and p ( H j V ;  X ) . The approximation of p ( H j V ;  X ) using q ( H ) can be achie ved by maximizing F ( q ( H )) , which forms a strict lower bound on log p ( V j  X ) In this way estimation of q ( H ) may be made computation-ally tractable. In particular , for computational con venience, q ( H ) is expressed in a factorized form, with the same func-tional form as the priors p ( H j  X ) . For the model in (8), we assume q ( H ) = q (  X  0 ) q (  X  ) q (  X  ) where q (  X  0 )  X  Ga (~ a; ~ b ) , q (  X  )  X  Ga (~ e; ~ f ) Q q (  X  i )  X  N (  X  i ;  X  i ) , q (  X   X  k )  X  Q m j =1 Ga (  X   X  By substituting (13) and (8) into (12), the lower bound F ( q ) is readily obtained. The optimization of the lower bound F ( q ) is realized by taking functional deri vatives with respect to each of the q (  X  ) distrib utions while  X xing the other q distrib utions, and setting @ F ( q ) =@q (  X  ) = 0  X nd the distrib ution q (  X  ) that increases F (Beal, 2003). The update equations for the variational posteriors are summa-rized in the Appendix. The con vergence of the algorithm is monitored by the increase of the lower bound F . One prac-tical issue of the variational Bayesian inference is that the VB algorithm con verges to a local maximum of the lower bound of the mar ginal log-lik elihood since the true poste-rior usually is multi-modal. Therefore the average of multi-ple runs of the algorithm from dif ferent starting points may avoid this issue and yield better performance. 4.1. Synthetic data In the  X rst set of examples we consider synthesized data to examine the sharing mechanisms associated with the DP-MT CS inversion. In the  X rst example we generate data with 10 underlying clusters. Figure 1 sho ws ten  X tem-plates X , each corresponding to a 256-length signal, with 30 non-zero components (the values of those non-zero com-ponents are randomly dra wn from N (0 ; 1) ). The non-zero locations are chosen randomly for each template such that the correlation between these sparse templates is zero. For each template,  X ve sparse signals (each with 256 samples) are generated by randomly selecting three non-zero ele-ments from the associated template and setting the coef- X cients to zero, and three zero-amplitude points in the tem-plate are randomly now set to be non-zero (each of these three non-zero values again dra wn from N (0 ; 1) ). In this manner the sparseness properties of the  X ve signals gener -ated from a given template are highly related, and the ten clusters of sparse signals have distinct sparseness proper -ties. For each sparse signal a set of CS random projections are performed, with the components of each projection vec-tor dra wn randomly from N (0 ; 1) (Donoho, 2006). The re-construction error is de X ned as jj ^ u  X  u jj 2 = jj u jj is the reco vered signal and u is the original one. Figure 2 sho ws the reconstruction errors of the CS inver-sion by DP-MT CS as well as the global-sharing MT CS discussed in Sec 2.1 (denoted as MT  X  CS for simplicity), as a function of the number of CS measurements. Both CS al-gorithms are based on the VB DP-MT algorithm described in Sec 3, howe ver for MT  X  , we set  X  i; 1 = 1 , and  X  i;k for k &gt; 1 for all tasks and  X x the values of  X  i;k in each it-eration without update. The experiment was run 100 times (with 100 dif ferent random generations of random projec-tion as well as initial membership), and the error bars in Figure 2 represent the standard deviation about the mean. From Figure 2 the adv antage of the DP-based formulation is evident. In Figure 2 we also present histograms for the number of dif ferent clusters inferred by the DP-MT CS. It is clear from Figure 2 that the algorithm tends to infer about 10 clusters, but there is some variation, with the variation in the number of clusters increasing with decreasing number of CS measurements.
 To further examine the impact of the number of underlying clusters, we now consider examples for which the data are generated for 5, 3, 2 and 1 underlying. For each of tem-plates,  X ve sparse signals are generated randomly , in the manner discussed abo ve for the ten-cluster case. In Fig-ures 3-6 are sho wn results in the form considered in Fig-ure 2, for the case of 5, 3, 2 and 1 underlying clusters for data generation. One notes the follo wing phenomenon: As the number of underlying clusters diminishes, the dif fer -ence between DP-MT and MT  X  CS algorithms diminishes, with almost identical performance witnessed for the case  X  ~ c k;j = c + 1 Baron, D., Wakin, M. B., Duarte, M. F., Sarv otham, S., &amp; Baraniuk, R. G. (2005). Distrib uted compressed sensing. Beal, M. J. (2003). Variational algorithms for appr oximate bayesian infer ence . Doctoral dissertation, Gatsby Com-putational Neuroscience Unit, Uni v. Colle ge London. Blei, D., &amp; Jordan, M. (2004). Variational methods for the dirichlet process. ICML .
 Candes, E. (2006). Compressi ve sensing. Proc. of ICM , 3 , 1433 X 1452.
 Candes, E., Romber g, J., &amp; Tao, T. (2006). Rob ust un-certainty principles: Exact signal reconstruction from highly incomplete frequenc y information. IEEE Trans. on Inform. Theory , 52 , 489 X 502.
 Charilaos, C. (1999). Jpe g2000 tutorial. ICIP .
 Donoho, D. L. (2006). Compressed sensing. IEEE Trans. Information Theory , 52 , 1289 X 1306.
 Donoho, D. L., Tsaig, Y., Drori, I., &amp; Starck, J.-C. (2006).
Sparse solution of underdetermined linear equations by stage wise orthogonal matching pursuit.
 Escobar , M. D., &amp; West, M. (1995). Bayesian density es-timation and inference using mixtures. JASA , 90 , 577 X  588.
 Gilks, W. R., Richardson, S., &amp; Spie gelhalter , D. J. (1996).
Introducing mark ov chain monte carlo. In Mark ov chain monte carlo in practice . London, U.K.: Chapman Hall. Ishw aran, H., &amp; James, L. F. (2001). Gibbs sampling meth-ods for stick-breaking priros. JASA , 96 , 161 X 173. Ji, S., Dunson, D., &amp; Carin, L. (2007a). Multi-task com-pressi ve sensing. Submit to IEEE Trans. on SP .
 Ji, S., Xue, Y., &amp; Carin, L. (2007b). Bayesian compressi ve sensing. Accepted to IEEE Trans. on SP .
 MacKay , D. J. C. (1994). Bayesian methods for backprop-agation netw orks. In E. Doman y, van J. L. Hemmen and K. Schulten (Eds.), Models of neur al networks III . Ne w York: Springer -Verlag.
 Neal, R. M. (1996). Bayesian learning for neur al networks . Springer .
 S. Chen, D. L. D., &amp; Saunders, M. A. (1999). Atomic de-composition by basis pursuit. SIAM Journal on Scienti X c Computing , 20 , 33 X 61.
 Sethuraman, J. (1994). A constructi ve de X nition of the dirichlet prior . Statistica Sinica , 2 , 639 X 650. Tipping, M. E. (2001). Sparse bayesian learning and the rele vance vector machine. JMLR , 1 , 211 X 244.
 Tropp, J. A., &amp; Gilbert, A. C. (2005). Signal reco very from partial information via orthogonal matching pursuit. Tsaig, Y., &amp; Donoho, D. L. (2006). Extensions of com-pressed sensing. Signal Processing , 86 , 549 X 571. West, M., Muller , P., &amp; Escobar , M. (1994). Hierarchical priors and mixture models with applications in regres-sion and density estimation. In P. R. Freeman and A. F.
Smith (Eds.), Aspects of uncertainty , 363 X 386. John Wi-ley.
 Wipf, D. P., &amp; Rao, B. D. (2007). An empirical bayesian strate gy for solving the simultaneous sparse approxima-
