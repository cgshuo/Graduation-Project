
Recently, there is much interest in applying machine lear-ning in domains with large scale spatiotemporal characteris-tics. Examples range from discriminating cognitive brain states using functional Magnetic Resonance Imaging (fMRI) [1], [2], [3], [4], [5], to developing techniques for classification of brain signals in Brain Computer Interfaces (BCI) [6], [7], [8], [9], performing automated video classification [10] and more.
However, many existing techniques prone insufficient when the data is temporal (spanned over a time course) and spa-tially exhaustive (consists of a large number of locations in space). Classification often becomes computationally intensive and unfeasible. Raw data collected along the time course in a high-resolutional space results in hundreds of thousands of data-points, for which classical, straightforward machine learning approaches become practically ineffective. Studies presented in [3], [11] face these obstacles, discuss why the existing methods fail and present possible solutions (which either produced medium accuracy results, or were applied to moderately scaled datasets).

In this work we present a methodology for both overcoming the scalability challenge and exploiting the spatiotemporal properties of the data for classification. Our methodology is comprised of three phases. First, we present a greedy pixel selection technique, i.e. choosing the most discriminative spatial characteristics within the full spatial range in a sample X  X  space, based on the random subspace method [12]. Second, we provide two alternatives for feature extraction , applied on the spatially-reduced samples produced by the first phase: features as pixels in time and spatial averaging of pixel groups based on inter-pixel correlation. Finally, we employ a simple and yet effective feature selection based on information gain filtering.
We apply our methodology in the neuroimaging domain, and demonstrate how it helps to decode neuronal population responses in the visual cortex of monkeys, collected using Voltage-Sensitive Dye Imaging (VSDI)[13]. VSDI is capable of measuring neuronal population responses at high spatial ( 10 , 000 pixels of size 60  X  60 to 170  X  170  X m 2 each) and temporal ( 10 ms or less) resolutions. The produced data consists of tens of thousands of numeric values, correlated to locations in space, rapidly changing during the time course. Our methodology makes it possible to process this massive amount of data in a computationally feasible manner. It serves as a tool that aids to decode these responses, as we show how to carefully pick and process those specific properties of the data that carry the most discriminative nature. While first attempts to decode neuronal population responses collected using VSDI were performed in [14], no machine learning methods were used X  X  proprietary statistical approach of pooling rules was developed (relying on the amplitude of the response and other neuronal characteristics). To the best of our knowledge, this is the first time where machine learning techniques are applied in this field.

Much research for decoding of cognitive brain states, em-ploying machine learning methods, has been done in fMRI. While being the most common non-invasive technique for brain study in humans, its deficiency is that it measures metabolic changes: the hemodynamic response occurring few seconds after the onset of the visual stimulus. Whereas the temporal resolution of neuronal activity is within tens of ms , the resolution of the fMRI signal is at least two orders slower. For this reason, fMRI studies don X  X  usually take advantage of the temporal aspect. Such studies include picking the top n most active voxels based on t -test [15] or on average between the voxels [16]; picking the top n most discriminating voxels, based on training a classifier per each voxel [1]; or, picking the n most active voxels per Region Of Interest (ROI) [1]. While they manage to produce moderate to high accuracy results, they rely on relatively small resolutions of data (where training a classifier per voxel is admissable), or on expert knowledge (defining an ROI). The methods we present in our work are domain independent (require no prior knowledge), aimed at very high resolutional data, and exploit both temporal and spatial dimensions.

As for fMRI exploiting the temporal dimension, [2] employs the following heuristic: features are defined as voxel-timepoint pairs, ranked by how well they individually classify a training set, and the top 100 features for the final classifier are chosen. While individual training of classifiers for all time-space combinations is totally unfeasible in large scale domains, we do adopt the time-space combination approach in our work. Additional work that has inspired us is [3], in which one of the introduced techniques for feature selection is defining voxel-specific time-series analysis, by ranking features by mutual information with respect to the class variable. From the ranked features, the n highest ranked are selected, and closeness of each pair of voxels X  time series is measured. Despite the high reported success rates, the techniques in [3] are subject to be computationally expensive in large scale domains.

A different spatiotemporal domain which is fundamentally based on techniques for classification of brain-emitted signals is BCI. Here, brain-controlled computer systems are developed in order to operate the machine (e.g. prostheses, communi-cation) by brain activity (e.g. imagining a hand movement will cause a prosthetic computer-controlled arm to move). For example, the method presented in [8] maintains the correlation information between spatial time-series items by utilizing the correlation coefficient matrix of each such item as features to be employed for classification. Then Recursive Feature Elimination (RFE, as proposed in gene selection problem [17]) is used for feature subset selection of time-series datasets. Applying RFE in a similar manner on our type of data is com-putationally expensive X  X owever, we do adopt the approach of correlation between spatial elements in our feature extraction.
A last example from spatiotemporal domains is automated video genre classification [10]. In this case, the problem is investigated by first computing a spatiotemporal combined audio-visual  X  X uper X  feature vector (of very high dimensional-ity). Then, the feature vector is further processed using Princi-pal Component Analysis (PCA) to reduce the spatiotemporal redundancy while exploiting the correlations between feature elements. However, the PCA-based techniques in multivariate time-series datasets are known to be problematic in regard to scalability, which is more than evident in our domain.

In this section, we present the three phase methodology for building scalable models for spatiotemporal data classification. To describe our methodology, we first need to formalize the problem. A spatiotemporal domain contains n pixels that constitute the global pixel set P = { p 1 ,p 2 ,...,p pixel p i , i  X  { 1 ,...,n } represents a concrete location in space, in which a series of m contiguous values in time is measured. The intervals between each two consequent values in time are equal. In turn, p t i , i  X  X  1 ,...,n } , t  X  X  1 ,...,m } indicates the specific timeframe t along the time course, at which the value of p i is measured. In fact, p t i represents the pixel-in-time combination of pixel p i and time t .

That being the case, the finite training samples set of size k in the spatiotemporal domain is defined as: S = { s 1 ,s 2 ,...,s k } , where a single sample s l , l  X  { 1 ,...,k } is a set of vectors: s l = { p 1 ,..., p n } , where a vector p i = v i 1 ,...,v i m , v i t  X  R , t  X  { 1 ,...,m } denotes the actual m values along the time course, measured for the pixel p in the sample s l . Each training sample s l  X  S is labeled with a class label c  X  C . For an infinitely large universal set U of all possible unlabeled samples u = { p 1 ,..., p n } , u  X  U , the classification problem is to build a model that approximates classification functions of the form f : U  X  X  X  C , which map unclassified samples from U to the set of class labels C .
In the next subsections we describe each of the phases in detail. Subsection III-A presents a technique for selecting the pixels that have the most discriminative characteristics among the global pixel set P . Next, in III-B, we introduce two alternative techniques for extracting the features from the pixels selected by the first phase. The third phase described in III-C shows an effective application of feature selection on the product of the second phase, to further improve the abilities of the remaining features that constitute the generated models. A. Pixel selection via greedy improvement of random spatial subspace
The technique described here uses common machine lear-ning tools in order to reveal the most informative pixels, which will define the features to be used with our model. The discriminative nature of the selected pixels stems from analyzing their measured values along the time course. Due to the high spatial and temporal resolutions of the domains in question, our data is comprised of hundreds of thousands of basic data-points. Hence, using the most granular, basic values of the sample X  X  space as features will lead to an extremelly high dimensional feature space, rendering classification, or even feature dimensionality reduction techniques, unfeasible. We present here a greedy approach based on the random subspace method [12] for selecting by iterative refinement, the set of pixel subsets from which we can eventually derive the sought-after pixel set.
In Algorithm 1, we randomly generate r pixel subsets of a requested size u (number of pixels in a subset). Handling small pixel subsets yields an easier handling of a reduced spatial dimension. However, in order to cover a large portion of pixels (inherently, features) in the data and to establish credibility for the selected pixels, we need to rely on a wide-enough selection of such subsets 1 . Each generated pixel subset X  X  classification capabilities are roughly evaluated (Algorithm 2): pixel values in time are defined as features (step 2), as was done in [2]. Then an Information Gain (InfoGain) based feature selection [18] is applied to select only the features with positive InfoGain scores (step 3). Our usage of InfoGain for ranking features by mutual information with respect to the class is insipired by [3], an fMRI study exploiting the temporal dimension. The resulting feature set is cross-validated using linear-kernel SVM (WEKA X  X  implementation of the SMO algorithm, [18]) to obtain an evaluation score (accuracy of the evaluated set). The scores are then ordered in a descending order, and the greedy phase begins.

During the greedy phase, we maintan a set  X  of pixel subsets, of which the desirable pixel set can be derived at any time. Initially,  X  is initialized with the highest-ranked pixel subset (along with its evaluation score). In each iteration over the ranked pixel subsets list, the next subset in the list joins  X  . A set of pixels of size u is then extracted from  X  (Algorithm 3), and evaluated (using Algorithm 2). The greedy step: if the resulting evaluation score is higher than the existing evaluation score of  X  , the current pixel subset remains in  X  . Otherwise, it X  X  discarded. Finally, when the iteration over the pixel subsets is over, the desirable set of pixels is extracted from  X  to serve as the pixel selection. The extraction of the pixel set from  X  (Algorithm 3) is done as follows: each individual pixel subset in  X  is turned into a feature set, where pixel values in time are defined as features (step 2a). An InfoGain based feature selection is applied on this feature set, and the InfoGain scores for each feature are taken (step 2b). The score for each individual pixel is calculated by averaging (along the number of pixel instances) the weighted averages of InfoGain scores (along the pixel X  X  time course in each of the feature sets) (step 2c). The evaluation score of each pixel subset in  X  is used as the weight for computing the grand-average, effectively giving higher weight to pixels and features stemmed from highly evaluated pixel subsets.
 B. Feature extraction
Methods described here are applied on the pixel selection results of the first phase (Subsection III-A). We present two alternative feature extraction approaches in order to cope with variability evident in different spatiotemporal datasets. Even when the datasets originate from the same domain, they can bear different spatial characteristics, expressed in the noise Algorithm 1 Greedy Improvement of Random Spatial Subspace X  GIRSS ( S,C,u,r ) Algorithm 2 Pixel Set Evaluation X  evaluatePixelSet S,C,P 0 ,u level and the resolution of the signal collected during the dataset construction. The alternatives provided here are each aimed at a different datasets sector. 1) Features as pixels in time: The straightforward ap-proach for extracting a feature set F from a given pixel set P  X  = { p  X  1 ,p  X  2 ,...,p  X  u } over the sample set S , is to define it as all pixel-in-time combinations F = p j | t  X  X  1 ,...,m } ,  X  p j  X  P used this approach in Subsection III-A for ranking pixel subsets and feature sets. While for simpler classification tasks this is satisfactory X  X ast, simple and effective (Section IV), a method described next is suggested for more complex tasks. Algorithm 3 Highest Ranked Pixels Extraction X  extractHighestRankedPixels ( S,C,  X  ,u,Z [1 : r ]) 2) Spatial averaging of pixel groups based on inter-pixel correlation: The motivation for this method is to overcome the negative effects of a possibly noisy data by performing a spatial-level averaging of pixels that share a common nature. This requires that the trends of their change along the time course will have similar characteristics. Two questions raised here are how to measure similarity, and how to choose  X  X im-ilar X  pixels in space, designated for averaging. The way we measure similarity is by employing Pearson X  X  product moment coefficient [19] between pairs of pixels. We then perform pixel averaging within groups of  X  X imilar X  neighboring pixels. The reason for this lies in the nature of our data X  X  non-trivial negative correlation exists between all pixel-pairs cor-relations and all pixel-pairs distances 2 , showing that higher distances between pixels lead to lower correlations between them. Therefore, choosing neighboring groups of pixels as a whole, having a high inter-group similarity, has the potential to reveal stronger discriminative characteristics X  X ather than picking individual pixels from the same group.

In Algorithm 4 we show how the neighborhood formation for pixel groups generation is done. This formation is based on a given pixel set, a product from the previous phase (III-A) X  X e refer to this set as  X  X eeds X . First, we calculate a correlation coefficient matrix C and a distances matrix D between all pixel pairs (step 3). Then we define the set of pixel subsets  X  , which will eventually hold the groups of neighboring pixels that share a similar nature. Next we employ a graded group formation phase (step 5), where the correlation strength dictates the group formation order: groups having the strongest inter-similarity are generated first, ensur-ing that the eventually formed groups exploit the similarity property to its full extent (only positive correlation coefficient Algorithm 4 Inter-Pixel COrrelation based Spatial Averaging X  IPCOSA ( S,C,P  X  , X  ) thresholds are used) 3 . The group formation is subject to the following guidelines: a group of pixels must contain at least one seed within it to base the group on. Once chosen, the seed X  X  proximate neighbors X  correlation scores are examined. Neighbors with scores that fit the graded correlation threshold join the seed X  X  group. Recursively, the correlation scores of the neighbors of each of the newly-joined group members are tested, and additional pixels conforming to the correlation and the proximity requirements join the group. Eventually, a group stops expanding once none of the group members X  neighbors fits the requirements. At this step, a formed group joins  X  , and its members are no longer available for formation of new groups. A group may consist of a sole seed (step 6). At the end of the group formation phase,  X  contains groups of neighboring pixels, each based on one or more seeds. Some groups have stronger inter-similarity than the others, but due to our graded group formation phase, even the weaker groups are generally based on non-negligible positive correlation scores At the final phase of our algorithm, the feature extraction is based on  X   X  X  pixel groups: pixel values at each of the points in time are averaged along their spatial dimension X  X cross all pixels within each of the groups of  X  . The resulting features represent the average-in-time of similar pixels, as opposed to the pixel-in-time approach presented in III-B1. For seeds pixel set of size u , there will be at most u  X  m features (number of formed groups will not exceed the number of seeds, as each group must contain at least one seed).
 C. Feature selection
To further improve model quality and reduce the feature-space dimensionality, feature selection is applied on the extracted features. InfoGain-based feature selection [18] is applied on the given feature set F , producing scores: IG ( f ) ,  X  f  X  F . Then, only the features with positive InfoGain scores: IG ( f ) &gt; 0 are selected. The motivation: the features produced in III-B are based on pixel selection from III-A, where the whole time-spectrum of pixels or pixel groups is preserved. However, points along the time course exist, during which the spatial discriminative nature is not realized (e.g. long before the onset of the signal in VSDI). Not only that these points in time are ineffective for the emphasis of the spatial characteristics, but they sometimes obscure their discriminating potential. InfoGain filtering drops those unwanted features with negligible scores, whose contribution is neutral or negative.

The primary goal in our work is to suggest a combination of effective techniques for obtaining scalable and accurate classification in large scale spatiotemporal domains. To reach this goal, we demonstrate how our techniques are evaluated in the VSDI domain and applied to VSDI datasets. The accuracy of the classification is validated by the evaluation of our classification performance. The scalability of our methods is shown by exploring their feasibility from the run-time perspective. This is done by emphasizing the lessons learned from the experience we had with applying approaches similar in nature to the ones reviewed in Section II. Many of these approaches use the most granular values of the sample X  X  space for feature selection and classification, which eventually leads to an extremelly high dimensional feature space. Our failure in employing these approaches is compared to the success of showing the feasibility of our methodology. We additionaly compare our results to those achieved by an Oracle  X  X  domain expert X  X aced with the same tasks, and validate their credibility.
 A. Datasets
Each evaluated dataset is based on a single imaging ex-periment performed in the visual cortex of one animal and composed from multiple trials. In each experiment, the mon-key was shown a set of different visual stimuli, one specific stimulus per trial. Each stimulus presentation was repeated 20 -30 times. Neuronal population responses in the visual cortex evoked by the stimulus, were recorded using VSDI. The imaged area was divided into a grid of pixels, and popula-tion response (summed membrane potentials of all neuronal elements) of each pixel was recorded during the time window of the trial [13]. Each trial in an experiment is a sample in our sample space. A sample consists of all pixels of the recorded area, where a pixel is a time-series of values collected along the time course of the trial. These values represent the rawest possible data-points X  X ith no averaging across trials, whether in time or space, therefore directly reflecting unprocessed measurement points. Hence, the VSDI decoding we did was performed at a single trial level. Each sample is labeled with a class that represents the appropriate stimulus. The datasets differ in the number and the type of the presented stimuli, both affecting the complexity of the decoding. Being able to perform successful classification of these datasets, is being able to  X  X ead X  what the monkey has seen without seeing it ourselves. 1) Dataset 1: Oriented Gratings (simple): The monkey was presented with two different drifted square gratings at horizontal and vertical orientations, and a blank control image with no stimulus (Fig. 1). Each of the 293 samples in the dataset had 2162 pixels (a 46  X  47 matrix) along 51 time points. The three classes had almost uniform distribution where the mode class consitutes 34 . 13% of the population (setting the baseline accuracy, i.e. ZeroR [18]). 2) Dataset 2: Gabors (complex): The monkey was pre-sented with five different Gabor based orientations in space and a blank control image (Fig. 2). Each of the 153 samples had 10 , 000 pixels (a 100  X  100 matrix) along 51 time points. The six classes had almost uniform distribution where the mode class consitutes 18 . 95% of the population (ZeroR baseline accuracy). 3) Dataset 3: Contours (hard): The monkey was presented with four different Gabor-based Contours in space and a blank control image (Fig. 3). The four Gabor-based Contours divide into two pairs, where the differences between the classes in each pair are very subtle and hardly noticeable. Each of the 124 samples had 10 , 000 pixels (a 100  X  100 matrix) along 61 time points. The five classes had almost uniform distribution where the mode class consitutes 23 . 39% of the population (ZeroR baseline accuracy).
 B. Experimental methodology
As a part of the evaluation methodology for the pixel selection technique presented in III-A, we define the Oracle : a pixel selection method, a best-effort attempt by a human expert who was asked to provide a pixel set which, in her professional opinion, has the most potential to successfully discriminate between the different classes of the training samples set. The human expert, or the Oracle , manually picked a set of pixels of some size:  X  = { p 1 ,p 2 ,... } ,  X   X  P , also known as the ROI (Region Of Interest). This set is referred to as the  X  X old standard X , where the aim is to build an accurate classification model using the most discriminating pixels. The success rates achieved by using  X  for pixel selection are compared to the success rates of using the pixel set selected by GIRSS , our pixel selection technique.

In the experimental setup, the domain expert was requested to provide an ROI of pixels for each dataset. In the case of Gabors, we were given an improved ROI, based on the results of using the original ROI 5 . In the case of Contours, three different ROIs of pixels were given in advance, each for individual evaluation by our techniques. We built models using both pixel selection techniques as the first phase, in combina-tion with the two feature extraction methods as the second: { Oracle,GIRSS } X { PixelInTime,IPCOSA } , with ap-plication of the feature selection phase (III-C). The resulting models were evaluated using a 10-fold cross-validation of the multi-class SMO implementation of SVM with linear kernel [18]. Each model X  X  evaluation was performed a number of times (each trial yielding a different random 10-fold division), as specified in the results Table I.
 C. Results
In regard to the classification performance, besides aspiring to achieve the most accurate results, it also was as much as important for us to show that the results we acquire are not inferior to the ones achieved by exploiting the domain expert X  X  guidelines. Indeed, we X  X e shown that for two types of data (Oriented Gratings, Contours), our pixel selection technique is capable of producing pixel sets having as good dicriminative abilities as the best of provided ROI sets. Moreover, for the Gabors data type, our results were superior not only to the initially provided ROI 1 , but also to the revised ROI 2 . In this case, we see major difference when our selected pixel sets are compared to the ROI pixel sets (please refer to Fig. 4 and 5 for example comparison). While both of the ROI sets were defined within the V1 area (primary visual cortex), our sets (of the same size) show a wide spread of pixels among numerous sites, including V2 (secondary visual cortex). One can claim that the comparison is not adequate, since the ROI was limited to V1. Nevertheless, we claim the opposite X  X ur results reveal that while the initial working hypothesis of a neuroimaging expert can be restricted to a specific cortical site (e.g. V1 activity is sufficient for decoding the Gabors X  visual stimuli), in practice, a collaboration between the repre-sentative populations from numerous sites shows much higher contribution to classification.
The high accuracy of the Oriented Gratings dataset is somehow expected due the apparent differences between its visual stimuli, but it X  X  not for granted considering the baseline of 34 . 13% . Due to the high resolution of the signal in the Ori-ented Gratings, we see that the spatial averaging only worsens the results instead of improving them. This is an expected result X  X he signal in this case arises from small orientation columns, while averaging over space smears them out, causing the loss of signal X  X ence, the loss of the data X  X  essential properties. However, with the Gabors and the Contours, we see quite the opposite X  X patial averaging provides additional enhancement to the classification abilities. Being much harder to distinguish than with the first dataset case, the types of the visual stimuli of these two datasets lead to collection of data in which the activation has, at least partially, low spatial frequency characteristics, as opposed to the Oriented Gratings (some of the information in this case has to do with the retinotopic activation). In conclusion, the spatial averaging role depends on the size of the neuronal spatial modules that encode it, leaving space for improvement by the advanced feature extraction technique in datasets characterized by low spatial frequency.

As for the feasibility of construction and evaluation of our models X  X ll early attempts to handle the data before basing our pixel selection on random subspace [12], such as employing techniques that base their feature extraction, selection and classification on the full spatial range (resembling methods proposed in [15], [16], [1], [2]), ended with impractical running times (waiting for weeks with no end in prospect) and memory requirements. However, GIRSS and IPCOSA were able to build models using a single-threaded Java application on a Core 2 Duo machine with 2GB of RAM, in less than 2 hours for the Oriented Gratings, roughly 8 hours for the Gabors, and between 8 to 13 hours for the Contours datasets. Truly, our proposed models are not only feasible, but practical. D. Validation of the results
To further establish the credibility of our results and dis-proof the likelihood of  X  X ree of charge X  high accuracy rates or of possible overfitting, we proceeded with additional validation of the results produced by using our three phase methodology. In VSDI data in particular, the significance of each of the stimuli conditions is realized only after the visual stimulus onset, that is to say X  X he discrimination between the different stimuli (i.e. the classification of the different classes), is only possible after the stimuli were shown to the monkey, and the appropriate neuronal population responses were provoked. Had we observed the responses of the same neuronal populations, solely before the onset of the stimuli, we wouldn X  X  expect to have the ability to discriminate between them X  X imply because of the fact that the behaviour of these responses is expected to be similar to the ones provoked by the blank control image, where no stimulus is presented (which is exactly the case).
The logic discussed above lays the foundations of our validation procedure. We carried the same experiments as detailed in Section III, with two differences. First, in all our datasets, the time course was reduced to only the first consecutive points in time where we know for sure that the onset of the stimuli was not present. Second, pixel selection via the Oracle wasn X  X  included in this procedure X  X nowing that GIRSS has at least as good classification capabilities as the Oracle , such type of comparison at this stage is redundant.
That being the case, we would expect the classification results to be close to baseline accuracies of each of the datasets. Indeed, we can safely say that the results of this stage were as expected X  X oughly the same as the chance level. In our validation procedure, the time course was reduced to the first 10 points in time before the visual stimulus onset for the Oriented Gratings and the Gabors datasets, and to the first 3 points in time for the Contours. For the Oriented Gratings, GIRSS with u (154) ,r (20) , applied with PixelInTime , produced the average accuracy of 31 . 6% ; the application of IPCOSA instead of PixelInTime yielded the average accuracy of 34% . In the Gabors case, GIRSS with u (100) ,r (150) and PixelInTime, produced the average ac-curacy of 17 . 5% , while using the IPCOSA generated roughly the same outcome. Applying GIRSS with u (151) ,r (100) on the Contours produced the accuracy of 21% when used along with the PixelInTime , and the accuracy of 22 . 7% when used with the IPCOSA . Using GIRSS with u (500) ,r (100) for the Contours dataset, generated the results of 19% and 22 . 4% when using the PixelInTime and the IPCOSA , respectively.
 E. Neuroimaging implications
Some questions arise in light of these results with respect to the neuroimaging perspective and neural decoding in par-ticular. Our results show that machine learning can definitely be applied on fields such as VSDI for decoding and possibly other tasks. Without prior knowledge in neuroimaging, we can successfully classify (to some extent) different neuronal population responses with respect to the provoking stimuli. We can support neuroimaging researchers in revealing the dominant areas in the brain responsible for visual processing. Can our results shed new light on the dynamics of the neuronal populations? We believe it can, for two reasons. First, the support of our domain expert, who believes that these results look interesting and promising, and that a further and deeper study is necessary in order to advance in their interpretation. Second, by analyzing the differences revealed between the expert X  X  ROI pixel sets to the ones selected by our technique. Not only that the pixels selected by a non-expert technique provide at least as good results as the expert X  X  ROIs, but they also provide new findings on their significance.

In this work, we presented a combination of methods that employ machine learning techniques to handle vast spatiotem-poral VSDI data. In addition to the results and implications discussed in Section IV, we consider this work as pioneering, in terms of combining these two perspectives to produce an interdisciplinary AI research, applied for the first time to the VSDI domain. With advanced neuroimaging technology and our proposed tools, we foresee further progress in the development of visual perception decoding algorithms to aid in decoding novel visual stimulus, such as movies or real-time streaming visual data. We plan to compare different decoding mechanisms over different cortical areas and be-havioral conditions. Thanks to the fact that our techniques are domain independent, we also plan to apply them in other spatiotemporal domains with resembling characteristics.
We ought to mention that our methods do not treat the time dimension as a dimensionality threat, thus not taking an effort to effectively reduce it. However, we did preliminary attempts to apply various sliding window techniques for temporal reduction, but without any apparent advantage (as expected with potential data loss). Expecting future data to have a much higher temporal resolution obligates temporal reduction. For this purpose, we believe that using Discrete Fourrier Transform (DFT) or Discrete Wavelet Transform (DWT) for dimension-ality reduction of time-series, as reported in [20], [21], will help us find a lower dimensionality time-course representation that preserves the original information X  X escribing the original shape of the time-series data as closely as possible.

This research was supported in part by ISF grants #859/05 (to H. S.) and #1357/07. We thank Elhanan Meirovithz for his help in VSDI data aquisition, and Ofer Garnett for his helpful advice. As always, thanks to K. Ushi and V. Ricki for their support.

