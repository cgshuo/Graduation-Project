 Rating is one of the key ingredients in social web (as well as many non-web settings). Rating plays an important role in influencing decision making and people X  X  choices. In a rating system, reviewers (users) assign rating scores to objects (products, content items, etc.). Multi-criteria ratings on objects in so-cial web have become very common as users often rate objects based on different evaluation criteria, derived from import ant object features. For example, to eval-uate a digital camera on a product review site, a reviewer may give scores to different camera features (e.g., ease of use, battery life, memory size), in addition to an overall score.

By observing the correlation between crit erion-level scores and overall scores (or between scores of two different criteria), we can derive insights about the dependency behavior of reviewers and objects, which are useful in many appli-cations. For example, a recommender system [1] may want to recommend to a reviewer who exhibit high dependenc y between the memory size and overall scores a camera that has a large memory size.

Given a set of rating data, we seek to determine the reviewer dependency of every reviewer, and the object dependency of every object from the rating data.  X  Reviewer Dependency. A reviewer is said to have high reviewer dependency  X  Object Dependency. For an object, its object dependency represents the ex-
A Naive approach to determine reviewer dependency and object dependency is to simply apply a standard correlation measure, such as Pearson correlation [2]. For instance, in this approach, a reviewer dependency is equated to the correlation between her scores on the fir st criterion and her scores on the sec-ond criterion. However, this approach fails to recognize the relationship between reviewer dependency and object dependency.

Highly correlated rating scores may be due to reviewer dependency or object dependency. For example, Naive may conclude that a reviewer places a premium on a camera X  X  memory size if her memory size scores correlate with her overall scores ( high reviewer dependency ). However, this would be justified only if the other reviewers of the camera do not show similar correlations ( low object de-pendency ). Otherwise, the correct conclusi on should be that memory size is a dominant selling feature of this camera such that any reviewer is bound to judge it mainly by its memory size ( high object dependency ).
 Interrelated Dependency Principle. Therefore, we propose the following principle relating reviewer dependency and object dependency.  X  Areviewerhas high reviewer dependency on a pair of evaluation criteria when  X  An object has high object dependency on a pair of evaluation criteria when
The rest of the paper is organized as follows. In Section 2, we describe our proposed Interrelated Dependency (or ID ) model to determine reviewer dependency and object dependency betw een two given criteria. In Section 3, we present experimental results on a rea l-life dataset. These are followed by an overview of related work in Sectio n 4 and conclusion in Section 5. Before describing the Interrelated Dependency (or ID )model,wefirstreview the notations to be used. We model a rating system as a bipartite network with reviewers and objects forming the two distinct sets of entities. A reviewer r i may assign to an object o j two rating scores a ij ,b ij  X  [0 , 1]basedontwodifferent criteria a and b . Without any loss of generality, the overall score is treated as a criterion. For each r i , we want to determine r i  X  X  reviewer dependency o  X  X  object dependency between a and b , denoted by d o j ( a, b )  X  [0 , 1]. When a and b are implicit, we may further simplify the reviewer dependency and object dependency notations as d r i and d o j respectively.

Our proposed ID model consists of a pair of equations: Equation 1 to deter-mine reviewer dependency d r i and Equation 2 to determine object dependency d o j . To determine d r i with Equation 1, we compute the correlation observed on r  X  X  scores [ F ( a r i , b r i )], and reduce it by the aggregate object dependency of r  X  X  objects [ A gg j (1  X  d o j )]. F is a correlation measure, which will be described low dependency. Thus, a reviewer has high d r i if she assigns highly correlated scores on objects with low d o j . Equation 2 to determine object dependency is symmetrical to Equation 1. An object has high d o j if it exhibits highly correlated scores by reviewers with low d r i . In both Equations 1 and 2, we require d r i ,d o j  X  [0 , 1] as well as F
In this paper, we use the average function for A gg , which is suitable as it takes into account d o j of all the o j  X  X  concerned. Another possible function is median . Examples of unsuitable functions include summation , which unfairly penalizes active reviewers (with many ob jects) and actively-rated objects (with many reviewers); minimum , which unfairly penalizes occasional dependency; and maximum , which unfairly rewards occasional lack of dependency.

The above equations are based on the principle of interrelated dependency be-d r i and d o j are inversely related, with d r i higher when d o j is lower. The reviewer dependency of various reviewers are related by co-rating common objects, and the object dependency of various objects are related by having common review-ers. As reviewers and objects are inter-connected within the rating data, the interrelatedness extends to all reviewers and objects, which means all d r i  X  X  and d o j  X  X  need to be solved simultaneously. Given average as the set of Equation 1 for each r i and Equation 2 for each o j forms a system of linear equations.Suchsystemscanbesolvedusi ng either linear algebra or iterative methods, provided the system is uniquely determined [3].

To determine reviewer dependency, we rely more on objects with low depen-dency. Ideally we should have as many low dependency objects per reviewer as possible. Given that low dependency objects are usually the majority, a reviewer will likely rate more low dependency objects when s/he has rated many objects. Correlation measure is also generally more meaningful for larger  X  X ample X  size.
Correlation Measure. F measures the correlation between two sets of scores. The measure we use in this paper is a slightly modified version of the Pearson product-moment correlation coefficient [2]. For two vectors a and b of N real-valued elements each, the correlation coefficient F ( a , b )  X  [0 , 1] is deter-mined by Equation 3. a [ n ]isthe n th element of a , while  X  a and  X  a are the mean and the standard deviation of a  X  X  elements respectively. Similar notations apply to b .Higher F ( a , b ) value indicates greater correlation between a and b .This measure is also symmetric, i.e., F ( a , b )= F ( b , a ).
 This measure is slightly different from the original Pearson correlation . While Pearson correlation ranges from -1 to 1, Equation 3 ignores the sign, thus confin-ing the range to 0 to 1. Secondly, Pearson correlation is undefined for  X  a =0or  X  b = 0. We choose to define pens when either a or b has uniform elements, in which case the two vectors can reasonably be considered independent. F ( a , b ) is high when elements of a and b rise and fall together, and low when they do not vary or vary independently.
Other possible correlation measures include mutual information [4], or rank correlations [5]. We will study these alternatives as part of future work.
Naive Model as a Special Case. A suitable baseline alternative to the ID model should be the basic model which does not incorporate the interrelated dependency between d r i and d o j . This baseline model, called the Naive model, consists of the pair of Equation 4 to determine d r i and Equation 5 to determine d o j . Naive  X  X  equations compute d r i and d o j respectively using only the correlation measure F ( a , b ). Given average as the A gg function, ID would degenerate into Naive when all d o j  X  X  (or d r i  X  X ) are uniform, in which case ID  X  X  Equation 1 (or Equation 2) practically degenerates into Naive  X  X  Equation 4 (or Equation 5). Our experimental objective is to verify t he effectiveness of the proposed model on a real-life dataset. First, we compare the reviewer dependency and object depen-dency ranks produced by the Naive and ID models. Next, we highlight a specific case example that illustrated how the contribution of interrelated dependency principle that makes the ID model more effective. 3.1 Dataset Our dataset was obtained from Epinions , a product review Web site. We crawled the site over three days (April 20-23, 2007), beginning from a seed page 1 .The collected dataset consisted of a subset of all products, reviewers, and scores.
In this dataset, each product belongs to a category and each category has its specific rating criteria. Each reviewe r may assign one or more scores based on these criteria. These scores, which ranged from 1 to 5, were normalized to a range from 0.2 to 1 by a simple division by 5. We chose to focus on the Beers category, as within the collected dataset, this category was relatively large and had more active reviewers and more actively-rated objects. There are four rating criteria for Beers category: Overall , Weight , Flavor ,and Complexity . In the following experiments, we mainly focus on the ( Overall , Weight )pairing.Wehavealso separately carried out other experiments with ( Overall , Flavor )and( Overall , Complexity ) pairings with similar results. For the ( Overall , Weight ) pairing, the original data size collected is shown in the first row of Table 1.
 We further filtered the dataset to make it more suitable for experiments. First, we considered a reviewer as having rated an object only if the reviewer had assigned both Overall and Weight scores. Second, we ensured that each reviewer must have at least 3 objects and each object must have at least 3 reviewers, by iteratively removing reviewers and obj ects not meeting the condition until the condition was met. The final data size is shown in the second row of Table 1. 3.2 Rank Comparison This part of the experiment compares the ranked lists produced by Naive and ID for the (Overall, Weight) pairing. We construct a ranked list of reviewers and a ranked list of objects for each solu tion. Reviewers and objects are ranked in decreasing order of d r i and d o j respectively. The highest d r i or d o j value is given rank 1. Same values share the same rank.
Figure 1(a) is a scatterplot of reviewer dependency ranks. A point in each scatterplot represents a reviewer, with the x -value being the rank assigned by ID and the y -value the rank assigned by Naive . The reviewer dependency ranks assigned by ID and Naive are positively correlated in general, but are not iden-tical. The positive correlation is expected as both are based on the same F function. However, ID also takes into account object dependency, resulting in a different ranking. Figure 1(a) shows large variances around the diagonal, indi-cating that ID and Naive  X  X  rankings are quite different. For example, there are instances where a significant number of reviewers are tied according to Naive , but are differentiated by ID . In Figure 1(a), 6 reviewers share rank 1 by Naive , but are given ranks ranging from 1 to 22 by ID .

Figure 1(b) is the corresponding scatterplot for object dependency ranks. As this figure shows much resemblance to the earlier Figure 1(a), similar observa-tions as for reviewer dependency ranks can also be made for object dependency ranks. This itself is an important observation, for it highlights the interrelated-ness between reviewer dependency and object dependency. If ID  X  X  object depen-dency ranks were identical to Naive  X  X , so would the reviewer dependency ranks be. Instead, the variance in reviewer dependency ranks results from the variance in object dependency ranks (and vice versa). 3.3 Case Example Object boulevard dry stout  X  X  profile is given in Table 2(a). Naive assigns this object high object dependency of 1.00 (rank 1). Meanwhile, ID assigns it much lower object dependency of 0.33 (rank 34). The rating scores assigned by its three reviewers indeed suggest high object dependency. However, Table 2(a) also shows that boulevard dry stout  X  X  three reviewers ( lafeet , wingdman , impydykiechick )all have high reviewer dependency. For instance, wingdman has high reviewer de-pendency of 0.71 (rank 17). We take a more detailed look at wingdman  X  X  rat-ing scores in Table 2(b). Visual ins pection reveals a trend whereby wingdman  X  X  Overall ( Ove ) score is high (or low) when his Weight ( Wei )scoreishigh(or low). Given the high reviewer dependency of its reviewers, it is expected that boulevard dry stout would show high correlation between its Ove and Wei scores. However, this correlation should be attributed more to its reviewers. Thus, ID justifiably assigns lower object dependency (and ranks) to boulevard dry stout . The work on systematic dependency uses correlation analysis [6] to test the dependency between a given factor and the assigned scores, which affect all reviewers and objects in general. For exam ple, [7] investigates whether reviewers X  scores on peer review submissions are dependent on the reviewers X  also being authors; [8] studies whether students ten d to assign higher scores to instructors who have given them higher grades; [9] show s that venture capitalists X  evaluation of start-up teams is dependent on their similarity to these teams. In contrast, we are concerned with measuring the specific dependency of individual reviewers and objects, as reviewers and objects may ex hibit varying dependency with respect to two given rati ng criteria.

The problem of multiple criteria deci sion making (MCDM) [10] is concerned with making an optimal decision, in view of two or more potentially conflicting criteria. The optimality of a decision nece ssarily depends on the preferences of the decision-maker. For exampl e, a user may wish to select among several hotels one that scores high on both luxury and affordability . This may involve comparing in some way the scores assigned to an object on various criteria in aggregate. This problem is different from our problem in that the optimal decision is not necessarily the object with the highest or lowest dependency. In this paper, we address the problem of determining reviewer dependency and object dependency with respect to two rating criteria. The key principle in our ID model is modeling the interrelatedness between reviewer dependency and object dependency, which is ignored by the Naive model. Experiments on real-life data show that our approach is more effective than Naive . Moreover, although our modeling is based on two given rating criteria, without loss of generality, it can be applied in a pairwise manner for multiple criteria, for instance in learning of the dominant rating criteria affecting the overall scores.
 This work was supported by A*STAR Public Sector R&amp;D, Singapore, Project Number 062 101 0031.

