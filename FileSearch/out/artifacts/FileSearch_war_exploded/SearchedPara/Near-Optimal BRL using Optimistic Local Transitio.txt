 Mauricio Araya-L  X opez, Vincent Thomas, Olivier Buffet maraya LORIA, Campus scientifique, BP 239, 54506 Vand X uvre-ls-Nancy Acting in an unknown environment requires trading off exploration (acting so as to acquire knowledge) and exploitation (acting so as to maximize expected re-turn). Model-Based Bayesian Reinforcement Learning (BRL) algorithms achieve this while maintaining and using a probability distribution over possible models (which requires expert knowledge under the form of a prior).These algorithms typically fall within one of the three following classes (Asmuth et al., 2009). Belief-lookahead approaches try to optimally trade off exploration and exploitation by reformulating RL as the problem of solving a POMDP where the state is a pair  X  = ( s, b ), s being the observed state and b the distribution over the possible models; yet, this problem is intractable, allowing only computationally expensive approximate solutions (Poupart et al., 2006).
 Optimistic approaches propose exploration mecha-nisms that explicitly attempt to reduce the model un-certainty (Brafman &amp; Tennenholtz, 2003; Kolter &amp; Ng, 2009; Sorg et al., 2010; Asmuth et al., 2009) by relying on the principle of  X  X ptimism in the face of uncer-tainty X .
 Undirected approaches, such as -greedy or Boltz-mann exploration strategies (Sutton &amp; Barto, 1998), perform exploration actions independent of the current knowledge about the environment.
 We focus here on optimistic approaches and, as most research in the field and without loss of generality, we consider uncertainty on the transition function, assum-ing a known reward function. For some algorithms, recent work proves that they are either PAC-MDP (Strehl et al., 2009) X  X ith high probability they of-ten act as an optimal policy would do (if the MDP model were known) X  X r PAC-BAMDP (Kolter &amp; Ng, 2009) X  X ith high probability they often act as an ideal belief-lookahead algorithm would do.
 This paper first presents background on model-based BRL in Section 2, and on PAC-MDP and PAC-BAMDP analysis in Section 3. Then, Section 4 intro-duces a novel algorithm, bolt , which, (1) as boss (As-muth et al., 2009), is optimistic about the transition model X  X hich is intuitively appealing since the uncer-tainty is about the model X  X nd, (2) as beb (Kolter &amp; Ng, 2009), is (almost) deterministic X  X hich leads to a better control over this approach. We then prove in Section 5 that bolt is PAC-BAMDP for infinite hori-zons, by generalizing previous results known for beb for finite horizon. Experiments in Section 6 then give some insight as to the practical behavior of these al-gorithms, showing in particular that bolt seems less sensitive to parameter tuning than beb . 2.1. Reinforcement Learning A Markov Decision Process (MDP) (Puterman, 1994) is defined by a tuple  X  X  , A ,T,R  X  where S is a finite set of states , A is a finite set of actions , the transi-tion function T is the probability to transition from state s to state s 0 when some action a is performed: scalar reward obtained during this transition. Rein-forcement Learning (RL) (Sutton &amp; Barto, 1998) is the problem of finding an optimal decision policy X  a mapping  X  : S 7 X  A  X  X hen the model ( T without R in our case) is unknown but while interacting with the system. A typical performance criterion is the ex-pected discounted return
V  X   X  ( s ) = E  X  where  X   X  M is the unknown model and  X   X  [0 , 1] is a discount factor. Under an optimal policy, this state value function verifies the Bellman optimality equation (for all s  X  X  ):
V  X   X  ( s ) = max and computing this optimal value function allows to derive an optimal policy by behaving in a greedy man-ner, i.e., by picking actions in arg max a  X  X  Q  X   X  where the state-action value function Q  X   X  is defined as
Q  X   X  ( s,a ) = X Typical RL algorithms either (i) directly estimate the optimal state-action value function Q  X   X  (model-free RL), or (ii) learn T to compute V  X   X  or Q  X   X  (model-based RL). Yet, in both cases, a major difficulty is to pick actions so as to trade off exploitation of the current knowledge and exploration to acquire more knowledge. 2.2. Model-based Bayesian RL We consider here model-based Bayesian Reinforcement Learning (Strens, 2000), i.e., model-based RL where the knowledge about the model is represented using a probability distribution b over all possible transi-tion models. An initial prior distribution b 0 = Pr (  X  ) has to be specified, which is then updated using Bayes rule. At time t the posterior b t depends on the ini-tial distribution b 0 and the state-action history so far h applied sequentially due to the Markov assumption, i.e., at time t + 1 we only need b t and the triplet ( s t ,a t ,s t +1 ) to compute the new distribution: The distribution b t is known as the belief over the model, and summarizes the information that we have gathered about the model at the current time step. If we consider the belief as part of the state, the re-sulting belief-MDP can be solved optimally in theory. Remarkably, modelling RL problems as belief-MDPs provides a sound way of dealing with the exploration-exploitation dilemma, because both objectives are nat-urally included in the same optimization criterion. The belief-state can thus be written as  X  = ( s, b ), which defines a Bayes-Adaptive MDP (BAMDP) (Duff, 2002), a special kind of belief-MDP where the belief-state is factored into the (visible) system state and the belief over the (hidden) model. Moreover, due to the integration over all possible models in the value function of the BAMDP, the transition function T (  X ,a, X  0 ) is given by where the first probability is 1 if b 0 complies with Eq. (1) and 0 else. The optimal Bayesian policy can then be obtained by computing the optimal Bayesian value function (Duff, 2002; Poupart et al., 2006): V  X  ( s, b ) = max = max with b 0 the posterior after the Bayes update with ( s,a,s 0 ). For the finite horizon case we can use the same reasoning, so that the optimal value can be com-puted in theory for a finite or infinite horizon, by per-forming Bayes updates and computing expectations. However, in practice, computing this value function exactly is intractable due to the large branching factor of the tree expansion.
 Here, we are interested in heuristic approaches follow-ing the optimism in the face of uncertainty principle, which consists in assuming a higher return on the most uncertain transitions. Some of them solve the MDP generated by the expected model (at some stage) with an added exploration reward which favors transitions with lesser known models, as in r-max (Brafman &amp; Tennenholtz, 2003), beb (Kolter &amp; Ng, 2009), or with variance based rewards (Sorg et al., 2010). Another approach, used in boss (Asmuth et al., 2009), is to solve, when the model has changed sufficiently, an op-timistic estimate of the true MDP (obtained by merg-ing multiple sampled models).
 2.3. Flat and Structured Priors The selection of a suitable prior is an important is-sue in BRL algorithms, because it has a direct im-pact on the solution quality and computing time. A naive approach is to consider one independent Dirich-let distribution for each state-action transition, known as Flat-Dirichlet-Multinomial prior (FDM), whose pdf is defined as where D (  X  ;  X  ) are independent Dirichlet distributions. FDMs can be applied to any discrete state-action MDP, but is only appropriate under the strong as-sumption of independence of the state-action pairs in the transition function. However, this prior has been broadly used because of its simplicity for computing the Bayesian update and the expected value. Consider that the vector of parameters  X  are the counters of ob-served transitions, then the expected value of a transi-tion probability is E [ Pr ( s 0 | s,a ) | b ] =  X  s,a ( s the Bayesian update under the evidence of a transition ( s,a,s 0 ), is reduced only to  X  0 s,a ( s 0 ) =  X  s,a ( s Even though FDMs are useful to analyze and bench-mark algorithms, in practice they are inefficient be-cause they do not exploit structured information about the problem. One can for example encode the fact that multiple actions share the same model by factor-ing multiple Dirichlet distributions, or allow the algo-rithm to identify such structures using Dirichlet distri-butions combined using Chinese Restaurant Processes or Indian Buffet Processes (Asmuth et al., 2009). Probably Approximately Correct Learning (PAC) pro-vides a way of analyzing the quality of learning algo-rithms (Valiant, 1984). The general idea is that with high probability 1  X   X  (probably), a machine with a low training error produces a low generalization error bounded by (approximately correct). If the number of steps needed to arrive to this condition is bounded by a polynomial function, then the algorithm is PAC-efficient. 3.1. PAC-MDP Analysis In RL, the PAC-MDP property (Strehl et al., 2009) guarantees that an algorithm generates an -close pol-icy with probability 1  X   X  in all but a polynomial num-ber of steps. An important result is the general PAC-MDP Theorem 10 in Strehl et al. (2009), where three sufficient conditions are presented to comply with the PAC-MDP property. First, the algorithm must use at least near optimistic values with high probability. Also, the algorithm must guarantee with high proba-bility that it is accurate , meaning that, for the known parts of the model, its actual evaluation will be -close to the optimal value function. Finally, the number of non--close steps (also called sample complexity ) must be bounded by a polynomial function.
 In mathematical terms, PAC-MDP algorithms are those for which, with probability 1  X   X  , the evalua-tion of a policy A t , generated by algorithm A at time t over the real underlying model  X  0 , is -close to the optimal policy over the same model in all but a poly-nomial number of steps: Several RL algorithms comply with the PAC-MDP property, differing from one another mainly on the tightness of the sample complexity bound. For ex-ample, r-max and Delayed Q-Learning (Strehl et al., 2009) are some classic RL algorithms for which this property has been proved, whereas BOSS (Asmuth et al., 2009) is a Bayesian RL algorithm which is also PAC-MDP.
 In PAC-MDP analysis the policy produced by an al-gorithm should be close to the optimal policy derived from the real underlying MDP model. This utopic policy (Poupart et al., 2006) cannot be computed, be-cause it is impossible to learn exactly the model with a finite number of samples, but it is possible to reason on the probabilistic error bounds of an approximation to this policy. 3.2. PAC-BAMDP Analysis An alternative to the PAC-MDP approach is to be PAC with respect to the optimal Bayesian policy, rather than using the optimal utopic policy. We will call this PAC-BAMDP analysis , because its aim is to guarantee closeness to the optimal solution of the Bayes-Adaptive MDP. This type of analysis was first introduced in Kolter &amp; Ng (2009), under the name of near-Bayesian property, where it is shown that a modified version of beb is PAC-BAMDP for the undis-counted finite horizon case 1 .
 Let us define how to evaluate a policy in the Bayesian sense: Definition 3.1. The Bayesian evaluation V of a pol-icy  X  is the expected value given a distribution over models b : This definition has already been presented implicitly by Duff (2002) , but it is very important to point out the difference between a normal MDP evaluation over some known MDP, and the Bayesian evaluation 2 . This definition is consistent with Eq. 2, where V ( s, b ) = max = max Let us define the PAC-BAMDP property: Definition 3.2. We say that an algorithm is PAC-BAMDP if, with probability 1  X   X  , the Bayesian evalu-ation of a policy A t generated by algorithm A at time t is -close to the optimal Bayesian policy in all but a polynomial number of steps, where the Bayesian eval-uation is parametrized by the belief b : with  X   X  [0 , 1) and &gt; 0 .
 A major conceptual difference is that in PAC-BAMDP analysis, the objective is to guarantee approximate correctness because the optimal Bayesian policy is hard to compute, while in PAC-MDP analysis, the ap-proximate correctness guarantee is needed because the optimal utopic policy is impossible to find in a finite number of steps. Sec. 2.2 has shown how to theoretically compute the optimal Bayesian value function. This computation being intractable, it is common to use suboptimal X  yet efficient X  X lgorithms. A popular technique is to maintain a posterior over the belief, select one repre-sentative MDP based on the posterior and act accord-ing to its value function. The baseline algorithm in this family is called exploit (Poupart et al., 2006), where the expected model of b is selected at each time step. Therefore, the algorithm has to solve a different MDP of horizon H  X  X n algorithm parameter, not the problem horizon X  at each time step t as can be seen in Fig. 1. We will consider for the analysis that H is the number of iterations i that value iteration performs at each time step t , but in practice convergence can be reached long before the theoretically derived H for the infinite horizon case. beb (Kolter &amp; Ng, 2009) follows the same idea as ex-ploit , but adding an exploration bonus to the reward function. In contrast, boss (Asmuth et al., 2009) does not use the exploit approach, but samples different models from the prior and uses them to construct an optimistic MDP. beb has the advantage of being an almost deterministic algorithm 3 and does not rely on sampling as boss . On the other hand, boss is opti-mistic about the transitions, which is where the un-certainty lies, meanwhile beb is optimistic about the reward function, even though this function is known. 4.1. Bayesian Optimistic Local Transitions In this section, we introduce a novel algorithm called bolt ( Bayesian Optimistic Local Transitions ), which relies on acting, at each time step t , by following the optimal policy for an optimistic variant of the cur-rent expected model. This variant is obtained by, for each state-action pair, optimistically boosting the Bayesian updates before computing the local expected transition model. This is achieved using a new MDP with an augmented action space A = A X S , where the transition model for action  X  = ( a, X  ) in state s is the local expected model derived from b t up-dated with an artificial evidence of transitions  X   X  s,a, X  { ( s,a, X  ) ,..., ( s,a, X  ) } of size  X  (a parameter of the al-gorithm). In other words, we pick both an action a plus the next state  X  we would like to occur with a higher probability. The MDP can be solved as follows: i ( s, b t ) bolt  X  X  value iteration neglects the evolution of b t , but the modified transition function works as an optimistic approximation of the neglected Bayesian evolution. Modifying the transition function seems to be a more natural approach than modifying the reward function as in beb , since the uncertainty we consider in these problems is about the transition function, not about the reward function.
 From a computational point of view, each update in bolt requires |S| times more computations than each update in beb . This implies computation times mul-tiplied by |S| when solving finite horizon problems us-ing dynamic programming, and probably a similar in-crease for value iteration. However, under structured priors, not all the next states  X  must be explored, but only those which are possible transitions.
 Here, the optimism is controlled by the positive param-eter  X   X  X n integer or real-valued parameter depending on the family of distributions X  X nd the behaviour us-ing different parameter values will depend on the used family of distributions. However, for common priors like FDMs, it can be proved that bolt is always op-timistic with respect to the optimal Bayesian value function.
 Lemma 4.1 ( bolt  X  X  Optimism) . Let ( s t , b t ) be the current belief-state from which we apply bolt  X  X  value iteration with an horizon of H and  X  = H . Let also b t be a prior in the FDM family, and let V H ( s t , b t ) be the optimal Bayesian value function. Then, we have [ Proof in (Araya-L  X opez et al., 2012) ] In this section we prove that bolt is PAC-BAMDP in the discounted infinite horizon case, when using a FDM prior. The other algorithm proved to be PAC-BAMDP is beb , but the analysis provided in Kolter &amp; Ng (2009) is only for finite horizon domains with an imposed stopping condition for the Bayes update. Therefore, we include in (Araya-L  X opez et al., 2012) an analysis of beb using the results of this section in order to be able to compare these algorithms theoretically afterwards.
 By Definition 3.2, we must analyze the policy A t generated by bolt at time t , i.e., A t = argmax  X  V bolt , X  H ( s t ), and show that, with high prob-ability and for all but a polynomial number of steps, this policy is -close to the optimal Bayesian policy. Theorem 5.1 ( bolt is PAC-BAMDP) . Let A t denote the policy followed by bolt at time t with  X  = H . Let also s t and b t be the corresponding state and belief at that time. Then, with probability at least 1  X   X  , bolt is -close to the optimal Bayesian policy [ Proof in Section 5.2 ] In the proof we will see that H depends on and  X  . Therefore, the sample complexity bound and the optimism parameter  X  will depend only on the de-sired correctness ( ,  X  ) and the problem characteristics (  X  , |S| , |A| ). 5.1. Mixed Value Function To prove that bolt is PAC-BAMDP we introduce some preliminary concepts and results. First, let us assume for the analysis that we maintain a vector of transition counters  X  , even though the priors may be different from FDMs for the specific lemma presented in this section. As the belief is monitored, at each step we can define a set K = { ( s,a ) |k  X  s,a k  X  m } of known state-action pairs (Kearns &amp; Singh, 1998), i.e., state-action pairs with  X  X nough X  evidence. Also, to analyze an exploit -like algorithm A in general (like exploit , bolt or beb ) we introduce a mixed value function  X  V obtained by performing an exact Bayesian update when a state-action pair is in K , and A  X  X  up-date when not in K . Using these concepts, we can revisit Lemma 5 of Kolter &amp; Ng (2009) for the dis-counted case.
 Lemma 5.2 (Induced Inequality Revisited) . Let V
H ( s t , b t ) be the Bayesian evaluation of a policy  X  , a =  X  ( s, b ) be an action from the policy. We define the mixed value function, where  X  T and  X  R can be dif-ferent from T and R respectively. Here, b 0 is the pos-terior parameter vector after the Bayes update with ( s,a,s 0 ) . Let also A K be the event that a pair ( s,a ) /  X  K is generated for the first time when starting from state s t and following the policy  X  for H steps. Assum-ing normalized rewards for R and a maximum reward  X  R where Pr ( A K ) is the probability of event A K . [ Proof in (Araya-L  X opez et al., 2012) ] 5.2. BOLT is PAC-BAMDP Let  X  V A t H ( s t , b t ) be the evaluation of bolt A t using a mixed value function where R ( s,a,s 0 ) the reward function, and  X  T ( s,a,s  X  T ( s, X ,s 0 , b t ) = E [ Pr ( s 0 | s,a ) | b t , X   X  s,a, X  sition model, where a and  X  are obtained from the policy A t . Note that, even though we apply bolt  X  X  update, we still monitor the belief at each step as pre-sented in Eq. 5. Yet, for  X  T we consider the belief at time t , and not the monitored belief b as in the Bayesian update Lemma 5.3 ( bolt Mixed Bound) . The difference be-tween the optimistic value obtained by bolt and the Bayesian value obtained by the mixed value function under the policy A t generated by bolt with  X  = H is bounded by [ Proof in (Araya-L  X opez et al., 2012) ] Proof of Theorem 5.1. We start by the induced in-equality (Lemma 5.2) with A t the policy generated by bolt at time t , and  X  V a mixed value function using bolt  X  X  update when ( s,a ) /  X  K . As  X  R max = 1, the chain of inequalities is V  X   X  V A t H ( s t , b t )  X   X  V bolt H ( s t , b t )  X   X  V  X  H ( s t , b t )  X   X  V  X  ( s t , b t )  X  where the 3 rd step is due to Lemma 5.3 (accuracy) and the 4 th step to Lemma 4.1 (optimism). To simplify the analysis, let us assume that  X  H (1  X   X  ) = 2 and fix If Pr ( A K ) &gt;  X  2 m = (1  X   X  ) 4 , by the Hoeffding 4 bounds we know that A K occurs no more than O time steps with probability 1  X   X  . By neglecting log-arithms we have the desired theorem. This bound is derived from the fact that, if A K occurs more than |S||A| m times, then all the state-action pairs are known and we will never escape from K anymore. For Pr ( A K )  X   X  2 m , we have that V which verifies the proposed theorem.
 Following Kolter &amp; Ng (2009), optimism can be en--close steps (see (Araya-L  X opez et al., 2012)), which is a worse result than bolt . Nevertheless, the bounds used in the proofs are loose enough to expect the op-timism property to hold for much smaller values of  X  and  X  in practice. To illustrate the characteristics of bolt , we present here experimental results over a number of domains. For all the domains we have tried different parameters for bolt and beb , but also we have used an  X  -greedy variant of exploit . However, for all the presented problems plain exploit (  X  = 0 . 0) outperforms the  X  -greedy variant.
 Please recall that the theoretical values for parame-ters  X  and  X   X  X hat ensure optimism X  X epend on the horizon H of the MDPs solved at each time step. In these experiments, instead of using this horizon we re-lied on asynchronous value iteration, stopping when k V i +1  X  V i k  X  &lt; . For solving these infinite MDPs we used  X  = 0 . 95 and = 0 . 01, but be aware that the performance criterion used here is averaged undis-counted total rewards. 6.1. The Chain Problem In the 5-state chain problem (Strens, 2000; Poupart et al., 2006), every state is connected to state s taking action b and every state s i is connected to the next state s i +1 with action a , except s 5 that is con-nected to itself. At each step, the agent may  X  X lip X  with probability p , performing the opposite action as intended. Staying in s 5 had a reward of 1 . 0 while com-ing back to s 1 had a reward of 0 . 2. All other rewards are 0. The priors used for these problems were Full (FDM), Tied , where the probability p is factored for all transitions, and Semi , where each action has an independent factored probability.
 Table 1 shows that beb outperforms other algorithms with a tuned up  X  value for the FDM prior as already shown by Kolter &amp; Ng (2009). However, for a large value of  X  , this performance decreases dramatically. bolt on the other hand produces results comparable with boss for a tuned parameter, but does not de-crease too much for a large value of  X  . Indeed, this value corresponds to the theoretical bound that en-sures optimism,  X  = H  X  log( (1  X   X  )) / log(  X  )  X  150. Unsurprisingly, the results of beb and bolt with infor-mative priors are not much different than other tech-niques, because the problem degenerates into a easily solvable problem. Nevertheless, bolt achieves good results for a large  X  , in contrast to beb that fails to provide a competitive result for the Semi prior with large  X  .
 This variability in the results depending on the param-eters, rises the question of the sensitivity to parameter tuning. In a RL domain, one usually cannot tune the algorithm parameters for each problem, because the whole model of the problem is unknown. Therefore, a good RL algorithm must perform well for different problems without modifying its parameters.
 Fig. 2 shows how beb and bolt behave for differ-ent parameters using a Full prior. In the low res-olution analysis beb  X  X  performance decays very fast, while bolt also tends to decrease, but maintaining good results. We have also conducted experiments for other values of the slip probability p , the same pattern being amplified when p is near 0, i.e., worse decay for beb and almost constant bolt results, and obtaining almost identical behavior when p is near 0 . 5. In the high resolution results beb goes up and down near 1, while bolt maintains a similar behaviour as in the low resolution experiment.
 6.2. Other Structured Problems An other illustrative example is the Paint/Polish prob-lem where the objective is to deliver several polished and painted objects without a scratch, using several stochastic actions with unknown probabilities. The full description of the problem can be found in Walsh et al. (2009). Here, the possible outcomes of each action are given to the agent, but the probabilities of each outcome are not. We have used a structured prior that encodes this information and the results are summarized in Fig. 3, using both high and low resolu-tion analyses. We have also performed this experiment with an FDM prior, obtaining similar results as for the Chain problem. Unsurprisingly, using a structured prior provides better results than using FDMs. How-ever, the high impact of being overoptimistic shown in Fig. 3, does not apply to FDMs, mainly because the learning phase is much shorter using a structured prior. Again, the decay of beb is much stronger than bolt , but in contrast to the Chain problem, the best parameter of bolt beats the best parameter of beb . The last example is the Marble Maze problem 5 (As-muth et al., 2009) where we have explicitly encoded the 16 possible clusters in the prior, leading to lit-tle exploration requirements. exploit provides very good solutions for this problem, and bolt provides similar results with several different parameters. In contrast, for all the tested  X  parameters, beb behaves much worse than exploit . For example, for the best  X  = 2 . 0 bolt scores  X  0 . 445, while for the best  X  = 0 . 9 beb scores  X  2 . 127, while exploit scores  X  0 . 590. In summary, it is hard to know a priori which algo-rithm will perform better for a specific problem with a specific prior and given certain parameters. However, bolt generalizes well (in theory and in practice) for a larger set of parameters, mainly because the optimism is bounded by the probability laws and not by a free parameter as in beb . We have presented bolt , a novel and simple algo-rithm that uses an optimistic boost to the Bayes up-date, which is thus optimistic about the uncertainty rather than just in the face of uncertainty. We showed that bolt is strictly optimistic for certain  X  parame-ters, and used this result to prove that it is also PAC-BAMDP. The sample complexity bounds for bolt are tighter than for beb . Experiments show that bolt is more efficient than beb when using the theoreti-cally derived parameters in the Chain problem, and in general that bolt seems more robust to parame-ter tuning. Future work includes using a dynamic  X  bonus for bolt , what should be particularly appropri-ate with finite horizons, and exploring general proofs to guarantee the PAC-BAMDP property for a broader family of priors than FDMs. Araya-L  X opez, M., Thomas, V., and Buffet, O. Near-optimal BRL using optimistic local transitions (ex-tended version). Technical Report 7965, INRIA, May 2012.
 Asmuth, J., Li, L., Littman, M.L., Nouri, A., and
Wingate, D. A Bayesian sampling approach to ex-ploration in reinforcement learning. In Proc. of UAI , 2009.
 Brafman, R.I. and Tennenholtz, M. R-max -a gen-eral polynomial time algorithm for near-optimal re-inforcement learning. JMLR , 3:213 X 231, 2003.
 Duff, M. Optimal learning: Computational procedures for Bayes-adaptive Markov decision processes . PhD thesis, University of Massachusetts Amherst, 2002. Kearns, M. and Singh, S. Near-optimal reinforcement learning in polynomial time. In Machine Learning , pp. 260 X 268, 1998.
 Kolter, J. and Ng, A. Near-Bayesian exploration in polynomial time. In Proc. of ICML , 2009.
 Poupart, P., Vlassis, N., Hoey, J., and Regan, K. An analytic solution to discrete Bayesian reinforcement learning. In Proc. of ICML , 2006.
 Puterman, M. Markov Decision Processes: Dis-crete Stochastic Dynamic Programming . Wiley-Interscience, 1994.
 Sorg, J., Singh, S., and Lewis, R. Variance-based rewards for approximate Bayesian reinforcement learning. In Proc. of UAI , 2010.
 Strehl, A.L., Li, L., and Littman, M.L. Reinforcement learning in finite MDPs: PAC analysis. JMLR , 10: 2413 X 2444, December 2009.
 Strens, Malcolm J. A. A Bayesian framework for rein-forcement learning. In Proc. of ICML , 2000.
 Sutton, R. and Barto, A. Reinforcement Learning: An Introduction . MIT Press, 1998.
 Szita, Istvn and Szepesvri, Csaba. Model-based re-inforcement learning with nearly tight exploration complexity bounds. In Proc. of ICML , 2010.
 Valiant, L. G. A theory of the learnable. In Proc. of STOC . ACM, 1984.
 Walsh, T.J., Szita, I., Diuk, C., and Littman, M.L.
Exploring compact reinforcement-learning represen-
