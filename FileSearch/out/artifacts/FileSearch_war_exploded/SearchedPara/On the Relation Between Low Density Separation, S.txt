 keywords: Clustering, Semi-Supervised Learning as shown in the left panel.
 Low Density Separation assumption. We will show that given a class boundary, this measure can be computed from spectral clustering.
 along the boundary considerably lower weighted length than the boundary in the right panel of our Fig. 1. smooth hypersurface we define the weighted volume of the cut to be the integral is taken over the surface of the boundary.
 graph-based methods. 2.1 Spectral Clustering size .
 different algorithm, which attempts to approximate the smallest (balanced) cut. connect it to the size of the resulting cut. 2.2 Graph-based semi-supervised learning (e.g.,[15, 1]).
 thread is much smaller than the radii of the clusters. 2.3 Convergence of Manifold Laplacians the proof of our Theorem 3 are significantly different. Let p be a probability density function on a domain M  X  R d .
 point, intersecting the manifold at only one point. It bounds the curvature of the manifold. Definition 1 Let K t ( x, y ) be the heat kernel in R d given by Let M t := K t ( x, x ) = 1 by Let W be the weight matrix. Let X 1 = X  X  S 1 and X 2 = X  X  S 2 be the data point which land in S 1 and S 2 Let f = ( f 1 , . . . , f N ) be the indicator vector for X 1 : There are two quantities of interest: Our main Theorem shows that after an appropriate scaling, the empirical cut size converges to the volume of the boundary.
 zero such that t N &gt; 1  X  and certain generic invariants of p and S ) such that with probability 1  X   X  , (  X  N &gt; N 0 ) , This theorem is proved by first relating the empirical quantity we need the following notation.
 Definition 2 Let Let and tend to 0 . Theorem 2 Let 0 &lt;  X  &lt; 1 . Let u := 1 / p and S such that with probability greater than 1  X  exp (  X  C 1 N  X  ) By letting N  X   X  and t N  X  0 at suitable rates and putting together theorems 2 and 3, we obtain the following theorem: Theorem 4 Let the number of random data points N  X  X  X  , and t N  X  0 , at rates so that u := 1 / with probability greater than 1  X  exp (  X  C 1 ( N  X  )) , P Theorem 4 is a direct consequence of Theorem 2 and Theorem 3.
 Theorem 2: We prove theorem 2 using a generalization of McDiarmid X  X  inequality from [7, 8]. McDiarmid X  X  inequality asserts convergence. Therefore we need to prove that the mean E [ If, instead, we had in the denominator of the right side using the linearity of Expectation, Using Chernoff bounds, we can show that with high probability, for all x  X  X , follows. Since the exact details require fairly technical calculations, we leave them to the Journal version. Theorem 3: The quantity the boundary between these two parts, which in our setting is above and below in terms of the weighted volume and condition number of the boundary. These bounds are obtained in S 2 that is within  X  of S 1 , as in Figure 4.
 of S nearest to P , Z We now indicate how a lower bound is obtained for
A key observation is that for R = near P contribute to the the integral Z halfspace whose boundary is at a distance  X   X  R 2 2  X  from the center as in figure 4. diffusing to P from B 2 in the right panel. received by P from the halfspace H 2 in the right panel. An upper bound for Z is obtained along similar lines.
 The details of this proof will be presented in the Journal version. uniformly over balanced partitions.
 [1] M. Belkin and P. Niyogi (2004). X  X emi-supervised Learning on Riemannian Manifolds. X  In Machine Learning [2] M. Belkin and P. Niyogi.  X  X oward a theoretical foundation for Laplacian-based manifold methods. X  COLT 2005. [3] A. Blum and S. Chawla,  X  X earning from labeled and unlabeled data using graph mincuts X , ICML 2001. [4] O.Chapelle, J. Weston, B. Scholkopf,  X  X luster kernels for semi-supervised learning X , NIPS 2002. [5] O. Chapelle and A. Zien,  X  X emi-supervised Classification by Low Density Separation X , AISTATS 2005. [6] Chris Ding, Spectral Clustering, ICML 2004 Tutorial. [7] Samuel Kutin, Partha Niyogi,  X  X lmost-everywhere Algorithmic Stability and Generalization Error. X , UAI 2002, [8] S. Kutin, TR-2002-04,  X  X xtensions to McDiarmid X  X  inequality when differences are bounded with high proba-[9] S. Lafon, Diffusion Maps and Geodesic Harmonics , Ph. D. Thesis, Yale University, 2004. [10] M. Hein, J.-Y. Audibert, U. von Luxburg, From Graphs to Manifolds  X  Weak and Strong Pointwise Consistency [12] M. Meila and J. Shi. A Random Walks View of Spectral Segmentation , NIPS 2001. [14] J. Shi and J. Malik.  X  X ormalized cuts and image segmentation. X 
