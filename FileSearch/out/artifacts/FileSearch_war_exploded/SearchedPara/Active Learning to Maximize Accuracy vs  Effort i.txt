 We consider an interactive information retrieval task in which the user is interested in finding several to many relevant documents with minimal effort. Given an initial document ranking, user interaction with the system produces relevance feedback (RF) which the system then uses to revise the rank-ing. This interactive process repeats until the user termi-nates the search. To maximize accuracy relative to user effort, we propose an active learning strategy. At each it-eration, the document whose relevance is maximally uncer-tain to the system is slotted high into the ranking in order to obtain user feedback for it. Simulated feedback on the Robust04 TREC collection shows our active learning ap-proach dominates several standard RF baselines relative to the amount of feedback provided by the user. Evaluation on Robust04 under noisy feedback and on LETOR collections further demonstrate the effectiveness of active learning, as well as value of negative feedback in this task scenario. H.3.3 [ Information Search and Retrieval ]: Relevance Feedback; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Performance Relevance Feedback, Active learning, Personalized Search
This paper presents an interactive information retrieval (IR) task [14] based on iterative relevance feedback (RF) [20, 4]. Given an initial document ranking, the user interacts with the system to explicitly or implicitly provide the system with labeled feedback documents. In standard RF fashion, the system then utilizes this feedback to generate an im-proved ranking. However, our interaction model involves
Figure 1: Task scenario and system architecture. an iterative back-and-forth between user and system, pro-viding the user a natural way to provide feedback and the system with an opportunity to request targeted feedback to improve ranking. User behavior is simulated via a simple model of top-to-bottom search result browsing (  X  2). Active learning [24, 13] is used to maximize system accuracy while minimizing the amount of feedback required (  X  4). Figure 1 depicts our overall task scenario and system architecture.
Brandt et al. [3] recently described an interactive IR cre-ating a dynamic ranking tree for user interaction. Ranking is based on recommendations from the history of other users. Radlinski and Joachims [17] also explore use of active learn-ing to quickly learn the document ranking function, focusing on estimating the general relevance measure (and ranking) of documents based on clickthrough data from query logs. Our work is more user-specific in personalizing the ranking based for the current user X  X  interactions.

We investigate two active learning methods for selecting the next document to slot high for feedback. The Simple Margin method [24] picks the document lying closest to the decision surface whose relevance is maximally uncertain. We also propose an enhancement, Local Structure , which cap-tures the idea that useful examples to label should also be far from already labeled examples and near to other unlabeled examples. We use Laplacian Score feature selection [10] to prune the vocabulary for more tractable learning (  X  5).
Our iterative RF task scenario presents several challenges to standard evaluation methodology typically used for ad hoc retrieval and single-iteration RF (  X  6). To address these challenges, we propose a paired approach of KeepAll and TakeOut evaluation which differ in how feedback documents are treated with regard to evaluation. Both strategies have advantages and disadvantges. In tandem, they provide a useful way to compare relative effectiveness of methods while not having to exclude the full union of feedback documents as typical with residual evaluation, something which can be difficult to apply when relevant documents are limited.
Empirical evaluation is conducted on the Robust04 TREC collection, as well as the LETOR 3.0 TD2003 and TD2004 collections [15] (  X  7). While we primarily assume relevance feedback provided by the user is correct, as in traditional RF settings, we also consider a noisy feedback setting in which the user sometimes provides erroneous feedback. This is simulated through simple false positive and negative rate Bernoulli parameters. Active learning methods are com-pared to standard RF baselines of Rocchio [20] and model-based feedback [28] (  X  3). We also evaluate Rocchio using positive-only feedback, and results show negative feedback is quite valuable in this task scenario, something rarely seen in prior work [26]. Overall, learning curves and average per-formance show active learning methods dominate baseline techniques, maximizing ranking accuracy for users relative to the amount of user feedback provided.

A simple prototype version of our system is available on-learning. The interface presents left and right views of search results. The original ranking remains fixed in the left view, while the right view is continually re-ordered based on user feedback. Clicking search results in either views will open the clicked page in a new tab, as well as re-order search results in the view on the right.

Our paper is organized as follows. Our task scenario and user behavior model is introduced in  X  2.  X  3 defines the base-line RF methods we employ. Active learning methods are presented in  X  4.  X  5 describes feature selection for dimension-ality reduction. Evaluation methodology for iterative RF is discussed in  X  6, and our evaluation of methods is presented in  X  7. We conclude with discussion of future work in  X  8.
Imagine a user who is interested in finding several to many relevant documents (with minimal effort). Given an initial document ranking, user interaction with the system pro-duces relevance feedback (explicitly or implicitly) which the system then utilizes to improve the ranking. This interac-tive process repeats iteratively until the user terminates the search. In practice, we expect different information needs, task situations, and users will drive different types of thresh-olds for stopping. These might include a limited amount of effort the user is willing to invest, a target number of relevant documents to be found, or perhaps a target system accuracy to achieve (e.g. for later use searching the collection as new documents are added to it). Iteration also stops whenever the set of relevant documents available for feedback is ex-hausted. In all cases, the system goal is to maximize ranking http://www.cs.utexas.edu/~atian/search http://www.bing.com accuracy for any amount of relevance feedback provided. In other words, the system should maximize the learning curve .
We generate relevance feedback from a simple model of user behavior. Given a document ranking, the user browses the document ranking from top-to-bottom and clicks on the first relevant document encountered. This interaction gener-ates relevance feedback consisting of both one relevant docu-ment (the one clicked) and a set of zero or more non-relevant documents (those ranked above the relevant document). We do not explicitly model the user X  X  stopping criterion for ter-minating the search (evaluation in  X  7 will measure system accuracy across varying amounts of user feedback).
Note that the user model just described assumes user feed-back is always correct, similar to a traditional RF setting with known relevant and non-relevant documents [4]. We also consider a second, noisy model of user feedback (  X  7.3). Such noise might arise in practice with either explicit or im-plicit feedback due to the user X  X  misperceptions of relevance prior to clicking or by simple clicking mistakes. This second user model introduces two Bernoulli parameters, f p and f which control the user X  X  false positive and negative rates, respectively. As the user browses each non-relevant docu-ment, with probability f p he will mistakenly click on it to indicate it as relevant. Similarly, whenever the user browses a relevant document, with probability f n he will fail to click on it and instead continue scanning down the results list, providing incorrect negative feedback.
We evaluate three baseline RF methods for this iterative feedback task: (1) Rocchio feedback [20], (2) Rocchio with positive feedback only, and (3) model-based feedback (lan-guage modeling paradigm) [28]. Since prior work has rarely observed benefits from negative feedback [26], we were par-ticularly interested in comparing the relative performance of (1) and (2) for this task scenario.

Let D denote the document collection, D U the set of un-labeled documents, D F documents with feedback, D F + pos-itive feedback documents, and D F  X  the set of negative feed-back documents. Let q and d denote the query and doc-ument vectors. Superscript k represents the k th iteration. S ( d ) represents d  X  X  final score.

Rocchio [20] performs query expansion based on both pos-itive and negative user feedback. Let q k =  X  q 0 +(1  X   X  ) q denote linear interpolation between original query q 0 and feedback vector q k f . Using term frequency representation for q and d vectors, we compute q k f by:
The score of d in iteration k is then computed by cosine similarity between the expanded query and document d  X  X  vector: S k ( d ) = cos ( q k , d ) = q k  X  d .

Model-based feedback [28] incorporates only positive feed-back into the language model. Let  X  Q ,  X  D ,  X  F , and  X  resent the model for query Q , collection D , positive feedback documents F , and given document d . Similar to Rocchio, let  X  k Q =  X   X  0 Q +(1  X   X  ) X  k F denote the updated query model computed by linear interpolation of the original query model and the feedback model. To limit the noise of the feedback documents, feedback documents are assumed to be gener-ated based on linear combination of feedback model and the collection model (2). Let c ( w, d ) denote the frequency of word w in document d . The feedback model is estimated by maximum likelihood (ML) using the EM algorithm where log p ( D k F + ,  X  k F ) is computed by: We estimate the document model by  X  d = (1  X   X  ) d | d | +  X   X  where smoothing parameter  X  mixes d  X  X  ML estimate with the collection model  X  D . Document relevance to the query is then measured by KL divergence: S k ( d ) =  X  X  ( X  k Q ||  X 
To maximize ranking accuracy with respect to the amount of user effort (i.e. relevance feedback) invested in training the system, we propose an active learning strategy for uti-lizing feedback. At the core of this active learning is super-vised classification: learning to distinguish between relevant and non-relevant classes given training data. Documents are ranked in order of decreasing estimated relevance [19]. We describe two active learning approaches, Simple Margin and Local Structure , which we later evaluate (  X  7).
We adopt a support vector machine (SVM) [24] approach to active learning. Training a linear SVM consists of solving a quadratic optimization subject to linear inequality con-straints. Optimization maximizes the margin between posi-tive and negative samples so the classifier generalizes well to unseen data. The version (i.e. parameter) space is the dual space of feature space. A point in version space corresponds to a hyperplane in the feature space and vice-versa. SVM optimization in the feature space corresponds to finding the largest sphere in the version space, subject to constraints of the hyperplanes for the current feature points.

An active learning algorithm selects which unlabeled ex-ample to be labeled next in order to maximize learning rela-tive to user effort required.The key problem of active learn-ing is estimating the expected benefits of labeling different data points. Tong and Koller [24] find that the best exam-ple to label next halves the version space. Unfortunately, explicitly computing the size of version space is computa-tionally infeasible. Consequently, Tong and Koller identify useful approximation criteria for example selection. Their Simple Margin criterion requires minimal computational costs so can be practically applied in real-time. It predicts utility of labeling all unlabeled data based on current SVM model, then chooses the example lying nearest to the clas-sification hyperplane via M ( d ,  X ), where d is the document vector in TF-IDF (normalized tf logdf ),  X  parameterizes the SVM model trained with all labeled data, and M ( d ,  X ) is the positive distance of d from the model X  X  hyperplane (cap-turing uncertainty of d  X  X  classification under the model).
Because this criterion does not require additional training, its computational cost is low. Its effectiveness, however, de-pends on the shape of the version space. If the version space is far from spherical, this criterion may not perform well. The middle figure in Figure 2 shows such an example failure case of this criterion. Tong and Koller X  X  approximation cri-teria select which example to label next entirely based upon the learned SVM model [24]. In other words, the structure information of the whole data set is missed in those criteria.
However, often it is the case (as it is here) that we actu-ally know about the entire dataset, including the labeled and Figure 2: An example illustrating the difference between Simple Margin and Local Structure crite-ria.  X + X ,  X - X , and circle represent positive, nega-tive and unlabeled examples. The left figure shows the predicted classification prior to active learning. The black solid line denotes the current hyperplane, while the green dashed line shows the optimal hyper-plane. The middle and right figures demonstrate the behavior of Simple Margin and Local Structure cri-teria, respectively. Red points represent examples selected by active learning for labeling, and the red dashed line shows the updated hyperplane after this labeling. While Simple Margin selects the example closest to the current hyperplane, Local Structure selects an example both close to the hyperplane and away from already labeled examples. unlabeled. Such additional information can be usefully ex-ploited by the learner. For example, work in semi-supervised learning often exploits the imbalance between plentiful unla-beled data and scarce labeled data by utilizing more accurate estimates of variance on the unlabeled data to improve clas-sifier accuracy. In this case, we propose another selection criterion which considers both the performance of SVM and the structure information of the whole data set in tandem.
Intuitively, a useful unlabeled example to label should em-body several characteristics. First, the classifier should be uncertain of its label prediction for the example given cur-rent model parameters; this is captured by the Simple Mar-gin criterion. A second property, however, is that example should not lie close to already labeled data since close exam-ples are likely to have the same label (e.g. standard nearest neighbor approaches to classification or utilizing unlabeled data in semi-supervised learning). Such close examples are unlikely to improve the performance of the classifier much more than a single, more useful example (e.g. see Figure 2). As a third characteristic, the data should have nearby unla-beled neighbors so knowing the example X  X  label is likely to help us better label other nearby examples.

To capture the three properties above, we propose a new active learning selection criterion we refer to as Local Struc-ture . Let SL ( d ) = max d f  X  D F cos ( d , d f ) find the maximum cosine similarity between d and all labeled d f  X  D F , i.e. how close d is to another already labeled document. Let d m de-note d  X  X  m th closest neighboring document, where m is a parameter. SN ( d ) = cos ( d , d m ) measures cosine similarity of d and d m to model d  X  X  local neighborhood. The intu-ition is that smaller values of SN indicate a greater number of nearby documents that might benefit by labeling d . We minimize a linear combination of SL , SN and M ( d ,  X ) to select an example d 0 to label capturing the three properties: d 0 = arg min
Figure 2 visually compares the differing behaviors of ac-tive learning based on Simple Margin and Local Structure criteria. In this figure, Simple Margin criterion is seen to select the point nearest to the hyperplane without consider-ing structure information. The chosen example lies near to an already labeled example, resulting in minimal change to the classification hyperplane and thereby minimal benefit to learning. In contrast, Local Structure criterion chooses the point close to the hyperplane and away from labeled data. In doing so, it thereby avoids the failure case of Simple Margin.
In our iterative RF scenario, the document selected for feedback is  X  X lotted X  into the top position in the ranking to guarantee the user provides feedback for it. While this en-sures the system obtains the most useful feedback, this will often hurt ranking accuracy (since the maximally uncertain document will be non-relevant roughly half of the time). We discuss this issue further in regard to evaluation methodol-ogy (  X  6.2) and our future work (  X  8).
High dimensionality represents a general obstacle to ef-fective machine learning. Natural language learning tasks exhibit an especially visible example of this due to fast vo-cabulary growth from many words used infrequently. Var-ious prior work in IR has investigated dimensionality re-duction methods to mitigate issues of term mismatch be-tween semantically related terms (e.g. LSI [8], pLSI [12], and LDA [27]). Learning to rank methods are also impacted if we use term-level features rather than higher-level aggregate features as used in LETOR [15, 1]. As such, we employ di-mensionality reduction methods for more tractable learning.
In particular, we apply a dimensionality reduction tech-nique known as Laplacian score (LS) [10]. This unsupervised technique is attractive due to the sparsity of labeled data available in our task scenario (i.e. relatively few feedback documents in comparison to the size of the feature space). LS is a filter-based method for feature selection, where we refer to the different dimensions of a feature vector as the feature set. Given m input features, LS scores each feature independently and selects the top N features, where N is a parameter. LS is based on Laplacian Eigenmaps and Lo-cality Preserving Projection [2, 11]. The basic assumption of LS is that if two data points are close, they are likely to have the same class label. An effective feature set, there-fore, should preserve such proximity for examples belonging to the same class. This is the same property exploited by our Local Structure method for example selection described earlier (  X  4). LS for the r th feature ( LS r ) is computed ac-cording to LS r = 1  X  feature for all examples, f ri is the r th feature for example i ,  X  is the variance, and S is a sparse, inverse distance matrix over all examples in which only examples in close proximity have non-zero values. S thereby stores the local structure of the whole example space based on all features.

Let x 1 : k i denote the k nearest neighbors to feature vector x , where k is a parameter. Non-zero entries in S are given feature vectors for examples i and j along all dimensions.
Evaluating iterative RF presents several different chal-lenges from typical Cranfield-style evaluation of ad hoc and single-iteration RF. Acknowledging these challenges directly helps to ensure we establish a solid evaluation framework for measuring accuracy in this setting, comparing methods, and understanding limitations of evaluation methodology used.
Feedback document selection vs. use . Relevance feedback methods can differ in how documents are selected for feedback as well as how those documents are utilized. Re-cent work in the TREC RF track has sought to isolate this confounding effect to better understand what makes one RF method more effective than another [4]. Our iterative feed-back scenario makes it difficult to avoid this confound since each method produces a unique ranking at each iteration which the user then browses to provide feedback (  X  2).
Scarcity of relevant documents . Multiple RF meth-ods are typically compared via residual evaluation in which feedback documents used by any of the methods being com-pared are excluded from evaluation (i.e. removed from both  X  X old standard X  judgments and system rankings). This yields a single, common document collection for fair comparison of methods. With iterative feedback, ideally the residual col-lection would be determined by removing all feedback docu-ments used by any method at any iteration considered. Once the residual collection was known, we could X  X o back in time X  to evaluate accuracy of RF methods at earlier iterations.
The problem with this ideal strategy is scarcity of rele-vant documents in typical test collections available: using as-sessed documents for feedback often leaves relatively few for evaluation (especially relevant documents). Consequently, we sought an evaluation methodology that would allow us to meaningfully evaluate relevance feedback without the full exclusion required by traditional residual evaluation.
Another challenge due to scarcity of relevant documents lies in computing average system accuracy across topics. Re-call our task scenario involves the user selecting a relevant document for feedback at each iteration (  X  2). Once the set of relevant documents is exhausted for a given topic, iterative RF terminates. However, if RF for a given topic terminates at iteration 8, how do we compute average system accu-racy across topics at iteration 9? Do we (a) omit the query from the average or (b) include its accuracy as of iteration 8? Choice (a) would yield increasing error bars across iterations as progressively fewer topics contribute to the computed av-erage, while choice (b) would dampen the learning effects (positive or negative) we are trying to observe. We adopt (a) with awareness of needing to monitor result instability at late iterations of feedback.
To address the challenge of scarce relevant documents, we evaluate accuracy of methods in two distinct ways, which we denote as KeepAll and TakeOut . Both have strengths and weaknesses, and our general thought is that evaluating methods under both strategies in tandem provides a bal-anced view for comparing relative accuracy of methods.
KeepAll evaluation compares methods without removal of feedback documents. This means that even after a given document X  X  relevance is made known to the system via user feedback, we continue to include that document in the eval-uation set. However, we also follow prior work X  X  strategy [9] of imposing the restriction that systems cannot memorize the list feedback documents, but instead must limit learn-ing to parameter updates. Nonetheless ranking documents that were previously viewed still results in artificially inflated ranking accuracies observed across systems. On the positive side, KeepAll evaluation does preserve the expected trend of accuracies rising across systems with increasing feedback.
A possible problem with KeepAll in our task scenario is that it could lead to the same document being selected re-peatedly for RF across iterations. While such repeated se-lection is actually a useful property in some learning models such as Boosting [21], it would be odd to the user to keep seeing the same feedback documents over-and-over again in the results list, let alone having to tell the system multiple times that a given document is relevant.

To address this, we distinguish two separate rankings: a feedback ranking and an evaluation ranking . The evaluation ranking is generated over all documents and is used to eval-uate system accuracy. The feedback ranking is presented to the user for feedback and filters out all previous feedback documents from the evaluation ranking. This is similar to a search interface that lets users  X  X ookmark X  known relevant webpages in a sidebar and thereafter excludes them from the ranking. Since the user is never presented the same relevant document twice for feedback, RF iteration terminates for each topic once all of its relevant documents are exhausted.
TakeOut evaluation, unlike KeepAll , removes feedback documents from evaluation. We do not perform full residual evaluation , but rather evaluate each method on a unique residual document subset at each iteration based on feed-back documents used up to the current iteration. This means different methods will typically be evaluated on dif-ferent document subsets in the same iteration, though they will still have the same number of relative documents un-der the correct user feedback. Moreover, since progressively fewer relevant documents remain to be found as iteration progresses, topics become increasingly difficult. Relevant documents that are easier to find are taken out in earlier it-erations, so remaining relevant documents in the pool tend to become progressively more difficult to find individually as well as fewer in number overall. Consequently, we observe a counter-intuitive effect whereby the general trend across sys-tems is for ranking accuracy to decrease rather than increase as iteration proceeds. Nonetheless, we would expect in com-parative evaluation of methods to see a stronger RF method to decrease in accuracy more slowly than other methods.
A final challenge with both KeepAll and TakeOut evalua-tion is specific to active learning (  X  4). Recall active learning selects a document whose relevance is maximally uncertain and slots it at the top rank. A practical consequence of eval-uating ranking accuracy is that early precision metrics like MRR become entirely dominated by this single document, and so fail to provide any meaningful measure of the actual ranking. To address this artifact, we focus evaluation of ac-tive learning on the point at which the user X  X  effort threshold is exceeded. The user then terminates the training phrase and only wants to use the system thereafter. This disables active learning and so eliminates these slotting effects. To model this, we evaluate system accuracy on the evaluation ranking (without slotting) rather than the feedback ranking (with slotting). User effort is still measured off the feedback ranking, ensuring that slotting any non-relevant documents increases measured user effort since the user must browse more results before finding a relevant document.
Recall our interactive search task involves iterative back-and-forth between system ranking and user feedback (  X  2). The system goal in this scenario is to maximize ranking accu-racy for any amount of relevance feedback provided. Conse-quently, our evaluation measures ranking accuracy achieved under varying amounts of user feedback in order to compute the learning curve of each relevance feedback method. Mea-suring this curve enables us to compare the relative strength of each method across a range of possible stopping points which might be encountered in practice and suggest different methods being better suited to different use cases. Our main results assume user feedback to the system is always correct, similar to a traditional RF setting with known relevant and non-relevant documents (  X  7.2). We also present results for a noisy feedback scenario (  X  7.3) and an alternative learning scenario on LETOR TD2003 and TD2004 collections (  X  7.4). Experiments in  X  7.2 and  X  7.3 are reported on the TREC 3 Robust04 document collection of newswire articles. Topics 301-450 were used for evaluation during system development and tuning, with topics 601-700 held out for final testing. System queries are taken from title field of each topic. An initial ranking of 200 documents for each query is generated using Indri [23]. This initial ranking provides both a base-line measure of system accuracy as well as the document pool subsequently re-ranked by each RF method. To reduce vocabulary growth from rare words, we prune out any term which does not occur at least twice in some document and at least four times in the entire collection. This reduces the vocabulary size of the query-specific document pools from about 10,000-20,000 terms to about 3,000-4,000 terms.
We compare the relative performance of several RF base-line methods in this task setting of iterative feedback (  X  3): (1) Rocchio feedback [20] (denoted by  X  X occhio X ), (2) Roc-chio with positive feedback only (denoted by  X  X occhioPos X ), and (3) model-based feedback [28] (denoted by X  X anguage X ). Since prior work has only observed modest benefits from negative feedback, we were particularly interested in com-paring the relative performance of (1) and (2) in our task setting. Additional details of baseline methods are discussed in  X  3. For all baselines, parameters are tuned on the devel-opment set to maximize the accuracy of each method: Roc-chio (  X  = 0 . 05,  X  = 0 . 5), RocchioPos (  X  = 0 . 05,  X  = 1), and model-based (Language) (  X  = 0 . 05,  X  = 0 . 8, and  X  = 0 . 3).
For active learning, we evaluate both Simple Margin and Local Structure criteria (  X  4). Simple Margin has no parameters, while with Local Structure we use m = 10 neighbors and  X  = 0 . 5. LS feature selection (  X  5) further reduces vocabulary size for active learning using k = 15 neighbors and N = 2500 features (a modest further reduc-tion from the already reduced vocabulary size). Feature se-lection is not used with baseline methods. While baselines benefit from interpolating the original query with feedback (an anchoring effect to prevent query drift), active learn-ing methods perform ranking based on feedback documents only. Exploring anchoring between the query model and active learning via rank fusion [6] remains for future work.
Ranking accuracy is measured by three metrics: mean-average precision (MAP), top-10 precision (P@10), and mean ActiveMargin 0.3952  X  0.3116  X  0.6251  X  0.7427  X  0.7399  X  ActiveStructure 0.3978  X  0.3135  X  0.6248  X  0.7475  X  * 0.7525  X  reciprocal rank (MRR). User effort is measured by the total number of clicks, which equals the number of positive docu-ments with correct feedback and upper-bounds the number of positive documents with noisy feedback. We also tracked the total number of documents viewed by the user in brows-ing search results but saw the number of clicks and views were very correlated and largely yielded the same learning curve plots. Consequently, we omit these results to sim-plify presentation. Another related metric we did not mea-sure is Cooper X  X  expected search length: the number of non-relevant documents a user needs to skip over to find some number of relevant documents desired [5]. Figure 3 presents empirical results for both TakeOut and KeepAll evaluation conditions (see  X  6.2) assuming relevance feedback provided by the user is always correct. We also compute the average accuracy of each method across iter-ations and present this in Table 1, along with results of statistical significance tests. Statistical significance of im-provements is determined using a two-sided paired t-test [22] at the 5% significance level over all queries. The proposed active learning methods (Simple Margin and Local Struc-ture) are seen to by-and-large dominate baselines across it-erations, metrics, and for both TakeOut and KeepAll con-ditions. We omit the plot of MRR due to space limitations, though we do report it for TakeOut evaluation in Table 1. MRR is not meaningful for KeepAll since the labeled docu-ments are essentially always ranked at the top.

As mentioned earlier (  X  6), an experimental challenge is that while we expect system accuracy to improve with in-creasing feedback, we have a confounding factor that tends to work against this: as more relevant documents are  X  X sed up X  for feedback, fewer remain to be found in subsequent iterations, effectively increasing query difficult. Moreover, documents that are  X  X asier X  to find are taken out in ear-lier iterations, so remaining relevant documents in the pool become progressively more difficult to find individually as well as fewer in number overall. This effect is particularly pronounced in the TakeOut condition in which system ac-curacies tend to decrease due to fewer relevant documents remaining in the residual collection used for evaluation.
With the KeepAll condition, all documents are reranked and included in evaluation at each iteration. As noted earlier (see  X  6.2), this system does not  X  X emember X  earlier feedback documents and must rerank them like any other document based on learned parameters. However, it is still the case that the system has learned from having seen some of the relevant documents that are now being evaluated on, which tends to introduce the opposite evaluation effect that system accuracy tends to be inflated across methods with increas-ing feedback. Allowing for this effect, Figure 3(a) shows that under KeepAll evaluation, all feedback methods are seen to exhibit the expected (and more intuitive) behavior of achiev-ing increased accuracy with greater feedback. While Take-Out and KeepAll evaluation conditions each have their own respective limitations, the key point to observe with regard to our evaluation of active learning is that it dominates the other baselines across both methods of evaluation.
Another interesting point to note is the observed value of negative feedback in our results vs. what has traditionally been observed in ad hoc search or single-iteration relevance feedback. The traditional wisdom has been that negative documents have relatively little in common, so learning what one looks like does not help the system avoid ranking another one highly. In our experiments, three methods use both positive and negative feedback: Rocchio, Simple Margin, and Local Structure. RocchioPos and Language only use positive feedback. Of particular note, Rocchio outperforms RocchioPos on nearly all metrics in all iterations. Both ac-tive learning methods dominate all baselines, including all methods using only positive feedback, in MAP and P@10. One possible explanation for our findings is that our user model and method for obtaining relevance feedback tend to produce many more non-relevant documents than relevant documents for feedback, thus en masse , many examples of non-relevant documents do help to characterize the overall space of non-relevance. Another possible effect we are ob-serving is due the entire document pool coming from the initial Indri ranking, thus the set of non-relevant documents used for feedback might otherwise be highly ranked by the system. The TREC Relevance Feedback track [4] has inves-tigated issues of feedback document selection in recent years, though the question remains open as to what constitutes a useful non-relevant document for negative feedback [26].
This section reports relative accuracy achieved by differ-ent RF methods under noisy feedback, when the user clicks on non-relevant documents or fails to click on relevant ones. As discussed earlier (  X  2), noisy feedback is modeled using two Bernoulli parameters, f p and f n , to control the false positive and negative rates, respectively. Setting both pa-rameters to zero yields the same correct feedback setting evaluated above. We evaluate methods for three noise con-ditions: (1) f p = 0 and f n = 0 . 1, (2) f p = 0 . 1 and f and (3) f p = 0 . 1 and f n = 0 . 1. Results of KeepAll evalua-tion are shown in Figure 5.
While all methods are seen to decrease in accuracy in com-parison to the noiseless condition, active learning methods continue to dominate the other baselines. However, results with noise rates of 20% and higher have shown Rocchio to dominate active learning, suggesting a threshold for method selection based upon the level of noise. Our hypothesis for this behavior is that active learning, which updates the deci-sion surface based on feedback, is inherently more sensitive than Rocchio, which simply updates its linear combination of feedback documents. Thus the same virtue which enables active learning to learn more efficiently from correct feed-back appears to render it less robust to noisy feedback. This section presents experimental results conducted on LETOR 3.0 [15]. LETOR defines a standard feature rep-resentation of documents for supporting machine learning studies, enabling easier comparison of alternative learning strategies for the same features. In comparison to the term-based features used in our earlier experiments on Robust04, LETOR features are quite different and potentially more powerful, providing another useful benchmark for assessing the relative effectiveness of our active learning methods.
While LETOR includes several document collections, not all are suitable for our study. In particular, because we are interested in RF (which requires many relevant documents), we exclude OHSUMED, as well as Homepage or Named Page Finding collections. Instead, we focus on Topic Distillation: TD2003 and TD2004. These collections are derived from the 2003 and 2004 TREC Web track [7] tasks on the .GOV collection (a 1.25 million web crawl of the .gov domain per-formed in 2002). No dimensionality reduction is performed.
For LETOR, we do not have the query, so model-based feedback [28] cannot be evaluated. For other methods, we use two positive and two negative documents to form the query for Rocchio, and to initialize the active learning mod-els. Active learning parameters are identical to those used in our earlier experiments on Robust04 with one notable ex-ception: we restrict the ratio of non-relevant feedback to rel-evant feedback at a heuristic 7:1 ratio. That is, we ignore ad-ditional non-relevant documents provided as feedback when-ever it would exceed this threshold. While TD2003 and TD2004 have more relevant documents per topic than other LETOR collections, relevant documents remain relatively sparse. We found that an excess of non-relevant documents hurt active learning accuracy. Parameters are tuned on TD2003, and tested on TD2004.

A significant difference between our evaluation on LETOR vs. Robust04 stems from the differing nature of features in the two collections. With term-based features (Robust04), RF let us learn term-based weights to improve ranking ac-curacy for the current query. For example, while the word  X  X og X  might be very important to the current query, RF on this query cannot tell us anything about how important this word should be for other queries. With LETOR, in contrast, features capture general properties of query-document com-patibility. This means, for example, that if the PageRank feature is important for the current query, there is a good chance it will be important for other queries as well.
Due to this difference in features, we can simplify our evaluation methodology enormously by adopting a slightly altered task scenario. In this new scenario, training and testing phases are completely distinct: the user trains the system via iterative feedback on one topic, then uses the system to rank documents for all other topics. Because the user cannot provide iterative feedback on multiple queries in parallel, we abandon LETOR X  X  standard 5-fold partition of topics. Instead, for each topic we train on it alone and eval-uate on all other topics, then we average this over all topics. Since we are only training on a single topic, resulting ac-curacy is far lower than what typically published LETOR results. As with our first batch of results presented for Ro-bust04, we again assume the user provides correct feedback.
This revised task means that the user no longer cares about finding relevant documents for the current topic. How-ever, he stills terminates iteration whenever either (a) the trained system is  X  X ood enough X , (b) his effort threshold in training the system is exceeded, or (c) no additional relevant documents remain for feedback. As in earlier experiments on Robust04, once (c) occurs, we exclude the given topic from average accuracy calculations in subsequent iterations. This means that learning curve plots reflect an average over pro-gressively fewer topics as the number of relevant documents is exhausted. As the average is computed over progressively fewer topics, stability decreases, especially at late iterations.
Consequently, we only present results for 10 iterations with TD2003 and 20 iterations with TD2004. These termi-nation points stem from the corresponding histograms for TD2003 and TD2004 indicating number of relevant docu-ments per query (Figures 4(c) and Figure 4(f), respectively). We stop iteration when the number of topics remaining would fall below 20 (a loose threshold for result stability from observation). While the histogram for the Robust04 collection is not shown, the choice of terminating after 35 iterations there is the same: there would be less than 20 topics with any relevant documents left to be found.
Results in Figure 4 show that active learning methods perform as well or better than Rocchio.
This paper considers an interactive IR scenario in which the user is interested in finding several to many relevant documents with minimal effort. Given an initial document ranking, the user interacts with the system to provide it-erative relevance feedback. Evaluation on the TREC Ro-bust04 collection and LETOR TD2003 and TD2004 collec-tions shows our active learning approach dominates several standard baselines in terms of effectiveness achieved relative to the amount of user effort required.

Future work will continue to develop our active learning methods, evaluate on additional document collections, com-pare stability of parameters across collections, and evaluate noisy feedback using real user data. We are also interested in modeling and evaluating active learning for diversity rank-ing [16, 18, 25] with interactive retrieval.

Investigation of the exploration vs. exploitation learning tradeoff represents another interesting direction for future work: varying the rank at which to slot-in an uncertain doc-ument for feedback, as well as potentially varying the num-ber of uncertain documents used. While a lower ranking of uncertain documents will increase ranking accuracy at the current iteration, doing so may reduce accuracy at future iterations by increasing the probability of a different rele-vant document occurring at a higher rank. In that event, the user would never see the uncertain document (under as-sumptions of the current user model), thereby failing to label the example selected by active learning.
 We would like to thank the anonymous reviewers for their valuable feedback. This work was partially supported by a John P. Commons Fellowship for the second author. [1] B. Bai, J. Weston, D. Grangier, R. Collobert, [2] M. Belkin and P. Niyogi. Laplacian eigenmaps and [3] C. Brandt, T. Joachims, Y. Yue, and J. Bank.
 [4] C. Buckley and S. Robertson. Relevance feedback [5] W. Cooper. Expected search length: A single measure [6] G. Cormack, C. Clarke, and S. Buttcher. Reciprocal [7] N. Craswell and D. Hawking. Overview of the [8] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, [9] F. Diaz and D. Metzler. Improving the estimation of [10] X. He, D. Cai, and P. Niyogi. Laplacian score for [11] X. He and P. Niyogi. Locality preserving projections. [12] T. Hofmann. Probabilistic latent semantic indexing. In [13] A. Kapoor, E. Horvitz, and S. Basu. Selective [14] P. Over. The TREC interactive track: an annotated [15] T. Qin, T. Liu, J. Xu, and H. Li. LETOR: A [16] F. Radlinski and S. Dumais. Improving personalized [17] F. Radlinski and T. Joachims. Active exploration for [18] D. Rafiei, K. Bharat, and A. Shukla. Diversifying web [19] S. Robertson. The probability ranking principle in IR. [20] G. Salton and C. Buckley. Improving retrieval [21] R. Schapire. A brief introduction to boosting. In Proc. [22] M. D. Smucker, J. Allan, and B. Carterette. A [23] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. [24] S. Tong and D. Koller. Support vector machine active [25] J. Wang and J. Zhu. Portfolio theory of information [26] X. Wang, H. Fang, and C. Zhai. A study of methods [27] X. Wei and W. Croft. LDA-based document models [28] C. Zhai and J. Lafferty. Model-based feedback in the
