 Andreas Argyriou ARGYRIOUA @ ECP . FR  X  Ecole Centrale Paris, Center for Visual Computing Francesco Dinuzzo FRANCESD @ IE . IBM . COM One of the dominant approaches in machine learning and statistics is to formulate a learning problem as an optimiza -tion problem to be solved. In particular, regularization has been widely used for learning or estimating functions or models from input and output data, particularly in super-vised and semisupervised learning.
 Regularization in a Hilbert space H frames the problem of learning from data as a minimization of the type min { f ( h w,w 1 i ,..., h w,w m i ) +  X   X ( w ) : w  X  X } . The objective function is the sum of an error term f which ularization penalty  X  , which favors certain desirable prop-erties of the solution. An optimal solution of problem ( 1 ) yields the desired function or vector, depending on the con-text of the original learning problem.
 It is known that, for a certain class of regularization and interpolation problems, one of the optimal solutions of ( 1 ) can be expressed as a linear combination of the data. More specifically, this is the case when the penalty  X  is a Hilber-tian norm (or a nondecreasing function of that). This prop-erty, known as the representer theorem , has proven very useful because it renders many high or infinite dimensional regularization problems amenable to practical computa-tion. This  X  X lassical X  representer theorem was formulated in various guises in ( Girosi , 1998 ; Kimeldorf &amp; Wahba , 1970 ; Sch  X olkopf et al. , 2001 ) and has been the topic of ex-tensive further study ( Argyriou et al. , 2009 ; De Vito et al. , 2004 ; Dinuzzo &amp; Sch  X olkopf , 2012 ; Dinuzzo et al. , 2007 ; Gnecco &amp; Sanguineti , 2010 ; Mukherjee &amp; Wu , 2006 ; Steinwart , 2003 ; Yu et al. , 2013 ). In machine learning, the representer theorem is the main factor that enables application of the so-called  X  X ernel trick X  and underpins a ll of the widely used kernel methods ( Sch  X olkopf &amp; Smola , 2002 ), such as support vector machines, regularization networks, etc.
 Besides the classical result, more recently new types of representer theorems have been proven and studied. For example, it has been realized that analogous optimality conditions apply to the learning of vector-valued func-tions ( Micchelli &amp; Pontil , 2005 ),  X  learning ( Evgeniou et al. , 2005 ) and structured prediction ( Lafferty et al. , 2004 ). Further developments occurred with the advent of matrix regularization problems used for multitask learning or collaborative filtering. Thus it has been shown that a type of representer theorem holds when the penalty  X  is a spectral function of matrices ( Amit et al. , 2007 ; Argyriou et al. , 2009 ; 2010 ) or opera-tors ( Abernethy et al. , 2009 ). Very recently these results have been extended to matricizations of tensors as well peared in the contexts of domain adaptation ( Kulis et al. , 2011 ), dimensionality reduction ( Jain et al. , 2010 ) and met-ric learning ( Jain et al. , 2012 ).
 Some variants of the classical theorem were shown in the contexts of semisupervised learning ( Belkin et al. , 2006 ), semiparametric representer theorems and kernel PCA ( Sch  X olkopf et al. , 2001 ). Moreover, there have ap-peared alternative approaches which lie outside the scope of this paper, such as a Bayesian variant of the classical theorem ( Pillai et al. , 2007 ), the theory of reproducing ker-nel Banach spaces ( Zhang &amp; Zhang , 2012 ) and an algorith-mic theorem for matrices ( Warmuth et al. , 2012 ). Clearly therefore, representer theorems are important and ubiqui-tous tools in regularization and underly a wide range of frequently used machine learning methodologies.
 In this paper, we address the topic of representer theorems from a new and more abstract viewpoint. One of our con-tributions is to provide a unifying framework which sub-sumes the results that have already appeared in the liter-ature. These include the classical, vector valued, struc-tured prediction, multitask, tensor, semisupervised, sem i-parametric, dimensionality reduction, domain adaptation , metric learning results etc. In particular, we show that the se theorems are only examples from a larger family. Each theorem in this family corresponds to a class of regular-ization penalties which are characterized by an orthomono-tonicity property that we introduce. Another implication of our results is that we can now put the study of representer theorems on a formal basis and provide calculus rules and recipes for deriving new results. Most commonly used ker-nel methods (support vector machines, kernel ridge regres-sion etc.), as well as methods for multitask learning, colla b-orative filtering and metric learning, fall within our frame -work. As an illustration of the theory, we demonstrate that regularization problems with a generalized family of ma-trix penalties, as well as similar problems on the positive semidefinite cone, admit appropriate representer theorems . In many practical situations this implies that the number of degrees of freedom and hence the complexity of solving the In this section, we introduce the notation and the mathe-matical concepts necessary for our framework and the main results of Section 3 . 2.1. Notational conventions Let H be a real Hilbert space with inner product h X  ,  X i and associated norm k X k . We use L ( H ) to denote the set of linear operators from H to itself. We denote the identity operator by Id  X  L ( H ) and the set of linear subspaces of H by V ( H ) .
 Also let N the set of real d  X  n matrices and M matrices. Moreover, let S n semidefinite matrices and S n ones. We denote the t -th column of a matrix W  X  M by w sets A,B  X  H : A + B := { a + b : a  X  A,b  X  B } , A  X  B := { a  X  b : a  X  A,b  X  B }  X A := {  X a : a  X  A } , for every  X   X  R .
 In the following, we will be working with subspace-valued maps S : H  X  V ( H ) . This choice is natural, since repre-senter theorems are statements that solutions of certain op -timization problems belong to certain subspaces. For more details, see Section 3 and our general definition of repre-senter theorems. Given two subspace-valued maps S S , their sum S subspaces S 2.2. Quasilinear Subspace-Valued Maps To extend the concept of representers, we first introduce a Definition 2.1. We call the map S : H  X  V ( H ) quasilin-ear if for every x,y  X  X  ,  X , X   X  R .
 Proposition 2.1. Let S denote a quasilinear subspace-valued map. Then and for every x,y  X  X  ,  X , X   X  R .
 Definition 2.2. We call the map S : H  X  V ( H ) idempo-tent if Lemma 2.1. Let S be a quasilinear and idempotent subspace-valued map. Then, for every m  X  N and every set { x Thus addition of subspaces can be used to generate sub-spaces invariant under S .
 In addition to the quasilinearity and idempotence assump-tions, we require that sums of images under S are closed. This ensures that orthogonal projection on such subspaces is feasible, which is a crucial step in the proof of represent er theorems. For simplicity, to satisfy this property we assum e that all images under S are finite dimensional. Another as-sumption necessary for the proof of our main result is that any point belongs to its image under S . Summarizing, we collect all of the above assumptions in the following defi-nition.
 Definition 2.3. Let r  X  N . We call the subspace-valued map S : H  X  V ( H ) r -regular quasilinear if it is quasilin-ear, idempotent and if, for all x  X  X  , S ( x ) has dimension-ality at most r and contains x .
 The simplest example of regular quasilinear subspace-valued map S is the map that associates a given vector to its own linear span, which thus has dimensionality one. Example 2.1. Suppose that S maps Then S is 1-regular quasilinear.
 More generally, we can map each point to a subspace by applying a set of linear transformations and taking the lin-ear subspace spanned by the resulting vectors.
 Example 2.2. Let r  X  N and suppose that S maps where T This map is r -regular quasilinear if  X  Id  X  span { T i : i  X  N r } ,  X  T j T  X   X  span { T i : i  X  N r }  X  j, X   X  N r .
 Remark 2.1. It may not hold that S maps any linear sub-space of H to a linear subspace (as illustrated in Example 2.2 when, say, r = 2 , and T independent for some x,y  X  X  ). Even when this condition holds, the image of a linear subspace may be a different subspace (consider S ( x ) = H ,  X  x 6 = 0 , and span { x } A special case of Example 2.2 is the following, defined for a space of matrices. As we shall see, this example is rele-vant to representer theorems for multitask learning. Example 2.3. Let H = M inner product, and suppose that S maps Then S is min { n 2 ,dn } -regular quasilinear. Our focus of interest is the variational problem of minimiz-ing, over a Hilbert space, a regularization functional of th e form The functional J is the sum of an error term f : R m  X  R  X  { +  X  X  , which depends on prescribed data w 1 ,...,w m  X  H , and a regularization term  X  : H  X  R  X  X  +  X  X  , which enforces certain desirable properties on the solution, sca led by a regularization parameter  X  &gt; 0 . We allow both and  X  to take the value +  X  , so that interpolation problems and regularization problems of the Ivanov type can also be taken into account.
 Since the same functional J might be decomposed into a form like ( 4 ) in multiple ways, we fix m  X  N and use the tuple ( f,  X  , X ,w tion functional.
 Example 3.1 (Interpolation in a Hilbert space) . Let w ,...,w m  X  H and y 1 ,...,y m  X  R be prescribed data and Then the interpolation problem is equivalent to the problem of minimizing ( 4 ) over H . Example 3.2 (Ivanov regularization) . Ivanov regulariza-tion amounts to solving a problem of the form min { f ( h w,w 1 i ,..., h w,w m i ) : w  X  X  ,  X  ( w )  X  1 } , where  X  : H  X  R is a prescribed constraining function. Defining this problem can be rewritten as the minimization of a func-tional of the form ( 4 ) .
 Example 3.3 (Regularization in an RKHS) . Reproducing Kernel Hilbert Spaces (RKHS) are Hilbert spaces H of functions w : X  X  R defined over a nonempty set X such that all point-wise evaluation functionals are bounded, th at is, for all x  X  X there exists a constant C that It can be shown that RKHS exhibit the so-called reproduc-ing property w ( x ) = h w,K representers K metric and positive semidefinite kernel K : X  X X  X  R such that K property of an RKHS allows for rewriting any regulariza-tion functional of the form in the standard form ( 4 ) , where the representers w cide with the kernel sections K Example 3.4 (Regularization with averaged data) . In some estimation problems, it may be appropriate to assume that the measured output data are obtained by averaging a func-tion w (to be estimated) with respect to suitable probability measures. Let P the measurable space ( X , A ) , where A is a  X  -algebra of subsets of X , and let H denote a Hilbert space of functions w : X  X  R . If, for every i  X  N m , the expectation is a bounded linear functional over H , then one may con-sider synthesizing a function w by minimizing a functional of the form which can be rewritten in the form ( 4 ) by introducing suit-able representers w with a bounded reproducing kernel, the representers of the expectation functionals E embeddings  X  see, for example, ( Muandet et al. , 2012 ; Sriperumbudur et al. , 2010 ) and references therein  X  and can be explicitly expressed as Clearly, we are interested only in cases in which the opti-mization problem is well defined, that is, a minimizer of J exists. This always holds by construction in machine learning and statistics ap -plications. More generally, existence of a minimizer can be ensured under lower semicontinuity and coercivity con-ditions on J . We will avoid specifying such precise con-ditions since they are not relevant to our purposes, instead assuming existence of minimizers for each problem of in-terest.
 The main question we address in this paper is to charac-terize the functions  X  for which minimizers of the reg-ularization functional ( 4 ) admit certain convenient rep-resentations. As already mentioned in the introduction, representer theorems have been proven for regulariza-tion with the Hilbertian norm, Schatten  X  ( Abernethy et al. , 2009 ; Argyriou et al. , 2009 ; 2010 ) and in some other cases. These theorems state that a minimizer must lie in a subspace which depends on the data points w ,...,w m . This dependence on the data varies according to the regularization penalty  X  . For example, in the classi-cal representer theorem (  X  = k X k the span of the data points. In the multitask theorem (  X  a spectral function on matrices), the subspace is generated by the columns of the data matrices.
 Our goal is to unify this prior work under one framework and at the same time to extend the applicability of represen-ter theorems to other regularization problems. The key to this is to associate representations of minimizers with the data points in an abstract way, specifically to associate a subspace to each data point. Hence we assume a subspace-valued map S : H  X  V ( H ) and require that the represen-tation for a minimizer of ( 4 ) be spanned by the elements of S ( w i ) , i  X  N m .
 Definition 3.1. Let m  X  N , S : H X  X  ( H ) be a subspace-valued map and J = ( f,  X  , X ,w tion functional of the form ( 4 ) . Then J is said to admit a representer theorem with respect to S if there exists a min-imizer  X  w of J such that Definition 3.2. Let m  X  N , S : H X  X  ( H ) be a subspace-valued map and F a family of regularization functionals of the form ( 4 ) . Then F is said to admit a representer theo-rem with respect to S if every J  X  F admits a representer theorem with respect to S .
 Our main tool for characterizing regularization function-als that admit representer theorems is the property defined below, which we call orthomonotonicity . The connection between orthomonotonicity and representer theorems has appeared in ( Argyriou et al. , 2009 ) in the context of reg-ularization with the Hilbertian norm or with orthogonally invariant matrix penalties. In Theorem 3.1 , we extend this connection to a broader class of regularization penalties which arise by varying the choice of the map S .
 Definition 3.3. We call the function  X  : H  X  R  X  X  +  X  X  orthomonotone with respect to the map S : H X  X  ( H ) , if Note that in this definition the left hand side of ( 5 ), or even both sides, may equal +  X  .
 Theorem 3.1. Let r,m  X  N , f : R m  X  R  X  { +  X  X  ,  X  : H  X  R  X  X  +  X  X  and suppose that S : H  X  V ( H ) is an r -regular quasilinear map. Then the following hold: 1. If  X  is orthomonotone w.r.t. S then, for any 2. Let F denote the following family of regularization Proof. The first part of the theorem (sufficiency) can be proven by adapting a classical orthogonality argument. Take any w the regular quasilinearity of S , L is a finite dimensional subspace that contains R = span { w fore any minimizer  X  w of the regularization functional J = ( f,  X  , X ,w 1 ,...,w m ) can be decomposed as Applying Lemma 2.1 we obtain that S ( u )  X  L and hence that v  X  S ( u )  X  . If  X  is orthomonotone then
J (  X  w ) = f ( h u + v,w 1 i ,..., h u + v,w m i ) +  X   X ( u + v ) so that u  X  X  is also a minimizer.
 Now, let us prove the second part of the theorem (neces-sity). Let us fix arbitrary x  X  X  and y  X  S ( x )  X  . The goal of the proof is to establish orthomonotonicity, namely the inequality The proof is organized in three cases. 1. First, we observe that for x = 0 the inequality follows 2. Secondly, observe that if  X  ( x + y ) = +  X  , inequality 3. It remains to prove ( 7 ) in the case when 3.1. Loss Functions Which Lead to Orthomonotonicity Observe that part 1 of Theorem 3.1 (sufficiency of or-thomonotonicity) only requires existence of minimizers of J , without any specific additional assumptions on the error term. On the other side, part 2 (necessity of orthomono-tonicity) holds under additional assumptions on f . In the following, we provide examples of functions f that satisfy such assumptions, showing that most of the error functions considered in practice do so. The vast majority of error functions used are additively separable , namely of the form where V : R  X  R  X  R  X  X  +  X  X  and y output data. In this section we show that, for a broad class of regres-sion loss functions, it is possible to find output data such that, if the family of regularization functionals ( 6 ) admits a representer theorem, then  X  is orthomonotone.
 Definition 3.4. We call the function V : R  X  R  X  R  X  { +  X  X  a regression loss function if where  X  : R  X  R  X  X  +  X  X  is lower semicontinuous with bounded sublevel sets and minimized at zero.
 The class of functions defined above includes any loss of absolute loss), the interpolation loss as well as the  X  -insensitive loss  X  ( t ) = max { 0 , | t | X   X  } which is not uniquely minimized at zero.
 Lemma 3.1. Assume that V is a regression loss function. Then, for every p  X  N , there exist output data { y N the hypothesis of Theorem 3.1 , Part 2, such that the error functional w 7 X  f ( h w,w 1 i ,..., h w,w p i , h w,w 1 i ,..., h w,w with f defined by ( 14 ) for m = 2 p , equals the error func-tional Definition 3.5. We call the function V : R  X { X  1 , +1 } X  R a regular binary classification loss function if where  X  : R  X  R is lower semicontinuous, nonincreasing, there exists  X  &gt; 0 such that the function admits a unique minimizer  X  t 6 = 0 and there exists  X  &gt; 0 such that the sublevel set { t  X  R :  X  bounded.
 It can be seen easily that this definition is satisfied by most commonly used binary classification loss functions, includ -ing the logistic loss  X  ( t ) = log(1 + e  X  t ) , the exponential To verify the uniqueness of the minimizer for these three losses, choose for instance  X  = 1 / 2 .
 Lemma 3.2. Assume that V is a regular binary classifi-cation loss function. Then, for every p  X  N , there exist output data { y f u : R p  X  R Part 2, such that the error functional w 7 X  f ( h w,w 1 i ,..., h w,w p i , h w, X w 1 i ,..., h w, X w with f defined by ( 14 ) for m = 2 p , equals the error func-tional 3.2. Properties of Orthomonotone Functions An obvious first fact about orthomonotone functions is that nesting of maps preserves orthomonotonicity.
 Proposition 3.1. If S,S  X  : H  X  V ( H ) are such that S ( x )  X  S  X  ( x ) for all x  X  X  , then any  X  : H X  R  X  X  +  X  X  orthomonotone with respect to S is also orthomonotone with respect to S  X  .
 Thus, enlarging the map S enlarges the class of orthomono-tone functions as well. In the extreme case when S maps every point to H , the orthomonotone class includes all functions. At the other extreme, S maps every point to { 0 } and the orthomonotone class equals the set of constant functions.
 A convenient way to obtain new orthomonotone functions (and hence new representer theorems) is by applying sim-ple operations to known orthomonotone functions. For example, shifting the argument inside an orthomonotone function yields an orthomonotone function with respect to a larger map. This fact implies that Theorem 3.1 can be mod-ified to apply to functions  X  that are minimized at points other than 0 .
 Proposition 3.2. Let a  X  H and  X  : H  X  R  X  X  +  X  X  orthomonotone with respect to the map S : H  X  V ( H ) . If
S is quasilinear then the function x 7 X   X ( x + a ) is or-thomonotone with respect to the map x 7 X  S ( x ) + S ( a ) Another useful rule combines functions which are or-thomonotone with respect to different maps.
 Proposition 3.3. Let  X  tone with respect to a map S H  X  R  X  X  +  X  X  be orthomonotone with respect to a map S 2 : H X  X  ( H ) be elementwise nondecreasing, that is, h ( a  X  ,b  X  )  X  h ( a,b ) whenever a  X   X  a and b  X   X  b . Then the function  X  : H  X  R  X  X  +  X  X  , is orthomonotone with respect to the map S This rule holds more generally for any finite number of orthomonotone functions. In particular, any nonnegative linear combination of orthomonotone functions is also or-thomonotone with respect to the sum of the corresponding maps. The same applies to the maximum and to the mini-mum of orthomonotone functions.
 Finally, there is a composition rule for orthomonotone functions, similar to the chain rule for differentiation. Proposition 3.4. Let  X  : H  X  R  X  X  +  X  X  be orthomono-tone with respect to a map S : H  X  V ( H ) and let T  X  L ( H ) be a continuous operator. Then the function  X   X  T is orthomonotone with respect to T  X   X  S  X  T . We now proceed to describe the set of orthomonotone func-tions for specific regularization problems of interest. For each problem, we describe the map S , provide a class of orthomonotone functions and state the resulting represen-ter theorem.
 Example 4.1. Assume that the dimension of H is at least two and let S be defined as in Example 2.1 . Then, the def-inition of representer theorem 3.1 reduces to the classical linear combination of the representers where c reduces to If
 X  is lower-semicontinuous, this last condition is satisfied if and only if with h : R  X  R  X  X  +  X  X  nondecreasing.
 ( Argyriou et al. , 2009 ; Yu et al. , 2013 ) for related results. This is a generalized version of the well known  X  X lassical X  representer theorem ( Girosi , 1998 ; Kimeldorf &amp; Wahba , 1970 ; Sch  X olkopf et al. , 2001 ) which has found wide appli-cation to regularization methods in Hilbert spaces. The case of regularization with a bias term can be recov-ered easily by choosing the error function f as a minimum with respect to the bias variable. This technique also yield s semiparametric theorems ( Sch  X olkopf &amp; Smola , 2002 ). Example 4.2. Let H and S be defined as in Example 2.3 . Then the representer theorem 3.1 reduces to where C thomonotonicity ( 5 ) reduces to  X ( X + Y )  X   X ( X ) ,  X  X,Y  X  M d,n : X T Y = 0 .
 Moreover, in this case the orthomonotonicity property is equivalent to with h : S n ing function (with respect to the partial order of positive semidefinite matrices).
 See ( Argyriou et al. , 2009 ; Yu et al. , 2013 ) as well as ( Amit et al. , 2007 ; Argyriou et al. , 2008 ; 2010 ; Evgeniou et al. , 2005 ) for special cases. The above representation extends the classical representer theorem to matrix learning problems, such as regularization with penalties involving the Frobenius norm, the trace norm and general spectral penalties. These methods have been used for multitask learning, collaborative filtering, kernel learning, domain adaptation and other problems. Problems like multitask learning benefit substantially from the representer theorem since in those cases the data matrices are rank-one (and in collaborative filtering they are also sparse). Indeed, whenever the data matrices are rank-one, that is, W write  X  W = m P problem with substantially fewer degrees of freedom can equivalent to an optimization problem whose number of variables is mn , which can be much smaller than dn , the size of matrix W .
  X ( W ) = g ( R  X  W  X  GWR ) , with R  X  M n,k , G  X  S d ++ and g matrix nondecreasing, other representer theorems can be derived from the above result, by applying the change of variable W  X  = G 1 2 W . These apply, for example, to spectral functions of QWR , with Q  X  M have been proposed for multi-task learning ( Dinuzzo , 2013 ; Dinuzzo &amp; Fukumizu , 2011 ).
 In addition, Example 4.2 relates to certain optimiza-tion problems with positive semidefinite matrix vari-ables. Indeed, problem ( 4 ) with H = M g ( R  X  W  X  WR ) , g matrix nondecreasing and rank-one data, yields a problem of the type by the change of variable Z = W  X  W . Thus a representer theorem for this family of problems follows directly from Example 4.2 . Some results for special cases of ( 15 ), ap-plied to metric and semisupervised learning, have already appeared in ( Jain et al. , 2010 ; 2012 ).
 Example 4.3. Assume H = M dard inner product, and suppose that S maps Then S is nd -regular quasilinear. For this map, definition 3.1 reads and the definition of orthomonotonicity ( 5 ) reduces to  X ( X + Y )  X   X ( X ) ,  X  X,Y  X  M d,n : which is satisfied by all functions such that where h : S n in each matrix argument.
 The family of regularizers described in the last example includes, for instance, functions of the form where k X k is any orthogonally invariant norm. Such penal-ties are of considerable interest in many matrix learning problems, since they allow for incorporating information about both row and column dependencies, by designing the matrices Q and R . This can be applied, for instance, to col-laborative filtering problems when side information about both users and items is available. When the data matri-ces are rank-one (such as in multi-task learning and collab-orative filtering problems), the representer theorem above makes it again possible to obtain a significant reduction in the number of degrees of freedom, since the solution  X  W can be rewritten in the form  X  W = m P number of variables, m ( n + d ) , can be much smaller than nd , the size of W . We have presented a framework which unifies existing re-sults about representer theorems for regularization prob-lems and allows for a more formal study of these results. We introduced a new definition of representer theorem to include a broader family of representation results. We showed that each theorem in this family corresponds to a regular quasilinear subspace-valued map. Moreover, we characterized the class of regularization penalties corre -sponding to each representer theorem via the orthomono-tonicity property. Orthomonotone functions exhibit simpl e calculus rules, which can be used to obtain new representer theorems by combining existing ones.
 Our new framework opens a number of possibilities for fur-characterizations of regular quasilinear subspace-value d maps and orthomonotone functions, given their importance in the mathematical construction that leads to the repre-senter theorems. Secondly, it can lead to the derivation of new families of regularization penalties and correspondin g methodologies, for example, for matrix and tensor regular-ization. Finally, it lays the foundation for a new and more general class of kernel methods, obtained by plugging the expression of the generalized representer theorem into the objective functional J and considering the resulting opti-mization problem.

