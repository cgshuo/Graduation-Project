 Kernel methods have been applied successfully in many data mining tasks. Subspace kernel learning was recently pro-posed to discover an effective low-dimensional subspace of a kernel feature space for improved classification. In this paper, we propose to construct a subspace kernel using the Hilbert-Schmidt Independence Criterion (HSIC). We show that the optimal subspace kernel can be obtained efficiently by solving an eigenvalue problem. One limitation of the ex-isting subspace kernel learning formulations is that the ker-nel learning and classification are independent and the sub-space kernel may not be optimally adapted for classification. To overcome this limitation, we propose a joint optimization framework, in which we learn the subspace kernel and sub-sequent classifiers simultaneously. In addition, we propose a novel learning formulation that extracts an uncorrelated subspace kernel to reduce the redundant information in a subspace kernel. Following the idea from multiple kernel learning, we extend the proposed formulations to the case when multiple kernels are available and need to be com-bined. We show that the integration of subspace kernels can be formulated as a semidefinite program (SDP) which is computationally expensive. To improve the efficiency of the SDP formulation, we propose an equivalent semi-infinite linear program (SILP) formulation which can be solved ef-ficiently by the column generation technique. Experimental results on a collection of benchmark data sets demonstrate the effectiveness of the proposed algorithms.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms Classification, subspace kernel, Hilbert-Schmidt independence criterion, support vector machines
Kernel methods have been applied successfully in various data mining tasks such as clustering, regression, and classi-fication [1, 4, 5, 18, 19, 20]. They work by mapping the data from the original input space to a high-dimensional (possibly infinite-dimensional) feature space. The key fact underlying the success of kernel methods is that the embedding into the feature space can be determined implicitly by specifying a symmetric kernel function that computes the dot product between pairs of data points in the feature space.
In many applications, the interesting patterns of the data may lie in a low-dimensional subspace of a certain kernel fea-ture space. Subspace kernel learning that finds such a low-dimensional subspace for effective pattern discovery has re-ceived considerable attention recently [13, 16, 17, 26]. In [25], a discriminative subspace kernel learning algorithm was pro-posed to find a low-dimensional subspace of the kernel fea-ture space. Kernel Target Alignment (KTA) [6] was em-ployed as the learning criterion, resulting in a complex non-linear optimization problem, for which the conjugate gradi-ent algorithm [15] was applied to compute a locally optimal solution.

In this paper, we propose to construct a subspace kernel using the Hilbert-Schmidt Independence Criterion (HSIC) recently proposed for measuring the statistical dependence of random variables [10]. Under HSIC, an optimal subspace kernel maximizes its dependence with the ideal kernel con-structed from the class labels. We show that a globally op-timal subspace kernel can be obtained efficiently by solving an eigenvalue problem. One limitation of the existing sub-space kernel learning formulations is that the kernel learning is independent of the classifier employed subsequently, thus the subspace kernel may not be optimally adapted for clas-sification. To overcome this limitation, we propose a joint optimization framework in which we perform subspace ker-nel learning and classification simultaneously. In addition, we propose a novel learning formulation that extracts an un-correlated subspace kernel to reduce redundant information in a subspace kernel.

Following the idea from multiple kernel learning (MKL) [12], we extend the proposed formulations to the case when multiple kernels are available and need to be combined. For example, when we employ the Gaussian kernel for classifica-tion, we need to estimate the optimal value of the hyperpa-rameter. Cross-validation is commonly applied for the hy-perparameter estimation. In MKL, however, a set of Gaus-sian kernels (with different hyperparameters) can be inte-grated for improved classification performance. This is par-ticularly useful when there exists complementary informa-tion among different kernels. In contrast, cross-validation selects only a single best kernel and fails to exploit such complementary information. We show that the integration of (uncorrelated) subspace kernels can be formulated as a semidefinite program (SDP) [2], which is computationally expensive to solve. To improve the efficiency of the SDP formulation, we propose an equivalent semi-infinite linear program (SILP) formulation which can be solved efficiently using the column generation technique [11, 23]. Experimen-tal results on a collection of benchmark data sets demon-strate the effectiveness of the proposed algorithms.
The remainder of this paper is organized as follows: We review the basics of subspace kernel learning and the Hilbert-Schmidt Independence Criterion in Section 2. We present the subspace kernel learning via dependence maximization as well as the joint framework for simultaneous subspace ker-nel learning and classification in Section 3. We present the concept of uncorrelated subspace kernel and uncorrelated subspace kernel learning in Section 4, followed by subspace kernel integration in Section 5. We report the experimental results in Section 6 and the paper concludes in Section 7.
In this section, we review the basics of subspace kernel learning and the Hilbert-Schmidt independence criterion.
In classification tasks, we are given a set of training data { ( x i ,y i ) } n i =1 ,where x i  X  R m and y i  X  X  1 ,  X  X  X  ,k } input and output, respectively. In kernel methods, a sym-metric function K : R m  X  R m  X  R is called a kernel function if it satisfies the finitely positive semidefinite property [18]. That is, for the input data { x i } n i =1  X  R m ,the Gram (kernel) matrix G  X  R n  X  n , defined as G ij = K ( x i ,x j ), is positive semidefinite. Any valid kernel function K implicitly maps the input data from the input space R m to a Reproducing Kernel Hilbert Space (RKHS) F equipped with the inner product  X  ,  X  through a mapping function  X  K : R m  X  X  as: for all x, x  X  R m .Let X =[ x 1 ,  X  X  X  ,x n ]  X  R m  X  n be the data matrix in R m ,andlet  X  K ( X )=[  X  K ( x 1 ) ,  X  X  X  , X  be the corresponding images of the data in F .

In subspace kernel learning [25], a low-dimensional sub-space of the feature space is computed to extract informative features. Let S be an -dimensional subspace of F ,andlet Z =[ z 1 ,  X  X  X  ,z ] be a basis of the subspace S . It follows that each vector in S can be expressed as a linear combination of {  X  K ( x i ) } n i =1 , and hence Z can be expressed as: for some transformation matrix W  X  R n  X  .

Let Z = U z  X  z V T z be the Singular Value Decomposition (SVD) [9] of Z ,where U z consists of orthonormal columns, V z  X  R  X  is orthogonal, and  X  z  X  R  X  is diagonal. Since the subspace S can be spanned by { z i } i =1 , the columns of U z form an orthonormal basis of the subspace S .Hence,the projection of  X  K ( X )into S , denoted as X w ,isgivenby It follows that the kernel matrix in S is given by [25]: where the second equality above follows from Note that G =  X  K ( X ) , X  K ( X ) is the (symmetric) ker-nel matrix computed from F ,and M + denotes the pseudo-inverse [9] of the matrix M .
 In [25], the subspace kernel is optimized based on the Kernel-Target Alignment (KTA) criterion [6]. This leads to a complex nonlinear optimization problem. The conju-gate gradient algorithm [15] is applied for computing the solution, which is only locally optimal.
Hilbert-Schmidt Independence Criterion (HSIC) is pro-posed recently for measuring the statistical dependence of random variables [10]. Let F x be a RKHS defined on the do-main X associated with the kernel function K x : X X X X  R and the mapping function  X  K x : X X  X  x ,andlet F y be another RKHS defined on the domain Y with the ker-nel function K y : Y X Y X  R and the mapping function  X 
K y : Y X  X  y . Assume that x  X  X  and y  X  X  be drawn from some joint measure p xy (probability distribution), then the cross-variance operator C xy : F y  X  X  x is defined as [7]: where  X  is the tensor product operator,  X  x = E [  X  K x ( and  X  y = E [  X  K y ( y )]. Given that F x and F y are separable RKHSs, HSIC is then defined as the squared Hilber-Schmidt norm of the cross-covariance operator C xy given by HSICcanbeexpressedintermsofkernelsas[10]: E where ( x, y )and( x ,y ) are two independent pairs drawn independently from p xy ,and E xx yy is the expectation over these two pairs. In practice, given a finite set of data pairs Z = { ( x i ,y i ) } n i =1  X  R m  X  R independently drawn from the empirical HSIC is estimated by the trace of kernel ma-trices product as [10]: where tr(  X  ) denotes the trace of a matrix, G x ,G y  X  R n  X  n the kernel matrices given by ( G x ) ij = K x ( x i ,x j ), ( K y ( y i ,y j ), and P = I  X  ee T /n is the centering matrix, and e is the vector of all ones of length n . In essence, HSIC amounts to computing the trace of the product of two cen-tered kernel matrices.

HSIC has been applied successfully in clustering [21] and supervised feature selection [22] tasks. In this paper, we propose to employ HSIC for subspace kernels learning. This is motivated by a number of appealing features of HSIC [10]: (1) HISC is an independence measure; (2) HSIC is unbiased and concentrated; and (3) it can be computed efficiently.
In this section, we propose to learn a subspace kernel via dependence maximization based on HSIC measure. We also propose a joint framework in which the subspace kernel and Support Vector Machines (SVM) [18] are learned (trained) simultaneously.
We propose to learn the subspace kernel in Eq. (3) via dependence maximization. That is, the dependency between the optimal subspace kernel and the ideal kernel derived from the labels is maximized.
 Assume that we are given a centered kernel matrix G  X  R n  X  n , i.e., Ge = e T G =0,where e is the vector of all ones of length n ,andlet y  X  R n be the associated label vector with the i th entry y i  X  X  1 ,  X  X  X  ,k } . Following Eq. (3), we propose to construct a regularized subspace kernel e G w as follows: where a regularization term co ntrolled by the regularization parameter  X &gt; 0 is included to avoid the singularity prob-lem. Mathematically, the subspace kernel learning problem based on HSIC can be formulated as the following trace maximization problem: where H ( y )  X  R n  X  n is the ideal kernel derived from the la-bel vector y  X  R n . A similar formulation has been proposed for clustering in [21]. It follows from the properties of ma-trix trace that the optimization problem in Eq. (7) can be reformulated equivalently as follows: The optimal transformation W  X  to Eq. (8) can be obtained by solving an eigenvalue problem [8], as summarized below.
Theorem 3.1. Let G be a centered kernel matrix, and let columns of V =[ v 1 ,  X  X  X  ,v ] be the first eigenvectors of (
G +  X I )  X  1 GH ( y ) G corresponding to the largest eigenval-ues. Then the optimal W  X  to Eq. (8) is given by V . The optimal W  X  of Eq. (7) is determined by the two ker-nel matrices G and H ( y ) derived from the data and the corresponding labels, respectively. With different choices of kernel functions for G and H ( y ), the optimal W  X  forms a family of transformations for subspace kernel learning. The commonly used data space kernels include the linear kernel, polynomial kernel, and RBF kernel for vectorial data, and the string kernel for structured data [18]. Similarly, various choices for the label space kernels can be employed, and we present some representative kernels below:
Definition 1. Let y  X  R n be the label vector with the i th entry y i  X  X  1 ,  X  X  X  ,k } ,andlet Y  X  R n  X  k be the class indica-tor matrices defined as: Y ( ij )=1 if y i = j and Y ( ij otherwise. Two representative label kernel matrices are: (1) .H 1 ( y )= YY T , (2) .H 2 ( y )= LL T ,L = Y ( Y T Y )  X  1 2 .
 Note that both label kernels defined above capture the cor-relation among labels, but in different ways. Based on the optimal transformation matrix W  X  to Eq. (7), we can con-struct the optimal subspace kernel e G w as in Eq. (6), which can then be used in kernel machines such as SVM.
Existing approaches for subspace kernel learning learn the subspace kernel without taking into account the subsequent kernel classifiers such as SVM [25]. In the following, we propose a joint framework in which we learn the subspace kernel and SVM simultaneously.

Given a centered kernel matrix G  X  R n  X  n and the asso-ciated label vector y  X  R n , we define an indicator matrix  X  Y =[ X  y 1 ,  X  X  X  ,  X  y k ]  X  R n  X  k ,where X  y ij =1if y  X  y ij =  X  1 otherwise. By substituting the subspace kernel in Eq. (6) into the dual formulation of the SVM optimization problem [5], we obtain the following min-max problem: min where {  X  i } k i =1 are the vectors of Lagrange dual variables [2], diag( X  y i ) is a diagonal matrix with the diagonal entries as  X  y  X  R n , C is the pre-specified tradeoff parameter, and e is the vector of all ones of length n .

The min-max problem in Eq. (9) is difficult to solve di-rectly. However, if one of the two optimization variables (
W or {  X  i } k i =1 ) is fixed, the other one can be optimized in terms of the fixed one. We thus propose an iterative proce-dure to solve Eq. (9), in which W and {  X  i } k i =1 are updated iteratively.

In particular, for a fixed e G w , the optimal {  X   X  i } k computed by solving the following optimization problem: subject to  X  T i  X  y i =0 , 0  X   X  i  X  C, i =1 ,  X  X  X  ,k. Note that the maximization problem in Eq. (10) decouples with a fixed e G w , and thus the optimal {  X   X  i } k i =1 puted independently. In this case, the optimization problem in Eq. (10) is equivalent to k independent SVMs [18].
For a fixed {  X  i } k i =1 , the optimal W  X  can be computed by solving the following optimization problem: tion problem in Eq. (11) can be reformulated in a compact form given by max Similar to Theorem 3.1, the optimal W  X  to Eq. (12) is given by the first eigenvectors of ( G +  X I )  X  1 ( GA w A T w G sponding to the largest eigenvalues.

Based on the discussion above, we propose an iterative optimization procedure for solving Eq. (9). Note that we determine the convergence of the algorithm by computing the relative change of the objective value in Eq. (9), and the iterative procedure stops if the relative change in objective value is smaller than a pre-specified parameter or if the number of iterations exceeds a pre-specified number.
In this section, we introduce the concept of uncorrelated subspace kernel. Similarly, we propose to learn uncorrelated subspace kernel based on HSIC and in a joint framework, respectively.
Recall that in Eq. (3), the subspace kernel G w is gener-ated by projecting the data image  X  K ( X ) into the subspace S using the projection matrix U z . The projection matrix U is required to have orthonormal columns, i.e., U T z U z = However, redundant information may still exist in S .We propose uncorrelated subspace kernels , i.e., subspace kernels with uncorrelated features, thus reducing the redundant in-formation in the subspace.

Formally, let U q be the transformation matrix that projects the data image  X  K ( X )from F into S as: where X q is the projection of  X  K ( X )in S . It follows from the Representer Theorem [18] that U q can be expressed as: for some matrix Q  X  R n  X  where is the dimensionality of the subspace. We compute a projection such that the resulting features are orthonormal, that is, Let Q =[ q 1 ,  X  X  X  ,q l ], and denote R = Q T G as the data matrix after the projection. It follows that the i th feature component (row vector) of R is R i = q T i G ,andthecovari-ance between R i and R j is
Cov( R i ,R j )= E ( R i  X  ER i )( R j  X  ER j )= q T i GGq where the last equality follows since G is centered. It follows that their correlation coefficient is given by Since the features (after projection) are required to be or-thonormal as in Eq. (15), we have Cor( R i ,R j )=0if i = and Cor( R i ,R j ) = 1 otherwise. Therefore, the feature vec-tors in R (the data matrix after projection) are mutually uncorrelated and hence retain minimum redundancy. The resulting uncorrelated subspace kernel is given by subject to the constraint in Eq. (15). Note that the key difference between the subspace kernel G w in Eq. (3) and the uncorrelated subspace kernel G q in Eq. (18) is that the former employs an orthogonal projection, while the latter leads to orthonormal features.
We propose to optimize the uncorrelated subspace kernel via dependence maximization as follows: where G q  X  R n  X  n is the uncorrelated subspace kernel de-finedinEq.(18), H ( y ) is the ideal kernel derived from the labels as in Definition 1, and  X &gt; 0 is a pre-specified regu-larization parameter.

The optimal Q  X  to Eq. (19) can be obtained by solving a generalized eigenvalue problem, as summarized below. Theorem 4.1. Given a centered kernel matrix G .Let G q be the subspace kernel matrix defined in Eq. (18), and let H ( y ) be defined as in Definition 1. Let V =[ v 1 ,  X  X  X  ,v consists of the first eigenvectors of ( GG +  X G ) + GH ( y corresponding to the largest eigenvalues. Then the optimal Q  X  to Eq. (19) is given by Q  X  = V .
We propose to learn the uncorrelated subspace kernel and the subsequent SVM classifier simultaneously in a joint frame-work as follows: min where G q is defined in Eq. (18). The min-max problem in Eq. (20) has optimization variables Q and {  X  i } k i =1 . Similar to the case in Section 3.2, we apply an iterative procedure to solve this optimization problem, in which Q and {  X  i } k are updated iteratively.

For a fixed G q , the optimal {  X   X  i } k i =1 can be obtained by solving k independent SVMs. For a fixed {  X  i } k i =1 ,theopti-mal Q  X  can be obtained by solving the following maximiza-tion problem: Denoting A q = [(diag( X  y i )  X  i ) ,  X  X  X  , (diag( X  y k ) optimization problem in Eq. (21) can be reformulated as: Similar as in Theorem 4.1, it can be shown that the optimal Q  X  to Eq. (22) is given by the first eigenvectors of ( GG  X G ) + ( GA q A T q G ) corresponding to the largest eigenvalues.
In this section, we propose to learn the optimal uncor-related subspace kernel from a set of pre-specified kernel matrices. We start with the formulation based on the de-pendence maximization.
Following Eq. (19), the uncorrelated subspace kernel learn-ing problem can be formulated as: where  X  G K is restricted to be a convex combination of p specified kernel matrices { G i } p i =1 as: subject to the constraints that  X  i  X  0 ,i =1 ,...,p ,and P performs multiple kernel learning (computation of  X  G K )and subspace kernel learning (computation of Q ) simultaneously. However, this optimization problem is nonlinear and hence difficult to solve directly. In the following, we derive an equivalent formulation for this problem which leads to an efficient algorithm.
The key observation that leads to the equivalent formula-tion is that the optimal Q  X  to Eq. (23) is given by a closed-form function on  X  G K , and thus it can be factored out from the objective function in Eq. (23).

For notational simplicity, we denote the objective function in Eq. (23) as It follows from Theorem 4.1 that, for any fixed (positive semidefinite)  X  G K , the optimal Q  X  maximizing F ( G K subject to the constraint in Eq. (23) is given by the first eigenvectors of it can be verified that
F (  X  G K ,Q  X  )=tr Note that, if  X  rank ues of F (  X  G K ,Q  X  ) as in Eq. (26). It follows that Thus the maximization problem in Eq. (23) can be reformu-lated equivalently as: where  X  G K is constrained as in Eq. (24).
We show that the minimization problem in Eq. (27) can be formulated as a semidefinite program (SDP) [2]. Fol-lowing Definition 1, let H ( y )bedecomposedas H ( y )= L ing variables { t i } k i =1 and following the Schur Complement Lemma [9], we can rewrite the inequalities: as the following generalized inequalities [2]: Let  X  =[  X  1 ,  X  X  X  , X  p ]and r =[ r 1 ,  X  X  X  ,r p ], where The optimization problem in Eq. (23) can be reformulated as the SDP problem given below: The SDP problem in Eq. (30) can be solved by standard optimization solvers such as SeDuMi [24]. However, it may not be scalable to large data set (a large value of n ) due to its positive semidefinite constraints. We propose to reformulate the maximization problem in Eq. (23) as a semi-infinite program (SIP) [11], which can then be solved more efficiently. The SIP problem refers to optimization problems that maximize a functional S ( a ) sub-ject to a system of constraints on a , i.e., s ( a, b )  X  b in some set B . When both the objective function and the constraints are linear, the optimization problem is known as semi-infinite linear program (SILP) [23].

It can be shown (using Lagrangian methods) that the op-timization problem in Eq. (27) is equivalent to the following min-max problem: min where  X  G K is constrained as in Eq. (24), {  X  j } k j =1 the dual variables of the optimization problem in Eq. (27), and L hj is defined as in Eq. (30). Let  X   X  =[  X  1 ,  X  X  X  , X  denote S i (  X   X  )(for i =1 ,  X  X  X  ,p )as
S i (  X   X  )= where r =[ r 1 ,  X  X  X  ,r i ] is defined as in Eq. (30). The opti-mization problem in Eq. (31) can be expressed as: By assuming that  X   X   X  is the optimal solution of Eq. (33), we then have Denote  X  = Eq. (33) can be reformulated as: The optimization problem in Eq. (35) has two optimization variables (  X  and  X  ) with an infinite number of linear con-straints, i.e., one linear constraint for each  X   X  . When there is only one fixed  X   X  , the optimization problem is simplified as the standard linear program (LP).
We propose to use the column generation technique to solve this SILP problem as in [23]. In this technique, the variables  X  and  X  are optimized from a restricted subset of constraints in Eq. (35) and this problem is called the restricted master problem . Constraints that are not satisfied by current  X  and  X  are added successively to the restricted master problem until all constraints are satisfied. For fast convergence of the algorithm, it is desirable to add constraint that maximizes the violation for current  X  and  X  .Thatis, the  X   X  value that solves is desired. If satisfied and  X  and  X  reach their optimal values. Otherwise, this constraint is added to the restricted master problem and the iteration continues.

It follows from the definition of S i (  X   X  ) in Eq. (32) that the probleminEq.(36)canbewrittenas For a fixed  X  G K (determined by the optimal  X  computed from the restricted master problem), the problem in Eq. (37) is an unconstrained convex quadratic program whose optimal solution can be obtained via solving k linear systems of equa-tionsasfollows: After  X   X  =[  X  1 ,  X  X  X  , X  k ] is obtained, the corresponding con-straint is added to the restricted master problem to update the intermediate  X  and  X  . Note that the restricted master problem is a linear program. Thus the proposed algorithm for solving the SILP problem alternates between solving k linear systems and a linear program.
We can further extend the subspace kernel integration for-mulation to the joint learning framework in Section 4.3.
Following Eq. (20), the joint learning framework with sub-space kernel integration can be formulated as follows: min where  X  G K is constrained as in Eq. (24).

The min-max problem in Eq. (39) has optimization vari-ables  X  G K ,Q ,and {  X  i } k i =1 .Forafixed  X  G K and Q timal {  X   X  i } k i =1 can be obtained by solving k independent SVMs. For a fixed {  X  i } k i =1 , following a similar derivation as in Section 4.3, the optimal  X  G  X  K and Q  X  can be obtained by solving the following optimization problem: where  X  A q = [(diag( X  y i )  X  i ) ,  X  X  X  , (diag( X  y k )  X  tion problem in Eq. (40) has the same form as the one in Eq. (23). Hence it can be reformulated as a SDP and a SILP problem following a similar derivation as in Section 5.
In this section, we empirically evaluate the proposed sub-space kernel learning algorithms and conduct sensitivity stud-ies on various parameters of the algorithms. The algorithms are implemented in MATLAB, and the codes are available at the supplemental website 1 .

Seven benchmark data sets are employed in our experi-ments. Five of them are from UCI Machine Learning Repos-itory 2 : satimage, waveform, segment, wine, and USPS. Two of them are gene expression data sets 3 : B. Tumor1 and B. Tumor2. For wine and two gene data sets, we use the entire data sets. For others, we randomly sample 300 data points from each class. All of the data sets are normalized. The statistics of the data set are summarized in Table 1. Table 1: Statistics of the benchmark data sets.

We evaluate the proposed algorithms in terms of classifi-cation error rate (in percentage). The reported error rates are averaged over 20 random partitions of the data sets into training and test sets using the ratio 1 : 1. http://www.public.asu.edu/~jchen74/SKL http://archive.ics.uci.edu/ml http://www.gems-system.org 240 5 . 248  X  0 . 263 5 . 170  X  0 . 478 4 . 502  X  0 . 342 . 112 17 . 039  X  1 . 254 17 . 032  X  0 . 959 16 . 089  X  305 3 . 205  X  0 . 011 3 . 692  X  0 . 265 3 . 301  X  0 . 194 363 3 . 224  X  1 . 318 3 . 141  X  1 . 874 3 . 016  X  1 . 806 060 2 . 116  X  0 . 149 2 . 021  X  0 . 103 2 . 001  X  0 . 056 626 7 . 134  X  0 . 600 7 . 035  X  1 . 364 6 . 067  X  1 . 192 . 164 18 . 124  X  1 . 291 17 . 543  X  1 . 146 16 . 179  X 
Satimage 5 . 831 8 . 527 7 . 082 4 . 416 4 . 063 5 . 927 7 . 782 6 . 981 6 . 845 3 . 456
Waveform 19 . 331 17 . 432 17 . 579 18 . 441 16 . 021 15 . 681 14 . 672 16 . 39 17 . 781 11 . 216
Segment 5 . 489 5 . 286 4 . 878 5 . 674 3 . 571 4 . 796 5 . 694 4 . 204 6 . 265 3 . 033
Wine 7 . 471 3 . 948 3 . 776 3 . 865 3 . 965 8 . 511 3 . 031 4 . 200 2 . 822 2 . 918
USPS 5 . 721 5 . 179 2 . 222 2 . 128 2 . 105 6 . 345 5 . 026 2 . 319 2 . 622 2 . 205
B. Tumor1 7 . 112 6 . 667 6 . 212 6 . 140 5 . 781 6 . 781 6 . 216 6 . 132 6 . 127 5 . 139
B. Tumor2 20 . 833 18 . 750 17 . 708 18 . 125 15 . 625 20 . 917 17 . 625 16 . 625 16 . 667 12 . 458
We perform a comparative study on the proposed sub-space kernel learning algorithms: HSIC (learning via depen-dence maximization), SVM joint (joint learning with SVM), and uHSIC and uSVM joint for learning the uncorrelated subspace kernels accordingly. Our comparative study also includes two baseline algorithms: SVM org (classification with SVM on the original kernels) and SKFE algorithm proposed in [25]. Following [25], we set the subspace dimen-sion as the number of classes in the corresponding data sets. LIBSVM toolbox [3] is used for solving SVM optimization problems in the following experiments.

In data space, we employ the Gaussian kernel: K ( x, x )= exp(  X  x  X  x 2 / X  ). For the UCI data, we apply 5-fold cross-validation to select the best value for the hyperparameter from the set { 10  X  3 , 5  X  10  X  3 , 10  X  2 , 5  X  10  X  2 } X  X  For the gene data, we choose the best  X  from { 10  X  i } 20 In label space, we use H 2 ( y ) in Definition 1 as the kernel function. The obtained subspace kernels are evaluated using SVM. The parameter C for SVM is tuned from the set { 1+ 2  X  i } 9 i =1  X  X  20 + 5  X  i } 16 i =1  X  X  100 + 50  X  i } 18 validation, and the regularization parameters  X  and  X  are tuned from the set { 10 i } 5 i =  X  5 .

We present the average (classification) error rates and the standard deviations of the algorithms in Table 2. From Ta-ble 2, we have the following major observations: (1) the pro-posed algorithms achieve smaller error rates than SVM org and meanwhile they outperform or perform competitively compared to SKFE ; (2) the uncorrelated subspace kernel learning algorithms: uHSIC and uSVM joint perform better than HSIC and SVM joint , respectively, which demonstrates the significance of learning uncorrelated subspace kernels; perform favorably among all the compared algorithms, which gives strong support for our rationale of improving classifi-cation performance by learning subspace kernel and SVM classifiers simultaneously; and (4) uSVM joint achieves the best performance among the six compared algorithms on all data sets except Segment.
We evaluate the proposed multiple kernel learning formu-lations (SILP) in terms of classification error rates. The formulations based on uHSIC and uSVM joint are denoted as HSIC mkl and SVM mkl , respectively. For all of the data sets, we construct four candidate Gaussian kernels with  X   X  { lations are applied to compute optimal linear combinations of the candidate kernels, and the obtained kernel combi-nations are then evaluated using uHSIC and uSVM joint ,re-spectively. Each of the candidate kernels is evaluated by uH-SIC and uSVM joint , and their performance are used as the baseline measures, denoted as HSIC keri and SVM keri (  X  i  X  { 1 , 2 , 3 , 4 } ), respectively.

The experimental results are presented in Table 3. We can observe that HSIC mkl and SVM mkl achieve favorable perfor-mance on all of the benchmark data sets. Specifically, on the data sets: satimage, waveform, segment, and USPS, HSIC mkl and SVM mkl achieves better performance (lower error rates) than the corresponding baseline measures. The improved performance resulting from multiple kernel learn-ing may be due to the existence of some complementary information among different kernels. This demonstrates the effectiveness of incorporating multiple kernel learning scheme into uncorrelated subspace kernel learning. We can also ob-serve that, given the same set of kernel matrices, SVM mkl can achieve smaller error rate than HSIC mkl , showing its en-hanced ability to explore the informative domain knowledge underlying the data by jointly learning subspace kernel and SVM classifier. alities on USPS (right figure) and satimage (left figure) data sets.
We perform sensitivity studies on the various parameters of the proposed subspace kernel learning algorithms. Figure 2: Convergence plots of the iterative proce-dure for solving the min-max optimization problems umn) on the wine and satimage data sets. In each iteration, the objective value of the maximization and minimization problems are denoted as Upper bound and Lower bound, respectively.
 Effect of Subspace Dimension We vary the subspace di-mensionality ( ) from 1 to 15 for the subspace kernel learn-ing algorithms (the proposed formulations and SKFE), and study the corresponding change of classification performance. We use USPS and satimage data sets for this experiments, and the experimental results ( error rates) are depicted in Figure 1. We can observe that, for all of the compared al-gorithms, the error rates decrease with the increase of the subspace dimensionality ( )when is smaller than the num-ber of classes ( k ) in the data. We also observe that the al-gorithms generally achieve t he smallest error rates when is close to k  X  1.
 Algorithm Convergence We study the convergence prop-erty of the iterative procedure for solving the min-max opti-mization problems in SVM joint and uSVM joint on wine and satimage data sets. We plot the change of objective values with respect to iterations (for the minimization and maxi-mization problems separately) in Figure 2. We can observe that the upper bound and lower bound are approaching to each other within a small number of iterations. It follows from the theory for min-max problems in [14] that the it-erative procedure converges to the saddle point of the opti-mization problems. Figure 3: The change of performance for SVM joint and uSVM joint with training iterations on two data sets: Segment (left plot) and satimage (right plot). Subspace Kernel Optimization We study the successive optimization effect on the subspace kernel in the iterative procedure of solving SVM joint and uSVM joint .Aftereach iteration, we evaluate the obtained intermediate subspace kernel in terms of classificatio n error rate. In this experi-ment, we use the data sets: segment and satimage, and the results are presented in Figure 3. We can observe that the algorithms: SVM joint and uSVM joint improve the result-ing classification performance, and generally converge to the best performance within 3 or 4 iterations.
We investigate the computation time of the proposed sub-space kernel learning algorithms, and compare them with SKFE. The average computation time over each data set is presented in Table 4. We can observe that HSIC and uHSIC have comparable computation time, while SKFE re-quires relatively larger amount of computation time. Since SVM joint and uSVM joint involve quadratic programs, they have relatively higher computation cost. It is worth noting that SVM joint and uSVM joint generally achieve the best classification performance among the compared algorithms, while they have higher computational costs. Table 4: Average computation time (in seconds) for the subspace kernel learning algorithms.
 Satimage 3 . 594 3 . 359 15 . 015 21 . 360 4 . 547 Waveform 0 . 359 0 . 375 1 . 422 2 . 297 1 . 251 Segment 3 . 656 2 . 891 17 . 812 31 . 188 7 . 125 B. Tumor1 0 . 016 0 . 016 0 . 063 0 . 078 0 . 234
B. Tumor2 0 . 016 0 . 016 0 . 047 0 . 047 0 . 234
We study the problem of learning subspace kernels for classification. We propose to construct a subspace kernel us-ing the Hilbert-Schmidt Independence Criterion. We show that an optimal subspace kernel can be computed effectively by solving an eigenvalue problem. We further propose a joint framework in which we learn the subspace kernel and the subsequent kernel classifier simultaneously. In addition, we propose to learn uncorrelated subspace kernels to reduce redundant information in the subspace kernel. We extend the proposed formulations to the case when multiple ker-nels are available and need to be combined, following the idea in multiple kernel learning. We show that the integra-tion of subspace kernels can be formulated as a semidefinite program (SDP). To improve the efficiency of the SDP for-mulation, we propose an equivalent semi-infinite linear pro-gram (SILP) formulation which can be solved efficiently. We have conducted experiments on a collection of benchmark data sets. Experimental results demonstrate the effective-ness of the proposed algorithms.

Our subspace kernel integration is based on the uncorre-lated subspace kernel learning formulations. The derivation presented in this paper can not be directly extended to the original subspace kernel learning formulations. We plan to explore this further in the future. We plan to apply the proposed kernel integration formulation to real-world appli-cations involving multiple data sources as in [12, 27]. This research is supported in part by funds from the Arizona State University and the National Science Foundation (NSF) under Grant No . IIS-0612069. [1] G. H. Bakir, T. Hofmann, B. Sch  X  olkopf,A.J.Smola, [2] S. Boyd and L. Vandenberghe. Convex Optimization . [3] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [4] N. Cristianini and M. Hahn. Introduction to [5] N. Cristianini and J. Shawe-Taylor. An Introduction to [6] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. S. [7] K. Fukumizu, F. R. Bach, and M. I. Jordan.
 [8] K. Fukunaga. Introduction to Statistical Pattern [9] G. H. Golub and C. F. Van Loan. Matrix [10] A. Gretton, O. Bousquet, A. J. Smola, and [11] R. Hettich and K. O. Kortanek. Semi-infinite [12] G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, [13] S.Mika,G.R  X  atsch, and K.-R. M  X  uller. A [14] A. Nemirovski. Efficient methods in convex [15] J. Nocedal and S. J. Wright. Numerical Optimization [16] C. H. Park and H. Park. Nonlinear feature extraction [17] R. Rosipal and L. J. Trejo. Kernel partial least squares [18] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels: [19] B. Sch  X  olkopf, K. Tsuda, and J.-P. Vert. Kernel [20] J. Shawe-Taylor and N. Cristianini. Kernel Methods [21] L.Song,A.J.Smola,A.Gretton,andK.M.
 [22] L.Song,A.J.Smola,A.Gretton,K.M.Borgwardt, [23] S. Sonnenburg, G. R  X  atsch, C. Sch  X  afer, and [24] J. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox [25] M. Wu and J. D. R. Farquhar. A subspace kernel for [26] T. Xiong, J. Ye, Q. Li, R. Janardan, and [27] J. Ye and et al. Heterogeneous data fusion and
