 1. Introduction
Nature is inspiring researchers to develop effective and power-ful optimization methods ( Karaboga and Akay, 2009 ). In the past, many optimization methods were adopted to solve various real-world optimization problems which were constrained by the complexities of non-linearity in the model formulation and affected by the increase in the number of constraints and decision variables ( Kumar and Reddy, 2006 ).

Nowadays evolutionary stochastic search methods are very popular for solving optimization problems in the research arena of computational intelligence ( Karaboga and Basturk, 2007 ). The routine feature of meta-heuristic algorithms is the point that they often employ combinations of rules and randomness to imitate natural processes ( Lee and Geem, 2005 ). Although the algorithms do not always ensure the global optimum solution, quite good results in a reasonable computation time are achieved. This is why many researchers have been eager to develop newer tech-niques and improve existing methods over the past years ( Kumar and Reddy, 2006 ).
 particle swarm optimization, tabu search, ant colony optimization, bees X  algorithm, artificial immune system and simulated annealing have been extensively employed for various science and engineer-ing problems. One of the major disadvantages of these algorithms is the fact that most of the meta-heuristic algorithms are successful for solving some certain class of problems. Furthermore, in some cases, although the algorithms show superior performance on low dimensional problems, they cannot preserve their superior perfor-mance on high dimensional cases ( Karaboga and Akay, 2009 ). explained by Holland (1975) and further developed by Goldberg (1989) . GA-based algorithms are global search methods found on concepts from natural genetics and the Darwinian survival-of-the-fittest code. During the past two decades, GA has been studied extensively by many researchers to solve difficult and complicated real-world and engineering optimization problems.
GAs are generally capable in finding good solutions in reasonable amounts of time. However, applying to harder and bigger pro-blems increases the time required to find adequate solutions.
Several papers, book chapters, special issues and books have surveyed GAs literature (e.g. Cheng et al., 1999 ; Coello et al., 2007 ; Nicklow et al., 2010 ).
 memory was originally suggested by Glover (1977) . Details about tabu search can also be found in Glover (1989 , 1990 ), Hertz et al. (1997) and Glover and Laguna (1997) . In the TS method the solution space is explored by moving from a solution to the best solution in a subset of its neighborhood trough different iterations while, to avoid cycling, recently explored solutions are tempora-rily declared tabu or forbidden. Many different models based on
TS have been developed to improve the efficiency of the algorithm and applied to solve optimization problems (e.g. Griinert, 2002 ; Glover and Kochenberger, 2003 ; Gendreau, 2002 ).

The Simulated Annealing (SA) algorithm has been modeled on the annealing of solids in nature. The SA initially, proposed by
Kirkpatrick et al. (1983) as a method/tool for solving single objective combinatorial problems. However, recently there are many reports of SA applications in different fields of engineering problems containing continuous and discrete spaces ( Suman and Kumar, 2006 ).
 An important approach which is based on ants behavior, called Ant Colony Optimization (ACO), proposed by Dorigo et al. (1991) .
The approach has been studied by many researchers, while several new variants have been proposed and applied to solve optimization problems in different areas. Numerous publications related with the applications of ACO models have been presented to the literature ( Blum, 2005 ; Dorigo and Blum, 2005 ; Pedemonte et al., 2011 ; Chandra Mohan and Baskaran, 2012 ).

Inspired by the social behavior of the birds flocking or fish schooling, Kennedy and Eberhat (1995) introduced an evolution-ary computational method named Particle Swarm Optimization (PSO) algorithm. PSO, similar to the other evolutionary computa-tional algorithms like GA, is a population-based interactive method. As the most important swarm intelligence paradigms ( Kennedy et al., 2001 ), PSO is popular and useful to solve various kinds of real-world optimization problems ( Eberhart and Shi, 2001 ); however, a number of PSO variants have been developed to overcome some weaknesses which have restricted wider applications of the standard-PSO. Several survey papers regarding the PSO variants applications have been presented ( Eberhart and Shi, 2004 ; Reyes-Sierra and Coello, 2006 ; AlRashidi and El-Hawary, 2009 ).

Artificial Bee Colony (ABC) presented by Karaboga (2005) inspired by the intelligent behavior of honey bees for seeking a quality food source in nature. The algorithm was originally pro-posed for solving numerical problems; however, the success of the algorithm as a single-objective optimizer has motivated researchers to extend its application to other study areas ( Karaboga et al., 2012 ).
 Recently, a new meta-heuristic technique, namely Harmony
Search (HS) algorithm has been proposed by Geem et al. (2001) , which simulates the improvisation process of musicians. In HS algorithm, solution vectors correspond to the harmony in music and the local and global search schemes correspond to the musician X  X  improvisations ( Lee and Geem, 2005 ). HS is a stochastic search technique without the need of derivative information, and with reduced memory requirement. In comparison with other meta-heuristic methods, HS is computationally effective and easy to implement for solving various kinds of engineering optimization problems ( Mahdavi et al., 2007 ; Omran and Mahdavi, 2008 ).
The algorithm is capable for identifying the high performance regions of solution space in a reasonable run time; however, it is not successful in performing local search in numerical optimiza-tion applications ( Mahdavi et al., 2007 ). Moreover, in the case of problems with large space of decision variables, the likelihood of obtaining the global optimum is considerably reduced. Karaboga and Akay (2009) presented a comparative investigation of the performance of basic Harmony Search, Bees algorithms, and
Artificial Bee Colony algorithm. It was concluded that HS algo-rithm is less efficient than the ABC in solving different optimiza-tion problems.
 The initial development of HS Algorithm was conducted by Geem (2000) , during his Ph.D. studies. Design of water distribution networks was the main aim, while the study covered benchmark optimization, parameter estimation, and the traveling salesman problem (TSP) ( Ingram and Zhang, 2009 ). Since then, a variety of HS models have been adopted to diverse field of problems, such as structural design, Sudoku puzzles, musical composition, medical imaging, heat exchanger design, course timetabling, web page clustering, robotics, water network design, dam scheduling, vehicle routing, energy system dispatch, cell phone network, satellite heat pipe design, and medical physics.

There are several attempts to improve the performance of basic HS algorithm for enhancing solution accuracy and conver-gence rate, like Improved Harmony Search algorithm (IHS) ( Mahdavi et al., 2007 ), global best Harmony Search algorithm (GHS) ( Omran and Mahdavi, 2008 ), self-adaptive global best harmony search algorithm (SGHS) ( Pan et al., 2010b ), novel global harmony search (NGHS) ( Zou et al., 2010 ). All improvements are categorized into two classes by Alia and Mandava (2011) ; the first one is improvement of HS in terms of parameter setting, and the second one is improvement in terms of hybridizing HS compo-nents with other meta-heuristic algorithms. Ingram and Zhang (2009) have classified various modifications of HS in seven categories and briefly explained each category.
 strategy in solving the so called Generalized Oriented Problem. Using nearest cities was proposed in their study. Mahdavi et al. (2007) developed an Improved HS algorithm, denoted as IHS, by introducing a method to dynamically adjust the algorithm computational para-meters (i.e. PAR and bw ). Their algorithm was applied to solve four engineering and four mathematical optimization problems. According to their comparative investigation between obtained results and those from other techniques in the literature, they remarked that the algorithm can find better solutions. Omran and Mahdavi (2008) presented a Global-best Harmony Search algorithm, (GHS) GHS algorithm, by borrowing the concepts from swarm intelligence. They studied the sensitivity of the HS parameters and compared the performance of HS, IHS and GHS on ten continuous optimization functions and six integer programming problems. Both IHS and GHS algorithms could find better solutions, compared with the basic HS algorithm. Coelho and Mariani (2009) , proposed an improved har-mony search (IHS) algorithm based on exponential distribution, for solving Economic Dispatch Problems, which updated the PAR para-meter dynamically. The application of HS and IHS for solving thirteen there. Numerical results show that IHS the algorithm converged to more reasonable results compared with basic HS algorithm. Another new improvement to HS, named (DHS) which was inspired by mutation operator of Differential Evolution (DE) is proposed by Chakraborty et al. (2009) . They replaced the pitch adjustment operation in basic HS with a mutation strategy borrowed from DE algorithm. Inspired by the local version of the particle swarm optimization algorithm, Pan et al. (2010a) proposed the local-best harmony search algorithm with dynamic subpopulations (DLHS) for solving the bound-constrained continuous optimization problems. In DLHS method the Harmony Memory ( HM ) is divided into many sub-harmony memories. New harmonies are independently generated in these small-sized sub-harmony memories, which are regrouped frequently by using a regrouping schedule. A recent variant of HS algorithm was proposed by Wang, Huang (2010) .Themodeltotally replaces bandwidth parameter ( bw ) parameter with a new concept based on using the maximal and minimal values in HM . While the search process is going on, PAR values are dynamically adapted using the modification proposed by Mahdavi et al. (2007) . Kattan et al. training. They proposed a new stopping criterion that is based on the best-to-worst (BtW) harmony ratio in the current harmony memory, and applied the ratio within the existing improved version of HS (proposed by Mahdavi et al. 2007 ). They remarked that the modifica-tion is more proper for ANN training since parameters and termina-tion depend on the quality of the obtained solutions ( Alia and
Mandava, 2011 ). A self-adaptive global best harmony search algo-rithm denoted as (SGHS), was proposed by Pan et al. (2010a) for solving continuous optimization problems. In SGHS, the algorithm parameters, Harmony Memory Considering Rate ( HMCR )and PAR ,are self-adaptive by a learning mechanism and bw bandwidth parameter ( bw ), is dynamically decreased with increasing the generation num-ber. Numerical experiments revealed that SGHS algorithm is more
Zou et al. (2010) developed a novel global harmony search method, called NGHS, for solving unconstrained problems. A novel location updating strategy is designed wh ich makes the algorithm easier to converge. The experimental results showed better performance for
NGHS algorithm for solving most of unconstrained problems com-pared with other harmony search methods (i.e. HS, IHS, and SGHS).
For enhancing the performance of HS, Al-Betar et al. (2010) proposed eight procedures instead of using one PAR value, to solve the Course
Timetabling Problem, which were controlled by their certain PAR value ranges.

There are several hybrid models of HS with other meta-heuristic methods in the literature. Alia and Mandava (2011) categorized this hybridization into two classes. The first class consists of models which are the integration of some components of the other meta-heuristic algorithms within HS, while the second class consists of methods which integrate some HS components within other meta-heuristic algorithms.

More recently, inspired by basic concepts applied in HS algo-rithm, an innovative improved version of HS algorithm is presented by authors ( Ashrafi and Dariane 2011 ). The algorithm named
Melody Search (MS) is designed in accordance with the concept of melody instead of harmony. This algorithm is based on musical performance processes and interactive relations occurred between members of a group of musicians attempting to find better and better series of pitches within a melodic line. In such a group, the music players can improvise the melody differently and lead each other to achieve the best subsequence of pitches. Furthermore, a novel improvisation scheme is introduced and applied in this study through which the efficiency of the algorithm for solving shifted and rotated optimization problems would be increased.
In order to demonstrate the performance of the proposed algorithm, MS with the novel improvisation scheme is applied to various benchmark problems and the results are compared with those of the basic HS, IHS, GHS, SGHS, NGHS and basic MS algorithms.

The remaining of this paper is organized as follows: basic concepts of HS algorithm are explained in Section 2 . Some variants of HS algorithms including IHS, GHS, SGHS and NGHS are briefly described in Sections 3 and 4 . describes the proposed
MS algorithm, and the basic differences between HS and MS methods. Some experimental studies regarding numerical bench-mark problems, along with their analysis and discussions are summarized in Section 5 . Finally Section 6 provides a brief summary and conclusion. 2. Harmony search (HS) algorithm
Harmony search was developed by Geem et al. (2001) , based on mimicking music improvisation process where music players improvise the pitches of their instruments to obtain better harmony.
 1. initialize the problem and algorithm parameters, 2. initialize the harmony memory with the random solution 3. improvise a new harmony vector, 4. update the harmony memory and 5. check stopping criterion and repeat steps 3 X 4.
 parameters are specified; including the harmony memory size ( HMS ) (i.e. the number of solution vectors in harmony memory), harmony memory considering rate ( HMCR ) where HMCR A [0,1], pitch adjusting rate ( PAR ) where PAR A [0,1], bandwidth distance ( bw ) for problems with continuous space of variables or neigh-boring index ( m ) for problems with discrete space of variables, and Maximum number of improvisation (termination criterion, MaxImp )( Lee and Geem, 2005 ).
 randomly generated harmony vectors and relevant objective function values as can be seen in Eqs. (1) and (2) . As a harmony vector, X i  X  { x i (1), x i (2), y , x i ( D ), Fitness i the optimization problem ( D is the number of decision variables); HM  X  where, rand ( ) is a random number between 0 and 1, and [ LB ( j ), UB ( j )] is the entire feasible range of j th variable. against the worst harmony in the memory. Improvising a new harmony is conducted considering the following three rules: 1. memory consideration, 2. pitch adjustment 3. random selection.
 vector are chosen randomly from the existing vectors in the HM with a probability of HMCR . In the randomization, decision variable values are randomly chosen according to their feasible range with a probability of (1 HMCR )( Lee and Geem, 2005 ). The operators are illustrated in Eq. (3) .
Pitches can be adjusted with a probability of HMCR PAR in the pitch adjusting operation as follows; x new  X  x old  X  bw e  X  4  X  where, x old is the existing pitch, chosen from HM , x new pitch after the pitch adjusting operation, bw is the bandwidth distance for continuous problem, and e is a random number in the range of [ 1,1]. Pitch adjusting and randomization increase the diversity of the solution in HS algorithm ( Alia and Mandava, 2011 ).
 is better than the worst one in the Harmony memory, the new harmony is included in the HM and the worst harmony is set aside.
 3. IHS, GHS, SGHS and NGHS Algorithms
In order to perform a good comparison between proposed algorithm and HS variants, algorithms including IHS, GHS, SGHS and NGHS are briefly described in this section. 3.1. Improved harmony search (IHS)
Mahdavi et al. (2007) introduced an improved variant of HS denoted as IHS. In this algorithm PAR and bw parameters, are updated dynamically with increasing generation number, as described in Eqs. (5), (6) and (7) ;
PAR  X  gn  X  X  PAR min  X   X  PAR max PAR min  X  Max Im p gn  X  5  X  bw  X  gn  X  X  bw max exp  X  c : gn  X  ,  X  6  X  c  X  Ln  X  bw min = bw max  X  where MaxImp is the maximum number of improvisation, PAR min and PAR max are the minimum and maximum values of pitch adjusting rate, PAR ( gn ) is the calculated pitch adjusting rate for the gn th generation, bw min and bw max are the minimum and maximum values of distance bandwidth and bw ( gn ) is the obtained distance bandwidth for the gn th generation. Other steps of IHS are the same as the basic HS algorithm. 3.2. Global-best harmony search (GHS)
GHS method, inspired by the particle swarm optimization was proposed by Omran and Mahdavi (2008) . Pitch adjustment operator of HS algorithm is modified in GHS, while the new improvised harmony can consider the best harmony in the memory. Additionally, PAR parameter is determined with a dynamic updating procedure. Except the Improvisation step all other steps of the GHS are the same as the basic HS algorithm. The modified improvisation step is as follows: For each i A [1, y ,D] do If rand () r HMCR then harmony in the memory Else
Done 3.3. Self-adaptive global-best harmony search (SGHS)
Inspired by the GHS algorithm, Pan et al. (2010b) proposed (SGHS) algorithm, which adopts a new improvisation scheme and an adaptive parameter tuning method. According to the applied pitch adjustment rule in the new improvisation scheme, X i assigned to the corresponding decision variable ( X i Best harmony. In order to avoid getting trapped into local optimum solution, a modified memory consideration operator is used in the algorithm.

Furthermore, HMCR and PAR parameters are dynamically updated to a suitable range by recording their historical values corresponding to generated harmonies entering the HM . The value of bw parameter is decreased with increasing generations by a dynamic method.

The computational procedure of the SGHS improvisation can be summarized as follows: For each i A [1, y ,D] do memory
Done 3.4. Novel global harmony search (NGHS)
Inspired by the swarm intelligence, Zou et al. (2010) proposed a different variation of HS named, novel global harmony search. Harmony memory consideration and pitch adjustment are excluded from NGHS and genetic mutation with a low probability is included in the NGHS. Furthermore a position updating tech-nique is proposed and applied in NGHS. The improvisation procedure of the NGHS can be summarized as follows: For each i A [1, y ,D] do
Done where,  X  X  X est X  X  and  X  X  X orst X  X  indicate the best harmony and the worst harmony in the harmony memory, respectively. Note that in the NGHS algorithm, the worst harmony ( X worst )of HM is replaced with the new harmony ( X new ), even if X new is worse than X 4. Proposed algorithm: melody search (MS)
In music, harmony is the use of simultaneous pitches or chords and is often referred to the vertical aspect of music space; while, melodic line refers to the horizontal aspect ( Sturman, 1983 ), as illustrated in Fig. 1 . Melody might be defined as a linear succes-sion of individual pitches, one followed by another one in order, A melody where the composite order of applied pitches forms a single entity. Melodic pitches are not randomly ordered; however, they are subject to basic principles of musical design ( Schoenberg, 1982 ).

In order to enhance the efficiency of HS algorithm introduced by Geem et al. (2001) , a novel improved optimization technique, called Melody Search (MS) was proposed by authors ( Ashrafi and
Dariane, 2011 ). MS algorithm mimics the musical performance processes and interactive relations occurred between members of a musicians group; while, they are looking for the best series of pitches within a melodic line. In such a group, the existing several music players  X  with different tastes, ideas and experiences  X  can lead in achieving the best subsequence of pitches faster. played by music players, and components of the optimization problem. In Melody Search algorithm, each melodic pitch is replaced by a decision variable of the real problem and each melody is replaced by a solution of the optimization problem.
Each music player sounds a series of subsequent pitches within their possible ranges, if the succession of pitches makes a good melody, that experience is stored in the player memory. the structure is quite different. Unlike HS that uses a single harmony memory, MS algorithm employs several memories named Player Memory ( PM ). Memories interact to each other like musicians performance in a group. Furthermore, in the MS Melody (PMS) Pitch 2 algorithm the possible range of each pitch for random selection can be changed going through different iterations. Fig. 3 shows the structure of Melody Memory ( MM ), identified in MS algorithm.
Main body of the algorithm consists of two different computa-tional phases. In the initial phase, each music player can impro-vise his/her melody without the influence of others; while in the second phase, the algorithm acts as a group performance. Existing different melodies in the group of musicians can lead players to select better random pitches and the possibility of composing a better melody is increased in the next step.
 Main steps of MS algorithm are as follows;
Step 1. Initializing the optimization problem and adopting algorithm parameters
Step 2. Initial Phase;
Step 3. Second phase;
The three described steps forms a framework for solving pro-blems through MS algorithm. There are seven major parameters defined in MS algorithm, including number of player memories ( PMN ), player memory size ( PMS ), maximum number of iterations ( NI ), maximum number of iterations for the initial phase ( NII ), distance bandwidth ( bw ), player memory considering rate ( PMCR ) which is identical to HMCR in HS algorithm and pitch adjusting rate. The mentioned parameters are adopted in the first step.
Searching for the best arrangement of pitches in the melody is carried out separately by any of the music players in the second step (the initial phase). In the primary part of the second step the player memories are initialized. In MS algorithm, melody memory ( MM ) consists of several player memories. The player memories matrixes are generated with random initial melodies; as described in Eqs. (8) X (10) .
 MM  X  X  PM 1 , PM 2 , ... , PM PMN  X  8  X  PM i  X  where, D is the number of pitches of melodic line or decision feasible range of the k th pitch or variable which is not changed in the initial phase (Step 2) but it can be changed in the second phase (Step 3).

In the secondary part of Step 2, a new melody is improvised for each PM . In the proposed algorithm, the new melodic line or vector of decision variables from each PM , X i , new  X  X  x i , new  X  , are generated using a novel Alternative Improvisation Procedure (AIP) based on the main concepts of harmony improvisation.

During sub-Step 2.3, the player memories are updated. If the objective function value of the new melody in each PM is better than the worst objective function value, the new melody is included in the specified PM and the existing worst melody is excluded. The latter two steps are repeated while the iteration number is smaller than the maximum iteration number of initial phase.

In the third step (the second phase), the best arrangement of pitches in the melody is searched through an interactive process x x x x PM(...)
Possible range of first variable for next randomization between the music players, while the possible range of pitches are being updated. A new melody is improvised by AIP procedure in sub-Step 3.1 from each PM according to the possible range of pitches which can be varied here through different iterations. Again, the player memories are updated, as done during sub-Step 2.3. Sub-step 3.3 is a main part of MS algorithm and there is a major difference between MS and HS here. In the second phase, the best melody variables of each PM are stored and the new possible variable ranges can be calculated for the next randomization, as described below; For each k A [1, y ,D]do Done
The best -subscript stands for the best melody in any certain player memory. Fig. 4 graphically shows the contributing pro-cesses in Sub-step 3.3. The latter two sub-steps are repeated while the iteration number is smaller than the maximum number of iterations ( NI ). A flowchart for Melody Search algorithm is presented in Fig. 5 . 4.1. The novel alternative improvisation procedure (AIP) proposed in this section. In MS algorithm, melody improvisation is performed for each PM separately. In the memory consideration process, two different rules are alternatively applied while each rule forms a linear combination of a chosen variable from current PM and a proportion of bw . In the first rule, value of each new variable ( x k i , new for k th variable in i th player memory) is generated using a corresponding decision variable from PM one of the decision variables of PM (i.e. x h i , l , l A first rule increases the algorithm convergence, second rule enhances the diversity of generated solutions. Furthermore, in order to avoid getting trapped in a local optimal solution a proportion of bw is adopted in the rules.
 Inspired by the modified pitch adjustment rule proposed by
Pan et al. (2010b) for SGHS algorithm, variables of the best melody in the current player memory are applied exactly in the pitch adjustment process. Finally, random generation is per-formed in the possible ranges of variables which can be varied through different iterations.

The function of proposed Alternative Improvisation Procedure for MS algorithm can be summarized as follows: For each i A [ 1, y , PMN ] do For each k A [ 1, y ,D ] do Done Done
Here, randn ( ) is a uniform random number between 0 and 1, i , best is the k th variable of the best melody in the i th PM .
Parameter PAR t is the pitch adjusting rate of the t th iteration, which is determined by the following equation:
PAR  X  PAR min  X  where, PAR min and PAR max are the minimum and maximum adjusting rates, respectively. The described method of dynami-cally updating PAR was proposed by Mahdavi et al. (2007) for IHS algorithm. bw ( k ) is the specified distance bandwidth for k th variable which is dynamically determined as follows: bw  X  k  X  X   X  UB k LB k  X  200  X  12  X 
Note that, in the proposed MS algorithm the distance bandwidth of each variable can be varied through different iterations while the upper and lower bounds of variables are varied. 5. Experiments
Eighteen classical benchmark functions are considered to evaluate the performance of proposed algorithm. Results of the
MS algorithm with new improvisation scheme (AIP_MS) are compared with the results obtained from basic HS, IHS, GHS,
SGHS, NGHS and basic MS algorithms. 5.1. Test functions
The first function is Sphere function, defined as f  X  x !  X  X  where the global minimum of the function is f Opt 1  X  X n and X *  X  (0.0,0.0, y ,0.0). The initial range for each variable is [ 100,100].

The second function is Griewank function, defined as f  X  x !  X  X  1 where the global minimum of the function is f Opt 2  X  X n X  X  (0.0,0.0, y ,0.0). The initial range for each variable is [ 600, 600].

The third function is Rastrigin function, defined as f  X  x !  X  X  X D where the global minimum of the function is f Opt 3  X  X n and X *  X  (0.0,0.0, y ,0.0). The initial range for each variable is [ 5.12,5.12].

The fourth function is Rosenbrock function, defined as f  X  x !  X  X  X D 1 where, the global minimum of this function is f Opt 4  X  X X  X  (1,1, y ,1). The initial range for each variable is [ 30,30].
The fifth function is Ackley function, defined as f  X  x !  X  X  20  X  e 20exp 0 : 2 where, the global minimum of this function is f Opt 5  X  X X  X  (0.0,0.0, y ,0.0). The initial range for each variable is [ 32,32].

The sixth function is Schwefel function 2.22, defined as f  X  x !  X  X  X D where the global minimum of the function is f Opt 6  X  X n X  X  (0.0,0.0, y ,0.0). The initial range for each variable is [ 10,10].
The seventh function is Schaffer function f6, defined as f  X  x !  X  X  0 : 5  X  where the global minimum of the function is f Opt 7  X  X n and X *  X  (0.0,0.0, y ,0.0). The initial range for each variable is [ 100,100].

The eighth function is Shifted Sphere function, defined as ( Qin et al., 2009 ): f  X  x !  X  X  X D optimum. The global minimum of this function is f Opt 8  X  X and X *  X  O . The initial range for each variable is [ 100,100].
The ninth function is Shifted Schwefel function 1.2, defined as f  X  x !  X  X  X D optimum. The global minimum of this function is f Opt 9  X  X and X *  X  O . The initial range for each variable is [ 100, 100].
The 10th function is Shifted Rosenbrock function, defined as f  X  x !  X  X  optimum. The global minimum of this function is f Opt 10  X  X and X *  X  O . The initial range for each variable is [ 100,100].
The 11th function is Shifted Rastrigin function, defined as f  X  x !  X  X  optimum. The global minimum of this function is f Opt 11  X  X and X *  X  O . The initial range for each variable is [ 5.0,5.0].
The 12th function is Shifted Griewank function, defined as f  X  x !  X  X  optimum. The global minimum of this function is f Opt 12  X  X and X *  X  O . The initial range for each variable is [ 600,600].
The 13th function is Shifted Ackley function, defined as f  X  x !  X  X  20  X  e 20exp 0 : 2 optimum. The global minimum of this function is f Opt 13  X  X and X *  X  O . The initial range for each variable is [ 32,32].
The 14th function is Shifted Rotated Rastrigin function, defined as f  X  x !  X  X  optimum, M is linear transformation matrix and condition number  X  2. The global minimum of this function is f Opt 14 and X *  X  O . The initial range for each variable is [ 5.0, 5.0].
The 15th function is Shifted Rotated Ackley function, defined as f  X  x !  X  X  20  X  e 20exp 0 : 2 optimum, M is linear transformation matrix and condition number  X  1. The global minimum of this function is f Opt 15 and X *  X  O . The initial range for each variable is [ 32,32].
The 16th function is Shifted Rotated Griewank function, defined as f  X  x !  X  X  optimum, M is linear transformation matrix and condition number  X  3. The global minimum of this function is f Opt 16
The 17th function is Shifted Expanded Griewank plus Rosen-brock function, defined as ( Suganthan et al., 2005 ). f  X  x !  X  X  f 4  X  f 2  X  z 1 , z 2  X  X  X  f 4  X  f 2  X  z 2 , z 3  X  X  X  ... f 4 : Griewank function optimum. The global minimum of this function is f Opt 17  X  X and X *  X  O . The initial range for each variable is [ 3.0,1.0]. function, defined as f 18  X  x !  X  X  f 9  X  z 1 , z 2  X  X  f 9  X  z 2 , z 3  X  X  :::  X  f f 9 : Schaffer f 6 function optimum, M is linear transformation matrix and condition number  X  3. The global minimum of this function is f Opt 18 function and Schwefel X  X  problem 2.22, are unimodal. Rosenbrock function is a unimodal function with a narrow parabolic shaped valley from the perceived local optima to the global optimum.
Some researchers believe that this function can be also treated as a multimodal function ( Liang et al., 2006 ). Rastrigin, Ackley,
Schaffer f6 and Griewank functions are well-known as difficult nonlinear multimodal functions ( Digalakis and Margaritis, 2002 ).
The main characteristic of these functions is that the number of local optima increases exponentially with the problem dimension the global optimums are shifted to a random position; conse-quently, the global optimum positions have different numerical values for different dimensions (Liang et al., 2005). Functions f f 15 , f 16 and f 18 are shifted Rotated functions; while f expanded ones. Table 1 briefly shows the characteristics of the functions. 5.2. Common experimental settings all benchmark functions are tested with dimensions 30 and 50.
The computational parameters of HS, IHS and GHS algorithms are set to values recommended in ( Omran and Mahdavi, 2008 ). The harmony memory size ( HMS ) is equal to 5, HMCR  X  0.9, PAR  X  0.3 for HS, PAR min  X  0.01 and PAR max  X  0.99 for IHS and GHS methods, bw  X  0.01 for HS and bw min  X  0.0001 and bw max  X  (UB LB)/20 for IHS algorithm. For SGHS, according to the values which are recommended in ( Pan et al., 2010b ), main parameters of the algorithm are set as follows; HMS  X  5, HMCRm  X  0.98, PARm  X  0.9, bw min  X  0.0005 and bw max  X  (UB-LB)/10 . For NGHS, the algorithm parameter values applied by Zou et al. (2010) are adopted here and NGHS parameters are set as HMS  X  5, P m  X  0.005. In order to equalize the maximum number of evaluations performed in all algorithms, the maximum number of iteration ( NI ) is 50000 for all test cases. Parameters of basic MS algorithm are adopted as;
PMS  X  5, PMN  X  5, PMCR  X  0.98, PAR min  X  0.01, PAR max  X  0.99, bw  X  0.0 in initial phase and bw  X  5E 5 in second phase ( Ashrafi and Dariane, 2011 ).

In MS algorithm the PMN is equal to the number of evaluations performed in the each iteration of the algorithm. Therefore, the maximum numbers of iterations ( NI ) of proposed algorithm (AIP_MS) MS algorithm for different PMN can be calculated as follows; NI  X  Total Evaluations
The values of the other parameters of the algorithm are chosen to be equal to PMN  X  5, PMS  X  5, PMCR  X  0.98, PAR min  X  0.01,
PAR max  X  0.99, and bw k  X  ( UB k -LB k )/200 for the k th variable. NII parameter is determined by sensitivity analysis carried out for each test function. 5.3. Experimental results and discussion
For each test case, 30 trials were carried out using different random seeds and the average optimal values, standard devia-tions, maximum and minimum values of the results as well as success rates are presented in Tables 2 X 5 . The best results are specified in bold. In the results illustrations, Basic_MS refers to the primary MS algorithm introduced by Ashrafi and Dariane (2011) and the AIP_MS refers to the proposed algorithm in this study. The obtained results of centered functions (i.e. f are summarized in Tables 2 and 4 for dimensions 30 and 50 respectively. In most real-world problems, numerical values of variables in global optimum are not the same in different dimensions. In order to evaluate the performance of proposed algorithm in such problems, eleven shifted functions ( Suganthan et al., 2005 ) are applied in this study while the obtained results are compared with those of HS, IHS, GHS, SGHS, NGHS and basic MS algorithms. Tables 3 and 5 present the results of Shifted functions for dimensions 30 and 50, respectively.
 Tables 2 and 4 indicate that although the performance of basic
MS algorithm is slightly better than the proposed algorithm in optimizing Ackley function with dimension 30, AIP_MS algorithm shows much better performance for all centered test functions with dimensions 30 and 50. For Sphere, Griewank, Rastrigin and
Schwefel 2.22 functions, the obtained results from AIP_MS algo-rithm are significantly better than the ones from other methods.
For Griewank and Rastrigin functions the proposed algorithm is capable for finding the global optimum.

Based on the results presented in Table 3 for dimensions 30, it is clear that AIP_ MS algorithm produced better solutions for f f , f 12 , f 15 , f 17 and f 18 functions as compared with the other algorithms; where, the entire feasible ranges of decision variables for Shifted Griewank ( f 1 2 ) are wider than those for other func-found by SGHS algorithm while SGHS cannot preserve the accuracy of the results for dimension 50 of f 9 and f 14 . The NGHS algorithm could produce the best results for f 8 and f 16 dimensions.

The results presented in Table 5 for dimension 50; show that the proposed algorithm is superior over other abovementioned algorithms for all test functions except for Shifted Sphere and
Shifted Ackley functions. For Shifted Sphere function the best result was found by NGHS and for Shifted Ackley function, SGHS could produce the best result; however, AIP_MS algorithm could find appropriate results. Therefore, capability of MS algorithm for solving multimodal optimization problems with wide range of variables is presumable, especially in high dimensional problems. The results also indicate that the performance of AIP_MS algo-rithm is less sensitive to the increase of problem dimensions as compared to the other methods. Figs. 6 and 7 present typical solution history graphs along fitness evaluations for f 1 f , f 12 , f 13 , f 14 and f 16 functions with 50 dimension.
Maximum number of iterations for the initial phase, (para-meter NII ) for all test functions and all dimensions was deter-mined by sensitive analysis. The best NII values for centered test cases were obtained between 3% and 15% of maximum number of iterations and for shifted and rotated test cases were obtained between 45% and 92% of maximum number of iterations.
 Although determining the exact range or mathematical relation-ship for this parameter requires further investigations, the obtained results show that the maximum number of iterations in the initial phase for AIP_MS algorithm must be increased for finding appropriate results for shifted and rotated functions. The mean of functions X  fitness values versus different values of maximum iteration numbers in the initial phases are shown in , f , f 11 , f 12 and f 13 ) functions ( D  X  50), respectively. 5.4. Statistical tests and effects of parameters
Mann X  X hitney U tests are conducted to show whether the proposed algorithm results are statistically different from those obtained by the other aforementioned algorithms. The Mann X 
Whitney U test ( Mann and Whitney, 1947 ) is a non-parametric statistical rank-based test for considering differences between independent populations.
 smaller value of U 1 and U 2 is the one used when consulting significance tables ( Zou et al., 2010 ). Six groups of Mann X  X hitney
U tests are performed applying all eighteen test functions with 50 dimensions, while 30 independent experiments are carried out in each case, and the results are presented in Table 6 . all test functions are significantly smaller than the U HS U values. Thus it is safely concluded that the performance of AIP_MS is statistically better than HS, IHS and GHS algorithms. Table 6 shows that the values of U AIP_MS are smaller than those of U SGHS for all test functions, except for Shifted Sphere and Shifted
Ackley functions. It is consequently revealed that the proposed algorithm is more effective than SGHS for solving all test cases except those two functions. Although the efficiency of AIP_MS in case of Shifted Schwefel 1.2 and Shifted Rotated Griewank functions is better than the SGHS, there are no statistically significant differences between results of two compared algo-rithms. As can be seen from Table 6 , the values of U AIP_MS clearly smaller than those of U NGHS except for Shifted Sphere function. In other words, AIP_MS has obtained better solutions compared to the NGHS algorithm which indicates that the proposed algorithm outperforms NGHS. For centered functions ( f  X  f 7 ) the numbers of AIP_MS solutions beaten by HS, HIS, GHS, SGHS and NGHS solutions are 0. It can be seen that the proposed method produces significantly better results than basic MS for all test cases. For Schaffer function f6, there are no significant differences between results of AIP_MS and basic MS algorithm.
In order to evaluate the influence of PMN value on the performance of AIP_MS algorithm, four PMN values (i.e. 2, 5, 10, 1E-121 Log (Functions Value) 1000000 1E+09 Log (Functions Value) Functions Value 100000 1000000 1E+09 1E+12 Log (Functions Value) Functions Value Log (Functions Value) Fitness Evaluations Functions Value and 20) were adopted for each test case with 50 dimensions.
Table 7 represents the results generated by using different PMN values for all test cases with a specified maximum number of iteration ( NI  X  10,000). Although for centered functions the best results are obtained by using medium values of this parameter (e.g. PMN  X  5 or 10), for shifted functions the bigger PMN values reach better results. Note that, in these experiments, increasing the PMN value increases the total number of function evaluations Log (Function Function Fitness Log (Function Function Fitness Function Fitness Function Fitness Function Fitness Function Fitness
Function Fitness Function Fitness ( NoFE ). The results generated by using different PMN values and a specified number of function evaluations ( NoFE  X  50,000) for all test cases are presented in Table 8 . In these cases increasing the
PMN value decreases the total number of iterations ( NI ). Totally it is presumable that, the best results are produced applying medium values (e.g. PMN  X  5) for this parameter. From
Tables 7 and 8 it is presumable that the algorithm is not significantly influenced by this parameter for solving f 17 functions. From the obtained results using medium values (e.g. 5 or 10) for PMN parameter is recommended. It can be seen that choosing low values for this parameter (i.e. PMN  X  2) reduces the algorithm performance for all test cases. Actually, since the algorithm simulates the group performances and PMN is the number of the group members, using too small PMN would not be a good choice.

Table 9 summarizes the calculated results with different values of PMCR parameter using PMN  X  5, PMS  X  5 and
NI  X  10,000 for all functions with 50 dimensions. The NII values are determined by sensitive analysis for each test case. From the calculated results it is revealed that an increase in PMCR value enhances the algorithm performance for all test functions except for Sphere and Schwefel 2.22 functions which found the best results with PMCR  X  0.9. Griewank and Rastrigin functions are capable to produce global optimum using PMCR values greater than 0.9. Finally using large values (i.e. 4 0.9) for PMCR is recommended generally.

Table 10 presents the results generated by using different PMS values for all test functions with 50 dimensions. For all test cases it is assumed that NI  X  10,000, PMN  X  5, PMCR  X  0.98 and NII values are equal to the best values determined by sensitivity analysis.
For Shifted Rotated Rastrigin function using PMS  X  2 might not be a good choice. For Rastrigin and Griewank functions, all values of
PMS result in the global optimum solution. Calculated results of f f ,f
PMS are in the same order of accuracy. Generally it may be concluded from Table 10 that using middle values of PMS (i.e. 5 and 10) is preferred to using other values for all benchmark problems; while reducing space requirements is beneficial. Since PMS resembles the capacity of musician X  X  short term memory, it is reasonable to adopt small values for PMS , but not too small. 6. Conclusion This paper introduced a novel improved version of Harmony Search optimization algorithm called Melody Search (MS). Prin-ciples of the new algorithm were described and similarities and differences were explained comparing with basic HS. Moreover, a novel alternative improvisation procedure (AIP) was proposed in this study. In order to demonstrate the performance of MS algorithm with the new improvisation scheme (AIP_MS), it was tested using eighteen high dimensional numerical well-known functions. The obtained results were compared with those of basic HS, IHS, GHS, SGHS, NGHS and basic MS algorithms. Based on the experimental results, it is concluded that the proposed algorithm is more effective in finding better solutions comparing with the aforementioned algorithms. Furthermore, the main advantage of AIP_MS algorithm is that the algorithm can better preserve the accuracy of the results comparing with other described methods in the case that the dimensionality of the problem or the entire feasible range of the search space is increased.

Further research on issues, such as the investigation of the control parameters X  effects on the performance of MS algorithm and the convergence speed of the algorithm in different condi-tions, is sought in our future research.
 References
