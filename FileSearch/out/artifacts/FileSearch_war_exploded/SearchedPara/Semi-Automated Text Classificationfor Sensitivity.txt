 Sensitive documents are those that cannot be made pub-lic, e.g., for personal or organizational privacy reasons. For instance, documents requested through Freedom of Informa-tion mechanisms must be manually reviewed for the presence of sensitive information before their actual release. Hence, tools that can assist human reviewers in spotting sensitive information are of great value to government organizations subject to Freedom of Information laws. We look at sensi-tivity identification in terms of semi-automated text classifi-cation (SATC), the task of ranking automatically classified documents so as to optimize the cost-effectiveness of human post-checking work. We use a recently proposed utility-theoretic approach to SATC that explicitly optimizes the chosen effectiveness function when ranking the documents by sensitivity; this is especially useful in our case, since sen-sitivity identification is a recall-oriented task, thus requiring the use of a recall-oriented evaluation measure such as F We show the validity of this approach by running exper-iments on a multi-label multi-class dataset of government documents manually annotated according to different types of sensitivity. Government documents may be deposited in archives for public viewing after a period of years, or released into the public domain through Freedom of Information mechanisms. However, documents containing sensitive information should not be released, as they may reveal personal information, thereby infringing on someone X  X  privacy, or reveal informa-tion that may offend other countries.

Classically, for paper documents, the identification of sen-sitive documents has taken place using human reviewers. However, with limited government budgets, the adoption of text classification techniques that aid in the identification of F abrizio Sebastiani is currently on leave from Consiglio Nazionale delle Ricerche, Italy.
 c sensitive documents is attractive, since it can increase the efficiency of human reviewers. The possibility of treating sensitivity review as an automated text classification task has recently been shown in [7], where text classifiers were used in order to automatically detect sensitive documents, and where X  X ensitive X  X an have different interpretations (e.g., defence-related issues, or issues related to law enforcement).
The task of sensitivity identification bears strong resem-blances with  X  X eview for privilege X  in e-discovery [8], where expert attorneys must check that  X  X rivileged X  (i.e., sensi-tive) information is not accidentally disclosed to a request-ing party in the context of a civil litigation process [3, 10]. Another task that bears resemblances with sensitivity iden-tification is record anonymisation, as when e.g., medical records have to be anonymised before they are released for epidemiological studies; in this case, sensitive information such as patients X  names and medical doctors X  names have to be spotted in order to be redacted [9]. Sensitivity iden-tification and privilege identification are text classification tasks, while record anonymisation is an information extrac-tion task. Notwithstanding the differences, all these cases are characterized by the fact that the costs of accidental disclosure of sensitive information are high.

In this paper we follow in the steps of [7] and investigate automatic techniques for aiding sensitivity review. How-ever, while [7] was concerned with automatically classifying documents by sensitivity, here we are concerned with aiding a human annotator who validates (i.e., inspects and cor-rects where appropriate) these automatically classified doc-uments, with the goal of maximizing the cost-effectiveness of the annotator X  X  work. In other words, while [7] was con-cerned with  X  X tep 1 X  in the workflow, we tackle  X  X tep 2 X .
We frame the task of aiding our human annotator as a semi-automatic text classification (SATC) task. SATC (see [2, 5, 6]) is defined as the task of ranking a set D of auto-matically classified textual documents in such a way that, if a human annotator validates the documents in a top-ranked portion of D with the goal of increasing the overall classifi-cation accuracy of D , the expected increase in accuracy is maximized. Therefore, we envisage our annotators as vali-dating documents by sensitivity, starting from the top of the ranked list we generate, and working downwards (until they are confident that the dataset has been cleared up, or until the budget for annotation work has been spent).

We approach SATC by adopting the utility-theoretic ap-proach of [2] (hereafter: U-Theoretic ). Essentially, U-Theo-retic ranks the automatically labelled documents by taking two factors into consideration, i.e., 1. the probability that the document has been misclassi-2. the increase (or gain ) in the overall accuracy of the Concerning (2), while the same gain occurs from validating either a true positive (TP) or a true negative (TN)  X  this gain is 0, since the human annotator will not change their labels  X  the gain that occurs from validating a false positive (FP) or a false negative (FN) may be, as shown in [2], differ-ent. When the two gains are different, the utility-theoretic approach tested in this paper: Note that the gains from validating TPs and TNs are differ-ent (as shown in Equations (3) and (4) below) when accuracy is measured via functions such as F 1 (which pays equal im-portance to precision and to recall). However, this is even more true when using metrics that give more importance to recall than to precision (or vice versa), since the imbal-ance between the gains deriving from validating a FP or a FN is even higher. This is indeed our case, since sensitiv-ity identification is a recall-oriented task (there is a higher cost involved in missing a sensitive document than in erro-neously catching a non-sensitive document), which means that a measure (such as e.g., F 2 ) that emphasizes recall over precision needs to be adopted.

Hence, in this paper, our application of the utility-theoretic approach for semi-automated text classification to the recall-oriented task of sensitivity identification, brings two contri-butions: (i) it verifies the utility-theoretic approach on the more difficult task of sensitivity identification, and (ii) it moves the state-of-the-art in sensitivity identification from automatic text classification to an assistive approach that can benefit the efficiency of an annotator examining docu-ments for sensitivities. The rest of the paper is structured as follows. In Section 2 we describe our approach to top-ranking sensitive information via semi-automated text clas-sification. Section 3 describes our experiments carried out on a dataset of documents annotated according to different types of sensitivity. Section 4 provides concluding remarks. The U-Theoretic approach described in [2] tackles SATC in a  X  X ulti-label multi-class X  context, i.e., there is a set of classes problem) and each document d i may belong to zero, one, or several among the classes in C (this makes it a  X  X ulti-label X  problem). The U-Theoretic approach assumes that binary classifiers h j , one for each c j  X  C , have classified the set of unlabelled documents (which, for the purpose of our ex-periments, will here be equated with the test set T e ), also returning a confidence score for each classification decision; the binary decisions returned by classifier h j will be collec-tively denoted as h j ( T e ).
 U-Theoretic ranks documents in terms of total utility , i.e., where U j ( d i ) ( class-specific utility ) is defined as: Here, P (  X  i j ) is the system X  X  estimate of the probability that event  X  i j occurs, tp i j is the event  X  d i is a true positive for c  X , and fp i j , fn i j , tn i j are defined similarly. If the classifier is a probabilistic one, the P (  X  i j ) X  X  are the posterior prob-abilities directly generated by the classifier (e.g., if d positive example of c j , then P ( tp i j ) is P ( c j | d not a probabilistic one, these probabilities can be obtained from the confidence scores output by the classifier via the application of a generalized logistic function (see [2, Section 3.3] for details).

In Equation (2), G (  X  i j ) is the gain , defined as the av-erage increase in the value of the evaluation function that derives when an event of type  X  i j occurs, where the average is computed across all documents of the same type (e.g., if  X  j  X  fp i j , the average is computed across all of the false pos-itives for c j ). For instance, we take G ( fp i j ) to be the average increase in the accuracy of T e that derives by correcting a false positive for c j  X  i.e. removing from T e a false positive and adding to T e a true negative  X  calculated as:
G ( fp i j ) = where by T P j we indicate the number of true positives for class c j (and analogously for FP j , FN j , T N j ).We assume F 1 to be the chosen evaluation function, where F 1 ( h j ( T e )) denotes the value of F 1 computed on h j ( T e ), and where by F 1 ( h j ( T e )) we indicate the value of F 1 that would derive by correcting all false positives in h j ( T e ) (i.e., turning all of them into true negatives). Similarly, we take G ( fn i j ) to be:
Here, the values of T P j , FP j , FN j , T N j are unknown (since in a real application the true labels of the examples we have automatically classified are unknown), but may be estimated via k -fold cross-validation (see [2, Sect. 3.4] for details).
Note that the method generates a single ranking (accord-ing to the document scores computed via Equation (1)), and not |C| different ones; this allows the human annotator to scan the document list only once, validating a document for all the classes in C before moving on to the next document. I n this section we report on experiments that we have con-ducted in order to test how well U-Theoretic performs on the task of sensitivity identification. For our experiments we have used the same dataset as used in [7]. This dataset contains 1,111 government records sam-pled from a larger corpus of documents addressing interna-tional relations. Unlike the rest of the corpus, which is still unlabelled, these 1,111 records have been manually labelled by government experts according to two types of sensitivity namely  X  X ection 40 X , which deals with the occurrence of per-sonal information and X  X ection 27 X , which describes material damaging to international relations. Of the 1,111 judged records, 104 were judged to be sensitive for Section 27 and 86 for Section 40 (see [7] for more details on this dataset).
For each of the 1,111 records we use the same features em-ployed in [7] for text classification purposes. In particular, we use the contents of the document represented as a vector of term frequencies, along with four other features, namely the presence of dictionary last names, presence of interna-tionally famous persons as listed in DBpedia (e.g., Royal Family, government leaders, etc.), number of subjective sen-tences as identified by the OpinionFinder toolkit [11], and a risk score for the record depending on the countries men-tioned within the record. Again, see [7] for more details on the feature representations used.

As a measure of classification accuracy we use F 2 , an in-stance of the well-known F  X  function defined as: We adopt F 2 since (as already observed in the introduction) ours is a recall-oriented task, and F 2 is a standard choice for this kind of tasks. Note that, as a consequence, in Equa-tions (3) and (4) we use F 2 (and its definition from Equa-tion (5)) in place of F 1 ; in other words, for us a gain G (  X  is defined in terms of increases in F 2 (and not F 1 ) obtained from the annotator X  X  validation activity.
 As the learning algorithm we use SVMs, c.f. Joachims X  SVM-light implementation [4]; in all our experiments we use a linear kernel. Due to the imbalance of the training data, we oversample each training set by generating duplicates of sensitive records until we obtain the same numbers of sensitive and non-sensitive records in the training set. This solution (see e.g., [1]) turns out to be effective in improving the SVM performance despite its simplicity, especially when the number of the minority class examples is small. (Initial experiments we have performed without oversampling have yielded radically worse accuracy results.)
We perform our experiments using repeated random sub-sampling validation, i.e., (a) we generate multiple (in our case: 10) random training / test splits of the original dataset (in our case: always using 80% of the data for training and 20% for testing), (b) for each such split we train our classi-fiers on the training set and (c) classify + rank the test data using the trained classifier, and finally (d) we compute the final effectiveness results as the average effectiveness across the different splits.

For each of the 10 splits, as a substep of step (b) we per-form parameter optimization on the training data. We do this by first optimizing (via 10-fold cross-validation, and in-dependently for each class) the C parameter of SVMs, which determines the tradeoff between the training error and the margin. Then, using the value of C deemed optimal we optimize the parameter of the generalized logistic function (used for calibrating the P (  X  i j ) needed in Equation (2)  X  see Section 2) via a second 10-fold cross-validation phase (in this phase we also obtain the estimates of T P j , FP j , FN T N j which are needed for computing G ( fp i j ) and G ( fn Equations (3) and (4)). The test sets are then classified and ranked by employing the parameter values deemed optimal.
In order to evaluate the effectiveness of our approach, we use the ENER M  X  (  X  ) ( X  X xpected normalized error reduc-tion X ) measure proposed in [2]. ENER M  X  (  X  ) measures the reduction in classification error (of the automatically la-belled document set) obtained when a human annotator validates (i.e., inspects and corrects where appropriate) the top-ranked documents in the ranking generated by ranking method  X  ; here, the M superscript stands for  X  X acroaver-aging X , to indicate that the error is computed for each class separately and the results are then averaged. ENER M  X  (  X  ) is based on a probabilistic user model, and  X  represents the ex-pected percentage of the ranking that the human annotator inspects; for instance, ENER M  X  (0 : 10) examines a scenario in which the expected number of documents that the user validates is one tenth of the entire automatically labelled set.
In ENER M  X  (  X  ),  X  X lassification error X  X ay be measured ac-cording to any desired measure of error; differently from [2], which used (1  X  F 1 ), we obviously use (1  X  F 2 ). The values on which ENER M  X  (  X  ) ranges are a strict subset of [0,1], with better rankings corresponding to higher values of ENER M  X  (  X  ). We refer the reader to [2, Section 4] for more details on ENER M  X  (  X  ).

We follow [2] in using, as the baseline for our experiments, a probabilistic (as opposed to utility-theoretic) system that ranks documents according to the probability of misclassifi-cation only, i.e., the documents most likely to be misclassi-fied are top-ranked. This is obtained by using U-Theoretic with both G ( fp i j ) and G ( fn i j ) set to 1. This is a  X  X ower bound X  baseline, which we expect the U-Theoretic approach to outperform. Along with [2], we also report two  X  X pper bound X , idealised baselines (named Oracle1 and Oracle2 ), that we expect our system to underperform; both baselines are versions of the U-Theoretic approach endowed with fore-knowledge ( Oracle1 has foreknowledge of the true values of T P j , FP j , FN j , T N j and can thus compute G ( fp i G ( fn i j ) precisely, while Oracle2 even has foreknowledge of the true labels in the test set, and can thus use binary val-ues in place of probabilities in Equation (2)). These two  X  X pper bound X  baselines thus indicate how far U-Theoretic is from the ideal performance. The results of our experiments are reported in Table 1, for three different values of  X  . As an example, the result ENER M  X  (0 : 05) = 0 : 215 obtained by U-Theoretic can ap-proximately be interpreted as saying that, when using this ranking method, by only validating 5% of the automatically classified documents an annotator is expected to obtain a Table 1: Results of different ranking methods in t erms of ENER M  X  (  X  ) , for  X   X  { 0 : 05 ; 0 : 10 ; 0 : 20 } , with F as the measure of classification error. Improvements are relative to the baseline. reduction in classification error (measured in terms of (1-F ) ) strictly higher than 0.215 2 .

The first interesting fact we may observe is that the utility-theoretic method substantially outperforms the baseline, with achieved improvements with respect to it ranging from +3% to +14%. A second interesting fact is that the results of the utility-theoretic method are very close to the results of Ora-cle1 , an idealized method that has foreknowledge of the true values of T P j , FP j , FN j , T N j ; this means that U-Theoretic does a good job in estimating these quantities.
 Note that the improvements obtained by U-Theoretic over Baseline are lower than the ones (even exceeding +100%) reported in [2] for training sets of comparable size. The likely reason is that classification by sensitivity is much harder than classification by topic (which is the task [2] tackles), as shown by the fact that the SVM classifiers of [2] obtain (for training sets of comparable size) F 1 = 0 : 527 while our SVM classifiers here obtain (as an average across the 10 test sets) F 2 = 0 : 213. We conjecture that a classifier (such as ours) that obtains lower classification accuracy also generates less reliable confidence scores, which are the key input to U-Theoretic . We can thus expect U-Theoretic to achieve even bigger margins on the probabilistic baseline once training sets are larger and (as a consequence) both the classification accuracy and the quality of the confidence scores are higher.
While the improvements we have obtained are substantial, there are reasons to believe that in realistic applications, characterized by much larger unlabelled sets than the one we had access to, the improvements would be even larger; in fact, as [2] shows, the larger the set to be ranked, the more U-Theoretic shines with respect to the baseline. The sensitivity review of  X  X igital-born X  textual records is an important process for Freedom of Information initiatives at the heart of the open-government agenda. Framing sensi-tivity identification as an automatic text classification task is challenging, as noted in previous work [7]. For this rea-son, assistive techniques that can aid a sensitivity reviewer  X  such as semi-automatic text classification (SATC)  X  play an important role. In this work, we investigate the applica-bility of the utility-theoretic approach for SATC, previously proposed in [2]. Our experiments indicate that, when rank-ing documents in order to maximize the cost-effectiveness of human post-inspection work for sensitivity identification, there are clear benefits to gain from using a utility-theoretic approach instead of the purely probabilistic approach. does not measure the  X  X bsolute X  reduction in error, but a normalized version of it where we factor out the reduction in error (equal to 5%, i.e., 0.050) we would obtain by using a random ranker. The improvement obtained by U-Theoretic is thus 0.215+0.050=0.265. [1] G. E. Batista, R. C. Prati, and M. C. Monard. A [2] G. Berardi, A. Esuli, and F. Sebastiani. A [3] M. Gabriel, C. Paskach, and D. Sharpe. The challenge [4] T. Joachims. Making large-scale SVM learning [5] M. Martinez-Alvarez, A. Bellogin, and T. Roelleke. [6] M. Martinez-Alvarez, S. Yahyaei, and T. Roelleke. [7] G. McDonald, C. Macdonald, I. Ounis, and T. Gollins. [8] D. W. Oard and W. Webber. Information retrieval for [9] G. Szarvas, R. Farkas, and R. Busa-Fekete.
 [10] J. K. Vinjumur, D. W. Oard, , and J. H. Paik. [11] T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
