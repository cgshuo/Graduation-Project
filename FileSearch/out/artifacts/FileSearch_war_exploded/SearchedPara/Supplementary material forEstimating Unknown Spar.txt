 Proof of Proposition 1. To prove the implication (i) , we calculate Hence, if the left hand side is at most  X  , we must have T  X  s ( x ) To prove the second implication, note that T  X  c log( p ) k x k 2 1 is the same as Combining this with line (2) proves (ii) . Proof of Theorem 1. Define the noiseless version of the measurement y i to be and let the noiseless versions of the statistics b T 1 and b T 2 be given by It is convenient to work in terms of these variables, since their limiting distribu-tions may be computed exactly. Due to the fact that y  X  1  X  k x k sample from the standard Cauchy distribution C (0 , 1), the asymptotic normality of the sample median implies et al., 2007, Lemma 3). Similarly, the variables ( y sample from the chi-square distribution on one degree of freedom, and so it follows from the delta method that on any model parameters. It is for this reason that the limits hold even when the model parameters are allowed to depend on n . We conclude from the limits (5) and (6) that for any  X   X  (0 , 1 / 2), and Consequently, if we note that  X  0  X  k x k k ( y n The proof may now be completed by assembling the last several items. Recall the parameters  X  n and  X  n , which are given by following asymptotic bounds for the statistics b T 1 and b T 2 , and Due to the independence of b T 1 and b T 2 , and the relation we conclude that The proof of Theorem 2 is almost the same as the proof of Theorem 1 and we omit the details. One point of difference is that in Theorem 1, the bounding probability 1  X  2  X  in the case of Theorem 1.
 in the deterministic case. The idea is that for any measurement matrix A , it is possible to find two signals that are indistinguishable with respect to A , and yet giving the proof of the lemma. Lemma 1. Let A  X  R n  X  p be an arbitrary matrix, and let x  X  R p be an arbitrary signal. Then, there exists a non-zero vector  X  x  X  R p satisfying Ax = A  X  x , and suffices to lower-bound the second ratio. The overall approach to finding a dense columns are an orthonormal basis for the null space of A , where r = rank( A ). Also of z . We begin the argument by defining the function where probability. To see this, notice that the event { f ( z ) &gt; 0 } is equivalent to upper-bounding the expected value of k x +  X  Bz k  X  .
 implies Using this in conjunction with the inequality (18), and the fact that r = rank( A ) is at most n , we obtain the lower bound g in van der Vaart and Wellner van der Vaart &amp; Wellner (1996), the inequality a standard bound for the expectation of Gaussian maxima which follows from a modification of the proof of Massart X  X  finite class lemma (Mas-sart, 2000, Lemma 5.2) 1 . Combining the last two steps, we obtain Finally, applying the bounds (19) and (20) to the definition of the function f in (17), we have which proves E [ f ( z )] &gt; 0, as needed.
 Proof of Theorem 3. We begin by making several reductions. First, it is enough to show that To see this, note that the general inequality s ( x )  X  p implies and we can optimize over both sides with p being a constant. Next, for any fixed matrix A  X  R n  X  p , it is enough to show that as we may take the infimum over all matrices A without affecting the right hand is replaced with any smaller set, as this can only make the supremum smaller. In where by Lemma 1, we may choose  X  x and x  X  to satisfy Ax  X  = A  X  x , as well as We now aim to prove that and we will accomplish this using the classical technique of constructing a Bayes procedure with constant risk. For any decision rule  X  : R n  X  R and any point x  X  X  x  X  ,  X  x } , define the (deterministic) risk function Also, for any prior  X  on { x  X  ,  X  x } , define By Propositions 3.3.1 and 3.3.2 of Bickel &amp; Doksum (2001), the inequality (24) R n  X  R with the following three properties: Namely, Beckner, W. A generalized Poincar  X e inequality for Gaussian measures. Proceedings of the American Mathematical Society , 105(2):397 X 400, 1989.
 Bickel, P.J. and Doksum, K.A. Mathematical Statistics, volume I . Prentice Hall, 2001.
 David, HA. Order statistics. 1981. J. Wiley .
 dimension reduction in l 1 using cauchy random projections. Journal of Machine Learning Research , pp. 2497 X 2532, 2007.
 Marshall, A.W., Olkin, I., and Arnold, B.C. Inequalities: theory of majorization and its applications . Springer, 2010.
 Annales-Faculte des Sciences Toulouse Mathematiques , volume 9, pp. 245 X 303.
Universit  X e Paul Sabatier, 2000. van der Vaart, A.W. and Wellner, J.A. Weak convergence and empirical processes .
Springer Verlag, 1996.
