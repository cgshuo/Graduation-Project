 learning domains such as expert systems, medical diagnosis , decision making, speech recognition, and natural language processing. There are many different t ypes of graphical model, each with its own properties and benefits, including Bayesian networks, u ndirected Markov random fields, and more or less appropriate. For example, static Bayesian netw orks are quite useful when the size of set of random variables in the domain does not grow or shrink f or all data instances and queries of interest.
 Hidden Markov models (HMMs), on the other hand, are such that the number of underlying random variables changes depending on the desired length (which ca n be a random variable), and HMMs are applicable even without knowing this length as they can b e extended indefinitely using online inference. HMMs have been generalized to dynamic Bayesian n etworks (DBNs) and temporal con-ditional random fields (CRFs), where an underlying set of var iables gets repeated as needed to fill any finite but unbounded length. Probabilistic relational m odels (PRMs) [5] allow for a more com-plex template that can be expanded in multiple dimensions si multaneously. An attribute common model is finite. In other words, these forms of GM allow the spe cification of models with an unlim-ited number of random variables (RVs) using a finite descript ion. This is achieved using parameter tying, so while the number of RVs increases without bound, th e number of parameters does not. In this paper, we introduce a new class of model we call multi-dynamic Bayesian networks. MDBNs are motivated by our research into the application of graphi cal models to the domain of statistical machine translation (MT) and they have two key attributes fr om the graphical modeling perspective. First, an MDBN generalizes a DBN in that there are multiple  X  X  treams X  of variables that can get unrolled, but where each stream may be unrolled by a differin g amount. In the most general case, connecting these different streams together would require the specification of conditional probabil-ity tables with a varying and potentially unlimited number o f parents. To avoid this problem and (defined later in the text), we employ a form of existence unce rtainty [7] (that we call switching existence ), whereby the existence of a given random variable might dep end on the value of other random variables in the network.
 Being fully propositional, MDBNs lie between DBNs and PRMs i n terms of expressiveness. While PRMs are capable of describing any MDBN, there are, in genera l, advantages to restricting ourselves to a more specific class of model. For example, in the DBN case, it is possible to provide a bound on simple, while at the same time making possible the easy expre ssion of MT systems, and opening doors to novel forms of probabilistic inference as we show be low.
 In section 2, we introduce MDBNs, and describe their applica tion to machine translation showing how it is possible to represent even complex MT systems. In se ction 3, we describe MDBN learning machine translation, and future work is discussed in sectio n 5. A standard DBN [4] template consists of a directed acyclic gr aph G = ( V, E ) = ( V E are the intra-slice edges between nodes in V and V are cloned T  X  1 times (where parameters of cloned variables are constraine d to be the same as the template) and re-connected at the corresponding places.
 An MDBN with K streams consists of the union of K DBN templates along with a template struc-ture specifying rules to connect the various streams togeth er. An MDBN template is a directed graph k T . There can be arbitrary nesting of the streams such as, for ex ample, it is possible to specify a model that can grow along several dimensions simultaneousl y.
 An MDBN also utilizes  X  X witching existence X , meaning some s ubset of the variables in V bestow existence onto other variables in the network. We call these variables existence bestowing (or eb-to a variable length DBN. For example, we may have a joint dist ribution over lengths as follows: where here N is an eb-node that determines the number of other random vari ables in the DGM. Our notion of eb-nodes allows us to model certain characteri stics found within machine translation systems, such as  X  X ertility X  [3], where a given English word is cloned a random number of times in the generative process that explains a translation from F rench into English. This random cloning might happen simultaneously at all points along a given MDBN stream. This means that even for a given fixed stream length T variables. Our graphical notation for eb-nodes consists of the eb-node as a square box containing variables whose existence is determined by the eb-node.
 We start by providing a simple example of an expanded MDBN for three well known MT systems, that our goal is to translate from a string of French words F = f of length M = m into a string of English words E = e of length L = l  X  of course these can be any two languages. The basic generative (noisy channel) approach when translating from French to English is to represent the joint the French word, leading to P ( f | e ) = Figure 1(a) is a 2-stream MDBN expanded representation of th e three models, in this case  X  = 4 and m = 3 . As shown, it appears that the fan-in to node f bound. However, a switching mechanism whereby P ( f parameters regardless of L . This means that the alignment variable a e words not explained by any of e that, in Models 1 and 2, there are no edges between a on the set { 1 , . . . , L } ; in Model 2, the distribution over a the English and French lengths  X  and m respectively. In the M-HMM model, the a a first order Markov chain. Figure 1: Expanded 2-stream MDBN description of IBM Models 1 and 2, and the M-HMM model for MT; and the expanded MDBN description of IBM Model 3 with fert ility assignment  X  3 ,  X  2 = 1 ,  X  3 = 0 .
 From the above, we see that it would be difficult to express thi s model graphically using a standard DBN since L and M are unequal random variables. Indeed, there are two DBNs in o peration, one consisting of the English string, and the other consisting o f the French string and its alignment. family of model, but it also represents models whose paramet er space grows without bound  X  the switching function allows the model template to stay finite r egardless of L and M . With our MDBN descriptive abilities complete, it is now poss ible to describe the more complex IBM models 3, and 4[3] (an MDBN for Model3 is depicted in fig. 1(b)) . The top most random variable,  X  , is a hidden switching existence variable corresponding to t he length of the English string. The box thus resulting in three English words e To each English word e how many times e set of variables under it. Given the fertilities (the figure d epicts the case  X  each word e what is called the tablet [3] of e observed French sequence f f  X  i,k  X  { 1 , . . . , m } determines which f The bottom variable m is a switching existence node (observed to be 6 in the figure) w ith corre-sponding French word sequence and alignment variables. The French sequence participates in the v constraint described above, while the alignment variables a each a connecting the alignment a variables represent an extension to implement an M3/M-HMM h ybrid. The null submodel involving the deterministic node m  X  (= French words that are not explained by any of the English word s e successive permutation variables are ordered and this cons traint is implemented using the observed child w of  X  Model 4 [3] is similar to Model 3 except that the former is base d on a more elaborate distortion model that uses relative instead of absolute positions both within and between tablets. Multi-dynamic Bayesian Networks are amenable to any type of inference that is applicable to regular Bayesian networks as long as switching existence relations hips are respected and all the constraints (aggregation for example) are satisfied. Unfortunately DBN inference procedures that take advan-tage of the repeatable template and can preprocess it offline , are not easy to apply to MDBNs. A specify dependencies and constraints spanning the entire u nrolled graph, it is not obvious how we In section 4, we discuss sampling inference methods we have u sed. Here we discuss our extension to a backtracking search algorithm with the same performance g uarantees as the JT algorithm, but with the advantage of easily handling determinism, existence un certainty, and constraints, both learned and explicitly stated.
 Value Elimination (VE) ([1]), is a backtracking Bayesian ne twork inference technique that caches factors associated with portions of the search tree and uses them to a void iterating again over the about VE inference. We have extended the VE inference approa ch to handle explicitly encoded constraints, existence uncertainty, and to perform approximate local domain pruning (see section 4). the first step of the learning procedure, our main contributi on in this section.
 A VE factor , F , is such that we can write the following marginal of the joint distribution referred to as a dependency set (F.Dset) . X is referred to as a subsumed set (F.Sset) . By caching the tuple ( F.Dset, F.Sset, F.val ) , we avoid recomputing the marginal again whenever (1) F.Dset is active , meaning all nodes stored in F.Dset are assigned their cached values in the current branch of the search tree; and (2) none of the variables in F.Sset are assigned yet.
 all Newly Single-valued (NSV) CPTs i.e., CPTs that involve the current node, V , and in which all Figure 2: Learning example using the Markov chain A  X  B  X  C  X  D  X  E , where E is observed. (to avoid branching on them). In line 10, we add V to the Sset. In line 11, we cache a new factor F with value F.val = sum . We store V into F.head , a pointer to the last variable to be inserted into F.Sset , and needed for parameter estimation described below. F.Dset consists of all the variables, except V , that appeared in any NSV CPT or the Dset of an activated facto r at line 6. conditioning X  X hat this means is that to answer a query of the t ype P ( Q | E = e ) , where Q is query variable and E a set of evidence nodes, we force Q to be at the top of the search tree, run the along each of the outgoing edges of Q . Parameter estimation would require running a number of queries on the order of the number of parameters to estimate.
 We extend VE into an algorithm that allows us to obtain Expect ation Maximization sufficient statis-evidence procedure in the Junction Tree algorithm, but here we do this via a search tree. Let  X  when they have value y . Assuming a maximum likelihood learning scenario 3 , to estimate  X  which is a sum of joint probabilities of all configurations th at are consistent with the assignment { X = x, pa ( X ) = y } . If we were to turn off factor caching, we would enumerate all such variable configurations and could compute the sum. When standard VE fac tors are used, however, this is no longer possible whenever X or any of its parents becomes subsumed. Fig. 2 illustrates an example of a VE tree and the factors that are learned in the case of a Mar kov chain with an evidence node at the end. We can readily estimate the parameters associate d with variables A and B as they are not subsumed along any branch. C and D become subsumed, however, and we cannot obtain the correct counts along all the branches that would lead to C and D in the full enumeration case. To address this issue, we store a special value, F.tau , in each factor. F.tau holds the sum over all path probabilities from the first level of the search tree to the level at which the factor F was either created or activated. For example, F 6 .tau in fig. 2 is simply P ( A = 1) . Although we can compute F 3 .tau directly, we can also compute it recursively using F 5 .tau and F 6 .tau as shown in (unique) value d subsumption relationship. In general, we can show that the f ollowing recursive relationship holds: where F pa is the set of factors that subsume F , F come active in the context of { F pa .Dset, F pa .head = d of all newly single valued CPTs under the same context. For to p-level factors (not subsumed by any factor), F.tau = P Alg. 2 is a simple recursive computation of eq. 1 for each fact or. We visit learned factors in the (line 13) by any F that might have activated F  X  (line 12). For example, in fig. 2, F 4 uses F 1 and counts for any NSV CPT entries since F.tau will account for the possible ways of reaching the configuration { F.Dset, F.head = d } in an equivalent full enumeration tree.
 Algorithm 1 : FirstPass(level) Algorithm 2 : SecondPass() Most Probable Explanation We compute MPE using a very similar two-pass algorithm. In th e also keep track of the value of F.head at which the maximum is achieved. In the second pass, we when we assign each F.head variable to its maximum value starting from the last learned factor. make it possible to solve computationally-intensive real-world problems using large amounts of data, while retaining the full generality and expressiveness aff orded by the MDBN modeling language. In the experiments below we compare running times of MDBNs to GI ZA++ on IBM Models 1 through 4 and the M-HMM model. GIZA++ is a special-purpose optimized MT word alignment C++ tool that is widely used in current state-of-the-art phrase-bas ed MT systems [10] and at the time of this writing is the only publicly available software that implem ents all of the IBM Models. We test on (Europarl [9]) and train on 10000 sentence pairs from the sam e corpus and of maximum number of words 40. The Alignment Error Rate (AER) [13] evaluation met ric quantifies how well the MPE assignment to the hidden alignment variables matches human -generated alignments.
 Several pruning and smoothing techniques are used by GIZA an d MDBNs. GIZA prunes low lexical ( entries. For models 3 and 4, for which there is no known polyno mial time algorithm to perform the full E-step or compute MPE, GIZA generates a set of high pr obability alignments using an M-HMM and hill-climbing and collects EM counts over these alig nments using M3 or M4. For MDBN together, account for the lowest specified percentage of the total probability mass of the product of all newly active CPTs in line 6 of alg. 1. This is a more effecti ve pruning than simply removing low-probability values of each CPD because it factors in the joint contribution of multiple active variables.
 Table 1 shows a comparison of timing numbers obtained GIZA++ and MDBNs. The runtime num-bers shown are for the combined tasks of training and decodin g; however, training time dominates given the difference in size between train and test sets. For models 1 and 2 neither GIZA nor MDBNs perform any pruning. For the M-HMM, we prune 60% of probabili ty mass at each level and use a Dirichlet prior over the alignment variables such that long -range transitions are exponentially less likely than shorter ones. 6 This model achieves similar times and AER to GIZA X  X . Interes tingly, without any pruning, the MDBN M-HMM takes 160 minutes to comp lete while only marginally improving upon the pruned model. Experimenting with severa l pruning thresholds, we found that AER would worsen much more slowly than runtime decreases.
 Models 3 and 4 have treewidth equal to the number of alignment variables (because of the global constraints tying them) and therefore require approximate inference. Using Model 3, and a drastic pruning threshold that only keeps the value with the top prob ability at each level, we were able to achieve an AER not much higher than GIZA X  X . For M4, it achieve s a best AER of 31.7% while we do not improve upon Model3, most likely because a too restric tive pruning. Nevertheless, a simple variation on Model3 in the MDBN framework achieves a lower AE R than our regular M3 (with pruning still the same). The M3-HMM hybrid model combines th e Markov alignment dependencies from the M-HMM model with the fertility model of M3.
 MCMC Inference Sampling is widely used for inference in high-treewidth mod els. Although MDBNs support Likelihood Weighing, it is very inefficient wh en the probability of evidence is very small, as is the case in our MT models. Besides being slow, Mar kov chain Monte Carlo can be problematic when the joint distribution is not positive eve rywhere, in particular in the presence of determinism and hard constraints. Techniques such as block ing Gibbs sampling [8] try to address the problem. Often, however, one has to carefully choose a probl em-dependent proposal distribution. We used MCMC to improve training of the M3-HMM model. We were a ble to achieve an AER of 32.8% (down from 39.1%) but using 400 minutes of uniprocesso r time. The existing classes of graphical models are not ideally sui ted for representing SMT models because directed Bayesian network semantics: switching parents found in Bayesian Multinets [6], aggrega-Table 1: MDBN VE-based learning versus GIZA++ timings and %AER using 5 EM itera tions. The columns have introduced a generalization of dynamic Bayesian netwo rks to easily and concisely build models consisting of varying-length parallel asynchronous and in teracting data streams. We have shown that our framework is useful for expressing various statistical machine translation models. We have also introduced new parameter estimation and decoding algorith ms using exact and approximate search-based probability computation. While our timing results are not yet as fast as a hand-optimized C++ program on the equivalent model, we have shown that even in th is general-purpose framework of MDBNs, our timing numbers are competitive and usable. Our fr amework can of course do much more than the IBM and HMM models. One of our goals is to use this framework to rapidly prototype novel MT systems and develop methods to statistically induc e an interlingua. We also intend to use MDBNs in other domains such as multi-party social interacti on analysis.

