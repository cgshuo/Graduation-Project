 In traditional multi-instance learning (MIL), instances are typically represented by using a single feature view. As MIL becoming popular in domain specific learning tasks, ag-gregating multiple feature views to represent multi-instance bags has recently shown promising results, mainly because multiple views provide extra information for MIL tasks. N-evertheless, multiple views also increase the risk of involving redundant views and irrelevant features for learning. In this paper, we formulate a new cross-view feature selection prob-lem that aims to identify the most representative features across all feature views for MIL. To achieve the goal, we de-sign a new optimization problem by integrating both multi-view representation and multi-instance bag constraints. The solution to the objective function will ensure that the iden-tified top-m features are the most informative ones across all feature views. Experiments on two real-world applica-tions demonstrate the performance of the cross-view feature selection for content-based image retrieval and social media content recommendation.
 H.2.8 [ Data Management ]: Database Applications X  Data Mining Algorithm, Experimentation Multi-instance learning, Cross-view feature selection
Multi-Instance Learning (MIL) represents a type of spe-cially designed learning tasks [26, 30, 25] where instances F igure 1: The conceptual view of different learning paradigms: (a) Traditional multi-instance feature s-election in a single view; and (b) Cross-View Feature Selection for MIL with multiple views. in bags are represented by features collected from a single view 1 . For example, in image retrieval, a bag can be used to denote an image with each instance in the bag corre-sponding to a small region of the image. By using color histogram as a feature view, an image can be represented by a bag of color histogram vectors (instances). Because features used to describe multi-instance bags often contain redundant and irrelevant features, existing works [28, 14, 26, 5] have demonstrated that feature selection can help improve MIL performance. A conceptual view of feature selection for traditional MIL is shown in Figure 1(a).

Despite of the increasing popularity of MIL for many real-world learning tasks, researchers have found that using fea-tures from a single view is inadequate to fully represent the content of bags [11, 27]. For example, in image re-trieval, combining features from two views (such as color histogram and texture) can describe each small region in the image more accurately than using each single feature view alone. Accordingly, one can represent each image as a multi-instance bag using features selected from different views [12]. Because multiple views can provide more fea-ture information than a single view, multi-view features are
I n this paper, a view corresponds to one type of features used to represent an object, as defined in Definition 1. potentially useful for improving MIL X  X  performance. On the o ther hand, combining multi-view features for learning needs to address the following issues:
The importance of multiple feature views have been stud-ied in the literature, and a handful of works [22, 23, 24] have discussed the feature extraction problem from multi-view data. Some works [3, 4] also proposed to use unlabeled data for unsupervised multi-view feature selection. However, all existing multi-view feature selection methods are designed for generic learning tasks where a class label is associated to a single instance . They cannot be directly applied to MIL tasks, where a class label is associated to a bag of instances .
In a multi-view setting, in order to find high quality fea-tures for multi-instance learning, two simple solutions are to (1) treat each feature view independently and use existing multi-instance feature selection approaches [5] to select the most informative features for each view. After that, the s-elected features can be concatenated [20] to represent each bag for learning; or (2) concatenate all feature views as one s-ingle view, and then apply traditional multi-instance feature selection to the concatenated features to find most effective features for learning. As we will soon demonstrate in our experiments in Section 4, the two baseline methods, how-ever, cannot identify the most informative global features across all views, because they either locally selects and com-bines features from each single view or they treat all views equally and concatenate them to form one single view. On the other hand, a recent method uses unsupervised feature selection by exploring feature correlations between multiple views in general learning tasks [17], but requires users to specify the number of features to be selected from each s-ingle view (which is impractical because uses often do not have knowledge about the usefulness of each feature view). Therefore, no effective method exists to automatically iden-tify the top-m informative features across multiple views, especially in the MIL setting. To the best of our knowledge, our work is the first to explore the feature selection problem crossing multiple views for multi-instance learning.
The above observations motivate the proposed cross-view feature selection for MIL, where the ultimate goal is to iden-tify the top-m most informative features across all views to support MIL, as shown in Figure 1(b). In our problem set-ting, it is unnecessary to specify the number of features to be selected from each single view. Instead, our method aim-s to automatically calculate and identify the discriminative power of each single feature across all the views.
In order to achieve the goal, the following three challenges need to be considered:
In the paper, we propose a C ross-V iew F eature S election framework for M ulti-I nstance L earning ( i.e., CVFS-MIL). To address the three challenges, CVFS-MIL combines both bag-level and instance-level constraints ( Challenge #1 ) to evaluate and select the most informative features in a single view ( Challenge #2 ). After that, weight values are assigned to the views according to their contributions to feature se-lection. At the last step, the weight updating and cross-view feature selection are repeated in a cross-view manner to find the optimal top-m feature set ( Challenge #3 ). Experimen-tal results will demonstrate the performance of the proposed CVFS-MIL method.
Definition 1. View and Multi-view: A view (or a fea-ture view) denotes a type of features used to represent an ob-ject. Similarly, multi-view represents multiple types of fea-tures used to represent the same object.

For example, color histogram in Hue-Saturation-Value col-or space [18] is one view for image representation. Bag-of-word is one view for text representation. Multi-view fea-tures can be collected from different types of features (such as color histogram and texture), or collected from different channels. For example, a post/article in a social network-ing site (such as Reddit: http://www.reddit.com/) can be represented by multi-view features with one view using bag-of-word to represent content of the post, and a second view represents information about the community to which the post belong and the time of posting etc. (Detailed in Sec-tion 4.3).

Definition 2. Multi-instance learning and Bag con-straint: In multi-instance learning (MIL), a bag contains a number of instances. A bag is labeled positive if at least one instance inside the bag is positive, and negative otherwise. This is commonly referred to as the bag constraint for MIL.
In a multi-view setting, a bag B i = { B 1 i , , B k i , , B consists of a number of objects from different views, in which B i denotes a sub-bag with instances in the k th view of bag B . The class label of the bag B i is denoted by y i  X  X  , with Y = { X  1 , +1 } . The set of all positive (or negative) bags in the k th view is denoted by B k + ( B k  X  ).

According to the multi-instance bag constraint, all in-stances in a negative bag are negative. So a negative bag B i  X  B k  X  in the k th view can be represented by an instance feature vector  X  x k i = P n i j =1 (  X  x j i ) k /n i , where n number of instances in B k i and (  X  x j i ) k is the j th instance in B i under the k th view. The resulting representation for all bags in B k  X  is denoted by  X  B k  X  . We also use  X  x k j j th instance.

For a positive bag B k i  X  B k + , in which the genuine in-stance label information is unknown, the instance with the cross-view feature set until convergence. largest distance from negative bags ( i.e., the most posi-tive instance) is used to denote the positive bag. In this case, the instance feature vector for a positive bag is  X  x arg max  X  sentation can also be found in some previous works [20, 19].
Given a set of labeled bags B = { B 1 , , B v } with v views, CVFS-MIL aims to build a cross-view feature selec-tion model from B and select the optimal m features from different feature views to train classifiers and predict previ-ously unseen multi-view bags with maximum accuracy.
The cross-view feature selection for MIL needs to address two questions:
To answer the above questions, we introduce a new cross-view feature selection algorithm CVFS-MIL which uses a two-level ( i.e., bag-level and instance-level) optimization to model each single view and carry out a cross-view optimiza-tion by assigning a proper weight value to each view. All selected features from different views are unified to refine weight value of each single view, through which the most discriminative features can be discovered for multi-instance learning. The resulting framework is illustrated in Figure 2.
Given the k th view, we define the corresponding total fea-ture set as S k = { s k 1 , s k 2 , ..., s k h number of features in S k . Our objective is to find a subset T k  X  X  k using a feature selection matrix D k (a diagonal ma-trix, diag ( D k ) = d ( S k )), where d ( S k ) is a indicator vector, if s k i  X  X  k , d ( S k ) i = 1, otherwise 0.

Accordingly, we define J ( T k ) as a single-view evaluation function to measure the T k as follows: where | | denotes the cardinality of the feature set, and m k is the number of features to be selected from T k in the k th view. The single-view objective function in Eq. (1) represents that the bag constrained features for MIL T k  X  should have maximum discriminative power.
To calculate the score of a feature set T k , i.e., J ( T impose bag-level and instance-level constraints as follows: for any two bags B k i and B k j in the k th view, if they have the same labels, we form a pairwise must-link constraint between B k i and B k j . If B k i and B k j have different labels, we form a cannot-link constraint between them. To further take the data distributions inside each bag into consideration, we also add instance-level constraints to ensure that instances in each negative bag are close to each other (with respect to the selected features), and instances in each positive bag are maximally separated by the features.

Accordingly, candidate features should satisfy four con-straints: (a) Bag-Level Must-Link . Because each bag B k i associated with a class label (positive or negative), the se-lected features should ensure that bags with the same label are similar to each other. (b) Bag-Level Cannot-Link . For bags with different class labels, the features should repre-sent the disparity between them. (c) Instance-Level Must-Link . In multi-instance scenarios, instances in negative bags are known to be genuinely negative, so the selected features should ensure that instances in each negative bag are simi-lar to each other. (d) Instance-Level Separability . Genuine labels of instances in positive bags are unknown, although at least one instance must be positive. In this case, we use P rinciple Component Analysis (PCA) principle [6] and seek to find features that preserve the diverse information in the positive bags, i.e., instances in positive bags are maximal-ly separable. Specifically, we represent instances in a lower dimensional space by using a sparse combination of the o-riginal features, which resembles to the sparse PCA [21].
Based on the above four constraints, we derive a dual level criterion to measure the informativeness of J ( T k ), which consists of bag-level J ( T k ) B and instance-level J ( T the k th single view as in Eq. (2),
For the bag-level criterion J ( T k ) B , where  X  W k i,j s class label information between two bags with  X  W k i,j = 1 } denotes the must-link pairwise bag constraint sets with B = { ( i, j ) | y i y j =  X  1 } denoting the cannot-link pair-wise sets. Similarly, for the instance-level J ( T k ) I , { X  1 / | C | ,  X   X  x k a ,  X  x k b  X  B k  X  ; 1 / | D | ,  X   X  x where C = { (  X  x k a ,  X  x k b ) |  X  x k a ,  X  x k b  X  B pairwise instance constraint in negative bag set B k  X  with wise constraint in positive bag set B k + . Besides, we adopt an Euclidean norm to measure K B and K I .
 For bag-level criterion J ( T k ) B , we have where tr ( ) is the trace operator for a matrix,  X  D k is a di-agonal matrix generated from  X  W k with  X  D k i,i = P j  X  L k =  X  D k  X   X  W k is a Laplacian matrix.  X  X k = [  X  x [  X  f 1 , , of features and bags in the k th view, respectively.
Similarly, the instance-level evaluation J ( T k ) I can be rewritten in a matrix form. By combining the underlying J ( T k ) I which shares the similar derivation as the one in bag-level J ( T k ) B , Eq. (2) can be rewritten as,  X  D k is a diagonal matrix with diag (  X  D k ) = d k , where d P where q k is the number of instances in the k th view. Ac-Algorithm 1 C VFS-MIL 6: while not convergent for  X  do 7: T  X  Apply  X  to update T via Eq. (8); 8:  X   X  Apply T to update  X  via Eq. (11); 9: end while cording to Eq. (4), it is
Since L k is the Laplacian matrix that only encodes the class label information, considering the view concept in Def-inition 1, L 1 = = L v = L . By denoting the feature score as ~ ( s k i ) = ( f k i )  X  L f k i , the problem of maximizing J ( T the k th view is equivalent to finding a set of features that can maximize ~ ( s k i ), which can be represented as follows: T k  X  = arg max
From Eq. (6), the optimization problem in Eq. (1) aims to calculate the sum of ~ ( s k i ) in the k th view, where each ~ ( s k i ) only associates with the attribute vector of s f ). In order to find the subset T k that maximizes J ( T k defined in Eq. (1) in the k th view, we can calculate the score value of each individual feature in S k for ranking, i.e., ~ ( s k 1 )  X  ~ ( s k 2 )  X  ~ ( s k h features T k = { s k 1 , s k 2 , , s k m in the k th view.
In practice, views have different contributions to represent complicated objects. Accordingly, we use a weight value  X  k  X  [0 , 1] , 1  X  k  X  v to denote the importance of each view. The larger the weight value  X  k , the more important role the view will play in generating the final feature subset. By combining all views, the cross-view optimization for MIL can be formulated as follows, where T = {T 1 , , T k , , T v } , and S = {S 1 , , S k , , S } . v is the number of views, and the number of selected features m = P v k =1 m k .
The solution to Eq. (7) is a nonlinear  X  n onconvex opti-mization problem. To the best of our knowledge, there is no direct solution to find its global optimum. In this paper, we derive an iterative algorithm by using an alternation op-timization approach, which iteratively updates T and  X  in an alternating fashion, to obtain a local optimal solution. Noticing that, when we fix T to update  X  , for each view, J ( T k ) is also fixed. The solution to  X  is  X  k = 1 correspond-ing to the maximum J ( T k ) over different views, and  X  k otherwise. This trivial solution means that only one view is finally selected. Therefore, the performance of this method is equivalent to the one from one single best view, which does not meet our objective of exploring the complementary knowledge across multiple views for feature selection. To avoid this phenomenon, we use  X  r k (the r th power of  X  k replace  X  k , then we have where r  X  X  X  1. By doing so, P v k =1  X  r k achieves its maximum when  X  k = 1 /v with respect to P v k =1  X  k = 1 and 0  X   X  Similarly,  X  k for different views can be obtained by setting r  X   X  1. In this case, each view will have its respective contribution to the final feature subset.

During the alternation optimization process, we first fix  X  to update T . Once  X  is fixed, for each view, we can calculate the score of each individual feature in S k with the product der. By choosing the top-m features according to the global sequence over all views, we can maximize J ( T ) and obtain the current optimal T  X  .

After the above process, we fix T to update  X  . By using a Lagrange multiplier  X  to take the constraint P v k =1  X  into consideration, we have the Lagrange function as follows,
By setting the derivative of f (  X ,  X  ) with respect to  X   X  to zero, we have
Then, we can obtain  X  k accordingly,
Because Laplacian matrix L is positive semi-definite, for any feature s k i , ~ ( s k i )  X  0 [8, 13], thus  X  k  X  0. According to Eq. (11), if r  X   X  X  X  ,  X  k in all views will be close to each other. While if r  X  1, only  X  k = 1 corresponding to the maximum arg max T k J ( T k ) across different views, and  X  = 0 otherwise. Noticing that when r = 0,  X  r k will always be 1. In this case, the view weights are useless. So, in our Figure 3: Example images from the Corel image cat-e gorization database. The first three rows show im-ages in category  X  X ats X , including  X  X ion X  (The first row),  X  X iger X  (The second row) and  X  X eopard X  (The third row), and the last three rows represent the negative image examples. The similar image data generation can also be found in the pervious work [19] and Andrews X  X  2003 NIPS work [1]. setting, we have r  X  X  X  1. Furthermore, the global alignmen-t objective function P v k =1  X  r k P s k the number of iterations increases, therefore CVFS-MIL will converge to a local optimum. More specifically, the optimal T can increase the value of the objective function with the fixed  X  , and vice versa.

According to the aforementioned descriptions, we can for-m an alternating optimization procedure, depicted in Algo-rithm 1, to obtain a local optimal solution for cross-view multi-instance feature learning.
We empirically compare the performance of CVFS-MIL with baselines. After the feature subsets are selected, we use MILR [15], a multi-instance logistic regression algorithm, to train classification models (other MIL approaches could also be applied for learning, but because that we are mainly fo-cusing on cross-view feature selection issue, we omit detailed results from other MIL classifiers). For CVFS-MIL, the de-fault value of r is  X  1 and the number of selected features is 200 for Corel Image data set and 40 for Reddit social media recommendation data set. All results are based on ten-fold cross-validation, and all experiments are conduct-ed on a Linux computer cluster with an Interl(R)Xeon(R) @3.33GHZ CPU and 3GB memory.
Because there is no existing cross-view feature selection method for MIL, we implement the following, bag-level and instance-level, baseline approaches which carry out feature selection at each single view or concatenate multiple views for feature selection:
In the first type of baselines, we separate each multi-view multi-instance data set into multiple multi-instance data set-s, each of which containing only one single view. Our pur-pose is to demonstrate the performance of feature selection 600. The similar representations are also used in Figures 6 and 7. on a single view and further investigate the performance gain/loss of the cross-view feature selection. For the single view based baselines, we implement the following methods:
In summary, in the above single view baselines, the feature selection approaches ( i.e., BIG+MI,IIG+MI, and FS+MI) are directly applied to each single view, with each view being handled separately.
For comparison purposes, we also implement the following three baseline strategies which concatenate features across different views for feature selection. The three types of cross-view feature combination strategies will serve as baselines to demonstrate whether concatenating features selected from each single view can result in comparable performance as our proposed method.

View Combination Strategy I: This strategy concate-nates the top-m/v features selected from each view by using BIG+MI, IIG+MI, or FS+MI, respectively, to form concate-nated m -dimensional cross-view features. This is similar to a state-of-the-art multi-view feature selection method [17] with the number of selected features from each single view being pre-determined.

View Combination Strategy II: This strategy first concatenates multi-view features as one single feature view, and then selects the m -dimensional features by applying BIG+MI, IIG+MI, or FS+MI, to the concatenated features, respectively. This strategy does not need to specify the num-ber of selected features for single view, which is similar to our CVFS-MIL.

View Combination Strategy III: This strategy is based on a state-of-the-art view combination approach for multi-view multi-instance learning [11]. More specifically, after obtaining the top-m/v features by using the above feature s-election methods such as BIG+MI, IIG+MI, or FS+MI from Title-Keyword view. bination methods.
 be found in Section 4.1.
 each single view, a multi-instance classier, such as an MILR, i s trained for each view. The ensemble of all classifiers are used to predict label of each test bag.

In our experiments, we will combine each baseline learn-ing methods (BIG+MVMI, IIG+MVMI, and FS+MVMI) with different view combination strategies (-I, -II or -III), to demonstrate various settings for multi-instance learning un-der multiple views. For example, the baseline BIG+MVMI-I uses the view combination strategy -I with BIG under MV-MI (multi-view MI setting) for multi-instance learning.
In this subsection, we report CVFS-MIL X  X  performance for content based image retrieval. The original images from Corel data set [10] are preprocessed and segmented using bipartite graph partitioning, which achieves a better perfor-mance on the Berkeley Segmentation Database compared to state-of-the-art techniques [2]. In this case, each image is considered as a bag with each region (or a segment) inside the image denoting an instance.

In order to represent each image region, we employ mul-tiple types of features [7] including Hue-Saturation-Value (HSV), Histogram of Oriented Gradient (HOG), and Local Binary Patterns (LBP). More specifically, HSV is a com-monly used color model, where HSV stands for hue, satura-tion, and intensity. HOG is a gradient-based feature that captures the gradient distribution of an object. LBP is a model to represent texture in a local region. For HOG the negative class and orange corresponding to the positive class. feature, we first extract a 16-dimensional dense HOG fea-t ure for each pixel using the code provided by Kota 2 . The extracted HOG representations are fed to k -means cluster-ing to construct a 256-dimensional codebook. After that, a one-dimensional code is assigned to each pixel based on the similarity between the pixel representation and the clus-ter centers. The HOG-based representation for a region is constructed as a 256-dimensional histogram-based vector by counting the probabilities of the occurrences of the codes. Similarly, two other 256-dimensional histogram-based vec-tors are constructed for each region using the HSV and LBP features, respectively.

To build positive bags, we use category  X  X ats X , consisting of  X  X iger X ,  X  X ion X  and  X  X eopard X , as positive bags (300 bags with 1500 instances) and randomly draw 300 images of oth-er animals to form negative bags with 1500 instances ( i.e., segmentations), where some sample images are shown in Fig-ure 3. We compare the performance of CVFS-MIL with the baselines on each single view in Figures 4(A-C). The results show that CVFS-MIL is normally 10% more accurate than baseline methods on HOG and LBP views, and also clearly outperforms all other methods on HSV view. Similarly, the performance of CVFS-MIL also improves significantly on all views under different view combination learning strategies as shown in Figure 5.

At the first column of Table 1, we also report the pairwise t -test (with confidence level  X  = 0 . 05) to validate the sta-tistical significance between our proposed method and base-lines. Each entry (value) denotes the p -value for a t -test between two algorithms, and a p -value less than  X  = 0 . 05 indicates that the difference is statistically significant. The h ttp://www.cs.sunysb.edu/%7ekyamagu/software/misc/ results show CVFS-MIL statistically outperforms baselines both on each isolated view and multiple views, in all cases.
Our second multi-instance learning task is to understand social media content recommendation in Reddit, where the original data are collected from Stanford Network Data set Collection 3 . The purpose of the Reddit data set is to un-derstand the key factors which will make a post ( i.e., an URL) to receive popular votes. In their experiments, each URL link was submitted to Reddit.com for multiple times by using different titles, at different times, and to different Reddit communities. This naturally forms a multi-instance learning task, where each URL link denotes a bag, and each time the link is posted to Reddit, it will form one instance. So multiple posts of the same URL will form a bag. A bag is positive ( i.e., an URL is indeed interesting), if one or mul-tiple posts receive a large number of positive user voting, and negative otherwise. In our experiments, if the number of up-votes minus the number of down-votes, for a post, is greater or equal to 2000, we will set the bag to be positive. If the scores of all posts are less than 100, we will set the bag to be negative. We choose 600 links ( i.e., bags) to form 300 positive (the highest score  X  2000) bags with 2043 in-stances and 300 negative (each score &lt; 100) bags with 1995 instances. More details about the Reddit data descriptions can be found in [9]. The success of a Reddit submission de-pends on many factors other than the quality of the content ( e.g., title). A particularly unique and important property of our data set is that every link we considering has been submitted multiple times, with multiple titles to multiple h ttp://snap.stanford.edu/data/web-Reddit.html Figure 9: The average CPU runtime for CVFS-MIL v .s. baseline approaches on multiple views via learn-ing strategy -I, on Corel Image and Reddit Submis-sion data respectively. communities. This means that it naturally corresponds to a MIL problem with multiple views.

For each instance, we consider two views. More specif-ically, we consider the link title as the title-view, and col-lect 55 factors ( i.e., features) to form another factor-view including: Month (Jan-Dec); Week (Mon-Sun); Day: Morn-ing (6am-9am), Forenoon (9am-12am), Noon (12am-1pm), Afternoon (1pm-6pm), Night (6pm-6am); Communities (28 types used in [9]); and the order of the post (First-3, Last-3, and Others). By doing so, we intend to learn the most important factors to better target social media content: by using the right title, for the right community, at the right posting time. For each link title, a  X  X ag-of-word X  represen-tation based on TFIDF [16] is adopted to convert the title to an instance represented by the top 200 TFIDF features (the similar text representation has been used in [29]). The results in Figures 6(A) and 6(B) show that CVFS-MIL achieves a better performance than the baselines on each single view, especially when the number of feature is sufficiently large. Figure 7(A), 7(B), and 7(C) reports the comparison result on all views using the strategy -I, -II and -III, respectively. For view combination learning strategies (-I and -III), although BIG+MVMI and IIG+MVMI are s-lightly superior to FS+MVMI, their performances are infe-rior to the proposed CVFS-MIL. It is also worth mentioning that CVFS-MIL may achieve comparable performances over baselines in some cases ( e.g., # of selected features is 40-52 for FS+MVMI-II in Figure 7(B)). To furthermore validate the statistical performance of CVFS-MIL, we also report the pairwise t -test with confidence level  X  = 0 . 05 in the last col-umn of Table 1. The p -values (less than 0.05) in each entry confirm that CVFS-MIL statistically significantly outper-forms all baselines.
To better understand features selected from our cross-view strategy, we report the coverage of top-10 features for the Reddit submission data set in Figure 8(a). The y -axis de-notes the ratio between the number of bags covered by any feature in the majority class and the number in the minor-ity class. For each feature, the majority class is defined as the class with the largest number of bags covered by the feature. As a result, the coverage ratios are always greater than 1 (with 1 indicating that the feature covers the same number of positive and negative bags, and therefore is not a discriminative feature). Each blue bar means that the majority class covered by the given feature belongs to the negative class ( i.e., each submission X  X  score is less than 100), and orange bar means positive class ( i.e., the submission X  X  highest score is more than 2000). Additionally, we also list the top-20 features in Figure 8(b), in which the color has the same meaning as Figure 8(a), and the word size is pro-portional to the bag coverage ratio. Among the above 20 features,  X  X eel X ,  X  X eddit X , X  X ost X  and X  X ics X  (with a large font size) are from the title-view, and the rest features are from factor-view.

The results from Figure 8(a) show that the top-9 features are from factor-view, including: (1)  X  X ep X ,  X  X ov X ,  X  X ug X  and  X  X ct X  for the type of Month; (2)  X  X TF X ,  X  X ics X  (correspond-ing to the version with a small font size in Figure 8(b)) and  X  X ifSound X  for the type of Communities; and (3)  X  X ight X  and  X  X fternoon X  for the type of Day. Among them, the or-ange bars reflect that the submissions in X  X ov X  X nd X  X ct X  X re more popular than those in  X  X ep X  and  X  X ug X . Additional-ly, submissions posted during certain hours of the day ( e.g.,  X  X fternoon X  with orange bar) are more popular than oth-ers. We also observe that the subtle community can affect the popularity of submissions. For instance, the contents in  X  X ics X  draw more attention than those in  X  X ifSound X  and  X  X TF X . Further more, the last feature  X  X eddit X  correspond-ing to the blue bar from the title view indicates that the word  X  X eddit X  is a bad word ( i.e., titles containing the word  X  X eddit X  are more likely being ignored by users). Interest-ingly, these observations from our experiments are consistent with the ones from a previous study [9], which tries to under-stand the interplay between titles, content, and communities in social media.
Table 2 shows the performance of CVFS-MIL with respect to different r values. The results show that selecting r =  X  8 and  X  9, and r =  X  7 can result in the best performance for Reddit and Corel data sets, respectively. As discussed in Section 3.2: r values are related to the view correlations. Our experiments indicate that the two data sets have similar complementary information among feature views. Overall, the accuracy for Reddit is not very sensitive to different r values, indicating that the impact of the r for Corel Image data set seems to be larger than that for Reddit data set.
In Figure 9, we report the runtime performance (efficien-cy) for CVFS-MIL v.s. baselines on multiple views via learn-ing strategy -I. The results show that the data reliability based multi-instance feature selection baseline, FS+MVMI, demands the most runtime ( i.e., the worst efficiency). This is mainly because this method needs to compare each pair of bags at feature level, which is very time consuming.
CVFS-MIL requires slightly more time than bag-level BIG model and instance-level model IIG based baselines. This is mainly attributed to the alternating optimization strategy of CVFS-MIL which is computationally demanding.
In this paper, we formulated a new cross-view feature s-election problem for multi-instance learning (MIL), which intends to consider multiple feature views to obtain an opti-mal subset of features across all views. This problem setting is different from exiting feature selection for multi-instan ce learning, and also differs from existing multi-view feature selection, because the former only has one single feature view and the latter cannot provide feature ranking under multi-instance bag constraints to find the most informative features. In the paper, we proposed a new optimization framework to iteratively explore feature set and alternately update the weight value for each view, through which we can ensure that the whole process will find high quality features across all views. Experiments and comparisons on real-world tasks confirmed that the proposed cross-view CVFS-MIL ap-proach significantly outperforms baseline methods. The work was supported by the Key Project of the Natural Science Foundation of Hubei Province, China (No. 2013C-FA004), the National Scholarship for Building High Level U-niversities, China Scholarship Council (No. 201206410056), the Chinese National  X 111 X  Project hosted by SA Centre for Big Data Research in Renmin University of China, and by Australian Research Council (ARC) Discovery Projects under Grant No. DP140100545 and DP140102206. [1] S. Andrews, I. Tsochantaridis, and T. Hofmann. [2] S.-F. Chang. Segmentation using superpixels: A [3] C. Christoudias, R. Urtasun, and T. Darrell.
 [4] Z. Fang and Z. M. Zhang. Discriminative feature [5] R. Gan and J. Yin. Feature selection in multi-instance [6] M. Grbovic, C. Dance, and S. Vucetic. Sparse [7] Z. Hong, X. Mei, D. Prokhorov, and D. Tao. Tracking [8] X. Kong and P. S. Yu. Semi-supervised feature [9] H. Lakkaraju, J. McAuley, and J. Leskovec. What X  X  in [10] J. Li and J. Z. Wang. Real-time computerized [11] M. Mayo and E. Frank. Experiments with multi-view [12] C. Nguyen, X. Wang, J. Liu, and Z.-H. Zhou. Labeling [13] S. Pan, X. Zhu, C. Zhang, and P. S. Yu. Graph stream [14] W. Ping, Y. Xu, K. Ren, H. Chi, and S. Furao. [15] S. Ray and M. Craven. Supervised versus multiple [16] A. Sun. Short text classification using very few words. [17] J. Tang, X. Hu, H. Gao, and H. Liu. Unsupervised [18] K. E. A. Van de Sande, T. Gevers, and C. G. M. [19] J. Wu, Z. Hong, S. Pan, X. Zhu, C. Zhang, and Z. Cai. [20] J. Wu, X. Zhu, C. Zhang, and Z. Cai. Multi-instnace [21] J. Wu, X. Zhu, C. Zhang, and P. Yu. Bag constrained [22] T. Xia, D. Tao, T. Mei, and Y. Zhang. Multiview [23] B. Xie, Y. Mu, D. Tao, and K. Huang. m-sne: [24] J. Yu, D. Liu, D. Tao, and H. S. Seah. On combining [25] H. Yuan, M. Fang, and X. Zhu. Hierarchical sampling [26] A. Zafra, M. Pechenizkiy, and S. Ventura. Relieff-mi: [27] D. Zhang, J. He, and R. Lawrence. Mi2ls: [28] M.-L. Zhang and Z.-H. Zhou. Improve multi-instance [29] Z.-H. Zhou, Y.-Y. Sun, and Y.-F. Li. Multi-instance [30] Z.-H. Zhou, M.-L. Zhang, S.-J. Huang, and Y.-F. Li.
