 XML Keyword Search is a user-friendly information discovery technique, which at-tracts many interests these years. Differen t with keyword search over flat documents, the search object is a single XML document/database with structure information inside and the results are supposed to be fragments of it containing keywords. Since it is dif-ficult and sometimes impossible to identify users X  intentions through keywords, it is indeed a difficult task to determine which fragments should be returned. Many valuable models are proposed to define the results, and the most popular ones are the Small-est Lowest Common Ancestor (SLCA) model [11] and its variations [2],[3],[4],[6], [7],[9],[10],[12], all of which are called as the LCA-based models in this paper. These researches regard the XML document as a rooted, labeled, unordered tree in which each inner node is an element or an attribute a nd each leaf is a value which may contain some keywords. In SLCA model a result is defined as a subtree that: (1) the labels of whose nodes contain all the keywords, (2) none of its subtree satisfies the first condi-tion except itself. The root of such a subtree is called a SLCA node. It X  X  recognized that SLCA model is definitely not a perfect one. Later works like [4],[5],[6],[12] il-lustrate that some results of the SLCA method are meaningless and some meaningful ones are missing by giving different examples, which are called the false positive and the false negative problems respectively. Interesti ngly, the remedy approaches they pro-pose sometimes conflict with each other, and counterexamples can always be found to testify that the two problems still happen.

In order to explain these issues we employ the original examples from former re-searches [4],[6],[12], which are illustrated in Figure 1 as three separate XML trees. In these trees keywords are marked in bold and important nodes are identified by num-bers. Besides, we use node i to denote the node with the number i in any of the trees. Consider keywords {  X  X ML X ,  X  X avid X  } issued on the XML document in Figure 1(a), apparently SLCA method can find two subtrees rooted in node 7 and node 17 .Inmany cases the subtrees are not appropriate for u sers because their sizes are too large and plenty of meaningless information is involved. GDMCT [3] approach proposes a good way to handle this that it returns the Minimum Connecting Trees (MCTs) instead. A MCT is defined as a subtree which employs the SLCA node as root and the keyword nodes as leaves. Since the refining of the final results is not a focus in this paper, for convenience a search result is considered as a group of keyword nodes instead of a document fragment by us. In this example, SLCA method finds two results { node 9 , node 11 } and { node 19 , node 21 } which are two separat e papers in semantics.
MLCA method [7] and GDMCT method [3] s hould agree with the answers because they both define a result model very similar to SLCA. But, ELCA [2][12] method will claim that several reasonable results are missed. For instance, { node 3 , node 25 } and { node 16 , node 23 } , which indicate a conference and one of its sessions respectively and are perfectly exclusive to each other, along with two SLCA results, are regarded as qualified ELCA results. However, neither node 1 nor node 14 is a SLCA node due to at least one SLCA node is their descendant node.

The semantics of Exclusive Lowest Common Ancestor (ELCA) is firstly proposed in [2] although it is not named as this. Afterwards, Xu et al. [12] and Zhou et al. [14] provide more efficient algorithms to retrie ve the ELCA nodes. The formal definition of ELCA is complicated, however all the ELCA nodes can be retrieved through a straight-forward two-step process which can help understanding the concept: (1) find all the SLCA nodes, halt the process if there isn X  X  any; (2) remove all the SLCA nodes along with the subtrees rooted in them, then turn to the first step. The union of all the SLCA nodes obtained each time in the first step is indeed the set of ELCA nodes. Obviously, ELCA method can obtain more reasonable results.

Seemingly ELCA model has fixed the false negative issue of SLCA model and thus can find results perfect enough. However, based on the example illustrated in Figure 1(b), Li et al. [6] claim that both SLCA and ELCA models suffer from the false positive problem. Suppose {  X  X ML X ,  X  X ohn X  } are the keywords, either SLCA or ELCA method returns { node 6 , node 15 } as the only result which is thought meaningless in [6]. Actually, more examples can be found to support this point of view because in some cases the keyword nodes in a result could be really far from each other in the tree. Rather than implement seman tics inference to improve the r esults as XSeek [8] does, Li et al. introduce a simple rule to filter all the ELCA nodes. For any two keyword nodes n and n j in an ELCA result, suppose lca ( n i ,n j ) is the LCA node of n i and n j and two respectively satisfy that n i and n j have the same elementary type, then the result is unqualified. Accordingly, { node 6 , node 15 } is not a qualified result because node 4 and node 9 have the same elementary type. After the filtering, the left ELCA nodes are called Valuable Lowest Common Ancestor (VLCA) nodes.

The rule of VLCA method is actually first proposed by Cohen et al. [1]. They use this to determine if two keyword nodes are  X  X eaningfully related X . Indeed it is kind of overstrict, and more relaxed criteria could be used such as the LCA have to be low or the result should have a good compactness. Kong et al. present a counterexample in [4]. To search the keywords {  X  X iu X ,  X  X hen X ,  X  X Seek X  } in the tree from Figure 1(c), { node 4 , node 6 , node 8 } is a reasonable answer yet will be eliminated by VLCA method because node 4 and node 5 have the same elementary type  X  X uthor X . Kong et al. [4] also propose a concept called Related Tightest Fragments (RTF) as final search results, which is equal to representing the results of ELCA method in MCTs. Obviously, it keeps the vague problem of false positive results being existed.

Following the common sense that results should be returned as many as possible, we shouldn X  X  care too much about the false positive issue actually. At least we can still return them to users with lower rank scores. On the other hand, another thing needs to be paid more attention to is how to evaluate the results properly and so that to judge if returned results are good enough or are there better ones can be found. No doubt out of the aforementioned models ELCA method can get the maximum number of results which is actually a superset of the results returned by any other LCA-based method. Next we use another example to discuss the ELCA results to explain the problem. Example 1 : Figure 2 illustrates another XML document in tree structure which stores the information of proceedings and journals in DBLP. There is a recursive situation in the document that a paper element coul d have a descendant which is also a paper. Given three keywords {  X  X ML X ,  X  X ob X ,  X  X avid X  } , there are four ELCA results could be found in the document which are illustrate d in closed dashed curves. Apparently, { node 7 , node 10 , node 12 } is a very good answer which is a paper whose title is about  X  X ML X  and its authors are  X  X ob X  and  X  X avid X . From another point of view, the LCA node is appropriately low and the compactness (which is recognized as an important measure by many researches and can be calculated through dividing the number of key-word nodes by the number of all nodes in a result) is good. Another result { node 15 , node 18 , node 25 , node 27 , node 29 } consists of two connected papers and also can be regarded as a meaningful one. With respect to the other two results some people prob-ably have different opinions. For { node 32 , node 35 , node 37 } , although it satisfies the constraint VLCA model requires, it is hard to say that node 35 and node 37 are meaning-fully related. For { node 40 , node 43 , node 46 , node 50 } , many may argue that it will be Suppose another keyword  X  X IGMOD X  is added into the query, there will be a single large result which can be represented by the subtree rooted in node 2 returned by ELCA approach. It is too large to be an appropriate result for users, yet it cannot be divided since any of its subparts (such as the three in the closed dashed curves) is considered totally meaningless because it doesn X  X  contai n all the keywords. Besides, the keyword nodes in the subtree rooted in node 38 can never be included in any result because of the same reason. The false negative problem still happens in a way.

Here we enumerate four defects of ELCA result model, which are actually the flaws of all the LCA-based models. 1. A lack of universal criteria to judge whether a result is qualified or not. 2. Not enough features are considered.
 3. Some useful information is omitted in the results.
 4. Despite different users probably have distinct intentions by using the same key-In this paper, we propose a novel result model for XML keyword search which can avoid all these defects perfectly. Instead of applying restrictions upon the results we give each result a score to evaluate the quality of it. Such a score is generated by a scoring function considering sufficient f eatures, and each is weighted so that can be adjusted as necessary. Obviously, a result gets a better quality if it has a higher score. Since the results could be distinct when th e features are given different weights. We provide multiple algorithms to generate results and based on the values given to the parameters in the scoring function the most suitable algorithm will be chosen to serve better results. Each of our algorithms is supposed to find not only the results with the highest scores as many as possible but also those second-best ones with lower scores.
The remainder of the paper is organized as follows.Section 2 defines the novel model for XML keyword search results. The algorithms are proposed in Section 3. After-wards, experimental results are exhibited in Section 4. Finally we conclude the paper in Section 5. In this section, we provide the formal definition of our search results. As explained before we use a set of keyword nodes to represent a final result for simplicity. Besides, rather than applying some restrictions upon the results, we give each result a score to evaluate its quality (how meaningful it is). Such a score of a keyword node set R is and in our opinion any R satisfying score ( R ) &gt; 0 can be regarded as a result. Unlike any LCA-based model, in which overlapping and inclusion are not allowed between two individual results, we hold a different opinion that we think two results can share some common nodes as long as none of them is a keyword node, which means our model is more relaxed to the results.
 Definition 1: ( XML Keyword Search Results ) T is an XML document/databse which couldbeviewedasatree.Givenasetof t keywords { w 1 , ... w t } , K is the set of all the keyword nodes whose labels contain any keyword. Then, the result set of searching { w 1 , ... w t } in T is R which satisfies:  X   X  R  X  R is a set of keyword nodes that score ( R ) &gt; 0 ;  X   X  R i ,R j  X  R , R i  X  R j =  X  ; From these three conditions in Definition 1 we can see that the XML tree is divided into separate fragments each of which is meaningf ul. In other words, XML keyword search can be regarded as a problem that dividing the set of all keyword nodes into groups that are meaningful. Clearly, excessive partitions of K can be qualified result sets in accordance with Defin ition 1 and we have to judge which ones are gratifying and which ones are not. However, comparing with the LCA-based models it has two advantages. First, each result is not given specific restri ctions yet can be appr aised. Second, all the keyword nodes are involved in the results which definitely brings a high recall. Next we define a concept of the optimal result set which is supposed to be a standard for our result-finding algorithms to pursue.
 Definition 2: ( Optimal XML Keyword Search Result Set ) Suppose R is a partition set of K and | R | = n . We arrange the items in R to be a sequence S = { R 1 , ..., R n } which satisfies: for any 1  X  i&lt;j  X  n , score ( R i )  X  score ( R j ) . Then, R is called an optimal result set iff:  X   X  R  X  ( R k  X  ...  X  R n ) , 1  X  k  X  n , score ( R k )  X  score ( R ) .
 In other words, score ( R 1 ) is the biggest score can be found in the set of all the keyword nodes K ,and score ( R 2 ) is the greatest score in the set ( K  X  R 1 ) , and so on. The rationality of this definition is quite clear that users always prefer the best results and in our model the best results are those results with the largest scores.

A proper scoring function represents a well-designed evaluation model, in which at least four metrics need to be involved: (1) the content information, which mainly refers to the keywords it contains; (2) the structure information, which specifically means the hierarchical position of the root (the LCA of the keyword node set); (3) the compactness, which usually can be calculated through dividing the number of keyword nodes by the number of all nodes in the fragment; (4) the size, which must not be too large. More importantly, the evaluation model should be adjustable to suit different contexts. As a matter of fact, in the evaluation model we can use as many features as possible when they are reasonable. Meanwhile, the LCA-based models only consider the first two features, the content and the structure information, and that X  X  why they suffer from those defects aforementioned.

Some extra notations have to be defined as follows before the scoring function is proposed:  X  K is the set of all the keyword nodes;  X  for any node v , dpt ( v ) returns the depth of v in the tree (the depth of the root is 1,  X  for any set of keyword nodes N , kn ( N ) is the function to get the exact number of  X  for any set of keyword nodes N , mct ( N ) is the set of all the nodes contained by For any result R we provide separate formulas to evaluate the four kinds of features a result possesses: T ( R ) , H ( R ) , C ( R ) and S ( R ) . Each of the functions returns a real number between [0, 1], and the greater the value is R is better in one respect.  X  Content Information: T ( R )= kn ( R ) /t ;  X  Structure Information: H ( R )= dpt ( lca ( R )) /h ;  X  Compactness: C ( R )= | R | / | mct ( R ) | ;  X  Size: In the formula of S ( R ) , st is an integer which is the size threshold of R  X  X  MCT. When returned. Finally, the scoring function is defined as follows. In formula (1)  X  ,  X  ,and  X  are three adjustable parameters, each of which should be a positive real number greater than or equa l to 1. Furthermore, according to former discussion under normal circumstances  X  should be set much greater than  X  ,atthe same time  X  is usually larger than  X  . Hence we have  X &gt; X &gt; X   X  1 . It is easy to find that any score ( R ) is either equal to zero or between ( 1 / 2 (  X  +  X  +  X  ) ,1]. For an XML tree and given keywords, to find the optimal result set is certainly the ultimate goal of us. Nevertheless, there is a lack of an efficient way to achieve this purpose due to a complicated situation that any feature in the scoring function is allowed to be weighted casually. In a naive approach, we have to enumerate all the possible subsets of K to find the result with the greatest score which alone has a time complexity of O (2 n ) . Therefore, in this section we propose several heuristic algorithms with high efficiency to generate results hoped to be close to the optimal one. Especially, some of them are developed to adapt to specific circumstances. In the next section, many experimental results show the effectiveness of these algorithms. 3.1 Matrix Algorithm From Definition 1, the XML keyword search problem is equivalent to dividing the set of all the keyword nodes into groups that are meaningful, which is actually the clustering techniques are supposed to handle. In this subsection, we provide a basic agglomerative hierarchical cluste ring algorithm to obtain the result set R . Before conduct the algorithm (or any algorithm we propose later) the nodes in the XML tree have been encoded by Dewey code and the inverted index of term-node has been built. So that, each time the calculation of a score costs O (1) .

The input of the clustering algorithm is the set of all the keyword nodes K which is then transformed to a candidate set C that each entry C  X  C is a node set and originally contains an individual keyword node. Afterwards, a Score Matrix of C is built. Suppose |
K | = n and C = { C 1 , ..., C n } , then the score matrix of C is an n -by-n matrix M that each item m ij is set to be score ( C i  X  C j ) if score ( C i  X  C j ) is greater than both score ( C i ) and score ( C j ) ,otherwise m ij is 0. At each step we find the highest m ij in the matrix and merge C i and C j to be a new result in C . Then, the matrix is updated to be a | C | X | C | one for current C . The program stops when there is no positive value left in the matrix, then C is the final result.

Such an algorithm is called the Matrix algorithm and is shown in Algorithm 1. As we know the space complexity of this algorithm is O ( n 2 ) , and in the worst case the time complexity is O ( n 3 ) . If the scores are stored as sorted lists (or heaps), the time complex-ity is reduced to O ( n 2 logn ) . Still it is not as good as the performance of the algorithms from the LCA-based approaches. However, theoretically it generates much better re-sults because it calculates possible scores a s many as possible and always chooses the largest one.
 3.2 Content-Information-First (CIF) Algorithm Many believe that the Content information should be an overwhelming criterion to eval-uate a result and thus in our scoring function they would prefer  X  to be much larger than  X  and  X  . Under this circumstance, the results containing all the keywords should be re-turned as many as possible and a results with insufficient content information R will only be generated in two cases. First, there is no result R can be found that satisfies: (1) R  X  R contains all the keywords; (2) score ( R  X  R ) &gt;score ( R ) . Second, such a R can be found however the size of R  X  R exceeds the limit. Among these results with insufficient content information one definitely dominates another when it contains more keywords. In this subsection, we present an algorithm called the Content-Information-First (CIF) algorithm to retrieve such results.

Given a set of t keywords { w 1 , ..., w t } there are t sets of keyword nodes K 1 , ..., K t possessed through the inverted index and each K i stores all the keyword nodes contain-ing the keyword w i .Let K 1 be the one with the smallest size. In line with the principle that a keyword node only exists in a single result, we can only obtain | K 1 | results con-taining all the keywords at most. Thus, w e employ a program that for each set from K 2 to K t we distribute one or more keyword nodes in it to every node in K 1 which can form a result with highest score. We use an example to explain the details before presenting the CIF algorithm.

Let K = { K 1 , ..., K t } and R to be the set of results which is empty originally, then the pseudo-code of CIF algorithm is illustrated in Algorithm 2. In the best case (when the sizes of the keyword node sets are even) the time complexity of CIF is O ( | K 3.3 Structure-Information-First (SIF) Algorithm In some specific cases, we concern what the structure of a result much more than how much keyword information inside. For example, it is quite reasonable to assume that no matter how many keywords a result contains only those papers are qualified when proceeding keyword search on DBLP data set. In this case, we can set  X  close to or even smaller than  X  and  X  and then generate results based on some restrictions built on the structure. Here we provide an algorithm called the Structure-Information-First Algorithm (SIF) which actually comes from another work of us [13] in which it is called the Core-driven Clustering Algorithm. To save space we don X  X  explain the details here. Normally the time complexity of Algorithm SIF is O(n), and O( n 2 ) in the worst case. Extensive experiments are performed to compare our approaches with SLCA and ELCA approaches. For SLCA and ELCA approaches the Indexed Lookup Eager Algorithm [11] and the Indexed Stack Algorithm [ 12] are implemented respectively.

We use two metrics to evaluate them: r ecall and precision. Since we presume any keyword node is meaningful to users, the recall value can be simply calculated by di-viding the number of the keyword nodes in the result set with the number of all keyword nodes in the document. The standard definition o f precision from Information Retrieval is difficult to be followed here. Because in an y search, each result returned is believed to be a satisfying one, which means they all consider the precision of their result set to be 1. Therefore, we design a variation called the proximity precision. Out of the set of results we find the one with the greatest s core which is then considered as the best result and used as a standard . For a certain small number k , the top-k results with the largest scores are found and then an average score value is calculated which afterwards is divided by the greatest score to get the value of proximity precision.

Figure 3(a) and Figure 3(b) show that the re sults generated by our approach have a overwhelming recall value comparing w ith SLCA and ELCA. We can see that SLCA always gets a poor value because lots of keyword nodes are abandoned. Our approach always has the largest recall since any keyword node is considered meaningful and included in a result. Figure 3(c) and Figure 3(d) illustrate how the proximity precision values change when we vary the number of k eywords. The parameters are set to static values, and for any approach we only consider the scores of top-10 results. Obviously, Matrix and CIF overcome the other three. SIF doesn X  X  act quite good since in the scoring function  X  is set much larger than  X  and  X  . Similarly, when we give the parameters some static values and use three keyword s to search each of the data sets, Figure 3(e) and Figure 3(f) show how the proximity precision values change as the value of k varies. There is a dramatic decline while k is enlarged for SLCA and ELCA. This can be explained that they usually only find a few best results and omit those second-best ones. In the last two graphs in Figure 3 the keyword numbers are both 4. We can see that when we change the values of parameters Mat rix has a stable and excellent performance which is much better than SLCA and ELCA do. The most interesting thing is, for either CIF or SIF the proximity precision changes s everely with different parameter values. That X  X  why it is so important to select th e appropriate algorithms according to them. In this paper, several defects of LCA-based result models are explored. After that we propose a result model more effective and flexible which is mainly built upon a scoring function. Based on the novel model heuristic algorithms are provided to generate the keyword search results. Most importantly acco rding to the given values of some param-eters in the scoring function the most suitable algorithm can be automatically selected to generate the quality results efficiently. Finally, experimental results show that our approach outperforms any LCA-base d ones with higher recall and precision.

