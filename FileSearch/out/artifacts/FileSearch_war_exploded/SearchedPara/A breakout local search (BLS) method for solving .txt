 fi 1. Introduction
For a mechanical product A composed of n parts (i.e., A  X  { p , ... , p n }) with known positions and geometries, the Assembly
Sequence Planning (ASP) problem concerns with fi nding a sequence of l Z n collision-free assembly operations O  X  ( o 1 , o the assembled parts make up the product A . Since assembly of manufactured goods constitutes over half of the total production time in manufacturing industry, and about one-third to half of labor costs ( Su, 2007 ), ASP plays a key role in the whole lifecycle of a product and has a great impact on variation propagation, produc-
Assembly process constraints include topological relationships of the assembly, geometrical constrains and precedence relationships of parts or subassemblies, assembly direction changes, and issues the number of feasible assembly sequences. Optimal or near-optimal assembly sequences are those that minimize assembly cost or time while satisfying the above constraints. ASP is a highly combinatorial constraint satisfaction problem as the number of potential assembly sequences (i.e., the size of the search space of assembly sequences) is proportional to the factorial of the number of parts or components composing the whole product ( Wolter, 1989 ). In general terms, automatically generating feasible assembly sequences is, in its full generality, an extraordinarily dif fi cult task. It was shown to be
NP-complete problem and its solution is therefore believed to requiremorethanpolynomialtime( Wilson and Watkins, 1990 ). sequences may have four basic properties ( Homem de Mello and
Lee, 1991 ): (1) Sequentiality , which refers to the maximum number of moving subassemblies with respect to one another in any assembly operation. Sequentiality also addresses the number of required hands for assembling a product, and two-handed (also known as binary or sequential ) sequences are the simplest ASP problems. (2) Monotonicity , which refers to the need for intermedi-ate placement(s) of at least one part of the assembly. This means that in order for the problem to be solved, some parts must be moved more than once. (3) Linearity , which refers to the need for simultaneous insertion of more than one part in at least one stage of the operation. So, in a linear ASP problem all assembly operations include inserting of a single part into the rest of the assembly. (4)
Coherency , which refers to the need for attaching some parts as a subassembly before inserting it into the main assembly. In a coherent assembly all parts inserted into the assembly must touch some previously-assembled part(s), and therefore it is simpler than an incoherent assembly because the latter requires grasping or maintaining the stability of a subassembly before its assembly. In this article, Sequential, Monotone, Linear, and Coherent/Incoherent assembly sequence plans are considered.
 A closely related problem to assembly sequence planning is
Disassembly Sequence Planning (DASP), which computes a sequence of collision-free operations for rem oving the assembly parts from the fi nal product, having given the geometry of the fi nal product and the positions of parts in the fi nal product. The DASP problem has also beenshowntobeNP-complete,andasurveyonDASPispresentedin ( Lambert, 2003 ), which covers topics such as different ways of representing the space of feasible disassembly sequences: namely, (1) component-oriented approaches that consider automatic path generation, motion and stability ana lyses and collision detection, (2) product-oriented appr oaches that perform automatic analyses of the ability to decompose a product from its assembly drawing, (3) the hierarchical-tree approach which is based on the inverse Material
Requirement Planning (MRP) and relates disassembly processes to the hierarchical product structure, and (4) metaheuristic approaches that have been used to generate optimal or near-optimal disassembly sequences such as Particle Swarm Optimization (PSO) ( Tseng et al., 2011a, 2011b ), Ant Colony Optimization (ACO) ( Mi et al., 2011, Wang and Shi, 2014 ), Max  X  Min Ant System (MMAS) ( Liu et al., 2012 ), Genetic Algorithms (GA) ( Chen et al., 2012; Elsayed et al., 2011 ), and
Arti fi cial Immune Systems (AIS) ( Lu and Liu, 2012 ). 1.1. Related works
In order to meet product development requirements, numerous methods have been used to generate optimal or near-optimal asse-mbly sequences. Assembly generation methods lie in four general categories of (1) human interaction methods, (2) geometric feasible reasoning approaches, (3) knowledge-based reasoning approaches, and (4) intelligent methods. An inclusive survey on the ASP and its methods is presented in Jim X nez (2013) , which overviews the elements of sequence planning such as fi nding a feasible sequence, determining an optimal sequence according to one or more opera-tional criteria, representing the space of feasible assembly seque-nces in different ways, applying search and optimization algo-rithms, and satisfying precedence constraints existing between subassemblies.

Human interaction is the earliest approach in ASP which relies on user responses to program queries on the connection between a pair of parts, and the feasibility of a single assembly operation ( De
Fazio and Whitney, 1987 ). Since this approach was hardly directed toward factory automation, it was soon superseded by the geo-metric feasible reasoning approach.

The Geometric feasible reasoning approach is mainly based on the  X  assembly by disassembly  X  principle, which indicates that (under certain conditions) disassembly sequence is the inverse of assembly sequence. This is an important strategy for solving the ASP since a product in its assembled state is subject to far more precedence and motion constraints on its components than in its disassembled state the DASP problem is drastically reduced compared to that of its reverse (ASP) problem. There is a bijection between assembly and disassembly sequences when merely geometric constraints are concerned and all parts are rigid, but this relation does not remain correct when physics (e.g., gravity, friction) and motion control uncertainty are taken into account, or when some parts are deformable (as considered in ( Rakshit and Akella, 2013 )) or are toleranced (as discussed in Latombe et al. (1997) ). Some works employing the geometric feasible reasoning approach are Homem de Mello and Sanderson (1991) , Ou and Xu (2013) ,and Morato et al. (2013) . This approach suffers from combinatorial explosion and considers only geometric information of the parts, whereas non-geometric assembly information should also be considered in assembly planning in order to generate better assembly plans.
The Knowledge-based reasoning approach considers both geo-metric and non-geometric (such as human knowledge, past experi-ence in assembling similar parts, etc.) assembly information. In Dong et al. (2007) both geometric and non-geometric information is hierarchically organized in a connection semantics-based assembly tree, and in Hsu et al. (2011) a knowledge-based engineering system for promptly predicting a near-optimal assembly sequence is developed. The drawback of this approach is that the knowledge base is dif fi cult to construct and it is hard to match a given assembly with an existing assembly in the knowledge base.

In recent years, intensive research efforts have been put in developing intelligent methods to solve the ASP problem, as they have been able to improve the ef fi ciency of fi nding optimal assembly sequences while avoiding the combinatorial explosion problem as the number of assembly components increases. As an intelligent method, Arti fi cial Neural Networks (ANN) are used to encode the precedence knowledge by expressing the precedence constraints of AND/OR graphs either as the weights of connections between neurons of a Hop fi eld net ( Chen, 1992 ), or as the probability of each part to be assembled at each step in the neurons' outputs of an n n network ( Hong and Cho, 1995 ). Other works using back-propagation neural networks for assembly sequence optimization are Sinanoglu and B X rkl X  (2005) and Chen et al. (2008) .

After Bonneville et al. (1995) implemented GA in ASP for the time in 1995, many soft computing/metaheuristic algorithms were developed to solve the problem. Although numerous papers like Smith (2004) , Marian et al. (2006) , Tseng et al. (2004) ,and Wang and Tseng (2009) have used GA, this method in its original and basic form is inappropriate to be directly implemented to solve and optimize the ASP problem because its inherently binary strings of chromosomes is incompatible with permutation-like solutions of combinatorial problems such as ASP. Also, through the crossover recombination operator, the GA tends to generate infeasible off-springs that violate precedence constraints. To address this issue, researchers have used different approaches like penalty and repair strategies. As another improvement, Gao et al. (2010) proposed the Memetic Algorithm (MA) for the ASP to combine the parallel search nature of evolutionary algorithms with local search to improve individual solutions. The results showed that their proposed appr-oach could reach a tradeoff between exploration and exploitation of the search space.

As a major optimization algorithm, PSO has been used to the ASP problem by a number of researchers. Although the original PSO is not suitable to be directly applied to ASP problem due to its continuous-space nature, a discrete version of it was implemented in ( Lv and Lu, 2010 ). Wang and Liu (2010) introduced a chaotic operator to diversify updated particle positions in order to reduce premature convergence and trapping in local optima. To remedy this, Yu et al. (2009) introduced a two-formula mechanism of updating velocities (versus the usual single formula) in which one of the formulas was used randomly for each particle.

The Simulated Annealing (SA) method for ASP consists of generating a random initial sequence, creating a neighboring solu-tion, computing the energy corresponding to this new sequence, computing the Boltzmann probability of changing to the new energy state, and accepting the new sequence if this probability is larger than a random number in the interval [0, 1] (in order to escape local minima). In Motavalli and Islam (1997) and Hong and Cho (1999) , the SA algorithm was used with a multi-criteria objective function integrating total assembly time and number of reorientations. Motavalli and Islam (1997) and Hong and Cho (1999) applied the SA to minimize an energy function as the optimality criterion with a part-exchange operator for generating neighboring solutions. They compared their algorithm with a neural-network-based method for the assembly of an electrical relay product.

In the Arti fi cial Immune Systems approach to ASP, antibodies represent the assembly sequences of the product encoded in their genes as parts' numbers. Cao and Xiao (2007) implemented the
AIS for ASP and concluded that it outperforms the standard GA due to the immune selection mechanism which selects individuals (antibodies) for the next generation and chooses the best anti-bodies (with higher fi tness values) while at the same time favoring diversity; that is, by avoiding premature convergence and helping global optimization.

Wang et al. (2005) used the ACO algorithm to overcome a shortcoming of GA that highly depends on initial chromosomes. In their algorithm the positive feedback process, distributed computa-tion, and the greedy constructive heuristic search work together to fi nd the optimal or near-optimal assembly sequence quickly and ef fi ciently. Since one of the common drawbacks of the original ACO pertains to the positive feedback system that accumulates only good solutions, Guo et al. (2007) focused on resolving its premature convergence.

Several other heuristic methods such as Harmony Search (HS) ( Wang et al., 2013 ), Imperialist Competitive Algorithm (ICA) ( Zhou clonal algorithm ( Tiwari et al., 2005 ), bacterial chemotaxis algo-rithm ( Zhou et al., 2011 ), etc., have also been used to solve the ASP problem. A survey of works that have applied soft computing approaches in ASP has been presented in ( Rashid et al., 2012 ), covering the years 2001  X  2011.

In reviewing the literature of ASP methods it was noticed that a large majority of solution methods were in the category of intelligent methods, among which the metaheuristics comprise a large portion.
However, a question that arises here is that what type of metaheuristic is more appropriate for ASP. In fact, deciding upon implementing a metaheuristic with powerful intensi fi cation or diversi fi ities must be justi fi ed based on the problem's fi tness landscape char-acteristics, which gives a sense about the  X  shape  X  of the problem's solution space through describing the distribution, amplitudes, and correlation of its local optima.
 Despite the large number of metaheuristics applied to solve the
ASP problem, the landscape of this problem has not been analyzed in any research so far, and since the effectiveness of metaheuristics depends on the properties of the landscape associated with the problem to solve, in this paper the landscape of the ASP problem is analyzed for the fi rst time in Section 2 . Analysis of distribution and correlation measures shed light on the nature of the problem and guided us in implementing an effective and ef fi cient algorithm called Breakout Local Search (BLS) algorithm for the ASP problem for the fi rst time, which is presented in Section 3 in detail.
Experimental results, comparisons with various existing methods, and detailed statistical analyses are presented in Section 4 , and summarizing remarks and conclusions are provided in Section 5 . 2. Fitness landscape analysis of the ASP problem
In designing metaheuristic algorithms for solving optimization problems, two inconsistent strategies may be adopted: exploitation (intensi fi cation), and exploration (diversi fi cation). In intensi regions around local optima are explored more thoroughly in the hope to fi nd better solutions, while in diversi fi cation non-explored regions must be visited to ensure that all regions of the search space are evenly explored and that the search is not con fi ned to only a reduced number of regions. In this design space, the extreme search algorithms in terms improvement local search), as illustrated in Fig. 1 . In general, basic single-solution-based metaheurist ics (or S-metaheuristics) are more exploitation-oriented, whereas basic population-based metaheuristics (or P-metaheuristics) are more exploration-oriented ( Talbi, 2009 ). This does not mean of course that S-metaheu ristics (resp. P-metaheuristics) do not achieve exploration (resp. exploitation). Local search (LS) and simulated annealing (SA) are examp les of S-metaheuristics, and GA,
PSO, ACO, NN, AIS, and MA are examples of P-metaheuristics. There are also hybrid algorithms, which usuall y nest an S-metaheuristic inside a
P-metaheuristic, such as GA  X  SA ( Shu-Xia and Hong-Bo, 2008 ), GA ( Li et al., 2003 ), etc.
 designer must consider the properties of the problem's landscape as it in fl uences the effectiveness and ef fi ciency of the metaheuristic.
These properties include types of solution representation, neighbor-hood, and the objective function, which completely de fi ne the landscape of a problem. The Search space of a problem can be de fi ned as a hyper graph G  X  ( S , E ), in which the set of vertices S corresponds to the solutions of the problem represented by some encoding, and the set of edges E corresponds to the move operators used to generate new solutions. If the solution s q can be generated from the solution s p using a move operator once, there is an edge between these solutions and they are neighbors. If the search space is considered as a fl oor, then the landscape of problems can be made ing on the shape of the landscape, speci fi c types of search methods with certain exploitation and exploration capabilities will be more effective. By computing and analyzing statistical measures such as correlation and distribution of local optima over the landscape of the
ASP problem, we will be able to form an idea about the shape and ruggedness of the fi tness landscape and thus propose a suitable metaheuristic that will work well in that context. The most common motivation for fi tness landscape analysis is to gain a better under-standing of algorithm performance on a related set of problem instances. Therefore, in order to fi nd out what type of metaheuristics is suitable for a speci fi c problem, the landscape of the problem's search space should be analyzed ( Pitzer and Affenzeller, 2012 ). sample assembly products with vari ous geometries, number of parts, complexities, and dimensions. A si mple hill-climbing local search algorithm is developed and run on 5000 uniformly-sampled random starting assembly sequences (the set of which is called population U ), and then for each starting sequence in U (which is in the form of a permutation), the local search is run until a local optimum is achieved.
As a result, 5000 (not necessarily distinct) local optima are generated, the set of which is called population O .Inordertoperformalocal search on a random solution in Population U ,itis fi rst represented as an encoding ( Section 2.1 ), and a neighboring solution is generated by applying a neighborhood generation operator ( Section 2.2 ). If the current solution, then it replaces the current solution, otherwise, another random neighbor is generated and its fi tness compared with the current solution. This procedure is continued iteratively until at an iteration no better neighboring solution is found for the current solution after 100 trials. Thus, we allegedly call this solution a optimum  X  and add it to the Population O . After completely generating the Population O , some statistical measures of the distribution and correlation of local optima in the search space of the ASP problem are calculated ( Section 2.4 ), based on which fi tness landscape analysis is performed for the studied assembled products and the results reported in Section 2.5 . 2.1. Solution representation
A solution s to the ASP problem can be represented by the assembling sequence and directions of the n parts of the assembly
A in the form of a row vector of ordered pairs (  X  i , d i { p , p 2 , ... , p n } is the i -th element of the sequence and d y ,  X  y , z ,  X  z } represents the assembly direction of part 3D assemblies). Fig. 2 shows a typical encoding of a solution as  X  ( p , x ), ( p 3 ,  X  y ), ... ,( p 6 ,  X  x )  X  , which indicates that part must move at direction  X  x toward the assembly, then part (  X  p ) must move at direction  X  y toward the assembly, and so on.
Since the total number of possible sequences of n parts is n !, and the number of possible directions to assemble a particular part is 2 m , then the size of the whole search space (including feasible encoding has 2 n cells, the maximum distance between two solutions in the search space (i.e., the diameter of the search space) is 2 n , as all cells in two solutions may be different. 2.2. Neighborhood generation operators
In order to conduct the fi tness landscape analysis, a set of 5000 solutions are generated randomly as the initial population U , and a steepest descent (hill-climbing) local search is performed on each initial solution to reach a solution with locally maximum value. For this purpose, fi rst we need to generate neighboring solutions, which is done by four different neighborhood genera-tion operators in this work. These operators are exchange, inser-tion, inversion, and fl ip, as explained below:  X 
Exchange operator : This operator randomly selects two pairs of the sequence and exchanges (swaps) their positions in the encoding, as shown in Fig. 3 (a). The neighborhood size for any solution through applying this operator is C ( n ,2)  X  n ( n 1)/2.  X 
Insertion operator : This operator randomly selects a pair in the sequence and relocates to another position in the string, such that all pairs after that position shift one place to the right, as shown in Fig. 3 (b). The neighborhood size for any solution through applying this operator is n n  X  n 2 , since n pairs can be selected as the start, and n positions (the position after the last pair included) can be selected as the target of the relocation.  X 
Inversion operator : This operator randomly selects two posi-tions in the solution (two parts of the product) and reverses the position of parts between them in the sequence, as shown in
Fig. 3 (c). The neighborhood size for any solution through applying this operator is  X  n 2 i  X  1  X  n i 1  X  .  X 
Flip operator : This operator randomly selects a pair in the sequence and changes its assembly direction randomly to another direction, as shown in Fig. 3 (d). The neighborhood size for any solution through applying this operator is (2 m 1) which m A {2, 3} is the dimension of the assembly.

It is important for neighborhood generating operators to have the connectivity property, which guarantees reaching any solution from any other solution through successive application of the operators. It can be shown that for any two solutions s p (which are in fact nodes of the search space G ), our used neighborhood generation operators have the connectivity prop-erty: by successively implementing each of the exchange, inser-tion, or inversion operators on a given solution, all possible sequences of parts can be obtained, and successively applying the fl ip operator can generate all assembly directions for any part in a given sequence. Therefore, it is necessary to use the operator in conjunction with the other three operators in order to guarantee the connectivity property, such that when the above operators are applied randomly over an in fi nite time interval, there will always be a path between every two solutions in G , and the probability of reaching the global optimal solution s from any initial solution tends to 1. Nevertheless, our developed method, as all other metaheuristics, aims to reduce the search time as much as possible and improve the quality of solutions obtained.

In each iteration of the local search, the above neighborhood generation operators are selected uniformly at random and applied on the current solution representation. Therefore, the maximum number of neighbors of a solution s will be:
N  X  s  X   X  max n  X  n 1  X  = 2 ; n 2 ;  X  n 2 i  X  1  X  n i 1  X  2.3. Fitness of solutions
Solving an ASP problem requires fi nding assembly sequences and directions that are both feasible and high quality as measured by a quality metric. In fact, these two properties guide us in selecting the  X  best  X  neighbor(s) of a speci fi c solution during the search process and hence converging to desired fi nal solutions. The feasibility and fi tness (quality) of a solution are discussed in the following subsections. 2.3.1. Feasibility of a solution
Assume BB (  X  i ) be the bounding box of the parts  X  j ( j  X  1, assembled prior to  X  i which has axes parallel to the main Cartesian BB (  X  i ) and ending at the fi nal con fi guration of  X  i 5 5 5 5 5 5 5 5
Then, a solution s  X   X  (  X  1 , d 1 ), (  X  2 , d 2 ), ... ,( is feasible if there is no already-assembled part  X  j ( 8 j along AV (  X  i , d i ). Put more formally, if cl( V (  X  i the volume occupied by part  X  i in its fi nal con fi guration, and SV  X  i represent the swept volume of part  X  i along AV (  X  i the workspace, then a solution s is feasible if: SV
Since all solutions at each iteration of the local search need to be checked for feasibility, in order to expedite the search we pre-calculate all possible collisions between any two parts at all directions and organize this information as an n n 2 m matrix called Assembly Interference Matrix (AIM). Each entry of the AIM is denoted by a binary variable I AV  X  p i ; d i  X  p does not block the movement of part p i along direction d takes 1 otherwise, as de fi ned below: An AIM is created for a sample part illustrated in Fig. 9 . ::: 000000 2 6 6 6 6 6 4 2.3.2. Quality of a solution
In this paper, as in some other works on ASP, the objective function is to simultaneously minimize the total number of Assembly Direction Changes (ADC) and maximize the number of Satis fi ed Geometric Constraints (SGC) of assembly sequences. ADC (  X  i ) is a binary variable indicating the change of assembly direction between parts  X  i and  X  i 1 in the solution encoding:
ADC  X   X  i  X  X 
Also, SGC (  X  i ) is a binary variable that takes value 1 if no part assembled prior to part  X  i blocks its movement along AV ( Using the AIM , this condition is expressed mathematically in (6) . Obviously, SGC (  X  1 )  X  1.
 SGC  X   X  i  X  X  1if
Now the total fi tness function can be de fi ned as a weighted sum of the above objectives expressed as follows: Maximize f  X  s  X  X  w 1
Maximizing the fi rst term in (7) is equivalent to minimizing the total number of assembly direction changes, and maximizing the second term is equivalent to maximizing the total number of feasible assembly operations. In order to bias the search toward feasible solutions, we set the values of the weighting factors as w  X  1 and w 2  X  9 empirically by testing several sets of these factors. 2.4. Landscape properties describe fi tness landscape properties of the ASP problem, namely, (1) Distribution measures, that study the topology of locally optimal solutions in the search space and the objective space, and (2) Correlation measures, that analyze the rugosity (rugged-ness) of the landscape and the correlation between the quality of solutions and their relative distance, as well as the correlation between the quality of solutions and their distance to the best-known solution. The following measures are discussed and calcu-lated for our fi ve sample assemblies: (i) local optima distribution, (ii) entropy in the search space, (iii), distribution in the objective space, (iv) length of the walks, (v) autocorrelation function, and (vi) fi tness  X  distance correlation, among which the fi rst three measures pertain to distribution measures, and the last three belong to correlation measures. Most of the material in this subsection is taken from ( Talbi, 2009 ).
 concentration of a population P of solutions in the whole search space S . The normalized average distance of a population P is de fi ned as follows: 0 r Dmm  X  P  X  X  where dist ( s , t ) denotes the Hamming distance of two solutions, and diam ( P )  X  max dist ( s , t ); 8 s , t A P is the diameter of the
Fitness Fitness population. A near zero Dmm indicates that the solutions in population P are concentrated in a small region of the search space S . The variation of average difference between Dmm of the starting population U and the fi nal population O is de fi
Entropy in the search space : Entropy is an indicator for the diversity of a given population in the search space. Weak (near zero) entropy implies concentration of solutions, whereas high entropy shows signi fi cant dispersion of the solutions in the search space. Since there is no general-purpose formulation for entropy as it depends on the optimization problem at hand, we have worked out a novel entropy formulation for the ASP problem as follows: 0 r ent  X  P  X  X  where N ijd is the number of solutions in P in which part p j -th position of the sequence and is assembled along direction d ,or 6 correspond to x ,  X  x , y ,  X  y , z ,  X  z directions. When all solutions of the population P were similar, then for speci On the other hand, if all solutions were totally different, then entropy variation between the uniformly random and local optima populations is de fi ned as follows: distribution of solutions in the objective space, the amplitude indicator Amp ( P ) is used to measure the difference between the fi tness values of best and worst solutions in the population P ,as de fi ne in (12) . Small values of Amp ( P ) denote a relatively search space. Relative variation of the amplitude between a starting random population U and the fi nal population O is given by (13) . Also, the average distance between a population O of local optimal solutions and the best found solution ( s n ) is called the  X 
Gap  X  of the population, as de fi ned in (14) . A small Gap implies that the considered problem is easy to solve.
 Amp  X  P  X  X  P jj  X  Amp  X  U  X  Amp  X  O  X  Amp  X  U  X  r 1 ;  X  13  X  1 r Gap  X  O  X  X 
Length of the walks: The average length of the walks of a population P reveals some information about the ruggedness of the landscape. In an unsmooth landscape, the number of local optimal solutions and therefore the length of the walks between an initial random solution and a fi nal local optimum solution are short. On the other hand, in a smooth landscape the length of the walks is longer. The average length of the walks of the population O of local optimal solutions is de fi ned as follows: Lmm  X  O  X  X  where l ( s ) is the number of required walks to achieve solution s from its corresponding initial solution. Indeed, it is the number of times the neighborhood generation operators are applied to the initial solution until its corresponding local optimal solution in the population O is reached. Also, | O | is the cardinality of the popula-tion O that is set to 5000 for our analysis.
 Autocorrelation function: The autocorrelation function  X  ( d measures the correlation of all solution pairs with Hamming distance d in the search space, and investigate the effect of the distance between solutions on their fi tness variation. Since construction of the function  X  ( d H ) for problems with too many parts is formidable, one way to estimate this function is t o create a population of solution pairs with mutual distance d H and calculate their autocorrelation using (16) . The size of this population was set to 10,000 in our analysis. If |  X  (1) | E 1, then two neighboring solutions have almost equal fi tness values, and so the landscape is smooth. For a population of n pairs of solutions with distance d H from each other, the autocorrelation function is de fi ned as follows: 1 r  X   X  d H  X  X 
Fitness  X  Distance correlation: The Fitness  X  Distance Correla-tion (FDC) analysis measures the correlation of the fi tness of a solution and its distance to the global optimal or the best-known solution. To perform this analysis, fi rst a population of n randomly-generated solutions is created and then the set F  X  { f 1 containing the fi tness value of solutions, and the set D  X  { d ... , d Hn } containing the distance of the solutions to the best-known solution are computed. Afterwards, the fi tness-distance correla-tion coef fi cient r is computed as follows: 1 r r  X  where  X  f and  X  dH are the standard deviation of the fi tness and distance measures, and the numerator of (17) is the covariance between the sets F and D .If r E 1, then the direct correlation between the fi tness of the solutions and their distance to the best-known solution is strong, and so the problem is easy to solve. On the contrary, r E 1 means that the problem is  X  misleading meaning that the move operator will guide the search away from the global optimum. By knowing this information, solving the problem will be not hard. The hardest situation arises when which indicates that there is no correlation between the fi the solutions and their distance to the best-known solution, and thus no useful rule for biasing the search in the solution space can be found. A convenient method for visualizing the FDC analysis is through using fi tness  X  distance plot, which depicts the the solutions versus their distance to the global optimal solution.
We have also used this tool for our analysis in the next section. 2.5. Landscape analysis results
Since the fi tness landscape of a problem depends on the structure of the problem instance and the method that neighbor-ing solutions are constructed, we implemented four neighborhood construction schemes (operators) on fi ve different ASP instances (products) (as illustrated in Fig. 4 ) in order to reach a reliable conclusion about the shape of the general ASP problem's fi landscape. For this purpose, 5 4  X  20 scenarios with different combinations of products and neighborhood operators were designed and the hill climbing local search algorithm was applied 5000 times for each scenario to obtain local optimal solutions (in total, 80,000 runs). Then, the statistical measures introduced above were calculated for each scenario, the results of which are reported in Table 1 . In the following subsections we provide our critical analyses on the ASP landscape. 2.5.1. Distribution measures
In order to evaluate the distribution of the local optimal solutions obtained from the hill climbing approach across the landscape, the variation of the local optima distribution in the search space (  X  Dmm ) and the variation of entropy in the search space (  X  ent ) play a key role. The following combinations of these two measures reveal some facts about the scatter of local optima across the search space ( Talbi, 2009 ): (1) If both  X  Dmm and  X  ent are low, then the search space is called (2) If both  X  Dmm and  X  ent are high, then the search space is called (3) If  X  Dmm is high and  X  ent is low, then the search space is called
The last row of the Table 1 provides the average values of the used statistical measures in all scenarios, and thus can re landscape properties of the general ASP problem. It is observed that  X  Dmm  X  0.0046 and  X  ent  X  0.0022, which are small values and thus according to the case (1) above, imply that the local optima are scattered over the search space uniformly. On the other hand,
Amp is a measure in the objective space and does not concern distribution of local optima, and its relatively small value ( shows that the landscape is almost plain in a 2 n  X  1-dimensional space and the quality of local optima is not much better than initial random solutions. Note that a plain landscape may be rugged (non-smooth) based on values of the correlation measures.
On the other hand, the large average gap of the obtained local optimal solutions with global solutions (about 59.3% worse) indicates that fi nding global optimal solutions in the ASP problem is very dif fi cult and the basic local search method is not able to solve the problem effectively. 2.5.2. Correlation measures together with the correlation between the quality of solutions and their distance to a global optimal solution.

Lmm ( P ) is, the more rugged the space will be, since in a rugged space the number of local optima is large and we encounter a local optimum within a few search iteration. However, as Table 1 indicates, the value of Lmm ( P ) depends on the type of neighbor-hood generation operator, and in the studied scenarios, the exchange  X  fl ip operators produced shorter walks compared to other operators, which indicates their faster convergence to a local optimal solution and more rugged landscape. Therefore, using only this combination of operators is not recommended.
For the autocorrelation measure, we calculated it for three Ham-ming distances of 1  X  3. The results showed that on average,  X  (2), and  X  (3) are very small and almost equal, indicating that the variation of fi tnesses between 1-neighbors, 2-neighbors, and 3-neighbors are equal on average to the variation between any two solutions, and thus the landscape is rugged. Such a weak correlation between neighboring solutions con fi rms that the ASP problem is hard.
 very small, which shows there is no correlation between the of solutions and their distance to the best-known solution. There-fore, achieving the global optimal solution is very dif fi needs an effective search algorithm. For the puzzle-like assembly product ( Fig. 4 (a)) and for each neighborhood generating operator, we also plotted the fi tness of 5000 local optimal solutions versus their distance to the best-known solution (the fi tness  X  plots), as shown in Fig. 5 . Solid (red) dots indicate feasible, and hollow (blue) circles show infeasible solutions. As stated earlier, the maximum Hamming distance between any two solutions of an n -part assembly is 2 n , and so for the puzzle-like product, the values on the abscissa in FD plots will be at most 38.
 1. All local optimum solutions have distances to the best-known 2. At many fi tness  X  distance points, feasible (red) and infeasible 3. The number of feasible solutions for the XF, SF, VF, and ALL
Insertion, and Inversion operators together with the Flip operator, the combination of the ALL operators is a better choice for neighborhood generation of the ASP problem. Based on the above analysis and for the ALL operators, the F  X  D plots of the benchmark assembly products in Fig. 4 are illustrated in Fig. 6 . It is observed that there is no correlation between the distance to the best known solution and the fi tness value of local optimal solutions.
Also, the number of found feasible solutions were 92, 14, 0, and 0 for the Industrial assembly, electrical relay, Generator and tank, respectively, which exposes that fi nding feasible solutions for the generator assembly is extremely dif fi cult due to the very limited number of feasible assembly sequences. 3. Break-out local search (BLS) for the ASP problem
In the Section 2.3.2 we mentioned that the goal of the ASP problem is to fi nd the best sequence of assembling the parts of a product while minimizing the number of assembly direction changes and maximizing the total number of satis fi ed geometrical constraints. On the other hand, analysis of the problem's landscape showed that it is a rugged plain with uniformly distributed local optima, and so a single-solution-based metaheuristic algorithm may work well. But since the number of feasible solutions for certain ASP instances can be as few as 0.4% of all solutions, an
S-metaheuristic with effective exploration (perturbation) capabil-ities is needed. Such requirements properly match with the qualities of the Breakout Local Search (BLS) algorithm developed recently in ( Benlic and Hao, 2013a ) for solving the maximum clique problem. BLS has also been successfully applied to solve other combinatorial optimization problems like minimum sum coloring ( Benlic and Hao, 2011 ), quadratic assignment ( Benlic and
Hao, 2013c ), maximum cut ( Benlic and Hao, 2013b ), and the vertex separator problem (VSP) ( Benlic and Hao, 2013d ). Encouraged by the fi ne results of the BLS in those domains and considering its powerful exploitation and exploration capabilities, we selected it for solving the ASP problem. The general framework of the BLS algorithm and its components are detailed in the following subsections. 3.1. General framework
BLS can be considered as an Iterated Local Search (ILS) algo-rithm that uses information of the search history for perturbing its solutions. The basic idea behind BLS is to use local search in each iteration to discover a local optimum and then employ adaptive diversi fi cation strategies to move from the current local optimum to another in the search space. Thus, exploration of new search areas is achieved by continually and adaptively applying weak to strong perturbations, depending on the search state. BLS starts from an initial solution s 0 and applies a hill-climbing local search to reach a local optimum s n . At each iteration of the local search algorithm, the current solution is replaced by a randomly gener-ated neighboring solution that improves the objective function. A local optimum is said to be reached if after L max attempts no improving neighbor was found. At this point, BLS tries to escape (  X  break out  X  ) from the current local optimum and move to a neighboring solution perturbed by a number of dedicated moves to the current optimum s n . Each time a local optimum is perturbed, the new solution is used as the starting point for the next round of the local search procedure. The whole procedure iterates maxiter times. Details of the BLS method are presented in Fig. 7 .

As indicated in Fig. 7 , the BLS algorithm requires the following fi ve subroutines to be executed:  X 
Initial_Solution(): For generating a starting sequence (point) for the search.  X 
Local_Search(): A hill-climbing local search procedure for exploiting a part of the neighborhood of a solution in order to improve the current solution, and terminates if a better solution is not found.  X 
Jump_Magnitude(): Determines the number M of perturbation moves (or jump magnitude).  X 
Perturbation_Type(): Determines the type of perturbation moves ( PT ) among alternative neighborhood generation opera-tors (exchange, insertion, inversion, and fl ip).  X 
Perturb(): Applies the selected perturbation PT operators M times to the current local optimum. The perturbed solution becomes the new starting point for the next round of the Local_Search().

Details of the above procedures are provided below. 3.2. Initial solution generation
Although an initial solution for the BLS algorithm can be constructed by creating a random permutation of parts associated with random assembly directions, we propose a novel greedy heuristic algorithm for generating an initial solution which prior-itizes assembly operations according to the number of satis geometric constraints before and after adding a particular part to the current subassembly. In this way, the probability of creating a  X  proposed initial solution generation algorithm are as follows:
At fi rst, the Directional Assembly Interference Matrix ( DAIM )is constructed using the AIM , as below: DAIM  X  p i ; d i  X  X 
Then, among all directions, a random direction d 1 is selected for assembling the fi rst part of the sequence, and based on that, the fi rst part to be assembled (  X  1 ) is selected to be the one that causes minimal violation of the precedence relations for assembling other parts along the direction d 1 , as follows:  X  p i DAIM  X  p i ; d 1  X  r DAIM  X  p k ; d 1  X  ; k  X  1 ;:::;
Assume that a subassembly with m parts (together with their assembly directions) is de fi ned as Asbl m  X  {(  X  1 , d 1 ( m , d m )} and we intend to select the next part of the assembly, i.e., ( m  X  1 , d m  X  1 ). For this purpose, we de fi ne two more matrices: (1)  X 
Assembled Blocking Parts (ABP)  X  , for recording the number of violated precedence relations if p i is added to Asbl m for each direction, and (2)  X  Unassembled Blocked Parts (UBP)  X  , for recording the number of violated precedence relations when all unas-p for each direction. Mathematically, each element of these matrices equals to: ABP  X  p i ; d i  X  X  8 i  X  1 ; ... ; n ; d i A f x ;  X  x ; y ;  X  y ; z ;  X  z g UBP  X  p i ; d i  X  X  d i A f x ;  X  x ; y ;  X  y ; z ;  X  z g :  X  20  X 
Now, the next assembly part and direction pair (  X  m  X  1 , d the one for which the sum of the numbers of violated precedence relations for already assembled and still unassembled parts is minimum:  X  X  X  p i ; d i ABP  X  p i ; d i  X  X  UBP  X  p i ; d i  X  r ABP  X  p
When two or more parts have equal ABP  X  UBP sums, the part is selected that has smaller ABP value. The aforementioned routine is continued until all parts of the product are added to the assembly sequence. The pseudocode of this algorithm (Algorithm 2) is pre-sented in Fig. 8 , and a representative example for a simple product with four parts is shown in Fig. 9 .
 3.3. Local search and neighborhood generation
The BLS algorithm utilizes a simple hill-climbing local search proc-eduretomovetowardsthe fi nal solution (lines 6 and 14 of Algorithm 1). In each iteration of the local search, a neighborhood generation order to obtain a new solution, which replaces the current solution if it improves the objective function. In each iteration, if the current solu-tion is not improved after L max attempts, then it is deemed a optimum  X  . Selection between Insertion and Flip neighborhood gen-eration operators is based on the roulette wheel selection strategy, which opts for the Flip operator with a probability of p F
Insertion operator with a probability of p S (  X  0.7). Pseudocode of the implemented local search procedure (Algorithm 3) is shown in Fig. 10 . 3.4. Jump magnitude determination
As the basic hill climbing local search algorithm gets stuck to the fi rst found local optimum (which may be very inferior to the global optimum), the BLS method implements a perturbation mechanism to  X  jump  X  to new, hopefully better, local optima. At each main iteration of the BLS algorithm, a local optimum ( s found by the Local_Search() procedure and then perturbed weakly or strongly, depending on the state of search. The magnitude (weakness or strength) of perturbation ( M ) is expressed as the number of times a neighborhood generation operator is consecu-tively applied to the solution s n , which fl uctuates between M and M max . Determining the Jump Magnitude is realized through a simple algorithm presented in Fig. 11 , in which the counter enumerating consecutive non-improving local optima plays a decisive role. Since in the early iterations of the search, (where N max is the maximum allowed number of consecutive non-improving local optima), BLS performs a weak perturbation by selecting a small jump magnitude, i.e., M  X  M min , which is hope-fully just strong enough to escape the current local optimum s and fall under the in fl uence of a neighboring local optimum. If the jump was not suf fi cient to escape s n (i.e., the search returns to the last local optimum found, or s n  X  s lo ), BLS perturbs s strongly by setting M  X  M  X  1. After visiting local optima for N time without improving the best solution found so far BLS applies the strongest perturbation ( M  X  M max ) in order to drive the search categorically toward a new and distant region in the search space. After deciding the jump magnitude, the types of perturbations must be determined, as described next. 3.5. Perturbation type determination The Breakout Local Search method employs the Flip, Exchange, Insertion, and Inversion operators (introduced in Section 2.2 )to perturb solutions and explore new regions of the search space. The selection among different types of perturbation operators is done based on the ratio  X  / N max : the larger the ratio, the stronger the perturbation should be. Generally, at the beginning of the search, / N max o 0.25, and so a mild perturbation seems suf fi cient for escaping the current local optimum, which is realized by applying a Flip operator for just changing assembly directions. When 0.25  X   X  / N max o 0.50, the Exchange operator is selected as the Perturbation Type ( PT ) to moderately perturb the solution. For when 0.50  X   X  / N max o 0.75 and  X  / N max Z 0.75 the Insertion and
Inversion operators are selected to bring about strong and radical perturbations, respectively. Once a search stagnation is detected, that is, the best found solution has not been improved after N visits of local optima (i.e.,  X  / N max 4 1), BLS applies the Inversion operator to break out towards distant regions of the search space and resets the  X  . Fig. 12 shows pseudocode of the Perturbation_-Type procedure.

Subsequently, through the Perturb procedure, the selected perturbation operator is applied M (jump magnitude) times to yield the perturbed solution s p . Afterwards, the Local_Search procedure is run starting from s p until a new local optimum s n is reached and compared with the best solution found so far ( s best ). In case of an improvement in the objective function, s best is replaced with s ; otherwise,  X  is incremented and the next BLS iteration starts.
The parameters of the proposed BLS algorithm were deter-mined by extensive experiments with different sets of values, and their best values are presented in Table 2 . 4. Experimental results
In this section, the results of implementing the BLS algorithm for assembly sequence planning are presented. The program was coded in
MATLABandrunonaPCwithanIntelPentium s IV 2.8 GHz CPU and 2.45 GB of RAM. The performance of the BLS algorithm was compared to that of various algorithms existing in the ASP literature, including
Simulated Annealing (SA) ( Hong and Cho, 1999 ), Genetic Algorithms (GA) and Memetic Algorithms (MA) ( Gao et al., 2010 ), Immune
System-Particle Swarm Optimization hybrid method (IPSO) ( Zhang et al., 2013 ), and Harmony Search (HS) ( Wang et al., 2013 ).
Iterative Local Search (ILS) methods to solve the ASP problem for the purpose of evaluating the effectiveness of the perturbation mechanism of the BLS. Both MLS and ILS algorithms used the same objective function, hill climbing local search (with Insertion and
Flip operators), and initial solution and neighborhood generation methods, as in the BLS method. However, instead of employing the perturbation mechanism in BLS, the MLS simply starts from a random solution in each iteration, and the ILS iterates by conduct-ing a new local search from a solution that is a perturbation of the last local optimum via the Inversion operator. Parameters of the implemented algorithms, as originally proposed by their devel-opers (except for the MLS and ILS algorithms that were tuned by us), are listed in Table 3 .
 paring the aforementioned algorithms: (vi). FFE : Average number of Fitness Function Evaluations (smal-(vii). f : Average of fi tness values of all generated solutions in 30 (viii). n (ix).

While in some works on ASP, infeasible assembly sequences (those that violate geometrical constraints) are discarded and therefore satis fi ability of geometrical constraints do not appear in the objective function, in this article we incorporated the number of satis fi ed geometrical constraints (SGC) into the objec-tive function. However, due to the large weight given to the SGC ( w  X  9 w 1 ) in the objective function (7) , larger values of the metric (ix) leads to larger f .
 For comparisons, we solved the industrial assembly problem in
Fig. 4 (b) by the BLS method and compared its performance metrics with those obtained by the GA and MA ( Gao et al., 2010 ), IPSO ( Zhang et al., 2013 ), and HS ( Wang et al., 2013 )population-based metaheuristics, as reported in Table 4 . The stopping criterion of the algorithms was set to 30 iterations. The global optimal assembly sequence, which is feasible and has no reorientations, was gener-ated after 25 iterations of the BLS method with a fi tness value (  X  110) better than the other algorithms, except for the HS, which was equally fi t. The BLS also outperformed the other algorithms in the average number of satis fi ed geometrical constraints and the average fi tness value thanks to the effective methods used for generating the initial solution, neighboring solutions, and perturbed solutions. The results in Table 4 show that considering the land-scape shape of the ASP problem, the single-solution-based BLS method was more successful than the population-based methods.
In order to further compare the performance of the algorithms, we solved all the fi ve problems shown in Fig. 4 by our designed BLS, MLS, ILS, as well as the SA ( Hong and Cho, 1999 ), GA and MA ( Gao et al., 2010 ) methods, the results of which are presented in Tables 5  X  7 . Here the stopping criterion of the algorithms was set to 100 iterations. The results in the above Tables reveal the following facts: 1. The BLS outperformed the other methods in all fi ve benchmark problems in terms of best fi tness value f ( s n ), the average of the best fi tness value f ( s n ), and the success rate. This fact shows the reliability of this algorithm in optimality and feasibility aspects. 2. While the average FFE value of the BLS algorithm was in 1.3 times that of the GA (which had almost the least average
FFE among others), the BLS dominated GA in all criteria of all solved problems. 3. The MA had the best average same-direction assembly seq-uences in all (except for the puzzle-like assembly) problems thanks to its guided local search which is based on reducing the number of assembly direction changes by sequencing compo-nents whose assembly directions are the same. 4. The Swap  X  Exchange operators used by the SA algorithm caused more minor perturbations to the solutions and hence premature convergence to a relatively good (but not the best) solution, which led to superior average fi tness values of all generated solutions  X  f  X  and average number of satis fi ed geometrical con-straints electrical relay problems. For problems with more complex interference relations (i.e. the puzzle-like assembly) or larger number of parts (i.e. the generator and tank assemblies), the somewhat premature convergence of the SA algorithm gener-ated solutions with larger gaps from the best known solution, while the BLS algorithm had better performance in the f and 4.1. Statistical analysis
In order to statistically analyze and verify the signi fi outperformance of the BLS algorithm, two nonparametric statistical tests have been conducted: (1) the  X  Friedman two-way ANOVA by ranks  X  test ( Friedman, 1937 ) for detecting any signi fi between behaviors of all algorithms, and (2) the  X  Kruskal one-way ANOVA by ranks  X  test ( Kruskal and Wallis, 1952 )for detecting a signi fi cant difference between the behaviors of a pair of algorithms. Note that parametric tests could not be used here because of their assumptions on independency, normality, and homoscedasticity of the variables ( Derrac et al., 2011 ). nonparametric test and a counterpart to the parametric two-way
ANOVA used to detect signi fi cant differences between the means of data from several groups. In our context, the null hypothesis (H 0 ) of the Friedman's test will be:  X  The six algorithms BLS, SA, GA,
MA, MLS, and ILS have equivalent performances in solving the benchmark ASP problems  X  , and the Alternative Hypothesis (H claims the opposite of H 0 . An important result of the Friedman test is the relative performance ranks of the algorithms, the most superior one being ranked fi rst (for maximization criteria), and so on. The average relative rank r j i of each algorithm j in solving each benchmark problem i for 30 times is determined according to its performance in some selected criteria, and the global rank R that algorithm is calculated by computing the average of its relative ranks.

Another output is the p -values related to the similarity bet-ween algorithms, which indicate the maximum errors incurred when rejecting the H 0 and accepting the H 1 . For example, a p -value of zero indicates that the algorithms are de fi nitely (with 100% of con fi dence) different. A commonly used con fi dence level for the p -value is  X   X  0.05; thus, if the calculated p -value of a statistical test is larger than 0.05, we can reject the H the difference of algorithms. Table 6 summarizes the numerical results of performing the Friedman test for the six algorithms and the fi ve benchmark problems. The p -values in the Table 8 show that performances of the algorithms in all criteria are signi different at a con fi dence level of 100.0%. Also, values of the four important criteria vary from one problem to another.

Now that there are signi fi cant differences between the perfor-mances of all algorithms, we conduct a Kruskal  X  Wallis test to characterize these differences for each pair of the algorithms. The
Kruskal  X  Wallis one-way analysis of variance by ranks test is a pairwise comparisons nonparametric test and an analog of the parametric one-way ANOVA ( Kruskal and Wallis, 1952 ). Here we conducted 5 5 4  X  100 tests using the best solutions of 30 runs of each algorithm as the test population of the respective method, and with a null hypothesis stating that  X  The performance of the BLS algorithm is equivalent to the performance of algorithm i in problem j and criterion k  X  . If the null hypothesis for a speci fi speci fi c criterion is rejected (i.e., when p -value o  X  ), then the BLS algorithm stochastically dominates the compared algorithm.
For each pair of BLS and another algorithm, the Kruskal-Wallis test fi rst sorts all the performance values regardless of their generating algorithms and computes the relative rank of each value (in case of a tie their average value is set as their common rank) and assigns to its respective algorithm. The sum of the ranks for each algorithm is calculated and through a formula representing the variance of the ranks among algorithms (with an adjustment for the number of ties), an H statistic is calculated, which approximately has a Chi-square distribution. The p -value is then determined as
Pr  X 
Z H  X  , and if it is smaller than the con fi dence level  X  the Null hypothesis is rejected. Numerical results of performing the
Kruskal  X  Wallis test for the six algorithms, fi ve benchmark problems, and four performance criteria are reported in Table 9 .
In each column of Table 9 , since all the four performance criteria are of maximization type, ranks larger than 1 (shown in boldface) indicate that the BLS was better than the compared algorithm and criterion for a speci fi cproblem,and(1 p ) denote the con level of stating that one of the algorithms outperform the other one. The O BLS and U BLS parameters in the last two rows count the times BLS outperformed and underperformed relative to a competing algorithm in a speci fi ccriterion.
 It is interesting to observe that most underperformances of the BLS relative to other algorithms occurred in the third criterion ( n method, whenever the ratio  X  / N max increases, more severe pertur-bation methods (via Insertion and Inversion operators) are applied with larger magnitudes, as a result of which the number of assembly direction changes (ADC) tends to increase as well. Moreover, due to the small weight of the term w 1 function (7) (that is, w 1  X  1/9 w 2 ), the BLS is less sensitive to the assembly direction changes criterion compared to the average number of satis fi ed geometrical constraints, and so it accepts solutions with large ADCs but small SGCs. That is why BLS generally produces highest percentage of feasible solutions with relatively large number of assembly direction changes.

Further analysis of the test results for each compared algorithm is as follows: 1. SA: The BLS algorithm outperformed the SA in all criteria for products 4 and 5, with at least 99.7% con fi dence. In the (puzzle-like) product, while BLS was also signi fi cantly better than the SA method in the fi rst three criteria, the same claim cannot be made with a suf fi ciently high con fi dence level for the fourth criterion (the average number of satis fi ed geometrical constraints, the product 2, BLS outperformed the SA in the fi rst ( f ( s third ( n but was inferior to it in the second and fourth criteria because of the fact No. 4 mentioned below Table 4 . For the product 3, although the BLS overtook SA in the third criterion, no conclu-sion can be made about the superiority of either BLS or SA due to p -values larger than the acceptance level. 2. GA: The BLS algorithm outperformed the GA in all criteria for all products with at least 99.3% con fi dence, except in the third criterion of the third (electrical relay) product, which is due to the fact that all the solutions produced by the GA for this product were infeasible (as indicated by its zero success rate SR % in the Table 6 ) but with small assembly direction changes. 3. MA: For the product 1, the BLS outperformed the MA method in all four criteria with at least 98.3% con fi dence. For the products 2 and 3, while the BLS overtook MA in the fi rst criterion (with 99.9% con fi dence), MA obviously performed better in the third criterion due to the fact No. 3 mentioned below Table 4 .Onthe other hand, since p -values of the second and fourth criteria are much larger than the typical cutoff value  X   X  0.05, the performance differences between the BLS and the MA methods are not signi fi cant in these criteria. For products 4 and 5 the BLS dominated the MA in the fi rst, second, and fourth criteria with over 99.99% con fi dence, and in the third criterion it lost to the MA for the same reason mentioned in the fact No. 3 below Table 4 . 4. MLS: The BLS algorithm dominated the MLS in all criteria for all products with at least 98.8% con fi dence, except in the third criterion of the third (electrical relay) product, in which the underperformance of the BLS still cannot be proved due to the large p -value (0.597). 5. ILS: The BLS algorithm outperformed the ILS in all criteria for all products with at least 90.0% con fi dence, except in the third criterion of the third product (electrical relay). This speci product cannot be assembled feasibly with less than 4 assembly direction changes (which is rather large for a 10-part product), and while the BLS algorithm produced feasible solutions with large ADCs in 80% of its runs, ILS produced less feasible solutions (only in 40% of its runs) with smaller average ADCs with 91.2% con fi dence for the third product.

The analysis of Kruskal  X  Wallis tests is concluded with refer-ence to the values of O BLS and U BLS in the last two rows of the
Table 9 , as follows: (1) The developed BLS method proved to be better than the other algorithms in producing highest-quality solutions ( f ( s criteria) in all fi ve benchmark problems on average. This con fi rms the aforementioned fact No. 1. (2) Although the BLS method did not outperform the MA and SA algorithms in the average number of same-direction assembly operations and the average number of satis fi ed geometrical constraints criteria, respectively, it was better with respect to the weighted sum of these criteria, which is equivalent to the objective function (7) . (3) The BLS was very successful in solving the benchmark problems 1, 4, and 5 (with 19 times outperformance over all algorithms out of 20 tests for each product). On the other hand, in case of products 2 and 3, the BLS could not surpass the MA and SA methods in 3outof5mutualtestsforeachofthemethods.Thereasonmight be that the implemented MA and SA methods were originally developed, tailored, and parameter-tuned for the product 2 ( Gao et al., 2010 ) and product 3 ( Hong and Cho, 1999 ), respectively, whichresultedinef fi cient and effective performance. These two
Since the convergence of metaheuristic algorithms to a fi solution is an important issue in their design and implementation, we are interested in comparing the six algorithms under study in that respect too. A proper tool is plotting the convergence behavior of each algorithm in solving each benchmark problem as it improves its best-found solution through successive iterations. However, the notion of  X  iteration  X  is not the same in single-solution-based and population-based metaheuristics: in the former, an iteration is elapsed when a single solution is replaced by a new, preferably better, solution; while in the latter, an iteration is completed when a bunch of solutions evolve together, creating an updated population of solutions. There-fore, in order to set up a fair basis for comparing our studied S-and P-metaheuristics, we will use the number of Fitness Function Evalua-tions (FFE) as an indicator of the algorithms' progress toward their best solutions. While the runtime of an algorithm is not adequately accurate for measuring its computational effort (due to its dependency to many factors of hardware and software), the FFE provides a rather coherent measure of the algorithms' computational burden. Fig. 13 illustrates the convergence curve of each method (for its best run in 30 experiments) in the form of a plot of its best-found objective function value as the FFE grows. As the stopping criterion, we let each algorithm to evaluate its fi tnessfunctionfor5000timesafterthelast improvement of its best-found solution.

In order to analyze the above convergence plots quantitatively, we implement the average Normalized Convergence Curve Area (NCCA) index proposed in ( Masehian et al., 2013 ), which is a measure of an algorithm's convergence toward its optimal solu-tion. In fact, by calculating the area under a convergence curve we can infer how fast a method improves the objective function: for minimization (resp. maximization) problems, a relatively small (resp. large) area implies that the algorithm succeeded in reducing (resp. increasing) the objective function value at its early itera-tions. For a given algorithm, the NCCA measures the area under the convergence curve (CCA) of a complete run of the algorithm, which is normalized to lie in the interval (0, 1) as follows: NCCA  X  CCA f  X  s n  X  U FFE in which f ( s n ) is the best-known (global best) fi tness value and FFE max stands for the maximum number of fi tness function evaluations, which is counted from the beginning of the search until 5000 non-improving solutions are generated. Table 10 pre-sents the NCCA and FFE max of the six compared methods after solving all the fi ve benchmark ASP problems.

The Table shows that in all products the BLS algorithm attained the highest NCCA and f best values among all algorithms. Also, the
BLS achieved the best global solution f ( s n ) of each problem (except for the problem 3 where it ultimately reached f best  X  f ( s 6750 non-improving fi tness function evaluations). The perfor-mance gap between the BLS and other algorithms is very notable especially in the problem 5 (the Tank mockup with 63 parts, which is the largest among all products), and is an evidence of the BLS algorithm's ef fi ciency and effectiveness in solving large-size, real-world ASP problems. 5. Conclusions
Assembly Sequence Planning (ASP) is the process of computing a sequence of assembly motions for constituent parts of an assembled fi nal product. The problem is well studied and several works in the literature have been dedicated to its solution, as the problem is proved to be NP-complete. Nevertheless, the fi landscape of the ASP problem has not been analyzed previously and therefore the selection of solution methods has not been justi fi ed fully in the existing research. In this paper, the landscape of the ASP problem is analyzed through several dis-tribution and correlation measures, which showed that the pro-blem's landscape is plain rugged with local optima scattered uniformly in the search space. As a result, it was concluded that a local search method with effective exploitation and exploration capabilities is suitable for solving the ASP problem, and as such, the Breakout Local Search method developed in 2013 was selected for this purpose. The distinctive characteristic of the BLS algorithm is its ability to perturb local optimal solutions in a hope to lead the search toward unexplored areas and attain better local optima.
In this paper, we developed a greedy heuristic procedure for generating a proper initial solution that is tried to be as feasible as possible. Also, different combinations of neighborhood generation operators are proposed and their effectiveness is analyzed. The objective function used for eval uating the quality of an assembly sequence is a weighted sum of the number of same-direction assembly operations and the number of satis fi ed geometrical con-straints in a sequence. By solving fi ve real-world ASP cases, the proposed BLS method was compared to other methods like SA, GA,
MA, IPSO, and HS, which have reported results in the literature, as well as to Multi-start and Iterated Local Search methods (MLS and
ILS), which we tailored for ASP originally. Computational experi-ments showed that the BLS outperforms nearly all of the competing methods, especially in the fi tness of the best found solution and the of the initial solution, neighborhood generation operators, perturba-tion magnitude, perturbation operators, and algorithm parameters.
The performance of the BLS algorithm was then veri fi ed by conducting Friedman two-way and Kruskal-Wallis one-way
ANOVA tests to fi nd out any signi fi cant difference with other algorithms. A detailed analysis of the test results per each product and competing method recon fi rmed our fi ndings in the Experi-mental Results Section. As a result, based on the computational results, statistical tests, and convergence curve analysis, we can con fi dently maintain that the developed BLS-based ASP solver can help design and manufacturing engineers in fi nding higher quality assembly sequences more quickly and reliably, which in turn reduces the product's lead time and overall production costs.
While nearly all existing works in the fi eld of ASP deal with merely rigid parts, most real-world assembled products like ships, aircrafts, and automobiles are composed of rigid and fl exible parts, and so automatic generation of assembly sequence plans for such products requires the deformability of fl exible parts to be taken into account. Accordingly, our future research addresses assembly sequence planning of products with fl exible parts as well, which requires incorporating parameters like elasticity, force, and toler-anced geometry of such parts in the model.
 References
