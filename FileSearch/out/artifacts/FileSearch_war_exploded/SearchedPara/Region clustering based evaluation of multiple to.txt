 1. Introduction strategies to evaluate top-N queries has been one of the primary focuses of top-N query research and has
For example, a popular website may receive multiple top-N queries (say finding the best used cars based on various conditions) at about the same time. As another example, a headhunting company (website) may have many job openings and may receive many resumes for these openings, and the company wants to find the top-lectively. We show that this method is significantly more efficient than the one-query-at-a-time method.
Example 1. Consider a database of used cars. Let the schema of the used cars be: Usedcars(id#, make, model, year, price, mileage). Many prospective used car buyers may submit their queries with conditions on year , to respond to these prospective buyers as quickly as possible is a problem that needs a solution.
A top-N query may be evaluated in three steps. (1) A search range is determined based on some technique The retrieved tuples are ranked based on their distances with the query and the top N tuples are displayed.
When evaluating multiple top-N queries, one way is to evaluate them one by one independently, we call this method the na X   X  ve method (NM) in this paper. Another method is to analyze the relationships among the method (RCM) to be presented in this paper. When a cluster has multiple regions, we find the smallest con-taining region (SCR) that contains all these regions, and retrieve all the tuples from this SCR into memory; have overlaps, RCM may be more efficient for the following reasons. First, the number of random I/Os can be reduced. Retrieving tuples from each region incurs at least one random I/O and when there are many regions, many random I/Os will occur. In contrast, when a (larger) continuous space (the SCR) is searched, most I/Os are likely to be sequential I/Os. As random I/Os are much more expensive than sequential I/Os, once by RCM if the k regions are clustered together. On the other hand, RCM may retrieve more tuples that for RCM to be effective, the clustering must be carried out in such a way that maximizes the benefits while that the search region for each top-N query has been obtained using an existing method [5,11,43] .
In the literature, the phrase  X  X  X ultiple top-N queries X  X  has been used in different contexts with different distance measure. This type of queries is also known as group nearest neighbour queries [31] or multipoint
In this paper, we consider the simultaneous evaluation of multiple independent top-N queries. Note that for from those considered in previous work. More details about the differences will be provided in Section 2 .
The main contribution of this paper is the development and evaluation of a region clustering based method for evaluating multiple top-N queries collectively. Note that even though this method is presented in the multi-dimensional spaces. This method can be used in arbitrary dimensional spaces and it works well for both low-dimensional and high-dimensional data. In our experiments, datasets with 2, 3 and 4 dimensions (low tigated before.
 ent the experimental results. Finally, in Section 6 we conclude the paper. 2. Related work
The need to rank the results of database queries has long been recognized. Motro [28] gave the defini-tions of vague queries. He emphasized the need to support approximate and ranked matches in a database query language, and introduced vague predicates. Carey and Kossmann [6,7] proposed techniques to opti-mize top-N queries when the scoring is done through the SQL  X  X  X rder By X  X  clause by reducing the size of found within the convex hull of the dataset. Prefer considers both linear [18] and non-linear [19] scoring of algorithms for materialized top-N views have been proposed in [12,40] . The authors of [13] use a prob-[39] proposed a computational model, ranking cube , for efficient answering of top-N queries with multidi-mensional selections.

Fagin et al. [15] introduced the threshold algorithms (TA) that perform index scans over pre-computed uling methods based on a Knapsack-related optimization for sequential accesses and a cost model for random accesses [3] , and the distributed TA-style algorithm has been presented in [25,27] . search region.

Most existing works focus on the evaluation of top-N queries with only selection conditions (no joins). In this paper, we present a method to evaluate multiple such selection queries efficiently. This paper does not more, it delivers good performance for either low-dimensional data or high-dimensional data and guarantees the retrieval of all top-N tuples for each query.
 Yu et al. [41,42] introduced the methods for processing top-N queries in multiple database environments. queries in top-N query evaluation. The authors of [2] developed methods to optimize the communication costs in P2P networks. In this paper, our environment consists of a single database at a central location.
Top-N queries involving joins have also been considered [6,17,20 X 22,24,44] . Ilyas et al. [20] introduced a for evaluating a hierarchy of join operators. The RankSQL work [22,24] considered the order of binary rank joins at query-planning time. For the planning time optimization, RankSQL uses simple statistical models, paper and join queries will be considered in the future.

Multiple-query processing was first studied in late 1980s [32,33] , and it still is an active research area [1,14,35,36,38] . But existing works do not consider multiple top-N queries. The authors of [16] proposed a
UB-tree based method to process a set of query boxes (i.e., range queries), then used query boxes to approx-imate a non-rectangular shape (say, a triangle). The goal of this work was to minimize the number of loaded pages that overlap and the experiments were based on 2-dimensional data. As mentioned in [4] , index-based (larger than 12). Our region clustering method (RCM) does not use any index and is suitable for high-dimen-sional dataset (104 dimensions in our experiments).

The phrase  X  X  X ultiple top-N queries X  X  has appeared in several recent papers but it does not have the same meaning as the type of multiple top-N queries we consider in this paper. In [17] , it means to find the top-N results that are closest to a set of queries based on a collective distance measure. This type of queries is also known as group nearest neighbour (GNN) queries [31] or multipoint queries [8] .As an example, in [31] , the problem is to find the N ( P 1) data point(s) with the smallest sum of distances to all query points in a given query set Q ={ Q 1 , ... , Q the problem we are trying to solve in this paper, namely, find the top-N tuples for each query of a given set of queries. The example in Fig. 1 a can be used to illustrate the difference. In Fig. 1 a, { P tances given in the figure, P 3 is the desired data point for the GNN problem with N = 1, because d ({ Q 1 , Q 2 }, P 3 )= d ( Q 1 , P 3 )+ d ( Q 2 , P 3 ) = 4 is the minimum value in { d ({ Q problem, we need to find the top-N results for each query ; when N = 1, the top-1 tuples for Q in [31] . The collective distance measure used in [17] is the minimum of distances , i.e., d ( P , Q ) = min of multiple query points is Q ={ Q 1 , Q 2 , Q 3 } and the three tuples are P for Q 1 , Q 2 and Q 3 are P 1 , P 2 and P 3 , respectively.

The authors of [29] studied continuous monitoring of top-N queries over a fixed-size window of the most recent data. This paper also discussed the processing of  X  X  X ultiple top-N queries X  X . However, there are two to find the top-N results for each query. 3. Problem definition and analysis Let R n be an n -dimensional metric space with distance function d ( ), where R is the real line. Suppose
R R n is a relation (or dataset) with n attributes ( A to find a sorted set of N tuples in R that are closest to Q according to the given distance function. The retrieve all tuples in R .

As mentioned in Section 1 , a number of approaches exist for mapping a top-N query on a relational data-determine a search region, denoted R ( Q , N ), for the top-N query.
 In this paper, a region R  X  v  X  R  X  X 
N ries, Q ={( Q 1 , N 1 ), ... ,( Q m , N m )}.

We now use an example to explain the differences in evaluation costs between NM and RCM. Fig. 2 shows the search regions R ( Q 1 , N 1 ) and R ( Q 2 , N 2 ) of two top-N queries ( Q smallest containing region (SCR) of these two regions is the rectangle with the dotted line. sorting and displaying the results. Using NM, the total cost for evaluating ( Q
When RCM is used, we still need to construct the search regions for the two queries as in NM. Then we construct the SCR and retrieve all tuples from SCR into the memory. Next, we identify the retrieved tuples that belong to R ( Q 1 , N 1 ) and R ( Q 2 , N 2 ), respectively. Finally, sort the tuples in R ( Q and output the top-N tuples for Q 1 and Q 2 , respectively. Thus, using RCM, the total cost will be: one random I/O plus some sequential I/Os. Since the cost of a random I/O is about 10 times higher than that cient. As I/O cost is usually the main cost in most database applications, reducing the I/O cost, especially the random I/O cost, is an effective way to improve the efficiency of query evaluation.
The RCM we propose in this paper aims to identify situations where the total cost for RCM is smaller than example in Fig. 3 . Since R ( Q 3 , N 3 ) R ( Q 2 , N 2 ), Q d ( Q 1 , Q 2 ) is less than d ( Q 3 , Q 2 ). In this case, if we have retrieved all the tuples in R ( Q can obtain the top-N tuples for Q 2 from those in R ( Q 3 the same cluster, the above savings on I/Os cannot be obtained.

Clearly, the distribution of the search regions of the input queries plays an important role in how to eval-uate these queries collectively. We now provide some measures to characterize the distribution of a set of regions. Fig. 4 shows several examples of the distributions of some 2-dimensional regions. Let R be k regions. By comparing the three volumes insight about the distribution of these regions. Denote
It is easy to see that a 6 1 and b 6 1. The values of a indicate the degree of overlap of the regions and a smaller a indicates more overlap. The values of b reflect the closeness between the regions and the size of the lacuna in the SCR; a larger b indicates closer regions and smaller lacuna in general. The value of s may be greater than, less than or equal to 1, and it indicates how much of the SCR is occupied by the R a smaller s denotes larger volumes of R i  X  X  in the SCR. If s &lt; 1, then there must be some R one another.

In some cases, using the SCR of [ k i  X  1 R i as a single search region for all R and such SCRs would contain too many useless tuples. Fig. 4 a, c and d represent more random cases and they in Section 4.3 . 4. Multiple top-N query processing rent clustering techniques employ some similarity measure to compute the distance among the objects to be clustered. In most cases, the objects are points. However, the objects are regions in this paper. 4.1. Algorithms and terminologies
In this section, we introduce the basic algorithms and terminologies that will be used by RCM. The algo-set of candidate top-N tuples. 4.1.1. The volume of the union of m regions (VUR)
Given m regions R 1 , R 2 , ... , R m , v ( R i ) denotes the volume of R
P we have Suppose R 1  X  also an n -dimensional region. Let p i = max{ a i , c i }, and q that p i
R \ R 2 6  X  / ; R 1 \ R 2  X  Note that v  X  R 1 \ R 2  X  X 
Based on the associativity of  X  X  \  X  X , Formulas (3) and (4) , the exact volume of [ m items in Formula (3) are empty. As a result, the number of non-empty items in Formula (3) is usually much smaller than 2 m . For example, if R i \ R j = / or v ( R v ( R i \ R j \ R k ) = 0, for all k , j &lt; k 6 m , and all such items can be discarded from Formula (3) . 4.1.2. The difference of two regions (DTR) Let S  X  difference T S is the union of some n -dimensional sub-regions of T , that is, T S  X [
T j T , j =1, ... , p , v ( T i \ T j )=0, i 5 j , can be constructed by the following algorithm: Algorithm DTR ( T , S )/ * T S = T [1] [ ... [ T [ p ] * / Input: T  X 
Output: T [ p ]/ * the array of hyper-rectangles * /
Local variables: int i ; double t ; Hyper-rectangle H ; p :  X  1; H :  X  T ;/ * then H  X  for ( i =1; i 6 n ; i ++) end for p ; It is clear that Algorithm DTR also holds if T is contained in S and T S = / . Fig. 5 shows three cases of T S in 2-dimensional space.
 We can use Algorithm DTR to reduce the query evaluation time in many cases. For example, if the tuples in
S in Fig. 5 a have been retrieved, it is sufficient to retrieve the tuples in T tuples from S that are in T to obtain the tuples in T .
 cases, j { T j } j can be much smaller than 2 n . For example, in Fig. 5 b and c, j { T itively, DTR is effective only when j { T j } j is small because a large j { T
Os. In our experiments, DTR is used only when j { T j } j is not larger than 2. 4.1.3. The partition of points in n-dimensional space (PPS)
Let T ={ t (1) , ... , t ( m ) } be a set of m points in R
Given a threshold m 0 , there is a number p such that R is divided into p cells { P contains at most m 0 points in T . The following algorithm can achieve this:
Algorithm PPS ( T , R , m 0 , n 1 , ... , n k )/ * n i &gt;1, i =1, ... , k (1) Find the k longest edges of R , denote them as e 1 , e (2) For each cell P i containing more than m 0 points in T , let R :  X  P
It is clear that p = h +( N ite 1) * ( h 1) = h * N ite N executed. It is easy to know the relationship between the depth d of the partitions in PPS and the volume of the cells. Let v ( d ) denote the volume of a cell after d consecutive partitions from R . Then v convergence of the algorithm PPS is very fast, in fact, it is not slower than O(1/2 with k = 1 and n 1 =2.
 Let m 0 =8. Fig. 6 a X  X  illustrate three cases of the algorithm PPS in 2-dimensional space. Note that, in
Fig. 6 c, R is divided into three segments along the x -axis and four segments along the y -axis; moreover, the two smaller cells have four segments along the x -axis and three segments along the y -axis.
In our experiments, threshold m 0 = 8 is used for all datasets of both low and high dimensions. The reason we choose m 0 = 8 is as follows. With m 0 = 8, each cell will have no more than 8 points. Each point corre-
VUR algorithm has a complexity of O(2 m ), where m is the number of regions. When no more than eight regions are considered at a time, the VUR algorithm can run very fast. For the 2 dimensional case, we take k =2, n 1 =4, and n 2 = 3 as in Fig. 6 c. For the n dimensional case ( n P 3), we take k =3, n and n 3 = 2. Thus, each time, the SCR (or a cell) is partitioned into 12 smaller cells. For example, for the 104 -dimensional dataset Lsi104D, we will find the 3 longest edges e in each partition. In the first partition, suppose e 1 , e respectively, and by dividing them into three, two, and two equal-length segments respectively, the SCR is divided into h =3 * 2 * 2 = 12 cells { P i : i =1, ... , 12}. In the next partition, for a cell, say P may belong to other dimensions. 4.1.4. Get candidate set of top-N tuples (CST) Given two regions S  X  can get the candidate set of T using the following algorithm:
Algorithm CST ( T , S ) (1) if T S , then the candidate set of T is already obtained; simply identify the tuples from S that are in (2) if T X S and T \ S 5 / , then obtain T S  X [ p j  X  1 T (3) if T \ S = / or p is not less than a threshold, get tuples in T from the database directly;
Now we introduce the concept of best-super-region ( BSR ). Suppose that { R
R is called the BSR of R i if R i R k and v ( R k ) = min{ v ( R est region that contains R i . The index k of R k is called the best-super-region number (BSR number) of R denoted as R i . bsn .In Fig. 7 (where R 3 = R 7 ), R 5 . bsn =3; R sequential I/Os), and then we can obtain the candidate sets from the BSRs for others. Note that R
R . bsn = 7, but R 7 . bsn = 10. If R 3 . bsn = 7 and R 7 . bsn = 3, there would be an infinite loop.
In addition, if we have R 1 R 2 ... R k , i.e., each R i is the best-super-region of R candidate sets for all regions from R k . 4.1.5. Get top-N tuples from candidate set (TTC)
Algorithm TTC first computes the distance between a top-N query ( Q , N ) and every tuple in the candidate forward, its detail will not be presented here.
 discussed in Section 4.4.2 . 4.2. Models of clustering of multiple top-N queries
In this section, we present three models of region clustering and discuss the clustering conditions for each model. Let T ={ T i : i =1, ... , m } be a set of m regions and C  X ={ H satisfies some conditions, it is called a cluster. Suppose SCR is the smallest containing region of C  X . 4.2.1. I-clustering
C  X  is called an I-cluster (for I ntersection based cluster) if it satisfies the following conditions: (1) for each 0 &lt; j 6 L , there is 0 6 k &lt; j such that H (2) v  X [ L k  X  0 H k  X  = v  X  SCR  X f H k g X  X  P c 1 , where c condition is to limit the lacuna of the SCR, making the regions in a cluster reasonably close to each other. Fig. 8 a shows an example of I-clustering.

The conditions of I-clustering are called I-cluster conditions ,and c 0 &lt; v  X [ L k  X  0 H k  X  = v  X  SCR  X f H k g X  X  6 1, c 1 2 [0,1]. It can be seen that in general a larger c which leads to more compact clusters (i.e., the lacuna of the SCR is smaller). On the other hand, a larger c determine an approximate optimal value of c 1 through training (see Section 5.2 ). Note that volume v  X [ L can be obtained using algorithm VUR . 4.2.2. C-clustering
C  X  is called a C-cluster (for C enter based cluster) if it satisfies the following conditions: (1) for each 0 &lt; j 6 L , there is 0 6 k &lt; j such that the center of H (2) v  X [ L k  X  0 H k  X  = v  X  SCR  X f H k g X  X  P c 2 , where c an example of C-clustering. The two conditions above are called C-cluster conditions ,and c threshold . The impact of c 2 on C-clustering is the same as that of c determine an approximate optimal value for c 2 . 4.2.3. S-clustering tion: v  X  SCR  X f H k g X  X  =
S-clustering and the three regions form an S-cluster. The value of c region s. There is no clear upper bound for c 3 . When c 3
Therefore, it is reasonable to consider c 3 2 [1,3]. Training will again be used to determine an approximate optimal value for c 3 . 4.2.4. Comparison of the three clustering models
Based on the above discussions about the three clustering models defined above, the relationships among the three models can be summarized as follows: est), followed by I-clustering and then by S-clustering. 2. S-clustering tends to be most applicable, followed by I-clustering and then by C-clustering. 4.3. Cluster search regions
Let Q ={( Q 1 , N 1 ), ... ,( Q m , N m )} be a set of m top-N queries and R regions. In this section we discuss how to cluster the search regions in R ={ R basic algorithms and clustering models described Section 4.1 and 4.2 . For R denote R k . a i = a i and R k . b i = b i . Also, we sometimes do not distinguish the top-N query ( Q search region R k in the following discussion. Suppose min( A values of attribute A i of all tuples in a relation. Without loss of generality, for the set R ,if R
R . a i :  X  min( A i ), and if R k . b i &gt; max( A i ), let R
Based on the relationships among the three clustering models (see Section 4.2.4 ), it seems natural to used to cluster the remaining regions as much as possible. However, our experiments indicate that for some as effective as I-clustering.

Based on the above observation, we adopt the following general clustering strategy in this paper. 1. Apply either I-clustering or C-clustering to cluster the input regions based on the sizes of their search are small. 2. Apply S-clustering to cluster the remaining regions.
 For convenience, the I-clustering and C-clustering models will be called primary clustering models while the the distribution of data in a dataset, the query point and the value of N in a top-N query.
More detailed description of our clustering algorithm is given below. Again R ={ R of search regions under consideration. 1. Label all the regions in R that have a super-region and remove them temporarily. For each k ,if R super-region, determine its best-super-region (BSR) (see Section 4.1.4 ) and save the BSR number, how to cluster those that have no super regions.

Let T ={ T i } denote the subset of the regions in R ={ R m T = j T j . Obviously, m T 6 j R j . 2. Obtain the smallest containing region (SCR) of all { T 3. Partition the centers of the search regions in T into K subsets, 1 6 K 6 m tion 4.1.3 ). This effectively partitions the SCR into K sub-regions, S most m 0 centers in each sub-region. The search regions in T whose centers are in the same sub-region are likely to be grouped into the same cluster. tering model (C-clustering or I-clustering) and then apply the secondary clustering model (S-clustering).We need a threshold L 0 as the maximum number of regions in a cluster C  X  . Because the VUR algorithm has a complexity of O(2 m ), where m is the number of regions, and we use VUR to calculate the volume of the unions of the search regions in C  X  (see Section 4.2 ), L iments, which allows VUR to run fast.First, for each sub-region S
T k  X  X  in T whose centers are in S i . Let T i ={ T k : the center of T
Second, arbitrarily select a region from T i , denote as H
Repeat this process to find H h from T  X  X  H 0 , H 1 , ... , H conditions of the clustering model or h = L 0 is reached. Thus we get a cluster C  X ={ H j =0,1, ... , h 1, h 6 L 0 }. If T i is not empty, select another un-clustered region from T and get another cluster by repeating the above process.Let {C  X  structed above and build the smallest containing region SCR not be clustered with the remaining regions. cluster. If there is such a region T j SCR h , add T j into C  X 
L 0 , it is possible that T j could not be added to a C  X  final cluster is more than L 0 . The remaining un-clustered regions will be processed individually. 6. For each smallest containing region SCR i = SCR(C  X  i BSR number SCR i . bsn = k (see Section 4.1.4 ).
 search regions are small. By applying I-clustering as the primary clustering model first, three clusters
C  X  1 ={ R 1 , R 2 }, C  X  2 ={ R 3 , R 4 , R 5 }andC  X  3 ={ R applying S-clustering to the remaining regions, C  X  4 ={ R of C  X  1 ,C  X  2 ,C  X  3 and C  X  4 are SCR 1 , SCR 2 , SCR cluster with its super-region R 1 . By step 5, R 10 is added to C  X  regions in C  X  2 . Due to the constraint of L 0 ( L 0 = 10), R because it is contained in SCR 3 . Four regions R 21 , R 22 corresponding queries will be evaluated independently. Although S example, the total number of I/O requests to the database is 8, one for each of SCR
R 4.4. Top-N tuple retrieval 4.4.1. Search top-N tuples and the other is for individual queries.
 Case 1: clustered queries. There are three steps.

Step 1. From {SCR i }, find the first SCR that has no super region, denote it as SCR
SELECT FROM R WHERE  X  a 1 6 A 1 6 b 1  X  AND ... AND  X  a n
For each search region R in C  X  i (a) If R has no super region, search the candidate set of R from { t (b) Else (i.e. R has a super region), find out its best-super-region BSR, search the candidate set from the
Step 2. For each SCR i , i &gt; i 0 , that has no super region, (a) If there is no SCR j ( i 0 6 j &lt; i ) that intersects with SCR (b) Else, among all SCR j  X  X  ( i 0 6 j &lt; i ) that intersect with SCR
Step 3 . For each SCR i , i P 1, that has a super region, find its best-super-region SCR Case 2: individual queries
Step 1. For each R i , if it has no super region, find all the clustered R
Step 2. If R i has a super region, it is sufficient to get all the candidates in R didates once. 4.4.2. Guarantee exact top-N tuples of Section 4.4.1 cannot guarantee to get the top-N tuples. Two cases are considered as follows. 1. R has a best-super-region R k or SCR, and the size of R j R j = N 0 , take N N 0 tuples from R k R or SCR R , and calculate the distances between those tuples and the query Q . Now we use the maximum distance to define the new search region for Q and this search region guarantees at least N tuples will be retrieved. Get the top-N tuples -using algorithm TTC . region that contains at least N tuples. 5. Experimental results
In this section, we report our experimental results and compare our region clustering method (RCM)
Microsoft X  X  SQL Server 2000 and VC++6.0 on a PC with Windows XP and a Pentium 4 processor with 2.8 GHz CPU and 768MB memory. 5.1. Data sets and preparations
The datasets we used include data of both low dimensionality (2, 3, and 4 dimensions) and high dimension-are used. The real datasets include Census2D and Census3D (both with 210,138 tuples), and Cover4D (581,010 tuples). The synthetic datasets are Gauss3D (500,000 tuples) and Array3D (507,701 tuples). In the real datasets derived from LSI are used in our experiments. They have 20,000 tuples and the same 25, 50 and 104 attributes as used in [11] are used to create datasets of 25, 50 and 104 dimensions, denoted by Lsi25D, Lsi50D and Lsi104D, respectively.

Each dataset uses three training-workloads tw 1 , tw 2 and tw training-workload includes 100 queries and each test workload includes 2000 queries that are the tuples ran-tering models and their threshold values, and three test workloads will be used to report our experimental results, which are the average of the results of the three test workloads.

Two top-N query evaluation techniques are used to construct the search region of a top-N query: construct the search region that just contains the top-N tuples for each query.
D and these data points are used as sample query points. For each such query point, a top-N query ( Q i , N i ) is generated and a profile is created for it. The profile contains the search distance r search region. Once the profiles are created and saved, the learning-based technique uses them to estimate method). The experimental results reported in [43] indicate that using less than 1000 sample queries to create the profiles is sufficient. For the experiments conducted in this paper, 178, 218, 250, 833, 909 and 954 profiles are created for datasets of 2, 3, 4, 25, 50 and 104 dimensions, respectively. These numbers histogram used in [5] and the size of sampling set used in [11] when these methods are compared using the same dataset [43] .

For any given top-N query, the search region generated by the Opt method is usually smaller than that gen-
In our experiments, we report results based on a default setting. This default setting uses a 2000-query top 100 tuples for each query) and the distance function is the maximum distance (i.e., L tion is Euclidean distance (i.e., L 2 -norm distance) [43] .

The following measures are used in our experiments: time to construct the search regions is relatively small compared to the time needed to retrieve the tuples from the regions. For example, it takes about 3 ms to construct one search region and about 150 ms to retrieve the data from a region for Lsi25D.  X  The number of I/Os : It is the total number of I/O operations for accessing the database. using our algorithms, i.e., it corresponds to the d in Formula (2) in Section 3 . the respective dataset for all queries. A smaller number of tuples retrieved indicates a better efficiency.
The values of the above four measures depend on the following set of factors { m , D, T, A, W}, where 40, 100, 400, 1000 and 2000.
 D : The dataset used. Its valid  X  X  X alues X  X  include Census2D, Census3D, Array3D, Gauss3D, Cover4D, Lsi25D, Lsi50D, and Lsi104D.

T : The technique used to obtain the search region of a top-N query. Its valid  X  X  X alues X  X  are LB (Learning-based technique) and Opt (Optimum technique).

A: The algorithm used to retrieve the top-N tuples for the m top-N queries. It X  X  either RCM or NM. The [16] because it is simple, easy to understand, and generally usable.
 measures, and then their average is taken to obtain the reported results.

Because the elapsed time and the extra time do not include the time needed to construct the search regions formulas: time of sorting the results. In (2 0 ) , f SCR k : k  X  1 ; ... ; M g includes all single-region clusters. memory. When presenting cost estimates we generally assume the worst-case scenario [34] and let the size of 5MB for Lsi50D, 8MB for Lsi104D, and 4MB for the other datasets. Note that 4MB is the smallest value for the  X  max server memory  X  in Microsoft X  X  SQL Server 2000. 5.2. Determining clustering model and thresholds by training
Each of the three clustering models we introduced in Section 4.2 has a threshold whose value can affect the quality of the produced clusters. From the discussion in Section 4.2 , we have 0 6 c In this section, we discuss how to obtain the appropriate values for these thresholds for each dataset. the LB technique and the other based on the Opt technique. Next, for each set of search regions, we run two clustering will not be used in both cases), with different values of c mances of these experiments, we find the best values for c add the S-clustering to the RCM algorithm for each best case and try different values of c determine the best value for c 3 .

Example 3. Consider the dataset Gauss3D in Section 5.1 . For each training-workload tw for the three training workloads and then the ratios of the number of I/Os of RCM to NM:
Similarly, based on the number of tuples retrieved we obtained for each run using the I-clustering, we com-number of tuples retrieved by RCM to that by NM:
We can repeat the above computations for C-clustering. Let  X  X  X verage-num-IO2 X  X ,  X  X  X atio-IO2 X  X ,  X  X  X verage-num-tuple2 X  X  and  X  X  X atio-Tuple2 X  X  denote the corresponding averages and ratios for C-clustering.
Fig. 10 a shows the results for different values of c 1 and c technique. Obviously, 0.5 is a pivotal point with I-clustering; when c clustering models.

Fig. 10 b shows the results where the search regions are constructed using the Opt technique. We can see choose I-clustering with threshold c 1 = 0.5 as the primary clustering model for dataset Gauss3D.
Table 1 lists the primary clustering model and the approximate optimal thresholds that are obtained for each dataset using the training workloads tw 1 , tw 2 , and tw sequent sections are based on the thresholds in Table 1 . 5.3. Performance comparison
Using the thresholds in Table 1 , for each of the 8 datasets D, each of the three test workloads, and one of the two techniques LB and Opt for generating search regions, we first obtain 2000 search regions correspond-the following 8 values: 1, 4, 10, 40, 100, 400, 1000 and 2000. Thus, there are 8 experimental results. 5.3.1. Comparison of the elapsed time
We use the ratio of RCM X  X  performance to NM X  X  performance to compare RCM and NM. Denote
Fig. 11 a shows that, using the regions constructed by the LB-technique, ratio-elapsed-time &lt;1.0 for all datasets when m P 10, except for Census3D which has the ratio = 1.0 when m = 10, and ratio-elapsed-time 6 0.82 for all datasets when m = 100, i.e., RCM is more efficient than NM when m P 10. Even when m &lt; 10, we have Ratio-time 1.0. This means that RCM is generally usable for all m .
When m = 100, Fig. 11 a shows that there are four datasets (Cover4D, Lsi25D, Lsi50D, and Lsi104D) with time of RCM is less than 62% of that of NM for these datasets. Therefore, our RCM can save the time by at least 38% for these four datasets, and save the time by 67% and 72% for datasets Lsi25D and Lsi50D, respec-tively. The ratios for the other four datasets (Census2D, Census3D, Array3D and Gauss3D) are between 0.62 and 0.82.

We can see that more savings on elapsed time can be achieved with the increase of the number of queries from 400 to 2000. When m = 2000, in the worst case, RCM can save the time by 50% for Gauss3D; but in the best case, RCM can save the time by nearly 90% for Census2D, Census3D and Lsi25D.
We should point out that when the number of queries m becomes sufficiently large, the I/O cost for eval-uating most additional queries would become zero. This occurs when the SCRs of the formed clusters cover contained in one of the existing SCRs, leading to no additional I/O operation. In other words, when m becomes sufficiently large, the total I/O cost approaches a constant when RCM is used. For example, for
Lsi25D, the number of I/O operations by RCM is 70 for both m = 1000 and m = 2000. Moreover, if we assume that every query incurs the same I/O cost, IO-time, and the same sorting cost, S -time, based on the na X   X  ve method, then the ratio-elapsed-time of RCM to NM is bounded by S -time/(IO-time + S -time), which represents the extreme case when RCM incurs no I/O cost.

Fig. 11 b shows the results when the search regions are constructed using the Opt technique. When m = 100, practice. The results in Fig. 11 b show that RCM can outperform NM even when the smallest search region is used for each query; for m = 2000, all ratios are between 0.3 and 0. 7.

In Fig. 11 a and b, the ratios should be close to 1.0 if no cluster contains two or more regions. However, there are some ratios that are greater than 1.0, say, when m = 1 for Lsi50D in Fig. 11 a and when m =4 a small random perturbation can cause the ratio to have a large change. The effect of random perturbation diminishes when m becomes larger. This will also be discussed in Section 5.3.3 . 5.3.2. Comparison of the number of I/Os
The total number of I/O requests for accessing the database plays an important role in the total response base queries. We denote
Because average-num-IO( m , D , T , NM) is always m , we have datasets when m P 10, except for Census3D which has ratio 1.0 when m = 10. For m = 100, the ratio is less than 0.6 for four datasets (Cover4D, Lsi25D, Lsi50D and Lsi104D) and between 0.6 and 0.8 for the other four cost of RCM will approach a constant. Consequently, the ratio-num-IO will approach zero when m becomes larger and larger because the total I/O cost of NM will keep on increasing as m increases.
Fig. 12 b shows the case when the search regions are constructed using the Opt technique. When m P 40, which means there is less chance to form clusters. As a result, the ratio-num-IO has a slower speed of approaching zero compared with the case when the LB technique is used to generate search regions. and proportional impact on the elapsed time for sufficiently large m . 5.3.3. Comparison between the extra elapsed time and total elapsed time using RCM
In this subsection, we discuss the extra elapsed time for RCM. From Formula (2 first formula in Section 5.3.1 , we have:
We denote ratios exceed 0.04 and 0.05, respectively.

We can see that the curve of ratio-extra-time rises with the increase of the number of queries when m P 100
RCM increases more slowly as the total I/O cost approaches a constant. Nevertheless, the extra time remains very small compared with the total elapsed time of RCM. When m is small ( m 6 10), the total elapsed time is very small; therefore, a small random perturbation can lead to the curve of ratio-extra-time to have a large change. 5.3.4. Comparison of the number of tuples retrieved
With the requirement that the top-N tuples for each query must be retrieved, an evaluation method is usu-retrieved. We denote
In Fig. 14 a, for regions generated by the LB technique, the curves of ratio-num-tuple are either near 1 or below 1 except for datasets Cover4D and Gauss3D. Actually for most datasets, when m P 40, the ratios are less than 1. In other words, other than for datasets Cover4D and Gauss3D, the number of tuples retrieved by method. The reason for the exception of Cover4D and Gauss3D is due to the nature of the two datasets (their (see Section 4.2), causing more tuples to be not in the search regions of the queries.
In Fig. 14 b, for regions generated by the Opt technique, when m &gt; 10, the curves of ratio-num-tuple are either near 1 or below 1 except for datasets Cover4D, Gauss3D, Lsi50D and Lsi104D. So the number of all tuples retrieved by the RCM method is almost equal to that by the na X   X  ve method even when optimal regions are used except for datasets Cover4D, Gauss3D, Lsi50D and Lsi104D. The reasons for the exceptions are that smaller clustering thresholds tend to generate larger lacunas in the SCRs , and c
Lsi50D and Lsi104D, respectively (see Section 4.2 and Table 1 in Section 5.2 ). Note that the number of all shows that RCM performs very well. 6. Conclusions
In this paper, we proposed a new method for evaluating multiple top-N queries concurrently over a rela-accessing the same region multiple times and reduce the number of random I/O accesses to the underlying posed method based on eight different datasets with dimensionality ranging from 2 to 104, the number of con-(Learning-based and optimum). Our experimental results showed that our region clustering method yielded sional data. When 2000 concurrent queries are considered and when more realistic initial search regions (the ones generated by the learning-based method) are used, the newly proposed method reduced the elapsed-time of the na X   X  ve method by 50 X 90% with most saving coming from the saving on I/O cost.
We believe our method is directly applicable to evaluating multiple range queries in multi-dimensional clustering method may have more relative advantage in evaluating multiple range queries than evaluating mul-and evaluate its effectiveness.

The experiments conducted in this paper assumed that a single server evaluates all the queries. As such, the sider the case when queries come in as a stream and to devise a strategy for clustering a stream of queries. Acknowledgement
This work is supported in part by: NSFC (60496322, 60496327) and NSFHEE (2004305). The authors would also like to express their gratitude to the anonymous reviewers for providing some very helpful suggestions.

References
