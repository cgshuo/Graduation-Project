 James C. Ross 1 , 2 jross@bwh.harvard.edu Jennifer G. Dy 2 jdy@ece.neu.edu 1: Brigham and Women X  X  Hospital, Boston, MA 02115 USA Personalized medicine holds the promise of providing individuals tailored medical care optimally suited to their needs. In recent years, there has been an ex-plosion of clinical, biological, and genetic data, the analysis of which will hopefully bring us closer to real-izing this goal. Understanding distinct mechanisms of disease  X  unique biological pathways and their genetic determinants  X  is at the core of this endeavor and is often referred to as  X  disease sub-typing  X .
 In this paper, we are specifically motivated by the task of identifying novel and clinically relevant cat-egories of Chronic Obstructive Pulmonary Disease (COPD), a smoking related lung disease with a signifi-cant health burden worldwide. Alpha 1 -antitrypsin de-ficiency is one known form of genetic disorder leading to COPD (Silverman &amp; Sandhaus, 2009); experts hy-pothesize that there are other distinct, as yet unkown, categories of this disease determined by genetic predis-position (Cho, 2010; 2012; Barker &amp; Brightling, 2013). The challenge is to identify these subgroups given large amounts of data obtained from clinical studies. The key difficulty is grouping individuals with similar ge-netic make-up in spite of significantly different levels of disease severity. For example, a younger person with little exposure to smoke and relatively healthy lungs should be placed in the same category with an older, life-long smoker with advanced lung disease pro-vided they have the same genetic or biological pre-disposition.
 The manner in which lung health changes as a func-tion of age and smoke exposure can be used to identify meaningful subgroups. Some people are genetically re-sistant to the effects of smoke exposure and have pre-served lung health even after years of smoking. On the other hand, others are highly sensitive to smoke and experience rapid health decline given similar levels of exposure. This leads to the notion of  X  disease trajecto-ries  X , and indeed there is an analogy to the trajectories of projectiles moving through space. We seek meaning-ful disease trajectories with the hypothesis that those individuals associated with the same trajectory have similar genetic predispositions to lung health decline. The problem is that we do not know how many such trajectories (disease subgroups) exist, nor do we know the functional forms of those trajectories.
 The traditional way to discover unknown subgroups given data is by clustering (Jain et al., 1999). Clus-tering algorithms group data based on some notion of similarity. Standard clustering algorithms typically define similarity in the form of some metric or a prob-ability model. Most standard methods do not take the structure of the problem into account and treat all the features/variables in the same way; however, in our COPD sub-typing problem, we have variables such as age and smoking that are causative agents of variables that indicate lung function and disease severity. The type of grouping we are interested in discovering re-lates to how different groups of individuals respond to exposure. This led us to the design of a mixture of Gaussian process (GP) regression model.
 There are algorithms for clustering time series data (Li &amp; Prakash, 2011). These methods assume that each sample has a time-sampled measurement. In our case, it is not always possible to work with longitudinal data (data in which a given individual is studied at multi-ple time points); many studies are cross-sectional . Our approach is flexible in the sense that input variables can represent any entity that directly affects measur-able lung health or disease severity, including age and smoke exposure. Our model is also able to learn com-ponent  X  X rajectory X  functions even when we only have one sample per patient. This is possible because com-plete clinical datasets typically have multiple repre-sentatives of the same trajectory captured at different stages of the disease process.
 We use a mixture of GPs rather than the standard mixture of regression models (Gr  X un &amp; Leisch, 2007), because we do not know what the regression model is. A Gaussian process (Rasmussen &amp; Williams, 2006) provides a nonparametric distribution over functions. There has been work combining GPs with mixture models (Rasmussen &amp; Ghahramani, 2002; Meeds &amp; Osindero, 2006; Yuan &amp; Neubauer, 2009). These works address modeling data where there are local disconti-nuities. In a local region of the input space, there is a gating function that determines which GP component it is generated from. Our work addresses GP compo-nents at a global scale.
 In 2012 L  X azaro-Gredilla et al. (2012) introduced a mix-ture of Gaussian Processes to address the data associ-ation problem, which arises in multi-target tracking scenarios. As alluded to in the earlier paragraphs, this scenario is similar to the one we are interested in, with one important difference: whereas they as-sumed the number of trajectories is known, we do not. To address this issue, we recast their formula-tion in a Bayesian nonparametric framework using the stick-breaking Dirichlet Process Model (Blei &amp; Jordan, 2006).
 The added flexibility provided by the nonparamet-ric model makes finding local minima more likely. We steer inference towards meaningful solutions by incorporating must-link and cannot-link constraints (Wagstaff &amp; Cardie, 2000; Zhu, 2008) between data in-stances. This is an important feature of our model as it provides a mechanism to include expert input (doctors, biologists, geneticists, etc.). Basu et al. (2006) demon-strated the use of Hidden Markov Random Fields (HMRF) to apply such constraints for semi-supervised clustering. Orbanz &amp; Buhmann (2008) used MRFs to impose constraints in a nonparametric setting for spa-tial smoothing in image segmentation; they performed inference using Gibbs sampling. Inspired by these ap-proaches, we also use MRFs to encode must-link and cannot-link constraints, and we further demonstrate a variational approach for performing approximate in-ference.
 In this paper, we introduce a novel variational Dirich-let process mixture of Gaussian processes that can also learn from must-link and cannot-link constraints. The contributions of this work are: 1) our model is able to learn the number of clusters (trajectories) automat-ically for a mixture of GPs; 2) we provide a model allowing a mixture of GPs to learn from constraints; 3) we derive a variational inference approach to clus-tering with contraints encoded using MRFs; and 4) we present a transformative way of looking at sub-typing COPD; instead of applying traditional clustering al-gorithms, we utilize our domain knowledge regarding the disease mechanism and cast it as a problem of dis-covering multiple  X  disease trajectories  X .
 The rest of the paper is organized as follows. In Sec-tion 2 we give a brief overview of the theory behind our model. In Section 3 we describe our probabilis-tic model; we define both the structure and the con-stituent probability distributions. The update equa-tions used for variational inference are given in 4, and we describe the conditions under which efficient com-putation is possible. We demonstrate algorithm per-formance on both synthetic and real-world datasets in Section 5, and we conclude in Section 6. In this section we briefly review theory on which our model builds: Gaussian processes, Markov random fields, and Dirichlet process mixtures. 2.1. Gaussian Processes Gaussian Processes (GPs) have been used extensively for Bayesian nonlinear regression. We cover the key concepts here as they pertain to our framework and refer the reader to Rasmussen &amp; Williams (2006) for details.
 Gaussian Processes can be interpreted as a nonpara-metric prior over functions. They have the property that given a finite sampling of the domain, the corre-sponding vector of function values, f , are distributed according to a multivariate Gaussian with mean 0 (ar-bitrary but used in standard practice) and covariance matrix K : The elements of K are determined by the kernel func-function and selection of its parameter values controls the behavior of the GP. One popular kernel function (and the one used throughout our experiments) is the exponential of a quadratic form given by In order to perform GP regression, we assume an ob-served dataset of inputs and corresponding (noisy) tar-gets, D  X { x n , y n } N n =1 , where we model the targets as p ( y | f ) = N y | f , X  2 I N . Here,  X  2 is the variance on the target variables. It can then be shown that the pre-dicted mean and variance of target value y  X  at some new input x  X  are given by where k  X  X  X  = k ( x  X  , x  X  ) and [ k  X  ] n = k ( x n , x  X  2.2. Markov Random Fields A Markov random field (MRF) is represented by an undirected graphical model in which the nodes repre-sent variables or groups of variables and the edges indi-cate dependence relationships. An important property of MRFs is that a collection of variables is condition-ally independent of all others in the field given the variables in their Markov blanket. The Hammersley-Clifford theorem states that the distribution, p ( Z ), over the variables in a MRF factorizes according to where Z is a normalization constant called the parti-tion function , C is the set of all cliques in the MRF, z c are the variables in clique c , and H c is the energy function over clique c (Geman &amp; Geman, 1984; Be-sag, 1974). The energy function captures the desired configuration of local variables.
 2.3. Dirichlet Process Mixtures Ferguson (1973) first introduced the Dirichlet process (DP) as a measure on measures. It is parameterized by a base measure, G 0 , and a positive scaling parameter  X  : The notion of a Dirichlet process mixture (DPM) arises if we treat the k th draw from G as a parameter of the distribution over some observation (Antoniak, 1974). DPMs can be interpreted as mixture models with an infinite number of mixture components. More recently, Blei &amp; Jordan (2006) described a variational inference algorithm for DPMs using the stick-breaking construction introduced by Sethuraman (1991). The stick-breaking construction represents G as where  X   X   X  distributed according to a beta distribution: v i  X  Beta (1 , X  ), and  X   X  i  X  G 0 . The use of the stick-breaking construction in our formulation will be dis-cussed in Section 3. In this section we describe our formulation, including a definition of the variables in our model.
 Let X = [ x 1  X  X  X  x Q ] be the N  X  Q matrix of observed inputs where N is the number of instances and Q is the dimension of the inputs. Let Y = [ y 1  X  X  X  y D ] be the N  X  D matrix of corresponding target values, where D represents the dimension of the target variables. We introduce the N  X  X  binary indicator matrix, Z , to represent the association between the data instances and the latent regression functions. Following the no-tation in L  X azaro-Gredilla et al. (2012), we designate the set of latent functions as n f (k ) d ( x ) o collect all latent functions of trajectory k in the matrix F set of latent functions as F (k) .
 The probabilistic graphical model describing our for-mulation can be seen in Figure 1. The set C is a collec-tion of data instance pairs representing given must-link and cannot-link constraints. With these quantities de-fined, we give the joint distribution of our model: p Y , Z , v , { F ( k ) } = p { F ( k ) }| X p Y |{ F ( k ) } , Z p ( Z | v , C ) p ( v |  X  ) where p Y |{ F ( k ) } , Z = Equation 10 represents the prior distribution over the infinite collection of Gaussian processes. The likeli-hood in our model is given in Equation 11; note that this distribution factorizes over the target dimensions but that the same Gaussian process covariance matrix for a given regressor is used for all dimensions. We also assume that the variances for each target variable dimension,  X  2 , are known and constant. This is a re-alistic assumption for our disease sub-typing use case: devices that measure disease severity can have their measurement variance characterized. For applications where  X  2 is not known, this and other hyperparame-ters can be automatically learned via empirical Bayes. Equation 12 describes the distribution over Z and con-sists of two terms: the first is a MRF that captures the pairwise constraints, and the second is a multinomial distribution with parameters drawn for a Dirichlet process using the stick-breaking construction. Equa-tion 13 expresses the distribution over the variable, v , used for the stick-breaking process; here  X  is the concentration parameter.
 The energy function used in our experiments is given by H ( z i , z j ) = where &lt; z i , z j &gt; represents the inner product between z and z j , ML stands for must-link and CL for cannot-link, and w i , j is in the interval [0 , 1] with lower values expressing less confidence in the constraint and vice-versa.
 While our formulation has similarities to L  X azaro-Gredilla et al. (2012), we emphasize that our algorithm is both nonparametric in the number of mixture com-ponents and semi-supervised, important features for our intended application. Additionally, while Orbanz &amp; Buhmann (2008) showed that MRFs can be incor-porated with DPMs, but they performed inference us-ing Gibbs sampling. In Section 4 we will show that variational inference can be applied provided certain conditions are satisfied by the constraints. In this section we give the variational inference up-date equations used in our model. Variational infer-ence is a method of approximate inference that makes assumptions (typically a factorization) over the dis-tribution of interest, and it turns an inference prob-lem into an optimization problem (Jordan et al., 1999; Jaakkola, 2001). Additionally, whereas approximate inference methods based on sampling (such as Monte Carlo Markov Chain) can be slow to converge, varia-tional inference enjoys a greater computational advan-tage in this regard.
 For our application, we are interested in the distribu-tion over the latent variables in our model given our observations: p Z , v , { F ( k ) }| X , Y . The posterior probability is approximated by optimizing the varia-tional lower bound. The standard variational inference approach is to assume a factorized approximation of this distribution, in our case p  X  ( Z ) p  X  { F ( k ) } p In order to derive the expression for one of these fac-tors, the expectation with respect to the other factors is considered. Derivation of the variational distribu-tions begins with the following expressions Given space limitations, we provide the expressions for each factor without derivation.
 The variational distribution over { F ( k ) } is given as where and Note that as in Blei &amp; Jordan (2006), our approximate distribution truncates the stick-breaking construction, so that k ranges from 1 to K (set to 20 in all our experiments).
 The expression for p  X  ( v ) is given by Finally, the distribution for p  X  ( Z ) is given by p  X  ( Z ) = Y where In Equation 23, V represents a set of sets. Each el-ement V of V is a set of data indices belonging to a connected subgraph of the constraint MRF. Because the set of constraints is generally sparse, the MRF can be characterized by a collection of disconnected subgraphs. If the constraint set is dense, we can ap-proximate the distribution by truncating the neigh-borhood to enforce low cardinality. It is important to note that the distribution factorizes over the resul-tant subgraphs. Given that each subgraph cardinality is small, it is feasible to compute the corresponding partition function, Z V . This in turn enables efficient computation of E Z { Z } .
 As an example, consider the MRF shown in Fig. 2. that each subgraph cardinality is low (with a max-imum of four in this example), so that their corre-sponding partition functions are easily computed. Inference begins by randomly initializing the matrix Z such that each element is equal to or greater than zero and each row sums to one. We then iteratively update equations 18, 22, and 23 until we observe no change in E
Z { Z } or until a pre-specified number of iterations is reached. In this section we demonstrate algorithm performance on both synthetic and real-world datasets. For all our experiments, the cardinality of the constraint sub-graphs was kept below 5. No special attention was given to the reported parameter settings for  X  ,  X  0 , or  X  . Rather, a coarse parameter selection of reasonable values was used. 5.1. Experiments on Synthetic Data We tested algorithm performance on two synthetic datasets. The first consists of noisy samples taken from two curves: a sinusoid and a sinusoid with a modest linear offset. The second dataset is made up of noisy samples taken from two interlaced helices in 3D. For both cases, the algorithm was run for 50 iter-ations.  X  was set to 1 . 0, and  X  0 was set to 1 . 0 in both cases. For the sinusoids experiment,  X  2 = 0 . 02 and  X  1 = 0 . 005. For the helices experiment,  X  2 = 0 . 1 and  X  1 = 0 . 0005. Must-link constraints were generated by randomly choosing pairs of points from a given func-tion, preferring pairs that are spaced farther apart. Cannot-link constraints were generated by randomly choosing pairs of points from different functions, pre-ferring pairs in regions where the functions tend to be closer to one another.
 We used normalized mutual information ( NMI ) (Strehl &amp; Ghosh, 2003) to investigate al-gorithm performance for a number of different constraints. Letting A represent the cluster assign-ments determined by the algorithm and B represent the ground-truth cluster assignments, the NMI is entropy. Higher NMI values mean that the clustering results are more similar to ground-truth; the criterion reaches its maximum value of one when there is perfect agreement.
 Figure 3 gives the results of the synthetic experiments. Each entry in the rightmost plot of this figure repre-sents the average NMI score across fifty, randomly ini-tialized runs. There is a clear increase in performance with added constraints in both cases. The center plots illustrate the regression curves found by the algorithm, and the data instances are color coded according to their association to each curve.
 Without constraints, the algorithm has a greater ten-dency to converge on solutions that may not be of interest. This is illustrated in Figure 4. While the so-lution shown does a reasonably good job of explaining the data, this particular solution might not be  X  X pti-mal X . By adding constraints, the optimization land-scape is modified to one more favorable for finding interesting solutions. 5.2. Experiments on Real-World Data As stated in the introduction, the motivation for our model stems from the need to identify clinically mean-ingful subtypes of lung disease. Here we show results on data from the Normative Aging Study (NAS) (Bell, 1972), a longitudinal study designed to investigate the role of aging on various health issues, including lung function. The complete dataset includes a large num-ber of features; here we focus on the effects of age on a widely used measure of lung function, FEV1 (forced expiratory volume in one second). We randomly chose a subset of forty subjects such that each subject was represented at a minimum of five time points and ev-eryone had approximately the same height.
 Since our goal is to identify regression curves that asso-ciate subjects according to genetics, longitudinal stud-ies like NAS provide a good arena in which to test and implicitly provide constraints: all data instances be-longing to a given subject are must-linked together (i.e., there are  X  X uilt-in X  constraints).
 It is known that lung function decline (as measured by a decrease in FEV1) is a natural part of the aging process, even in healthy individuals. However, some individuals are thought to experience a more rapid de-cline while others a more modest decline. This effect is thought to be even more pronounced as a function of smoke exposure. For our initial analysis we focus on age as the input variable. (Accurately capturing ex-posure to smoke is nontrivial and will be the focus of our future work). We investigate our algorithm X  X  per-formance by performing a five-fold cross validation. For each fold the test set consists of a randomly se-lected time point for each individual, and the training set consists of the remaining data. No data point is re-peated as a test instance across the different folds. For each of the five training sessions, we learn regression curves both with and without constraints, identifying solutions with the lowest variational bound in each case.
 We want to identify curves that are geneti-cally/biologically meaningful despite various levels of measured lung function, so we desire solutions such that the data points for an individual are associated to the same curve during the training phase. We re-port the percentage of times this occurs for each of the five folds, both with and without constraints. We are also interested in the predictive power of the learned regressors. For each instance in the test set we identify the curve most often associated to that indi-vidual in the training set and use that regressor to pre-dict the FEV1 value associated with the test instance; we do this for both the constrained and unconstrained cases. Additionally, we compare our predictions to those made by the currently accepted prediction equa-tion used in clinical practice (Hankinson, 1999) given by We ran 50 iterations for all experiments and set  X  2 = 0 . 0225,  X  = 1 . 0,  X  0 = 1 . 0, and  X  1 = 0 . 002. The results are summarized in Table 1.
 We also highlight examples of learned regressors for both the constrained and unconstrained case in Fig-ure 5. The constrained case depicts trends that agree well with clinical expectation, while the unconstrained case shows an unexpected increase in lung health for one of the sub-populations, clearly contrary to what is known about lung physiology and the aging process. Although our algorithm was designed specifically for application to lung disease sub-typing, our last exper-iment shows that it is potentially useful for related tracking scenarios. We demonstrate this by consid-ering a video-sequence taken from the EU CAVIAR dataset 1 . This is a human-labeled benchmark se-quence featuring four individuals walking through a scene. Each of the 1 , 164 data instances used here con-sist of the sequence X  X  frame number, and the target val-ues are the centroids of each detected bounding-box. Each of the four individuals in the scene is assigned a unique ID in the available ground-truth, and we use that information to impose ML and CL constraints on our algorithm. We again run for 50 iterations and set  X  2 = 2 . 0,  X  = 0 . 03,  X  0 = 100 . 0, and  X  1 = 0 . 0005. Results are shown in Figure 7.
 For all experiments described in this section, our algo-rithm was able to identify meaningful results both in terms of the number of regressors as well as their func-tional forms. As the number of constraints increases, the results converged on are more likely to represent the solution of interest. The flexibility to automati-cally identify both the number of regressors and their forms while honoring valuable expert input are the key advantages of our approach. We have introduced a nonparametric, mixture of Gaussian process regression framework that uses must-link and cannot-link constraints to identify solutions of interest. Our motivation for building this model is to assist with lung disease sub-type idenfication; we have provided a new way of looking at this problem by recasting it in terms of discovering associations of individuals to disease trajectories, and we have demon-strated the efficacy of our approach on real-world clin-ical data. In the process of designing an appropriate learning model for solving this clinical problem, we have developed a novel Dirichlet process mixture of Gaussian processes with constraints. It is applicable to other applications requiring clustering/data associ-ation to trajectories or nonparametric functions. We have also successfully shown its effectiveness on syn-thetic and tracking data.
 This work was supported by US NIH grants R01 HL089856 and R01 HL089897. We thank Michael H. Cho and Peter J. Castaldi for their guidance on clinical aspects of our model, and we thank Augusto Litonjua for supplying Normative Aging Study data.
 Antoniak, Charles E. Mixtures of dirichlet processes with applications to bayesian nonparametric prob-lems. The annals of statistics , pp. 1152 X 1174, 1974. Barker, Bethan L and Brightling, Christopher E. Phe-notyping the heterogeneity of chronic obstructive pulmonary disease. Clinical Science , 124(6):371 X  387, 2013.
 Basu, S., Bilenko, M., Banerjee, A., and Mooney, R.J.
Probabilistic semi-supervised clustering with con-straints. Semi-supervised learning , pp. 71 X 98, 2006. Bell, Benjamin et al. The normative aging study: an interdisciplinary and longitudinal study of health and aging. The International Journal of Aging and Human Development , 3(1):5 X 17, 1972.
 Besag, Julian. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Sta-tistical Society. Series B (Methodological) , pp. 192 X  236, 1974.
 Blei, David M and Jordan, Michael I. Variational infer-ence for dirichlet process mixtures. Bayesian Anal-ysis , 1(1):121 X 143, 2006.
 Cho, Michael H et al. Variants in fam13a are asso-ciated with chronic obstructive pulmonary disease. Nature genetics , 42(3):200 X 202, 2010.
 Cho, Michael H et al. A genome-wide association study of copd identifies a susceptibility locus on chromo-some 19q13. Human molecular genetics , 21(4):947 X  957, 2012.
 Ferguson, Thomas S. A bayesian analysis of some non-parametric problems. The annals of statistics , pp. 209 X 230, 1973.
 Geman, Stuart and Geman, Donald. Stochastic relax-ation, gibbs distributions, and the bayesian restora-tion of images. Pattern Analysis and Machine Intel-ligence, IEEE Transactions on , (6):721 X 741, 1984. Gr  X un, Bettina and Leisch, Friedrich. Fitting finite mix-tures of generalized linear regressions in r. Computa-tional Statistics &amp; Data Analysis , 51(11):5247 X 5252, 2007.
 Hankinson, John L et al. Spirometric reference values from a sample of the general us population. Ameri-can journal of respiratory and critical care medicine , 159(1):179 X 187, 1999.
 Jaakkola, Tommi S. 10 tutorial on variational approxi-mation methods. Advanced mean field methods: the-ory and practice , pp. 129, 2001.
 Jain, Anil K, Murty, M Narasimha, and Flynn,
Patrick J. Data clustering: a review. ACM com-puting surveys (CSUR) , 31(3):264 X 323, 1999.
 Jordan, M.I., Ghahramani, Z., Jaakkola, T.S., and
Saul, L.K. An introduction to variational methods for graphical models. Machine learning , 37(2):183 X  233, 1999.
 L  X azaro-Gredilla, Miguel, Vaerenbergh, Steven Van, and Lawrence, Neil D. Overlapping mixtures of gaussian processes for the data association problem. Pattern Recognition , 45(4):1386 X 1395, 2012.
 Li, Lei and Prakash, B Aditya. Time series cluster-ing: Complex is simpler! In Proceedings of the 28th International Conference on Machine Learning (ICML-11) , pp. 185 X 192, 2011.
 Meeds, Edward and Osindero, Simon. An alternative infinite mixture of gaussian process experts. Ad-vances in Neural Information Processing Systems , 18:883, 2006.
 Orbanz, P. and Buhmann, J.M. Nonparametric bayesian image segmentation. International Jour-nal of Computer Vision , 77(1):25 X 45, 2008.
 Rasmussen, C.E. and Ghahramani, Z. Infinite mix-tures of gaussian process experts. Advances in neu-ral information processing systems , 2:881 X 888, 2002. Rasmussen, C.E. and Williams, C.K.I. Gaussian pro-cesses for machine learning , volume 1. MIT press Cambridge, MA, 2006.
 Sethuraman, Jayaram. A constructive definition of dirichlet priors. Technical report, DTIC Document, 1991.
 Silverman, Edwin K and Sandhaus, Robert A. Alpha1-antitrypsin deficiency. New England Journal of Medicine , 360(26):2749 X 2757, 2009.
 Strehl, Alexander and Ghosh, Joydeep. Cluster ensembles X  X  knowledge reuse framework for com-bining multiple partitions. The Journal of Machine Learning Research , 3:583 X 617, 2003.
 Wagstaff, Kiri and Cardie, Claire. Clustering with instance-level constraints. In Proceedings of the Seventeenth International Conference on Machine Learning , pp. 1103 X 1110, 2000.
 Yuan, Chao and Neubauer, Claus. Variational mixture of gaussian process experts. Advances in Neural In-formation Processing Systems , 21:1897 X 1904, 2009. Zhu, Xiaojin. Semi-supervised learning literature sur-
