 Electronics and Information Systems Department In 2001, Jaeger [1] and Maass [2] independently introduced t he idea of using a fixed, randomly stability where the system has fading memory). A memoryless readout is then trained on these basis filters in order to approximate a given time-invariant targe t operator with fading memory [2]. Jaeger used analog sigmoidal neurons as network units and named the model Echo State Network (ESN). Maass termed the idea Liquid State Machine (LSM) and most of t he related literature focuses on net-works of spiking neurons or threshold units. Both ESNs and LS Ms are special implementations of a concept now generally termed Reservoir Computing (RC) whic h subsumes the idea of using general dynamical systems (e.g. a network of interacting optical am plifiers [3])  X  the so-called reservoirs  X  in conjunction with trained memoryless readout functions as computational devices. These RC systems have already been used in a broad range of applicatio ns (often outperforming other state-of-the-art methods) such as chaotic time-series prediction [4 ], single digit speech recognition [5], and robot control [6].
 Although ESNs and LSMs are based on very similar ideas (and in applications it seems possible to switch between both approaches without loss of performance [7]) an apparent dichotomy exists in the influence of the reservoir X  X  topological structure on it s computational performance. The perfor-mance of an ESN using analog, rate-based neurons, is e.g. lar gely independent of the sparsity of the network [8] or the exact network topology such as small-worl d or scale-free connectivity graphs 1 . For LSMs, which consist of spiking or binary units, the oppos ite effect has been observed. For the latter systems, the influence of introducing e.g. small-wor ld or biologically measured lamina-specific [10] it can be observed (although not specifically stated the re) that for networks of threshold units with a simple connectivity topology of fixed in-degree per ne uron, an increase in performance can be found for decreasing in-degree. None of these effects can be reproduced using ESNs. In order to systematically study this fundamental differen ce between binary (spiking) LSMs and analog ESNs, we close the gap between them by introducing in S ec. 2 a class of models termed quantized ESNs. The reservoir of a quantized ESN is defined as a network of discrete units, where the number of admissible states of a single unit is controlle d by a parameter called quantization level. LSMs and ESNs can be interpreted as the two limiting ca ses of quantized ESNs for low and high quantization level respectively. We numerically stud y the influence of the network topology in terms of the in-degree of the network units on the computatio nal performance of quantized ESNs for different quantization levels. This generalizes and syste mizes previous results obtained for binary LSMs and analog ESNs.
 In Sec. 3 the empirical results are analyzed by studying the L yapunov exponent of quantized ESNs, which exhibits a clear relation to the computational perfor mance [11]. It is shown that for ESNs with low quantization level, the chaos-order phase transit ion is significantly more gradual when the networks are sparsely connected. It is exactly in this trans ition regime that the computational power of a Reservoir Computing system is found to be optimal [11]. T his effect disappears for ESNs with high quantization level. A clear explanation of the influenc e of the in-degree on the computational performance can be found by investigating the rank measure p resented in [11]. This measure charac-and the generalization ability. We show that for highly conn ected reservoirs with a low quantization level the region of an efficient trade-off implying high perf ormance is narrow. For sparser networks this region is shown to broaden. Consistently for high quant ization levels the region is found to be independent of the interconnection degree.
 In Sec. 4 we present a novel mean-field predictor for computat ional power which is able to reproduce the influence of the topology on the quantized ESN model. It is related to the predictor introduced in [10], but it can be calculated for all quantization levels , and can be determined with a signifi-cantly reduced computation time. The novel theoretical mea sure matches the experimental and rank measure findings closely. We consider networks of N neurons with the state variable x ( t ) = ( x in discrete time t  X  Z . All units have an in-degree of K , i.e. every unit i receives input from K other randomly chosen units with independently identicall y distributed (iid.) weights drawn from a normal distribution N (0 , X  2 ) with zero mean and standard deviation (STD)  X  . The network state is updated according to: where g = tanh is the usual hyperbolic tangent nonlinearity and u denotes the input common to all units. At every time step t , the input u ( t ) is drawn uniformly from { X  1 , 1 } . The function  X  called quantization function for m bits as it maps from (  X  1 , 1) to its discrete range S Here  X  x  X  denotes the integer part of x . Due to  X  assume values in S A m = 1 B m = 3 C m = 6 Figure 1: The performance p N = 150 , the results have been averaged over 10 circuits C , initial conditions and randomly drawn input time series of length 10 4 time steps. The dashed line represents the numerically dete rmined critical line. a given delay parameter  X   X  0 , i.e., it is given by f f
T  X  { f | f : { X  1 , 1 } n  X  X  X  1 , 1 }} the form sign( P N  X  consisting of the network and the linear classifier is called a quantized ESN of quantization level m in the remainder of this paper.
 We assessed the computational capabilities of a given netwo rk based on the numerically determined performance on an example task, which was chosen to be the  X  -delayed parity function of n bits n  X  1 . A separate readout classifier is trained for each combinati on of n and  X  , all using the same reservoir. We define p where  X  ( C, PAR for the general computational capabilities of a circuit C as qualitatively very similar results were obtained for the AND In Fig. 1 the performance p quantization levels m = 1 , 3 , 6 . p K and the logarithm 3 of the weight STD  X  . Qualitatively very similar results were obtained for different network graphs with e.g. Poisson or scale-free di stributed in-degree with average K (results not shown). A numerical approximation of the critical line, i.e. the order-chaos phase transition, is also shown (dashed line), which was determined by the root of an estimation of the Lyapunov networks with a small in-degree K reach a significantly better peak performance than those wit h Figure 2: Phase transitions in binary networks ( m = 1 ) differ from phase transition in high resolu-tion networks ( m = 6 ). An empirical estimate  X  of the Lyapunov exponent is plotted as a function of the STD of weights  X  for in-degrees K = 3 (solid), K = 12 (dashed), and K = 24 (gray line). In order to facilitate comparison, the plot for each K is centered around log(  X  of weights for which  X  is zero (i.e.,  X  sharpens with increasing K for binary reservoirs (A) , whereas it is virtually independent of K for high resolution reservoirs (B) . high in-degree. The effect disappears for a high quantizati on level ( m = 6 ). This phenomenon is consistent with the observation that network connectivity structure is in general an important issue if the reservoir is composed of binary or spiking neurons but less important if analog neurons are employed. Note that for m = 3 , 6 we see a bifurcation in the zones of optimal performance whic h is not observed for the limiting cases of ESNs and LSMs. Where does the difference between binary and high resolution reservoirs shown in Fig. 1 originate from? It was often hypothesized that high computational pow er in recurrent networks is located in a parameter regime near the critical line, i.e., near the pha se transition between ordered and chaotic behavior (see, e.g., [12] for a review; compare also the perf ormance with the critical line in Fig. 1). Starting from this hypothesis, we investigated whether the network dynamics of binary networks near this transition differs qualitatively from the one of h igh resolution networks. We estimated the network properties by empirically measuring the Lyapunov e xponent  X  with the same procedure as in the estimation of the critical line in Fig. 1 (see text abov e). However, we did not only determine the critical line (i.e., the parameter values where the esti mated Lyapunov exponent crosses zero), but also considered its values nearby. For a given in-degree K ,  X  can then be plotted as a function of the STD of weights  X  (centered at the critical value  X  the dependence of  X  on the STD  X  near the critical line is qualitatively quite different bet ween the two types of networks. For binary networks the transition be comes much sharper with increasing K which is not the case for high resolution networks. How can th is sharp transition explain the reduced computational performance of binary ESNs with high in-degree K ? The tasks considered in this article require some limited amount of memory which h as to be provided by the reservoir. Hence, the network dynamics has to be located in a regime wher e memory about recent inputs is phase transition could be stated in the following way. For lo w  X  (i.e., in the ordered regime), the memory needed for the task is not provided by the reservoir. A s we increase  X  , the memory capacity increases, but older memories interfere with recent ones, m aking it hard or even impossible to extract the relevant information. This intuition is confirmed by an a nalysis which was introduced in [11] and which we applied to our setup. We estimated two measures of th e reservoir, the so called  X  X ernel-quality X  and the  X  X eneralization rank X , both being the rank of a matrix consisting of certain state vectors of the reservoir. To evaluate the kernel-quality of the reservoir, we randomly drew N = 150 input streams u m=1bit m=6bit Figure 3: Kernel-quality and generalization rank of quanti zed ESNs of size N = 150 . Upper plots are for binary reservoirs ( m = 1 bit), lower plots for high resolution reservoirs ( m = 6 bit). A) The difference between the kernel-quality and the generalizat ion rank as a function of the log STD of weights and the in-degree K . B) The kernel-quality (solid), the generalization rank (dash ed) and the difference between both (gray line) for K = 3 as a function of log(  X  ) . C) Same as panel B, but for an in-degree of K = 24 . In comparison to panel B, the transition of both measures is much steeper. D,E,F) Same as panels A, B, and C respectively, but for a high resolut ion reservoir. All plotted values are means over 100 independent runs with rand omly drawn networks, initial states, and input streams. reservoir-readout system to generalize from the training d ata to test data. The generalization rank is evaluated as follows. We randomly drew N input streams  X  u rank of the N  X  N matrix whose columns are the circuit states resulting from t hese input streams. Intuitively, the generalization rank with this input distr ibution measures how strongly the reservoir state at time t is sensitive to inputs older than three time steps. The rank m easures calculated here will thus have predictive power for computations which requ ire memory of the last three time steps (see [11] for a theoretical justification of the measures). I n general, a high kernel-quality and a low generalization rank (corresponding to a high ability of the network to generalize) are desirable. Fig. 3A and D show the difference between the two measures as a function of log(  X  ) and the in-degree K for binary networks and high resolution networks respectiv ely. The plots show that the peak value of this difference is decreasing with K in binary networks, whereas it is independent of
K in high resolution reservoirs, reproducing the observatio ns in the plots for the computational performance. A closer look for the binary circuit at K = 3 and K = 24 is given in Figs. 3B and 3C. When comparing these plots, one sees that the transition o f both measures is much steeper for K = 24 than for K = 3 which leads to a smaller difference between the measures. We interpret this finding in the following way. For K = 24 , the reservoir increases its separation power very fast as log(  X  ) increases. However the separation of past input difference s increases likewise and thus early input differences cannot be distinguished from late ones. T his reduces the computational power of binary ESN with large K on such tasks. In comparison, the corresponding plots for hi gh resolution reservoirs (Figs. 3E and 3F) show that the transition shifts to lower weight STDs  X  for larger K , but apart from this fact the transitions are virtually ident ical for low and high K values. Comparing A m = 1 B m = 3 C m = 6 Figure 4: Mean-field predictor p function of the STD  X  of the weights and in-degree K . A ) m = 1 . B ) m = 3 . C ) m = 6 . Compare this result to the numerically determined performance p Fig. 3D with Fig. 1C, one sees that the rank measure does not ac curately predict the whole region of good performance for high resolution reservoirs. It also does not predict the observed bifurcation in the zones of optimal performance, a phenomenon that is rep roduced by the mean-field predictor introduced in the following section. The question why and to what degree certain non-autonomous d ynamical systems are useful de-vices for online computations has been addressed theoretic ally amongst others in [10]. There, the computational performance of networks of randomly connect ed threshold gates was linked to their separation property (for a formal definition see [2]): It was shown that only networks which exhibit sufficiently different network states for different instan ces of the input stream, i.e. networks that separate the input, can compute complex functions of the inp ut stream. Furthermore, the authors in-troduced an accurate predictor for the computational capab ilities for the considered type of networks based on the separation capability which was quantified via a simple mean-field approximation of the Hamming distance between different network states.
 Here we aim at extending this approach to a larger class of net works, the class of quantized ESNs introduced above. However a severe problem arises when dire ctly applying the mean-field theory developed in [10] to quantized ESNs with a quantization leve l m &gt; 1 : Calculation of the important quantities becomes computationally infeasible as the stat e space of a network grows exponentially with m . Therefore we introduce a modified mean-field predictor whic h can be efficiently computed and which still has all desirable properties of the one intro duced in [10].
 Suppose the target output of the network at time t is a function f f
T  X  F ent function values. In order to quantify this so-called sep aration property of a given network, we introduce the normalized distance d ( k ) : It measures the average distance between two networks states x 1 ( t ) = ( x 1 u 2 ( t  X  k ) =  X  u 1 ( t  X  k ) . Formally we define 7 : 0 ,  X  &lt; k  X  n +  X  , is a necessary but not a sufficient condition for the ability of the network to calculate the target function. Beyond this, it is desired that the network  X  X orgets X  all (for the A m = 1 m = 1 B m = 6 m = 6 Figure 5: Contributions d (2) (dotted) and d (  X  ) (solid gray) to the mean-field predictor p of STD  X  of the weights. The plots show slices of the 2d plots Fig. 4 A and C for constant K . A ) For larger for K = 3 than for K = 24 . B ) For m = 6 this region is roughly independent of K except a shift. for k &gt; n +  X  . We use the limit d (  X  ) = lim the quantity p As the first contribution to p combination of two mechanisms: In order to exhibit a high val ue for d (2) the network has to separate in the next time step t  X  1 . We therefore consider d (2) to be a measure for input separation on short time-scales relevant for the target function. p one presented in [10] which itself is rooted in the annealed a pproximation (AA) introduced in [13]. In the AA one assumes that the circuit connectivity and the co rresponding weights are drawn iid. at every time step. Although being a drastic simplification, the AA has been shown to yield good results in the large system size limit N  X   X  . The main advantage of p defined in [10] (the NM-separation) is that the calculation o f p independent inputs needed for the NM-separation, resultin g in a significantly reduced computation time.
 In Fig. 4 the predictor p in-degree K for three different values of the quantization level m  X  { 1 , 3 , 6 } . When comparing these results with the actual network performance p can see that p larger values of m the predictor p of the quantization level m on the performance discussed in Sec. 2 is well reproduced by p m = 1 the in-degree K has a considerable impact, i.e. for large K maximum performance drops significantly. For m &gt; 2 however, for larger values of K there also exists a region in the parameter space exhibiting maximum performance.
 The interplay between the two contributions d (2) and d (  X  ) of p dence of p of inputs on short time scales relevant for the target task, a property that is found predominantly in networks that are not strongly input driven. A small value of d (  X  ) guarantees that inputs on which the target function assumes the same value are mapped to near by network states and thus a linear m = 1 , as can be seen in Fig. 5 the region in log(  X  ) space where both conditions for good perfor-mance are present decreases for growing K . In contrast, for m &gt; 2 a reverse effect is observed: for increasing K the parameter range for  X  fulfilling the two opposing conditions for good performance grows moderately resulting in a large region of high p in close analogy to the behavior of the rank measure discusse d in Sec. 3. Also note that p the novel bifurcation effect also observed in Fig. 1. By interpolating between the ESN and LSM approaches to RC, th is work provides new insights into the question of what properties of a dynamical system lead to improved computational performance: Performance is optimal at the order-chaos phase transition , and the broader this transition regime, the better will the performance of the system be. We have confi rmed this hypothesis by several analyses, including a new theoretical mean-field predictor that can be computed very efficiently.The importance of a gradual order-chaos phase transition could explain why ESNs are more often used [7], it is significantly harder to create a LSM which operates at the edge-of-chaos: the excitation and inhibition in the network need to be finely balanced becau se there tends to be a very abrupt transition from an ordered to a epileptic state. For ESNs how ever, there is a broad parameter range in which they perform well. It should be noted that the effect of quantization cannot just be emulated by additive or multiplicative iid. or correlated Gaussian n oise on the output of analog neurons. The noise degrades performance homogeneously and the differen ces in the influence of the in-degree observed for varying quantization levels cannot be reprodu ced. The finding that binary reservoirs have superior performance for low in-degree stands in stark contrast to the fact that cortical neurons have very high in-degrees of over 10 4 . This raises the interesting question which properties and mechanisms of cortical circuits not accounted for in this ar ticle contribute to their computational power. In view of the results presented in this article, such mechanisms should tend to soften the phase transition between order and chaos.
 Acknowledgments Written under partial support by the FWO Flanders project # G. 0088.09, the Photonics@be Interuni-versity Attraction Poles program (IAP 6/10), the Austrian S cience Fund FWF projects # P17229-N04, # S9102-N13 and projects # FP6-015879 (FACETS), # FP7-2 16593 (SECO) of the EU.

