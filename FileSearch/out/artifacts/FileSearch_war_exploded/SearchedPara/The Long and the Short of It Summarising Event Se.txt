 An ideal outcome of pattern mining is a small set of informative patterns, containing no redundancy or noise, that identifies the key structure of the data at hand. Standard frequent pattern miners do not achieve this goal, as due to the pattern explosion typically very large numbers of highly redundant patterns are returned.
We pursue the ideal for sequential data, by employing a pattern set mining approach X  X n approach where, instead of ranking patterns individually, we consider results as a whole. Pattern set mining has been successfully applied to transactional data, but has been surprisingly understudied for sequential data.

In this paper, we employ the MDL principle to identify the set of sequential patterns that summarises the data best. In particular, we formalise how to encode sequential data using sets of serial episodes, and use the encoded length as a quality score. As search strategy, we propose two approaches: the first algorithm selects a good pattern set from a large candidate set, while the second is a parameter-free any-time algorithm that mines pattern sets directly from the data. Experimentation on synthetic and real data demonstrates we efficiently discover small sets of informative patterns. H.2.8 [ Database management ]: Database applications X  Data mining Theory, Algorithms, Experimentation serial episodes, event sequence, pattern mining, pattern set mining
Suppose we are analysing an event sequence database, and are interested in its most important patterns. Traditionally, we would apply a frequent pattern miner, and mine all patterns that occur at least so-many times. However, due to the well-known pattern explosion we would then quickly be buried in huge amounts of highly redundant patterns, such that analysing the patterns becomes the problem, as opposed to the solution.

In this paper we therefore adopt a different approach. Instead of considering patterns individually, which is where the explosion stems from, we are after the set of patterns that summarises the data best. Desired properties of such a summary include that it should be small, generalise the data well, and be non-redundant. To this end, we employ the Minimum Description Length principle [5], which identifies the best set of patterns as that set by which we can describe the data most succinctly.

This approach has been shown to be highly successful for sum-marising transaction data [23], where the discovered patterns provide insight, as well as high performance in a wide range of data mining tasks, including clustering, missing value estimation, and anomaly detection [17, 22, 23]. Sequence data, however, poses additional challenges compared to itemsets. For starters, the order of events is important, and we have to take gaps in patterns into account. As such, encoding the data given a cover, finding a good cover given a set of patterns, as well as finding good sets of patterns, is much more complicated for sequences than for itemsets.
 We are not the first to consider summarising sequential data. Existing methods, however, are different in that they require a single pattern to generate a full sequence [12], do not consider gaps [2, 12], or do not punish gaps in patterns [9]. In short, none of these methods take the full expressiveness of episodes into account.

As we identify the best model by best lossless compression, and we consider strings as data, standard compression algorithms such as Lempel-Ziv and Huffman coding are related [16]. While general purpose compressors can provide top-notch compression, they do not result interpretable models. In our case, compression is not the goal, but a means: in order to summarise the data well, we are after those serial episodes that describe it most succinctly. We discuss related work in closer detail in Section 5.

In this paper, we introduce a statistically well-founded approach for succinctly summarising event sequences, or S QS for short X  pronounced as  X  squeeze  X . We formalise how to encode a sequence dataset given a set of episodes, and using MDL identify the best set as the set that describes the data most succinctly. To optimise this score, we give an efficient heuristic to determine which pattern best describes what part of your data. To find good sets of patterns, we introduce two heuristics: S QS -C ANDIDATES filters a given can-didate collection, and S QS -S EARCH is a parameter-free any-time algorithm that efficiently mines models directly from data.
Experiments on real and synthetic data show S QS efficiently dis-covers high-quality models that summarise the data well, correctly identify key patterns. The number of returned patterns stays small, up to a few hundred X  most importantly, though, the returned mod-els do not show redundancy, and none of the patterns are polluted by frequent, yet unrelated, events.

Altogether, the long and the short of it is that S QS mines small sets of the most important, non-redundant, serial episodes that together succinctly describe the data at hand.
In this section we formally introduce the problem we consider. As data type we consider event sequences . A sequence database D over an event alphabet  X  consists of | D | sequences S  X  D . Every S  X  D is a sequence of | S | events e  X   X  , i.e. S  X   X  | S | S [ i ] to mean the i th event in S and S [ i,j ] to mean a subsequence S  X  D , i.e. || D || = P S consider time stamps, however we can extend our framework to time stamped events.

The support of an event e in a sequence S is simply the number of occurrences of e in S , i.e. supp ( e | S ) = |{ i  X  S | i = e }| . The support of e in a database D is defined as supp ( e | D ) = P S  X  D supp ( e | S ) .

As patterns we consider serial episodes. A serial episode X is a sequence of events and we say that a sequence S contains X if there is a subsequence in S equal to X . Note that we are allowing gap events between the events of X . A singleton pattern is a single event e  X   X  .

All logarithms in this paper are to base 2 , and we employ the usual convention of 0 log 0 = 0 .
The Minimum Description Length principle (MDL) [5] is a prac-tical version of Kolmogorov Complexity [11]. Both embrace the slogan Induction by Compression . For MDL, this can be roughly described as follows.

Given a set of models M , the best model M  X  X  is the one that minimises L ( M ) + L ( D | M ) , in which L ( M ) is the length in bits of the description of M , and L ( D | M ) is the length of the data when encoded with model M .

This is called two-part MDL, or crude MDL X  X s opposed to refined MDL, where model and data are encoded together [5]. We use two-part MDL because we are specifically interested in the model: the patterns that give the best description. Further, although refined MDL has stronger theoretical foundations, it cannot be computed except for some special cases. Note that MDL requires the compression to be lossless in order to allow for fair comparison between different M  X  M , and that we are only concerned with code lengths, not actual code words.
 To use MDL, we have to define what our models M are, how a M  X  X  describes a database, and how we encode these in bits.
As models we consider code tables . A code table is essentially a look-up table, or dictionary, between patterns and associated codes. A code table has four columns, of which the first column contains patterns, the second column consists of codes for identifying these patterns, and the two right-most columns contain pattern-dependent codes for identifying gaps or the absence thereof within an embed-ding of a pattern. To ensure any sequence over  X  can be encoded by a code table, we require that all the singleton events in the alphabet, X  X   X  , are included in a code table CT .

To refer to the different codes in CT , we write code p ( X | CT ) when we refer to the code corresponding to a pattern X , as stored in the second column of a code table CT . Similarly, we write code g ( X | CT ) and code n ( X | CT ) to resp. refer to the codes stored in the third and fourth column, which indicate whether or not the next symbol is part of a gap in the usage of pattern X . For readability, we do not write CT wherever clear from context.
Our next step is to explain our encoding scheme. As we will see later, there are typically very many ways of encoding a database, apart from using only singletons. Hence, for clarity, we will first explain our encoding scheme by considering how to decode an already encoded database, and postpone finding a good encoding, or cover , as well as how to find a good code table to Sections 3 and 4. An encoded database consists of two code streams, C p and C follow from the cover C chosen to encode the database. The first code stream, the pattern-stream , denoted by C p , is a list of | C codes, code p (  X  ) , for patterns X  X  CT corresponding to those pat-terns chosen by  X  X over X  algorithm. For example, code p ( a ) code code p ( c ) encodes the sequence  X  abc  X .

Serial episodes are not simple subsequences, however, as they allow for gaps. That is, a pattern de specifies that event d , after possibly some other events, is followed by event e . As such, this pattern occurs both in sequence  X  de  X  as well as in sequence  X  dfe  X . In the former there is no gap between the two events, and in the latter there is a gap of length one, in which event f occurs.

As such, only when we read the code for a singleton pattern X we can directly unambiguously append X to the sequence S k are decoding X  X here can be no gap in a singleton pattern. When X is a non-singleton pattern, on the other hand, we may only append the first symbol x 1 of X to S k ; before appending event x we first need to know whether there is a gap between the two events in this usage of X , and if so, what event(s) occur in the gap. This is what the second code stream is for. This stream, the gap-stream , denoted by C g , is a list of codes from the third and fourth column of CT , indicating whether gaps occur when decoding patterns.
Given the gap-stream, we can determine whether the next event of S k may be read from the current pattern X , or we have to read a singleton pattern to fill the gap. Starting with an empty sequence for S , and assuming that we know its final length, we read the code for the first pattern X , code p ( X ) , from C p , and append the first event x of X to the sequence we are decoding. If X is a non-singleton pattern, we read from C g whether the next event is a gap-event, or not. If we read the gap-code code g ( X ) from C g there is indeed a gap, after which we read from C p the code ( Y ) for the (singleton) event Y  X  CT associated with this gap X  X nd append it to S k then read again from C g whether there is another gap, etc, until we encounter the no-gap code code n ( X ) in C g indicating we should append the next symbol x j of X to S k . Whenever we are finished decoding pattern X , we read the code for the next pattern X from C , until we have read as many events as S k should be long, after which we continue decoding S k +1  X  D until C p is depleted and all S  X  D are reconstructed.

Consider the toy example given as Fig. 1. One possible encoding would be to use only singletons, meaning that gap stream is empty. Another encoding is to use patterns. For example, to encode  X  abdc  X , we first give the code for abc in the pattern stream, then a no-gap code (white) in C g to indicate b , then a gap code (black) in C the code for d in C p , and we finish with a no-gap code in C Given the above decoding scheme we know what codes we can expect to read where and when, and hence can now formalise how to Data D : a b d c a d b a a b c Encoding 1: using only singletons
C Encoding 2: using patterns
C
C alignment Figur e 1: T oy example of tw o possible encodings. The first en-coding uses only singletons. The second encoding uses single-tons and tw o patter ns, namely , ab c and da calculate the lengths of these codes, as well as the encoded lengths of the code table and database.

W e start with L ( c o de p ( X )) , the lengths of pattern codes in C which we can look up in the second column of CT . By Shannon Entrop y [3] we kno w that the length, in bits, of the optimal prefix-free code for an e v ent X is  X  log Pr( X ) , where Pr( X ) is the probability of X . Let us write usage ( X ) for ho w often c o de occurs in C p . That is, usage ( X ) = |{ Y  X  C p | Y = c o de in C p . So, we ha v e
Similarly , the lengths of the codes for indicating the presence or absence of a g ap in the usage of a pattern X , resp. L ( c o de Let us write gaps ( X ) to refer to the number of g ap e v ents withi n the usages of a pattern X in the co v er of D . W e then resp. ha v e for the number of non-g ap codes in the usage of a pattern X , and
W e say a code table CT is code-opt imal for a co v er C of a database D if all the codes in CT are of the length according to their respecti v e usage frequencies in C p and C g as defined abo v e.
From the lengths of the indi vidual codes, the encoded length of the code streams no w follo ws straightforw ardly , with resp. for the encoded size of the pattern-stream, and for the encoded size of the g ap-stream.

Combining the abo v e, we define L ( D | CT ) , the encoded size of a database D gi v en a code table CT and a co v er C , as where | D | is the number of sequences in D , and | S | is the length of a sequence S  X  D . T o encode these v alues, for which we ha v e no prior kno wle dge, we emplo y the MDL optimal Uni v ersal code for inte gers [5, 15]. F or this encoding, L N , the number of bits required to encode an inte ger n  X  1 , is defined as where only the positi v e terms are included in the sum. T o mak e L a v alid encoding, c 0 is chosen as c 0 = P J  X  1 2  X  L N such that the Kraft inequality is satisfied.

Ne xt we discuss ho w to calculate L ( CT ) , the encoded size of a code table CT . T o ensure lossless compression, we need to encode the number of entries, for which we emplo y L N as defined abo v e. F or later use, and to a v oid bias by lar ge or small alphabets, we encode the number of singletons, |  X  | , an d the number of non-singleton entries, | CT \  X  | , separately . W e disre g ard an y non-the data, and has no v alid (or infinite length) pattern code.
F or the size of the left-hand side column, note that the simplest v alid code table c onsists onl y of the single e v ents. This code table we refer to as the standar d code table , or ST . W e enc ode the patterns in the left-hand side column using the pattern codes of ST . This allo ws us to decode up to the names of e v ents.
 As singletons cannot ha v e g aps, the usage of a singleton Y gi v en ST is simply the support of Y in D . Hence, the code length of Y in ST is defin ed as L ( c o de p ( Y ) | ST ) =  X  log supp ( Y | D ) Before we can use these codes, we must transmit these supports. W e transmit these using a data-to-model code [21], an inde x o v er a canonically ordered enumeration of all possibilities; here, the number of w ays || D || e v ents can be distrib uted o v er |  X  | labels, where none of the bins may be empty , as supp ( Y | D ) &gt; 0 . The number of such pos sibilities is gi v en by || D || X  1 |  X  | X  1 a log we ha v e the number of bits requ ired to identify the right set of v alues. Note that || D || is kno wn from L ( D | CT ) . In general, for the number of bits for an inde x of a number composition, the number of combinations of summing to m with n non-zero terms, we ha v e L U ( m, n ) = log m  X  1 n  X  1 , where for m = 0 , and n = 0 , we define L U ( m, n ) = 0 .

Combined, this gi v es us the information required to reconstruct the left-hand side of CT for the singletons, as well as the infor -mation needed to decode the non-si ngleton patterns of CT . F or a pattern X , the number of bits in the left-hand column is the length of X , | X | , as encoded by L N , and the sum of the singleton codes
Ne xt, we encode the second column. T o avoid bias, we treat the singletons a nd non-singleton entries of CT dif ferently . Let us write P to refer to the non-singleton patterns in CT , i.e. P = CT \  X  . F or the elements of P , we first encode the sum of thei r usages, denoted by usage ( P ) , and use a data-to-model code lik e abo v e to identify the correct set of indi vidual usages. W ith these v alues, and the singleton supports we kno w from ST , we can reconstruct the usages of the singletons in CT , and hence reconstruct the pattern codes associated with each pattern in CT . This leaves us the gap-codes for the non-singleton entries of CT . For reconstructing these, we need to know gaps ( X ) , which we encode using L N . The number of non-gaps then follows from the length of a pattern X and its usage. As such, we can determine code g ( X ) and code n ( X ) exactly.

Putting this all together, we have L ( CT | C ,D ) , the encoded size in bits of a code table CT for a cover C of a database D , as where L ( X, CT ) , the number of bits for encoding the events, length, and the number of gaps of patterns X in CT , is L ( X, CT ) = L N ( | X | ) + L N ( gaps ( X ) + 1) + X
By MDL, we can then define the optimal set of serial episodes for a given sequence database as the set for which the optimal cover and associated optimal code table minimises the total encoded size More formally, we define the problem as follows.
 Minimal Code Table Problem Let  X  be a set of events and let D be a sequence database over  X  , find the minimal set of serial episodes P such that for the optimal cover C of D using P and  X  , the total encoded cost L ( CT ,D ) is minimal, where CT is the code-optimal code table for C .

Clearly, this problem entails a rather large search space. First of all, given a set of patterns, there are many different ways to cover a database. Second, there are very many sets of serial episodes P we can consider, namely all possible subsets of the collection of serial episodes that occur in D . However, neither the full problem, or these sub-problems, exhibit trivial structure that we can exploit for fast search, e.g. (weak) monotonicity.

We hence break the Minimal Code Table Problem into two sub-problems. First, in the next section we discuss how to optimise the cover of a sequence given a set of episodes. Then, in Section 4, we will discuss how to mine high quality code tables.
Encoding, or covering, a sequence is more difficult than decoding one. The reason is simple: when decoding there is no ambiguity, while when encoding there are many choices, i.e. what pattern to encode a symbol with. In other words, given a set of episodes, there are many valid ways to cover a sequence, where by our problem definition we are after the cover C that minimises L ( CT ,D ) .
Due to lack of space, we provide the proofs in the Appendix
Assume we are decoding a sequence S k  X  D . Assume we decode the beginning of a pattern X at S k [ i ] and that the last symbol belonging to this instance of X is, say, S k [ j ] . We say that S an active window for X . Let P be the set of non-singleton patterns used by the encoding. We define an alignment A to be the set of all active windows for all non-singleton patterns X  X  X  as A = { ( i,j,X,k ) | S k [ i,j ] is an active window for X,S http://adrem.ua.ac.be/sqs/ An alignment corresponding to the second encoding given in Fig-ure 1 is { (1 , 4 , abc , 1) , (6 , 8 , da , 1) , (9 , 11 , abc , 1) } .
Note that an alignment A does not uniquely define the cover of the sequence, as it does not take into account how the intermediate symbols (if any) within the active windows of a pattern X are encoded. However, an alignment A for a sequence database D does define an equivalence class over covers of the same encoded length. In fact, given a sequence database D and an alignment A , we can determine the number of bits our encoding scheme would require for such a cover. To see this, let X be a pattern and let W = { ( i,j,X,k )  X  A } , then where The remaining symbols are encoded as singleton patterns. Hence, the usage of a singleton is equal to
Given an alignment A for D , we can trivially construct a valid cover C for D , simply by following A and greedily covering S with pattern symbols if possible, and singletons otherwise. That is, if for a symbol S k [ i ] we have, by A , the choice for covering it as a gap or non-gap of a pattern X , we choose non-gap.

Then, from either C , or directly from A , we can derive the as-sociated code-optimal code table CT . Given an alignment A , let us write CT ( A ) for this code table. Wherever clear from context, we will write L ( D | A ) to mean L ( D | CT ( A )) , and similarly L ( D,A ) as shorthand for L ( D,CT ( A )) .

Our next step is to show what kind of windows can occur in the optimal alignment. We say that W = S [ i,j ] is a minimal window of a pattern X if W contains X but no other proper sub-windows of W contain X . For example, in Figure 1 S [6 , 8] is a minimal window for da but S [6 , 9] is not.

P ROPOSITION 1. Let A be an alignment producing an optimal encoded length. Then all active windows in A are minimal windows. Proposition 1 says that we need to only study minimal windows. Let F be a set of episodes and let X  X  F . Since an event S can be a starting point to only one minimal window of X , there are only k D k minimal windows of X in D , at most. Consequently, the number of minimal windows we need to investigate is bounded by k D k|F| . Moreover, we can use F IND W INDOWS in [20] to discover all the minimal windows for a pattern X in O ( | X |k D k ) time.
Discovering an optimal alignment is non-trivial due to the com-plex relation between code lengths and the alignment. However, if we fix the alignment, Eqs. 1 X 3 give us the codes optimising L ( D | A ) . In this section we will show the converse, that if we fix the codes, we can easily find the alignment optimising L ( D | A ) . In order to do that let w = ( i,j,X,k ) be a minimal window for a pattern X . We define the gain to be gain ( w ) =  X  L ( code p ( X ))  X  ( j  X  i  X  X  X | ) L ( code P ROPOSITION 2. Let D be a dataset and A be an alignment. Then the length of encoding D is equal to where const does not depend on A .

This proposition suggests that if we fix the code lengths we need to maximise the total gain. In order for an alignment to be valid, the windows must be disjoint. Hence, given a set of W , consisting of all minimal windows of the given patterns, we need to find a subset O  X  W of disjoint windows maximising the gain.
 Assume that W is ordered by the first index of each window. For a window w , define next ( w ) to be the next disjoint window in W . Let o ( w ) be the optimal total gain of w and its subsequent windows. Let v be the next window of w , then the optimal total simple dynamic program, A LIGN , given as Algorithm 1.

Algorithm 1: A LIGN ( W ) input : minimal windows W sorted by the first event output : mutually disjoint subset of W having the optimal gain 1 o ( N + 1)  X  0 ; opt ( N + 1)  X  none ; 2 foreach i = N,..., 1 do 3 c  X  0 ; 4 if next ( i ) then c  X  o ( next ( i )) ; 5 if gain ( w i ) + c &gt; o ( i + 1) then 6 o ( i )  X  gain ( w i ) + c ; opt ( i )  X  i ; 7 else 8 o ( i )  X  o ( i + 1) ; opt ( i )  X  opt ( i + 1) ; 9 O  X  optimal alignment (obtained by iterating opt and next ); 10 return O ;
We can now use A LIGN iteratively. Given the codes we find the optimal alignment and derive the optimal codes given the new align-ment. We repeat this until convergence, which gives us a heuristic approximation to the optimal alignment A  X  for D using patterns P . As initial values, we use the number of minimal windows as usage and set gap code length to be 1 bit. The pseudo code of S stands for Summarising event seQuenceS, is given as Algorithm 2.
Algorithm 2: S QS ( D, P ) . Summarising event seQuenceS input : Database of sequences D , set of patterns P output : Alignment A 1 foreach s  X   X  do usage ( s )  X  supp ( s | D ) ; 2 foreach X  X  X  , | X | &gt; 1 do 3 W X  X  F IND W INDOWS ( X,D ) ; 4 usage ( X )  X  X  W X | ; gaps ( X )  X  X  X | X  1 ; 5 W  X  merge sort { W X } X  X  X  based on first event; 6 while changes do 7 compute gain for each w  X  W ; 8 A  X  A LIGN ( W ) ; 9 recompute usage and gaps from A (Eqs. 1 X 3); 10 return A ;
The computational complexity of single iteration comes down to the computational complexity of A LIGN ( W ) , which is O ( | W | )  X  O ( |P|k D k ) . Also note that next is precomputed before calling A LIGN and this can be also computed with a single scan, taking O ( | W | ) steps. Note that the encoded length improves at every iteration, and as there are only finite number of alignments, S converge to a local optimum in finite time. In practice, the number of iterations is small X  X n the experiments typically less than 10 .
With the above, we both know how to score the quality of a pattern set, as well as how to heuristically optimise the alignment of a pattern set. This leaves us with the problem of finding good sets of patterns. In this section we give two algorithms to do so.
Our first algorithm, S QS -C ANDIDATES , assumes that we have a (large) set of candidate patterns F . In practice, we assume the user obtains this set of patterns using a frequent pattern miner, although any set of patterns over  X  will do. From this set F we then select that set of patterns P  X  X  such that the optimal alignment A and associated code table CT minimises L ( D, CT ) .

For notational brevity, we simply write L ( D, P ) as shorthand for the total encoded size L ( D, CT ) obtained by the code table CT containing a set of patterns P and singletons  X  , and being code-optimal to the alignment A as found by S QS .

We begin by sorting the candidates X  X  X  by L ( D, { X } ) from lowest to highest. After sorting, we iteratively greedily test each pattern X  X  X  . If adding X to P improves the score, i.e. fewer bits are needed, we keep X in P , otherwise it is permanently removed. The pseudo-code for S QS -C ANDIDATES is given as Algorithm 3.
Algorithm 3: S QS -C ANDIDATES ( F ,D ) input : candidate patterns F output : set of non-singleton patterns P that heuristically 1 order patterns X  X  X  based on L ( D, { X } ) ; 2 P  X  X  X  ; 3 foreach X  X  X  in order do 4 if L ( D, P  X  X ) &lt; L ( D, P ) then 5 P  X  P RUNE ( P  X  X,D, false ) ; 6 P  X  P RUNE ( P ,D, true ) ; 7 order patterns X  X  G by L ( D, P )  X  L ( D, P \ X ) ; 8 return P ;
During the search we iteratively update the code table Hence, it may be that over time, previously included patterns start to harm compression once their role in covering the sequence is taken over by new, more specific, patterns. As such, they become redundant, and should be removed from P .

To this end, we prune redundant patterns (see Algorithm 4) after each successful addition. During pruning, we iteratively consider each pattern Y  X  P in order of insertion. If P \ X improves the total encoded size, we remove X from P . As testing every pattern in P at every successful addition may become rather time-consuming, we use a simple heuristic: if the total gain of the windows of X is higher than the cost of X in the code table we do not test X .
After S QS -C ANDIDATES considered every pattern of F , we run one final round of pruning without this heuristic. Finally, we order the patterns in P by L ( D, P )  X  L ( D, P\ X ) . That is, by the impact on the total encoded length when removing X from P . This order tells us which patterns in P are most important.

Let us consider the execution time needed by S QS -C ANDIDATES Ordering patterns can be done in O ( |F|k D k ) time. Computing L ( D, P  X  X ) can be done in O ( |P|k D k )  X  O ( |F|k D k ) time.
Algorithm 4: P RUNE ( P ,D, full ) input : pattern set P , database D , boolean variable full , output : pruned pattern set P ; 1 foreach X  X  X  do 2 CT  X  code table corresponding to S QS ( D, F ) ; 3 CT 0  X  code table obtained from CT by deleting X ; 5 if full or g &lt; L ( CT )  X  L ( CT 0 ) then 6 if L ( D, P \ X ) &lt; L ( D, P ) then P  X  X  \ X ; 7 return P ; Pruning can be done in O ( |P| 2 k D k )  X  O ( |F| 2 k D k ) . Combined, this gives us a total time complexity of O ( |F| 3 k D k ) . In practice, the algorithm is much faster, however, as first, due to MDL the code tables remain small, and hence |P| |F| , second, the execution time of S QS is typically faster than O ( |P|k D k ) , and third, the pruning heuristic further reduces the computational burden.
The S QS -C ANDIDATES algorithm requires a collection of candi-date patterns to be materialised, which in practice can be trouble-some; the well-known pattern explosion may prevent patterns to be mined at as low thresholds as desired. In this section we propose an alternative strategy for discovering good code tables directly from data. Instead of filtering a pre-mined candidate set, we now discover candidates on the fly, considering only patterns that we expect to optimise the score given the current alignment.

To illustrate the general idea, consider that we have a current set of patterns P . We iteratively find patterns of form XY , where X,Y  X  P  X   X  producing the lowest L ( D, P  X  X  XY } ) . We add XY to P and continue until no gain is possible. Unfortunately, as we cannot do this exhaustively and exactly within reasonable time. Hence, we resort to heuristics.

To guarantee the fast discovery of good candidates, we design a heuristic algorithm that, given a pattern P , will find a pattern PQ of high expected gain in only O ( |P| + |  X  | + k D k ) time.
Our first step is to demonstrate that if we take N active windows of P , and N active windows of Q , and convert them into N active windows of PQ , the difference in total encoded length can be computed in constant time.
 P ROPOSITION 3. Fix a database D and an alignment A . Let P and Q be two patterns. Let V = { v 1 ,...,v N } and W = { w 1 ,...,w N } be two set of candidate windows for P and Q , re-spectively. Assume that either P ( Q ) is a singleton or each v occurs in A . Assume that v i and w i occur in the same sequence and write v i = ( a i ,b i ,P,k i ) and w i = ( c i ,d i ,Q,k b &lt; c i . Write R = PQ and let Assume that U has no overlapping windows and has no overlapping windows with A \ ( V  X  W ) . Then the difference depends only N , gaps ( V ) , gaps ( W ) , and gaps ( U ) and can be computed in constant time from these values.
 The conditions given in Proposition 3 are needed so that A \ ( V  X  W ) is a proper alignment. We denote the aforementioned difference by diff ( V,W,U ; A,D ) . Note that this difference partly depends on A and D . However, since we keep these fixed in the proposition they only contribute constant terms. Further note that U should not overlap with A \ ( V  X  W ) . We will address this limitation later. We should point out that in practice we do not keep lists of U , V , and W , but instead exploit the gap counts and number of windows, as this is sufficient for computing the difference.

Now that we have a way of computing the gain of using windows for PQ , we need to know which windows to use in the alignment. The following proposition suggest that we should pick the windows with the shortest length.
 P ROPOSITION 4. Let D be a database and A be an alignment. Let v = ( i,j,X,k )  X  A . Assume that there exists a window S containing X such that w = ( a,b,X,l ) does not overlap with any window in A and b  X  a &lt; j  X  i . Then A is not an optimal alignment.
This proposition gives us an outline of the heuristic. We start enumerating minimal windows of PQ from shortest to largest. At each step we compute the score using Proposition 3, and among these scores we the pick optimal one.

We cannot guarantee linear time if we consider each Q individu-ally. Instead, we scan for all candidates simultaneously. In addition, to guarantee linear time we consider only active windows of P and Q , and do not consider singletons occurring in the gaps. The scan starts by finding all the active windows (ignoring singletons in gaps) of P . We then continue by scanning the patterns occurring after each P . We interleave the scans in such a way that the new minimal windows are ordered, from shortest to longest. We stop the scan after we find next occurrence of P or the end of the sequence.
There are two constraints that we need to take into account. When enumerating minimal windows of PQ we need to make sure that we can add them to the alignment. That is, a new minimal window cannot intersect with other new minimal windows, and the only windows it may intersect in the alignment are the two windows from which it was constructed. The first constraint can only happen when Q = P , in which case we simply check if the adjacent scans have already used these two instances of P for creating a minimal window for pattern PP . To guarantee the second constraint, we need to delete the intersecting windows from the alignment. We estimate the effect of deleting w by adding gain ( w ) (computed from the current alignment) to the score. The pseudo-code for calculating this estimate is given as Algorithm 5.

P ROPOSITION 5. E STIMATE ( P,  X  ,D ) returns a pattern with optimal score.

Next, let us consider the computational complexity of this ap-proach. The initialisation in E STIMATE can be done in O ( |  X  | + |P| + k D k ) , where P are the current non-singleton patterns. After selecting the next window, each step in the main loop can be done in constant time. The only non-trivial step is picking the next smallest window. However, since the window lengths are integers smaller or equal than k D k , we can store the candidates into an array of lists, say N d , where N d contains the windows of length d . Finding the next window may take more than a constant time since we need to find the next non-empty list N d but such search may only contribute k D k checks in total. Since we stop after encountering P , every event is visited only twice at maximum, hence the running time for E STIMATE is O ( |  X  | + |P| + k D k ) .

The actual search algorithm, S QS -S EARCH , calls E STIMATE each pattern P . The algorithm, given as Algorithm 6, continues by sorting the obtained patterns based on their estimated scores and attempts to add them into encoding in the same fashion as in
Algorithm 5: E STIMATE ( P,A,D ) . Heuristic for finding a pat-tern X used by the current encoding with a low L ( D,A  X  PX ) input : database D , current alignment A , pattern P  X  CT output : pattern PX with X  X  CT and a low L ( D,A  X  PX ) 1 foreach X  X  CT do V X  X  X  X  ; W X  X  X  X  ; U X  X  X  X  ; d X  X  0 ; 2 T  X  X  X  ; 3 foreach occurrence v of P in the encoding (ignoring gaps) do 4 ( a,b,P,k )  X  v ; 5 d  X  the end index of the active window following v ; 6 t  X  ( v,d, 0) ; l ( t )  X  d  X  a ; 7 add t into T ; 8 while T is not empty do 9 t  X  arg min u  X  T l ( u ) ; 10 ( v,d,s )  X  t ; a  X  first index of v ; 11 w = ( c,d,X,k )  X  active window of a pattern ending at d ; 12 if X = P and (event at a or d is marked) then 13 delete t from T ; 14 continue ; 15 if S k [ a,d ] is a minimal window of PX then 16 add v into V X ; 17 add w into W X ; 18 add ( a,d, PX ,k ) into U X ; 19 d X  X  min( diff ( V,W,U ; A ) + s,d X ) ; 20 if | X | &gt; 1 then s  X  s + gain ( w ) ; 21 if X = P then 22 mark the events at a and d ; 23 delete t from T ; 24 continue ; 25 if w is the last window in the sequence then 26 delete t from T ; 27 else 28 d  X  the end index of the active window following w ; 29 update t to ( v,d,s ) and l ( t ) to d  X  a ; 30 return PX with the lowest d X ; S
QS -C ANDIDATES . After each successful addition of pattern X , we scan for the gap events occurring in the active windows of X , and test patterns obtained from X by adding a gap event as intermediate event. The scan can be done in O ( k D k ) time, and in theory we may end up testing |  X  | ( | X | X  1) patterns. In practice, the number is much smaller since accepted patterns typically have small gaps. If any of these patterns in successfully added we repeat this procedure in a recursive fashion. In practice, testing X is relatively fast, and the total computational complexity is dominated by E STIMATE
Discovering frequent sequential patterns is an active research topic. Unlike for itemsets, there are several definitions for frequent sequential patterns. The first approach counts the number of se-quences containing a pattern [24]. In such setup, having one long sequence do not make sense. In the second approach we count multiple occurrences within a sequence. This can be done by sliding a window [13] or counting disjoint minimal windows [10].
Mining general episodes, patterns where the order of events are specified by a DAG is surprisingly hard. For example, testing whether a sequence contains a pattern is NP -complete [19]. Con-sequently, research has focused on mining subclasses of episodes, such as, episodes with unique labels [1, 14], and strict episodes [20].
Algorithm 6: S QS -S EARCH ( D ) input : database D output : significant patterns P 1 P  X  X  X  ; A  X  S QS ( D,  X  ) ; 2 while changes do 3 F  X  X  X  ; 4 foreach P  X  CT do add E STIMATE ( P,A,D ) to F ; 5 foreach X  X  X  ordered by the estimate do 6 if L ( D, P  X  X ) &lt; L ( D, P ) then 7 P  X  P RUNE ( P  X  X,D, false ) ; 8 if X is added then test recursively X augmented with 9 P  X  P RUNE ( P ,D, true ) ; 10 order patterns X  X  G by L ( D, P )  X  L ( D, P \ X ) ; 11 return P ;
Discovering statistically significant sequential patterns is a sur-prisingly understudied topic. One reason is that unlike for itemsets, computing an expected frequency under a null-hypothesis is very complex. Using independence assumption as a null-hypothesis has been suggested in [7, 18] and a Markov-chain model has been suggested in [6]. In [1] the authors use information theory-based measure to determine which edges to include in a general episode.
Summarising sequences using segmentation is a well-studied topic. The goal in segmentation is to divide the sequence in large segments of homogenous regions whereas our goal is to find a set of compact patterns that occur significantly often. For an overview in segmentation, see [4], and for a segmentation tool see [8].
Mannila and Meek [12] regard general episodes, as generative models for sequences. Their model generates short sequences by selecting a subset of events from an episode and select a random order compatible with the episode. They do not allow gaps and only one pattern is responsible for generating a single sequence. This is not feasible for our setup, where we may have long sequences and many patterns occurring in one sequence.

S QS draws inspiration from the K RIMP [23] and S LIM [17] al-gorithms. K RIMP pioneered the use of MDL for identifying good pattern sets; specifically, mining sets of itemsets that describe a transaction database well. As serial episodes are much more expres-sive than itemsets, we here need a much more elaborate encoding scheme, and in particular, a non-trivial approach for covering the data. For mining the patterns, S QS -C ANDIDATES shares the greedy selection over an ordered set of candidates.

Smets and Vreeken recently gave the S LIM algorithm [17] for directly mining K RIMP code tables from data. With S QS -S we adopt a strategy that resembles S LIM , by considering joins XY of X,Y  X  CT , and estimating the gain of adding XY to CT . Whereas S LIM iteratively searches for the best addition, for effi-ciency, S QS -S EARCH adopts a batch-wise strategy.

Lam et al. introduced G O K RIMP [9] for mining sets of serial episodes. As opposed to the MDL principle, they use fixed length codes, and do not punish gaps within patterns X  X y which their goal is essentially to cover the sequence with as few patterns as possible, which is different from our goal of finding patterns that succinctly summarise the data. Bathoorn et al. [2] also cover greedily, and do not consider gaps at all.
We implemented our algorithms in C++, and provide the source code for research purposes, together with the considered datasets, as well as the generator for the synthetic data. 2 As candidates for S C
ANDIDATES , we mined frequent serial episodes [10, 20] using disjoint minimal windows of maximal length 15 , with minimal support thresholds as low as feasible X  X .e. at the point where the number of patterns starts to explode. All experiments were executed single-threaded on a six-core Intel Xeon machine with 12GB of memory, running Linux.
 In our experiments we consider both synthetic and real data. Table 1 shows the base statistics per dataset, i.e. number of distinct events, number of sequences, total number of events per database, and the total encoded length by the most simple code table ST .
Synthetic Data. First, we consider the synthetic Indep , Plants-10 , and Plants-50 datasets. Each consists of a single sequence of 10 000 events over an alphabet of 1 000 . In the former, all events are independent, whereas in the latter two we planted resp. 10 and 50 patterns of 5 events 10 times each, with 10% probability of having a gap between consecutive events, but are independent otherwise. Table 1 shows the results given by S QS -C ANDIDATES and S S
EARCH . For the Indep dataset, while over 9 000 episodes occur at least 2 times, both methods correctly identify the data does not contain significant structure. Similar for Plants-10 both methods correctly return the 10 planted patterns. Plants-50 has a very high density of pattern symbols ( 25% ), and hence poses a harder chal-lenge. S QS -C ANDIDATES and S QS -S EARCH identify resp. 47 and 46 patterns exactly, the remainder consisting of fragments of cor-rect patterns. The imperfections are due to patterns being partly overwritten during the generation of the data.

Real Data. For the experiments on real data, in order to interpret the patterns, we consider text data. The events are the stemmed words from the text, with stop words removed. Addresses contains speeches of American presidents, JMLR consists of abstracts of papers from the Journal of Machine Learning Research website, whereas Moby contains the novel  X  X oby Dick X  by Herman Melville. Let us first consider the number of returned patterns, as shown in Table 1. We see that for all datasets small numbers of patterns are returned, in the order of 100 s, two orders of magnitude less than the number of frequent patterns S QS -C ANDIDATES considers. When we consider the gains in compressed size, i.e.  X  L = L ( D, ST )  X  L ( D, CT ) , we see these few patterns in fact describe a lot of structure of the data; recall that 1 bit of gain corresponds to an increase of factor 2 in likelihood. We note S QS -S EARCH outperforms S QS -C ANDIDATES , which is due to the former being able to consider candidates of lower support without suffering from the pattern explosion.

The largest  X  L is recorded for JMLR , with almost 30 k bits. This is not surprising, as the type of text, abstracts of machine learning papers, has a relatively small vocabulary X  X he use of which is quite structured, with many key phrases and combinations of words.
Table 2 depicts the top-10 most compressing patterns for JMLR , as found by S QS -S EARCH . Here, as  X  L we give the increase in bits the pattern would be removed from CT . Clearly, key machine learning concepts are identified, and importantly, the patterns are neither redundant, nor polluted with common words. In fact, in none of the CT s patterns incorrectly combine frequent events.
Further examples of patterns reported for JMLR include  X  non neg matrix factor  X ,  X  isotrop log concav distribut  X , and  X  reproduc[ing] kernel Hilbert space  X . For the presidential Addresses , we unsur-patterns. An example of a pattern with many gaps ( 5 . 2 gap events, on average), we find the rather current  X  economi[c] public expendi-http://adrem.ua.ac.be/sqs/ http://jmlr.csail.mit.edu/ 1. supp. vector machine 850 6. large scale 329 2. machine learning 646 7. nearest neighbor 322 3. state [of the] art 480 8. decision tree 293 4. data set 446 9. neural network 289 5. Bayesian network 374 10. cross validation 279 Figure 2: Addresses dataset, gain in compression. (left) vary-ing support thresholds for S QS -C ANDIDATES . (right) S C
ANDIDATES and S QS -S EARCH per accepted candidate. tur[e]  X . From the Moby Dick novel we find the main antagonist X  X  species,  X  sperm whale  X , and name,  X  moby dick  X , as well as the phrase  X  seven hundr[ed] seventy seventh  X  which occurs 6 times.
Next, we investigate our search strategies. First, in the left-hand plot of Fig. 2, for S QS -C ANDIDATES on the Moby data, we show the gain in compression for different support thresholds. It shows that lower thresholds, i.e richer candidate sets, allow for (much) better models X  X hough by the pattern explosion, mining candidates at low  X  can be infeasible.

Second, in the right-hand plot, we compare S QS -C ANDIDATES and S QS -S EARCH , showing the gain in bits over ST per candidate accepted into CT . It shows both search processes are efficient, considering patterns that strongly aid compression first. The slight dip of S QS -S EARCH around iteration 100 is due to its batch-wise search. At the expense of extra computation, an iterative search for the best estimated addition, like S LIM [17] may find better models.
In these experiments, using these support thresholds, mining the candidates took up to 4 minutes, after which S QS -C ANDIDATES took up to 15 minutes to order and filter the candidates. S S
EARCH resp. took 10 , 18 , and 91 minutes. As Moby has a large alphabet and is one long sequence, S QS -S EARCH has to consider many possible pattern co-occurrences.
The experiments show both S QS -C ANDIDATES and S QS -S EARCH return high-quality models. By using synthetic data we showed that S
QS reveals the true patterns without redundancy, while further not picking up on spurious structure. Analysis of the results on the text data experiments show that key phrases are identified X  combinations of words that may be interspersed with  X  X andom X  words in the data. Importantly, for all of the datasets, no noisy or redundant patterns are returned. As expected, the more structure a dataset exhibits, the better the attained compression.
With S QS -C ANDIDATES we allow the user the freedom to provide a set of candidate serial episodes. In general, lower thresholds correspond to more candidates, more candidates correspond to a larger search space, and hence better models. S QS -S EARCH other hand, besides an any-time algorithm, is parameter-free, as it generates and tests patterns that are estimated to improve the score. S
Both algorithms are fast, our prototype implementations taking only few minutes on the data here considered. The algorithms have many opportunities for parallelisation: candidates can be estimated or evaluated individually, as can the scanning for minimal windows.
MDL does not provide a free lunch. First of all, although highly desirable, it is not trivial to bound the score. While for Kolmogorov complexity we know this is incomputable, for our models we have no proof one way or another. Furthermore, although MDL gives a principled way to construct an encoding, this involves many choices that determine what structure is rewarded. As such, we do not claim our encoding is suited for all goals, nor that it cannot be improved.
Future work includes the extension of S QS for parallel and gen-eral episodes X  X hich surpass serial episodes in expressiveness. Al-though seemingly opposed to MDL (why describe the same thing twice?) allowing patterns to overlap may provide more succinct summarisations. Last, but not least, we are interested in applying the S QS code tables for clustering and anomaly detection.
In this paper we employed the MDL principle to mine sets of sequential patterns that summarise the data well. In particular, we formalised how to encode sequential data using set of patterns, and use the encoded length as a quality measure. As search strategy for good models, we adopt two approaches. The first algorithm, S
QS -C ANDIDATES , selects a good pattern set from a large candidate set, while S QS -S EARCH is a parameter-free any-time algorithm that discovers good pattern sets directly from the data. Experimenta-tion on synthetic and real data showed both methods to efficiently discover small, non-redundant sets of informative patterns. And that X  X  the long and the short of it.
 Nikolaj Tatti and Jilles Vreeken are supported by Post-Doctoral Fellowships of the Research Foundation  X  Flanders ( FWO ). [1] A. Achar, S. Laxman, R. Viswanathan, and P. S. Sastry. [2] R. Bathoorn, A. Koopman, and A. Siebes. Reducing the [3] T. M. Cover and J. A. Thomas. Elements of Information [4] S. Dzeroski, B. Goethals, and P. Panov, editors. Inductive [5] P. Gr X nwald. The Minimum Description Length Principle . [6] R. Gwadera, M. J. Atallah, and W. Szpankowski. Markov [7] R. Gwadera, M. J. Atallah, and W. Szpankowski. Reliable [8] J. Kiernan and E. Terzi. EventSummarizer: a tool for [9] H. T. Lam, F. M X rchen, D. Fradkin, and T. Calders. Mining [10] S. Laxman, P. S. Sastry, and K. P. Unnikrishnan. A fast [11] M. Li and P. Vit X nyi. An Introduction to Kolmogorov [12] H. Mannila and C. Meek. Global partial orders from [13] H. Mannila, H. Toivonen, and A. I. Verkamo. Discovery of [14] J. Pei, H. Wang, J. Liu, K. Wang, J. Wang, and P. S. Yu. [15] J. Rissanen. Modeling by shortest data description. Annals [16] D. Salomon and G. Motta. Handbook of Data Compression . [17] K. Smets and J. Vreeken. S LIM : Directly mining descriptive [18] N. Tatti. Significance of episodes based on minimal windows. [19] N. Tatti and B. Cule. Mining closed episodes with [20] N. Tatti and B. Cule. Mining closed strict episodes. Data Min. [21] N. Vereshchagin and P. Vitanyi. Kolmogorov X  X  structure [22] J. Vreeken and A. Siebes. Filling in the blanks: Krimp [23] J. Vreeken, M. van Leeuwen, and A. Siebes. K RIMP : Mining [24] J. Wang and J. Han. Bide: Efficient mining of frequent closed
