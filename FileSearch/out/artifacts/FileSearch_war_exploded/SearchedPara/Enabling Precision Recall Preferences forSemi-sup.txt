 Semi-supervised learning is an essential approach to classifi-cation when the available labeled data is insufficient and we need to also make use of unlabeled data in the learning pro-cess. Numerous research efforts have focused on designing algorithms to improve the F 1 score, but have any mecha-nism to control precision or recall individually. However, many applications have precision/recall preferences. For in-stance, an email spam classifier requires a precision of 0.9 to mitigate the false dismissal of useful emails. In this pa-per, we propose a method that allows to specify a preci-sion/recall preference while maximising the F 1 score. Our key idea is that we divide the semi-supervised learning pro-cess into multiple rounds of supervised learning, and the classifier learned at each round is calibrated using a sub-set of the labeled dataset before we use it on the unlabeled dataset for enlarging the training dataset. Our idea is appli-cable to a number of learning models such as Support Vector Machines (SVMs), Bayesian networks and neural networks. We focus our research and the implementation of our idea on SVMs. We conduct extensive experiments to validate the effectiveness of our method. The experimental results show that our method can train classifiers with a precision/recall preference, while the popular semi-supervised SVM training algorithm (which we use as the baseline) cannot. When we specify the precision preference and the recall preference to be the same, which indicates to maximise the F 1 score only as the baseline does, our method achieves better or similar F 1 scores to the baseline. An additional advantage of our method is that it converges much faster than the baseline. I.2 [ Artificial Intelligence ]: Miscellaneous Support Vector Machines, Precision, Recall
Preferences our method the baseline r p F Sem i-supervised SVM training, Precision, Recall, Preference
Semi-supervised learning is an essential approach to clas-sification when the available labeled data is insufficient and we need to also make use of unlabeled data in the learning process. Unlabeled data are shown to be useful for train-ing better classifiers in many applications [1, 2]. Numer-ous research efforts have focused on designing algorithms to improve the F 1 score in semi-supervised classification, but have any mechanism to control precision or recall individu-ally. However, many applications have high precision/recall preferences. An email server needs a classifier to filter out unwanted or harmful content known as spam [3]. The email spam classifier requires a precision of 0.9 (i.e., a precision preference of 0.9) to mitigate the dismissal of useful emails. A classifier for cancer detection has a recall preference of 0.95 trying to avoid missing patients with cancer [4]. In this paper, we propose a semi-supervised learning method that allows to specify a precision/recall preference while max-imising the F 1 score. Our key idea is that we divide the semi-supervised learning process into multiple rounds of su-pervised learning, and the classifier learned at each round is calibrated using a subset of the labeled dataset before we use it on the unlabeled dataset for enlarging the train-ing dataset of the next round. Our idea is applicable to a number of learning models such as Support Vector Machines (SVMs) [5], Bayesian networks [6] and neural networks [7]. We focus our research and the implementation of our idea on SVMs. Table 1 gives an example of what our method can achieve in comparison to the popular semi-supervised SVM in this paper; r g represents the recall preference, and all the other settings such as kernel parameters of SVMs are the same; the values highlighted by bold font are the recall of our final classifiers. As can be seen from the table, our method can train classifiers with different recall preferences while the baseline gives precision and recall from the algo-rit hm that does not have mechanism to control the precision and recall individually and only optimises the F 1 score.
The key steps of our method are as follows. (i) We ran-domly divide the labeled dataset into two subsets L and B . Let T i denote the training dataset in the i th round; L is used as the initial training dataset, i.e., T 1 . (ii) In the i T is used for training an SVM classifier H and the dataset B is used for measuring the precision and recall of H . (iii) If the precision (or recall) preference is satisfied, we adjust the decision value of H to improve the F 1 score. Otherwise, we increase (or decrease) the decision value of H to improve the precision (or recall). (iv) Then, the classifier H with the adjusted decision value is used to classify the unlabeled dataset to form a dataset S of highly confident instances. The result of the union of the dataset S and the dataset L is used as the training dataset, T i +1 , in the next round. The steps (ii) to (iv) are repeated until the termination condition is reached.

We conduct extensive experiments to validate the effec-tiveness of our method. When we specify a preference, the other preference is set to be 0 by default. The experimen-tal results show that our method can train classifiers with a algorithm (i.e., Transductive SVM [8] and TSVM for short) does not have any mechanism to allow for preferences. When we specify the precision preference and the recall preference to be the same for our method, which indicates to maximise the F 1 score only as TSVM does, our method achieves better F 1 scores than or similar F 1 scores to TSVM.

An additional advantage of our method is that it converges faster than TSVM. This is because TSVM trains the SVM classifier in the way similar to a combinatorial optimisa-tion approach which requires more training iterations, while training process into a small number of the supervised SVM training processes. Our experimental results show that our method converges at least several times faster than TSVM and in some cases the improvement factor is more than one order of magnitude.

The remainder of the paper is organised as follows. We discuss related work in Section 2, and present preliminaries in Section 3. Following that, we elaborate our method in Section 4 and provide our experimental results in Section 5. Finally, we conclude the paper in Section 6. Zhu [9] gives a nice survey on semi-supervised learning. Our study falls into the category called  X  X elf-learning X  of the semi-supervised learning research field. We focus on semi-supervised SVM training. In what follows, we review the and partially supervised SVM training, respectively.
Supervised SVM training: Supervised SVM train-ing only uses labeled instances, which is a foundation for ing adopted supervised SVM training algorithms. Osuna et al. [10] proposed a supervised SVM training algorithm based on decomposition. This approach decomposes the training instances into groups. In each training iteration, one group of the instances is used to update the currently found hy-perplane. To improve Osuna et al. X  X  algorithm, Platt [11] proposed the Sequential Minimal Optimisation (SMO) algo-rithm. SMO also uses decomposition, but in each training iteration, only two training instances are used to update the currently found hyperplane. Other SVM training algo-rithms, such as Joachims X  algorithm [12] and  X  X egasos X  [13], focus on improving the training efficiency for linear SVMs. As SMO is space efficient and fast, and can train both lin-ear and non-linear SVMs, we use it as the supervised SVM training algorithm in our method.

Semi-supervised SVM training: Bennett et al. [14] find a hyperplane that separates the two classes of instances in the labeled dataset with the maximum margin and mean-while, to minimise the number of unlabeled instances falling training problem is NP-hard [9], and hence some existing cess to multiple supervised SVM training processes. These S 3 VM training algorithms focus on maximising the F 1 score but cannot train SVMs with a precision/recall preference. Among these algorithms, Transductive SVM (TSVM) [8] is can train both linear and non-linear SVMs. We use TSVM as our baseline algorithm, and will discuss it in Section 3.
Partially supervised SVM training: Another cate-SVM training. Partially supervised SVM training does not require labeled negative instances.  X  X EBL X  [17] is a par-tially supervised SVM training algorithm based on positive and unlabeled instances. The algorithm first identifies some negative instances from the unlabeled dataset based on some features of the positive instances. Then those identified neg-ative instances are put together with the positive instances to train an SVM classifier using the supervised SVM train-ing algorithm. The trained SVM classifier is used to iden-tify more negative instances from the unlabeled dataset for enlarging the training dataset of the next round of the su-pervised SVM training. These steps are repeated until no more negative instances are identified. To achieve faster convergence, Fung et al. [18] proposed an algorithm similar to PEBL. Their algorithm not only identifies more negative instances but also identifies more positive instances for en-larging the training dataset of the next round of the super-vised SVM training. Liu et al [19] gave a more comprehen-sive study on partially supervised learning. These studies have different settings from ours that has positive, negative and unlabeled instances. More importantly, we propose a In this section, we first present the details of supervised we provide our problem definition.
A labeled training instance x i is attached with an integer y  X  { +1 ,  X  1 } as its label. A positive (negative) instance is a training instance with the label of +1 (  X  1). Given a set X of n training instances, the goal of training SVMs is to find a hyperplane that separates the positive from the negative instances in X wit h the maximum margin, and meanwhile, with the minimum misclassification error on the training instances. The training is equivalent to solving the following optimisation problem: where w is the normal vector of the hyperplane, C is the penalty parameter,  X  is the slack variable vector to toler-ant some training instances falling in the wrong side of the hyperplane, and b is the bias of the hyperplane.

To handle the non-linearly separable data, SVMs uses a mapping function to map the training instances from the original data space to a higher dimensional data space where the data may become linearly separable. The optimisation problem 1 can be rewritten to a dual form [20] where map-ping functions can be replaced by kernel functions [21] which make the mapping easier. The optimisation problem in the dual form is shown as follows. where F (  X  ) is the objective function;  X   X  R n is a weight vector, where  X  i denotes the weight of the training instance x ; Q is a symmetric matrix, where Q = [ Q ij ], Q ij = y y j K ( x i , x j ) and K ( x i , x j ) is a kernel value computed from a kernel function (e.g., the Gaussian kernel function, K ( x i , x j ) = exp { X   X  || x i  X  x j || 2 } ).
The goal of the training translates to finding a weight vector  X  that maximises the value of the objective func-tion F (  X  ). The training instances with their weights greater than 0 are called support vectors . In our method, we use a popular training algorithm, the Sequential Minimal Opti-misation (SMO) algorithm as discussed in Section 2. SMO iteratively improves the weight vector until the optimal con-ditions (i.e., the Karush-Kuhn-Tucker conditions [22]) are met. For more details of SMO, please consult the original paper of SMO [11].
A labeled instance x i is attached with an integer y i  X  { +1 ,  X  1 } as its label, while the label y  X  j  X  { +1 ,  X  1 } of an unlabeled instance x  X  j is unknown. Given n labeled instances and k unlabeled instances, the S 3 VM training problem can be formulated as follows. where C and C  X  are the penalty parameters of the labeled instances and the unlabeled instances, respectively;  X  and for the labeled and unlabeled instances, respectively. TSVM [8]. Initially, the TSVM algorithm trains an SVM using only the labeled instances. Then the trained SVM classifier is used to classify the unlabeled instances, and to assign each unlabeled instance with a label that we call  X  X oft label X  in this paper. Following that, TSVM switches two of the soft-labeled instances and retrains the SVM classifier with all the training instances such that the value of the objective function is reduced. The label switching process and the training process are repeated until the value of the object function is minimised.
 similar to a combinatorial optimisation approach, we break SVM training processes. The training dataset (except which of the initial training) in TSVM is always all the labeled instances and all the unlabeled instances. In comparison, the training dataset in our method is a subset of the labeled instances and a subset of the unlabeled instances, which makes each training iteration faster. We also design our algorithm to train an SVM classifier with a precision/recall preference. We will detail our method in Section 4. process, the trained SVM classifier can be used to classify an instance x l using the following equations. where x sv i is the i th support vector of the SVM classifier, m is the number of the support vectors, b is the bias of the hyperplane, y l is the predicted label of x l , and  X  is the decision value which equals to 0 by default.
Given a labeled dataset M , an unlabeled dataset U and a testing dataset G , S 3 VM training with a precision (or recall) preference p g (or r g ) is to train a classifier H using M and U such that H classifies G with precision p (or recall r ) meeting one of the following conditions: As our method can train SVM classifiers with precision/recall preferences, we call our method P reference enabled semi-supervised SVM training ( PSVM for short). Our key idea is that we divide the semi-supervised learning process into training measuring adjusting classification S multiple rounds of supervised learning, and the classifier trained at each round is calibrated using a subset of the labeled dataset before we use it on the unlabeled dataset for enlarging the training dataset of the next round.
The key steps of our method are as follows. (i) We ran-domly divide the labeled dataset M ( X  X  X  for m anually la-beled dataset) into two datasets, denoted by L and B ( X  X  X  for l abeled training dataset and X  X  X  X or cali B ration dataset), respectively. Let T i ( X  X  X  for t raining) denote the training dataset in the i th round; L is used as the initial training dataset, i.e., T 1 . (ii) In the i th round, the dataset T for training an SVM classifier H using the supervised SVM training algorithm. The calibration dataset B is used for measuring the precision and recall of the classifier H . (iii) If the precision (or recall) preference is satisfied, we adjust the decision value (i.e.,  X  in Equation 4) of H to improve the F 1 score. Otherwise, we increase (or decrease) the decision value to improve the precision (or recall). (iv) The classifier H with the adjusted decision value is used for classifying the instances in the unlabeled dataset U ( X  X  X  for u nlabeled). We refer to the unlabeled instances classified by H as the  X  X oft-labeled X  instances. Those soft-labeled instances form a dataset denoted by S ( X  X  X  for s oft-labeled). The result of the union of the dataset S and the labeled training dataset L forms the training dataset, T i +1 , for the next round, and the steps (ii) to (iv) are repeated until the termination con-dition is achieved. We discuss the termination condition in detail in Section 4.5.1.

A round in PSVM can be summarised in four phases: training, measuring, adjusting and classification. Figure 1 gives an overview of PSVM. Adopting our idea to other mod-els (e.g., Bayesian networks and neural networks) is straight-forward except that the adjusting phase needs some special cares. In what follows, we first describe the necessity of a separated calibration dataset. Then, we give the details of the training, measuring, adjusting and classification phases.
As we can see from Figure 1, there are three purposes of using the labeled dataset M at each round: In our method, we randomly divide the labeled dataset M into two subsets (i.e., the labeled training dataset L and cali-bration dataset B ). We call this approach X  PART  X , because L and B are part of M . Alternatively, we may use the whole labeled dataset M for training, measuring and adjusting without division (i.e., L = B = M ). We call this approach  X  FULL  X , since both L and B contain the whole dataset M . The advantage of the PART approach is that the calibration dataset B is independent of the training dataset for training the SVM classifier. Hence, the measuring is a better estima-tion of the effectiveness of the SVM classifier on the unseen data, and the adjusting is more effective than that using the FULL approach. In comparison, using the training dataset to measure the classifier itself (i.e., L = B = M ) can lead to the over-fitting problem. We conduct experiments to val-idate the effectiveness of PART in comparison to FULL in Section 5. The training phase in Figure 1 is a standard supervised SVM training process. The training dataset T for the su-pervised SVM training is the union of the labeled training dataset L and the soft-labeled dataset S (cf. Figure 1). Note that the training dataset for training the first SVM classifier is L , since the dataset S is empty initially.
 We use SMO as the supervised SVM training algorithm. We implement the algorithm using the GPU ( X  X PU X  for the Graphics Processing Unit [23]) based on Catanzaro et al. X  X  GPU SVM algorithm [24]. The implementation details of the GPU-based algorithm are out of the scope of this paper and hence we will not discuss it any further.
As shown in Figure 1, after the training phase, we mea-sure the precision, recall and F 1 score of the trained SVM classifier using the calibration dataset B . The measuring phase is a standard classification process on the dataset B . If the computed value v of an instance using Equation 4 is larger than 0 (i.e.,  X  = 0), then the instance is assigned a label of +1. Otherwise the instance is assigned a label of  X  1. After the classification on all the instances on the dataset B , we compare those assigned labels with the true labels of the instances. Then, we can compute the precision p , recall r and F 1 score using the following equations. w here tp is the number of true positive instances, fp is the number of false positive instances and fn is the number of false negative instances.

The measuring phase has two purposes. The first purpose is to check if the SVM classifier satisfies the precision (or recall) preference. Specifically, we compare the precision p (or recall r ) with the precision (or recall) preference p r ), and check if the precision p (or recall r ) satisfies the precision (or recall) preference, i.e., p  X  p g (or r  X  r
The second purpose of the measuring phase is to identify the best SVM classifier H best for the final result of the S 3 VM
Figure 2: Adjusting  X  to satisfy the preference training process. There are two cases when we identify the best SVM classifier H best .
After the measuring phase, we can decide to improve the precision (or recall) or to improve the F 1 score (i.e., the first purpose of the measuring phase). When we adjust the decision value  X  in Equation 4, we have two cases as follows:
Next, we first describe the technique to adjust the decision value for improving precision (or recall) to satisfy the preci-sion (or recall) preference. Then, we explain the approach to adjusting the decision value for improving the F 1 score. Finally, we discuss adjusting the decision value for identi-fying the negative instances from the unlabeled dataset U more confidently.
If the trained SVM classifier at the current round does not satisfy the precision (or recall) preference, we can increase (or decrease) the decision value  X  of the SVM classifier to improve its precision (or recall). Note that when we have a precision (or recall) preference, the recall (or precision) preference r g (or p g ) equals to 0.

Without loss of generality, we analyse the approach for increasing the precision p . Let us first rewrite Equation 5 into the following equation: p = 1 sion p , we need to reduce fp false positive and that of true positive). We can increase the decision value  X  of Equation 4 to reduce fp a positive instance is likely to have a larger v value while a negative instance is likely to have a smaller v value (i.e., the property of SVMs). As a result, by increasing  X  , the num-ber of false positive instances decreases more significantly than that of true positive instances. Hence, fp and precision p increases. Similarly, we can decrease  X  to increase recall r .

In what follows, we describe the approach for computing the new decision value  X  + . Figure 2 shows an example of the classification results on the calibration dataset B , where a  X  X  X  represents a positive instance and a  X - X  represents a negative instance in the calibration dataset B . The dashed line indicates the decision value  X  which equals to 0. The position of an instance indicates the v value of the instance. As can be seen from the figure, 14 positive instances have v values greater than  X  (i.e., true positive), and 7 negative instances have v values greater than  X  (i.e., false positive). So, the precision is p = 14 is r = 14 we would like to train an SVM classifier with a precision preference of 0.7 (i.e., p g = 0 . 7). In this example, we increase the decision value from 0 to 0.2 (i.e.,  X  + = 0 . 2) to get the precision of 13 13 + 4 = 0 . 76 at the cost of decreasing recall to 13 13 + 7
Instead of having a precision preference, suppose we would like to train an SVM classifier with a recall preference of 0.9 (i.e., r g = 0 . 9). We decrease the decision value from 0 to -0.4 (i.e.,  X  + =  X  0 . 4) to get the recall of 15 15 + 1 the cost of decreasing precision to 19
After we obtain the new decision value  X  + , we expect that the classification result of the SVM classifier using the de-cision value  X  + on the calibration dataset B satisfies the precision (or recall) preference.
There are two scenarios that we need to improve the F 1 score. First, the trained classifier satisfies the precision (or recall) preference, but we want to improve its F 1 score. Sec-(a ) Original boundary ond, we would like to train a classifier with as high F 1 as possible without having a specific precision (or recall) preference. These two scenarios are essentially the same, i.e., we have the precision preference p g and the recall pref-erence r g to be the same (e.g., both to be 1) in the adjusting phase. In Figure 3, the decision value for satisfying the pre-cision preference p g is the rightmost dashed line where  X  equals to 1.0, while the decision value for satisfying the recall preference r g is the leftmost dashed line where  X  + equals to -0.6. As we cannot find a decision value which satisfies both of the preferences, we adjust the decision value  X  to a value that improves the F 1 score. The approach that we use to improve the F 1 score is to reduce the gap between precision p and recall r , i.e., the value of | p  X  r | .

Theorem 1. Given p , r and p + r = z where z is a con-stant, the F 1 score is maximised when p equals to r . The proof is straightforward and omitted. Although z may gives a direction of improving the F 1 score. Intuitively, while improving the F 1 score, our method treats the precision and recall equally and minimises both the number of false posi-tive instances and the number of false negative instances. In the example in Figure 3, we improve the precision since the precision is smaller than the recall (0.67 v.s. 0.7). To han-dle this case, our method temporarily specify the precision preference to be 0.7 (i.e., the recall of the trained classifier) and compute the decision value  X  + using the same idea we discussed in Section 4.4.1.
So far, we only consider the decision value  X  + for deciding the positive instances. In other words, v values of the pos-itive instances should be larger than  X  + . All the instances with v values no larger than  X  + are identified as negative instances. It is also important to identify the negative in-stances more confidently. We use a decision value  X   X  to identify negative instances. In our method,  X   X  equals to the average v value of all the negative instances in the calibra-tion dataset B . Please note that the decision value  X   X  for negative instances has no effect on adjusting the precision and recall of the SVM classifiers.

Figure 4 shows an SVM classifier before and after the decision value adjusting. The SVM with the decision values  X  + and  X   X  is applied to classify the instances in the unlabeled dataset U . In what follows, we discuss the classification phase of PSVM on the unlabeled dataset.
We use the SVM classifier with the adjusted decision val-ues to classify an unlabeled instance x j to be positive, neg-ative or uncertain. The label y j of the instance x j can be predicted by the following equation.
We refer to these labeled instances by the SVM classifiers as soft-labeled instances and they form a dataset denoted by S . The dataset S is used as part of the training dataset describe two approaches to forming the S dataset. training process outputs soft-labeled instances to S . We provide the following two approaches for forming the soft-labeled dataset S . (i) The soft-labeled dataset S is emp-tied after the training phase at each round and accepts the soft-labeled instances from the classification phase. (ii) The soft-labeled dataset S unions itself with the soft-labeled in-stances from the classification phase at each round, and the instances in S are removed from the unlabeled dataset U forming a new set of unlabeled data. We call the first ap-proach  X  X econstruction X  X enoted by REC , since each round S is reconstructed. We refer to the second approach  X  X ncre-ment X  denoted by INC , because each round S is increased based on the soft-labeled dataset S of the previous round. Termination: A termination condition for both INC and REC is that the trained SVM classifier (denoted by H c ) at the current round, is identical to any one of the SVM classi-fiers trained in the previous rounds, denoted by H p . Under this condition, for the INC approach, the classification phase cannot identify any more instances as positive or negative from the unlabeled dataset, since all the possible positive or negative instances have been identified by H p and already stored in S . For the REC approach, the classification phase using H c outputs the same soft-labeled dataset as that out-put by H p .

Compared with the REC approach, the INC approach has the following advantages. Firstly, the INC approach has an additional termination condition that the classification phase cannot identify any new soft-labeled instances. This cess. Secondly, those soft-labeled instances are of highly confident and tend to be labeled with the same labels as the previous rounds. Hence, we do not need to classify them again and can reduce the computation cost. One drawback of the INC approach is the unrecoverable mistake. For in-stance, an unlabeled instance is wrongly labeled as a positive instance by an SVM classifier. In the INC approach, that instance is always used as a positive instance in the rest of rect it. In contrast, that wrongly labeled instance may be corrected by other SVMs classifier if we use the REC ap-proach. Due to the SVM X  X  good property of error-tolerance, some wrongly labeled instances actually are allowed. Fur-thermore, the instances in the soft-labeled dataset S are of highly confident and tend to be labeled with the same labels in different rounds.
In summary, we recommend the INC approach which makes the S 3 VM training converge faster. We conduct experiments to validate our claim in our experimental study.
 Algorithm 1 : S 3 VM training in PSVM
In w hat follows, we describe the overall S 3 VM training al-gorithm of PSVM. Without loss of generality, we assume the Gaussian kernel function is used in the training algorithm and using the INC approach for forming the soft-labeled is summarised in Algorithm 1. Initially, the soft-labeled dataset S (line 3) is empty. In the supervised SVM training, the training dataset includes the soft-labeled dataset S and the labeled training dataset L (line 4). After an SVM H is trained, we measure the precision, recall and F 1 score of the trained SVM H , compare it with the identified best SVM H best , and keep the better SVM to H best (line 5). Based on the precision (or recall) preference p g (or r g ), we compute two decision values  X  + and  X   X  (i.e., adjusting the decision value) of the trained SVM using the calibration dataset B (line 6). For each unlabeled instance that is not in the soft-labeled dataset S , we compute the value v using Equation 4, and use Equation 8 to classify the instance (lines 7 to 13). The above steps are repeated until no instance in the unla-beled dataset is identified as a positive or negative instance by the SVM classifier with the adjusted decision value.
The number of the supervised SVM training processes is determined by the classification phase (line 9). In the worse case, there is only one instance added to the soft-labeled dataset S (lines 7 to 13) at each round of the S 3 VM training. Hence, the number of rounds at the worse case equals to the number of unlabeled instances. In the average case, many of the unlabeled instances are added to the soft-labeled dataset at each round, and the number of rounds is much smaller than the number of unlabeled instances.

As discussed in Section 4.5.1, the REC approach can be used to form the soft-labeled dataset S . The implementation for this method is straightforward. We just need to add a line right after line 4 to empty S + , S  X  , S  X  + , S  X  The termination condition is changed to that the currently (a) Optimal hyperplane (b) Initial hyperplane (c) Intermediate hyperplane (d) Final hyperplane trained SVM H is identical to any one of the previous SVM classifiers. To show the intuition of our method, we give an example. The optimal hyperplane for the given dataset is shown in Figure 5a. Figure 5b shows the hyperplane found only us-ing the labeled training dataset L . After adding some soft-labeled instances (denoted by circles containing  X  X  X  or  X - X ), the hyperplane is improved to the one shown in Figure 5c. Figure 5d approaches close to the optimal hyperplane shown in Figure 5a. In this section, we empirically evaluate our PSVM method. method (i.e., Transductive SVM [8]), denoted by SSVM and TSVM respectively, are served as our baselines. We im-plemented PSVM and SSVM using GPUs in CUDA-C [23], based on Catanzaro et al. X  X  GPU SVM implementation [24]. TSVM is written in C and downloaded from the SVM light experiments. The cardinality of the datasets varies from 17,766 to 60,000 and the dimensionality of them varies from 22 to 780. All the SVM implementations use the Guassian kernel function and the same parameters which are selected using the grid search [25]. The information of the datasets and the parameters are listed in Table 2. We randomly sam-ple 10% of each dataset to form the testing dataset to vali-date the effectiveness of the finally obtained classifier. The size of the labeled dataset M is 10% of the whole dataset by default, and the remaining instances form the unlabeled dataset U . In PSVM, a half of the labeled dataset M is used as the labeled training dataset L , and the other half is used as the calibration dataset B . SSVM uses only the labeled htt p://svmlight.joachims.org/ http://www.csie.ntu.edu.tw/~cjlin/libsvm/ precision preference p g 0.55 0.6 0.65 0.7 dataset M as i ts training dataset, and TSVM uses M as the labeled training dataset and U as the unlabeled training dataset. All the experiments are conducted on a desktop computer running Linux with a Xeon E5-2643 CPU, 32GB main memory and a Tesla C2075 GPU.

In what follows, we first show that PSVM can train classi-fiers with precision/recall preferences. Then, we investigate the effectiveness of PSVM on maximising the F 1 score and the efficiency of PSVM. Finally, we study the effect of each proposed technique.
We use the Adult dataset as the representative to demon-strate the training with precision/recall preferences of PSVM. The experiments on the other datasets showed similar results which are omitted.

To validate the effectiveness of training classifiers with precision preferences, we specify the precision preference p to be 0.55, 0.6, 0.65 and 0.7, respectively, and specify r be 0. As can be seen from Table 3, the precision preference can be satisfied when the precision preference is between 0.55 and 0.65. While the precision preference is too high (i.e., at 0.7), PSVM cannot satisfy the preference but tries its best to satisfy the preference. In fact, when the precision preference is specified to be 1, the preference is unlikely to be satisfied for the chosen hyper-parameters C and  X  . However, specifying precision preference to be 1 indicates to train a classifier with the maximum precision. This is useful when we would like to train a classifier with as high precision as possible.

Next, we show the results of training classifiers with recall preferences. Table 4 shows the results on specifying the recall preference r g to be 0.5, 0.6, 0.7 and 0.8, respectively, and on specifying p g to be 0. As can be seen from the table, when the recall preference is between 0.5 and 0.7, PSVM can train classifiers satisfying the preferences. When the recall preference is specified to be 0.8, PSVM cannot satisfy the preference but it tries its best to train a classifier with recall of 0.77.

Note that SSVM and TSVM do not have any mechanism to control the precision/recall individually, and give only the precision and recall of 0.66 and 0.41, and 0.63 and 0.59, respectively.
To validate the effectiveness of PSVM on maximising F 1 only, we specify both the precision and recall preferences recall preference r g 0.5 0.6 0.7 0.8 to be 1 (i.e., both of the preferences are the same). We com pare the F 1 scores of the three methods using the four datasets. The results in Table 5 are the average values of the results obtained by repeating the experiments 20 times. The variance (e.g.,  X  0 . 01) is because of the random sam-pling for constructing the datasets in the experiments. As can be seen from Table 5, PSVM significantly outperforms SSVM by over 10% of improvement on the F 1 score. This demonstrates the usefulness of using the unlabeled data for training SVM classifiers. Compared with TSVM, PSVM achieves better or similar F 1 scores. This indicates that
We also conducted a set of experiments to show the effi-TSVM. Figure 6 gives the result on the efficiency compari-son. As can be seen from the figure, PSVM constantly out-performs TSVM by three orders of magnitude. In the exper-iment, we observed that TSVM took more than 20 hours for the MNIST dataset while PSVM only took around 8 min-utes to complete the whole semi-supervised training process and both methods trained a similar classifier ( F 1 score: 0.93 v.s. 0.94). This demonstrates that PSVM is highly efficient and is a promising semi-supervised training method.
As the speedup factor is achieved by both the computa-tion power of the GPU and the fast convergence of PSVM, we show the experimental results to inference the speedup contributed from the fast convergence. Table 6 shows the total number of the training iterations in the whole semi-supervised training. In all the datasets tested, the total number of the training iterations of PSVM is several times smaller than that of TSVM. On the IJCNN1 dataset, the total number of training iterations of PSVM is 10 times less than that of TSVM. Note that a training iteration of Table 6: The total number of training iterations PSVM is faster than a training iteration in TSVM because of the smaller training dataset, as we discussed in Section 3.2. Catanzaro et al. [24] reported that the GPU implementation of SVMs is one to two orders of magnitude faster than the CPU implementation of SVMs. Therefore, we can infer that the reduction of the total number of the training iterations (i.e., faster convergence) contributes to around one order of magnitude speedup.
In what follows, we first investigate the effects of the size of the labeled dataset M . Then we study the effect of using a calibration dataset B which is independent of the train-ing dataset. Following that, we compare the results of two techniques for forming the soft-labeled dataset S . Finally, we show the experimental results on the effect of adjusting the decision values.
This set of experiments is to demonstrate the effect of the size of the labeled dataset M . We vary the size of the labeled dataset M by varying the percentage of the whole dataset that is used to form M . The percentage varies from 5% to 15%. The experimental results on TSVM are similar to the results on PSVM and hence are omitted. We use SSVM as the base and compute the improvement indicator using the following equation: In the experiments on the four datasets, we noticed that the F 1 score of SSVM increases by around 10% while the F 1 score of PSVM increases by less than 5%. This indicates that the size of the labeled dataset M has a more significant impact on the F 1 score of SSVM while PSVM is relatively insensitive to the size of the labeled dataset, thanks to the usage of the unlabeled dataset. Hence, the improvement indicator imp is expected to decrease as the size of M in-creases. The phenomenon is shown in Figure 7.
As discussed in Section 4.1, we can use the whole labeled dataset M for the training, measuring and adjusting (i.e., the FULL approach), instead of taking a half of the labeled dat aset to form the calibration dataset B (i.e., the PART approach). We denote our method that uses the PART ap-proach by PSVM-part and the method that uses the FULL approach by PSVM-full. Table 7 shows the results on F 1 . As we can see from the table, PSVM-part significantly out-performs PSVM-full on the MNIST and Protein datasets, and PSVM-part has similar F 1 scores to PSVM-full on the Adult and IJCNN1 datasets. PSVM-part demonstrates the robustness among the datasets tested, while PSVM-full suf-fers from the over-fitting problem on the MNIST and Protein datasets. Although PSVM-full has a larger labeled training dataset, our experiment on the effect of the dataset size of M show that PSVM is insensitive to the size of the labeled dataset. Therefore, we recommend the PSVM-part method which is more robust.
As discussed in Section 4.5.1, we can incrementally enlarge the soft-labeled dataset S (i.e., the INC approach). We can also empty S after the training phase and reconstruct the dataset at each round (i.e., the REC approach). We denote our method using the INC approach by PSVM-inc, and de-note that using the REC approach by PSVM-rec. As can be seen from Table 8, the F 1 scores of PSVM-inc and PSVM-rec are almost the same. In the experiment, we noticed that the PSVM-inc is several times faster than PSVM-rec. This is because the convergence is slower for PSVM-rec than PSVM-inc which has one more termination condition as dis-cussed in Section 4.5.1. We recommend using the PSVM-inc method which is more efficient but retains similar effective-ness of the classifiers.
Here, we show experimental results to validate the effect of adjusting the decision values. We compare our method with decision value adjusting with two methods that do not adjust the decision value  X  . Specifically, dataset PSVM PSVM-part-NA PSVM-full-NA IJCNN1 0.75 0.68 0.66 MNIST 0.94 0.78 0.83 Protein 0.51 0.25 0.24 All these methods use the INC approach to forming the soft-lab eled dataset S as INC is efficient. PSVM-part-NA and PSVM-full-NA do not adjust the decision value and hence the decision value  X  equals to 0.

As can be seen from Table 9, PSVM significantly outper-forms the methods without adjusting the decision value on all the datasets tested. This demonstrates the importance by adjusting the decision value we only add highly confident instances to the soft-labeled dataset S .
Semi-supervised learning is an essential approach to clas-sification when the available labeled data is insufficient and we need to also make use of unlabeled data in the learn-ing process. Numerous research efforts have focused on de-signing algorithms to improve the F 1 score, but have any mechanism to control precision or recall individually. In this paper, we proposed a method called PSVM that allows to specify a precision/recall preference while maximising the F 1 score. Our key idea is that we divide the semi-supervised learning process into multiple rounds of supervised learn-ing, and the classifier learned at each round is calibrated using a subset of the labeled dataset before we use it on the unlabeled dataset for enlarging the training dataset. Our idea is applicable to a number of learning models such as SVMs, Bayesian networks and neural networks. We focused our research and the implementation of our idea on SVMs. We conducted extensive experiments to validate the perfor-mance of our method. The experimental results showed that our method can train classifiers with a precision/recall pref-When we specified both the precision preference and the re-call preference to be the same, which indicates to maximise the F 1 score only as the baseline does, our method achieved better or similar F 1 scores to the baseline. An additional advantage of our method is one order of magnitude faster than TSVM.
 This work is supported in part by the Australian Research Council (ARC) Discovery Project DP130104587. Dr. Rui Zhang is supported by the ARC Future Fellowships Project FT120100832. Zeyi Wen is supported by the Commonwealth Scientific and Industrial Research Organisation (CSIRO).
