 Test collections play an important role in adhoc and diver-sity retrieval evaluation. Constructing a test collection for adhoc evaluation involves (1) selecting a set of queries to be judged, (2) selecting an intent (topic) description for that query, and (3) obtaining relevance judgments with respect to the specific intent description for that particular query. Recent work showed that the selection of intents play an important role in the relative performance of retrieval sys-tems for diversity evaluation. However, no previous work has analysed how the choice of a specific intent description may affect adhoc evaluation. We show that intent descrip-tions have a significant impact in adhoc evaluation and that special care should be given as to how the intent descrip-tions are selected. We further show that it is better to have very general intent descriptions or no intent descriptions at all when constructing test collections for adhoc evaluation. We then focus on diversity evaluation and identify the effect intent descriptions have on diversity based retrieval evalua-tion. We quantify this effect and discuss experimental de-sign decisions for the optimal distribution of judgment effort across different intents for a query vs. different queries. H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance Evaluation Experimentation; Measurement test collections; evaluation; query intents; mixed-effects model
In Information Retrieval (IR), test collections, compris-ing documents, topics and human-generated relevance judg-ments, are typically used to evaluate and optimize the per-formance of IR systems [21]. To build an IR system that maximizes the satisfaction of its end users test collections used should reflect the real-world usage of the system. Hence, one needs to select a corpus and a sample of queries that is representative of the real-world usage scenario. Further-more, one needs to design a relevance judging process that can lead to relevance labels that reflect the actual user sat-isfaction when encountering the documents under consid-eration. In the traditional (TREC-style) evaluation, this requires maximizing the judges X  understanding of the real users X  information needs. This is a particularly hard task given the diversity of user needs when posing a query.
In order to enable judges to understand the users X  infor-mation need conveyed by a query, when the first TREC test collection was created there was a decision made to provide a  X  X ser need X  statement together with the query text [12]. Current TREC collections still follow this decision. This is described by Harman et al. [12, 15] as follows :  X  X wo ma-jor issues were involved in this decision. First there was a desire to allow a wide range of query construction methods by keeping the topic (the need statement) distinct from the query (the actual text submitted to the system). The second issue was the ability to increase the amount of information available about each topic, in particular to include with each topic a clear statement of what criteria make a document relevant. The topics were designed to mimic a real user X  X  need, and were written by people who are actual users of a retrieval system. X  Therefore, there has been a strong desire when constructing test collections that user need statements (or topic descriptions) faithfully represent real-world users X  need(s).

Topic descriptions remain an important part of TREC and have become even more important for tasks like diver-sity evaluation [9, 10]. In the Web diversity task, for each query, different topic descriptions are provided to the judges representing various possible intents a user might have when submitting a query (referred to as  X  X ubtopic descriptions X  or  X  X ntent descriptions X ) in an attempt to be more represen-tative of the diverse population of users that issued these queries.

Despite the importance of topic descriptions in both adhoc and diversity based retrieval evaluation, there has not been a consistent methodology neither in identifying topic descrip-tions, nor in validating them against the actual intent(s) of real-world users. In adhoc evaluation topic descriptions are often times generated by a relevance assessor while in diversity evaluation subtopic descriptions are generated by looking into query reformulations and clicks as proposed by Radlinski et al. [14, 11]. None of these approaches are guar-anteed to identify representative intents (information needs) for a query.

It was recently shown that the subtopic descriptions given to the judges play an important role in diversity based eval-uation of retrieval systems, and that the selection of intents have a significant impact on the relative performance of re-trieval systems [16]. However, even though there is a num-ber of studies in the literature regarding the effect of various choices made during the test collection construction phase on evaluating the quality of retrieval systems, such as the effect of query selection [13, 1], document selection [4], and assessor error in judging [6], the effect of the topic descrip-tions provided to judges has never been studied for adhoc evaluation.

In this work we study the effect topic/subtopic/intent de-scriptions have on evaluation of retrieval systems. First, we focus on traditional adhoc evaluation and show that eval-uation conclusions are highly affected by the topic descrip-tions provided to judges during the judgment process. We propose an alternative judging process that leads to more stable, reliable, effective and efficient evaluation. Then, we focus on the diversity evaluation of retrieval systems. We quantify the variability of the evaluation results both due to the queries and due to the intents chosen per query and we discuss experimental design decisions with respect to dis-tributing judgment effort across different intents per query versus different queries.
In this section we first analyze the effect of topic descrip-tions on the results and conclusions of adhoc evaluation. We demonstrate that for the same set of queries, choosing differ-ent topic descriptions could lead to radically different con-clusions regarding the effectiveness of retrieval algorithms.
In order to analyze the effect of topic descriptions on ad-hoc evaluation of the quality of retrieval systems, we use data from the TREC 2010, 2011 and 2012 diversity task. Each query in the TREC diversity tasks has up to 6 subtopic descriptions (intents) and documents are judged separately for each subtopic description of a query. Hence, using this data, it is possible to simulate the effect of using different topic descriptions for a query in the adhoc evaluation of retrieval systems by (1) evaluating the quality of retrieval systems using judgments generated for a single subtopic of a query, and (2) analyzing how the evaluation of retrieval systems changes if judgments for another subtopic descrip-tion were to be used.

In Figure 1 one can observe the distribution of the MAP values for each system over different topic descriptions per query for TREC 2010, 2011 and 2012. The violin bars indi-cate this distribution when a specific subtopic description is used as the topic of the query, with this subtopic being sam-pled with replacement from the pool of available subtopics. As one can observe the distribution of MAP values for most of the systems are overlapping indicating a low confidence in comparative evaluation. The red dashed lines in the plots designate the minimum MAP value the top system may re-ceive depending on the topic descriptions used. Any system whose violin bar crosses this red dashed line can potentially be considered the top-performing system. For instance, in TREC 2010 (left-most plot) there are as many as 21 sys-Figure 2: The minimum, maximum, and mean Kendall X  X  tau correlations observed across different topic descriptions for each query of the TREC 2010 diversity runs according to AP. tems that could be considered as top-performing systems for a certain selection of topic descriptions.

To better quantify this effect we evaluate the quality of the submitted systems using each possible subtopic descrip-tions on the basis of average precision (AP), and ERR. Then, for each query separately, we compute the Kendall X  X   X  cor-relation between the rankings of systems on the particular query, obtained by evaluating the systems using all possible pairs of intents for the query.

Figure 2 shows some indicative results of this experiment for TREC 2010 when AP is being used. Similar plots are obtained for ERR and TREC 2011 and 2012. Each bar corresponds to the Kendall X  X   X  values (minimum, average, and maximum) for a particular query. It can be seen that even though there are some queries for which the Kendall X  X   X  correlation of evaluating using different topic descriptions is quite high, evaluations using some topic descriptions end up ranking systems completely differently. In fact, if we compute the average Kendall X  X   X  correlation obtained us-ing different topic descriptions of the same query, over all queries, the correlation is 0 . 274 for AP and 0 . 2295 for ERR for TREC 2010, suggesting that topic descriptions have a big impact in the evaluation of retrieval systems and more attention should be given to how they are generated.
The queries with ids 86, 84, 75, 67, and 70 are the queries with the highest Kendall X  X   X   X  X  (that also exhibit a small vari-ationinthe  X  values) when systems are evaluated by AP for TREC 2010. On the other hand queries with ids 94, 74, 72, (as most of the queries) exhibit a rather small Kendall X  X   X  , often below zero. In Table 1 one can view the subtopic descriptions for queries 86 and 94. As one can see, even though both queries are tagged as ambiguous, the subtopics of query 86 (which exhibits high Kendall X  X   X  correlation) are rather related with each other and on the same general topic, which leads to a good performing system on one subtopic to perform well on another subtopic. On the other hand, the subtopics for query 94 have rather different meanings, and in this case systems that perform well on one subtopic exhibit poor performance on another subtopic. (when t-test is used and results are significant). tion of the effectiveness and efficiency of evaluation when a specific intent per query is being used versus no intents.
When a statistical test is used to compare two different systems, the variance across queries plays a significant role on the power of the test. When differences between systems vary a lot across queries, we can say two systems apart with lower confidence than when differences are stable. To in-crease the confidence of the conclusions (hence the power of the test) in such a case one would need to increase the query set size over which systems are being evaluated. In the analysis that follows, we considered each pair of systems in TREC 2010, 2011 and 2012 diversity track. We computed the difference in the AP and ERR scores for all pairs using no intent description and obtained the standard deviation of these differences across all queries. This results in a distri-bution of standard deviation values across all system pairs. If we were to predict the standard deviation for a future system pair, given the data from one of the diversity tracks, we could assume that the standard deviation would follow the calculated distribution. To be on the safe side, and fol-lowing Webber et al. [22], we consider the 95th percentile of the computed standard deviation values as a pessimistic estimation of the standard deviation of a future system pair. By doing so, there is only a 5% chance that a future system pair will have a larger standard deviation.

Having the variance of the differences across queries in hand, a desirable Type I error (typically 0.05), a desirable level of power of the statistical test (typically 0.80), one can define a desirable detectable difference between two systems and obtain the topic set size required to detect such a dif-ference. Following the work by Webber et al. [22], for all experiments we considered the difference in the measure be-tween the median performing system in the first quartile of systems (the best 25%) and the median performing system in the second quartile of systems (the second best 25%) as the desirable detectable difference. Table 3 shows the num-ber of queries required to detect such a difference. As one can observe, when a specific intent is used per query, a large number of queries needs to be included in the collection com-pared to the case where no intent description is used.
Therefore, the results in these sections designate that us-ing no intent descriptions to collect relevance judgments not only leads to more stable comparative evaluation results but it also leads to more efficient and effective evaluation compared to collecting judgments with respect to specific intent descriptions.
 Table 3: Number of queries required to distinguish a system in the top 25th tier of systems from a system in the second top 25th tier.
Having observed that the choice of intent descriptions can have an enormous impact on the evaluated quality of a re-trieval system, we move into quantifying this effect for the case of diversity evaluation. In the IR literature there have been numerous attempts to quantify the variability in the effectiveness scores due to the choice of queries retrieval sys-tems are evaluated on [1, 2, 5]. To the best of our knowledge this is the first attempt to quantify the variability in the ef-fectiveness scores of diversity metrics due to the choice of intents.

In order to analyze the effect of intent descriptions on di-versity evaluation, we use data from the TREC 2010, 2011 and 2012 Web Track diversity task. All test collections con-tain 50 queries, where each query contains up to 6 subtopic (intent) descriptions. The collections were created by judg-ing each document separately for each subtopic description of a query. We focus on intent aware ERR (IA-ERR) and intent aware AP (IA-AP) as the metrics for diversity evalua-tion [7, 11]. We have also experimented with other diversity evaluation metrics, such as  X  -NDCG [17] and our conclu-sions are consistent when these alternative metrics are used.
Intent-aware effectiveness measures treat each query in-tent as a distinct interpretation for a given query. Assuming that each query has a fixed set of interpretations (intents) then the effectiveness of a system for a given query can be computed as the weighted average effectiveness over all the query interpretations (e.g. IA-AP or IA-ERR). The weight for each intent corresponds to the probability of this intent being the true intent of the user when submitting the query. In TREC all intents are considered to be equally likely and hence a simple average over all intents is computed.
Assuming that each query has a fixed set of interpretations (intents), there is a single random effect to be considered, the query effect. That is, it can be assumed that queries are sampled uniformly from an infinite (or very large) pop-ulation of queries, but each query has a fixed number of interpretations. Analysis of Variance and Generalizability Theory have been used in the past to quantify the query effect in adhoc evaluation [1, 2, 5]. This past work assumed that the effectiveness scores for a set of systems as measured by some evaluation metric conflate the true quality of the retrieval system with the effect of the particular query set being used in the test collection.

In the analysis that follows we start with the traditional assumption of a single random effect on the evaluation scores, that of the particular selection of queries. Then, we relax this assumption considering that intents are also sampled from the universe of all possible intents per query and ex-tend the analysis to quantify this effect. Mixed effects mod-els and analysis of variance are the frameworks we are using to quantify how important the selection of intents are in diversity evaluation.
 To illustrate the analysis, we take as example the effective-ness scores for systems submitted in TREC 2010 with their quality measured by IA-MAP. Given our assumption that up to 6 interpretations of the query are the only possible interpretations (hence fixed), we simply take the IA-AP as the measure of quality for a given query. Then, the linear model that accounts for the query effect (i.e. the effect the particular choice of queries have on the scores) is, where y ij is the value of an evaluation measure calculated on query j for system i ,  X  i is the  X  X rue X  quality of system b is the effect of query j on the evaluated quality, and ij is the residual error. The system effect is a so-called  X  X ixed effect X  X incethesystemswewouldliketocomparearefixed and not a sample from some system distribution. On the other hand the query effect is a  X  X andom effect X , assuming that the queries are actually sampled from a population of queries. The assumption behind this model is that the ran-dom variable, b j , is independent and identically normally distributed with zero mean and  X  2 1 variance. The residual error is also assumed to be independent and identically nor-mally distributed with zero mean and  X  2 variance.
In the statistical programming environment R the follow-ing procedure fits the linear effect model of Eq. 1.
Looking at the results of fitting Eq. 1 in the TREC 2010 data, we can see the actual difference between systems un-der the fixed effects section. The important part here is however the estimation of the standard deviations which can be found under the random effects section. The stan-dard deviation in the effectiveness scores due queries,  X  estimated to be 0 . 048. Given that differences in the scores between systems are in the second and third decimal place of IA-AP, the query effect is a rather significant one, which is exactly the conclusion of past work on this matter. We move on by relaxing the assumption that the user in-tents provided in the TREC dataset are the only possible interpretations of a query; instead we assume that a query may have a variety of interpretations and the ones consid-ered in the evaluation exercise (e.g. TREC Web track) are only a random sample of all possible interpretations. Under this assumption the effectiveness of a system not only varies across queries but it also varies within each query due to different intents. The question that remains to be answered is how significant is this effect and how can we quantify its significance. To do so we model the two effects by another mixed-effect model. where y ijk is the value of an evaluation measure calculated on query j for system i on the query intent k ,  X  i is the effect of system i , b j is the effect of query j , c ij is the effect of sampling intents (conflated with the system-query effect), and ijk is the residual error. As it can be seen, the intent effect is a random effect normally distributed with zero mean and  X  2 2 variance. We can fit Eq. 2 in R by, Let X  X  focus on the random effects section of the output. We can observe that  X  1 , the st.dev. of the query effect dis-tribution, is estimated to 0 . 0478, while,  X  2 ,thest.dev. of the intent effect distribution is estimated to 0 . 0312. 1
Note that even though system %in% query appears as a nested effect of the systems within each query, the function in fact calculates the variance within each system-query pair, i.e. the intent effect, because of the crossed design of our experiment, in which the systems are run against all queries.
Hence with the presented method we have managed to decompose the overall variance appearing in the evaluation scores into two significant components, (a) the variance due to the query effect which has been analyzed before in the lit-erature, and (b) the variance due to the intent effect within each query, which appears to be almost as significant as the variance due to query effect and it X  X  the first time that is quantified in the literature. We have repeated the experi-ment above for TREC 2010, 2011, and 2012 both for IA-MAP and IA-ERR; the results of which can be view in the table below.

As one can observe the effect of the choice of intents per query on IA-AP and IA-ERR is particularly significant and it should be considered both during the experimental design and during the comparative evaluation. In the section that follows we focus on decisions regarding the experimental de-sign.
In this section we consider a major design decision that needs to be addressed when diversity test collections are constructed with limited judgment budget: Is it better to include more queries or more intents per query in the diver-sity based test collection? In order to answer this question, we perform two experiments, the first is based on the anal-ysis of variance performed in the previous section and the second on Kendall X  X   X  statistics.

On the basis of the analysis of variance, we first express the IA measures in terms of a random experiment. Let X  X  consider the following random experiment, 1. draw a query from the set of all possible queries, 2. draw an intent from the set of all possible intents for 3. run a system on the query, obtain judgments for the The expected outcome of the afore-described experiment computes the true IA measure (e.g. IA-MAP or IA-ERR) over the population of all queries and intents. When a spe-cific test collection is being used one can characterize the afore-described experiment as a two stage sampling. At the first stage queries are sampled from the the universe of all queries and at the second stage intents are sampled from the universe of all intents within each query. Based on this observation we can compute the variance of the mean IA measure if N queries and M intents per query are used. In particular, the variance of the IA measure is: Figure 4: Variance of IA-MAP for TREC 2010 ver-sus the number of queries used, when the total num-ber of judged intents is equal to 100.

An optimal experimental design is a design that minimizes the variance of the measure. Such an experiment exhibits the largest generalizability of the results. As it can be ob-served from Eq. 3 and Table 3.1, given that in all cases the variance due to the query effect is larger than the variance due to the intent effect, to increase the generalizability of the experimental results it is preferable to add more queries than more intents per query. Just to demonstrate this, we plot the variance of IA-MAP against the total number of queries, N, used given that the total intents judged over all queries, i.e. N * M, is equal to 100 in Figure 4. That is, we present the variance of IA-MAP if one uses 1 query and 100 intents per query, 2 queries and 50 intents per query, up to 100 queries and a single intent per query (which is identi-cal to adhoc evaluation). As one can observe the larger the ratio between queries and intents per query the less the vari-ance and hence the largest the generalizability. However, as one can observe after the ratio of 50 queries with 2 intents per query the variance comes to a plateau. Hence, we can assume that this is a minimum ratio to have generalizable conclusions in diversity evaluation. As an exercise, if one can judge 1000 total intents over all queries (i.e. queries times intents per query = 1000), then this ratio indicates that it is good enough to have about 160 queries or more with 6 intents per query or less. Similar behavior was obtained for the IA-ERR metric.

On the basis of Kendall X  X   X  statistics, we reduce the num-ber of judgments in a test collection by either (1) randomly sampling queries from the collection, or (2) randomly sam-pling intents from each query. Each query in our diversity test collections contain up to 6 different intents. Hence, we reduce the size of the collections by either randomly selecting k  X  X  1 , 2 , 3 , 4 , 5 , 6 } intents from each query, or by randomly selecting p  X  X  1 / 6 , 2 / 6 , 3 / 6 , 4 / 6 , 5 / 6 , 6 / set of all queries. Then, focusing on the ranking of systems obtained with the entire collection as the baseline, we com-pute the Kendall X  X   X  correlation with the ranking obtained from the sampled collections and the baseline. This way, we can compare how the Kendall X  X  tau correlation obtained by reducing the number of intents differ from the Kendall X  X  tau correlation obtained by reducing the number of queries when the heteroscedastic model allows for different variances  X  2 query j group of measurements. This can be expressed in lme by a varIdent parameter that defines the grouping fac-tors.
Apart from the random and fixed effects section, there is a variance function section. The values of this section give the ratio of the standard error of each query group to the standard error of the first query group. Thus, for instance, the standard error for the query 66 is about 3 times less than that of the first query, while the standard error for the query 54 is about 2 times more than the standard error for the first query. We can confirm the adequacy of the heteroscedastic fit by re-examining plots of the residuals (not shown due to space limitations).

The results of the further decomposition of the intent vari-ance on a per query basis show that although the query effect is on average larger than the intent effect, there are certain queries for which the stability and reliability of the evalu-ation would benefit from using more intents. Hence, when extra judgment effort is available there may be the case that instead of adding an extra query to optimize stability, one may need to add more intents in a particular query. Based on this one can devise an online algorithm for optimal judg-ment distribution. Given an initial set of queries and intents per query and running the afore-described analysis one may optimally distribute any extra relevance judgment effort in a step-by-step (online) fashion, i.e. add the extra judgment to the most significant effect and repeat the analysis. We leave the details and the analysis of such an online algorithm as a topic of future work.
Constructing a test collection of 50 queries with just two intents per query may sound rather limiting with respect to diversity evaluation. Having seen in adhoc evaluation that merging all intents (or otherwise providing no intent descrip-tion) leads to more stable evaluation results in this section we analyze the effect of the specificity (or granularity) of subtopic descriptions in diversity evaluation, i.e., if it is pos-sible to only get judgments for a few intents, how should the Figure 7: The Kendall X  X   X  correlations observed within the different topic descriptions of a query for the TREC 2010 diversity runs according to IA-AP (top) and IA-ERR (bottom). subtopics corresponding to these intents be selected? Should we make the subtopic descriptions corresponding to these intents very specific (highly constraining what is considered relevant) or should we try to keep them very generic?
In order to analyze the effect of specificity of subtopic de-scriptions, we reduce the number of intents for each query by either (1) randomly removing a subtopic from the set of subtopics, or (2) randomly picking two subtopics i and j and merging their judgments together. Here, (1) simu-lates the effect of having fewer, highly specific subtopic de-scriptions and (2) simulates the effect of fewer, more generic subtopic descriptions. Considering mean Intent Aware AP (or Intent Aware ERR) computed using complete subtopic descriptions as a baseline, we compute the Kendall X  X   X  cor-relation between mean Intent Aware AP (or Intent Aware ERR) computed using the reduced subset of subtopics. Fig-ure 7 shows the result of this experiment. The line with  X + X  marks shows the effect of evaluation using more general subtopic descriptions and the line with  X  X  X  marks shows the effect of using more specific subtopic descriptions. The axis shows the number of subtopics used for each query and the y axis shows the correlations with the baseline. It can be seen that using more general subtopic descriptions, one can obtain much higher correlations with the baseline than using more specific subtopic descriptions. This behavior is consistent for both metrics. Based on this, one can conclude that similar to adhoc evaluation, it is better to have more generic subtopic descriptions in the test collection construc-tion phase for diversity evaluation.
To the best of our knowledge, this is the first work that investigates the effect of query intent description in adhoc evaluation. Further, this is the first work that quantifies the variance (and hence possible instability and unreliabil-ity of the evaluation conclusions) due to the intent effect in diversity evaluation. We have observed that intent de-scriptions have a significant effect on the stability, reliabil-ity, effectiveness and efficiency of IR evaluation as well as making decisions regarding the experimental design (e.g., deciding upon the number of queries vs. number of intent descriptions). Regarding the adhoc evaluation paradigm, we proposed an alternative judgment process with no intent description given to the human assessors which lets them free to assess the relevance of a document to any possible user intent. Such a process leads to more stable, reliable and efficient evaluation. Regarding diversity evaluation, we quantified the average effect of intent descriptions on the evaluation conclusions, we compared it to the query effect and suggested allocating judgment effort on queries than intents for more stable results. We showed that this sugges-tion may change when the effect of intents is decomposed separately for each query and it may be possible to devise an online algorithm for optimal judgment effort allocation. However, we left the analysis of these results and the sug-gestion of an online algorithm for optimal judgment effort allocation as a future work.
