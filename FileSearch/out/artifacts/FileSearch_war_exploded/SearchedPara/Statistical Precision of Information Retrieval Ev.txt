 We introduce and validate bootstrap techniques to compute confidence intervals that quantify the effect of test-collection variability on average precision (AP) and mean average pre-cision (MAP) I Reffectiveness measures. We consider the test collection in I Revaluation to be a representative of a population of materially similar collections, whose docu-mentsare drawn from an infinitepool with similar character-istics. Our model accurately predicts the degree of concor-dance between system results on randomly selected halves of the TREC-6 ad hoc corpus. We advance a framework for statistical evaluation that uses the same general framework to model other sources of chance variation as a source of input for meta-analysis techniques.
 H.3.3 [ Information Search and Retrieval ]: Systems and Software  X  performance evaluation Experimentation, Measurement bootstrap, confidence interval, precision
The purpose of I Revaluation is to measure the effective-ness, or relative effectiveness, of information retrieval sys-tems. Statistical precision 1 is the degree to which the mea-surement is free from random error; validity is the degree to which the measurement truly reflects retrieval effective-ness. Validity may be further qualified as internal validity , the aptness of the measure under test conditions, or external
Known simply as precision in the statistics literature; de-noted statistical precision here to distinguish it from the IR effectiveness measure of the same name.
 Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. validity , the generalizability of the results to other situations ([8], pp. 115-134).

Our primary concern is the statistical precision of infor-mation retrieval experiments. If an experiment were to be repeated with different but materially similar data, how sim-ilar would the results be? Is it possible, when the test is conducted, to predict accurately this degree of similarity? While these questions are largely amenable to statistical in-ference, they may be understood only in the context of a general investigative framework that includes questions of validity which are not necessarily statistical. Instead they are addressed by the tools of scientific inquiry  X  observation, induction, deduction and experiment.

We argue here that one source of random error  X  that associated with the test corpus  X  should be considered in assessing I Revaluation methods. We develop and evaluate bootstrap methods that estimate this source of statistical imprecision.

We further argue that another source of random error  X  that associated with the topics  X  is poorly modelled by re-garding the set of test topics as a random sample of some  X  X rue X  population; experiments assuming such a model are uncompelling to establish either statistical precision or va-lidity. Instead, we propose, each topic should be regarded as a separate test, and the results of these tests should be combined using meta-analysis techniques ([8], pp. 643-673).
The object of our study is the TREC ad hoc retrieval evaluation technique[19]. Given a topic T and a set of doc-uments D , each tested I Rsystem returns an ordered subset S = s 1 s 2 ...s n of D , ranked by the system X  X  estimate of the likelihood that each document is relevant to T . Several ef-fectiveness measures are computed, including average preci-sion ( AP ), precision at k returned documents ( P @ k ), and R-precision ( P @ R ) defined as
The test is repeated for several topics; the effectiveness measures from these tests are reported separately and also averaged across topics. Inparticular, mean average precision ( MAP ) is typically used and accepted as avalid effectiveness measure.

The evaluation measures depend on the truth value of if document d is relevant to T, which must be adjudicated. TREC uses the pooling method in which the top-ranked t documents from a set of systems (perhaps the systems under test) are combined, eliminating duplicates, and presented in random order to a human assessor. The assessor records a judgement of relevant or not relevant for each document in the pool. Documents not in the pool are assumed to be not relevant .

TREC evaluations have typically tested about 50 systems using 50 topics, n = 1000, | D | = 500000, t = 100, and a pool size of 40000 (for all 50 topics).[19]
The notion of population has been the subject of historical and currentphilosophical debate [7]. Weadopt Fisher X  X  view [4] of an infinite hypothetical population: A model is a characterization with a few parameters that abstracts the hypothetical population, removing irrelevant information while preserving that which reflects measures of interest; in this instance, measures of retrieval effective-ness. As such a model is a scientific theory that is posed to explain known facts; its worth is judged by its simplicity, its ability to explain existing observations and its ability to pre-dict new ones. A theory is never  X  X roved X  as it is simply a model, but our confidence in it builds as these criteria (de-gree of abstraction, explanatory ability, predictive ability) are demonstrated.

With respect to I Revaluation, it is possible to identify several hypothetical target populations: thetopics thatmight be presented to a system, the corpora from which the sys-tem may be expected to retrieve documents relevant to the topic, the set of relevance assessments for the topics; even the set of systems that might be subject to test may be con-sidered to be a hypothetical population of interest. How-ever, these populations are exceedingly difficult to specify, let alone model with a small number of parameters. And sampling them would be a hopeless task as many members of the population exist only in the future. Instead, we select readily available data and observations, and treat them as representing the hypothetical population of all data like that which we collected  X  the source population. The meaning of the word  X  X ike X  must be considered carefully in modelling such a population; a narrow definition may improve sta-tistical precision while a broad one may improve external validity.

External validity  X  the applicability of the model to the target population  X  is established, not by statistical infer-ence, but by scientific inquiry in which (a) predictions about other data are made and tested by experiment, and (b) sources of possible systematic or random error are identi-fied and tested by experiment.
Tague-Sutcliffe [12] argues that of validity, reliability and efficiency should be considered in a qualitative assessment of various design issues. Validity is used to mean internal validity; reliability 2 subsumes (statistical) precision and ex-ternal validity; efficiency relates to the resources that are expended in achieving validity and reliability.
 Tague-Sutcliffe [13] performed a statistical analysis of the TREC-3 results, under the assumption that the set of topics was a random sample of  X  X ll possible queries that might be asked of the database. X  Paired testing was rejected so as to avoid the fallacy of multiple hypothesis testing (cherry-picking); analysis of variance (ANOVA) was used to com-pute significant differences among systems according to a number of performance measures. Very large differences in performance  X  spanning approximately three-quarters of the tested systems  X  were necessary to distinguish systems with 95% confidence (i.e. p&lt;. 05). Neither the choice of measure nor an arcsine transformation had substantial impact on the results.

Savoy [11], under the assumption that topics are a random sample, examines the use of classical and bootstrap methods [3] to test the relative performance of pairs of systems. The bootstrap builds a concrete model for a hypothetical pop-ulation in which each element of the sample is replicated an equal and infinite number of times; this population may, in effect, be sampled any number of times by drawing el-ements from the original sample, with replacement. Savoy performs significance tests to support the proposition that
In testing, reliability is the degree to which the same test, administered to the same subject, will yield a consistent score ([8], p. 507). Assuming one interprets  X  X he same X  lit-erally, I Rtests are 100% reliable. Figurative interpretations are captured by Fisher X  X  hypothetical population. the bootstrap yields higher statistical precision than para-metric approaches, and that median, as opposed to mean, is a better summary statistic.

Voorhees and Buckley [17] explore the effect of topic set size, also assuming thetopicsto bearandomsample. Rather than building a statistical model, they measure the propor-tion of discordant results between evaluations performed us-ing disjoint sample subsets. Results are stratified by the dif-ference in evaluation measure between each pair of systems. For each stratum an exponential curve on two parameters is used to estimate the proportion of discordant pairs.
Sanderson and Zobel [10] measure discordance propor-tion stratified by p-value of a significance test and at the same time by the magnitude of the difference between MAP scores, and observe that a large difference coupled with a small p-value predicts low discordance.

Several studies [15, 2, 20, 9, 18, 10] have considered the effect of variations in relevance judgments and judging pools on retrieval evaluation. Buckley and Voorhees [1] consider the effect of using differently formed queries to represent the same information need.

Reports on IR evaluations often 3 include standard tests such as paired t-tests, Wilcoxon signed-ranked tests, sign tests, or analysis of variance, notwithstanding questions as to their applicability [6]. Reports typically include signifi-cance judgements based on a fixed  X  threshold; p-values are less common and confidence intervals are rarer still. The vast majority, if not all, assume that topic variation is the only source of random error.

Statistical hypothesis testing in general, particularly that based on a fixed  X  threshold, has come under criticism lately ([8], pp. 183-199). H 0  X  the null hypothesis that two pop-ulations are the same  X  is a strawman that is too easy to refute. In the real world, no two distinct things are the same[5], and a large enough sample will show this. Such a hypothesis should be replaced by an estimate of the magni-tude of the difference and an argument as to whether or not that difference is important.
To measure statistical imprecision due to collection vari-ability, we use the hypothetical population of all collections that are materially similar to the test collection. We for-mulated a simple characterization of this population and conducted a pilot experiment in which we constructed confi-dence intervals for AP using a bootstrap estimation of model parameters, and predicted the number of AP values in a second test that should fall in this interval, according to the model. The results show good precision but for some out-liers which led us to examine the special cases which they represent, and to adapt the model to account for them.
Our initial model assumes that D , the set of documents in the test collection, is an independent and identically dis-tributed (i.i.d.) sample of a population of similar docu-ments. By similar, we mean having the same relevance value, and yielding a comparable score (or at least a compa-rable ranking relative to other documents) when retrieved by the I Rsystem under test. The hypothetical population to which D belongs is the set of all such samples.
Not often enough, according to Sanderson and Zobel [10], who surveyed published SIGI Rpapers and found that 14 of 28 claiming retrieval results reported no statistical tests.
Let D be some other collection of the same size from the same population as D ,and AP be the average preci-sion from applying the same I Rsystem to D .Wewishto compute a 95% confidence interval  X  a range of possible val-ues such that, with 95% probability, contains the expected value E ( AP ). We are not aware of any direct paramet-ric method of estimating this confidence interval; therefore we use the bootstrap to sample the population to which D and D belong. By repeated sampling, we may estimate thevarianceof AP and compute parametric confidence in-tervals assuming a normal (Gaussian) distribution. Or we may estimate the variance of a monotonic transform t ( AP ) which is better distributed, in effect computing confidence limits for E ( t ( AP )) which may be more accurate. Or we may compute confidence intervals nonparametrically by se-lecting the 2 . 5 th through 97 . 5 th percentile of the bootstrap samples. A bias-corrected variant of percentile method is known as BC a [3]. We used three methods for our pilot: variance of AP values; variance of logit ( AP ) 4 ; BC a .
The bootstrap constructs repeated examples of D by re-sampling D . That is, the elements of each example of D are selected from D , with replacement. For this applica-tion we assume that | D | is large compared to n ,thesize of the ranked list of documents to be retrieved (typically |
D | &gt; 100 n ). This assumption allows us to use the Pois-son distribution to generate S , the list of documents re-trieved from D , without considering the irrelevant elements of D . Specifically, each document of S (the retrieved set of documents) is assumed to be replicated k times in S with probability 1 e  X  k ! .

So to construct S we take each s i  X  S in rank order, gen-erate a random k according to the Poisson distribution, and replicate the element k times. We assume that the repli-cated elements all receive comparable scores from the IR system and thus are consecutively ranked in S .Usingthis construction, | S | X  X  S | . The difference in sizes is inconse-quential to the AP calculation.

It is also necessary to compute R for the bootstrap sam-ple. To do this we partition R :
R ret is determined directly from S ; R not is computed post-hoc as: where each k i is randomly generated according to the Pois-son distribution.

The net effect is that we may compute as many exam-ples of AP as necessary to compute model parameters for our hypothetical population. For our untransformed para-metric confidence interval estimate, we compute the stan-dard deviation  X  of the AP values. The 95% confidence interval is AP  X  1 . 96  X  . For the logit-transformed paramet-ric estimate, we first replace AP values of 0 and 1 by  X  logit ( x )= log ( x 1  X  x ) and 1  X   X  respectively, and compute the standard devia-tion  X  logit of logit ( AP ). The 95% confidence interval is logit  X  1 ( logit ( AP )  X  1 . 96  X  logit ). BC a confidence intervals were computed directly using the System Rimplementation of Efron X  X  S-Plus code ([3], pp. 402-403).
We used the raw results from the 74 I Rsystem runs eval-uated over 50 topics in the the TREC 6 ad hoc task[14]  X  3652 non-empty ranked-result lists in total. The corpus documents were split into two subsets, A and B, of roughly equal size using an MD5 hash on the document identifier. Similarly, each result list was split into two  X  one repre-senting the documents retrieved from A ; the other from B . These two sets of result lists were assumed to represent the retrieval results on two independent corpora drawn from a common source population. We used 2000 bootstrap sam-ples of the A corpus and the three bootstrap techniques to compute 95% confidence intervals.
Recall that the confidence interval is defined to be an in-terval within which contains the true value E ( AP )with 95% probability. If we knew the value of E ( AP )wecould simply count the proportion of times E ( AP ) fell within the computed confidence interval, expecting this proportion to be about 95% if the intervals were accurate. Similarly, if the intervals were unbiased, we would expect an equal propor-tion (about 2.5%) to fall above as below the interval. linear 3.7% 77.8% 11.1% 2.8% 82.0% 15.1% logit 11.1% 76.5% 12.5% 9.4% 81.4% 9.1%
BC a 7.4% 77.1% 15.5% 6.7% 80.8% 12.5% model 8.25% 83.5% 8.25% 8.25% 83.5% 8.25% Figure 1: Distribution of Normalized AP B  X  AP A
But we don X  X  know E ( AP ); therefore, we validate the model by using it to predict how many times AP B ( AP computed on the B corpus) should fall within the confi-dence interval created by bootstrapping the A corpus. Note that this frequency is not 95%, as commonly assumed, but considerably lower.

Fortheparametric models, recall that theintervalis AP A  X  1 . 96  X  . We wish to predict how likely AP B is to fall in this interval; more precisely or
Our model predicts that AP A and AP B both have stan-dard deviation  X  ,so  X  AB , the standard deviation of AP B AP A is given by  X  AB =
This range bounds 83.5% of the area under the normal curve for AP B  X  AP A and hence we expect the inequalities to be satisfied, i.e. AP B to fall within the confidence interval, 83.5% of the time. Furthermore, AP B should fall about equally to the left and to the right of the interval.
The same argument holds for the logit-transformed para-metric model. There is no similar mechanism for making direct prediction from the non-parametric confidence inter-vals. However, we may still generally compare the results to those of the parametric methods.
Confidence intervals were computed using the A subset of the results for each topic within each run ( n = 3652). AP was computed for the corresponding B subset and compared to the confidence interval. Table 1 reports the proportion of AP B values above, within, and below the intervals. Figure 1 shows (for the BC a method only) the distribution of AP B AP A , normalized so that the confidence interval occupies the range  X  1 .. 1.

Figure 1 makes it apparent that there are a large number of outliers at the high end of the distribution. Further inves-tigation reveals that these extreme values are almost entirely accounted for by small-sample effects. Most of these arise when R  X  5( R  X  1 in particular). Selecting only those top-ics for which R&gt; 5 gives the proportions listed in second half of Table 1. The number of AP B values within the predicted interval approaches, but does not quite reach, the predicted 83.5%. Furthermore, the linear and BC a estimates show evidence of bias.

Even when we restrict our attention to the situation in which R&gt; 5, a handful of outliers remain. These consist mainly of cases with AP A =0or AP A = 1, where the Boot-strap erroneously reports  X  = 0. In this situation, AP B falls within the interval only if it is exactly equal to AP A ,which occurs in substantially less than 95% of the cases. Further investigation revealed that the high error rate among the cases with R  X  5 was also largely due to cases with AP A =0 or AP A =1.
From the pilot we conclude that the model works well for the majority of the situations, but special attention needs to be paid to situations in which R is small, or in which AP A =0or AP A =1.
It is well known that many statistical methods are inappli-cable to small samples. One way to deal with this problem is to design studies and experiments that yield sufficient num-bers. In studying the prevalence of disease, for example, an epidemiologist would ensure that the sample population would be expected to contain a sufficient number of positive examples to be amenable to statistical inference. TREC X  X  corpus design process pays careful attention to this consid-eration, rejecting topics expected to yield very low or very high values of R .Thecaseof R = 0 is avoided in particu-lar, because AP is undefined in this situation. The TREC-6 corpus that we used had 5 of its fifty topics with R  X  5(3 with R =5;1with R =4;1with R =3;nonewith R  X  2.
 Our split-sample technique created A and B corpora which effectively halved R , giving rise to substantially more (9 and 8resp.)with R  X  5andalsotoseveral(3and4resp.)with R  X  2, smaller than any in the TREC-6 corpus. Therefore we would expect that the estimation techniques used in our pilot would be considerably more accurate were they applied to the full corpus.

Experimental design issues notwithstanding, the results of our pilot prompted us to investigate ways to augment our model to handle cases with small R .
We modelled cases with small R (and some cases with larger R ) by considering AP =0and AP =1asspecial cases. AP = 0 means that, among the R relevant doc-uments in the collection, the retrieval system found none. But it may be that in the source population  X  the set of materially similar collections  X  there exist silver bullets  X  relevant documents that the system could retrieve but did not happen to be among the R sampled for this particular test. Using exact binomial probabilities, we establish con-fidence limits for the proportion of such silver bullets that may exist in the source population. The lower confidence limit is 0 and the upper confidence limit is the smallest u such that the probability (1  X  u ) R of having no silver bullets in our sample does not exceed the significance threshold, under the assumption that u is the true proportion. For ex-ample, if R = 4, the 95% confidence limit for the proportion of silver bullets is [0 , 0 . 53]. That is, we cannot statistically reject the hypothesis that 53% of all relevant documents are silver bullets!
We take 53% (or whatever the appropriate fraction is for agiven R ) to represent the extreme case that results in our upper confidence limit. But the confidence limit must be expressed as an AP value, not a proportion of silver bul-lets. To compute AP , we assume that the retrieved rank of a silver bullet is uniformly distributed between 1 and n (i.e. could appear anywhere in the retrieved list) and, using dy-namic programming, compute by enumeration the resulting E ( AP ). This value is chosen as the upper confidence limit in place of 0.
 linear 2.1% 82.8% 15.0% 2.4% 83.9% 13.6% logit 8.5% 83.6% 7.9% 8.7% 82.7% 8.7%
A similar model was used for the cases of AP =1.Weuse binomial probabilities to establish bounds for the the pro-portion of lead balloons  X  relevant documents that the sys-tem is unable to retrieve, but are not represented in the R relevant documents sampled for this particular test. A sim-ilar dynamic approach converts the worst-case lead-balloon proportion to a lower confidence limit.

The same corrections were applied to cases with AP  X  0 and AP  X  1; for such values, we use the larger of the original confidence interval and the interval under the assumption that AP =0( AP =1,resp.).

Table 2 shows the result of applying small-Rcorrection to the results of the two bootstrap methods. 5 The left
We did not use the computationally intensive BC a method as it showed poor results in the pilot and was not amenable half shows the fraction of AP B values that fall within the confidence intervals computed from A ; the right half shows the the fraction of AP A values that fall with intervals com-puted from B .Inbothcases,bothmethodsyieldin-interval fractions that are extremely close to those predicted by the model. However, the linear model exhibits considerable bias, as evidenced by the fact that the fraction above is roughly six times larger than the fraction below in both tests. The logit model demonstrates no apparent bias.

Figure 2 shows normalized AP B  X  AP A for the linear and logit models. Logit is clearly more symmetric with fewer outliers. All of the (few) logit outliers are cases with AP A = 0, for which a nonparametric model was used. Such a model predicts only the number in-interval, not the dis-tribution of those outside. The outliers should therefore not be considered to contradict the model.
Mean Average Precision (MAP)  X  the average of AP val-ues over several topics  X  is commonly reported as an overall summary measure. We investigated methods to compute the sensitivity of MAP to corpus variation. We applied the same methods to a summary measure we call logistic MAP (L-MAP), which averages logit ( AP ) instead of AP . L-MAP is closely related (and nearly identical for small values) to geometric MAP (G-MAP)  X  the average of log ( AP )values  X  which has been proposed recently to increase the contri-bution of low AP values to the overall measure. [16] MAP 27.0% 68.9% 4.0% 21.6% 75.6% 2.7% L-MAP 1.3% 83.7% 14.8% 16.2% 82.4% 1.3%
Table 3: Bootstrap 50 topic mean within interval MAP 1.3% 78.3% 20.2% 16.2% 81.0% 2.7% L-MAP 2.7% 74.3% 22.9% 22.9% 74.3% 2.7% Table 4: Parametric 50 topic mean within interval The first row of table 3 shows the fraction of the 74 MAP ( MAP A resp.) fractions falling within the interval computed by bootstrap sampling A ( B resp.). That is, MAP was computed for each of the 2000 bootstrap samples, and the variance of these values was used to estimate the standard error. The second row shows the same method applied to L-MAP. In the case of MAP, we see that the in-interval frac-tion falls considerably below that predicted by the model, suggesting that the model is inappropriate. The L-MAP in-interval fraction, on the other hand, suggests that in this case the model is appropriate. The imbalance between above and below fractions suggests random skew between A and B rather than a systematic bias in the model. Note that these fractions are determined from 74 data points, as opposed to 3652 for the AP computations. Therefore these observations to the further experiments detailed below. should be taken as indications to be confirmed by a larger experiment.

Table 4 shows the results of using a parametric approach to combine the 74 separate AP confidence intervals into a single MAP (L-MAP) confidence interval. To compute the MAP confidence interval, we averaged the variances derived from the logistic model, but weighted them according to their relative contribution to MAP statistic. We used a mul-tiplicative weight of AP  X  AP 2 , the derivative of AP with respect to logit ( AP ). To compute the L-MAP confidence interval, we simply averaged the unweighted variances of the 74 separate estimates. We observe that the parametric estimates for MAP are considerably better than the boot-strap estimates, further validating the logit model. However, they appear to be slightly optimistic, yielding about 80% in-interval as opposed to the predicted 83.5%. On the other hand, the parametric estimate for L-MAP is much worse than the bootstrap estimate. We attribute this error to the fact that the estimate weights heavily the small-R-corrected estimates, which are themselves non-parametric and there-fore not suitable variance estimates. This error is much less important for MAP because most of these estimates receive extremely low weight.

Figure 3 shows the MAP parametric confidence intervals computed from A , along with the MAP A and MAP B val-ues, marked x and o respectively. This graphic shows that the confidence intervals generally do a good job of predicting the range of possible MAP B values, and the out-of-interval values are near-misses rather than outliers. Figure 3 also shows the corresponding L-MAP bootstrap confidence in-tervals, and reflects the same general observation. Figure 4 shows the MAP and L-MAP values based on the full corpus (as opposed to the A subset). As expected, the confidence intervals are smaller, typically with a width of about 0.05.
Experimental evidence suggests that our model for corpus variability aptly predicts confidence intervals for individual AP values. logit ( AP ) has better algebraic properties than AP and therefore yields a better model from which AP con-fidence intervals can be derived. AP valuescloseto0and 1 are problematic, and arise often when R  X  X henumberof relevant documents  X  is small. These anomalies may be ad-dressed by using a non-parametric binomial model to predict silver bullets and lead balloons  X  relevant documents whose properties are not represented at all in the corpus.
MAP exhibits the same algebraic anomalies as AP ;in this situation, a weighted average of logit ( AP ) variances is used to predict indirectly the effect of averaging non-logit-transformed AP values. L-MAP, on the other hand, may be estimated directly using the bootstrap. We expect that G-MAP would exhibit similar properties to L-MAP, as they differ substantially only for values close to 1  X  values which occur rarely in I Revaluation.

Our framework and validation technique applies equally to models for evaluating the relative effectiveness of a pair of I Rsystems. One simply has to model the difference d between the two systems according to some measure of in-terest; for example d = AP x  X  AP y . If we construct sep-arate models for AP x and AP y with standard deviations  X  and  X  y we may estimate  X  d = dence interval of  X  1 . 96  X  d . Our results indicate that defining d = logit ( AP x )  X  logit ( AP y ) would yield a better estimate. Confidence intervals for the difference contain strictly more information than fixed-threshold hypothesis tests; the dif-ference between AP x and AP y is significant (two-tailed test,  X  =0 . 05) exactly when 0 does not fall within the confidence interval for d . The same approach may be applied to the difference between MAP, L-MAP or G-MAP scores.

A more powerful estimate for  X  d  X  X hichtakesintoac-count correlations between the two systems X  results  X  may be effected by bootstrapping d . For each bootstrap sample, directly compute AP x , AP y [ logit ( AP x ), logit ( AP hence d ;thenestimate  X  d from the various d values.
Ouranalysis indicates thatcurrentmethods 6 poorlymodel random error due to topic variability. By the argument in section 6.1, if a test concluded from one sample that AP x &gt;AP y (p = 0.05) we would expect that for some other sample AP x &gt;AP y would occur with probability 0 . 835, ( not 0 . 95). Furthermore, this prediction should be insensitive to the sample size and the magnitude of the difference between AP x and AP y . Experimental results [10] are inconsistent with these predictions, contradicting thevalidity of thetests. We conjecture that using the logit transform would mitigate but not overcome the shortcomings of the underlying model.
If the logit transform were to yield a reasonable model for topic variability  X  a proposition the experimental inves-tigation of which we leave to future work  X  it would be a simple matter to use the bootstrap method developed here to model it, or to model both topic and collection variability at once. One must simply compute L-MAP (or the differ-ence between L-MAPs) using bootstrap resampling to select both the topic and the corpus for each sample. The under-lying foundation is the same.

A more promising approach, we argue, is to regard the re-sults from each topic as separate tests and to combine them using meta-analysis ([8], pp. 643-673; [5]). For a simple paired hypothesis test, one may simply combine the values of d i and  X  i arising from k separate tests to compute an over-all single-tailed p-value p =1  X   X  cumulative normal distribution. More sophisticated meta-analysis involves identifying a quantitative  X  X ffect X  and mea-suring it with confidence intervals. While it is difficult to argue that d = AP x  X  AP y is a meaningful quantity, the value d = logit ( AP x )  X  logit ( AP y ) represents the logarithm of the ratio of the effectiveness of the two systems, which we advance as a worthwhile measure.

Meta-analysis may also be used to estimate the perfor-mance of a single system over several tests. The most ob-vious measure is simply AP but our results indicate that logit ( AP ) would be more appropriate. Even more appro-priate would be a measure that compensated for topic diffi-culty; we suggest d = logit ( AP )  X  logit ( X )where X is the performance of some baseline system or some other estimate of  X  X ormal X  system performance.

Fixed-effect model meta-analysis computes the effect d and standard error  X  as follows:
The overall effect estimate is the average of the individual estimates, weighted by their statistical precision. Random-effect model meta-analysis further compensates for hetero-geneity of tests such as might occur when using diverse top-ics or corpora. [1] Buckley, C., and Voorhees, E. M. Evaluating
In particular, a t-test which tacitly models d = AP x  X  AP over the population of all topics with a fixed corpus. [2] Cormack, G. V., Palmer, C. R., and Clarke, C. [3] Efron, B., and Tsibirani, R. J. An Introduction to [4] Fisher, R. A. Theory of statistical estimation. [5] Glass, G. V. Meta-analysis at 25. [6] Hull, D. A. Using statistical testing in the evaluation [7] Lenhard, J. Models and statistical inference: The [8] Rothman, K. J., and Greenland, S. Modern [9] Sanderson, M., and Johno, H. Test collections [10] Sanderson, M., and Zobel, J. Information retrieval [11] Savoy, J. Statistical inference in retrieval [12] Tague-Sutcliffe, J. The pragmatics of information [13] Tague-Sutcliffe, J., and Blustein, J. A statistical [14] Voorhees, E., and Harman, D. Overview of the [15] Voorhees, E. M. Variations in relevance judgements [16] Voorhees, E. M. Overview of the TREC-2004 robust [17] Voorhees, E. M., and Buckley, C. The effect of [18] Voorhees, E. M., and Buckley, C. Retrieval [19] Voorhees, E. M., and Harman, D. K. ,Eds. TREC [20] Zobel, J. How reliable are the results of large-scale
