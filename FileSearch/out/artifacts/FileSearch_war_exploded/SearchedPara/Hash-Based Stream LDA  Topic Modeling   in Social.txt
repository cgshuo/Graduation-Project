 In this paper we are motivated by the problem of topic discovery in social media. We set of challenges associated with the scale of modern social media outlets such as continuously for extended periods of time, as social conversations do not stop, produce output in a timely fashion to remain relevant and ensure high quality of output. 
Commonly used data mining techniques handle the problem of social stream topic discovery by applying batching heuristics to process the never-ending stream of messages. Since retaining all messages is not feasible in practice, current topic modeling approaches improve quality of topic discovery by retaining globally applicable statistics such as topic-word counters, but fail to take advantage of information in a scalable and meaningful way. Therefore, in this work we propose a new generative probabilistic model called Hash-based Stream LDA (HS-LDA), which is a generalization of the popular Latent Dirichlet Allocations (LDA) [1]. The model improves upon previous works by introducing a theoretical framework that makes it possible to retain the knowledge of historical stream messages in a scalable way and use this knowledge to improve the quality of topic discovery in social streams. Further, an efficient inference mechanism for the HS-LDA model is outlined, which makes use of the scalable hashing algorithm called Locality Sensitive Hashing (LSH) [2]. We show that the HS-LDA streams by comparing the predictive power of the topic models inferred by HS-LDA with that of topics learned by applying the classical LDA, On-line LDA [3] and SparseLDA [4] approaches to stream da ta. Evaluation was performed using data collected from the Twitter microblog site and an IRC chat system. Our experiments showed that HS-LDA outperformed other techniques by more than 12% for the Twitter dataset and by 21% for the IRC data in terms of average perplexity. modeling and stream mining is discussed. Section 3 introduces the HS-LDA model, In section 4, comparison of performance of our method to that of other modeling approaches in terms of perplexity is presented. Section 5 concludes the paper and outlines future work. The seminal work on Latent Dirichlet Allocation (LDA) [1] provides basis for numerous extensions and generalizations in the field topic modeling. LDA considers document collections as bag-of-words assemblies that are generated by stochastic processes. To generate a document, a random process first selects a topic from a distribution over topics and then generate s a word by sampling the associated topic-word distribution. Both the topic and the word distributions are governed by hidden (or latent) parameters. 
The LDA framework is designed to operate on a fixed set of documents and cannot be applied to stream data directly as c onverting an unbounded number of documents to hash tag annotations and training models based on these aggregates [5], [6, 7]. 
An interesting recent work by Want et al. introduced an efficient topic modeling technique called TM-LDA for stream data. This approach is based on the notion that if document topic model is known at time  X  , at time  X 1 X  a new topic model can be predicted and an error can be computed by comparing the  X  X ld X  and the  X  X ew X  topic models. This error computation reduces the challenge of estimating topic models for new documents to a least-squares problem, which can be solved efficiently. Focusing on the popular Twitter micro-blog data, TM-LDA selects a set of individual authors and trains a separate model for each of the authors. To accomplish this, TM-LDA monitors Twitter for an extended period of time (a week X  X  worth of data was collected in the original work) and then trains a model to be able to predict new messages. The idea of using authorship to improve topic modeling quality is not unique to TM-LDA. A recent work by Xu et al. modified the well-known Author-Topic [8] model for Twitter data [6]. Xu et al. extended the insight of the Author-Topic model by taking advantage of additional features available in Twitter such as links, tags, etc. 
Another way to approach topic modeling in streams is to apply LDA machinery to snapshots or buffers of documents of fixed size. Online Variational Inference for LDA [9] is one such technique. The algorithm assembles mini-batches of documents at periodic intervals and uses Expectation Maximization (EM) algorithm to infer distribution parameters by holding topic proportions fixed in the E-step and then re-computing topic proportions as if the entire corpus consisted of document mini-batches repeated numerous times. Topic parameters are then adjusted using the weighted average of previous values of each topic proportion. 
Another approach termed On-line LDA [3] co nsiders the data stream as a sequence of time-sliced batches of documents. The approach processes each time-slice batch using the classical LDA sampling techniques, with the variation being that the corresponding collapsed Gibbs sampler initialization is augmented with the inclusion of topic-word counters from histories of pervious time-slice batches. The histories are maintained using a fixed-length sliding window and the contribution of each history each element in the sliding window. 
In another work, Yao et al in [4] considered topic discovery in streaming documents and proposed the SparseLDA model. Noticing that the efficiency of sampling-based inference depends on how fast the sampling distribution can be evaluated for each token, their work enhanced the inference procedure in a way as to allow parts of computations used in sampling to be pre-computed, thus improving performance. collection of training documents and then, for each test document, sampled topics using counts from the training data and test document only, ignoring the rest of the stream. 
The explosion of micro-blog popularity has attracted much attention from outside of the topic modeling community. One particularly interesting application is the field emergent clusters of similar stream messages, which are said to be indicative of particularly interesting and currently relevant stories. First story detection approaches require the ability to discover clusters of similar documents in near real-time fashion, computational complexity of commonly used clustering algorithms (hierarchical, seized upon the concept of Locality Sensitive Hashing (LSH) [2], which is an approach for identifying a datum neighborhood in constant time [10]. In [10], Petrovic et al use a combination of LSH and inverse index searching to show that clusters of similar documents may be identified in constant time with exceptional accuracy and low variability. As noted in the preceding survey of related works, many approaches to topic modeling in streams have been developed in recent years. A number of these approaches [3, 9] attempted to enhance quality by preserving various aspects of topic inference calculations and predicating topic learning upon past knowledge. Unfortunately, none of these techniques were successful in retaining the knowledge of stream documents relying instead on storing global structures such as topic-word multinomials. Hurdles for retaining document knowledge are two-pronged  X  1) the number of documents in streams is unbounded making storage of individual document information not feasible, and 2) since previous documents do not get replayed in streams, retaining records of their presence directly may be meaningless for topic modeling. 
Therefore, this section introduces the new Hash-Based Stream LDA (HS-LDA) model, which provides a mechanism for retaining document knowledge for stream modeling in a scalable and meaningful way. HS-LDA is a generative probabilistic model that describes a process for generating a document collection. Like LDA, in HS-LDA each document is viewed as a mixture of underlying topics and each word is generated by drawing from a topic-word distribution. HS-LDA departs from LDA by imagining that, in addition to words, the generative process also emits certain objects in an intuitive way, we reach out to the physical world for a descriptive analogy. We borrow from particle physics nomenclature and recall that, in physics, a LDA model are similarly ethereal, we introduce the notion of HS-LDA neutrinos (or pseudo-neutrinos for short), as the analogy with the real particle seems appropriate. collection of categories [11], the HS-LDA pseudo-neutrinos are also thought to belong to a fixed set of possible types (or flavors). The physics analogy is abandoned at this point, however, as HS-LDA makes no further claims as to the properties or nature of each flavor. The generative process is graphically outlined in Figure 2. 
In Figure 3, the generative process is outlined. There, words are generated in a way common to many LDA-type models by drawing from a distribution over words. Unlike other approaches, however, a pseudo-neutrino is also emitted by a draw from a multinomial distribution parameterized by a vector of topic-specific neutrino type proportions. types to just a single type (say { X  X oot X  X ), HS-LDA would become equivalent to LDA as all draws of type label assignments would be the same making the generative branch from  X  to  X  redundant. Therefore, HS-LDA is a generalization of Latent LDA suggests that its insight can be applied to other models that extend LDA, of experimental results of application of HS-LDA to other successful models. 3.1 Gibbs Sampling with HS-LDA The generative probabilistic HS-LDA model describes the process of document collection creation. The hidden model parameters  X  ,  X  and  X  may be estimated using a Monte Carlo procedure, which is relatively easy to implement, does not more complicated and slower algorithms [3, 12]. The rest of the section describes the derivation of an efficient sampling algorithm used to infer models parameters with HS-LDA. 
We start by framing the problem of topic discovery in terms of collections of  X  documents containing  X  topics expressed over  X  words and  X  pseudo-neutrino which can be estimated by evaluating the probability of a topic having observed both a word and a pseudo-neutrino. The posterior distribution is formally stated as 
The joint distribution  X   X   X , X , X   X  can be computed by co nsidering that Dirichlet Since  X   X   X , X , X   X   X  X   X   X  |  X , X   X   X  X  X  X  X  X  X  X  X  X  X  X  by the chain rule and since  X  and  X  are conditionally independent in our model (see Figure 2),  X   X   X  |  X , X   X   X  X  X  X  X  X  X  X  which simplifies the joint distribution as each term may be evaluated separately. Integrating out  X , X  and  X  in each term gives standard gamma function. follow the pattern in other topic modeling approaches [3, 4, 6-8, 13] and estimate  X  ,  X  and  X  by relying on the Gibbs sampling procedure. The Gibbs procedure operates by iteratively sampling all variables from their distributions conditioned on their current values and data and updating variables for each new state. The full conditional distribution  X   X   X   X   X  X  |  X   X  X   X , X ,  X  that is necessary for the Gibbs sampling algorithm is each of the Equations 1a-c are constant and values of denominators and numerators of second terms are proportional to the argume nts of their gamma functions. Therefore, the sampling equation is as follows: document excluding current assignment. Reader may notice that denominators in the first and third product terms in Equation 2 have identical counters. That is because, in the HS-LDA model, the number of words is always exactly the same as the number of neutrino emissions by process construction. 
The Gibbs sampling algorithm can be implemented in an on-line fashion by first words to topics. The algorithm operates by reconsidering data for a number of iterations during which new states of topic assignments are found using Equation 2. The algorithm is fast as the only information necessary to estimate the new state is the word, topic and neutrino counters, which can be cached and updated efficiently [12]. 3.2 Pseudo-Neutrino Detection The sampling algorithm outlined in the previous section estimates parameter values by relying on two detectable quantities  X  words and pseudo-neutrino emissions. To detect pseudo-neutrinos, which cannot be observed directly in text, we assumed a Gaussian distribution of pseudo-neutrinos in documents, as this distribution was common to many phenomena [14]. With this assumption, we could refer to all pseudo-neutrinos in a given document in a meaningful way by identifying the most common (or mean) neutrino type. That is, for  X  X  X   X  possible pseudo-neutrino types, we assumed that there existed a mean pseudo-neutrino type  X 1 X   X   X   X  X  for each document  X  . With that, a rough approximation vector of pseudo-neutrino  X  such that  X   X , X   X  X  X 
Constructing the vector  X   X  as described in the previous paragraph suggested that a meaningful approximation of document pseudo-neutrinos could be found by identifying a representative (mean) neutrino type for each document. To locate the representative flavor, we noticed that pseudo-neutrino types essentially constituted a kind of vocabulary akin to that of word s. With that, considering topics from conceptual point of view, intuitively, documents on the same topic would be close to one another in terms of similarity of their content regardless of the vocabulary used to express the content (e.g. for any language, documents about the  X  X orld Cup X  sporting number of pseudo-neutrino types was known, clustering documents into  X  clusters based on word similarity would approximate document-level (mean) neutrino types as cluster indices could be used as the neutrino type identifiers. 
To implement this intuition in practice, we searched for a clustering strategy that would perform in a scalable way while at the same time ensuring that similar documents were likely to share a cluster. We realized that by restricting  X 2 X   X  for some positive integer  X  , it would be possible to make use of Locality Sensitive Hashing (LSH)[2]. be efficiently implemented with the help of Random Projections (RP) [15]. To use LSH, we start by defining a function space  X  : X   X   X  X 0,1 X  and constructing a a random projection vector  X   X  random  X   X   X  with components that are selected at random from a Gaussian distribution  X   X  0,1  X  [16]. Each random projection is used to compute a dot-product between it and any point  X  X  X   X  allowing the mapping function to be constructed in the following way: Then, for any  X  X  X   X  , LSH hash value is constructed by invoking each of the string as a binary number, a mapping function assigns  X  to a number between one and  X  as follows: possible values is bound by 2 | X | . Recalling that  X 2 X   X  and |  X  |  X  X og  X   X  X  X  , function  X  X  X  can be used to map each point in  X   X  to a positive integer bound by  X  . 
Further, since it is proven in [17] (proof omitted here) that P  X   X   X   X  X  X  X  X  X   X   X  X  X  X   X   X 1 X   X  holds for any function hash collision for two vectors increases with the decrease to the angle between them. angle where  X  is the angle between the two vectors in radians 1 . 
Therefore, since LSH hashing allowed for fast clustering of vectors in a way that preserved document similarity, LSH was used to approximate the mean pseudo-neutrino type by treating LSH hash value as the type identifier. To make use of LSH and rewrote the sampling equation (Equation 2) in terms of LSH hash family  X  of size log  X   X  as: where  X   X   X  is the hash value of document  X  ,  X   X  X  X , X   X  X  X   X  documents with hash value  X   X   X  assigned to topic  X  excluding current assignment and  X   X . X  is the total number of words in any document assigned to topic  X  excluding current assignment. The sampling algorithm, then, proceeds as outlined in section 3.1 using Equation 3 to assign words to topics. data sets. Our first data set consisted of 1,000,000 English language messages collected from Twitter micro-blog site using its public sampling API over a period of one week. The second data set was comprised of 300,000 English language chatroom messages collected by connecting to the public irc.freenode.net public chat server and monitoring chat rooms with more than 150 chatters for the same one week period. Filtering of non-English texts was accomplished with the help of the open source language-detection 2 library. 
The language models produced by our approach were compared to those learned by On-line LDA [3] and SparseLDA [4] as these models were designed to operate efficiently on stream data. In addition, to provide a common baseline, topic models learned by HS-LDA were compared to those discovered by the classic LDA [1] algorithm. We did not evaluate our approach against TM-LDA as it required partitioning by author as well as a significant and static training sample to be collected prior to producing any output at all. These constraining requirements made TM-LDA unfit for continuous topic modeling application, which was the motivation of this work. 
To compare language models, evaluation was performed using the perplexity measure over held-out subset of data  X   X   X  X  X  X   X   X   X ,...,  X   X   X  given language model  X  and the training data calculating perplexity using the following formula: where  X | X  X   X  | ,  X   X  X  X   X  ,  X   X  is the jth term in the ith string in the held-out collection Further, to account for possible overfitting, our evaluation was validated using the 5-fold cross-validation. 4.1 Parameter Selection As pointed out in earlier works [10, 18] Locality Sensitive Hashing is highly sensitive bucket as chance of collision decreases wi th the increase of hash family size. Therefore, hash family size selection was approached from the point of view of estimating a reasonable number of buckets for the number of messages expected. 
Considering the Twitter micro-blog service as being one of the most vibrant and popular social forums today, we experimented with the numbers of English language messages that could be downloaded over a given period. Recalling the industry-oriented motivation for this work and selecting one working week as the target period (timeframe common to the industry environment) the number of messages that could be gathered from Twitter X  X  sampling service was empirically estimated to number in some millions. Realizing that if the number of hash family function was chosen to be high (ex.: 2  X  X   X 1,048,576 ) the algorithm could potentially map every message into an individual bucket, negating the entire insight of HS-LDA. With that, the reasonable number of hash functions for our experiments was chosen to be 17 ( 2  X  X   X 131,072 ) as this number would allow for variability within each cluster while at the same time providing reasonable specificity. 4.2 Experimental Setup and Results Having thus chosen the hash family size, HS-LDA was evaluated against LDA, On-line LDA and Sparse LDA using the two test datasets. For all models, the number of topics was chosen to be 100 and experimented with various hyperparameter settings. Results reported here were for hyperparameter values of  X 0.05  X  ,  X 0.05  X  and  X 1 X  as these values produced best results for all models. readable graphic, the Simple Moving Average (SMA) smoothing technique was applied to raw results, setting the moving average window set to 10,000. 
While cross-model comparison shows that HS-LDA approach outperformed other models in terms of perplexity, performance of LDA-type models could be sensitive to parameter choices [19]. Since some paramete r choices could be more beneficial to performances of some frameworks and less so for others and since all models used for evaluation were derivative of the classic LDA model, we applied the insight of the HS-LDA approach to each test algorithm and conducted a pairwise comparison in terms of perplexity, thus controlling for model parameter sensitivity. Figures 5-6 show pairwise comparisons for each test model with the same approach augmented with HS-LDA (LDA/HS-LDA pairwise comparison is not reported as it can be found in Figure 4). 
To summarize results in numerical way, average perplexities are reported for all tested models in Table 1. The purpose of this report is to identify the model with the highest predictive prowess as well as to quantify amount of improvement in terms of percentages. Model Average Perplexity (Twitter) Average Perplexity (IRC) On-Line LDA 2773.99 1835.74 Sparse LDA 2860.27 1998.53 HS-LDA 1803.67 1023.12 
In Table 1, HS-LDA outperformed other models by at least approximately 12% for the Twitter dataset and 21% for the IRC chatroom data. Significantly better predictive power of resulting topic models learned from the chatroom discourse may be explained by noting that chatrooms are often oriented towards particular themes, thus Twitter where the discourse is entirely unstructured, making the job of theme discovery more difficult. To improve the quality of topic models learned from social media streams, we introduced the new HS-LDA model for topic modeling, which was a generalization of the well-known LDA topic discovery technique. We experimented on large data sets collected from popular social media services and showed that our model outperformed other state-of-the-art stream topic modeling techniques in all cases. Further, we enhanced other topic modeling approaches with the insight of HS-LDA and showed that applying core notions of HS-LDA to other techniques improves their performance in terms of predictive power of resulting topic models. 
While our results showed improvement in all cases where HS-LDA insight was used, combining HS-LDA with other models aimed at preserving global context did not immediately result in substantial performance gains. It seems, however, that such a combination has merit and we will continue this investigation in the future work. 
Further, while this work was instrumental in moving towards the goal of constructing an industry-grade stream topic monitoring system, one of the major hurdles for constructing such a system with HS-LDA was the necessity to specify the number of topics. In our future work, we plan to investigate topic modeling approaches based on the popular Chinese Restaurant Process paradigm and will attempt to apply the insight of HS-LDA to dynamically discovered topic allocations. 
