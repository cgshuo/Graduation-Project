 Ensemble methods (EMs) have proved their efficiency in data mining, especially in supervised machine learning (ML). An EM generates a set of classifiers using one or several machine learning algorithms (MLA) and aggregates them into a Many papers [3,18,2,14] have shown that a set of classifiers produces a better prediction than the best among them, re gardless of the MLA used. Theoretical and experimental results have encourage d the implementation of EM techniques recommender systems [9] and many others too numerous to mention here. The efficiency of EMs lies in the fact that aggregating different and independent key concepts for effective classifiers.

Instance based (IB) MLAs such as k-Nearest Neighbors (kNN) are very popu-of k . Thus, using the kNN principle as an EM algorithm is immediate. How-problem, we can use approaches based on neighborhood graphs as alternatives. For example, Relative Neighborhood Graphs (RNG) or Gabriel Graphs (GG) are  X  X ood X  candidates. Like kNN , for an unlabeled observation, the classifier, based on neighborhood graphs, assigns a label according to the labels in the neighborhood. As an example, we can simply use the majority rule vote in the neighborhood of the unlabeled observation. While there have been many studies using kNN in the context of EM, we did not find any study that assesses the ad-vantages of such neighborhood graphs, based more particularly on RNGs, in EM approaches. In this paper, we propose an EM approach based on neighborhood graphs. We provide comparisons with many EM approaches based on kSVM, Decision Tree (Random Forest), kNN etc. We carried out our experiments on an R platform.
 notations and definitions. In section 3, we introduce the EMs based on neigh-borhoods. Besides the classic kNN neighborhood, we will present RNG and GG neighborhoods. Section 4 is devoted to evaluations and comparisons. Section 5 provides the main conclusions of this study. 2.1 Notations Let  X  be a set of individuals represented by p attributes X j ,j =1 ,...,p in a representing space , and a membership class Y  X  X  = { y 1 ,...,y K } .Let X be the function mapping an individual to its representation and Y the function mapping to the class :  X  indistinguishably.
 For the sake of illustration, we will use the toy example shown in Table 1. This is a two-class data set of 17 individuals mapped into a two-dimensional class y 2 = 2 by an empty dot.
The goal of any machine learning algorithm is to produce a classifier capable the knowledge we can obtain on the probability distribution: By thresholding at the maximum value for this vector, the membership class can be represented by a zero vector except for the most likely class by a value for an individual  X  is  X  ( X (  X  ))  X  X  y 1 ,...,y k ,... } . 2.2 Neighborhood Structure There are many types of neighborhood that can be used to build a classifier. Among the most well known are:  X  The well-known k -nearest neighbors;  X  The  X  -neighbors, which are defined by the subset of E l that are in the ball  X  The neighborhood regions brought about by a decision tree where each leaf  X  Parzen s window neighbors;  X  The neighbors in random spaces. For example, we can cite the weak models  X  The neighbors in the sense of a s pecific property. For example, Gabriel 2.3 Neighborhood Classifiers The neighborhood classifiers depend on three components : 1. Neighborhood set P : the set of all subsets of E l . This is the set of all 2. The neighborhood function V : this defines the way in which an individual 3. The decision rule C : this leads to probability distribution of the classes Hence, we can define a neighborhood classifier  X  as based on a combination of the triplet ( P , V ,C ): 2.4 Partition by Neighborhood Graphs Here we focus on geometrical graphs, We thus build P using neighborhood graphs, such as Voronoi diagrams [13] or their dual (the Delaunay poly-hedral), Gabriel graphs [10], relative neighbors graphs [16] or the minimum erty. Below we give the properties that define RNGs and GGs:
For a given distance measure d , a learning sample E l and a set of individuals  X  , X  2 ,... , any two points  X  i and  X  j are linked by the following rules :  X  Gabriel graph (GG) :  X  Relative neighbors graph (RNG) : All these geometric structures induce a related neighborhood graph with a sym-metric neighborhood relationship. Figures 1 and 2 show the neighbor structures of the relative neighbor graph and the Gabriel graph, using the dataset intro-duced above (cf 2.1). We call this framework  X  X andom Neighborhood Classifier (RNC) X . The principle ofEMsistogenerate M classifiers and then aggregate them into one (see 3). To do so, M randomized iterat ions are performed. At iteration m ,RNC: 1. generates a new learning set E m l with a given size; 2. generates a new classifier  X  m =( P m , V m ,C m ); 3. uses the generated classifier to det ermine the membership class of the un-Following these steps, the RNC then aggregates the M predicted values related to an unclassified individual to determine its final membership class. The two key points in this procedure are the sampling procedure for generating the M some details of the two key points: 3.1 Sampling Procedures From the training data set E l which is an n  X  p table of values, we carry out M random samples.
 The sampling can be achieved in different ways:  X  Sampling on rows with or  X  Sampling on columns;  X  Building new columns by a  X  Generating new individuals  X  Randomly adding x %ofrows Each sample produced leads to a specific classifier. 3.2 Aggregating Function Generally, the aggregating func-tionisbasedonthemajorityrule  X  Average vector where the score for each class is the mean of the answers for  X  Weighted version (majority or mean)  X  Maximum Likelihood calculated as the product of the answers for all the  X  Naive Bayes [15].  X  Decision Templates [15]. This method is based on the concept of a decision  X  Linear regression. In this method, we assume that the probability of a class To assess the performance of RNC, we ca rried out many experiments on differ-ent data sets taken from the UCI Irvine repository. For this, we made a number of distinctions depending on the type of neighborhood used. As our work was motivated by the absence of studies on EMs based on geometrical graphs such as RNGs, we designed two separate experiments for RNC. One was based on RNGs and the other on kNN where k = 1, 2, 3. The comparison was also extended to random forests (RFs), K support vector machines (KSVMs), Adaboost, dis-criminant analysis (DA), logistic regression (RegLog) and C4.5. All experiments were carried out using R software. 4.1 Implementation of RNC In our test, RNC use Relative Neighborhood classifiers. Each iteration uses the following scheme (see Figure 3) :  X  Sampling : describing variables and individuals from learning sets are sam- X  X earning : Mahalanobis distance between individuals is computed to con- X  Simple classification :  X  X gregation : the results are aggregated using Decision Templates (DT). 4.2 Other Methods The implementation of RF is the randomForest library using 500 trees. We used the R kernlab library to apply the KSVM algorithm with classification type C-svc. For DA and RegLog, we used the lda and glm functions of the R MASS the Weka learner.

For kNN, we simply replaced the neighborhood graph with the k-nearest neighbors in the RNC algorithm using the same distances and the same number tested:k=1,2or3. 4.3 The Test mentioned above. For each experiment, we applied 10-Cross Validations to obtain to evaluate the results.

The results are shown in Table 2. For each dataset, we computed the average error rate, the rank of each method amo ng the others and the p-values detected by the Wilcoxon test.

As can be seen in Table 2, RNCs based on RNGs performed well in comparison to kNN as well as in comparison to the other methods. Indeed, from the 14 data sets, RNC placed first once, second 6 tim es and third 4 times. RNCs were thus one of the 3 top methods in most cases.

We also computed the mean rank. Th is was done twice, using or not the classifiers adaBoost and Logistic Regression (which could not give an answer for more than 2 classes). The results are shown in Table 3 where the RNCs man test [4], which showed a difference with a p-value of 6 . 849  X  10  X  6 and the post-hoc test (comparing each classifier by pair) gave the results shown in Table 4.

These results are very encouraging, b ecause we believe that they can be im-proved by varying certain parameters such as:  X  The choice of neighborhood structure, especially as we know that the neigh- X  The type of base classifier. Should we use the closest connected homogeneous  X  The selection methods to improve the quality of the data sets or the classi-All these issues are currently being studied and should produce significant im-provements for RNCs based on geometrical graphs. 4.4 Computational Analysis The theoretical complexity for graph computation using n individuals repre-it is necessary to test the RNG condition : each for Mahalanobis ) and compare the distance d (  X  1 , X  2) with all distance d (  X  1 , X  )dans d (  X  2 , X  ) for each individuals  X  .

But optimization can be carried out :  X  Distance can be computed us ing matrix representation and a powerful linear  X  Using the RNG condition, it is only necessary to test 1 for all  X  such that using k-NN. For example with k=3, and the data set Twonorm (almost 2000 individuals), it takes 40s for k-NN method and 1min44s for the RNG method to compute N=100 classifiers. These tests use the BLAS library atlas ,ona Intel TM Core TM i5 2.60GHz computer with 4G memory. Here we have provided a new approach for using neighborhood structures in ensemble methods. The results obtained show that they are challenging the most powerful techniques such as random forests and kSVM. Methods based on geometrical neighborhood graphs outperform the classic methods such as kNN . There are many possibilities for improving RNC based on RNG. A library the authors.
