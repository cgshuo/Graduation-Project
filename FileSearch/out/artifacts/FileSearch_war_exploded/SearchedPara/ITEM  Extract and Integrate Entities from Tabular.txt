 tions with high agility and flexibility. RDF can maintain all the data X  X  relations, auto-matically build tables for RDBMS deployment, navigate graphically, support multiple navigation trees, and link across diverse content sets [3]. There have been many RDF Knowledge Bases ( RDF KB for short), such as LUBM, tracted from a very few domains even a single domain. For example, YAGO is mainly created using the Wikipedia and WordNet X  X  data. Furthermore, these RDF 
In order to use RDF for knowledge management, we should have large RDF existing RDF datasets. 
An intuitive approach is to integrate all the existing RDF KB into an entire bigger one [6]. But this approach will have some problems. Firstly, the same data may have many copies in different RDF datasets with different accuracy. It is hard to determine which copy should be the correct one. Secondly, it is difficult to design schema map-ently. And their data models are heterogeneous, such as data semantics, expression of data with same semantic, environments of data sources, etc [7]. [10] enables users to collaborate effectively on data management in the cloud, so we the following example. birthday, etc. And we may think the last column may be not useful. Ibra. and C.Ronaldo already exist in this RDF KB. But some other players in Table 1 integrated into the RDF KB, and then the RDF KB will be enlarged. ITEM ( extract entities from relational tables and integrate them into an RDF KB. 
Our contributions in this paper are listed below: i. We propose a novel tool, ITEM, for enlarging RDF KB by extracting entities ii. We propose an efficient algorithm for matching tuples with entities. iii. We efficiently find the optimal schema mapping between a table and an RDF iv. We examine our proposed method by conducting experiments over real analyze related works. Finally, we conclude this paper and discuss some future work in Section 6. formally define the problem focused by us. 2.1 Schemas of Table and RDF Knowledge Base = to denote the n attributes of T . Also, ) ( t  X  X  th i attribute. 
An RDF knowledge base , denoted by D , is a collection of entities. Each entity is a = to denote its m predicates, in which larly, ) ( i P e means the object corresponding to the th i predicate of e . 2.2 Mappings between Table and RDF KB Definition 1 . A mapping between a tuple and an entity .
 P . Hence, a mapping must contain an edge connecting to the entity X  X  name. Definition 2. The optimal mapping between a tuple and an entity .
 the score of a mapping ) , ( e t M as follows: among all mappings between t and e . When there are multiple mappings sharing the same largest score, we just randomly choose one as the optimal mapping. any other entities in this KB. 
Formally, e is a match of t in D , iff For the case that the score of the mapping between t and its match equals to zero, we say t can X  X  match with this KB. Definition 4. A schema mapping between a table T and an RDF KB D , denoted by  X  , is also a matching in a complete bipartite graph. schema mapping also has a ) ( t score  X  which is equal to )) , ( ( e t M score opt . Definition 5. Paradoxical schema mappings between a table and an RDF KB . following two conditions is satisfied: mergeable . We can merge this two schema mappings into a bigger one as follows: and ) ' ' ( ) ' ( ) (  X  +  X  =  X  score score score . Definition 6. The optimal schema mapping between a table T and a KB D . mapping t  X  . Let } | { T t S i t M mappings in S are not paradoxical. Given  X  mapping ) , ( D T opt  X  is the schema mapping such that Finally, the problem focused by us is: according to this optimal schema mapping, into D . In this section, we begin with a brief description of the overall system architecture of compute the mapping between a tuple and an entity, and how to efficiently compute the optimal schema mapping between a table and a KB if given each tuple X  X  optimal mapping with the KB. 3.1 System Architecture main parts. The first part is a crawler, which can crawl over the Web and collect the pages containing relational data. The second part is the main part of the system, which can generate the optimal schema mapping between a table and the KB. The third part them into the KB. Algorithm 1. Vector&lt;Entity&gt; ExtractEntitiesFromTable(T, D) 1 create M opt [T.size()] to store T X  X  optimal mappings with D 2 for (each tuple t in T) { 3 initialize t X  X  optimal mapping M opt [t] X  X  score to zero 4 Vector&lt;Entity&gt; vre = findAllRelatedEntities(t); 5 for (each entity e in vre) { 6 compute similarities between t X  X  attributes and e X  X  objects 7 construct a weighted bipartite graph G 1 8 invoke KM algorithm to find the matching M 9 if (score(M) &gt; score(M opt [t])) 10 M opt [t] = M 11 } 12 } 13 14 construct all the schema mapping S[] according to M opt [] 15 compute all the paradoxical schema mappings in S[] 16 construct a graph G 2 according to the paradoxical schema mappings 17 invoke MWIS algorithm in G 2 to find the optimal schema mapping S opt in S[] 18 20 21 return VE; 
In the second part, the Tuple Extractor will extract each tuple from the collected ta-between a tuple and the KB. After collecting all the optimal mappings between each tuple and KB, the Mapper2 will generate the final schema mapping between the table and the KB, and output it. The framework of ITEM is illustrated in Algorithm 1. We will introduce the details in the following sections. 3.2 Compute Similarity of Attributes and Objects card. For number strings, we use the absolute difference. And for time-format strings, we use the time interval between those two times. 
If two strings both fall into the same category, we use the comparator of that cate-similarity in order to let each value between zero and one. 3.3 Filter Out the Entities can X  X  Be the Match In order to compute the mapping between a tuple and the KB, we should compute the the one with the largest score. There are a large number of entities in KB, but only a similarity of every possible mapping pair. We should filter out the non-related or less-related entities to improve efficiency. 
In our method, we construct a gram based inverted index for all the entities. We re-word, we can then use this inverted index to find the entities containing this keyword. 
We regard each attribute of a tuple as a keyword, then perform keywords search us-entities are irrelevant ones that don X  X  need further work. 3.4 Find the Optimal Mapping of a Tuple and an Entity each entity among them, we can find its mapping, and then find the optimal mapping to these similar entities. weighted bipartite graph, as shown in Figur e 4. In this graph, each attribute and each object are regarded as vertices, and we a dd a weighted edge to each attribute and each object, the weight is the similarity of the attribute and object. modifition. 3.5 Find the Optimal Schema Mapping of a Table and KB We now have all the optimal mappings for all the tuples in the given table. We should merge all these mappings and find the optimal mapping between a table and a KB. 
For this objective, we generate a graph as follows. Each mapping is regarded as a vertex which has a weight indicating the score of this mapping computed by Mapper 1 . Next, for each pair of mappings, if thes e two mappings are paradoxical, we add an edge between these two vertices. Then we shou ld find a subset of the vertices set, and adopt a greedy algorithm to solve it. example, supposing we have six schema mappings, we will generate the optimal mapping  X  from them. 
Firstly, we generate a graph with six vertices which is 1 M to 6 M . For each vertex, ping. For example, 20 . 2 ) ( 1 1 = = M score W for vertex 1 M . the maximum weight independent set in the graph generated above. We adopt a move all the vertices which are adjacent to th e vertex. And then repeat this step until which is shown in the bottom of Figure 5. 
Lastly, we will merge all the four schema mappings to one mapping. We extract all the different matches in the four mappings, and put them together to form the optimal schema mapping  X  , which is shown on the upper right part of Figure 5. 4.1 Experiment Setup All experiments are conducted on a PC with Intel 2.0GHz Core 2 Duo CPU and 2GB memory under Windows Vista Home Basic operating system. All programs are im-plemented in Java. Google Fusion Tables is a cloud-based serv ice for data management and integration. KML). Users can keep the data private, or make it public. [16] Since Google Fusion 200K+ currently. 
We download all the public tables of Google Fusion Tables. From them, we randomly about 27.96% are plain strings, 0.66% is date strings, and 71.38% are number strings. 
We use YAGO as our RDF Knowledge Base [5]. There are 2,796,962 entities, and 92 different predicates. And the average numbe r of predicates in an entity is 8.46. 4.2 Experiment Results Analysis Effectiveness: lated to the name predicate. Then 78 tables can be integrated among all the 100 tables. Hence, about 4,000 tuples find their match entities in YAGO. By learning their map-pings, we transfer all other tuples into en tities and insert them into YAGO. Therefore, our method can effectively enlarge RDF KB by automatically mapping existing rela-tional data with RDF KB. more than 90% matches are correct. ITEM. After review them one by one, we find that one major shortcoming of ITEM is that ITEM can X  X  match a predicate with multiple attributes. For example, there are two indicate the whole name of a person. We will address this issue in our future work. Efficiency: As shown in Sec. 3.3, we construct an inverted list to find the entities which are pos-sible to be the matches of a given tuple. Specifically, we adopt a token-based index, examines all mappings between a tuple with all entities in KB. The result is shown in Figure 6. Clearly, gram_invert performs much better than baseline . of two schemas. [11] There are many approaches on schema matching. According to a matching approaches. Furthermore, we can also combine many different matchers as types such as Cupid, SEMINT, COMA++ [13]. 
Almost all these methods are schema-level. And the previous instance-level ap-Our method considers each tuple as a unit for single matching, and derives spastically matching of the whole table and the KB by aggregating all single matching of tuples. mapping rather than considers their instances. And they only transform data between relations and RDF KBs. They therefore don X  X  have the ability to extract entities from relation tables. 
The problem is also relevant to the set expansion problem [15]. Given a set of enti-ties in an RDF KB, we want to find even more entities which can be added to this set. more sense if we want to incrementally enlarge the knowledge base. In this paper, we propose ITEM, a tool to extract and integrate entities from relation between table and RDF knowledge base. Finally, we give some experiments to show the effectiveness and efficiency of ITEM. 
In the future, we want to implement our system as a web service and develop a web the multi-column matching problem shown in our experiment analysis. 
