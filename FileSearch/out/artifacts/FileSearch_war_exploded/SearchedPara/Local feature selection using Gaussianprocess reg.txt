 Pontificia Universidad Cat X lica de Chile, Santiago, Chile 1. Introduction
Performing classification in high dimensional spaces has strong limitations for most classification models, mainly due to the Curse of Dimensionality problem and the usual high computational load [3, 32]. In case of distance based models it is hard to reach high levels of accuracy since distance metrics lose part of their discriminative power in high dimensions [3]. In case of probabilistic models that esti-mate class conditional density functions, the ability to perform a good fit using a training set decreases exponentially with the number of dimensions [5].

Traditional dimensionality reduction techniques, such as feature selection and feature transformation, partially overcome the previous problem [18,22]. Global feature selection removes irrelevant and redun-dant dimensions by finding a global subset of features where all the data instances are projected. Feature transformation reduces dimensionality by summarizing a dataset usually by means of algebraic combi-nations of dimensions. As a relevant weakness, in both cases all the data instances are projected onto the same subspace in order to improve classification. Furthermore, in the case of feature transformation, the resulting new features usually do not have a clear interpretation.

In many applications, particularly in high dimensions, projecting all data instances into the same sub-space does not produce satisfactory results. This is particularly relevant in cases where the distinctive properties of each instance are associated to a specific subset of features. For example, consider the case of face recognition where each face has usually a particular subset of highly distinctive visual fea-tures [15], such as a particular eyes or nose shape; or consider the case of gene function prediction where each gene usually exhibits distinctive expression levels only under a particular subset of experimental conditions [36]. In such cases, the identification of a particular local subset of discriminative features to characterize each data instance appears as an attractive alternative to a global feature selection scheme. Unfortunately, an exhaustive search for suitable local subspaces is generally not possible, particularly in high dimensions, where in a n-dimensional space there are 2 n  X  1 possible subsets of features to characterize each data instance.

In this paper, we present a local feature selection method that for each data instance selects a subset of discriminative features, such that by projecting each instance to the respective subspace it is possible to facilitate its classification. Our method starts by constructing for each available feature a function that estimates its discriminativity with respect to the target classes. We estimate these functions introducing a score for discriminativity and Gaussian Process (GP) regressions [29,38]. After learning the GP regres-sions for all features, we can obtain a discriminative subspace for each possible input by locally joining the most discriminative and non-redundant features. As a relevant implementation issue, we reduce the computational complexity of the regular implementation of a GP regression by introducing an iterative optimization method that alternates between obtaining informative observations for the GPs and finding suitable values for their hyperparameters.

Accordingly, the main contributions of this work are: i) A new local feature selection method that for each possible input provides a particular subset of discriminative features, ii) A new view that casts the feature selection problem as a regression task based on GP regressions and a suitable score to access feature discriminativity, iii) A new optimization method that uses an iterative strategy to efficiently solve GP regressions.

This paper is organized as follows. Section 2 presents a brief overview of GPs. Section 3 describes related work on global and local feature selection techniques. Section 4 presents the main details of our method. Section 5 shows experimental results. Finally, Section 6 presents the main conclusions of this work. 2. Gaussian processes
A GP is a collection of random variables V = { v 1 ,...,v n } such that any finite subset of variables from V has a joint Gaussian distribution. GPs have proven to be a powerful tool for regression, being able to model smooth functions and to provide a measure of uncertainty about the estimations [38]. Specifically, a GP is fully specified by a mean function M ( x ) and a covariance function K ( x i ,x j ) , becoming a distribution over functions of possible infinite dimensional mean vector and covariance matrix. In a standard regression problem, the main goal is to estimate a function F ( x ): x  X  X  X  y  X  Y given a set of observations A y  X  Y forasetofvalues A x  X  X . In a GP regression problem, the jointly Gaussian distributed variables are any subset of predictions S y  X  Y , with mean M ( S y ) and covariance matrix K ij = K ( S x i ,S x j ) ,where S x  X  X are the respective preimages of S y under F .Covariance function K ( x i ,x j ) typically depends on a set of hyperparameters that have to be estimated. Note that the covariance functions for the output variables S y depend only on the input variables S x . The posterior mean and variance of S y given observations A y with respective points A x are [29,38]: to note that the posterior variance of S y does not depend on the observations A y .Inotherwords,the uncertainty about the predicted values for S y only depends on the points A x related to the observations A 3. Related work
An extensive literature presents a large list of me thods to select global subsets of features for classifi-cation [12,17,21,22,26].

In supervised scenarios, common approaches are divided into two main types: filters and wrap-pers [17]. Filter approaches perform feature selection as a preprocessing step where features are first scored and ranked using a statistical criterium. Afterwards, features with highest ranking are selected. Commonly used statistical criteria include mutual information [9], Pearson correlation [14], Fisher score [8], and chi-square statistic [27], among others [12]. In particular, probabilistic techniques have been a popular alternative among filter approaches [14,23,28].

Close to our ideas, several filter approaches aim to directly measure the discriminative properties of each feature to classify input instances. Guided by this idea, and in the context of a binary classification problem, the Relief algorithm [21] ranks features according to the values of a set of weights. These weights quantify discriminativity by using a set of samples to find their nearest elements from the same and different class (nearest-hit and nearest-miss, respectively). Dimensions where positive instances are, in average, close to other positive instances and far from negative instances receive higher weights. Relief-F [24] extends the Relief algorithm to the case of multiple classes and improves robustness by considering k nearest misses and hits. Regressional Relief-F (R-Relief-F) [40] introduces a probabilistic criterium by estimating the probability that the predicted class values of two instances are different. In contrast to the method presented in this paper, Relief approaches are based on a global feature selection strategy and do not consider the case of redundant dimensions.

In terms of wrapper methods, they use a specific classifier to guide the feature selection process [17, 22]. For different subsets of features, wrapper methods execute the classification model and evaluate the resulting predictive power [22], u sually using accuracy or F-measur e [46]. Then, the s ubset of f eatures with greatest predictive power is selected. Main issues associated with these methods are difficulties defining the best metric to measure predictive power, as well as, the computational complexity associated to the evaluation of a large number of possible subsets of features. Furthermore, a search based on a particular classifier usually increases the risk to suffer overfitting problems [22].

It terms of wrapper methods, it is common to find ad hoc heuristics related to the feature selection cri-variables that introduce dependencies and using a leave-one-out cross validation strategy to chose suit-able subsets of features. Also in the context of a Naive Bayes classifier, in [34] the authors search for dependencies among features by identifying groups of variables that share dependencies and selecting the features represented by each of these groups. In the context of a K-nearest neighbors (K-NNs) clas-sifier, in [43] the authors introduce a Monte Carlo technique to choose training instances and features. Support Vector Machines (SVMs) have also been used for feature selection. In [13] a recursive wrapper feature elimination method is proposed by using a SVM classifier to recursively eliminate features that produce a minimal contribution to the margin of the classifier. In all previous cases, feature selection is based on global scores that do not consider the case of a local feature selection scheme.
Embedded methods correspond to a variant of wrappers methods for feature selection. These methods combine feature selection and model fitting in a single optimization problem. Examples of embedded methods are decision trees [37] where each branch of a tree classify a subset of instances using a particu-lar subset of features, or Adaboost algorithm [10] where decision stumps can be used as weak classifiers to build an ensamble that performs classification using only a selected subset of features [47]. SVM classifiers have also been used as underlying models for embedded approaches [4,33]. In [4], the authors propose an iterative method called Successive Linearization Algorithm to minimize the zero norm of the standard weights vector in the SVM optimization. In [33], the authors modify the regular cost function of a SVM model by adding a term that encourages sparse solutions based only on a subset of the avail-able features. This strategy is similar to the popular Lasso technique [45] used to simultaneously achieve feature selection and model fitting in the context of logistic regression.

Distance metric learning can also be cast as a feature selection approach, where usually a distance matrix is learned in order to improve classification. Commonly, these methods find linear transformations of the input space that minimize distances among elements from the same class and maximize distances among elements from different classes. In [41] the authors propose a Pseudometric Online Learning Algorithm called POLA. POLA receives at each step pairs of input instances and attempts to learn a distance metric M and a scalar threshold b , such that pairs from the same class are at most b  X  1 units apart while pairs from different classes are at least b +1 units apart. Distance metric M and threshold b are updated after each step to correct any violation of the desired relations. A related distance metric algorithm is Neighborhood Component Analysis (NCA) [11], this technique computes the expected leave-one-out classification error achieved by a stochastic variant of a K-NNs classifier. This stochastic classifier uses a Mahalanobis distance metric parameterized by a linear transformation that minimizes expected classification error. Unfortunately, the previous algorithms obtain poor results in cases where input instances from the same class display multimodal patterns. In [44], the authors propose a Local Fisher Discriminant Analysis technique to deal with multimodal data. The model merges ideas from an unsupervised technique called Locally Preserved Projections (LPP) [16] and from Fisher discriminant analysis. In a more recent approach, in [48] the authors propose a method named Large Margin Nearest Neighbor (LMNN) classifier. As a main feature, the LMNN classifier uses a mixture of loss functions that are expressed in terms of positive semidefinite distance matrices. These distance matrices are associated to different classes and they can be estimated in closed-form. Several other distance metric learning approaches have been proposed [2,6,42,49]. In contrast to our work, all these methods find a global distance matrix that is applied over all the input space.

There are several other approaches that can be cast as feature selection techniques. For example, in object recognition by image analysis, the method proposed in [35] groups the images taken from a spe-cific view and build a feature space for each view. This idea is extended in [20] with the Locally Linear Discriminant Analysis (LLDA). LLDA finds linear transformations that map data points to lower dimen-sional spaces where inter-class and intra-class covariances are maximized and minimized, respectively. Closely related to our approach, but in the context of face recognition, the Optimal Local Basis method (OLB) [15] uses a reinforcement learning approach to find a particular local feature subspace to char-acterize the face of each individual in the training set. These local subspaces are then used by a K-NNs classifier to achieve face recognition. As a main drawback, the need to repeatedly execute a slow rein-forcement learning algorithm to find a local basis for each input instance limits the ability of this method to scale properly with the size of the training set. As a main difference between OLB and our approach, our model builds a function that is able to infer a set of relevant local features for any position of the input space, not only for the positions of the training instances.
 4. Our approach
A key step of our approach consists of building for each available feature a GP regression that es-timates the discriminative power of each feature over its complete input range. Afterwards, by locally joining these regressions we can achieve local feature selection for any position of the input space. Fig-and F 3 . In the Figure, we add to each regression an observation corresponding to the discriminative score with respect to the class value of input instance x i while F 3 is not. Consequently, we add a high scoring observation to GP ( F 1 ) and a low scoring observation to GP ( F 3 ) (black dots).

Next, we present the main components of our approach. First, we describe the score used to evaluate the discriminative power of each individual feature. Then, we present our strategy to efficiently build the GP regression associated to each feature. Afterwards, we describe our technique to locally join these regressions in order to achieve local feature selection. Then, we propose a classification scheme based on the selected features. Finally, we present a complexity analysis of the running time of the proposed method. 4.1. Discriminative score
In this work we measure the discriminative power of each feature with a score that uses the very well known near-hit and near-miss strategy from Relief algorithm [21,24]. Let x i = [ x i (1) ,x i (2) ,...,x i ( N ) ,C x i ] , i j  X  [1 ...N ] be a feature. The discriminative score associated to feature j with respect to input instance x i depends on the distance along dimension j among x i and elements from its own (near-hits) and dif-input location x i ( j ) is defined as:  X  is a normalization constant that keeps the discriminative score in the range 0 to 1. Function z ( x i ,x k ) accounts for hits and misses according to: K (  X  ) is a Gaussian kernel that assigns more relevance to instances in the neighborhood of x i along dimension j . Specifically, where and Function d j (  X  ,  X  ) can be extended to other types of variables: ordinal, binary, scaled, etc. 4.2. Construction of GP regressions
As we mentioned before, we use GP regressions to estimate for each possible input the value of the discriminative score associated to each of its features. Unfortunately, there are important limitations to the number of observations that is possible to consider in a GP regression. Specifically, for n observations in a regression model, the training cost to estimate a GP is O ( n 3 ) . In our case, this computational complexity is even more stringent because to obtain each observation we need to calculate the score given by Eq. (3). As an additional inconvenience, we also need to estimate suitable hyperparameters for each GP. Fortunately, a GP not only estimates a regression but also provides uncertainty estimates of its predictions over the complete input space. This property facilitates the practical implementation of our method by allowing us to implement efficient strategies to select informative observations to estimate each GP and its hyperparameters.

Specifically, to obtain each GP regression we need to solve two main problems: i) Determination of the set of instances to be included as observations, and ii) Determination of values for the kernel hyperparameters. This is a chicken-and-egg problem because we need hyperparameter values to select informative observations and at the same time we also need observations to estimate the value of the hy-perparameters. We solve this problem by using an iterative strategy that resembles the operation of the Expectation Maximization (EM) algorithm [7]. Under this strategy, we start the iterations by randomly selecting initial values for the GP hyperparameters. Afterwards, we sequentially iterate the following two steps: Step-1: Selecting observations given GP hyperparameters.

We base our policy to select observations in a method proposed in [30]. In that work, McKay et al. show that for a fixed covariance function and known values for the GP hyperparameters, it is possible to obtain a suitable estimation of the GP by greedily selecting observations according to the position with highest posterior variance. This posterior variance can be calculated using Eq. (2). As a main advantage, this strategy requires only one data scan to determine the next point to be observed. Furthermore, using this strategy we can also determine a suitable number of observations to be included in each GP regres-sion by monitoring the evolution of the corresponding mean squared error (MSE). As it can be seen in Algorithm 1, we add observations to each GP until the corresponding MSE value does not present a significant change with the addition of further observations.
 Step-2: Estimating GP hyperparameters given selected observations.

Given a set of observations, a suitable method to determine the kernel hyperparameters of a GP is the maximum likelihood (ML) approach. As shown in [38], an efficient strategy to perform this ML estimation is to use a conjugate gradient technique that optimizes the marginal log-likelihood given a set of observations. Specifically, considering a set A y of n observations relative to a set of data points A x , the marginal log-likelihood, as a function of the hyperparameters  X  of a GP, is given by:
The first term in the right hand side of Eq. (4) corresponds to the data fit, i.e., how the regression fits the observations. The second term corresponds to a complexity penalty that depends on the number of inputs and the covariance function. Finally, the third term corresponds to a normalization constant.
In this work, we use the conjugate gradient method to numerically find  X  that maximizes Eq. (4). In particular, we use a covariance function based on a squared exponential isotropic function with Gaussian noise, a common choice for machine learning problems [38]. This covariance function is given by: where P = l 2 I with I the identity matrix,  X  is the signal variance,  X  2 n is the noise variance, l is the characteristic length scale, and  X  pq is the Kronecker delta which is one if p = q and zero otherwise.
Algorithm 1 summarizes the operation of the previous two steps to estimate hyperparameters and to select observations for each GP. The algorithm iterates until the values of the hyperparameters do not suffer a significant change between consecutive iterations. We empirically find a suitable threshold to stop the iterations.

We can guarantee convergence of Algorithm 1 by showing that at each of its steps the likelihood function in Eq. (4) always increases and that this function has a finite bound. In terms of step-1 of Algorithm 1, we can consider that the next selected observation always reduces the variance of the re-gression. As a consequence, this also reduces the penalization term in Eq. (4), which in turns always increases the log-likelihood. In terms of step-2 of Algorithm 1, this is a maximization step based on the conjugate gradient method, therefore, the corresponding log-likelihood always increases until conver-gence. In terms of a finite bound for the likelihood function, the use of a kernel that satisfies Mercer X  X  condition guarantee this constraint [38]. Algorithm 1 : Strategy to select observations and to estimate GP hyperparameters.
 4.3. Construction of local discriminative subspaces
Once we have estimated a GP regression for each available feature, we can use these regressions to select a specific discriminative subspace for every possible input. As we describe next, we achieve this by locally selecting features that exhibit high discriminativity according to the regressions.
At each position of the input space each GP regression provides a Gaussian estimation of the dis-criminative score associated to the respective feature. As a consequence, the process of locally select-ing discriminative features needs to consider: i) Local values of the discriminativity scores given by the local means of the GP regressions, and ii) Level of uncertainty in the estimations given by the lo-cal variances of the GP regressions. We achieve this by introducing a reference Gaussian distribution F Gaussian distributions associated to different features and input locations. In particular, we choose F ref as a Gaussian distribution associated to poor discriminative properties. This is equivalent to assign low values to  X  ref and  X  ref . In this way, features whose local Gaussian distribution are more similar to F ref are locally less discriminative, therefore they receive a lower position in the local rank that quantifies discriminativity. It is important to note that we compare all local regressions against the same reference distribution, therefore, as observed in our experiments, the specific values assigned to  X  ref and  X  ref are not a critical decision. In particular, in this work we use the Kullback Leibler (KL) divergence to score similarity between probability distributions.

Figure 2 provides further insights behind the idea of using a reference distribution to locally rank features according to discriminativity. In this case there are two hypothetical features j and k , such that, reference distribution F ref . As shown in Fig. 2, this implies that feature j is more discriminative than feature k with respect to input instance x i .

As an additional characteristic, we select features sequentially avoiding to include features that are redundant with respect to features previously selected. Formally, let DS x i be a discriminative subspace score: is the Pearson correlation between features j and k . Constant  X  is used to normalize the sum in order to compare scores.

Algorithm 2 presents pseudocode for the resulting procedure to estimate a local discriminative sub-space DS x i for an input instance x i . Note that  X  is a parameter that regulates the number of features selected. In Section 5.2 we analyze the sensibility of our results with respect to the value of this param-eter. 4.4. Classification of new data instances
For a new test instance x i , we first build its local discriminative subspace DS xi by using the method described in Section 4.3. Afterwards, we project all the training instances to DS xi , and we apply a K -NN classifier in this subspace using Euclidean distance. 4.5. Running time of the method
At training time, the selection of each new observation to be included in a GP regression needs the evaluation of Eq. (2) to estimate a posterior variance at each candidate location. The complexity of this Algorithm 2 Selection of a local discriminative subspace DS x i for x i evaluation using a naive implementation is O ( n 3 ) ,where n is the number of observations. This is due to the inversion of a covariance matrix in Eq. (2). By noticing that this matrix is constant when we compare posterior variances among candidate locations, and the fact that we use a squared exponential isotropic covariance function, we can avoid the inversion of the covariance matrix. This result can be obtained from Equation 2. Suppose we need to choose the next location S x (whose respective observation is S y ) that maximizes the posterior variance  X  2 S tions A x (with respective observations A y ). The posterior variance of S y can be expressed as: isotropic covariance function we can write: where constant  X  is the sum of constant K ( S x ,S x ) and the noise variances. We can see that instance S x that maximizes the posterior variance  X  in Equation 9. This term requires the computation of a sum of differences between candidate elements inversion of the covariance matrix  X  A x A x . Finally, considering that the selection process is performed for each of the m available features, to learn all of the GPs the computational complexity is O ( mn 2 ) .
At testing time, the estimation of the local discriminative space for a query instance requires the esti-mation of the respective posterior mean and variance at each of the GPs. This takes O ( mB 3 ) operations for a set of B observations included in the GPs.

Unfortunately, the computational cost calculated above is still expensive for large datasets. This forces us to perform a preprocessing step in order to reduce the number of candidate locations n at each iter-ation. We achieve this by preprocessing the training data adding a clustering step using a k -medoids algorithm [19]. This step provides a set of clusters that are used to reduce the size of the training set by considering only the centers of each cluster as a possible candidate location to be used by the GP regressions. 5. Experimental results In this section, we use synthetic and real datasets to evaluate the main aspects of the proposed method. We start describing the datasets used in our experiments. Afterwards, we describe our experiments de-signed to evaluate: i) Accuracy in the identification of local discriminative subspaces, ii) Capacity to boost classification accuracy, iii) Sensibility and efficiency of strategy to select observations to build the GP regressions, and iv) Robustness of strategy to estimate GP hyperparameters. 5.1. Datasets 5.1.1. Synthetic datasets
As a proof of concept, we generate synthetic datasets designed to analyze the performance of our method in a case where there exist groups of instances that present local discriminative class patterns in subsets of dimensions. To generate this data, we associate to each class a set of local patterns defined in subsets of dimensions. Specifically, each local pattern is given by a multivariate Gaussian distribu-tion with a diagonal covariance matrix. For each instance, we fill irrelevant dimensions taking samples from a uniform distribution. Main parameters for Gaussian and uniform distributions are randomly se-lected within a finite range. Algorithm 3 resumes the main steps of the data generation process. Briefly, the generation process starts by randomly selecting subspaces and Gaussian parameters for the local patterns in each subspace. Then, the subspaces are randomly assigned to a set of target classes. After-wards, each instance is sampled from a randomly selected subspace, using the corresponding Gaussian distribution to generate values for the relevant dimensions and a uniform distribution for the remaining dimensions. We also generate noisy instances by adding samples that contain values taken only from a uniform distribution. In particular, the datasets used in our experiments consist of 500 instances, a total of 50 dimensions, 4 possible class labels, 6 underlying subspaces for each class, and a level of noisy of approximately 15% of the total instances. Algorithm 3 Generation of synthetic data 5.1.2. Real datasets
We evaluate the performance of the proposed method using 4 real datasets: Breast cancer dataset (digitized images of a fine needle aspirate of breast masses [1]), Isolet dataset (isolated letter speech recognition [1]), Spectrometer dataset (infra-red astronomy satellite data [1]), and X-ray dataset (X-ray images from aluminum wheels [31]). Table 1 provides further details about these datasets. In the case of Isolet dataset, as described in Section 4.5, as a preprocessing step we reduce the size of the training set by running the k -medoids clustering algorithm using k = 1000 . 5.2. Accuracy in the detection of discriminative subspaces
As a first part of our experimental evaluation, we use ground truth information available for the syn-thetic datasets to evaluate the capacity of the proposed technique to correctly identify the underlying subspace used to generate a test set of instances. To quantify the quality of the subspaces identified by our method, we use two indicators: 1) Recall: defined as the average proportion of features used to generate each instance that is correctly identified by the method, and 2) Precision: defined as the av-erage proportion from the total number features identified by the method that corresponds to features used to generate each instance. Furthermore, we analyze our results for different values of parameter  X  described in Section 4.3. In each case, parameter  X  determines the number of features included in the selection of each local subspace. In all cases, we evaluate Recall and Precision using an independent test set equivalent to 40% of the instances used for training. Table 2 shows our results. We can observe that for higher values of  X  our method tends to choose less features for each local subspace, achieving higher levels of Precision but lower levels of Recall. As we decrease the value of  X  , the method chooses more features for each local subspace, achieving lower levels of Precision but higher levels of Recall. 5.3. Classification accuracy using the proposed feature selection method
In general we do not know the true relevant features underlying each possible class pattern, but we can indirectly evaluate the quality of the feature selection process by testing the resulting classification accuracy. Consequently, in this Section we use the synthetic and real datasets to analyze the performance of the proposed approach to boost classification accuracy. Furthermore, we compare our approach with respect to 6 alternative techniques, 4 global feature selection methods: 1) Best First with CFS subset evaluator [14], 2) Greedy Stepwise with Consistency Subset Evaluator [28], 3) Relief-F [24], 4) A wrap-per approach with a Naive Bayes Classifier [25]; 2 distance metric learning methods: 1) Local Fisher Discriminant Analysis (LFDA) [44] and 2) Large Margin Nearest Neighbor Classifier (LMNN) [48], and regular K-NN method with no feature selection. In each case, we evaluate performance by mea-suring the classification accuracy reached by a K -NN classifier that uses the set of features provided by each of the methods under test. In our experiments we notice that different number of neighbors K for the classifier do not change substantially the relative results among the different feature selection methods. Specifically, we choose K =8 for all our experiments. To evaluate accuracy, we use 10-fold cross-validation.

Table 3 shows for the synthetic dataset the classification performance corresponding to each of the feature selection methods considered in this work. We can observe that our method outperforms the results of all the alternative feature selection techniques under evaluation. In this experiment we use  X  = 0.55. The average number of features locally selected among data points is 19 with a standard deviation of 8.

Table 5 shows the classification accuracy for the case of the real datasets. We can observe that our method outperforms all the alternative feature selection methods under evaluation. We run a T-test to check if these results are statistically significant (Behrens-Fisher problem [39]). We discover that in most cases the superior classification performance achieved by the proposed technique is statistically significant with a 90% of confidence. Exceptions are the X-ray dataset for the case of the Best First CFS and LMNN methods, where our results are statistically significant with a confidence of 80%, and the Isolet dataset for the case of the Relief-F, Wrapper-NB, and LMNN methods, where our results are statistically significant with a confidence of 60%. On each case we select the value of parameter  X  that maximizes the cross validation accuracy of our model. Table 4 shows values of  X  for each of the real datasets and the average (and standard deviation) number of features locally selected among data points. 5.4. Evaluation of strategy to select observations to build GP regressions
As a further test, we evaluate our strategy to obtain informative observations to build the GP regres-sions. To achieve this, we use the real datasets to run the GP regressions using only a limited set of observations that are selected using two different strategies: i) Our proposed technique, or ii) Random selection. Afterwards, we use the set of instances that are not considered in the construction of the re-gressions, to evaluate the MSE between the estimation of the discriminative score provided by the GP regressions, and the direct calculation of this score using Eq. (3) and the full set of observations. The previous process provides an estimation of the generalization capabilities of each regression at known observed locations.

Figure 3 shows the average MSE error considering the GP regressions corresponding to all the features in the datasets. We can observe that the proposed strategy to select observations achieves lower error levels than the random selection policy. This is true for all the real datasets considered in this work, highlighting the impact of an efficient selection policy to reduce the number of observations needed to achieve suitable accuracy levels. Additionally, we can also see in Fig. 3 that after adding a certain number of observations the GP regressions slightly overfit the training data. This effect is more clear for the case of the X-ray dataset Fig. 3(d). We believe that this is due to the great model flexibility exhibited by a non-parametric technique such as a GP regression. This observation stresses the need to monitor the MSE error of each regression in order to control the number of instances used as observations in the regressions.

As a further experiment, we analyze the sensibility of the classification results with respect to the number of observations used to build the GP regressions. Figure 4 shows the percentage of observations used to build the GP regressions versus classification accuracy using the K-NN scheme described in the previous Section. We can observe that in the case of the Breast cancer dataset, it is possible to use less than 20% of the available data to obtain competitive accuracy levels with respect to the other techniques considered in this work. In other cases, like x-ray dataset, we need less than 70% of the available data to obtain a competitive classification accuracy. In general, we can observe that the strategy to select informative observations to build each GP regression allows us to reduce the number of observations needed to select discriminative features. 5.5. Evaluation of strategy to estimate GP hyperparameters
Finally, we evaluate the sensibility of our model with r espect to different values of the hyperparameters used to initialize Algorithm 1. To perform this evaluation, we use the synthetic dataset described in Section 5.1.1. We run our model with 6 different starting values for the GP hyperparameters. Table 6 shows our results. We can see that the final values of the hyperparameters are slightly different for that the hyperparameters correspond to the characteristic length scale and the signal variance of the GPs, respectively. 6. Conclusions
In this work we present a new local feature selection approach that for each possible input instance finds a particular local discriminative subspace that facilitates its classification. As a relevant insight, this approach casts local feature selection as a GP regression problem where the key step is the non-parametric estimation of a score that quantifies the discriminative power of each feature. When we test our method using synthetic datasets that present relevant discriminative patterns in local subspaces, depending of parameter  X  our method is able to identify most of the features associated to each local pattern. Furthermore, when we use synthetic and real datasets to compare the classification performance reached by a K-NN classifier using the output of several popular global and local feature selection techniques, we find that the proposed technique outperforms the alternative approaches in all the cases.
In terms of the proposed iterative strategy to select observations and estimate GP hyperparameter, our results support its relevance by showing that selecting observations considering uncertainty plays an important role in the computational efficiency of the method, reducing also the risk of suffering overfitting. Also, our results indicate that classification performance keeps highly stable with respect to different initial configurations for the estimation of GP hyperparameters.

As future work, we are developing new techniques for GP regression in order to deal with higher num-ber of instances without losing accuracy in regression. We are also considering new scores to evaluate the discriminative power of subsets of features instead of single features, we believe that our current greedy approach based on a best first search heuristic can be improved by searching subspaces where features become discriminative when they are jointly considered. Finally, we are exploring to use the proposed method with other classifiers besides K-NNs.
 References
