 The Principle of Compositionality states that the meaning of an expression is determined by the meaning of its constituents and by its grammatical structure (Montague, 1974). By extension, senti-ment composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, based on its constituents. In this work, we study sentiment composition in phrases that in-clude at least one positive and at least one nega-tive word X  X or example, phrases such as happy ac-cident , couldn X  X  stop smiling , and lazy sundays . We refer to them as opposing polarity phrases . Such phrases present a particular challenge for automatic sentiment analysis systems that often rely on bag-of-word features.

Word X  X entiment associations are commonly cap-tured in sentiment lexicons . However, most existing manually created sentiment lexicons include only single words. Lexicons that include sentiment asso-ciations for multi-word phrases as well as their con-stituent words can be very useful in studying sen-timent composition. We refer to them as sentiment composition lexicons (SCLs) .

We created a sentiment composition lexicon for opposing polarity phrases and their constituent phrases and single words were manually annotated with real-valued sentiment association scores using an annotation scheme known as Best X  X orst Scal-ing. 2 We refer to the created resource as the Sen-timent Composition Lexicon for Opposing Polarity Phrases (SCL-OPP) . The lexicon includes entries for 265 trigrams, 311 bigrams, and 602 unigrams.
In this paper, we use SCL-OPP to analyze regu-larities present in different kinds of opposing polar-ity phrases. We calculate the extent to which differ-ent part-of-speech combinations result in phrases of positive and negative polarity. We also show that for most phrases, knowing the parts of speech and po-larities of their constituents is not enough to reliably predict the sentiment of the phrase.

We apply several unsupervised and supervised techniques of sentiment composition to determine their efficacy on predicting the sentiment of oppos-ing polarity phrases. Our experiments indicate that the sentiment of the last unigram or the sentiment of the most polar unigram in the phrase are not strong predictors of the overall sentiment of the phrase. Similarly, adjectives and verbs do not always domi-nate the sentiment in such phrases. Finally, we show that the constituent words, their parts of speech, their sentiment association scores, and their embedding vectors are all useful features X  X  supervised senti-ment composition system that incorporates them ob-tains accuracies over 80% on both bigram and tri-gram opposing polarity phrases. A number of approaches have been proposed to ad-dress sentiment composition, which include man-ually derived syntactic rules (Moilanen and Pul-man, 2007; Neviarouskaya et al., 2010), combina-tion of hand-written rules and statistical learning (Choi and Cardie, 2008), and machine learning ap-proaches (Nakagawa et al., 2010; Yessenalina and Cardie, 2011; Dong et al., 2015). Much work has been devoted to model the impact of negators and (to a lesser degree) intensifiers, words commonly referred to as contextual valence shifters, on senti-ment of words they modify (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2005; Liu and Seneff, 2009; Wiegand et al., 2010; Taboada et al., 2011; Kiritchenko et al., 2014). Kiritchenko and Moham-mad (2016b) created a sentiment composition lexi-con for negators, modals, and adverbs (SCL-NMA) through manual annotation and analyzed the effect of these groups of modifiers on sentiment in short phrases. Recently, recursive deep model approaches have been proposed for handling sentiment of syn-tactic phrases through sentiment composition over parse trees (Socher et al., 2013; Zhu et al., 2014; Ir-soy and Cardie, 2014; Tai et al., 2015). In this work, we apply several unsupervised and supervised tech-niques of sentiment composition for a specific type of phrases X  X pposing polarity phrases. This section summarizes how we created a sen-timent composition lexicon for opposing polarity phrases using the Best X  X orst Scaling annotation technique. For more details we refer the reader to (Kiritchenko and Mohammad, 2016c). Table 1 shows a few example entries from the lexicon. Term selection: We polled the Twitter API (from 2013 to 2015) to collect about 11 million tweets that contain emoticons:  X :) X  or  X :( X . We will refer to this corpus as the Emoticon Tweets Corpus . From this corpus, we selected bigrams and trigrams that had at least one positive word and at least one negative word. The polarity labels (positive or negative) of the words were determined by simple look-up in existing sentiment lexicons: Hu and Liu lexicon (Hu and Liu, 2004), NRC Emotion lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013), MPQA lexicon (Wilson et al., 2005), and NRC X  X  Twitter-specific lexicon (Kiritchenko et al., 2014; polarity n -grams (bigrams and trigrams) were selected. We also chose for annotation all unigrams that appeared in the selected set of bigrams and trigrams. There were 602 such unigrams. Note that even though the multi-word phrases and single-word terms were drawn from a corpus of tweets, most of the terms are used in everyday English.
 Best X  X orst Scaling Method of Annotation: Best X  Worst Scaling (BWS), also sometimes referred to as Maximum Difference Scaling (MaxDiff), is an an-notation scheme that exploits the comparative ap-proach to annotation (Louviere and Woodworth, 1990; Louviere et al., 2015). Annotators are given four items (4-tuple) and asked which term is the Best (highest in terms of the property of interest) and which is the Worst (least in terms of the property of interest). Responses to the BWS questions can then be translated into real-valued scores through a simple counting procedure: For each term, its score is calculated as the percentage of times the term was chosen as the Best minus the percentage of times the term was chosen as the Worst (Orme, 2009). The scores range from -1 to 1.

We employ Best X  X orst Scaling for sentiment an-notation by providing four (single-word or multi-word) terms at a time and asking which term is the most positive (or least negative) and which is the least positive (or most negative). Each question was answered by eight annotators through a crowdsourc-ing lexicon as the Sentiment Composition Lexicon for Opposing Polarity Phrases (SCL-OPP) .

Portions of the created lexicon have been used as development and evaluation sets in SemEval-2016 Task 7  X  X etermining Sentiment Intensity of English objective of that task was to test different meth-ods of automatically predicting sentiment associa-tion scores for multi-word phrases. SCL-OPP allows us to explore sentiment composi-tion patterns in opposing polarity phrases. We de-fine a Sentiment Composition Pattern (SCP) as a rule that includes on the left-hand side the parts of speech (POS) and the sentiment associations of the constituent unigrams (in the order they appear in the phrase), and on the right-hand side the sentiment as-sociation of the phrase. Table 2 shows examples. SCPs that have a positive phrase on the right-hand side will be called positive SCPs , whereas SCPs that have a negative phrase on the right-hand side will be called negative SCPs . Below are some questions re-garding SCPs and opposing polarity phrases that we explore here:  X  Which SCPs are common among opposing po- X  With the same left-hand side of an SCP, how  X  Are some parts of speech (of constituent words) To answer these questions, each of the entries in SCL-OPP is marked with the appropriate SCP. The part-of-speech sequence of a phrase is determined by looking up the most common part-of-speech se-quence for that phrase in the Emoticon Tweets Cor-determine the ratio of  X  X ow often occurrences of such combinations in SCL-OPP resulted in a posi-tive phrase X  to  X  X ow often such combinations were seen in total X . We will refer to these scores as the occurrence rates ( X  X cc. X ) of positive SCPs . The oc-currence rates of negative SCPs are calculated in a similar manner.

Table 2 presents all SCPs with the left-hand side combination appearing at least ten times in SCL-OPP, and whose occurrence rate is equal to or greater than 50%. For example, the second row tells us that there are 68 bigrams in SCL-OPP such that the first word is a negative adjective and the second word is a positive noun. Out of these 68 bigrams, 59% are negative, and the remaining 41% are positive, so the occurrence rate of this pattern is 0.59.

The most common SCPs in our lexicon are  X  4 adj. + 5 noun  X 5 phrase X  (73) and  X  5 adj. + 4 noun  X  5 phrase X  (68). Observe that the occurrence rates of the patterns are spread over the entire range from 52% to 91%. Only two patterns have very high occurrence rates (around 90%):  X  4 adverb + 5 adj.  X 5 phrase X  and  X  4 adverb + 5 verb  X 5 phrase X . Thus, for most opposing polarity phrases, their sen-timent cannot be accurately determined based on the POS and sentiment of the constituents alone.
Both SCPs with high occurrence rates include ad-verbs that serve as intensifiers X  X ords that increase or decrease the degree of association of the follow-ing word with positive (negative) sentiment (e.g., in-credibly slow , dearly missed ). Only the degree of as-sociation for the next word is changed while its po-larity (positive or negative) is often preserved. Some adjectives can also play the role of an intensifier when combined with another adjective (e.g., crazy talented ) or a noun (e.g., epic fail ). For example, the adjective great , often considered highly positive, becomes an intensifier when combined with some nouns (e.g., great loss , great capture ). Other adjec-tives determine the polarity of the entire phrase (e.g., happy tears , bad luck ). Therefore, the occurrence rates of patterns like  X  5 adj. + 4 noun  X 5 phrase X  are low. Overall, even though adjectives and verbs are frequently the primary source of sentiment in the phrase, some nouns can override their sentiment as in new crisis or leave a smile . SCL-OPP includes phrases corresponding to many different kinds of sentiment composition patterns, and therefore, it is a useful resource for studying linguistic underpin-nings of sentiment composition as well as for eval-uating sentiment composition algorithms for oppos-ing polarity phrases. We now investigate whether accurate models of sen-timent composition for opposing polarity phrases can be learned. We conduct experiments with sev-eral baseline unsupervised classifiers as well super-vised techniques using features, such as unigrams, POS, sentiment scores, and word embeddings.

The problem of sentiment composition can be for-mulated in two different ways: a binary classifi-cation task where the system has to predict if the phrase is positive or negative; and a regression task where the system has to predict the real-valued sen-timent association score of the phrase. We evalu-ate binary classification with simple accuracy ( acc. ) and the regression task with Pearson correlation co-efficient ( r ). Learning and evaluation are performed separately for bigrams and trigrams. 5.1 Baseline Classifiers The oracle  X  X ajority label X  baseline assigns to all instances the most frequent polarity label in the dataset. The  X  X ast unigram X  baseline returns the sen-timent score (or the polarity label) of the last un-igram in the phrase. For the regression task, we use the real-valued sentiment score of the unigram whereas for the binary classification task we use the polarity label (positive or negative). The  X  X ost po-lar unigram X  baseline assigns to the phrase the sen-timent score (or the polarity label) of the most polar word in that phrase, i.e., the word with the high-est absolute sentiment score. The  X  X art-of-speech (POS) rule X  baseline assigns sentiment as follows: 1. If the phrase has an adjective, return the senti-2. Else, if the phrase has a verb, return the senti-3. Else, return the sentiment score (polarity) of the 5.2 Supervised Classifiers We train a Support Vector Machines classifier with RBF kernel for the binary classification task and a Support Vector regression model with RBF kernel for the regression task using the LibSVM package (Chang and Lin, 2011). For both tasks, the mod-els are trained using different combinations of the following features obtained from the target phrase: all unigrams, POS tag of each unigram, sentiment label of each unigram, sentiment score of each uni-gram, and the word embedding vector for each un-igram. The word embeddings are obtained by run-ning word2vec software (Mikolov et al., 2013) on the Emoticon Tweets Corpus. We use the skip-gram model with the default parameters and generate 200-dimensional vectors for each unigram present in the corpus. For each task, ten-fold cross-validation is repeated ten times, and the results are averaged. 5.3 Results The results for all baseline and supervised meth-ods are presented in Table 3. The  X  X ajority label X ,  X  X ast unigram X ,  X  X ost polar unigram X , and  X  X OS rule X  baselines are shown in rows a to d. Observe that the sentiment association of the last unigram is not very the  X  X ost polar unigram X  and the  X  X OS rule X  classi-fiers perform markedly better than the majority base-line. Interestingly, the  X  X ost polar unigram X  clas-sifier outperforms the slightly more sophisticated  X  X OS rule X  approach on most tasks. Also, we found that within bigram phrases that contain adjectives or verbs, the adjective or verb constituents are the most polar words in only about half of the instances (and even less so in trigrams). This indicates that adjec-tives and verbs do not always dominate the senti-ment in a phrase.

The results obtained using supervised techniques with various feature combinations are presented in rows e to k (Table 3). Using only POS and bi-nary sentiment labels of the constituent unigrams, the supervised learning algorithm does not perform much better than our  X  X OS rule X  baseline (the ac-curacies in row e are just slightly higher than those in row d). With access to real-valued sentiment scores of unigrams much more accurate models can be learned (row f). Furthermore, the results show that the sentiment of a phrase depends on its con-stituent words and not only on the sentiment of the constituents (row g shows markedly better perfor-mance than row f; all the differences are statistically significant, p &lt; . 01 ). Concatenating word embed-dings was found to be more effective than averag-ing. (Averaging is common when creating features for sentences). Having access to both unigrams and word embedding features produces the best results. (The differences between the scores in row i and row j are statistically significant, p &lt; . 01 .) Row k shows results of the model trained without the gold sentiment scores of the unigrams. Observe that for bigrams, there is a substantial drop in performance compared to row j (6.3-point drop in accuracy on the binary task, 6.7-point drop in Pearson correlation on the regression task) whereas for trigrams the per-formance is not affected as much (less than 1-point change on both tasks). Thus, having access to senti-ment scores of constituents is particularly useful for determining sentiment of bigram phrases. We created a real-valued sentiment composition lex-icon for opposing polarity phrases and their con-stituent words, through manual annotation. We analyzed patterns of sentiment composition across phrases formed with different POS combinations. Further, we applied several unsupervised and super-vised techniques of sentiment composition to deter-mine their efficacy on opposing polarity phrases. We showed that for most phrases the sentiment of the phrase cannot be reliably predicted only from the parts of speech and sentiment association of their constituent words, and that the constituent words, their parts of speech, their sentiment scores, and their embedding vectors are all useful features in su-pervised sentiment prediction on this dataset.
We intend to use SCL-OPP in the following appli-cations: (1) to automatically create a large coverage sentiment lexicon of multi-word phrases and apply it in downstream applications such as sentence-level sentiment classification, and (2) to investigate how the human brain processes sentiment composition.
