 { agelfand,yutianc,welling } @ics.uci.edu The invention of the perceptron [12] goes back to the very beginning of AI more than half a century ago. Rosenblatt X  X  very simple, neurally plausible learning rule made it an attractive algorithm for learning relations in data: for every input x i , make a linear prediction about its label: y  X  i = w T x i and update the weights as, A critical evaluation by Minsky and Papert [11] revealed the perceptron X  X  limited representational power. This fact is reflected in the behavior of Rosenblatt X  X  learning rule: if the data is linearly separable, then the learning rule converges to the correct solution in a number of iterations that can be bounded by ( R/ X  ) 2 , where R represents the norm of the largest input vector and  X  represents the margin between the decision boundary and the closest data-case. However,  X  for data sets that are not linearly separable, the perceptron learning algorithm will never converge  X  (quoted from [1]). While the above result is true, the theorem in question has something much more powerful to say. The  X  perceptron cycling theorem  X  (PCT) [2, 11] states that for the inseparable case the weights re-main bounded and do not diverge to infinity. In this paper, we show that the implication of this theorem is that certain moments are conserved on average. Denoting the data-case selected at itera-tion t by i t (note that the same data-case can be picked multiple times), the corresponding attribute vector and label by ( x i t ,y i t ) with x i  X  X , and the label predicted by the perceptron at iteration t for data-case i t by y  X  i This result implies that, even though the perceptron learning algorithm does not converge in the inseparable case, it generates predictions that correlate with the attributes in the same way as the true labels do. More importantly, the correlations converge to the sample mean with a rate 1 /T , which is much faster than sampling based algorithms that converge at a rate 1 /  X  ( x ) , the above result can be extended to the matching of arbitrarily complicated statistics between data and predictions. In the inseparable case, we can interpret the perceptron as a bagging procedure and average predic-tions instead of picking the single best (or last) weights found during training. Although not directly motivated by the PCT and Eqn. 2, this is exactly what the voted perceptron (VP) [5] does. Interest-ing generalization bounds for the voted perceptron have been derived in [5]. Extensions of VP to chain models have been explored in, e.g. [4].
 Herding is a seemingly unrelated family of algorithms for unsupervised learning [15, 14, 16, 3]. In traditional methods for learning Markov Random Field (MRF) models, the goal is to converge to a single parameter estimate and then perform (approximate) inference in the resulting model. In contrast, herding combines the learning and inference phases by treating the weights as dynamic quantities and defining a deterministic set of updates such that averaging predictions preserves cer-tain moments of the training data. The herding algorithm generates a weakly chaotic sequence of weights and a sequence of states of both hidden and visible variables of the MRF model. The in-termediate states produced by herding are really  X  X epresentative points X  of an implicit model that interpolates between data cases. We can view these states as pseudo-samples, which analogously to Eqn. 2, satisfy certain constraints on their average sufficient statistics. However, unlike in perceptron learning, the non-convergence of the weights is needed to generate long, non-periodic trajectories of states that can be averaged over.
 In this paper, we show that supervised perceptron algorithms and unsupervised herding algorithms can all be derived from the PCT. This connection allows us to strengthen existing herding results. For instance, we prove fast convergence rates of sample averages when we use small mini-batches for making updates, or when we use incomplete optimization algorithms to run herding. Moreover, the connection suggests new algorithms that lie between supervised perceptron and unsupervised herding algorithms. We refer to these algorithms as  X  conditional herding  X  (CH) because, like con-ditional random fields, they condition on the input features. From the perceptron perspective, condi-tional herding can be understood as  X  voted perceptrons with hidden units  X . Conditional herding can also be interpreted as the zero temperature limit of discriminative RBMs (dRBMs) [10]. We first review the perceptron cycling theorem that was initially introduced in [11] with a gap in the proof that was fixed in [2]. A sequence of vectors { w t } , w t  X  R D ,t = 0 , 1 ,... is generated by the following iterative procedure: w t +1 = w t + v t , where v t is an element of a finite set , V , and the norm of v t is bounded: max i || v i || = R &lt;  X  .
 Perceptron Cycling Theorem (PCT).  X  t  X  0 : If w T t v t  X  0 , then there exists a constant M &gt; 0 such that k w t  X  w 0 k &lt; M .
 The theorem still holds when V is a finite set in a Hilbert space. The PCT immediately leads to the following result: Convergence Theorem. If PCT holds, then: || 1 T P T t =1 v t || X  X  (1 /T ) .
 and dividing all terms by T . 2.1 Voted Perceptron and Moment Matching The voted perceptron (VP) algorithm [5] repeatedly applies the update rule in Eqn. 1. Predictions of test labels are made after each update and final label predictions are taken as an average of all intermediate predictions. The PCT convergence theorem leads to the result of Eqn. 2, where we In maximum entropy models, one seeks a model that satisfies a set of expectation constraints (mo-ments) from the training data, while maximizing the entropy of the remaining degrees of free- X  [ y  X   X  arg max y ( y w T x )] that has zero entropy and gets every prediction on every training case correct (where  X  is the delta function). Entropy is created in p ( y  X  | x ) only when the weights w t do not converge (i.e. for inseparable data sets). Thus, VP and maximum entropy methods are related, but differ in how they handle the degrees of freedom that are unconstrained by moment matching. 2.2 Herding A new class of unsupervised learning algorithms, known as  X  X erding X , was introduced in [15]. Rather than learning a single  X  X est X  MRF model that can be sampled from to estimate quantities of interest, herding combines learning and inference into a single process. In particular, herding produces a trajectory of weights and states that reproduce the moments of the training data. Consider a fully observed MRF with features  X  ( x ) , x  X  X = [1 ,...,K ] m with K the number of states for each variable x j ( j = 1 ,...,m ) and with an energy function E ( x ) given by: In herding [15], the parameters w are updated as: hood (ML) gradient update, with constant learning rate and maximization in place of expectation in the right-hand side. This follows from taking the zero temperature limit of the ML objective (see Section 2.5). The maximization prevents the herding sequence from converging to a single point estimate on this alternative objective.
 Let { w t } denote the sequence of weights and { x  X  t } denote the sequence of states (pseudo-samples) produced by herding. We can apply the PCT to herding by identifying V = {  X   X   X  ( x  X  ) | x  X   X  X } . It is now easy to see that, in general, herding does not converge because under very mild conditions we can always find an x  X  t such that w T t v t &lt; 0 . From the PCT convergence theorem, we also see to the data averages  X  at a rate 1 /T 1 . This is considerably faster than i.i.d. sampling from the corresponding MRF model, which would converge at a rate of 1 / Since the cardinality of the set V is exponentially large (i.e. | V | = K m ), finding the maximizing state x  X  t at each update may be hard. However, the PCT only requires us to find some state x  X  t such that w T t v t  X  0 and in most cases this can easily be verified. Hence, the PCT provides a theoretical justification for using a local search algorithm that performs partial energy maximization. iteration (a so-called persistent chain [13, 17]). Or, one may consider contrastive divergence -like algorithms [8], in which the sampling or mean field approximation is replaced by a maximization. difference between the average over the data-cases minus the average over the { x  X  i } found after For obvious reasons, it is now guaranteed that w T t v t  X  0 .
 In practice, we often use mini-batches of size n &lt; N instead of the full data set. In this case, the cardinality of the set V is enlarged to | V | = C ( n,N ) K m , with C ( n,N ) representing the  X  X  choose N X  ways to compute the sample mean  X  ( n ) based on a subset of n data-cases. The negative term Depending on how the mini-batches are picked, convergence onto the overall mean  X  can be either O (1 / has picked all data-cases after d N/n e rounds). 2.3 Hidden Variables The discussion so far has considered only constant features:  X  ( x ,y ) = x y for VP and  X  ( x ) for herding. However, the PCT allows us to consider more general features that depend on the weights w , as long as the image of this feature mapping (and therefore, the update vector v ) is a set of finite cardinality. In [14], such features took the form of  X  X idden units X : In this case, we identify the vector v as v =  X  ( x , z )  X   X  ( x  X  , z  X  ) . In the left-hand term of this expression, x is clamped to the data-cases and z is found as in Eqn. 5 by maximizing every data-case The quantity  X  ( x , z ) denotes a sample average over the training cases. We note that  X  ( x , z ) indeed maps to a finite domain because it depends on the real parameter w only through the discrete state z . We also notice again that w T v  X  0 because of the definition of ( x  X  , z  X  ) . From the convergence extended to mini-batches as well. 2.4 Conditional Herding We are now ready to propose our new algorithm: conditional herding (CH). Like the VP algorithm, CH is concerned with discriminative learning and, therefore, it conditions on the input attributes { x i } . CH differs from VP in that it uses hidden variables, similar to the herder described in the previous subsection. In the most general setting, CH uses features: In the experiments in Section 3, we use the explicit form: where W , B ,  X  and  X  are the weights, z is a binary vector and y is a binary vector in a 1-of-K scheme (see Figure 1). At each iteration t , CH randomly samples a subset of the data-cases and their labels D t = { x i t , y i t } X  X  . For every member of this mini-batch it computes a hidden variable z i t using Eqn. 6. The parameters are then updated as: In the positive term, z i t , is found as in Eqn. 5. The negative term is obtained (similar to the percep-tron) by making a prediction for the labels, keeping the input attributes fixed: For the PCT to apply to CH, the set V of update vectors must be finite. The inputs x can be real-valued because we condition on the inputs and there will be at most N distinct values (one for each data-case). However, since we maximize over y and z these states must be discrete for the PCT to apply.
 Eqn. 8 includes a potentially vector-valued stepsize  X  . Notice however that scaling w  X   X  w will have no affect on the values of z , z  X  or y  X  and hence on v . Therefore, if we also scale  X   X   X   X  , then the only scale that matters is the relative scale between w 0 and  X  . In case there would just be a single attractor set for the dynamics of w , the initialization w 0 would only represent a transient affect. However, in practice the scale of w 0 relative to that of  X  does play an important role indicating that many different attractor sets exist for this system.
 Irrespective of the attractor we end up in, the PCT guarantees that: In general, herding systems perform better when we use normalized features: k  X  ( x , z , y ) k = and features with large norms will therefore become more likely to be selected. In fact, one can show that states inside the convex hull of the  X  ( x , y , z ) are never selected. For binary (  X  1 ) vari-ables all states live on the convex hull, but this need not be true in general, especially when we use continuous attributes x . To remedy this, one can either normalize features or add one additional fea-allowed to vary over the data-cases.
 Finally, predictions on unseen test data are made by: The algorithm is summarized in the algorithm-box below.
 Conditional Herding (CH) 2.5 Zero Temperature Limit of Discriminative MRF Learning Regular herding can be understood as gradient descent on the zero temperature limit of an MRF model. In this limit, gradient updates with constant step size never lead to convergence, irrespective of how small the step size is. Analogously, CH can be viewed as constant step size gradient updates on the zero temperature limit of discriminative MRFs (see [10] for the corresponding RBM model). The finite temperature model is given by: Similar to herding [14], conditional herding introduces a temperature by replacing w by w /T and takes the limit T  X  0 of ` T , T` , where ` = P i log p ( y i | x i ) . We studied the behavior of conditional herding on two artificial and four real-world data sets, com-paring its performance to that of the voted perceptron [5] and that of discriminative RBMs [10]. The experiments on artificial and real-world data are discussed separately in Section 3.1 and 3.2. We studied conditional herding in the discriminative RBM architecture illustrated in Figure 1 (i.e., we use the energy function in Eqn. 7). Per the discussion in Section 2.4, we added an additional feature  X  0 ( x ) = p R 2 max  X  X | x || 2 with R max = max i k x i k in all experiments. 3.1 Artificial Data To investigate the characteristics of VP, dRBMs and CH, we used the techniques to construct de-cision boundaries on two artificial data sets: (1) the banana data set; and (2) the Lithuanian data set. We ran VP and CH for 1 , 000 epochs using mini-batches of size 100 . The decision bound-ary for VP and CH is located at the location where the sign of the prediction y  X  tst changes. We used conditional herders with 20 hidden units. The dRBMs also had 20 hidden units and were trained by running conjugate gradients until convergence. The weights of the dRBMs were ini-tialized by sampling from a Gaussian distribution with a variance of 10  X  4 . The decision bound-ary for the dRBMs is located at the point where both class posteriors are equal, i.e., where p ( y  X  tst =  X  1 |  X  x tst ) = p ( y  X  tst = +1 |  X  x tst ) = 0 . 5 .
 Plots of the decision boundary for the artificial data sets are shown in Figure 2. The results on the banana data set illustrate the representational advantages of hidden units. Since VP selects data points at random to update the weights, on the banana data set, the weight vector of VP tends to oscillate back and forth yielding a nearly linear decision boundary 3 . This happens because VP can regress on only 2 + 1 = 3 fixed features. In contrast, for CH the simple predictor in the top layer can regress onto M = 20 hidden features. This prevents the same oscillatory behavior from occurring. 3.2 Real-World Data In addition to the experiments on synthetic data, we also performed experiments on four real-world data sets -namely, (1) the USPS data set, (2) the MNIST data set, (3) the UCI Pendigits data set, and (4) the 20-Newsgroups data set. The USPS data set consists of 11,000, 16  X  16 grayscale images of handwritten digits ( 1 , 100 images of each digit 0 through 9) with no fixed division. The MNIST data set contains 70 , 000 , 28  X  28 grayscale images of digits, with a fixed division into 60 , 000 training and 10 , 000 test instances. The UCI Pendigits consists of 16 (integer-valued) features extracted from the movement of a stylus. It contains 10 , 992 instances, with a fixed division into 7 , 494 training and 3 , 498 test instances. The 20-Newsgroups data set contains bag-of-words representations of 18 , 774 documents gathered from 20 different newsgroups. Since the bag-of-words representation comprises over 60 , 000 words, we identified the 5 , 000 most frequently occurring words. From this set, we created a data set of 4 , 900 binary word-presence features by binarizing the word counts and removing the 100 most frequently occurring words. The 20-Newsgroups data has a fixed division into 11 , 269 training and 7 , 505 test instances. On all data sets with real-valued input attributes we used the  X  X ormalizing X  feature described above.
 The data sets used in the experiments are multi-class. We adopted a 1-of-K encoding, where if y i label of the i th data point is k and y i,k =  X  1 otherwise. Performing the maximization in Eqn. 9 is difficult when K &gt; 2 . We investigated two different procedures for doing so. In the first procedure, we reduce the multi-class problem to a series of binary decision problems using a one-versus-all scheme. The prediction on a test point is taken as the label with the largest online average. In the second procedure, we make predictions on all K labels jointly. To perform the maximization in Eqn. 9, we explore all states of y in a one-of-K encoding -i.e. one unit is activated and all others are inactive. This partial maximization is not a problem as long as the ensuing configuration satisfies w t v t  X  0 4 . The main difference between the two procedures is that in the second procedure the weights W are shared amongst the K classifiers. The primary advantage of the latter procedure is it less computationally demanding than the one-versus-all scheme.
 We trained the dRBMs by performing iterations of conjugate gradients (using 3 linesearches) on mini-batches of size 100 until the error on a small held-out validation set started increasing (i.e., we employed early stopping) or until the negative conditional log-likelihood on the training data stopped coming down. Following [10], we use L2-regularization on the weights of the dRBMs; the regularization parameter was determined based on the generalization error on the same held-out validation set. The weights of the dRBMs were initialized from a Gaussian distribution with variance of 10  X  4 .
 CH used mini-batches of size 100 . For the USPS and Pendigits data sets CH used a burn-in period of 1 , 000 updates; on MNIST it was 5 , 000 updates; and on 20 Newsgroups it was 20 , 000 updates. Herding was stopped when the error on the training set became zero 5 .
 The parameters of the conditional herders were initialized by sampling from a Gaussian distribution. Ideally, we would like each of the terms in the energy function in Eqn. 7 to contribute equally during updating. However, since the dimension of the data is typically much greater than the number of classes, the dynamics of the conditional herding system will be largely driven by W . To negate this effect, we rescaled the standard deviation of the Gaussian by a factor 1 /M with M the total number of elements of the parameter involved (e.g.  X  W =  X / (dim( x ) dim( z )) etc.). We also scale the step sizes  X  by the same factor so the updates will retain this scale during herding. The relative scale between  X  and  X  was chosen by cross-validation. Recall that the absolute scale is unimportant (see Section 2.4 for details).
 In addition, during the early stages of herding, we adapted the parameter update for the bias on the hidden units  X  in such a way that the marginal distribution over the hidden units was nearly uniform. This has the advantage that it encourages high entropy in the hidden units, leading to more useful dynamics of the system. In practice, we update  X  as  X  t +1 =  X  t +  X  |D where  X  z i t  X  is the batch mean.  X  is initialized to 1 and we gradually half its value every 500 updates, slowly moving from an entropy-encouraging update to the standard update for the biases of the hidden units.
 VP was also run on mini-batches of size 100 (with step size of 1 ). VP was run until the predictor started overfitting on a validation set. No burn-in was considered for VP.
 The results of our experiments are shown in Table 1. In the table, the best performance on each data set using each procedure is typeset in boldface. The results reveal that the addition of hidden Furthermore, the results of our experiments indicate that conditional herding performs on par with discriminative RBMs on the MNIST and USPS data sets and better on the 20 Newsgroups data set. The 20 Newsgroups data is high dimensional and sparse and both VP and CH appear to perform X X X X Data Set MNIST 7.69% 3.57% 3.58% 3.97% 3.99% USPS 5.03% (0.4%) 3.97% (0.38%) 4.02% (0.68%) 3.49% (0.45%) 3.35% (0.48%)
UCI Pendigits 10.92% 5.32% 5.00% 3.37% 3.00% 20 Newsgroups 27.75% 34.78% 34.36% 29.78% 25.96% X X X X Data Set MNIST 8.84% 3.88% 2.93% 1.98% 2.89% 2.09% 2.09%
UCI Pendigits 6.78% 3.80% 3.23% 8.89% 3.14% 2.57% 2.86% 20 Newsgroups 24.89%  X  30.57% 30.07%  X  25.76% 24.93% Table 1: Generalization errors of VP, dRBMs, and CH on 4 real-world data sets. dRBMs and CH results are shown for various numbers of hidden units. The best performance on each data set is typeset in boldface; missing values are shown as  X - X . The std. dev. of the error on the 10 -fold cross validation of the USPS data set is reported in parentheses. quite well in this regime. Techniques to promote sparsity in the hidden layer when training dRBMs exist (see [10]), but we did not investigate them here. It is also worth noting that CH is rather resilient to overfitting. This is particularly evident in the low-dimensional UCI Pendigits data set, where the dRBMs start to badly overfit with 500 hidden units, while the test error for CH remains level. This phenomena is the benefit of averaging over many different predictors. The main contribution of this paper is to expose a relationship between the PCT and herding algo-rithms. This has allowed us to strengthen certain results for herding -namely, theoretically vali-dating herding with mini-batches and partial optimization. It also directly leads to the insight that non-convergent VPs and herding match moments between data and generated predictions at a rate much faster than random sampling ( O (1 /T ) vs. O (1 / a new conditional herding algorithm that is the zero-temperature limit of dRBMs [10].
 The herding perspective provides a new way of looking at learning as a dynamical system. In fact, the PCT precisely specifies the conditions that need to hold for a herding system (in batch mode) to be a piecewise isometry [7]. A piecewise isometry is a weakly chaotic dynamical system that divides parameter space into cells and applies a different isometry in each cell. For herding, the combination applies. Therefore, the requirement of the PCT that the space V must be of finite its own isometry. Many interesting results about piecewise isometries have been proven in the mathematics literature such as the fact that the sequence of sampled states grows algebraically with T and not exponentially as in systems with random or chaotic components [6]. We envision a fruitful cross-fertilization between the relevant research areas in mathematics and learning theory. Acknowledgments This work is supported by NSF grants 0447903, 0914783, 0928427 and 1018433 as well as ONR/MURI grant 00014-06-1-073. LvdM acknowledges support by the Netherlands Organisation for Scientific Research (grant no. 680.50.0908) and by EU-FP7 NoE on Social Signal Processing (SSPNet).
 [1] C.M. Bishop. Pattern Recognition and Machine Learning . Springer, 2006. [2] H.D. Block and S.A. Levin. On the boundedness of an iterative procedure for solving a system [3] Y. Chen and M. Welling. Parametric herding. In Proceedings of the Thirteenth International [4] M. Collins. Discriminative training methods for hidden markov models: Theory and exper-[5] Y. Freund and R.E. Schapire. Large margin classification using the perceptron algorithm. [6] A. Goetz. Perturbations of 8-attractors and births of satellite systems. Internat. J. Bifur. Chaos, [7] A. Goetz. Global properties of a family of piecewise isometries. Ergodic Theory Dynam. [8] G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural [9] E.T. Jaynes. Information theory and statistical mechanics. Physical Review Series II , [10] H. Larochelle and Y. Bengio. Classification using discriminative Restricted Boltzmann Ma-[11] M.L. Minsky and S. Papert. Perceptrons; An introduction to computational geometry . Cam-[12] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization [13] T. Tieleman. Training Restricted Boltzmann Machines using approximations to the likeli-[14] M. Welling. Herding dynamic weights for partially observed random field models. In Proc. of [15] M. Welling. Herding dynamical weights to learn. In Proceedings of the 21st International [16] M. Welling and Y. Chen. Statistical inference using weak chaos and infinite memory. In [17] L. Younes. Parametric inference for imperfectly observed Gibbsian fields. Probability Theory
