 Spam, also known as junk, is one of the greatest challenges to the email world these days. Spam not only wastes the time, but also wastes bandwidth, server space and some contents like pornographic contents are even harmful to under-aged recipients. Many anti-spam filtering techniques are being already proposed and in use to fight with ever growing spams. However, the new non-stop clever tricks of spammers necessitates further improvement in the filtering approaches. Among many machine learning based spam filters, Bayesian filters are the most popular and widely used because of efficient training, quick classification, easy extensibility, adaptive learning and fewer false positives.
 spam or legitimate. In the series of papers, Androutsopoulos [2, 3, 4] extended sizes on the filter X  X  performance. Paul Graham in [5, 6] defined various tokeniza-tion rules, treating tokens in different parts of emails separately, and computed token probabilities and combined spam probability based on Bayes rule, but in a different way. Gary Robinson, in [7] suggested enhancements to Paul X  X  approach by proposing Bayesian approach of handling rare words and in [8] further rec-ommended to use Fisher X  X  inverse chi-square function for combining probability estimations. This paper presents a new and novel approach which, on the top of all evolutions and enhancements in Bayesian approach, takes into account of the relation between the same tokens occurred in different areas of the email. evolutions and variants of statistical spam filtering algorithms. Sect. 3 presents the main idea of this paper. Sect. 4 describes the experiments and analysis. Finally, Sect. 5 presents the conclusion of the paper and possible future work. 2.1 Naive Bayes (NB) Algorithm Naive Bayes algorithm is the simplified version of Bayes theorem with the as-sumption of feature independence. It computes the probability of a Class  X  { Spam, Legitimate } given an Email as: conditional probability of the token t i given the Class , which is calculated as in [9] using the following formula. Class and | V | is the size of the vocabulary. The filter classifies an email as Spam or Legitimate according to whether the P (Spam | Email ) is greater than P (Legitimate | Email ) or not. Many implementations have shown that the al-gorithm is fairly robust and powerful in filtering spams and outperforms many other knowledge base filters [4, 10]. 2.2 Paul Graham X  X  (PB) Algorithm Paul Graham [5] calculated the probability estimates for a given email being spam and legitimate, given a token appears in that using the formulas: p ( t )= P (Spam | t )= spam and legitimate emails respectively, and nbad and ngood are the number of spam and legitimate emails respectively. tgood is multiplied by 2 to bias towards legitimate emails. The combined probability for spam is obtained using Bayesian approach using the formula: greater than a defined threshold value of 0.9. 2.3 Gary Robinson X  X  (GR) Algorithm Gary in [7] pointed out several drawbacks with Paul X  X  algorithm and suggested several improvements:  X  Gary proposed consistent and smooth way of dealing rare words by using the  X  In [8], Gary further suggested to use Fisher X  X  inverse chi-square function to In this section, we present our new approach of co-weighted multi-area informa-tion along with preprocessing and feature extraction techniques. 3.1 Preprocessing Due to the prevalence of headers, html and binary attachments in modern emails, We use following preprocessing steps:  X  The whole email structure is divided into 4 areas: (1.) Normal header com- X  All other headers and html tags (except those mentioned above) are ignored  X  All binary attachments, if any, are ignored and so removed. 3.2 Feature Extraction or Tokenization Our approach considers tokens as the sole features for the spam filtering. The remaining text after preprocessing is tokenized using the following tokenizer rules:  X  All terms constituting alphanumeric characters, dash(-), underscore( ), apos- X  IP addresses, domain names, money values (numbers separated by comma  X  For domain name, it is broken into sub-terms (like www.hnu.net is broken  X  Spammer X  X  one of newest tricks of non-HTML text, interspersed with HTML 3.3 Main Idea and Algorithm Description The main idea in our approach lies in the fact that the same token occurred in area separately from that occurring in other areas like in all previous algorithms described in Sect.2 wouldn X  X  reflect the realistic estimation. In this paper, we relate the individual area-wise token probability estimations by co-weighting and obtain the combined integrated estimate for the token. The estimation steps are described in details below.
 a of spam and legitimate emails respectively, Ns and Nl be the number of spam and legitimate emails respectively. Then the probability estimation for spam given the token t and the area a is computed as: out bias factor. Next, the GR X  X  degree of belief estimation f ( t, a ) is computed are, like in GR X  X  algorithm, the belief factor and the assumed probability for an unknown token whose values are determined while tuning the filter for optimal performance. Then the combined probability estimation for the token is calcu-lated by co-weighting the individual estimations corresponding to different areas: factor for the token t corresponding to area a is computed by the ratio of the number of occurrences of the token in that area and total number of occurrences of the token in all areas in all spam emails: cording to token occurrences, it represents better and more realistic estimation. Moreover, since fixed number of interesting tokens as suggested in PG X  X  algo-rithm is unrealistic and unreasonable, we consider all tokens whose probability values are above and below certain offset value PROB OFFSET from the neu-tral 0.5 as interesting. If it gives less than predefined MIN INTTOKENS of tokens, the range is extended to that number. Now values for those interesting tokens are used to obtain the final indicator I of spamminess and hamminess using (7), whereby H and S are calculated by Fisher X  X  inverse chi-square func-tions (6). Finally the email is classified as spam if I is greater than certain threshold value, SPAM THRESHOLD , otherwise classified as legitimate. First we will introduce the corpora collection and performance measures used for performance comparisons and then discuss the experiments and analysis. 4.1 Corpora Collection This paper used three publicly available corpora and from each corpus, training and test datasets are prepared by randomly picking two-thirds of the total corpus data as training dataset and the rest one-third as test dataset. The corpora are: 1. Ling Spam corpus which was made available by Ion Androutsopoulos [2] and 2. Spam Assassin corpus used to optimize the open source SpamAssassin filter. 3. Annexia/Xpert corpus, synthesis of 10,025 spam emails from Annexia spam 4.2 Performance Measures Let N S and N L be the total number of spam and legitimate email messages to be classified by the filter respectively, and N X  X  Y the number of messages belonging to class X that the filter classified as belonging to class Y ( X, Y  X  {
Spam(S), Legitimate(L) } ). Then the seven performance measures are calcu-lated as shown below in four categories: 1. Weighted Accuracy ( WAcc ) and Weighted Error ( WErr ) : 2. Total Cost Ratio ( TCR ) : 3. Spam Recall ( SR ) and Spam Precision ( SP ) : 4. False Positive Rate ( FPR ) and False Negative Rate ( FNR ) : 4.3 Experiments and Analysis We have performed experiments on the filter application we developed in Java. All experiments are carried out five times for all three datasets by randomly picking training and test datasets as described above in Sect. 4.1 and the average results are reported. Our experiments consist of two parts:  X  With individual area-wise estimations:  X  With all areas but treating same tokens in different areas as different tokens ,  X  With our new approach , the performance is further improved with even lesser co-relation between tokens in different areas results significant improvement in the performance of the filter, and at the same time exhibit more stable, robust and consistent performances with all three corpora. In this paper we present the new approach to statistical Bayesian filter based on co-weighted multi-area information. This new algorithm co-relates the area-wise token probability estimations using weight coefficients, which are computed according to the number of occurrences of the token in those areas. Experimental results showed significant improvement in the performance of spam filtering than using individual area-wise as well as using separate estimations for all areas. Moreover, the performances are much more stable and consistent with all three datasets.
 and/or other lexical analyzers and with rich feature extraction methods which can be expected to achieve even better performance.

