 Along the past decade there has been a growing interest in the application of Gaussian Processes (GPs) to machine learning tasks. GPs are probabilistic non-parametric Bayesian models that com-bine a number of attractive characteristics: They achieve state-of-the-art performance on supervised learning tasks, provide probabilistic predictions, have a simple and well-founded model selection scheme, present no overfitting (since parameters are integrated out), etc.
 Unfortunately, the direct application of GPs to regression problems (with which we will be con-cerned here) is limited due to their training time being O ( n 3 ) . To overcome this limitation, several sparse approximations have been proposed [2, 3, 4, 5, 6]. In most of them, sparsity is achieved by projecting all available data onto a smaller subset of size m n (the active set), which is selected according to some specific criterion. This reduces computation time to O ( m 2 n ) . However, active set selection interferes with hyperparameter learning, due to its non-smooth nature (see [1, 3]). These proposals have been superseded by the Sparse Pseudo-inputs GP (SPGP) model, introduced in [1]. In this model, the constraint that the samples of the active set (which are called pseudo-inputs) must be selected among training data is relaxed, allowing them to lie anywhere in the input space. This allows both pseudo-inputs and hyperparameters to be selected in a joint continuous optimisation and increases flexibility, resulting in much superior performance.
 In this work we introduce Inter-Domain GPs (IDGPs) as a general tool to perform inference across domains. This allows to remove the constraint that the pseudo-inputs must remain within the same domain as input data. This added flexibility results in an increased performance and allows to encode prior knowledge about other domains where data can be represented more compactly. We will briefly state here the main definitions and results for regression with GPs. See [7] for a comprehensive review.
 Assume we are given a training set with n samples D  X  { x j ,y j } n j =1 , where each D -dimensional the corresponding output y  X  based on D .
 The GP regression model assumes that the outputs can be expressed as some noiseless latent function plus independent noise, y = f ( x )+  X , and then sets a zero-mean 1 GP prior on f ( x ) , with covariance k ( x , x 0 ) , and a zero-mean Gaussian prior on  X  , with variance  X  2 (the noise power hyperparameter). The covariance function encodes prior knowledge about the smoothness of f ( x ) . The most common choice for it is the Automatic Relevance Determination Squared Exponential (ARD SE): with hyperparameters  X  2 0 (the latent function power) and { ` d } D d =1 (the length-scales, defining how rapidly the covariance decays along each dimension). It is referred to as ARD SE because, when coupled with a model selection method, non-informative input dimensions can be removed automat-ically by growing the corresponding length-scale. The set of hyperparameters that define the GP are  X  = {  X  2 ,  X  2 0 , { ` d } D d =1 } . We will omit the dependence on  X  for the sake of clarity. If we evaluate the latent function at X = { x j } n j =1 , we obtain a set of latent variables following a it is possible to express the joint distribution of training and test cases and then condition on the observed outputs to obtain the predictive distribution for any test case denote the identity matrix of size n . The O ( n 3 ) cost of these equations arises from the inversion of the n  X  n covariance matrix. Predictive distributions for additional test cases take O ( n 2 ) time each. These costs make standard GPs impractical for large data sets.
 To select hyperparameters  X  , Type-II Maximum Likelihood (ML-II) is commonly used. This amounts to selecting the hyperparameters that correspond to a (possibly local) maximum of the log-marginal likelihood, also called log-evidence. In this section we will introduce Inter-Domain GPs (IDGPs) and show how they can be used as a framework for computationally efficient inference. Then we will use this framework to express two previous relevant models and develop two new ones. 3.1 Definition Consider a real-valued GP f ( x ) with x  X  R D and some deterministic real function g ( x , z ) , with z  X  R H . We define the following transformation: There are many examples of transformations that take on this form, the Fourier transform being one of the best known. We will discuss possible choices for g ( x , z ) in Section 3.3; for the moment we will deal with the general form. Since u ( z ) is obtained by a linear transformation of GP f ( x ) , it is also a GP. This new GP may lie in a different domain of possibly different dimension. This transformation is not invertible in general, its properties being defined by g ( x , z ) . IDGPs arise when we jointly consider f ( x ) and u ( z ) as a single,  X  X xtended X  GP. The mean and covariance function of this extended GP are overloaded to accept arguments from both the input and transformed domains and treat them accordingly. We refer to each version of an overloaded function as an instance , which will accept a different type of arguments. If the distribution of the original GP distribution of the extended GP over both domains. The transformed-domain instance of the mean is The inter-domain and transformed-domain instances of the covariance function are: Mean m (  X  ) and covariance function k (  X  ,  X  ) are therefore defined both by the values and domains of their arguments. This can be seen as if each argument had an additional domain indicator used to select the instance. Apart from that, they define a regular GP, and all standard properties hold. In a transformation of the input space, and not the other way around. This allows to pre-specify the desired input-domain covariance. The transformation is also more general: Any g ( x , z ) can be used. which allows to perform inference across domains. We will only be concerned with one input domain and one transformed domain, but IDGPs can be defined for any number of domains. 3.2 Sparse regression using inducing features In the standard regression setting, we are asked to perform inference about the latent function f ( x ) from a data set D lying in the input domain. Using IDGPs, we can use data from any domain to perform inference in the input domain. Some latent functions might be better defined by a set of data lying in some transformed space rather than in the input space. This idea is used for sparse inference.
 D = { Z , u } . The following derivation is analogous to that of SPGP. We will refer to Z as the inducing features and u as the inducing variables. The key approximation leading to sparsity is to set m n and assume that f ( x ) is well-described by the pseudo data set D , so that any two samples (either from the training or test set) f p and f q with p 6 = q will be independent given x p , x q and D . With this simplifying assumption 2 , the prior over f can be factorised as a product of marginals: Operator diag (  X  ) sets all off-diagonal elements to zero, so that  X  f is a diagonal matrix. Since p ( u | Z ) is readily available and also Gaussian, the inducing variables can be integrated out from (7), yielding a new, approximate prior over f ( x ) : p ( f | X , Z ) = Using this approximate prior, the posterior distribution for a test case is: where we have defined Q = K uu + K &gt; fu  X   X  1 y K fu and  X  y =  X  f +  X  2 I n . The distribution (2) is approximated by (8) with the information available in the pseudo data set. After O ( m 2 n ) time precomputations, predictive means and variances can be computed in O ( m ) and O ( m 2 ) time per test case, respectively. This model is, in general, non-stationary, even when it is approximating a stationary input-domain covariance and can be interpreted as a degenerate GP plus heteroscedastic white noise.
 The log-marginal likelihood (or log-evidence) of the model, explicitly including the conditioning on kernel hyperparameters  X  can be expressed as log p ( y | X , Z ,  X  ) =  X  which is also computable in O ( m 2 n ) time.
 Model selection will be performed by jointly optimising the evidence with respect to the hyperpa-rameters and the inducing features. If analytical derivatives of the covariance function are available, conjugate gradient optimisation can be used with O ( m 2 n ) cost per step. 3.3 On the choice of g (x, z) The feature extraction function g ( x , z ) defines the transformed domain in which the pseudo data set lies. According to (3), the inducing variables can be seen as projections of the target function f ( x ) on the feature extraction function over the whole input space. Therefore, each of them summarises information about the behaviour of f ( x ) everywhere. The inducing features Z define the concrete set of functions over which the target function will be projected. It is desirable that this set captures the most significant characteristics of the function. This can be achieved either using prior knowledge about data to select { g ( x , z i ) } m i =1 or using a very general family of functions and letting model selection automatically choose the appropriate set.
 Another way to choose g ( x , z ) relies on the form of the posterior. The posterior mean of a GP is often thought of as a linear combination of  X  X asis functions X . For full GPs and other approximations such as [1, 2, 3, 4, 5, 6], basis functions must have the form of the input-domain covariance function. When using IDGPs, basis functions have the form of the inter-domain instance of the covariance covariance function.
 function r (  X  ) , then both yield the same sparse GP model. This property can be used to simplify the expressions of the instances of the covariance function.
 { g i ( x , z i ) } m i =1 where each z i may even have a different size (dimension). In the sections below we will discuss different possible choices for g ( x , z ) . 3.3.1 Relation with Sparse GPs using pseudo-inputs The sparse GP using pseudo-inputs (SPGP) was introduced in [1] and was later renamed to Fully Independent Training Conditional (FITC) model to fit in the systematic framework of [10]. Since the sparse model introduced in Section 3.2 also uses a fully independent training conditional, we will stick to the first name to avoid possible confusion.
 IDGP innovation with respect to SPGP consists in letting the pseudo data set lie in a different do-lie in the input domain. Thus there is no longer a transformed space and the original SPGP model is retrieved. In this setting, the inducing features of IDGP play the role of SPGP X  X  pseudo-inputs. 3.3.2 Relation with Sparse Multiscale GPs Sparse Multiscale GPs (SMGPs) are presented in [11]. Seeking to generalise the SPGP model with ARD SE covariance function, they propose to use a different set of length-scales for each basis function. The resulting model presents a defective variance that is healed by adding heteroscedastic white noise. SMGPs, including the variance improvement, can be derived in a principled way as IDGPs: With this approximation, each basis function has its own centre  X  = [  X  1 ,  X  2 ,...,  X  d ] &gt; and its inducing features. Equations (10) and (11) are derived from (4) and (5) using (1) and (9). The even if permitted in [11], should be avoided for the model to remain well defined. 3.3.3 Frequency Inducing Features GP If the target function can be described more compactly in the frequency domain than in the input domain, it can be advantageous to let the pseudo data set lie in the former domain. We will pursue that possibility for the case where the input domain covariance is the ARD SE. We will call the resulting sparse model Frequency Inducing Features GP (FIFGP).
 Directly applying the Fourier transform is not possible because the target function is not square integrable (it has constant power  X  2 0 everywhere, so (5) does not converge). We will workaround this by windowing the target function in the region of interest. It is possible to use a square window, but this results in the covariance being defined in terms of the complex error function, which is very slow to evaluate. Instead, we will use a Gaussian window 3 . Since multiplying by a Gaussian in the input domain is equivalent to convolving with a Gaussian in the frequency domain, we will be working with a blurred version of the frequency space. This model is defined by: The inducing features are  X  = [  X  0 ,  X  1 ,...,  X  d ] &gt; , where  X  0 is the phase and the remaining com-ponents are frequencies along each dimension. In this model, both global length-scales { ` d } D d =1 and using (4) and (5). 3.3.4 Time-Frequency Inducing Features GP Instead of using a single window to select the region of interest, it is possible to use a different sulting model combines SPGP and FIFGP, so we will call it Time-Frequency Inducing Features inter-domain and transformed-domain instances of the covariance function are: FIFGP is trivially obtained by setting every centre to zero {  X  i = 0 } m i =1 , whereas SPGP is obtained by setting window length-scales c , frequencies and phases {  X  i } m i =1 to zero. If the window length-scales were individually adjusted, SMGP would be obtained.
 While FIFGP has the modelling power of both FIFGP and SPGP, it might perform worse in prac-tice due to it having roughly twice as many hyperparameters, thus making the optimisation problem harder. The same problem also exists in SMGP. A possible workaround is to initialise the hyperpa-rameters using a simpler model, as done in [11] for SMGP, though we will not do this here. In this section we will compare the proposed approximations FIFGP and TFIFGP with the current state of the art, SPGP on some large data sets, for the same number of inducing features/inputs and therefore, roughly equal computational cost. Additionally, we provide results using a full GP, which is expected to provide top performance (though requiring an impractically big amount of computation). In all cases, the (input-domain) covariance function is the ARD SE (1).
 We use four large data sets: Kin-40k , Pumadyn-32nm 4 (describing the dynamics of a robot arm, used with SPGP in [1]), Elevators and Pole Telecomm 5 (related to the control of the elevators of an F16 aircraft and a telecommunications problem, and used in [12, 13, 14]). Input dimensions that remained constant throughout the training set were removed. Input data was additionally centred for use with FIFGP (the remaining methods are translation invariant). Pole Telecomm outputs actually take discrete values in the 0-100 range, in multiples of 10. This was taken into account by using the corresponding quantization noise variance ( 10 2 / 12 ) as lower bound for the noise hyperparameter 6 . the range spanned by training data along each dimension. For SPGP, pseudo-inputs are initialised deviation of input data, frequencies are randomly chosen from a zero-mean `  X  2 d -variance Gaussian distribution, and phases are obtained from a uniform distribution in [0 ... 2  X  ) . TFIFGP uses the same initialisation as FIFGP, with window centres set to zero. Final values are selected by evidence maximisation.
 Denoting the output average over the training set as y and the predictive mean and variance for test sample y  X  l as  X   X  l and  X   X  l respectively, we define the following quality measures: Normalized Mean Square Error (NMSE)  X  ( y  X  l  X   X   X  l ) 2  X  /  X  ( y  X  l  X  y ) 2  X  and Mean Negative Log-Probability (MNLP)  X  ( y  X  l  X   X   X  l ) 2 / X  2  X  l + log  X  2  X  l + log 2  X   X  , where  X  X  X  averages over the test set. For Kin-40k (Fig. 1, top), all three sparse methods perform similarly, though for high sparseness (the most useful case) FIFGP and TFIFGP are slightly superior. In Pumadyn-32nm (Fig. 1, bottom), only 4 out the 32 input dimensions are relevant to the regression task, so it can be used as an ARD capabilities test. We follow [1] and use a full GP on a small subset of the training data (1024 data points) to obtain the initial length-scales. This allows better minima to be found during optimisation. Though all methods are able to properly find a good solution, FIFGP and especially TFIFGP are better in the sparser regime. Roughly the same considerations can be made about Pole Telecomm and Elevators (Fig. 2), but in these data sets the superiority of FIFGP and TFIFGP is more dramatic. Though not shown here, we have additionally tested these models on smaller, overfitting-prone data sets, and have found no noticeable overfitting even using m &gt; n , despite the relatively high number of parameters being adjusted. This is in line with the results and discussion of [1]. In this work we have introduced IDGPs, which are able combine representations of a GP in differ-ent domains, and have used them to extend SPGP to handle inducing features lying in a different domain. This provides a general framework for sparse models, which are defined by a feature extrac-tion function. Using this framework, SMGPs can be reinterpreted as fully principled models using a transformed space of local features, without any need for post-hoc variance improvements. Further-more, it is possible to develop new sparse models of practical use, such as the proposed FIFGP and TFIFGP, which are able to outperform the state-of-the-art SPGP on some large data sets, especially for high sparsity regimes. Choosing a transformed space for the inducing features enables to use domains where the target function can be expressed more compactly, or where the evidence (which is a function of the fea-tures) is easier to optimise. This added flexibility translates as a detaching of the functional form of the input-domain covariance and the set of basis functions used to express the posterior mean. inducing features . Using ML-II to select the inducing features means that models providing a good fit to data are given preference over models that might approximate the full GP more closely. This, though rarely, might lead to harmful overfitting. To more faithfully approximate the full GP and avoid overfitting altogether, our proposal can be combined with the variational approach from [15], in which the inducing features would be regarded as variational parameters. This would result in more constrained models, which would be closer to the full GP but might show reduced performance. We have explored the case of regression with Gaussian noise, which is analytically tractable, but it is straightforward to apply the same model to other tasks such as robust regression or classification, using approximate inference (see [16]). Also, IDGPs as a general tool can be used for other purposes, such as modelling noise in the frequency domain, aggregating data from different domains or even imposing constraints on the target function.
 Acknowledgments We would like to thank the anonymous referees for helpful comments and suggestions. This work has been partly supported by the Spanish government under grant TEC2008-02473/TEC, and by the Madrid Community under grant S-505/TIC/0223. References
