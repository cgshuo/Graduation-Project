 In order to recommend products to users we must ultimately pre-dict how a user will respond to a new product. To do so we must uncover the implicit tastes of each user as well as the properties of each product. For example, in order to predict whether a user will enjoy Harry Potter , it helps to identify that the book is about wiz-ards, as well as the user X  X  level of interest in wizardry. User feed-back is required to discover these latent product and user dimen-sions. Such feedback often comes in the form of a numeric rating accompanied by review text. However, traditional methods often discard review text, which makes user and product latent dimen-sions difficult to interpret, since they ignore the very text that justi-fies a user X  X  rating. In this paper, we aim to combine latent rating dimensions (such as those of latent-factor recommender systems) with latent review topics (such as those learned by topic models like LDA). Our approach has several advantages. Firstly, we ob-tain highly interpretable textual labels for latent rating dimensions, which helps us to  X  X ustify X  ratings with text. Secondly, our approach more accurately predicts product ratings by harnessing the informa-tion present in review text; this is especially true for new products and users, who may have too few ratings to model their latent fac-tors, yet may still provide substantial information from the text of even a single review. Thirdly, our discovered topics can be used to facilitate other tasks such as automated genre discovery, and to identify useful and representative reviews.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval recommender systems, topic models
Recommender systems have transformed the way users discover and evaluate products on the web. Whenever users assess products, there is a need to model how they made their assessment, either to suggest new products they might enjoy [19], to summarize the important points in their reviews [13], or to identify other users who may share similar opinions [24]. Such tasks have been studied across a variety of domains, from hotel reviews [6] to beer [21].
To model how users evaluate products, we must understand the hidden dimensions, or facets, of their opinions. For example, to understand why two users agree when reviewing the movie Seven Samurai , yet disagree when reviewing Casablanca , it helps to know that these films belong to different genres; these users may have similar preferences toward action movies, but opposite preferences for romantic dramas.

Modeling these hidden factors is key to obtaining state-of-the-art performance on product recommendation tasks [2]. Crucially, rec-ommender systems rely on human feedback , which typically comes in the form of a plain-text review and a numeric score (such as a star rating). This type of feedback is used to train machine learning al-gorithms, whose goal is to predict the scores that users will give to items that they have not yet reviewed [12].

In spite of the wealth of research on modeling ratings , the other form of feedback present on review websites X  X amely, the reviews themselves X  X s typically ignored. In our opinion, ignoring this rich source of information is a major shortcoming of existing work on recommender systems. Indeed, if our goal is to understand (rather than merely predict) how users rate products, we ought to rely on reviews, whose very purpose is for users to explain why they rated a product the way they did. As we show later,  X  X nderstanding X  these factors helps us to justify users X  reviews, and can aid us in tasks like genre discovery and identification of informative reviews.
Thus our goal in this paper is to develop statistical models that combine latent dimensions in rating data with topics in review text. This leads to natural interpretations of rating dimensions, for in-stance we automatically discover that movie ratings are divided along topics like genre, while beer ratings are divided along topics like beer style. Rather than performing post-hoc analysis to make this determination, we discover both rating dimensions and review topics in a single learning stage, using an objective that combines the accuracy of rating prediction (in terms of the mean squared er-ror) with the likelihood of the review corpus (using a topic model). We do this using a transform that aligns latent rating and review terms, so that both are determined by a single parameter. Essen-tially, the text likelihood acts as a  X  X egulariser X  for rating prediction, ensuring that parameters that are good at predicting ratings are also likely in terms of the review text.

Not only does our model help us to  X  X xplain X  users X  review scores by better understanding rating dimensions, but it also leads to better predictions of the ratings themselves. Traditional models (based on ratings alone) are difficult to apply to new users and products, that have too few ratings to model their many latent dimensions or factors [14]. In contrast, our model allows us to accurately uncover such factors from even a single review.

Furthermore, by understanding how hidden rating dimensions relate to hidden review dimensions, our models facilitate a variety of novel tasks. For instance, we apply our models to automatically discover product categories, or  X  X enres X , that explain the variation present in both ratings and reviews. We also apply our models to find  X  X nformative X  reviews, whose text is good at explaining the fac-tors that contributed to a user X  X  rating.
Our main contribution is to develop statistical models that com-bine latent dimensions in rating data with topics in review text. In practice, we claim the following benefits over existing approaches.
Firstly, the topics we obtain readily explain the variation present in ratings and reviews. For example, we discover that beer style (e.g. light versus dark) explains the variation present in beer rat-ing and review data; platform (e.g. console versus PC) explains the variation present in video game data; country (e.g. Italian versus Mexican) explains the variation present in restaurant data, etc.
Secondly, combining ratings with review text allows us to predict ratings more accurately than approaches that consider either of the two sources of data in isolation. This is especially true for  X  X ew X  products: product factors cannot be fit from only a few ratings , though we can accurately model them from only a few reviews . We improve upon state-of-the-art models based on matrix factorization and LDA by 5-10% in terms of the mean squared error.

Thirdly, we can use our models to facilitate novel tasks. We apply our models to the task of automatic genre discovery, where they are an order of magnitude more accurate than methods based on ratings or reviews alone. We also apply our models to the task of identifying informative reviews; we discover that those reviews whose text best  X  X xplains X  the hidden factors of a product are rated as being  X  X seful X  by human annotators on sites like yelp.com .
Last, we apply our models to novel corpora consisting of over forty million reviews, from nine million users and three million products. To our knowledge, this is the largest-scale study of pub-lic review data conducted to date. Our methods readily scale to datasets of this size.
Although many works have studied ratings and review text in isolation, few works attempt to combine the two sources of infor-mation. The most similar line of work is perhaps that of aspect discovery [6, 25, 29]. Aspects generally refer to features that are relevant to all products. Individual users may assign such aspects different weights when determining their overall score; for example a spendthrift hotel reviewer might assign a low weight to  X  X rice X  but a high weight to  X  X ervice X , thus explaining why their overall rating differs from a miserly reviewer who is only interested in price [6].
Though our work shares similarities with such studies, the top-ics we discover are not similar to aspects.  X  X spects X  explain di-mensions along which ratings and reviews vary, which is also our goal. Yet while aspects are dimensions common to every individual review, they may not explain the variation present across entire re-view corpora. For instance, although one may learn that individual users assign different weights to  X  X mell X  and  X  X aste X  when reviewing beers [21], this fails to explain why one user may love the smell of a beer, while another believes the same beer smells terrible.
An early work that combines review text and ratings is [6]. They observe that reviews often discuss multiple aspects, and that a user X  X  rating will depend on the importance that they ascribe to each as-pect. They find that these dimensions can be recovered from review
Symbol Description r u,i rating of item i by user u d u,i review ( X  X ocument X ) of item i by user u rec ( u,i ) prediction of the rating for item i by user u  X  global offset term  X  u bias parameter for user u  X  i bias parameter for item i  X  u K -dimensional latent features for user u  X  i K -dimensional latent features for item i
K number of latent dimensions/topics  X  i K -dimensional topic distribution for item i  X  k word distribution for topic k  X  k unnormalized word distribution for topic k w u,i,j j th word of user u  X  X  review of item i z u,i,j topic for the j th word of user u  X  X  review of item i N d number of words in document d text, and that this information can be harnessed for rating predic-tion. However, their method differs from ours in that their  X  X spects X  are provided by human annotators (e.g. they use domain knowledge to determine that the  X  X spects X  of a restaurant are price, service, etc.) for each of which a sentiment classifier must be trained.
We briefly mention works where aspects are explicit , i.e., where users rate individual features of a product in addition to their overall rating [1, 9, 17, 25], though this work differs from ours in that it re-quires multiple ratings per review. We also briefly mention orthogo-nal work that studies  X  X nterpretations X  of ratings using sources other than reviews, e.g. information from a user X  X  social network [24].
A few works have considered the task of automatically identi-fying review dimensions. Early works discover such dimensions based on frequently occurring noun phrases [7], or more sophisti-cated grammatical rules [23]. More recent works attempt to address this problem in an unsupervised manner [5, 26, 30], however these sophisticated methods are limited to datasets with only a few thou-sand reviews. Furthermore, although related to our work, the goal of such studies is typically summarization [5, 13, 18], or feature discovery [23], rather than rating prediction per se .

Beyond product reviews, modeling the dimensions of free-text is an expansive topic. The goal of Latent Dirichlet Allocation (LDA) [4], like ours, is to discover hidden dimensions in text. Such models have been applied in recommendation settings, e.g. to recommend scientific articles in citation networks [28]. Although related, such approaches differ from ours in that the dimensions they discover are not necessarily correlated with ratings. Variants of LDA have been proposed whose dimensions are related to output variables (such as ratings), for example supervised topic models [3]. This is but one of a broad class of works on sentiment analysis [10, 15], whose goal is to model numeric scores from text.

In spite of the similarity between our work and sentiment anal-ysis, there is one critical difference: the goal of such methods X  X t test time X  X s to predict sentiment from text. In contrast, in rec-ommendation settings such as our own, our goal at test time is to predict ratings of products that users have not reviewed.
We begin by briefly describing the  X  X tandard X  models for latent factor recommender systems and Latent Dirichlet Allocation, be-fore defining our own model. The notation we shall use throughout the paper is defined in Table 1. The  X  X tandard X  latent-factor model [11] predicts ratings r user u and item i according to where  X  is an offset parameter,  X  u and  X  i are user and item biases, and  X  u and  X  i are K -dimensional user and item factors (respec-tively). Intuitively,  X  i can be thought of as the  X  X roperties X  of the product i , while  X  u can be thought of as a user X  X   X  X references X  to-wards those properties. Given a training corpus of ratings T , the parameters  X  = {  X , X  u , X  i , X  u , X  i } are typically chosen so as to minimize the Mean Squared Error (MSE), i.e., where  X ( X ) is a regulariser that penalizes  X  X omplex X  models, for example the ` 2 norm k  X  k 2 2 . A variety of methods exist to optimize (eq. 1), for instance alternating least-squares, and gradient-based methods [11].
 Unlike latent factor models, which uncover hidden dimensions in review ratings , Latent Dirichlet Allocation (LDA) uncovers hidden dimensions in review text . LDA associates each document d  X  D with a K -dimensional topic distribution  X  d (i.e., a stochastic vec-tor), which encodes the fraction of words in d that discuss each of the K topics. That is, words in the document d discuss topic k with probability  X  d,k .

Each topic k also has an associated word distribution,  X  k encodes the probability that a particular word is used for that topic. Finally, the topic distributions themselves (  X  d ) are assumed to be drawn from a Dirichlet distribution.

The final model includes word distributions for each topic  X  topic distributions for each document  X  d , and topic assignments for each word z d,j . Parameters  X  = {  X , X  } and topic assignments z are traditionally updated via sampling [4]. The likelihood of a particular text corpus T (given the word distribution  X  and topic assignments for each word) is then where we are multiplying over all documents in the corpus, and all words in each document. The two terms in the product are the like-lihood of seeing these particular topics (  X  z d,j ), and the likelihood of seeing these particular words for this topic (  X  z d,j
Our model, which we title  X  X idden Factors as Topics X , or HFT for short, attempts to combine these two ideas. Unlike supervised topic models (for example), that learn topics that are correlated with an output variable [3], HFT discovers topics that are correlated with the  X  X idden factors X  of products and users,  X  i and  X  u .
Topic models operate on documents , so first we must define the concept of a  X  X ocument X  in HFT. We shall derive documents from review text, so an obvious choice is to define each review as a doc-ument d u,i (for user u and item i ). Alternately, we could define a document d i as the set of all reviews of an item i , or d of all reviews by a user u .

Although we will later consider other alternatives, for the mo-ment, let us consider the set of all reviews of a particular item i as a document d i . Our reasoning is that when users review products, they tend to discuss properties of the product more than they dis-cuss their own personal preferences; as we see in Section 4, model-ing documents in this way leads to the best performance in practice.
With documents defined in this way, for each item i we learn a topic distribution  X  i (a stochastic vector, i.e.,  X  i  X   X  tor encodes the extent to which each of K topics is discussed across all reviews for that product. Note that we implicitly assume that the number of rating factors is the same as the number of review topics.
Given our motivation, we do not wish to learn rating parameters  X  and review parameters  X  i independently. Rather, we want the two to be linked. Intuitively, rating  X  X actors X   X  i can be thought of as properties that a product possesses; users will then give the product a high rating if they  X  X ike X  these properties according to  X  other hand,  X  X opics X   X  i define particular distributions of words that appear in reviews of a product. By linking the two, we hope that if a product exhibits a certain property (high  X  i,k ), this will correspond to a particular topic being discussed (high  X  i,k ).

However, such a transformation is non-trivial, for example, we cannot simply define the two to be equal. Critically,  X  i tic vector, i.e., each of its entries describes a probability that a topic is discussed, while rating factors  X  i can take any value in
If we simply enforced that  X  i was also stochastic, we would lose expressive power in our rating model, yet if we relaxed the require-ment that  X  i was stochastic, we would lose the probabilistic inter-pretation of review topics (which is crucial for sampling). In short, we desire a transform that allows arbitrary  X  i  X  R K , while enforc-ing  X  i  X   X  K (i.e., P k  X  ik = 1 ). We also desire a transform that is monotonic , i.e., it should preserve orderings so that the largest values of  X  i should also be the largest values of  X  i . Therefore to link the two we define the transformation Here the exponent in the denominator enforces that each  X  positive, and the numerator enforces that P k  X  i,k = 1 . In this way  X  i acts as a natural parameter for the multinomial defined by  X  [27]. We introduce the parameter  X  (which we fit during learn-ing) to control the  X  X eakiness X  of the transformation. As  X   X   X  ,  X  will approach a unit vector that takes the value 1 only for the largest index of  X  i ; as  X   X  0 ,  X  i approaches a uniform distribu-tion. Intuitively, large  X  means that users only discuss the most important topic, while small  X  means that users discuss all topics evenly.

Note that in practice, we do not fit both  X  and  X  , since one uniquely defines the other (in practice we fit only  X  ). For conve-nience, we shall still use  X  i when referring to rating parameters, and  X  i when referring to topics.

Our final model is based on the idea that the factors  X  i accurately model users X  ratings (as in eq. 1), but also that the review corpus should be  X  X ikely X  when these factors are transformed into topics (as in eq. 3). To achieve this, we define the objective of a corpus T (ratings and reviews) as f ( T|  X  ,  X  , X ,z ) = X Recall that  X  and  X  are rating and topic parameters (respectively),  X  controls the transform (eq. 4), and z is the set of topic assign-ments for each word in the corpus T . The first part of this equation is the error of the predicted ratings as in (eq. 1), while the second part is the (log) likelihood of the review corpus as in (eq. 3).  X  is a hyperparameter that trades-off the importance of these two effects.
Note that the presence of the corpus likelihood in (eq. 5) is criti-cal, even if our ultimate goal is only to predict ratings. Essentially, the corpus likelihood acts as a regulariser for the rating prediction model, replacing the regulariser  X  from (eq. 1). When few ratings are available for a product i (or user u ), the regulariser  X  of (eq. 1) has the effect of pushing  X  i and  X  u toward zero. What this means in practice is that the standard model of (eq. 1) reduces to an offset and bias term for products and users that have few ratings.
Alternately, HFT can accurately predict product factors  X  for items with only a few reviews; or, for users with few reviews if we model documents at the level of users. In essence, a small number of reviews tells us much more about a product or a user than we can possibly glean from the same number of ratings .
In the next section, we describe in detail how we fit the parame-ters  X  and  X  of the HFT model.
Our goal is to simultaneously optimize the parameters associated with ratings  X  = {  X , X  u , X  i , X  u , X  i } and the parameters associated with topics  X  = {  X , X  } . That is, given our review and rating corpus T our objective is to find Recall that  X  and  X  are linked through (eq. 4), so that both  X  and  X  depend on  X  . So, a change in  X  modifies both the rating error and the corpus likelihood in (eq. 5). Thus we cannot optimize  X  and  X  independently.

Typically, recommender parameters like those in (eq. 1) are fit by gradient descent, while those of (eq. 3) would be found by Gibbs sampling [4]. Since HFT consists of a combination of these two parts, we use a procedure that alternates between the two steps.
We define a stochastic optimization procedure consisting of the following two steps, which we describe in more detail below:
In the first of these steps (eq. 7) topic assignments for each word (our latent variable z ) are fixed. We then fit the remaining terms,  X  ,  X  , and  X  , via gradient descent. We use L-BFGS, a quasi-Newton method for non-linear optimization of problems with many variables [22]. This is similar to  X  X tandard X  gradient-based meth-ods [11], but for the presence of the additional terms arising from (eqs. 3 and 4), whose gradients are easily computed.

The second step (eq. 8) iterates through all documents d and all word positions j and updates their topic assignments. As with LDA, we assign each word to a topic (an integer between 1 and K ) randomly, with probability proportional to the likelihood of that topic occurring with that word. Recalling that each item i has a K -dimensional topic distribution  X  i , for each word w u,i,j review of item i ), we set z u,i,j = k with probability proportional to  X  i,k  X  k,w u,i,j . This expression is the probability of the topic k being used for this product (  X  i,k ), multiplied by the probability of the particular word w u,i,j being used for the topic k (  X 
To ensure that  X  k (the word distribution for topic k ) is a stochas-tic vector ( P w  X  k,w = 1 ), we introduce an additional variable  X  and define As with (eq. 4), we can then optimize  X  k  X  R D to determine  X  k  X   X  D (where D is the size of the dictionary, i.e., the num-ber of unique words in the corpus). That is,  X  k  X  R D acts as a natural parameter for the multinomial  X  k  X   X  D .

The above procedure (eq. 8) is analogous to updating topic as-signments using LDA. The critical difference is that in HFT, topic proportions  X  are not sampled from a Dirichlet distribution, but in-stead are determined based on the value of  X  t found during the pre-vious step, via (eq. 4). What this means in practice is that only a sin-gle pass through the corpus is required to update z . We only sample new topic assignments z once  X  t has been updated by (eq. 7).
Finally, the two steps (eqs. 7 and 8) are repeated until conver-gence, i.e., until the change in  X  and  X  between iterations is suffi-ciently small. Although our objective (like other  X  X tandard X  recom-mender system objectives) is certainly non-convex [12] and subject to local minima, in our experience it yields similar topics (though possibly permuted) when restarting with different initial conditions.
In this section, we show that: 1. Our model leads to more accurate predictions (in terms of the 2. HFT allows us to address the  X  X old-start X  problem. HFT X  X  3. HFT allows us to automatically discover product categories 4. HFT can be used to automatically identify representative re-
We collect review data from a variety of public sources. Our primary source of data is Amazon , from which we obtain approx-imately 35 million reviews. To obtain this data, we started with a list of 75 million asin-like strings (Amazon product identifiers) ob-tained from the Internet Archive; 1 around 2.5 million of them had at least one review. We further divide this dataset into 26 parts based on the top-level category of each product (e.g. books, movies). This dataset is a superset of existing publicly-available Amazon datasets, such as those used in [8, 16, 19] and [21].

We also consider 6 million beer and wine reviews previously studied in [19, 21], though in addition we include pub data from ratebeer.com , and we consider restaurant data from citysearch.com , previously used in [6]. Finally, we include 220 thousand reviews from the recently proposed Yelp Dataset Challenge . 2
A summary of the data we obtain is shown in Table 2. In total, we obtain 42 million reviews, from 10 million users and 3 million items. Our datasets span a period of 18 years and contain a total of 5.1 billion words. Data and code are available online. 3
We compare HFT to the following baselines: (a) Offset only: Here we simply fit an offset term (  X  in eq. 1), by taking the average across all training ratings. In terms of the MSE, this is the best possible constant predictor. (b) Latent factor recommender system: This is the  X  X tandard X  latent factor model, i.e., the model of (eq. 1). We fit all parameters using L-BFGS [22]. http://archive.org/details/asin_listing/ https://www.yelp.com/dataset_challenge/ http://snap.stanford.edu/data/ per review, time of oldest review). (c) Product topics learned using LDA: Finally, as a baseline that combines text with product features, we consider Latent Dirichlet Allocation [4]. By itself, LDA learns a set of topics and topic pro-portions (stochastic vectors) for each document. By treating each  X  X ocument X  as the set of reviews for a particular product, LDA gen-erates a stochastic vector per product (  X  i ), which we use to set  X  With  X  i fixed, we then fit  X  ,  X  u ,  X  i , and  X  u using L-BFGS. In other words, we fit a  X  X tandard X  recommender system, except that  X  using the per-document topic scores produced by LDA. Like HFT, this baseline benefits from review text at training time, but unlike HFT its topics are not chosen so as to explain variation in ratings.
We compare these baselines to two versions of HFT: (d) HFT, user topics In this version of HFT, topics in review text are associated with user parameters  X  u . (e) HFT, item topics In this version of HFT, topics in review text are associated with item parameters  X  i .
We randomly subdivide each of the datasets in Table 2 into train-ing, validation, and test sets. We use 80% of each dataset for train-ing, up to a maximum of 2 million reviews. The remaining data is evenly split between validation and test sets. Offset and bias terms  X  ,  X  u , and  X  i are initialized by averaging ratings and residuals; other parameters are initialized uniformly at random. Parameters for all models are fit using L-BFGS, which we run for 2,500 it-erations. After every 50 iterations, topic assignments are updated (for HFT), and the Mean Squared Error (MSE) is computed. We report the MSE (on the test set) for the model whose error on the validation set is lowest.

Experiments were run on commodity machines with 32 cores and 64gb of memory. Our largest dataset fits on a single machine, and our update equations (eqs. 7 and 8) are na X vely parallelizable. Using L-BFGS on Beeradvocate data (for example), our algorithm required 30 minutes to fit, compared to 11 minutes for a  X  X tandard X  latent-factor recommender system. Our largest model (Amazon books) required around one day to fit. Although our optimization procedure yields local optima, we observed less than 3% deviation in MSE and energy across ten random initializations.
Results in terms of the Mean Squared Error are shown in Table 3. HFT achieves the best performance on 30 of the 33 datasets we consider (columns d and e). Due to space constraints, we only show average results across the 26 Amazon product categories. 4
On average, across all datasets with K = 5 topics, a latent fac-tor model achieves an MSE of 1.262; latent dirichlet allocation achieves a similar MSE of 1.253. HFT (with item topics) achieves on average an MSE of 1.178. Increasing the number of topics to K = 10 improves the performance of all models by a small amount, to 1.262 (latent factor model), 1.253 (LDA), and 1.176 (HFT). With K = 10 HFT achieves the best performance on 32 out of 33 datasets.

On several categories, such as  X  X lothing X  and  X  X hoes X , we gain improvements of up to 20% over state-of-the-art methods. Ar-guably, these categories where HFT is the best performing are the most  X  X ubjective X ; for example, if a user dislikes the style of a shirt, or the size of a shoe, they are arguably revealing as much about
Refer to our extended version for complete tables [20]. themselves as they are about the product. By using review text, HFT is better able to  X  X ease apart X  the objective qualities of a prod-uct and the subjective opinion of the reviewer.

It is worth briefly addressing the fact that we obtain only small benefits with K &gt; 10 topics, contrary to established wisdom that traditional latent-factor models often use many more [12]. We note that while there may be many factors that influence a user X  X  rating, only a few of these factors may be discussed in a typical review. A natural extension of HFT would be to augment it with additional latent factors, some of which are not constrained to be related to review text.
 We have primarily focused on how HFT can be used to align prod-uct features  X  i with review topics  X  i . However, it can also discover topics associated with users . This simply means treating the set of all reviews by a particular user u as a document, with an associated topic vector  X  u ; HFT then associates  X  u with  X  u as in (eq. 4).
Results for HFT, trained to model user features with text, are shown in Table 3 (column d). Both models exhibit similar per-formance; their MSEs differ by less than 1%. A few individual datasets show greater variation, e.g. item features are more infor-mative in the  X  X usic X  category, while user features are more infor-mative in the  X  X ffice X  category (see our extended version [20]).
This high level of similarity indicates that there is not a sub-stantial difference between user and product topics. This seems surprising in light of the fact that our product topics tend to cor-respond with product categories (as we show later); we would not necessarily expect product categories to be similar to user cate-gories. However a simple explanation is as follows: users do not review all products with the same likelihood, but rather they have a preference towards certain categories. Thus, while a  X  X ark beer X  topic emerges from reviews of dark beers, a  X  X ark beer X  topic also emerges from users who preferentially review dark beers. In both cases, the topics we discover are qualitatively similar.
The  X  X old-start X  problem is a common issue in recommender sys-tem. In particular, when a product or a user is new and one does not have enough rating data available, it is very hard to train a rec-ommender system and make predictions.
 In particular, with latent factor recommender systems, as with HFT, each user and product is associated with K + 1 parameters ( K -dimensional latent factors and a bias term). As a consequence, when given only a few ratings for a particular user u or product i (e.g. fewer than K ratings), a latent factor recommender system cannot possibly estimate  X  u or  X  i accurately due to lack of data. The presence of the regulariser  X ( X ) in (eq. 1) pushes  X  u towards zero in such cases, meaning that such users and products are modeled using only their bias terms.

However, review text provides significant additional information and our hope is that by including review text the relationship be-tween products and ratings can be more accurately modeled. Even a single review can tell us many of a product X  X  properties, such as its genre. Our hypothesis is that when training data for a product is scarce (e.g. when a new product appears in the corpus), the benefit gained from using review text will be greatest.

In Figure 1 we compare the amount of training data (i.e., the number of training reviews available for a particular product, x-axis) to the improvement gained by using HFT (y-axis). Specif-ically, we report the MSE of a latent-factor recommender system minus the MSE of HFT; thus a positive value indicates that HFT has better performance. Results are shown for K = 5 on our three largest datasets (Amazon books, movies, and music). Figure 1 (left) shows the absolute improvement in MSE when using HFT compared to a latent factor recommender system. Notice that our model gains significant improvement when training data is partic-ularly scarce. The improvement on movie data is the largest, while for books the  X  X tandard X  model is on par with HFT once ratings are abundant. Similar results are obtained for new users . Some of the topics discovered by HFT are shown in Table 4. Topics are shown with K = 5 for beer, musical instruments, video game, and clothing data, and for K = 10 for Yelp data. We find that the most common words (in all topics) are stop words ( X  X he X / X  X nd X / X  X t X  etc.), so to visualize each topic, we first compute averages across all topics for each word w : b w = 1 K P k This generates a  X  X ackground X  distribution that includes words that are common to all topics. Subtracting this value from  X  k,w topics with common words and stop words removed. These values (i.e.,  X  k,w  X  b w ) are shown in Table 4. Note that we show all K topics, i.e., none are hand-selected or excluded.

The topics we discover are clean and easy to interpret. Discov-ered topics are similar to genres, or categories of products. For in-stance, beer topics include pale ales, lambics (sour Belgian beers), dark beers, and wheat beers, with an additional topic describing spices. From a modeling perspective, there is a simple explana-tion as to why HFT discovers  X  X enre-like X  topics. Firstly, users are likely to rate products of the same genre similarly, so separating products into genres explains much of the variation in rating data. Secondly, different language is used to describe products from dif-ferent genres, so genres also explain much of the variation in review data. As we see in the next section, if our goal is to discover genres, combining these two sources of data (ratings and reviews) leads to much better performance than using either in isolation.
We noted that the  X  X opics X  discovered by HFT are similar to prod-uct  X  X ategories X , or  X  X enres X . Now, we confirm this quantitatively, and investigate the possibility of using HFT to automatically dis-cover such categories. For brevity we focus on the Yelp Phoenix dataset, which contains detailed product category information.
Each of the models compared in the previous section outputs a K -dimensional vector per product, where K is the number of latent product/user dimensions (  X  i in the case of LDA,  X  case of the latent factor model and HFT). We define the category of a product, which we denote c i , to be the latent dimension with the highest weight, i.e., (or argmax k  X  i,k for LDA). Thus each product has a  X  X ategory X  between 1 and K . We then compute the best alignment between these K categories and the K most popular (i.e., most frequently occurring) categories in the Yelp dataset. We denote by C of products whose predicted category is k (i.e., { i | c i C k the set of products whose true category is k . The score of the optimal correspondence is then where f (  X  ) is a one-to-one correspondence (computed by linear as-signment) between predicted product categories k and ground-truth categories f ( k ) ; (eq. 11) is then the average F 1 score between the predicted and the true product categories. to a latent factor model on three Amazon datasets.
 easily interpretable. Table 5: Genre discovery (on Yelp data). Values shown are average F 1 score between the predicted and the ground-truth product categories (higher is better).

This score is shown in Table 5 for different values of K . Nei-ther latent factor models nor LDA produce latent dimensions that are similar to genres, while HFT recovers these genres accurately; with 50 genres HFT outperforms latent factor models and LDA by over 300%. It is worth noting that K = 50 is far larger than what we found beneficial when predicting ratings in Section 4.4. We note that when K is small,  X  X imilar X  genres tend to be combined in a single topic, e.g.  X  X oga X  and  X  X assage X  in Table 4. Pleasingly, this implies that even after further topics no longer improve rating prediction accuracy, HFT continues to generate meaningful topics.
In addition to modeling how users rate products, HFT can iden-tify reviews that users are likely to find  X  X seful X . As we have shown, for each product i , HFT generates a topic vector  X  i , which deter-mines the distribution of topics, and consequently the distribution of words , that are likely to be used when describing that product. We use this topic distribution to identify  X  X epresentative X  reviews. Specifically, we identify reviews r u,i whose language matches the topic distribution  X  i closely. Recall that in HFT,  X  X opics X  are learned in order to explain the variation present in product ratings. Thus, in order for a reviewer to adequately explain their rating, they ought to discuss each of these topics in proportion to their importance.
Recall that for each word w u,i,j ( j th word in user u  X  X  review of item i ), HFT estimates a topic assignment z u,i,j for that word (i.e., each word belongs to a topic from 1 to K ). Given the full set of words in a review d u,i , we can then compute the proportion of each topic in that review. Specifically, we compute Here we are simply counting the number of times each topic k oc-curs in a review d u,i , and normalizing so that the entries of  X  Figur e 2: Reviews that are  X  X seful X  (according to Yelp users) are those whose choice of language matches the topic distribution learned by HFT. sum to 1. In this way, we have a topic distribution for each product,  X  , and a topic distribution for each individual review,  X  u,i which are stochastic vectors).

Our definition of a  X  X epresentative X  review is one for which  X  is similar to  X  i , i.e., a review that discusses those topics that explain the variation in product i  X  X  ratings. We identify this by computing the distance
We hypothesize that reviews with small d (  X  i , X  u,i ) will be  X  X ep-resentative X  of the product i . We evaluate this quantitatively by comparing d (  X  i , X  u,i ) to  X  X sefulness X  ratings on Yelp.
In Figure 2 we compare Yelp  X  X seful X  ratings (non-negative in-tegers) to d (  X  j , X  u,i ) . Results are shown for K  X  { 5 , 10 , 20 , 50 } . We observe a clear relationship between the two quantities. When K = 5 (for example), reviews with even two useful votes are half the distance from  X  i compared to reviews with no useful votes; re-views with 10 useful votes are one third the distance from  X  experiment suggests that in addition to predicting ratings, HFT can also be used to identify  X  X seful X , or  X  X epresentative X  reviews.
We have presented HFT, a model that combines ratings with re-view text for product recommendations. HFT works by aligning hidden factors in product ratings with hidden topics in product re-views . Essentially, these topics act as regularisers for latent user and product parameters. This allows us to accurately fit user and product parameters with only a few reviews , which existing models cannot do using only a few ratings . We evaluated HFT on large, novel corpora consisting of over forty million product reviews. In addition to more accurately predicting product ratings, HFT discov-ers highly interpretable product topics, that can be used to facilitate tasks such as genre discovery and to suggest informative reviews. Thanks to David Blei for comments on an initial draft of this paper, and to Dan Jurafsky for discussions. This research has been sup-ported in part by NSF IIS-1016909, CNS-1010921, CAREER IIS-1149837, IIS-1159679, ARO MURI, DARPA SMISC, the Okawa Foundation, Docomo, Boeing, Allyes, Volkswagen, Intel, as well as the Alfred P. Sloan and the Microsoft Faculty Fellowships.
