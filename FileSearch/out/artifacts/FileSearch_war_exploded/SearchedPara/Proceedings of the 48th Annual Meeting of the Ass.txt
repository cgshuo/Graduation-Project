 1 Introduction Probabilistic grammars have been found to be useful for investigating the architecture of the human sentence processing mechanism (Jurafsky, 1996; Crocker and Brants, 2000; Hale, 2003; Boston et al., 2008; Levy, 2008; Demberg and Keller, 2009). For example, probabilistic models shed light on so-called locality effects: contrast the non-probabilistic hypothesis that dependants which are far away from their head always cause processing difficulty for readers due to the cost of storing the intervening material in memory (Gibson, 1998), compared to the probabilistic prediction that there are cases when faraway dependants facilitate processing, because readers have more time to predict the head (Levy, 2008). Using a computational model to address funda-mental questions about sentence comprehension motivates the work in this paper.

So far, probabilistic models of sentence pro-cessing have been largely limited to syntactic factors. This is unfortunate because many out-standing questions in psycholinguistics concern interactions between different levels of process-ing. This paper addresses this gap by building a computational model which simulates the influence of discourse on syntax.

Going beyond the confines of syntax alone is a sufficiently important problem that it has attracted attention from other authors. In the literature on probabilistic modeling, though, the bulk of this work is focused on lexical semantics (e.g. Pad  X  o et al., 2006; Narayanan and Jurafsky, 1998) or only considers syntactic decisions in the preceed-ing text (e.g. Dubey et al., 2009; Levy and Jaeger, 2007). This is the first model we know of which introduces a broad-coverage sentence processing model which takes the effect of coreference and discourse into account.

A major question concerning discourse-syntax interactions involves the strength of communica-tion between discourse and syntactic information. The Weakly Interactive (Altmann and Steed-man, 1988) hypothesis states that a discourse context can reactively prune syntactic choices that have been proposed by the parser, whereas the Strongly Interactive hypothesis posits that context can proactively suggest choices to the syntactic processor.

Support for Weak Interaction comes from experiments in which there are temporary ambi-guities, or garden paths , which cause processing difficulty. The general finding is that supportive contexts can reduce the effect of the garden path. However, Grodner et al. (2005) found that sup-portive contexts even facilitate the processing of unambiguous sentences. As there are no incorrect analyses to prune in unambiguous structures, the authors claimed their results were not consistent with the Weakly Interactive hypothesis, and suggested that their results were best explained by a Strongly Interactive processor.
 The model we present here implements the Weakly Interactive hypothesis, but we will show that it can nonetheless successfully simulate the results of Grodner et al. (2005). There are three main parts of the model: a syntactic processor, a coreference resolution system, and a simple pragmatics processor which computes certain limited forms of discourse coherence. Following Hale (2001) and Levy (2008), among others, the syntactic processor uses an incremental proba-bilistic Earley parser to compute a metric which correlates with increased reading difficulty. The coreference resolution system is implemented in a probabilistic logic known as Markov Logic (Richardson and Domingos, 2006). Finally, the pragmatics processing system contains a small set of probabilistic constraints which convey some intuitive facts about discourse processing. The three components form a pipeline, where each part is probabilistically dependent on the previous one. This allows us to combine all three into a single probability for each reading of an input sentence. The rest of the paper is structured as follows. In Section 2, we discuss the details two experiments showing support of the Weakly and Strongly In-teractive hypotheses: we discuss Grodner et al. X  X  result on unambiguous syntactic structures and we present a new experiment on involving a garden path which was designed to be similar to the Grod-ner et al. experiment. Section 3 introduces techni-cal details of model, and Section 4 shows the pre-dictions of the model on the experiments discussed in Section 2. Finally, we discuss the theoretical consequences of these predictions in Section 5. 2 Cognitive Experiments 2.1 Discourse and Ambiguity Resolution There is a fairly large literature on garden path experiments involving context (Crain and Steed-man, 1985; Mitchell et al., 1992, ibid). The experiments by Altmann and Steedman (1988) involved PP attachment ambiguity. Other authors (e.g. Spivey and Tanenhaus, 1998) have used reduced relative clause attachment ambiguity. In order to be more consistent with the design of the experiment in Section 2.2, however, we performed our own reading-time experiment which partially replicated previous results. 1
The experimental items all had a target sen-tence containing a relative clause, and one of two possible context sentences, one of which supports the relative clause reading and the other which does not.

The context sentence was one of: (1) a. There were two postmen, one of The target sentences, which were drawn from the experiment of McRae et al. (1998), were either the reduced or unreduced sentences similar to: (2) The postman who was carried by the The reduced version of the sentence is produced by removing the words who was . We measured reading times in the underlined region, which is the first point at which there is evidence for the relative clause interpretation. The key evidence is given by the word  X  X y X , but the previous word is in-cluded as readers often do not fixate on short func-tion words, but rather process them while overtly fixating on the previous word (Rayner, 1998).
The relative clauses in the target sentence act as restrictive relative clauses, selecting one referent from a larger set. The target sentences are there-fore more coherent in a context where a restricted set and a contrast set are easily available, than one in which these sets are absent. This makes the context in Example (1-a) supportive of a reduced relative reading, and the context in Example (1-b) unsupportive of a reduced relative clause. Other experiments, for instance Spivey and Tanenhaus (1998), used an unsupportive context where only one postman was mentioned. Our experiments used a neutral context, where no postmen are mentioned, to be more similar to the Grodner et al. experiment, as described below.

Overall, there were 28 items, and 28 partici-pants read these sentences using an EyeLink II eyetracker. Each participant read items one at a time, with fillers between subsequent items so as to obfuscate the nature of the experiment.
 Results An ANOVA revealed that all conditions with a supportive context were read faster than one with a neutral context (i.e. a main effect of con-text), and all conditions with unambiguous syntax were read faster than those with a garden path (i.e. a main effect of ambiguity). Finally, there was a statisically significant interaction between syntax and discourse whereby context decreases reading times much more when a garden path is present compared to an unambiguous structure. In other words, a supportive context helped reduce the effect of a garden path. This is the prediction made by both the Weakly Interactive and Strongly Interactive hypothesis. The pattern of results are shown in Figure 2a in Section 4, where they are directly compared to the model results. 2.2 Discourse and Unambiguous Syntax As mentioned in the Introduction, Grodner et al. (2005) proposed an experiment with a supportive or unsupportive discourse followed by an unam-biguous target sentence. In their experiment, the target sentence was one of the following: (3) a. The director that the critics praised They also manipulated the context, which was either supportive of the target, or a null context. The two supportive contexts are: (4) a. A group of film critics praised a The target sentence in (3-a) is a restrictive relative clause, as in the garden path exper-iments. However, the sentence in (3-b) is a non-restrictive relative clause, which does not assume the presence of a constrast set. Therefore, the context (4-a) is only used with the restrictive relative clause, and the context (4-b), where only one director is mentioned, is used as the context for the non-restrictive relative clause. In the conditions with a null context, the target sentence was not preceded by any contextual sentence. Results Grodner et al. measured residual reading times, i.e. reading times compared to a baseline in the embedded subject NP ( X  X he critics X ). They found that the supportive contexts decreased reading time, and that this effect was stronger for restrictive relatives compared to non-restricted relatives. As there was no garden path, and hence no incorrect structure for the discourse processor to prune, the authors conclude that this must be evidence for the Strongly Interactive hypothesis. Unlike the garden path experiment above, these results do not appear to be consistent with a Weakly Interactive model. We plot their results in Figure 3a in Section 4, where they are directly compared to the model results. Because these results are computed as regressions against a baseline, a reading time of 0ms indicates average difficulty, with negative numbers showing some facilitation has occured, and positive number indicating reading difficulty. 3 Model The model comprises three parts: a parser, a coreference resolution system, and a pragmatics subsystem. Let us look at each individually. 3.1 Parser The parser is an incremental unlexicalized proba-bilistic Earley parser, which is capable of comput-ing prefix probabilities. A PCFG parser outputs the generative probability P parser ( w, t ) , where w is the text and t is a parse tree. A probabilistic Earley parser can retrieve all possible derivations at word i (Stolcke, 1995), allowing us to compute the prob-ability P ( w i . . . w 0 ) = P t P parser ( w i . . . w 0
Using the prefix probability, we can compute the word-by-word Surprisal (Hale, 2001), by taking the log ratio of the previous word X  X  prefix probability against this word X  X  prefix probability:
Higher Surprisal scores are interpreted as being correlated with more reading difficulty, and likewise lower scores with greater reading ease. For most of the remainder of the paper we will simply refer to the prefix probability at word i as P ( w ) . While the prefix probability as presented here is suitable for syntax-based computations, a main technical contribution of our model, detailed in Sections 3.2 and 3.3 below, is that we include non-syntactic probabilities in the computation of Surprisal.

As per Hale X  X  original suggestion, our parser can compute Surprisal using an exhaustive search, which entails summing over each licensed deriva-tion. This can be done efficiently using the packed representation of an Earley chart. However, as the coreference processor takes trees as input, we must therefore unpack parses before resolving referential ambiguity. Given the ambiguity of our grammar, this is not tractable. Therefore, we only consider an n -best list when computing Surprisal. As other authors have found that a relatively small set of analyses can give meaningful predictions (Brants and Crocker, 2000; Boston et al., 2008), we set n = 10 .

The parser is trained on the Wall Street Journal (WSJ) section of the Penn treebank. Unfortu-nately, the standard WSJ grammar is not able to give correct incremental parses to our experimen-tal items. We found we could resolve this problem by using four simple transformations, which are shown in Figure 1: (i) adding valency information to verb POS tags (e.g. VBD1 represents a tran-sitive verb); (ii) we lexicalize  X  X y X  prepositions; (iii) VPs containing a logical subject (i.e. the agent), get the -LGS label; (iv) non-recursive NPs are renamed NPbase (the coreference system treats each NPbase as a markable). 3.2 Discourse Processor The primary function of the discourse processing module is to perform coreference resolution for each mention in an incrementally processed text. Because each mention in a coreference chains is transitive, we cannot use a simple classifier, as they cannot enforce global transitivity constraints. Therefore, this system is implemented in Markov Logic (Richardson and Domingos, 2006), a probabilistic logic, which does allow us to include such constraints.

Markov Logic attempts to combine logic with probabilities by using a Markov random field where logical formulas are features. The Expression Meaning Coref ( x , y ) x is coreferent with y .
 First ( x ) x is a first mention.
 Order ( x , y ) x occurs before y .
 SameHead ( x , y ) ExactMatch ( x , y ) x and y are same string. SameNumber ( x , y ) x and y match in number. SameGender ( x , y ) x and y match in gender. SamePerson ( x , y ) x and y match in person. Distance ( x , y , d ) Pronoun ( x ) x is a pronoun.

EntityType ( x , e ) Markov Logic Network (MLN) we used for our system uses similar predicates as the MLN-based corference resolution system of Huang et al. (2009). 2 Our MLN uses the predicates listed in Table 1. Two of these predicates, Coref and First , are the output of the MLN  X  they provide a labelling of coreference mentions into entity classes. Note that, unlike Huang et al., we assume an ordering on x and y if Coref ( x , y ) is true: y must occur earlier in the document than x . The remaining predicates in Table 1 are a subset of features used by other coreference resolution systems (cf. Soon et al., 2001). The predicates we use involve matching strings (checking if two mentions share a head word or if they are exactly the same string), matching argreement features (if the gender, number or person of pairs of NPs are the same; especially important for pronouns), the distance between mentions, and if mentions have the same entity type (i.e. do they refer to a person, organization, etc.) As our main focus is not to produce a state-of-the-art coreference system, we do not include predicates which are irrevelant for our simulations even if they have been shown to be effective for coreference resolution. For example, we do not have predicates if two mentions are in an apposition relationship, or if two mentions are synonyms for each other.

Table 2 lists the actual logical formulae which are used as features in the MLN. It should be noted that, because we are assuming an order on the arguments of Coref ( x , y ) , we need three formulae to capture transivity relationships. To test that the coreference resolution system was producing meaningful results, we evaluated our system on the test section of the ACE-2 dataset. Using b 3 scoring (Bagga and Baldwin, 1998), which computes the overlap of a proposed set with the gold set, the system achieves an F -score of 65.4%. While our results are not state-of-the-art, they are reasonable considering the brevity of our feature list.

The discourse model is run iteratively at each word. This allows us to find a globally best assignment at each word, which can be reanalyzed at a later point in time. It assumes there is a mention for each base NP outputted by the parser, and for all ordered pairs of mentions x, y , it outputs all the  X  X bserved X  predicates (i.e. ev-erything but First and Coref ), and feeds them to the Markov Logic system. At each step, we compute both the maximum a posteriori (MAP) assignment of coreference relationships as well as the probability that each individual coreference assignment is true. Taken together, they allow us to calculate, for a coreference assignment c , P coref ( c | w, t ) where w is the text input (of the entire document until this point), and t is the parse of each tree in the document up to and including the current incremental parse. As we have previ-ously calculated P parser ( w, t ) , it is then possible to compute the joint probability P ( c, w, t ) at each word, and therefore the prefix probability P ( w ) due to syntax and coreference. Overall, we have:
P ( w ) = X
Note that we only consider one possible as-signment of NPs to coreference entities per parse, as we only retrieve the probabilities of the MAP solution. 3.3 Pragmatics Processor The effect of context in the experiments described in Section 2 cannot be fully explained using a coreference resolution system alone. In the case of restrictive relative clauses, the referential  X  X is-match X  in the unsupported conditions is caused by an expectation elicited by a restrictive relative clause which is inconsistent with the previous discourse when there is no salient restricted subset of a larger set. When the larger set is not found in the discourse, the relative clause becomes incoherent given the context, causing reading difficulty. Modeling this coherence constraint is essentially a pragmatics problem, and is under the purview of the pragmatics processor in our sys-tem. The pragmatics processor is quite specialised and, although the information it encapsulates is quite intuitive, it nonetheless relies on hand-coded expert knowledge.

The pragmatics processor takes as input an incremental pragmatics configuration p and computes the probability P prag ( p | w, t, c ) . The pragmatics configuration we consider is quite simple. It is a 3-tuple where one element is true if the current noun phrase being processed is a discourse new definite noun phrase, the second element is true if the current NP is a discourse new indefinite noun phrase, and the final element is true if we encounter an unsupported restrictive relative clause. We simply conjecture that there is little processing cost (and hence a high proba-bility) if the entire vector is false; there is a small processing cost for discourse new indefinites, a slightly larger processing cost for discourse new definites and a large processing cost for an incoherent reduced relative clause.

The first two elements of the 3-tuple depend on the identity of the determiner as recovered by the parser, and on whether the coreference system adduces the predicate F irst for the current NP. As the coreference system wasn X  X  designed to find anaphoric contrast sets, these sets were found using a simple post-processing check. This post-processing approach worked well for our experi-mental items, but finding such sets is, in general, quite a difficult problem (Modjeska et al., 2003).
The distribution P prag ( p | w, t, c ) applies a processing penalty for an unsupported restrictive relative clause whenever a restrictive relative clause is in the n best list. Because Surprisal computes a ratio of probabilities, this in ef-fect means we only pay this penality when an unsupported restrictive relative clause first appears in the n best list (otherwise the effect is cancelled out). The penalty for discourse new entities is applied on the first word (ignoring punctuation) following the end of the NP. This spillover processing effect is simply a matter of modeling convenience: without it, we would have to compute Surprisal probabilities over regions rather than individual words. Thus, the overall prefix probability can be computed as: P ( w ) = P which is then substituted in Equation (1) to get a Surprisal prediction for the current word. 4 Evaluation 4.1 Method When modeling the garden path experiment we presented in Section 2.1, we compute Surprisal values on the word  X  X y X , which is the earliest point at which there is evidence for a relative clause interpretation. For the Grodner et al. experiment, we compute Surprisal values on the relativiser  X  X ho X  or  X  X hat X . Again, this is the earliest point at which there is evidence for a relative clause, and depending upon the presence or absence of a pre-ceding comma, it will be known to be restrictive or nonrestrictive clause. In addition to the overall Surprisal values, we also compute syntactic Surprisal scores, to test if there is any benefit from the discourse and pragmatics subsystems. As we are outputting n best lists for each parse, it is also straightforward to compute other measures which predict reading difficulty, including pruning (Jurafsky, 1996), whereby processing difficulty is predicted when a parse is removed from the n best list, and attention shift (Crocker and Brants, 2000), which predicts parsing difficulty at words where the most highly ranked parse flips from one interpretation to another.

For the garden path experiment, the simulation was run on each of the 28 experimental items in each of the 4 conditions, resulting in a total of 112 runs. For the Grodner et al. experiment, the sim-ulation was run on each of the 20 items in each of the 4 conditions, resulting in a total of 80 runs. For each run, the model was reset, purging all dis-course information gained while reading earlier items. As the system is not stochastic, two runs us-ing the exact same items in the same condition will produce the same result. Therefore, we made no attempt to model by-subject variability, but we did perform by-item ANOVAs on the system output. 4.2 Results Garden Path Experiment The simulated results of our experiment are shown in Figure 2. Comparing the full simulated results in Figure 2b to the experimental results in Figure 2a, we find that the simulation, like the actual experiment, finds both main effects and an interaction: there is a main effect of context whereby a supportive context facilitates reading, a main effect of syntax whereby the garden path slows down reading, and an interaction in that the effect of context is strongest in the garden path condition. All these effects were highly significant at p &lt; 0 . 01 . The pattern of results between the full simulation and the experiment differed in two ways. First, the simulated results suggested a much larger reading difficulty due to ambiguity than the experimental results. Also, in the unambiguous case, the model predicted a null cost of an unsupportive context on the word  X  X y X , because the model bears the cost of an unsupportive context earlier in the sentence, and assumes no spillover to the word  X  X y X . Finally, we note that the syntax-only simulation, shown in Figure 2c, only produced a main effect of ambigu-ity, and was not able to model the effect of context. Grodner et al. Experiment The simulated results of the Grodner et al. experiment are shown in Figure 3. In this experiment, the pattern of simulated results in Figure 3b showed a much closer resemblance to the experimental results in Figure 3a than the garden path experiment. There is a main effect of context, which is much stronger in the restrictive relative case compared to non-restrictive relatives. As with the garden path experiment, the ANOVA reported that all effects were significant at the p &lt; 0 . 01 level. Again, as we can see from Figure 3c, there was no effect of context in the syntax-only simulation. The nu-merical trend did show a slight facilitation in the unrestricted supported condition, with a Surprisal of 4 . 39 compared to 4 . 41 in the supported case, but this difference was not significant. 4.3 Discussion We have shown that our incremental sentence pro-cessor augmented with discourse processing can successfully simulate syntax-discourse interaction effects which have been shown in the literature. The difference between a Weakly Interactive and Strongly Interactive model can be thought of computationally in terms of a pipeline architecture versus joint inference. In a weaker sense, even a pipeline architecture where the discourse can influence syntactic probabilities could be claimed to be a Strongly Interactive model. However, as our model uses a pipeline where syntactic probabilities are independent of the discourse, we claim that our model is Weakly Interactive. Unlike Altmann and Steedman, who posited that the discourse processor actually removes parsing hypotheses, we were able to simulate this pruning behaviour by simply re-weighting parses in our coreference and pragmatics modules.

The fact that a Weakly Interactive system can simulate the result of an experiment proposed in support of the Strongly Interactive hypothesis is initially counter-intuitive. However, this naturally falls out from our decision to use a probabilistic model: a lower probability, even in an unambigu-ous structure, is associated with increased reading difficulty. As an aside, we note that when using realistic computational grammars, even the struc-tures used in the Grodner et al. experiment are not unambiguous. In the restrictive relative clause condition, even though there was not any compe-tition between a relative and main clause reading, our n best list was at all times filled with analyses. For example, on the word  X  X ho X  in the restricted relative clause condition, the parser is already predicting both the subject-relative ( X  X he postman who was bit by the dog X ) and object-relative ( X  X he postman who the dog bit X ) readings.

Overall, these results are supportive of the growing importance of probabilistic reasoning as a model of human cognitive behaviour. Therefore, especially with respect to sentence processing, it is necessary to have a proper understanding of how probabilities are linked to real-world behaviours. We note that Surprisal does indeed show processing difficulty on the word  X  X y X  in the garden path experiment. However, Figure 4 (which shows the top three parses on the word  X  X y X ) indicates that not only are there still main clause interpretations present, but in fact, the top two parses are main clause interpretations. This is also true if we limit ourselves to syntactic probabilities (which are the probabilities listed in Figure 4). This suggests that neither Jurafsky (1996) X  X  notion of pruning as processing difficulty nor Crocker and Brants (2000) notion of attention shifts would correctly predict higher reading times on a region containing the word  X  X y X . In fact, the main clause interpretation remains the highest-ranked interpretation until it is finally pruned at an auxiliary of the main verb of the sentence ( X  X he postman carried by the paramedics was having  X ).
This result is curious as our experimental items closely match some of those simulated by Crocker and Brants (2000). We conjecture that the differ-ence between our attention shift prediction and theirs is due to differences in the grammar. It is possible that using a more highly tuned grammar would result in attention shift making the correct prediction, but this possibly shows one benefit of using Surprisal as a linking hypothesis. Because Surprisal sums over several derivations, it is not as reliant upon the grammar as the attention shift or pruning linking hypotheses. 5 Conclusions The main result of this paper is that it is possible to produce a Surprisal-based sentence process-ing model which can simulate the influence of discourse on syntax in both garden path and unambiguous sentences. Computationally, the inclusion of Markov Logic allowed the discourse module to compute well-formed coreference chains, and opens two avenues of future re-search. First, it ought to be possible to make the probabilistic logic more naturally incremental, rather than re-running from scratch at each word. Second, we would like to make greater use of the logical elements by applying it to problems where inference is necessary, such as resolving bridging anaphora (Haviland and Clark, 1974).

Our primary cognitive finding that our model, which assumes the Weakly Interactive hypothesis (whereby discourse is influenced by syntax in a reactive manner), is nonetheless able to simulate the experimental results of Grodner et al. (2005), which were claimed by the authors to be in support of the Strongly Interactive hypothesis. This suggests that the evidence is in favour of the Strongly Interactive hypothesis may be weaker than thought.

Finally, we found that the attention shift (Crocker and Brants, 2000) and pruning (Jurafsky, 1996) linking theories are unable to correctly simulate the results of the garden path experiment. Although our main results above underscore the usefulness of probabilistic modeling, this obser-vation emphasizes the importance of finding a tenable link between probabilities and behaviours. Acknowledgements We would like to thank Frank Keller, Patrick Sturt, Alex Lascarides, Mark Steedman, Mirella Lapata and the anonymous reviewers for their insightful comments. We would also like to thank ESRC for their financial supporting on grant RES-062-23-1450.
 References Gerry Altmann and Mark Steedman. Inter-action with context during human sentence processing. Cognition , 30:191 X 238, 1988.
 Amit Bagga and Breck Baldwin. Algorithms for scoring coreference chains. In The First Inter-national Conference on Language Resources and Evaluation Workshop on Linguistics Coreference (LREC 98) , 1998.
 Marisa Ferrara Boston, John T. Hale, Reinhold
Kliegl, and Shravan Vasisht. Surprising parser actions and reading difficulty. In Proceedings of ACL-08:HLT, Short Papers , pages 5 X 8, 2008. Thorsten Brants and Matthew Crocker. Probabilis-tic parsing and psychological plausibility. In
Proceedings of 18th International Conference on Computational Linguistics (COLING-2000) , pages 111 X 117, 2000.
 Stephen Crain and Mark Steedman. On not being led down the garden path: the use of context by the psychological syntax processor. In
D. Dowty, L. Karttunen, and A. Zwicky, edi-tors, Natural language parsing: Psychological, computational, and theoretical perspectives . Cambridge University Press, 1985.
 Matthew Crocker and Thorsten Brants. Wide coverage probabilistic sentence processing.
Journal of Psycholinguistic Research , 29(6): 647 X 669, 2000.
 Vera Demberg and Frank Keller. A computational model of prediction in human parsing: Unifying locality and surprisal effects. In Proceedings of the 29th meeting of the Cognitive Science Society (CogSci-09) , 2009.
 Amit Dubey, Frank Keller, and Patrick Sturt.
A probabilistic corpus-based model of paral-lelism. Cognition , 109(2):193 X 210, 2009.
 Amit Dubey, Patrick Sturt, and Frank Keller. The effect of discourse inferences on syntactic am-biguity resolution. In Proceedings of the 23rd Annual CUNY Conference on Human Sentence Processing (CUNY 2010) , page 151, 2010.
 Ted Gibson. Linguistic complexity: Locality of syntactic dependencies. Cognition , 68:1 X 76, 1998.
 Daniel J. Grodner, Edward A. F. Gibson, and
Duane Watson. The influence of contextual constrast on syntactic processing: Evidence for strong-interaction in sentence comprehension. Cognition , 95(3):275 X 296, 2005.
 John T. Hale. A probabilistic earley parser as a psycholinguistic model. In In Proceedings of the Second Meeting of the North American Chapter of the Asssociation for Computational Linguistics , 2001.
 John T. Hale. The information conveyed by words in sentences. Journal of Psycholinguistic Research , 32(2):101 X 123, 2003.
 Susan E. Haviland and Herbert H. Clark. What X  X  new? acquiring new information as a process in comprehension. Journal of Verbal Learning and Verbal Behavior , 13:512 X 521, 1974.
 Shujian Huang, Yabing Zhang, Junsheng Zhou, and Jiajun Chen. Coreference resolution using markov logic. In Proceedings of the 2009 Conference on Intelligent Text Processing and Computational Linguistics (CICLing 09) , 2009. D. Jurafsky. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science , 20:137 X 194, 1996.
 Roger Levy. Expectation-based syntactic compre-hension. Cognition , 106(3):1126 X 1177, March 2008.
 Roger Levy and T. Florian Jaeger. Speakers optimize information density through syntactic reduction. In Proceedings of the Twentieth Annual Conference on Neural Information
Processing Systems , 2007. Ken McRae, Michael J. Spivey-Knowlton, and
Michael K. Tanenhaus. Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and Language , 38:283 X 312, 1998.
 Don C. Mitchell, Martin M. B. Corley, and Alan
Garnham. Effects of context in human sentence parsing: Evidence against a discourse-baed proposal mechanism. Journal of Experimental
Psychology: Learning, Memory and Cognition , 18(1):69 X 88, 1992.
 Natalia N. Modjeska, Katja Markert, and Malvina
Nissim. Using the web in machine learning for other-anaphora resolution. In Proceedings of the 2003 Conference on Empirical Methods in
Natural Language Processing (EMNLP-2003) , pages 176 X 183, Sapporo, Japan, 2003.
 Shrini Narayanan and Daniel Jurafsky. Bayesian models of human sentence processing. In Pro-ceedings of the 20th Annual Conference of the Cognitive Science Society (CogSci 98) , 1998. Ulrike Pad  X  o, Matthew Crocker, and Frank Keller.
Modelling semantic role plausability in human sentence processing. In Proceedings of the 28th Annual Conference of the Cognitive Science Society (CogSci 2006) , pages 657 X 662, 2006. Hoifung Poon and Pedro Domingos. Joint unsu-pervised coreference resolution with markov logic. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08) , 2008.
 Keith Rayner. Eye movements in reading and information processing: 20 years of research. Psychological Bulletin , 124(3):372 X 422, 1998. Matthew Richardson and Pedro Domingos.
Markov logic networks. Machine Learning , 62 (1-2):107 X 136, 2006.
 W. M. Soon, H. T. Ng, and D. C. Y. Lim. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics , 27(4):521 X 544, 2001.
 M. J. Spivey and M. K. Tanenhaus. Syntactic ambiguity resolution in discourse: Modeling the effects of referential context and lexical frequency. Journal of Experimental Psychol-ogy: Learning, Memory and Cognition , 24(6): 1521 X 1543, 1998.
 Andreas Stolcke. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics , 21(2):165 X 201, 1995.
