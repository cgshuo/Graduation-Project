 REGULAR PAPER Eduardo Torres Schumann  X  Klaus U. Schulz Abstract The replacement of textual units by synonymous canonical forms is an important prerequisite for many variants of automated text analysis. In scientific texts, one common normalization step is the consistent replacement of acronyms by their definitions. For many acronyms, the definition is found at a certain position of the text where the acronym is introduced and  X  X xpanded X  to a synonymous sequence of full words. A recent approach to detecting acronym-expansion pairs by Park and Byrd [ 19 ] describes possible graphical correspondences between acronyms and expansions by means of fine-grained rules. Here we show how rule sets as used in [ 19 ] can be translated into hidden Markov models that abstract from details of the graphical correspondence and improve recall in a significant way. Sta-bility in terms of precision is ensured by exploiting simple properties of the expansion with an optional reinforcement of linguistic knowledge. With this extension of the original formalism, the introduction of large rule sets can be avoided and a fixed model can be applied to a large variety of texts without retraining, with good values both for recall and precision.
 Keywords Acronym recognition  X  Biomedical texts  X  Automated text analysis  X  Terminological expressions  X  Hidden Markov models 1 Introduction Terminological expressions [ 2 , 4 , 11 , 27 ] play a crucial role in document indexing and information retrieval [ 6 , 22 ], text classification [ 3 , 13 ], machine-assisted translation [ 7 , 8 ], and computational lexicology [ 20 ]. They yield a more precise picture of the contents of a given document and a more succinct description of a specific concept than general key-words. In domain-specific scientific and technical texts, ter-minological expressions are often replaced by acronyms. For automated text understanding, normalization techniques are important that expand acronyms and lead to a unique sym-bolic representation of a fixed concept or entity. at some position: the meaning of the acronyms is explained in a corresponding sequence of words, the expansion, or def-inition. In this paper we consider methods for automatically detecting acronym-expansion pairs in texts. These methods help to normalize the given text, and they may also be used for constructing acronym dictionaries for narrow domains where ambiguities can be controlled.
 mining are particularly relevant for the field of biomedi-cal texts [ 1 , 9 , 10 , 24 ]. The current interest in genetics and biomedicine has recently led to an enormous number of pub-lications. For example, more than 12 million publications are now available in the PubMed (Medline) database [ 15 ], and 460,000 references were added in 2002 [ 18 ]. Since the use of acronyms in biomedical texts is very popular, automated detection of acronyms and their definitions represents one important step toward improved methods for automated text analysis in this genre.
 originated in [ 25 ] and have been studied by various authors [ 12 , 17 , 19 , 21 , 26 , 29 ]. In our own work, we chose rule-based approaches [ 12 , 19 , 26 ] as a starting point. In these approaches, only a modest set of linguistic background re-sources is needed, which simplifies implementation. For our initial experiments, we implemented and evaluated the ap-proach described in [ 19 ]. This method models the creation of an acronym in a declarative, flexible, and natural way as a kind of rewriting process, improving ideas from earlier work [ 12 , 26 ].
 tion, the central problem is the selection of a good rule set on the basis of a given training corpus. If the set of rules is too small, then many acronym definitions in the evalua-tion/application corpus are not captured appropriately. The manual collection of rule sets with sufficient coverage on the basis of training data is costly, in particular in situations where most rules have a low frequency. Large rule sets, on the other hand, may give rise to ambiguities that are difficult to resolve and lead to reduced precision. Hence the question arises as to whether the selection of rule sets is convergent in the sense that after a series of training experiments a sta-ble rule set can be found that leads to good results on any (realistic) evaluation corpus.
 pora. An error analysis showed that most of the rules needed for training or test corpora are very specific. This explains why rules derived on training corpora often do not help for test corpora. We then developed a new method that can be considered as a modification and extension of the original formalism.
 in Park X  X  and Byrd X  X  approaches can be directly translated into hidden Markov models (HMMs) of a particular type. Roughly speaking, transition sequences in these HMMs rep-resent  X  X eneralized rules X  where we abstract from details of the graphical correspondence between acronyms and expan-sions. States and probability parameters for the HMMs are directly obtained from the corresponding rule sets, so no ad-ditional training is needed. In general, the derived HMMs al-low for many transition paths that are  X  X ew X  in the sense that they do not correspond to a rule already seen in the training corpora. This explains why translated HMMs are more flex-ible and able to find many acronym-expansion pairs where a new rule would be needed in the original approach. expansion candidates, which in the hybrid approach was mainly expressed in the rules, is now more strongly con-trolled and evaluated in the preference scheme. Combining HMMs with these methods, a loss of precision can be avoided and we end up with a method that is stable in the above sense and leads to satisfactory values for precision and recall. Since trained HMMs can be directly obtained by translating rule sets as those used in [ 19 ], our formalism inherits most advantages of the latter method, such as declarativity, flexible user customization, adaptivity to new styles, and editorial conventions.
 look at typical acronyms occurring in biomedical texts and introduce the terminology that is used in the remaining sec-tions. In Sect. 3 we briefly summarize the algorithmic prob-lem considered in this paper and formally define variants of precision and recall that are used later. Section 4 gives a compact description of the hybrid approach [ 19 ]. Evalua-tion results that measure the stability (in the aforementioned sense) of this approach are given in Sect. 5. We also analyze the errors that typically occur when applying the method to a new class of documents. In Sect. 6 we describe our tech-nique for translating rule sets into HMMs. Evaluation results for our own method are given in Sect. 7. Section 8 comments on related work. Some points for future work are summa-rized in the conclusion. 2 Acronyms in biomedical texts Using examples from biomedical texts we briefly review dis-tinct ways in which acronyms and their explanations are in-troduced in texts and examine in more detail the possible graphical relationship between an acronym and its expan-sion. 2.1 Terminology Acronyms can be described as short sequences of sym-bols that represent abbreviations for compound technical ex-pressions and concepts. The acronym and its expansion are treated as synonyms. We may distinguish between proper acronyms , where we have a purely graphical correspon-dence between the acronym and its expansion, and pseu-doacronyms , where the relationship between the abbrevia-tion and the underlying expression is more indirect. In this paper, by acronym we always mean a proper acronym unless noted otherwise.
 Example 1 The sequence  X  X PA X  represents a possible acronym for  X  X rokinase-type plasminogen activator X  since each letter of  X  X PA X  occurs in the expansion. In contrast,  X  X  2 O X  is not a proper acronym for  X  X ater X  since the associ-ation between both expressions is not graphical but rather based on special encoding conventions for chemical sub-stances.
 classes. For some acronyms, the proper expansion is given in the text. Other  X  X opular X  acronyms are used without any explanation, assuming that the reader knows how to interpret the sequence.
 Example 2 Figure 1 depicts an abstract from the Med-line [ 15 ] database. Occurrences of acronyms and pseu-doacronyms are written in bold. Acronyms where the ex-pansion can be found somewhere in the abstract are written in Roman letters.  X  X opular X  acronyms not defined in the text are written in italics.
 Remark 1 Biomedical texts, as well as other texts, use a considerable number of acronyms without explicitly ex-plaining their meaning in the text. Even with special background dictionaries, the correct interpretation of these  X  X lien X  acronyms is difficult due to a large number of ambi-guities. Acronyms are introduced for an enormous number of distinct entities and concepts. Often a given entity can be referred to using different terminological expressions. A given terminological expression may be encoded using dis-tinct acronyms. Since short sequences are preferred, most acronyms can be associated with a substantial number of concepts or entities. When interpreting a given text, the con-text and domain may be used to obtain a partial disambigua-tion. However, often a complete disambiguation is not pos-sible.
 are not defined in the text is ignored for the rest of this paper. In order to analyze text passages where the meaning of an acronym is explained, we distinguish between the following: 1. The acronym ; 2. The expansion of the acronym; 3. The introduction context , i.e., the minimal window of the text where we find both the expansion and the acronym, possibly additional intermediate text; 4. The graphical relationship between acronym and expan-sion. This relation ship can typically be described as a mapping from the symbols of the acronym to occurrences of symbols in the expansion.
 Example 3 Figure 2 illustrates these concepts using two introductions for the acronym  X  X AI-1 X  from the Medline database. The acronym is written in bold. Its expansion  X  X lasminogen activator inhibitor type-1 X  is highlighted us-ing a bright window. The introduction context is highlighted using a dark window. 2.2 Introduction contexts The recognition of introduction contexts for acronyms is one important subtask in the automatic collection of acronyms and their expansions. In texts, the following techniques for introducing acronyms may be distinguished.
 ply written in brackets. The situation where the acronym is written immediately after the expansion can be seen as a standard case. Since deviations are possible, difficulties may arise: tween the acronym and its expansion is explicitly stated using natural language terms such as in the following examples: language expressions that are used to express synonymy. Still, such a list will necessarily be incomplete. Furthermore, problems may be caused by multifunctional expressions like  X  X r. X  ples. The automated mapping from acronyms to expansions is complicated by a number of further phenomena.
 duction  X  ... antisense (AS) oligodeoxynucleotide (ODN) technology ... . X  Later the combined acronym  X  X S-ODN X  is used. In order to find its expansion, we have to combine the distributed expansions for  X  X S X  and  X  X DN. X  acronym is further extended in a new introduction. In Fig. 1 we find  X  ... phosphorothioate (PS)-derivatized AS-ODN ... , X  which explains the expressions  X  X ntisense-PS-ODN X  and  X  X S-PS-ODN X  used below. Similarly,  X  X PA X  is extended to  X  X PA receptor (uPAR). X  acronyms are explained in an enumerative context. To find the correct expansion, special mechanisms for these constructions have to be taken into account:  X  ... total body (TBW) and extracellular (ECW) water ... . X  2.3 Graphical correspondence The most obvious graphical relationship is given if the acronym is composed of initial letters of words of the ex-pansion and if the order of initial letters is preserved in the acronym. However, we often find a more complex situation. word of the expansion that are used in the acronym. Fur-thermore, symbols from distinct words may be separated or written in a directly consecutive way. For example, in the Medline corpus the acronyms  X  X S-ODN, X   X  X sOdn, X  and  X  X S ODN X  are used for  X  X ntisense oligodeoxynucleotide. X  Generally, the use of uppercase and lowercase letters in acronyms does not follow any systematic scheme, which may be seen from introduction contexts such as  X  X ecombi-nant interferon (rIFN) X . Another phenomenon is deviations concerning the expected order of symbols:  X  X orseradish per-oxidase conjugated cholera toxin B (CB-HRP). X  The latter two examples show that often inner symbols from tokens are selected for creating acronyms. Other acronyms copy prefixes.
 ogy. For example,  X  X DNs X  may indicate a plural, and a derivation  X  X nti-ODN X  may result in  X  X ODN. X  Variants of acronyms are used for pragmatic reasons to add emphasis or to suppress parts of the expansion that are clear from the given context (e.g.,  X  X S-PS-ODN X  as  X  X S-AS-ODN, X   X  X ntisense-PS-ODN, X  or  X  X S-ODN X ). 3 Algorithmic problem and evaluation parameters The algorithmic problem studied in this paper is the follow-ing: given a text T , find the acronyms defined in the text and their expansions. The quality of an algorithm A for solving the above problem may be measured in terms of standard no-tions of precision and recall. To formalize these notions we assume that the output set of A contains pairs of the form
A , E ,where A represents a sequence of symbols treated as an acronym by the algorithm and E represents portions of text treated as the expansion of A . The pair A , E is correct if in fact A is an acronym and E is its expansion. The preci-sion of A (w.r.t. T ) is defined as the percentage of all correct answer pairs among the number of all answer pairs, and re-call is defined as the percentage of all correct answer pairs w.r.t. the total number of all acronym-expansion pairs in T . were introduced using implicit synonymy. To search texts for acronyms, we used a regular expression, (see below for details). Matches for in a given text that are surrounded by brackets are treated as acronym candidates . We first search for occurrences of acronym candidates and then calculate the most plausible expansion in the neighborhood of a given occurrence. By -restricted recall we mean the modified recall value where we only refer to those acronym-expansion pairs in the given text where the acronym is parenthesized and matches .
 tion of acronym candidates and the same kind of search strategy.  X  X ecall X  values in the literature very often repre-sent values for some form of restricted recall. These  X  X e-call X  values are not directly comparable; their significance strongly depends on the generality of the pattern used for finding acronym candidates. Note that in order to measure absolute recall we have to find all acronym-expansion pairs of the evaluation corpus, which is difficult if the corpus is large. In the conclusion, we briefly comment on absolute re-call values for the methods discussed below.
 recall we only consider the top-ranked expansion candidate for a given acronym candidate. Related recall values based on top-n expansion candidate sets would make sense for in-teractive systems but are ignored here.
 values relies on improved methods for correctly recogniz-ing introduction contexts. To analyze distinct sources of er-ror and to measure the relevance of improved methods for context recognition, we sometimes look at  X  X dealized X  pre-cision values where the answer set is restricted in the sense that only proper acronyms in real introduction contexts are taken into account. The idealized precision value then says in which percentage of all these cases the proper expansion was found. The value can be seen as an upper limit for the improvements that can be expected from better methods for recognizing real introduction contexts.
 metaproblem faced here is the following: find a stable method for extracting acronym-expansion pairs, i.e., a method that is applicable to a large variety of texts without retraining and leads to good values for precision and recall. 4 The hybrid approach  X  X ybrid text mining X  [ 19 ] represents a sophisticated rule-based method for extracting acronyms and building up on and refining techniques from earlier approaches [ 12 , 26 ]. The graphical relationship between acronym and expansion is described by a special set of matching operators. For searching expansion candidates and for selecting a preferred candidate, morphological and syntactic properties of words are taken into account. The approach is called  X  X ybrid X  since it uses a variety of linguistic background knowledge bases (prefix list, replacement table, dictionary, see below) and since each rule comes with an application probability that is used for selecting the preferred expansion candidate. In the remainder of this section, we introduce the variant of hybrid text mining that we used in our experiments with biomedical texts. Our own HMM-based method uses this variant as an ingredient for parameter estimation at the end of the initial training phase. Some places where we deviate from the orig-inal approach were motivated by our experiments. All major modifications are mentioned below.
 mented into sentences using some simple heuristics similar to those described in [ 16 ]. Each sentence is then split into tokens. Token delimiters are tabulators, white spaces, occur-rences of the symbols  X ; X ,  X : X ,  X , X  followed by white space, and periods marking a sentence end. 4.1 Acronym candidates and skeletons Acronym candidates are expressions A that occur in a paren-thesized form in the text and match the following regular expression, 1 . ( [ a  X  zA  X  Z ]\ X  ? [ a  X  zA  X  Z ] ) | ( [ a  X  zA  X  Z ] ( \ X  ? [ a  X  zA  X  Z ] ) { 2 } ) | ( [ a  X  zA  X  Z ] ( \ X  ? [ a  X  zA  X  Z ] ) { 3 } ) | ( [ a  X  zA  X  Z ] ( \ X  ? [ a  X  zA  X  Z ] ) { 4 } ) | ( [ a  X  zA  X  Z ] ( \ X  ? [ a  X  zA  X  Z ] ) { 2 }\ X  ? [ 1  X  9 ]+ ) captures tokens that may contain hyphens and have 2 X 5 (uppercase or lowercase) letters, or three letters followed by a number. Letters of the acronym candidate are said to have type c , and maximal connected sequences of digits are said to have type n . In this way, each candidate is split into a sequence of acronym units and associated with a unique se-quence over the alphabet { c , n } , which is called the skeleton of the acronym candidate. The  X - X  symbol is ignored in the structural description.
 Example 4 The units of SN-1999 are  X  X , X   X  X , X  and  X 1999, X  which means that the skeleton is ccn.
 Remark 2 In [ 19 ] acronym candidates are all tokens of length l ,2  X  l  X  10, that start with a letter or a digit and contain at least one uppercase letter. Excluded are the initial words of sentences, uppercase words from a general background dictionary, and words in a predefined list of stop words and false positives. 4.2 Search windows, morphemes, expansion candidates The search for expansion candidates takes place within a window in front of a given acronym candidate A . 2 The win-dow length (number of tokens) is L + 10, where L = 2  X | A if | A | &lt; 5and L =| A |+ 5 otherwise. Here | A | denotes the number of symbols of A . Text analysis in the search win-dow is based on a simplified morphological analysis where tokens are split into units called morphemes 3 using the back-ground dictionary. For splitting tokens into morphemes we use the following borders: (1) transitions from letters to dig-its and vice versa and (2) the symbols  X / X ,  X  \  X ,  X   X   X ,  X ( X , X ) X ,  X  X  X , X  X  X , and  X - X . From the resulting units words with prefixes from the list is found in an English general dictionary used for this pur-pose. Both parts are then treated as independent morphemes. Eventually five types of morphemes are distinguished: s : stop words (of, the, and, in, to, a, with, by), p : prefixes from the aforementioned predefined list, h : rest of a word obtained after deleting a prefix (the rest must be in the dictionary), n : sequences of digits, w :otherwords. search window represents an expansion candidate . As with acronym candidates, the sequence of morpheme types of an expansion candidate is used as a simplified representation . Example 5 The morphemes of  X 10% transplantation rate X  are  X 10 X  (type n ),  X  X rans X  (type p ),  X  X lantation X  (type h ),  X  X ate X  (type w ), which means that the representation is nphw . 4.3 Rule-based correspondence between acronym and expansion Acronym construction is described as a process whereby certain rewrite operators are applied to the morphemes of the expansion. The resulting string, after an optional additional permutation, yields the acronym (cf. Fig. 3 ). The possible ways of deriving acronyms from expansions are described by means of a finite collection of rules . The left-hand side of a rule specifies the sequence of morpheme types of the expansion. The right-hand side describes the form of the acronym and encodes the graphical correspondence.
 Example 6 The rule www s w  X  X  F ( 1 ) : c ][ F ( 2 ) : c ][ F ( 3 ) : c ][ F is applicable to any expansion E consisting of a sequence of morphemes respectively of type w, w, w, s ,w (left-hand side). The first unit of the acronym, which has type c , is ob-tained from applying the operator F to the first morpheme of E (right-hand side, [ F ( 1 ) : c ] ) . Operator F selects the first letter of a given morpheme. The remaining units of the acronym are the letters obtained by applying operator F respectively to the second, third, and fifth morphemes of the expansion.
 in the acronym the original order of symbols in the text is permuted. 4 The operators used in the original approach [ 19 ] are the following.
 F : (First match) Defined as above.

I : (Inner match) This operator nondeterministically selects
L : (Last match) This operator selects the last letter of a
E : (Exact match) This operator completely selects a mor-
R : (Replacement) This operator replaces a morpheme by Example 7 Figure 3 illustrates the rewriting operations en-coded in the rule www n  X  X  F ( 1 ) : c ][ F ( 2 ) : c ][ [
E ( 5 ) : n ] , which is applied to the expansion  X  X lasmino-gen activator inhibitor type-1 X  from Fig. 2 and produces the acronym PAI-1 with skeleton cccn .
 training corpus, we add a new operator:
C : (Contiguous match) This operator may only be used im-consecutive letters of an expansion morpheme is directly copied into the acronym.
 Example 8 The acronym Chr-22, which is split into units C, h, r, 22, has the skeleton cccn .Itisusedfor X  X hromosome 22, X  which is of the form w n (morphemes  X  X hromosome X  and  X 22 X ). Our rule describing the relationship has the form w n  X  X  F ( 1 ) : c ][ C ( 1 ) : c ][ C ( 1 ) : c ][ E ( 2 ) : After F selects c , the two applications of the C-operator re-spectively select the letters h and r.
 within the hybrid approach for describing the derivation of acronyms from expansions.
 Remark 3 In some of our experiments we also looked at an-other operator. The operator Ins (Insertion) inserts a unit of type n or c into the acronym that does not have a cor-respondence in the expansion. It enables a correspondence between an acronym and an underspecified expansion. For example, the acronym ODN, which has the skeleton ccc ,is used in one particular Medline abstract as short for  X  X ligonu-cleotides, X  which is of the form w . Our rule describing the graphical relationship for this case is as follows: w  X  X  F ( 1 ) : c ][ Ins : c ][ I ( 1 ) : c ] .
 Many other Medline abstracts provide evidence of ODN usually standing for  X  X ligodeoxynucleotides, X  and it is natural to assume that the letter D comes from the missing  X  X eoxy X  rather than corresponding to the letter  X  X  X  at the end of the expansion. The use of the operator Ins is controversial. On the one hand, we found a nonnegligible number of examples in the Medline corpus where Ins is in fact needed to obtain the correct graphical correspondence between acronym and expansion. On the other hand, Ins is too powerful since it can  X  X xplain X  any symbol in any acronym. Hence an uncontrolled use leads to many ambi-guities and decreased precision. In practice, the use of Ins has to be controlled in some suitable way. For simplicity, we describe all our experiments in a variant where the use of Ins is completely excluded. For the rule-based approach, this generally leads to better results. For the HMM-based method, an interesting variant tolerates one single application of Ins , adding linguistic conditions on expansion candidates in the preference scheme as a kind of additional control. Details are described below. 4.4 Selection of preferred expansion candidate Given an acronym candidate A , the skeleton of A determines a unique subset M of rules where the right-hand side has the appropriate form. For each rule in M , for each consecutive sequence of morphemes in the search window we check if the morphemes have the appropriate type as specified in the left-hand side of the rule. Some conditions must be consid-ered when selecting expansion candidates in this way. All morphemes of an expansion candidate E have to belong to the same sentence of the text. The first and the last mor-pheme of E must not be a preposition, a form of the verb be , a conjunction, or a determinant. This can be regarded as a simplified recognition of noun phrase borders. 6 to the acronym candidate. The candidate that is nearest to the acronym and was obtained with the most frequent rule in the training corpus is chosen as the expansion for A in the out-put. The preference scheme used in [ 19 ] is slightly distinct. We also tested it and other preference schemes, without im-proving results. 5 Evaluating convergence and stability In the original paper [ 19 ], the hybrid approach is used in an interactive environment where the user may add new rules suggested by the system during a session. In such a context, new rules are added by demand, and recall can be controlled by the user. We are interested in methods that, after initial training, work in a fully automated way, which seems more useful when analyzing large corpora with many acronyms. A formalism is needed where training is convergent and leads to a stable procedure in the sense explained in the introduc-tion.
 approach in the area of biomedical texts, we conducted a se-ries of experiments on the Medline corpus. As mentioned above, we focused on experiments that disregarded the op-erator Ins . The precision and recall measurements were per-formed on two corpora with sentences from the Medline database.
 texts from 1999. A subcorpus with 500 acronym candi-dates was singled out using the regular expression in parenthesized form. After inspecting the acronym candi-dates, we found that 464 candidates represented proper acronyms. Among those, 449 were defined in the same sen-tence before the acronym. Hence we had a total of 449 acronym-expansion pairs. We manually derived the rules that correctly describe the graphical correspondence be-tween acronyms and expansions and annotated all acronym-expansion pairs as well as pseudoacronyms for evaluation purposes. Table 3 shows the frequency distribution for the 107 rules that were obtained. For the training corpus, we achieved a -restricted recall of 96% and a precision value of 99%.
 tences using Medline texts from 2000. We again added sen-tences until we obtained 500 acronym candidates. In this case the number of proper acronyms (acronym-expansion pairs) was 465 (436). In order to support an automated evaluation of precision and recall, we again annotated all acronym-expansion pairs as well as the pseudoacronyms. Using the rule set R derived from the training corpus, we then automatically retrieved acronym-expansion pairs from the test corpus. A -restricted recall (precision) of 78% (96%) was obtained.
 ing corpus to the test corpus is considerable and raises two obvious questions: 1. Can we expect a better recall on the test corpus when 2. How is precision affected when we add more rules dur-the test corpus. We found 118 rules, only 35 of which were from the rule set R ! Besides, these rules in the intersection were accompanied by a large set of rules with just one ap-plication. The high number of rules with very low frequency gave a first hint that the derivation of a sufficiently large rule set during training might be difficult.
 reached in the test corpus depend on the number of rules that are derived in the training corpus, we imitated an  X  X de-alized X  training process where the most frequent rules are found first. Starting with the subset R 1 of R that contains the 10 most frequent rules found on the training corpus, an experiment with 12 rounds was designed. The rule set R i + for round i + 1 contained R i and in addition the 10 most fre-quent rules from R \ R i . In the final round 12, the last 9 rules were added. In each round, we measured the -restricted re-call and precision values that were achieved for the (training and) test corpus using the rule set R i . The results are shown in Fig. 4 (solid lines).
 12 rounds had the expected effect: we gradually improved recall, the precision was not affected. On the test corpus, however, recall  X  after a significant initial improvement  X  did not exceed a limit of 78%. The numbers in rounds 6 X  12 suggest that no significant gain in recall can be expected when adding new rules from a larger training corpus. Fur-thermore, the addition of new rules is likely to reduce preci-sion values initially raised. Obviously, the use of the system as a tool for automated extraction becomes suboptimal with such a behavior.
 be distinguished: 7 1. False positives. A pseudoacronym is erroneously treated 2. Wrong rule. For a proper acronym, the wrong rule was 3. Missing rule 1. For a proper acronym, the correct rule 4. Missing rule 2. For a proper acronym, the correct rule from a better recognition of introduction contexts. Since candidates not occurring in an introduction context were marked in the corpus, we could easily measure the  X  X deal-ized X  precision values where only candidates occurring in proper introduction contexts are taken into account and er-rors of type 1 (false positives) are thus excluded (cf. Sect. 3). The corresponding curves are represented in Fig. 4 with dashed lines. The results show that even with a perfect recognition of introduction contexts, precision values on the test corpus are only lifted by about 1% but still decrease with the number of rules.
 helps to explain the reasons for the difference in precision between training and test. The total height of each bar gives the number of false expansions extracted for proper acronyms in a round and for a corpus. When rules are added during the rounds, more proper acronyms are retrieved and found in the answer set, which explains why the number of errors of both kinds may grow. The lower part (in darker gray) corresponds to type 2 errors (proper rule available, wrong rule selected), the upper (in lighter gray) to type 3 errors (proper rule not available, wrong expansion selected). In the training corpus , where adequate rules are derived, er-rors of type 3 are eventually eliminated in round 12, where all needed rules are available. Some errors of type 2 persist where the preference schema selects the wrong rule and ex-pansion. In the test corpus , in contrast, the majority of errors are of type 3. Note that most of the 107 rules that are at our disposal are in fact inappropriate since they do not yield a correct explanation for any example of the test corpus. The presence of these useless rules increases the probability of selecting the wrong rule, and during the 12 rounds we ob-tain an almost constant number of false expansions. In this sense the system is overtrained for the test corpus. pus by adding new rules can only be done at the risk of a loss of precision when moving to another corpus. Most rules only cover a few very specific cases. When adding these rules, the danger of errors of type 2 increases.
 Remark 4 A parallel series of experiments were conducted allowing the use of Ins (cf. Remark 4) in the rules. Twelve additional rules could then be written for the training corpus, covering 3% of the introduction contexts. The results look very similar to those in Fig. 4 . Recall is improved by 3% in the training corpus and by one point in the test corpus. The major difference is the negative evolution of precision in the test corpus, which decreases after round three (98%) and reaches a final value of 94% after round 12. 6 Replacing rule sets by Hidden Markov Models The numbers seen in the previous section show that the high specificity of rules represents the main obstacle to ob-tain a fully satisfactory recall in the application phase. We now introduce the hidden Markov models (HMMs) used in our approach and show how parameters are directly es-timated from an existing rule set. 8 Afterwards we show how the HMM is actually used given an acronym candi-date and a search window. In a third step we show how the probabilistic information obtained from the HMM and additional parameters from the expansion candidates are combined in the preference scheme to select a preferred expansion candidate. In our experiments the remaining setup (tokenization routine, morpheme determination, def-inition of acronym candidates, search window, etc.) was as before. 6.1 Converting rule sets into HMMs The basic idea behind the use of HMMs can be described as follows. Given an acronym candidate A , each letter of the skeleton is treated as an emission of a matching oper-ator. Matching operators are treated as states. Our HMMs are  X  X idden X  in the sense that the sequence of all emissions (the skeleton) is visible, but we cannot see the underlying sequence of states (matching operators). From the HMM, we obtain different suggestions for plausible operator se-quences, which in a second step are compared with the mor-phemes found in the search window. 9 Details are given be-low; see also Fig. 7 .
 used in the hybrid approach and two additional states, b and 0: is the first state of any transition through the HMM; it is not accessible from the other states. State b always emits the null symbol . State 0 is needed because the application of oper-ator E can result in more than one symbol in the acronym skeleton, but in our HMM there is a one-to-one correspon-dence between operators (states) and emitted symbols. We modified E , allowing it to produce just the first symbol. All the following symbols formerly produced by E are now pro-duced by state 0, which is accessible only from E or from 0. K := { , c , n } .
 is defined as  X  b := 1and  X  i = 0 for all states i  X  S distinct from b .
 and the emission probabilities b ik ( i  X  S , k  X  K directly from a set of rules obtained from the hybrid approach using a maximum-likelihood estimator (MLE). As a preparation, the rules were automatically converted into a different format in a first step, in which the states b and 0 are used in the aforementioned way.
 Example 9 Rule w ph ww  X  X  F ( 2 ) : c ][ F ( 4 ) : c ][ E is converted into the sequence b , cF , cF , cE , c 0 , c 0. state s j . With the MLE the emission probability for such an event is estimated from the relative frequency of events where s j is emitted when the HMM is in state s j : P Here C ( k i s j ) represents the absolute frequency of the event k s j ,and indicates that the symbol is not specified for the count. tive frequency of a transition with respect to all transitions starting from the same state: P No kind of smoothing was performed that tried to reserve probability space for unseen events.
 the final rule set for round 12 in the experiment from the previous section for the hybrid approach. Accidentally, the operator R was not used in the rules derived for the train-ing corpus, so all probabilities for transitions to this state are null. Nevertheless, from other experiments we know that this operator is useful. In general, in experiments with larger rule sets the probability distribution also became more complex.
 Remark 5 Our HMM only formalizes the sequence of oper-ations that are used to built an acronym and the letter type c or n of the acronym skeleton that results from the appli-cation of the operator. The HMM does not specify to which morpheme in the search window a given operator is applied. This point, which is illustrated in Fig. 7, will become clearer below, when we explain how the HMM is used for search. Remark 6 An alternative, more fine-grained working model results from creating a state in the HMM for each combi-nation of an operator with an expansion morpheme type. The training costs, however, would be much higher since the number of transitions is given by the square of the number of states.
 Remark 7 The fact that we do not consider the rule use fre-quency in our estimations for the different parameters can be seen as a way of factoring out noise introduced by the ex-pansion skeleton that is used in the rule. Rules can only have a high frequency if they use a frequent expansion skeleton. However, when applying the HMM we do not want to make any assumption on the expansion. 6.2 Applying the HMM The HMM acts as a network with an infinite set of possi-ble transition paths. For each observed acronym candidate A , there are different paths through states of the HMM that can produce the skeleton of A . When looking for an ex-pansion for A , the HMM produces alternative paths (oper-ator sequences) based on the form of the skeleton, which we can check against the search window, selecting suitable morphemes for applying the operators. The correspondence between acronym and expansion is thus not driven by a finite set of rules but emerges dynamically.
 are deterministically grouped into subsequences, each sub-sequence corresponding to a set of operators that are all ap-plied to the same expansion morpheme. We use the fact that operator F marks the beginning of a new morpheme and that each of the operators E and R 11  X  X tands for X  a whole morpheme. In an operator sequence we can draw a border in front of these operators. Sequences between borders are applied to the same morpheme.
 Example 10 EFCCRFI is divided into the subsequences E | FCC | R | FI .
 quences that are not compatible with some structural prop-erties of the acronym not visible in the skeleton. Dashes ap-pearing in the acronym usually indicate morpheme divisions in the expansion. We can thus ignore operator sequences where the number of borders is smaller than the number of dashes found in the acronym.
 Example 11 The operator sequence FII | FI cannot gen-erate the acronym P-CH-R.
 lowercase to uppercase turned out to be unreliable as divi-sion markers and were not used in our experiments. operator subsequences are matched against the morphemes in the window. There is a match for the whole operator se-quence if each subsequence matches at least one morpheme in the window such that all matched morphemes are dif-ferent. Each possible combination of expansion morphemes that matches the operator sequence is treated as an expan-sion candidate. Each candidate encloses the words between the two outermost matched morphemes.
 erence scheme. Consider an expansion candidate associated with an operator sequence X that generates an observed acronym skeleton O . Given the model n , the probability of producing O on path X is P ( O , X |  X ) = P ( O | X , X )  X  P ( X |  X ) (1) In order to be able to directly compare the probabilities of all operator sequences X that generate the same skeleton O ,we associated to each sequence X the normalized probability N : N ( O , X |  X ) = P 6.3 Preference scheme and graphical correspondence As the preference schema used for the hybrid approach worked well, we developed a similar one for the HMM ver-sion. Again, expansion candidates are first ordered by as-cending distance to the acronym candidate. From among all candidates with minimal distance we select the one where a second parameter, N M , F , is maximal. The parameter N modifies the normalized probability N ( O , X |  X ) of the op-erator sequence suggested by the HMM, introducing two weakening factors, M and P . M takes into account how many expansion morphemes appear as a letter or number in the acronym. It is given by M := where m is the number of candidate morphemes not repre-sented in the acronym ( X  X issing X  morphemes) and | E | the total number of morphemes of the candidate. Note that M decreases with the number of missing morphemes.
 the expansion candidate differs from the order of the corre-sponding letters/numbers in the acronym. We used the for-mula P := where lcs denotes the longest subsequence of nonpermuted positions in the graphical correspondence between expan-sion morphemes and acronym symbols.
 by a rule with right-hand side [ F ( 2 ) : c ][ F ( 3 ) : [
F ( 1 ) : c ] ,wehave lcs = 3 since in the subsequence [
F ( 2 ) : c ][ F ( 3 ) : c ][ F ( 4 ) : c ] the order of expansion mor-phemes is completely copied to the corresponding order of acronym letters.
 minological expressions [ 4, 11], we tried to refine the pref-erence scheme with linguistic filters that help to recognize good expansion candidates. In the above variant of the HMM approach, these filters did not improve results. In the variant where HMMs use the operator Ins , the use of linguistic fil-ters in the preference scheme helped to improve precision (see below). 7 Evaluation results for the HMM-based method To evaluate the HMM-based method, we repeated the experiments designed for the hybrid approach using the same training and test corpora. The effect of training the model on different rule sets was observed, translating the rule sets obtained after the 12 rounds into 12 corresponding HMMs.
 the recall of 82% reached with the translated HMM was bet-ter than the highest recall reached with the hybrid method after round 12. In the final round, the recall reached with the HMM-based method was 94%. In terms of precision, the HMM-based method achieves 95% and is comparable to the hybrid approach (96%). Summing up, the main advantage of the HMM-based method is the improved recall coupled with stability with respect to precision.
 candidates. In the parallel experiment where we used HMMs with state Ins , we only considered paths with at most one occurrence of Ins . We obtained a better recall, and the improvement was 1%. At the same time, precision de-creased by 4 X 5%. With a sophisticated linguistic preference scheme, this loss of precision could be reduced.
 computes for a given acronym skeleton O the normalized probability N ( O , X |  X ) for all paths X . In a second step, for each such operator sequence X we try to find admissi-ble matches on the basis of the morphemes found in the search window. This matching step consumes most of the time. Hence a speedup can be obtained by excluding from the matching all paths X where the normalized probability is below a given threshold. We found that we could safely ig-nore all paths X with a normalized probability below 1 / 10 where l is the number of acronym letters, without major changes for precision and recall. 8 Related work An early and influential contribution to acronym-expansion detection is Taghva X  X  Acronym Finding Program (AFP) [ 25 , 26 ]. Acronym candidates are defined as  X  X ppercased X  words of length l ,3  X  l  X  10, excluding sequences from a given list of false positives (e.g., FIGURE, TABLE). Any occurrence of a candidate A triggers a search for the asso-ciated expansion within a text window surrounding A with two parts, the pre-and the postwindow. Each subwindow contains 2 | A | consecutive words ( | A | denotes the num-ber of symbols of A ). For each subwindow W a confidence Here I ( W ) denotes the sequence of all initial letters of W , lcs ( A , A ) denotes the length of a maximal common subse-quence of two sequences A and A ,and error rate is a con-figurable parameter with default value 0.2. If cf ( W , A the search for an expansion of A in W stops. Otherwise, all longest common subsequences of A and I ( W ) are com-puted. Each longest subsequence defines a unique subse-quence of (not necessarily consecutive) words of W . Filling the holes between these words (i.e., adding skipped words) we obtain an expansion candidate. The preference scheme for selecting a best expansion candidate basically tries to maximize the number of content words in an expansion that contribute to the acronym.
 computational efficiency. Drawbacks are the simplicity of the search pattern and the fact that only matches correspond-ing to the operator described above are formalized. Other possibilities are only taken into account by accepting subop-timal matches between acronym and expansion, which leads to a loss of precision.
 extracted from documents on the Web. For the construc-tion of the dictionary, four acronym-finding algorithms have been designed and evaluated. While the base algorithm uses conventional search in arbitrary text windows surrounding acronym candidates, three refined versions take the structure of possible introduction contexts into account. In contrast to AFP, detailed descriptions of possible acronym candidates in terms of regular patterns and additional restrictions are given. Each of the four Acrophile algorithms comes with its own definition of expansion candidates. Generalizing the acronym-building principle of AFP, several letters of one word may contribute to a given acronym in the Acrophile ap-proach. Matching basically means checking different com-binations of F and I operator sequences on the search win-dow. In this sense, the hybrid approach [ 19 ] refines both AFP and Acrophile with its richer set of matching operations. Acrophile X  X  four algorithms use distinct preference schemes for selecting a best candidate. For the refined variants, oc-currences in admissible introduction contexts are preferred. ysis of the text is reported in [ 21 ]. In the more improved ver-sion, introduction contexts are described as a nominal phrase standing for the expansion and an acronym candidate deter-mined by a regular expression, which can appear in different configurations, e.g., NP (acronym-candidate) or acronym-candidate (NP). For processing, the text is split into sen-tences. Each sentence is analyzed by a shallow parser, which marks chunks of words in the sentence as phrases of vari-ous types. A finite-state automaton scans the outputted sen-tence for the patterns defined for the introductory contexts. If one is found, a simple matching procedure is used to val-idate the NP and the acronym candidate. They are consid-ered an expansion-acronym pair if the ratio of the matchable expansion words (not including stop words) to the number of acronym characters remains below a specified threshold. The method is very accurate (99% for some introduction context patterns) with an acceptable recall (60% X 70%), but the resource overhead (parsing procedure, corpus-dependent lexica) is considerable.
 Yeates [ 29 ] uses machine learning techniques to develop a naive Bayes classifier that distinguishes acronyms from nonacronyms. In [ 28 ] acronyms are detected by comparing the performance of a special compression model developed for acronyms with the performance achieved with other stan-dard compression models for natural language. 9Conclusion In this paper we developed an extension of a rule-based approach for extraction of acronym-expansion pairs, trans-lating a given set of extraction rules into a hidden Markov model and using a preference schema based on properties of the graphical correspondence between acronym and expansion.
 the test corpus with the original approach [ 19 ] could be lifted to 94% with the HMM-based extension of the formalism, while the precision remained stable.
 HMM can be used for new corpora. The flexibility of the matching process and the richness of the used set of opera-tors guarantee that special graphical relationships can be de-tected that have not been observed in the training corpus. As a matter of fact, it is possible to (re)translate a successful run of the HMM for some acronym-expansion pair into a rule. In this way, the given HMM can also be used for suggesting rules. The translation of enlarged rule sets into HMMs can then be considered as a form of additional training. techniques for recognizing introduction contexts when the graphical correspondence is only approximate and the oper-ator Ins (cf. Remark 4.7) is used.
 process whereby an operator sequence produced by the HMM is matched with the morphemes in the search window is complex. Since the HMM produces several sequences, search times are longer than for the original approach. Im-provements on the efficiency side represent one point for fu-ture research.
 restricted recall (as defined in Sect. 3 and measured above) on the one hand and absolute recall on the other. The search pattern  X ( ) X  for detecting acronym candidates described in Sect. 4 simplified the above experiments where we compared the original method with our extension. When the new approach is used in practice, absolute recall can be further improved using a more general set of patterns. In an experiment with Pustejovsky X  X  Acronym/Alias Identification Corpus [ 14 ] we found that 123 of 149 acronyms occurring in actual introduction contexts could be identified using the pattern  X ( ). X  The method gives a detection rate of 82.5%. The 149 introduction contexts had the following form and distribution: 139 Expansion ( Acronym ) 4 Acronym (for Expansion ) 2 Expansion [ Acronym ] 2 Acronym ( Expansion ) 1 Expansion-prefix ( Acronym-for-prefix ) Exp.-rest 1 Expansion ( Acronym ) Expansion-rest Hence, for 16 missing acronyms a more general regu-lar expression was needed. Most of the remaining 10 cases can be covered with a straightforward modification of the search pattern  X ( ). X  References
