 The advent of DNA microarray technology, such as the cDNA arrays and the high density oligonucleotide chips, has revolutionized the field of molecular biology in recent years. This new technology allows scientists to study thousands of genes simultaneously in a single experiment. This is a significant improvement because in the past, only several specific genes in an organism could be investigated at a time.

While the revolution generates much hope, the large amount of data obtained from microarray experiments, along with the structures of the resulting data sets, also challenges the conventional ways of analysis and modeling. One particular obstacle for analyzing a microarray data set is that often the number of genes is much greater than the number of samples; typically, the number of samples is less than a hundred, while the number of genes is usually in the thousands. In this regard, modern machine learning techniques provide a valuable toolkit for gaining insights into such data sets and extracting useful information from them.
Out of a large number of genes that exis t in a microarray data set, it is often the case that most of them are irrelevant for the diagnosis of a particular disease, say, cancer, and hence are redundant. It is well-known that the performance of a modeling procedure can be significantly degraded, when many redundant genes are included in the training. Finding relevant genes can not only improve the ac-curacy of the resultant classifier for diagnosis purposes, but can also narrow down the potential set of cancerous genes and help gain important discipline knowledge.
Several methods for gene selection are av ailable in the literature. One state-of-the-art technique is the method of Support Vector Machine Recursive Feature Elimination (SVM-RFE), proposed in [1]. The ranking criterion of SVM-RFE is constructed not by the intrinsic property of the data, but by the feedback from the support vector machine (SVM) classifiers. Specifically, the magnitudes of weights found by linear SVMs are used to rank the genes. At each iteration of the algorithm, a linear SVM is fitted to the training data with the remaining genes, and one or several genes are eliminated for their least significance in terms of the ranking criterion.

In this paper, we propose an alternative SVM-based method for gene selection, and call it TSVM-RFE. In particular, we c onsider selecting genes by combining the classical t -statistic and the modern SVM-RFE method. By taking care of the problems that may exist in either the univariate or multivariate worlds, TSVM-RFE is more robust to noisy genes than SVM-RFE and other methods, as confirmed by simulation studies. In this section, we describe our gene selection method, TSVM-RFE, and illus-trate its strengths, as well as giving a brief introduction to the support vector machines and t -statistic as needed by our method. 2.1 Data The results from the microarray exper iments can be represented by a matrix of expression levels. For microarray experiments having n tissue samples and p genes, the results can be represented by a p  X  n matrix X .Inthispaper,we shall focus on the classification problems with two classes, labeled by 1 and 2, respectively, and let n k denote the sample size for class k ; i.e., n 1 + n 2 = n .The response variable y j , j =1 ,...,n , takes on the values of +1 or  X  1forthetwo classes, respectively. For gene i ,weuse x ki to denote the vector of values on the i th row of X that belong the class k  X  X  1 , 2 } . The mean of the values in x ki is denoted by  X  x ki , and the sample standard deviation by s ki . 2.2 Support Vector Machines The objective of SVMs is to find a classifie r with the largest margin between the observations belonging to two different classes, while minimizing the training error. Here, the principle is that the classifier with the maximal margin is more likely to have a better generalization ability. A remarkable feature of SVMs is that the classifier is determined by only a few training samples, known as  X  X upport vectors X . These vectors are borderline samples, in the sense that they are closest to the decision boundary or simply lie on the margin.

In the studies below, we shall simply use the linear support vector machines, following [1], [3]. From these and other studies, linear SVMs appear to work reasonably well for the purposes of gen e selection; for nonlinear support vector machines, we refer the reader to [4]. In order to select genes, the method of support vector machine recursive feature elimination for gene selection uses the absolute weight value | w i | given in the vector of parameters w to rank genes. 2.3 The t -statistic The t -statistic measures the separability between classes using a standardized distance for a single gene, which gives a relevance score for each gene. The ranking criterion is given as where  X  x ki , s ki and n k aredefinedinSection2.1. 2.4 TSVM-RFE The basic idea of TSVM-RFE is to combine the t -statistic and SVM-RFE. The recursive feature elimination (RFE) algorithm [1] is used as the search engine of TSVM-RFE. In order to combine two di fferent gene selection criteria, each criterion is transformed into a comparable scale. In particular, denoting by v the statistic used by a ranking criterion, the linear transformation is employed. Since  X  (min( v )) = 0 and  X  (max( v )) = 1, the range of  X  ( v )is[0 , 1] for the training data. It is also possible to use other transformations, such as the sigmoid function or the probability function of a distribution, say, Gaussian.
From (2), the ranking statistic used for TSVM-RFE is where w i the weight found by linear SVMs and t i the t -statistic. When  X  =1, TSVM-RFE is equivalent to SVM-RFE; when  X  =0,TSVM-RFEisequivalent to using the t -statistic.

To use (3), one problem remains to be solved, i.e., a value for  X  needs to be provided. For this, we use the 10-fold cross-validation. Specifically, a grid of  X  i values are tested on the training data by 10-fold cross-validation, and the one that gives the lowest cross-validation error is deemed as  X  X ptimal X . In our observation, using 11 equally spaced points for  X  between0and1(  X  = 0 , 0 . 1 ,..., 1) seems enough. It is also possible to choose a finer grid, at a higher computational cost. When more than one  X  valueproducesthesamecross-validation error, we use the smallest of them if  X  = 0 gives a smaller cross-validation error than  X  = 1; otherwise, we use the largest. In other words, the weight in this case is determined in such a way that TSVM-RFE is as close as possible to the better of the two individual methods. 2.5 An Illustration The motivation for TSVM-RFE is to over come the weaknesses of each individual criterion. The t -statistic is a great criterion in measuring the class separability for each individual gene. However, it can only summarize at most the patterns that exist in the univariate world. Those multivariate patterns that are common in mi-croarray data, such as correlation among genes, may never be represented by it. By contrast, SVM-RFE is expected to captur e multivariate patterns well due to its foundation in the maximal margin principle, and could outperform the t -statistic for a number of data sets. However, sin ce support vector machines are prone to overfitting when there exist a l arge number of noisy genes, the t -statistic can have an advantage in such cases. It is typical in practice that both noisy genes and mul-tivariate patterns exist in microarray data. Hence, a criterion that combines the information provided by both the t -statistic and SVM-RFE is likely to preform reasonably well: at least as well as the better of the two individuals.
The above consideration is illustrated in the following using two simple exam-ples. As shown in the left panel of Figure 1, the two classes of a two-dimensional data set are completely linearly separab le. Here, according to SVM-RFE (using x 1 is more relevant than x 2 , because basedonthe t -statistic appears more reasonable than SVM-RFE. This is be-cause statistically speaking, the variation of x 2 for separating the two classes is large, while x 1 has none. This is a situation where the maximal margin principle fails to work well.

The second example is shown in the right panel of Figure 1. In this example, the data are two-dimensional and x 1 and x 2 are positively correlated. The two classes are also linearly separable. Here, the magnitude of the t -statistic for the two features are very different: | t 1 | =0and | t 2 | =3 . 098. Due to the t -statistic, x 2 is relevant and x 1 irrelevant. From SVM-RFE ( C =1),however, x 1 and x 2 are equally relevant, because | w 1 | = | w 2 | =0 . 500. Since in this example x 1 and x 2 appear to be equally important for identifying the pattern, the t -statistic fails to select all relevant features while SVM-RFE performs well. This is a situation where a multivariate pattern exists and the support vector machines work well, but not the t -statistic.

Admittedly, the above two examples are rather crude, but they demonstrate the difficulties that, if used individually, the support vector machines and t -statistic may have for gene selection. It is not uncommon for microarray data that genes are correlated and a large number of them are irrelevant. Given this, our combined criterion is expected to perfor m better than each of the two individual methods; in the worst scenario, it is simply equivalent to the better one. Experiments were conducted to compare different gene selection methods and the results are given in this section. Two real data sets that are publicly available were used. A few points need to be clarified here. First, as part of pre-processing, we follow standardize each sample to mean zero and standard deviation one so as to treat each sample with an equal weight and thus to reduce array effects. Second, we follow [1] to select a fixed number of genes in the model apriori .Inour experiments for the real data, the number of the genes retained are 10 , 20 ,..., 70 for each method. Third, we use external cross-validation errors for comparison to avoid selection bias. Internal cross-va lidation errors are subject to selection bias, which are typically too optimistic [5]. Fourth, the SVM-RFE algorithm due to [3] was adopted. Fifth, a support vector machine classifier is constructed for each method after the genes are selected to assess its classification accuracy.
In the experiments, three ge ne selection methods, the t -statistic, SVM-RFE, and TSVM-RFE, are used to select genes. Classifiers based on linear SVM are then built using all training data, and subseq uently examined using the test data. 3.1 Leukemia Data The acute leukemia data consists of 72 samples and 7129 genes. They were ob-tained from Affymetrix oligonucleotide arrays. There are two types of leukemia: ALL (acute lymphocytic leukemia) and AML (acute mylogenous leukemia). We follow the procedure used in [5] to split the leukemia data set into a training set of size 38 and a test set of size 34 by sampling without replacement from all the samples, while ensuring that the training set has 25 ALL and 13 AML and the test set has 22 ALL and 12 AML. Different gene selection methods combined with linear SVMs are only applied to the training set, and then the methods are used on the test set to estimate their accuracies. Twenty such random parti-tions were carried out. Note that many proposed methods in the literature use a test set with size 34 only, whereas the test ing procedure used here is equivalent to use a test set with 680 samples, so it is much more reliable than using the independent test set of size 34 only. In our observation, C = 1 is a reasonable value for the penalty parameter of SVMs in this data set.

The results for the leukemia data are summarized in Figure 2. TSVM-RFE gives the smallest minimal error of 3 . 68%, and strictly smaller errors than SVM-RFE and the t -statistic-based method for 30 , 40 ,..., 70 genes. The minimal test error obtained by TSVM-RFE is also smaller than the test errors obtained by using several other methods for this data set in the literature: the minimal test error 5 . 00% from SVM-RFE, obtained based on fifty similar random partitions [5]; the test error 7 . 00% from the nearest shrunken centroid method [7]; and the minimal test error 6 . 00% using soft-thresholding combined with kNN classifier obtained in [7]. Interestingly, all three methods give the lowest error when 60 genes are used. This provides a reasonable suggestion for the number of relevant genes that should be used for the leukemia data. 3.2 Colon Data The colon cancer data consists of 62 samples and 2000 genes. They were also obtained from Affymetrix oligonucleotide arrays. The task is to distinguish be-tween the normal and tumor samples. There are 22 normal samples and 40 tumor samples in the given data. We follow the procedure used in [6] to randomly split the colon data set into a training and test set by sampling without replacement from all the samples, while ensuring that the training set has 15 normal and 27 tumor samples and the test set has 7 normal samples and 13 tumor samples. Different methods are only applied to the training set, and the test set is used to estimate the classification a ccuracy. Twenty such rando m partitions were carried out. Note that it was suggested that there were some wrongly labeled data in the training data set [5]. We follow [3] and use C =0 . 01 for this data set. The results for the colon data are su mmarized in Figure 3. TSVM-RFE and SVM-RFE give the same minimal test error of 8 . 75%, and their performance is similar in this data set. In this data set, TSVM-RFE is always better than the methodbasedonthe t -statistic alone. The minimal test error obtained by TSVM-RFE here is also smaller than the test errors obtained by several other methods: the leave-one-out cross-validation error 9 . 68% obtained in [2], using correlation metric combined with SVMs; the minimal test error 17 . 50% from SVM-RFE obtained based on fifty similar random partitions [5]; the minimal jackknife error 12 . 50% obtained in [6] using weighted penalized partial least squares method; the minimal test error 11 . 16% from SVM-RFE with various values of C [3]; the test error 18 . 00% from the nearest shrunken centroid method [7]; the minimal test error 13 . 00% obtained in [7], using Wilcoxon statistic combined with kNN classifier; and the leave-one-out cross-validation error 8 . 90% obtained in [8], using the top scoring pair method. We have proposed a new gene selection method, TSVM-RFE, for gene selec-tion and classification. The criterion of TSVM-RFE combines the t -statistic and SVM-RFE, due to the consideration that the t -statistic only summarizes well the information in the univariate world and that SVM-RFE distinguishes the mul-tivariate patterns better but is sensitive to noisy genes. We have chosen a linear transformation so that individual criteria are combined on a comparable scale, and the weight for each individual criterion is determined via cross-validation.
The proposed method was compared ba sed on experiments with SVM-RFE and the t -statistic, using two practical data sets. The method presented in this paper gives competitive, if not better, results, compared to the other two. The improvement of the method proposed here upon the better-known SVM-RFE method is significant. The method pres ented here seems to be rather suitable for microarray data analysis, where it is likely that a large number of irrelevant genes exist and the signal-to-noise ratio is fairly low. Experiments have shown that TSVM-RFE is better than both SVM-RFE and the t -statistic, in terms of reducing misclassification errors and low ering false discovery rates. It helps to identify more accurately th e truly cancerous genes.

