 Our aim is to investigate if and how the performance of Dis-tributed Information Retrieval (DIR) systems can be im-proved through personalization. Toward this aim we are building a testbed of document collections and correspond-ing personalized relevance judgments. In this paper we dis-cuss our intended approach for personalizing the three differ-ent phases of the DIR process. We also describe the test col-lection we are building and discuss our methodology for eval-uating personalized DIR using relevance information taken from social bookmarking data.
 H.3.0 [ Information Storage and Retrieval ]: General Design, Experimentation personalization, distributed information retrieval, evaluation
Distributed Information Retrieval (DIR) is an important subfield of Information Retrieval that is gaining much atten-tion as the number of Deep Web databases on the Internet continues to grow. Research over the last few years has shown that context aware searching and in general the per-sonalization of search results, can improve the overall search user experience by providing more relevant results to indi-vidual users [2]. Our plan is to investigate the applicability of personalization techniques to DIR.

There are three distinct phases in the DIR process, namely resource description , resource selection and results fusion . In the first phase a description is built for each of the dis-tributed collections. Collections are usually assumed to be non-cooperative in the sense that one cannot access their internal indexes directly, nor crawl all of their documents, but may only access their documents via a search interface. In this case query-based sampling (QBS) techniques [1] are used to generate a small set of sample documents (usually of the order of hundreds) that will be used to represent the resource. Various statistical representations can be built from the sample depending on the resource selection algo-rithm chosen, often involving either the document frequency (the number of documents containing each term in the sam-ple) or the average term frequency (the number of times a term appears on average in each of the sampled documents). The estimated size of the collection is usually also calculated based on statistics of the sample.

In the second phase the system selects for a particular query, a subset of the collections that is most likely to con-tain relevant documents. Typically, the number of collec-tions chosen is predetermined and considerably less than the total set, so that the system can benefit from the cost savings of not needing to access all resources. Commonly employed algorithms for performing resource selection include CORI and ReDDE [4].

Finally, the system must aggregate (or fuse ) the results from the individual collections before presenting a unified list to the user. Algorithms for result fusion differ on how much information they expect to receive from each collection (some require scores for each document in the ranked list) and whether they need to download the content of every document (resulting in considerable overhead and latency). Common algorithms for results fusion include CombMNZ and ProbFuse [3].
We now discuss how we plan to personalize each of the three phases of DIR. Our aim is to first determine which of the phases are best suited for personalization. In the follow-ing, we assume that we have a profile of the user, denoted  X  , which contains a list of previous queries along with the collections accessed and documents downloaded, as well as a distribution of terms describing the user X  X  interests (mined from the content of the documents and weighted by recency).
In order to personalize the resource descriptions generated by QBS, we bias the sampling toward the interests of the user. The idea is to use the user profile  X  u (either the query log or the content term distribution) as a source of terms for generating the single term probe queries required for QBS. The intuition here, is that we force the QBS algorithm to spend more time sampling documents that are closer to the interests of the user, and thereby build resource representa-tions that have higher fidelity in these areas. The resource selection algorithm should then have a better chance of dis-cerning which resources are useful for a particular query pro-vided the query is somewhat related to the user X  X  interests. It is possible, however, that biasing sampling toward user interests may have a detrimental affect on resource selection performance in some cases by causing the resource selection algorithm to overestimate the set of relevant documents in a collection or underestimate the total size of a collection.
Our approach to personalizing the results of resource se-lection is to use the data in the user profile  X  u a mapping from the terms in the user X  X  queries to the re-sources accessed. In this way we will be able to characterize the user X  X  preference for certain collections over others (e.g. news from CNN over the ABC), which may not be fully captured by the term distributions in the resource descrip-tions. The mapping will be used to generate a personalized score for each resource that will be combined with the score provided by standard (not-personalized) resource selection algorithms. In this way we can personalize the resource se-lection process without needing to change the internals of the selection algorithm and can test the same personaliza-tion strategy on different algorithms concurrently.
We plan to personalize the results of the fusion step by taking into account the user X  X  historical use of each resource, so as to give precedence to documents from trusted resources. We also plan to investigate the re-ranking of documents based on the similarity of their content to the term distri-bution in the user profile  X  u .
We briefly discuss our strategy for large scale evaluation of personalized DIR, where by  X  X arge-scale X  we intend that the number of users will far outnumber that which is possible for a typical user case-study.

In order to evaluate personalized DIR we require a testbed that has two properties: Firstly, the set of documents must be distributed across (or at least separable into) multiple resources. Secondly we need a stream of user queries and corresponding personal relevance judgments. Commercial search engines maintain such data in the form of query logs and click-through data. Unfortunately, we do not have ac-cess to such personal data. Thus we investigate publicly available personalized data in the form of social bookmark-ing (tag) data. We use data from the website del.icio.us to simulate a personalized DIR test collection, where we ap-proximate query logs and URL clicks with del.icio.us tags and bookmarked URLs. In other words, we use an individ-ual X  X  personal but public bookmark history as an approxi-mate substitute for their private search activity. We have recently collected a large quantity of bookmark data:
In order to build a distributed test collection we need to take this collection of documents (URLs) and distribute the documents across different collections. The most obvious way to do that, while preserving natural homogeneity in the data, is to put all URLs from the same domain (the same URL hostname) into the same collection. So we looked for the most common domains in the del.icio.us dataset and created a collection for each. The table below shows the five most common domains in our sample of del.icio.us:
Obviously, the set of distinct documents per domain is too small to constitute viable test collections for DIR. Test collections should be at least of the order of hundreds of thousands if not millions of documents. In order to create such collections we are currently crawling each domain using these distinct URLs as seeds. We are crawling the 100 most frequent domains from del.icio.us up to depth 10, where each domain has over 200 seed URLs associated with it.

Evaluating personalization in a Distributed IR setting is an non-trivial undertaking. Our plan is to compare directly a personalized system with a control (a non-personalized system) over a set of more than 100,000 queries. For each query (taken from the user X  X  search history), we check to see if a known relevant document (i.e. the document that the user had clicked on) moves up or down on the ranked list, as a result of personalization. Since we don X  X  have query logs and click-through data we will use user tags in lieu of queries and bookmarked documents in lieu of the clicked ones.
On average, the relevant document should move up on the list as the result of personalization. Thus we will count the number of times this occurs to test the efficacy of each personalization approach. We will also measure how far doc-uments move in the ranking relative to their initial position in terms of the Mean Reciprocal Rank (MRR). Together these metrics give us a good indication of the quality of the personalization approach. We note that more traditional metrics such as the Mean Average Precision (MAP) are not directly applicable in our case, since we only know of one relevant document per query and have an open-world as-sumption (other documents may also be relevant) and thus can only compare two systems based on the position of the known document.

The proposed evaluation setting in which personalization techniques for the various phases of DIR are evaluated in situ , is different from traditional techniques for evaluating QBS and resource selection algorithms. Previously, researchers have defined metrics to measure the quality of resource rep-resentations or selection rankings. We believe such metrics to be necessarily sub-optimal and prefer to evaluate differ-ent personalization approaches in terms of their overall DIR performance.

Unfortunately preliminary results for our experiments could not be given in this paper, due to the time-consuming na-ture of the Web crawling required to generate the large (over 100,000 queries) test collections that we are building. [1] J. Callan and M. Connell. Query-based sampling of [2] X. Shen, B. Tan, and C. Zhai. Context-sensitive [3] M. Shokouhi. Segmentation of search engine results for [4] L. Si and J. Callan. Relevant document distribution
