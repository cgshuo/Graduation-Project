 A commercial Web page typically contains many information blocks. Apart from the main content blocks, it usually has such blocks as navigation panels, c opyright and privacy notices, and advertisements (for business purposes and for easy user access). page the noisy blocks. We show th at the information contained in these noisy blocks can serious ly harm Web data mining. Eliminating these noises is thus of great importance. In this paper, we propose a noise elimination technique based on the following observation: In a given Web site, noisy blocks usually share some common contents and presentation styles, while the main content blocks of the pages are often diverse in their actual contents and/or presentation styles. Base d on this observation, we propose presentation styles and the actual contents of the pages in a given built for the site, which we call the Site Style Tree (SST). We then introduce an information based measure to determine which parts of the SST represent noise s and which parts represent the main contents of the site. The SST is employed to detect and eliminate noises in any Web page of the site by mapping this page to the SST. The proposed technique is evaluated with two data Experimental results show that our noise elimination technique is able to improve the mining results significantly. H.3.3 [INFORMATION ST ORAGE AND RETRIEVAL]: Information Search and Retrieval  X  clustering, information filtering, selection process.
 Algorithm, Design, Expe rimentation, Theory. Noise detection, noise e limination, Web mining. The rapid expansion of the Internet has made the WWW a popular place for disseminating and collecting information. Data mining on the Web thus becomes an important task for discovering useful knowledge or in formation from the Web [6][9]. However, useful information on the Web is often accompanied by a large amount of noise such as banner advertisements, navigation bars, copyright notices, etc. A lthough such information items are functionally useful for human viewers and necessary for the Web site owners, they often hamper automated information gathering and Web data mining, e.g., Web pa ge clustering, classification, information retrieval and inform ation extraction. Web noises can be grouped into two categories according to their granularities: Global noises : These are noises on the Web with large granularity, Local ( intra-page ) noises : These are noisy regions/items within a In this work, we focus on detecting and eliminating local noises in Web pages to improve the perform ance of Web mining, e.g., Web page clustering and classification. This work is motivated by a practical application. A commercial company asked us to build a classifier for a number of products. They want to download product description and review pa ges from the Web and then use the classifier to classify the pages into different categories. In this paper, we will show that local noises in Web pages can seriously harm the accuracy of data mining. Thus cleaning the Web pages before mini ng becomes critical for improving the data mining results. We call this preprocessing step Web page cleaning . Figure 1 gives a sample page from PCMag 1 . This page contains an evaluation report of Samsung ML-1430 printer. The main content (segment 3 in Figure 1) only occupies 1/3 of the original Web page, and the rest of the page contains many advertisements, navigation links (e.g., segment 1 in Figure 1), magazine subscription forms, privacy statements, etc. If we perform clustering on a set of product pages like this page, such items are irrelevant and should be removed. Despite its importance, relatively little work has been done on Web page cleaning in the past (see our related work section). In this paper, we propose a highly effective technique to clean Web pages with the purpose of improving Web data mining. Note that although XML 2 Web pages are more powerful than HMTL pages for describing the cont ents of a page and one can use XML tags to find the main contents for various purposes, most current Web pages on the Web are still in HTML rather than in XML. The huge number of HTML pages on the Web are not likely to be transformed to XML pages in the near future. Hence, we focus our work on cleaning HTML pages. Our cleaning technique is based on the following observation. In a typical commercial Web site, Web pages tend to follow some fixed layouts or presentation styl es as most pages are generated automatically. Those parts of a page whose layouts and actual contents (i.e., texts, images, links, etc) also appear in many other pages in the site are more likely to be noises, and those parts of a page whose layouts or actual c ontents are quite different from other pages are usually the main contents of the page. tree , to capture the common layouts (or presentation styles) and the actual contents of the pages in a Web site. We then propose an information based measure to dete rmine which parts of the style http://www.w3.org/XML/ main contents of the pages in the Web site. To clean a new page from the same site, we simply map the page to the style tree of the site. According to the mapping, we can decide the noisy parts and delete them. Our experiment results based on two popular Web mining tasks, i.e., Web page clustering and Web page classification, show that classification accuracy over all our datasets increases from 0.625 before cleaning to 0.954 after cleaning. This represents a remarkable improvement. We al so compare our proposed method with the existing template based cleaning method [2]. Our results show that the proposed method out performs this existing state-of-the-art method substantially. Our contributions  X  A new tree structure, called Style Tree , is proposed to capture  X  Experimental results show that the proposed page cleaning Although Web page cleaning is an important task, relatively little work has been done in this field. In [17], a method is proposed to detect informative blocks in ne ws Web pages. The concept of informative blocks is similar to our concept of main contents of a page. However, the work in [17] is limited by the following two assumptions: (1) the system knows a prori how a Web page can be partitioned into coherent cont ent blocks, and (2) the system knows a priori which blocks are the same blocks in different Web pages. As we will see, partitioning a Web page and identifying corresponding blocks in different pages are actually two critical issues in Web page cleaning. Our system is able to perform these tasks automatically (with no user he lp). Besides, their work views a Web page as a flat collection of blocks which correspond to  X TABLE X  elements in Web pages, and each block is viewed as a collection of words. These assumptions are often true in news Web pages, which is the domain of their applications. In general, these assumptions are too strong. In [2], Web page cleaning is defined as a frequent template detection problem. They propose a frequency based data mining algorithm to detect templates and views those templates as noises. The cleaning method in [2] is not concerned with the context of a Web site, which can give usef ul clues for page cleaning. Moreover, in [2], the partitioning of a Web page is pre-fixed by considering the number of hyperli nks that an HTML element has. This partitioning method is simple and useful for a set of Web pages from different We b sites, while it is not suitable for Web pages that are all from the same Web site because a Web site typically has its own common layouts or presentation styles, which can be exploited to partition Web pages and to detect noises. We will compare the results of our method with those of the method in [2] and give a discu ssion in the experiment section. Other related work includes data cleaning for data mining and data Warehousing [13], duplicate records detection in textual databases [16] and data prepro cessing for Web Usage Mining [7]. Our task is different as we deal with semi-structured Web pages and also we focus on removing noisy parts of a page rather than duplicate pages. Hence, different cleaning techniques are needed. Web page cleaning is also related to feature selection in features are individual words or attributes. However, items in Web pages have some structures , which are reflected by their nested HTML tags. Hence, different methods are needed in the context of the Web. [8][10] propose some learning mechanisms to recognize banner ads, redundant and irrelevant links of Web pages. However, these techniques are not automatic. They require a large set of manually labeled training data and also domain knowledge to generate classification rules. [11] enhances the HITS algorithm [12] by using the entropy of anchor text to evaluate the importance of links. It focuses on improving HITS algorithm to find more informative structures in Web sites. Although it segments Web pages into content blocks to avoid unnecessary authority and hub propagations, it does not detect or eliminate noisy contents in Web pages. The proposed cleaning technique is based on the analysis of both the layouts and the actual contents (i.e., texts, images, etc.) of the suitable data structure to represent both the presentation styles (or layouts) and the actual contents of the Web pages in the site. We propose a Style Tree (ST) for this purpose. Below, we start by giving an overview of the DOM (Document Object Model) 3 tree, which is commonly used for repres enting the structure of a single Web page, and showing that it is insufficient for our purpose. We then present the style tree, which is followed by our entropy measure for evaluating the nodes in the style tree for noise detection. Each HTML page corresponds to a DOM tree where tags are internal nodes and the detailed texts, images or hyperlinks are the leaf nodes. Figure 2 shows a se gment of HTML codes and its corresponding DOM tree. In the DOM tree, each solid rectangle is a tag node. The shaded box is the actual content of the node, e.g., for the tag IMG, the actual conten ts is  X src=image.gif X . Notice that our study of HTML Web pages begins from the BODY tag since all the viewable parts are within the scope of BODY. Each node is also attached with its display properties. For convenience of analysis, we add a virtual root node without any attribute as the parent tag node of BODY in the DOM tree. Figure 2: A DOM tree example (lower level tags are omitted) Although a DOM tree is sufficient for representing the layout or overall presentation style and conten t of a set of HTML pages and to clean them based on indivi dual DOM trees. Thus, DOM trees are not enough in our cleaning work which considers both the presentation style and real content of the Web pages. We need a more powerful structure for this purpose. This structure is critical because our algorithm needs it to find common styles of the pages structure, called style tree (ST), which is able to compress the common presentation styles of a set of related Web pages. A style tree example is given in Figure 3 as a combination of DOM trees d 1 and d 2 . We observe that, except for the four tags (P, IMG, P and A) at the bottom level, all the tags in d corresponding tags in d 2 . Thus, d 1 and d 2 can be compressed. We use a count to indicate how many pages have a particular style at both pages start with BODY, and thus BODY has a count of 2. Below BODY, both pages also have the same presentation style of TABLE-IMG-TABLE. We call this whole sequence of tags (TABLE-IMG-TABLE) a style node , which is enclosed in a dash-style at this point. A style node is thus a sequence of tag nodes in a DOM tree. In the style tree, we call these tag nodes the element nodes so as to distinguish them from tag nodes in the DOM tree. For example, the TABLE-IMG-TABLE style node has three element nodes, TABLE, IMG and TABLE. An element node also contains slightly different info rmation from a tag node in a DOM tree as will be defined later. In Figure 3, we can see that below the right most TABLE tag, d and d 2 diverge, which is reflected by two different style nodes in the style tree. The two style nodes are P-IMG-P-A and P-BR-P respectively. This means below the right TABLE node, we have two different presentation styles . The page count of these two style nodes are both 1. Clearly, the style tree is a compressed representation of the two DOM trees. It enables us to see which parts of the DOM trees are common and which parts are different. We now define a style tree, which consists of two types of nodes, namely, style nodes and element nodes. In Figure 3, the style node (in a dash-lined rectangle) P-IMG-P-A has 4 element nodes, P, IMG, P and A, and n = 1. Definition: An element node E has three components, denoted by Note that an element node corre sponds to a tag node in the DOM tree, but points to a set of child style nodes Ss (see Figure 3). For convenience, we usually denote an element node by its tag name, and a style node by its sequence of tag names corresponding to its element node sequence. Building a style tree (called site style tree or SST) for the pages of a Web site is fairly straightforw ard. We first build a DOM tree for each page and then merge it into the style tree in a top-down has the corresponding tag node T in the DOM tree, we check the same as the sequence of element nodes in a style node S below E (in the style tree). If the answer is yes, we simply increment the and the DOM tree to merge the rest of the nodes. If the answer is style tree. The sub-tree of the tag node T in the DOM tree is copied to the style tree after converted to style nodes and element nodes of the style tree. In our work, the definition of noise is based on the following assumptions: (1) The more presentation styles that an element node has, the more important it is, and vice versa. (2) The more importance values are used in evaluating the importance of an element node. The presentation im portance aims at detecting noises with regular presentation styles while the content importance aims at identifying thos e main contents of the pages that may be presented in similar presentation styles. Hence, in the proposed method the importance of an element node is given by combining its presentation impor tance and content importance. The greater the combined importa nce of an element node is, the more likely it is the main content of the pages. In the example of Figure 4, the shaded parts of the SST are more likely to be noises since their presentation styles (together with their actual contents which cannot be shown in the figure) are highly regular and fixed and hence less important. The double-lined Table element node has many child style nodes, which the double-lined Table is more likely to contain the main contents of the pages. Specially, the double-lined Text element node is also meaningful since its content is diverse although its presentation pages of a Web site. We need a metric to measure the importance of a presentation style. Information theory (or entropy) is a natural choice. Definition ( node importance ) : For an element node E in the SST, different styles is small. Hence the value of NodeImp(E) is small. If E contains many presentation styles, then the value of NodeImp ( E ) is large. For example, in the SST of Figure 4, the importance of the element node Body is 0 ( l log 100 l = 0) since l = 1. That is, below Body, there is only one presentation style Table-Img-Table-Table. The importan ce of the double-lined Table is -0.35 log 100 0.35 -2*0.25 log 100 0.25-0.15 log 100 0.15 = 0.292 &gt; 0 However, we cannot say that Body is a noisy item by considering only its node importance because it does not consider the importance of its descendents. We use composite importance to measure the importance of an element node and its descendents. Definition ( composite importance ) : For an internal element node the weight of NodeImp ( E ) when l is large. It decreases the weight of NodeImp(E) when l is small. This means that the more child style nodes an element node ha s, the more its composite importance is focused on itself, and the fewer child style nodes it has, the more its composite importance is focused on its descendents. Leaf nodes are different from inte rnal nodes since they only have actual content with no tags. We define the composite importance of a leaf element node based on the information in its actual contents (i.e., texts, images, links, etc.) Note that if m = 1, it means that only one page contains E , then E is a very important node, and its CompImp is 1 (all the values of CompImp are normalized to between 0 and 1). Calculating composite importance (using the CalcCompImp ( E ) procedure) for all element nodes and style nodes can be easily done by traversing the SST. We will not discuss it further here. As mentioned earlier, our defin ition of noise is based on the assumptions that the more presentation styles that are used to compose an element node the more important the element node is and that the more diverse that the actual contents of an element node are, the more important the element node is. We now define what we mean by noises and give an algorithm to detect and to eliminate them. Definition ( noisy ) : For an element node E in the SST, if all of its Figure 5 gives the algorithm MarkNoise ( E ) to identify noises in not. If any one of them is not noisy, then E is not noisy. If all its descendents are noisy and E  X s composite importance is also small, then E is noisy. Definition ( maximal noisy element node ) : If a noisy element 1: for each S  X  E.Ss do 2: for each e  X  S.Es do 3: if ( MarkNoise ( e ) == FALSE ) then 4: return FALSE 5: end if 6: end for 7: end for 8: if ( E.CompImp  X  t ) then 9: mark E as  X noisy X  10: return TRUE 11: else return FALSE 12: end if In other words, if an element node E is noisy and none of its ancestor nodes is noisy, then E is a maximal noisy element node, which is also marked by the algorithm in Figure 5. Definition ( meaningful ) : If an element node E in the SST does Definition ( maximal meaningful element node ) : If a meaningful Notice that some element nodes in the SST may be neither noisy nor meaningful, e.g., an element node containing both noisy and meaningful descendents. Similar to MarkNoise(EN) , the algorithm MarkMeaningful(EN) marks all the maximal meaningful element nodes. Note that in the actual implementation, the function CalcCompImp(EN) , MarkNoise(EN) and MarkMeaningful(EN) are all combined into one in order to reduce the number of scans of the SST. Here we discuss them separately for clarity. Since we are able to identify maximal meaningful element nodes and maximal noisy element nodes in the STT, we need not traverse the whole SST to detect and eliminate noises. Going down from the root of the SST, when we find a maximal noisy node, we can instantly confirm that the node and its descendents are noisy. So we can simplify the SST into a simpler tree by removing descendents of maxi mal noisy nodes and maximal meaningful nodes in the SST. Let us go back to the SST in Figure 4. Assume that we have identified the element nodes in the shaded areas to be noisy and the double-lined element nodes to be meaningful, the SST can be simplified to the one in Figure 6. We now give the algorithm for de tecting and eliminating noises (Figure 7) given a SST and a ne w page from the same site. The algorithm basically maps the DOM tree of the page to the SST, and depending on where each part of the DOM tree is mapped to the SST, we can find whether the part is meaningful or noisy by checking if the corresponding element node in the SST is meaningful or noisy. If the corre sponding element node is neither noisy nor meaningful, we simply go down to the lower level nodes. For easy presentation of the algorithm, we assume that the DOM (called a page style tree or PST). The algorithm MapSST takes two inputs, an element node E in the SST and an element node E of the page style tree. At the beginning, they are the respective root nodes in the SST and the page style tree. Figure 8 summarizes all the steps of our Web cleaning algorithm. Given a Web site, the system fi rst randomly crawls a number of Web pages from the Web site (lin e 1) and builds the SST based on these pages (line 2-6). In many sites, we could not crawl all its pages because they are too large. By calculating the composite importance of each element node in the SST, we find the maximal noisy nodes and maximal meaningful nodes. To clean a new page P , we map its PST to the SST to eliminate noises (lines 10-13). The algorithm introduced above is the basic algorithm. Some minor tunings are needed to make it more effective. 1. For any two style nodes S 1 and S 2 belonging to the same 1: Randomly crawl k pages from the given Web site S 2: Set null SST with virtual root E (representing the root); 3: for each page W in the k pages do 4: BuildPST(W); 5: BuildSST( E, E w ) 6: end for 7: CalcCompImp( E ); 8: MarkNoise( E ); 9: MarkMeaningful( E ); 10: for each target Web page P do 11: E p = BuildPST( P ) /* representing the root */ 12: MapSST( E , E p ) 13: end for 1: if E is noisy then 2: delete E P (and its descendents) as noises 3: return NULL 4: end if 5: if E is meaningful then 6: E p is meaningful 7: return the content under E P 8: else returnContent = NULL 9: S 2 is the (only) style node in E P .Ss 10: if  X  S 1  X  E.Ss  X  S 2 matches S 1 then 11: e 1,i is the i th element node in sequence S 1 .Es ; 12: e 2 , i is the i th element node in sequence S 2 .Es ; 13: for each pair ( e 1,i , e 2,i ) do 14: returnContent += MapSST ( e 1,i , e 2,i ) 15: end for 16: return returnContent 17: else E P is possibly meaningful; 18: return the content under E P 19: end if 2 0: end if 2. The leaf tag nodes used for the algorithm should not be the 3. It is possible that although an element node in the SST is This section evaluates the propos ed noise elimination algorithm. Since the purpose of our noise elim ination is to improve Web data mining, we performed two data mi ning tasks, i.e., clustering and classification, to test our system . By comparing the mining results before and after cleaning, we show that our cleaning technique is able to improve mining results s ubstantially. We also compare our results with the mining results obtained after cleaning using the template based technique proposed in [2]. To distinguish the method proposed in [2] with our method in discussion, we denote the method in [2] as the template based method and denote our method as the SST based method . Note that we could not compare task. It is designed specifically for identifying main news articles in news Web pages, and it makes some assumptions that are not suitable for general page cleaning (see Section 2). Below, we first describe datase ts used in our experiments and evaluation measures. We then pres ent our experiment results of clustering and classification, and also give some discussions. Our empirical evaluation is done using Web pages from 5 commercial Web sites, Amazon 4 , CNet 5 , PCMag, J&amp;R ZDnet 7 . These sites contain many introduction or overview pages of different kinds of products. To help the users navigate the site and to show advertisem ents, the pages from these sites all contain a large amount of noise. We will show that the noise misleads data mining algorithms to produce poor results (both in clustering and in classification). However, our technique of detecting and eliminating noise is able to improve the mining results substantially. The five Web sites contain Web pages of many categories or classes of products. We choose th e Web pages that focus on the following categories of products : Notebook, Digital Camera, Mobile Phone, Printer and TV. Table 1 lists the number of documents downloaded from each Web site, and their corresponding classes. Since we test our system using clustering and classification, we use the popular F score measure to evaluate the results before and after cleaning. F score is defined as follows: average effect of both precision and recall. We will also include the accuracy results for classification. We now present the experimental results of Web page clustering and classification before and after cleaning and compare our method with the template based method. For the experiments, we implem ented the template based method HTML pages into flattened pagelets according to the number of hyperlinks each HTML element contains (see Section 4.2.2 for the definition of pagelet). Then it uses the shingle technique [5] to fingerprint that is invariant under small perturbations [2]. For our application, we use the local template detection algorithm in [2] to detect templates. According to the algorithm, a group of (no less than 2) pagelets whose shingles are the same is treated as a template and is deleted. Additi onally, we use the template based method to clean the Web pages in each individual site separately which proves to be more effective. For cleaning in our method, the site style tree of each Web site is Web sites Amazon CNet J&amp;R PCMa g ZDnet built using 500 randomly sampled pages from the site. We also tried larger numbers. However, we found that 500 pages are sufficient and more sampled pa ges do not improve the cleaning results. Our cleaning algorithm needs a threshold to decide noisy and meaningful elements. We set the threshold for each Web site as follows: For each Web site, we choose a small number of pages (20), and then clean them using a number of threshold values. We then look at the cleaned pages, and according to these cleaned pages, we set the final threshold. We use the popular k -means clustering algorithm [1]. We put all the 5 categories of Web pages into a big set, and use the means algorithm selects the initial cluster seeds randomly, we performed a large number of experiments (800) to show the behaviors of k -means clustering before and after page cleaning. The cumulative distributions of F scores before and after cleaning are plotted in Figure 9, where X-axis shows 10 bins of F score from 0 to 1 with each bin size of 0.1 and Y-axis gives the number of experiments whose F scores fall into each bin. The F score for each experiment is the average value of the 5 classes. It is computed as follows: By comparing the pages X  original classes and the k -means clustering results, we find the optimal assignment of classes to clusters that gives the best average F score for the 5 classes. From Figure 9, we can clearly observe that clustering results after our SST based cleaning are dramatically better than the results using the original noisy Web pa ges. Our method also helps to produce much better clustering results than the template based method. Table 2 gives the sta tistics of F scores over the 800 clustering runs using the original Web pages, the pages cleaned with the template based method and the pages cleaned with the SST based method respectively. We observe that over the 800 0.506, and the average F score for the template based cleaning case is 0.631, while the average F score for the SST based cleaning case is 0.751, which is a remarkable improvement. More specifically, before cleaning, only 0.5% of the 800 results (4 out of 800) have the F scor es no less than 0.7, and 47.63% lower than 0.5. After template based cleaning, 23.25% of the 800 lower than 0.5. While after the SST based cleaning, 78.13% of the 800 results have F scores no less than 0.7, and only 3.25% lower than 0.5. Thus, we can conclude that our proposed noise elimination method is much more effective than the template based method for Web page clustering. For classification, we use the Naive Bayesian classifier (NB), which has been shown to perform very well in practice by many researchers [14][15]. The basic idea of NB is to use the joint probabilities of words and classes to estimate the probabilities of classes given a document. In order to study how Web page noise affects classification accuracy and to better understand the situations where noise elimination is most effective, we performed a comprehensive configurations. Method Ave(F) F &lt; 0.5 F &gt;= 0.7 F &gt;= 0.8 F &gt;= 0.9 In each experiment, we build a classifier based on training pages the test pages. We denote the two classes by C 1 may be camera and C 2 may be notebook. Let the five Web sites training and test sets fro m different Web sites: 1. TR = { C 1 ( Site i ) and C 2 ( Site j )}, and TE = {all C 3. TR = { C 1 ( Site i ) and C 2 ( Site j )} ( i  X  j ), and TE = {all C We tried all possible two class combinations of the 5 sites for the three configurations. Table 3 and Table 4 respectively show the average F scores and the aver age accuracies of the three configurations before and after cl eaning. In Table 3 and Table 4, F ( i = 1, 2, 3) and A i ( i = 1, 2, 3) respectively denote the average F score and accuracy of classification under the i -th configuration. The average F scores (or accuracies) are computed by averaging the F scores (or accuracies) of all possible two class combinations within 5 sites according to different configurations. Note that since there are no TV pages in PCMag and ZDnet sites, so we only averaged the results from thos e possible experiments. Again, in Table 3 and Table 4, N stands for no cleaning, T stands for cleaning using the template method, and S stands for the proposed method. From these two tables we can see that cleaning in general improves F score and accuracy in all cases. We also observe that in almost all cases the improvements made by our method are more significant than those made by the template based method. Below, we discuss the results of each configuration.  X  In the first configuration, since site specific noisy items occur  X  In the second configuration, cleaning makes a major (N) F 2 (T) F 2 (S) F 3 (N) F 3 (T) F 3 (S) (N) A 2 (T) A 2 (S) A 3 (N) A 3 (T) A 3 (S)  X  In the third configuration, our SST technique still performs We now explain why the templa te based method is not as effective as the SST based met hod. The template based method defines a pagelet as follows: The HTML elements in the parse tree are actually the tag nodes in our DOM tree. The template base d method gives the best results on average when we set k = 3 (which is the same as that given in [2]). However, since the granularity of partitioning a Web page completely depends on the number of linkages in HTML elements, the partitioning result may not coincide with the natural partitions of the Web page in question. This can result in under cleaning due to pagelets that are too large. For example, for k = 3, segment 2 in product pages from PCMag site have similar pagelets like P . The words  X Home X  and  X Product Guides X  in this pagelet are actually not useful for mining in our case. However, the pagelet P will not be removed because its content (together with the words  X Printer X  and  X Samsung ML-1430 X ) are differe nt from the pagelet contents in other Web pages. In our SST based method, segment 2 is a tag node T in the DOM tree of the page in Figure 1. In the SST of the PCMag site, similar tag nodes in th e rest of the Web pages will be grouped together with T to form a leaf element node E in the SST. Within the element node E , the words  X Home X  and  X Product Guides X  are very likely to be identified as noises because they appear too frequently in E although the element node E is meaningful as a whole. The template based method may al so result in excessive cleaning the idiosyncrasy of the pages a nd thus may result in removal of too much information from the pages because the template based method considers any repeating pagelet as noise. In contrast, our SST based met hod does not have these problems considers the importance of actua l content features within the context of their host element nodes in SST. Execution time: In our experiments, we randomly sample 500 Web pages from each given Web site to build its SST. The time taken to build a SST is always below 20 seconds. The process of computing composite im portance can always be finished in 2 seconds. The final step of cleaning each page takes less than 0.1 second. All our experiments were conducted on a Pentium 4 1.6GHz PC with 256 MB memory. In this paper, we proposed a t echnique to clean Web pages for Web data mining. Observing that the Web pages in a given Web site usually share some common la yout or presentation styles, we propose a new tree structure, called Style Tree (ST) to capture those frequent presentation styles and actual contents of the Web site. The site style tree (SST) provides us with rich information for analyzing both the structures and the contents of the Web pages. We also proposed an information based measure to evaluate the importance of elemen t nodes in SST so as to detect noises. To clean a page from a site , we simply map the page to its SST. Our cleaning technique is ev aluated using two data mining tasks. Our results show that the proposed technique is highly effective. Bing Liu was supported in part by the National Science Foundation (NSF) under the NSF grant IIS-0307239. [1] Anderberg, M.R. Cluster Analysis for Applications , [2] Bar-Yossef, Z. a nd Rajagopalan, S. Template Detection via [3] Beeferman, D., Berger, A. and Lafferty, J. A model of lexical [4] Beeferman, D., Berger, A. and Lafferty, J. Statistical models [5] Broder, A., Glassman, S., Ma nasse, M. and Zweig, G. [6] Chakrabarti, S. Mining the Web: Discovering Knowledge [7] Cooley, R., Mobasher, B. and Srivastava, J. Data [8] Davision, B.D. Recognizing Nepotistic links on the Web . [9] Han, J. and Chang, K. C.-C. Data Mining for Web [10] Jushmerick, N. Learning to remove Internet advertisements , [11] Kao, J.Y., Lin, S.H. Ho, J.M. and Chen, M.S. Entropy-based [12] Kleinberg, J. Authoritative Sources in a Hyperlinked [13] Lee, M.L., Ling, W. and Low, W.L. Intelliclean: A [14] Lewis, D. and Gale, W. A sequential algorithm for training [15] McCallum, A. and Nigam, K. A comparison of event models [16] Nahm, U.Y., Bilenko, M. and Mooney R.J. Two Approaches [17] Shian-Hua Lin and Jan-Ming Ho. Discovering Informative [18] Yang, Y. and Pedersen, J.O. A comparative study on feature 
