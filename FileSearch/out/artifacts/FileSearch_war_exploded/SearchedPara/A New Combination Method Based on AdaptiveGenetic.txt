 Traditional of information retrieval models uses the exact correspondence be-tween the document terms and the query t erms to select a document to return to the user. The problem of these models is that the meaning of a word can be expressed in different words, and on e word can express different meanings in different contexts. This is due to the richness of the mechanisms of reflection and linguistic expression.

Some studies as [11], [26] have highlighted the inadequacy of document rep-resentation based on simple words [11], [26]. The authors in [8] showed that only 20% of Internet users use an application 100% accurately to their needs. Indeed, this wealth can be a source of ambiguity in natural language. To over-come the problem of term ambiguity, some research works proposed the use of relevance feedback. This technique allow to partially circumvent the problem of term synonymy, but it is not a satisfactory solution.

Recently, many approaches based on se mantic indexing have been proposed [2], [6], [19]. The idea of the semantic indexing is to identify all the document terms, project them on an external resour ce [17](as ontologies) to extract con-cepts. The authors in [16] have shown that this technique does not cover all the meanings of words. Therefore, this approache needs a semantic resource with a rich terminology covers all terms addre ssed in the documents of the collection area. All areas require an aspect of optimization, including optimization algo-rithm we quote the genetic algorithm that is inspired by the genetic operations. Generally, GA is used either for optimization or for selection of parameters [18], [21]. In this paper, we focus on medical image retrieval using surrounding text as the annotation. We propose to compute two scores: the first based on the tex-tual representation of the image annotation, and the second based on conceptual representation of the image annotation. Then, we adapt the genetic algorithm to combine these two scores. Wherein the two weights are two independent num-bers. The rest of the paper is organized as follows. Some related work about the combination of two scores is described in Section 2. Following this, in section 3 we give the details of our approach about computing textual and conceptual scores for medical images. In section 4, we describe the genetic algorithm and how we adapt it to combine textual and conceptual scores to one medical image final score. Section 5 presents the corpus us ed in our experiments, the evaluation metrics and experimental results. Finally, we conclude in Section 6 with possible future work. Recently, many studies have highlighted th e inadequate representation of doc-uments (D) and requests (R) based on simple words [3], [26] and proposed to explore the semantics textual representation of D / R [19]. The idea of these ap-proaches is to represent the D / R in the form of concepts extracted by projecting the text of the D / R to an external resource such as a semantic ontology. Thus, for a given text, only the words or recogn ized by the semantic resource sentences will be translated into concepts. We are talking about a conceptual representa-tion or a conceptual indexing [5], [17], [19]. The disadvantage of this design is that indexing is based on the assumption that all the terms of the D / R exist in the semantic resource and therefore the passage of a textual representation to a conceptual representation will be well done. This hypothesis requires the use of a semantic resource with a rich terminology that covers the entire area covered in the documents of the collection. In its approach, Baziz [5] indicated that an ontology (e.g. Wordnet) does not cover all the vocabulary used in the collection. Therefore, Baziz [5] and Hinrich [14] pr oposed to combine two types of indexing: one using keywords and one using concepts. This idea allows to have significant results. One of the factors that influence performance is probably the quality of the ontology and especially its coverage of the vocabulary in the corpus.
Several methods are typically based on well known voting-based data fusion techniques [9] (e.g., CombMAX, CombLin, CombRank, etc.) that have been used to combine data from different information sources [4], [20], [24]. The existing methods of combination are manual methods, and do not cover the entire search space. In addition, these methods uses dependent weights, which minimizes the number of possible combinations. Table 1 depicts some voting techniques. They are grouped into two categories accord ing to the source of evidence used. Four phases are the components of our attempt to develop a retrieval model for medical information. This model comb ines two different annotation methods: conceptual and textual. The first phase is the pre-processing. The second step is the step of indexing. For the textual indexing, we used Terrier IR platform, the open source search engine written in Java and developed at the School of Com-puting, University of Glasgow, and the conceptual indexing step is discrupted in the next section. The third phase is the calculation of the score for each image by both textual and conceptual model, using the vector space model.The last step is consists of searching the optimal weights of two scores from both textual and conceptual model, this step are based on the genetic algorithm. An overview of our model is done in Fig. 1. 3.1 Mapping Text to Concepts The aim of this step is to map text into concepts. For this purpose, we start by preprocessing the collection and re move the stop word in order to keep only significant words. After the preproce ssing step, we extract the concepts from the text, the switch of text to concepts is realized by an innate procedure sys-tem named MetaMap [3], followed by the U.S. National Library of Medicine. MetaMap scrutinized biomedical free-te xt and identified concepts that are de-rived from Unified Medical Language System (UMLS). MetaMap is broadly im-plemented in clinical NLP and IR. By means of MetaMap, the both queries and papers are transformed, [3] endow us with supplementary fine points of this course of action. The extraction of co ncepts step is a process which allows to highlight the most significant topics of the document by extracting the most relevant concepts. For example, the paradigm of  X  X rain cancer trial X  MetaMap instrument engenders the concepts ID C0153633 and C0008976, which stand for correspondingly  X  X rain cancer X  from the semantic brand  X  X eoplastic Process X  and  X  X rial X  from the semantic brand  X  X esearch Activity X  , as the paramount con-sequences amid the seven meta-nominees . After a thorough study on the UMLS ontology, we found that it suffers from several problems in relations between concepts such as erroneous relationship.

Authors in [7] reveale that a total of 17 022 (24.3%) of associations (parent-child) between UMLS notions can not be justified according to the semantic categories of concepts. Among several cases that can produce artificial relations, we cite:  X  Cases where the semantic category o f the child is very broad whereas the  X  Situations where the parent-child relationship is erroneous;  X  Cases where a parent-child relationship is lacking and have to be added to  X  Conditions where the parent or the child is missing a semantic category; In fact, these problems can lead to false co nceptual annotations. It should, how-ever, be noted those mistaken concepts may be chosen and hence the implicating of the article will be influenced. To surmount this problem, we use the method of enhancement concepts based on graph th eory proposed by Gasmi [12], they put forward the computation of associatio ns between concepts, and no more than associating concepts will be chosen.

Therefore, this method aims to select re levant concepts and to eliminate mis-conceptions. This method involves calc ulating distances between two types of concepts generated. Both distances are: one based on the arcs and another based on the information content of a concept. So, they have found as shown by a graph of concepts linked by arcs formed by the hybridization of two types of distances. Thereafter, the concepts which are no t connected with other concepts will be deleted [12].
 According to Fig. 1, D1 is a document presented by C1, C2, C3, C5, C7 and C8. After the weighting step, D1 is presented only by C1, C2, C5 and C7. C3 and C8 are removed because they do not have an y relationship with other concepts. 3.2 Conceptual and Textual Score Fusion To automate the combination of conceptual and textual results, we chose to im-plement the genetic algorithm to find the most optimal weight for each method, we followed the following equation: score text i :This is the score of the document i, obtained by the textual method score con i :This is the score of the document i, obtained by the conceptual method  X  ,  X  :are the weights of the two scores. Wherein the two weights are two inde-pendent numbers. Several researchers have used a genetic algorithm (GA) to find the most optimal solution in the retrieval information field. Including the use of this algorithm, the authors in [18], [21] use this algorithm to find the optimal combination of query. In its thesis, Yahya [1] proposed a method of combination of multiple similarity measures in the field of chemical information retrieval. This combination based on genetic algorithm can produce better results.

In genetic algorithms [13], the basic idea is to simulate the population evo-lution process. We start from a population of N solutions to the problem rep-resented by individuals. The randomly selected population is called a relative population. The individual adaptation degree to the environment is expressed by the value of the cost function f (x), where x is the solution that the individ-ual represents. It is declared that more an individual is better adapted to its environment, more the cost of the solution is lower.
For each problem to solve, a fitness function f should be provided. Its choice is crucial for the proper functioning of the algorithm. Given a chromosome, the fitness function must return a numeric value that represents its utility. This score will be used in the selection process of the parents, so that the fittest individuals will have a greater chance of being selected.

Within this population, a random selection of one or both parents is done, producing a new solution through genetic operators such as crossover and muta-tion. The new population, obtained by the choice of N individuals among parent and child populations is called next generation. By repeating this process, it pro-duces a richer individuals that are better adapted to the population. Fig. 2 shows the procedure of the proposed GA weighting combining-annotation method as follows:
Most operations require the Genetic Algorithm fitness function to calculate the adaptation of the individual. In the proposed approach, our fitness function depends on tow factors: (1) it depends on results returned by the retrieval model according to mean average precision (M AP), MAP is taken as a user who selected in each iteration the relevant documents f ound for calculated the Fitness for each chromosome; (2) the absolute value of subtraction between the two weights is added. The aim of the second part is to minimize the cost of any influence of annotations compared to the other on the Fitness function 2.
 With: where APq is the average precision of a query q ,and M is the number of queries. Precision = | relevantdocuments retrievaldocuments | / | retrievaldocuments |
In our proposed method, we use the MAP measure, that is connected directly by the number of relevant documents found as he shows his formula, MAP is taken as a user who selected in each iterat ion the relevant documents found for calculated the Fitness for each chromosome.  X  Step 1 : Encodes the chromosomes and the parameters representing the  X  Step2 : Initializes the population and produces the initial population of chro- X  Step3 : The fitness for each chromosome must be computed, this is related  X  Step 5 : The main feature is that the fitness value decreasing during the last  X  Step 6 : The iteration process stops only when the two criteria are achieved.  X  Step 7 : To generate a offspring generation, genetic operations should be 4.1 Crossover The main operator acting on the population of parents is the crossover, which is applied with a certain probability, called crossover rate Pc (typically close to unity). The crossing is to choose two individuals represented by their chains of genes randomly selected from the general population and define random or more crossing points. The new children are, then, created in inter changing different parts of each string.

Let G  X  and G  X  be two selected parent chromosomes, which are represented respectively as follows: 4.2 Mutation This operation protects genetic algorithms premature loss of relevant informa-tion. It allows introducing some information in the population, which could be lost during the crossing operation. Thus, it helps to maintain diversity, useful for a good exploration of the research area. The mutation operator is applied with a certain probability, called mutation rate Pm, typically between 0.05 and 0.10. In binary code, the mutation involves changing a 1 bit at bit 0, and vice versa, for each bit of the string, with the probability Pm. Let G  X  be the parent chromosome, 5.1 Data Sets and Evaluation Metrics To evaluate our approach, we use the 2009 and 2010 ImageCLEF collection composed respectively of 74,902 and 77000 medical images and annotations as-sociated with them. This collection contains images and captions from Radiology and Radiographic, two Radiological Society of North America (RSNA) journals. The number of queries is 25 from 2009 collection and 16 from 2010 collection. They are queries selected by experts in i nformation retrieval company to evalu-ate results by ImageCLEF collection. Ta ble 2 contains the parameters used for the genetic algorithm.

To evaluate our approach, we have used P@5, P@10 and Mean Average Pre-cision MAP. To statistically validate our results, we used the signed-rank test of the Wilcoxon test [27] which is the non-parametric equivalent of the paired sam-ple test. This test consists in evaluating a value of significance p  X  [0 , 1] which estimates the probability that the difference between the two methods is due to chance. We can thereby conclude that two methods are statistically different when p  X   X  ,where  X   X  0 . 05 is commonly used [15]. More precisely, the more p  X  0, the more two methods are supposed to be different.

In our experiments, we consider that the difference between two methods is significant when p  X  0 . 1 , and it is very significant when p  X  0 . 05. 5.2 Retrieval Model For both textual and conceptual representations, we have used the vector space model [22,23] to compute the similarity between documents and queries. Each dimension in this model represents a t erm or an ontology concept. Documents and queries are represented by a vector with n dimensions where n is the number of the terms or ontological concepts [25].
 The value of system relevance is calculated using the similarity function RSV ( Q, d ) (Retrieval Status Value) where Q is a query and D j is a document. The RSV is calculated as follows:  X  The term/concept frequency is: where : cf ij (respectively cf qi ) is the number of occurrences of the concept/term C i in the document D j (respectively the query q ); And idf ij that stands for the inverse document frequency, is equal to devide the number of documents containing the concept/term i , by the number of all documents in the collection. 5.3 Experimental Results In this section, we present the different results obtained by different combination methods. Results according to P@5, P@10 and MAP measures for 2009 and 2010 data sets are presented in Table 3.
 We note that our model results are better than conceptual and textual model. This observation affirms that the use of a combination solution improves the re-trieval accuracy. Consequently, we can conclude that the use of both model con-ceptual and textual is a good solution, on the one hand to improve the outcome, and on the other to improve the semantic representation of the document.
Best results are obtained by our combination method for both data sets. Gains in MAP measure are presented in Table 4.

Symbol * after the gain indicates statistical significance using the Wilcoxon test at p  X  0 . 1 . Symbol ** after the gain indicates statistical significance using the Wilcoxon test at p  X  0 . 05. Table 4 shows how our indexing approach is statistically significant and the improvements of our method compared to other methods rates. We computed the Wilcoxon test between means of each ranking obtained by each indexing method. The Wilcoxon test validated our method for the 2010 query set ( p  X  0 . 1) compared to the textual baseline. Additionally, Table 4 shows that our proposed method (GENTComb) is statistically significant compared to the conceptual baseline ( p  X  0 . 05) for the 2010 and 2009 query set. Concerning the Map measure, our method outperforms every combination method rated at a result and on the other hand the proposed method uses two annotation-weight which are two automatic and separate numbers, unlike other method used an manually and dependent weight. 5.4 Comparison of Our Method with Official Submissions of In this section, we compare our method with runs submitted for medical Im-ageCLEF 2009 and 2010. Only runs based on textual approaches are taken into account because our method is based only on textual annotation of images. For Medical IMAGE CLEF 2009, Table 5 compares our method (Gent-Comb) with the other official submissions.

For the best result of the IRIS team in 2009, the authors use a combination of two analysis tools, one with the MiniPar parser and the second with the TreeTag-ger tool. Then, they compare the combination of conceptual representation with the Kullback-Leiber divergence instead of the combination with likelihood func-tion. Two RSV are used : one based on log-probability and the other based on divergence function. The best result is obtained with the log-probability method. This method is improved by using a relevance feedback extension of queries with the n first returned documents. Best results are obtained with n = 100. Using our proposition, we are ranked second. We recall that we do not use the relevance feedback.

For Medical IMAGE CLEF 2010, Table 6 compares our method (Gent-Comb) with the other official submissions.
 Organisers consider the XRCE laboratory run is wrong and not applicable. Then, the best official result in 2010 is the one of Information Processing Lab-oratory team, the authors indexed the text usind Lucene framework. For the retrieval, first, the documents are expanded with the Mesh-terms, after that a combination of textual score and the sco re obtained by Mesh terms is released. To compute the fibal score of each medical image, a function proposed by Fang and Zhai [10] is used. Thanks to our proposed method, we are classified on first rank. The purpose of this paper is to better study the importance of combining two re-trieval methods; textual and conceptual method. To do this, we used the UMLS as a specific ontology for the medical domain, which allowed us to extract con-cepts representative of each document . Our approach begins with extracting concepts by MetaMap tool, after words, it uses the graph theory to retain only the relevant concepts, and later to refine the results obtained we used the genetic algorithm to combine the two types of indexing: textual and conceptual. Our re-sults showed that the combination method can on the one hand to improve the efficiency of our model and secondly to imp rove the semantic representation of the document.

