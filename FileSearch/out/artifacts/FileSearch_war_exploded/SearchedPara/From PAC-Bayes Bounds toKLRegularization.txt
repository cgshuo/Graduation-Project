 Pascal Germain, Alexandre Lacasse, Franc  X ois Laviolette, Mario Marchand, Sara Shanian ing data have been provided by using different inductive principles. But the universally accepted guarantee on the true risk, however, always comes with a so-called risk bound that holds uniformly mizes a tight risk (upper) bound.
 Among the data-dependent bounds that have been proposed recently, the PAC-Bayes bounds [6, 8, design of a bound-minimizing learning algorithm. In that respect, [4, 5, 3] have proposed to use of the posterior. Consequently, the resultant PAC-Bayes bound may have several local minima for certain data sets X  X hus giving an intractable optimization problem in the general case. To avoid such computational problems, we propose here to use convex loss functions for stochastic Gibbs classifiers that upper-bound the standard zero-one loss used for the weighted majority vote. By dinate descent learning algorithm to minimize the proposed KL -regularized cost function. We show uniform posterior and the uniform prior. We present numerical experiments where the proposed learning algorithm generally outperforms ridge regression and AdaBoost [7]. We consider binary classification problems where the input space X consists of an arbitrary subset of
R d and the output space Y = { X  1 , +1 } . An example is an input-output ( x ,y ) pair where x  X  X  and y  X  Y . Throughout the paper, we adopt the PAC setting where each example ( x ,y ) is drawn according to a fixed, but unknown, distribution D on X  X Y .
 any classifier h is defined by the frequency of training errors of h on S . Hence where I ( a ) = 1 if predicate a is true and 0 otherwise.
 over a space H of classifiers such that the Q -weighted majority vote classifier B Q will have the (sometimes called the Bayes classifier) is given by G
Q . To classify an input example x , the Gibbs classifier G Q chooses randomly a (deterministic) Gibbs classifier are thus given by R ( B Q )  X  2 R ( G Q ) . As shown in [5], this factor of 2 can sometimes be reduced to (1 + ) . In this paper, we use the following PAC-Bayes bound which is obtained directly from Theorem 1.2.1 of [1] and Corollary 2.2 of [3] by using 1  X  exp(  X  x )  X  x  X  x  X  R .
 Theorem 3.1. For any distribution D , any set H of classifiers, any distribution P of support H , any  X   X  (0 , 1] , and any positive real number C 0 , we have where KL( Q k P ) def = E R
S ( G Q ) and the PAC-Bayes regularizer KL( Q k P ) . As in boosting, we focus on the case where a tractable optimization problem for a learning algorithm to solve, we propose here to use a loss  X  deterministic majority vote B Q . convex loss  X  Q ( x ,y ) that can be expanded in a Taylor series around W Q ( x ,y ) = 1 / 2 : that upper bounds the risk of the majority vote B Q , i.e. , Gibbs classifier described by a transformed posterior Q on N  X H  X  , i.e. , where c a def = P  X  k =1 | a k | and where Since W Q ( x ,y ) is the expectation of boolean random variable, Theorem 3.1 holds if we replace ( P,Q ) by ( P, Q ) with R ( G Q ) def = E over, it has been shown [2] that If we define vote. More precisely, we have the following theorem.
 any loss function  X  Q ( x ,y ) defined above, we have Pr where g ( c a ,C 0 ) def = 1  X  c a + C 0 Q that minimizes the upper bound given by Theorem 3.2 is equivalent to finding Q that minimizes where C def = C 0 / (2 c a k ) . exponential loss given by we will also consider, for  X  Q ( x ,y ) , the quadratic loss given by has the minimum value of zero for examples having a margin y P h  X  X  Q ( h ) h ( x ) =  X  . is convex in Q (which has a convex domain). Consequently, f has a single local minimum which coincides with the global minimum. We therefore propose to minimize f coordinate-wise, similarly 4.1 Quasi Uniform Posteriors each h  X  H , the boolean complement of h is also in H . More specifically, we have H = { h say that ( h i ,h n + i ) constitutes a boolean complement pair of classifiers. We consider a uniform prior distribution P over H , i.e. , P i = 1 2 n  X  i  X  X  1 ,..., 2 n } . Q { 1 ,...,n } whereas Q i  X  [0 , 1 /n ]  X  i  X  X  1 ,..., 2 n } .
 For any quasi uniform Q , the output B Q ( x ) of the majority vote on any example x is given by Consequently, the set of majority votes B Q over quasi uniform posteriors is isomorphic to the set ourselves to quasi uniform posteriors.
 according to: for some optimally chosen value of  X  .
 Let Q  X  and w  X  be, respectively, the new posterior and the new weight vector obtained with such a change. The above-mentioned convex properties of objective function f imply that we only need to look for the value of  X   X  satisfying  X  1 /n, Q 1  X  0 , Q n +1  X  1 /n . Otherwise, we accept the change described by Equation 1 with  X  =  X   X  .
 For objective function f we simply have where For the quadratic loss, we find where Consequently, for the quadratic loss case, the optimal value  X   X  satisfies For the exponential loss, we find where Consequently, for the exponential loss case, the optimal value  X   X  satisfies following update rules.
 Since, initially we have the dot product present in Equations 6 and 9 never needs to be computed. Consequently, updating D w takes  X ( m ) time.
 The computation of the summations over the m examples in Equation 7 or 10 takes  X ( m ) time. Once these summations are computed, solving Equation 7 or 10 takes  X (1) time. Consequently, it takes  X ( m ) time to perform one basic iteration of the learning algorithm which consist of (1) Equation 11 or 12. The complete algorithm, called f minimization , is described by the pseudo code of Algorithm 1. Algorithm 1 : f minimization 1: Initialization: Let Q i = Q n + i = 1 2 n , w i = 0 ,  X  i  X  X  1 ,...,n } . 2: repeat 3: Choose at random h  X  X  and call it h 1 ( h n +1 is then the boolean complement of h 1 ). 4: Find  X   X  that solves Equation 7 or 10. 6: If [ w 1 +  X   X   X  1 n ] then Q 1  X  1 n ; Q n +1  X  0 ; w 1  X  1 n . 8: Update D w according to Equation 11 or 12. 9: until Convergence The repeat-until loop in Algorithm 1 was implemented as follows. We first mix at random the n w i and D w . We repeat this sequence until no weight change exceeds a specified small number . 4.2 From KL( Q k P ) to ` p Regularization we use we obtain, for the uniform prior P i = 1 / (2 n ) , With this approximation, the objective function to minimize becomes of w and  X  ( x ) = ( x  X  1) 2 for the quadratic loss and e  X  x for the exponential loss. P To sum up, the KL -regularized objective function f immediately follows from PAC-Bayes theory and ` p regularization is obtained from a relaxation of f . Consequently, PAC-Bayes theory favors good generalization. 2 For the sake of comparison, all learning algorithms of this subsection are producing a weighted although Algorithm 1 needs a set H of 2 n classifiers containing n boolean complement pairs, it outputs a majority vote with n real-valued weights defined on { h 1 ,...,h n } . The results obtained for all tested algorithms are summarized in Table 1. We have compared Al-gorithm 1 with quadratic loss (KL-QL) and exponential loss (KL-EL) to AdaBoost [7] (AdB) and ridge regression (RR).
 Except for MNIST, all data sets were taken from the UCI repository. Each data set was randomly testing set T .
 objective function f and to the  X  parameter present in the loss functions. These hyperparameters were determined from the training set only by performing the 10-fold cross validation (CV) method. The hyperparameters that gave the smallest 10-fold CV error were then used to train the Algorithms on the whole training set and the resulting classifiers were then run on the testing set. turn, gives a risk bound (computed from Theorem 3.2) having very large values (results not shown here).
 cases, Algorithm 1 turned out to be statistically significantly better. Our numerical results indicate that Algorithm 1 generally outperforms AdaBoost and ridge regres-cross-validation method for selecting hyperparameters and provide better guarantees on the gener-generic optimization problem for learning.
 Acknowledgments Work supported by NSERC discovery grants 122405 (M.M.) and 262067 (F.L.).
 [2] Pascal Germain, Alexandre Lacasse, Franc  X ois Laviolette, and Mario Marchand. A pac-bayes [3] Pascal Germain, Alexandre Lacasse, Franc  X ois Laviolette, and Mario Marchand. PAC-Bayesian [5] John Langford and John Shawe-Taylor. PAC-Bayes &amp; margins. In S. Thrun S. Becker and [7] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new [8] Matthias Seeger. PAC-Bayesian generalization bounds for gaussian processes. Journal of Ma-[9] Manfred K. Warmuth, Karen A. Glocer, and S.V.N. Vishwanathan. Entropy regularized LP-
