 Arash Afkanpour afkanpou@ualberta.ca Andr  X as Gy  X orgy gyorgy@ualberta.ca Csaba Szepesv  X ari szepesva@ualberta.ca Michael Bowling bowling@cs.ualberta.ca We look into the computational challenge of finding a good predictor in a multiple kernel learning (MKL) set-ting where the number of kernels is very large. In partic-ular, we are interested in cases where the base kernels come from a space with combinatorial structure and thus their number d could be exponentially large. Just like some previous works (e.g. Rakotomamonjy et al., 2008; Xu et al., 2008; Nath et al., 2009) we start with the approach that views the MKL problem as a nested, large scale convex optimization problem, where the first layer optimizes the weights of the kernels to be com-bined. More specifically, as the objective we minimize the group p -norm penalized empirical risk. However, as opposed to these works whose underlying iterative methods have a complexity of  X ( d ) for just any one iter-ation, following (Nesterov, 2010; 2012; Shalev-Shwartz and Tewari, 2011; Richt  X arik and Tak  X a X c, 2011) we use a randomized coordinate descent method, which was effectively used in these works to decrease the per iter-ation complexity to O (1). The role of randomization in our method is to use it to build an unbiased estimate of the gradient at the most recent iteration. The issue then is how the variance (and so the number of itera-tions required) scales with d . As opposed to the above mentioned works, in this paper we propose to make the distribution over the updated coordinate dependent on the history. We will argue that sampling from a distri-bution that is proportional to the magnitude of the gra-dient vector is desirable to keep the variance (actually, second moment) low and in fact we will show that there are interesting cases of MKL (in particular, the case of combining kernels coming from a polynomial family of kernels) when efficient sampling (i.e., sampling at a cost of O (log d )) is feasible from this distribution. Then, the variance is controlled by the a priori weights put on the kernels, making it potentially independent of d . Under these favorable conditions (and in particular, for the polynomial kernel set with some specific prior weights), the complexity of the method as a function of d becomes logarithmic, which makes our MKL algorithm feasible even for large scale problems. This is to be contrasted to the approach of Nesterov (2010; 2012) where a fixed distribution is used and where the a priori bounds on the method X  X  convergence rate, and, hence, its compu-tational cost to achieve a prescribed precision, will de-pend linearly on d (note that we are comparing upper bounds here, so the actual complexity could be smaller). Our algorithm is based on the mirror descent (or mir-ror descent) algorithm (similar to the work of Richt  X arik and Tak  X a X c (2011) who uses uniform distributions). It is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005; 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for  X  X mall X  d . The algo-rithm of Bach (2008), though practically very efficient, suffers from the same deficiency. A very interesting proposal by Cortes et al. (2009) considers learning to combine a large number of kernels and comes with guarantees, though their algorithm restricts the family of kernels in a specific way.
 The rest of the paper is organized as follows. The problem is defined formally in Section 2. Our new algorithm is presented and analyzed in Section 3, while its specialized version for learning polynomial kernels is given in Section 4. Finally, experiments are provided in Section 5. In this section we give the formal definition of our problem. Let I denote a finite index set, indexing the predictors (features) to be combined, and define the set of predictors considered over the input space X as F = f w : X  X  R : f w ( x ) = P i  X  X   X  w i , X  i ( x )  X  , x  X  X  . Here W i is a Hilbert space over the reals,  X  i : X  X  X  i is a feature-map,  X  x,y  X  is the inner product over the Hilbert space that x,y belong to and w = ( w i ) i  X  X   X  W . =  X  i  X  X  W i (as an example, W i may just be a finite dimensional Euclidean space). The prob-lem we consider is to solve the optimization problem minimize L n ( f w )+Pen( f w ) subject to w  X  X  , (1) where Pen( f w ) is a penalty that will be specified later, of predictor f w , defined in terms of the convex losses ` : R  X  R (1  X  t  X  n ) and inputs x t  X  X  (1  X  t  X  n ). The solution w  X  of the above penalized empirical risk minimization problem is known to have favorable gener-alization properties under various conditions, see, e.g., Hastie et al. (2009). In supervised learning problems ` ( y ) = ` ( y t ,y ) for some loss function ` : R  X  R  X  R such as the squared-loss, ` ( y t ,y ) = 1 2 ( y  X  y t the hinge-loss, ` t ( y t ,y ) = max(1  X  yy t , 0), where in the former case y t  X  R , while in the latter case y t  X  { X  1 , +1 } . We note in passing that for the sake of simplicity, we shall sometimes abuse notation and write L n ( w ) for L n ( f w ) and even drop the index n when the sample-size is unimportant.
 As mentioned above, in this paper we consider the spe-cial case in (1) when the penalty is a so-called group p -norm penalty with 1  X  p  X  2, a case considered ear-lier, e.g., by Kloft et al. (2011). Thus our goal is to solve where the scaling factors  X  i &gt; 0 ,i  X  I , are assumed to be given. We introduce the notation u = ( u i )  X  R I to denote the column vector obtained from the values u i . The rationale of using the squared weighted p -norm is that for 1  X  p &lt; 2 it is expected to encourage sparsity at the group level which should allow one to handle cases when I is very large (and the case p = 2 comes for free from the same analysis). The actual form, however, is also chosen for reasons of computational convenience. In fact, the reason to use the 2-norm of the weights is to allow the algorithm to work even with infinite-dimensional feature vectors (and thus weights) by resorting to the kernel trick. To see how this works, just notice that the penalty in (2) can also be written as
X where for  X   X  1,  X   X  = {  X   X  [0 , 1] |I| : k  X  k  X   X  1 } is the positive quadrant of the |I| -dimensional `  X  -ball (see, e.g., Micchelli and Pontil, 2005, Lemma 26). Hence, defining for any w  X  X  , X   X  [0 , 1] |I| , an equivalent form of (2) is where  X  = p/ (2  X  p )  X  [1 ,  X  ) and we define 0 / 0 = 0 and u/ 0 =  X  for u &gt; 0, which implies that w i = 0 if  X  i = 0. That this minimization problem is indeed equivalent to our original task (2) for the chosen value of  X  follows from the fact that J ( w, X  ) is jointly convex in ( w, X  ). Let  X  i : X  X X  X  R be the reproducing kernel under-lying  X  i :  X  i ( x,x 0 ) =  X   X  i ( x ) , X  i ( x 0 )  X  ( x,x H i = H  X  i the corresponding reproducing kernel Hilbert space (RKHS). Then, for any given fixed value of  X  , the above problem becomes an instance of a standard penalized learning problem in the RKHS H  X  underlying the kernel  X   X  = P i  X  X   X  i  X   X  2 i  X  i . In particular, by the theorem on page 353 in Aronszajn (1950), the problem of finding w  X  X  for fixed  X  can be seen to be equivalent to minimize f  X  X  be equivalent to minimize f  X  X  Thus, we see that the method can be thought of as finding the weights of a kernel  X   X  and a predictor minimizing the H  X  -norm penalized empirical risk. This shows that our problem is an instance of multiple kernel learning (for an exhaustive survey of MKL, see, e.g., G  X onen and Alpayd X n, 2011 and the references therein). When I is small, or moderate in size, the joint-convexity of J allows one to use off-the-shelf solvers to find the joint minimum of J . However, when I is large, off-the-shelf solvers might be slow or they may run out of memory. Targeting this situation we propose the following approach: Exploiting again that J ( w, X  ) is jointly convex in ( w, X  ), find the optimal weights by finding the minimizer of or, alternatively, J (  X  ) = J ( w  X  (  X  ) , X  ), where w (  X  ) . = arg min w J ( w, X  ) (here we have slightly abused notation by reusing the symbol J ). Note that J (  X  ) is convex by the joint convexity of J ( w, X  ). Also, note that w  X  (  X  ) exists and is well-defined as the minimizer of J (  X  , X  ) is unique for any  X   X   X   X  (see also Proposition 3.2 below). Again, exploiting the joint convexity of J ( w, X  ), we find that if  X   X  is the minimizer of J (  X  ), then w  X  (  X   X  ) will be an optimal solution to the original problem (2). To optimize J (  X  ) we propose to use stochastic gradient descent with artificially injected randomness to avoid the need to fully evaluate the gradient of J . More precisely, our proposed algorithm is an instance of a randomized version of the mirror descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in each time step only one coordinate of the gradient is sampled. 3.1. A randomized mirror descent algorithm Before giving the algorithm, we need a few definitions. Let d = |I| , A  X  R d be nonempty with a convex interior A  X  . We call the function  X  : A  X  R a Legendre (or barrier) potential if it is strictly convex, its partial derivatives exist and are continuous, and for every sequence { x k }  X  A approaching the boundary of A , lim k  X  X  X  k X   X ( x k ) k =  X  . Here  X  is the gradient oper-ator:  X   X ( x ) = (  X   X  X   X ( x )) &gt; is the gradient of  X . When  X  is applied to a non-smooth convex function J 0 (  X  ) ( J may be such without additional assumptions) then Algorithm 1 Randomized mirror descent algorithm 1: Input: A,K  X  R d , where K is closed and convex 2: Initialization:  X  (0) = arg min  X   X  K  X  A  X (  X  ). 3: for k = 1 , 2 ,... do 4: Obtain  X  g k = GradSampler (  X  ( k  X  1) ) 7: end for  X  J 0 (  X  ) is defined as any subgradient of J 0 at  X  . The corresponding Bregman-divergence D  X  : A  X  A  X   X  R is defined as D  X  (  X , X  0 ) =  X (  X  )  X   X (  X  0 )  X  X  X  X   X (  X  0 The Bregman projection  X   X  ,K : A  X   X  K correspond-ing to the Legendre potential  X  and a closed convex set K  X  R d such that K  X  A 6 =  X  is defined, for all  X   X  A  X  Algorithm 1 shows a randomized version of the stan-dard mirror descent method with an unbiased gradient estimate. By assumption,  X  k &gt; 0 is deterministic. Note that step 1 of the algorithm is well-defined since  X   X  ( k )  X  A  X  by the assumption that k X   X ( x ) k tends to infinity as x approaches the boundary of A . The perfor-mance of Algorithm 1 is bounded in the next theorem. The analysis follows the standard proof technique of analyzing the mirror descent algorithm (see, e.g., Beck and Teboulle, 2003), however, in a slightly more general form than what we have found in the literature. In par-ticular, compared to (Nemirovski et al., 2009; Nesterov, 2010; 2012; Shalev-Shwartz and Tewari, 2011; Richt  X arik and Tak  X a X c, 2011), our analysis allows for the conditional distribution of the noise in the gradient estimate to be history dependent. The proof is omitted due to space limitations and is given in (Afkanpour et al., 2013). Theorem 3.1. Assume that  X  is  X  -strongly convex with respect to some norm k X k (with dual norm k X k  X  ) for some  X  &gt; 0 , that is, for any  X   X  A  X  , X  0  X  A Suppose, furthermore, that Algorithm 1 is run for T time steps. For 0  X  k  X  T  X  1 let F k de-note the  X  -algebra generated by  X  1 ,..., X  k . Assume that, for all 1  X  k  X  T ,  X  g k  X  R d is an unbi-E exists a deterministic constant B  X  0 such that for all 1  X  k  X  T , E k  X  g k k 2  X  F k  X  1  X  B a.s. Finally, Then, if  X  k  X  1 = q 2  X  X  BT for all k  X  1 , it holds that Furthermore, if k  X  g k k 2  X   X  B 0 a.s. for some determinis-any 0 &lt; &lt; 1 , it holds with probability at least 1  X  that J The convergence rate in the above theorem can be improved if stronger assumptions are made on J , for example if J is assumed to be strongly convex, see, for example, (Hazan et al., 2007; Hazan and Kale, 2011). Efficient implementation of Algorithm 1 depends on efficient implementations of steps 1-1, namely, computing an estimate of the gradient, solving the minimization for  X   X  ( k ) , and projecting it into K . The first problem is related to the choice of gradient esti-mate we use, which, in turn, depends on the structure of the feature space, while the last two problems depend on the choice of the Legendre function. In the next subsections we examine how these choices can be made to get a practical variant of the algorithm. 3.2. Application to multiple kernel learning It remains to define the gradient estimates  X  g k in Algo-rithm 1. We start by considering importance sampling based estimates. First, however, let us first verify whether the gradient exist. Along the way, we will also derive some explicit expressions which will help us later. Closed-form expressions for the gradient. Let us first consider how w  X  (  X  ) can be calculated for a fixed value of  X  . As it will turn out, this calculation will be useful not only when the procedure is stopped (to construct the predictor f w  X  (  X  ) but also during the iterations when we will need to calculate the derivative of J with respect to  X  i . The following proposition summarizes how w  X  (  X  ) can be obtained. Note that this type of result is standard (see, e.g., Shawe-Taylor and Cristianini, 2004; Sch  X olkopf and Smola, 2002), thus we include it only for the sake of completeness.
 Proposition 3.2. For 1  X  t  X  n , let ` t : R  X  R denote the convex conjugate of ` t : ` ( v ) = sup  X   X  R { v X   X  ` t (  X  ) } , v  X  R . For i  X  I , K i = (  X  i ( x t ,x s )) 1  X  t,s  X  n be the n  X  n kernel ma-trix underlying  X  i and let K  X  = P i  X  X   X  i  X  2 kernel matrix underlying  X   X  = P i  X  X   X  i  X  2 any fixed  X  , the minimizer w  X  (  X  ) of J (  X  , X  ) satisfies where  X  Based on this proposition, we can compute the predictor f w  X  (  X  ) using the kernels {  X  i } i  X  X  and the dual variables (  X  P Let us now consider the differentiability of J = J (  X  ) and how to compute its derivatives. Under proper conditions with standard calculations (e.g., Rakotoma-monjy et al., 2008) we find that J is differentiable over  X  and its derivative can be written as Importance sampling based estimates. Let d = |I| and let e i , i  X  I denote the i th unit vector of the standard basis of R d , that is, the i th coordinate of e is 1 while the others are 0. Introduce to denote the i th component of the gradient of J in iteration k (that is, g k,i can be computed based on (3)). Let s k  X  1  X  [0 , 1] I be a distribution over I , computed in some way based on the information available up to the end of iteration k  X  1 of the algorithm (formally, s k  X  1 is F k  X  1 -measurable). Define the importance sampling based gradient estimate to be  X  g That is, the gradient estimate is obtained by first sampling an index from s k  X  1 ,  X  and then setting the gradient estimate to be zero at all indices i  X  I except when i = I k in which case its value is set to s k  X  1 ,i &gt; 0 holds whenever g k,i 6 = 0, then it holds that E Let us now derive the conditions under which the sec-ond moment of the gradient estimate stays bounded. sion for the gradient of J shown in (3), we see that sup k  X  1 C k  X  1 &lt;  X  will always hold provided that  X  is continuous since (  X  ( k  X  1) ) k  X  1 is guaranteed to belong to a compact set (the continuity of  X   X  is discussed in Afkanpour et al., 2013).
 Define the probability distribution q k  X  1 ,  X  as follows: q k  X  g Algorithm 2 Projected stochastic gradient algorithm. 2: for k = 1 , 2 ,... do 3: Sample a gradient estimate  X  g k of g (  X  ( k  X  1 5: end for Therefore, it also holds that E k  X  g k k 2  X  F k  X  1 C This shows that sup k  X  1 E k  X  g k k 2  X  F k  X  1 &lt;  X  will gradient estimate becomes  X  g k,i = C k  X  1 I { I in this case we see that in order to be able to calculate  X  g k,i , we need to be able to calculate C k  X  1 efficiently. Choosing the potential  X  . The efficient sampling of the gradient is not the only practical issue, since the choice of the Legendre function and the convex set K may also cause some complications. For example, if  X ( x ) = P i  X  X  x i (ln x i  X  1), then the resulting algorithm is exponential weighting, and one needs to store and update |I| weights, which is clearly infeasible if |I| is very large (or infinite). On the other hand, if  X ( x ) = 1 2 k x k 2 2 and we project to K =  X  2 , the positive quadrant of the ` 2 -ball (with A = [0 ,  X  ) I ), we obtain a stochastic projected gradient method, shown in Algo-rithm 2. This is in fact the algorithm that we use in the experiments. Note that in (2) this corresponds to using p = 4 / 3. The reason we made this choice is because in this case projection is a simple scaling operation. Had we chosen K =  X  1 , the ` 2 -projection would very often cancel many of the nonzero components, resulting in an overall slow progress. Based on the above calculations and Theorem 3.1 we obtain the following performance bound for our algorithm.
 Corollary 3.3. Assume that  X   X  (  X  ) is continuous on  X  2 . Then there exists a C &gt; 0 such that k  X   X  X  J (  X  ) k If Algorithm 2 is run for T steps with  X  k  X  1 =  X  = 1 /  X  Note that to implement Algorithm 2 efficiently, one has to be able to sample from s k  X  1 ,  X  and compute the impor-tance sampling ratio g k,i /s k,i efficiently for any k and i . In this section we show how our method can be applied in the context of multiple kernel learning. We provide an example when the kernels in I are tensor products of a set of base kernels (this we shall call learning polynomial kernels). The importance of this example follows from the observation of G  X onen and Alpayd X n (2011) that the non-linear kernel learning methods of Cortes et al. (2009), which can be viewed as a restricted form of learning polynomial kernels, are far the best MKL methods in practice and can significantly outperform state-of-the-art SVM with a single kernel or with the uniform combination of kernels.
 Assume that we are given a set of base kernels {  X  1 ,..., X  r } . In this section we consider the set K D of product kernels of degree at most D : Choose I = { ( r 1 ,...,r d ) : 0  X  d  X  D, 1  X  r i  X  r } and the multi-index r 1: d = ( r 1 ,...,r d )  X  I defines the kernel  X   X  tations of each other define the same kernel. On the language of statistical modeling,  X  r tions of order d between the features underlying the base kernels  X  1 ,..., X  r . Also note that |I| =  X ( r D ), that is, the cardinality of I grows exponentially fast in D . We assume that  X  r of interactions in  X  r write  X  d in the rest of this section to emphasize this. 1 Our proposed algorithm to sample from q k  X  1 ,  X  is shown in Algorithm 3. The algorithm is written to return a multi-index ( z 1 ,...,z d ) that is drawn from q k  X  1 ,  X  key idea underlying the algorithm is to exploit that algorithm is shown in Section 4.1. In the description of the algorithm denotes the matrix entrywise product (a.k.a. Schur, or Hadamard product) and A s denotes A ... A | {z } s than that of the ordinary matrix product (by definition, all the entries of A 0 are 1).
 Let us now discuss the complexity of Algorithm 3. For this, first note that computing all the Hadamard products S d 0 ,d 0 = 0 ,...,D requires O ( Dn 2 ) com-putations. Multiplication with M k  X  1 can be done in O ( n 2 ) steps. Finally, note that each iteration of the for loop takes O ( rn 2 ) steps, which results in the overall worst-case complexity of O ( rn 2 D ) if  X   X  (  X  k  X  1 is readily available. The computational complexity of determining  X   X  (  X  k  X  1 ) depends on the exact form of ` t , and can be done efficiently in many situations: if, for example, ` t is the squared loss, then  X   X  can be Algorithm 3 Polynomial kernel sampling. 1: Input:  X   X  R n , the solution to the dual problem; 3:  X  ( d 0 )  X   X   X  2 d 0 D M,S d 0 E , d 0  X  X  0 ,...,D } 4: Sample d from  X  (  X  ) / P D d 0 =0  X  ( d 0 ) 5: for i = 1 to d do 7: Sample z i from  X  (  X  ) 8: M  X  M K z i 9: end for 10: return ( z 1 ,...,z d ) computed in O ( n 3 ) time. An obvious improvement to the approach described here, however, would be to subsample the empirical loss L n , which can bring further computational improvements. However, the exploration of this is left for future work.
 Finally, note that despite the exponential cardinality of |I| , due to the strong algebraic structure of the space of kernels, C k  X  1 can be calculated efficiently. In fact, it is not hard to see that with the notation of the algorithm, C  X  X ast enough X , C k  X  1 can be bounded independently of the cardinality of I . 4.1. Correctness of the sampling procedure In this section we prove the correctness of Algorithm 3. As said earlier, we assume that  X  r the order of interactions in  X  r tion, we will write  X  d to emphasize this. Let us now con-sider how one can sample from q k  X  1 ,  X  . The implementa-tion relies on the fact that ( P r j =1  X  j ) d = P r Remember that we denoted the kernel matrix un-derlying some kernel k by K k , and recall that K k is an n  X  n matrix. For brevity, in the rest of this section for  X  =  X  r K to (3) and the rotation property of trace, we have g q draw the order of interactions, 0  X   X  d  X  D . Given  X  d = d , we restrict the draw of the random multi-index R 1: d to the set { r 1: d  X  I} . A multi-index will be sampled in a  X  d -step process: in each step we will randomly choose an index from the indices of base kernels according to the following distributions. Let S = K 1 + ... + K r , let and, with a slight abuse of notation, for any 1  X  i  X  d define = where we used the sequence notation (namely, s 1: p denotes the sequence ( s 1 ,...,s p )). We have, by the linearity of trace and the definition of S that Thus, by telescoping, as desired. An optimized implementation of drawing these random variables is shown as Algorithm 3. The algorithm is written to return the multi-index R 1: d . In this section we apply our method to the problem of multiple kernel learning in regression with the squared loss: L ( w ) = 1 2 P n t =1 ( f w ( x t )  X  y t ( x t ,y t )  X  R r  X  R are the input-output pairs in the data. In these experiments our aim is to learn polynomial kernels (cf. Section 4).
 We compare our method against several kernel learning algorithms from the literature on synthetic and real data. In all experiments we report mean squared error over test sets. A constant feature is added to act as offset, and the inputs and output are normalized to have zero mean and unit variance. Each experiment is performed with 10 runs in which we randomly choose training, validation, and test sets. The results are averaged over these runs. 5.1. Convergence speed In this experiment we examine the speed of convergence of our method and compare it against one of the fastest standard multiple kernel learning algorithms, that is, the p -norm multiple kernel learning algorithm of Kloft et al. (2011) with p = 2, 2 and the uniform coordinate descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov, 2010; 2012; Shalev-Shwartz and Tewari, 2011; Richt  X arik and Tak  X a X c, 2011). We aim to learn polynomial kernels of up to de-gree 3 with all algorithms. Our method uses Algorithm 3 for sampling with D = 3. The set of provided base kernels is the linear kernels built from input variables, input variable. For the other two algorithms the kernel set consists of product kernels from monomial terms for D  X  X  0 , 1 , 2 , 3 } built from r base kernels, where r is the number of input variables. The number of distinct product kernels is r + D D . In this experiment for all al-gorithms we use ridge regression with its regularization parameter set to 10  X  5 . Experiments with other values of the regularization parameter achieved similar results. We compare these methods in four datasets from the UCI machine learning repository (Frank and Asuncion, 2010) and the Delve datasets 3 . We run all algorithms for a fixed amount of time and measure the value of the objective function (1), that is, the sum of the empirical loss and the regularization term. Figure 1 shows the performance of these algorithms. In this figure Stoch represents our algorithms, Kloft represents the algorithm of Kloft et al. (2011), and UCD represents the uniform coordinate descent algorithm.
 The results show that our method consistently out-performs the other algorithms in convergence speed. Note that our stochastic method updates one kernel coefficient per iteration, while Kloft updates r + D D kernel coefficients per iteration. The difference between the two methods is analogous to the difference between stochastic gradient vs. full gradient algorithms. While UCD also updates one kernel coefficient per iteration its naive method of selecting coordinates results in a slower overall convergence compared to our algorithm. In the next section we compare our algorithm against several representative methods from the MKL literature. 5.2. Synthetic data In this experiment we examine the effect of the size of the kernel space on prediction accuracy and training time of MKL algorithms on synthetic data. Experiments with real datasets also give promising results. Due to the lack of space these results are presented in (Afkanpour et al., 2013). We generated data for a regression problem. Let r denote the number of dimensions of the input space. The inputs are chosen uniformly at random from [  X  1 , 1] r . The output of each instance is the uniform combination of 10 monomial terms of degree 3 or less. These terms are chosen uniformly at random among all possible terms. The outputs are noise free. We generated data for r  X  { 5 , 10 , 20 ,..., 100 } , with 500 training and 1000 test points. The regularization parameter of the ridge regression algorithm was tuned from { 10  X  8 ,..., 10 using a separate validation set with 1000 data points. We compare our method ( Stoch ) against the algorithm of Kloft et al. (2011) ( Kloft ), the nonlinear kernel learning method of Cortes et al. (2009) ( Cortes ), and the hierarchical kernel learning algorithm of Bach (2008) ( Bach ). 4 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3.
 The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the num-bers inside brackets indicate the total number of dis-tinct product kernels for each value of r . This is the number of kernels fed to the Kloft algorithm. Since this method deals with a large number of kernels, it was possible to precompute and keep the kernels in memory (8GB) for r  X  25. Therefore, we ran this algorithm for r  X  25. For r &gt; 25, we could use on-the-fly implemen-tation of this algorithm, however that further increases the training time. Note that the computational cost of this method depends linearly on the number of kernels, which in this experiment, is cubic in the number of in-put variables since D = 3. While the standard MKL algorithms, such as Kloft , cannot handle such large kernel spaces, in terms of time and space complexity, the other three algorithms can efficiently learn kernel com-binations. However their predictive accuracies are quite different. Note that the performance of the method of Cortes et al. (2009) starts to degrade as r increases. This is due to the restricted family of kernels that this method considers. The method of Bach (2008), which is well-suited to learn sparse combination of product ker-nels, performs better than Cortes et al. (2009) for higher input dimensions. Among all methods, our method per-forms best in predictive accuracy while its computa-tional cost is close to that of the other two competitors. We introduced a new method for learning a predictor by combining exponentially many linear predictors using a randomized mirror descent algorithm. We derived finite-time performance bounds that show that the method efficiently optimizes our proposed criterion. Our proposed method is a variant of a randomized stochastic coordinate descent algorithm, where the main trick is the careful construction of an unbiased randomized estimate of the gradient vector that keeps the variance of the method under control, and can be computed efficiently when the base kernels have a certain special combinatorial structure. The efficiency of our method was demonstrated for the practically important problem of learning polynomial kernels on a variety of synthetic and real datasets comparing to a representative set of algorithms from the literature. For this case, our method is able to compute an optimal so-lution in polynomial time as a function of the logarithm of the number of base kernels. To our knowledge, ours is the first method for learning kernel combinations that achieve such an exponential reduction in complexity while satisfying strong performance guarantees, thus opening up the way to apply it to extremely large number of kernels. Furthermore, we believe that our method is applicable beyond the case studied in detail in our paper. For example, the method seems extendible to the case when infinitely many kernels are combined, such as the case of learning a combination of Gaussian kernels. However, the investigation of this important problem remains subject to future work. This work was supported by Alberta Innovates Technology Futures and NSERC.

