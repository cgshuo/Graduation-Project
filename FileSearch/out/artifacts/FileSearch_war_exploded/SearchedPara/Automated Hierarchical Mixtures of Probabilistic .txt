 Ting Su tsu@ece.neu.edu Jennifer G. Dy jdy@ece.neu.edu Dimension reduction is an important problem. First, usually not all the features are useful for producing a desired clustering. Some features are redundant, some may be irrelevant. Second, dimension reduction can save storage and time when dealing with data sets with huge number of features. Dimension reduction for unsupervised clustering is difficult, because we do not have class labels.
 There are two main approaches to reduce dimensions: feature selection and feature transformation. Fea-ture selection algorithms select the hopefully best fea-ture subset that discovers  X  X atural X  groupings from data (Dy &amp; Brodley, 2000) (M. H. Law, 2002) (Mi-tra et al., 2002). Feature transformation methods transform data from the original d -dimensional feature space to a new q -dimensional ( q&lt;d ) feature space. Principal component analysis (PCA) (Jolliffe, 1986) is one of the most popular methods for feature transfor-mation. However, PCA is limited since it only defines a single global projection of the data. For complex data, different clusters may need different projection directions; hence, a mixture of local PCA models is desirable. In fact, a hierarchical mixtures of models is even better, because it can provide a coarse-to-fine structure and give more flexibility. In this paper, we introduce an automated algorithm that generates a hi-erarchical mixtures of models.
 Mixture of Probabilistic PCA (PPCA) models (Tip-ping &amp; Bishop, 1999a) is an extension to the Proba-bilistic PCA model (Tipping &amp; Bishop, 1999b), which can determine the principal sub-space of the data through maximum-likelihood estimation of the param-eters in a Gaussian latent variable model. More-over, it can be extended to a hierarchical represen-tation as shown in (Bishop &amp; Tipping, 1998). One can also look at other mixture models, such as mix-ture of factor analyzers (FA) (Ghahramani &amp; Hinton, 1997), and mixture of independent component analyz-ers(ICA) (Roberts &amp; Penny, 2001). Mixtures of PPCA is merely a special case of mixtures of FA, where PPCA assumes an isotropic covariance matrix for noise while FA assumes a diagonal covariance matrix for noise. Previous work on hierarchical mixtures of models in-clude building an interactive environment for visual-ization (Bishop &amp; Tipping, 1998) (Tino &amp; Nabney, 2002). While the human-driven nature of their algo-rithms is good for visualization, it may make the algo-rithms expensive and slow, and can produce varying results depending on the user. Moreover, the num-ber of retained principal dimensions in a visualization algorithm is limited to either one, two or three. In this paper, we introduce an automated hierarchical algorithm. As such, our algorithm allows the flexibility on deciding the number of retained dimensions. Differ-ent clusters can have potentially different dimensional-ities, thus varying the dimensionality for each cluster may lead to better performance. An automated ap-proach requires automated methods for initialization, determining the number of principal component di-mensions, and determining when to split/merge clus-ters. We address each of these in Section 3.
 In section 2, we review the PPCA model, mixture of PPCA, and hierarchical mixtures of PPCA. In section 3, we describe our automated hierarchical algorithm. We, then, report our experimental results in section 4. Finally, we present our conclusions and directions for future work in Section 5. This section reviews the theory of PPCA, mixture of PPCA, and hierarchical mixtures of PPCA. 2.1. PPCA Conventional PCA seeks a q -dimensional ( q&lt;d ) lin-ear projection that best represents the data in a least-squares sense. Consider a data set D of observed d -dimensional vector D = { t n } , where n  X  1 , ..., N .we first compute the sample covariance matrix: Then, the q principal axes u j are given by the q dom-inant eigenvectors (i.e. those with the q largest eigen-values). The projected value of data t n is given by x be shown that PCA finds the linear projection that maximizes the variance in the projected space. Conventional PCA does not define a probability model. PCA can be reformulated as a maximum like-lihood solution to a latent variable model (Tipping &amp; Bishop, 1999b). Let x be a q -dimensional latent variable. The observed variable t is then defined as a linear transformation of x with additional noise : t = Wx +  X  + , Here W is a d  X  q linear transfor-mation matrix,  X  is a d -dimension vector that allows t to have a non-zero mean. Both the latent variable x and noise are assumed to be isotropic Gaussian: p ( x )  X  X  (0 ,I q )and p ( )  X  X  (0 , X  2 I d ). Then, the distribution of t is also Gaussian : One can compute the maximum-likelihood estimator for the parameters  X  , W and  X  2 from data set D . The log-likelihood under this model is: L = n =1 log[ p ( t n )].
 The maximum-likelihood estimates for these parame-ters are: where  X  q +1 ,..., X  d are the smallest eigenvalues of sam-ple covariance matrix S ,the q columns in the d  X  q orthogonal matrix U q are the q dominant eigenvectors of S , diagonal matrix  X  q contains the corresponding q largest eigenvalues, and R is an arbitrary q  X  q orthog-onal matrix. We set R = I in our experiments. 2.2. Mixture of PPCA Clustering using finite mixture models is a well-known method(McLachlan &amp; Peel, 2000). In this model, one assumes that data is generated from a mixture of component density functions, in which each compo-nent density function p ( t | i ) represents a cluster. Now that PPCA is defined as a probabilistic model, we can model each mixture component with a single PPCA distribution (Tipping &amp; Bishop, 1999a). The proba-bility density of the observed variable, t , is expressed by: where p ( t |  X  i , X  2 i , W i ) denotes a PPCA density func-tion for component i , k 0 is the number of compo-nents, and  X  i is the mixing proportion of the mixture component i (subject to the constraints:  X  i  X  0and i =1  X  i = 1). The log-likelihood of the observed data is then given by: It is difficult to optimize (6), so we use the Expectation-Maximization (EM) (Dempster et al., 1977) algorithm to find a local maximum of (6). If we hypothesize a set of indicator variables z ni (also known as  X  X issing data X ) specifying which model is responsible for generating each data point t n , then the log-likelihood of the complete-data is given by: We apply the EM algorithm to compute the maximum-likelihood estimation for parameters  X  i ,  X  i ,  X  2 i and W as follows: E-step: The posterior probability of data t n belonging to com-ponent i , R ni is given by: where E [  X  ] is the expected value operator. M-step: To update  X  2 i and W i , we first compute the weighted sample covariance matrices, given by: then apply (3) and (4). If d is large, one should use an alternative EM approach proposed in (Tipping &amp; Bishop, 1999b) to update  X  2 i and W i for speed up. 2.3. Hierarchical mixtures of PPCA One can extend the mixture of PPCA models to a hierarchical mixture models (Bishop &amp; Tipping, 1998). Consider an example of extending a two-level mixture models to a three-level mixture models. Suppose each PPCA component i in the second level is extended to agroup g i of PPCA components in the third level, the probability density can be expressed as: where p ( t |  X  i,j , X  2 i,j , W i,j ) denotes a single PPCA com-ponent,  X  j | i denotes the mixing proportion (subject to the constraints  X  j | i  X  0and j  X  j | i = 1). If  X  X iss-ing data X  at the second level z ni are known, then the corresponding log-likelihood is given by: To maximize the expectation of (13) with respect to z ni , we use an EM algorithm again. This has a simi-lar form as the EM algorithm discussed in 2.2, except that in the E-step, the posterior probability of data t n belonging to component ( i, j ) is given by: and We can recursively apply the above approach to gener-ate a hierarchy of mixtures of PPCA with any number of levels. To build our hierarchy, we employ a divisive approach. This way, we start from a coarse data representation and then get a more and more fine data representation until a stopping criterion is reached. One can also build a hierarchy with an agglomerative algorithm. However, to make an agglomerative approach prac-tical in terms of time efficiency, it requires O ( K 2 max memory space, where K max is the number of clusters in the initial partitions (Fraley &amp; Raftery, 1998), and in the worst case, agglomerative methods may start at K max = N , where N is the number of data points. Another advantage of a divisive approach is that it can be parallelized easily.
 There are several issues that need to be addressed for an automated divisive hierarchical algorithm. 3.1. Building the hierarchy and determining We build the hierarchy as follows: in each level, we perform an order identification test on each cluster to see if it should be split into two children clusters in the next level. In particular, we apply a hierarchical mixtures of PPCA model described in section 2.3, with the number of children clusters in g i as two. We then compare the parent model with its two children based on a criterion evaluation measure. If the two children outperforms the parent model, we replace the parent with its two off-springs in the next level, otherwise, we copy the parent down unchanged into the next level, and we will not consider to split that cluster in any lower level. We repeat this process until either all the single PPCA clusters in the last level cannot be split into two or the number of clusters reaches K max . This leads us to the question of  X  X hich criterion should we use for splitting? X  This question is similar to decid-ing the number of clusters in a mixture model, which is a difficult task that has not been completely re-solved. There are many ways for accessing mixture order, readers can refer to (McLachlan &amp; Peel, 2000) chapter 6 for a review. One cannot simply use the maximum likelihood criterion to decide the number of clusters, because this will lead to a final level where each data point is a cluster (which is a case of over-fitting). Some form of regularization (such as penalty methods) is needed. Here, we utilize the integrated classification likelihood (ICL) criterion proposed by (Biernacki et al., 2000): L c is the complete-data log-likelihood as defined in equation (7), m is the number of free parameters to be estimated, and N is the number of data points. Note that m varies with k . (Biernacki et al., 2000) es-timated z ni in equation (7) by one if argmax j R nj = i and zero otherwise (a MAP (maximum a posterior) es-timate), whereas (McLachlan &amp; Peel, 2000) estimated z ni by its conditional expectation R ni .Wechoseto replace z ni by R ni because this represents a  X  X oft X  1 clustering solution to the mixture problem. Whereas a MAP estimate of z ni represents a  X  X ard X  cluster-ing solution. The criterion we used is called ICL-BIC in (McLachlan &amp; Peel, 2000) and they showed that it outperforms other criteria such as Bayesian infor-mation criterion (BIC) (Schwarz, 1978), and Akaike X  X  information criterion (Akaike, 1974).
 ICL chooses the number of clusters to maximize Equa-tion (16). ICL basically looks like the more famil-iar BIC , but instead of penalizing the observed-data log-likelihood L , we penalize the expectation of the complete-data log-likelihood L c . Recall that the ex-pectation of L c is equal to L + N n =1 k 0 i =1 R ni log R (
L minus the estimated entropy of the fuzzy classifica-tion matrix (( R ni )). ICL, thus, measures the observed-data log-likelihood minus the degree of cluster overlap minus the penalty for the complexity of the model pa-rameters.
 We choose ICL out of the several other ways to pe-nalize log-likelihood for two main reasons. Firstly, note that equation (6) contains the log of a sum-mation. One cannot compute the observed-data log-likelihood L for each component separately, as would be required when determining the number of clusters in the lower levels of the hierarchy. Therefore, in a hierarchical model, it is difficult to apply some popu-lar criteria based on the observed-data log-likelihood, such as BIC, unless one chooses to forego the flexibility of  X  X oft X  assignments in a mixture model by assigning  X  X ard X  clustering in the lower levels. Secondly, previ-ous experiments reported in (McLachlan &amp; Peel, 2000) chapter 6 showed that ICL outperforms other criteria. (Biernacki et al., 2000) claimed that ICL appears to be more robust than BIC due to the violation of some of the mixture model assumptions, since BIC will tend to overestimate the number of clusters regardless of the cluster overlap if the true model is not in the family of the assumed models. ICL, on the other hand, as explained above penalizes overlaps between clusters. For clarity, let us summarize the expressions we used for computing ICL for a single PPCA component and for its two offsprings: ICL for component i is given by: ICL for the two children of component i takes the form: where R ni,j is defined in (14), similarly  X  i,j =  X  i  X  j m 1 and m 2 denote the number of free parameters for component i and its two children components respec-tively. Our approach for cluster order identification is similar to the one used by X-means, a hierarchical K-means algorithm (Pelleg &amp; Moore, 2000). The two main differences between our approach and their ap-proach are: we use ICL rather than the BIC criterion, and we apply a mixture of component models that as-sumes soft membership.
 Variants and Extensions Instead of splitting the clusters into one or two at every level, one can extend the approach described above by considering splitting to k = one, two, three, or more clusters at every level and apply ICL to pick the best k . One can also build the hierarchy by merging clusters: start at the lowest level by performing a flat clustering and ICL to de-termine the number of clusters, and then merge the clusters which lead to the largest likelihood, until all the clusters are merged into one component. 3.2. Determining the number of principal Recently some researchers presented methods for choosing the intrinsic dimension of the data set for mixtures of PPCA models (Bishop, 1999)(Bishop, 1998)(Minka, 2000). Those methods are for density estimation. The number of dimensions picked by those methods may be far more than one would use for fea-ture reduction, and hence may not be appropriate for dimension reduction (Minka, 2000). We introduce a simple and fast method analogous to the dimension reduction technique for conventional PCA. For com-ponent i , the dimension q i to be retained in the corre-sponding sub-components in the next level is the small-est q i which allows the mean-square-error to be smaller than a threshold, say 10%. In our experiment, we let q &gt; 1, then q i is given by: where S i is defined in equation (11),  X  j are the eigen-values of S i . In this way, each cluster component can have potentially different dimensionality, thus provid-ing more flexibility. 3.3. Initialization It is well known that the EM algorithm may converge to local minima. Different initial parameter values can lead to quite different estimates. Here, we apply 20 random starts to initialize the parameters. The de-tails for initializing the parameters on the top level are as follows: Given the number of clusters k ,we select random k data points as the initial centroids, then assign all the data points t n to the nearest seed, then we compute the corresponding posterior proba-bility R ni by putting R ni =1if t n belong to centroid i and R ni = 0 otherwise. Start from initial values of R ni , where n =1 ,...,N, and i =1 ,...,k . Then, we apply the M-step to update  X  i ,  X  i ,  X  i and W i for each i . We perform the EM algorithm until convergence initialized with the above process 20 times, and pick the one set of parameters which provided the largest likelihood. The initialization of the model parameters for the lower levels is similar to that presented above, except that we compute the initial posterior proba-bility R ni,j corresponding to sub-component j by set-ting R ni,j = R ni if t n belongs to centroid ( i, j )and R ni,j = 0 otherwise. 3.4. Avoiding the spurious clusters A common problem with the expectation maximiza-tion of mixture models is dealing with  X  X purious clus-ters X . A fitted component with very small mixing pro-portion  X  i or singular covariance matrix may lead to a relatively large local maximum, but indeed this com-ponent is not useful in practice and should be consid-ered as a  X  X purious cluster X .
 Since we are working with a dimension reduction algo-rithm, we need to deal with the singularity of the sam-ple covariance matrices in the projected space. The determinant of a matrix is equal to the product of its eigenvalues. In our algorithm, if the q th largest eigen-value of a cluster X  X  sample covariance matrix is less than a small number (default 1 e  X  5), then we con-sider that cluster as a spurious cluster. In the following experiments, we 1) investigate whether mixtures of PPCA results in better clustering com-pared to conventional PCA plus EM of multivariate Gaussian mixtures, 2) examine the flexibility of a hi-erarchical model, and 3) validate the appropriateness of the clusters discovered at each level. 4.1. Data sets We test our algorithm on three synthetic data sets (toy, oil and chart) and six real data sets. Table 1 summa-rizes the data set characteristics. Toy and oil data sets are obtained from (Tipping, 1998) and were used in (Bishop &amp; Tipping, 1998) for data visualization. All the other data sets are either from the UCI Machine Learning Repository (Merz et al., 1996) or (Bay, 1999). 4.2. Evaluation Criteria Since we know the true class labels, we can measure the clustering quality by using measures such as normal-ized mutual information (Strehl &amp; Ghosh, 2002) and Fowlkes-Mallows index (Fowlkes &amp; Mallows, 1983). We report results for both criteria because no evidence show one is better than the other. These two criteria measure the agreement between the labeled classes and the estimated clusters. Both criteria are in the range [0 , 1] and bigger value means better agreement. How-ever, note that a labeled class is not necessarily uni-modal, and if our algorithm finds this multi-modality, the value of both criteria will become smaller.  X  Normalized Mutual Information(NMI)  X  Fowlkes-Mallows index (FM index) 4.3. Experimental Results Since conventional PCA is one of most popular meth-ods for feature reduction, we compare our algorithm with PCA and EM hierarchical clustering. To remove the effect of other factors, we use the same criterion (ICL) to decide the number of clusters, the same ini-tialization method, and also equation (19) to deter-mine the retained dimensions in our implementation for PCA+EM algorithm. Note that the number of re-tained dimensions for PCA+EM is fixed for all clusters in all levels. Table 2 shows the results for the lowest level of PCA + hierarchical EM and for automated hi-erarchical mixtures of PPCA. We present the results with their NMI, FM, and the number of clusters in the lowest level, K .(Weset K max to be twice the number of labeled classes.) We include the number of dimen-sions, q , retained by conventional PCA in PCA+EM. We do not provide a q column for Auto-PPCA because we can not simply provide a single q value for Auto-PPCA, since each cluster in the hierarchy has different number of retained dimensions.
 Interestingly, the mixture of PPCA approach is not al-ways better than PCA+EM. Mixtures of PPCA per-formed better than PCA+EM in terms of NMI and FM on most small datasets (toy, oil, and glass), which are well modeled by mixtures of Gaussians. PPCA with fewer clusters has a comparable performance with EM + PCA on the large data sets (optical digits, satel-lite image, segment), except for the letter data, where PPCA performed better. Finally, mixtures of PPCA are worse than PCA+EM on the chart and wine data. Upon closer inspection on the chart data, we observe that the first level of mixture of PPCA grouped the data with cluster one isolating class two, and grouped the rest into cluster two. ICL cannot split cluster two into more sub-clusters because it is highly overlapping. On the wine data, we noticed that if on the first level mixture of PPCA projects the data to three dimen-sions and eight dimensions on the second level, hier-archical PPCA results in much better clusterings (as shown on the last row of the Table 2). This indicates that better results can be obtained if we have a better method for determining the number of dimensions in each level than just retaining 90% of the information. We will investigate this further in future work. A hierarchical mixture of PPCA provides a flexible representation of the data. We obtain different dimen-sions q for each level and different local projections for each cluster. In fact, due to this allowed flexibility (three on the first level and eight dimensions on the second level) for the wine data, we are able to attain better clustering results than PCA+EM (when q =2 and q = 8 as shown on the last two rows of Table 2). We examine the clustering results for each level and check whether ICL splits up clusters, which does not look like uni-modal Gaussians. Figures 1 and 2 provide a hierarchical visualization of the results for the toy data set and the satellite image data. Due to space lim-itations, we display the results for one synthetic data and one real data. These are representative of the re-sults for the other data sets. Note that the dimensions for each cluster obtained by our automated approach maybe more than two, and that each data point be-longs to all clusters with some probability. To plot the results in two-dimensions, we plot each data point us-ing two leading posterior mean projection components. To prevent confusion, we only plot each data point to the cluster to which it has the largest posterior proba-bility. To show the labeled classes in the scatterplots, we display each class with a different symbol and color. The toy data set is generated from a mixture of three Gaussians. Two of the clusters are highly overlapped, while the third is well separated from the first two. As shown in Figure 1, our algorithm was able to find the hierarchy and the number of clusters almost perfectly. It discovered two clusters in level one. Then, based on ICL, it splits one of the clusters into two sub-clusters in level two.
 The satellite data consists of four digital images of the same scene in 36 different spectral bands. For visu-alization purpose here, we applied our algorithm on 20% of random subsamples of the original 4435 data points (Note that in Table 2, we ran our algorithm on all the 4435 data points). Our algorithm gener-ated four levels, we can only show the first three levels (again due to space). Again, our approach discovered reasonable clusters. Note that the evaluation criteria, both NMI and FM, decreased from level two to level three since our approach detected the multi-modality of some labeled classes (Recall that a class can ac-tually be multi-modal). Previous study on this image data also indicated the multi-modality of some labeled classes (Bishop &amp; Tipping, 1998).
 These figures demonstrate that our automated ap-proach was able to find reasonable clusters and that ICL broke multi-modal clusters appropriately. We have developed an automated hierarchical mixture of principal component analyzers algorithm. To ini-tialize, we apply twenty random restarts, to determine the number of retained dimensions, we keep 90% of the information, and to determine the number of clus-ters, we utilize the integrated classification likelihood criterion (ICL). Our experimental results show that we were able to obtain reasonable clusters, and that ICL was able to split multi-modal clusters if there is enough separation between those clusters.
 Without dimension reduction, EM of a mixture of Gaussians on the original data fails on high-dimensional data.
 The additional flexibility offered by an automated hi-erarchical mixture of PPCA, which allows the algo-rithm to represent each cluster with a different dimen-sion and a different local PCA projection, enabled it to find better solutions than a global PCA followed by EM. This was well demonstrated by the results on wine data where hierarchical PPCA utilized three di-mensions on the first level and eight dimensions on the second level.
 Hierarchical clustering provides a flexible representa-tion showing relationships among clusters in various perceptual levels. It results in a coarse to fine local component model with varying projections and with different number of dimensions for each cluster. The automated hierarchical mixtures of PPCA presented here can easily be extended to mixtures of factor ana-lyzers.
 Another direction for future work is to investigate how the number of dimensions chosen affects the choice for the number of clusters and vice versa.
 This research is supported by NSF Grant No. IIS  X  0347532.

