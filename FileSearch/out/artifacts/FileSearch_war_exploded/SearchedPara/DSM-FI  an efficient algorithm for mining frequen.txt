 Hua-Fu Li  X  Man-Kwan Shan  X  Suh-Yin Lee Abstract Online mining of data streams is an important data mining problem with broad applications. However, it is also a difficult problem since the streaming data possess some inherent characteristics. In this paper, we propose a new single-pass algorithm, called DSM-FI (data stream mining for frequent itemsets), for online incremental mining of frequent itemsets over a continuous stream of online transactions. According to the proposed algo-rithm, each transaction of the stream is projected into a set of sub-transactions, and these sub-transactions are inserted into a new in-memory summary data structure, called SFI-forest (summary frequent itemset forest) for maintaining the set of all frequent itemsets embedded in the transaction data stream generated so far. Finally, the set of all frequent itemsets is determined from the current SFI-forest. Theoretical analysis and experimental studies show that the proposed DSM-FI algorithm uses stable memory, makes only one pass over an on-line transactional data stream, and outperforms the existing algorithms of one-pass mining of frequent itemsets.
 Keywords Data mining  X  Data streams  X  Frequent itemsets  X  Single-pass algorithm  X  Landmark window 1 Introduction In recent years, database and knowledge discovery communities have focused on a new data model, in which data arrive in the form of continuous streams . It is often referred to as data streams or streaming data . Data streams possess some computational characteristics, such as unknown or unbounded length, possibly very fast arrival rate, inability to backtrack over previously arrived data elements (only one sequential pass over the data is permitted), and a lack of system control over the order in which that data arrive [ 3 , 10 ]. Many applications generate data streams in real time, such as sensor data generated from sensor networks, transaction flows in retail chains, Web record and click-streams in Web applications, per-formance measurement in network monitoring and traffic management, and call records in telecommunications.

Online mining of data streams differs from traditional mining of static datasets in the following aspects [ 10 ]. First, each data element in streaming data should be examined at most once. Second, the memory usage for mining data streams should be bounded even though new data elements are continuously generated from the stream. Third, each data element in the stream should be processed as fast as possible. Fourth, the analytical results generated by the online mining algorithms should be instantly available when requested by the users. Finally, the frequency errors of outputs generated by the online algorithms should be as small as possible. The online processing model of data streams is shown in Fig. 1 .
As described earlier, the continuous nature of streaming data makes it essential to use the online algorithms which require only one scan over the data streams for knowledge discovery. The unbounded characteristic makes it impossible to store all the data into the main memory or even in secondary storage. This motivates the design of summary data structure with small footprints that can support both one-time and continuous queries of streaming data. In other words, one-pass algorithms for mining data streams have to sacrifice the exactness of its analytical results by allowing some tolerable counting errors. Hence, traditional multiple-pass techniques studied for mining static datasets are not feasible to mine patterns over streaming data. 1.1 Related work Frequent itemsets mining is one of the most important research issues in data mining. The pro-blem of frequent itemsets mining of static datasets (not streaming data ) was first introduced called items . Let database DB be a set of transactions, where each transaction T contains a set of items, such that T  X  .The size of database DB is the total number of transac-tions in DB and is denoted by | DB | . A set of items is referred to as an itemset . An itemset X with l items is denoted by X = ( x 1 x 2 ,..., x l ) , such that X  X  .The support of an itemset X is the number of transactions in DB containing the itemset X as a subset, and denoted by sup ( X ). An itemset X is frequent if sup ( X )  X  minsup  X | DB | ,where minsup is a user-specified minimum support threshold in the range of [0, 1]. Consequently, given a database DB and a user-defined minimum support threshold minsup , the problem of mining frequent itemsets in static datasets is to find the set of all itemsets whose support is no less than minsup  X | DB | . In this paper, we will focus on the problem of mining frequent itemsets in data streams .

Many previous studies contributed to the efficient mining of frequent itemsets in streaming data [ 4  X  9 , 12  X  17 ]. According to the stream processing model [ 18 ], the research of mining frequent itemsets in data streams can be divided into three categories: landmark windows model, knowledge discovery is performed based on the values between a specific times-tamp called landmark and the present. In the sliding windows model, knowledge discovery is performed over a fixed number of recently generated data elements which is the target of data mining. In the damped windows model, recent sliding windows are more impor-tant than previous ones. In other words, older transactions contribute less toward itemset frequencies.

In [ 14 ], Manku and Motwani developed two single-pass algorithms, sticky-sampling and lossy counting, to mine frequent items over landmark windows. Moreover, Manku and Motwani proposed a lossy-counting based three module method, called BTS (Buffer-Trie-SetGen), for mining the set of frequent itemsets (FI) from streaming data. Chang and Lee [ 5 ] proposed a BTS-based algorithm for mining frequent itemsets in sliding windows model. Moreover, Chang and Lee [ 4 ] also developed another algorithm, called estDec, for mining frequent itemsets in streaming data in which each transaction has a weight decrea-sing with age. Teng et al. [ 15 ] proposed a regression-based algorithm, called FTP-DS, to find frequent itemsets across multiple data streams in a sliding window. Lin et al. [ 13 ] proposed an incremental mining algorithm to find the set of frequent itemsets in a time-sensitive sliding window. Giannella et al. [ 8 ] proposed a frequent pattern tree (abbreviated as FP-tree [ 11 ]) based algorithm, called FP-stream, to mine frequent itemsets at multiple time granularities by a novel titled-time windows technique. Yu et al. [ 17 ] discussed the issues of false negative or false positive in mining frequent itemsets from high speed transactional data streams. WongandFu[ 16 ] proposed an efficient algorithm to mine top-k frequent itemset in a stream sliding window without a user-defined minimum support constraint. Jin and Agrawal [ 12 ] proposed an algorithm, called StreamMining, for in-core frequent itemset mining over data streams. StreamMining is based on the BTS algorithm. Chi et al. [ 7 ] proposed an algorithm, called MOMENT, which might be the first to find closed frequent itemsets from data streams. A lattice-based summary data structure, called CET, is used in the MOMENT algorithm to maintain the information of closed frequent itemsets. 1.2 Our contributions Because the focus of the paper is on frequent itemsets mining over data streams with a landmark window, we mainly address it by comparison with the algorithms BTS [ 14 ]and StreamMining [ 12 ].
In the BTS algorithm, two estimated parameters: minimum support threshold s ,and maxi-mum support error threshold  X  ,areused,where0 &lt; X   X  s &lt; 1. The incoming data stream is conceptually divided into buckets of width w = 1 / X  transactions each, and the current length of the stream is denoted by N transactions. The BTS algorithm is composed of three steps. In the first step, BTS repeatedly reads a batch of buckets into main memory. In the second step, it decomposes each transaction within the current bucket into a set of itemsets, and stores these itemsets into a summary data structure D which contains a set of entries of the form ( e , e . freq , e . ), where e is an itemset, e. freq is an approximate freq uency of the itemset e ,and e . is the maximum possible error in e . freq . For each itemset e extracted from the incoming transaction T , BTS performs two operations to maintain the summary data structure D . First, it counts the occurrences of e in the current batch, and updates the value e . freq if the itemset e already exists in the structure D . Second, BTS creates a new entry ( e , e . freq , e . )in D , if the itemset e does not occur in D , but its estimated frequency e.freq in the batch is greater than or equal to | batch | X   X  , where the value of maximal possible error e . is set to | batch | X   X  ,and | batch | denotes the total number of transactions in the current batch. To bound the space requirement of D , BTS algorithm deletes the updated entry e if e . freq + e .  X | batch | X   X  . Finally, BTS outputs those entries e i in D ,where e . freq  X  ( s  X   X )  X  N , when a user requests a list of itemsets with the minimum support threshold s and the support error threshold  X  .
 StreamMining algorithm [ 12 ] is an in-core frequent itemset mining algorithm based on the BTS algorithm. StreamMining uses a new approach (derived from the problem of finding a majority element) to reduce the memory requirements for determining the frequent itemsets. Then, StreamMining uses such a reduced set of frequent 2-itemsets and the a priori property to reduce the number of i -itemsets, for i &gt; 2, and establishes a bound on false positives.
The motivation of the study is to develop a method that utilizes some space-effective summary data structures (such as FP-tree [ 11 ] developed for frequent itemsets mining of a static dataset) to reduce the cost in mining frequent itemsets over data streams. In this paper, an efficient single-pass algorithm, referred to as Data Stream Mining for Frequent Itemsets (abbreviated as DSM-FI), is proposed to improve the efficiency of frequent itemset mining in data streams. A new summary data structure called summary frequent itemset forest (abbreviated as SFI-forest) is developed for online incremental maintenance of the essential information about the set of all frequent itemsets of data streams generated so far. The proposed algorithm has three important features: a single pass of streaming data for counting the support of itemsets; an extended prefix tree-based, compact pattern representation of summary data structure; and an effective and efficient search and determination mechanism of frequent itemsets. Moreover, the frequency error guarantees provided by DSM-FI algorithm is the same as that of BTS algorithm. The error guarantees are stated as follows. First, all itemsets whose true support exceeds s  X  N are output. Second, no itemsets whose true support is less than ( s  X   X )  X  N is output. Finally, estimated supports of itemsets are less than the true support by at most  X   X  N [ 12 ]. The comprehensive experiments show that our algorithm is efficient on both sparse and dense datasets. Furthermore, DSM-FI algorithm outperforms the algorithms BTS and StreamMining, by one order of magnitude for discovering the set of all frequent itemsets over the entire history of the data streams. 1.3 Roadmap The remainder of the paper is organized as follows. Section 2 defines the problem of single-pass mining frequent itemsets in landmark windows over data streams. The proposed DSM-FI algorithm is described in Sect. 3 . The extended prefix tree-based summary data structure SFI-forest is introduced to maintain the essential information about the set of all frequent itemsets of the stream generated so far. Theoretical analysis and experiments are presented in Sect. 4 . Section. 5 remarks on future work, and concludes the work. 2 Problem definition Based on the estimation mechanism of the BTS algorithm, we propose a new, single-pass algorithm to improve the efficiency of mining frequent itemsets over the entire history of data streams when a user-specified minimum support threshold s  X  (0, 1), and a maximum support error threshold  X   X  ( 0 , s ) are given.

Let ={ i 1 , i 2 ,..., i m } be a set of literals, called items .An itemset is a nonempty set of items. A l -itemset, denoted by ( x 1 x 2 ,..., x l ), is an itemset with l items. A transaction tid ,( x 1 x 2 ,..., x q )&gt; ,where x i  X  ,  X  i = 1 , 2 ,..., q .A basic window W consists of k transactions. The basic windows are labeled with window identifier wid , starting from 1. Definition 1 A data stream ,DS =[ W 1 , W 2 ,..., W N ) , is an infinite sequence of basic win-dows, where N is the window identifier of the  X  latest  X  basic window. The current length of DS , written as DS.CL, is k  X  N , i.e., | W 1 |+| W 2 |+ X  X  X +| W N | . The windows arrive in some order (implicitly by arrival time or explicitly by timestamp), and may be seen only once.
Online mining of frequent itemsets in a landmark window of data streams is to mine the set of all frequent itemsets from the transactions between a specified window identifier, called landmark , and the current window identifier N . Note that the value of landmark is set to 1 in this paper.

To ensure the completeness of frequent itemsets for data streams, it is necessary to store not only the information related to frequent itemsets, but also that related to infrequent ones. If the information about the currently infrequent itemsets were not stored, such information would be lost. If these itemsets become frequent later on, it would be impossible to figure out their correct support and their relationship with other itemsets [ 9 ]. The data stream mining algorithms have to sacrifice the exactness of the analytical results by allowing some tolerable support errors since it is unrealistic to store all the streaming data into the limited main memory. Hence, we define two types of support (or occurrence frequency ) of an itemset, and divide the itemsets embedded in the stream into three categories: frequent itemsets , semi-frequent itemsets ,and infrequent itemsets .
 Definition 2 The true support of an itemset X , denoted by X . tsup , is the number of tran-sactions in the data stream containing the itemset X as a subset. The estimated support of an itemset X , denoted by X.esup , is the estimated true support of X stored in the summary data structure, where 0 &lt; X . esup  X  X . tsup .
 Definition 3 The current length ( CL ) of data stream with respect to an itemset X stored in the where W j is the first basic window stored in the current summary data structure containing the itemset X .
 Definition 4 An itemset X is frequent if X . tsup  X  s  X  X . CL . An itemset X is semi-frequent if s  X  X . CL &gt; X . tsup  X   X   X  X . CL . An itemset X is infrequent if  X   X  X . CL &gt; X . tsup . Definition 5 A frequent itemset is maximal if it is not a subset of any other frequent itemsets generated so far.
Therefore, given a continuous data stream DS =[ W 1 , W 2 ,..., W N ) , a user-defined minimum support threshold s in the range of [0, 1], and a user-specified maximum sup-port error threshold  X  in the range of [0 , s ], the problem of online mining of frequent itemsets in a landmark window over data streams is to find the set of all frequent itemsets by one scan of the streaming data. 3 The proposed DSM-FI algorithm In this section, we describe the proposed algorithm DSM-FI (data stream mining for frequent itemsets) for online mining of frequent itemsets in a landmark window of a continuous data stream. The DSM-FI algorithm consists of four steps. (a) Step 1: the proposed DSM-FI algorithm reads a basic window of transactions from the (b) Step 2: DSM-FI algorithm constructs and maintains an in-memory prefix-tree based (c) Step 3: DSM-FI algorithm prunes the infrequent information from the current SFI-forest. (d) Step 4: DSM-FI finds the frequent itemsets from the current SFI-forest.
 Steps 1 and 2 are performed in sequence for a new incoming basic window. Step 3 is performed after every basic window has been processed. Finally, step 4 is usually performed periodically or when it is needed. Since the reading of a basic window of transactions from the buffer in main-memory is straightforward, we shall henceforth focus on Steps 2 (discussed in Sect. 3.1 ), 3 (discussed in Sect. 3.2 ), and 4 (discussed in Sect. 3.3 ), and devise new methods for effective construction and maintenance of summary data structure, and efficient determination of frequent itemsets.

Before discussing the proposed DSM-FI algorithm, we use an example to illustrate the construction of the summary data structure SFI-forest.
 Example 1 Assume that the current basic window W j contains six transactions: &lt; acd f &gt;, &lt; constructed by DSM-FI algorithm is described as follows. Note that each node of the form ( id : id . esup : id . wid ) consists of three fields: item-id , estimated support ,and window-id . For example, ( a : 2 : j ) indicates that, from basic window W j to current basic window W
N ( 1  X  j  X  N ) , item a appeared twice. 3.1 Effective construction and maintenance of summary data structure In this section, we describe the method which constructs and maintains the proposed in-memory prefix-tree based summary data structure.
 Definition 6 A summary frequent itemset forest ( SFI-forest ) is an extended prefix-tree based summary data structure defined below. 2. Each entry e in the FI-list consists of four fields: e , e.esup , e.window -id ,and 3. Each node in the e.SFI-tree consists of four fields: e , e .esup , e .window -id , 4. Each e.SFI-tree has a specific OFI-list (a list of Opposite Frequent Items) with k The construction process of SFI-forest is described as follows. First, DSM-FI algorithm reads a transaction T with m items ( m  X  1) from the current window W N for SFI-forest construction. At this time, DSM-FI projects the transaction T into m sub-transactions, and inserts the m sub-transactions into the SFI-forest. The detail of the effective projection is described as follows. A transaction T with m items, i.e., ( e 1 e 2 ,..., e m ), should be projected These m sub-transactions are called itemset-suffix transactions , since the first item of each sub-transaction is an itemset-suffix of the original transaction T . This step, called transaction ( e e i + 1 ,..., e m ),  X  i = 1 , 2 ,..., m .The projecting cost of a transaction T with m items for constructing the SFI-forest is O ( m 2 ) .
 After performing the transaction projection of transaction T , two operations of DSM-FI are executed. First, DSM-FI inserts the items e 1 , e 2 ,..., e m of T into the FI-list ,and then removes T from the current window W N . Second, the items of these itemset-suffix transactions are inserted into the e i . SFI-trees (  X  i , i = 1 , 2 ,..., m ) as branches, and the estimated support of the corresponding e i . OFI-lists are updated. If an itemset share a prefix with an itemset already in the SFI-tree, the new itemset will share a prefix of the branch representing that itemset. In addition, an estimated support counter is associated with each node in the tree. The counter is updated when an itemset-suffix transaction causes the insertion of a new branch. Figure. 5 outlines the algorithms of SFI-forest construction in the DSM-FI algorithm and Fig. 6 shows the subroutines of SFI-forest construction and maintenance.
In the next section, we describe the steps of pruning infrequent information of DSM-FI algorithm. 3.2 Pruning infrequent information from the current SFI-forest According to the a priori property, only the frequent 1-itemsets are used to construct candi-date k -itemsets, where k  X  2. Thus, the set of candidate itemsets containing the infrequent 1-itemsets stored in the summary data structure SFI-forest is pruned. The pruning is usually performed periodically or when it is needed.

Let the maximum support error threshold be  X  in the range of [0 , s ], where s is a user-defined minimum support threshold in the range of [0, 1]. The summary data structure pruning mechanism of DSM-FI algorithm is that the item x and its supersets are deleted from SFI-if its x.esup is less than  X   X  x . CL , it can be regarded as an infrequent item . At this time, three operations are performed in sequence. First, the proposed DSM-FI algorithm deletes the x .OFI-list, x .SFI-tree, and the infrequent entry x from the FI-list. Second, DSM-FI removes the infrequent item x of other OFI-lists by traversing the FI-list. Third, DSM-FI deletes the infrequent item x from other SFI-trees, and reconstructs these SFI-trees.
After pruning all infrequent items from SFI-forest, SFI-forest contains the set of all frequent itemsets and semi-frequent itemsets of the data stream generated so far. Now, we use an example to illustrate the pruning operation of DSM-FI algorithm.
 Example 2 Let the maximum support error threshold  X  be 0.2. Hence, an itemset X is infrequent in Fig. 6 if X .esup &lt; X   X  X .CL. Note that  X   X  X . CL = 0 . 2  X  6 = 1 . 2. After computing the current window W j , the next step of DSM-FI is to prune all the infrequent items from the current SFI-forest. At this time, DSM-FI deletes the b .SFI-tree, b .OFI-list, Then, DSM-FI reconstructs the a .OFI-list and a .SFI-tree, because a .OFI-list and a .SFI-tree contains the infrequent item b . The result is shown in Fig. 7 .

The next step of DSM-FI is to determine the set of all frequent itemsets from SFI-forest constructed so far. The step is performed only when the analytical results of the data stream is requested. Note that the number of candidate 2-itemsets is a performance bottleneck in the a priori-based frequent itemset mining algorithms [ 11 ]. The proposed DSM-FI algorithm can avoid the performance problem, because DSM-FI can generate the set of all frequent 2-itemsets immediately by combining the frequent items in the FI-list with the frequent items in their corresponding OFI-lists. 3.3 Determining frequent itemsets from the summary data structure Once the SFI-forest is constructed and maintained, we can derive the set of all frequent itemsets by traversing the SFI-forest according to the a priori principle. We propose an efficient mechanism called top-down Frequent Itemset Selection ( todoFIS ), as shown in Fig. 8 , for mining frequent itemsets. It is especially useful in mining long frequent itemsets. The method is described as follows.

Assume that there are k frequent items, namely e 1 , e 2 ,..., e k , in the current FI-list, and | e . OFI-list | . Note that the items, namely, o 1 , o 2 ,..., o j , within the e i .OFI-list are denoted entry e i ,  X  i = 1 , 2 ,..., k , in the current FI-list, DSM-FI first generates a maximal candidate the set of all frequent items in the e i .OFI-list. Then, DSM-FI uses the following scheme to count the estimated support of the ( j + 1)-maximal candidate itemset.

First, DSM-FI starts with a specific frequent item e i . o l ( 1  X  l  X  j ) , whose estimated support is smallest, and traverses the paths containing e i . o l via node-links of e i .SFI-tree to set. All subsets of this frequent itemset are also frequent itemsets according to the a priori property. 1 Hence, the complete set of the frequent itemsets stored in the e i .SFI-tree can be generated by enumeration of all the combinations of the subsets of frequent ( j + 1)-itemset, ( e e i . o 1 e i . o 2 ,..., e i . o j ).

On the other hand, if the estimated support of the candidate ( j + 1)-itemset is less than the threshold ( s  X   X )  X  e i . CL , it is not a frequent itemset. Now, we need to use the same mechanism to test all the subsets of the ( j + 1)-itemset until the candidate 3-itemsets. This is because all frequent 2-itemsets can be generated by combining the item e i and the frequent items of the e i .OFI-list. Note that a ( j + 1)-itemset can be decomposed into C ( j + 1 , j ) j -itemsets. We decompose one candidate j -itemset from the ( j + 1)-itemset at a time, and use the same scheme described above to count the estimated support of this candidate j -itemset. Finally, all the maximal frequent itemsets are maintained in a temporal MFI-list, called MFI temp -list, for efficient generation of the set of all frequent itemsets. If such the MFI temp -list is obtained, all the frequent itemsets can be generated efficiently by enumerating the set of all maximal frequent itemsets in the current MFI temp -list without any candidate generation and support counting. Note that if the user request is just to find the set of all maximal frequent itemsets generated so far, DSM-FI outputs all maximal frequent itemsets efficiently by scanning the MFI temp -list.
 Example 3 Let the minimum support threshold s be 0.5. Therefore, an itemset X is frequent in Fig. 7 if X . esup  X  s  X  X .CL. Note that s  X  X . CL = 0 . 5  X  6 = 3 in this example. The online mining steps of frequent itemsets of DSM-FI are described as follows. 1. First of all, DSM-FI starts the frequent itemset mining scheme from the first frequent 2. Next, DSM-FI starts on the second entry c for frequent itemset mining. DSM-FI generates 3. Next, DSM-FI starts on the third entry d and generates a candidate maximal 2-itemset 4. On the fourth entry f , DSM-FI algorithm generates one frequent 1-itemset ( f ) directly, 5. Finally, on the fifth entry e , DSM-FI generates a frequent 2-itemset ( ef ) directly. However,
After processing all the entries in the FI-list, the MFI temp -list generated by DSM-FI algo-( df ), ( d ) } . 3.4 Theoretical analysis In this section, we discuss the maximal estimated support error of frequent itemsets generated by DSM-FI algorithm, the space upper bound of the prefix-tree-based summary data structure, and the differences between the proposed SFI-forest and the FP-tree. 3.4.1 Maximal estimated support error analysis In this section, we discuss the maximal estimated support error of all frequent itemsets generated by DSM-FI algorithm. Let X . wid is the window-id of itemset X stored in the current SFI-forest. Let the window contains k transactions. Let the maximum support error threshold be  X  .Letthecurrent window-id of the incoming stream be wid ( N ). Now, we have the following theorem of maximal estimated support error guarantee of frequent itemsets generated by the proposed algorithm.
 Theorem 1 X . tsup  X  X . esup  X   X   X  ( X .w id  X  1 )  X  k.
 Proof We prove by induction. Base case ( X.wid =1): X.tsup = X.esup . Thus, X . tsup  X  X . esup  X   X   X  ( X .w id  X  1 )  X  k .

Induction step: Consider an itemset of a form ( X , X.esup , X.wid ) that get deleted for some wid ( N )&gt; 1. The itemset is inserted in the SFI-forest when wid ( N + 1) is being processed. The itemset X whose window-id is wid ( N + 1) in the FI-list could possibly have been deleted as late as the time when X . esup  X   X   X  (w id ( N + 1 )  X  X .w id + 1 )  X  k . Therefore, X.tsup of X when that deletion occurred is no more than  X   X  (w id ( N + 1 )  X  X .w id + 1 )  X  k . Furthermore, X.esup is the estimated true support of the itemset X since it is inserted. It follows that X.tsup which is the true support of X in the first window containing X though the current window, is at most X .
Because our algorithm is a false-positive algorithm, the answers produced by DSM-FI will have the following guarantees as same as that of BTS algorithm [ 14 ]: (a) All itemsets whose true frequency exceeds s  X  N are output. There are no false negatives. (b) No itemsets whose true frequency is less than ( s  X   X )  X  N is output. (c) Estimated frequencies are less than the true frequencies by at most  X   X  N . If we want that the error dose not increase linearly with the value of window id, we can modify the line 5 of algorithm todoFIS from  X  if E . esup  X  ( s  X   X )  X  N then  X  X o X  if E . esup  X  s  X  N then  X . After that DSM-FI algorithm becomes a false-negative algorithm.
Note that a false-positive approach returns a set of itemsets that includes all frequent itemsets but also some infrequent itemsets. A false-negative algorithm returns a set of itemsets that does not include any infrequent itemsets but misses some frequent itemsets. 3.4.2 Space upper bound of prefix tree-based summary data structures In this section, we discuss the space upper bound of any single-pass algorithm for constructing a prefix tree-based summary data structure.
 Theorem 2 A prefix tree-based summary data structure has at most 2 m nodes for storing the set of all frequent itemsets of data streams, when m frequent items are given. Proof Let m be the number of frequent items, i.e., 1-itemsets, in the data stream generated so far. Hence, the number of potential frequent itemsets is C( m , 1) regarding one item, C( m , 2) regarding two items, ..., C ( m , i ) regarding i items, ... ,andC( m , m ) regarding m items according to the a priori property. In such a prefix tree-based summary data structure, an itemset is represented by a path and its appearance support is maintained in the last node of the path. Thus, there are C( m , 1) nodes in the first level, C( m , 2) nodes in the second level, ..., C ( m , i ) nodes in the i th level, ... ,andC( m , m ) nodes in the m -th level. There are totally C( m , 1 ) + C ( m , 2 ) + X  X  X + C ( m , i ) + X  X  X + C ( m , m ) nodes in the prefix tree-based summary data structure. Consequently, the space upper bound of a prefix-tree based summary data structure is O ( 2 m ) .
 The construction cost of summary data structure of DSM-FI algorithm is extremely less than that of BTS algorithm although theoretically, their worst case space complexities are same, i.e., O ( 2 m ) ,when m frequent items are given. 3.4.3 Differences between SFI-forest and FP-tree The SFI-forest can be regarded as an enhanced version of FP-tree [ 11 ], but there are some differences between SFI-forest and FP-tree. First, the construction step of both prefix tree-based structures is different. SFI-forest adopts an online incremental maintaining manner, but FP-tree is not. Second, the method used to construct a FP-tree needs two dataset scans, but DSM-FI scans the data only once. Third, SFI-forest uses the OFI-list to overcome the bottleneck (to generate a huge number of candidate 2-itemsets) of the a priori-based frequent itemset mining algorithms, but FP-growth uses a recursive method. 4 Performance evaluation All the experiments are performed on a 1 GHz IBM X24 with 384 MB, and the program is written in Microsoft Visual C++ 6.0. To evaluate the performance of algorithm DSM-FI, we conduct the empirical studies based on the synthetic datasets. In Sect. 4.1 , we report the scalability study of algorithm DSM-FI. In Sect. 4.2 , we compare the memory and execution time requested by DSM-FI with algorithms BTS [ 14 ] and StreamMining [ 12 ]. The parameters of synthetic data generated by IBM synthetic data generator [ 2 ] are described as follows. IBM synthetic dataset T 10 . I 5 . D 1M and T 30 . I 20 . D 1M. The first synthetic dataset T 10 . I 5 has average transaction size T of 10 items and the average size of maximal frequent itemset I is 5-items. It is a sparse dataset. In the second dataset T 30 . I 20, the average transaction size T and average size of maximal frequent itemset I are set to 30 and 20, respectively. It is a dense dataset. Both synthetic datasets have 1,000,000 transactions. Items were drawn from a universe of 10 K distinct items. In the experiments, the synthetic data stream is broken into data windows with size 50 K (i.e., 50,000 transactions) for simulating the continuous characteristic of streaming data. Hence, there are total 20 windows in these experiments. 4.1 Scalability study of DSM-FI In this experiment, we examine the two primary factors, execution time and memory usage , for mining frequent itemsets in a data stream environment, since both should be bounded online as time advances. Therefore, in Fig. 9 a, the execution time grows smoothly as the dataset size increases from 2 , 000 K to 10 , 000 K. The default value of minimum support threshold s is 0.01%. The memory usage in Fig. 9 b for both synthetic datasets is stable as time progresses, indicating the scalability and feasibility of algorithm DSM-FI. Notice that, the synthetic data stream used in Fig. 9 b is broke into 20 basic windows each of 50 K, i.e., 50,000 transactions. 4.2 Comparisons with algorithms BTS and stream mining In this experiment, we examine the execution time and memory usage among DSM-FI, BTS and StreamMining using dataset T 30 . I 20 . D 1M. In Fig. 10 a, we can see that the execution time incurred by DSM-FI is quite steady and is less than that of BTS. The execution time of StreamMining is less than our proposed algorithm in small datasets (2,000 K and 4,000 K), but is greater than DSM-FI in large datasets (8,000 K and 10,000 K). It shows that DSM-FI performs more efficiently than BTS. In Fig. 10 b, the memory usage of DSM-FI is more stable and extremely less than that of BTS and StreamMining. It also shows that DSM-FI algorithm is more suitable for mining frequent itemsets in large-scale data streams. 5 Conclusions and future work In this paper, we proposed a new, single-pass algorithm, called DSM-FI (data stream mining for frequent itemsets), which mines the set of all frequent itemsets in the landmark model of data streams. In the DSM-FI algorithm, a new in-memory summary data structure, called SFI-forest (summary frequent itemset forest), is constructed for storing the frequent and significant itemsets of the streaming data generated so far. An efficient frequent itemset search mechanism, called todoFIS (top-down Frequent Itemset Selection), is developed to find the set of all frequent itemsets from the current SFI-forest. Experiments with synthetic data streams show that DSM-FI is efficient on both sparse and dense datasets, and demonstrates linear scalability to vary long data streams. Moreover, DSM-FI outperforms the well-known, single-pass algorithms BTS and StreamMining for mining frequent itemsets over the entire history of the streaming data.

There are still many interesting research issues related to the extensions of DSM-FI algorithm, such as mining dynamic data streams, mining top-k frequent itemsets in streaming data, and mining constraint-based frequent itemsets in a landmark window over continuous data streams.
 References Author Biographies
