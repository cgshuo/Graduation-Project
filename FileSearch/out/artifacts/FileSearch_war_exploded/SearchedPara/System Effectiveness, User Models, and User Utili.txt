 There is great interest in producing e ff ectiveness measures that model user behavior in order to better model the utility of a system to its users. These measures are often formulated as a sum over the product of a discount function of ranks and a gain function mapping relevance assessments to nu-meric utility values. We develop a conceptual framework for analyzing such e ff ectiveness measures based on classifying members of this broad family of measures into four distinct families, each of which reflects a di ff erent notion of system utility. Within this framework we can hypothesize about the properties that such a measure should have and test those hypotheses against user and system data. Along the way we present a collection of novel results about specific measures and relationships between them.
 Categories and Subject Descriptors :H.3[ Information Storage and Retrieval ]; H.3.4 [ Systems and Software ]: Performance Evaluation General Terms: Experimentation, Measurement Keywords: information retrieval, evaluation, user models
There has always been interest in producing e ff ectiveness measures that model user behavior for systems-based eval-uations with test collections. These measures frequently in-volve summing over the product of a discount function of ranks and a gain function mapping relevance assessments to numeric utility values, i.e.
 The widely-used discounted cumulative gain (DCG) mea-sure, for instance, is typically formulated with an exponen-tial gain function and a log-harmonic discount [10], while rank-biased precision (RBP) uses binary relevance and a ge-ometric discount [12]. Expected reciprocal rank (ERR) maps graded relevance judgments to probabilities and discounts dynamically according to the relevance judgments at previ-ous ranks [6]. Many traditional measures can be interpreted this way as well [21].

The discount function is often viewed as modeling a user that scans down a ranked list, growing less interested with each successive rank; the gain function models the utility the user derives from each document. This interpretation hides a great deal of diversity in the choices one can make in such a measure. Some use a probability density function as the discount; others do not. Some discount dynamically depending on relevance; others use a static discount. De-pending on these choices, the actual user model can vary in subtle ways. In this work we aim to formulate a conceptual framework  X  X  way to organize and describe these choices so as to provide structure for reasoning about properties of these measures in general.

To illustrate this point, consider DCG and RBP: click log analysis has suggested that RBP matches  X  X eality X  (in the sense of being more closely correlated to observed click be-havior) much closer than DCG [6, 20, 21]. Despite that, DCG continues to be far more widely-used in research and development. We may hypothesize why: inertia? familiar-ity? or is it possible that DCG is actually modeling some-thing quite di ff erent than RBP, and what it measures as a result is more useful to developers? We lean towards the latter explanation, and with this work we hope to provide a framework within which to test it.

Though we use RBP and DCG as motivators, our interest is not specifically in them but in model-based measures in general. The primary contribution of this work is increased understanding of e ff ectiveness measures based on explicit user models. We define our framework in Section 2; this framework generates many possible measures (Section 3). Given the measures generated by the framework, we for-mulate specific hypotheses about qualities such a measure should have and test them in data (Section 4). Finally, we explore di ff erences in modeling document utility indepen-dently of rank discounting (Section 5). Along the way we prove a number of novel results about individual measures and relationships between them.
We argue that model-based measures are actually com-posed from three distinct underlying models: 1. a browsing model that describes how a user interacts 2. a model of document utility , describing how a user de-3. a utility accumulation model that describes how a user Decisions about each component model can be made inde-pendently of the others. This establishes a framework for evaluating the outcomes of those decisions.

By far the most well-developed browsing model is that of a user scanning down ranked results one-by-one and stop-ping at some rank k . In this work we focus entirely on that model, comparing di ff erent ways of modeling the stopping rank. Similarly, binary and graded relevance are the two most common ways to model document utility. For simplic-ity, throughout this section and the next we only consider binary relevance. Graded relevance (and other types of rel-evance or utility judgments) can be included in measures in relatively straightforward ways discussed in Section 5 below.
Thus the scope of this section and the next is limited to describing four utility accumulation models that we see in existing measures, as well as looking at di ff erent choices in modeling the stopping rank k in the simple browsing model. While there are other, more accurate browsing models [20, 11, 2], we believe they can be studied within some framework similar to the one we present here.
Consider a model of a user as progressing down a ranked list, looking at each document, and stopping at some rank k . The probabilistic component of the model is a distribu-tion P ( k ). Since a user can only stop at exactly one rank, P i =1 P ( i ) = 1. To model the cost of browsing, we usually constrain P ( k )sothat P (1)  X  P (2)  X  ...  X  P ( n ).
Given a probability distribution, a measure reflecting this model has the form: This expression can be understood as the expected relevance of the document at the stopping rank. This follows from the fact that the events of stopping at each rank k are mutually exclusive, and an expectation is computed as the sum of event probabilities times a numeric value of the event (in this case document relevance).

An exemplar for this model is rank-biased precision (RBP) [12], in which k is assumed to be geometrically distributed with  X  X ersistence X  or  X  X atience X  parameter  X  : 1  X  is a value between 0 and 1 reflecting the patience of users for progressing down the ranked list. It can be thought of as an apriori probability of quitting at any given rank; the probability that a user will stop at rank k =2isthe probability that they do not stop at rank k =1timesthe probability that they do stop at rank k =2: (1  X   X  )  X  .
The model above captures a user stopping at a particular rank, but not that the user looked at the documents above
RBP was not originally defined as the expected relevance of one document, but it is a valid (and we argue more parsi-monious) way to backfit to the expression. We discuss this further in Section 3.2. that rank. No matter where a user chooses to stop, they will see the first document with probability 1. They will only see the second if they do not stop at rank 1, so the probability of viewing document 2 is 1  X  P (1). Continuing this way, it becomes apparent that the viewing probability at rank k is simply the cumulative probability of stopping at all ranks from k to n . Define this cumulative probability F ( k )as: Given a distribution, we can define a measure with the form: The underlying user model becomes more apparent after algebraic manipulation. Arranging summands rel k P ( i )in an n  X  n matrix, we obtain the following: Summing each row first, then summing the results, gives the expression above. Alternatively, summing each column first shows that: where R k = ments retrieved from rank 1 to rank k . This reveals an alternative user model: a user picks a stopping rank k ,and then derives utility from all of the relevant documents from ranks 1 through k . The measure is the expected total utility.
We claim that discounted cumulative gain (DCG) [10] is an exemplar of this model, with F ( k )=1 / log 2 ( k + 1). Define a stopping probability: Then: For large enough n ,1 / log 2 ( n +2)becomesnegligible,and the expression reduces to: This means that DCG can be interpreted as modeling a user that picks a stopping rank k with probability P DCG ( k ), then derives utility from all relevant documents to that rank. While others have treated DCG X  X  discount as a viewing prob-ability rather than a stopping probability [6], as far as we know this overall interpretation of DCG is novel, and clearly di ff erent from the usual interpretation which is closer to M above. Furthermore, the stopping probability distribution is shaped similarly to the geometric distribution used by RBP (Fig. 1), meaning that DCG may encode a more realistic browsing model than previously thought.
Rather than compute expected utility ,as M 1 and M 2 do, we could compute the expected e ff ort a user must put forth to achieve a particular amount of utility.
 Here P ( k ) is conditional on relevance judgments (the  X  X ain times discount X  formulation is recovered from the use of rel in P ( k )). Unlike the previous two models, this is an expec-tation of e ff ort modeled by rank rather than an expectation of relevance. If f ( k )= k , it is the expected stopping rank; if f ( k )=1 /k it is the expected reciprocal stopping rank. The presumption is that f ( k )canbedefinedinawaythat accurately reflects user e ff ort.

The expected reciprocal rank (ERR) [6] measure is exem-plar for this class. Let f ( k )=1 /k and Here  X  rel i is a parameter indicating the probability that a document with relevance rel i would be useful to the user. With binary relevance ( rel i =0or1),andwith  X  0 =0and  X  =  X  , this can be rewritten as: revealing that P ERR is geometric over total accumulated rel-evance, with non-zero probability only at ranks at which rel-evant documents appear. Here  X  can still be understood as a patience parameter, as in RBP, but now the probability of stopping at a nonrelevant document is zero while the prob-ability of stopping at a relevant document is  X  .Thuswe could reasonably re-express this model as: This makes explicit the idea of evaluating the e ff ort the user expends to find R k relevant documents (or in general to achieve a given amount of utility).
The final model combines the utility-or reward-based models M 1 and M 2 and the e ff ort-based model M 3 . Intuitively, this models a user that, after each relevant docu-ment, considers the expected e ff ort of further browsing. As with M 2 , the underlying user model becomes more trans-parent after algebraic manipulation: and if f ( k )=1 /k , Thus the user model is based on average gain per document viewed: a user stops at a rank k and gains R k total utility over k documents for an average of prec @ k each.
Average precision (AP) is the exemplar for this model (note the similarity to Robertson X  X  AP model [13]). If where R is the total number of relevant documents in the corpus, we recover AP from the expression. Like P ERR ( k ), this models a user that will never stop at a nonrelevant doc-ument, but unlike any of our other distributions, it uses a simple uniform distribution over all possible stopping points. AP assumes knowledge of the complete set of relevant doc-uments, but this is not required for measures in this family.
The previous section establishes our framework with four utility accumulation models for one common browsing model. Each accumulation model has a well-known exemplar mea-sure, and those four measures have four di ff erent ways to model the stopping rank for browsing: For compactness we will assume base  X  = 2 for P DCG .For simplicity of exposition we still assume binary relevance, which is where the simplified P ERR comes from. R k is the total number of relevant documents retrieved at ranks 1 through k . We will add two more distributions to those: These distributions are interesting because their cumulative forms are reciprocal ranks X  X ence the names P RR (for re-ciprocal rank) and P RRR (for reciprocal relevant rank) X  X nd because they arise naturally as a mixture of the geometric distributions in both RBP and binary-ERR (Section 3.4).
The six distributions can be characterized by whether they are static , i.e. independent of relevance judgments, or dy-namic , i.e. dependent on a specific ranking (cf. Yilmaz et al. [20]). P RBP , P DCG ,and P RR are static; P ERR , P AP and P RRR are dynamic.

All six distributions and their cumulative forms F ( k ) are illustrated in Figure 1. The three static distributions have dynamic cumulative density functions. similar shape, but P DCG and P RR have much fatter tails than P RBP  X  X hile this is di ffi cult to see by inspection of Figure 1, it is very clear on a log-scale plot. Furthermore, P
DCG has a substantially fatter tail than P RR ,whichbe-comes clear when looking at their cumulative densities. The dynamic distributions all give zero stopping probability to ranks at which nonrelevant documents appear. P ERR and P RRR have a similar shape over relevant documents, but P
RRR has a fatter tail (visible by inspection and very clear in the cumulative density). P AP is uniform over relevant documents, so its cumulative density decreases slowly. For RBP and ERR, we used  X  =0 . 5; these properties hold for other common values.
The choice of a probability distribution produces up to four di ff erent measures, one for each model. We have mixed and matched models and distributions to produce eleven measures in addition to our four exemplars (Table 1). We have attempted to name them in a way that makes clear both the choice of model and distribution.
 RBTR (rank biased total relevance, M 2 with P RBP ): RBAP (rank biased average precision, M 4 with P RBP ): CDG (cumulated discounted gain, M 1 with P DCG ): DAG (discounted average gain, M 4 with P DCG ): EPR (expected precision, M 4 with P ERR ): ARR (average reciprocal rank, M 3 with P AP ): RRG (reciprocal rank gain, M 1 with P RR ): RR (reciprocal rank, M 2 with P RR ): RAP (reciprocal average precision, M 4 with P RR ): RRR (reciprocal relevant rank, M 3 with P RRR ): RRAP (reciprocal relevant average precision, M 4 with P RRR
We do not claim that these measures are altogether new to IR X  X e know that some have been described before in various contexts (particularly our so-called RR, which has frequently been described as  X  X CG with a reciprocal rank discount X ). Furthermore, there are other model-based mea-sures that we do not discuss (e.g. the NCU family [16]). Our interest is less in developing or arguing for any particular measures than in using them to explore hypotheses about model-based measures in general.
Some of the measures require normalization. The M 2 fam-ily in particular is heavily dependent on the total number of judged relevant documents for a topic. In addition, some of the probability distributions may not sum to one if the total number of relevant documents is low or if the rank-ing is truncated. Both of these are confounding e ff ects that normalization helps resolve.

To normalize, we simply divide the value of the measure by the maximum possible value given the judged relevant documents, i.e. the value of the measure on a perfect rank-ing. This ensures that the maximum achievable value is 1 for all topics and all systems. This is of course a well-known normalization procedure, commonly used with DCG to pro-duce nDCG [10]. We will apply it to all measures in the M family, as well as a few other measures that do not naturally fall between 0 and 1 such as ARR.

Normalization is a somewhat thorny topic. In present-ing RBP, Mo ff at and Zobel started with an unnormalized version that would fit in our M 2 family, but explicitly chose not to normalize by the maximum achievable e ff ectiveness X  instead normalizing by the expected number of documents viewed [12] and thereby moving the measure into the M 1 family. There is a tradeo ff between preserving the user model and having a measure that can be averaged across topics, and it is not always clear how to resolve it.
Some measures are calculated to a pre-specified rank cut-o ff rather than over the full ranking of n documents. This is particularly true of DCG. In the case of a rank cut-o ff K n , the math in Section 2.2 does not work X  K must be large enough that 1 / log 2 ( K + 2) is close to zero, and that is certainly not the case for the usual values of K .
There is a simple resolution that fits with the user model, though. First, note that we can express DCG@K in terms of P ( k )and F ( K +1) with some simple algebraic manipulation: DCG @ K = We can then apply the same algebraic trick we used in Sec-tion 2.2 to complete the expression as: Note that R K F DCG ( K +1)= the user model is exactly the same. The di ff erence is that calculation of the measure now assumes the worst case for a user that chooses to stop beyond rank K  X  X hat the user will not find any new relevant documents, and therefore will only derive utility from those at ranks 1 X  K .

This argument generalizes to any M 2 measure.Therefore a rank cut-o ff K solves a problem with non-converging dis-counts by making a worst-case assumption about the e ff ec-tiveness of the system below K .
Here we show how our P RR and P RRR distributions emerge from a mixture of geometric distributions, and that their cu-mulative forms are reciprocal ranks.

The geometric distribution has a parameter  X  that requires the researcher/developer to specify a value apriori . Perhaps being unwilling to make any strong statements about user patience, one could instead use several di ff erent values of  X  and average the results. Taken to the limit, they could obtain P ( k ) by averaging geometric distributions over all possible values of  X  : where p (  X  ) is uniform over the range [0 , 1]. Since it is uni-form, we can disregard it; then integration by parts gives: Thus P RR can be seen as an average of infinitely many geo-metric distributions.
 The cumulative distribution is: F
RR ( k )= As n  X  X  X  , 1 n +1  X  0, so for large enough n this is approxi-mated as reciprocal rank 1 /k . It does not require very large n for the e ff ect to be negligible.

The same argument generalizes the binary-relevance ver-sion of P ERR to P RRR as an average of infinitely many geo-metric distributions over ranks of relevant documents.
We have presented a framework for classifying and gen-erating measures that model system utility to a user. The benefit of a framework is that it poses questions and also provides a guide to answering them. Our goal is not to eval-uate the new measures we propose, but to formulate specific hypotheses about model-based measures in general and an-swer them by appealing to our suite of measures.

Some of the questions the framework raises are: 1. Are utility-based models ( M 1 ,M 2 ) better than e ff ort-2. Are measures based on stopping probabilities ( M 1 ,M 3 3. What properties should P have to produce a good mea- X  X etter X  and  X  X ood X  are of course qualitative words whose meaning depends on the retrieval task being studied, the users of the system, and a host of other factors. We will look at goodness-of-fit to user data and robustness of evaluation, though there are other ways to evaluate these questions.
One possible definition of a  X  X ood X  measure is one that more closely models user behavior. We compared our static stopping rank distributions to aggregated clicks from the 2006 AOL log. Of course, processing click log data is it-self implicitly model-based. Comparing models to click logs should be seen not as comparing models to reality, but as comparing one model to another under all the assumptions that both models require. In this case we cannot know stop-ping ranks; we need to model them from recorded clicks.
We tried two di ff erent models. The first simply maps each recorded click to a stopping rank to estimate an empirical distribution of stopping ranks, with each rank mutually ex-clusive of the others. The implicit model is that every click is a new event for a new user/query pair, even if it is in not actually the last click by that user for that query. The second maps only the last recorded click for a user/query to a stopping rank. This may be more realistic, but it results in throwing out all clicks but the last. Other models are possible (e.g. the  X  X ap X  model of Zhang et al. [21]).
Both empirical distributions are shown in Figure 2 along with our static distributions. Among the distributions we are considering, P RR is clearly the best fit to both models of the data. Previous work has suggested that DCG and reciprocal rank discounts do not adequately model users [20, 21, 6]. This analysis suggests they might model users well in some data, provided they are considered cumulative densities rather than probability densities.
While click log analysis can provide a guide for evaluating a probability distribution P ( k ), it is not clear to us how it could be used to evaluate a model of utility accumulation. How can we use click log data to choose between M 1 (in which a user derives utility only from the document at the stopping rank) and M 2 (in which a user derives utility from all relevant documents from rank 1 to the stopping rank)? How can we use it to choose between M 1 and M 3 (in which a user expends a certain amount of e ff ort to achieve a given total utility)? These decisions must be made on the basis of more than just user data.

Furthermore, even if only a small fraction of users are negatively a ff ected by a poor model fit, the cost of not pro-viding those users with the best possible system may be disproportionately large. Suppose that the probability of a user becoming frustrated and quitting the search engine al-together increases over ranks in a reverse geometric way up to rank K ,sothat This is distinguished from Then: which is completely independent of rank! We may as well use a uniform stopping probability X  X eading us right back to traditional precision and recall.

We do not advocate that model; we only wish to point out the pitfalls in focusing on behavior evidenced in logs. This analysis suggests to us that fatter-tailed distributions are superior even if they do not fit behavior data, because those distributions are better-equipped for the risk of not satisfying users that are in the tail.
The previous section supposes that one purpose of an ef-fectiveness measure is to model users in order to estimate the utility of the system. Another purpose of evaluation best fit to both distributions (though better fits are possible). is to choose among di ff erent retrieval models, features, and system implementations. For that purpose we would like decisions to be roughly the same whether they are based on a few topics versus many, or extensive relevance judg-ments versus shallow pools, or one group of assessors ver-sus another. The extent to which conclusions are di ff erent depending on di ff erences in the data used to compute the measures reflects the robustness and stability of those mea-sures. To investigate these properties we will look at how evaluation measure scores and relative rankings of systems change as the underlying data changes.
Our primary data is the TREC-6 ad hoc data consisting of TREC topics 301 X 350, 72,270 total relevance judgments, and 74 submitted runs over a corpus of about 550,000 doc-uments [18]. This is a small corpus, but its deep judgments make it useful for our study. Furthermore, there is an al-ternate set of relevance judgments from the University of Waterloo for these topics, allowing us to investigate the ef-fect of assessor disagreement [9, 17].

We also used two more recent test collections:
The approach is simple: we select some subset of the data (e.g. a subset of topics or a subset of judgments) and evaluate all systems with all 15 of our measures. We then use Kendall X  X   X  rank correlation to compare the results to the  X  X rue X  rankings using the full TREC data. Kendall X  X   X  ranges from -1 to 1, with greater values indicating greater correlation. In practice, for meta-evaluation studies like this one Kendall X  X   X  is nearly always over 0 . 6, and a value of 0 . 9 would be considered an e ff ectively  X  X erfect X  correlation con-sidering the presence of variance [17].

Since our hypotheses are about model families or distribu-tions rather than individual measures, we average Kendall X  X   X  results for each measure within a family or with a particu-lar property. We use the averages to evaluate the hypothesis.
The single clearest fact from the results is that measures in the M 2 family with fatter tails tend to be more robust. However, this is not true in all cases: when the judgment pool is shallow, models with distributions that put more weight on top-ranked documents tend to be more robust. For tasks with few relevant documents, tail fatness does not appear to matter. Detailed results follow below.
 Varying assessors: We evaluated all 74 TREC-6 systems with all 15 of our measures over two di ff erent sets of rele-vance judgments. Table 2 shows the  X  correlations for every measure between the rankings from the two judgment sets. The final column shows the mean  X  for each stopping dis-tribution; it is clear that the fatter-tail distributions P and P AP are most robust w.r.t. assessor disagreement, while the slimmer-tail distributions P RBP and P ERR are least ro-bust. Dynamic distributions actually appear to be more robust than static distributions on average, which we found surprising since they seem to have a greater dependence on which documents have been judged relevant.

The last row shows the mean  X  for each model family; M 2 is more robust to assessor disagreement than the others. M and M 4 are about equally robust, with M 3 taking the edge if the outlying fat-tailed P AP is removed. M 1 is least robust to assessor disagreement.
 Varying topic sample: We evaluated all 74 TREC-6 sys-tems with all 15 of our measures over increasing topic sample sizes from N =5to45. Foreachlevelof N , we performed 100 trials with a random subset of topics; each trial used the same topic sample to evaluate all 15 measures.

Figure 3 plots summary results for distribution model family (left) and tail type (right). Again we see that fat-ter tails result in more stable results, and M 2 provides more stable results than the other models. There are di ff erences from the assessor disagreement results, however. M 3 ap-pears to be least robust to varying the topic sample despite being more robust to assessor disagreement, while M 1 and M 4 look equally robust. The di ff erence in robustness due to tail fatness is less pronounced, with a maximum di ff erence stability to topic set size. Figure 4: Kendall X  X   X  correlations as pool depth in-creases. Correlation is already very good with a pool depth of just 1; this suggests that these systems are retrieving many common documents. of 0.026 between any two points (though this is statistically significant).
 Varying pool depth: We formed shallower judgment pools from the original TREC-6 qrels by iterating over systems and pooling only documents that appeared above a partic-ular rank cuto ff . For each of these pools we evaluated all 74 systems by all 15 measures and correlated the resulting ranking of systems to the X  X rue X  X anking using all judgments. Figure 4 shows increasing  X  s for each of our models. The M 2 family is actually least robust to missing judgments, while the M 1 family is most robust. This makes sense, as the M 1 family places much more weight on the top-most documents than the others, and the top-ranked documents are the ones that are judged in both datasets. The M 4 fam-ily is more robust to missing judgments than the M 3 fam-ily, possibly because M 4 discounts lower-ranked documents more. Nevertheless, all four are quite robust on average, most likely because five of our six distributions weight the top-ranked documents very highly. Results for tail fatness are not shown, but here too there is a reversal: slim-tail dis-tributions are more robust to shallow pools than fatter-tail ones. Again, this is most likely because the slimmer tails result in much more weight on the top-ranked documents. Varying test collection: For our other two collections, we varied topic sample size and calculated  X  correlations.
For the TREC 2006 Terabyte named page task X  X  high-precision task X  X he distribution does not appear to matter. There is almost no di ff erence between  X  correlations with fat-or slim-tailed distributions. M 2 measures are still more robust than other families, though M 3 is a very close second. This supports our intuition that M 3 is particularly useful for tasks where there are only a few highly-relevant documents.
For the TREC 2009 Web ad hoc task, M 3 measures with slim-or medium-tail distributions are most robust to vary-ing topic sample. Fat-tail distributions are quite poor. M measures are least robust, though not by much. This is most likely due to the challenge of evaluating an ad hoc retrieval task with sparse relevance judgments, but it may suggest that ERR is the best measure we have for web evaluation.
To this point our discussion has focused on stopping prob-ability distributions within a common browsing model and models of how users accumulate utility over documents. We simplified the idea of document utility itself to simple binary relevance, but we can investigate alternative models for that independently of stopping probabilities.
Some documents are more useful than others. A com-mon way to model this is with judgment grades, such as the ternary scale nonrelevant, relevant, highly relevant and the quinary scale bad, fair, good, excel lent, perfect .Again function maps grades to numeric values.

Though we have focused on binary judgments, graded judgments fit easily within our framework. DCG and ERR are, of course, explicitly designed with graded judgments in mind. AP can be adapted to graded judgments using a user-modeling distribution mapping grades to probabilities of relevance [14]. At any point where we use rel k or R k binary judgments, we can substitute the mapping function gain ( rel k )orthecumulativegain CG k = respectively. In ERR, we can write P ( k )as: where the product is over unique grades,  X  g is a patience parameter for grade g ,and G k is the total number of docu-ments with grade g up to rank k . This is a slightly di ff erent formulation that originally presented by Chapelle et al. [6], but we feel it more clearly shows ERR as modeling cumu-lated gain up to rank k .
Another way to model the idea that some documents are more useful is with preference judgments of the form  X  A is preferred to B for query q  X . When transitive, such judg-ments result in a total ordering of documents by utility [15]. Previous work has shown that preferences can be made eas-ier and faster than graded judgments [5]. Kendall X  X   X  rank correlation is based on preferences, and it can be extended to other measures for IR e ff ectiveness [4].

Models M 2 and M 4 can be seen as instances of a more general family of preference-based measures. Consider the following stochastic process applied to a system ranking: sample a rank k with probability P ( k ). Then sample one or more documents ranked above k .Foreachofthosedoc-uments that was preferred by assessors to the document at k , increment a count of total concordant pairs.

Suppose we just have binary relevance and a preference relation stating that A is preferred to B if and only if A is judged relevant. If in the second sampling stage we sample only one document uniformly at random, the expectation of the process is exactly M 4 with f ( k )=1 /k .Ifweuseall documents above k ,theexpectationisexactly M 2 .

For natural preference relations, uniform P ( k ) results in the process having expectation proportional to Kendall X  X   X  .Thissuggeststhatboth M 2 and M 4 can be viewed as weighted versions of  X  . This was already known for AP, our exemplar M 4 measure [19]; the fact that DCG can be viewed in this way is novel.
In the novelty and diversity retrieval setting, document utility is a function of its relevance to di ff erent possible user intents as well as its redundancy with other documents in a ranking. The so-called  X  X ntent aware X  (IA) family of mea-sures uses a distribution of intents P ( i | q ) for a given query q to compute a weighted average of a measure like AP or DCG computed with document judgments for each intent [1]. Any measure that fits in our framework can be turned into an in-tent aware variant by computing such a weighted average.
Redundancy can be penalized when computing total util-ity. Up to this point we have computed total utility as the number of retrieved relevant documents R k .Wecouldin-stead define the utility of the top k retrieved documents as: where R j is the number of relevant documents up to rank j and F ( R j ) is a redundancy discount taking the form of a cumulative probability density based on P ( R j ), the proba-bility that the last relevant document a user would look at is the R j th. The  X  -nDCG measure [8] uses F ( R j )=  X  R it is based on the same geometric penalty that ERR uses.
A full intent-aware, redundancy-penalizing novelty/diversity measure in the M 2 family could then be given as: where I q is the set of intents for query q and U ik is defined as the utility to intent i (using relevance judgments distin-guished by intent). Measures in the M 3 and M 4 families follow straightforwardly. Note that M 1 cannot truly model redundancy penalization in a natural way, since it does not model accumulated utility.

This suggests two directions for diversity evaluation:
On one level this work is a collection of novel observations about common evaluation measures and user models. At that level, these observations emerge primarily from alge-braic manipulation.
At a deeper level, these observations all emerged from a conceptual framework in which we describe a measure in terms of its browsing model, specific attributes of its browsing model, and utility accumulation model. This alone shows the value of the framework: it led us to discover things that were not previously known about measures. We went further and used the framework to formulate specific hy-potheses about models and measures. Some of our hypothe-ses turned out to be true in most cases: that fatter-tail distri-butions would be more robust; that measures in the M 2 fam-ily would be more robust. Others turned out to be wrong: that measures in the M 4 family would be more robust; that M 3 would not be significantly better than M 1 ;thatdynamic distributions would be less robust than static distributions. These results open the door to additional hypotheses and discoveries about evaluation based on user models.
On a personal note, we will confess to beginning this study with an unease about DCG X  X e felt that the discount was perhaps too flat to model real users, that its user model was ad hoc, and that it was unclear what it really measured. We come out of it with a newfound appreciation of DCG. The stopping probability may still be ad hoc (it is not a formal probability distribution), but it is much more clear to us that the discount is not too flat and that it actually measures something very useful. Furthermore, it is a highly robust measure; within our framework, the fact that it uses M 2 and a fat-tail distribution predicts that it would be. To answer the question we posed in Section 1, perhaps this is why DCG has continued to find such wide use: not due to inertia or familiarity, but because it really is a useful user-centered measure of system e ff ectiveness.
 This work was supported in part by the National Science Foundation (NSF) under grant number IIS-1017026. Any opinions, findings and conclusions or recommendations ex-pressed in this material are the author X  X  and do not neces-sarily reflect those of the sponsor.
