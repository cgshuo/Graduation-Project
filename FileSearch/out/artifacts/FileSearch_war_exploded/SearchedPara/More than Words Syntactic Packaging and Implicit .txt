 As Pang and Lee (2008) observe, the last several years have seen a  X  X and rush X  in research on senti-ment analysis and opinion mining, with a frequent emphasis on the identification of opinions in evalua-tive text such as movie or product reviews. How-ever, sentiment also may be carried implicitly by statements that are not only non-evaluative, but not even visibly subjective. Consider, for example, the following two descriptions of the same (invented) event: 1(a) On November 25, a soldier veered his jeep into (b) On November 25, a soldier X  X  jeep veered into a Both descriptions appear on the surface to be objec-tive statements, and they use nearly the same words. Lexically, the sentences X  first clauses differ only in the difference between  X  X  and his to express the rela-tionship between the soldier and the jeep, and in the second clauses both kill and death are terms with negative connotations, at least according to the Gen-eral Inquirer lexicon (Stone, 1966). Yet the descrip-tions clearly differ in the feelings they evoke: if the soldier were being tried for his role in what hap-pened on November 25, surely the prosecutor would be more likely to say (1a) to the jury, and the defense
Why, then, should a description like (1a) be per-ceived as less sympathetic to the soldier than (1b)? If the difference is not in the words, it must be in the way they are put together; that is, the structure of the sentence. In Section 2, we offer a specific hy-pothesis about the connection between structure and implicit sentiment: we suggest that the relationship is mediated by a set of  X  X rammatically relevant X  se-mantic properties well known to be important cross-linguistically in characterizing the interface between syntax and lexical semantics. In Section 3, we val-idate this hypothesis by means of a human ratings study, showing that these properties are highly pre-dictive of human sentiment ratings. In Section 4, we introduce observable proxies for underlying seman-tics (OPUS), a practical way to approximate the rele-vant semantic properties automatically as features in a supervised learning setting. In Section 5, we show that these features improve on the existing state of the art in automatic sentiment classification. Sec-tions 6 and 7 discuss related work and summarize. Verbal descriptions of an event often carry along with them an underlying attitude toward what is be-ing described. By framing the same event in differ-ent ways, speakers or authors  X  X elect some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to pro-mote a particular problem definition, causal inter-pretation, moral evaluation, and/or treatment recom-mendation X  (Entman, 1993, p. 52). Clearly lexi-cal choices can accomplish this kind of selection, e.g. choosing to describe a person as a terrorist rather than a freedom fighter , or referencing killer whales rather than orcas . 2 Syntactic choices can also have framing effects. For example, Ronald Rea-gan X  X  famous use of the passive construction,  X  X is-takes were made X  (in the context of the Iran-Contra scandal), is a classic example of framing or spin: used without a by -phrase, the passive avoids iden-tifying a causal agent and therefore sidesteps the is-sue of responsibility (Broder, 2007). A toddler who says  X  X y toy broke X  instead of  X  X  broke my toy X  is employing the same linguistic strategy.

Linguists have long studied syntactic variation in descriptions of the same event, often under the general heading of syntactic diathesis alternations (Levin, 1993; Levin and Hovav, 2005). This line of research has established a set of semantic prop-erties that are widely viewed as  X  X rammatically rel-evant X  in the sense that they enable generalizations about syntactic  X  X ackaging X  of meaning within (and across) the world X  X  languages. For example, the verb break in English participates in the causative-inchoative alternation (causative event X broke Y can also be expressed without overt causation as Y broke ), but the verb climb does not ( X also causes the event in X climbed Y , but that event cannot be expressed as Y climbed ). These facts about partic-ipation in the alternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not. Grammati-cally relevant semantic properties of events and their participants  X  causation, change of state, and others  X  are central not only in theoretical work on lex-ical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).
The approach we propose draws on two influ-ential discussions about grammatically relevant se-mantic properties in theoretical work on lexical se-mantics. First, Dowty (1991) characterizes gram-matically relevant properties of a verb X  X  arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expres-sions like X murders Y or X interrogates Y entail and Thompson (1980) characterize  X  X emantic transi-tivity X  using similar properties, connecting semantic features to morphosyntactic behavior across a wide variety of languages.
 Bringing together Dowty with Hopper and Thompson, we find 13 semantic properties or-ganized into three groups, corresponding to the three components of a canonical transitive clause, expressed as X verb Y in English. 4 Proper-ties associated with X involve volitional involve-ment in the event or state, causation of the event, sentience/awareness and/or perception, causing a change of state in Y , kinesis or movement, and ex-istence independent of the event. Properties asso-ciated with the event or state conveyed by the verb include aspectual features of telicity (a defined end-point) and punctuality (the latter of which may be inversely related to a property known as incremen-tal theme). Properties associated with Y include affectedness, change of state, (lack of) kinesis or movement, and (lack of) existence independent of the event.

Now, observe that this set of semantic proper-ties involves many of the questions that would nat-urally help to shape one X  X  opinion about the event described by veer in (1). Was anyone or anything affected by what took place, and to what degree? Did the event just happen or was it caused? Did the event reach a defined endpoint? Did participation in the event involve conscious thought or intent? Our hypothesis is that the syntactic aspects of  X  X raming X , as characterized by Entman, involve manipulation of these semantic properties, even when overt opinions are not being expressed. That is, we propose a con-nection between syntactic choices and implicit senti-ment mediated by the very same semantic properties that linguists have already identified as central when connecting surface expression to underlying mean-ing more generally. We validated the hypothesized connection between implicit sentiment and grammatically relevant se-mantic properties using psycholinguistic methods, by varying the syntactic form of event descriptions, and showing that the semantic properties of descrip-3.1 Semantic property ratings Materials. Stimuli were constructed using 11 verbs of killing, which are widely viewed as proto-typical for the semantic properties of interest here (Lemmens, 1998): X killed Y normally involves conscious, intentional causation by X of a kinetic event that causes a (rather decisive and clearly ter-minated!) change of state in Y . The verbs comprise two classes: the  X  X ransitive X  class, involving ex-ternally caused change-of-state verbs ( kill , slaugh-ter , assassinate , shoot , poison ), and the  X  X rgative X  class ( strangle , smother , choke , drown , suffocate , starve ), within which verbs are internally caused (McKoon and MacFarland, 2000) or otherwise em-phasize properties of the object. Variation of syntac-tic description involved two forms: a transitive syn-tactic frame with a human agent as subject ( X  X ransi-tive form X , 2a), and a nominalization of the verb as subject and the verb kill as the predicate ( X  X ominal-ized form X , 2b). 2(a) The gunmen shot the opposition leader (b) The shooting killed the opposition leader Participants and procedure. A set of 18 vol-unteer participants, all native speakers of English, were presented with event descriptions and asked to answer questions probing both Dowty X  X  proto-role properties as well as Hopper and Thompson X  X  se-mantic transitivity components, responding via rat-ings on a 1-to-7 scale. For example, the questions probing volition were:  X  In this event , how likely is it that h subject i chose to be involved? X , where h subject i was the gunmen and the shooting , for 2(a-3.2 Sentiment ratings Materials. We used the materials above to con-struct short, newspaper-like paragraphs, each one accompanied by a  X  X eadline X  version of the same syntactic descriptions used above. For example, given this paragraph: the three alternative headlines would be: 3(a) Man suffocates 24-year old woman (b) Suffocation kills 24-year-old woman (c) 24-year-old woman is suffocated Some paragraphs were based on actual news sto-nal referent for both the perpetrator and the victim, it is clear that the victim dies, and the perpetrator in the scenario is responsible for the resulting death directly rather than indirectly (e.g. through negli-peared in the event description in either verbal or nominal form.
 Participants and procedure. A set of 31 volun-teers, all native speakers of English, were presented with the paragraph-length descriptions and accom-panying headlines. As a measure of sentiment, par-ticipants were asked to rate headlines on a 1-to-7 scale with respect to how sympathetic they perceive the headline to be toward the perpetrator. For exam-ple, given the paragraph and one of the associated headlines in (3), a participant would be asked to rate  X  X ow sympathetic or unsympathetic is this headline 3.3 Analysis and discussion Unsurprisingly, but reassuringly, an analysis of the sentiment ratings yields a significant effect of syn-tactic form on sympathy toward the perpetrator (
F (2 , 369) = 33 . 902 ,p&lt;. 001 ), using a mixed model ANOVA run with the headline form as fixed effect. The transitive form of the headline yielded significantly lower sympathy ratings than the nom-inalized or passive forms in pairwise comparisons (both p&lt;. 001 ). We have thus confirmed empir-ically that Reagan X  X   X  X istakes were made X  was a wise choice of phrasing on his part.

More important, we are now in a position to ex-amine the relationship between syntactic forms and perceived sentiment in more detail. We performed regression analyses treating the 13 semantic prop-erty ratings plus the identity of the verb as indepen-dent variables to predict sympathy rating as a de-pendent variable, using the 24 stimulus sentences ering semantic properties individually, we find that volition has the strongest correlation with sympathy (a negative correlation, with r =  X  . 776 ), followed by sentience ( r =  X  . 764 ) and kinesis/movement ( r =  X  . 751 ). Although performing a multiple re-gression with all variables for this size dataset is im-possible, owing to overfitting (as a rule of thumb, 5 to 10 observed items are necessary per each in-dependent variable), a multiple regression involving verb, volition, and telicity as independent variables yields R = . 88 , R 2 = . 78 ( p&lt;. 001 ). The value for small number of observations, is 74 . 1 .

In summary, then, this ratings study confirms the influence of syntactic choices on perceptions of im-plicit sentiment. Furthermore, it provides support for the idea that this influence is mediated by  X  X ram-matically relevant X  semantic properties, demonstrat-ing that these accounted for approximately 75% of the variance in implicit sentiment expressed by al-ternative headlines describing the same event. Thus far, we have established a predictive connec-tion between syntactic choices and underlying or im-plicit sentiment, mediated by grammatically relevant semantic properties. In an ideal world, we could har-ness the predictive power of those properties by us-ing volition, causation, telicity, etc. as features for regression or classification in sentiment prediction tasks. Unfortunately, the properties are not directly observable, and neither automatic annotators nor la-beled training data currently exist.

We therefore pursue a different strategy, which we refer to as observable proxies for underlying seman-tics (OPUS). It can be viewed as a middle ground between relying on construction-level syntactic dis-tinctions (such as the 3-way transitive, nominalized subject, passive distinction in Section 3) and an-notation of fine-grained semantic properties. The key idea is to use observable grammatical relations, drawn from the usages of terms determined to be relevant to a domain, as proxies for the underlying semantic properties that gave rise to their syntactic realization using those relations. Automatically cre-ated features based on those observable proxies are then used in classification as described in Section 5.
In order to identify the set T of terms relevant to a particular document collection, we adopt the relative frequency ratio (Damerau, 1993), R ( t ) = R term t  X  X  frequency in corpus c to the size N corpus. R ( t ) is a simple but effective comparison of a term X  X  prevalence in a particular collection as compared to a general reference corpus. We used the British National Corpus as the reference because it is both very large and representative of text from a wide variety of domains and genres. The threshold of R ( t ) permitting membership in T is an experi-mental parameter.

OPUS features are defined in terms of syntactic dependency relations involving terms in T . Given a set D of syntactic dependency relations, features are of the form t : d or d : t , with d  X  D,t  X  T . That is, they are term-dependency pairs extracted from term-dependency-term dependency tuples, preserv-ing whether the term is the head or the dependent in the dependency relation. In addition, we add two construction-specific features: TRANS :v, which rep-resents verb v in a canonical, syntactically transitive usage, and NOOBJ :v, present when verb v is used
Example 4 shows source text (bolded clause in 4a), an illustrative subset of parser dependencies (4b), and corresponding OPUS features (4c): 4(a) Life Without Parole does not eliminate the risk (b) nsubj(murder, prisoner); aux(murder, will); (c) TRANS :murder, murder:nsubj, nsubj:prisoner, Intuitively the presence of TRANS :murder suggests the entire complex of semantic properties discussed in Section 2, bringing together the impliciation of volition, causation, etc. on the part of prisoner (as does nsubj:prisoner), affectedness and change of state on the part of guard (as does dobj:guard), and so forth.
The NOOBJ features can capture a habitual read-ing, or in some cases a detransitivizing effect as-sociated with omission of the direct object (Olsen and Resnik, 1997). The bold text in (5) yields NOOBJ :kill as a feature. 5(a) At the same time, we should never ignore the In this case, omitting the direct object decreases the extent to which the killing event is interpreted as telic, and it eliminates the possibility of attributing change-of-state to a specific affected object (much like  X  X istakes were made X  avoids attributing cause to a specified subject), placing the phrasing at a less  X  X emantically transitive X  point on the transi-tivity continuum (Hopper and Thompson, 1980). Some informants find a perceptible increase in neg-ative sentiment toward inmate when the sentence is phrased as in 5(b): 5(b) At the same time, we should never ignore the Having discussed linguistic motivation, empirical validation, and practical approximation of seman-tically relevant features, we now present two stud-ies demonstrating their value in sentiment classifica-tion. For the first study, we have constructed a new data set particularly well suited for testing our ap-proach, based on writing about the death penalty. In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al. (2006). 5.1 Predicting Opinions of the Death Penalty Corpus. We constructed a new corpus for exper-imentation on implicit sentiment by downloading the contents of pro-and anti-death-penalty Web sites and manually checking, for a large subset, that the viewpoints expressed in documents were as expected. The collection, which we will refer to as the DP corpus, comprises documents from five pro-death-penalty sites and three anti-death-penalty sites, and the corpus was engineered to have an even Frequent bigram baseline. We adopted a super-vised classification approach based on word n -gram features, using SVM classification in the WEKA machine learning package. In initial exploration us-ing both unigrams and bigrams, and using both word forms and stems, we found that performance did not differ significantly, and chose stemmed bigrams for our baseline comparisons. In order to control for the difference in the number of features available to the classifier in our comparisons, we use the N most fre-quent stemmed bigrams as the baseline feature set where N is matched to number of OPUS features used in the comparison condition.
 OPUS-kill verbs: OPUS features for manually selected verbs. We created OPUS features for 14 verbs  X  those used in Section 3, plus murder , exe-cute , and stab and their nominalizations (including both event and -er nominals, e.g. both killing and killer )  X  generating N = 1016 distinct features. OPUS-domain: OPUS features for domain-relevant verbs. We created OPUS features for the 117 verbs for which the relative frequency ratio was greater than 1. This list includes many of the kill verbs we used in Section 3, and introduces, among others, many transitive verbs describing acts of physical force (e.g. rape , rob , steal , beat , strike , force , fight ) as well as domain-relevant verbs such as testify , convict , and sentence . Included verbs near the borderline included, for example, hold , watch , allow , and try . Extracting OPUS features for these verbs yielded N = 7552 features.
 Evaluation. Cross-validation at the document level does not test what we are interested in, since a classifier might well learn to bucket documents ac-cording to Web site, not according to pro-or anti-death-penalty sentiment. To avoid this difficulty, we performed site-wise cross-validation. We restricted our attention to the two sites from each perspec-tive with the most documents, which we refer to as pro1 , pro2 , anti1 , and anti2 , yielding 4-fold cross-validation. Each fold f taining all documents from one pro and one anti site for training, using all documents from the remain-ing pro and anti sites for testing. So, for exam-ple, fold f anti1 in training, and all documents from pro2 and anti2 for testing. 13 As Table 1 shows, OPUS fea-tures provide substantial and statistically significant gains ( p&lt;. 001 ).

As a reality check to verify that it is domain-relevant verb usages and the encoding of events they embody that truly drives improved classification, we extracted OPUS features for the 14 most frequent verbs found in the DP Corpus that were not in our manually created list of kill verbs, along with their nominalizations. Table 2 shows the results of a clas-sification experiment using a single train-test split, training on 1062 documents from pro1 , pro2 , anti1 , anti2 and testing on 84 test documents from the sig-nificantly smaller remaining sites. Using OPUS features for the most frequent non-kill verbs fails to beat the baseline, establishing that it is not sim-ply term frequency, the presence of particular gram-matical relations, or a larger feature set that the kill -verb OPUS model was able to exploit, but rather the properties of event encodings involving the kill verbs themselves. 5.2 Predicting Points of View in the In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we re-port on sentiment classification using OPUS features in experiments using a publicly available corpus in-volving opposing perspectives, the Bitter Lemons (hence BL) corpus introduced by Lin et al. (2006). Corpus. The Bitter Lemons corpus comprises es-says posted at www.bitterlemons.org , which, in the words of the site,  X  X resent Israeli and Pales-tinian viewpoints on prominent issues of concern X . As a corpus, it has a number of interesting proper-ties. First, its topic area is one of significant interest and considerable controversy, yet the general tenor of the web site is one that eschews an overly shrill or extreme style of writing. Second, the site is orga-nized in terms of issue-focused weekly editions that include essays with contrasting viewpoints from the site X  X  two editors, plus two essays, also contrasting, from guest editors. This creates a natural balance be-tween the two sides and across the subtopics being discussed. The BL corpus as prepared by Lin et al. contains 297 documents from each of the Israeli and Palestinian viewpoints, averaging 700-800 words in length.
 Lin et al. classifiers. Lin et al. report results on distinguishing Israeli vs. Palestinian perspectives using an SVM classifier, a naive Bayes classifier NB-M using maximum a posteriori estimation, and a naive Bayes classifier NB-B using full Bayesian in-ference. (Document perspectives are labeled clearly on the site.) We continue to use the WEKA SVM classifier, but compare our results to both their SVM and NB-B, since the latter achieved their best results. OPUS features. As in Section 5.1, we experi-mented with OPUS features driven by automati-cally extracted lists of domain-relevant verbs. For these experiments, we included domain-relevant nouns, and we varied a threshold  X  for the rela-tive frequency ratio, including only terms for which log( R ( t )) &gt; X  . In addition, we introduced a gen-eral filter on OPUS features, eliminating syntactic dependency types that do not usefully reflect seman-tically relevant properties: det, predet, preconj, prt, aux, auxpas, cc, punct, complm, mark, rel, ref, expl. Evaluation. Lin et al. describe two test scenar-ios. In the first, referred to as Test Scenario 1, they trained on documents written by the site X  X  guests, and tested on documents from the site X  X  editors. Test Scenario 2 represents the reverse, training on docu-ments from the site editors and testing on documents from guest authors. As in our site-wise cross vali-dation for the DP corpus, this strategy ensures that what is being tested is classification according to the viewpoint, not author or topic.

Figure 1 (top) summarizes a large set of experi-ments for Test Scenario 1, in which we varied the values of  X  for verbs and nouns. Each experiment, using a particular h  X  ( verbs ) , X  ( nouns ) i , corresponds to a vertical strip on the x -axis. The points on that strip include the  X  values for verbs and nouns, mea-sured by the scale on the y -axis at the left of the figure; the accuracy of Lin et al. X  X  SVM (88.22% ac-curacy, constant across all our variations); the accu-racy of Lin et al. X  X  NB-B classifier (93.46% accu-racy, constant across all our variations), and the ac-curacy of our SVM classifier using OPUS features, which varies depending on the  X  values. Across 423 experiments, our average accuracy is 95.41%, with the best accuracy achieved being 97.64%. Our clas-sifier underperformed NB-B slightly, with accura-cies from 92.93% to 93.27%, in just 8 of the 423 experiments.

Figure 1 (bottom) provides a similar summary for experiments in Test Scenario 2. The first thing to no-tice is that accuracy for all methods is lower than for Test Scenario 1. This is not terribly surprising: it is likely that training a classifier on the more uniform authorship of the editor documents builds a model that generalizes less well to the more diverse au-thorship of the guest documents (though accuracy is still quite high). In addition, the editor-authored documents comprise a smaller training set, consist-ing of 7,899 sentences, while the guest documents have a total of 11,033 sentences, a 28% difference. In scenario 2, we obtain average accuracy across ex-periments of 83.12%, with a maximum of 85.86%, in this case outperforming the 81.48% obtained by Lin X  X  SVM fairly consistently, and in some cases ap-proaching or matching NB-B at 85.85%. Pang and Lee X  X  (2008) excellent monograph pro-vides a thorough, well organized, and relatively re-cent description of computational work on senti-ment, opinion, and subjectivity analysis.

The problem of classifying underlying sentiment in statements that are not overtly subjective is less studied within the NLP literature, but it has received some attention in other fields. These include, for ex-ample, research on content analysis in journalism, media studies, and political economy (Gentzkow and Shapiro, 2006a; Gentzkow and Shapiro, 2006b; Groseclose and Milyo, 2005; Fader et al., 2007); au-tomatic identification of customer attitudes for busi-ness e-mail routing (Durbin et al., 2003). And, of course, the study of perceptions in politics and me-dia bears a strong family resemblance to real-world marketing problems involving reputation manage-ment and business intelligence (Glance et al., 2005).
Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al. (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science (e.g. (Laver et al., 2003)). Other recent work focusing on the notion of perspective or ideology has been reported by Martin and Vanberg (2008) and Mullen and Malouf (2008).
Among prior authors, Gamon X  X  (2004) research is perhaps closest to the work described here, in that he uses some features based on a sentence X  X  logical form, generated using a proprietary system. How-ever, his features are templatic in nature in that they do not couple specific lexical entries with their logi-cal form. Hearst (1992) and Mulder et al. (2004) de-scribe systems that make use of argument structure features coupled with lexical information, though neither provides implementation details or experi-mental results.

In terms of computational experimentation, work by Thomas et al. (2006), predicting yes and no votes in corpus of United States Congressional floor debate speeches, is quite relevant. They combined SVM classification with a min-cut model on graphs in order to exploit both direct textual evidence and constraints suggested by the structure of Congres-sional debates, e.g. the fact that the same individ-ual rarely gives one speech in favor of a bill and an-other opposing it. We have extend their method to use OPUS features in the SVM and obtained signifi-cant improvements over their classification accuracy (Greene, 2007; Greene and Resnik, in preparation). In this paper we have introduced an approach to implicit sentiment motivated by theoretical work in lexical semantics, presenting evidence for the role of semantic properties in human sentiment judgments. This research is, to our knowledge, the first to draw an explicit and empirically supported connection be-tween theoretically motivated work in lexical se-mantics and readers X  perception of sentiment. In ad-dition, we have reported positive sentiment classifi-cation results within a standard supervised learning setting, employing a practical first approximation to those semantic properties, including positive results in a direct comparison with the previous state of the art.

Because we computed OPUS features for opin-ionated as well as non-evaluative language in our corpora, obtaining overall positive results, we be-lieve these features may also improve conventional opinion labeling for subjective text. This will be in-vestigated in future work.
 The authors gratefully acknowledge useful discus-sions with Don Hindle and Chip Denman.
