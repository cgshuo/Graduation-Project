 The term  X  X APTCHA X  (Completely Automated Public Turing test to tell Computers and Humans Apart) was created by Luis von Ahn in 2000 [1], to name a simple Tur-CAPTCHA has become a widely applied security measure on the Web, to prevent around the world type more than 100 million CAPTCHAs every day. Suppose it takes 10 seconds to enter a CAPTCHA. In accordance with the use of 100 million times a them into useful services. In recent years, there have been several proposals for better utilization the labor resources on CAPTCHA. A typical example is ReCAPTCHA, a special type of CAPTCHA that can be used to digitize printed material. In this work, we explore the possibility of utilizing the labor resources on CAPTCHA to annotate textual corpus. We propose a new type of CAPTCHA, named AnnoCAPTCHA, which not only provides the functionality of CAPTCHA but also CAPTCHA, a user is presented with a set of short textual items and is asked to select the right annotation for each item. Among the textual items, some items X  annotations mance of NLP. In the paper, we conduct analytical study to evaluate its effectiveness human users. AnnoCAPTCHA. Finally, Section 5 provides a conclusion. 2.1 Human Computation NLP. The concept of human computation was first coined by Luis Von Ahn. Guided by its philosophy, Ahn invented ReCAPTCHA [2] and the well known ESP Game already been a number of online games [3,4] that can exploit players X  intelligence to label the contents of images. CAPTCHA is another typical area to apply human com-putation. Except ReCAPTCHA, there are a number of proposals for generating values out of CAPTCHA. For instance, ASIRRA [5] has shown that CAPTCHA can be used for humanitarian purpose. TagCaptcha [6] uses CAPTCHA to perform image annota-CAPTCHA to perform corpus annotation. 2.2 Corpus Annotation text mining. According to our study, the existing methods for large-scale corpus anno-tation usually fall in the following three categories: perform manual annotation. The well known Brown Corpus was generated by this human efforts and thus are very costly. 2. WWW-based non-experts method. The main idea is to call up non-expert volun-teers on the Web to annotate corpuses. The most famous example is the Open 
Mind Commonsense project 3 . The efficiency of this method is also questionable, as there is a lack of incentive for Web users to perform annotation for free. 3. Game-based method. This method applies human computation. It exploits human intelligence in playing games to perform corpus annotation. For instance, by using image repository. However, the existing methods only deal with multimedia data such as images. There is no game designed to annotate textual corpus. corpus annotation. Our work in this paper aims to provide such a mechanism. To illustrate the functionality of AnnoCAPTCHA, we use sentiment annotation as an expensive, it is an ideal application scenario for AnnoCAPTCHA. 3.1 User Interface phrase items, two X  X  sentiment polarities are already known to the system and the other The items with known sentiment polarities are used to verify if the user is a human. If regard that the user X  X  judgment on the unknown items is very likely to be correct too. Then, the user X  judgment on each unknown item is recorded and counted as a vote for that item X  X  sentiment polarity. As more and more users use AnnoCAPTCHA, the perform user verification. 3.2 Effectiveness as CAPTCHA AnnoCAPTCHA can provide the functionality of CAPTCHA. It performs human already known. Usually, a human X  X  judgment on sentiment polarity is much more much harder to a computer. Suppose a computer can only perform random guess. Its chance of passing the test is thus only about 25% (=0.5 2 ). Even though a user may fail the test occasionally, the user can try more tests. Eventually, the verification will suc-tests by chance. According to the theory of probability, as the number of tests increas-es, the machine X  X  success rate will quickly approach its expected accuracy. As a con-sequence, a machine can be easily recognized by a small number of tests, before it can perform any effective attack. Upon detection, the website can immediately cut off the connection with the attacker to prevent it from taking further action. 3.3 Efficiency of Anno tation Generation AnnoCAPTCHA. The Model.  X 
We assume that each CAPTCHA contains k +2 items, in which 2 items X  annotations are known and the other k items X  annotations are unknown.  X 
A user of AnnoCAPTCHA is either a human or a machine. Let s % be the accuracy of a human X  X  judgment. Let t % be the accuracy of a machine X  X  judgment. Obvious-ly, s % is much greater than t %.  X 
As mentioned previously, once a machine is detected by AnnoCAPTCHA, its con-nection will be cut off. This effectively restricts the number of annotations by ma-chines. Let the proportion of CAPTCHA tests finished by humans be m % and that by machines/attackers be 1-m %. Thus, m % should be far greater than 1-m %.  X  record user X  X  votes on the k unknown items in a database. The vote record of each positive polarity and j represents the number of users voting for negative polarity. calculated as: where P 0 denotes the probability that a vote is incorrect, that is 
In principle, we regard the sentiment polarity of an unknown item positive if i &gt; j make a wrong judgment. AnnoCAPTCHA uses a predefined threshold of the error probability, e.g. 1%, to represent the maximum error rate it can accept. Once the P ij of an unknown item falls below the threshold, AnnoCAPTCHA finalizes its senti-ment polarity and turns it into a known annotation. Analysis of Efficiency measures the cost of the annotation, i.e. th e number of votes required to determine the annotation cost, which we use to depict the efficiency of AnnoCAPTCHA. or i : j-1 . Thus, Q ij can be calculated recursively as follows: votes and j negative votes. Then O ij can be computed by: Finally, the expected annotation cost can be calculated as: We did not manage to integrate Equations (1), (2), (3) and (4) into a single formula to measure the expected annotation cost. However, it does not prevent us from evaluat-ing the relationship between the expect cost and the various factors in our model. As suggested by our analysis, the main influence factors of the expected annotation cost ers X  votes, i.e. 1-m %. 
We created a Java program to compute th e expect annotation cost based on Equa-human X  X  accuracy ( s %) is plotted in Fig. 2. Obviously, the higher the human accura-cy, the smaller of the expected cost. As we can see, to guarantee the efficiency of the 10 votes, the human accuracy must be above 75%. Fortunately, this condition can be satisfied for most corpus annotation tasks. 
To evaluate the influence of attacks, we vary m % from 50% to 100%, and fix s % the proportion of human X  X  votes ( m %) is plotted in Fig.3. As we can see, the higher the proportion of human X  X  votes, the smaller the average cost. The proportion of hu-man X  X  votes depends on how effectively a website detects and blocks attackers. When the proportion is more than 70%, the average cost can be controlled below 5 votes. We carried out a user study to evaluate AnnoCAPTCHA X  X  accessibility. In the study, we asked 9 volunteers to do the same 10 CAPTCHAs. The volunteers are all universi-CAPTCHA. In addition, we let the machine to annotate the same 10 CAPTCHAs by random guess, to study the impact of attacks to the annotation efficiency. average cost 4.1 User Response Ti m User response time measu r test. Fig.4 shows the respo can see, there is a learning c presented with the first C A requirements and get used t seconds, and can occasion a volunteers become familia r interval between 5 second seconds. Overall, no volu n The accessibility of AnnoC 4.2 User Accuracy The accuracy of the 9 volu n provide correct answers to m is 96.67%. This also indic a rate users. accuracy 4.3 Efficiency notation generation by AnnoCAPTCHA. We increased the proportions of random efficiency. We assume that human X  X  accuracy is 96.67%, the exact value we measured in the user study. And we set the error probability threshold to 1%. The average cost need to annotate each phrase) is shown in Fig. 6. As we can see, with the accuracy of 96.67%, the annotation is very efficient. Without the disturbance of machines, the annotation cost is only about 2 votes. As more and more test results of machines are added, the annotation cost goes up. However, the overall cost still appears acceptable. utilize crowd intelligence to do corpus annotation. We used the sentiment corpus an-notation as example and discussed the effectiveness and efficiency of our system. We study to prove our system is easy to access and efficient in annotation generation. 
