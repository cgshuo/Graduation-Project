 Roser Morante Abstract Semantic role labeling is traditionally viewed as a sentence-level task concerned with identifying semantic arguments that are overtly realized in a fairly local context (i.e., a clause or sentence). However, this local view potentially misses important information that can only be recovered if local argument structures are linked across sentence boundaries. One important link concerns semantic arguments that remain locally unrealized (null instantiations) but can be inferred from the context. In this paper, we report on the SemEval 2010 Task-10 on  X  X  X inking Events and Their Participants in Discourse X  X , that addressed this problem. We discuss the corpus that was created for this task, which contains annotations on multiple levels: predicate argument structure (FrameNet and PropBank), null instantiations, and coreference. We also provide an analysis of the task and its difficulties.
 Keywords SemEval Null instantiation Semantic roles Frame semantics Automatic semantic role labeling (SRL) is a relatively novel task that was introduced into NLP when resources annotated with semantic argument structure became available in the early 2000s as part of the FrameNet 1 and PropBank 2 projects. Gildea X  X  and Jurafsky X  X  seminal paper (Gildea and Jurafsky 2002 ) was the first that addressed this task. Since then SRL has gained a lot of attention from the NLP community and numerous papers on the topic as well as several shared tasks at Senseval/SemEval (Ma ` rquez et al. 2007 ; Litkowski 2004 ; Baker et al. 2007 ; Diab et al. 2007 ), and CoNLL (Carreras and Ma ` rquez 2004 , 2005 ; Surdeanu et al. 2008 ) bear witness to the importance of the task.

Semantic role labeling has been defined as a sentence-level task in which semantic roles are assigned to the syntactic arguments of a predicate. Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text amounts to knowing who did what to whom when where how, etc.
The view of SRL as a sentence-internal task is partly due to the fact that large-scale manual annotation projects such as FrameNet and PropBank typically present their annotations lexicographically by lemma rather than by source text. FrameNet, for example, added full-text annotations to their data releases only relatively recently.
While viewing SRL as a sentence-level task is clearly a useful approximation, it also misses a lot of information. It is clear that there is an interplay between local argument structure and the surrounding discourse (Fillmore 1977 ). But so far there have been few attempts to find links between argument structures across clause and sentence boundaries. Two notable exceptions are Fillmore and Baker ( 2001 ) and Burchardt et al. ( 2005 ). Fillmore and Baker ( 2001 ) analyze a short newspaper article and discuss how frame semantics could benefit discourse processing but without making concrete suggestions of how to model this. Burchardt et al. ( 2005 ) provide a detailed analysis of links between the local semantic argument structures in a short text; however their system is not fully implemented either.

One area that is particularly affected by context is argument realization. It is relatively rare that all possible semantic arguments of a predicate are realized overtly. Which arguments are realized and how they are realized depends not only on what information the speaker wants to convey and which syntactic constraints apply to the predicate but, crucially, it also depends on the discourse context. For instance, in (11,000) the C HARGES role of the predicate clear is not realized because the charges were already mentioned in the previous sentence ( for murder ). Sentence (1b) provides another example. Here, the E XPERIENCER and the O BJECT of jealousy are not overtly expressed as dependents of the noun jealousy but can be inferred to be Watson and the speaker, Holmes, respectively. While a human has no trouble making these connections, this is beyond state-of-the-art SRL systems. (1) a. In a lengthy court case the defendant was tried for murder. In the end, he
Given the recent successes in sentence-level SRL, we believe the time is ripe to extend the task and take context into account. This move will not only result in richer representations, e.g., argument structures in which fillers of non-realized roles are annotated if they are recoverable from the context; taking context into account may also lead to better, more robust systems, especially for semantically deeper argument structure annotation as provided by FrameNet. To support research in this direction, we organized the SemEval-2010 Shared Task on  X  X  X inking Events and their Participants in Discourse X  X , in which we focused on the problem of recovering fillers for roles that were not overtly realized (so-called null instantiations (henceforth also NI)). This task was challenging, both for us as the organizers and for the participants. The difficulty of the task can be partly attributed to its novelty and partly to its inherent complexity, which requires deep semantic processing, possibly even inference. That the task is nonetheless worthwhile and timely is evidenced by the fact that other researchers have also started to work on it (Gerber and Chai 2010 ).

In this paper, we describe the task and the data. A major focus will be on the resource creation. Because the task was novel and X  X o our knowledge X  X o similar annotated resource existed prior to it, we had to make several non-trivial annotation decisions, ranging from the choice of text and the decision of what and how to annotate to the question of how to ensure sufficient annotation quality. In addition to the resource itself, we also briefly describe the task and its specific challenges.
In detail, this paper is organized as follows: Sect. 1 discusses and motivates the choice of texts for the annotation. Section 2 provides an in-depth overview of the annotations we carried out for the task (coreference, predicate argument structure, and null instantiations). We highlight specific annotation decisions, discuss the annotation procedure and address the question of annotation quality. Following this discussion of the annotation, Sect. 3 gives a short overview of the shared task, focusing specifically on why the task was hard. Section 4 discusses some related work on recovering implicit arguments. Finally, Sect. 5 concludes. 1 Source texts While mainstream NLP tends to focus on newswire corpora, we deliberately deviated from this and settled on narrative texts because we believe that this genre is better suited to studying discourse effects on predicate argument structure. The texts in our corpus are taken from works of fiction by Arthur Conan Doyle. The first text we used was  X  X  X he Adventure of Wisteria Lodge X  X  (1908), a lengthy, two-part story of which we annotated the second part, titled  X  X  X he Tiger of San Pedro X  X  (henceforth  X  X  X iger X  X ). The other text is  X  X  X he Hound of the Baskervilles X  X  (1901/02), of which we annotated the last three chapters, 12 X 14 (henceforth  X  X  X ound X  X ). In the shared task, the annotated part of the Tiger served as training data, while chapters 13 and 14 of the Hound served as test data. Chapter 12 of the Hound was annotated after the completion of the task. Basic statistics of the annotated texts are given in Table 1 .
A major motivation for choosing texts by Doyle was that his books are no longer subject to restrictions, which means that our corpus can be distributed freely. Choosing these texts over more contemporary fiction potentially comes at the cost of slightly old-fashioned language. However, we believe that this effect is negligible. Classical crime fiction also has the advantage of being relatively focused on a particular topic and typically also containing a fairly small set of protagonists and locations. We believe that this makes this genre ideal for studies of the interaction between discourse and semantics. The decision to choose these particular two texts was motivated by the fact that we wanted longer texts, also in view of a possible future extension of the corpus. We also tried to choose texts with comparable content, in order to keep the frame inventory relatively constant across both texts.
 Prior to annotation, digital versions of the texts were downloaded from the University of Virginia X  X  E-Text Center. 3 As we needed the data in Salsa/TIGER-XML format (see Sect. 2.5.2 ) and also wanted to provide syntactic information, we preprocessed the raw texts with the Shalmaneser (Erk and Pado  X  2006 ) semantic parser. The semantic parser produces the desired xml-format and moreover calls the Collins parser (Collins 1997 ) to produce syntactic parse trees. The texts were then annotated with the following information (see Sect. 2 for more details): 4 .  X  semantic predicate argument structure (FrameNet and PropBank) 5  X  null instantiations and fillers of null instantiated roles  X  coreference (necessary for evaluation purposes)
In addition to creating training and test data for the shared task, we had two complementary motivations in creating this corpus. We wanted to create a resource that supports the study of how the particular levels of annotation that we have added to the texts interact. And we also wanted to be able to study this in texts that are longer than the average newspaper article included for instance in the Penn Treebank or the various news corpora that are available. 2 Annotations for SemEval Task-10 In this section, we describe the annotations carried out for our SemEval shared task, namely coreference, predicate argument structure, and null instantiation (NI). These annotations were carried out at Saarland University.
 2.1 Coreference annotation The coreference annotations were intended only as a tool for evaluating NI linking. We allowed NIs to be linked to any mention of the relevant referent rather than only to the most recent one, or to the most prominent one in the proximal prior context. mention had the correct referent.

We generally follow the format used in the MUC-7 shared task. 6 We do, however, deviate from MUC-7 in some respects.  X  One major difference is that in some cases we use non-nominal XPs as  X  Another more minor difference is that we only annotate whole phrases while  X  MUC also marks all NPs of certain morphosyntactic types as markables even if
It should be pointed out that MUC coreference is extensionally oriented. As an example consider that in MUC two markables should be treated as coreferential if the text asserts them to be coreferential at any time . Thus, (2) Henry Higgins, who was formerly sales director for Sudsy Soaps, became should be annotated as (3) &lt;COREF ID =  X  X 1 X  X  MIN =  X  X  X enry Higgins X  X &gt; Henry Higgins, who was In our data, we have some cases where the true identities of some characters are revealed only later in the story, e.g. the character of Henderson is revealed to really be Murillo, The Tiger of San Pedro. In this case, we formed one big coreference chain, even though intensionally these are different referents. 2.2 Predicate argument-structure annotations: FrameNet The frame semantic annotations as they were used in the SemEval Shared Task reflected the state of FrameNet in release 1.4 alpha, which was a special release for use by SemEval Task-10 participants. The most recent official FrameNet release, 1.5, differs only very little from our release in terms of the frame semantic analysis and so we have not updated our annotations.

We generally follow FrameNet X  X  annotation policies and style. However, the format of our frame semantic annotations is somewhat different from those produced by FrameNet. The reason for this is that our annotation tool, Salto (Burchardt et al. 2006 ), 8 doesn X  X  use layers as FrameNet X  X  Desktop tool does. As a result:  X  We treat relativization as a separate frame that is evoked by the relativizer. Note  X  We let support verbs and prepositions evoke a Support frame in which only one  X  We had no way of annotating frame elements on a second FE-layer, which 2.3 Predicate argument-structure annotation: PropBank/NomBank The data set for the FrameNet version of the task was built at Saarland University in close co-operation with the FrameNet team in Berkeley. As we wanted to give the participants a choice of either working with FrameNet or PropBank argument structures, we also produced a PropBank version of the annotations by semi-automatically mapping the original FrameNet annotations. This task was carried out in close collaboration with Martha Palmer X  X  PropBank group.

The data-set for PropBank was created by mapping FrameNet annotations of verbs and nouns onto PropBank and NomBank labels. Targets of other parts of speech, in particular adjectives and prepositions, have no entries in PropBank or NomBank and thus their annotations could not be mapped. For verbal targets, we used the SemLink 9 mappings created by the PropBank group. SemLink provides pairwise mappings between VerbNet and PropBank and between VerbNet and FrameNet. The connection between PropBank and FrameNet accordingly went through VerbNet. For nominal targets, there existed no hand-checked mapping between FrameNet and NomBank but we established links between FrameNet and NomBank with the help of SemLink. In particular, since PropBank verbs and NomBank nouns may have a mapping to VerbNet classes, and NomBank nouns also may reference PropBank verb senses, we were able to connect some NomBank senses to FrameNet by way of their direct or indirect (via VerbNet) connections to PropBank verbs. For instance, as shown below, the mapping of hatred  X  X  NomBank roleset to the Experiencer_subj frame proceeded by way of the verb hate .

Our mapping is, however, not a complete one. Most importantly, the mapping was attempted only for predicates that are attested in our data. We did not try to create a full-coverage FrameNet-PropBank/NomBank ma pping. Additional factors result in missing mappings. First, PropBank and NomBank lacked entries for some lemmas that were covered by FrameNet. For instance, many nouns relating to terrain features or habitats such as moor and mire are not included in NomBank. Second, the pre-existing mappings in SemLink and NomBank, on which our ow n automatic mappings are based, are not complete. Third, our mappings were conser vative, relying only on the pre-existing mappings. No attempt was made to, for insta nce, align senses without an explicit mapping with the help of definitions and/or examples . Neither were mappings established between lemmas that were monosemous in all resour ces but for which no explicit mappings existed. As a result, the PropBank version of our corpus is less complete and more sparse than the FrameNet version. Providing a com plete manually checked and manually enhanced PropBank annotation of the data was beyond the scope of the shared task.
For the mappings of verbal predicates, we can rely on the quality control undertaken by the SemLink creators. For nominal predicates, due to a lack of resources and the limited scope and conservative nature of our mappings, we did not perform a separate evaluation of the quality of the mappings. 2.4 Null instantiation The theory of null complementation used here is the one adopted by FrameNet, which derives from the work of Fillmore ( 1986 ). 10 Briefly, omissions of core arguments of predicates are categorized along two dimensions, the licensor and the interpretation they receive. The idea of a licensor refers to the fact that either a particular lexical item or a particular grammatical construction must be present for the omission of a frame element (FE) to occur. For instance, the omission of the A
GENT in (4) is licensed by the passive construction. The omission is constructional because it can apply to any predicate with an appropriate semantics that allows it to occur in the passive construction. On the other hand, the omission in (5) is lexically specific: the verb arrive allows the G OAL to be unspecified but the verb reach , another member of the Arriving frame, does not. (4) That has not been attempted before 0 Agent . (5) We arrived 0 Goal at 8 p.m.
 The above two examples also illustrate the second major dimension of variation. Whereas, in (4) the A GENT making the attempt is only existentially bound within the discourse (indefinite null instantiation, INI), the G OAL location in (5) is an entity that must be accessible to speaker and hearer from the discourse or its context (definite null instantiation, DNI). Note that the two parameters, licensor and interpretation, can also combine to yield lexically licensed INI and constructionally licensed DNI. In (6), the FE T EXT of the Reading frame is omitted existentially (INI) by the verb read , whereas in (7) the imperative licenses omission of the T HEME -subject of leave under identity with the addressee, i.e. T HEME is a DNI here. (6) I have been reading 0 Text all afternoon. (7) Please 0 Theme leave the auditorium through the side doors.

Finally, note that the licensing construction or lexical item fully and reliably determines the interpretation. Whereas missing by -phrases always have an indefinite interpretation, whenever arrive omits the G OAL lexically, the G OAL has to be interpreted as definite, as it is in (5). In the context of the task, the focus was on cases of DNI, whether they were licensed lexically or constructionally.

For the annotation of NIs and their links to the surrounding discourse we created new guidelines as this was a novel annotation task. We adopted ideas from the annotation of coreference information, linking locally unrealized roles to explicit mentions as part of a coreference chain. We marked only identity relations but not part-whole or bridging relations between referents. The set of unrealized roles under consideration includes only the core arguments but not adjuncts (peripheral or extra-thematic roles in FrameNet X  X  terminology). Possible antecedents are not restricted to noun phrases but include all constituents that can be (local) role fillers for some predicate plus complete sentences (which can sometimes fill roles such as MESSAGE ). Table 2 provides some basic statistics about the texts, the annotated frames and the overtly realized roles as well as the number of NIs (resolved and unresolved).
The training data and the test data have very similar ratios of frame instances per word token, that is, they have a comparable density of annotation. The training data had more instances than the test data, on average, of each occurring frame type. With regard to null instantiations, it can be seen that while the number of NIs is much smaller than the number of overt frame elements, it is not negligible. Moreover, the majority of DNIs can be resolved within the text. Current SRL systems are not able to recover information about fillers of NIs and thus miss important information. 2.5 Annotation procedure and quality 2.5.1 Annotators Only some of the data, chapters 13 and 14, was annotated as test data for the 2010 SemEval Shared Task 10 by three annotators. All three annotators were advanced students with a background in linguistics and had one or two years prior experience of annotating semantic argument structures, though they were new to coreference and NI annotation. After the annotation, chapters 13 and 14 were adjudicated by two pairs of expert annotators (one for argument structures and null instantiation and the other for coreference). Each pair of adjudicators subsequently meta-adjudicated the data by discussing any remaining disagreements and arriving at one gold-standard annotation. Chapter 12 of the Hound of the Baskervilles as well as the excerpt from the Tiger of San Pedro were initially annotated by one expert annotator and then collaboratively adjudicated by that annotator and another expert annotator. Throughout the annotation and adjudication process, we discussed difficult cases. To ensure consistency, we also maintained a wiki, in which we documented difficult cases and the annotation decisions we took. Most of the questions were of the kind that also arise in lexicographic annotation by native speakers, having to do with frame distinctions. Some questions related to the exact boundaries for frame elements in cases where our non-gold, automatically provided syntax was wrong. More challenging cases mostly had to do with usages that are either very low-frequency or no longer known in present-day English. For instance, close is now rarely used, as it is in (8), to evoke the Volubility frame, within which it would indicate a low degree of forthcomingness or openness. The verb lay is arguably used in (9) to evoke the Killing frame but that is not a regular use in contemporary English. 11 In both of these cases, it was ultimately decided after discussions with the FrameNet team not to assign these predicates to the frames involved, as otherwise new lexical units would have had to be created in the FN database for which hardly any instances could be found in available reference corpora, i.e., these predicates were left unannotated. (8) You X  X e mighty close about this affair, Mr. Holmes. (9) We X  X e laid the family ghost once and forever.

The wiki also contained specific annotation guidelines for NI and coreference annotations as well as for argument annotations. Additionally, we created software that checked the consistency of our annotations against the frame, frame element and FE-relation specifications of FrameNet and alerted annotators to potential problems with their annotations that might have arisen due to oversight. This tool, for instance, detected unannotated instances of a lemma that was associated with one or more lexical units in the FrameNet database. Similarly, it detected cases where a core FE was neither annotated overtly nor accounted for by an NI annotation. 2.5.2 Format and tool All the annotations of the text were carried out with the Salto tool (Burchardt et al. 2006 ) (see Fig. 1 ), which displays the texts sentence by sentence. For each sentence, the syntactic parse tree is shown (as produced by the Collins parser (Collins 1997 ), see Sect. 1 ). Users can then decorate the syntax trees with labels representing semantic argument structures. For a given target word, annotators first select a frame from a predefined inventory (which could also be extended, e.g., if the frame wasn X  X  yet in FrameNet). 12 After selecting a frame the annotators can then decide which of the frame elements should be connected to which constituents. Salto permits the annotation of frames and frame elements in different sentences. This is important for the annotation of null instantiations whose fillers can often be found in other sentences. Null instantiations were annotated in the same way as overt arguments but given a specific flag encoding the interpretation type (DNI, INI). If a null instantiated frame element could be resolved in the text, it was simply attached to the relevant filler. To annotate coreference, we defined a new frame which permits linking a target word with a coreferent element. The input and output data format for Salto is Salsa/TIGER-XML (Erk and Pado  X  2004 ). 2.5.3 Annotation quality The annotation task performed by the annotators has two phases. In the first one, annotators have to detect the units of annotation, that is, they have to decide, for example, whether an instance of take is part of a multi-word such as take off rather than being a single word predicate. In the second phase, once an annotator has identified an occurrence of a particular single or multi-word unit, she needs to check if there exist one or more frames for that unit and which if any of them are appropriate for the instance at hand. As a result of this way of proceeding, there is ambiguity in how to interpret an unannotated instance of a predicate that is associated with at least one frame in FrameNet. Either an unannotated instance represents an oversight due to a lack of attention on the annotator X  X  part, or it reflects a conscious decision by the annotator that the instance truly was not covered by the available sense inventory for the lemma. The use of the consistency checking software mentioned in Sect. 2.5.1 should, however, have minimized the number of overlooked instances.

Although the longer chapter 14 has more frame instances, there were about the same number of different frame types, a bit more than 200, in both chapters (see Tables 3 , 4 ). For lemma types, there is a larger difference between the two chapters (Tables 5 , 6 ). Both for frames and lemmas, the annotators vary quite a bit in the number of annotated instances, with annotator A1 usually being numerically closest to the the gold standard and annotator A3 being farthest. On average, each lemma type occurs 1.8 times.

Table 7 lists the number of lemmas with a given number of listed senses. For instance, of the lemmas that occur exclusively in Chapter 13, 67 are not actually in FrameNet but were assigned to frames by the annotators anyway. 13 217 of the lemmas that occur only in Chapter 13 have only one sense according to FrameNet, and 135 lemmas have more than one sense.

Given the nature of our annotation task and data X  X e have no pre-defined set of instances to annotate and cannot assume that the available categories for a given predicate are exhaustive X  X e cannot use a measure such as kappa. We therefore report precision and recall for frame and frame element assignment for pairs of annotators on chapters 13 and 14 of the Hound. We present the numbers only once for each pair. However, the missing combination, with system and gold annotator switched, can easily be derived since one person X  X  recall is the other person X  X  precision. The results are shown in Tables 8 and 9 . The first two columns identify a pair of annotators as the gold standard and the system for the comparison at hand. Columns three and four show the precision and recall values for frame assignment for the pair. Columns five and six report precision and recall for frame element assignment. For the latter, we require only overlap for a match rather than identical boundaries. Also, frame element agreement is calculated only over frames that the annotators agreed on.

The precision of individual annotators for frame assignment against the adjudicated gold standard lies between 0.772 and 0.832, while recall ranges from 0.654 to 0.803. For FEs, both values are higher. Precision ranges between 0.800 and 0.912 and recall between 0.740 and 0.907. Given that the gold standard for the two chapters features 228 and 229 different frame types, respectively, and that our annotators had no prior experience with full-text annotation, this level of agreement seems quite good. One factor that may have depressed the recall of FEs slightly is that for SemEval our annotators had to label core and non-core FEs, while in the context of their regular work on the German SALSA-project they did not have to label non-core FEs. Our annotators missed relatively many instances of FEs like T
IME for verbal frames and also often missed or confused non-core FEs of noun-evoked frames, of which there were many instances in the data. Also, one annotator very consistently mixed up certain pairs of  X  X  X umbered X  X  FEs (e.g. P ARTNER _ 1 and P
ARTNER _2 in the Personal_relationship frame) which are not very intuitively named. The fact that our annotators were not native speakers did not seem to cause any major problems. Most of the differences among the annotators, and between them and the adjudicated gold standard, can also be observed with native annotators.
Since the labeling of null instantiations was of particular concern for our task, we give a more detailed breakdown for the agreement on null instantiations in Tables 10 and 11 . 14 We report separate numbers for instances of INI, DNI, and for their merged super-category NI. As in the general case of frame element agreement, these numbers are calculated over instances where frames matched.

Given that the annotators had no prior experience in annotating null instanti-ations, recall and precision for null instantiations in general (NI) was acceptable. The annotators did, however, do less well on the specific subcategories, DNI and INI. Mismatches came about in four main ways. First, if no element of a CoreSet is realized explicitly, the annotators may have disagreed over which of the possible FEs to label. Second, the annotators may have determined the same FE to be null instantiated but then disagreed on the interpretation of the missing element. This is a frequent source of a disagreement. However, inspection of the particular FEs where such disagreement occurs does not yield a ready analysis of what makes certain frames and/or FEs harder to deal with. A third common reason for disagreement are cases where one annotator treated a particular FE as null instantiated while the other did not. The non-treatment of an omission may just represent an oversight (despite the use of a consistency checking tool), or it might be a reflection of uncertainty over what the right type of NI to annotate might be, with the annotator holding off on committing to an annotation but never revisiting the token. Unfortunately, these two cases cannot be distinguished. Fourth, in another important set of cases, the annotators did not seem to recognize that no null instantiation needed to be assumed and that an explicit mention was syntactically  X  X  X vailable X  X . For instance, in sentence 193 of chapter 13 (example (10) below), the frame Intentionally_act is evoked by do but one annotator did not recognize the pro-form so as an annotatable referent and instead treated the A CT FE as null instantiated. Another fairly typical case of unnecessary NI annotation are so-called incorporated FEs. For instance, the FE A GE in the People_by_age frame, evoked by lexical units such as geezer or neonate , can never be realized as dependents of (the vast majority) of a frame X  X  lexical units: FrameNet annotation policy is not to mark such FEs as NI. Our annotators did, however, frequently mark such FEs as NI anyway, thereby incurring false positives against the gold standard. (10) And if you can do so --!
To provide an indication of the upper bound for performance on annotating null instantiations, we briefly discuss the agreement in NI annotation obtained by two experts performing a corpus linguistic study (among them one of the authors).
The task of these expert annotators is as follows. For a particular lemma, they are given a series of instances randomly chosen from the BNC (with 5 sentences of prior context). Of these instances, they are to select the first 20 that evoke a particular frame and in which a particular pre-specified frame element is missing. For these 20 instances, they are to assign the interpretation type of the missing frame element in context. The task does not include finding antecedents. The annotators performed this task for 5 lemmas each in 2 different frames, for a total of 200 instances. 15 Comparing both annotators to a subsequently jointly adjudicated gold standard, the annotators have perfect precision for detecting instances of the missing FEs but on average recall is only 0.875. 16 Precision was also perfect for assigning the interpretation type to the instances. Overall, this suggests that at least under conditions where annotators can focus on a particular FE in a particular frame, very high levels of agreement for uninstantiated FEs and their interpretation type are possible.
 Finally, we note that we have nothing to say about the quality of the PropBank/ NomBank-data which was automatically derived from the FrameNet version. We were able to evaluate, at least to some degree, the quality of the type-level mappings via SemLink between entries in FrameNet and entries in PropBank and NomBank. Since the PropBank/NomBank-data is generated automatically by converting the FrameNet-style adjudicated gold standard, there is nothing to say about inter-annotator agreement of PropBank-annotations. Unfortunately, we lacked the resources or expertise to evaluate the generated annotations on the token-level as to their quality or usefulness within the PropBank framework. Since there were no participants for the PropBank version of the SemEval task, we also did not receive any feedback on that point from researchers who might have inspected our PropBank-style training data more closely. Nevertheless, we make the PropBank/ NomBank versions available in the hope that they will be of use to the PropBank community, maybe after hand validation or expansion with additional annotations. 3 Tackling the null instantiation task Our corpus was created in the context of the SemEval-2010 Shared Task on  X  X  X inking Events and Their Participants in Discourse X  X . In this section, we discuss the task itself, its challenges and the performance of the systems participating in it. 3.1 The task We gave the participants the option of tackling two different tasks: In the NI-only task , only the NIs had to be identified and resolved; the gold standard semantic argument structures were given. In the full-task , the semantic argument structures had to be inferred, too. However, the correct semantic frame was provided for each target predicate. The decision to also offer a full task was motivated by the fact that we wanted to give the participants the opportunity to develop enhanced semantic role labeling systems, which could carry out both tasks jointly. Table 12 summarizes the information provided in the test set for the two tasks. For both tasks the training set was fully annotated with gold standard semantic argument structure and gold standard NI information. Note that we did not release gold standard coreference information; we used this only in the evaluations (see below). We also gave the participants a choice of working with FrameNet or PropBank data. All of them decided on the former, though.

We evaluated both the role recognition and labeling (for systems participating in the full task) and the null instantiation linking. For role recognition, we computed the accuracy with respect to the gold standard. For role labeling, we calculated precision, recall, and F-Score.

As null instantiation linking is a new task, we had to introduce a novel evaluation measure. In the gold standard, we identified antecedents for NIs. In some cases, more than one antecedent might be appropriate, e.g., because the omitted argument refers to an entity that is mentioned multiple times. In this case, a system should be given credit if the NI is linked to any of these expressions. To achieve this we annotated coreference chains in order to create equivalence sets for the referents of NIs. If the null instantiation was linked to any item in the equivalence set, the link was counted as a true positive. We then defined NI linking precision as the number of all true positive links divided by the number of links made by a system, and NI linking recall as the number of true positive links divided by the number of links between a null instantiation and its equivalence set in the gold standard. NI linking F-Score is then the harmonic mean between NI linking precision and recall.

Since it is sometimes difficult to determine the correct extent of the antecedent of an NI, we scored an automatic annotation as correct if it included the head of the gold standard filler in the predicted filler. However, in order to penalize systems which link NIs to excessively large text spans to maximize the likelihood of linking to a correct antecedent, we introduced a second evaluation measure, which computes the overlap (Dice coefficient) between the words in the predicted filler (P) of a null instantiation and the words in the gold standard one (G): Example (12) illustrates this point. The verb won in the second sentence evokes the Finish_competition frame whose COMPETITION role is null instantiated. From the context it is clear that the competition role is semantically filled by their first TV debate (head: debate ) and last night X  X  debate (head: debate ) in the previous sentences. These two expressions make up the equivalence set for the COMPETITION role in the last sentence. Any system predicting a linkage to a filler that covers the head of either of these two expressions would score a true positive. However, a system that linked to last night X  X  debate would have an NI linking overlap of 1 [i.e., 2*3/(3 ? 3)] while a system linking the whole second sentence Last night X  X  debate was eagerly anticipated to the NI would have an NI linking overlap of 0.67 [i.e., 2*3/(6 ? 3)] (12) US presidential rivals Republican John McCain and Democrat Barack Obama 3.2 System descriptions While a fair number of people expressed an interest in the task and 26 groups or individuals downloaded the data sets, only three groups submitted results for evaluation. Feedback from the teams that downloaded the data suggests that this was due to coinciding deadlines and the difficulty and novelty of the task. Only the SEMAFOR (Chen et al. 2010 ) group addressed the full task, using a pipeline of argument recognition followed by NI identification and resolution. Two groups (
VENSES ?? (Tonelli and Delmonte 2010 ) and SEMAFOR ) tackled the NI only task. The final participating system, CLR (Litkowski 2010 ), did not tackle NI resolution at all and instead only performed semantic role labeling for overtly realized FEs, which was not a recognized sub-task of the shared task. Since the present paper focuses on NI resolution, we will not say more about CLR here. Both VENSES ?? and SEMAFOR represent existing systems for semantic processing that were modified for the task. The fact that no group built a system from scratch is additional evidence for the task X  X  complexity. Table 13 provides an overview of the properties of the participating NI-resolution systems and two further systems that have been published since.

VENSES ?? employed relatively deep semantic processing. The system first applies cascaded Finite State Automata in combination with different lexicons and an anaphora resolution module to produce semantic predicate argument structures (PAS). These are then mapped to the corresponding gold standard frame semantic argument structures as given in the test set (see Delmonte ( 2008 ) for details). For the NI identification and resolution, the system uses two modules: one for verbal and one for nominal predicates. For verbal predicates the system first tries to classify a potential NI as DNI or INI by checking whether the FE in question is null instantiated in the FrameNet corpus and, if so, what its interpretation is. If the FE has occurred both as a DNI and and INI in FrameNet, VENSES ?? employs heuristics to determine whether the target FE in the test set is constructionally licensed and X  X f it is X  X hooses the implied interpretation (e.g., INI for passive constructions). For DNIs the system then tries to find an antecedent by searching for predicates in the local context which are (1) semantically related (via a WordNet link) to the target (frame evoking) predicate and (2) share at least one argument slot with the target. If a matching predicate can be found, VENSES ?? determines if one of the FEs is semantically related to the null instantiated FE, with semantic relatedness being computed between the overtly realized FE and the heads of all arguments that fill the null instantiated FE when it is overtly realized in FrameNet. For nominal predicates VENSES ?? does not attempt an upfront DNI versus INI classification; instead NIs of nominal predicates are immediately resolved to their antecedents by finding NPs in the context which are semantically related to the fillers of the target FE in FrameNet. Semantic relatedness is computed using ConceptNet (Liu and Singh 2004 ). If a relation between the target FE and the head of an NP in the context can be found, the system then checks whether the latter occurs as a filler of the target FE in FrameNet and if it does, it resolves the target FE to the NP.

SEMAFOR used a shallower, probabilistic system. As mentioned above the SEMAFOR group tackled both the full task and the NI only task. For the semantic role labeling step of the full task, SEMAFOR uses supervised machine learning. The system is trained on the SemEval 2010 training data and employs features similar to those commonly used in semantic role labeling (the syntactic dependency parse, voice, word overlap, part-of-speech information) (see Das et al. 2010 for details). SEMAFOR does not attempt to explicitly classify NIs into DNI and INI. Instead it directly tries to resolve each identified NI. To do so it first identifies a set of candidate fillers, containing all nouns, pronouns and NPs from the previous three sentences. Then supervised machine learning is used to choose a filler for the NI. The model is similar to the one used to find fillers for overtly realized FE fillers. However, the feature set is slightly different. Instead of the syntactic path, the system computes (distributional) semantic similarity and lexical identity between a potential filler and the fillers of the target FE in FrameNet X  X  lexicographic data and uses these as features. However, these two features were found to have a negligible effect on the results. In addition, the distance (i.e., number of sentences) between the potential filler and the NI is encoded as a feature.

Since the completion of the shared task, two further systems made use of the shared task data. For completeness, we describe them here as well. Tonelli and Delmonte ( 2011 ) proposed a variant of the VENSES ?? system. In the new system NIs are resolved by computing for each potential filler f (all nominal heads in a window of five sentences either side of the target NI) a relevance score, which takes into account the number of times f has been observed as a filler of the target FE in the training set divided by the distance (in sentences) between f and the target FE in the test set. Like Tonelli and Delmonte ( 2010 , 2011 ), Ruppenhofer et al. ( 2011 ) split the task into three subtask: NI identification, interpretation classification, and NI resolution. NI identification is performed by devision heuristics that take into account FE relations similar to Tonelli and Delmonte ( 2011 ). To distinguish between DNIs and INIs, the system first determines whether the NI is construc-tionally licensed and then assigns the interpretation associated with that construction (e.g., INI for passives). If no NI licensing construction can be found, the DNI versus INI decision is based on statistics collected from FrameNet. For NI resolution, the system proposed by Ruppenhofer et al. ( 2011 ) deviates from all other systems by trying to resolve a DNI not to individual constituents in the context but to coreference chains (using the gold standard coreference chains in the shared task data). The motivation is that coreference chains provide more information about a referent than individual mentions. The decision for a specific coreference chain as a filler of the NI is based on semantic similarity between the referent of the chain and the target FE. To model semantic similarity, the system determines whether the semantic type of the target FE is shared by one or more elements in the coreference chain. The semantic type is taken from FrameNet (e.g., Sentient ) or X  X f no type is given in FrameNet X  X s inferred via heuristics, which map FE fillers (or heads of the elements in the chain) to FrameNet semantic types via WordNet. If several chains match the target type, the chain whose latest element is closest to the target is preferred.

Table 14 shows the results for the participating systems. While we did not officially evaluate the performance on the NI identification and interpretation (DNI vs. INI) subtasks, we include the accuracies for these two subtasks here as part of the analysis. This is motivated by the fact that the results for the whole pipeline (NI resolution) were generally very low (around 1 % F-Score). Due to this fact, we also dispense with computing the NI linking overlap. The low results for the overall pipeline underline the difficulty of the task (see the discussion below). Both systems showed comparable performance, except on the NI identification task, where SEMAFOR did better. The difference can be explained by the fact that the systems used different heuristics for this step. While the results for the overall pipeline were very low for the participating systems, Tonelli and Delmonte ( 2011 ) were able to improve the NI resolution F-Score to 8 % and Ruppenhofer et al. ( 2011 ) report that their system reduced the resolution error by 14 % compared to SEMAFOR . However, in absolute terms the number of correctly resolved NIs is still so low that it seems risky to undertake a detailed analysis of why one system performs better than another. Accordingly, we choose to focus on some of the difficulties of the task in what follows. 3.3 Challenges and difficulties of the task Tackling the resolution of NIs proves to be a difficult problem due to a variety of factors. First, the NI resolution task was a completely new task in the SemEval context. Prior to the SemEval task for which the corpus was created there was no annotated full-text training data available that contained all the kinds of information that is relevant to the task, namely overt FEs, null instantiated FEs, resolutions of null instantiations, and coreference. Even with the corpus in place, the amount of data available is very small. Moreover, the data we annotated also represents a switch to a new domain compared to existing FrameNet full-text annotation, which predominantly comes from newspapers, travel guides, and the nuclear proliferation domain. Accordingly, the most frequent frames in our data are different from the ones in FrameNet full-text annotation, as shown by Table 15 .

Second, solving the task involves making a series of interdependent decisions: (1) which, if any, of the core frame elements are missing; (2) the interpretation type of the missing element (DNI vs. INI); (3) in the case of DNIs, the detection of an antecedent mention of the unexpressed referent. All of these decisions turn out to be difficult for several reasons. Regarding the first decision, it was not well understood at the beginning of the task that, in certain cases, FrameNet X  X  null instantiation annotations for a given FE cannot be treated in isolation of the annotations of other FEs. Specifically, null instantiation annotations interact with the set of relations between core FEs that FrameNet uses in its analyses. For example, FrameNet uses a so-called Excludes-relation to specify mutual exclusiveness between the FEs of a given frame. If two frame elements are in an Excludes-relation the presence of one frame element prevents the occurrence of the other. The latter is then not treated as null instantiated. A typical case of an Excludes relation involves frames with predicates that can have either a symmetric or an asymmetric construal of the relationship between multiple participants. For instance, the Similarity frame involves multiple entities that are being compared; they can be either expressed as distinct elements in separate syntactic positions, as shown in (13), or they can be referenced as a single set, as in (14). The FE E NTITY _1 excludes the FE E NTITIES (and vice versa) and likewise does the FE E NTITY _2 exclude the FE E NTITIES (and vice versa). (13) [A mulberry E NTITY _1] is very SIMILAR in shape [to a loganberry E NTITY _2]. (14) [Mulberries and loganberries E NTITIES ] are SIMILAR .
 While all three frame elements have core status in FrameNet, the absence of E
NTITIES from sentences with the asymmetric construal such as (13) or the absence of E NTITY _1 and E NTITY _2 from sentences with symmetric construal such as (14) does not result in null instantiation annotations because these elements could not possibly be co-present to begin with. Automatic systems that are not aware of the semantics of the Excludes relation and of FrameNet X  X  annotation practices may erroneously posit null instantiations in sentences like (13) and (14), thereby creating false positives. It should be stressed that this is not an arbitrary annotation decision but an inherent linguistic property of (some) predicate argument structures (as defined by FrameNet), i.e. some FEs are inherently exclusive and omitting one of them is not a case of null instantiation. A similar kind of problem arises with the CoreSet frame element relation type. This relation type specifies that from a set of core FEs at least one must be instantiated overtly, though more of them can be. As long as one of the FEs in the set is expressed overtly, null instantiation is not annotated for the other FEs in the set. For instance, in the Statement frame, the two FEs T OPIC and M ESSAGE are in one CoreSet and the two FEs S PEAKER and M
EDIUM are in another. If a frame instance occurs with an overt S PEAKER and an overt T OPIC , the M EDIUM and M ESSAGE FEs are not marked as null instantiated. Automatic systems that treat each core FE separately, may propose DNI annotations for M EDIUM and M ESSAGE , resulting in false positives.

The second step, deciding on the interpretation type of a missing element, is not trivial because a given FE is not always omitted with the same interpretation. For instance, the FE C ONTENT of the Awareness frame evoked by know is interpreted as indefinite in the blog headline in (15) but as definite in a discourse like (16). (15) More babbling about what it means to know . (16) Don X  X  tell me you didn X  X  know !
In cases like (15) and (16), the interpretational difference correlates with a difference in what enables the omission: in (15) it is the genericity of the context that allows the omission with existential interpretation, while in (16) it is the lexical unit know itself that licenses the anaphoric omission in an episodic context. The interpretational difference can thus be tied to a difference in licensing. However, knowing that does not directly help one solve the problem because in English  X  X  X here does not appear to be any grammatical marking of the generic-episodic distinction X  X  (Michaelis 2006 ).

In an analysis after the conclusion of the task, we also discovered an additional complication that makes settling on the interpretation type of a missing element difficult. As mentioned in our 2009 task description paper (Ruppenhofer et al. 2009 ), we had foreseen that an automatic system could use FrameNet X  X  lexicographic annotation data in deciding whether a missing argument was interpreted anaphorically and thus might have an antecedent in the discourse, or existentially and thus lack one. The idea was to either inspect the annotations of a specific lexical unit (LU) (i.e., a frame-lemma pair) or to build aggregate frame-level statistics on the majority interpretation type of missing instances of each frame element. For instance, in FrameNet X  X  annotated data, when the G OAL frame element is missing with the verb arrive in the Arriving frame, it is always interpreted as DNI, never as INI. Therefore, if a system encountered an instance of arrive in the test data, it should make the bet that the G OAL FE was missing with an anaphoric (DNI) interpretation. The same regularity also holds for the other lexical units with annotations in the Arriving frame. Based on this second observation, a system should predict that a lexical unit in the Arriving frame such as the noun influx for which FrameNet provides no annotations at all will still behave the same way when it has the G OAL FE missing, i.e. the frame element will be interpreted anaphorically. Now, while we believe that the above heuristics are useful in principle, in practice it turns out that applying them may give the wrong result. 17 This happens because FrameNet may for instance have annotated only very few instances for the lexical units of a particular frame. If one of the annotated lexical units happens to be  X  X  X eviant X  X  from the other  X  X  X ell-behaved X  X  LUs; if special constructions occur disproportionately often in the annotated data; or if annotators make mistakes, then the annotations may lead the system to come to an incorrect conclusion on the interpretation type for a missing FE. As an example, consider the frame element B
ENEFITTED PARTY in the Assistance frame. 99 instances of the FE are overtly realized, 7 are annotated as DNI and 9 as INI. Thus, by a very small margin, we are erroneously prevented from treating the FE B ENEFITTED PARTY of the Assistance frame evoked by help in (17) as anaphorically omitted. The fact that such statistics can be unreliable is a direct consequence of FrameNet X  X  annotation strategy which is lexicographically driven; most of FrameNet X  X  annotation proceeded in a frame-by-frame fashion with the aim of providing example sentences for each usage rather than providing an annotated corpus with accurate statistics for training NLP systems. (17) Sufficient for me to share the sport and lend my humble help to the capture
In the worst cases for our heuristics, no instances of a frame are annotated at all, or, if instances of the frame are annotated, missing frame elements are not accounted for in the annotation. Under those circumstances, we are reduced to guessing.

Finally, the third subtask (NI resolution) requires computing the semantic similarity between a potential filler and the target FE. While this task is in principle similar to the role labeling task in standard semantic role labeling (SRL), it is made more difficult for NIs because the decision has to be made largely on the basis of semantics. SRL systems typically employ a mixture of syntactic and semantic feature to label arguments with FEs, but in the case of NIs syntax is of limited use. The only syntactic information that might be of use is the grammatical form of a potential antecedent. For instance the MESSAGE role can be filled by complete sentences, while the COGNIZER role tends to be resolved to NPs. All systems modeled semantic similarity to resolve NIs but with limited success. The VENSES ?? group mentioned in their system description that it is difficult to learn the semantics of a frame element based on its annotated instances because many FEs X  X he authors give the example of C HARGES in crime-related frames X  X ave very diverse fillers. The SEMAFOR group likewise found that their features encoding distributional similarity between possible fillers and known fillers from FrameNet X  X  lexicographic annota-tions proved to be negligible. This consistent result may suggest that in finding possible antecedents one should give a greater role to the predicates of which the antecedents are themselves arguments. 4 Related work A line of research that is clearly related to the goals of our shared task is the work on zero pronoun resolution carried out for pro-drop languages such as Japanese or Spanish. Iida et al. ( 2007 ) point out the relevance of the semantic role labeling and zero-anaphora resolution tasks to each other and study how methods used in one can help in the other. Nevertheless, their work is different from ours in two respects. Most importantly, it has a different coverage. Of the kinds of omissions that we consider to be null instantiations, Iida et al. ( 2007 ) target only what we call constructionally licensed omissions. In addition, they seem to treat cases of co-instantiation or argument sharing X  X or instance across conjoined VPs X  X s argument omission, which is not how similar cases are treated in our FrameNet-style annotations. Further, in their implemented system Iida et al. ( 2007 ) use only syntactic patterns but no semantic information about the semantic class ( &amp; frame) of the predicate missing an argument or about the interconnections between the predicate missing an argument and the predicate(s) where coreferent mentions of the missing argument appear. Palomar et al. ( 2001 ) also rely on syntactic rather than semantic information in their work on Spanish which only focuses on construc-tionally licensed subject omissions.

Campbell ( 2004 ) and Gabbard et al. ( 2006 ) work on recovering empty syntactic categories, and in the case of the latter authors also function tags, in the Penn Treebank, using rule-based and statistical approaches, respectively. Gabbard et al. ( 2006 ) modify the Collins parser with the goal of, on the one hand, decreasing annotator effort when creating new English Penn-style treebanks and, on the other hand, aiding the accurate recovery of semantic structure. These authors X  interest in empty categories thus stems from a similar interest in supporting adequate semantic analysis as ours. However, the phenomena that they and Campbell ( 2004 ) tackle are complementary to what we are working on. Both papers focus on non-local dependencies which in generative-derived syntactic theories result in empty categories. For instance, for sentence (18) the analysis to be recovered is given in (18a) [cf. Gabbard et al. ( 2006 , p.185)]. (18) The dragon I am trying to slay is green.
FrameNet-style semantic analysis, by contrast, assumes as its (implicit) background a monostratal theory of syntax along the lines of Construction Grammar or Sign-Based Construction Grammar (Michaelis 2010 ), where no empty categories are employed and, therefore, none need be recovered. As (18b) and (18c) show, the non-local (parts of) Frame elements are simply annotated directly by FrameNet without any use of a book-keeping mechanism registering the relevance of e.g. the control predicate try for the realization of slay  X  X  K ILLER argument. Thus, since our point of departure are FrameNet X  X  annotations, our work on recovering antecedents of arguments that are not locally instantiated is not concerned e.g. with cases of displacement by control-predicates or constructions like wh -questions; sentence (18) exhibits no null instantiation in our sense. The antecedents that we are interested in are not in any kind of direct or mediated syntactic construction with the predicates with implicit mentions to coreferents. This group of cases is not within the purview of Gabbard et al. X  X  ( 2006 ) work.

Most closely related to our work is the work by Gerber and Chai ( 2010 ) who present a study of implicit arguments for a group of frequent nominal predicates. They show that implicit arguments are pervasive for these predicates, adding 65 % to the coverage of overtly instantiated roles in NomBank. The differences with our work are the following. Gerber and Chai ( 2010 ) work exclusively with NomBank/ PropBank roles and focus on 10 nominal predicates from the business domain, while we mainly work in the FrameNet paradigm, treat many different general-language predicates of all parts of speech, and use narrative data. Another important difference is that they level the distinction between anaphoric versus existential interpretation. Further, Gerber and Chai ( 2010 ) created extensive annotations for their 10 predicates to train a supervised system on, whereas we performed full-text annotation and consequently have relatively sparse data. This makes it much harder to obtain good results on our data. Specifically, our task is best modeled as a semi-supervised task which combines the training data with other resources in which null instantiations are not annotated, such as the FrameNet data or unannotated corpora, e.g., for computing semantic similarities between potential argument fillers. While our task is harder, we also believe it is more realistic. Given the complexity of annotating semantic argument structures in general and null instantiations in particular, it seems infeasible to annotate large corpora with the required information. Hence, automated systems will always have to make do with scarce resources. A final difference between Gerber and Chai ( 2010 ) and our work is that the former evaluate against coreference chains automatically created by OpenNLP while we use human-annotated coreference chains. 5 Conclusion In this paper, we discussed the task of identifying and automatically resolving null instantiations, which we organized for SemEval 2010 (Task-10). Our motivation for organizing this task was to add a discourse dimension to the traditional semantic role labeling paradigm, which considers only arguments that are overtly realized in a fairly local context and thus misses potentially important information. Identifying and resolving null instantiations is not only a novel task, it is also a challenging endeavor that requires deep semantic processing.

Moreover, up until the task no annotated resources had been available. We addressed this problem by compiling a new corpus of narrative texts that is annotated with semantic predicate argument structure (FrameNet and PropBank), null instantiation information (interpretation type and NI resolution), and corefer-ence. This corpus is potentially useful for a number of NLP, and possibly also interdependencies between discourse and semantics. A smaller version of the corpus (excluding coreference) was made available to task participants. We are currently preparing the first public release of the complete corpus. We intend to extend the corpus in the future and would welcome other researchers providing further annotations, e.g., sentiment.

However, even with this resource the task of identifying and resolving null instantiations remains challenging. One main problem is data sparseness. Our corpus is relatively small (23,000 words) and given the inherent complexity of annotating discourse-level information, it is unlikely that a significantly larger resource will be available in the near future. The task is thus best viewed as a semi-supervised task, complementing annotated data with various other resources (e.g., raw texts and texts annotated with semantic argument structure but not with NIs). The second difficulty lies in the inherent complexity of the task. Identifying fillers for null instantiations requires deep semantic processing and possibly even inference. This presupposes a sophisticated semantic model that also takes into account discourse context.
 References
