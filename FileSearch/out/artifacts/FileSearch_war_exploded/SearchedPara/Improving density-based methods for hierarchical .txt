 1. Introduction
Data clustering is an important task in data mining that can provide useful and valuable results. This task has great sig-nificance on the web, where it can be used for improving search engine results and enhancing web crawling operations, as well as for organization and management of domain knowledge [1,4,11,15] . The information explosion on the World Wide
Web has made it difficult to obtain required information. One way to solve this problem is to cluster web pages [16] . The goal of clustering is to create groups of data objects in an unsupervised fashion so that data items in the same cluster are similar to each other, yet dissimilar to data items residing in other clusters.
 Various methods have been developed for data clustering [17,39] , but some have not been applied to data on the web.
One important category is density-based algorithms (first introduced in [13] ), which thus far have only examined spatial data sets. The main idea of these algorithms is that a random data item is first selected, and its neighborhood distance is investigated to determine whether it has an acceptable number of the data points (as a dense unit). If it cannot satisfy this condition, it is labeled as noise. Then, according to various criteria, finding the remaining dense units continues until the clustering process is completed. 1.1. Motivation
One method of analyzing the web is to cluster its contents during several periods and evaluate the results. As is known, web pages change dynamically. If we cluster and organize web data into structures periodically, we can analyze these struc-tures (clusters) and their changes, and by this means analyze web changes and transitions. On the other hand, since the rate of change is rapid, clustering may be needed at shorter periods, and therefore developing fast and effective clustering algo-rithms is necessary. Density-based methods are fast and high quality algorithms that have been developed for clustering spa-tial data. They provide advantages such as removing noise and creating arbitrarily shaped clusters that are interesting for environments like the web, since the web may have the largest proportion of noisy data among all real data sets.
In addition, density-based methods are also notable as scalable algorithms [13] . The web contains a huge number of pages, requiring a clustering method to have good scalability properties. With respect to the incremental nature of web growth, this requirement seems to be necessary.
 Nevertheless, density-based methods have shortcomings that possibly make them insufficient for clustering web data.
These methods create clusters locally at only one level, while it is advantageous for data sets like the web to have hierarchi-cal clusters.

In this paper, we attempt to improve and adapt density-based methods for the web environment. The main attention is focused on hierarchical clustering. As will be explained, web clustering research mainly focuses on two things: the clustering algorithm and the distance measure specified for the web. Here, we intend to examine the usability of density-based meth-ods for hierarchical clustering of web pages; therefore, we mainly concentrate on the algorithmic details. Defining the best measure for web clustering is not a globally agreed-upon problem, and also each of the previously developed measures can be easily used in corporation with the proposed method.

To provide a hierarchical clustering, we need to apply a global ordering on an extension of the dense units. To obtain the global ordering, we suggest two distinct steps for the density-based method: insertion, in which the neighborhood regions are created and the entire distances are ordered, and extraction, which combines the distances to construct the base clusters.
However, in many situations constructing the base clusters may not be sufficient. These steps limit the extent of the hier-archy as the extent of the stored distances. As we will show by experiments, this is not sufficient. We propose another step to complete the hierarchy and structure a more complete hierarchy, i.e., a combination step. 1.2. Contribution
In this paper, we examine the use of density-based approaches for clustering web pages. We improve the existing meth-ods to create hierarchical clusters. To complete the hierarchy, we establish some relationships between K -Means [24] , den-sity-based, and AHC [10,44] algorithms as complements of each other. K -Means is used to reduce the number of parameters needed in the density-based methods. Two AHC methods, including average linkage and single linkage, are im-proved such that they can be performed with O( n log n ) time complexity, and then are combined with the density-based methods.

The method at first finds the dense units of the entire data set and then combines them to create the final clusters. These dense units are constructed using the expansion of K -Means, which reduces the number of required parameters. After con-structing the dense units, existing distances are stored into a structure such as min heap. These stored distances are then extracted one by one, and their effects on the clustering are examined. At the end, we will have the base clusters, and it re-mains to complete the hierarchy. For this, we improve the hierarchical single and average linkage methods to be adapted with the density-based methods. The main improvement is reducing the time complexity of the methods to O( n log n ). Link linkage methods have the complexity of O( n 2 ), while the proposed methods take advantage of the lower time complexity. Moreover, using the dense units enables the method to be resistant to noise and can create clusters with various shapes.
For combination thresholds, we define a new measure that is sensitive to the newly extracted distance. This new measure gets smaller values for dense clusters while using larger values for bigger clusters. As experimental results show, these enhancements have considerable effects on the improvement of the quality of the resultant clusters.

The remainder of the paper is organized as follows. In Section 2 , we will describe previous research associated to web data clustering. Section 3 contains the complete description of the proposed method and its time complexity analysis. The exper-imental results of the method are evaluated in Section 4 , and the conclusion is given in Section 5 . 2. Related works
Many clustering algorithms have been presented in Refs. [17,39] , including K -Means [24] , agglomerative hierarchical clustering [10] , graph partitioning [40] and spectral clustering [15] , density-based methods [13] , grid-based methods, and model-based methods. So far, agglomerative hierarchical clustering (AHC), K -Means, and some graph partitioning methods have been employed for most web clustering applications. Among these algorithms, AHC is a well-known hierarchical meth-od that combines the smaller data items to create larger clusters. An AHC algorithm starts with each web page in a single cluster and merges the two most similar clusters iteratively until a halting criterion is satisfied. The measure used for com-bination can be the nearest, the average, and the farthest distances, and accordingly these methods can be classified into determined constant [25] . Because of sensitivity of AHC to the halting criterion, the algorithm may mistakenly merge two good clusters. Although hierarchical methods are often said to have better quality clustering results [17] , they usually do not reallocate the pages that may have been poorly clustered in the early stages of the process [44] . Moreover, the time com-plexity of hierarchical methods is quadratic [32] .

Partitioning methods try to partition a data set into a set of groups in order to maximize a pre-defined fitness value. The clusters may or may not be overlapped. Due to computational considerations, it has been shown that partitioning methods are well suited for clustering a large document dataset [32,42] . The best known partitioning algorithm is K -Means [24] , which simply selects K initial cluster centers and assigns each data point to the nearest center. The updating and reassigning process is continued until a convergence criterion is met. This algorithm can be performed on a large data set at almost a linear time complexity. The method converges to a local optimum after a limited number of iterations, so it is said to be lin-ear with respect to the number of the data items [39] . This algorithm has some drawbacks, including selection of appropriate initial centroids, as well as determining their number, convergence to the local optimum, and sensitivity to noisy and out-local optimum is achieved.

To solve these problems and adapt the algorithm for the web environment properties, several enhancements have been proposed. The most important improvements try to exploit optimization approaches for deeply exploring the search space.
These methods usually use different mutation operators to examine more areas from the search space. The first attempts of using the optimization techniques in the clustering area were initiated with genetic algorithms in [30] . Other examples from utilization of the genetic algorithm can be found in [18,20] . Ant clustering and Particle Swarm Optimization (PSO [19] ) have also been used to improve the clustering results [21] . The main problem of the optimization methods is their time complex-ity, which may be increased to the exponential order [39] . This may make them infeasible for web page clustering.
However, for the web page clustering, some previous works have adapted K -Means and AHC algorithms for the web [28,36,38] , and some works have used different techniques to improve them [9,36,37] . In the following, we present some of these works in more detail.

In addition to content, the hyperlink structure of the web may be exploited as a rich resource of useful information. Nev-works concentrated on hyperlink structure alone, more recent work in this area has combined link and content information.
From the viewpoint of the clustering distance measure, the initial works were based on text mining and traditional informa-tion retrieval approaches. Content-based clustering [14,41] , according to the common terms shared among the pages, has been well studied in information retrieval.

Then several works in [4,11] have tried to explore link analysis for improving the quality of web mining operations. While initially most of the works regarded direct hyperlinks between the pages [5,26,28,34,36] , more complicated measures, such as those that use hyperlink transitivity [23,37] , those that incorporate page importance and hyperlink transitivity simulta-neously [16] , or those using the web page structures [8] , were subsequently developed. The next efforts considered a com-bination of link structure and content information. A hierarchical link and content based search engine was proposed in [35] that clusters the hypertext documents to structure a given information space for supporting various services. In [37] , link-based clustering was extended by combining content and link information appearing in the anchor text, snippet, meta-con-tent, and anchor window of the in-links.

As mentioned before, density-based methods are a main category of data clustering methods that have not been applied on web data. Different density-based methods using the expansion criteria (from one dense unit to another) were developed, distance) and MinPts (minimum required data points that must reside inside a dense unit).DBSCAN and OPTICS first find a dense On the other hand, DBRS extends the dense units of several clusters simultaneously.

All of these methods create flat clusters in a single level. DBSCAN was the first density-based method proposed for data clustering [13] . This method proposed new concepts such as dense unit, neighborhood distance, and neighborhood radius. In this algorithm, to create a new cluster or extend an existing cluster, a neighborhood distance with radius Eps must contain at least a minimum number of points denoted by MinPts . This algorithm uses R * -tree [3] data structures to find the neighbor-hood distance of a core point. This data structure provides O(log n ) time complexity for each access, and therefore the total time complexity of DBSCAN is O( n log n ).

DBSCAN first selects a random point q and performs a query to find the neighborhood of q . If the neighborhood is sparsely resident in q  X  X  neighborhood. Then the neighborhood of each neighbor is examined to see whether it can be added to the chooses another unlabelled random point and repeats the process. This procedure is iterated until all points in the dataset have been clustered or labeled as noise.
 Although DBSCAN shows very good results, it has some shortcomings. First, if the clusters have widely varying densities,
DBSCAN is not able to handle them efficiently. Since all neighbors of a core object are checked, much time may be spent in dense clusters examining the neighborhoods of all points. If the data set contains clusters with various densities, DBSCAN selects Eps large enough to cover the sparse areas, too. However, this selection is not optimal for dense regions. For these regions, a large radius causes many data items to be placed inside the neighborhood areas and therefore the complexity increases. It is possible that the number of the data items returned by a query is not a constant value, and therefore the com-plexity may even increase theoretically.

To overcome this problem, OPTICS [2] starts from a random point and after finding its neighborhood, if it was dense, or-ders the neighbors and so applies a kind of priority on the development of the expansible points. When OPTICS finds new dense neighborhoods, the points existing in these regions are also sorted with the previously ordered points, and for the next expansion, one point with the minimum distance is selected. If there is no point for further expansion, a new un-clustered (unlabeled) point is selected randomly and the process continues. Of course, OPTICS does not solve the problem completely. It seems that it is better to consider different radii for neighborhood distances when areas have different densities.
Second, DBSCAN is not suitable for finding approximate clusters in very large datasets. Since DBSCAN and similar algo-rithms such as OPTICS create and complete a cluster fully and then start another cluster, if we intend to stop the clustering process unfinished, we cannot have a suitable approximation from other clusters. For this problem, DBRS [33] was devel-oped, which iteratively picks an un-clustered (unlabeled) data item randomly and checks its neighborhood. If a neighbor-hood is sparsely populated, DBRS labels it as noise. Otherwise, if one or more points from the neighborhood belong to a previously created cluster(s), all points in this neighborhood join that cluster(s) and they altogether form a unique cluster.
Otherwise, a new cluster with the data items of the neighborhood is created. Then, the next data item is selected between the unlabeled and un-clustered points randomly, and in this way DBRS finds several clusters simultaneously (by Random Sampling).

However, DBRS also does not obviate the problem completely. Its samples may not be suitable representatives from the clusters. If we want to have a representative instead of the entire cluster, it is better to construct it from more dense areas that can represent the cluster more precisely.

On the other hand, all the above methods take two parameters as input while setting accurate values for them may be difficult. These problems, along with the new properties required for clustering web data (discussed above), require improv-ing the density-based methods from several aspects. 3. Hierarchical density-based method
In this section we introduce the proposed method in detail. The method consists of three stages: the insertion stage, which finds the neighborhood distances and sorts the extracted distances of the dense units, the extraction stage, which ex-tracts and examines the stored distances and creates initial clusters, and finally the combination stage for completing the hierarchy of the clusters. These steps provide an approach for finding and joining dense units of the whole data set that pro-vides high quality clustering. We also propose a more dynamic method for joining the units and clusters providing a more accurate hierarchy. In the following, we describe each stage in detail. 3.1. Insertion stage
In the insertion stage, we construct the neighborhood distances and sort the extracted distances that are resident in the dense units. At first, all the data points are inserted into an m-tree [6] that provides range and k-nn ( k nearest neighbors) on the basis of their relative distances, as measured by a specific distance function, and stores these objects into fixed-size nodes that correspond to constrained regions of the metric space. Leaf nodes of any m -tree store all indexed data objects, represented by their keys, whereas internal nodes store the so-called routing objects . A routing object is a database object to which a routing role is assigned by a specific promotion algorithm. After insertion of all data objects, we can perform the queries more effectively. The range query Q(o, r ) selects all the data objects such that dist ( o queries can be performed with a time of O(log n ) using an m -tree. m -Trees do this using specific pruning and filtering oper-ations [6] .

After storing all the data points in the m -tree, the neighborhood distance of each data point is found (by an appropriate range query), and the neighborhood is checked to see whether it satisfies the dense conditions or not. In the case that the neighborhood is not dense, another data point is selected randomly and its neighborhood is examined. If the neighborhood satisfies the dense conditions (having more than a specified number of data points), the data points in the neighborhood re-based on the distances they have from the nearest core objects. To keep such ordering, a min heap data structure is used.
The process is repeated for each data point, and while finding a new dense neighborhood distance, all the data points are inserted into the min heap containing previously sorted data points. When a smaller distance is found for a data point (bor-der point), this point and its distance are updated in the min heap. When all the neighborhood regions are examined and their corresponding distances are inserted into the heap, this stage is finished. A more formal description of the steps of the process is depicted in Fig. 1 . In this algorithm, steps 2, 4, and 5 can be performed in O(log n ) time complexity, which causes the total complexity to be O( n log n ).

In the algorithm of Fig. 1 , there are some problems for determining suitable values of the parameter Eps . This problem is inherited originally from the density-based methods. In these methods, the dense unit is defined according to the two parameters MinPts and Eps . Determining suitable values for these parameters is a subjective problem dependent on the no-tion of a single small dense unit. However, it would be better if we can reduce the parameters so that the user can apply his idea more easily. The problem may be aggravated when data has diverse densities, requiring different Eps values at various places. Previous density-based methods consider a definite value of Eps for the entire data set, while we may require smaller values in dense areas and greater values for sparse areas. To solve the problem, we turn to K -Means, which creates spherical clusters, and develop it to construct small spherical units. The method first selects a constant value for centroids denoted by
C . It is a large value proportional to the number of all the data items. If n is the number of the data items, then C can be selected as where MinPts is the minimum number of required data points that must reside in a small region for the region to be con-sidered as a dense unit (similar to the traditional density-based methods). Therefore the definition of a dense unit will be limited to just one parameter (instead of two parameters). In the next step, each data point is assigned to the nearest cen-troid and after all assignments, the centroids are updated according to (1) ( a is the assignment matrix, C ={ c
Means, the convergence condition is satisfied when the position of the centroids does not change. Therefore, at the end of this step we will have numerous spherical dense units that each has a center point representing the distribution of the den-manner, we will have more centroids in dense areas resulting in smaller units. Whatever C is selected as a larger value, we will have more (and finer) units in which the local optimum is limited near the global optimum. Therefore, by selecting C as a constant ratio of n , we will have numerous small units in which the local optimum is located near the global one.
At the end of the process, the radius of each small spherical unit (cluster) gives a good option for determining the value of parameter Eps in the covering area.
The typical K -Means requires a time complexity of O( n ) to find a limited number of cluster centroids, but here the number of the centroids is itself O( n ), so the total time complexity will be O( n We apply some changes in the implementation of the method so that it can be performed with O( n log n ) time complexity.
One main structure used in this extension is the m -tree. The extended process for finding the small spherical units is shown in Fig. 3 . In the algorithm, each assignment step of K -Means is replaced with finding the nearest center from the m -tree, which can be done with O(log n ) time complexity (instead of O( n ) in the typical K -Means). Below, we show how this process can be done in O( n log n ).
 Lemma 1. The above process for finding the small spherical units is performed with O( n log n ) time complexity .
Proof. According to the mentioned process, before updating, each data point passes the following steps: 1. being inserted in the m-tree (with O(log n ) time complexity), 2. executing a 1-nn query (with O(log n ) time complexity), 3. being assigned to the result of the query (with a constant time complexity), 4. being deleted from the m-tree (with O(log n ) time complexity).

Therefore, before each updating, an O( n log n ) time complexity is required. After updating, the process is continued until
Means, the number of iterations until achieving the convergence is limited (the algorithm rapidly converges to the nearest local optimum, so t is a constant). Therefore the total process can be done with O( n log n ) time complexity with respect to n . h 3.2. Extraction stage
So far, we have sorted the suitable distances (distances that can be merged together to construct the initial clusters) and now we need to extract them from the min heap to construct the desired clusters. At each extraction, the extracted distance is examined; if it has intersection with one or more previously constructed cluster(s), they are joined together. During the joining process it is possible that the composite cluster become a more general cluster that should be located at a higher level. This happens when the extracted distance extends a cluster to a more general one or fills the gap between two clusters. and the joined cluster(s). This process enables the algorithm to create the clusters in a hierarchical manner. To define the mutation criterion, we have two choices: 1. Consider a constant value (as a new parameter of the algorithm) for mutation from the current level to the higher level.
This approach is not appropriate, because the mutation criterion cannot inherently be a constant value; it must obtain small values at lower levels and larger values at higher levels. 2. Define a measure and try to optimize it during the extraction process. If a newly added distance does not satisfy the con-dition, a new cluster is created at the higher level. This measure should have following properties: (a) Since the distances are sorted in a min heap and are extracted in order, this measure should be applied locally (global (b) Since each user may want to have the clusters with his particular abstraction, the degree of granularity may have dif-
Since the distances are extracted ascending from small values to larger ones, in each extraction we assume that the monotony may be violated by the newly extracted distance and the previous distances are examined before. Therefore, the measure should be defined in such a way that it has the most sensitivity to the newly extracted distance, and by con-sidering the previously extracted distances can decide whether the new distance can reside in the same cluster or must mi-grate to a higher level.
 With respect to these properties, we define the measure in relation (2) : where Gr is the desired granularity and n is the number of total distances (with the new distance).

Gr is known as the homogeneity or monotony of the clusters and determines whether the distances inside the clusters are nearly alike or the new distance is larger. The monotony means the uniformity of the cluster according to the distances between its members, i.e., each member should have suitable distances with an acceptable number of members of the clus-ter. Gr can be a limited number of values, so a user may perform the clustering with several values of Gr to obtain a desired value.

After extracting a new distance, the clusters that have intersections are found and it is determined whether adding the distance can maintain the monotony. Otherwise, a new cluster should be created at a new level. In fact, after the insertion stage we have a set of graphs that show the relations among the data points of the clusters. These graphs are pruned in the extraction stage and provide the appropriate clusters. A more accurate description of the process is presented in Fig. 4 . The method starts from smaller edges and examines each for three cases: 1. The extracted distance may be surplus. In this situation, both of the ends of the distance are placed in a unique cluster or in two clusters where one cluster is the (direct or indirect) parent of another. In this situation, the extracted distance is considered as useless and is rejected. 2. The extracted distance may not be large enough to cause a change from the current level to a higher one, so it is added to the intersected cluster(s). 3. The extracted edge is large enough to cause a change to a higher level. In this case, we may be faced with several situations: (b) The new distance may have intersections with two clusters and will join them at a higher level.
In the algorithm in Fig. 4 , we have restricted the retrieved clusters to two. As shown in Lemma 2 , this is because we can have at most two clusters that have intersections with the new distance.
 Lemma 2. Each extracted distance has intersections with at most two previously created clusters .

Proof. We know that each extracted distance has two end points and each end point can reside at most in one separate clus-ter. Therefore, the number of clusters that have intersections with the new distance will be at most two. h
In the extraction stage, all of the operations, including finding the intersected clusters and checking the different situa-tions, can be done with a constant time. Reconstructing the heap takes O(log n ), and this causes the time complexity of this stage to be O( n log n ). 3.3. Combination stage
Now that the base clusters are constructed, they must be joined to complete the hierarchy. According to the aforemen-tioned reasons, we define the same measure to be optimized during the completion of the hierarchy.

To measure the distance between two clusters, we can use either the nearest (single linkage) or the average (average link-age) distances. Since using them causes a complexity of O( n
O( n log n ) time complexity. The detailed description of each alternative is explained in following sections. 3.3.1. O(nlogn) version of average linkage method
Fig. 6 shows the different steps for the improved version of the average linkage method. In this algorithm, we first find the nearest center to each center and sort the corresponding distances. We then extract the distances in an ordered manner and tial clusters, we obtain the center of the new cluster using relation (3) : where N i and N j are the number of the members of the clusters i and j , respectively.

Next, we should find the nearest center to the new center. We delete previous centers from the m -tree and insert the new pressed as Influenced centers . The more formal statement of this concept is given in Definition 1 . Definition 1. Center a is influenced by center b if center b is selected as the nearest center to center a . In the algorithm, steps 2, 5, and 6 are performed in O(log n ) and so the total complexity of the corresponding loops will be repeated at most m 1 times ( m is the number of base clusters that is not greater than n ), then the total time complexity of on updateInfluencedCenters .

In fact, updateInfluencedCenters finds those centers (influenced centers) for which one of the combined centers has been selected as the nearest center to them. If we can prove that the number of such influenced centers is limited, then we can hope that updating the influenced centers (finding the new nearest centers) will be done in O(log n ). This issue is proved in
Lemma 5 . According to Lemma 5 , the maximum number of influenced centers is a limited number (e.g., 6). Therefore, we can consider an array and store the influenced centers associated to each (active) center. With this array, the operation update-
InfluencedCenters can be done as follows: 1. For each of the two combined centers, do: (b) For each retrieved influenced center do: According to Lemma 5 , step a can be done with a constant time, and each of the steps i and ii are performed with O(log n ).
Therefore the total time complexity of the process will be O( n log n ). In the following, we prove some lemmas used in the proof of Lemma 5 , and then we prove Lemma 5 .
 Therefore a contradiction occurs that allows the lemma to hold. h data point a 7 that cannot be influenced by center a .

Proof. When more than six data points are placed on the perimeter of a circle (with the same radius), there exist at least two
Fig. 8 a). As depicted in Fig. 8 b, we have a triangle in which for vertex a (according to Lemma 3 ). Therefore, a 7 cannot be influenced by a . h (in other words, for each center, there is a limited number of influenced centers) .

Proof. We prove that at most six influenced data points (centers) can reside on the perimeter of center a . We denote the influenced data points by a i , where i indicates the index of the data point. We prove this by contradiction and show that it is contradictory to have more than six influenced data points.

We denote the nearest point to a by a 1 . Similar to Fig. 9 a, we consider a circle with radius j a a a has more than six influenced data points (for example, seven influenced data points), of which a This is because the assumption that a 1 is the nearest will be violated.
Lemma 3 this situation leads to a contradiction, because in this situation there is a point like a that can be nearer to a j than to a . a j on the perimeter to a data point like a j 1 (as depicted in Fig. 9 d), we will have an isosceles triangle with the base a a j ( Fig. 9 e). According to Lemma 3 , the nearest point to a
From the above discussions, we can conclude that all six influenced data points must reside uniformly on the perimeter of the circle. In the following, we argue about the possible location of the next influenced data point a three possible situations: 1. a 7 cannot reside inside the circle, because in this case the assumption that a 2. a 7 cannot reside on the perimeter of the circle, because according to Lemma 4 and as depicted in Fig. 10 a, if there exist more than six data points on the perimeter of a circle, then there will be at least one data point a enced by the center a . Therefore the assumption that all data points are influenced by a is violated. with vertexes a , a 7 , and a j for which, according to Lemma 3 , point a sible since it violates the assumption.
 Therefore any additional data point such as a 7 cannot be influenced by the center a , and its nearest center is another one.
Thus, we can conclude that the maximum number of centers that are influenced by a center like a is limited (to six). h 3.3.2. O(nlogn) version of the single linkage method
In this section, we propose some enhancements for the single linkage algorithm to be adapted with the density-based methods to obtain O( n log n ) time complexity. The approach is described in the algorithm shown in Fig. 11 . The main idea is that we find some external nearest distances for each base cluster and examine these distances in an ordered manner to complete the clustering hierarchy.

In the algorithm, we first create an m -tree for each cluster. Then for each data point we find some nearest external neigh-bors. The number of these neighbors should be sufficient to complete the hierarchy, and finding additional neighbors is not necessary. These distances are stored into a min heap in an ordered manner. Creating the m -tree structure and finding the nearest external neighbors takes O( n log n ) time complexity (here we may assume that the number of base clusters produced during the previous stages is constant).

In the next step, the distances are extracted one by one and their effects on the hierarchy are examined. These distances time complexity will be O( n log n ). 4. Experimental results
In this section, we set up several experiments to investigate the different aspects of the proposed methods. First, we examine the need for the third phase, i.e., combination stage. We then compare the proposed methods with other den-sity-based methods. We evaluate the quality and the running time of the methods in comparison with similar algorithms, and finally some different aspects of the proposed methods are examined experimentally. 4.1. Role of the combination stage in completing the hierarchy
Here, we illustrate the reasons for supplementing the process with the combination stage. For this, a data set containing 6278 data items was selected, and only the first and the second stages of the algorithm were applied. This data set is created by combining several data sets that had been used in previous studies (i.e., DBSCAN, OPTICS and DBRS).

Fig. 12 shows the resultant clustering after applying the first and the second stages. As is obvious, we have some parent X  X hild relationships in the middle part of the data, but the hierarchy requires more such relationships to be com-pleted. In a worse case, we may have many island base clusters apart from each other. To complete the process, we have two options: 1. We may increase the neighborhood distance ( Eps ) to store and then retrieve the distances that connect the base clus-ters. In the proposed methods, we must select a larger value for MinPts to obtain larger values for Eps . In this way, the number of the cluster centers ( C ) will be reduced, so the spherical units will have greater radii, implying larger values for Eps . This solution has two main problems. First, by increasing Eps , the number of data items resident in the neigh-borhood distances grows rapidly, causing increased complexity in the running time of the algorithm. Thus we cannot claim that the range queries return a constant number of data items. Second, by extending the neighborhood dis-tances the associated regions will cover noisy data as well. Therefore, the ability of the methods to remove the noise data will be reduced and so the quality decreases. An example of a situation in which the above problems occur is depicted in Fig. 13 . The clustering in Fig. 12 was obtained with MinPts = 6, while in the Fig. 13 , a value of MinPts =25 was used.
 2. To overcome the above problems, we proposed an alternative approach. We append a new stage applied on the clustering results of the first and the second stages. According to the former proofs, the third stage completes the hierarchy with the time complexity of O( n log n ) without the need to incorporate larger values for Eps . The role of the third stage is shown in
Fig. 14 . In fact, we have used the density-based methods to create high quality base clusters and the AHC methods to pro-vide the hierarchical clustering.
 Comparison with OPTICS can reveal the hierarchical clustering ability of the proposed method. The results of applying
OPTICS on the data set are shown in Fig. 15 . By comparing the results, we can conclude that the base clusters of both meth-ods are nearly similar. This means that our method utilizes the high quality clustering of the density-based approaches. In addition, clusters of the proposed method are created and grow rapidly, resulting in faster convergence with this method. On the other hand, it is obvious that the algorithms like OPTICS, which scan and cluster the data items locally, cannot create hierarchical clusters. For example, OPTICS at first finds one of the dense regions of the middle and then expands it to cover the other dense regions. It does not select the dense regions first so they can then be combined to construct the hierarchical clusters. 4.2. Clustering by sampling
As argued before, we may intend to cluster data at short periods. Because of the huge amount of web data, it may be not possible to cluster the entire data. On the other hand, there may be situations in which we are restricted by processing
If the algorithm can find better representatives from the clusters, it is considered a more suitable method for clustering by sampling [27] . 2 In this section we investigate the different density-based methods as well as the proposed method from this from one cluster and follows it until completed. DBRS selects some random regions, and if they have any intersections joins them with each other. Thus, these methods cannot produce acceptable representatives from the whole data set. On the other fore selects them as the cluster representatives. 4.3. Experiments on web data
In this section, we perform some experiments on web data to evaluate the quality of the clusters created by the proposed methods. These experiments include performing all of the proposed methods on the selected datasets. Table 1 summarizes and compares the properties of these datasets. The last two columns of the table show the parameters of the proposed meth-ods on these datasets (we denote the proposed methods by Improved Single Linkage and Improved Average Linkage ).
The first dataset was selected from the DMOZ collection and contains 697 web pages that were selected from 14 topics nal . For all topics, some web pages were selected and included in the data set.

The second data set was collected from news sites and contains 500 news reports from topics such as politics, economics, and sports. This dataset was constructed in 2006. Finally, a third data set was obtained from a set of REUTERS documents.
For all of the data sets, after performing preprocessing tasks such as elimination of stop-words and stemming, web page a vector was constructed including the tf-idf weight of each term.

Then, the Latent Semantic Indexing (LSI) [12] technique was applied to the vectors to reduce the number of dimensions to two (we found this reduction promising in some experiments, which provides a good way of calculating the two by two dis-tances between data items). We did this step using MATLAB. LSI transforms the original document vectors to a lower dimen-sional space by analyzing the correlational structure of terms in the document collection such that similar pages that do not share the same terms are placed in the same category. By reducing the number of the dimensions, it is possible to use dis-tance measures more efficiently.

In the next step, the algorithms were applied on these vectors to create the desired clusters. We implemented other algo-rithms, such as single linkage and the average linkage, to compare with the proposed methods. The single and average link-age methods are very popular in web page clustering, and have some similarities to the proposed methods, so we considered them in our experiments. These methods are examined and evaluated in detail in [10,42,43] . We also selected K -Means, which is the base of many other web page clustering methods. K -Means is sensitive to the initial centroids; therefore we ran the algorithm five times with different initial centers and selected the best result. We performed the proposed methods age linkage methods, we considered agglomeration levels of 50, 100, 150, 200, and ... that are associated to values ranging from 0.5 to 0.9.

Several methods have been proposed for evaluating clustering results, of which the most common methods are the en-tropy [29,32] and F -measure [22] methods. In the first method, the entropy of a cluster is calculated by where i is the correct class, j is the produced cluster, and P weighted average ( E cs ) will be A good clustering algorithm minimizes the total entropy.

The F -measure method combines two measures, precision and recall , and evaluates whether the clustering can remove the (4) , where precision and recall are obtained by (5) . In these formulas, n is the number of pages of class i in cluster j .

Because of the popularity of the F -measure method for evaluating web page clustering, we used it in this research for our evaluations.

Evaluation methods such as F -measure and entropy have been developed for the evaluation of flat clusters (or the lowest level of the hierarchical clusters), so we must adapt F -measure for evaluation of the hierarchical clustering algorithms. For order should be reflected in the evaluations. One way of considering this ordering can be as participation of the values of the child clusters in the values of the parents. In this manner, the real parent will have preference and the child will affect the parameters of its parent.

To do this, we first apply the F -measure method on clusters that do not have any child (clusters of the lowest levels) and then compute the F -measure of higher clusters according to their children. Formula (6) defines the method of computing the precision of a parent cluster based on the precision of its children: cluster members, and n k and P k are the number of members and the precision of the k th child. Recall can be computed in a similar way. Since we need to scan all the web pages, a simpler way can be to calculate recall separately, without considering pre-calculated values for sub-clusters. Therefore, to find the F -measure values, we follow a button-up process: we first cal-culate the F -measure, precision, and recall values for clusters without any child and then compute the parameters of each parent cluster using the parameters of its children.

By means of the improved measure, we evaluated the results of the clustering by the improved single and average linkage methods. The F -measure values of the methods on the different data sets are depicted in Figs. 19 X 21 .

According to Figs. 19 X 21 , the quality of the clusters produced by the proposed methods is high. The F -measure values of the methods are higher than those of the link linkage and K -Means methods. On the other hand, in comparison with the link linkage methods, the proposed algorithms show less sensitivity to the changes of the mutation value. This is because in these algorithms the mutation approaches smooth the effect of the parameter value. Since the results of the clustering are not so variable toward the value of Gr , determining an approximate value of Gr can be sufficient. We can perform some limited experiments and select the clustering with the better result.

Average values of the obtained F -measures for different methods and datasets are given in Table 2 . These values demon-strate the efficiency of the proposed methods. 4.4. Further experiments
To give a more detailed sense of the efficiency of the proposed methods, we established some experiments to investigate the ability to cluster larger data sets as well as the running time considerations. We selected several subsets of DMOZ and performed the methods. For Gr we considered the values 0.7, 0.75, and 0.8 and averaged from the resulting F -measure values (according to the previous experiments, these values seem to give better results). The results are shown in Table 3 .
The results show that the proposed methods have considerable advantages over other clustering methods. This efficiency, along with the inherited scalability of the density-based methods, provides a good opportunity to apply them on web data.
We also examined the running time aspects of the proposed methods. We obtained the associated running times from the above datasets. The experiments were performed by a Pentium IV 1.5 GHz. The results are shown in Table 4 . According to the results, all of the methods have nearly identical running times, the lower times of the proposed methods are revealed in the larger datasets. For the dataset with 20,000 data items, traditional AHC methods run about four times slower than the pro-posed methods.

We also considered other methods in our experiments. We selected the methods proposed by Wang and Kitsuregawa [35] and Zamir and Etzioni [41] and compared the clustering results. Wang and Kitsuregawa [35] proposed a clustering method by combining content and link structure as the similarity measure. This combination (or any other measure) can be also con-sidered as the similarity measure of our proposed methods. Here, we mainly focused on the clustering method. Wang and
Kitsuregawa [35] extended the standard K -Means to improve its efficiency. The method acts as follows [35] : 1. Assign each relevant page to the top C existing cluster(s ) based on the similarities (above a threshold) between the page and the correspondent centers. 2. The page will be its own cluster if no existing cluster meets step 1. 3. Recompute the centers of the clusters if its cluster members are changed. 4. Repeat steps 1 X 3 until all relevant pages are assigned and all centers do not change. 5. Merge two base clusters produced by step 4 if they share most members based on a merge threshold.
 Similar to the original research, we selected a similarity threshold of 0.1 and a merge threshold 0.75 in our experiments. The results are shown in Table 5 .

Another examined method was a phrase-analysis algorithm called STC [41] (Suffix Tree Clustering). In essence, the algo-rithm builds a suffix tree of phrases in documents; each representative phrase becomes a candidate cluster, and candidates with large overlaps are merged together. Each base cluster is assigned a score that is a function of both the number of doc-uments it includes and the number of words that make up its phrases. In fact, the base clusters are clustered using the equiv-alent of a single linkage clustering algorithm where a predetermined minimal similarity between base clusters serves as the halting criterion. In STC, the suffix tree constructs the base clusters, while in our research, the base clusters are obtained using the density concepts. The results of applying this method are also shown in Table 5 . According to the results, our meth-ods create clusters with higher quality than the examined methods. We selected the examined methods because they try to improve the K -Means and link linkage methods, and as the results show, our improvements are more efficient. 4.5. Comparison with DBSCAN
In this section, we compare the proposed method with DBSCAN. As mentioned before, our method provides some advanta-ges over other density-based methods. While all the density-based methods use two parameters, Eps and MinPts , our methods reduced them to only one parameter ( MinPts ). On the other hand, we developed the notion of a dense unit as a more dynamic definition varying in different areas of the entire data set. The dense units of the compact clusters would be smaller and more crammed than what they were in the sparser clusters. This avoids overcrowding a dense unit and reduces the number of exam-ined dataitems.To coverthe sparse areas,DBSCAN (andtheother density-based methods)selectsthe valueof Eps large enough the complexity increases. For a more precise illustration, we compared the proposed method with DBSCAN in the Number of Examined Data Items ( NoEDI ). The examined data items are those data that are extracted during the range queries.
Therefore, we applied the proposed method as well as DBSCAN with the specified parameters on different datasets and evaluated the complexity from this viewpoint. The results are shown in Table 6 . According to the results, the proposed meth-od retrieves fewer data items than DBSCAN during the range queries. As explained above, the reason is that our method finds and considers diverse neighborhood distances varying according to the density of the data set. 4.6. Evaluation of the  X  X omogeneity X  in the hierarchical clusters
In this section, we examine the efficiency of the measure that investigates the homogeneity of the clusters. For this, we considered the correct clustering, evaluated the application of the measure, and compared the results. We did this to acquire a good estimate of the effectiveness of the proposed measure. Our estimation should consider two issues: the establishment of the measure inside the clusters, and the disproval of the measure at the borders of the clusters. Accordingly, we defined two measures as follows: 1. Number of internal distances of the correct clusters for which the homogeneity is satisfied. 2. Number of cases in which the homogeneity is not established at the borders of the correct clusters.

In fact, we act in a reverse manner, obtaining the correct distances and evaluating the homogeneity. It is obvious that when these measures contain greater values, there will be a stronger relation between the homogeneity and the correct clus-tering. To get more formal measurements, we normalize the values. Therefore, we define the measures in (7) and (8) .At these formulas, the homogeneity is calculated according to (2)
Similar to the F -measure, we can combine these measures and calculate the weighted harmonic mean. This is shown in for-mula (9) , and is denoted by HomMeasure .

To use these measures, as mentioned before, we selected the correctly pre-clustered datasets and used formula (9) . To obtain the distances, we constructed the minimum spanning trees according to the Prim algorithm [7] . The results are shown in
Table 7 , and show that in a correct clustering, the homogeneity of the clusters is approved appropriately. Thus it can be used effectively during the construction of the clusters as a way of distinguishing the borders. 5. Conclusions
In this paper, we applied some improvements on web page clustering algorithms to obtain higher quality clustering. The improvements also reduce the time complexity of the algorithms. We combined different data clustering methods and uti-lized the advantages of each algorithm. The proposed methods first apply K -Means with numerous centers to find the dense units (parameter Eps ) of the density-based algorithms. After constructing the dense units, the extracted distances are sorted by insertion into a min heap (insertion stage).

During the extraction stage, all the ordered distances are extracted one by one and are examined to test their effects on the hierarchy. For this, a new measure was defined that is examined for each newly extracted distance. This measure has the most sensitivity to the new distance and so can be checked during the extraction of the distances from the smaller ones to the larger ones. In the last stage (combination stage), generated base clusters are combined together to complete the hier-archy. For combination, two well-known link linkage methods, single linkage and average linkage, are used. These methods were improved so they can be performed with O( n log n ) time complexity. They also utilize the advantage of a joining mea-sure that creates clusters with higher qualities, instead of constant thresholds. All stages of the proposed methods have the time complexity of O( n log n ). Therefore, as experiments showed, they can be performed faster than the traditional link link-age methods. According to other experimental results evaluated by an enhanced F -measure, the proposed methods show considerable improvements in the clustering quality.

We also examined other aspects of the proposed methods. We compared them with other density-based methods, and experimental results showed that they benefit from advantages such as speed of convergence, hierarchical clustering, and clustering by sampling. Finally, we evaluated the measure of homogeneity and showed its efficiency.
 Acknowledgement This research was in part supported by a grant from IPM (No. CS1386-4-06).

References
