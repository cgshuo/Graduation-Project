 In order to promote IR activities involving Asian languages and also to facilitate technological transfers into products, the latest NTCIR evaluation campaign [1] created test-collections for the traditional Chinese, Japanese and Korean lan-guages. Given that English is an important language for Asia and that we also wanted to verify that the various approaches suggested might also work well with European languages, a fourth coll ection of newspaper articles written in English was used.

Even with all participants working wi th the same newspapers corpora and queries, it is not always instructive to directly compare IR performance results achieved by two search systems. In fact, given that their performance is usually based on different indexing and search strategies involving a large number of underlying variables (size and type of stopword lists, stemming strategies, token segmentation, n -grams generation procedures, indexing restrictions or adapta-tions and term weighting approaches).

Based on the NTCIR-5 test-collections [1], this paper empirically compares various indexing and search strategies involving East Asian languages. In order to obtain more solid conclusions, this paper also considers various IR schemes, and all comparisons are analyzed statistically. The rest of this paper is orga-nized as follows: Section 2 describes the main features of the test-collections. Section 3 contains an overview of the various search models, from vector-space approaches to recent developments in both probabilistic and language models. Section 4 portrays the different indexing strategies used to process East Asian languages, and Section 5 contains various evaluations and analyzes of the re-sultant retrieval performance. Finally , Section 6 compares decisions that might result from using other statistical tests and Section 7 presents the main findings of our investigation. The test-collections used in our experiments include various newspapers covering the years 2000-2001 [1]. The Chinese and Japanese corpora were larger in size (1,100 MB) but the Chinese collection contained a slightly larger number of documents (901,446) than did the Japanese (858,400). The Korean and English corpora were smaller, both in terms of size (438 MB for the English and 312 MB for the Korean) and number of newspaper articles (259,050 for the English and 220,374 for the Korean).

When analyzing the number of pertinent documents per topic, only rigid assessments were considered, meaning that only  X  X ighly relevant X  and  X  X elevant X  items were viewed as being relevant, under the assumption that only highly or relevant items would be useful for all topics. A comparison of the number of relevant documents per topic indicates that for the English collection the median number of relevant items per topic is 33, while for the Asian languages corpora it is around 25 (ZH: 26, JA: 24, KR: 25.5). The number of relevant articles is also greater for the English (3,073) corpus, when compared to the Japanese (2,112), Chinese (1,885) or Korean (1,829) corpora.
 The 50 available topics covered various subjects (e.g.,  X  X im Dae-Jun, Kim Jong Il, Inter-Korea Summit, X  or  X  X arry Potter, circulation X ), including both regional/national events ( X  X ori Cabinet, support percentage, Ehime-maru X ) or topics having a more international coverage ( X  X 8 Okinawa Summit X ). The same set of queries was available for the fou r languages, namely Chinese, Japanese, Korean and English. According to the TREC model, the structure of each topic consisted of four logical sections: brief title ( &lt; title &gt; ), one-sentence descrip-( &lt; back &gt; ) and a relevance assessment criterion ( &lt; rel &gt; ) for the topic. Finally a concept section ( &lt; conc &gt; ) provides some related terms. In our experiments, we only use the title field of the topic description. In order to obtain a broader view of the relative merit of the various retrieval models, we examined six vector-space schemes and three probabilistic mod-els. First we adopted the classical tf idf model, in which the weight (denoted w ij ) attached to each indexing term t j in document D i was the product of its term occurrence frequency (or tf ij ) and its inverse document frequency (or idf j = ln ( n/df j ), where n indicates the number of documents in the corpus, and df j the number of documents in which the term t j appears). To measure simi-larities between documents and requests, we computed the inner product after normalizing indexing weights (model denoted  X  X ocument=ntc, query=ntc X  or  X  X tc-ntc X ).

Other variants might also be created, especially in cases when the occurrence of a particular term in a document is considered as a rare event. Thus, the proper practice may be to give more importance to the first occurrence of a term, as compared to any successive occurrences. Therefore, the tf component might be computed as the ln ( tf ) + 1 (denoted  X  X tc X ,  X  X nc X , or  X  X tn X ) or as 0 . 5+0 . 5  X  [ tf / max tf in D i ] ( X  X tn X ). We might also consider that a term X  X  presence in a shorter document would be stronger evidence than its occurrence in a longer document. More complex IR models have been suggested to account for document length, including the  X  X nu X  [2], or the  X  X tu X  IR models [3] (more details are given in the Appendix).

In addition to vector-space approaches, we also considered probabilistic IR models, such as the Okapi probabilistic model (or BM25) [4]. As a second prob-abilistic approach, we implemented the PB2 taken from the Divergence from Randomness (DFR) framework [5], based on combining the two information measures formulated below: w ij = Inf 1 ij ( tf )  X  Inf 2 ij ( tf )=  X  log 2 Prob 1 ij ( tf )  X  (1  X  Prob 2 ij ( tf )) where w ij indicates the indexing weight attached to term t j in document D i , Prob 1 ij ( tf ) is the pure chance probability of finding tf ij occurrences of the index-ing unit t j in the document D i . On the other hand, Prob 2 ij ( tf ) is the probability of encountering a new occurrence of t j in the document given that we have al-ready found tf ij occurrences of this indexing unit. Within this framework, the PB2 model is based on the following formulae: where tc j indicates the number of occurrences of t j in the collection, mean dl the mean length of a document and l i the length of document D i .

Finally, we also considered an approach based on a language model (LM) [6], known as a non-parametric probabilistic model (the Okapi and PB2 are viewed as parametric models). Probability estimates would thus not be based on any known distribution (as in Equation 1) but rather be estimated directly, based on occurrence frequencies in document D or corpus C. Within this language model paradigm, various implementations and smoothing methods might also be considered, and in this study we adopted a model proposed by Hiemstra [6], as described in Equation 4, which combines an estimate based on document ( P [ t j | D i ]) and corpus ( P [ t j | C ]).
 smoothing factor (fixed at 0.3 for all indexing terms t j )and lc an estimate of the corpus size. In the previous section, we described how each indexing unit was weighted to reflect its relative importance in describing the semantic content of a document or a request. This section will explain how such indexing units are extracted from documents and topic formulations.

For the English collection, we used words as indexing units and we based the indexing process on the SMART stopword list (571 terms) and stemmer. For European languages, it seems natural to consider words as indexing units, and this assumption has been generally confirmed by previous CLEF evaluation campaigns [7].

For documents written in the Chinese and Japanese languages, words are not clearly delimited. We therefore indexed East Asian languages using an over-lapping bigram approach, an indexing scheme found to be effective for various Chinese collections [8], [9]. In this case, the  X  X BCD EFG X  sequence would gen-erate the follow-ing bigrams  X  X B, X   X  X C, X   X  X D, X   X  X F, X  and  X  X G X . Our choice of an indexing tool also involves other factors. As an example for Korean, Lee et al . [10] found more than 80% of nouns were composed of one or two Hangul char-acters, while for Chinese Sproat [11] reported a similar finding. An analysis of the Japanese corpus reveals that the mean length of continuous Kanji characters to be 2.3, with more than 70% of continuous Kanji sequences being composed of one or two characters (for Hiragana: mean=2.1, for Katakana: mean=3.96).
In order to stop bigram generation in our work, we generated overlapping bigrams for Asian characters only, using spaces and other punctuation marks (as collected for each language from its respective encoding). Moreover, in our experiments, we did not split any words written in ASCII characters, and the most frequent bigrams were removed before indexing. As an example, for the Chinese language we defined and removed a list of 90 most frequent unigrams, 49 most frequent bigrams and 91 most frequent words. For the Japanese language, we defined a stopword list of 30 words and another of 20 bigrams, and for Korean our stoplist was composed of 91 bigrams and 85 words. Finally, as suggested by Fujii &amp; Croft [12], before generating bigrams for the Japanese documents we removed all Hiragana characters, given that these characters are mainly used to express grammatical words (e.g., doing , do , in , of ), and the inflectional endings of verbs, adjectives and nouns. Such removal is not error-free because Hiragana could also be used to write Japanese nouns.

For Asian languages, there are of course other indexing strategies that might be used. In this vein, various authors have suggested that words generated by a segmentation procedure could be used to index Chinese documents. Nie &amp; Ren [13] however indicated that retrieval performance based on word indexing does not really depend on an accurate word segmentation procedure and this was confirmed by Foo &amp; Li [14]. They also stated that segmenting a Chinese sentence does affect retrieval performance and that recognizing a greater number of 2-characters words usually contributes to retrieval enhancement. These authors did not however find a direct relationship between segmentation accuracy and retrieval effectiveness. Moreover, manual segmentation does not always result in better performance when compared to character-based segmentation.

To analyze these questions, we also considered automatic segmentation tools, namely Mandarin Tools (MTool, www.mandarintools.com ) for the traditional Chinese language and the Chasen ( chasen.aist-nara.ac.jp ) morphological analyzer for Japanese. For Korean, the pr esence of compound construction could harm retrieval performance. Thus, in order to automatically decompose them, we applied the Hangul Analyser Module (HAM, nlp.kookmin.ac.kr )tool.With this linguistic approach, Murata et al . [15] obtained effective retrieval results while Lee et al . [9] showed that n -gram indexing could result in similar and sometimes better retrieval effectiveness, compared to word-based indexing ap-plied in conjunction with a decompounding scheme. To measure retrieval performance, we adopted mean average precision (MAP) as computed by TREC EVAL . To determine whether or not a search strategy might be better than another, we applied a statistical test. More precisely, we stated the null hypothesis (denoted H 0 ) specifying that both retrieval schemes achieved similar performance levels (MAP), and this hypothesis would be rejected at the significance level fixed at  X  = 5% (two-tailed test). As a statistical test, we chose the non-parametric bootstrap test [16]. All evaluations in this paper were based on the title-only query formulation.

The MAP achieved by the six vector-space schemes, two probabilistic ap-proaches and the language model (LM) are shown in Table 1 for the English and Chinese collections. The best performance in any given column is shown in bold and this value served as baseline for our first set of statistical tests. In this case, we wanted to verify whether this highest performance was statistically better than other performances depicted in the same column. When performance dif-ferences were detected as significant, we placed an asterisk (*) next to a given search engine performance. In the English corpus for example, the PB2 model achieved the highest MAP (0.3728). The difference in performance between this model and the  X  X nu-ltc X  approach (0.3562) was statistically significant while the difference between it and the Okapi model (0.3692) was not significant.
For the Chinese corpus, the PB2 probabilistic model also resulted in the best performance, except for the unigram-based indexing scheme where the best per-formance was obtained by the language model LM (0.2965). With these var-ious indexing schemes, the difference between either the PB2, the LM, the Okapi or the  X  X nu-ltc X  models were not statistically significant. PB2 was the preferred model but by slightly changing the topic set, other models might per-form better.
 Based on an analysis of the four different indexing schemes used with the Chinese corpus, the data in Table 1 indicates that the combined  X  X ni+bigram X  indexing scheme tends to result in the best performance levels. As shown in the last row of this table, we computed mean improvements over the bigram indexing strategy, considering only the 7-best performing IR models (rows ending with the  X  X tn-ntc X  model). From this overall measure we can see for example that the character-based indexing strategy results in lower performance level than does the bigram scheme (-5.0%). Using the bigram indexing strategy as a baseline, we verified whether performance differences between the various indexing schemes were statistically significant, and then underlined those that were statistically significant. Table 1 illustrates that the differences between the bigram and word-based indexing strategies (row labeled  X  X Tool X ) are usually not significant. The differences between the bigram approach and the combined indexing strategy (last column) are usually significant and in favor of the combined approach.
Evaluations done on the Japanese corpus are given in Table 2. With this language, the best performing search model was always PB2, often showing sig-nificant improvement over others (indicated by  X * X ). Comparing the differences between the four indexing strategies shows that both Chasen (automatic segmen-tation) and the combined indexing approaches ( X  X ni+bigram X ) tend to result in the best performance levels. Using the bigram indexing strategy as baseline, the differences between the word (Chasen) or the combined ( X  X ni+bigram X ) index-ing strategies are however usually not significant. Moreover, performances that result from applying the bigram scheme are always better than with the unigram approach.

Our evaluations on the Korean collection are reported in Table 3. In this case, the best performing search model varies according to the indexing strategy. The performance differences between the best performing models ( X  X tu-dtn X ,  X  X nu-ltc X , PB2) are usually not significant. Using the bigram scheme as baseline, the performance differences with the word-based indexing approach were always de-tected as significant and in favor of the bigram approach. Comparing bigrams with the automatic decompounding strategy (under the label  X  X AM X  in Ta-ble 3), the bigram indexing strategy tends to present a better performance, but the differences are usually not significant.

General measurements such as MAP always hide irregularities found among queries. It is interesting to note for example that for some queries, retrieval per-formance was poor for all search models. For example, for Topic #4 entitled  X  X he US Secretary of Defense, William Sebastian Cohen, Beijing X , the first relevant item appears in rank 37 with the PB2 model (English corpus). When inspect-ing top-ranked articles for this query, we found that these articles more or less contained all words included in the topic description. Moreover, their length was relatively short and these two aspects were taken into account when ranking these documents high in the response list. From a semantic point of view, these short and non-pertinent articles do not specify the reason or purpose of the visit made by the US Secretary of Defense, with content being limited to facts such as  X  X he US Secretary of Defense will arrive next week X  or  X  X illiam Sebastian Cohen will leave China tomorrow X .

Topic #45  X  X opulation issue, hunger X  was another difficult query. After stem-ming, the query is composed by the stem  X  X ung X  present in 3,036 documents, the indexing term  X  X opulat X  (that occurs in 7,995 articles), and  X  X ssu X  (appear-ing in 44,209 documents). Given this document frequency information, it would seem natural to assign more importance to the stem  X  X ung X , compared to the two other indexing terms. The term  X  X unger X  however does not appear in any relevant document, resulting in poor retrieval performance for this query. The inclusion of the term  X  X ood X  (appearing in the descriptive part of the topic) resulted in some pertinent articles being found by the search system. In the previous section, we based our statistical validation on the bootstrap ap-proach [16] in order to determine whether or not the difference between two given retrieval schemes was really significant. The null hypothesis (denoted H 0 ) stated that both IR systems produce the same performance level and the ob-served difference was simply due to random variations. To verify this assumption statistically, other statistical tests could be considered.

The first might be the Sign test [17, , pp. 157 X 164], in which only the direction of the difference (denoted by a  X + X  or  X - X  sign) is taken into account. This non-parametric test does not take the amount of difference into account, but only the fact that a given system performs better than the other for any given query. For example, for a set of 50 queries, System A produced better MAP for 32 queries (or 32  X + X ), System B was better for 16 (or 16  X - X ), and for the two remaining requests both systems showed the same performance. If the null hypothesis were true, we would expect to obtain roughly the same number of  X + X  or  X - X  signs. In the current case involving 48 experiments (the two ties results are ignored), we had 32  X + X  and only 16  X - X  signs. Assuming that the null hypothesis is true, the probability of observing a  X + X  is equal to the probability of observing a  X - X  (= 0.5). Thus for 48 trials the probability of observing 16 or fewer occurrences of the same sign ( X + X  or  X - X , for a two-tailed test) is only 0.0293. This value is rather small (but not null) and, in this case, when the limit was fixed at  X  =5%,we must reject the H 0 and accept the alternative hypothesis that there were truly retrieval performance differences between System A and B.

Instead of observing only the direction of the difference between two systems, we might also consider the magnitude of the difference, not directly but by sorting them from the smallest to the largest difference. Then we could apply the Wilcoxon signed ranking test [17, pp. 352-360]. Finally, we might apply the paired t-test, a parametric test assuming that the difference between two systems follows a normal distribution. Even if the distribution of the observations was not normally shaped but the empirical distribution found to be roughly symmetric, the t-test would still be useful, given that it is a relatively robust test, in the sense that the significance level indicated is not far from the true level. However, previous studies have shown that IR data do not always follow a normal distribution [16].

Based on 264 comparative evaluations (most of them are shown in Section 5), we applied the four statistical tests to the resultant differences. Among them for all four tests, 143 comparisons were found to be significant and 88 non-significant. Thus, for 231 (143+88) comparisons out of 264 (or 87.5%), the four tests resulted in the same decision. These four statistical tests thus are clearly in agreement, even though they use different kinds of information (e.g., for the Sign test, only the difference direction).

For the other 33 (264-231) comparisons, there was some disagreement and these cases can be subdivided into three cat egories. First, in 11 cases, three tests were detected to have a significant difference while the other one did not. Fol-lowing inspection, we found that in 10 (out of 11) observations only the Sign test did not detect a significant difference by obtaining a p -value greater than 0.05 (see Example A in the second row of Table 4). Second, for 16 cases, two tests indicated a significant difference while the other two did not. After inspecting this sample, we found 8 observations for which both the t-test and the bootstrap detected a significant difference (see for example Case C in Table 4). In 7 other cases, both the Sign and Wilcoxon tests detected significant retrieval perfor-mance differences (see Case D in Table 4). Finally, in 6 only one test detected a significant difference while for the three others the performance difference could be due to random variations (see, for example, Case E in Table 4).

To provide a more general overview of the relationship between two tests, in Figure 1 we plotted the p -values for performance comparisons from the two tests. We also computed the Pearson correlation coefficient and drew a line representing the corresponding slope. The first plot in the top left corner of Figure 1 indicates a strong correlation (r=0.9996) between the bootstrap p -values and those obtained by the t-test. Clearly, the bootstrap test agrees with the t-test results, without having to assume a Gaussian distribution.
We also tested to find out whether or not the differences distribution follows a normal distribution. In 228 (out of 264) observations, the underlying distribution of performance difference did not follow a Gaussian distribution (Shapiro-Wilk test, significance level  X  = 5% [18]). In both cases, the Pearson correlation coef-ficient between the bootstrap and t-test p -values is very high.
The relationship between the t-test and the Wilcoxon test is not as strong (top right) but still relatively high (Pearson coefficient correlation of 0.8246). When comparing p -values obtained from the t-test and the Sign test, the correlation coefficient is lower (0.709) but statistically different from 0. Finally, we plotted the same number of points obtained by generating values randomly according to the normal distribution. In this case, the true correlation coefficient is a null value, even though the depicted value is not (0.0529). The latter picture is an example of no correlation between two variables. The experiments conducted with the NTCIR-5 test-collections show that the PB2 probabilistic model derived within the Divergence from Randomness frame-work usually produces the best mean average precision, according to different indexing strategies and languages. For the Chinese language (Table 1), the best indexing strategy seems to be a combined approach (unigram &amp; bigram) but when compared with a word-based approach (obtained with an automatic seg-mentation system), the difference is not always statistically significant.
For the Korean language, the simple bigram indexing strategy seems to be the best. When compared with the automatic decompounding strategy (HAM in Table 3), the performance difference is u sually not-significant. For the Japanese language (Table 2), we may discard the unigram indexing approach, but we were not able to develop solid arguments in favor of a combined indexing approach (unigram + bigram), compared to a word-based or a simple bigram indexing scheme.

Upon analyzing the decisions that resulted from our application of a non-parametric bootstrap test, the evidence obtained strongly correlated with the (parametric) t-test conclusions. Moreover, the conclusions drawn following an application of the Wilcoxon signed ranking test correlate positively with those of the t-test. From our data, it seems that the Sign test might provide different results than the three other tests, but this divergence is not really important. Acknowledgments. This research was supported in part by the Swiss NSF under Grant #200020-103420.
 In Table 5, n indicates the number of documents in the collection, t the number of indexing terms, df j the number of documents in which the term t j appears, the document length of D i (the number of indexing terms) is denoted by nt i . We assigned the value of 0.55 to the constant b ,0.1to slope , while we fixed the constant k 1 at 1.2 for the English, Korean and Japanese collection and 1.0 for the Chinese corpus. For the PB2 model, we assigned c = 3 for the English and Korean corpus, c = 6 for the Japanese and c = 1 for the Chinese collection. These values were chosen because they usually result in improved levels of retrieval performance. Finally, the value mean dl , slope or avdl were fixed according to the corresponding statistics (e.g., for bigram-based indexing, 321 for ZH, 133 for JA, and 233 for KR).

