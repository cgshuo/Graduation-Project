 Aditya Krishna Menon akmenon@ucsd.edu University of California, San Diego, La Jolla CA 92093, USA Harikrishna Narasimhan harikrishna@csa.iisc.ernet.in Shivani Agarwal shivani@csa.iisc.ernet.in Indian Institute of Science, Bangalore 560012, India Sanjay Chawla sanjay.chawla@sydney.edu.au University of Sydney and NICTA, Sydney, Australia Classification problems with class imbalance  X  where one class is rare compared to another  X  arise in several machine learning applications, ranging from medical diagnosis and text retrieval to credit risk prediction and fraud detection. Due to their practical impor-tance, such class imbalance settings have been widely studied in several fields, including machine learning, data mining, artificial intelligence, and several oth-ers (Cardie &amp; Howe, 1997; Kubat &amp; Matwin, 1997; Japkowicz, 2000; Elkan, 2001; Japkowicz &amp; Stephen, 2002; Chawla et al., 2002; 2003; Zadrozny et al., 2003; Chawla et al., 2004; Drummond &amp; Holte, 2005; 2006; Liu, 2009; Gu et al., 2009; He &amp; Garcia, 2009; Liu &amp; Chawla, 2011; Wallace et al., 2011).
 The usual misclassification error is ill-suited as a per-formance measure in class imbalance settings, since a default classifier predicting the majority class does well under this measure. A variety of performance measures have been proposed for evaluating binary classifiers in such settings; these include for example the arithmetic, geometric, and harmonic means of the true positive and true negative rates, which attempt to balance the errors on the two classes, and related mea-sures based on the recall (true positive rate) and pre-cision (see Table 1). Several algorithmic approaches have also been proposed, for example under-sampling the majority class, over-sampling the minority class, changing the decision threshold of a score-based classi-fier, and modifying algorithms to incorporate different weights for errors on positive and negative examples. Despite the large number of studies on the class imbal-ance problem, surprisingly little is understood about the statistical consistency of the algorithms proposed with respect to the performance measures of interest, i.e. about whether the algorithms converge to the op-timal value of the performance measure in the large Measure Definition References G-Mean (GM) H-Mean (HM) 2 / ( 1 TPR + 1 TNR ) (Kennedy et al., 2009) Q-Mean (QM) 1  X  ((FPR) 2 + (FNR) 2 ) / 2 (Lawrence et al., 1998) F G-TP/PR AUC-ROC Area under ROC curve (Ling et al., 1998) sample limit. In this paper, we study this question for one such performance measure that is widely used in class imbalance settings, namely the arithmetic mean of the true positive and true negative rates (AM). The usual Bayes optimal classifier that minimizes the clas-sification error rate is not optimal for this measure, and therefore standard binary classification algorithms that are designed to converge to the Bayes error are not consistent with respect to this measure. We show con-sistency with respect to the AM measure (under mild conditions on the underlying distribution) of two sim-ple families of algorithms that have been used in class imbalance settings in practice: (1) algorithms that ap-ply a suitable threshold (determined from the empiri-cal class ratio) to a class probability estimate obtained by minimizing an appropriate strongly proper loss, and (2) algorithms that minimize a suitably weighted form of an appropriate classification-calibrated loss (with the weights determined from the empirical class ratio). Our results build on several recent tools that have been developed for studying consistency of learning algorithms: regret bounds for standard binary clas-sification using classification-calibrated losses (Zhang, 2004; Bartlett et al., 2006), proper and strongly proper losses (Reid &amp; Williamson, 2009; 2010; Agarwal, 2013), balanced losses that have been used recently to un-derstand consistency in ranking problems (Kotlowski et al., 2011), and regret bounds for cost-sensitive clas-sification (Scott, 2012). In addition, a key tool we introduce is a decomposition lemma that allows us to reduce the problem of analyzing the AM regret for class imbalance settings to analyzing an empirical cost-sensitive regret, in which the cost parameter is de-termined from the empirical class ratio. For each of the above two families of algorithms, we then show that under suitable conditions, this empirical cost-sensitive regret converges in probability to zero, thus establish-ing consistency with respect to the AM measure. The paper is organized as follows. Section 2 contains preliminaries and background; Section 3 contains our decomposition lemma. Sections 4 X 5 give consistency results for the above two families of algorithms, respec-tively. Section 6 contains our experimental results. 2.1. Problem Setup and Notation Let X be any instance space. Given a training sample is to learn a binary classifier h S : X X  X  X  1 } to clas-sify new instances in X . Assume all examples (both training examples and future test examples) are drawn iid according to some unknown probability distribu-tion D on X  X { X  1 } . Let  X  ( x ) = P ( y = 1 | x ), and let p = P ( y = 1) (both under D ).We shall assume p  X  (0 , 1). In the class imbalance setting, p departs significantly from 1 2 ; by convention, we assume the positive class is rare, so that p is small.
 For any candidate classifier h : X X  X  X  1 } , we can de-fine the true positive rate (TPR), true negative rate (TNR), false positive rate (FPR), false negative rate (FNR), and precision (Prec) of h w.r.t. D as follows: TPR D [ h ] = P ( h ( x ) = 1 | y = 1) TNR D [ h ] = P ( h ( x ) =  X  1 | y =  X  1) FPR D [ h ] = P ( h ( x ) = 1 | y =  X  1) = 1  X  TNR D FNR D [ h ] = P ( h ( x ) =  X  1 | y = 1) = 1  X  TPR D Prec D [ h ] = P ( y = 1 | h ( x ) = 1) As noted in Section 1, a variety of performance mea-sures that combine the above quantities and seek to balance errors on the two classes have been proposed for classification with class imbalance (see Table 1). In this work, we focus on the arithmetic mean of the TPR and TNR (AM): In particular, we would like to find a classifier that has AM performance close to the optimal: More precisely, define the AM -regret of h as Then we would like an algorithm to be AM -consistent , i.e. we would like the AM-regret of the learned classi-fier h S to converge in probability to zero: 1 Next we recall various notions related to loss functions that will be used in our study. 2.2. Loss Functions A binary loss function on a prediction space b Y  X  is a function ` : { X  1 } X  b Y X   X  R + that defines a penalty ` ( y, label is y  X  { X  1 } (here  X  R = [  X  X  X  ,  X  ],  X  R + = [0 ,  X  ]). The ` -error of a function f : X X  b Y w.r.t. D is then The optimal ` -error w.r.t. D is and the ` -regret of f w.r.t. D is As an example, for b Y = { X  1 } , the familiar 0-1 loss ` 0-1 : { X  1 } X { X  1 } X  where 1 (  X  ) denotes the indicator function with value 1 if its argument is true and 0 otherwise, and the ` 0-1 -error of h : X X  X  X  1 } w.r.t. D takes the form In this case the optimal ` 0-1 -error er 0-1 ,  X  D is the usual Bayes classification error.
 For any loss ` : { X  1 } X  b Y X   X  R + , the conditional ` -risk L ` : [0 , 1]  X  b Y X  and the conditional Bayes ` -risk H ` : [0 , 1]  X   X  R defined as Cost-Sensitive Losses. For any base loss ` : { X  1 } X  b Y X   X  R + and c  X  (0 , 1), the cost-sensitive ` -loss with cost parameter c , ` ( c ) : { X  1 } X  b Y X   X  R + , is defined as ` Note that for the 0-1 loss ` 0-1 , the cost-sensitive loss ` 0-1 with cost parameter c simply assigns a cost of c to false positives and 1  X  c to false negatives: ` It is well known that for any  X   X  [0 , 1], L ( c ) 0-1 minimized by mal classifier w.r.t. the ` ( c ) 0-1 -error is given by h sign(  X  ( x )  X  c ) (Elkan, 2001). 3 Balanced Losses. For any base loss ` : { X  1 }  X  b Y X   X  R + and distribution D with p = P ( y = 1)  X  (0 , 1), the balanced ` -loss ` bal : { X  1 } X  b Y X   X  R + is defined as Note that a balanced loss depends on the underlying distribution D via p and therefore typically cannot be evaluated directly; however it is a useful analytical tool that has been used recently to analyze consistency of ranking algorithms in (Kotlowski et al., 2011), and will be useful for our purposes as well.
 Classification-Calibrated Losses. Let c  X  (0 , 1). A loss ` : { X  1 }  X   X  R  X   X  R + is said to be classification-calibrated at c (Bartlett et al., 2006; Reid &amp; Williamson, 2010; Scott, 2012) if  X   X   X  [0 , 1] , X  6 = c , This condition ensures that  X   X   X  [0 , 1], if f  X   X   X  a minimizer of L ` (  X ,f ), then For c = 1 2 , this ensures if f  X  is a minimizer of L ` (  X ,f ), then Proper and Strongly Proper Losses. Proper losses in their basic form are defined on b Y = [0 , 1] and facilitate class probability estimation. A loss Williamson, 2009; 2010) if  X   X   X  [0 , 1], This condition ensures that  X   X   X  [0 , 1], the set of min-imizers of L ` (  X , A proper loss is said to be strongly proper (Agarwal, 2013) if  X   X  &gt; 0 such that  X   X , This condition ensures that  X   X   X  [0 , 1],  X  is the unique minimizer of L ` (  X , L (  X , X  ) is well separated from L ` (  X , A loss ` : { X  1 } X  b Y X   X  R + on a general prediction space b Y  X  R is said to be (strongly) proper composite (Reid &amp; Williamson, 2010; Agarwal, 2013) if  X  a (strongly) proper loss  X  : { X  1 } X  [0 , 1]  X   X  R + and invertible  X  X ink X  function  X  : [0 , 1]  X  b Y such that  X  y  X  X  X  1 } , We now prove a key decomposition lemma that will allow us to reduce the problem of analyzing the AM-regret of a classifier h S learned from a sample S to the problem of analyzing a certain empirical cost-sensitive regret derived from S (for distributions D satisfying a mild assumption, namely Assumption A below). We start with the following simple equivalence between the AM measure and (one minus) the balanced 0-1 error: Proposition 1. For any h : X X  X  X  1 } , Proof. We have, er = = = = 1  X  AM D [ h ] .
 In particular, the above result implies that the AM-regret is equal to the balanced 0-1 regret: The lemma below will need the following assumption: Assumption A. We say a probability distribution D on X X { X  1 } with  X  ( x ) = P ( y = 1 | x ) and p = P ( y = 1) satisfies assumption A if the cumulative distribution functions of the random variable  X  ( x ) conditioned on continuous at z = p .
 We note that the above assumption is quite mild, in that it holds for any distribution D for which the ran-dom variable  X  ( x ) conditioned on y = 1 and on y =  X  1 is continuous, and also for any distribution D for which  X  ( x ) conditioned on y = 1 and on y =  X  1 is mixed or discrete as long as p is not a point of discontinuity. The decomposition lemma below requires an empirical estimator converges in probability to p ; while we can use any such estimator, for concreteness, we will consider the following simple estimator in our study: b p It is easy to verify that this estimator satisfies (0 , 1) and Lemma 2 ( Decomposition Lemma ) . Let D be a probability distribution on X  X  { X  1 } satisfying As-sumption A. Let h S : X X  X  X  1 } denote the classifier learned by an algorithm from training sample S , and let b p S  X  (0 , 1) and b p S 0-1 regret of h S with cost parameter c = then Proof. From Eq. (1), for any training sample S , we may express the AM-regret of h S as: the above decomposition converges in probability to 1 / (2 p (1  X  p ))  X  0 = 0. We will show that under the given conditions, the other two terms also converge in probability to zero, thereby establishing the result. For the first term, we have er
D [ h S ]  X  =  X  X  X  0 , since Now, let h  X  sign(  X  ( x )  X  p ). Then for the third term in the decom-position, it can be seen that Now, FNR D [ h  X  p | y = 1 = FNR D [ h  X  p ] , since tinuity of the cumulative distribution function of  X  ( x ) given y = 1 at p (Assumption A). Similarly, FPR D [ h  X  and the fact that term above also converges in probability to zero. Thus, to show AM consistency of an algorithm (for distributions satisfying Assumption A), it suffices to show the empirical cost-sensitive regret in the above lemma converges to zero. Note that this cannot be achieved by a direct application of results for cost-sensitive learning, since the costs there have to be fixed in advance. In the next two sections we show AM consistency for two families of algorithms that have often been used in class imbalance settings in practice. Algorithm 1 Plug-in with Empirical Threshold 1: Input: S = (( x 1 ,y 1 ) ,..., ( x n ,y n ))  X  ( X X { X  1 } ) 2: Select: (a) Proper (composite) loss ` : { X  1 }  X  5: b p S = (as in Eq. (2)) 6: Output: Classifier h S ( x ) = sign( b  X  S ( x )  X  b p From Proposition 1, it follows that an optimal classifier w.r.t. the AM measure has the form Therefore if p were known, one could use a class prob-ability estimate ably regularized empirical risk minimization (ERM) using a proper loss, and construct a plug-in classifier sign( a natural approach is to use an estimate ing a plug-in classifier with an empirical threshold, sign( ing proper losses in a reproducing kernel Hilbert space (RKHS). The following establishes general conditions under which such algorithms are AM-consistent: Theorem 3. Let D be a probability distribution on X X { X  1 } satisfying Assumption A. Let estimator of p = P ( y = 1) satisfying b p estimator satisfying E x | r  X  1 , and let h S ( x ) = sign( The proof makes use of the following lemma: Lemma 4. Let c  X  (0 , 1) and r  X  1 . For any X X  [0 , 1] and h ( x ) = sign( Lemma 4 follows directly from a result of (Scott, 2012); see the supplementary material for details.
 Proof of Theorem 3. By Lemma 4, we have The result follows from Lemma 2.
 Loss ` ( y,f ) Theorem 5 Theorem 6 Logistic ln(1 + e  X  yf ) XX Exponential e  X  yf XX Square (1  X  yf ) 2 XX Sq. Hinge ((1  X  yf ) + ) 2 XX Hinge (1  X  yf ) +  X  X As a special case, for suitable strongly proper loss, we have the following: Theorem 5 ( Consistency of Algorithm 1 with certain strongly proper losses ) . Let D be a proba-bility distribution on X  X { X  1 } satisfying Assumption A. Let ` : { X  1 } X   X  R  X   X  R + be a strongly proper compos-ite loss, and let f S ,h S denote the real-valued function and classifier learned by Algorithm 1 from a training sample S using this loss. If the kernel K and regular-ization parameter sequence  X  n can be chosen such that regret ` D [ f S ] P  X  X  X  0 , then The proof of Theorem 5 involves showing that under the conditions of the theorem, the class probability es-timator of Theorem 3 with r = 2; this follows as a direct con-sequence of the definition of strongly proper losses. Details can be found in the supplementary material. Table 2 shows several examples of strongly proper com-posite losses; see (Agarwal, 2013) for more details. For each of these losses, (Zhang, 2004) gives prescriptions for K and  X  n satisfying the conditions of Theorem 5; with these choices, Algorithm 1 is AM-consistent. Given the result of Proposition 1, another approach to optimize the AM measure is to minimize a balanced surrogate of the 0-1 loss; however this requires knowl-edge of p . Again, a natural approach is to use an empirical estimate of algorithms that involves minimizing an empirically balanced loss; see Algorithm 2 for a prototype. The following establishes conditions under which such an algorithm is AM-consistent: Theorem 6 ( Consistency of Algorithm 2 with certain convex classification-calibrated losses ) . Let D be a probability distribution on X  X { X  1 } satis-fying Assumption A. Let ` : { X  1 } X   X  R  X   X  R + be a loss that is convex in its second argument, classification-calibrated at 1 2 , and for which  X   X  &gt; 0 ,r  X  1 such that  X   X   X  [0 , 1] , L ` (  X , 0)  X  H ` (  X  )  X   X  |  X   X  1 2 Algorithm 2 Empirically Balanced ERM 1: Input: S = (( x 1 ,y 1 ) ,..., ( x n ,y n ))  X  ( X X { X  1 } ) 2: Select: (a) Loss ` : { X  1 } X   X  R  X   X  R + ; (b) RKHS 3: b p S = (as in Eq. (2)) 5: Output: Classifier h S ( x ) = sign( f S ( x )) valued function and classifier learned by Algorithm 2 from a training sample S using this loss. If the ker-nel K and regularization parameter sequence  X  n can be chosen such that regret `, ( b p S ) D [ f S ] P  X  X  X  0 , then The proof makes use of the following lemma: 4 Lemma 7. Let ` : { X  1 } X   X  R  X   X  R + be convex in its sec-ond argument and classification-calibrated at 1 2 , and suppose  X   X  &gt; 0 ,r  X  1 s.t.  X   X   X  [0 , 1] , L ` (  X , 0)  X  H ` (  X  )  X   X  |  X   X  1 2 | r , and L ` ( 1 2 , 0) = H ` ( 1 The proof of Lemma 7 makes use of some results of (Scott, 2012); details can be found in the supplemen-tary material. The proof of Theorem 6 then follows by an application of Lemma 7 with c = the result. Details are in the supplementary material. Table 2 gives examples of convex classification-calibrated losses for which  X   X  &gt; 0 ,r  X  1 such that  X   X   X  [0 , 1], L ` (  X , 0)  X  H ` (  X  )  X   X  |  X   X  1 L 2006) for more details. For each of these losses, it is possible to show that K and  X  n can be chosen to satisfy the conditions of Theorem 6, yielding AM-consistency of Algorithm 2. The proof, which involves a detailed stability analysis extending the analyses in (Zhang, 2004; Bousquet &amp; Elisseeff, 2002), is heavily techni-cal and beyond the scope of the current paper; details will be provided in a longer version of the paper. We conducted two types of experiments to evaluate the algorithms studied in the previous sections: the first involved synthetic data from a known distribu-tion for which the AM-regret could be calculated ex-actly; the second involved a large range of real data sets. We compared the algorithms with the standard underlying ERM algorithms (which seek to minimize the usual misclassification error), and also with under-sampling and over-sampling methods that seek to  X  X al-ance X  imbalanced data sets and have been highly pop-ular in class imbalance settings in practice: these in-clude random under-sampling, in which examples from the majority class are randomly sub-sampled to equal the number of minority class examples; random over-sampling, in which examples from the minority class are randomly over-sampled to equal the number of ma-jority class examples; and synthetic over-sampling us-ing the SMOTE technique, in which synthetic exam-ples from the minority class are generated along lines joining pairs of actual minority class examples in the data set (Chawla et al., 2002); in each case, the under-sampled/over-sampled data set was then given as in-put to the standard regularized ERM algorithm. 6.1. Synthetic Data Our first goal was to evaluate the behavior of the AM-regret for different algorithms in a setting where it could be calculated exactly. For these experiments, we generated data in X = R d ( d = 10) with varying de-each case, examples in R d  X { X  1 } were generated as fol-lows: each example was positive with probability p and negative with probability 1  X  p , with positive instances drawn from a multivariate Gaussian distribution with mean  X   X  R d and covariance matrix  X   X  R d  X  d , and negative instances drawn from a multivariate Gaus-sian distribution with mean  X   X  and the same covari-ance matrix  X ; here  X  was drawn uniformly at random from { X  1 , 1 } d , and  X  was drawn from a Wishart dis-tribution with 20 degrees of freedom and a randomly drawn invertible PSD scale matrix. In these exper-iments we used the logistic loss as a prototype of a loss that satisfies the conditions of both Theorem 5 and Theorem 6. For the distributions considered, the AM-optimal classifier is linear, as are the real-valued functions minimizing the expected values of the logis-tic and empirically balanced logistic losses; this both makes it sufficient to learn a linear function, (i.e. to use a linear kernel), and simplifies the subsequent calcula-tion of the AM-regret of the learned classifiers under these distributions (see supplementary material). Figure 1 shows plots of the AM-regret as a function of the number of training examples n for different val-ues of p for all the algorithms (all using the logistic loss; in all cases, the regularization parameter was set to  X  n = 1 / a perfectly balanced distribution, the AM-regret for all methods converges to zero. For p &lt; 0 . 5, when the classes are imbalanced, as expected, the AM-regret of the standard logistic regression algorithm does not converge to zero; on the other hand, for both the em-pirical plug-in and empirically balanced algorithms, the AM-regret converges to zero. The AM-regret for the sampling methods also converges to zero; how-ever the under-sampling method has slower conver-gence (since it throws away information), while the over-sampling methods are computationally expensive (since they blow up the training sample size). 6.2. Real Data Our second goal was to evaluate the performance of the class imbalance algorithms studied here on a wide range of real data. We used 17 data sets with varying degrees of class imbalance, taken from the UCI reposi-tory (Frank &amp; Asuncion, 2010) and other sources; due to space constraints, we discuss results on 3 of these here in detail (see Table 3; full results for all 17 data sets are included in the supplementary material). In this case we evaluated all the above algorithms (ex-cept the random over-sampling algorithm, which for ERM-based algorithms is similar to empirical balanc-ing but computationally more expensive), using all the five loss functions given in Table 2. In all experiments, we learned a linear function with ` 2 regularization; in each case, the regularization parameter  X  was selected by 5-fold cross-validation on the training sample from the range { 2  X  20 ,..., 2 4 } (the value of  X  maximizing the average AM value on the validation folds was se-lected). For the empirical plug-in algorithm with logis-tic, exponential, square, and square hinge losses, which are all proper (composite) losses, class probability esti-mates were obtained using the standard  X   X  1 transform based on the link function associated with the proper loss (Reid &amp; Williamson, 2010; Agarwal, 2013); for hinge loss, we used Platt scaling (Platt, 1999). The results, averaged over 10 random 80%-20% train-test splits for each data set (and 5 random sam-pling runs for the under-/over-sampling methods), are shown in Figure 2 (see supplementary material for re-sults on additional data sets). Performance is shown in terms of AM (top panel) as well as GM (bottom panel). As expected, the standard ERM algorithms do not give good AM performance, while the empir-ical plug-in, empirically balanced ERM, and under-/over-sampling algorithms are mostly similar and all give good AM performance. A detailed rank analysis across the full 17 data sets suggests the empirically bal-anced ERM method wins overall (see supplementary material). Among loss functions, we see similar perfor-mance overall. The main exception is the square loss, which sometimes gives poor performance; this may be due to the fact that unlike other losses, it penalizes predictions with a large positive margin yf , suggest-ing the other losses may be preferable. Performance on the GM measure shows similar trends as for AM. We also point out that when performance is measured in terms of AUC-ROC, we see no significant differ-ence between the standard ERM algorithms and other methods (see supplementary material), consistent with the observation made in Table 1 that the AUC-ROC applies to a scoring function and not to a classifier, and therefore sampling and other class imbalance methods do not significantly impact the AUC-ROC. We have studied the problem of binary classification under class imbalance, and have given the first for-mal consistency analysis for this problem that we are aware of. In particular, we have focused on the AM performance measure, and have shown that under cer-tain conditions, some simple algorithms such as plug-in rules with an empirical threshold and empirically bal-anced ERM algorithms are AM-consistent. Our exper-iments confirm these findings. This suggests that at least when the AM performance measure is of interest, it may be unnecessary to throw away information as is done in under-sampling, or to incur additional compu-tational cost as in over-sampling. A natural next step is to conduct a similar analysis for other performance measures used in class imbalance settings.

