 As an important supporting technology of Market Intelligence (MI), Web data integration is f acing new challenges, such as the integrity of data acquisition, the quality of data extraction and data consolidation. To solve such problems, we propose an MI-oriented web data integration system (MI-WDIS), which achieves excellent performances in integrating Surface Web and Deep Web data with much less manual work . Based on MI-WDIS, we have developed a platform for intelligent analysis of job data. The platform collects tens of thousands of job data daily and provides personalized services for job seek ers through diversified channels. Besides, it provides other a dvanced services, including intelligence analysis, automatic monitoring and alerting, for various organizations, such as ente rprises, training institutions and recruitment agencies. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.5 [ Information Storage and Retrieval ]: Online Information Services X  Web based services. Algorithms, Experimentation. Web Data Integration, Market Intelligence. With the rapid development of the Internet, Web has become a huge data source with massive information. We could obtain abundant useful information and knowledge by effectively analyzing and mining these data. For Market Intelligence(MI) aiming at offering accurate and authentic decision support for users [1], it has become essential to integrate the Web data and provides timely abundant informa tion by data analysis and mining. To carry out effective analysis and mining, MI needs to integrate all the structural data from Web data sources. However, because of the heterogeneity, autonomy, and distribution of web data, it is quite a challenge to achieve the integrity of data acquisition, the high-quality of data extraction and data consolidation, and so on. Targeting at solving such problems, we propose an MI-oriented Web data integration system (MI-WDIS) achieving excellent performances in data acquisition, data extraction, and data integration with much less manual effort, and providing comprehensive structural data. and integrates them tim ely. Using SVM methods [2] in the dynamic updating of entities pattern and multi-strategies methods in the detections of entities pattern re lationships, MI-DWIS utilizes the data existing in the system as well as the visual information of destination page to detect and establish relationships between different entities effectively. The Data Acquisition component is mainly responsible for Surface Web crawling and Web database crawling. Surface Web crawling, which is implemented by Web crawler, could crawl surface web pages from various web sites a nd obtains brief description information. Web database crawli ng is in charge of detecting dynamical Deep Web data source as much as possible. By evaluating the data, MI-DWIS sel ects high-quality data sources and automatically identifies query interfaces, and thus obtains maximum Deep Web pages. We apply the extended evidence theory [3] to understand the semantic of query interfaces in a uniform way and use the methods based on query harvest rate model to crawl maximum deep web pages at small costs. As a result, it effectively overcomes the simple and empirical limitations of the traditional heuristic rules. The Data Extraction component consists of page extraction and semantic annotation. This compone nt can extract target data from target pages accurately, and perform semantic understanding of data elements. The Page Extraction part uses the method based on ensemble learning [4] for the Surface Web pages and the method based on hierarchical clustering [5] for the Deep Web pages. Therefore, it can accurately identify data elements and attributes labels in sampling pages, and increase extraction accuracy in target data. The Semantic Annotation part, however, uses the method based on 2D Correlative-Chain Conditional Random Fields [4] for Surface Web pages; for the Deep Web pages, it uses the method based on constraints Conditional Random Fields [6]. This approach could not only make full use of relationships between Web data elements, but also increase the accuracy of semantic annotation effectively. The Data Consolidation component consists of duplicate record detection and data fusion. This component identifies massive records, which come from different data sources but represent the same entity in the real world, and merges them into one record. For duplicate records, it uses the method based on unsupervised learning to automatically build training samples, to train classification model iteratively and to construct a domain duplicate record detection model based on individual classifica tion models. Thus it effectively solves the detection problem of massive duplicate records. In the proposed system, we apply appr oaches based on Markov Logic Networks in Data fusion. After the collection of data source characteristics, the modeling base d on Markov Logic Networks, and the selection of true value from c onflict data by inference, data fusion can be applied to effectively solve the fusion problem of massive duplicate records. Altoge ther, the entire approach has a relatively high accuracy and strong expandability for web data fusion. The Data Analysis Component analyzes the integrated, high-quality, and structural data to acquire usef ul information. It focuses on trend analysis, decision support, monitoring and warning, etc. It displays the data using graphical presentation such as reporting, drill-down analysis and graphical monitoring. 
