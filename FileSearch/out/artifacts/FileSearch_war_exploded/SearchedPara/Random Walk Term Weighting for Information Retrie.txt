 We present a way of estimating term weights for Informa-tion Retrieval (IR), using term co-occurrence as a measure of dependency between terms. We use the random walk graph-based ranking algorithm on a graph that encodes terms and co-occurrence dependencies in text, from which we derive term weights that represent a quantification of how a term contributes to its context. Evaluation on two TREC collec-tions and 350 topics shows that the random walk-based term weights perform at least comparably to the traditional tf  X  idf term weighting, while they outperform it when the distance between co-occurring terms is between 6 and 30 terms. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Performance, Experimentation Keywords: Random Walk Algorithm, TextRank
Graph-based ranking algorithms like PageRank [4] have been used in citation analysis, social networks, and the anal-ysis of the link structure of the Web. Generally, a graph-based ranking algorithm is a way of deciding on the im-portance of a vertex within a graph, by taking into ac-count global information recursively computed from the en-tire graph, rather than relying only on local vertex-specific information. Recently, graph-based ranking algorithms were used with graphs extracted from text (TextRank [3]) and successfully applied to summarisation [1], text classifica-tion [2], keyphrase extraction [3], and word sense disam-biguation [3].

We use the random walk graph-based ranking algorithm and its TextRank adaption [3] to derive term weights from textual graphs, similarly to [2]. Textual graphs encode term dependencies in text, which are defined as terms occurring within a maximum set distance of each other. Unlike pre-vious studies [2, 3], we vary the window of co-occurring terms from 2 up to 40, and we apply the resulting ran-dom walk weights to Information Retrieval (IR). We hy-pothesise that the random walk term weights ( rw ) can per-form at least comparably to traditional term frequency ( tf ) weights used in IR. We plug the random walk weights into the tf  X  idf weighting model, in the place of traditional term frequency weights. We evaluate the resulting rw  X  idf weight-ing using tf  X  idf as a baseline, on WT10G and Disks 4&amp;5. We use tf  X  idf with pivoted document length normalisation, instead of term frequency normalisation, because it allows us to plug rw in the place of tf without further parameter tuning. Experiments show that random walk weights per-form at least comparably to term frequency weights, while they outperform the latter when the distance between co-occurring terms is between 6 and 30.
Graph-based ranking algorithms are a way of deciding the importance of a vertex within a graph, based on global in-formation recursively drawn from the entire graph. When one vertex links to another one, it casts a vote for that other vertex. The higher the number of votes that are cast for a vertex, the higher the importance of the vertex. Also, the importance of the vertex casting the vote determines how important the vote itself is, and this information is taken into account by the ranking model. Hence, the score of a vertex is determined based on the votes that are cast for it and the score of the vertices casting these votes. Let G = ( V, E ) be a directed graph with the set of vertices V and set of edges E , where E is a subset of V  X  V . For a vertex V i , let In ( V i ) be the set of vertices that point to it (predecessors), and let Out ( V i ) be the set of vertices that vertex V i points to (successors). The score of a vertex V defined as follows [4]: where d is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the prob-ability of jumping from a given vertex to another random vertex in the graph. In the context of the Web, this graph-based ranking algorithm implements a random walk model, where a user clicks on links at random with a probability d , and jumps to a completely new page with probability 1  X  d .
In order to build a graph encoding term dependencies from a document, we follow the steps used in the TextRank key-word extraction application [2]: 1) Add the document terms as vertices in a graph. 2) Identify terms co-occurring withi n a window of maximum size N . The window size is the max-imum vicinity of a term, within which terms are considered co-occurrent. This is represented in the graph by a set of edges that connect this term to all the other terms in the window. 3) Iterate the graph-based algorithm until conver-gence. 4) Sort vertices based on their final score. After the graph is constructed and the edges are in place, we apply the TextRank algorithm. We set the maximum number of iter-ations to 100, the damping factor to 0.85, the convergence threshold to 0.0001, and the initial weight of each graph node to 0.25, following [2, 3]. We set window size N to 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, and 40.
To evaluate the random walk-based term weights, we plug them in place of term frequency weights in tf  X  idf, thus pro-ducing rw  X  idf. We evaluate retrieval performance using rw  X  idf, with tf  X  idf as a baseline, on WT10G (topics 451-550) and Disks 4&amp;5 without the Congressional Record (topics 301-450 &amp; 601-700) 1 . We experiment with short (title-only) and long (title-description) queries. We apply stopword remov al and Porter stemming, and use the Terrier IR platform 2 . Table 1 contains the Mean Average Precision (MAP) and Precision at 10 (P@10) scores, for short and long queries. Overall, performance improves when random walk weights are used instead of term frequency weights, for terms co-occurring within a window of between to 6 and 30 terms, at all times, and in a similar range for both query lengths tested. Most of these improvements are very statistically significant ( p &lt; 0 . 01, Wilcoxon matched-pairs signed-ranks test). Varying the window size of co-occurring terms affects performance, with best scores noted between N =8 (+5.3% MAP, short queries, WT10G) and N =35 (+17.3% MAP, long queries, Disks 4&amp;5). Best window sizes for MAP are close or even identical to best window sizes for P@10. This indicates that term dependency is mod-elled accurately within those window sizes. Comparing the two collections, we see that MAP improves more for Disks 4&amp;5, (+18.3% (+17.3%), short (long) queries), than for WT10G, (+5.3% (+9.1%), short (long) queries), with ran-dom weights. Also, the best MAP window sizes are smaller for WT10G, ( N =8 (15), short (long) queries), than for Disks 4&amp;5 ( N =30(35), short (long) queries). Both observations (smaller MAP improvement and smaller best MAP win-
TREC datasets: http://trec.nist.gov/ http://ir.dcs.gla.ac.uk/terrier/ dows for WT10G) may be because WT10G is a Web col-lection that contains more noise, than Disks 4&amp;5. Hence, modelling term dependency appears more effective when the co-occurring terms contain less noise. Finally, we re-port that, for most terms, tf and rw are positively corre-lated (Pearson X  X  correlation coefficient &gt; 0.7). This correla-tion decreases as window size increases, since including mo re co-occurring terms alters rw , and co-occurring terms tend to become less relevant as window size increases.
We used a graph-based ranking model for text process-ing, namely the TextRank random walk algorithm, to derive term weights for Information Retrieval. Evaluation on two TREC collections and 350 topics showed that the random walk weights, when plugged into the tf  X  idf weighting scheme, perform at least comparably to tf  X  idf, while they significantly outperform it when the distance between co-occurring terms is between 6 and 30 terms. These results, which are consis-tent for two collections and queries of two different lengths , agree with previous findings that random walk weights can be successfully used in automatic text processing [1, 2, 3]. To our knowledge, this is the first time random walk weights are used as term weights in Ad-hoc retrieval. Future work includes integrating random walk weights into the weight-ing model in more refined ways, for example as probabilities, and in the context of Language Modelling.
 Acknowledgements: Author 1 was supported by SEUI and FEDER funds under project MEC TIN2005-08521-C02 and  X  X unta de Galicia X  under project PGIDIT06PXIC10501PN. [1] G. Erkan and D. Radev. LexRank: Graph-based [2] S. Hassan and C. Banea. Random-Walk Term [3] R. Mihalcea and P. Tarau. TextRank: Bringing Order [4] L. Page, S. Brin, R. Motwani, and T. Winograd. The
