 Representing words as low dimensional vectors (also known as word embeddings) has been a widely adopted technique in NLP. Word representations can be used as features for classification tasks such as named entity recognition or chunking (Turian et al., 2010), and as a pretraining method for initializing deep neural network representations (Collobert et al., 2011; Kim, 2014). Word embeddings provide better generalization to unseen examples since they can capture general semantic and syntactic proper-ties of words. One of the most popular methods of learning word embeddings is the skipgram model of Mikolov et al. (2013a; 2013b) where embeddings are trained by making predictions of context words appearing in a window around a target word.

The standard skipgram model ignores syntax and only partially takes into consideration the sequen-tial structure of text, but still captures certain syn-tactic properties of words. A significant amount of previous research has explored methods for di-rectly taking syntax into account for word embed-ding learning (Pham et al., 2015; Cheng and Kart-saklis, 2015; Hashimoto et al., 2014). One simple method is based on traditional count-based distribu-tional semantic spaces and utilizes words with syn-tactic types from a dependency parse graph as con-text features (Pad  X  o and Lapata, 2007; Baroni and Lenci, 2010). This method has also been applied to skipgram models, where word embeddings are op-timized to predict dependency context features in-stead of other words (Levy and Goldberg, 2014).
Syntax-based embeddings have been shown to have different properties in word similarity evalua-tions than their window based counterparts, better capturing the functional properties of words. How-ever, it is not clear if they provide any advantage for NLP tasks. We show that using dependency context features can be a general method of providing syn-tactic information for several sentence classification tasks. Furthermore, the dependency context embed-dings improve performance with all classifiers we tested.

We consider the usage of word and dependency context features for three common sentence classi-fication tasks: TREC question type classification, binary sentiment prediction on Stanford Sentiment Treebank, and SemEval 2010 relation identification. We evaluate different methods of using the depen-dency context embeddings as extra features besides word embeddings to inject information into sentence classifiers about the syntactic structure of a sentence. The advantage of such a method is that it can be applied to any classifier that utilizes standard word embeddings. We evaluate the usefulness of syntax-based word embeddings and dependency context embeddings with three different sentence classifica-tion methods: a Support Vector Machine (SVM), a Convolutional Neural Network (CNN) and a Long Short Term Memory network (LSTM).

In order to better utilize the structure of depen-dency graphs, we propose an extended version of the simple dependency based skipgram of Levy et al. (2014). This extended version considers co-occurrences in a dependency graph between pairs of words, words and dependency context features, and between different dependency context features. This scheme results in word embeddings that share properties between window based models and de-pendency graph based ones. More importantly, it provides additional structural information for the de-pendency context feature embeddings making them more effective when used in sentence classification tasks.

Our evaluation provides several insights on the role of syntax for embeddings and how they can be used for sentence classification. First, we con-firm past claims about the different properties be-tween dependency and window based skipgram em-beddings in word similarity tasks. Second, we show that dependency based embeddings perform better in question classification and relation identification than window based ones. These results are robust across multiple classification methods. We show that combining dependency context feature embed-dings together with word embeddings provide a sim-ple and effective way to improve sentence classifica-tion performance. Finally, the performance gain is higher for the extended dependency based skipgram developed in this paper. Estimating word representations from text has been the focus of a lot of research in NLP. Traditional count-based models learn representations by apply-ing SVD in a word-word co-occurrence matrix (Tur-ney et al., 2010). More recently, neural models have been used to learn word embeddings by optimizing for a word prediction task (Collobert et al., 2011; Mnih and Teh, 2012; Mikolov et al., 2013a). How-ever, the most commonly used word representation techniques like word2vec X  X  skipgram and CBoW take little consideration of syntactic structure.
Several modifications have been proposed so that word embedding learning algorithms can better uti-lize syntax or the sequence structure of sentences. One such model is the dependency based skipgram of Levy and Goldberg (2014) which we further ex-tend in this paper. Evaluation of this model is limited to word similarity or lexical substitution in context (Melamud et al., 2015), and little is known about performance within other NLP tasks. Hashimoto et al. (2014) proposed a log-bilinear language model based on predicate-argument structures and report improvements on phrase similarity tasks compared to standard skipgram. In Ling et al. (2015), skip-gram and CBoW models are adapted to include po-sition specific weights for the words inside the co-occurrence window and the resulting embeddings provide slight improvements for parsing and POS tagging tasks. The C-PHRASE model (Pham et al., 2015) is another modification of the CBoW model that uses an external parser to replace windows with syntactic constituents. In Cheng and Kartsak-lis (2015), a recursive neural network structured ac-cording to a sentence X  X  parse learns word embed-dings by composing into valid sentences rather than distorted ones.

Structured skipgram models (Levy and Goldberg, 2014; Ling et al., 2015) have a notable difference with other approaches of incorporating structural in-formation into embeddings (e.g. C-PHRASE), since they also produce embeddings of the structural con-text features at the prediction layer. We show that in the case of dependency contexts, these structural features can provide valuable information to sen-tence classifiers. In our extended dependency based skipgram, we do not make a distinction between words and structural features in the training pro-cess, which results into better performing depen-dency context embeddings when used in sentence classification. Another difference of our skipgram model with other structured skipgram variants is that we keep the long distance word contexts used in standard window based skipgram training with the purpose of capturing both functional and topic re-lated semantic properties of words.

Our work is also related to methods of provid-ing explicit syntactic information to sentence clas-sifiers. Most of the previously proposed approaches rely on tree-structured neural architectures to drive composition of word embeddings to a sentence rep-resentation (Socher et al., 2012; Tai et al., 2015; Li et al., 2015). We use a different approach where syntactic information is provided only through em-beddings. Our approach is not orthogonal to using tree-structured models and the two of them could be applied together. An advantage of providing syn-tactic information through embeddings is that large amounts of automatically parsed textual data can be utilized in order to learn representations of depen-dency types. The skipgram model of Mikolov et al. (2013a; 2013b) optimizes vector representations of words (word embeddings) such that they can predict other context words occurring in a small window. The ar-chitecture consists of a single hidden layer feedfor-ward network without any non-linearity applied on the hidden layer. The input to the network is the in-dex of a target word (a one-hot vector) and the output is a vector of probabilities of appearance for con-text words. The network learns word embeddings by maximizing the log probability of a context word c given a target word t observed in a large corpus of textual data D . To avoid the large computational cost of applying a softmax for the whole vocabulary, a commonly used strategy is to train with negative sampling. For each target-context pair ( t,c ) com-ing from the observed data D , a small number of context words is sampled from unobserved data D 0 according to a simple distribution and then used as the negative classes.

The probability of the target context pair ( t,c ) be-ing observed in the data is given by: where v t and v c are target and context word embed-dings, and  X  is the sigmoid function. For a negative sampled pair ( t,c ) , the probability of the pair not being observed in the data is given by: The objective becomes: arg max
The network learns two sets of weights for each word: one for embedding words to a low dimen-sional representation in the hidden layer that we will refer to as the embedding layer weights, and one for assigning a probability to context words that we will refer to as the prediction layer weights. Both sets of weights assign representations to words such that words that have similar co-occurrence patterns with other words are closer in the embedding space. Typ-ically, the embedding layer weights are used as fea-ture representations of words for other other tasks. Due to its scalability to large corpora and the good performance of its derived word embeddings in sev-eral NLP tasks the skipgram model has become a standard solution for unsupervised learning of word representations.

While typical training of skipgarm is performed by optimizing for the prediction of other words in a window around the target word, it is possible to use other contextual features, such as contexts from dependency graphs of sentences.

We consider three variations of skipgram based on different target-context pairs: 3.1 Window-5 based skipgram (Win5) This is a standard skipgram model that consid-ers target-context word pairs inside a window of 5 words to the right and to the left of the target word. The window size for every target instance in the cor-pus is uniformly sampled from the [1,5] range, ef-fectively providing a weighting scheme for context words according to their distance from the target word. 3.2 Skipgram with dependency contexts (LG) Levy and Golberg X  X  (2014) modification to the skip-gram model replaces context words in a window by dependency contexts. A dependency context is a discrete symbol denoting a word and its syntactic role in a dependency parse graph (e.g. nsubj she , of : nmod coffee , of : nmod  X  1 cup ). The directionality of dependency edges is encoded by introducing features with inverse relations. Train-ing of this skipgram variant is similar to window based approaches, but each word is considered as a node in a dependency graph obtained by a parser, and embeddings are optimized to predict their corre-sponding word X  X  immediate syntactic contexts (Fig-ure 1). The network X  X  weight matrices have different shapes, where representations coming from the em-bedding layer weights correspond to word embed-dings, while representations coming from the pre-diction layer weights to dependency context embed-dings. 3.3 Extended Dependency Skipgram (EXT) We propose another variation of skipgram based on dependency graphs that utilizes additional co-occurrences compared to the LG variant. Each target word is taken as a node in the dependency graph and then optimize word embeddings such that they max-imize the probability of other words within distance one and two in the graph. As with the Win5 model, we apply a weighting according to distance, with words having distance one from the target counted twice. This word-word prediction behaves similarly to the Win5 model, but considers the dependency parse to filter coincidental co-occurrences. The sec-ond type of predictions that embeddings are opti-mized for is similar to the LG model, where each word predicts its dependency contexts. We also op-timize for a third type of context prediction where for each node, dependency contexts become the tar-gets and predict the rest of dependency contexts of the same node. An example of the different target-context pairs that each skipgarm variant utilizes can be seen in Figure 1. The three types of target-context pairs for the extended dependency skipgram are in-terleaved during training. The weight matrices of this network are symmetric resulting in two embed-dings per word and dependency context feature. 3.4 Implementation Details We trained 300 dimensional versions of the above skipgram variants on English Wikipedia August 2015 dump of 2 billion words. Vocabularies con-sist of words and dependency contexts that appear more than 100 times (approximately 220k words and 1.3m dependency contexts). Training was done by applying negative sampling with 15 negative sam-ples per target-context pair for 10 iterations over the entire corpus using stochastic gradient descent. The following commonly used methods (Mikolov et al., 2013b; Levy et al., 2015) were applied during train-ing: drawing negative samples according to their un-igram distribution raised to the power of 0.75, linear decay of learning rate with initial  X  = 0 . 25 , and subsampling of target words with probability given frequency. Dependency parsing for LG and EXT training was done with the Stanford Neural Network dependency parser (Chen and Manning, 2014) us-ing Universal Dependency tags (De Marneffe et al., 2014). We evaluate the effect of the different contextual fea-tures for skipgram word embeddings in two word similarity datasets: WordSim-353 (Finkelstein et al., 2001) and SimLex-999 (Hill et al., 2015). For both datasets, we compare the cosine similarity of word embeddings for a pair of words to human judgements and report Spearman X  X  correlation in Ta-ble 1. The two datasets use a different notion of word similarity for scoring. Wordsim-353 mostly captures topical similarity (or relatedness), giving high similarity to pair of words like clothes-closet . SimLex-999 uses a more strict version of similar-ity, often called substitutional similarity, where the pair clothes-closet has a low similarity score and pairs like shore-coast have high similarity. Win5 skipgram version achieves a higher correlation for WordSim-353 compared to LG, but the results are reversed for SimLex-999. This agrees with previ-ous research that shows that syntactic contexts corre-late better with substitutional similarity judgements than using words in a window as contexts (Levy and Goldberg, 2014). As expected, the extended model represents a middle ground solution between the two. While similarity based evaluation makes ob-vious that different contextual features capture dif-ferent properties of words, it is not clear which kind similarity notion is more useful when word repre-sentations are used as features for NLP tasks. We answer this question for sentence level classification tasks in the next section.
 We consider three common sentence classification tasks: TREC question type classification (QC), bi-nary sentiment classification on Stanford X  X  Senti-ment Treebank (SST), and relation identification be-tween pairs of nominals (RI) using the SemEval 2010 dataset. The experiments aim to answer two questions. First, to assess the effect of different context features for word embeddings when used in sentence classification tasks, given their differ-ent behaviour on word similarity evaluation. Sec-ond, to experiment with methods of using the de-pendency context embeddings themselves as a way to provide classifiers with dependency syntactic in-formation. We carry out experiments with three dif-ferent classification methods: SVMs with averaged embeddings, the Convolutional Neural Network of Kim (2014), and a Long Short Term Memory recur-rent neural network (Hochreiter and Schmidhuber, 1997). These classifiers have some distinct charac-teristics. The SVM does not take into account the structure of the sentence, nor does it build any in-ternal representations. On the other hand, both the CNN and LSTM networks operate on sequences of words and build internal representations before pre-dicting the class label distribution. However, they do not have access to explicit syntactic information.
We first give a description of the classification methods and the way embeddings are used as fea-tures, followed by the description of the tasks and results. 5.1 Classification Methods SVM with averaged embeddings We create a sentence representation by averaging embeddings of sentence features (words and dependency contexts). This can be considered the equivalent of a Bag-of-Words sentence representation in the embedding space, hence called Bag-of-Embeddings (BoE). We then train a classifier by applying a Support Vector Machine with a Gaussian kernel: For hyperparameter tuning, we set parameter  X  of the kernel to 1 /k , where k is the number of features (dimensionality of embeddings), and then perform cross validation for the c parameter using the stan-dard Win5 word embeddings in the question classi-fication task.
 Convolutional Neural Network (CNN) We use the simple Convolutional Neural Network of Kim (2014) that has been shown to perform well in mul-tiple sentence classification tasks. The network X  X  in-put is a sentence matrix X formed by concatenating k -dimensional word embeddings. Then a convolu-sequence of length h to get a feature map: followed by a max-over-time pooling operation to get the feature with the highest value: The pooled features of different filters are then con-catenated and passed to a fully connected softmax layer to perform the classification. The network uses multiple filters with different sequence sizes covering different size of windows in the sentence. All hyperparameters of the network are the same as used in the original paper (Kim, 2014): stochastic dropout (Srivastava et al., 2014) with p = 0 . 5 on the penultimate layer, 100 filters for each filter region with filter regions of width 2,3 and 4. Optimization is performed with Adadelta (Zeiler, 2012) on mini-batches of size 50.
 Long Short Term Memory (LSTM) LSTM net-works (Hochreiter and Schmidhuber, 1997) are re-current neural networks where recurrent units con-sist of a memory cell c and three gates i , o and f . Given a sequence of input embeddings x , LSTM outputs a sequence of states h given by the following equations: where W  X  R 4 k  X  2 k ,  X c t is a candidate state for the memory cell and is element-wise vector multi-plication. The distribution of labels for the whole sentence is computed by a fully connected softmax layer on top of the final hidden state after applying stochastic dropout with p = 0 . 25 . We use 150 di-mensions for the size of h , Adagrad (Duchi et al., 2011) for optimization and mini-batch size of 100. 5.2 Sentence Feature Representations We provide syntactic information to each classifier in the following manner. First we parse each sen-tence to get a dependency graph. Each node in the graph is associated with a word w having an embed-ding v w and a set of dependency context features d ,d 2 ,...,d C with embeddings v d exactly like during the dependency based skipgram training process. We then create a representation x of that node using different combinations of its as-sociated word and dependency context embeddings:  X  Words : Using only word embeddings  X  Dep : A node X  X  representation becomes the av- X  Wavg : Combination of the word and depen- X  Conc : Similar to the Wavg, but dependency
The above methods are used with the LG and EXT variants to create context specific node representa-tions. For the EXT model, both word and depen-dency context embeddings used come from the em-bedding layer weights. The Words method is the only one that can be applied to the Win5 model. It is the most commonly used method to utilize word rep-resentations as features and our baseline. To make the comparison more fair for the Win5 model we in-clude two additional variations that utilize both the embedding and prediction layer weights as an en-semble method for creating a word X  X  representation:  X  Win5 AvgE : Ensemble made by averaging word  X  Win5 ConcE : Another ensemble made by con-
Ensemble techniques have been reported to out-perform simple word representations in some word similarity tasks (Levy et al., 2015). Since the EXT skipgram version uses symmetric weight matrices for the embedding and prediction layer, ensemble methods like the above could also be applied, but are not considered for these experiments. Note that contrary to the dependency based models, these en-semble methods do not create context specific repre-sentations.

The dependency graph X  X  node representations are used as a sequence of embeddings respecting the or-der of the sentence to become the input for the CNN and LSTM. For the SVM BoE, word and depen-dency contexts of the whole sentence are averaged separately for the Words and Dep method, and then averaged again for the Wavg method or concatenated for the Conc method. As we are evaluating perfor-mance of embeddings, we do not perform updates during training of CNNs and LSTMs. 5.3 Datasets and Results TREC Question Classification The TREC Ques-tion Classification dataset (Li and Roth, 2002) con-sists of 5452 training questions and 500 test ques-tions. The task is to classify each question with one of six labels (e.g. location, definition, ...) depend-ing on the answer they seek. For CNNs and LSTMs 10% of the training data were used as the dev set to pick the best model among different iterations. Clas-sification accuracy results for each input representa-tions and classification method can be seen in Table 2. We also report the state of the art result by the dependency convolutional neural network of Mu et al. (2015). Their model consists of a convolutional neural network that takes a dependency tree at the input layer instead of a sequence, and uses heuris-tics to choose the subset of nodes where pooling is applied.
 SST-2 The Stanford Sentiment Treebank dataset (Socher et al., 2013) has fine grained sentiment polarity scores for movie reviews on the phrasal and sentence level. The binary version of the task considers only positive and negative sentiment la-bels, resulting in a 6920/872/1821 split for train-ing/dev/testing sets. All the models were trained us-ing only the sentence level annotations. Classifica-tion accuracies for all models are reported in Table 3. The state of the art for this dataset comes from Kim (2014) using the same convolutional neural network as we do, but also utilizing the phrasal level anno-tations which provide about an order of magnitude larger training set. In addition, this specific configu-ration of the network (multichannel) uses two chan-nels at the input layer, one updating the word embed-dings during training and one that keeps them static as we do in our experiments.
 Embeddings SVM CNN LSTM Win5 Words 80.1 83.5 76.1 Win5 AvgE 79.5 83.2 76.9 Win5 ConcE 80.3 82.9 77.6 LG Words 78.5 84.5 77.2 LG Dep 76.0 76.8 69.1 LG Wavg 78.9 82.0 78.6 LG Conc 79.8 82.7 79.7 EXT Words 80.5 84.1 77.6 EXT Dep 77.7 77.2 69.6 EXT Wavg 80.6 84.6 75.7 EXT Conc 80.6 83.5 79.8 CNN-multichannel 88.1 SemEval 2010 Relation Identification The Se-mEval 2010 Relation Identification task (Hendrickx et al., 2009) considers the classification of semantic relations between pairs of nominals into 19 classes. The classes are formed by 9 types of relations (e.g. cause-effect, component-whole, ...) with direction-ality taken into account and an extra OTHER class. We only used the shortest dependency path between the two nominals as the input to classifiers. In table 4, we report results using the official SemEval metric of macro-averaged F1-Score for (9+1)-way classifi-cation, taking directionality into account. The best reported result for this dataset is 85.6 F1-score by Xu et al. (2015) also using a convolutional network on a sequence of word embeddings from the short-est dependency path between the pair of nominals. They also introduce negative samples during train-ing by reversing the subject and object of the rela-tion and WordNet features. Without using WordNet features their model achieves 84.0 F1-score. Our evaluation shows that dependency context em-beddings can provide valuable syntactic information for sentence classification tasks using the three clas-sification methods described. Out of the three tasks, Question Classification and Relation Identification showed great improvements when using dependency context embeddings compared to the baseline, while sentiment classification only showed moderate im-provements. This is in agreement with previous re-search (Li et al., 2015), where explicit syntactic in-formation was provided to classifiers by using tree structured networks and showed that syntax pro-vides small improvements for binary sentiment clas-sification in Stanford X  X  Sentiment Treebank.
It is notable that for QC and RI, using only word embeddings that are trained with syntactic informa-tion (LG and EXT Words models) still outperform the baseline window based skipgram. Using the de-pendency context embeddings as a means to rep-resent the dependency parse of sentences consis-tently outperforms the baseline method across the three tasks and for every classification method. This indicates that this additional syntactic information cannot be recovered by the CNN and LSTM even though they have access to the sequential structure of sentences, at least when trained on datasets of this size. As expected, the SVM BoE benefits the most by the addition of dependency context embeddings since these are its only source of structural informa-tion.
 The dependency context embeddings from the EXT model outperform the LG model, both when used alone and when in combination with the word embeddings. This can be attributed to the additional information they are exposed to during training. The effectiveness of the Wavg compared to the Conc method for combining word and dependency context embeddings seems to depend on the classi-fication method. In genearal, we observe that the CNN performs better with Wavg , while SVM and LSTM with Conc . On the other hand, the ensemble methods of the Win5 model ( AvgE and ConcE ) do not provide any consistent advantage over the base-line. In most cases, AvgE slightly hurts performance while ConcE slighty improves it.

Our evaluation also suggests that best perform-ing models in word similarity tasks do not neces-sarily achieve the best performance in other NLP tasks. When considering only word embeddings as features for sentence classification ( Words method), we observe that the EXT model on average performs better than the Win5 and LG models, while the op-posite is true for word similarity evaluation. This indicates that providing additional contextual infor-mation for training embeddings results in less spe-cialized embeddings for particular types of semantic similarity evaluations, but can be useful for a wide range of sentence level classification tasks.
While the purpose of our experiments is a com-parison of embeddings and little hyperparameter tuning was done for the classifiers, results of the CNN using EXT Wavg representations for QC (95.0) and RI (84.31) are close to the best re-ported results with specifically engineered systems for these tasks: 96.0 for QC (Mou et al., 2015) and 85.6 for RI (Xu et al., 2015). As our method does not depend on a specific classification setting it would be interesting to see if those approaches can further improve using dependency based representations. We compare a window based, a dependency based and an extended dependency based skipgram model in word similarity and sentence classification tasks of question classification, binary sentiment pre-diction and semantic relation identification. For the sentence classification, we use three classifiers (SVM, CNN, LSTM) and experiment with several methods of utilizing dependency context feature embeddings to create representations that capture the syntactic role of words in dependency graphs. We reaffirm that dependency based models pro-duce word embeddings that better capture functional properties of words and that window based models better capture topical similarity. The dependency based word embeddings largely improved the per-formance of the three classifiers for question classi-fication and semantic relation identification, but only marginally for sentiment prediction. Finally, using dependency context features along with the word embeddings we observed better performance for the three classifiers in each task.
 Alexandros Komninos was supported by EP-SRC via an Engineering Doctorate in LSCITS. Suresh Manandhar was supported by EPSRC grant EP/I037512/1, A Unified Model of Compositional &amp; Distributional Semantics: Theory and Applica-tion.

