 While conventional ranking algorith ms, such as the PageRank, learning to rank seeks a function capable of ordering a set of instances using a supervised l earning approach. Learning to rank has gained increasing popularity in information retrieval and machine learning communities. In this paper, we propose a novel nonlinear perceptron method for rank learning. The proposed introduces a kernel function to map the original feature space into a nonlinear space and employs a perceptron method to minimize the ranking error by avoiding converging to a solution near the decision boundary and alleviating the effect of outliers in the training dataset. Furthermore, un like existing approaches such as RankSVM and RankBoost, the propos ed method is scalable to large datasets for online lear ning. Experimental results on benchmark corpora show that our approach is more efficient and achieves higher or comparable accuracies in instance ranking than state of the art methods such as FRank, RankSVM and RankBoost. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Measurement, Performance. Web search, Learning to rank, Perceptron With the ever increasing growth of the Web, the quality of a search engine, measured by its ab ility to return the most relevant results to a user search, has become increasingly important. The PageRank algorithm, on which the first commercially successful search engine Google is based, ha s long been regarded as the de facto method of ranking Web pages [1]. Recently, however, a different ranking approach, known as learning to rank [2-9], has emerged as an active and growing area of research and attracted considerable attention from researchers in both information retrieval and machine learning communities. Unlike the PageRank algorithm, which relies on the vast link structures of the Web to decide the relevancy of a Web page, in learning to rank, a ranking function is learned from a training query is associated with a list of documents ordered by their true relevance to the query. A typical learning to rank algorithm, such as RankSVM [10] and RankBoost [11], studies every pair of the training examples to learn the ranking function. In other words, for any query and an ordered list of n documents associated with the query, the training data consists of n  X  n document pairs. for online learning. In this paper, we propose a novel kernel perceptron method to rank Web pages. Perceptron [12] is an efficient method for learning a linear classifier from training data. A perceptron maps an input vector x to a single binary value, which essentially divides instances of two classes by a linear separator in the feature space. In Fig. 1(a), we demonstrate a perceptron in a two dimensional constant term that does not depend on the input. The perceptron The perceptron method uses an ite rative refinement scheme to learn the weights w . At each iteration, if a given instance is misclassified, the perceptron compares the observed output of this data point with the desired output, and uses their difference to update the weights. It is proven that the perceptron algorithm converges (with bounded error) after a finite number of iterations if the data set is linearly separable [12]. The PRank algorithm [13] is the first to apply the perceptron method to the task of learning to rank. The training data contains which, besides the weight vector w , the PRank algorithm also learns an additional threshold vector 1 1 , ,  X  = k in Figure 1(a), the two vectors w and b linearly divide the feature space into k sub-intervals. During testing, a new document is interval in the feature space. As in the perceptron method, PRank manner. While learning a perceptron is computationally efficient for particular text datasets, are usually not linearly separable, for which the learning algorithm will fail to converge. For ranking problems, for a given query, the num ber of relevant documents is much smaller than that of irre levant documents. Consequently, the data in ranking problems are typically not linearly separable, which means that a linear scoring function may not be the best choice for rank learning. Indeed, the task of finding a linear perceptron that maximizes the number of correctly classified points for an arbitrary input labeled sample is known to be NP-hard. Support vector machines (SVMs) provide a possible solution to examples, SVM uses a kernel function to embed the training examples into a feature space of much higher dimension (in some cases, infinite dimensionality) wher e a hyperplane can be found to linearly separate the data. Howeve r, non-linear classifiers such as SVMs can be substantially slower and do not work well for online learning. It is clear that perceptron methods and SVMs have complementing strengths and w eaknesses when it comes to the task of learning to rank. In this paper, we develop a novel algorithm called kernel perceptron to learn a ranking function. Similar to SVM, the kernel perceptron method builds a non-linear scoring function by mapping, via special kernel functions, the the mapped examples are linearly separable. A perceptron criterion function is then defined to avoid learning solutions near the decision boundary and also alleviate the effect of outliers. Furthermore, just like the linear perceptron method, the kernel perceptron method is efficient and allows for online learning. The remaining of the paper is orga nized as follows. In Section 2, we discuss related work, and introduce the background of the problem. In Section 3, we presen t our approach of using kernel perceptron method for learning to ra nk. In Section 4, we compare our approach with leading learning to rank algorithms such as FRank, RankSVM, and RankBoost. We conclude our work in Section 5. An increasing number of machine learning approaches are being developed to improve the effectiv eness of information retrieval. In particular, much recent research has focused on methods of learning a ranking function that can order documents or Web pages by their relevancy with resp ect to given queries. To name a few, RankSVM [10], which is a variation of a linear SVM, RankBoost [11], which applies the idea of Adaboost using weak learners, RankNet [14], which is based on neural networks, and FRank [15], which improves RankNet by adopting a novel loss function called fidelity loss, all a ttempt to learn ranking functions that are able to preserve the pa ir-wise partial ordering of the Web pages. More specifically, the goal of learning to rank is to derive a ranking function f which can determine relative preference between two Web pages: where f denotes the preference relationship and x i extracted from the i -th Web page. The RankSVM algorithm transforms the learning task into a classification problem. It assumes that f is a linear function: where w is a weight vector. Thus, we have We can create a training dataset where each training example is in representations of two Web pages, and y is a label such that then learn an SVM from the training data to predict the label l for any two Web pages. Constructing the SVM is equivalent to solving the following quadratic optimizing problem: problem, the ranking function learnt by the RankSVM is then x w x f  X  = * ) ( . Similar to RankSVM, RankBoost also transforms the problem of ranking to the problem of binary classification. Just like any boosting approach, at each iterati on, RankBoost trains a weak classifier, and then adjusts the weights of those pairs that are wrongly classified by the weak classifier. RankNet uses a modified back-p ropagation neural network to learn a rank function. It tries to minimize the value of a cost function by adjusting each weight in the network based on the function used in RankNet is based on the difference between a pair of network outputs, while standard neural network on the difference between the network output and the desired output. Specifically, for each pair of Web pages ) , ( j i set, RankNet computes the network outputs ) ( i x f and ) ( Assume j i x x f is given by the training data, then the larger uses a logistic function to map the output of the ranking function f to a probability: ij where ) ( ) ( j i ij x f x f o  X  = , and loss function for training: The cross entropy has some desired properties for a probabilistic ranking framework. However, it al so has some problems. For example, the cross entropy loss function cannot achieve 0 loss upper bound, which makes it vulne rable to outliers. A new approach FRank [15], improves RankNet by adopting a novel loss function called the fidelity loss, which computes the loss of a derived probability distribution p against a desired probability distribution q by : which is bounded and achieves 0 lo ss. Both RankNet and FRank perform well in practice. In this section, we describe our novel kernel perceptron ranking (KeepRank) method for the task of learning to rank. Our goal is to combine the strength of the perceptron method (computational efficiency) and that of the SVM method (nonlinearity) while overcoming their weaknesses. the j -th document returned for query i Q . Each document represented by a feature vector () d x x x , , , 2 1 d and a label y denoting the relevance of document irrelevant). Without loss of generality, for each query training dataset can be represented as a set of M instances, where each instance is represented by a feature vector relevance score p y with respective to query the index of the query throughout this paper when referring to a query-document pair. documents returned for the query Q i . The task of learning to rank for a given query q , f ( q , D a ) &gt; f ( q , D b is ranked higher than document D b . training data such that for any given query q , f ( q , D D is any relevant document with respect to the query q and D any irrelevant document. One po ssible solution we can construct document and negative otherwise, or simply 0 ) , ( &gt; linear perceptron function is of the following form: constant that represents the bias . For convenience, we can rewrite dimensional space x . For typical ranking problems, the number of relevant documents is much larger than the number of irrelevant documents for a given query (data imbalance). Furthermore, the number of training data is much larger than that of features. Consequently, the data in ranking problems are typically not linearly separable and a linear scoring function may not be the best choice for rank learning. To deal with this problem, in stead of using a linear scoring function in the original feature space for nonlinear problems, one can perform a nonlinear transformation by mapping the original features into a higher-order feature space and apply the linear model in the new space, which results in a nonlinear model in the original space. For example, a quadratic scoring function can be represented as follows sophisticated separating boundary. In general, we can construct a nonlinear mapping function ) ( x  X  , and accordingly the scoring function becomes: where q is the dimensionality of the nonlinear feature space. to determine a good nonlinear function. Enlightened by the ideas in kernel machines [16], herein we introduce a nonlinear scoring function without using a specific nonlinear mapping. We consider a potential solution of Eq. 9 as follows: where k  X  are the parameters to learn. With Eqs. 9 and 10, the scoring function can be rewritten as in Eq. 11 where and is often called a kernel function. Clearly, we need only define a kernel function without considering the specific nonlinear mapping function ) ( x  X  . Two commonly used kernel functions are polynomials of degree t basis functions (RBF)  X   X  algorithm for classification [12], we attempt to solve this problem by minimizing a perceptron criterion function E ( k  X  ) defined as:  X  (13) where  X  is the set of misclassified samples. Since negative. One potential problem w ith the criterion function in Eq. 13 is that we might end up with the boundary point 0 which minimizes E ( k  X  ). Another potential problem is that the function can be dominated by outliners whose E values can be criterion function as follows:  X  (14) where  X  is the set of misclassified samples with converging to the boundary point. Apparently, if all the samples 14 is a normalization factor, which ensures that the effort of criterion function, we calculate the gradient of ) ( given by  X  (15) We call the proposed algorithm KE rnE l P erceptron Rank ing method (KeepRank). The pseudocode and the update rule of this algorithm are given below. KeepRank: Pseudocode for Kernel Perceptron Ranking Input: training samples Parameters: c (positive number), 2  X  (in RBF kernel). Initialization: ) 0 ( k  X  . While (convergence = false) { } In the pseudocode, instead of updating the coefficients in a batch mode, we modify the coefficients by considering one sample at a time. This allows for online l earning. The convergence condition can be specified as either (1) error bound; or (2) sufficiently large our kernel function where 2  X  is the parameter to be determined. Notice that the real value of c is not that important , as long as c is greater than zero. During training, we use the cross-validation method to set all the parameters. The ranking value of a new sample can be calculated using Eq. 11 and all the training samples ) ( k x . In this section, we evaluate the KeepRank algorithm using the LETOR dataset. We compare KeepRank to three well-known ranking algorithms: FRank, RankSVM, and RankBoost. LETOR (Learning TO Rank) is a benchmark collection for learning to rank research [18]. The most recent version, LETOR 3.0, was released in December 2008. There are seven datasets in LETO R3.0 extracted from information retrieval data collection: topic distillation 2003 (td_2003), topic distillation 2004 (td_2004), OHSUMED, homepage finding 2003, homepage finding 2004, named pa ge finding 2003, and named page finding 2004. We evaluate our method on three commonly-used datasets: td_2003, td_2004 and OHSUMED. Each dataset consists of a set of document-query pairs and is partitioned into five folds for 5-fold cross valida tion. In each fold, there are three groups of data: training, validation, and test. Tables 1-3 show the for td_2003, td_2004, and OHSUMED, respectively. For td_2003 and td_2004, there are about 1,000 retrieved examples per query, For OHSUMED, there are about 150 documents per query. Only training and validation datasets are used for determining our model and parameters. In td_2003 and td_2004, each document-query pair is labeled as either relevant or irrelevant and is represented by 64 features extr acted from all streams (URL, title, whole document, anchor and body), such as Term Frequency (TF) of body, TF of title, Inverse Document Frequency (IDF) of anchor, and Document Length(DL) of body. The OHSUMED collection is a subset of MEDLINE, where a query is a medical search need and a returned document includes title, abstract, MeSH indexing terms, author information, source, and publication type. Each document-query pair is represented by 45 features extracted from all streams (15 features extracted from title, 15 from abstract, and 15 from title plus abstract. The data we use are normalized on per query basis. Table 1. Number of queries and examples in fold 1 of td_2003 
Number Training data Validation data Test data # of queries 30 10 10 # of examples 29,058 10,000 10,000 Table 2. Number of queries and examples in fold 1 of td_2004 
Number Training data Validation data Test data # of queries 45 15 15 # of examples 44,146 15,000 15,000 
Number Training data Validation data Test data # of queries 63 21 22 # of examples 9,219 3,538 3,383 While many different measures ha ve been proposed for evaluating the performance of an information retrieval system, we consider three methods commonly used in learning to rank: precision at position n (P@n) [19], mean aver age precision (MAP) [19], and normalized discounted cumulative gain (NDCG) [20, 21]. These performance measures emphasize the top results returned.  X  Precision at position n (P@n) For a given query, P@n measures the relevance of the top n results in the ranking list, which is given by the ratio of number of compute the mean P@n value by averaging the P@n values over all the queries.  X  Mean average precision (MAP) For a given query, we can first calculate the average precision (AP) of the P@n for all relevant documents. where N is the number of retrieved documents for this query and rel ( n ) is a binary function: if the n-th document is relevant to the query, rel ( n ) = 1; otherwise, rel ( n ) = 0. MAP simply averages the AP values of all the queries.  X  Normalized discounted cumulative gain (NDCG) For a query, the NDCG of its ranking list at position n is defined as and the normalization constant Z n is chosen so that the NDCG of a perfect ranking is 1. In information retrieval, users are often interested in the top returned documents. Thus, we report P@1 to P@10 and NDCG@1 to NDCG@10. To compare average performance, we also report MAP scores. All the results are for test data. We compare the KeepRank algorithm with three learning-based baseline methods: FRank, Ra nkSVM and RankBoost. The baseline results used in this paper are extracted from LETOR. Table 4 shows the average MAP values for the three data sets. KeepRank outperforms the baseline algorithms for td_2003 data and OHSUMED data, and is comparable to FRank and RankSVM for td_2004 data. RankBoost has the best performance for td_2004, but not for td_2003. Table 4. MAP Comparison of KeepRank and the baseline methods on three LETOR datasets. Fig. 2 and Fig. 3 show the average P@n and NDCG results for td_2003 data, respectively. KeepRank outperforms all the baseline algorithms in terms of the very top ranked documents (e.g., top 3) and is comparable to RankBoost and RankSVM as the position n increases. Overall, FRank produces lowest P@n values. Similar results can be obs erved in Fig. 3, where KeepRank consistently outperforms the three baseline algorithms except these at positions n = 5 and 6. At positions 5 and 6, RankSVM produces the best results, but the results from KeepRank are comparable. 
Figure 2. Comparison of precision at position n for dataset Fig. 4 and Fig. 5 illustrate the average P@n and NDCG results for td_2004 data, respectively. Overall, RankBoost produces the best performance. KeepRank is comparable when n is smaller than 3 or just some of the folds, we compare KeepRank and RankBoost for each fold. Fig. 6 shows the P@n ratios of KeepRank to RankBoost for each fold in td_2004; Fig. 7 shows the ratios of NDCG values. As can be seen, KeepRank actually outperforms RankBoost in folds 1 and 4. KeepRank performs poorly for data in fold 3 (both P@n and NDCG values are lowest among the five folds). We hypothesize that the parameters used for fold 4 in KeepRank are not good enough. During the training process, 2  X  was selected from the six values [0.25 0.35 0.5 1 5 10] and the best value of the parameter is se lected by validation set. Based on the results, we conclude that K eepRank either outperforms or is comparable to three baseline methods in td_2003 and td_2004. set of parameters. 
Figure 3. Comparison of NDCG at position n for dataset 
Figure 4. Comparison of precision at position n for dataset Td_2003 0.20 0.26 0.23 0.28 Td_2004 0.24 0.22 0.26 0.23 OHSUMED 0.44 0.43 0.44 0.45 
Figure 5. Comparison of NDCG at position n for dataset Figure 6. The ratio of P@n values for each fold between KeepRank and RankBoost Figure 7. The ratio of NDCG values for each fold between KeepRank and RankBoost Figure 8. Comparison of precision at position n for the dataset Figure 9. Comparison of NDCG at position n for the dataset Fig. 8 and Fig. 9 show the average P@n and NDCG results for the OHSUMED data, respectively. KeepRank outperforms all the baseline algorithms in terms of the very top ranked documents (e.g., top 4) and is comparable to RankBoost and Frank as the position n increases. Overall, RankSVM produces lowest P@n values. Similar results can be obs erved in Fig. 9, where KeepRank consistently outperforms the three baseline algorithms. In this paper, we propose a kern el perceptron-based method called KeepRank for learning preferences in information retrieval. KeepRank introduces the kernel to construct a nonlinear scoring function without considering the specific nonlinear mapping. Furthermore, a perceptron criterion function is proposed to avoid solutions near decision boundary and to alleviate the affect of implement. It maps a feature vector to a relevance score which can then be used to ra nk documents for a query. We evaluate KeepRank usi ng td-2003, td_2004 and OHSUMED datasets in the LETOR3.0 benchmark data and compare it to three state-of-the-art methods, FR ank, RankSVM and RankBoost. Results show that KeepRank outpe rforms the baseline methods in two of the datasets and is comparable to RankSVM and FRank methods in the other dataset. Co mpared to FRank, RankSVM and RankBoost algorithms, KeepRank is eas ier to implement and is an simple alternative to the current rank learning algorithms in information retrieval. While KeepRank is currently desi gned and used for cases with binary judgment: relevant or irrelevant, future work will extend the method to handle multiple levels of relevance. Additionally, we will explore the use of kernel perceptron in ranking with click-through data. This material is based upon work supported by the National Science Foundation under Grant No. 0644366. [1] Page, L., Brin, S., Motwani, R., and Winograd, T. 1998. The [2] Burges, C. 2005. Ranking as Learning Structured Outputs. [3] Brinker, K. and Hullermeier , E. 2005. Calibrated Label-[4] Grangier, D. and Bengio, S. 2005. Exploiting Hyperlinks to [5] Panikkaia, T., Tsivtsiadze, E., Airola, A., Boberg, J., and [6] Cao, G., Nie, J., Si, L., and Bai, J. 2007. Learning to Rank [7] Yeh, J., Lin, J., Ke, H., and Yang, W. 2007. Learning to [8] Veloso, A., Almeida, H., Goncalves, M., and Meira Jr., W. [9] Cao, Z., Qin, T., Liu, T., Tsai, M., and Li, H., 2007. [10] Thorsten Joachims, 2002. Optim izing search engines using [11] Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram [12] Raul Rojas, 1996. Neural Networks: A Systematic [13] Koby Crammer, Yoram Singer, 2001. Pranking for ranking. [14] Chris Burges, Tal Shaked, Erin Renshaw, Matt Deeds, [15] T. Graepel, R. Herbrich, and R.C. Williamson, 2001. From [16] Scholkopf, B. and Smola, A. 2002. Learning with Kernels. [17] Cristianini, N. and Shawe-Taylor, J., 2000. An Introduction [18] Liu, T., Qin, T., Xu, J., Xiong, W., and Li, H. 2007. LETOR: [19] Baeza-Yates, R. and Ribeiro-Neto, B. 1999. Modern [20] Jarvelin, K. and Kekalainen, J. 2000. IR evaluation methods [21] Jarvelin, K. and Kekalainen, 2002. Cumulated gain-based 
