 Because of the high-dimensional nature of NLP datasets, estimating a large number of parameters (a parameter for each dimension), often from a limited amount of labeled data, is a challenging task for statistical learners. Faced with this chal-lenge, various unsupervised dimensionality reduc-tion methods have been developed over the years, e.g., Principal Components Analysis (PCA).

Recently, several supervised metric learning al-gorithms have been proposed (Davis et al., 2007; Weinberger and Saul, 2009). IDML-IT (Dhillon et al., 2010) is another such method which exploits labeled as well as unlabeled data during metric learning. These methods learn a Mahalanobis dis-tance metric to compute distance between a pair of data instances, which can also be interpreted as learning a transformation of the input data, as we shall see in Section 2.1.

In this paper, we make the following contribu-tions: 2.1 Relationship between Metric Learning We first establish the well-known equivalence be-tween learning a Mahalanobis distance measure and Euclidean distance in a linearly transformed space of the data (Weinberger and Saul, 2009). Let A be a d  X  d positive definite matrix which param-eterizes the Mahalanobis distance, d A ( x i ,x j ) , be-tween instances x i and x j , as shown in Equation 1. Since A is positive definite, we can decompose it as A = P &gt; P , where P is another matrix of size d  X  d . d A ( x i ,x j ) = ( x i  X  x j ) &gt; A ( x i  X  x j ) (1)
Hence, computing Mahalanobis distance pa-rameterized by A is equivalent to first projecting the instances into a new space using an appropriate transformation matrix P and then computing Eu-clidean distance in the linearly transformed space. In this paper, we are interested in learning a better representation of the data (i.e., projection matrix P ), and we shall achieve that goal by learning the corresponding Mahalanobis distance parameter A .
We shall now review two recently proposed metric learning algorithms. 2.2 Information-Theoretic Metric Learning Information-Theoretic Metric Learning (ITML) (Davis et al., 2007) assumes the availability of prior knowledge about inter-instance distances. In this scheme, two instances are considered simi-lar if the Mahalanobis distance between them is upper bounded, i.e., d A ( x i ,x j )  X  u , where u is a non-trivial upper bound. Similarly, two in-stances are considered dissimilar if the distance between them is larger than certain threshold l , i.e., d A ( x i ,x j )  X  l . Similar instances are rep-resented by set S , while dissimilar instances are represented by set D .

In addition to prior knowledge about inter-instance distances, sometimes prior information about the matrix A , denoted by A 0 , itself may also be available. For example, Euclidean dis-tance (i.e., A 0 = I ) may work well in some do-mains. In such cases, we would like the learned matrix A to be as close as possible to the prior ma-trix A 0 . ITML combines these two types of prior information, i.e., knowledge about inter-instance distances, and prior matrix A 0 , in order to learn the matrix A by solving the optimization problem shown in (2). min s . t . tr { A ( x i  X  x j )( x i  X  x j ) &gt; } X  u, where D ld ( A,A 0 ) = tr( AA  X  1 0 )  X  log det( AA  X  1 0  X  n , is the LogDet divergence.

To handle situations where exactly solving the problem in (2) is not possible, slack variables may be introduced to the ITML objective. To solve this optimization problem, an algorithm involving re-peated Bregman projections is presented in (Davis et al., 2007), which we use for the experiments re-ported in this paper. 2.3 Inference-Driven Metric Learning Notations: We first define the necessary notations. Let X be the d  X  n matrix of n instances in a d -dimensional space. Out of the n instances, n l instances are labeled, while the remaining n u in-stances are unlabeled, with n = n l + n u . Let S be a n  X  n diagonal matrix with S ii = 1 iff instance x i is labeled. m is the total number of labels. Y is the n  X  m matrix storing training label informa-tion, if any.  X  Y is the n  X  m matrix of estimated la-bel information, i.e., output of any classifier, with  X  Y il denoting score of label l at node i . .

The ITML metric learning algorithm, which we reviewed in Section 2.2, is supervised in nature, and hence it does not exploit widely available un-labeled data. In this section, we review Infer-ence Driven Metric Learning (IDML) (Algorithm 1) (Dhillon et al., 2010), a recently proposed met-ric learning framework which combines an exist-ing supervised metric learning algorithm (such as ITML) along with transductive graph-based la-bel inference to learn a new distance metric from labeled as well as unlabeled data combined. In self-training styled iterations, IDML alternates be-tween metric learning and label inference; with output of label inference used during next round of metric learning, and so on.

IDML starts out with the assumption that ex-isting supervised metric learning algorithms, such as ITML, can learn a better metric if the number of available labeled instances is increased. Since we are focusing on the semi-supervised learning (SSL) setting with n l labeled and n u unlabeled instances, the idea is to automatically label the unlabeled instances using a graph based SSL al-gorithm, and then include instances with low as-signed label entropy (i.e., high confidence label assignments) in the next round of metric learning. The number of instances added in each iteration depends on the threshold  X  1 . This process is con-tinued until no new instances can be added to the set of labeled instances, which can happen when either all the instances are already exhausted, or when none of the remaining unlabeled instances can be assigned labels with high confidence.
The IDML framework is presented in Algo-rithm 1. In Line 3, any supervised metric learner, such as ITML, may be used as the M
ETRIC L EARNER . Using the distance metric learned in Line 3, a new k-NN graph is constructed in Line 4 , whose edge weight matrix is stored in W . In Line 5 , G RAPH L ABEL I NF optimizes over the newly constructed graph, the GRF objective (Zhu et al., 2003) shown in (3). where L = D  X  W is the (unnormalized) Lapla-
Algorithm 1: Inference Driven Metric Learn-ing (IDML)
Input : instances X , training labels Y , training instance indicator S, label entropy threshold  X  , neighborhood size k
Output : Mahalanobis distance parameter A cian, and D is a diagonal matrix with D ii = j W ij . The constraint, makes sure that labels on training instances are not changed during inference. In Line 6, a currently unlabeled instance x i (i.e.,  X  S ii = 0 ) is consid-ered a new labeled training instance, i.e., U ii = 1 , for next round of metric learning if the instance has been assigned labels with high confidence in the current iteration, i.e., if its label distribution has low entropy (i.e., E NTROPY (  X  Y 0 i : )  X   X  ). Fi-nally in Line 7, training instance label information is updated. This iterative process is continued till no new labeled instance can be added, i.e., when U ii = 0  X  i . IDML returns the learned matrix A which can be used to compute Mahalanobis dis-tance using Equation 1. 3.1 Setup Table 1: Description of the datasets used in Sec-tion 3. All datasets are binary with 1500 total in-stances in each.

Description of the datasets used during experi-ments in Section 3 are presented in Table 1. The first four datasets  X  Electronics, Books, Kitchen, and DVDs  X  are from the sentiment domain and previously used in (Blitzer et al., 2007). WebKB is a text classification dataset derived from (Sub-ramanya and Bilmes, 2008). For details regard-ing features and data pre-processing, we refer the reader to the origin of these datasets cited above. One extra preprocessing that we did was that we only considered features which occurred more 20 times in the entire dataset to make the problem more computationally tractable and also since the infrequently occurring features usually contribute noise. We use classification error (lower is better) as the evaluation metric. We experiment with the following ways of estimating transformation ma-trix P : separate random split.
 a separate random split.

We also experimented with the supervised large-margin metric learning algorithm (LMNN) presented in (Weinberger and Saul, 2009). We found ITML to be more effective in practice than LMNN, and hence we report results based on ITML only. Each input instance, x , is now pro-jected into the transformed space as Px . We now train different classifiers on this transformed space. All results are averaged over ten random trials. 3.2 Supervised Classification We train a SVM classifier, with an RBF kernel, on the transformed space generated by the projection matrix P . SVM hyperparameter, C and RBF ker-nel bandwidth, were tuned on a separate develop-ment split. Experimental results with 50 and 100 labeled instances are shown in Table 2, and Ta-ble 3, respectively. From these results, we observe that IDML-IT consistently achieves the best per-formance across all experimental settings. We also note that in Table 3, performance difference be-tween ITML and IDML-IT in the Electronics and Kitchen domains are statistically significant. 3.3 Semi-Supervised Classification In this section, we trained the GRF classifier (see Equation 3), a graph-based semi-supervised learn-ing (SSL) algorithm (Zhu et al., 2003), using Gaussian kernel parameterized by A = P &gt; P to set edge weights. During graph construction, each node was connected to its k nearest neighbors, with k treated as a hyperparameter and tuned on a separate development set. Experimental results with 50 and 100 labeled instances are shown in Table 4, and Table 5, respectively. As before, we experimented with n l = 50 and n l = 100 . Once again, we observe that IDML-IT is the most effec-tive method, with the GRF classifier trained on the data representation learned by IDML-IT achieving best performance in all settings. Here also, we ob-serve that IDML-IT achieves the best performance across all experimental settings. using different methods (see Section 3.3), with n l = 50 and n u = 1450 . All results are averaged over using different methods (see Section 3.3), with n l = 100 and n In this paper, we compared the effectiveness of the transformed spaces learned by recently proposed supervised , and semi-supervised metric learning algorithms to those generated by previ-ously proposed unsupervised dimensionality re-duction methods (e.g., PCA). To the best of our knowledge, this is the first study of its kind in-volving NLP datasets. Through a variety of ex-periments on different real-world NLP datasets, we demonstrated that supervised as well as semi-supervised classifiers trained on the space learned by IDML-IT consistently result in the lowest clas-sification errors. Encouraged by these early re-sults, we plan to explore further the applicability of IDML-IT in other NLP tasks (e.g., entity classi-fication, word sense disambiguation, polarity lexi-con induction, etc.) where better representation of the data is a pre-requisite for effective learning. Thanks to Kuzman Ganchev for providing detailed feedback on a draft of this paper. This work was supported in part by NSF IIS-0447972 and DARPA HRO1107-1-0029.

