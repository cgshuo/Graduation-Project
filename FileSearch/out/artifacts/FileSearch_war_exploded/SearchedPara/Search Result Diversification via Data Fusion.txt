 In recent years, researchers have investigated search result diversification through a variety of approaches. In such sit-uations, information retrieval systems need to consider both aspects of relevance and diversity for those retrieved docu-ments. On the other hand, previous research has demon-strated that data fusion is useful for improving performance when we are only concerned with relevance. However, it is not clear if it helps when both relevance and diversity are both taken into consideration. In this short paper, we propose a few data fusion methods to try to improve per-formance when both relevance and diversity are concerned. Experiments are carried out with 3 groups of top-ranked re-sults submitted to the TREC web diversity task. We find that data fusion is still a useful approach to performance improvement for diversity as for relevance previously. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  retrieval models Algorithms, Experimentation, Measurement, Performance Search result diversification, data fusion, linear combination, weight assignment
In recent years, researchers have taken various approaches to investigate search result diversification [3, 1]. In such situations, information retrieval systems need to consider both relevance and diversity for those retrieved documents. In this short paper, we aim to find out if and how data fusion canhelpwiththis.

Previous research on data fusion (such as in [2, 6, 8]) demonstrates that it is possible to improve retrieval per-formance when we only consider relevance. Now with the new dimension of diversification, we need to re-evaluate the technology. In particular, some fusion methods need to be modified to accommodate for the new situation.

We may divide data fusion methods into two broad cate-gories, according to how they deal with component results: equal-treatment and biased methods. As their names would suggest, the former treats all component results equally, while the latter does not. CombSum, CombMNZ, and the Condorcet method belong to the first category, while the linear combination method is a representative of the second category. Equal-treatment methods can likely be used in the new situation without modification, but the linear combina-tion method needs more consideration.

In linear combination, weight assignment is a key issue for achieving good fusion performance and a considerable num-ber of weight assignment methods have been proposed. Gen-erally speaking, we need to consider two factors for weight assignment. One is the performance of every component retrieval system involved, and the other is the dissimilarity (or distance) between those component systems/results. For the information retrieval systems involved, well-performing systems should be given greater weights , while systems per-forming poorly should be assigned smaller weights. On the other hand, smaller weights should be assigned to those re-sults that are similar to the others, while greater weights should be assigned to those results that are more different to the others. When assigning weights, we may take into consideration performance or dissimilarity, or even both to-gether. It is also possible to use some machine learning tech-niques, known as  X  X earning to rank X , to train weights using some training data. This is especially popular for combining results at the feature level. These methods are aimed at op-timizing a goal that is related to retrieval effectiveness mea-sured by a given metric such as average precision. Because the metrics used for result diversification (e.g., ERR-IA@20) are very different from metrics such as average precision, al-most all methods in this category cannot be directly used for result diversification.

In this paper, we are going to investigate data fusion meth-ods, especially linear combination, for result diversification. Experiments are carried out to evaluate them with 3 groups of results submitted to the TREC web diversity task be-tween 2009 and 2011. Experiments show that the proposed methods perform well and have the potential to be used for this purpose in practice.
As aforementioned, weight assignment is a key issue for the linear combination method. In this section, we look at different ways of dealing with this issue. Firstly, we may consider the performance of the retrieval system in question and its similarity with other retrieval systems separately, so that we may obtain two types of weights. Then a combi-nation of these two types of weights can be used to fuse results. Note that performance and dissimilarity are two in-dependent factors. On the one hand, performance of a result concerns the ranking positions of relevant documents and how diversified those relevant documents are in the ranked list of documents; on the other hand, the dissimilarity of two or more results is concerned with how different the ranking positions of the same documents are in two or more indi-vidual results, making no distinction between relevant and non-relevant documents.

Suppose there are a group of information retrieval sys-to measure their performance and the dissimilarity between them. We further assume that their performances are p 1 , p ,..., p t , respectively, as measured by a given metric (e.g., ERR-IA@20), so that we may then assign the value of a func-tion of p i (such as p i , p i 2 ,andsoon)to w i for (1  X  as the performance-related weight of ir i .

Different approaches are possible for calculating the dis-similarity (or similarity) between two results. One approach is to refer to each result as a set of documents and calculate the overlap rate between two results (sets). It is also possi-ble to calculate the correlation (such as Spearman X  X  ranking coefficient or Kendall X  X  tau coefficient) of two ranked lists of documents. If all the documents in the two results are as-sociated with proper scoring information, then score-based methods such as the Euclidean distance or city block dis-tance can be used. In the following we discuss two different ways of doing it.

Let us consider the top-n documents in all component re-sults. Suppose that document d ij ,inresult r i ,appearsor is referred to in c ij of the other t  X  1 results, then all n top-ranked documents of r i are referred in all other results ref i = n j =1 c ij times. For each document d ij , the maxi-mum times it can appear in the other t  X  1resultsis t  X  1, This fact can be used to define the dissimilarity of r i to other results as dis i are always in the range of 0 and 1. We may de-fine dis i or a function of them as the dissimilarity-related weights. Methods using this definition are referred to as reference-based methods later in this paper. One advantage of using such methods for dissimilarity is that we can obtain the weights for component systems by considering all the documents of them together.

An alternative of calculating the dissimilarity between re-sults is to compare documents X  ranking difference for each pair of them. Let us consider the n top-ranked documents in both results r A and r B . Suppose that m ( m  X  n )documents appear in both r A and r B ,and( n  X  m )ofthemappearin only one of them. For those n  X  m documents that only appear in one of the results, we simply assume that they occupy the places from rank n + 1 to rank 2 n  X  m whilst retaining the same relative orders in the other result. Thus we can calculate the average rank difference of all the docu-ments in both results and use it to measure the dissimilarity of r A and r B . To summarize, we have Here p A ( d i )and p B ( d i ) denote the rank position of d r
A and r B , respectively. which guarantees that v ( r A ,r B ) is in the range of 0 and 1. Based on Equation 2, the dissimilarity weight of r i (1  X  t )isdefinedas
Methods that use this definition are referred to as rank-ing difference based methods. No matter how we obtain the weights for dissimilarity, we may combine dissimilarity-related weights with performance-related weights. Different options, such as p i  X  dis i , p i 2  X  dis i , p i  X  dis i be used to obtain weight w i for information retrieval system ir . At the fusion stage, the linear combination method uses the following equation to calculate scores: where g ( d ) is the global score that document d obtains dur-ingdatafusion, s i ( d ) is the (normalized) score that docu-ment d obtains from information retrieval system ir i (1  X   X  t ), and w i is the weight assigned to system ir i .Allthe documents can be ranked according to the global scores they obtain.
In the 3 successive years from 2009 to 2011, the web track of TREC used the collection of X  X lueWeb09 X . The collection consists of roughly 1 billion web pages crawled from the Web. 3 groups of results are chosen for the experiment. They are 8 top-ranked results 1 (measured by ERR-IA@20) submitted to the diversity task in the TREC 2009, 2010, and 2011 web track. The information about all the selected results is summarized in Table 1.

As we know, it is harder to get improvement over bet-ter component results through data fusion. However, the purpose of the experiments is going to see if we can obtain even better results by fusing a number of top-ranked results submitted.

In the 3 aforementioned groups of results, the 2009 group has the lowest average effectiveness (.1842), the lowest best effectiveness (.2144), and the smallest variance (.0003); the 2011 group has the highest average effectiveness (.4265), the highest best effectiveness (.5284), and the largest variance (.0098); for all three metrics the 2010 group comes second (average is .2645, best is .3356, variance is .0024). We will see that the effectiveness of the fused results is affected by these factors.

In the 2009 group, MSRAACSF [4] is the best performer (ERR IA@20: 0.2144). This run is submitted by Microsoft Research Asia in Beijing. For a given query, sub-topics are mined from different sources including anchor texts, search result clusters, and web sites at which search results are located; and documents are ranked by considering both rel-evance and diversity of mined sub-topics.

In the 2010 group, THUIR 10 DvNov is the best among all 8 runs selected for the experiment. Its performance is 0.3355 when measured by ERR IA@20. Two other runs msrsv 3 div and uwgym (baseline) are slightly better than THUIR 10 DvNov . The technical details of this run are not known because we cannot find the corresponding report for this. We speculate that this run is submitted by a research group in Tsinghua University.

In the 2011 group, uogTrA 45 Nmx 2 [7] is the best per-former (ERR IA@20: 0.5284). This run is submitted by the msrsv3div and uwgym in 2010 and UDCombine2 in 2011 are not chosen because they include much fewer documents than the others and using them would cause problems in calculating weights for the linear combination method and in the fusion process as well.
 Table 2: Performance (measured by ERR-IA@20) of a group of data fusion methods ( p de-notes performance-related weight and dis denotes dissimilarity-related weight; dis is calculated using either Equation 1 or Equation 3; the figures in parentheses indicate the improvement rate of each method over the best component result; the figures in bold indicate the highest value in the column)
Group 2009 2010 2011 Ave. best result 0.2144 0.3355 0.5284 0.3551 p 0.2544 0.3567 0.5398 0.3836 p 2 0.2499 0.3684 0.5343 0.3842 dis  X  p (Eq.1) 0.2552 0.3548 0.5398 0.3833 dis  X  p 2 (Eq.1) 0.2492 0.3705 0.5355 0.3851 dis 2  X  p (Eq.1) 0.2548 0.3533 0.5410 0.3830 dis  X  p (Eq.3) 0.2553 0.3531 0.5388 0.3824 dis  X  p 2 (Eq.3) 0.2503 0.3658 0.5347 0.3836 dis 2  X  p (Eq.3) 0.2534 0.3562 0.5330 0.3809 IR research group at Glasgow University. It uses Terrier with a component xQuAD for search result diversification. The primary idea is to find useful information of sub-topics by sending the initial query to three commercial web search engines.

In each year group, 50 queries are divided into 5 groups: 1-10, 11-20, 21-30, 31-40, and 41-50. 4 arbitrary groups of them are used as training queries, while the remaining one group is used for fusion test. This is referred to as the five-fold cross validation method in statistics and machine learning [5]. Every result is evaluated using ERR-IA@20 over training queries to obtain the performance weight p i On the other hand, either Equation 1 or Equations 2 and 3 are used with training data to obtain dis i for the dissimilar-ity weight. After that, we try 5 different ways of combining the weights: p i , p i 2 , p i  X  dis i , p i 2  X  dis i ,and p
In order to fuse component results by linear combination, reliable scores are required for all the documents required. In this study, we use the reciprocal function [2]. According to [2], the reciprocal function is very good for converting rankings into scores. For any resultant list r = &lt;d 1 , d d &gt; ,ascoreof 1 i +60 is assigned to document d i at rank i .
Experimental results are shown in Tables 2 and 3. Two metrics, ERR-IA@20 and  X  -nDCG@20, are used to evaluate all the fusion methods. The best component result is used as the baseline. When calculating dissimilarity weights by reference based method, or Equation 1, we use the top 100 documents in all component results. We have also tried some other options, including the top 50 and the top 200, though the experimental results are omitted here since they are so similar to what we observed for the top 100. When using rank difference based method, or Equations 2 and 3, Table 3: Performance (measured by  X  -nDCG@20) of a group of data fusion methods ( p de-notes performance-related weight and dis denotes dissimilarity-related weight; dis is calculated using either Equation 1 or Equation 3; the figures in parentheses indicate the improvement rate of each method over the best component result; the figures in bold indicate the highest value in the column)
Group 2009 2010 2011 Ave. best result 0.3653 0.4745 0.6298 0.4869 p 0.4130 0.5071 0.6510 0.5237 p 2 0.4108 0.5226 0.6468 0.5267 dis  X  p (Eq.1) 0.4141 0.5057 0.6513 0.5237 dis  X  p 2 (Eq.1) 0.4100 0.5241 0.6477 0.5273 dis 2  X  p (Eq.1) 0.4150 0.5045 0.6522 0.5239 dis  X  p (Eq.3) 0.4138 0.5054 0.6506 0.5233 dis  X  p 2 (Eq.3) 0.4108 0.5202 0.6478 0.5263 dis 2  X  p (Eq.3) 0.4126 0.5084 0.6488 0.5233 to calculate dissimilarity weights, we use all the documents in each component result.

From Tables 2 and 3, we can see that all the data fu-sion methods involved perform better than the best com-ponent result. However, improvement rates vary from one year group to another. For all the data fusion methods in-volved, the largest improvement of over 10% occurs in the 2009 year group, which is followed by the 2010 year group with improvement between 5% and 11%, while the small-est improvement of less than 4% occurs in the 2011 year group. According to [8], the target variable of performance improvement of the fused result over the best component result is affected by a few factors. Among other factors, the variance of performance of all the component results and the performance of the best component result (see Table 1) have negative effect on the target variable. This can partially ex-plain what we observe: all data fusion methods do the best in the 2009 data set, the worst in the 2011 data set, and the medium in the 2010 data set.

Intuitively, such a phenomenon is understandable. If a component result is very good and a large percentage of relevant documents in multiple categories are retrieved and top-ranked, then it must be very difficult to make any fur-ther improvements over this result; on the other hand, if some of the results are much poorer than the others, then it is very difficult for the fused result to outperform the best component result. Anyway, in all 3 data sets, all of the fused results exhibit improvements over the best component result.

If we compare performance-related weights to combined weights, it is not always the case that combined weights can achieve greater improvement. However, if we examine the greatest improvement in each case, it always happens when some form of combined weights is used. On average over three year groups, dis  X  p 2 (Eq.1) performs the best no matter if ERR-IA@20 or  X  -nDCG@20 is used for evaluation. This suggests that dis  X  p 2 (Eq.1) is a very good option for the combined weight.
In this short paper we have reported our investigation on the search result diversification problem via data fu-sion. Especially we focus on the linear combination method. Two options of calculating dissimilarity weights and sev-eral options of combining performance-related weights and dissimilarity-related weights have been proposed. Experi-ments with 3 groups of results submitted to the TREC web diversity task show that all the data fusion methods perform well and better than the best component result. Among those methods proposed, a combined weight of square per-formance and dissimilarity (calculated by comparing rank-ing difference of pair-wise results) outperforms the others on average.

In summary, the experiments demonstrate that data fu-sion is still a useful technique for performance improvement when addressing search result diversification. [1] E. Aktolga and J. Allan. Sentiment diversification with [2] G.V.Cormack,C.L.A.Clarke,andS.B  X  u ttcher.
 [3] V. Dang and W. B. Croft. Term level search result [4] Z. Dou, K. Chen, R. Song, Y. Ma, S. Shi, and J. Wen. [5] R. Kohavi. A study of cross-validation and bootstrap [6] J. H. Lee. Analysis of multiple evidence combination. [7] R. McCreadie, C. Macdonald, R. Santos, and I. Ounis. [8] S. Wu and S. McClean. Performance prediction of data
