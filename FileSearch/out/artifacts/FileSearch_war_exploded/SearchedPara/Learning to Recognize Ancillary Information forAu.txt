 Automatic PI is the task of detecting if two texts convey the same meaning. For example, the fol-lowing two sentences from the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004): S 1 a : Although it X  X  unclear whether Sobig was to blame, The New York Times also asked employees at its headquarters yesterday to shut down their com-puters because of  X  X ystem difficulties. X  S 1 b : The New York Times asked employees at its headquarters to shut down their computers yester-day because of  X  X omputing system difficulties. X  are paraphrases, while these other two are not: S 2 a : Dr. Anthony Fauci, director of the National In-stitute of Allergy and Infectious Diseases, agreed. S 2 b :  X  X e have been somewhat lucky, X  said Dr. An-thony Fauci, director of the National Institute of Al-lergy and Infectious Diseases.
 Most previous work on automatic PI, e.g., (Madnani et al., 2012; Socher et al., 2011), is based on a di-rect comparison between the two texts, exploiting different similarity scores into a machine learning framework. However, these methods consider sen-tences as monolithic units and can thus be misled by ancillary information that does not modify the main meaning expressed in the text.

For example, the additional text fragment (ATF),  X  Although it X  X  unclear whether Sobig was to blame  X , from S 1 a expresses ancillary information, which does not add much to the message of S 1 b , thus the sentences are considered paraphrases. In con-trast, S 2 b contains the ATF,  X  We have been some-what lucky  X , whose meaning is not linked to any constituent of S 1 b . Since such text expresses rele-vant information, the two sentences are not consid-ered paraphrases.

In this paper, we study and design models for ex-tracting ATFs from a sentence with respect to an-other one and classifying if their meaning is ancil-lary or important. For this purpose, we built a cor-pus of sentence pairs using MSRP, where at least one pair member always contains ATFs. We use SVMs with tree kernels applied to syntactic representations (Severyn and Moschitti, 2012) of ATFs for learning automatic ATCs.

The results derived on MSRP show ( i ) a promis-ing accuracy of our ATC and ( ii ) the output of ATC can be used as a feature for improving the state-of-the-art PI model. Our main purpose in studying computational ap-proaches to the detection of ancillary information is its practical application to PI. Thus, given a pair of sentences (in general two texts), we define ATFs as ancillary information if their semantics: ( i ) only appears in one of the two sentences and ( ii ) does not change the main meaning of the sen-
The definition above along with a syntactic repre-sentation of the sentences can be applied to a para-phrase corpus to build a dataset of ancillary vs. im-portant ATFs. For example, Fig. 1 shows the shallow Moschitti, 2012) of the sentences, S 2 a and S 2 b , re-ported in the introduction, where the red label  X  in-dicates that there is a link between the lemmas of the two sentences (also shown by the dashed edges).  X  can be propagated to the upper nodes to mark the re-lated constituents. For example, the lemma National is matched by the two sentences, thus both its father node, NNP, and its grandfather constituent, NP, are marked.

Such representation makes the extraction of ATFs easier. For instance, Fig. 1 shows the text fragment of S 2 b ,  X  We have been somewhat lucky  X , on the right. This is an ATF since it is not aligned with any frag-ments of S 2 a . Moreover, since it expresses a central information to the sentence meaning, S 2 b cannot be in paraphrase relation with S 2 a . Conversely, the ATF of S 1 a ,  X  Although it X  X  unclear whether Sobig was to blame  X , is ancillary to the main meaning of the sen-tence, indeed, the annotators marked S 1 a and S 1 b as a valid paraphrase. The previous section has shown an approach to ex-tract ATFs that can be potentially ancillary. This uses an alignment approach based on lexical similar-ity, which may fail to align some text constituents. However, these mistakes only affect the precision in extracting ATFs rather than the recall. In other words, we can build a corpus that considers most cases of additional information.

In particular, we design the following simple heuristic: let F i and F j be the largest not aligned (possibly discontinuous) word sequences appearing in the sentence pair ( S i , S j ) , where F i  X  S i and F j  X  S j . We define ATF as the largest text between F i and F j subject to d = | size ( F i )  X  size ( F j ) | &gt;  X  , where size ( F ) is the number of words 2 appear-ing in F . If the condition is not satisfied no ATF is extracted.

The condition over d is important because the sen-tence aligner may fail to match some subsequences, creating false ATFs. However, what is missed from one sentence will be missed also in the other sen-tence. Thus, in general, if we set a small d then F i and F j misalignments may generate false ATFs. In contrast, a large d would clearly prevent this problem, although small ATFs (of size &lt; d ) may be discarded. More precisely, smaller values of  X  may cause the selection of fragments that have corresponding fragments in the other sentence, ex-pressed with dissimilar words (i.e., the aligner failed to match those constituents). Larger values of  X  make the heuristic more precise, but less effective in retrieving smaller ATFs.
 The ATF corpus. We applied the heuristic above to extract an ATF (if exists) from each sentence pair of MSRP. The number of the extracted ATFs de-pends on  X  as reported in Table 1. A manual in-spection of the retrieved fragments revealed that: ( i ) small values of  X  , namely, 1 and 2, cause the extraction of many fragments from one sentence corresponding to fragments expressed with differ-ent words in the other sentence: these are not ATFs. ( ii ) With  X  = 3 , the heuristic is very precise and captures most ATFs appearing in the sentence pairs. ( iii ) Higher values of  X  cause many valid fragments to be missed.

Once ATFs are generated, we need to label them as ancillary or important for PI such that this data can be used for training and testing ATC. Interest-ingly, the data can be automatically labeled exploit-ing the MSRP annotation: given a sentence pair from MSRP, we ( i ) extract the ATF from it and ( ii ) automatically annotate it as ancillary if the pair is a paraphrase and not ancillary otherwise. In other words, an ATF is considered ancillary only if it is ex-tracted from a paraphrase pair. To verify the correct-ness of this approach, two experts manually labeled the obtained data extracted with  X  = 3 and found that only 3.3% of the data was mislabeled with re-spect to one annotator. The Cohen X  X  kappa agree-ment between the annotators was 85%. In these experiments, we first evaluate state-of-the-art PI models to create our baseline, then we experi-ment with our ATC and finally, we combine them to show that ATC can improve PI. 4.1 Deriving PI baselines Dataset. We used MSRP, which consists of 4,076 sentence pairs in the training set and 1,725 sentence pairs in test set. About 66% of the pairs are para-phrases. The pairs were extracted from topically similar Web news articles, applying some heuris-tics that select potential paraphrases to be anno-tated by human experts. We represent the sentence pairs using shallow trees generated with the Stanford Models. We adopted our state-of-the-art PI ap-proach we proposed in (Filice et al., 2015). This, given two pairs of sentences, p a =  X  a 1 ,a 2  X  and p b =  X  b 1 ,b 2  X  , represents instances as shown Fig. 1, and applies tree kernels to them. In particular, we used our best kernel com derived in the work above: SMK ( p a ,p b ) = sf SPTK ( a 1 ,b 1 )  X  SPTK ( a 2 ,b 2 where sf ( x 1 ,x 2 ) = 1 is the Smoothed Partial Tree Kernel (Croce et al., 2011). SMK considers the inherent symmetry of the PI task and evaluates the best alignment between the sentences in the input pairs. The sf is a softmax is not a valid kernel function. SPTK uses a simi-larity function between words: we generated it with skip-gram model applied to the UkWaC corpus (Ba-roni et al., 2009).
 Results. As illustrated in Table 2, a binary Sup-port Vector Machine equipped with SMK achieves a very high accuracy. Moreover, SMK combined with the state of the art in PI. 4.2 Experimental Evaluation on ATC Dataset and models. We created an ATC dataset with  X  = 3 as described in Sec. 3. We make this examples are represented using the shallow tree like the one on the right of Fig. 1. We used three dif-ferent tree kernels: the Syntactic Tree Kernel (STK) by Collins and Duffy (2001), the Partial Tree Ker-nel (PTK) by Moschitti (2006) and SPTK using the word2vec similarity defined before.
 Given the small size of such dataset (Only 8% of MSRP instances have additional fragments), we per-formed a 5-fold cross validation. Table 3 illustrates the Accuracy, Precision, Recall and F 1 of our mod-els. ATC based on SPTK provides the best accu-racy, i.e., 68.6%, which is a promising result for this research. The second most accurate classifier uses PTK, which is more flexible than STK. 4.3 Using ATC in PI We carried out error analysis on PI and observed that the used classifier commits a systematic error: when two sentences share a very similar large part (identical in the extreme case) and one sentence has an ATF, it almost always classifies the sentences as paraphrases, even if the ATF contains important in-formation that invalidates the paraphrase relation. This kind of mistakes can be corrected by ATC.
Thus, we created the following ensemble model: given a pair to be classified, we apply our heuristic for ATF extraction. If the heuristic does not find any fragment in the pair, we only rely on the prediction provided by PI. Otherwise, we combine the predic-tion of ATC applied to ATF with the one of the PI classifier using a stacking strategy (Wolpert, 1992), i.e., the two predictions become the input features of a third classifier that makes the final decision.
To train this meta-classifier, we need the predic-tions from ATC and PI computed on a validation set. Hence, we split the training set in two parts: one part is used for training ATC and PI, while the other is classified with the trained models to produce the predictions for the meta-classifier. Then, the roles of the two parts are inverted. The meta-classifier is a linear SVM (Fan et al., 2008) implemented with KeLP.

Note that: ( i ) since we use 5-fold cross-validation, for each fold, we needed to apply the process de-scribed above to each fold; and ( ii ) all the learning algorithms and kernels adopt default parameters to also facilitate the reproducibility of our results. Results. Table 4 reports the comparison between PI and PI combined with ATC (trained with SPTK). The performance is derived only on sentence pairs with ATFs.
 The first column indicates the kernel used by the PI classifier, while the second column reports  X + X  or  X - X  to indicate if PI is combined with ATC or not, respectively. We note that ATC produces a great im-provement, ranging from 8 absolute percent points over LK to about 3 points over SMK+LK, i.e., the state-of-the-art model. As expected, the more ac-curate the baseline is, the lower the improvement is produced.

It should be noted that only a relative small subset of MSRP contains additional fragments (about 8% when  X  = 3 ). Thus, the impact on the entire PI test-set cannot be large. Tab. 5 reports the accuracy of the previous models on the entire testset. An improve-ment over all models, state-of-the-art included, can be still observed, although it is less visible. In this paper, we study and design models for learn-ing to detect ancillary information in the context of PI. We used a heuristic rule for selecting additional fragments from paraphrase pairs, which, applied to MSRP, generates our ATF dataset. We manually an-notated the latter for training and testing our ATCs.
Our experiments using several kernel models show that ATC can achieve a good accuracy (about 69%) and significantly impact the PI accuracy. Our results suggest that: ( i ) it is possible to recognize information humans ( ii ) to go beyond the current results and technology
In the future, it would be interesting to use meth-ods similar to those successfully used in question answering research, e.g., matching entities in the sentence trees using linked open data (Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015) or enriching trees with semantic information automat-ically produced by classifiers, e.g., (Severyn et al., 2013a; Severyn et al., 2013b).
 This work has been partially supported by the EC project CogNet, 671625 (H2020-ICT-2014-2, Re-search and Innovation action) and by an IBM Fac-ulty Award.

