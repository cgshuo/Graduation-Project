 Sponsored search is a major revenue source for search com-panies. Web searchers can issue any queries, while adver-tisement keywords are limited. Query rewriting technique effectively matches user queries with relevant advertisement keywords, thus increases the amount of web advertisements available. The match relevance is critical for clicks. In this study, we aim to improve query rewriting relevance. For this purpose, we use an active learning algorithm called Trans-ductive Experimental Design to select the most informative samples to train the query rewriting relevance model. Ex-periments show that this approach improves model accuracy and rewriting relevance.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation Algorithm, Design, Experimentation Query rewriting, active learning, sponsored search
Sponsored search marketing is an important business for on-line advertising. On one hand, advertisers choose key-words to describe their advertisements. The keyword set is within a limit size. On the other hand, web searchers issue huge amount of queries per day. The search query set could be infinite large. One challenge for sponsored search is to match user queries with advertisement keywords.

Query rewriting is an effective approach to approximately match between user queries and advertisement keywords. The query rewriting system reformulates user queries into different forms (i.e. rewrites), and these new forms can be used as advertisement keywords to fetch sponsored results. In previous work, machine learning models were developed for rewrite relevance ranking. These models were trained us-ing human editorial relevance judgments. The hand-labeling process is expensive and thus limited.

In our study, we use a Transductive Experimental De-sign [2] to select the most informative samples for labeling and training. Our results show that this approach improves model performance with equal amount of labeled samples, compared with the previous query rewriting method [1]. In addition, no labels are required for selecting the most in-formative training examples, which is different from many other active learning methods. This property is especially important for real application, where labeling and modeling are usually performed separately due to the consideration of resource efficiency. The main contribution of this paper is to apply active learning technique to query rewriting.
Jones et al [1] proposed a query rewriting system for spon-sored search. First, the candidate set is generated by mining query logs and extracting frequent sequential queries issued by web searchers. This is the source of related queries or phrases. Second, a machine-learning model is developed to rank rewrite candidates in terms of their relevance to user original query. The most relevant rewritten queries are then used as keywords to fetch advertisements shown.

In order to train the relevance model, Jones et al [1] randomly sampled (original query, rewritten query) pairs. These pairs were hand-labeled by an editorial team using the four-point scale of relevance judgments given in Table 1.
For each (original query, rewritten query) pair, we gener-ate the following features and use them as model predictor variables: 1. Syntactic features : these features include edit distance, token overlap and character prefix overlap be-tween the original query and its rewrite; 2. Statistical rewrite frequency features : the frequency and proba-bility with which the original query is reformulated into its rewrite. These features are computed by mining query logs; 3. Web occurrence feature : the frequency with which the original query and its rewrite appear on same web pages; 4. Web search feature : the frequency with which the original query and its rewrite share common URLs as their search results; 5. Advertiser bid feature : the frequency with which the original query and its rewrite are bidded by the same advertisers; 6. Location feature : whether there is location change between the original query and its rewrite. Each training pairs is labeled using the guidelines shown in Table 1.
 k (=1,2,5) best rewrites. In this section, we apply an active learning approach called Transductive Experimental Design (TED, [2]) to select the most informative (query, rewrite) pairs. The reason we choose TED is that it explicitly considers both measured and unmeasured samples. Since the relevance of a query pair { q, q } is characterized by the feature vector x ,weuse x to denote the query pair { q i ,q i } hereafter. Let X denote the set of all feature vectors, i.e. X = { x 1 , x 2 ,  X  X  X  ,
Consider a linear regression model where y is the observation , x is the predictor variable , w is the weight vector and is an unknown error with zero mean and  X  2 variance. We define f ( x )= w T x to be the learner X  X  output (relevance score) given input x and the weight vector w . Given a subset Z = { z 1 , z 2 ,  X  X  X  , z k } X  X  ( k&lt;m y be the relevance score associated with z i . The maximum likelihood estimate for the weight vector,  X  w , is that which minimizes the sum squared error: We define Z =( z 1 ,  X  X  X  , z k )and y =( y 1 ,  X  X  X  ,y k ), thus  X  w is given by It is easy to check that E (  X  w Z ) = 0. Therefore,  X  w is an unbiased estimate. For any new x ,let X  y =  X  w T x be its predicted observation. The expected square prediction error can be written as follows: Clearly the expected square prediction error depends on the independent variable x , therefore the average expected square predictive error over the complete data set Z is 1 k In order to minimize the average expected square predictive error, one should find a subset Z which minimizes Eqn. (3).
We compare our approach with random sampling for learn-ing a linear query rewriting relevance model. We apply both random sampling and TED to select 1500 samples from a pool of 5000 examples for labeling and training, and use 1500 held-out examples for testing. We evaluate on a classi-fication task. The positive class includes label score 1 and 2 (i.e. precise and approximate match as mentioned in Table 1). The negative class includes score 3 and 4 (i.e. marginal match and mismatch).

Figure 1 shows the precision-recall curves for top 1 , 2 , 5) best query rewrites. As can be seen, our approach consistently outperforms random sampling. Especially, our approach improves the prediction accuracy when only the best rewrite is considered ( k = 1). In this case, we achieve 1.7% improvement in average precision, or 18% error reduc-tion.
We used the active learning algorithm to select the most informative samples to train the relevance model for query rewriting. Our approach improves the model prediction ac-curacy and query rewriting quality. In practice, our ap-proach can be applied in sponsored search to improve the relevance between user queries and matched advertisements keywords. It can also reduce labor-intensive labeling task by eliminating non-informative training samples, and thus improve production efficiency. [1] R.Jones,B.Rey,O.Madani,andW.Greiner.
 [2] K. Yu, J. Bi, and V. Tresp. Active learning via
