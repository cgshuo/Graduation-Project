
The analysis of social networks is crucial to addressing a diverse set of societally important issues including disease transmission, fraud detection, efficiency of communication networks, among many others. Although technological ad-vances have allowed the collection of these networks (often massive in scale), privacy concerns have severely restricted the ability of social scientists and others to study these networks. Valuable network data remains locked away in institutions, far from the scientists who are best equipped to exploit it, because the data is too sensitive to share.
The challenges of analyzing sensitive graph-structured data have recently received increased attention. It is now well-known that removing identifiers from a social network and releasing a  X  X aively anonymized X  isomorphic network can leave participants open to a range of attacks [1], [7], [17]. In response, a number of more sophisticated anonymization techniques have been proposed [3], [7], [13], [22], [23]. These techniques transform the graph X  X hrough the addition/removal of edges or clustering of nodes into groups X  X nto a new graph which is then published. The analyst uses the transformed graph to derive estimates of various properties of the original.

The conventional goals of such algorithms are privacy and utility , although neither is satisfactorily achieved. Existing approaches provide anonymity, typically through transfor-mations that make each node indistinguishable from others, but they lack rigorous guarantees of privacy. They rely on the assumption that the adversary X  X  knowledge is lim-ited [3], [13], [22] and/or fail to prove that the guaran-tee of anonymity ensures that private information is not disclosed [7], [13], [22], [23]. In terms of utility, while the transformed graph necessarily differs from the original, the hope is that it retains important structural properties of interest to the analyst. A drawback of existing techniques is that they lack formal bounds on the magnitude of the error introduced by the transformation. A common strategy for assessing utility is to measure familiar graph properties and compare these measures numerically to the original data. Empirically, it appears that with increasing anonymity, the graph is rapidly distorted and some metrics are systemati-cally biased [3], [7], [13], [22], [23].

A final limitation of existing techniques is that few scale to the massive graphs that are now collected and studied. Most existing techniques have been evaluated on graphs of about 5,000-50,000 nodes, and may be difficult to scale much larger [7], [13], [22], [23].

In this work, we focus on a specific utility goal X  estimating the degree distribution of the graph X  X nd develop an algorithm that provides provable utility, strong privacy, and excellent scalability. The algorithm returns the degree distribution of the graph after applying a complex two-phase process of random perturbation. The error due to random noise is provably low, yet sufficient to prevent even powerful adversaries from extracting private information. The algorithm can scale to graphs with hundreds of millions of nodes. The techniques we propose here do not result in a published graph. Instead we release only an estimate of the true degree distribution to analysts.

We choose to focus on the degree distribution because it is one of the most widely studied properties of a graph. It influences the structure of a graph and processes that operate on a graph, and a diverse line of research has studied properties of random ensembles of graphs consistent with a known degree distribution [12], [14], [18].

The simple strategy of releasing the exact degree distri-bution fails to provide adequate privacy protection. Some graphs have unique degree distributions (i.e., all graphs matching this degree distribution are isomorphic) making the release of the degree distribution no safer than naive anonymization. In general, it is unclear how to determine what the degree distribution reveals about the structure of the graph. The problem is compounded when either the adver-sary has partial knowledge of graph structure, or the degree distribution is only one of several statistics published. Our goal is to design an approach that provides robust privacy protection against powerful adversaries and is compatible with releasing multiple statistics.

The algorithms proposed here satisfy a rigorous privacy standard called differential privacy [4]. It protects against any adversary, even one with nearly complete knowledge of the private data. It also composes well: one can release multiple statistics under differential privacy, so long as the algorithm for each statistic satisfies differential privacy. Thus while we focus on the degree distribution, additional statistics can be incorporated into the privacy framework.
While an existing differentially private algorithm [5] can be easily adapted to obtain noisy answers to queries about the degree distribution, the added noise introduces consider-able error. In this work, we capitalize on a recent innovation in differentially private algorithms that has been shown to boost accuracy without sacrificing privacy [8]. The technique performs a post-processing step on the noisy answers, using the fact that the queries impose constraints on the space of possible answers to infer a more accurate result. We apply this technique to obtain an accurate estimate of the degree distribution of a graph.

Our contributions include the following.
These contributions are some of the first positive results in the private analysis of social network data, showing that a fundamental network analysis task can be performed ac-curately and efficiently, with rigorous guarantees of privacy.
Admittedly, the degree distribution is just one property of a graph, and there is evidence that a number of other properties are not constrained by the degree distribution alone [12], [14]. Nevertheless, it is hard to imagine a useful technique that distorts the degree distribution greatly. Thus it is important to know how accurately it can be estimated, independently of other properties. A long-term goal is to develop a differentially private algorithm for publishing a synthetic graph offering good utility for a range of analyses. Because of the degree distribution X  X  profound influence on the structure of the graph, we believe that accurate estimation of it is a critical step towards that long-term goal.
We first review the definition of differential privacy, and then propose how it can be adapted to graph data. A. Differential privacy To define differential privacy, we consider an algorithm A that operates on a private database I . The algorithm is randomized and the database is modeled as a set of records, each describing an individual X  X  private information. Differential privacy formally limits how much a single individual record in the input can influence the output of the algorithm. More precisely, if I is a neighboring database X  i.e., one that differs from I by exactly one record X  X hen an algorithm is differentially private if it is likely to produce the same output whether the input is I or I .

The following is a formal definition of differential privacy, due to Dwork [4]. Let nbrs ( I ) denote the set of neighbors of I  X  X .e., I  X  nbrs ( I ) if and only if | I  X  I | =1 where  X  denotes symmetric difference. 1 Definition II.1 ( -differential privacy) . An algorithm A is -differentially private if for all instances I , any I  X  nbrs ( I ) , and any subset of outputs S  X  Range ( A ) , the following holds: where probability Pr is over the randomness of A .
The parameter measures the disclosure and is typically also an input to the algorithm. For example, the techniques used in this paper add random noise to their outputs, where the noise is a function of . The choice of is a matter of policy, but typically is  X  X mall, X  say at most 1, making the probability  X  X lmost the same X  whether the input is I or I .
An example illustrates why this protects privacy. Suppose a hospital wants to analyze the medical records of their patients and publish some statistics about the patient popu-lation. A patient may wish to have his record omitted from the study, out of a concern that the published results will reveal something about him personally and thus violate his privacy. The above definition assuages this concern because whether the individual opts-in or opts-out of the study, the probability of a particular output is almost the same. Clearly, any observed output cannot reveal much about his particular record if that output is (almost) as likely to occur even when the record is excluded from the database.
 B. Differential privacy for graphs
In the above definition, the database is a table whereas in the present work, the database is a graph. Below we adapt the definition of differential privacy to graphs.
The semantic interpretation of differential privacy rests on the definition of neighboring databases. Since differential privacy guarantees that the output of the algorithm can-not be used to distinguish between neighboring databases, what is being protected is precisely the difference between neighboring databases. In the above definition, a neighboring database is defined as the addition or removal of a single record. With the hospital example, the patient X  X  private infor-mation is encapsulated within a single record. So differential privacy ensures that the output of the algorithm does not disclose the patient X  X  medical history.

With network data, which is primarily about relationships among individuals, the correspondence between private data and database records is less clear. To adapt differential pri-vacy to graphs, we must choose a definition for neighboring graphs and understand the privacy semantics of that choice. We propose three alternatives offering varying degrees of privacy protection.

We model the input as a graph, G =( V, E ) , where V is a set of n entities and E is a set of edges. Edges are undirected pairs ( u, v ) such that u and v are members of V . (Results are easily extended to handle directed edges.) While the meaning of an edge depends on the domain X  X t could connote friendship, email exchange, sexual relations, etc. X  X e assume that it represents a sensitive relationship that should be kept private. The focus of the present work is concerned with graph structure, so the inclusion of attributes on nodes or edges is left for future work.

The first adaptation of differential privacy to graphs is mathematically similar to the definition for tables. Neighbor-ing graphs are defined as graphs that differ by one  X  X ecord. X  Given a graph G , one can produce a neighboring graph G by either adding/removing an edge in E , or by adding/removing an isolated node in V . Restricting to isolated nodes ensures that the change to V does not require additional changes to E to make it consistent with V . Formally, G and G are neighbors if | V  X  V | + | E  X  E | =1 . Because this adaptation allows neighboring graphs to differ by at most one edge, we call it edge-differential privacy .

An edge-differentially private algorithm protects individ-ual edges from being disclosed. For some applications, edge-differential privacy seems to be a reasonable privacy standard. For example, consider the study of Kossinets and Watts [10], in which they analyze a graph derived from the email communication among students and faculty of a large university. What makes this dataset sensitive is that it reveals who emails whom; edge-differential privacy protects email relationships from being disclosed.

However, in some applications, it may be desirable to extend the protection beyond individual edges. For example, Klovdahl et al. [9] analyze the social network structure of  X  X  population of prostitutes, injecting drug users and their personal associates. X  In this graph, an edge represents a sexual interaction or the use of a shared needle. Edges are clearly private information, but so too are other properties like node degree (the number of sexual/drug partners) and even membership in the network.

A second adaptation of differential privacy to graphs pro-vides much stronger privacy protection. In node-differential privacy , two graphs are neighbors if they differ by at most one node and all of its incident edges. Formally, G and G are neighbors if | V  X  V | =1 and E  X  E = { ( u, v ) | u  X  ( V  X  V ) or v  X  ( V  X  V ) } .

Node-differential privacy mirrors the  X  X pt-in/opt-out X  no-tion of privacy from the hospital example. It assuages any privacy concerns, as a node-differentially private algorithm behaves almost as if the participant did not appear in at all.
While node-differential privacy is a desirable privacy objective, it may be infeasible to design algorithms that are both node-differentially private and enable accurate network analysis. A differentially private algorithm must hide even the worst case difference between neighboring graphs, and this difference can be large under node-differential privacy. For instance the empty graph ( n isolated nodes) is a neighbor of the star graph (a hub node connected to n nodes). We show in Sec III-A that estimates about node degrees are highly inaccurate under node-differential privacy.
To span the spectrum of privacy between edge-and node-differential privacy, we introduce an extension to edge-differential privacy that allows neighboring graphs to differ by more than a single edge. In k -edge-differential privacy , graphs G and G are neighbors if | V  X  V | + | E  X  E | X  k . A larger setting of k leads to greater privacy protection. If k =1 , then k -edge-differential privacy is equivalent to edge-differential privacy. If k = | V | , then k -edge-differential privacy is even stronger than node-differential privacy, as the set of neighboring graphs under k -edge-differential privacy is a superset of the neighbors under node-differential privacy. If 1 &lt;k&lt; | V | , then k -edge-differential privacy prevents the disclosure of aggregate properties of any subset of k edges. Notice that for those nodes whose degree is less than k , it provides essentially equivalent protection as node-differential privacy. Nodes whose degree is k or larger face more exposure. However, nodes with large degree also have greater influence on the structure of the graph. If our goal is to also allow analysts to accurately measure the graph structure, then it may be necessary to expose high degree nodes to greater privacy risk.

For the remainder of the paper, we will use k -edge-differential privacy as our privacy standard.

In this section, we review the two techniques that form the basis of our approach. The first is a technique by Dwork et al. [5] for answering queries under differential privacy. The second is a recent technique [8] that post-processes the noisy output of the Dwork et al. algorithm to improve accuracy. We use these techniques to obtain a noisy estimate of the degree distribution of the graph. In the next section, we present a fast and scalable implementation of the latter technique. A. Differentially-private query answering
Dwork et al. [5] give a general technique that allows an analyst to pose an arbitrary set of queries and receive noisy answers. The input to the algorithm is a sequence of queries Q where the answer to each query is a number in R . The algorithm computes the true answer Q ( I ) to the queries on the private data and then adds random noise to the answers. The noise depends on the query sequence X  X  sensitivity . Definition III.1 (Sensitivity) . The sensitivity of Q , denoted S Q , is defined as
The sensitivity of a query depends on how neighboring databases are defined. Intuitively, queries are more sensi-tive under node-differential privacy than edge-differential privacy, because the difference between neighboring graphs is larger under node-differential privacy.

However, regardless of how neighbors are defined, the following proposition holds. Let Lap (  X  ) d denote a d -length vector of independent random samples from a Laplace distribution with mean zero and scale  X  .
 Proposition 1 ([5]) . Let  X  Q denote the randomized algorithm that takes as input a database I , a query Q of length d , and some &gt; 0 , and outputs Algorithm  X  Q satisfies -differential privacy.

While this proposition holds for any of the adaptations of differential privacy, the accuracy of the answer depends on the magnitude of S Q , which differs across the adaptations. Using an example query, we illustrate the accuracy trade-offs between k -edge-and node-differential privacy. Let D denote the query that returns the degree of node u if u  X  and otherwise returns  X  1 .

Since the addition of Laplace noise introduces error of  X 
S Q / in expectation, the accuracy of the answer depends on and the sensitivity of D u . Under k -edge-differential privacy, the sensitivity is k  X  X n the worst case, neighboring graphs differ by k edges that are all adjacent to u , making u  X  X  degree differ by k . Thus we expect an accurate answer to
D
For node-differential privacy, however, the sensitivity is unbounded, unless we impose some restriction on the size of the input graph. If graphs are restricted to contain at most n nodes, then the sensitivity of D u is n  X  X he worst case is a pair of neighboring graphs where u is connected to the other n  X  1 nodes in one graph and absent in the other. Since the magnitude of the error is the same as the range of D u , the answer is useless. This example suggests it is infeasible to accurately estimate node degrees under node-differential privacy because the difference in node degrees between neighboring graphs is too large.

Finally, we comment on the relationship between k and . As observed previously, an algorithm that provides -differential privacy for neighboring databases that differ by a single record also provides k -differential privacy for neighboring databases that differ by at most k records [4]. To give a concrete example, suppose we run algorithm  X  Q with =0 . 01 and compute S Q assuming edge-differential privacy. Then as configured,  X  Q satisfies k -edge -differential privacy for k =1 and =0 . 01 ; it also satisfies k -edge -differential privacy for, say, k =10 and =0 . 1 ,or even k = 100 and =1 . 0 . In the next section and in the experiments, we assume that k =1 ; however, the results hold for k&gt; 1 provided that is appropriately scaled as in these examples.
 B. Constrained inference
Hay et al. [8] introduce a post-processing technique that operates on the output of algorithm  X  Q . It can be seen as an improvement on the basic algorithm of Dwork et al. [5] that boosts accuracy without sacrificing privacy. The main idea behind the approach is to use the semantics of a query to impose constraints on the answer. While the true answer Q ( I ) always satisfies the constraints, the noisy answer that is output by  X  Q may violate them. Let  X  q denote the output of  X  Q . The constrained inference process takes  X  q and finds the answer that is  X  X losest X  to  X  q and also satisfies the constraints of the query. Here  X  X losest X  is measured in L 2 distance and the consistent output is called the minimum L 2 solution . Definition III.2 (Minimum L 2 solution) . Let Q be a query sequence with a set of constraints denoted  X  Q . A minimum L 2 solution is a vector q that satisfies the constraints  X  and minimizes ||  X  q  X  q || 2 .

As discussed in [8], the technique has no impact on privacy since it requires no access to the private database, only  X  q , the output of the differentially private algorithm.
This technique can be used to obtain an accurate estimate of the degree distribution of the graph. Our approach is to ask a query for the graph X  X  degree sequence , a sequence of non-decreasing numbers corresponding to the degrees of the graph X  X  vertices. Of course, a degree sequence can be converted to a degree distribution by simply counting the frequency of each degree. The advantage of the degree sequence query is that it is constrained, as explained below.
We now define S , the degree sequence query. 2 Let deg ( i ) return the i th smallest degree in G . Then, the degree sequence of the graph is the sequence S = deg (1) ...deg ( n ) . Under 1-edge-differential privacy, the sensitivity of S is 2: suppose a neighboring graph has an additional edge between two nodes of degree d, d , then two values in S are affected, the largest i such that S [ i ]= d becomes d +1 , similarly for d . Let  X  S denote the application of the algorithm described in Proposition 1 to the S query. A random output of  X  S is denoted  X  s .

Query S is constrained because the degrees are positioned in sorted order. The constraint set for S is denoted  X  S , and contains the inequalities S [ i ] &lt; S [ i +1] for 1  X  i&lt;n . The following theorem gives the minimum L 2 solution for  X  s . Theorem 1 ([8]) . Given  X  s , let M [ i, j ] be the average of the subsequence  X  s [ i, j ] : M [ i, j ]= j k = i  X  s [ k ] / ( j minimum L 2 solution s is unique and is defined as s [ k ]= We use S to refer to the algorithm that first computes  X  and then applies constrained inference to obtain the above minimum L 2 solution. The following example provides an intuition for how S uses sort constraints to reduce the error.
Count Example 1. Figure 1 shows a degree sequence S ( I ) fora25 node graph, along with a sampled  X  s and inferred s . While the values in  X  s deviate considerably from S ( I ) , s lies very close to the true answer. In particular, for subsequence [1 , 20] , the true sequence S ( I ) is uniform and the constrained inference process effectively averages out the noise of  X  s . The twenty-first position is a unique degree in S ( I ) and constrained inference does not refine the noisy answer, i.e., s [21] =  X  s [21] .
As suggested by the example, the error of S can be lower than that of  X  S , particularly when the degree sequence contains subsets of nodes with the same degree. Hay et al. [8] theoretically analyze the error in terms of mean squared error. For a query  X  Q , the mean square error is MSE (  X  Q )= E [ ||  X  Q  X  Q ( I ) || 2 ]= i E [(  X  Q [ i ] where the expectation is over the randomness of  X  Q . Theorem 2 ([8]) . Let d be the number of unique degrees in S ( I ) . Then MSE ( S )= O ( d log 3 n/ 2 ) . In comparison, MSE (  X  S )= X ( n/ 2 ) .

This result shows that error scales linearly with the number of unique degrees, rather than the number of nodes. While this is a promising result, it is not clear what lower MSE in the degree sequence means for an analyst interested in studying the degree distribution . Furthermore, it is unclear whether the closed form solution described in Theorem 1 can be computed efficiently. These issues are addressed next.
We now describe an efficient algorithm for applying constrained inference to the noisy sequence  X  s . A straight-forward approach for computing s is to construct a dynamic program based the definition of s from Theorem 1. However, it requires linear time to compute each s [ k ] , making the total runtime quadratic, infeasible for many large graphs. We present a novel algorithm that reduces the complexity to linear time. The algorithm is a dynamic program that works backwards from the end of the sequence, constructing a partial solution for a subsequence of s . By working backwards, we can reuse computations from previous steps so updating the partial solution requires only (amortized) constant time rather than linear time.

Before describing the algorithm, we introduce some no-tation and restate the minimum L 2 solution of Theorem 1 using this notation. Let the minimum cumulative average at k be denoted as M k = min 1  X  j  X  k M [ k, j ] . Then we can rewrite the solution at s [ k ] as follows:
The basic idea behind the linear time algorithm is to construct s incrementally, starting at the end of the sequence and working backwards toward the beginning. At each step , the algorithm maintains a partial solution for the subsequence  X  s [ ,...,n ]  X  X eaning that the sort constraints are only enforced on this subsequence and the rest of  X  s is ignored. At each step, the subsequence is extended to include another element of  X  s and the partial solution is updated accordingly.

We denote the partial solution as  X  r , and from Equation 1, the value of  X  r at position k is equal to Observe that partial solution  X  r 1 is equal to the desired s .
Given a partial solution  X  r , we can extend the solution to  X  1 by extending the subsequence to include the observation  X  s [  X  1] and updating the partial  X  r to obtain  X  r  X  1 two components of the update procedure. First, we determine the value for the new observation at position  X  1 . From Equation 2 the solution is simply the minimum cumulative average starting at  X  1 ; i.e.,  X  r  X  1 [  X  1] = M  X  1 .
The second step is to check whether including the (  X  1) th element requires updating the existing solution for positions ,...,n . From Equation 2, we can see that we must update any k = ,...,n where the current solution  X  r [ k ] is smaller than the new value at  X  1 position,  X  r  X  1 [  X  1] : Thus, each step requires first computing M  X  1 and then updating the existing solution for positions ,...,n .We show next that we can use the partial solution  X  r to simplify the cost of finding M  X  1 . While computing an individual M  X  1 can take linear time in the worst-case, the amortized cost is only O (1) . Furthermore, we store partial solution  X  r in such a way that once M  X  1 is found, no additional work is required to update the solution.

Given a partial solution  X  r , break it into subsequences such that each subsequence is uniform and has maximal length. Let J be the set of indexes marking the ends of the uniform subsequences. E.g., if all of the elements in  X  r are distinct, then J = { ,...,n } ; if the values of  X  r are all the same, then J = { n } .

The following theorem shows how we can use  X  r to compute the minimum cumulative average M  X  1 . Re-call that M  X  1 = min  X  1  X  k  X  n M [  X  1 ,j ] . Let j  X  note the end index of this minimum average, i.e., j  X  = Theorem 3. Given  X  r and a corresponding J , one of the following conditions holds. Either, j  X  =  X  1 or j  X   X  J . Furthermore, the set J  X  1 = { j  X  } X  X  j | j  X  J and j&gt;j
Theorem 3 shows that the set J can be used to find the minimum cumulative average for  X  1 . The algorithm proceeds by considering the indexes in J in ascending order. The initial cumulative average is set to M [  X  1 , and the average is extended to the next endpoint in J so long as it reduces the average. When the average increases, the algorithm terminates the search. The following Lemma implies that this will be j  X  .
 Lemma 1. Let j  X  be defined as above, then
During the computation, we need only store the set J instead of the entire sequence of  X  r . Updating J is much faster than updating  X  r and from J we can easily reconstruct the solution  X  r . The details are shown in Algorithm 1, which computes J (lines 1-8) for = n,..., 1 . Then, it constructs s using J 1 (lines 10-16).
 Algorithm 1 An algorithm for computing s given  X  s 1: J  X  X  X  , J.push ( n ) 2: for k from n to 1 do 3: j  X   X  k , j  X  J.top () 4: while J =  X  and M [ j  X  +1 ,j ]  X  M [ k, j  X  ] do 5: j  X   X  j , J.pop () , j  X  J.top () 6: end while 7: J.push ( j  X  ) 8: end for 10: while J =  X  do 11: j  X   X  J.pop () 12: for k from b to j  X  do 13: s [ k ]  X  M [ b, j  X  ] 14: end for 16: end while 17: return s Example 2. The following table shows a sample input  X  s along with the computations used to compute s . The algorithm begins with J 5 = { 5 } . Stack J 4 becomes { 4 , 5 } after  X  s [4] is considered since  X  s [4] &lt; M comes to  X  s [3] , the stack J 3 is { 4 , 5 } since M 3 = M [3 , 4] . When  X  s [2] is added since M [2 , 5] &lt;M [2 , 4] , we know M [2 , 5] and J 2 = { 5 } . Then  X  s [1] arrives and makes J equal to { 1 , 5 } . The algorithm rebuilds s as 1 , 5 , 5 , 5 , 5 .
The time complexity of Algorithm 1 is O(n). First, once the stack J is completed, reconstructing s (lines 10-16) clearly takes O ( n ) . Second, the complexity of computing stack J is also linear: it can seen by considering the number of times that line 5 executes. Since each execution of line 5 reduces the size of stack J by 1 and there are only O ( n ) push operations on stack J , we know line 5 executes at most O ( n ) times. In the worst-case the stack J can require O ( n ) space. However, only the top of the stack is accessed during computations and the rest can be written to disk as needed. the algorithm, s , may be non-integral and include negative numbers, when in fact the values of the true degree sequence are constrained to lie in { 0 ,...,n  X  1 } . We would like a solution that respects these constraints since they are required in many applications. Theorem 4 shows that such a solution is computed from s by simply rounding. Theorem 4. Let  X  S be the constraint set  X  S augmented with the additional constraint that each count be an integer in the set { 0 ,...,n  X  1 } . Given s , the minimum L 2 solution for constraint set  X  S , let s denote the sequence derived from s in which each element s [ k ] is rounded to the nearest value in { 0 ,...,n  X  1 } . Then s is a minimum L 2 solution that satisfies the constraint set  X  S .

The goals of our experiments are two-fold. First, we assess the scalability of the constrained inference algorithm introduced in Section IV. Second, we want to understand the tradeoff between privacy and utility. To do so, we first characterize how the noise introduced for privacy distorts the degree distribution. Then, using several metrics to compare distributions, we assess how accurately the distributions derived from S and  X  S approximate the true degree dis-tribution. The accuracy depends on , which governs the amount of privacy. We also assess how the privacy-utility tradeoff changes with the size of the graph (does a bigger graph allow for better utility at a fixed level of privacy?). Finally, we consider one of the most common tasks that analysts perform on a degree distribution: assessing whether it follows a power-law. We measure how the added noise affects the fit of a power-law model.

We experiment on both synthetic and real datasets. The real datasets are derived from crawls of four online so-cial networking sites: Flickr (  X  1.8M nodes), LiveJour-nal (  X  5.3M), Orkut (  X  3.1M), and YouTube (  X  1.1M) [16]. To the best of our knowledge, these are the largest publicly available social network datasets. The synthetic datasets include Random , a classical random graph, which has a Poisson degree distribution (  X  =10 ), and Power , a random graph with a power-law degree distribution (  X  =1 . 5 ). A. Scalability
Figure 2 shows that the runtime of Algorithm 1 scales linearly and is extremely fast. The left figure shows the runtime on the real datasets and the right figure shows the runtime on even larger synthetic datasets of up to 200M nodes. In addition to Random and Power , we include two non-random synthetic distributions, corresponding to the best-and worst-case inputs for the runtime of the algorithm. The best-case is Regular , a uniform degree distribution (all nodes have degree 10 ), the worst-case is Natural , a distribu-tion having one occurrence of each degree in { 0 ,...,n  X 
The small variation in runtime across datasets shows that it is not particularly sensitive to the type of degree distribution. Furthermore, it is extremely fast: processing a 200 million node graph takes less than 6 seconds. In contrast, the algorithm of Hay et al. [8] takes 20 minutes for a 1 million node graph and over an hour to process a 2 million node graph. The efficiency of the improved algorithm makes the constrained inference approach practical for large graphs. B. Utility
We use two measures we use to assess accuracy. First, we use the Kolmogorov-Smirnoff (KS) statistic, a measure used to test whether two samples are drawn from the same distribution. Let the empirical cumulative distribution function (CDF) of sample X = X 1 ,...,X n be defined as F
X ( x )= X and Y is KS ( X, Y ) = max x | F X ( x )  X  F Y ( x ) | .
The KS statistic is insensitive to differences in the tails of the two distributions, so we also use the Mallows distance (aka Earth Mover X  X  distance) to capture deviations in the tail. Given samples X and Y each of size n , with X ( i ) denoting the i th largest sample in X , the Mallows p -distance is An example shows how Mallows distance is more sensitive than the KS statistic to the tail of the distribution. Consider three graphs A , B , and C in which all nodes have degree 1, except in B one node has degree 2 and in C one node has degree n  X  1 . The KS statistic between A and either B or C is O ( n  X  1 ) . The Mallows distance ( p =1 ) between A and B is O ( n  X  1 ) , but between A and C , the Mallows distance is O (1) , capturing the difference between their largest degrees. the true degree distribution along with the differentially private approximations, revealing that S produces a very accurate approximation while  X  S does not. The distributions are represented using the complementary CDF (CCDF), denoted CF and defined as CF X ( x )=1  X  F X ( x ) . Thus, each line shows what fraction of nodes have a degree greater than the given value on the x-axis. Abusing notation, we use S ( I ) ,  X  s , and s , which are all degree sequences, to refer to their corresponding degree distributions. Thus, the line labeled S ( I ) refers to the true degree distribution and the lines labeled  X  s and s refer to the degree distributions derived from differentially private sequences  X  s and s (here =0 . 01 ).
Figure 3(a) shows that noise added to produce  X  s substan-tially distorts the degree distribution. In contrast, s is a much more accurate approximation of S ( I ) . While s exhibits some deviations from the true distribution, the deviations appear to oscillate around the true distribution. This demonstrates that, by exploiting the sort constraints, constrained inference can filter out much of the noise in  X  s .
 individual samples  X  s and s , we also analyze the bias and variance of randomized algorithms  X  S and S . More pre-cisely, we measure bias of S as the expected difference between the CCDFs of S and S ( I ) for each degree X  X .e., bias S ( x )= E [ CF S ( x )  X  CF S ( I ) ( x )] where the expectation is over the randomness in S . The variance of S is var S ( x )= E [( CF S ( x )  X  E [ CF S ( x )]) 2 ] . We focus on S because it is evident from Figure 3(a) that  X  S exhibits substantial bias.
We evaluate the bias/variance of S empirically thru re-peated sampling. The results are shown in the bottom panel of Figure 3(a). The y-axis is the difference in cumulative probability between S and S , CF S ( x )  X  CF S ( I ) ( x ) . The line shows the average difference (bias) and the error bars depict the standard deviation from the average (square root of variance). The line remains near 0, suggesting that S may be an unbiased or nearly unbiased estimator of S ( I ) . The variance peaks wherever the CCDF exhibits steepest change. the relationship between privacy and the accuracy for two measures of accuracy X  X S in 3(b), Mallows in 3(c). We report the average accuracy over 10 trials (random samplings of  X  s ). The amount of privacy is controlled by the parameter (horizontal axis) X  X maller corresponds to stronger privacy.
The results show that S is uniformly more accurate than  X  S , across all datasets, settings of , and both measures of accuracy. Furthermore, for low settings of (stronger privacy), the difference in accuracy is greater, suggesting that the benefit of constrained inference increases with privacy.
Also shown in the figure is the accuracy of an estimate based on random sampling (10% of the degrees are sampled uniformly at random). While sampling does not provide differential privacy, it can serve as a useful reference point. Sampling has very low KS distance (as expected), but higher Mallows distance because random sampling is unlikely to select the high degree nodes in the tail. In fact, sampling has higher Mallows distance than S (except on Random , which is a distribution without long tails). Since analysts often cannot obtain complete graphs and must rely on samples, this result suggests that the additional error due to privacy can be small compared to the sampling error.
 utility tradeoff of S improves as the graph increases in size. The figure reports accuracy on Power graphs of varying size, from 10K to 5M nodes. The results show a clear separation between  X  S and S : as the size of the graph increases, the accuracy of  X  S remains constant whereas the accuracy of improves. Thus, with S , larger datasets yield either more privacy (given a fixed accuracy target, we can lower )or better utility (higher accuracy for fixed ).

The accuracy of  X  S does not improve with graph size because random noise is added to each degree, thus the average error per degree does not change with the size of the graph. However, as Example 1 showed, S can be very accu-rate when the degree sequence contains long subsequences of uniform degrees. As the graph size increases, accuracy improves because the subsequences of uniform degree grow longer (in a power-law graph, the expected proportion of nodes with a given degree is a constant independent of n ).
In this experiment, the parameters k and of the privacy condition remain fixed as n increases. If node degrees were to increase with graph size, then holding fixed would mean that while the absolute disclosure remains fixed, the relative disclosure about a node X  X  neighborhood would increase with n . When evaluating graph models where node degrees increase with size (e.g., forest-fire graphs [11]), it may be appropriate to decrease as n increases.
 iment assesses how accurately the analyst can estimate the parameters of a power-law model using  X  S or S . The experiment is designed as follows. First, we sample a Power graph with parameters  X  =(  X  =1 . 5 ,x min = 10) .We fix this as the true degree distribution. Then we sample  X  s and s and derive corresponding distributions. To each of these three degree distributions, we fit a power-law model using maximum likelihood [2]. The result is three different estimates for the parameters  X  , which we denote  X   X  ,  X   X   X  respectively. We are interested in comparing the model fit to the true degree distribution,  X   X  , to the models fit under differential privacy,  X   X  and  X   X  .

The individual parameter estimates are shown in the middle and right plot of Figure 3(e), but the leftmost plot provides a holistic assessment of the model fit. It assesses model fit using the D statistic of Clauset et al. [2] which measures the KS statistic on the power-law tail of the distribution. We consider two variants of this measure: in one, the tail is defined by the estimate of x min under  X  s or s ; in the other, x min is based on the true x min .
The plots reveal that using either  X  S or S , the analyst will estimate a model that has a close fit to the tail of the original (power-law) distribution, when the tail is defined by the x min estimated on the noisy distribution . However, it also shows that the size of the tail is under-estimated (the power-law behavior becomes apparent only for large degrees). If we compare the models based on how well they fit the true tail of the power-law distribution (solid lines of leftmost plot), we see that  X  S has considerable distortion (note the log-scale) while S is reasonably accurate even at small .
The constrained inference technique that underlies this work was originally proposed in [8]. That work focuses on using constraints to improve accuracy for a variety of count-ing queries. While applications to degree estimation were recognized by the authors, a number of issues necessary for practical network analysis were left open. The present paper shows that inference only requires linear time and that the framework can be extended to include the additional constraints of integrality and non-negativity constraints. Fur-ther, we resolve open questions about the practical utility of the algorithm X  X howing that it scales to large graphs and produces accurate, nearly unbiased estimates of the degree distribution X  X nd provide a more complete characterization of the privacy-utility tradeoffs.

Most prior work focuses on protecting privacy through anonymization , transforming the graph so that nodes cannot be re-identified [3], [7], [13], [22], [23]. The output is a published graph which the analyst can study in place of the original. While publishing a graph allows a broader range of analysis, anonymization is a much weaker notion of privacy and is vulnerable to attack (e.g., [6], [21]).

Furthermore, for the analyst interested in studying the degree distribution, these techniques may not scale to large graphs and can introduce considerable distortion. For exam-ple, the technique of Liu &amp; Terzi [13], which is the most scalable approach, appears to considerably bias power-law degree distributions, reducing the power-law coefficient by 0.5 for reasonable settings of the privacy parameters. Our estimates have much smaller error (e.g., 0 . 004 at =0 . 01 ) and satisfy a much stronger privacy condition.

Differential privacy has been an active area of research (Dwork [4] provides a survey). Enabling accurate analysis of social networks is an often mentioned goal, but we are aware of only a few concrete results: techniques for computing properties such as clustering coefficient that have high sensitivity [19], [20] and a forthcoming approach that estimates the parameters of a random graph model [15].
For the task of approximating the degree distribution of a private social network, we present an algorithm that protects privacy, scales to large graphs, and produces extremely accurate approximations. Our approach satisfies differential privacy, which means that, unlike approaches based on anonymization, it provides extremely robust protection, even against powerful adversaries. Finally, given the importance of the degree distribution to the structure of a graph, we believe that our techniques are a critical first step towards the ultimate goal of publishing synthetic graphs that are both accurate and ensure differential privacy.

