 Recommender systems based on user feedback rank items by aggregating users X  ratings in order to select those that are ranked highest. Ratings are usually aggregated using a weighted arithmetic mean. However, the mean is quite sensitive to outliers and biases, and thus may not be the most informative aggregate. We compare the accuracy and robustness of three different aggregators: the mean, median and mode. The results show that the median may often be a better choice than the mean, and can significantly improve recommendation accuracy and robustness in collaborative filtering systems.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Collaborative Filtering Performance, Reliability, Security
Recommendation systems use ratings provided by users to recommend products. Recommendations can be uniform for the entire user population, such as hotel recommenda-tions on Tripadvisor.com or product recommendations on Cnet.com, or personalised to the taste of a specific user, such as product recommendations on Amazon.com. Com-mon to both types of systems is that they collect ratings of items from their users, aggregate these ratings and allow users to filter the items that are ranked highest according to these aggregates.

Any collection of ratings is likely to contain outliers or even ratings that have been inserted with the purpose of manipulating the recommendation, therefore it is desirabl e that the aggregation function should be as robust as pos-sible against them. The most common way of aggregating Mobasher et al [6] have investigated methods for manipu-lating recommendations by inserting malicious feedback re -ports and show that such attacks are surprisingly easy to carry out in collaborative filtering systems. O X  X ahony et al [7] have shown that while the impact can be somewhat miti-gated by using different similarity metrics, this cannot sig nif-icantly reduce the effectiveness of attacks. Walsh and Sirer [10] have shown that a collaborative-filtering-like mechan ism incentivizes users to report feedback truthfully in order t o receive the best possible recommendations themselves.
Garcin et al [2] analyse in the context of reputation sys-tems how to aggregate feedback ratings into a single value. They consider different ways of aggregating ratings with respects to three criteria: informativeness, robustness a nd strategyproofness. On all these criteria, they show that th e mean seems to be the worst way of aggregating ratings. The median is radically more robust and has the advantage of being strategyproof.

Resnick and Sami [8] apply a reputation mechanism to the problem of manipulation in collaborative filtering systems . They propose an influence limiter where users X  ratings are weighted by their reputation and only users that gain a repu-tation for truthful feedback are given significant influence on the rankings. However, in [9] they show that the robustness thus achieved comes at a high cost.

Mehta et al [5] investigate robust collaborative filtering mechanisms using model-based algorithms. They adapt ro-bust statistical methods and present a Robust Matrix Fac-torisation algorithm that can produce stable recommenda-tions in the presence of spam and noise.

In this paper, we focus on an empirical study of how aggre-gation of ratings influences properties of the ranking. This issue is complementary to the techniques for obtaining hon-est ratings themselves, and our analysis is orthogonal to th e existing works on robustness of feedback systems.
Our main results are based on an empirical study of a col-The data set contains 1682 movies rated by 943 users. 100,000 ratings ranging from 1 to 5 were given by these users. Each user rated at least 20 movies. We constructed a user-based collaborative filtering system based on the k-Nearest Neigh -bour algorithm as outlined in [6].

Given a user u and a target item i for which the system must offer a recommendation, the algorithm first computes the k most similar users to u (neighbours of u ) based on the available ratings. The similarity between users u and v is computed using Pearson X  X  correlation coefficient: where r u,i and r v,i are the ratings of some item i for u and v respectively, and r u and r v are the average ratings of u and v over the set of items.

We implemented a second similarity metric which takes into account how many items two users rated in common. Table 1: The mean-average error (MAE) for differ-ent aggregators and similarities.

We evaluate the quality of the different algorithms for computing the predictions using precision, recall and the resulting F1 metric [3]. The mean-average error (MAE) is difficult to compare since both the median and mode use only a restricted set of values. Table 1 shows the mean-average error for the three aggregators. As expected, the MAE is lower for the mean since this aggregator minimises this measure. Therefore, the MAE is not indicative of the actual performance of the recommender system.

We consider as the set T of relevant target items the items that a user has rated at least as high as the 20th best (i.e. the 20 top-rated items plus any others that are tied for mem-bership in that set). Let k be the size of this set.
To evaluate recommendation recall, we adopt a leave-one-out method where we iteratively consider one of the target items as unrated, compute a set of k recommendations ex-cluding already rated items, and then check if the target item is among them. Since the ranking of items stays the same, we do this in a single run by considering the set R of the top l recommendations, where l + 1 is the rank of the k th non-target item. This works because an item will appear in the top k recommendations when all other target items are excluded if and only if it has at most k  X  1 non-target items that are ranked higher than itself, i.e. if it is in the maximal set of recommendations that just excludes the k th non-target item.

Using this  X  X xpanded X  set of recommendations, we can then compute the recall as the fraction of the target set contained in the recommendations, and the precision as the fraction of the recommendations that are in the target set.
We define:
Tables 2 and 3 show that the mean is by far not the best aggregator of recommendation scores; both precision and re -call are significantly higher when the median is used. The difference is more marked for the Pearson similarity metric, which is more likely to include outliers among the neigh-bours, and it seems that the robustness of the median allows the recommender to take advantage of the Pearson metric in a stronger way than when the mean is used. The mode has the best precision and recall when the Pearson similar-ity and the controversy tie-breaking rule of ratings is used . Figure 2: The hit ratio as a function of the size of the number of recommended items; median and mode use the controversy of ratings as a tiebreaking rule.

Figures 1 and 2 show the hit ratios as a function of the number of recommendations. We can see that when the number of recommended items is small, aggregating ratings by the median makes the recommender very resilient against attack, and it remains more resilient than the mean even when many recommendations are given. This again shows the much greater robustness of the median as a rating aggre-gator. Interestingly, the mode is the most robust aggregato r when the tie-breaking rule is based on the number of ratings. However, it performs poorly when the tie-breaking rule uses the controversy of ratings. Since this tie-breaking rule ma y be more desirable in practice to obtain novel recommenda-tions, the median seems to be the best choice.
Most recommendation systems aggregate user ratings to establish a ranking of alternatives and recommend the highe st-ranked items. It has been common to use a weighted or un-weighted arithmetic mean as an aggregation function. How-ever, there are other choices that may produce better result s in certain circumstances, and it is surprising that this que s-tion has not attracted more attention so far.

We considered three different ways of aggregating ratings: the mean, median and mode, with appropriate weighting when required. If ratings were unbiased and normally dis-tributed, the different notions would not differ much. How-ever, when ratings are collected from a population of users there are many biases [4], and the three methods give very different results.

Theoretical analysis [2] of the breakdown point already points to higher robustness of median and mode to outlier ratings. This seems to be quite important for user-based collaborative filtering recommendation systems. We observ e that the median and mode both result in dramatically higher recommendation accuracy than the arithmetic mean, and we conjecture that this is due to the greater robustness of thes e aggregators against outlier ratings. Another indication o f this is that the median seems to largely solve the problem of
