 1. Introduction
Personal identification based on biometrics is a field of increasing importance due to the prominence given to security in today X  X  society. Individual recognition based on personal traits is a good alternative to systems based on something possessed (ID cards, for instance) or something known (usually passwords). Also, biometric recognition can be used to complement classical systems when enhanced security is required.

Handwriting is a particular type of behavioural biometrics, the branch of biometrics based on measurements derived from an action performed by the user, as opposed to physiological bio-metrics, the branch based on direct measurements of a part of the human body. Handwriting-based biometric recognition has a significant property that is lacked by other forms of biometric recognition: the user can be asked for a specific text and the text used to validate a user can be changed whenever required, thus hindering replay attacks.
 for short, encompasses two different tasks: writer verification and writer identification . The former involves a one-to-one compar-ison where given two samples of handwriting the goal is to determine whether they have been produced by the same person or not, whereas the latter is a one-to-many search where given a sample of handwriting of unknown authorship, the objective is to determine the author.
 tion can follow two different approaches. When spatiotemporal data (i.e. x  X  y coordinates, pressure and/or angles as a function of time) is available, recognition is said to be performed online .
When only the scanned images of the handwriting are available, thus lacking the temporal dimension, recognition is said to be offline .
 a biometric trait for security purposes ( Jain et al., 2005 ). These very same factors can also be considered when assessing the quality of biometric technologies and biometric systems applied to security: universality , they should be based on traits possessed by every person, and uniqueness , the trait must be different for each individual; performance , often related to accuracy (rate of false rejections, rate of false acceptance, rate of failures to enrol, rate of failures to capture, etc.) and speed ; permanence , referred to the degree of invariance over time of the traits considered, non-circumventability , the system should be as robust as pos-sible against the use of substitutes  X  falsified traits-, and collect-ability and acceptability . Collectability refers to the acquisition of the traits and, more precisely, to the ease of acquiring those traits, while acceptability refers to the acceptance of the technology by its users, potential or actual. Although acceptability depends on many different issues such as privacy concerns, intrusiveness, tradition or ease of use, collectability may have a direct impact on it: users will tend to be reluctant to use systems which require burdensome acquisition processes.

Verification based on online handwriting, like all other bio-metric verification approaches, involves two stages: enrolment (or training) and testing. During the enrolment stage, the user has to produce a set of handwriting samples that are used to build a model of their handwriting. This set of samples is often referred as the enrolment set . Samples in the enrolment set are pre-processed (e.g. noise reduction) and the required features (e.g. x and y coordinates, pressure and writing angles) are extracted from the resulting data. With this information a model of the user is built and stored within the system.

During the testing stage, the user provides a new sample of their handwriting, along with an alleged identity. The latter sample is matched against the model and it is accepted (deemed authentic) or rejected, depending on the result of the matching.
Identification also involves these two stages, the only difference being that the sample produced in the testing stage is matched against all the stored models and the authorship is granted to the user whose model yields the best matching result. Fig. 1 graphically depicts the two stages involved in verification.
In the core of any biometric system there is a pattern matching engine. Given two models, this matching engine yields a matching score , a measure that quantifies the degree of similarity between them. The higher the score the more certain the system is that both models have been produced by the same individual. In verification, the decision whether the two models will be deemed from the same individual or not depends on a threshold t : models scoring over t will be considered from the same individual while they will be considered from different individuals if their score is below t .Inour system, the matching engine is based in a novel combination of a Self-Organising Map (SOM) and Dynamic Time Warping (DTW).
The number (and the quality) of the samples acquired during the enrolment stage is an issue of cornerstone importance because a low number leads to poor performance: when the number of samples per user decreases, their ability to effectively discriminate the modelled user from the rest decreases too. In fact, the size of the enrolment set is a critical parameter in recognition systems based on signature ( Fierrez and Ortega-Garcia, 2008 ) and on handwriting in general. Recognition approaches that build models that depend on a larger number of parameters, such as the approaches based on neural networks or statistical methods, tend to require larger enrolment sets in order to attain an accurate enough estimation of the aforementioned parameters.

Unfortunately, it is not always possible to acquire enrolment sets of the required size and quality. On the one hand, potential donors may be unwilling to donate large numbers of samples of their handwriting due to concerns regarding their future use and possible security compromise. On the other hand, the quality of samples may be compromised due to fatigue if the donor is requested to produce an excessive number of samples in a single session. If the solution to the last issue is to collect one or a little amount of samples in each session during an increased number of sessions, a considerable number of donors may be lost in the way. The generation of synthetic samples from real ones is a mean to increase the size of the enrolment sets without increasing the burden put on the user during the enrolment stage.

The uses of synthetically generated biometric data are mani-fold. The synthetic enlargement of datasets, the generation of synthetic human-like traits and the generation of synthetic individuals are among the most relevant.

The synthetic enlargement of datasets aims at increasing the number of samples per user. New samples are obtained transforming and/or combining real samples.

The synthesis of human-like traits is an approach typically followed in applications that require synthesising speech or handwriting. Primitive units taken from a pool constructed from real samples are combined to produce the required speech or writing.

The generation of synthetic individuals is based on the crea-tion of models for a given trait in a population. Once the model of a trait is created, it can be sampled to synthesise new individuals. This approach helps overcome the scarcity of biometric-data donors since synthetic individuals can be used to test the performance of new developed systems, even when no real data is available.

In 2007 , Yanushkevich et al. state that automatically generated data helps creating meaningful sets of data variations that can improve the performance of existing identification systems. Focusing on security issues, they also point out other uses for synthetically generated biometric data:
The improvement of the robustness of the biometric devices thanks to the availability of forged-like data that can help modelling, and thus detecting, forgeries.

The work presented in this paper is the proposal of a method to The main purpose of this method is to automatically enlarge the enrolment set used in the recognition system presented in Sesa-Nogueras and Faundez-Zanuy (2012) with the aim of improv-ing its performance (recognition accuracy).

Also, the ability to improve the performance without the need to ask the user for an increased number of samples during the enrolment phase may have a positive impact on collectability, when compared to a similar improvement achieved by just asking for more real samples. Furthermore, an improved collectability may also lead to a greater degree of acceptability. Thus, synthe-tically enlarging the enrolment set may have benefits spanning three of the seven aforementioned factors that impact on the quality of a biometric system: performance, collectability and acceptability.

The novelty of the proposed method lies in the combination of different aspects: firstly, the synthesis embraces both in-air (pen-ups) and on-surface (pen-downs) trajectories, with each type of trajectory being treated separately. This separation allows analys-ing the impact of the enlargement on each type of stroke and the impact on their combination. Secondly, not only the x  X  y compo-nents of the trajectories are synthesised but also other time-dependent features such as pressure, and the angles between the writing device and the writing surface (azimuth and altitude). And thirdly, the synthesis method not only can be seamlessly integrated into the recognition schema but also shares with it an important resource: the catalogues of strokes, unsupervisedly built by neans of Self-Organising Maps. As the recognition system is based on the application of Dynamic Time Warping to very short sequences of integers, the increase in the size of the users X  models due to the enlargement of the enrolment set does not noticeably impacts the computational burden of the overall system.

It is also worth mentioning that to the authors X  best knowl-edge, no relevant attempts to improve the accuracy of text-based online writer recognition systems by means of synthetically generated data have been previously reported.

The outline of this paper is as follows: the next section highlights some relevant papers that report the use of sample duplication in handwriting recognition and writer recognition. Section 3 gives an overview of the recognition system. Section 4 describes the proposed method. In Section 5 the experimental setup and the obtained results are presented. Section 6 concludes the paper. 2. Related work: sample duplication in handwriting
When the main goal of generating synthetic data is to increase the size of the available datasets, the strategy of choice is sample duplication ( Galbally, 2009 ). Through a series of different trans-formations, one or several samples from the same individual produce one or more new (different) samples that will be regarded as belonging to that individual. The method proposed in this paper belongs to this category. This section highlights some relevant aspects and papers related to the application of sample duplication aimed at the enlargement of the available datasets, first in handwriting recognition and later in writer recognition. 2.1. Sample duplication in handwriting recognition
Handwriting recognition refers to the recognition of the written text itself and not to the recognition of the writer that has produced it. The main goal of handwriting recognition is the extraction of a symbolic representation from the spatial forms that constitute a handwritten text ( Plamondon and Srihari, 2000 ).
Although they are different disciplines pursuing different goals, handwriting recognition and writer recognition are somehow intimately related fields that share some methods and techniques. dent way or it can be performed independently of the writer. In the former case, the recognition system is trained with samples from a single writer and its goal is to recognise the writing of this specific user. Writer-dependent recognition has in the later years attracted some research attention thanks to the flourish of pen-based devices such as PDAs and PC-tablets. In writer-independent recognition, the system is trained with samples from multiple writers and its goal includes recognising the writing of previously unknown individuals. In both cases, the sizes of the training sets are of paramount importance and their enlargement is a question that has been often addressed in the literature. In 2000 , Mori et al. report small improvements in the recognition performance of a
K-Nearest Neighbour ( K-NN ) classifier when synthetically gener-ated numerals are combined with real samples. Generation is performed by means of a method proposed by the authors. to images of single characters acquired from different writers. The transformed images are added to the training set of a
K-NN classifier and a 4% improvement in the recognition rate is reported. Varga and Bunke, in 2003 , generate synthetic textlines from existing lines of handwriting from different writers by means of geometrical transformations and thinning / thickening operations. When the synthetic lines are included in the training sets of an HMM-based cursive handwritten sentence recogniser, the recognition rate is improved in 29 out of 33 cases. Using the same HMM-based recogniser, Varga et al. in 2005 assess the impact produced by the enlargement of the training set with lines generated from character templates and the Delta LogNormal handwriting generation model. Some of the enlarged training sets perform better than the non-enlarged. Also Helmers and Bunke, in 2003 , report some improvements in the performance of the same recogniser when using a generation technique that synthesizes handwritten-like text from ASCII text by means of a dictionary of n-tuples (groups) of characters.
 worth mentioning the work of Mouch ere and Anquetil. In 2006 ,
Mouchere and Anquetil used synthetically generated characters to increase the number of available samples while trying to preserve the user X  X  writing style. Both offline and online transformations are applied to the original online samples. Offline transformations are classical image transformations, actually stretching and slope, while online transformations are speed and curvature changing. With sets of 360 samples (up to 10 original, the rest synthetic ones) of each character they achieve recognition rates that surpass those of reference (86.9% vs. 79.36% with 3 original samples) 2.2. Sample duplication in writer recognition the last decades it has produced a considerable amount of scientific literature regarding both offline and online methods.
As signature has a long tradition as an identity-proving method, a considerable amount of all this work deals with signature-based writer recognition. The interested reader can find extensive surveys of this particular biometric modality in Plamondon and Srihari (2000) , Plamondon and Lorette (1989) , Leclerc and
Plamondon (1994) and Impedovo and Pirlo (2008) . As for non-signature based approaches, the offline modality has received some attention, mainly due to its relevance in forensics, but the online approach has received much less attention and thus the number of available references is much smaller.

To the best of the authors X  knowledge no relevant references exist in the scientific literature dealing with sample duplication applied to non-signature-based writer recognition. For this rea-son, all the references in this section address sample duplication from within the signature based approach.

Duplicated samples are most often produced by distorting, in different ways and with different techniques, one or more real samples. For instance, Huang and Yan in 1997 present an offline signature verification method based on a neural network. In order to tackle the issue of the relatively large number of samples required to train the network, they propose to artificially enlarge the enrolment set by means of new samples obtained by applying perturbations to a small set of genuine ones. The perturbations applied are slant distortion, horizontal and vertical size distortion, rotation, and perspective view distortion. Slightly distorted sam-ples are presented to the network as genuine ones, whereas heavily distorted ones are presented as forgeries. Experimenting on a database of over 3000 (real) signatures donated by 21 people they achieve a false acceptance rate of 11.1% and a false rejection rate of 11.8%, when considering skilled forgeries. It is interesting to note the large number of synthetic samples used: regarding reference genuine samples, the network is trained with 8 real and about 300 synthetic. Regarding samples of forged signatures, over 600 synthetic samples (plus real ones, up to 3000) are presented to the neural network.

Also in the offline field, de Oliveira et al. propose, in 1997 ,a technique for the automatic generation of new signatures obtained from the deformation of real samples. This technique is applied to the reconstruction of the trajectories of the pen obtained from the offline images of the signatures. The signals that represent the trajectories of the pen are deformed by their convolution with deforming functions. Deformations include uni-form and non-uniform scaling, uniform and non-uniform rotation and their combination.

In 2008 , Rabasse et al. introduce a method for the generation of static signature images from dynamic (online) data. Their method aims at generating the x  X  y trajectories of synthetic signatures from two real ones produced by the same user (the seed signatures). Seed signatures are first size-normalised and then derivative dynamic time warping (DDTW) is applied to map points in one seed to points in the other. New signatures are generated by interpolating points between mappings. In a final stage, variability is introduced within the synthesised signatures.
The added variability is modelled according to the natural variability occurring within the user X  X  signature samples. In order to assess whether the synthesised signatures are representative of the real ones, the authors use a commercial signature verification engine to compare the verification rates obtained when compar-ing real signatures with other real signatures and the rates obtained when comparing real signatures with synthesised ones. The error rates (FAR and FRR) were found to be similar.
In the online field, distortion is again the method of choice to obtain synthetic samples. In Rabasse et al. (2007) ,themethod-ology introduced in Rabasse et al. (2008) is extended in order to synthesise dynamic data ( x  X  y coordinates plus pressure, time, status, azimuth and altitude). In the performed experiments, the authors found similar enrolment rates but lower verification rates.
Even if the verification performance in the latter work was slightly lower for synthetic signatures, the works by Rabasse et al. clearly point out the suitability of synthetically generated signa-tures to represent real ones.

Munich and Perona, in 2003 , use duplicated samples in training and in testing. In their system, d uplicated samples are produced by means of resampling by spline interpolation, and affine deforma-tions (horizontal and vertical affine scaling). In training, the dupli-cated samples increase the size of the training set. In testing, duplicated samples increase the nu mber of skilled forgeries avail-able. According to the authors, the use of duplicated samples in training and testing helps achieving a better estimation of the statistical performance of the system (measured by equal error rate  X  EER). In their experiments, this particular use of duplicated samples leads to a slight increase of the error rates.
In 2009 , Galbally et al. propose a generation method based on three different kinds of distortions sequentially applied to the original samples. These distortions are: (a) addition of noise, (b) resampling by a given factor and (c) amplification/attenuation. The parameters that characterize the distortions, are estimated from a dataset not used in testing and are aimed at capturing the intra-user variability. Using a state-of-the-art HMM-based signa-ture verification system, the authors perform some experiments the results of which show that when synthetic samples are added to the real ones, the verification performance is notably increased. They report up to a 70% improvement (the system yields a 23.84% EER when trained with one real sample. This ERR decreases to a 7.87% when 19 synthetic samples are added to the real one). The improvement reported by Galbally et al. show that synthetically enlarged enrolment sets can outperform non-enlarged ones.
The strategy of duplicated samples also has a potential in the analysis of the behaviour of signature-verification systems to help understand why forged signatures are accepted or genuine ones rejected. For instance, in 2006 , Djioua et al. present a software tool based on the Kinematic Theory of rapid human movements that can generate modified signatures from real samples by varying some parameters. The authors plan to use those modified signa-tures to assess how signature-verification systems react to them. 3. Overview of the recognition system
The synthetic generation method that will be presented in the next section can to some extent be regarded as a by-product of a recognition system developed by the authors. What is more, the synthetically generated executions will be used to enhance the performance of the aforementioned system which is used in the experiments described in this paper. The aim of this section is to provide a comprehensive overview of this system in order to facilitate the understanding of the forthcoming sections.
The writer recognition system was presented in Sesa-Nogueras and Faundez-Zanuy (2012) . It follows a stroke-based approach that regards words as two separate sequences: one of pen-up and one of pen-down strokes. A stroke is the trajectory and all the related data recorded by the acquisition device during the time that spans between a pen-down and a pen-up movement (or vice versa). Pen-down strokes correspond to the visible parts of the handwriting, while pen-up strokes are the zero-pressure in-air trajectories performed while transitioning from one pen-down stroke to the next. Fig. 2 shows pen-up and pen-down strokes from the execution of the word INEXPUGNABLE.

The system relies on data that adheres to the SVC format ( Yeung et al., 2004 ). Each execution of a word is given as seven time-sequences (features): x ( t ), the x coordinate; y ( t ), the y coordinate; ts ( t ), a time stamp value; bs ( t ), the button status value (0 for pen-up, 1 for pen-down); az ( t ), the azimuth; al ( t ), the altitude and pr ( t ) the pressure. Thus, the execution of a word can pr  X  t  X  with t A  X  1 , N where N is the length (number of sampling units) of the execution. Segmentation into strokes is straightfor-wardly achieved thanks to the bs ( t ) feature ( pr ( t ) could also have been used). A pen-down stroke starts at a point where bs ( t ) changes from 0 to 1 and ends at a point where bs ( t ) changes from 1 to 0. In pen-up strokes the pr ( t ) feature can be discarded since it is always zero-valued.

The cornerstones of the system are a pair of catalogues of strokes, one for pen-up and one for pen-downs. Each catalogue is built in an unsupervised manner by means of a self-organising map (SOM). A catalogue is a set of classes each one representing a whole set of similar strokes. Once the training of the SOM is completed, each of its cells can be regarded as the prototype of a class of strokes. Thus, a catalogue can be viewed as a vector quantizer ( Gersho and Grey, 1991 ) or a codebook, capable of assigning each stroke to a class. In order to avoid biased results the catalogues can be built from a dataset that will not be used during the testing phase. This particular kind of catalogues is referred as exocatalogues . Catalogues being a parameter of paramount importance for the recognition system, the use of exocatalogues guarantees that the results are not affected by overtraining. Furthermore, exocatalogues allow the bootstrapping of the system with data not coming from the same users that will later on be enroled.

Fig. 3 shows the catalogues obtained from executions of the word DELEZNABLE. Each stroke is the prototype of a whole class.
Although some prototypes look quite alike to others, the reader should notice that these images are just 2D-projections of 5D and 4D trajectories.
 word, a certain number of executions of this particular word (just 3 in our experiments). From these executions a writer X  X  model is built as follows: (a) Two sequences are extracted from each word, one of pen-up (b) Strokes in each sequence undergo pre-processing: length (c) Each sequence of pre-processed strokes is converted into a process.
Verification is performed as follows: the word whose author-ship is questioned is decomposed into the two sequences of its pen-down and pen-up strokes. Each sequence undergoes the same pre-processing and encoding process than the sequences in the users X  models. The catalogues are the same ones that were used in the enrolment phase. Then, each sequence is compared by means of dynamic time warping (DTW) against the sequences in the alleged user X  X  model. Each comparison yields a dissimilarity measure. Pen-up dissimilarity measures are combined into a single measure ( D up ) and the same is done with the measures obtained from pen-downs ( D down ). As a final step these two measures are combined into a single dissimilarity measure of the whole word ( D word ). If this measure is lower than global threshold then the questioned word is deemed authentic. Fig. 5 graphically depicts the verification process.

Identification is performed in a similar manner. In this case the comparison is made against all the known models and the authorship is granted to the user whose model produces the lowest dissimilarity.

It is worth noting that DTW is applied to the sequences of encoded strokes, not to the original sequences of strokes. The former are sequences of integers, each element the index of a prototype in a catalogue. Thanks to the neighbouring properties of the SOMs that materialize the catalogues ( Kiviluoto, 1996 ), the distance between two strokes is well approximated by the distance between their prototypes in the catalogues which is defined as the total number of cells in the shortest path between the prototypes. As DTW is applied to very short sequences of integers, the computational burden associated with the compar-ison is low.
 4. Proposed method 4.1. Overview
This subsection concisely summarises the algorithm that generates synthetic sequences of strokes. The next subsection will provide a more detailed explanation. 1. Two samples from a given user are taken. From each sample, two sequences are obtained, one of pen-up and one of pen-up strokes. 2. Each sequence of strokes is encoded as a sequence of integers, using an existing pair of catalogues. 3. The sequences of encoded pen-down strokes (one from each sample) are aligned using DTW. As a result, a sequence of alignment steps is obtained. Each alignment step is one of { Match, Insertion, Deletion }. 4. The processing of the sequence of alignment steps yields a new sequence of (non encoded) pen-down strokes. This processing is performed, alignment step by alignment step, as follows: Steps 3 and 4 are repeated for pen-up strokes.

Fig. 6 graphically depicts the synthesis process for pen-down strokes. Pen-up strokes follow the very same process. 4.2. Detailed description of strokes (pen-up or pen-down) that will be used to produce a new synthetic sequence. S 1 and S 2 come from different executions of the same word produced by the same writer. s j i denotes the i -th stroke of the j -th sequence.
 versions of S 1 and S 2 , respectively. Idx j i denotes the index of the stroke prototype corresponding to the i -th stroke in the j -th original sequence.
 obtained when DTW is applied to SE 1 and SE 2 . as i A f Match , Insertion , Deletion g .

The algorithm that generates the synthetic sequences from the real ones is as follows:
The function average ( stroke1, stroke2 ) computes a new synthetic stroke that is the average of the two parameters. Actually each feature of each stroke is transformed into the frequency domain by a discrete Fourier transform (DFT) and the resulting sequences of coefficients are averaged. The new sequences of coefficients, one per feature, are transformed back to the original time domain applying an inverse discrete Fourier transform (IDFT).
The sequence of alignment steps is computed by an imple-mentation of the following algorithm, based on classical DTW ( Deller et al., 1993 ):
The function COST ( Index1, Index2 ) computes a distance between two (indexes of) stroke prototypes. Specifically, this distance is the length, in number of units, of the shortest path between the two prototypes in the catalogue of strokes. This function is exactly the same one used in recognition (see Section 3 ).

Notice that DTW-alignment is non-commutative because it can yield different sequences of alignment steps depending on the order of the input parameters SE 1 and SE 2 .An insertion is a stroke in the first sequence with no corresponding matching in the second while a deletion is a stroke in the second sequence with no corresponding matching in the first. What will appear as an insertion when aligning SE 1 with SE 2 will appear as a deletion when aligning SE 2 with SE 1 . As the sequence of alignment steps drives the generation of new sequences of strokes, two different syn-thetic sequences can be generated from each pair of original sequences. In all the experiments described in this paper, two synthetic executions will be generated from every pair of original executions in the enrolment set.

Figs. 7 and 8 show a pair of real sequences of strokes and the new sequences synthetically generated from them. Notice that, in the case of pen-down strokes, the generated executions are perfectly legible. Although not shown in the figures, pressure and writing angles have also been synthesised. 5. Experimental results 5.1. Database and settings
The BiosecurID database ( Fierrez-Aguilar et al., 2010 ) com-prises 8 different biometric traits, including handwritten text. All data was collected during 4 different sessions in a time span of 4 months. The number of donors was 400, with a balanced gender distribution. Regarding handwritten text, each donor was requested to write 16 different Spanish words in uppercase, each one in a single line, without corrections or crossing outs. The acquisition was carried out with a WACOM INTUOS A4 USB pen tablet. 30 of the 400 users have been screened out because their handwriting did present corrections, crossing outs or more than one word in a single line. Table 1 shows the 16 words that donors were requested to write and their lengths (number of letters)
The fact that only uppercase words will be used in the experiments should not be regarded as a negative issue. Two executions of the same word by two different writers may be more alike when in uppercase that when in lowercase. Thus, contrary to some intuitions, uppercase-based writer recognition seems to pose a more challenging problem, since the less personal the style the more difficult to tell one writer from the rest. What is more, uppercase handwriting seems to be more resilient to changes due to aging and mental conditions.

For each one of the 16 words, the whole set of available users (370) has been partitioned into two disjoint subsets. The first subset, the exocatalogues partition, consists of 50 users and has been used to compute the catalogues of strokes (two catalogues, one for pen-up and one for pen-down strokes). This partition has also been used to compute, for each word and type of stroke, the value of a parameter required to transform dissimilarities into scores.
The second set, the testing partition, comprises the remaining 320 users. Sessions 1 to 3 are the original, non enlarged, enrol-ment sets used to build the writers X  models, while session 4 is always used for testing. Each session includes 1 execution of the word. When synthetic executions are generated, they are gener-ated from the executions in sessions 1 to 3. From each pair of original executions a new pair of synthetic ones is generated. Thus, from the 3 possible pairs, 6 synthetic executions are derived in total (See Fig. 9 ).
 Three different enrolment sets can be considered ( Fig. 10 ):
The original enrolment set, comprising the 3 executions acquired in sessions 1 to 3.

The synthetic enrolment set, comprising the 6 executions generated from the executions in the original enrolment set.
The enlarged enrolment set, comprising both the original and the synthetic executions (totalling 9 executions). 5.2. Experiments assesses the recognition performance of synthetic executions when compared to the recognition power of the original execu-tions they derive from. The second experiment reveals the impact of the addition of the synthetic executions to the enrolment set.
In all cases, the sample used in testing (i.e. the sample being compared with the models) is always a real one.

Both experiments consider all the writers in the testing partition without further subdivisions. As said before, this set comprises 320 users which is a number of users quite large when compared to the number of users reported in other online text-based handwriting experiments (for instance, 10 users in Bashir and Kempf (2008) ,15in Hook et al. (2004) ,45in Chapran (2006) , 55 in Li et al. (2007) or 200 in Schlapbach et al. (2008) ).
Regarding identification, the results are given by the identifica-tion rate ( IDR ). The IDR is the percentage of writers correctly identified (Top-1 approach). Rega rding verification, the results are given by the minimum of the detection cost function ( mDCF ). This figure is the lowest value yielded by the detection cost function ( DCF ), the trade-off between false acceptances and false rejections: DCF  X  C fa P fa P t  X  C fr P fr  X  1 P t  X  where C fa and C fr are the costs of a false acceptance and a false rejection respectively, P fa and P fr the probabilities of a false accep-tance and a false rejection and P t the a-priori probability that the user presented to the system is the one who claims to be (not an impostor). In all our experiments C fa and C fr are set to 1 while P set to 0.5. The values of P fa and P fr vary as a function of the rejection/ acceptance threshold. Notice that the minimum of the detection cost function is achieved when the best trade-off between P fa reached, that is when the best possible global threshold is consid-ered. Thus, mDCF is the experimentally-determined upper bound for the performance of the verification s ystem. Detection error trade-off curves (DET-curves) ( Martin et al., 1997 ) can be used to visualise error trade-off.

Table 2 summarises the verification and recognition results achieved by the recognition system, with the 320 users of the testing partition when no synthetic executions are considered.
They were obtained using the original enrolment set (executions in sessions 1 to 3) to obtain the writers X  models and the execution in session 4 for testing. These values will be used as baseline for comparisons. 5.2.1. First experiment. Recognition performance of the synthetic executions (synthetic enrolment set) performance, or discriminative power, that remains in the synthetic executions obtained with the method previously described, when compared to the recognition per formance of the samples from which they are generated. The enrolment sets used to build the writers X  models are the synthetic ones, comprising the 6 synthetic executions obtained from the 3 pairs of original ones.
Table 3 summarises the verification results obtained. In every case three figures are given: the original value for the minimum of the detection cost function (baseline), the minimum of the detection cost function achieved when the enrolment sets only contain synthetic samples and the difference of both values expressed as a percentage relative to the first value. Negative percentage mean that the verification error is greater in the synthetic case (worse performance) while positive values mean that synthetic samples perform better than the original ones from which they were obtained.

The last column in Table 3 (combination of pen-up and pen-down strokes) concisely summarises the verification performance of the synthetic samples relative the performance of the original ones.
Table 4 summarises the identification results. The same cases as in the previous table are considered. In each case three figures are given: the original value for the identification rate (baseline), the identification rate achieved when the enrolment set only contains synthetic samples and the difference between these two numbers expressed as a percentage relative to the original value. Negative percentages mean that the identification rate is lower (worse performance) in the synthetic case while positive values mean that synthetic samples perform better.

The figures shown in Table 3 and 4 and clearly show that synthetic samples retain most of the discriminative power of the real samples they are obtained from. Regarding verification, notice that the biggest difference is 39.5% (word 5, pen-down strokes) although it is 26.2% in the case of the combination (word 12). And not only this, in about one half of the cases, verification errors are lower for synthetic samples than for real ones, being the case of word 9 the most noticeable since, in the case of the combination of both types of strokes, synthetic samples exhibit a performance 21.1% higher than the obtained with the original samples.

Regarding identification, synthetic samples exhibit a perfor-mance very close to the performance of the original ones, often slightly higher. Notice that the biggest decrease is 5.7% (word 7, pen-down strokes). Also notice that in the case of the combination of both types of strokes the identification rate is always higher for synthetic samples except for words 1 ( 1.7%) and 9 (0%).
In both cases, verification and identification, synthetic samples exhibit, with respect to combination, the same behaviour exhib-ited by real samples: combination of both types of strokes achieve better results than each type of strokes individually. 5.2.2. Second experiment. Impact of the addition of the synthetic executions to the enrolment set (enlarged enrolment set)
This experiment can be regarded as having two phases. In the first phase recognition is performed without including synthetic executions in the writers X  models (original enrolment set). In the second phase the enrolment sets used to build the writers X  models are enlarged with the addition of the synthetic samples obtained from the real ones (enlarged enrolment set).

Table 5 summarises the verification results obtained. As in the previous experiment three cases have been considered: verifica-tion based only in pen-up strokes, verification based only in pen-down strokes and verification based in both pen-up and pen-down strokes. In every case, three figures are given: the minimum of the detection cost function achieved when no synthetic executions are considered, the minimum of the detection cost function achieved when synthetic executions are added to the users X  models and the difference of both values expressed as a percentage relative to the original value ( mDCF without synthetic executions). Positive percentages mean that verification error has decreased (improved) when synthetic executions have been used to extend the models.

The last column in Table 5 (difference in the case of the combination) concisely summarises the final impact of the addi-tion of synthetic executions. Notice that, in this case, the impact is always positive for all words but one (word 12, 8%). The decrement in the verification error ranges from a very low 0.5% (word 2) to a noticeable 26.5% (word 11). Also notice that the original verification errors were already quite low, thus the room for improvement was not ample. Fig. 11 shows the DET-curves corresponding to these two words. Fig. 12 shows the DET-curves corresponding to the only word for which the impact is negative. In this latter plot notice that except for the region and its surroundings where the minimums are achieved, the trade-off between false rejections and false acceptances is better when synthetics executions are included.
 as in the previous table are considered. In each case three figures are given: the identification rate when no synthetic executions are considered, the identification rate when the users X  models include synthetically generated executions and the difference between these two numbers expressed as a percentage relative to the original value. Positive percentages mean that the identi-fication rate has increased  X  improved  X  when synthetic execu-tions have been considered.
 the difference is positive. 6. Conclusions words from real samples has been proposed. With regard to this novel method, two issues have been addressed: first, the recogni-tion performance of the synthetic samples per se and, second, the impact of enlarging the enrolment sets with synthetically gener-ated samples.

With respect to the verification performance of the synthetic samples, the experiment carried out show that synthetic samples mayevenperformbetterthantherealonesfromwhichtheywere generated.Inthosecaseswheretheperformanceofthesynthetic samplesisworse,thedecreasetendstobesmallormoderate.Inthe case of the combination, the maximum decrease is of a 26.2%, meaning that the synthetic samples still retain more than 70% of thediscriminativepowerofthes amples they come from. When it comes to identification, for all words in the database but two, synthetic samples perform slightly better than the real ones. Sum-marizing, we can conclude that ou r synthesis approach generates samples that possess a noticeable discriminative power, which may render them appropriate for biometric verification purposes.
Regarding the impact of enlarging the enrolment sets used in a recognition system with synthetic samples produced by this new synthesis method, the results obtained allow drawing some interesting and important conclusions. Firstly, it has been shown that the synthetic enlargement of the enrolment sets of a writer recognition system based on handwritten short sequences of text can lead to a noticeable improvement in its recognition perfor-mance, especially in the verification task. Although an improve-ment of this kind has already been reported in the signature field (e.g. Galbally et al., 2009 ), this is the first time, to the authors X  best knowledge, that such a possibility is reported regarding a non-signature-based schema. Secondly the results also show the effectiveness of the proposed method. Regarding the verification task, the results may be deemed quite promising: in most cases, 15 out of 16, for the combination of both types of strokes, a decrease in the verification error is obtained. With regard to the identification task, the improvements, 16 out of 16 for pen-up strokes and for the combination of both types of strokes, are of a more humble magnitude.

The enhancement rates reported in the experiments have to be analysed within the context in which they have been obtained. It has to be taken into account that the recognition system already yielded high identification rates and low verification errors with-out the addition of synthetic samples (see Table 2 ). Although much care has to be taken when comparing the performance of different biometric modalities because of their inherent differ-ences and the great variability of the conditions under which they are tested, the baseline results do not lie much below than what can be obtained with state-of-the-art online signature verifica-tion systems ( Impedovo and Pirlo, 2008 ). Thus, considering the magnitude of the improvements, especially in verification, and the fact that there was little room for improvement, the results obtained may be deemed as quite encouraging.

The fact that the performance of the recognition system is improved by the synthetic enlargement of the enrolment sets may also have a positive impact in collectability and acceptability, two features to be taken into account when evaluating a bio-metric system. This possibility would help in keeping enrolment easy, little invasive and quickly performed.

Although the proposed synthesis method has proved its effec-tiveness in a particular recognition system it could be also applied to other text-based online recognition schemas or adapted to work with them. Also, the method might be extended to deal with other time-dependent features such as speed or acceleration. Acknowledgement
This work has been supported by FEDER and MEC, TEC2009-14123-C04-04.
 References
