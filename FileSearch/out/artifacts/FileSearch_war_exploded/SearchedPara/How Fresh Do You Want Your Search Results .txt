 Researchers have recognized the importance of utilizing temporal features for improving the performance of information retrieval sys-tems. Specifically, the timeliness of a web document can be a signif-icant factor for determining whether it is relevant for a search query. Previous works have proposed time-aware retrieval models with par-ticular focus on news queries, where recent web documents related with a real-world event are generally preferable. These queries typically exhibit bursts in the volume of published documents or submitted queries. However, no work has studied the role of time in queries such as  X  X redit card overdraft fees X  that have no major spikes in either document or query volumes over time, yet they still favor more recently published documents. In this work, we focus on this class of queries that we refer to as  X  X imely queries X . We show that the change in the terms distribution of results of timely queries over time is strongly correlated with the users X  perception of time sensi-tivity. Based on this observation, we propose a method to estimate the query timeliness requirements and we propose principled ways to incorporate document freshness into the ranking model. Our study shows that our method yields a more accurate estimation of timeliness compared to volume-based approaches. We experimen-tally compare our ranking strategy with other time-sensitive and non time-sensitive ranking algorithms and we show that it improves the results X  retrieval quality for timely queries.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Recency Query; Results Freshness; Web Search
Previous works have shown that timeliness is a key aspect for determining the relevance of a web document for a search query [1, 2, 3, 4]. For instance, a user who issues a query for  X  X S elections X  in November 2012, is more likely to be interested for web pages related to the 2012 elections. Although there exist several web pages about previous elections that are highly topically relevant to the user X  X  query, it is expected that boosting the more recent documents will improve the retrieval quality. Under this assumption, several techniques have been proposed to incorporate the time dimension in the ranking model. Previous approaches can be broadly categorized into: (i) those that focus on news queries related with real-world events, favoring recent documents [5, 6, 7, 3], and (ii) those that target general time-sensitive queries, where the results are preferably published during a specific time range [2, 4].

For both query types, it has been found that the most relevant time range is usually associated with spikes in the number of related user queries, or in the volume of related documents published during that time frame. Based on this observation, current approaches analyze the time series of related queries or documents, in order to (i) classify queries as news queries or not [6] and (ii) to identify important time periods [2, 4]. Thereby, they boost the ranking of documents published around that time frame. For instance, a user searching for  X  X oston Marathon X  might be looking for information about the terrorist attack that took place during the marathon on April 15, 2013; then documents published around that date are promoted.
For a large number of search queries however, recent studies [8, 9] have shown that, while freshness of query results is still very important, their query or document time series are either unclear or misleading. In particular, several queries exhibit no, multiple, random or periodic spikes in query popularity over a time period [8]. For instance consider a query like  X  X redit card overdraft fees X . For such a query, the number of published documents and/or search queries remains more or less constant over time, normalized with the total query volume. However, more recent documents are clearly more relevant because, due to policy changes on behalf of banking institutions, information contained in older documents might no longer be valid. Other queries, exhibit a seasonal or unpredictable rise in popularity. A query such as  X  X ax preparation tips X  usually has a burst in popularity around mid April every year; on the other hand a query for  X  X ihanna new single X  follows a more irregular trend pattern.

In general, for this type of queries, all other things being equal, a fresher document is probably much more relevant for a user X  X  search query. However, existing volume-based techniques cannot be ap-plied for such queries, since the number of published documents or issued queries cannot be leveraged as an indication of users X  inter-ests. In fact, the number of published documents or search queries might as well be negatively correlated with the users X  demands. For instance consider a query such as  X  X ong distance phone calls prices X  or  X  X ashionable haircuts X . Figure 1 shows the number of queries in a commercial search engine over years, normalized by the total query traffic during each time period, for the query  X  X ashionable haircuts X . As depicted, user searches for this query have dropped during the recent years. At the same time, the keyword  X  X ashionable X  indi-cates that users are interested on new hairstyles and that web pages related to older ones are no longer relevant, which indicates that it is a time-sensitive query. For this query, an approach that focuses only on the query or document volume might instead give higher ranking to older documents.

Moreover, different queries can have very different degree of freshness requirements. For example for a query like  X  X martphone reviews X , users might consider as relevant a document that is up to 6 months old, whereas for a query like  X  X ew movies in theaters X  the time range of interest is much shorter, typically 1-2 weeks. For the reasons mentioned above, volume-based approaches will not be able to capture different freshness requirements. Further, in addi-tion to identifying the timely queries, we must also quantify how timely a query is.

In this work we will focus on this important type of queries, which we refer to as timely queries . Timely queries have the following properties: (i) the interest on the query from document publishers or consumers does not show significant variance over time (normalized by the total query traffic or document volume respectively), and (ii) more recently published documents are strongly preferred over older ones. Since, the popularity of such queries can be steady, previous approaches are not effective.

Motivated by the above challenges, in this paper we propose a dif-ferent approach to measure the freshness requirements of a user X  X  query, i.e., the query timeliness . In particular, we argue that the users X  freshness requirements from search results are strongly cor-related with the degree of content change in the relevant documents. In other words, if the terms distribution inside the most relevant doc-uments changes significantly over time, this indicates that older doc-uments become stale shortly, and hence, they should be penalized with lower relevance scores.

In this paper we experimentally confirm this correlation. That is, if we identify significant content change in the relevant documents across time, then time becomes a major factor in our proposed rank-ing. We incorporate the query timeliness in our retrieval model in a principled manner by extending previous works on time-based language models [1, 3].

Note that, since our ranking model always favors more recent documents, our approach is not meant to handle news queries related with events that took place in the past. For example, for a query like  X  X upreme Court healthcare act X , results around June 28th 2012 are more relevant since this is when the Supreme Court ruled. Contributions . The contributions of this paper can be summarized as follows: Outline . Section 2 presents related work on temporal ranking of search results. In Section 3 we discuss query timeliness and we provide a method to measure timeliness based on the terms distri-bution. In Section 4 we present our time-aware ranking model by incorporating the concept of query timeliness. Section 5 contains an experimental evaluation of our methods. In Section 6 we dis-cuss how our proposed model can be applied in practice. Finally, in Section 7 we draw conclusions and sketch our future work.
Li and Croft [1] were the first that proposed a time-based language model to boost the ranking of more recent results. In particular, they introduced an exponential decay prior to the query likelihood language model, such that more recent documents are assigned a higher probability. Based on the proposed ranking model, experi-ments show significant improvements in retrieval quality for TREC queries that are related with recent events.

However, their proposed ranking model treats all recency queries as having the same freshness requirements from the search results. That is, they boost the ranking of recent results uniformly for all queries. Efron and Golovchinsky [3] improve upon this model [1] by proposing a query-specific exponential reranking method. In particular, they calculate a maximum likelihood estimator per query based on the time distribution of the most relevant results, as returned by a non time-aware ranking. A significant limitation with such an approach is that it will boost recent documents, only as long as the document volume increases over time, i.e., it can be applied mainly on news queries and not generally on time-sensitive ones. In con-trast, in this paper, we provide a ranking model that can be used for different query types, which is not dependent on the distribution of documents over time or the underline (non time-sensitive) ranking. We experimentally compare to both methods [3, 1] in Section 5 and we show that our method yields better retrieval quality for timely queries.

Some works [2, 4] focus on more general time-sensitive queries, where the results are preferably published during a specific time range. Jones and Diaz [2] propose building a time series on the number of top ranked documents of the query. According to the de-tected number of spikes in the time series, they classify queries into three classes: atemporal , temporally ambiguous and temporally un-ambiguous , which represent queries that exhibit no spike, only one spike and more than one spikes in their document volumes, corre-spondingly. The class of queries that we study in this paper would be classified as atemporal by [2], since these queries do not exhibit major volume spikes (see Section 5.2.2). Therefore if we followed this approach, the freshness of the results would not be considered as important. Dakka et al. [4] propose alternative methods to learn the most relevant time period for a query, and present solutions to incorporate temporal relevance into several popular ranking algo-rithms. Both [2, 4] rely on the spikes in the distribution of relevant documents.

It has been empirically shown [1, 3] that applying a time-aware ranking model can generally harm the retrieval quality of non time-sensitive queries. In order to address this problem, Dai et al. [10] in-troduce a machine learning framework for simultaneously optimize both relevance and freshness of results, by utilizing both temporal and non-temporal document features.

Another approach is to automatically identify whether a search query is time sensitive or not. If a query is not time-sensitive, stan-dard relevance methods can be used instead. Dong et al. [6] use machine learning techniques to classify a query as breaking news query or not. For this purpose, they measure the difference in the query probabilities in various time slots in the past, such as the last day, last week and last month. The probabilities are calculated based on the language model of both the query log and the docu-ment collection. If the query is classified as a breaking news query, the freshness of a document becomes important in ranking. Simi-larly to some of the above methods [2, 4], the underlying intuition is that in a specific time range, the results are much different from a regular search, for instance due to a burst in the number of relevant documents or due to the query being popular in the query stream. Similarly, using features such as the probabilities of the query gener-ated from recent content, Styskin et al. [11] train a linear regression model to predict a probability for the query X  X  freshness preference and they combine fresh documents with regular ones in order to enhance the temporal diversity of the results.

Assuming a news query, several works deal with how to improve the temporal relevance of returned results. Dong et al. [7, 12] extend former work [6] by enriching the results with documents discovered in the Twitter stream. For this purpose, they extract a set of features from both a regular documents X  corpus and a tweets collection, and they learn a ranking model in order to merge recent tweets with regular results. Diaz [5] and K X nig et al. [13] study the utility of showing news results among regular ones by using click-through data.

Elsas et al. [14] study the temporal factors of navigational queries, where there is usually a small number of highly relevant documents that are consistently relevant across time. For this type of queries, they experimentally show that there is a strong positive correlation between the relevance of a document and the frequency of the docu-ment X  X  content change. However, within the same document, terms that are present across different time ranges are more important in estimating the overall document X  X  relevance. Thereby, they propose the use of a document-specific prior in order to favor more dynamic documents. Our work has a similar motivation, i.e., to leverage the amount of content change in recency ranking. However, we focus on more general informational or transactional time-sensitive queries where more recent (and not necessarily highly dynamic) documents are preferable. Further, we measure the content change in the query rather than in the document level.
As we already noted, different queries can have very different timeliness degrees, i.e. requirements on the freshness of the search results. For instance, for a query such as  X  X op graduate schools X , a user might find results up to two years old as relevant, whereas for a query such as  X  X niversal Studios coupon X , she would be interested only on search results of the last few weeks, since older coupon offers will probably have already expired. Thus, the challenge is how to identify the appropriate timeliness degree for a given search query.

Previous works have used the query volume and the number of published documents as an indicator of the timeliness of a query. The most relevant work to our problem [3] computes a query-specific freshness parameter that is calculated based on the distri-bution of the publication times of the top k results that would be returned by a non time-sensitive ranking function. However, as we will demonstrate in our experiments in Section 5, the proposed ap-proach [3] fails for the class of queries that we study in this work, i.e., those having a relatively steady document volume, such as the one shown in Figure 1.

In order to overcome this problem for the class of queries (timely query) we study in this paper, we introduce a new method to esti-mate query timeliness. In particular, we propose to use the degree of change in the content of the most relevant documents of a query, as a measure of the timeliness of the query. Kullback-Leibler (KL) di-vergence [15] is a popularly used measure to compute the divergence of text documents [16, 17]. In this paper, we apply KL divergence to measure the changes of text documents, i.e. we calculate the difference in term probability distributions of text documents using the KL divergence: where P and R are two probability distributions and P ( t ) , R ( t ) denote the probability of term t in distributions P and R , respectively.
For a query Q , we define the degree of content change between two time slots as the difference in the probability distributions be-tween the sets of documents relevant to Q in these time slots. For simplicity, hereafter we assume discrete time slots, which might denote weeks, months etc. Further, let T i represent the set of docu-ments that are relevant for query Q , and were published during time slot t i (e.g., during July 2012). We will also symbolize as LM ( T the language model produced by T i . Then, we define the degree of content change between two time slots t i , t j as KL ( LM ( T
Assuming n consecutive time slots t 1 ,  X  X  X  , t n , we define the terms distribution change for a query Q , denoted as TDC ( Q ) as: i.e., we take the average KL-divergence acquired from consecutive pairs of time slots. For calculating LM ( T i ) we will apply a unigram language model approach.

Several other measures have been proposed in order to quantify the amount of content change (the opposite of similarity) between documents, such as the cosine similarity, Dice similarity, and Jac-card distance.

Previous work [17] evaluated the effectiveness of these measures for computing the similarity of text documents for document cluster-ing. The results have shown that all measures deliver similar results with KL divergence, and that differences depend on the particular characteristics of the document collection. Potentially, any of this measures could be used in our model to replace KL divergence.
In developing our time-aware ranking, we follow the language ranking model [18]. Li and Croft [1] were the first that proposed a ranking that incorporates a temporal dimension into the language model. According to the query likelihood approach, the probability that a document d is relevant to a query Q , P ( d | Q ) , is proportional to (i) the probability of deriving Q based on the language model of d , termed as P ( Q | d ) and (ii) an apriori probability of document d that depends on the publication date T d , termed as P ( d | T
In particular, in order to compute P ( d | T d ) , they assume an expo-nential decay calculated as: where  X  t d is the normalized age of document d , measured as the time distance between T d and the date of the most recent document in the document collection. Note that Li and Croft [1] use the same freshness parameter  X  for a set of queries that is manually identified as time-sensitive, regardless of the different degrees of timeliness requirements each query has. Thus, Efron et al. [3] improve [1] by proposing to use a query-specific  X  Q that is calculated based on the time distribution of relevant documents as returned by a non time-sensitive ranking model.

In this work we use Equations 3 and 4 as a starting point, and we modify them in order to consider the right amount of freshness for each query using the timeliness requirement estimation method proposed in Section 3. Specifically our ranking model assigns scores based on the following function: where BM 25 ( d , Q ) denotes the ranking score of document d for a query Q by the popular ranking function Okapi BM25 [19], which is based on the probabilistic model. Based on our ranking model, depends on the timeliness requirements of each query as measured by the amount of content change (Section 3). Intuitively, we would assign larger values of  X  Q for queries having higher timeliness de-grees, as predicted by TDC ( Q ) . Larger values for  X  Q will result in penalizing the scores for older documents, thus favoring the most recent ones. We calculate  X  Q as: where  X  &gt; 0 is a parameter of the ranking model and 1  X  e is the KL divergence score normalized in (0,1] 1 . We will assume a constant value of  X  for all queries. In Section 5.3 we provide more details on how we set  X  for experiments; in Section 6 we explain how  X  can be set up in practice.

Since the calculation of  X  Q is based on TDC ( Q ) , we will refer to the ranking model in Equation 5 as the Timeliness-Aware Ranking ( TAR ). In Section 5.3 we experimentally evaluate the retrieval qual-ity of the proposed ranking function with the previously proposed time-aware ranking models [1, 3].
In this section, we provide experimental results on the ranking quality of our methods. We first present the datasets that we used for our experiments in Section 5.1. Sections 5.2 and 5.3 contain the experimental evaluation of the proposed timeliness estimation method and timeliness-aware ranking respectively. Query workload . In order to build our query workload we consid-ered the Text REtrieval Conference (TREC) datasets (e.g. TREC Web Tracks [20]). Unfortunately most of the available datasets con-tain only a few timely queries. Thus, we manually created some ad-ditional queries that we considered as timely, following an approach similar to [3]. In particular, we asked 10 graduate students to sug-gest queries having diverse timeliness requirements. The complete query workload consists of 119 queries (taken from TREC or pro-posed by students) and is available at [21], in which we specified the queries from TREC.
 Documents . For our experimental evaluation we also needed doc-uments published during different time ranges and relevance judge-ments that take into account both the topical and the temporal rel-evance of results; however these data where not available in TREC dataset. Hence, we constructed our document collection by sub-mitting each of the queries to a commercial search engine and we conducted a user relevance study as explained in Section 5.2.1.
In order to collect documents published at various time ranges, we specified different start and end dates in our search, such that the returned results contain only documents that were published during the specified time frame. Note that accurately identifying the publi-cation date of each document is itself a challenging task [22, 6, 23]; moreover often the publication time and the time associated with the content contained in a document might differ. The search engine that we used for our experiments tries to estimate the publication date for each web page by using features such as the date when it was first crawled, or a byline date or an explicitly specified date of a news article or blog post if such information is available. For sim-plicity, hereafter we will assume that all documents returned by the search engine have been published during the specified time period. Following this method, we retrieved the top 400 results per year for years 2007-2011 and for the first half of 2012, i.e., we obtained for each query 2400 documents in total.
In the first set of experiments we compare the performance of our proposed method for estimating the query timeliness with the previous approaches that focused on the document [2, 6, 3, 4, 24] and query volume change [6] as discussed in Sections 2 and 3.
In order to calculate the degree of correlation between the docu-ment volume and the users X  perception of query timeliness, we set up a user survey to collect judgments w.r.t. the timeliness demands for each query on behalf of the users. In addition to the 10 gradu-ate students, our survey X  X  subjects include users recruited through the Amazon Mechanical Turk [25]. In particular, we forwarded all queries to the graduate students, and 60 queries to each Amazon Mechanical Turk worker (110 workers in total). For each query, we asked the users to select among the following five options: no time preference, up to 2 years old, up to 6 months old, up to 1 month old, up to 1 week old the one that best describes their preferences in terms of freshness of the search results. In order to increase the quality of the user survey: (i) we disqualified low-quality work-ers from our experimental study as explained in the Appendix, and (ii) we filtered out the 5 highest and the 5 lowest outlier timeliness values for each query.

For each query we collected 10 timeliness judgments from stu-dents and 20 valid timeliness judgments from Amazon Mechanical Turk workers. Next, for each query we calculated the average Time-liness Requirement in months. For this purpose we mapped each label to a specific number in months that represents the respective timeliness class. Since the maximum age of any document in our collection is 5.5 years, we mapped each label for no time preference to 66 months. Similarly we mapped the other timeliness ranges to 24, 6, 1 and 0.25 months respectively. Then we took the average over all judgments, which we will refer to as TR ( Q ) . Figure 2a shows some representative queries, along with the respective aver-age timeliness requirements, as specified by the users. The query ids are taken from the complete list of the query workload [21].
As shown, users have very low freshness requirement for queries such as  X  X ublic speaking tips X  and  X  X nterview thank you letter X . On the other hand, according to our user survey, users find the results published in the last 1-2 years for  X  X assport renewal X ,  X  X ancel a new car contract X , or  X  X ow income housing X  as relevant. Queries such as  X  X etail sales index X  have relevant results published in the last 6 months. Further, results published during the last month are con-sidered as relevant for queries such as  X  X ewest tablet X  or  X  X elebrity gossips X . Finally, for other queries such as  X  X BA scores X  or  X  X al-ifornia lottery results X  the relevant documents typically change per week, and users are looking for up-to-date information, as it is con-firmed by the user survey. Studying the Document Volume Change . We first examine to what extent previous approaches [2, 6, 3, 4, 24] can predict the timeliness of timely queries. For this experiment, for each query in our workload, we issued a web search where we also specified an one month time range. For each query, we retrieved the number of documents returned by the search engine for each month range, for each of the last 66 months (5  X  12 months for years 2007-2011 and 6 months for 2012).

Since the size of the web grows over time, we need to normalize the total number of documents per month, with the size of the web at the time. Since the total size of the (visible) web is unknown, we assume that its size can be approximated by a search query that returns as many relevant documents as possible. In particular, we issued a set of stopwords queries 2 and we calculated the average number of returned documents over all issued queries. Then, we used this number as an approximation of the size of the web for a specified time range. Finally, we normalized the document volumes for each query with the number of results of the stopwords query for this month, as shown in Figure 3. Figure 4 plots the normalized document volumes for some representative queries.
 Anecdotal Examples . The queries  X  X BA lockout X  and  X  X ccupy Wall Street X  are news queries that were not included in our query workload. The other two queries  X  X ySQL cluster setup X  and  X  X ire-fox updates X  are from our query workload. By comparing the docu-ment volume time series of the news queries with our timely queries, we can observe that news queries have a peak during the specific time when the news event happened. For instance, the time series Figure 3: Time series of the number of documents returned for stopwords line for  X  X ccupy Wall Street X  suddenly hikes during the end of 2011, whereas the time series for  X  X BA lockout X  exhibits an increase on document volume during the second half of 2011. In contrast, the time series of the document volumes for timely queries might not have any significant spikes. For instance,  X  X ySQL cluster setup X  query does not have any spikes and  X  X irefox updates X  has spikes with insignificant variance.
 Correlation of Timeliness to Document Volume Change . In order to calculate the correlation between the document volume change and query timeliness we followed two different approaches based on how we measure the volume change.

First, we used the number of documents published during each of the 66 monthly slots that we considered in our experiments. Then, for each consecutive pair of months we calculated the ab-solute change in volumes. We also calculated the average number of documents per month, and we used it in order to normalize the differences between months. Finally, we calculated the Pearson correlation coefficient between the normalized document volume changes and TR across all queries in our query workload. The calculated Pearson correlation coefficient is -0.298. Note that the computed correlation value is negative because larger document vol-ume changes result in more rapid change of the relevant information w.r.t. a query, which means that only the most recent documents should be considered as relevant. In that case the query would have a lower average timeliness value TR ( Q ) .

As a second measure, for each of the examined 119 queries we calculated the coefficient of the variation of its monthly time series, as: where  X  and  X  represent the standard deviation and mean of each time series. Similarly, we calculated the Pearson correlation co-efficient between CV and TR across all queries and we found a correlation equal to -0.281. Figure 4: Time series of normalized documents number returned for several Studying the Query Volume Change . Next, we examined the degree of correlation between the query volume change and the users X  perception of timeliness. We built a time series of query volumes, by using the service provided by Google Insights [26]. We issued each query by specifying a date range of 5.5 years. Google Insights could provide monthly query volumes only for 87 out of the 119 queries issued when we collected these data. The results provided by Google Insights are already normalized with the size of the query traffic on Google. Figure 5 shows the time series constructed for several representative queries.
 Anecdotal Examples . As shown, the query volume time series of  X  X ccupy Wall Street X  and  X  X BA lockout X  have quite different be-havior compared with the other two queries taken from the query workload. Also note that the hikes in query volumes of  X  X ccupy Wall Street X  and  X  X BA lockout X  are consistent with the hikes of their document volumes in Figure 4.
 Correlation of Timeliness to Query Volume Change . Similarly to how we calculated the timeliness estimation quality for the docu-ments volume time series, we correlate (i) the normalized query change, and (ii) the coefficient variation of the query volumes time series of each query with the average timeliness requirement TR . The Pearson correlation coefficient calculated over the 87 queries was -0.132 using the normalized query volume change, and -0.130 using the coefficient of the variation.
 Discussion . The relatively low correlation of both approaches shows that methods that leverage the document or query volume change are not suitable for timely queries, such as  X  X ySQL cluster setup X  and  X  X irefox updates X , but can only be applied on news queries. Note that in addition to monthly, we also tried other range lengths with similar results.
We now evaluate our method for estimating the timeliness of a query based on the terms distribution change in its relevant docu-ments as presented in Section 3.
 Figure 5: Time series of normalized query volume for several queries from
As discussed in Section 5.1, we collected 2400 documents per query over a period of 5.5 years. We created 6 document collections, where each collection contains all 400 documents retrieved for the corresponding time slot t i . In order to build a language model for each document collection we concatenated all 400 documents into a single document T i . Retrieving the relevant textual content from the HTML body of each document is a challenging and error-prone task [27]. In order to address this problem, mainly for performance reasons, we only considered the titles and snippets of each retrieved document. 3 Finally, when building each language model LM ( T we ignored all common stopwords and all terms occurring fewer than 3 times over different time slots (we assumed that they are either typos or irrelevant terms). Then, based on the definition of TDC ( Q ) in Equation 2, we calculated the terms distribution change for a query Q as: Finally, we calculated the Pearson correlation coefficient between TDC ( Q ) and TR ( Q ) for all the queries of our workload and we got a correlation score -0.427. This score indicates that there is a strong correlation between the terms distribution change of the documents content and the users X  perception of query timeliness. Further, it is much higher compared to the volume-based approaches that we presented in Section 5.2.2.

Note that instead of using one year as a unit to define the time slots, we also experimented with different time units. Because of the time range that we specified (5.5 years), there are not sufficient data in order to use a 2-year unit. Thus, we tried to use 1 week, 1 month, and 6 months units to build the language model LM ( T for the most recent 10 time slots, and applied a similar method as above to calculate TDC ( Q ) . The Pearson correlation coefficients between TDC ( Q ) and TR ( Q ) based on 1 week, 1 month, and 6 months time slots are -0.298, -0.357 and -0.413 respectively, which are all higher than volume-based approaches in Section 5.2.2. As the TDC ( Q ) score from yearly time slots yields the highest prediction quality, we will use the TDC ( Q ) results from yearly time slots in the following experiments.

Figure 2b plots the TDC ( Q ) and TR ( Q ) values for all queries in our workload. Some representative queries (those shown in Fig-ure 2a) are labeled with different symbols and numbers. For in-stance, for query Q88:  X  X ublic speaking tips X  and Q56:  X  X nterview thank you letter X , the relevant documents do not vary largely over time. Thus, the TDC ( Q ) scores computed for these two queries are relatively small. According to our user survey, users roughly prefer the results published in the last two years for Q83:  X  X ass-port renewal X  and Q16:  X  X ancel a new car contract X , and last 1 or 2 months for queries such as Q79:  X  X ewest tablet X . The TDC ( Q ) scores linearly decrease according to their TR ( Q ) . For other queries such as Q76:  X  X BA scores X  and Q14:  X  X alifornia lottery results X , the relevant documents change very frequently, usually per week for the latter one or even everyday during the season time for the former one. This results in getting higher TDC ( Q ) scores than other queries. At the same time, users are searching for up-to-date con-tent, as it is confirmed by the TR ( Q ) scores. As shown, the users X  perception of timeliness for all of the above queries is captured suf-ficiently using our method.
In Section 4 we proposed a principled way to incorporate time-liness into our ranking algorithm TAR. In this section, we experi-mentally evaluate the retrieval performance of TAR, compared with other time-aware and non time-aware rankings. We first describe the experimental setup, which is in addition to the setup described in Section 5.1. Subsequently, we present our experimental results in Section 5.3.2. Datasets . For our retrieval evaluation experiments we used the 2400 documents that we collected during the timeliness estimation survey. Note that for the performance evaluation experiments instead of using the results X  titles and snippets, we built an index on the actual HTML content of each web page.
 Effectiveness Metrics . For our retrieval evaluation, we applied two widely used relevance metrics: precision and Discounted Cumula-tive Gain (DCG) [31]. In particular, we measured the precision and DCG on the top-n results, denoted as Prec @ n and DCG @ n respec-tively. Instead of DCG @ n , we adopted the Normalized Discounted Cumulative Gain (NDCG), which is a normalization of DCG in the range [0, 1] and is calculated as: where IDCG@n is the ideal DCG@n, i.e., the maximum possible DCG value up to the ranking position n . DCG@n is calculated as [32]: where rel i denotes the binary relevance of the results at ranking position i , i.e., rel i is equal to 1 if the result at position i is valid and 0 otherwise.
 In our experiments we set n = 5 , i.e., we measured Prec@5 and NDCG@5. In particular we calculated the average Prec@5 and NDCG@5 across the complete query workload [21].
 Algorithms . In our evaluation we compared our TAR ranking with the following set of algorithms:
Li and Croft [1] experimentally show that EXP achieves the best ranking quality by setting  X  = 0 . 01 . Therefore, we used this value for our experiments. For BEX, we used the recommended parameter settings as described in [3], i.e., we set k = 500 ,  X  = 100 , and we calculated  X  such that (  X   X  1 ) /  X  = 0 . 015 (see [3] for more details). Relevance User Survey . We set up a user study to collect the rel-evance judgments for our dataset. For this purpose we applied a pooling method that has been popularly used to build test collec-tions in TREC [34, 35]. We randomly mixed the top results from each of the above ranking algorithms and asked a set of workers on the Amazon Mechanical Turk to label the results that are the most relevant to each query. The workers were asked to make their judgments considering both the topical and the temporal relevance of the presented results. The the whole dataset is available at [21]. We retrieved the top-5 results for each ranking algorithm BM25, EXP, BEX and TAR with different values of  X  5 . After taking the union of the top-5 rankings of all algorithms (duplicate results from different algorithms will only show once), for each query we got 17 unique document results on average. Note that to compare with other algorithms, we will report the retrieval quality of TAR based on a single value of  X  . We split our query workload into 6 groups of 20 queries each, such that each worker would have to provide relevance judgments for 20 queries. Thus, each user had to eval-uate around 340 ( 17  X  20 ) query-document pairs. A document is considered as relevant to a query if it is labeled by over 50% of the workers that provided judgments for this query. Again, we dis-qualified some low-quality workers from our study as detailed in the Appendix. We finally assumed as valid only the judgments provided from the 104 most high-quality workers (among the initial 126 work-ers). Thereby, each query has been evaluated by 17.3 (high-quality) workers on average. In total, for our experimental evaluation, we considered 35248 query-document pairs, out of which 11637 are labeled as relevant.

Setting of  X  : Additionally, we need to set the parameter Equation 6 for our TAR ranking. For this purpose we conducted a 5-fold cross-validation to train and test it. In particular, we split our query workload into 5 sets following a lexicographic order. Thereby, 4 out of the 5 test sets consist of 24 queries and one consists of 23 queries. We experimented with the following values for  X  0.03, 0.05, 0.07, 0.09, 0.1, 0.3, 0.5, 0.7, 0.9, 1, 3, 5, 7, 9 and 11, which are all included in the above user survey. Each row in Table 1 shows the value of  X  that achieves the best averaged Prec@5 on each training set, and the averaged Prec@5 and NDCG@5 scores based on this  X  for the respective testing sets. Note that we also conducted experiments using NDCG@5 as our retrieval quality measurement for training; since it produced similar results with Prec@5, we do not show the results of this experiment.

As shown in Table 1, the values of  X  that achieve the best Prec@5 are quite stable on different training sets. Thus, in order to evaluate the overall performance of our TAR method against the baseline algorithms, we calculated the average Prec@5 and NDCG@5 for all testing queries in our query workload under the setting Measuring the ranking differences. First, we experimentally val-idate that the time-aware rankings yield quite different results com-pared to the BM25 ranking. For this purpose, we measured the nor-malized Spearman footrule distance [36] between each time-aware ranked list and the BM25 ranking. Figure 6 shows the Spearman footrule distance between BM25 and BM25-T, EXP, BEX and TAR for different values of  X  , when considering the top-n results for n = 5, 10, 15 and 20. Larger values for the Spearman footrule indi-cate more disagreement between two lists. As shown, the ranking of search results changes largely when applying a time-sensitive ranking algorithm.
 Figure 6: Spearman footrule between the BM25 and different time-based Retrieval Quality. Table 2 compares the retrieval quality of TAR against the competitor methods for the complete query workload. As depicted, TAR achieves superior retrieval quality compared to all four competitor rankings. TAR performs 15% and 11% better than BEX in terms of average Prec@5 and NDCG@5, respectively. Further, BEX has better performance than EXP, which is consistent with the experimental findings in the original paper [3]. BM25-T delivers better NDCG@5 than BM25 because of the property of timely queries: more recently published relevant documents are preferred. Note that, as we described before, top-n results of BM25-T is the reranking of top-n results of BM25 based on time. Thus, for a query the value of Prec@5 will be the same for BM25-T and BM25 but NDCG@5 may be different. Further, the improvements of TAR are statistically significant with p-value &lt; 0.01 using the paired Student X  X  t-test over all competitor methods.
 Sensitivity of Retrieval Quality wrt. Query Timeliness. Next, we studied the retrieval quality of all ranking algorithms for queries with different timeliness requirements. Specifically, we split the query workload into three timeliness groups according to the values
Table 2: Retrieval quality for BM25, BM25-T, EXP, BEX and TAR of TR ( Q ) , as specified by the users in the survey described in Sec-tion 5.2.1. The three timeliness groups that we constructed have the following ranges: [0-6 months], [6-24 months] and [24-66 months] and contain 34, 45 and 40 queries respectively.

Figures 7 and 8 show the average Prec@5 and NDCG@5 results for each timeliness group for the different rankings that we exam-ined. The  X * X  symbol over the TAR bar denotes that the respective improvements of TAR over all baselines are statistically significant with p-value &lt; 0.05 using the paired Student X  X  t-test. Figure 7: Average Prec@5 of examined algorithms on different timeliness Figure 8: Average NDCG@5 of examined algorithms on different timeli-
These two figures show that for all timeliness groups, our pro-posed ranking achieves better retrieval quality than both BM25 and the other time-aware ranking algorithms. For the case of the [0-6 months] group, in which queries have the highest timeliness re-quirement, the improvements of TAR over all baselines (over 24% better than BEX on both Prec@5 and NDCG@5) are larger com-pared to the other two groups. This shows that our proposed model is especially useful for the queries with very intense timeliness re-quirements. For the case of the [6-24 months] group, TAR delivers over 10% improvement than BEX which is still the best among the baselines. Finally, for the queries in the [24-66 months] group, where the results freshness is a less important factor than the other two groups, TAR has a slight improvement over the baselines. Fur-ther, compared to the previous timeliness groups for the queries of this group we notice that BM25 achieves quite better retrieval qual-ity because the content relevance is becoming the more important factor when the timeliness requirement drops.
 Examples . We examine the retrieval quality for the query examples that we studied in Section 5.2.1. The Prec@5 and NDCG@5 scores for the example queries are shown in Tables 3 and 4 respectively. Table 4: NDCG@5 for a sample of queries using different rankings Sensitivity of Retrieval Quality wrt. Document Volume Distri-bution. An interesting observation on our data set is that even if we follow a non time-based ranking (e.g., BM25), the time distribution of the most relevant documents is skewed towards the more recent ones. Specifically, in the top-500 documents of BM25, the average number of documents per query is: 62, 71, 83, 92, 101, 90 for years 2007-2011 and the first half of 2012 respectively. The distributions for some queries are more skewed than the average; we identified 44 out of the 119 queries where the top-500 results contain more that 40% documents that have been published in the last 1.5 year, vs. 60% of documents from 2007-2010. One possible explanation for this is that, since the size of the web grows faster over time, recent documents have a larger number; hence the probability for a recent document to be relevant is higher. Further, some older relevant web pages are no longer accessible or might be penalized with lower scores by commercial search engines. Recall that previous algo-rithms, especially BEX [3], leverage the time distribution in their rankings and thus could benefit from this skewed distribution.
We studied the effect of the document distribution on retrieval quality. In particular, we computed the retrieval quality for two sets of queries; 75 queries that exhibit a quite steady time distribution in relevant documents and 44 queries with more skewed distributions towards recent documents. For the former subset, we got an average Prec@5 equal to 0.296 for BEX and 0.352 for TAR, i.e., TAR out-performs BEX (which is the best performing competitor) by 18.9%. For the 44 queries with more skewed distributions, the calculated average Prec@5 for BEX and TAR is 0.373 and 0.409 respectively, which is 9.6% improvement for TAR. The smaller improvement on this subset is expected, since BEX performs better for highly skewed time distributions. For both query sets, the improvement of TAR over BEX is statistically significant with p-value &lt; 0.05. Summary . All time-sensitive ranking algorithms outperform BM25 with significant improvements on both Prec@5 and NDCG@5 for timely queries. Further, consistent with former research [3], BEX generally exhibits better retrieval quality than EXP.
 TAR achieves the best performance among all ranking algorithms. In particular, TAR improves over 10% in terms of both Prec@5 and NDCG@5 over BEX ranking algorithm on our complete query workload. TAR can satisfy queries with different timeliness re-quirements better than other time-sensitive ranking algorithms as shown in the experiments (Figures 7 and 8). Further, if we remove the effect of the skewness in the time distribution of the documents, TAR achieves even higher improvement (18.9%) over BEX (which delivers the best ranking quality among the competitor rankings).
The retrieval quality results on queries from different timeliness groups validate our proposed model, i.e., the timeliness require-ments can be predicted accurately based on the degree of content change in relevant documents and it is used in an effective way in our proposed ranking model. Limitations. In this paper, we focus on timely queries that do not exhibit clear or significant variance in query or document popular-ity over time, but where recent results are preferred. The proposed model is not meant to handle other types of queries such as those tar-geting specific events. As a direction for future work, we will study a principled way to combine our model with previous works that focus on other types of time-sensitive queries, such as [4] which studies the volume distribution of relevant documents. In other words, we will study how to propose a unified model which considers different signals to estimate the temporal requirements for a broader set of queries.

Also note that because of a lack of public benchmarks that provide both the topical and temporal relevance of results for timely queries, it X  X  hard to conduct experiments on a very large query workload; however we believe that 119 queries is a reasonably large workload. Practical Issues. In a real-world scenario, instead of conducting a user survey, the timeliness requirements of each query ( TR ( Q ) ) can be extracted based on clickthrough data, for instance by observing the timestamps of results that are clicked or not-clicked by the users after each web search.

If a new query Q for which the timeliness requirement is unknown is issued to the search engine, we can use the timeliness requirements of similar queries in order to estimate a TDC value for Q . Similar queries can be found by considering keyword text similarity, or based on a query likelihood approach, etc. Further, in terms of implementation, one alternative approach to estimate TDC ( Q ) on query time would be to first compute the top-k results in a time-insensitive way for query Q , then compute TDC ( Q ) using these k results, and then rerank them using Equation 5.

With regard to learning an optimal value for  X  parameter, in a practical use case a search engine can train the model based on user feedback. Different values of  X  might be suitable for queries that exhibit different timeliness requirements; this is also an interesting direction that we aim to explore as our future work.

In a real-world web search system, our model can be applied as a complement to previous work studying news queries. Previous works have proposed methods to classify queries as news-related or not news-related [6, 10]. If the query is not news-related, our proposed TAR algorithm can be used, otherwise a news ranking approach [5, 6, 7, 10] can be applied.
In this paper we studied the freshness factor for a class of queries that we refer to as timely queries. We show that previous works on news queries cannot be applied effectively for predicting the timeli-ness requirement of queries if the query popularity from document publishers or consumers does not vary significantly over time. We propose a method to estimate query timeliness with high accuracy using the terms distribution change of a query X  X  relevant documents over time. Further, we present a ranking model that incorporates the timeliness factor in order to improve the results freshness for timely queries, and we experimentally show that our ranking improves upon previous methods over 10% in terms of both precision and NDCG. In our future work we plan to explore methods to automat-ically learn the TDC scores and to combine our proposed ranking model with other signals in order to support a broader set of queries.
This work was supported in part by National Science Foundation grants IIS-1216032 and IIS-1216007. Filtering Timeliness Judgments . We detected that some workers X  responses are very dissimilar from the rest in Amazon Mechanical Turk service. Thus, in order to remove the low-quality workers we applied the following filtering strategy. Based on the survey results from the 10 students, we picked 7 queries that receive the most consistent responses (i.e., having the lowest standard deviation in their timeliness values). For each query, we consider as valid timeliness response from a worker any value between the minimum and the maximum timeliness values given by the students. If a response from a worker yields a value that is not in the valid range, we treat this as an invalid response. We accept only those workers who did at most two invalid choices out of the responses for the 7 selected queries. Using these criteria, we got 20 valid Amazon Mechanical Turk responses for each query from unique workers. Filtering Relevance Judgments . We perform a postprocessing of the survey results in order to make sure careless judgments are fil-tered out. For each query group (each survey has about 20 queries), we select two timely queries. For these two queries, we get the 3 most frequently and least frequently selected results from Amazon Mechanical Turk workers (more than 3 if there are ties). If the se-lections of a worker have one miss in the most selected results or one hit in the least selected results, we increase the count of the worker X  X  inconsistent selections. We accept the worker X  X  selections as long as the worker has at most 6 inconsistent selections in total for the two queries. We got 126 worker responses in total, i.e., for each query we got relevant judgments from 21 workers on average. From this set we finally selected 104 valid ones (17.3 per query).
