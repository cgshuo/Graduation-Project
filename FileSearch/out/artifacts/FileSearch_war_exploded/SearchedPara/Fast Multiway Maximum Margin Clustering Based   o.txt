 derived from many domains like information retrieval, bioinformatics and social me-tering approaches have been proposed. Inspired by the preponderance of support vec-tor machine (SVM) which is successfully applied to many research areas, the concept of maximum margin has been extended to the unsupervised learning scenario, forming a new clustering method algorithms. 
Unfortunately different from SVM, MMC needs to identify an optimal labeling of data instances which results in the maximum margin cutting hyperplanes between different clusters. The additional integer programming of data-instance labels induces problem on non-convex condition, many schemes have been employed in MMC suc-cutting plane [7] and so on. However, the computational complexity of each of these these methods converge to the optimal solutions [8]. maximum margin clustering method based on genetic algorithm (GAM3C) for large datasets. GAM3C firstly employs the Nystr  X  m method to generate a low-rank ap-algorithm (GA) into the optimization procedure of MMC and alternating the solution space by its crossover and mutation operators, GAM3C avoids the premature conver-simultaneously, improving the computational accuracy of MMC. Experiments on real maximum margin based clustering algorithms, especially in analyzing large datasets. timization problem. Therefore, MMC was transformed into a semi-definite program-ming problem. Although the following generalized maximum margin clustering (GMMC) was proposed by Valizadegan et al. [10] to reduce the number of variables of MMC, the expensive computation makes MMC or GMMC impractical. 
To solve the non-convex optimization problem, Zhang et al. [11] adopted an alter-nating scheme SVR with Laplacian loss to avoid premature convergence. However, optimization problem by means of constrained concave-convex procedure (CCCP). Then Wang et al. [8] proposed an advanced maximum margin clustering algorithm (CPMMC), which decomposed the original clustering into a series of convex sub-vide efficient solutions for multiway MMC clustering, but the optimal results from a sequence of decomposed convex sub-problems are prone to get stuck in local minima. 
Recently, Gieseke et al. [13] exhibited a fast evolutionary maximum margin clus-tering method (containing EMMC and FEMMC), which speeds up the computational procedure by sampling a subset from either the data label set or the Lagrangian mul-tiplier set to approximate. However, this method is just a binary clustering. Moreover, entire accuracy of clustering results. 
In the applications of analyzing large dataset, many approaches [14][15] make use problem. Motivated by these applications, this paper employs the Nystr  X  m method to MMC for an approximate computation firstly; then based on the obtained approx-imate matrix, searches for the optimal solutions from the non-convex problem space. 3.1 The Nystr  X  m Method The Nystr X m method is originally used to solve the numerical approximation of integral equations in the following form [14]: where  X  is a probability density function,  X  is a kernel function, Eq.(1), sample  X  interpolation points  X  X  X   X   X ,  X   X ,..., proximate result by the empirical average is: where  X   X   X   X   X  is an approximate value of  X  X  X  X  X  in Eq.(1), and choose  X  X  X   X   X ,  X   X ,...,  X   X  as well to generate an eigen-decomposition  X  X  X  X  X   X   X ,  X   X  X  X  X 1,2,..., X , X  | X  . So any eigenvector Eq.(1) can be approximated by  X   X   X  X  X  X   X   X  X  X   X   X , X  X  X  X   X  and point  X  can be computed by: 3.2 Multiway Support Vector Machine Mathematically, given a instance set  X  X  X  X  X   X   X ,...,  X   X  , labels  X  X   X   X   X   X ,...,  X   X   X ,  X   X  X  X  X 1,..., X   X  , SVM defines a matrix sists of k class indicator vectors and the class of instance where  X   X  is the r th row of P (  X   X   X  is the indicator vector), similarity score with  X  from each row of P means the class affiliation of instances in  X  , an upper bound on the empirical loss is obtained as follows: which satisfies the following constraint: from support vectors to class indicator hyperplanes  X   X ,..., X 1 X   X  are maximized, where  X  is a feature projecting function. 4.1 Multiway Maximum Margin Clustering In essence, MMC is an extension of SVM to the unsupervised learning scenario. Dif-fering from theories of SVM [17], the label vector Y is often not in hand, so the for-mulized definition of multiway MMC is to compute the following minima: model complexity,  X   X   X   X   X   X  X  X  X  X   X   X   X ,...,  X ,...,1 ,  X  X   X   X   X  X  X  X   X   X 0 are slack variables.  X   X  straint which prevents all instances from being assigned to one cluster. 
The most important thing is that, similar to  X  and  X  function of MMC as follows: Lagrangian function, we rewrite the following term: and execute the partial derivative on  X   X  and  X  obtain: thus the following term can be rewritten as: based on Equation (9),(10),(11)and(12), we have:  X   X   X   X  , X   X   X , X ,  X   X   X  finally, the dual of multiway MMC is formulized as: where in the dual form of multiway MMC,  X   X  X  X   X  X   X   X   X   X   X   X  X  X  X  the nonlinear function  X  that maps the instances linear separable in the feature space. 4.2 Approximating Kernel Matrix via the Nystr X m Method Nystr X m method mentioned in Section 3.2, we generate a low-rank approximate matrix by the intersection of sampled columns and rows. Definition 1 ( Kernel Matrix Approximation ). Input: kernel matrix  X  X  X  X  X   X  X  X   X  X   X   X   X   X   X   X  X  X  X   X   X  X  X  X  X   X  X  X  X   X  X  X  X  X  X  X  X  X  X  X , Output: low-rank approximate matrix  X   X   X  X   X  X  X  X  , a rank parameter  X   X  X   X  X  X   X  /  X   X   X  X  X   X   X  , we sample m rows and m columns from generate  X   X  . In addition, if  X 0 X  and  X   X  is the best rank-m approximation of choosing  X  (  X / X   X  )columns and rows, the expectation of approximate error is: 
Where  X   X   X  is the best rank-m approximation of formed by the sampled m columns from  X  . According to the Drinear and Mahoney X  X  theorem [18], this approximation can be completed by  X  ( time and space, after passing the data from external storage two times. 4.3 Optimization with Genetic Algorithm Based on the obtained low-rank approximate matrix  X   X  , the computational burden of can be reduced enormously. But due to the integer programming of labeling Y , the optimization of  X   X  becomes a non-convex problem. Therefore, this paper is to embed lem efficiently. Definition 2 ( Optimizing  X   X  via Genetic Algorithm ). Input: approximate matrix  X   X  , initial population each of which is a genetic representation of clustering results on Y , a threshold T for objective function  X   X  , iterator i=0 and most iteration times I. Output: cluster indicator matrix P , k rows of which indicate k clusters obtained on Process: 1. generate c novel individuals by crossover and add them to 2. compute the minima of the convex objective function Objective Function and Genetic Representation. Objective function is the only guide for optimizing infinite variables which are encoded as individuals of population and if two genes of an individual take a same value, they belong to the same cluster. edge-weight-based gene representation to initialize  X  efficiently. Crossover. When each individual (or cluster labeling vector mined, the optimization of  X   X  becomes a convex problem. We can compute the saddle sover and elite selection to create offsprings of next generation of population. Mutation. Selecting the elite candidate parents for crossover is to guide the objective algorithm. But sometimes this improvement is prone to incur a premature conver-change the objective solution space into a global range. 4.4 Extension of Clustering Results to Out-of-Samples Until now, what has been obtained is the clustering results based on the low-rank approx-imate matrix  X   X  . How to extend it to the unsampled? According to the Nystr X m method in Eq.(9), the cluster affiliation of out-of-samples can be estimated by: Definition 3 ( Extension to Out-of Samples ). Output: final clustering results on  X  .
 Process: if an out-of-sample  X  belongs to the same cluster with the rth row 
The collinear property of two co-cluster vectors results from the following proposi-tion: if the term  X  X  X   X   X   X   X   X ,  X   X   X   X  equals to 1, with  X   X  [20]. The row cluster indicator  X   X  , which denotes the normal vector of one only one direction and no chance to rotate. 4.5 Computational Complexity Firstly, let X  X  give the implementation of GAM3C in Algorithm 1. By analyzing Algo-rithm 1 in detail, we deduce Lemma1 as follows: A lgorithm 1. GAM3C Input: instance set  X  X  X  X  X   X   X ,...,  X   X  X  X  X   X  , kernel matrix of generations I , mutation rate  X  , parameters  X 0 X , X  X  X  X  X   X  X  X , X , X , X , X  Output: an optimal labeling of  X  X  X  X  X   X   X ,...,  X   X  X  X  X 1,..., X  X   X  1 Begin 2  X   X  = intersection of m columns and m rows sampled by probability 3 Initialize  X   X   X  X  X  X   X   X ,...,  X   X  ; 4 While  X  X   X   X  X  &amp; i  X  X  ) do 5 Generate individuals  X   X   X ,...,  X  X  X  X  by crossover , 7 Mutate  X  individuals of parents and produce the next generation 8 End while 9 Obtain cluster indicator matrix  X  based on the optimal value of 10  X   X   X   X   X  X  X  X  X  X  X   X   X  X  X   X   X  X   X   X   X   X  ; //  X  is one out-of-sample instance 11 End  X   X   X   X   X  X  X  X   X   X  X  X   X   X   X  X  X  X  X   X   X  X   X   X  X  X  X  X  X   X   X  X  X   X   X  , where individuals in each generation of population, c is the incremental number of individu-als generated by crossover, m is the dimension of matrix mum value between the threshold T and the most iteration times I, k is the number of cluster number, and n is the number of instances in the original input dataset Proof. The generation of low-rank approximate matrix turning to the  X  X hile X  loop, each loop iteration needs time objective function  X   X  based on each individual in the i th generation, and we select the maxima between T and I to denote the worst case, thus the whole running time of clustering results based on  X   X  to the out-of-samples is the total time complexity of GAM3C is  X   X   X   X   X  X  X  X   X   X  X  X   X   X  X  X  X  X  X   X   X  X   X   X   X  X  X  X   X   X  X  X   X   X  in the worst case, especially on the condition of 5.1 Datasets and Experiment Setup In the sequel, we will evaluate the performance of GAM3C in terms of computational accuracy and running time on real world datasets. We choose K-Means (KM) [2] and the state-of-the-art MMC algorithms such as ISVR [11], CPM3C [8], EMMC [13] repositories 1 , while datasets in Table2 are collected from Stanford University X  X  SNAP All of the experiments are performed on a Linux machine with 4Core 2.6GHz CPU and 4G main memory, and the algorithms are implemented in Java. each algorithm by the following two criteria: 1. Normalized Mutual Information (NMI). NMI [21] is such a metric that it is capable of maintaining the balance between clustering quality and number of clusters. 
No matter which criterion is selected, the greater value means the better clustering results. 
On account of the premised kernel matrix approximation by the Nystr X m method, NMI , we firstly measure the variation of computational accuracy of GAM3C on each dataset in Table 2, with an increasing sampling proportion. Moreover, to avoid drop-ters of embedded genetic algorithm are set as: crossover rate is 0.9, population size is 0.4, 0.45, 0.5} in turn. The analyzing results are drawn as folding lines in Figure 1. 
Secondly, we run all of above algorithms on datasets both in Table 1 and Table 2 5.2 Experimental Results and Analysis interpolation points on four real datasets such as Youtube, Amazon, Orkut and DBLP. When the proportion of interpolation points is larger than 1%, all folding lines tend to dataset graph, we observe that the clustering results with mutation rate more than 0.4 0.45, 0.5 mutation rate are close to one another. So in summary, we can gain a much interpolation points 5% or larger. only fit for the binary clustering problems, and even in the binary cases, their RI are smaller than GAM3C X  X . When analyzing the multiway clustering problems, the RI of domness with a probability near to zero, GAM3C performs better than CPM3C. What is stated above demonstrates that the embedded genetic algorithm improves the entire the consuming time of GAM3C is the least among all the clustering methods. Although the time complexity of KM and CPM3C is almost linear, GAM3C which is based on the Nystr X m method executes much faster, making it scalable to analyze the cluster structure of larger datasets with more than millions of instances. This paper proposes a fast multiway maximum margin clustering method based on genetic algorithm (GAM3C) for large datasets. GAM3C makes use of the genetic algorithm to solve the non-convex maximum-margin optimization problem explicitly, avoiding the premature convergence efficiently by crossover and mutation operations, structure of objective datasets, GAM3C achieves a premised approximation for kernel matrix via the Nystr  X  m method, reinforcing its scalability for the further applications outperforms the state-of-the-art MMC algorithms such as ISVR, CPM3C, EMMC, FEMMC and the classical K-Means in terms of computational accuracy and running time, meanwhile exhibiting its scalable analyzing capability. 
