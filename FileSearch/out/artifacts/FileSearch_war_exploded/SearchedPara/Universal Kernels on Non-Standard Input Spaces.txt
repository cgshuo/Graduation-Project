 For more than a decade, kernel methods such as support vector machines (SVMs) have belonged to the most successful learning methods. Besides several other nice features, one key argument for using SVMs has been the so-called  X  X ernel trick X  [22], which decouples the SVM optimization problem from the domain of the samples, thus making it possible to use SVMs on virtually any input space X . This flexibility is in strong contrast to more classical learning methods from both machine learning and non-parametric statistics, which almost always require input spaces X  X  R d . As a result, kernel methods have been successfully used in various application areas that were previously infeasible for machine learning methods. The following, by no means exhaustive, list illustrates this:  X  SVMs processing probability measures, e.g. histograms, as input samples have been used to an-alyze histogram data such as colored images, see [5, 11, 14, 12, 27, 29], and also [17] for non-extensive information theoretic kernels on measures.  X  SVMs for text classification and web mining [15, 12, 16],  X  SVMs with kernels from computational biology, e.g. kernels for trees and graphs [23]. In addition, several extensions or generalizations of kernel-methods have been considered, see e.g. [13, 26, 9, 16, 7, 8, 4]. Besides their practical success, SVMs nowadays also possess a rich statistical theory, which provides various learning guarantees, see [31] for a recent account. In-terestingly, in this analysis, the kernel and its reproducing kernel Hilbert space (RKHS) make it possible to completely decouple the statistical analysis of SVMs from the input space X . For ex-ample, if one uses the hinge loss and a bounded measurable kernel whose RKHS H is separable and dense in L 1 (  X  ) for all distributions  X  on X , then [31, Theorem 7.22] together with [31, Theo-rem 2.31] and the discussion on [31, page 267ff] shows that the corresponding SVM is universally classification consistent even without an entropy number assumption if one picks a sequence (  X  n ) of positive regularization parameters that satisfy  X  n  X  0 and n X  n / ln n  X   X  . In other words, independently of the input space X , the universal consistency of SVMs is well-understood modulo an approximation theoretical question, namely that of the denseness of H in all L 1 (  X  ) . For standard input spaces X  X  R d and various classical kernels, this question of denseness has been positively answered. For example, for compact X  X  R d , [30] showed that, among a few others, the RKHSs of the Gaussian RBF kernels are universal , that is, they are dense in the space C ( X ) of continuous functions f : X  X  R . With the help of a standard result from measure theory, see e.g. [1, Theorem 29.14], it is then easy to conclude that these RKHS are also dense in all L 1 (  X  ) for which  X  has a compact support. This key result has been extended in a couple of different directions: For example, [18] establishes universality for more classes of kernels on compact X  X  R d , whereas [32] shows the denseness of the Gaussian RKHSs in L 1 (  X  ) for all distributions  X  on R d . Finally, [7, 8, 28, 29] show that universal kernels are closely related to so-called characteristic kernels that can be used to distinguish distributions. In addition, all these papers contain sufficient or necessary conditions for universality of kernels on arbitrary compact metric spaces X , and [32] further shows universal spaces.
 Unfortunately, however, it appears that neither the sufficient conditions for universality nor the proof of the existence of universal kernels can be used to construct universal kernels on compact metric spaces X 6 X  R d . In fact, to the best of our knowledge, no explicit example of such kernels has so far been presented. As a consequence, it seems fair to say that, beyond the X  X  R d -case, the theory of SVMs is incomplete, which is in contrast to the obvious practical success of SVMs for such input spaces X as illustrated above.
 The goal of this paper is to close this gap by providing the first explicit and constructive examples of universal kernels that live on compact metric spaces X 6 X  R d . To achieve this, our first step is to extend the definition of the Gaussian RBF kernels, or more generally, kernels that can be expressed by a Taylor series, from the Euclidean R d to its infinite dimensional counter part, that is, the space ` of square summable sequences. Unfortunately, on the space ` 2 we face new challenges due to its infinite dimensional nature. Indeed, the closed balls of ` 2 are no longer (norm)-compact subsets of ` 2 and hence we cannot expect universality on these balls. To address this issue, one may be tempted to use the weak  X  -topology on ` 2 , since in this topology the closed balls are both compact and metrizable, thus universal kernels do exist on them. However, the Taylor kernels do not belong to them, because  X  X asically X  the inner product  X  X  ,  X  X  ` 2 fails to be continuous with respect to the weak  X  -topology as the sequence of the standard orthonormal basis vectors show. To address this compactness issue we consider (norm)-compact subsets of ` 2 , only. Since the inner product of ` 2 is continuous with respect to the norm by virtue of the Cauchy-Schwarz inequality, it turns out that the Taylor kernels are continuous with respect to the norm topology. Moreover, we will see that in this situation the Stone-Weierstra X -argument of [30] yields a variety of universal kernels including the infinite dimensional extensions of the Gaussian RBF kernels.
 However, unlike the finite dimensional Euclidean spaces R d and their compact subsets, the compact subsets of ` 2 can be hardly viewed as somewhat natural examples of input spaces X . Therefore, we go one step further by considering compact metric spaces X for which there exist a separable Hilbert space H and an injective and continuous map  X  : X  X  X  . If, in this case, we fix an analytic function K : R  X  R that can be globally expressed by its Taylor series developed at zero and kernel on X and the same is true for the analogous definition of Gaussian kernels. Although this interesting explicit examples can be derived from this situation. Indeed, we will use this general result to present examples of Gaussian kernels defined on the set of distributions over some input space  X  and on certain sets of functions. The paper has the following structure. Section 2 contains the main results and constructs examples for universal kernels based on our technique. In particular, we show how to construct universal signal processing. Section 3 contains a short discussion and Section 4 gives the proofs of the main results. A kernel k on a set X is a function k : X  X  X  X  R for which all matrices of the form the so-called reproducing kernel Hilbert space (RKHS) of k , which is given by more information on kernels, we refer to [31, Chapter 4]. Moreover, for a compact metric space ( X,d ) , we write C ( X ) := { f : X  X  R | f continuous } for the space of continuous functions on X and equip this space with the usual supremum norm k X k  X  . A kernel k on X is called universal , if k is continuous and its RKHS H is dense in C ( X ) . As mentioned before, this notion, which goes back to [30], plays a key role in the analysis of kernel-based learning methods. Let r  X  (0 ,  X  ] . The kernels we consider in this paper are constructed by functions K : [  X  r,r ]  X  R that can be expressed by its Taylor series, that is For such functions [31, Lemma 4.8] showed that defines a kernel on the closed ball Taylor coefficients a n are non-negative. Following [31], we call such kernels Taylor kernels. [30], see also [31, Lemma 4.57], showed that Taylor kernels are universal, if a n &gt; 0 for all n  X  0 , while [21] notes that strict positivity on certain subsets of indices n suffices.
 Obviously, the definition (2) of k is still possible, if one replaces R d by its infinite dimensional and separable counterpart ` 2 := { ( w j ) j  X  1 : k ( w j ) k 2 ` unit ball in ` 2 by B ` 2 , or more generally, the closed unit ball of a Banach space E by B E , that is B
E := { v  X  E : k v k E  X  1 } . Our first main result shows that this extension leads to a kernel, whose restrictions to compact subsets are universal, if a n &gt; 0 for all n  X  N 0 := N  X  X  0 } . Theorem 2.1 Let K : [  X  r,r ]  X  R be a function of the form (1). Then we have: To consider a first explicit example, let K := exp : R  X  R be the exponential function. Then K clearly satisfies the assumptions of Theorem 2.1 for all r &gt; 0 , and hence the resulting exponen-tial kernel is universal on every compact subset W of ` 2 . Moreover, for  X   X  (0 ,  X  ) , the related Gaussian-type RBF kernel k  X  : ` 2  X  ` 2  X  R defined by is also universal on every compact W  X  ` 2 , since modulo the scaling by  X  it is the normalized version of the exponential kernel, and thus it is universal by [31, Lemma 4.55].
 Although we have achieved our first goal, namely explicit, constructive examples of universal ker-nels on X 6 X  R d , the result is so far not really satisfying. Indeed, unlike the finite dimensional Euclidean spaces R d , the infinite dimensional space ` 2 rarely appears as the input space in real-world applications. The following second result can be used to address this issue.
 Theorem 2.2 Let X be a compact metric space and H be a separable Hilbert space such that there exists a continuous and injective map  X  : X  X  H . Furthermore, let K : R  X  R be a function of the form (1). Then the following statements hold: It seems possible that the latter result for the Gaussian-type RBF kernel can be extended to other Indeed, [25] uses the fact that on R d such kernels have an integral representation in terms of the Gaussian RBF kernels to show, see [25, Corollary 4.9], that these kernels inherit approximation properties such as universality from the Gaussian RBF kernels. We expect that the same arguments can be made for ` 2 and then, in a second step, for the situation of Theorem 2.2.
 Before we provide some examples of situations in which Theorem 2.2 can be used to define explicit universal kernels, we point to a technical detail of Theorem 2.2, which may be overseen, thus leading to wrong conclusions.
 To this end, let ( X,d X ) be an arbitrary metric space, H be a separable Hilbert space and  X  : X  X  X  be an injective map. We write V :=  X  ( X ) and equip this space with the metric defined by H . Thus,  X  : X  X  V is bijective by definition. Moreover, since H is assumed to be separable, it is isometrically isomorphic to ` 2 , and hence there exists an isometric isomorphism I : H  X  ` 2 . We write W := I ( V ) and equip this set with the metric defined by the norm of ` 2 . For a function f : W  X  R , we can then consider the following diagram Since both  X  and I are bijective, it is easy to see that f not only defines a function g : X  X  R by g := f  X  I  X   X  , but conversely, every function g : X  X  R has such a representation and this representation is unique. In other words, there is a one-to-one relationship between the functions X  X  R and the functions W  X  R . Let us now assume that we have a kernel k W on W with RKHS H
W and canonical feature map  X  W : W  X  H W . Then k X : X  X  X  X  R , given by defines a kernel on X , since shows that  X  W  X  I  X   X  : X  X  H W is a feature map of k X . Moreover, [31, Theorem 4.21] shows that the RKHS H X of k X is given by Since, for f  X  H W , the reproducing property of H W gives f  X  I  X   X  ( x ) =  X  f,  X  W  X  I  X   X  ( x )  X  H W for all x  X  X we thus conclude that H X = { f  X  I  X   X  : f  X  H W } =: H W  X  I  X   X  . Let us now assume that X is compact and that k W is one of the universal kernels considered in Theorem 2.1 or the Gaussian RBF kernel (4). Then the proof of Theorem 2.2 shows that k X is one of the universal kernels considered in Theorem 2.2. Moreover, if we consider the kernel k V : V  X  V  X  R kernel. This raises the question, whether we need the compactness of X , or whether it suffices to assume that  X  is injective, continuous and has a compact image V . Surprisingly, the answer is that it depends on the type of universality one needs. Indeed, if  X  is as in Theorem 2.2, then the compactness of X ensures that  X  is a homeomorphism, that is,  X   X  1 : V  X  X is continuous, too. Since I is clearly also a homeomorphism, we can easily conclude that C ( X ) = C ( W )  X  I  X   X  , that is, we have the same relationship as we have for the RKHSs H W and H X . From this, the universality is easy to establish. Let us now assume the compactness of V instead of the compactness of X . Then, in general,  X  is not a homeomorphism and the sets of continuous functions on X and V are in general different, even if we consider the set of bounded continuous functions on X . To see the latter, consider e.g. the map  X  : [0 , 1)  X  S 1 onto the unit sphere S 1 of R 2 defined universality of k V (or k W ) to the universality of k X . However, if  X  V denotes the topology of V , Consequently, there are, in general, fewer continuous functions with respect to  X   X  1 (  X  V ) . Now, it since  X  is isometric with respect to this new metric, we can conclude that ( X,d  X  ) is a compact metric space. Consequently, we are back in the situation of Theorem 2.2, and hence k X is universal with respect to the space C ( X,d  X  ) of functions X  X  R that are continuous with respect to d  X  . In other words, while H X may fail to approximate every function that is continuous with respect to d X , it does approximate every function that is continuous with respect to d  X  . Whether the latter approximation property is enough clearly depends on the specific application at hand. Let us now present some universal kernels of practical interest. Please note, that although the func-tion  X  in our examples is even linear, the Theorem 2.2 only assumes  X  to be continuous and injective. We start with two examples where X is the set of distributions on some space  X  .
 Example 1: universal kernels on the set of probability measures.
 Let ( X  ,d  X  ) be a compact metric space, B ( X ) be its Borel  X  -algebra, and X := M 1 ( X ) be the set of all Borel probability measures on  X  . Then the topology describing weak convergence of probability measures can be metrized, e.g., by the Prohorov metric where A  X  := {  X  0  X   X  : d  X  (  X , X  0 ) &lt;  X  for some  X   X  A } , see e.g. [2, Theorem 6.8, p. 73]. Moreover, ( X,d X ) is a compact metric space if and only if ( X  ,d  X  ) is a compact metric space, see [19, Thm. 6.4]. In order to construct universal kernels on ( X,d X ) with the help of Theorem 2.2, it thus remains to find separable Hilbert spaces H and injective, continuous embeddings  X  : X  X  H . Let k  X  be a continuous kernel on  X  with RKHS H  X  and canonical feature map  X   X  (  X  ) := k (  X ,  X  ) ,  X   X   X  . Note that k  X  is bounded because it is continuous and  X  is compact. Then H  X  is separable and  X   X  is bounded and continuous, see [31, Lemmata 4.23, 4.29, 4.33]. Assume that k
 X  is additionally characteristic , i.e. the function  X  : X  X  H  X  defined by the Bochner integral  X  (P) := E P  X   X  is injective . Then the next lemma, which is taken from [10, Thm. 5.1] and which is a modification of a theorem in [3, p. III. 40], ensures the continuity of  X  .
 Lemma 2.3 Let ( X  ,d  X  ) be a complete separable metric space, H be a separable Banach space and  X  :  X   X  X  be a bounded, continuous function. Then  X  : M 1 ( X )  X  X  defined by  X  (P) := E P  X  is continuous, i.e., E P n  X   X  E P  X  , whenever (P n ) n  X  N  X  X  1 ( X ) converges weakly in M 1 ( X ) to P . Consequently, the map  X  : M 1 ( X )  X  H  X  satisfies the assumptions of Theorem 2.2, and hence the Gaussian-type RBF kernel is universal and obviously bounded. Note that this kernel is conceptionally different to characteristic kernels on  X  . Indeed, characteristic kernels live on  X  and their RKHS consist of functions  X   X  R , while the new kernel k  X  lives on M 1 ( X ) and its RKHS consists of functions M 1 ( X )  X  R . Consequently, k  X  can be used to learn from samples that are individual distributions, e.g. represented by histograms, densities or data, while characteristic kernels can only be used to check whether two of such distributions are equal or not.
 Example 2: universal kernels based on Fourier transforms of probability measures.
 Consider, the set X := M 1 ( X ) , where  X   X  R d is compact. Moreover, let  X  be the Fourier transform well-known, see e.g. [6, Chap. 9], that, for all P  X  M 1 ( X ) ,  X  P is uniformly continuous on R d and k  X  P k  X   X  1 . Moreover,  X  : P 7 X   X  P is injective, and if a sequence (P n ) converges weakly to some Borel measure on R d with support (  X  ) = R d , e.g.,  X  can be any probability distribution on R d with Lebesgue density h &gt; 0 . Then the previous properties of the Fourier transform can be used to show that  X  : M 1 ( X )  X  L 2 (  X  ) is continuous, and hence Theorem 2.2 ensures that the following Gaussian-type kernel is universal and bounded: In view of the previous two examples, we mention that the probability measures P and P 0 are often not directly observable in practice, but only corresponding empirical distributions can be obtained. In this case, a simple standard technique is to construct histograms to represent these empirical distri-butions as vectors in a finite-dimensional Euclidean space, although it is well-known that histograms can yield bad estimates for probability measures. Our new kernels make it possible to directly plug Moreover, other techniques to convert empirical distributions to absolutely continuous distributions such as kernel estimators derived via weighted averaging of rounded points (WAPRing) and (averag-ing) histograms with different origins, [20, 24] can be used in k  X  , too. Clearly, the preferred method will most likely depend on the specific application at hand, and one benefit of our construction is that it allows this flexibility.
 Example 3: universal kernels for signal processing.
 Let ( X  , A , X  ) be an arbitrary measure space and L 2 (  X  ) be the usual space of square  X  -integrable always, satisfied. In addition, let us assume that our input values x i  X  X are functions taken from some compact set X  X  L 2 (  X  ) . A typical example, where this situation occurs, is signal processing, where the true signal f  X  L 2 ([0 , 1]) , which is a function of time, cannot be directly observed, but a smoothed version g := T  X  f of the signal is observable. This smoothing can often be described by a signals. Hence, if we assume that the true signals are contained in the closed unit ball B L then the observed, smoothed signals T  X  f are contained in a compact subset X of L 2 ([0 , 1]) . Let us now return to the general case introduced above. Then the identity map  X  := id : X  X  L 2 (  X  ) satisfies the assumptions of Theorem 2.2, and hence the Gaussian-type kernel defines a universal and bounded kernel on X . As in the previous examples, note that the computation The main goal of this paper was to provide an explicit construction of universal kernels that are defined on arbitrary compact metric spaces, which are not necessarily a subset of R d . There is a still increasing interest in kernel methods including support vector machines on such input spaces, e.g. for classification or regression purposes for input values being probability measures, histograms or colored images. As examples, we gave explicit universal kernels on the set of probability distri-butions and for signal processing. One direction of further research may be to generalize our results to the case of non-compact metric spaces or to find quantitative approximation results. Elements of this set will serve us as multi-indices with countably many components. For j = ( j i )  X  N 0 , we will therefore adopt the multi-index notation Note that | j | &lt;  X  implies that j has only finitely many components j i with j i 6 = 0 . Lemma 4.1 Assume that n  X  N is fixed and that for all j  X  N N 0 with | j | = n , we have some such that for all summable sequences ( b i )  X  [0 ,  X  ) we have Proof: This can be shown by induction, where the induction step is similar to the proof for the Cauchy product of series.
 Lemma 4.2 Assume that n  X  N 0 is fixed. Then for all j  X  N N 0 with | j | = n , there exists a constant c  X  (0 ,  X  ) such that for all summable sequences ( b i )  X  [0 ,  X  ) we have Proof: This can be shown by induction using Lemma 4.1.
 that ` 2 ( J ) together with k X k 2 is a Hilbert space and we denote its inner product by  X  X  ,  X  X  ` Moreover, ` 2 := ` 2 ( N ) is separable, and by using an orthonormal basis representation, it is further known that every separable Hilbert space is isometrically isomorphic to ` 2 . In this sense, ` 2 can be viewed as a generic model for separable Hilbert spaces.
 The following result provides a method to construct Taylor kernels on closed balls in ` 2 . Proposition 4.3 Let r  X  (0 ,  X  ] and K : [  X  r,r ]  X  R be a function that can be expressed by its If a n  X  0 for all n  X  0 , then k : is a kernel. Moreover, for all j  X  J , there exists a constant c j  X  (0 ,  X  ) such that  X  : ` ( J ) defined by is a feature map of k , where we use the convention 0 0 := 1 .
 Proof: For w,w 0  X  and thus k is well-defined. Let w i denote the i -th component of w  X  ` 2 . Since (1) is absolutely convergent, Lemma 4.2 then shows that, for all j  X  N N 0 , there exists a constant  X  c j  X  (0 ,  X  ) such that a kernel. Before we can state our first main result the need to recall the following test of universality from [31, Theorem 4.56].
 Theorem 4.4 Let W be a compact metric space and k be a continuous kernel on W with k ( w,w ) &gt; 0 for all w  X  W . Suppose that we have an injective feature map  X  : W  X  ` 2 ( J ) of k , where J is some countable set. We write  X  j : W  X  R for its j -th component, i.e.,  X ( w ) = ( X  j ( w )) j  X  J , w  X  W . If A := span {  X  j : j  X  J } is an algebra, then k is universal.
 With the help of Theorem 4.4 and Proposition 4.3 we can now prove our first main result. Proof of Theorem 2.1: We have already seen in Proposition 4.3 that k is a kernel on now fix a compact W  X  only finitely many components j i with j i 6 = 0 . Consequently, there exists a bijection between J and the set of all finite subsets of N . Since the latter is countable, J is countable. Furthermore, we have for all w  X  W , and it is obvious, that the components of the feature map  X  found in Proposition 4.3 span an algebra. Finally, if we have w,w 0  X  W with w 6 = w 0 , there exists an i  X  1 such that w i 6 = w 0 i . For the multi-index j  X  J that equals 1 at the i -component and vanishes everywhere else we then have  X ( w ) = c j w i 6 = c j w 0 i =  X ( w 0 ) , and hence  X  is injective.
 Proof of Theorem 2.2: Since H is separable Hilbert space there exists an isometric isomorphism I : H  X  ` 2 . We define V :=  X  ( X ) , see also the diagram in (7). Since  X  is continuous, V is the bijective map I  X   X  : X  X  W is continuous. Consequently, there is a one-to-one relationship between the continuous functions f X on X and the continuous functions f W on W , namely C ( X ) = C ( W )  X  I  X   X  , see also the discussion following (7). Moreover, the fact that I : H  X  ` 2 is an the kernel k considered in Theorem 2.2 is of the form k X = k W ( I  X   X  (  X  ) ,I  X   X  (  X  )) , where k W is the corresponding kernel defined on W  X  ` 2 considered in Theorem 2.2. Now, the discussion following (7) showed H X = H W  X  I  X   X  . Consequently, if we fix a function g  X  C ( X ) , then f := g  X   X   X  1  X  I  X  1  X  C ( W ) can be approximated by H W , that is, for all  X  &gt; 0 , there exists an h  X  H W such that k h  X  f k  X   X   X  . Since I  X   X  : X  X  W is bijective and f  X  I  X   X  = g , we conclude that k h  X  I  X   X   X  g k  X   X   X  . Now the assertion follows from h  X  I  X   X   X  H X .

