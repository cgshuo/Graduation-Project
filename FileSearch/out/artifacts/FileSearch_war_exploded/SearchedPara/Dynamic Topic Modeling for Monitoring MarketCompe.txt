 We propose a dynamic topic model for monitoring temporal evolution of market competition by jointly leveraging tweets and their associated images. For a market of interest ( e.g . luxury goods ), we aim at automatically detecting the latent topics ( e.g . bags , clothes , luxurious ) that are competitive-ly shared by multiple brands ( e.g . Burberry , Prada , and Chanel ), and tracking temporal evolution of the brands X  s-takes over the shared topics. One of key applications of our work is social media monitoring that can provide companies with temporal summaries of highly overlapped or discrimi-native topics with their major competitors. We design our model to correctly address three major challenges: multi-view representation of text and images, modeling of compet-itiveness of multiple brands over shared topics, and tracking their temporal evolution. As far as we know, no previous model can satisfy all the three challenges. For evaluation, we analyze about 10 millions of tweets and 8 millions of as-sociated images of the 23 brands in the two categories of luxury and beer . Through experiments, we show that the proposed approach is more successful than other candidate methods for the topic modeling of competition. We also quantitatively demonstrate the generalization power of the proposed method for three prediction tasks.
 H.2.8 [ Information Systems ]: Database Applications X  Data mining ; G.3 [ Probability and Statistics ]: Proba-bilistic Algorithms; J.4 [ Computer Applications ]: Social and behavioral sciences X  Economics Dynamic topic models; Market competition; Text and im-ages
The increasing pervasiveness of the Internet has lead to a wealth of consumer-created data over a multitude of online c  X  platforms such as blogs, discussion forums, and social net-working sites. Such contents are valuable for companies to listen in consumers X  candidate opinions, and thus there have been many recent studies on online market intelligence [10, 17, 18], whose goal is collecting and analyzing online in-formation that is contributed by the general public toward companies X  products and services, and providing with pic-tures of ongoing brand performance in a set of given market conditions. The online market intelligence has been one of emerging fields in data mining research as market compe-tition becomes fierce, and consumers X  online reviews and e-valuations are considered more trustworthy and spontaneous than other information described by vendors.

In this paper, we address the problem of modeling tem-poral evolution of market competition by jointly leverag-ing text data and their associated image data on the Web. More specifically, we study tweets and their linked images. Fig.1 illustrates the problem statement of this paper. For a specified competitive market ( e.g . luxury goods ), multi-ple brands ( e.g . Burberry , Chanel , and Rolex ) compete one another to raise their stakes over shared values or topics, which include products-related topics such as bags , clothes , and watch , or consumers X  sentiments-related topics such as luxurious , expensive . The objective of this research is to build an automatic system that crawls tweets, extract text and images from tweets, identify shared topics that multiple brands compete to possess one another, and track the evo-lution of brands X  proportional dominance over the topics.
Our approach focuses on the joint analysis of text and im-age data tagged with the names of competing brands, which have not been explored yet in the previous studies of online market intelligence. The joint interpretation of text and im-ages is significant for several reasons. First, a large portion of tweets simply show images or links without any meaning-ful text in them. Hence, images play an important role for representing topics in this type of tweets. In our dataset, 70% of tweets are attached with urls, and 28% of tweets in the luxury category are with images. Second, many users prefer to use images to deliver their idea more clearly and broadly, and thus the topic detection with images reflects users X  intents better. The popularity of images can be seen in a simple statistics of our twitter dataset; our luxury corpus contains more images than tweets ( e.g . 5.5 millions tweets with 6.6 millions of images). Third, the joint use of images with text also helps marketers interpret the discovered top-ics. Due to the short length of tweets ( i.e . 140 characters), marketers may need to see the associated images to under-stand key ideas of tweets easier and quicker. Finally, since the Internet is where users cannot physically interact one another about actual products or services, images may be essential for users to make conversation about customers X  descriptions, experiences, and opinions toward the brands.
From technical viewpoints, we propose a novel dynamic topic model to correctly address the following three major challenges: (1) multi-view representation of text and images, (2) modeling of latent topics that are competitively shared by multiple brands, and (3) tracking temporal evolution of the topics. Some of existing work attain a subset of these challenges ( e.g . texts and images [4, 7] and dynamic mod-eling [1, 5]), but none of them satisfies all the challenges.
We evaluate our algorithm using newly collected dataset from Twitter from October 2014 to February 2015. Our au-tomatic crawler downloads all tweets tagged by brand names of interest, along with attached or linked images if avail-able. Consequently, our dataset contains about 10 millions of original tweets and 8 millions of associated images of the 23 brands in the two categories of luxury and beer . The experiments demonstrate the superior performance of the proposed approach over other candidate methods, for dy-namic topic modeling and three prediction tasks including prediction of the most associated brands, most-likely cre-ated time, and competition trends for unseen tweets. Note that while we mainly deal with brands of the two categories, our approach is completely unsupervised and thus applica-ble, without any modification, to any categories once input sets of text and image streams are collected.

The foremost application of our work is social media mon-itoring, which assists marketers to summarize their fans X  on-line tweets with sparse and salient topics of competition in an illustrative way. Especially, our algorithm can discover and visualize the temporal progression of what topics are highly overlapped or discriminative over other competitors. From our interaction with marketers, we observe that they are very curious to see and track what topics emerge and what pictures their fans (re-)tweet the most, but there is no such system yet. As another application, our method can be partly used for sentiment analysis [17] because the detected topics can be positive or negative. That is, multiple brands competes one another not only on positive topics ( e.g . mul-tiple cosmetics brands compete on the health+beauty topic) but also negative topics ( e.g . multiple beer brands compete on the drunk+driving topic). We do not perform in-depth analysis on sentiment analysis because it is out of the scope, but at least marketers can observe their brands X  distribution on both positive and negative topics, which is also useful for market analysis. Although we mainly focus on the applica-tions of brand competitions in a market, our problem formu-lation and approach are much broader and are applicable to other domains of competition, including tourism ( e.g . mul-tiple cities compete to attract more international tourists), and politics ( e.g . multiple candidates contest to take leads on major issues to win an election), to name a few.
The main contributions of this paper are as follows. (1) To the best of our knowledge, our work is the first attempt so far to propose a principled topic model to discover the top-ics that are competitively shared between multiple brands, and track the temporal evolution of dominance of brands over topics by leveraging both text and image data. (2) We develop a new dynamic topic model for market competition that achieves three major challenges of our problem; multi-view representation of text and images, modeling of competi-tiveness of multiple entities over shared topics, and tracking their temporal evolution. As far as we know, no previous model can satisfy all the challenges. (3) With experiments on more than 10 millions of tweets with 8 millions of im-ages for 23 competing brands, we show that the proposed algorithm is more successful for the topic modeling over oth-er candidate methods. We also quantitatively demonstrate the generalization ability of the proposed method for three prediction tasks.
Online Market Intelligence . One of most closely re-lated line of work to ours is online market intelligence [17], whose objective is, broadly speaking, to mine valuable infor-mation for companies from a wealth of consumer-generated online data. Due to vast varieties of markets, brands, and in-formation to mine, there have been many different directions to address the problem as follows. As one of early successful commercial solutions, the BrandPluse platform [10] mon-itors consumers X  buzz phrases about brands, companies, or any emerging issues from public online data. In [15], market-structure perceptual maps are automatically created to show which brands are jointly discussed in consumers X  forums e-specially for the two categories of market, which are sedan cars and diabetes drugs. The work of [24] focuses on extract-ing comparative relations from Amazon customer reviews, and visualize the comparative relation map ( e.g . Nokia N95 has a better camera than iPhone ). The authors of [2] al-so leverage Amazon data to discover the relations between product sales and review scores of each product feature ( e.g . battery life, image quality, or memory for digital cameras). In [22], a recommendation system on the blogosphere is de-veloped to learn historical weblog posts of users, and pre-dict which users the companies need to follow when they release new products. Our work has two distinctive features over existing research of this direction. First, we address an unexplored problem of detecting the latent topics that are competitively shared by multiple brands, and automatically tracking their temporal evolution. Second, we jointly lever-age two complementary modalities, text and images, which have been rare in market intelligence research.

Topic Models for Econometrics . Lately, there have been significant efforts to develop generative topic models for modeling and prediction of economic behaviors of users on the Web. In [8], a simple LDA model is applied to stock market data to detect the groups of companies that tend to move together. The work of [11] proposes a new dynamic topic model to predict the temporal changes of consumers X  interests and purchasing probabilities over catalog items. In [13], a geo-topic model is developed to learn the latent topic-s of users X  interests from location log data, and recommend new location that are potentially interesting to users. Final-ly, [14, 19] are examples of topic models that are applied to the tasks of opinion mining and sentiment analysis, in which they produce fine-grained sentiment analysis from user re-views or weblog posts. Compared to previous research of this direction, our problem of modeling market competition of multiple brands is novel, and our model is also unique as an econometric topic model that jointly leverages online texts and images.

Dynamic and Multi-view Topic Models . There has been a large body of work to develop dynamic topic models to analyze data streams [8, 11, 13, 14, 19], and multi-view topic models to discover the interactions between text and images in multimedia contests [4, 7, 9, 21]. Compared to ex-isting dynamic and multi-view topic models, our approach is unique in the ability of directly modeling the competition of multiple entities ( e.g . brands) over shared topic spaces. Since previous models cannot handle with the interactions between multiple entities, they are only applicable to the dataset of each brand separately. However, in this case, the detected topics can be different in each brand; thus it is dif-ficult to elicit shared topic spaces to model the competition.
We first discuss how to represent online documents and associated images, and then develop a generative model for market competition.
Suppose that we are interested in a set of competing brand-s B = { 1 ,..., B L } in the same market ( e.g . Chanel , Gucci , and Prada as luxury brands). We use B l to denote a set of documents ( i.e . tweets) that are downloaded by querying brand name l in the time range of [1 ,T ]. We assume that each document d  X  X  l consists of text and optionally URLs that are linked to images. That is, a tweet can be text on-ly or associated with one or multiple images. Some tweets may be associated with multiple brand labels, if they are retrieved multiple times by different brand names. We use a Figure 2: Plate diagram for the proposed topic model vector g d  X  R L to denote which brands are associated with document d .

For the text descriptor, we use the TF-IDF weighted bag of words model [4], where we build a dictionary of text vocab-ularies after removing words occurred fewer than 50 times. For image descriptor, we leverage ImageNet pre-trained deep learning features with vector quantization. Specifically, we use Oxford VGG MatConvnet and utilize their pre-trained model CNN-128 [20] 1 . which a compact 128-dimensional de-scriptor for each image. Then, we construct H visual clus-ters by applying K -means clustering to randomly sampled (at max) two millions of image descriptors. We assign the r -nearest visual clusters to each image with the weights of an exponential function exp(  X  a 2 / 2  X  2 ) + , where a is the distance between the descriptor and the visual cluster,  X  is a spatial scale, and is a small positive value to prevent zero denominator when normalization. Finally, each image is de-scribed by an H dimensional ` -1 normalized vector with only r nonzero weights. In our experiments, we set H = 1 , 024,  X  = 10, and r = k u k 0 which is the ` 0 -norm of its corre-sponding text descriptor, so that text and image descriptors have the same number of nonzeros.

As a result, we can represent every document and image as a vector. If we let U = { 1 ,...,G } and V = { 1 ,...,H } to denote sets of vocabularies for text and visual words re-spectively, each document d is represented by a pair of vec-tor ( u d , v d ), where u d = [ u d 1 ,  X  X  X  ,u d | N | ] index set of words in document d , and each u dn ( n  X  N ) represents the number of appearances of word n . Likewise, v d = [ v d 1 ,  X  X  X  ,v d | M | ] T where M is the index set of visual words. If a document has multiple associated images, v d represented by a vector sum of image descriptors. For a doc-ument with no associated image, v d becomes a null vector and M is an empty set.
Our model is designed based on our previous Sparse Top-ical Coding (STC) framework [26], which is a topic model http://www.robots.ox.ac.uk/  X  vgg/software/deep eval/. that can directly control the posterior sparsity. In our prob-lem setting, each document and word is encouraged to be associated with only a small number of strong topics. S-ince we aim at analyzing the possibly complex interaction between multiple brands, in practice a few salient topical representation can make interpretation easier rather than letting every topic make a non-zero contribution. In addi-tion, the sparsity leads a more robust text/image represen-tation since most of tweet documents are short and sparse in word spaces due to length limitation of 140 characters. Another practical advantage of the STC is that it supports simultaneous modeling of discrete and continuous variables such as image descriptors and brand associations.

However, our model significantly extends the STC in sev-eral aspects. First, we update the STC to be a dynamic model so that it handles the streams of tweets. Second, we extend to jointly leverage two complementary information modalities, text and associated images. Finally, we address an unexplored problem of detecting and tracking the topic-s that are competitively shared by multiple brands. All of them can be regarded as novel and nontrivial improvement of our method.

Fig.2 shows the graphical model for the proposed genera-tive process. Let  X   X  R K  X  G and  X   X  R K  X  H be the matrices of K topic bases for each text and visual word respectively. That is,  X  k. indicates the k -th text topic distribution over the vocabularies U . We also use  X   X  R K  X  L to denote the brand-topic occupation matrix, which expresses the propor-tions of each brand over topics. We denote  X  d  X  R K as the document code , which is a latent topic distribution of docu-ment d . z dn  X  R K and y dm  X  R K are the text / visual word code respectively, which are latent topic representation of individual text word n and visual word m in document d .
Below we discuss in detail the generative process of our model, which is summarized in Table 1.

Multi-view STC model . For text content, we use the similar generative process with that of the original STC [26]. For each document d : 1. Sample a document code  X  d  X  prior p (  X  ). 2. For each observed word n  X  N ,
In order to model documents with both text and images, we develop a multi-view extension. Specifically, for each document d , we let its text part u d and its corresponding image part v d share the same document code  X  d , as shown in Fig.2. In addition, we assume the same generative process for visual words with the text counterpart. Consequently, we supplement the following step. 3. For each observed visual word m  X  M , We now define the distributions used in the above process. Since each tweet is represented by a very sparse vector in a word space, the document code of a tweet is preferred to be sparse in a topic space in order to foreground the most salient topics and suppress noises. To achieve sparsity on  X  , we define the document code prior p (  X  ) as a Laplacian prior p (  X  )  X  exp(  X   X  k  X  k 1 ), which becomes a ` -1 regularizer in the negative log posterior. Similarly, to boost the topical sparsity of each word, we define the conditional distributions of word codes as the following composite distribution: which establishes a connection between the document code and word codes while encouraging sparsity on the word codes.
For the last step of generating word counts, the STC rec-ommends to use an exponential family distribution with the linear combination z &gt; dn  X  .n as a mean parameter to make op-timization easier and the model applicable to rich forms of data. That is, E p [ u ] = z &gt; dn  X  .n + where  X  .n n -th column of  X  and is a small positive number for avoid-ing degenerated distributions. We choose to use a Gaussian distribution with the mean of z &gt; dn  X  .n , and apply the same idea to the visual word counts. Therefore,
Dynamic extension . In order to model the temporal evolution of topics, we let  X  and  X  to change over time, based on the discrete dynamic topic model (dDTM) [5]. That is, we divide a corpus of documents into sequential groups per time slice t ( e.g . one week in our experiments), and assume that the documents in each group D t are ex-changeable. Then we evolve  X  t and  X  t from the ones in previous time slice t  X  1 by following the state space model with a Gaussian noise. Therefore, for each topic k , we use
Competition extension . We now extend the multi-view dSTC to capture the competition between multiple brands over topics. We first define a brand-topic occupation ma-trix  X   X  R K  X  L to represent the proportions of brands on latent topics. For each document d , we denote B  X  B as the index set of brands, and g d  X  R B as an ` -1 normalized vector representing associated brand labels. For example, if tweet document d is retrieved by keywords { prada , chanel } , then B = { prada , chanel } and g d = [ g d 1 g d 2 ], which are nor-malized values describing how strong the tweet is associated with the observed brands. One can use the same values ( e.g . g d 1 = g d 2 = 0 . 5) or proportional values according to relevance scores by the twitter search engine. For each b  X  B and g we use a latent brand code r db  X  R K as a representation of brand b in the topic space. We let r db to be conditioned on the document code  X  d , which governs the topic distributions of not only text/visual words but also brand labels.
There are two possible options of dynamics on the brand-topic occupation matrix  X  . First, similarly to  X  and  X  , we evolve  X  to capture potential dynamics between brands and latent topics over the time. In this case, we use the state space model with a Gaussian noise, and thus  X  has the same distribution of Eq.(3). Second, if we assume that the brand occupation over topics is stationary, we can sample  X  from a uniform distribution. We take the first approach.
Finally, we can apply the same distributions to the gen-erative process for the brands with the counterparts of text and visual words. That is, we use the composite distribu-tion of Eq.(1) for p ( r db |  X  d ), and the Gaussian distribution of Eq.(2) for p ( g db | r db ,  X  ). In summary,
In this section, we describe the optimization for learning and inference of the proposed model.
The generative process of Fig.2 provides a joint probabil-ity for a document d in each time slice t : p (  X  , z , u , y , v , r , g |  X  ,  X  ,  X  ) = p (  X  ) Y If we add the superscript t to explicitly represent the time slice for each variable, we can denote the parameter set as y d = { y the word index set of document d in time slice t , and likewise for M t d and B t d . Although we skip the derivation due to space limitation, it is not difficult to show the negative log posterior for time slice t satisfies
In the above,  X , {  X  i , X  i , X  i } 3 i =1 are hypeparameters, which are chosen by cross validation in our experiments.
We estimate the model parameters by minimizing the neg-ative log posterior derived in previous section. Since Eq.(6) is the one for the documents in a single time slice t , we ac-cumulate the negative log posteriors of all time ranges, and seek for an optimal solution for the whole corpus of all time slices. Therefore, the final objective is derived as +
X +
X +
X +
X s.t.  X  t d  X  0 ,  X  d,t. z t dn , y t dm , r t db  X  0 ,  X  d,n,m,b,t. where P U , P V , P B are the G  X  1 ,H  X  1 and L  X  1 simplex, respectively ( i.e . For  X  k,t , each of  X  t k ,  X  t
For each time slice t : 1. Draw a text topic matrix  X  t |  X  t  X  1  X  X  (  X  t  X  1 , X  2. Draw an image topic matrix  X  t |  X  t  X  1  X  X  (  X  t  X  1 3. Draw a brand topic matrix with two options: (i) dy-4. For each document d = ( u , v ) in D t , (a) Sample a document code  X  d  X  prior p (  X  ). (b) For each observed text word n  X  N , (c) If M is not an empty set: (d) For each observed brand b  X  B , Table 1: The generative process of the proposed model sum to one). We denote L as the negative log-loss of recon-struction for word counts and brand associations in Eq.(2):
L ( z t dn ,  X  t ) =  X  log p ( u t dn | z t dn ,  X  t ) =  X  Thanks to the use of an exponential family distribution for generating word counts ( e.g . Gaussian distributions of Eq.(2)), the loss function L is convex, and thus the opti-mization of Eq.(7) is multi-convex ( i.e . the optimization is convex over one parameter set when the others are fixed). Consequently, we can directly employ coordinate descent to solve the optimization problem.

Taking into consideration that tweet documents grow a-long with time, we propose two approaches for solving the above problem, namely online learning and smoothing . The two approaches are similar except that the online learning seeks for a local minimum in the current time slice, based on the data of one or several previous time slices, which can be more scalable for online monitoring of real-world big data. On the other hand, the smoothing approach globally opti-mizes the objective over the data in all time slices, which is less scalable but yields more accurate fitness for data, and thus can be more suitable for batch analysis.
In the smoothing approach, we directly optimize the ob-jective of Eq.(7). Note that every two adjacent time slices are only coupled by three parameters:  X  ,  X  and  X  . Hence, if we fix these three parameters, the objective for each time slice is independent one another. Based on this idea, we al-ternate between the optimization for  X  ,  X  ,  X  and the one for the other variables using the coordinate descent algorithm: 1. Fix all {  X  t ,  X  t ,  X  t } T t =1 . We now decouple the optimiza-tion of every time slice t . Since documents can be as-sumed to be independent one another, we further decou-ple per document d . Therefore we solve
Note that for every document d  X  D t , if  X  d is fixed, z y d and r d are independent one another. Thus, we can use the coordinate descent to alternatingly optimize  X  d and z d , y d , r d . (a) While fixing  X  d , we solve each of z dn , y dm (b) While fixing z d , y d , and r d , we solve the following 2. While fixing all parameters of {  X  t } T t =1 , we optimize + +
We can obtain the optimal of Eq.(11) by separately solv-pendent one another. When we solve each of them, for example of {  X  t } T t =1 , we utilize the coordinated descent and the projected gradient descent, in which we solve ev-ery  X  t one by one for each t . That is, at every iteration we fix all {  X  t } T t =1 \  X  t , and use projected descent to solve  X  t . We iterate until convergence for every t .
Instead of directly optimizing the objective of Eq.(7), the online learning approach assumes that at every time t , we only observe a new set of data at t , and have learned model parameters from the data up to t  X  1. This can be more practical in a real-world scenario; we may not always glob-ally optimize using all the past data when we observed new data. Instead we would better seek for a local minimal that may be good enough to reflect the current state of market competition. Formally, we assume that at each t we only consider its p previous time slices to form an evolving chain. To make our discussion easier, we set p = 1; however, it is not difficult to derive the optimization solver for p &gt; 1.
Given the optimization algorithm for smoothing approach in previous section, online learning optimization is readily straightforward. At time slice t we assume that we are giv-en the MAP solutions up to t  X  1, which are denoted by  X   X   X  t and  X  t as well. Once we have  X  t ,  X  t , and  X  t cussed in previous section, the objective of Eq.(7) for each time slice is independent one another. Thus, we can directly apply the algorithm presented in previous section to solve the decoupled objective in every time slice one by one along with time. At the start of the optimization, we initialize all {  X  We evaluate our model from the following four aspects. First, we qualitatively and quantitatively evaluate the qual-ity of learned text and visual topics (Section 5.2.1). Second, we show how our model can simultaneously monitor topic evolution and market competition along with time, com-pared to some baseline methods (Section 5.2.2). Third, we design three prediction methods based on our model, to show the generalization power of our model for unseen documents (Section 5.3). Finally, we conduct internal comparisons and provide some analysis on our model (Section 5.4).
Fig.3 summarizes some statistics of our Twitter dataset for two groups of competing brands: Luxury and Beer . We query brand names using Twitter X  X  RESTs API without any filtering, in order to obtain users X  free and uncensored views on the brands. The data range from 10/20/2014 to 02/01/2015, during which our crawler is scheduled to run once every week, 3 days per week to finish the weekly crawl-ing job. After obtaining raw tweets, we use a publicly avail-able tokenizer for Twitter [16] to extract text and valid URL-s from each tweet, and eliminate illegal, non-English char-acters, and stop words, while preserving emoticons, blocks of punctuation and twitter catchwords 2 . In addition, our crawler traverses every legal URL, and downloads images located in the body of HTML pages. We exclude the images that have too small file sizes or unreasonable aspect ratios.
We extract text and image descriptors as described in sec-tion 3.1. Note that our text and image descriptors for the same document have the same number of nonzero elements (i.e. | N | = | M | ). We then mean-align the two descriptors by setting mean ( u ) = mean ( v ). We standardize the text and image descriptor to avoid bias on any of them. For tweets with multiple images, we obtain the vector sum of all image descriptors, and standardize it.
Consequently, the Beer corpus involves 12 brands, yield-ing 1,101,192 raw tweets and 829,207 images. We build a dictionary of 12,488 text vocabulary words after remov-ing words occurred fewer than 50 times. Finally, we obtain 1,091,369 valid tweet documents, out of which 231,318 tweet-s contain images as well. The Luxury corpus is much larger than the beer corpus, including 5,572,017 raw tweets and 6,606,125 images. Following the same preprocessing step, we obtain a dictionary of 36,023 words, and 5,511,887 tweet documents and 1,523,177 ones associated with images.
In this section, we evaluate the performance of topic de-tection and tracking of our model, and demonstrate its ap-plication to the market competition monitoring.
We assess the quality of the learned topics by our model, compared to other commonly used topic models. Our goal here is to quantitatively show that (1) our model captures the common semantics shared in the tweet corpus textually and visually. (2) Our approach successfully tracks the topic evolution along with time.

While it is still an open problem how to quantitatively evaluate topic models, perplexity and held-out likelihood have been popular measures to assess how well a topic mod-el can be generalized to unseen documents. However, we do not use perplexity and held-out likelihood, because they are not a proper metric in our evaluation for the two following reasons. First, the work of [6] performs a large scale exper-iment on the Amazon Mechanical Turk, and suggests that the perplexity and human judgment are often not correlat-ed. Second, more importantly, our preliminary experiments reveal that they are not fair metrics for the comparison be-tween the algorithms that use different distributions in the model. For example, our model shows a perplexity 10 times lower than other methods, because we model text/visual word counts using Gaussian, which always leads a higher per-word likelihood than Multinomial distribution in LDA or Poisson regressor in STC.

Therefore, we quantitatively evaluate the coherence and validity of our learned topics by extending the Coherence Measure (CM) defined in [23], which is inspired by human evaluation methods of [6]. Specifically, for every text topic, we select the top 10 words with the highest probabilities. Then, we ask 10 human annotators to judge whether the 10 words can be understood as a single specific topic. If not, the topic is labeled as ineffective . The annotators are further asked to scan every word and classify it as relevant or irrelevant to the topic. If more than a half of words are classified as relevant, then the topic is regarded as coherent . Similarly, for each visual topic, we use the same evaluation cdSTC+multi 0.51 / 0.70 0.63 / 0.59 cdSTC+text 0.605 / 0.71 0.61 / 0.59 Table 2: Average VM/CM comparison on text topics.

LDA+multi 0.57 / 0.63 0.51 / 0.69 cdSTC+multi 0.57 / 0.65 0.66 / 0.71 Table 3: Average VM/CM comparison on visual topics. strategy: we first provide the labelers with the top 10 visual words of a visual topic, each of which is represented by top 10 nearest images. The labelers are asked to scan all 100 images globally to judge whether they illustrate a single specific topic. If yes, the visual topic is labeled as effective . Then the labelers classify the images in the each visual word as related or unrelated with the topic, and more than a half of images are classified as related, then the visual word is regarded as coherent with this topic. Based on the user study results, we define the validity measure (VM) and coherence measure (CM), as two metrics of the topic quality: V M = # of valid topics
For experiments, we train the text-only and multi-view version of our model, cdSTC+text and cdSTC+multi , using the data of all time slices. We set the topic number to 50. For the tests of text topics, we compare with two baselines: (1) dLDA [5]: dynamic LDA, and (2) STC+dyn [26]: the STC trained using the data up to t  X  1 time slice. For tests of visual topics, we compare our results with two baselines: (1) KMeans : a simple baseline of k-means clustering. Specifically, we cluster the descriptor vectors of documents with images to 50 clusters, and regard each center as a topic, extract the nearest 10 images as an illustration of every center. (2) LDA+multi : A multi-view LDA implemented based on [12]. Following [5], we use the data at t  X  1 time slice for training.
Table 2 and Table 3 show the average V M and CM re-sults rated by the 10 human annotators. For text topics, our cdSTC+text achieves the best results on the V M mea-sure, compared to the dLDA and STC+dyn models. For visu-al topics, our cdSTC+multi attains the highest score, which concludes that joint use of text and images help detect more human-interpretable topics.
In this section, we demonstrate the application of our model to the market competition monitoring. Given social media data of multiple brands, our model can solve the fol-lowing three tasks, from easy to difficult: (1) At one time slice, we monitor their occupations on latent topics. (2) A-long the timeline, we monitor the trend of each brand X  X  oc-cupation over the topics. (3) Along the timeline, we monitor the global competition trends between multiple brands.
Fig.4 illustrates the evolving chain of topic beauty on the luxury corpus in eight time slices from 2014-10-20 to 2014-12-15. We also show the brand competition pie graphes and the trend curve of every brand occupation on the time-line. Our model successfully captures the topic dynamics; the beauty topic gradually evolves with time, from makeup and lip to blackfriday , order and deals , and finally steps in-to winter , involving more health-related words like skin-care and hydra-potection . The visual words also carry variations along with time, which are consistent with the text topics. The following eight pie graphes shows the competitions of the top seven brands on each time slice. We observe that (1) the dior dominates the beauty topic all the time and overwhelm the gucci , which is the largest brand that occu-pies almost a half of our whole corpus, and (2) other brands ( e.g . burberry , chanel , gucci ) show dynamic up-and-downs over the topics along with the time, which can be useful pieces of information for marketers.
We further verify the generalization ability of the pro-posed model through three prediction tasks. The first two tasks are classification problems, which have been tested for evaluation in many topic model papers ( e.g . [4, 7, 25, 26]). The third task helps marketers compare between interpolat-ed trends and actual topic distribution side by side.
The goal of the first prediction task is to estimate the most associated brand for a novel tweet. Although this task can be seen as a multi-class classification problem where a plen-ty of other methods can be applied, we perform this task to prove the generalization power of our model on unseen data. For this prediction, we make two modifications to our model. First, we drop the terms related to brand competi-tions, which are not required for classification. Second, we develop a supervised extension to be applicable to classifica-tion problems. We use the document code as the input of a multi-class max-margin classifier, and jointly train the laten-t representations and multi-class classifiers. The supervised dSTC ( sdSTC ) solves the following problem 3 : s.t.  X  t d  X  0 ,  X  d,t. z t dn , y t dm  X  0 ,  X  d,n,m,t. (13) where M t = {  X  t ,  X  t } is a set of parameters, f ( X  t , M is the objective function for unsupervised dSTC in time slice t , and R is the multi-class hinge loss: R ( X  t ,  X  t ) = 1 | D t | The above optimization problem can also be solved using the coordinated descent. Specifically, we alternate between Figure 5: Comparison of accuracies of classification task Figure 6: Comparison of accuracies of classification task optimizing between  X  and  X  . It is worth noting that we learn different  X  t for each time slice t .

We design two experimental setups according to which data are used for training and test: (1) Task (I-I): we ran-domly divide the data in every time slice in [1 ,t ] into two parts: 90% for training and 10% for test. (2) Task (I-II): we use the data in previous time slices [1 ,t  X  1] for training, and use all the data at time t for test.

For quantitative comparison, we run the following algo-rithms: (1) sdSTC : our model trained using text data from all time slice. (2) sdSTC+multi : our full model with multi-view extensions. (3) dLDA [5]: the dynamic LDA trained on all time slices, and then training a separate classier for each time slice. (4) MedSTC [26]: the MedSTC trained using text data from all time slices. (5) sLDA [25]: the supervised L-DA trained using text from all time slices. (6) LDA+dyn [3]: the LDA trained using text data from time slice t  X  1. (7) sLDA+dyn [25], the supervised LDA trained using text data from time slice t  X  1. (8) MedSTC+dyn [26]: the MedSTC trained using text data from time slice t  X  1. Note that the baselines of (3) X (5) are used for task (I-I), while the baselines of (6) X (8) are for task (I-II).

Since the sLDA and sLDA+dyn are too slow to learn on millions of documents, we randomly partition the Beer and luxury corpus into 10 and 15 groups, respectively, and then apply the algorithm into 5 randomly chosen groups, and report the average performance.

In the task (I-I), the training and test data lie in the same ranges of time slices. We compare our methods sd-STC and sdSTC+multi with the baselines of dLDA , sLDA , and MedSTC . We separately acquire the accuracy in every time slice, and then report the average accuracy. Fig.5 shows that our model outperforms all the other baselines for the two corpora. The accuracy increase of our method is more significant when the the number of topics is smaller. It is mainly because we add sparse terms on both document and word codes, leading to a less noisy document representation for a small number of topics. In addition, our sdSTC+multi using both text and images achieves slightly better accura-cies than our text-only sdSTC , which prove that text and images complement each other to detect better topics.
In the task (I-II), we compare our methods sdSTC and sdSTC+multi with the baselines of LDA+dyn , sLDA+dyn , and MedSTC+dyn . Different from the task (I-I), we train with the data up to time t  X  1 and perform prediction for the data at t . Fig.6, show the results that our model achieves the best among all the methods, and the improvement of multi-view model over text-only model is significant, which indicates that image data is helpful to predict the future .
The second prediction task is, given an unseen past docu-ment d = ( u , v , g ), to predict to which time slice it is likely to belong. This is closely related to the timestamp predic-tion in the research of social media diffusion. Specially, we solve the following problem in dSTC: p ( d |M t ) = Y is the likelihood of document d given the parameters in time slice t . Similar to the task (I-I) in the previous section, we randomly split the data of every time slice into 90% for training and 10% for localization test. We compare our methods dSTC+multi and cdSTC+multi ( i.e . with or without brand competition-related terms) with the three baselines. (1) dLDA+text : dynamic LDA with only text data, (2) dL-DA+multi : multi-view dynamic LDA using both text and image data, and (3) dSTC+text : dSTC with only text data.
Fig.7 compares the average localization accuracies between our methods and baselines. We observe that our dSTC uti-lizing text, images and brands information achieve the best among all the methods. From a large accuracy rise from dL-DA+multi to cdSTC+multi , we see that the explicit modeling of brand information helps improve the performance.
In the last prediction task, we use our model to capture the market competition dynamics on the timeline. We evolve the brand competition matrix  X  along with time, based on which we predict the future market competition trends using the past data. Specifically, we train our cdSTC model using the data in the range of [1 ,t  X  1], and then predict the brand competition at t . Since there is no groundtruth for the brand occupation over the topics, we approximate the groundtruth as follows. We manually select the most interpretable top-Figure 7: Comparison of temporal localization accura-Figure 8: The KL -divergence D ( prediction || groundtruth ) ics, such as bag , watch , and perfume . For each topic, we collect all tweets at time slice t that contain the topic word denoted by S . Then, for each brand, we count the tweets of S that include the brand name in the text. Finally, we build an L -dimensional normalized histogram, each bin of which implicitly indicates the proportion of the brand in the topic S . Fig.8 shows pie graphs comparing between the estimat-ed  X  t k by our method and the approximated groundtruth. In the caption, we also report the KL -divergences for the three selected topics. Although it is hard to conclude that our prediction reflects well the actual proportions of brands over topics (mainly due to lack of accurate groundtruth), it is interesting to see that our method can visualize brand competitions over topics in a principled way while no previ-ous method has addressed so far.
To provide a deep understanding of our model, we em-pirically compare between online learning and smoothing Figure 9: Held-out perplexity comparison between on-Figure 10: Training time comparison between online approach. We split 5% of data as a held-out test set, and train the models using the other data from all time slices, in-cluding 1.04 and 5.23 millions of tweets for Beer and Luxury corpora with associated images. Fig.9 shows the perplexi-ty comparison between both approaches. We observe that online learning approach achieves a slightly higher perplex-ity than smoothing approach 4 , but both approaches does not show big difference with respect to the discovered topics and brand proportions. The training time of online learn-ing approach is significantly shorter than that of smoothing approach, especially when the topic number is large. There-fore, online approach is more scalable on a large data set. Fig.10 shows the training time for both approaches. All experiments are performed in a single-thread manner on a desktop with Intel Core-I7 CPU and 32GB RAM.
We have presented a dynamic topic model for monitor-ing temporal evolution of market competition from a large collection of tweets and their associated images. Our model is designed to successfully address three major challenges: multi-view representation of text and images, competitive-ness of multiple entities over shared topics, and tracking their temporal evolution. With experiments on a new twit-ter dataset consisting of about 10 millions of tweets and 8 millions of associated images, we showed that the proposed algorithm is more successful for the topic modeling and three prediction tasks over other candidate methods.
 Acknowledgement . This work is supported by NSF Award IIS447676. The authors thank NVIDIA for GPU donations.
