 Active and semi-supervised learning are important techniques when labeled data are scarce. Recently a method was sug-gested for combining active learning with a semi-supervised learning algorithm that uses Gaussian fields and harmonic functions. This classifier is relational in nature: it relies on having the data presented as a partially labeled graph (also known as a within-network learning problem). This work showed yet again that empirical risk minimization (ERM) was the best method to find the next instance to label and provided an efficient way to compute ERM with the semi-supervised classifier. The computational problem with ERM is that it relies on computing the risk for all possible in-stances. If we could limit the candidates that should be investigated, then we can speed up active learning consider-ably. In the case where the data is graphical in nature, we can leverage the graph structure to rapidly identify instances that are likely to be good candidates for labeling. This pa-per describes a novel hybrid approach of using of community finding and social network analytic centrality measures to identify good candidates for labeling and then using ERM to find the best instance in this candidate set. We show on real-world data that we can limit the ERM computations to a fraction of instances with comparable performance. I.2.6 [ Artificial Intelligence ]: Learning X  concept learn-ing ;J.4[ Social and Behavioral Sciences ]: Miscella-neous; E.1 [ Data Structures ]: Graphs and networks; G.2.2 [ Discrete Mathematics ]: Graph Theory X  graph al-gorithms Algorithms, Design, Experimentation, Performance active learning, statistical relational learning, semi-supervised learning, social network analysis, betweenness centrality, closeness centrality, community finding, cluster-ing, empirical risk minimization, within-network learning
Active learning and semi-supervised learning are both im-portant techniques when labeled data are scarce and unla-beled data are abundant. Active learning targets the situa-tion where it is costly to get more labeled data so which instance(s) should you get labels for in order to get the best learned model. In such a scenario, we let the learn-ing algorithm pick a set of unlabeled instances to be la-beled by an oracle (i.e., human), which will then be used as (or to augment) the labeled data set. In other words, we let the learning algorithm tell us which instances to la-bel, rather than selecting them randomly. Active learning is named as such because the learner actively asks for more la-bels in order to increase its efficacy, thereby minimizing the amount of labeled data needed to get a good model. Semi-supervised learning takes an orthogonal approach to active learning and instead uses unlabeled data to help supervised learning tasks. The name  X  X emi-supervised learning X  comes from the fact that the data used is between supervised and unsupervised learning. Semi-supervised learning promises higher accuracies with less annotating effort. Various semi-supervised learning methods have been proposed and show promising results. For an overview of methods, there is a good regularly-updated survey available on semi-supervised learning [36]. Combining these two learning frameworks seems intuitively to make sense, yet we are aware of only one such pairing [38].

Recent work in the active learning field suggests an opti-mal way to identify which instances to label next by min-imizing expected classification error [29]. This approach is also known as empirical risk minimization (ERM). The ap-proach, while having very good performance, is also com-putationally expensive as one need to consider, for each in-stance, what the classification error would be if one was to know the correct label for that instance. In other words, look at the expected classification error as the instance takes on each of the possible class-labels and weight it by the (pre-dicted) likelihood that the instance takes on that class label. This means that one needs to induce a new classifier for each unlabeled instance in order to estimate its risk.
One recently advocated method for semi-supervised learn-ing on relational data uses Gaussian fields and harmonic functions [37]. This particular method, and graph-based methods like it (e.g., [5, 18, 37, 34]), assume that data is presented in the form of a partially labeled graph, where all the data is interconnected. This setting is also known as within-network learning in statistical relational learning [21], although the graph-based methods consider only the univariate case (i.e., the graph consists only of one type of node and edge and the only variable is the class label, whose value is known for a subset of the nodes in the graph).
The Gaussian fields and harmonic functions approach has been used in conjunction with active learning with some success [38]. To our knowledge, this is the only published work where these two learning methodologies have been combined. Various active learning strategies were tested in this work, where they found that using ERM did indeed perform the best among the various strategies. While an efficient way of computing the risk was proposed (to mini-mize the computational cost), one still needs to consider all the unlabeled points, which is an expensive task. One key observation made in the work was that ERM often picked nodes that were centers of  X  X roups X  in the graphs.
This is a key observation which we seek to use here in order to speed up active learning. Specifically, if one con-siders the case where the data is graph-based, then it seems reasonable that one may use other graph-based methods to identify nodes in the graph that would be good to label next. The key contribution of this paper is an exploration into the use of social network analytic methods to identify which nodes in a partially labeled graph would be the best to label next as part of an active learning strategy. Specif-ically, we explore the use of various social network analysis centrality metrics (see, e.g., [35]) and community-finding al-gorithms (see, e.g., [11]) to find central nodes in the graph and in the communities (or clusters) in the graph. As is im-plied by this approach, we limit the range of query selection to the unlabeled data set, a practice known as poolbased active learning or selective sampling.

As our results will show, we find that using graph-based metrics by themselves do not perform very well as an ac-tive learning strategy, but combining them with the risk minimization principle turns out to work quite well. In other words, picking the most central node(s) using cen-trality measures such as closeness or betweenness does not work very well. Neither does it work well if we first find communities (clusters) and then pick central nodes in the clusters. However, by using these methods to pick a small subset of the unlabeled data as candidates for labeling and then using ERM to find which of those candidates to la-bel turns out to be a very good hybrid approach that gives us comparable performance to that of using ERM by itself. The key benefit and contribution is that we can use these methods to pick only a fraction of the unlabeled data on which to perform the expensive ERM calculations. The end result is that we achieve an order of magnitude speedup in active learning. We believe that we can achieve even higher speedups on larger data sets.

The remainder of the paper describes related work in Sec-tion 2, the semi-supervised algorithm used in this paper in Section 3, the active learning strategies we explore in Sec-tion 4. Section 5 describes our experimental study and re-sults. We conclude by discussing our findings in Section 6.
There has been a great deal of research in active learn-ing. For example, [32] select queries to minimize the version space size for support vector machines; [12] minimize the variance component of the estimated generalization error; [16] employ a committee of classifiers, and query a point whenever the committee members disagree. Most of the ac-tive learning methods do not take further advantage of the large amount of unlabeled data once the queries are selected. The work by [22] is an exception, where EM with unlabeled data is integrated into active learning. Another exception is [25], which uses a semi-supervised learning method during training. In addition to this body of work from the machine learning community, there is a large literature on the closely related topic of experimental design in statistics; [10] give a survey of experimental design from a Bayesian perspective.
Semi-supervised learning is similarly an active research area. We here briefly consider methods that are directly relevant to the graph-based setting for our study. We refer to that setting as the univariate within-network setting. For a good review of semi-supervised learning in general, we refer the reader to a regularly-updated survey [36].

Of particular relevance to the work presented in this paper are semi-supervised methods that work on graphs or net-worked data (see e.g., [5, 18, 37, 6, 34]). In these published works, the setting is not initially one of network classifica-tion. Rather, these techniques are designed to address semi-supervised learning in a transductive setting [33], but their methods have direct application to certain instances of the univariate network-classification. All the above-mentioned references induce graphs over initially non-graph data by connecting the data into a weighted network. This is done by adding edges (in various ways) based on similarity be-tween instances.

Previous experiments [6] have shown that a randomized min-cut method empirically does not perform as well as the Gaussian fields and harmonic functions method [37]. In this latter work, the induced network is treated as a Gaussian field (a random field with soft node labels) constrained such that the labeled nodes maintain their values. The value of the energy function is the weighted average of the function X  X  value at the neighboring points. The result is a principled, independent development of the weighted-voting relational neighbor classifier (wvRN) from statistical relational learn-ing [21, 20]. Experimental results show these two procedures to yield almost identical generalization performance, albeit the matrix-based procedure is much slower than the itera-tive wvRN. This suggests that wvRN can be used on larger data sets where the matrix-based procedure is not tractable to use, especially as part of an active learning strategy. We explore the use of the matrix-based method in this paper to extend prior work in that area and leave an exploration of wvRN for future studies.
We here employ the semi-supervised learning framework using Gaussian random fields and harmonic functions from recent work [37].

Following the semi-supervised paradigm, we assume that we are given l labeled instances L = { ( x 1 ,y 1 ) ,..., ( x and u unlabeled instances U = { x l +1 ,...,x l + u } . Usually l&lt;&lt;u . In our setting, we further assume that we are given a set of edges between instances, E = { w ij | i, j =1 ... ( l + u ) ,i = j } ,where w ij denote the weight (strength) of the edge between instances i and j and w ij =0ifthereisno edge connecting the two. We note that while w ij = w ji for most data (and for the data we use on our study below), this is not a necessary condition. We can then represent the data as a graph: G =( V, E ), where V = { x i | i =1 ... ( l + u ) E is as just defined.

The goal is to compute a real-valued function f : V  X  R , over G and then to assign labels according to f .Wewould like unlabeled points that are near each other to have the same label. Implicit in this is that we are assuming that the simple-yet-ubiquitous principle of homophily [4, 24] is true ( cf. assortativity, [27], and relational autocorrelation [17]). This assumption motivates the use of the following quadratic energy function on f : The minimum energy function f =argmin f L = f l E ( f )is harmonic , meaning that it satisfies f = 0 on unlabeled data points U and is equal to f l on the labeled points L . Here is the combinatorial Laplacian , given in matrix form as = D  X  W ,where D is the diagonal matrix with diagonal entries d i = [ w ij ] is the weight matrix.

The harmonic property means that that value of f at each unlabeled data point is the average of f at neighboring points:
We can compute the solution to f explicitly in terms of matrix operations. We first partition the Laplacian matrix into blocks for labeled and unlabeled nodes, and let f = values on the unlabeled data points. The solution is then given by: f is an C  X  n matrix, where n = l + u and C is the number of classes such that f ic | L =1for y i = c and f ic | L =0forall c = y i over the labeled instances, and f jc | U = p ( y j namely the predicted probability that instance j belongs to class c .
This section describes the specific active learning strate-gies we explore in this paper and our approach for using the cheap label strategies to identify a small subset of candidate instances that can then be analyzed by the more computa-tionally expensive risk minimization strategy.
Statistical models generate probability distribution over the class labels. If the classifier is certain of an instance belonging (not belonging) to a class, then the class proba-bility is closer to 1 (0). If the classifier is uncertain, then the probability gets closer to 0 . 5. In other words, the larger the margin, the more certain the classifier is. The empirical risk strategy seeks to label the instance that is most likely to increase the margin the most.

We define the estimated empirical risk  X  R or margin as: where f c ( i ) is the probability that instance i belongs to class c according to the function f and f max ( i ) = argmax c f
The empirical risk minimization (ERM) principle seeks to find the instance that minimizes the risk (maximizes the margin). We do not know the true label for any of the instances, but we can use the classifier to estimate the like-lihood that each unlabeled instance belongs a specific class. We define the estimated risk after knowing instance k as:
ERM picks the instance k that minimizes risk:
Computing f + k using the Gaussian fields approach would normally involve computing the new solution, which is O ( n as we need to invert the matrix. However, we note that [38] describes an efficient way ( O ( n )) to compute the change in risk after labeling instance k as class c . This change is computed as follows: where (  X  1 uu )  X  k is the k -thcolumnof  X  1 uu matrix, ( its k -th diagonal element and y c ( k )=1when k is labeled as c and 0 otherwise. This computation is linear since  X  1 is already computed when computing the solution f .This makes it possible to compute  X  R ( f + k )in O ( Cn ), where C is the number of classes. 1 Finding the best label then takes O ( Cn 2 )time.

This strategy is the gold standard we will compare against.
One strategy that intuitively seems that it should work well is that of labeling instances where the classifier is most uncertain. In other words, find the instances that are clos-est to the margin. We define the uncertainty, U ( i ), of a prediction as: where f max X  ( k ) is the second most likely class label for in-stance k .

It has been shown that this is often a sub-optimal labeling strategy, but we note that this actually performs quite well on some of our benchmark data and therefore include it in the library of strategies that we will use in our study below.
We now turn to one of the key contributions of this paper: graph-based selection strategies. When the data is relational
For clarity, the computation we show is O ( C 2 n ). It is straight forward to optimize the risk minimization compu-tation to be O ( Cn ). in nature and can be expressed as a graph, then it seems ob-vious that one might be able to leverage the graph structure to identify the nodes in the graph that are important (central or influential) from a communication flow perspective. Intu-itively one would expect such central nodes to also be good candidates to label. In fact, node centrality is a core con-cept in the analysis of social networks (see, e.g., [35]), from which a variety of centrality measures have been proposed. For example degree centrality computes the degree of a node: the more nodes you are connected to, the higher your cen-trality score. Of particular interest here are two particular and often-used metrics known as shortest-path betweenness centrality and shortest-path closeness centrality.
We also consider an important graph-based observation in the original work that combined semi-supervised learning and active learning: the nodes that ERM picked (on syn-thetic data sets with clear differ ently labeled clusters) were generally central nodes in the clusters. This suggests that clustering the graph and then finding central nodes in the clusters may be a good alternative way of finding candidate nodes. We explore the use of community finding algorithms ( cf. [28]) to cluster the graph and then the use of central-ity metrics within each cluster to find central nodes in the cluster to label.
Graph betweenness centrality is perhaps one of the most prominent measures of centrality [1, 15]. It measures the degree to which a node in the network is in a position of brokerage by summing up the fractions of shortest paths between other pairs of vertices that pass through it. This is therefore also known as shortest-path betweenness. Some centrality measures impose that a network need to be undi-rected and/or unweighted. This has not been imposed on the definition and computation of betweenness centrality.
Betweenness measures the degree of brokerage (or media-tion [7]) for each node in the graph, meaning that it measures how much information is propagated through each node. In graph-based semi-supervised learning, we often propagate la-bel information through the network, and identifying key nodes for such propagation seems a good choice for label selection in active learning.

We define shortest-path betweenness as: where  X  ( s, t ) is the number of shortest paths between nodes s and t in the graph (( s, t )-paths), and  X  ( s, t | v )isthenumber of ( s, t )-paths that go through node v . By convention, if s = t ,  X  ( s, t )=1andif v  X  X  s, t } ,then  X  ( s, t | v )=0. By convention, we let 0 0 =0.

In unweighted networks, the shortest path is defined in terms of the number of edges that connect two nodes. In weighted networks, we define shortest path as the sum of edge weights for the edges that connect two nodes.
We need to compute all-pairs shortest-paths to get the centrality measure for all nodes. However, we note that there are efficient ways for computing this [8]. In fact, the all-pairs shortest-path computation can be done in O ( nE ) time, where E is the number of edges and n is the number of nodes in the graph. We will not include the algorithm here, but refer the reader to the original document [8] for details.
We use this metric directly as a labeling strategy as well as part of our hybrid approach. We note that this is a global measure and this strategy therefore imposes a con-sistent ranking of nodes regardless of any initial state (i.e., any initial labels will have no effect on this label strategy). This is actually a nice property because it means that it is robust to the initial labeled state as it ignores any initially known labels.
Graph closeness centrality is one other very popular mea-sure of centrality [2, 30]. It measures how close a node is to all other nodes in the network as defined by the shortest path from the source node to the destination node. As with betweenness, this metric inherently handles weighted and directed networks. Intuitively it seems a good idea to label nodes that are close to other unlabeled nodes as they would have a high impact on the certainty the classifier would have in its classification.

To obtain large values for small sums of distances, shortest-path closeness is defined as the inverse of the to-tal distance: where d ( v, t ) is the (weighted, directed) distance from node v to node t in the graph. These nodes are important because they can easily reach (and possibly be reached by) other nodes in the network.

As with betweenness, we need compute all-pairs shortest-paths to get the centrality measure for all nodes. We note that the same core algorithm can be used to compute both betweenness and closeness.

We intended to use this metric directly, but preliminary experiments suggests that this does not perform very well by itself. We evaluated the strategy both in global terms as well as conditioned on the distances to unlabeled nodes. However, it turns out that we can use the strategy as part of the community-finding strategy described below.
If we are correct that instances of the same class tend to cluster together, then it makes a lot of sense to cluster a graph and selecting nodes within each found cluster. We note that the semi-supervised algorithms that are graph-based in fact assume that nodes that are near each other in the graph ought to have the same label. We therefore explore the use of a clustering algorithm to help us identify such nodes.

We note that a class of modularity-based algorithms for detecting community structure have over the recent years received a lot of attention (see, e.g. , [28, 11]). These al-gorithms seek to optimize a particular modularity function that maximizes within-cluster connectivity while minimiz-ing across-cluster connectivity. They are generally based on using edge-betweenness heuristics to remove edges one at a time until the optimal modularity is found. The particular algorithm that we use here is a modified version of a fast community detection algorithm which works in reverse: it starts with each node in a separate cluster and iteratively merges clusters until the optimum modularity is reached us-ing a greedy search [11]. This fast algorithm is shown to run in O ( n log 2 n ) time on sparse networks (which most real world networks are). We use a modified algorithm which can handle weighted networks as the published version works only on unweighted networks. We refer the reader to the literature for a description of this algorithm.

The way clustering is used here is to identify clusters that do not yet have any labeled nodes and then find a central node in that cluster that should be labeled. As we likely have multiple clusters, we then rank the central nodes based on how large they are. We would like to first label the larger clusters as that would presumably give the classifier a larger cluster that it could become more certain of. To do this, we modify the closeness centrality measure to be group specific: where C v is the set of nodes in the cluster that node v be-longs to.

The advantage in our setting to using the community find-ing algorithm is three-fold: (1) the algorithm automatically detects an optimal number of clusters, (2) the clustering is hierarchical such that we can look at sub-clusters, and (3) it is quite fast X  X lmost linear for sparse graphs.
The community finding algorithm finds the optimal clus-ters initially and we start by getting labels for those clus-ters: We identify any of these top-level clusters that have no known label and first get labels for those clusters. Once all those clusters have at least one label, then we go to their sub-clusters (all sub-clusters of all the top-level clusters) and repeat using a breadth-first approach.

We explored using this strategy directly, but as with the closeness strategy, preliminary experiments suggests that this does not perform very well by itself. However, closer analysis of its performance revealed that while its top-picks in each cluster were generally not the best, the top k  X  3 nodes in each cluster often had a node that ERM would have picked. We therefore do not evaluate this strategy but use it as part of the hybrid approach described below.
Our initial experiments showed that while betweenness of-ten worked well, it was not consistent and it was not as good as ERM. Further, neither the closeness nor the community-finding strategy worked well by themselves, although the community-finding algorithm often was in the right ball-park as it often had a good selection if one were to consider more than just the best node at each iteration.

This suggests that perhaps a better approach is to use the graph-based strategies to prune down the list of pos-sible nodes that one should look at in more detail using ERM. This approach constitutes the main contribution of this paper: using a hybrid strategy to first prune candidate nodes and then using ERM on this smaller selection with-out hurting performance and yet speeding up active learning significantly.

The hybrid strategy works by asking three strategies for their top picks, and then using ERM to pick the best pick among the union of these top picks. The strategies the hybrid approach uses are: uncertainty, betweenness and cluster-finding. The hybrid approach asks each strategy for its top 3 picks (for a maximum of 9 nodes, although there is often some amount of overlap). It then uses ERM to find the top candidate node among this set. We explored using other settings and get similar behavior although we need to get more than one pick per strategy for this to get good performance.
 As we limit each iteration to having to do at most 9 ERM node-computations, we achieve a significant amount of speedup. If we consider the amount of computations needed at each iteration, we can see that this is quite sig-nificant. The uncertainty labeling strategy takes O ( n )to find the top k for small k (although our implementation was not optimized and actually sorted the list to give an O ( n log n ) computational cost). Using the betweenness cen-trality strategy is likewise relatively cheap. It has an initial cost of O ( nE + n log n ), but is then O (1) as the sorted list of nodes will not change over the runs. The community-finding algorithm is more complex and takes more time. It takes an initial O ( n log 2 n ) to generate the clusters (although this can get arbitrarily close to O ( n 3 ) as the graph becomes more dense. However, since we need to invert a matrix for the semi-supervised learning step, this is at worst the same as one learning iteration). After that, we compute the most central node in each cluster, which is O ( n 2 C )percluster, where n C is the number of nodes in that cluster. However, we only compute this for a limited set of clusters in each it-eration, making n C &lt;&lt; n . However, this is the most costly operation after the ERM step. As we shall see, this is still significantly faster than using ERM by itself.
We performed an experimental study on 10 benchmark networked data sets to evaluate the efficacy of our proposed hybrid approach in terms of performing comparably to ERM as well as being significantly faster in terms of time spent. We here describe our study, which consists of a description of the data, our experimental methodology and our result.
We consider in this study 11 data sets from three domains: computer science department websites (WebKB), industry news, and academic papers. 3 The first domain we draw from is based on the WebKB Project [13]. 4 It consists of sets of web pages from four com-puter science departments, with each page manually labeled into 7 categories. As with other work [26, 19], we ignore pages in the  X  X ther X  category except as described below.
From the WebKB data we produce eight networked data sets, two for each of the four universities, ranging in size from 338 to 434. Four networks contained six classes, al-though two classes had so few instances that this, in effect, turned into a four-class pro blem. The second set of four networks simplified the classes into a student/not-student binary classification task.

Following prior work on web-page classification, we link two pages by co-citations (if x links to z and y links to z , then x and y are co-citing z ) [9, 19]. To weight the link between x and y , we sum the number of hyperlinks from x to z and separately the number from y to z , and multiply these two quantities. This weight represents the number of
We used NetKit-SRL for all of these studies, as it provided an easy way to ensure the same experimental environment across all runs. NetKit-SRL is available at http://netkit-srl.sourceforge.net
All data sets are available at http://netkit-srl.sourceforge.net
We use the WebKB-ILP-98 data. possible co-citation paths between the pages. Finally, to get a sparser and cleaner graph, we keep only the top 5 most highly weighted edges from every node.

To produce the final data sets, we removed pages in the  X  X ther X  category from the classification task, although they were used as  X  X ackground X  knowledge X  X llowing 2 pages to be linked by a path through an  X  X ther X  page.
The second domain we draw from involves classifying com-panies by industry sector. Companies are linked via cooccur-rence in text documents. We use two different data sets, rep-resenting different sources and distributions of documents and different time periods (which correspond to different topic distributions).
 Industry Classification (YH)
As part of a study of activity monitoring, [14] collected 22 , 170 business news stories from the web between 4/1/1999 and 8/4/1999. Following a prior study, we placed an edge between two companies if they appeared together in a story [3]. The weight of an edge is the number of such cooccur-rences found in the complete corpus. The resulting network comprises 1798 companies that cooccurred with at least one other company. To classify a company, we used Yahoo! X  X  12 industry sectors.
 Industry Classification (PR)
The second industry classification data set is based on 35 , 318 PR Newswire press releases gathered from April 1, 2003 through September 30, 2003. We used the same data preparation as above, resulting in a network of 2189 compa-nies that cooccurred with at least one other company. We use the same classification scheme as above.
The last domain is academic papers. We used the cora data set [23] comprises 4240 computer science research pa-pers. It includes the full citation graph as well as labels for the topic of each paper (and potentially sub-and sub-sub-topics). Following a prior study [31], we focused on papers within the machine learning topic with the classification task of predicting a paper X  X  sub-topic (of which there are seven).
Papers can be linked if they share a common author, or if one cites the other. Following prior work [19], we link two papers if one cites the other. The weight of an edge would normally be one unless the two papers cite each other (in which case it is two X  X here can be no other weight for existing edges).
The main objective of this study is to test the hybrid approach for performance and speed. We start off, for each data set, by randomly sampling one instance of each class and then run 100 iterations of active learning using various strategies. Wecomputetimespentandclassifieraccuracy at each iteration. We then average over 10 runs.
The primary objective in this work was to significantly speed up active learning and yet keep a performance com-parable to using empirical risk minimization (ERM). We first compare ERM to uncertainty, betweenness and uni-form random sampling [29] to explore the difference in per-Figure 1: Two prototypical performance signatures of ERM, uncertainty labeling, labeling based on be-tweenness centrality and uniform random sampling. As we can see, ERM is consistently a good per-formance while the other two have higher variance among different data sets. We tested community finding and closeness as well, but both performed so bad that they are not included here. formance between these four active learning strategies and to verify that ERM is in fact consistently the better strat-egy. Figure 1 shows prototypical performance signatures for the strategies on two of the 11 data sets we explored. As we can see, uncertainty labeling and betweenness labeling both turn out to perform close to ERM some of the time but they are not consistently as good as ERM. Also, we see that random sampling was significantly worse than ERM. In the 11 data sets, betweenness only beat ERM once in the be-ginning but then became worse as we sampled more points. Betweenness was otherwise consistently worse than ERM. Uncertainty labeling was somewhat comparable to ERM in 3 cases, but significantly worse in the other 8. Random sam-pling was always worse than ERM. These results shows that ERM is the best consistent strategy to use.

We now turn to the main evaluation of this study, namely that of the hybrid approach versus ERM. Specifically, we want to see that the hybrid approach has similar perfor-mance to that of ERM and yet is significantly faster. Fig-ure 2 shows three prototypical runs of ERM against the hy-Figure 2: Three prototypical performance signa-tures of ERM as compared against the hybrid strat-egy. As we can see, they perform comparably. In fact, the most prototypical run is at the top, where they are almost on top of each other. brid strategy. As we can see, the hybrid approach has very similar performance overall. It interestingly outperformed ERM early in the runs on the webkb washington binary data set, but they did end up converging again later on. We see that on the webkb wisconsin binary data set that ERM actually starts to outperform the hybrid approach towards the end, but still the two strategies have very similar perfor-dataset %picked # multiple cora 77 86 538 1 industry-pr 90 368 544 2 industry-yh 19 304 689 12 cornell-binary 126 623 369 118 cornell-multi 35 755 299 89 texas-binary 98 401 592 91 texas-multi 163 685 285 113 washington-binary 206 725 280 211 washington-multi 257 720 251 228 wisconsin-binary 388 620 274 282 wisconsin-multi 240 698 254 192 CF: Community finding strategy US: Uncertainty sampling BC: Betweenness centrality Table 1: How often was each strategy picked for each dataset(outof 1000 , except for cora, where it was out of 700 )? Last column shows how often more than one strategy picked the node ultimately selected by ERM. mance curves. We note that the most prototypical compar-ison is on the industry  X  pr data, where the two strategies are almost on top of each other. This is the behavior we see on 7 of the 11 data sets. ERM was slightly better (1  X  2%) on the remaining data sets.

One might wonder how much each of the three strategies in the hybrid approach contributes to this performance. In particular, do we need all three strategies and what would happen if we were to use only a subset of them? Table 1 shows how often each strategy was picked as well as how often more than one strategy picked the node selected by ERM. Three interesting observations can be made from this: First, all three strategies are often used a significant amount of the time; Second, the three larger data sets relied sig-nificantly more on betweenness than the smaller data sets, which relied heavily on uncertainty sampling; Three, com-munity finding was generally the least-used strategy. We explored whether removing betweenness or the community finding strategy would have any effect on the performance and removing either strategy did consistently make the hy-brid method perform worse.
 We finally turn to how long it took to do these runs. 5 Figure 3 shows two runs, the one with the best and the one with the worst performance gain. As we can see, we achieve a tenfold speedup in the industry-yh data set, which is the larger of the two. We also get a tenfold speedup on the industry-pr data set, whereas we generally see a 3  X  5times speedup on the webkb data sets. This is not all that sur-prising given the size of these data sets. The webkb data are only around 400 and its is therefore very quick to com-pute ERM even for all unlabeled nodes. As the data sets increase in size, the O ( Cn 2 ) cost of ERM becomes more sig-nificant as we see on the industry data sets. However, we run into a limitation of the classifier as the data sets be-come even larger as in the cora data set (4240 instances).
We emphasize that we did not spend much time optimiz-ing the hybrid strategy computations whereas we used the efficient ERM computations fr om the literature. The com-parisons here are therefore not in our favor. Figure 3: Two prototypical timed runs of ERM and the hybrid strategy. We pick these two as they show the two most extreme cases. In industry-yh (the larger data set), we see a ten-fold speedup, whereas we get only a three times speedup on the texas data set.
 Inthecoradataweachieveaspeedupof5overusingERM.
 This is because we use the matrix-based classifier ( O ( n with an optimized ERM computation ( O ( Cn 2 ). We would expect that as the data set increases, the classifier itself be-comes the bottle neck and any speedup in the sampling be-comes less significant. We remind the reader that we used the matrix-based algorithm here in order to extend existing literature. This finding suggests that using a faster approxi-mate method such as wvRN-RL as described in Section 2 is critical in order to scale active learning to larger networked data sets.
We observed that recent work on combining semi-supervised learning and active learning used a learning method that was inherently graph-based and, in effect, used label propagation to complete label a partially labeled graph. This setting is identical to the univariate within-network framework described in the statistical relational learning literature, and we observed that if the data is in fact networked then the structure of the network should be able to help in identifying important nodes that should be labeled in order to achieve maximum prediction accuracy. The use of network structure to help in node selection in active learning is a novel idea.

First, we reviewed current literature on combining semi-supervised learning and active learning, noting that the cur-rent state-of-the-art active learning strategy uses empirical risk minimization (ERM), but it is computationally expen-sive. We therefore suggested using graph-based centrality measures from social network analysis to either replace ERM or help it by suggesting a small fraction of nodes that should be considered, thereby significantly speeding up the active learning process.

We showed on 11 benchmark graph-based data sets that although graph-based measures did not perform as well as ERM by themselves, a hybrid approach of using the graph-based centrality measures to identify a small subset of can-didate nodes did in fact perform comparably to (and some-times even beating) ERM while dramatically speeding up the learning process by more than an order of magnitude.
We note that this is an initial exploration of using the structure of the graph and that our results suggest that this research direction has potential to dramatically improve ac-tive learning in terms of speed and perhaps even perfor-mance. For example, there are numerous centrality and community finding algorithms available that are good candi-dates for exploring better candidate selection. We explored one way of doing so with good success. However, there is still much that can be done in terms of optimizing node se-lection. The work presented here shows the potential for the use of such methods.
 Thanks to reviewers for suggesting improvements to this pa-per. Thanks to Foster Provost for initially suggesting to look at active learning in the context of within-network learning. This work was sponsored in part by the Office of Naval Re-search under award number N00014-07-C-0923. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Office of Naval Research or the U.S. Government. [1] J. Anthonisse. The rush in a directed graph. Technical [2] M. A. Beauchamp. An improved index of centrality. [3] A.Bernstein,S.Clearwater,andF.Provost.The [4] P. M. Blau. Inequality and Heterogeneity: A Primitive [5] A. Blum and S. Chawla. Learning from Labeled and [6] A. Blum, J. Lafferty, R. Reddy, and M. R.
 [7] S. P. Borgatti and M. G. Everett. A graph-theoretic [8] U. Brandes. On variants of shortest-path betweenness [9] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced [10] K. Chaloner and I. Verdinelli. Bayesian experimental [11] A. Clauset, M. E. J. Newman, and C. Moore. Finding [12] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active [13] M. Craven, D. Freitag, A. McCallum, T. Mitchell, [14] T. Fawcett and F. Provost. Activity monitoring: [15] L. C. Freeman. A set of measures of centrality based [16] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. [17] D. Jensen and J. Neville. Linkage and Autocorrelation [18] T. Joachims. Transductive Learning via Spectral [19] Q. Lu and L. Getoor. Link-Based Classification. In [20] S. A. Macskassy and F. Provost. A Simple Relational [21] S. A. Macskassy and F. Provost. Classification in [22] A. McCallum and K. Nigam. Employing em in [23] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. [24] M. McPherson, L. Smith-Lovin, and J. M. Cook. [25] I. Muslea, S. Minton, and C. Knoblock. Active + [26] J. Neville, D. Jensen, L. Friedland, and M. Hay. [27] M. E. J. Newman. Mixing patterns in networks. [28] M. E. J. Newman. The structure and function of [29] N. Roy and A. McCallum. Toward optimal active [30] G. Sabidussi. The centrality index of a graph. [31] B. Taskar, E. Segal, and D. Koller. Probabilistic [32] S. Tong and D. Koller. Support vector machine active [33] V. N. Vapnik. Statistical Learning Theory .John [34] F. Wang and C. Zhang. Label propagation through [35] S. Wasserman and K. Faust. Social Network Analysis . [36] X. Zhu. Semi-supervised learning literature survey. [37] X. Zhu, Z. Ghahramani, and J. Lafferty.
 [38] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining
