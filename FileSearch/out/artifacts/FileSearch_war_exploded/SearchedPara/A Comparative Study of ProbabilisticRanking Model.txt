 SHIH-HSIANG LIN and BERLIN CHEN National Taiwan Normal University and HSIN-MIN WANG Institute of Information Science, Academia Sinica 1. INTRODUCTION Because of the rapid development and maturity of multimedia technologies, huge quantities of multimedia content, including audio and video materials, are already available and databases continue to expand. In r ecent years, a great deal of research has focused on using speech content to understand and organize multimedia data [Lee and Chen 2005; Koumpis and Ren als 2005; Gilbert and Feng 2008; Chelba et al. 2008; Christensen et al. 2008]. Spoken document summarization, which tries to distill salient inf ormation and remove redundant or incorrect information from spoken documents, can help users re-view spoken documents efficiently and understand associate d topics quickly. Generally speaking, spoken document summarization techni ques can be clas-sified as either extractive or abstractive. Extractive summ arization produces a summary by selecting indicative sentences, passages, or p aragraphs from an original document according to a predefined target summar ization ratio. Abstractive summarization, on the other hand, provides a flu ent and concise abstract of a certain length that reflects the key concepts of the document [Paice 1990; Witbrock and Mittal 1999]. This requires highl y sophisticated techniques, including semantic representation and infere nce, as well as nat-ural language generation. Thus, in recent years, researche rs have tended to focus on extractive summarization.
 three broad categories: 1) approaches based on sentence str ucture or location information, 2) approaches based on proximity or significan ce measures, and 3) approaches based on sentence classification. Baxendale [ 1958] and Hirohata et al. [2005] suggested that important sentences can be sele cted from the sig-nificant parts of a document, for example, the introduction a nd the conclusion. However, such approaches can only be applied to documents in some specific domains or structures. In contrast, approaches based on pro ximity or signif-icance measures attempt to select salient sentences based o n the statistical features of the sentences or the words in the sentences, such as the term fre-quency (TF), inverse document frequency (IDF), N -gram scores, and the topic or concept information. Associated methods based on these f eatures have at-tracted much attention in recent years. For example, the vec tor space model (VSM) [Gong and Liu 2001; Lee and Chen 2005] and the maximum ma rginal relevance (MMR) method [Murray et al. 2005] represent the wh ole document and each of its sentences in vector form consisting of statis tical features, and then select important sentences based on the proximity meas ure between the vector representations of the document and its sentences; t he latent seman-tic analysis (LSA) method [Gong and Liu 2001] estimates the s ignificance of a sentence by projecting the vector representation of the se ntence into the latent semantic space of the document; and the sentence sign ificance score method (SIG) [Furui et al. 2004] estimates the significance o f a sentence by linearly combining a set of statistical features of the sent ence. In addition, a number of classification-based methods that use statistic al features and/or sentence structure information have been developed. Examp les of such meth-ods include the Bayesian classifier (BC) [Kupiec et al. 1999] , the Gaussian mixture model (GMM) [Murray et al. 2005], the hidden Markov m odel (HMM) [Conroy and O X  X eary 2001; Maskey and Hirschberg 2006], the s upport vector machine (SVM) [Zhang et al. 2007], and the conditional rando m fields (CRF) [Shen et al. 2007]. These methods usually formulate sentenc e selection as a binary classification problem, that is, a sentence can be inc luded in or excluded from a summary. A training set, comprised of documents and th eir correspond-ing handcrafted summaries (or labeled data), is needed to tr ain the classifiers. However, manual labeling is expensive in terms of time and pe rsonnel. To over-come this shortcoming, we proposed a probabilistic generat ive framework that can perform spoken document summarization tasks in a comple tely unsuper-vised manner [Chen et al. 2006a; Chen and Chen 2008]. The fram ework treats each sentence of a spoken document to be summarized as a proba bilistic gen-erative model for generating the document, and ranks and sel ects sentences according to their likelihoods.
 ranking models for extractive spoken document summarizati on, including supervised classification-based summarizers and unsuperv ised probabilistic generative summarizers. Furthermore, we also investigate the use of unsu-pervised summarizers to improve the performance of supervi sed summarizers when manual labels are not available for training the latter . A novel training data selection approach that leverages the relevance infor mation of spoken sentences to select reliable document-summary pairs deriv ed by the unsuper-vised summarizers is explored for training the supervised s ummarizers. describes three popular supervised classification-based m ethods for spoken document summarization, namely, the Bayesian classifier, t he support vector machine, and the conditional random fields. Section 3 sheds l ight on the theoretical foundations of two unsupervised probabilisti c generative summa-rizers, namely, the language modeling approach and the sent ence topic model. Then we elucidate the training data selection approach for u nsupervised train-ing of the classification-based summarizers in Section 4. Th e experimental setup and a series of summarization experiments are present ed in Sections 5 and 6, respectively. We then draw our conclusions and direct ions for future work in Section 7. 2. SUPERVISED CLASSIFICATION-BASED SUMMARIZERS Extractive spoken document summarization can be treated as a two-class (positive/negative) classification problem. A sentence S i with a set of J rep-resentative features X i = x i 1 , , x ij , , x iJ is input to the classifier. If it belongs to the positive class, it will be selected as part of t he summary; otherwise, it will be excluded. Several popular classifiers can be used for this purpose. In this article, we exploit three such classifiers, namely the Bayesian classifier, the support vector machine, and the conditional random fields. To summarize spoken documents with different summary ratios, the important sentences of a document D can be selected (or ranked) based on P S i  X  S | X i , the posterior probability of a sentence S i being included in the summary S given the feature set X i . 2.1 Bayesian Classifier (BC) BC is a simple but powerful supervised classification techni que based on Bayes X  theorem. The posterior probability of a sentence S i being included in the sum-mary class S can be computed as follows [Kupiec et al. 1999]: where the evidence P ( X i ) is the marginal probability that the set of represen-tative features of a sentence is observed, regardless of whe ther the sentence belongs to the summary (positive) class or the nonsummary (n egative) class. The evidence P ( X i ) can be expressed as: where P X i | S i  X  S and P X i | S i  X   X  S are the likelihoods that X i is generated by the summary class and the nonsummary class, respectively . In this article, the prior probability of S i belonging to the summary class P S i  X  S or the nonsummary class P S i  X   X  S is set to be equal. 2.2 Support Vector Machine (SVM) The concept of SVM is based on the principle of structural ris k minimization (SRM) in statistical learning theory [Vapnik 1998]. If a dat aset is linearly separable, SVM attempts to find an optimal hyper-plane by uti lizing a decision function that can correctly separate the positive and negat ive instances, and ensure that the margin is maximal. In a nonlinearly separabl e case, SVM uses kernel functions or defines slack variables to transfor m the problem into a linear discrimination problem. In this article, we use the LIBSVM toolkit [Chang and Lin 2001] to construct a binary SVM summarizer and adopt the radial basis function (RBF) as the kernel function. The post erior probability of a sentence S i being included in the summary class can be approximated by th e following sigmoid operation [Lin et al. 2003]: where the weights  X  and  X  are estimated from the development set by mini-mizing a negative log-likelihood function, and g ( X i ) is the decision value of X i provided by the SVM summarizer. 2.3 Conditional Random Fields (CRF) Although BC and SVM have proved effective for many classifica tion prob-lems, the bag-of-instances assumption (or the bag-of-sent ences assumption when applied in extractive spoken document summarization) is a major short-coming. More precisely, BC and SVM classify each sentence in dependently without considering the dependent relationships among sen tences. The CRF model, on the other hand, can effectively capture the depend ent relationships among sentences. It is an undirected discriminative graphi cal model that combines the advantages of the maximum entropy Markov model (MEMM) and the hidden Markov model (HMM). The probability of a state sequence sequence X = { X 1 , . . . , X i , . . . , X I } [Lafferty et al. 2001] is computed by where each y i can be a summary or nonsummary state; Z X is a normalization factor computed by summing all possible state sequences to e nsure that the summation of the probabilities of all state sequences is equ al to one; I is the number of sentences in a document D ; f k ( y i , X i ) is a function that measures a feature relating the state y i for sentence i with the input features X i ; and  X  k is the weight of each feature function learned from the devel opment set. In this article, we adopt a linear-chain CRF model to summari ze a spoken document, and simply apply the forward-backward algorithm to obtain the posterior probability of each sentence S i being a summary sentence given the whole sentence sequence for sentence ranking. 3. UNSUPERVISED PROBABILISTIC GENERATIVE SUMMARIZERS In our recent works, we addressed the issue of extractive sum marization under an unsupervised probabilistic generative framework [Chen et al. 2006a; Chen and Chen 2008]. Each sentence S i of a spoken document D is treated as a prob-abilistic generative model for generating the document, an d the sentences are ranked and selected according to their posterior probabili ties P ( S i | D ) , which can be expressed as: where P ( D | S i ) is the sentence generative probability, that is, the likeli hood that D is generated by S i ; P ( S i ) is the prior probability of S i being impor-tant; and P ( D ) is the probability of D . Note that P ( D ) in Equation (5) can be eliminated because it is identical for all sentences; hen ce, it will not affect their ranking. Furthermore, since the way to estimate the pr ior probability of a sentence is still under active study, we assume that the prio r probability is uni-formly distributed in this article. The sentence generativ e probability P ( D | S i ) can be taken as a relevance measure between the document and i ts sentences. Therefore, the sentences of a spoken document D can be ranked by means of the probability P ( D | S i ) , instead of the probability P ( S i | D ) . 3.1 Language Modeling Approach (LM) In the language modeling approach, each sentence in a docume nt is regarded as a probabilistic generative model consisting of N -gram distributions for predicting the document [Chen et al. 2006a]. The N -gram distributions are directly estimated from each sentence and smoothed by the N -gram distribu-tions estimated from a large text corpus. In this article, we only investigate the unigram (bag-of-words) modeling for the LM approach: where  X  is a weighting parameter, and c ( w n , D ) is the occurrence count of a specific type of word (or term) w n in D , reflecting that w n will contribute more in the calculation of P LM ( D | S i ) if it occurs more frequently in D [Croft and Lafferty 2003]. The sentence model P ( w n | S i ) and the collection model P ( w n | Collection ) are estimated from the sentence itself and a large general text collection (see Section 5.1), respectively, using max imum likelihood es-timation (MLE). Note that the process defined in Equation (6) is similar to interpolation-based language model smoothing or adaptati on for speech recog-nition [Zhai and Lafferty 2001; Bellegarda 2004]. The weigh ting parameter  X  can be further optimized by using the expectation-maximiza tion (EM) training algorithm [Dempster et al. 1977]. This relevance measure is computed based on the frequency that the document words occur in the sentenc e, which is ac-tually a kind of literal term matching [Lee and Chen 2005]. 3.2 Sentence Topic Model (STM) In the sentence topic model, a set of K latent topical distributions character-ized by unigram language models are used to predict document terms, and each latent topic is associated with a sentence-specific wei ght. In other words, each term can belong to several topics. Therefore, the sente nce generative probability can be expressed as follows: type of word w n occurring in a latent topic T k and the posterior probability (or weight) of the topic T k conditioned on the sentence S i . The topical unigram P ( w n | T k ) is shared by all sentences and can be estimated by maximizing the collection likelihood on a set of contemporaneous (or in-do main) text news doc-uments (see Section 5.1). In contrast, each sentence S i of a spoken document has its own probability distribution P ( T k | S i ) over the latent topics, which is unknown in advance but can be estimated on the fly during the su mmarization process by maximizing the log-likelihood of the document D generated by the STM model of the sentence S i , using the EM training algorithm [Chen et al. 2006a].
 frequency that the document words occur in the sentence. Ins tead, it is de-rived from the frequency of the document words in the latent t opics as well as the likelihood that the sentence will generate the respec tive topics. Hence, STM is actually a type of concept matching approach [Lee and C hen 2005]. In recent years, structures similar to the presented topic mod el have also been extensively investigated for information retrieval (IR) t asks [Hofmann 2001; Blei et al. 2003; Chen 2006]. 4. UNSUPERVISED TRAINING OF CLASSIFICATION-BASED SUMMARI ZERS One major disadvantage of classification-based summarizer s is the need for a certain amount of labeled data for model training. However, manual labeling of the document-reference summary information is time-consu ming and imprac-tical for many summarization tasks. The issue of semi-super vised (or purely unsupervised) training of various classification-based mo dels by utilizing a small amount of labeled data together with a large amount of u nlabeled data has attracted a great deal of interest in many speech and lang uage processing tasks over the decade [Duda et al. 2001; Nomoto and Matsumoto 2001; Chen et al. 2004; Zhu 2005; Wong et al. 2008]. In this article, we in vestigate the use of unsupervised probabilistic generative summarizers to improve the per-formance of supervised classification-based summarizers u nder the condition that the handcrafted document-reference summary pairs are not available for training the latter. Nevertheless, as will be discussed in S ection 6.3, it was experimentally observed that the performance of a classific ation-based sum-marizer (e.g., CRF) trained with the document-summary labe ls derived automatically by a probabilistic generative summarizer (e .g., STM) would be slightly worse than the original performance of the probabi listic generative summarizer. This observation seems to reflect the fact that t here exists a significant performance gap between the results obtained by the classification-based summarizer trained with manually labeled data and aut omatically labeled data. Thus, how to filter out unreliable automatic la bels or collect more reliable automatic labels for training the classificat ion-based summariz-ers without supervision is deemed to be an important issue fo r reducing the performance gap.
 selection approach, which leverages the relevance informa tion of the sentences in a training spoken document, to filter out unreliable summa ry or nonsum-mary sentence labels for training the classification-based summarizers with-out supervision. The relevance information of a spoken sent ence S i is defined by the average similarity of documents in the relevant text n ews document set D top M of S i , where D i top M is obtained by taking S i as a query and posing it to a information retrieval (IR) system to obtained a list of M most relevant docu-ments from a contemporaneous text news repository (see Sect ion 5.1). Our as-sumption is that the relevant text documents retrieved for a summary sentence might have the same or similar topics because a summary sente nce is usually indicative for some specific topic related to the document. I n contrast, the rele-vant text documents retrieved for a nonsummary sentence mig ht cover diverse topics. In other words, the relevance information estimate d based on the simi-larity of documents in the relevant text document set might b e a good indicator for determining the importance of a spoken sentence. Conseq uently, we can select reliable summary or nonsummary sentences derived by the probabilis-tic generative summarizers for training the classification -based summarizers based on such relevance information. The average similarit y of documents in the relevant text document set D i top M for a spoken sentence S i is computed by where E D l is the TF-IDF vector representation of the document D l , and M is the number of documents in the retrieved relevant text document set D i top M . The practical implementation issue of such data selection for u nsupervised training of the classification-based summarizers will be further det ailed in Section 6.3. 5. EXPERIMENTAL SETUP 5.1 Speech and Text Corpora The speech data set used in this research is the MATBN corpus [ Wang et al. 2005], which is different from the set of broadcast news docu ments used in our previous studies [Chen et al. 2006a; Chen and Chen 2008]. It c ontains approx-imately 200 hours of Mandarin Chinese TV broadcast news coll ected by Acad-emia Sinica and the Public Television Service Foundation of Taiwan between November 2001 and April 2003. The content has been segmented into separate stories and transcribed manually. Each story contains the s peech of one studio anchor person, as well as several field reporters and intervi ewees. A subset of 205 broadcast news documents (spoken documents that covere d a wide range of topics) compiled between November 2001 and August 2002 wa s reserved for the summarization experiments. Twenty-five hours of gender -balanced speech from the remaining speech data were used to train the acousti c models for speech recognition. The data was first used to bootstrap the a coustic model training with the MLE criterion. Then, the acoustic models w ere further opti-mized by the minimum phone error (MPE) discriminative train ing algorithm [Povey and Woodland 2002; Liu et al. 2007].
 Agency (CNA) between 1991 and 2002 (the Chinese Gigaword Cor pus released by LDC) were also used. The documents collected in 2000 and 20 01 were used to train N -gram language models for speech recognition with the SRI Language Modeling Toolkit [Stolcke 2005]. In addition, a su bset of about 14,000 text news documents, collected during the same perio d as the broadcast news documents to be summarized, was used to estimate the col lection model P ( w n | Collection ) in Equation (6) for the LM approach and the topical unigram P ( w n | T k ) in Equation (7) for the STM approach. This subset of text news doc-uments was also used as the basis for estimating the model par ameters for the VSM, LSA, MMR, and SIG approaches.
 5.2 Automatic Broadcast News Transcription System Front-end processing was implemented with the HLDA (Hetero scedastic Lin-ear Discriminant Analysis)-based, data-driven Mel-frequ ency feature extrac-tion approach [Kumar 1997], and processed by MLLT (Maximum L ikelihood Linear Transformation) for feature decorrelation [Saon et al. 2000]. We then applied utterance-based feature mean subtraction and vari ance normalization. synchronous Viterbi tree-copy search and a lexical prefix tr ee organization of the lexicon [Aubert 2002]. To select the most promising path hypotheses for each speech frame, we used a beam pruning technique, which co nsiders the decoding scores of the path hypotheses together with their c orresponding uni-gram language model look-ahead scores and syllable-level a coustic model look-ahead scores [Chen et al. 2004]. If the scores of the word hypo theses at the end of each speech frame were higher than a predefined threshold, their associated decoding information, such as the words X  start and end frame s, the identities of the current and preceding words, and the acoustic score, w ere kept to build a word graph for further language model rescoring [Ortmanns e t al. 1997]. The word bigram language model was used in the tree search proced ure, while the trigram language model was used in the word graph rescoring p rocedure. 5.3 Spoken Documents for the Summarization Experiments Three subjects were asked to create summaries of the 205 spok en documents for the summarization experiments as references (the gold s tandard) for eval-uation. The summaries were compiled by selecting 50% of the m ost important sentences in the reference transcript of a spoken document, and ranking them by importance without assigning a score to each sentence. Th e average Chinese character error rate (CER) obtained for the 205 spoken docum ents was about 30%. Detailed statistics of the 205 spoken documents are giv en in Table I. sisting of 100 documents, was taken as the development set, w hich formed the basis for tuning the parameters or settings. The second p art, consisting of the remaining 105 documents, was taken as the held-out tes t set. That is, all the summarization experiments conducted on the test set followed the same training (or parameter) settings and model complex ities that were optimized based on the development set. 1 Therefore, the experimental results can be used to estimate the effectiveness of the summarizers on comparable real-world data. 5.4 Performance Evaluation For the performance evaluation, we used the ROUGE measure [L in 2003]. It evaluates the quality of the summarization by counting the n umber of overlap-ping units, such as N -grams and word sequences, in the automatic summary and a set of reference (or manual) summaries. The ROUGE N is an N -gram recall measure defined as follows: where N denotes the length of the N -gram; S is an individual reference (or manual) summary; S R is a set of reference summaries; Count match ( gram N ) is the maximum number of N -grams cooccurring in the automatic summary and the reference summary; and Count ( gram N ) is the number of N -grams in the reference summary. Since ROUGE N is a recall measure, increasing the sum-mary length (or the summarization ratio) tends to increase t he chances of get-ting higher scores. In this article, we adopt the widely used ROUGE 2 measure [Maskey and Hirschberg 2005; Murray et al. 2005; Shen et al. 2 007], which uses word bigrams as the matching units.
 ratios (10%, 20%, and 30%), defined as the ratio of the number o f sentences in the automatic (or manual) summary to that in the reference tr anscript of a spo-ken document. As shown in Table II, the levels of agreement on the ROUGE 2 measure between the three subjects for important sentence r anking are about 0.65, 0.67, and 0.68 for the summarization ratios of 10%, 20% , and 30%, respec-tively. Each of these values was obtained by calculating the ROUGE 2 recall rate, using the summary created by one of the three subjects a s the reference summary, in turn for each subject, while those of the other tw o subjects as the test summaries, and then taking their average. These observ ations seem to reflect the fact that people may not always agree with each oth er in selecting the important sentences for representing a given document. 5.5 Features for Supervised Summarizers Several features have been designed and widely used in the su pervised sum-marization approaches [Shen et al. 2007; Zhang et al. 2007; W ong et al. 2008]. In this article, we use a set of 19 features to characterize a s poken sentence, including the structural features, the lexical features, t he acoustic features, and the relevance features. The features are outlined in Tab le III. The struc-tural features (or the so-called surface features) basical ly stem from two kinds of information: POSITION and DURATION . The structure of a broadcast news story is usually rather regular, and may include an intr oductory remark, event description, conclusion, and a footnote by the report er. Obviously, the summary often appears in the introduction and conclusion. T herefore, if a spoken document contains K sentences, the position feature of the j -th sen-tence in the document is defined as | j  X  ( K / 2 ) | . The duration features of a sen-tence consist of the information about the duration of the se ntence itself, as well as the duration of the preceding and subsequent sentenc es [Shen et al. 2007]. The lexical features (or the so-called content featu res) we consider are BIGRAM SCORE , SIMILARITY , and NUM NAME ENTITIES . The bigram language model score is computed as the product of the bigram probabilities of words occurring in a sentence, and then normalized by the s entence length (number of words). The similarity score between a sentence a nd its neighbor-ing sentences is computed using the cosine measure. To do thi s, we represent each sentence in vector form, where each dimension specifies a weighted statis-tic, that is, the product of the term frequency (TF) and the in verse document frequency (IDF) associated with an indexing term (or word) i n the sentence [Baeza-Yates and Ribeiro-Neto 1999]. We only use the simila rity of a sentence to those that immediately precede or follow it. The number of named entities (NEs) in a sentence is also taken as a predictor of the lexical cues [Lee and Chen 2005], which is based on the idea that a sentence contain ing more NEs can very often give an overview of the spoken documents and th us is more likely to be included in the summary [Maskey and Hirschberg 2 005]. The acoustic features are PITCH , ENERGY , and CONFIDENCE . The pitch and energy features are extracted from the broadcast news speec h using the Snack toolkit [Sj  X  olander 2001]. The confidence feature is computed as the aver age word posterior probability of a spoken sentence [Wessel et a l. 2001], which to some extent quantifies the degree of correctness of the recog nition transcripts. For each of these acoustic features, the minimum, maximum, m ean, and differ-ence values of a spoken sentence are extracted. The differen ce value is defined as the difference between the minimum and maximum values of t he spoken sentence. Finally, the relevance features consists of R-LSA and R-VSM, which are the relevance scores of a sentence to the whole document o btained by using the LSA and VSM summarizers [Gong and Liu 2001], respectivel y. However, in a sense they are still derived from the lexical informatio n. Each of the above features is further normalized by the following equation: where  X  m and  X  m are, respectively, the mean and standard deviation of a feature x m estimated from the development set. 6. EXPERIMENTAL RESULTS AND DISCUSSIONS 6.1 Results of Experiments on the Supervised Summarizers In the first set of experiments, we evaluate the BC, SVM, and CR F supervised summarizers discussed in Section 2. The experimental resul ts are detailed in Table IV, where each column lists the ROUGE 2 recall rates for different sum-marizers using different summarization ratios. The result s based on manual transcripts of the spoken documents (denoted as TD, text doc uments) are also shown for reference. For the TD case, the PITCH and ENERGY features were obtained by aligning the manual transcripts to their spoken documents coun-terpart by performing word-level forced alignment, while t he CONFIDENCE feature was set to 1. Unlike the TD case, in which there are no r ecognition errors and sentence boundary errors, the recognition trans cripts used in the SD (spoken documents) case may contain both recognition err ors and sentence boundary errors, which inevitably degrade the performance . In this research, sentence boundaries were determined by speech pauses. It is worth mention-ing that the number of labels used for training a summarizer w as in accordance with the target summarization ratio in the evaluation; that is, the summariz-ers trained with the manual summaries at a given summarizati on ratio were tested at the same summarization ratio. Table IV shows that t he discrimi-native summarizers (CRF and SVM) outperform the generative summarizer (BC). Moreover, the performance of CRF is considerably bett er than that of SVM, especially at lower summarization ratios (10% and 20%) . This may be because the CRF method can model the relationships among sen tences. In brief, the ROUGE 2 recall rates obtained by CRF for the TD case are approx-imately 0.55, 0.65, and 0.64, respectively, for summarizat ion ratios of 10%, 20%, and 30%; while they are 0.35, 0.37, and 0.36, respective ly, for the same summarization ratios in the SD case. Compared with the resul ts in Table II, both SVM and CRF yield results that are comparable to those ob tained by the human subjects for the TD case, except for the case of a ver y low sum-marization ratio (e.g., 10%). However, when SVM and CRF are a pplied to erroneous recognition transcripts of spoken documents (i. e., the SD case), the performance degrades severely. Compared to manual transcr ipts, recognition transcripts are always created in the face of speech recogni tion errors and sen-tence boundary detection errors. It has been shown that spee ch recognition errors are the dominating factor for the performance degrad ation of spoken document summarization when using recognition transcript s instead of man-ual transcripts, whereas erroneous sentence boundaries ca use relatively minor problems [Christensen et al. 2008; Liu and Xie 2008].
 document summarization task studied in this article, we fur ther conducted a set of summarization experiments using CRF and simulated re cognition tran-scripts of different Chinese character error rates (CER), w hich were obtained by randomly replacing a certain percentage of Chinese chara cters in the man-ual transcripts with other Chinese characters or inserting /deleting a certain percentage of Chinese characters into/from the manual tran scripts. As the re-sults show in Figure 1, the performance at all summarization ratios appears to degrade severely as CER increases; for example, when the CER is in the range between 20% and 30%, a performance drop of about 50% is encoun tered for the SD case at all summarization ratios, as compared to the TD cas e. It should be also noted that the performance achieved by using the simula ted recognition transcripts with a CER of 30% is somehow worse than that achie ved by using the  X  X eal X  recognition transcripts with the same CER (cf. Ta ble IV). One possi-ble reason is that the character errors of the real recogniti on transcripts tend to aggregate in some spoken words that are actually confusin g and difficult to recognize, whereas the character errors of the  X  X imulate d X  recognition tran-scripts instead are almost uniformly distributed (or scatt ered) over the words of the manual transcripts, which would lead to a higher word e rror rate and thus have relatively lower ROGUE 2 recall rates for all summarization ratios. Though the summarization methods, together with the associ ated experiments and evaluations, presented in this article are not intended to focus on dealing with the problems caused by speech recognition errors, they still remain wor-thy of further investigation, especially when summarizing spontaneous spoken documents such as voice mails, lectures, and meeting record ings [Koumpis and Renals 2005]. A straightforward remedy, apart from the many approaches im-proving recognition accuracy, might be to develop more robu st representations for spoken documents. For example, multiple recognition hy potheses, beyond the top scoring ones, obtained from N -best lists, word lattices, or confusion networks, can provide alternative (or soft) representatio ns for the confusing portions of the spoken documents [Chelba et al. 2008]. Moreo ver, the use of subword units (for example, syllables or segments of them), as well as the com-bination of words and subword units, for representing the sp oken documents has also been proven beneficial for Chinese spoken document s ummarization [Chen et al. 2006b].
 kinds of features (cf. Table III) make to the performance of a supervised sum-marizer. We, again, take the CRF model as an example because i t achieved the best performance among the three supervised summarizer s studied in this article. The results are shown in Table V, where the first four rows detail the ROUGE 2 recall rates obtained by CRF trained with only one set of fea tures. Interestingly, the acoustic features (Ac) make more substa ntial contributions to the summarizer than the lexical features (Le) and the rele vance features (Re). In the SD case, the performance of the structural featu res (St) is severely degraded probably because of poor sentence boundary detect ion. Since both the POSITION feature and the DURATION feature are closely related to the number of sentences extracted from a spoken document, the wr ong number of sentences given by sentence boundary detection might resul t in poor estima-tion of them. It has also been shown that the structural (or st ylistic) features tend to be affected by story or sentence segmentation inaccu racies; interested readers can also refer to the work of Christensen et al. [2008 ] for more com-prehensive analysis of such an issue. From Table V, we observ e that the com-bination of two kinds of features leads to a more consistent i mprovement than using the features separately. Combining the structural fe atures, acoustic fea-tures, and relevance features further improves the perform ance, but combin-ing the structural features, acoustic features, and lexica l features does not. These results show that lexical cues are not the dominating p redictors when recognition transcripts contain recognition errors. The r esults in Figure 1 also show that the performance degrades severely as the recognit ion error rate in-creases. Therefore, exploring more nonlexical features mi ght be beneficial for spoken document summarization, especially when the speech recognition ac-curacy is not perfect. Similar observations were also made b y other groups [Koumpis and Renals 2005; Maskey and Hirschberg 2005, 2006; Zhang and Fung 2007]. The results in Table V also show that incorporati ng more indica-tive features can improve the performance of the summarizer .
 lexical features (Le), as shown by the results in Table V. Thi s highlights the importance of capturing the relevance of a sentence to the wh ole document. Therefore, to further improve the performance of the CRF sum marizer, we aug-ment the feature sets with two additional generative scores (Ge) obtained from the LM and STM approaches defined in Equations (6) and (7), res pectively. The results in the last row of Table V show that incorporating add itional genera-tive scores improves the performance of the supervised summ arizer slightly but consistently in most cases. These results again justify our postulation that complicated features (e.g., Re or Ge) provide more useful in formation to the su-pervised summarizer than simple features extracted direct ly from the spoken document or sentences (e.g., Le, Ac, and St). 6.2 Results of Experiments on the Unsupervised Summarizers In the fourth set of experiments, we compare the performance of the following unsupervised summarizers: VSM [Gong and Liu 2001; Lee and Ch en 2005], MMR [Murray et al. 2005], LSA [Gong and Liu 2001], and SIG [Fur ui et al. 2004], as well as our previously proposed LM and STM models [C hen et al. 2006a]. The VSM approach represents each sentence of a docum ent and the document itself in vector form, and computes the relevance s core between each sentence and the document (i.e. , the cosine measure of the similarity between two vectors). Then, the sentences with the highest relevanc e scores are in-cluded in the summary [Gong and Liu 2001]. In contrast, LSA re presents each sentence of a document as a vector in the latent semantic spac e of the docu-ment, which is constructed by performing singular value dec omposition (SVD) on the  X  X erm-sentence X  matrix of the document. The right-si ngular vectors with larger singular values represent the dimensions of the more important latent semantic concepts in the document. Therefore, the se ntences with the largest index values in each of the top L right-singular vectors are included in the summary [Gong and Liu 2001]. The difference between VS M and LSA is that VSM performs spoken document summarization based on literal term matching, while LSA is based on concept matching [Lee and Che n 2005]. MMR is close in spirit to VSM because it also represents each sent ence of a document and the document itself in vector form, and then uses the cosi ne measure for sentence selection; however, MMR selects sentences iterat ively. In each round, the important sentence is selected according to two criteri a: 1) whether it is more similar to the whole document than the other sentences, and 2) whether it is less similar to the set of sentences that have been selec ted so far. Con-sequently, MMR not only selects relevant sentences for the s ummary, but also ensures that the summary covers more concepts [Murray et al. 2005]. SIG, on the other hand, simply selects indicative sentences from a s poken document based on a weighted sum of the lexical, grammar, and confidenc e scores for each sentence [Furui et al. 2004].
 a document D and each one of its sentences S i for spoken document sum-marization, we additionally incorporate the unigram proba bilities of a docu-ment word occurring in the sentence P ( w n | S i ) and a general text collection P ( w n | Collection ) into STM, for probability smoothing and better performance . The probability of the document D generated by the STM model of a sentence S [i.e., P STM ( D | S i ) in Equation (7)] is therefore modified as follows: Similar treatments also have been studied for the IR tasks [H offmann 1999; Wei and Croft 2006; Chen 2009]. P ( w n | S i ) and P ( w n | Collection ) actually are the two constituent probability terms for the LM model, as st ated earlier in Section 3.1, which can be estimated simply based on the MLE criterion; the weighting parameters  X  1 and  X  2 can be further optimized using the EM algorithm and the development set.
 The results obtained by random selection (denoted as RND) ar e also listed for comparison. All the unsupervised summarizers were tuned ba sed on exper-iments on the development set. However, the document-refer ence summary information of the development set was not utilized in the co nstruction of the models. Comparing the results with those in Table IV, we obse rve that the supervised summarizers significantly outperform all the un supervised sum-marizers. Although LM and STM do not perform as well as the sup ervised summarizers, they clearly outperform MMR, LSA, and SIG, and their per-formance is comparable to that of VSM at lower summarization ratios. It is interesting that MMR almost has the same performance as VSM a t all sum-marization ratios, despite that MMR is expected to outperfo rm VSM because it is designed to allow the summary to cover more topics. This , in a sense, reflects that the issue of topic redundancy seems to have only a very limited impact on the accuracy of the automatic summarization studi ed here, probably due to the reason that each of the broadcast news documents to be summa-rized is short in its nature and centers on some specific topic or concept [Wang et al. 2005]. Moreover, the summarization results indicate that STM is slightly more effective than LM. It is also worth mentioning that VSM, LSA, and MMR simply use TF-IDF features for the representations of a spok en document and its associated sentences, while only word or topical uni grams (multino-mial distributions) are employed for modeling the sentence generative proba-bility in LM or STM. Finally, the superiority of the supervis ed summarizers over the unsupervised summarizers can be explained by two fa ctors. The first is that the supervised summarizers make use of the handcraft ed document-reference summary information for model training, whereas the unsuper-vised summarizers do not utilize such information. The seco nd is that most of the unsupervised summarizers rely merely on lexical feat ures (e.g., TF-IDF and word or topic unigrams), whereas the supervised summ arizers fuse more indicative features besides the lexical features to fu lfill spoken docu-ment summarization. Nevertheless, almost all kinds of thes e features are more or less vulnerable to speech recognition errors. There fore, the super-vised summarizers show larger performance difference betw een the TD and SD cases than the unsupervised summarizers (except SIG) that m erely use lexical features. 6.3 Training the Supervised Summarizers with the Automatic In the last set of experiments, we take SVM and CRF as examples for studying data selection for unsupervised training of the supervised summarizers. The implementation of training data selection is conducted at t wo levels: the sen-tence level and the document level. For the sentence-level d ata selection, the sentences of each spoken document in the development set lab eled by STM as summary sentences and having the average similarity defined in Equation (8) higher than a threshold  X  s will be marked as reliable summary sentences, while those sentences labeled by STM as nonsummary sentence s and having the average similarity lower than  X  ns will be marked as reliable nonsummary sentences. The document-level data selection is then execu ted according to the ratio of the number of the reliable summary and nonsummar y sentences to the total number of sentences in a spoken document. More sp ecifically, the spoken documents having the ratio of reliable summary an d nonsummary sentences exceeding a threshold  X  D will be ultimately selected for training the supervised summarizers. The number M and thresholds  X  s ,  X  ns , and  X  D were tuned based on experiments on the development set. The summa rization re-sults of SVM and CRF trained in the above data-selection-bas ed unsupervised manner (STM Labeling + Data Selection) are shown in Table VII , where the results of SVM and CRF trained without supervision and data s election (STM Labeling) and trained with supervision (Manual Labeling) a re also listed for comparison. It can be found that the performance of SVM and CR F is substan-tially enhanced at the summarization ratio of 10% as compare d to that of SVM and CRF without using data selection in unsupervised model t raining; how-ever, no apparent performance improvement at higher summar ization ratios (20% and 30%) is obtained. Table VIII presents the average of the average sim-ilarity among the retrieved relevant text documents for the manual summary and nonsummary sentences of the development set at differen t summarization ratios. It is observed that the retrieved relevant text docu ments for a manual summary sentence of a spoken document have a higher similari ty than the retrieved relevant text documents for a manual nonsummary s entence, and the difference becomes smaller as the summarization ratio i ncreases. These observations seem to explain why training data selection ba sed on the average similarity among the retrieved text documents for the spoke n sentences can improve the performance of SVM and CRF at the lower summariza tion ratios when they are trained in an unsupervised manner. 6.4 Discussions The results shown in Tables IV to VIII allow us to draw several conclusions. First, for the SD case, the unsupervised summarizers (excep t SIG) are trained or constructed solely on the basis of erroneous lexical info rmation. In other words, other structural or acoustic clues are not considere d. To be fair, we should compare the results obtained by the unsupervised sum marizers to those of the supervised summarizers (e.g., CRF) trained with lexi cal features only [cf. the third row (Le) in Table V]. From this perspective, th e performance of the unsupervised summarizers is comparable to that of the su pervised sum-marizer (CRF).
 modify and augment the feature set with more indicative feat ures. The sum-marization performance can be improved steadily by includi ng a substantial number of indicative features in the supervised summarizer , as evidenced by the results in Table V.
 bility by using a set of handcrafted document-reference sum mary exemplars; however, such information might not always be available bec ause the summa-rization task changes over time. In contrast, the unsupervi sed summarizers (except SIG) usually consider the relevance of a sentence to the whole docu-ment, which might be more robust across different summariza tion tasks. summary/nonsummary classification tasks, a considerable a mount of human effort is usually required to label the training data. Thoug h we have proposed a data selection approach for unsupervised training of the s upervised summa-rizers, the issue of how to reduce the amount of human effort s till needs further investigation.
 tioned classification-based and probabilistic generative summarizers from sev-eral aspects, such as feature set augmentation and sentence selection criterion. 7. CONCLUSIONS We have studied the use of probabilistic ranking models for e xtractive spoken document summarization, and evaluated various mode ling and learning approaches. The experimental results on Mandarin Chinese broad-cast news show that CRF can achieve significant performance i mprovements compared to other summarizers. In addition, various kinds o f feature repre-sentations and their effectiveness have also been investig ated as well. Our future research directions include the following: 1) utili zing effective feature selection algorithms to select the features automatically , 2) exploiting more representative features for supervised summarizers, 3) ex ploring more elab-orate data selection approaches for training supervised su mmarizers without manual labels, and 4) seeking other ways to combine discrimi native summa-rizers and generative summarizers.

