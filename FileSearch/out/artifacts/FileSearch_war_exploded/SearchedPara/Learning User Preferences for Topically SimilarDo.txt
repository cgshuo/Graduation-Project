 Similarity measures have been used widely in information retrieval research. Most research has been done on query-document or document-document similarity without much attention to the user X  X  perception of similarity in the con-text of the information need. In this study, we collect user preference judgements of web document similarity in order to investigate: (1) the correlation between similarity mea-sures and users X  perception of similarity, (2) the correlation between the web document features plus document-query features and users X  similarity judgements. We analyze the performance of various similarity methods at predicting user preferences, in both unsupervised and supervised settings. We show that a supervised approach using many features is able to predict user preferences close to the level of agree-ment between users, and moreover achieve a 15% improve-ment in AUC over an unsupervised approach.
 H.3.3 [ Information Storage and Retrieval ]: Clustering Keywords: document similarity, similarity measures, users
Measures of similarity between documents are widely used in information retrieval: as scores to rank documents, for clustering, for diversity, and more. Most of these measures are based on simple textual features, primarily term counts, document counts, and document lengths.

Since most uses of such similarity measures are meant to help users with some task, it is worth asking whether they correspond to the notion of similarity that users actually have. The field of IR has always compared query-document similarity measures to human judgements of relevance  X  X his is the foundation of effectiveness evaluation X  X ut there is very little work comparing document-document similarity measures to human opinion.
 others are displayed below it side by side. We use D t to denote the document at the top, D l the document to the left, and D r the document to the right. The participant is asked to choose which of D l or D r is more similar to D t in terms of satisfying the given information need.

Since we have 8 documents per query, there are a total of 168 triplets (= 8  X  7 2 ) covering all possible placement of 3 documents for each query. For any given D t there are 21 possible  X  D l , D r  X  pairs. In order to ensure that each pair would be judged at least once, and that some would be judged twice (so that we could evaluate agreement), we assigned 14 of these to one participant and 14 to another in such a way that guaranteed that all 21 preferences would be judged at least once, and exactly 7 would be judged twice for each D t in each query.
 All participants of the experiment were graduate students. They were paid 8 US dollars per hour of work.

Table 1 shows some statistics of the collected data. All tasks were completed, resulting in 1,680 (= 10  X  8  X  7 2 ) distinct triplets judged and 560 (= 10  X  8  X  7) that were judged twice.
We investigated agreement among participants that worked on the same document triplets. Overall agreement, calcu-lated as the total number of identical preferences over the total number worked on, is about 71% (402/560), which is above previously-reported human agreement about docu-ment relevance [1, 4]. We believe agreement is high because we carefully chose topics and documents of a high quality. Also, it seems that it is easier for participants to judge sim-ilarity relative to a reference document (in this case our top document D t ) than to judge similarity on an absolute scale. Zengin and Carterette reported much lower agreement for the latter case [6].
We will use the data collected from our participants in an experiment to determine the ability of similarity measures and machine-learned classifiers to capture our participants X  notion of similarity. Similarity measures like cosine similar-ity, Jaccard distance, and others have a long history in IR. Recently, Whissell and Clarke proposed that most similarity measures are composed of three components: a term weight-ing method, a normalization technique, and a distance mea-sure [5]. The term weighting method determines the impor-tance of a term occurring in the document. Normalization is used to adjust the term weights in order to normalize the effect of document length. Distance measures quantify the distance between two document vectors.

Table 2, due to Whissell and Clarke, provides various types of term weighting methods. Combining a term weight-
Let Sim(Q, D i , D j ) be the similarity between D i and D j with respect to query Q . We will define a simple binary classifier classify that predicts only a left or right user pref-erence. Given a similarity method sim , the documents in a triplet D t , D l , D r , and the query Q , if the similarity between the top document D t and left document D l is greater than the similarity between the top document D t and right doc-ument D r (i.e. sim ( Q, D t , D l ) &gt; sim ( Q, D t , D r )), then the output of classify ( sim , Q, D t , D l , D r ) is left . Otherwise it is right . Note that no training is necessary: the predicted class is based solely on whether similarity between two documents is greater than the same similarity measure between two other documents. We can then compare these predictions to the actual user preferences obtain as described above.
We first calculated the predictions of classify with each of the similarity measures derived from Whissell and Clarke X  X  framework along with the BM25 similarity methods. We evaluated predictions using classification accuracy, area un-der the ROC curve (AUC), and Pearson correlation between predictions.
Figure 1 summarizes the classification performances of similarity measures by AUC. As the figure shows, AUC is determined primarily by the method used for term weight-ing. Within any given term weighting scheme, there is only a small amount of variation due to distance measure and normalization. Figures for classification accuracy and corre-lation show very similar results, so they have been excluded. This suggests that term weighting plays the greatest role in in classification performance, and specifically that the B, L, I, and F weightings giving the best predictions of user prefer-ence. Results for classification accuracy and correlation are essentially the same as those for AUC, so we have omitted them for space.

Figure 2 shows AUC results for the OK similarity measure with different values of free parameters k and b . The best classification performance is achieved with higher values of b and lower values of k , though increasing b causes a steeper decrease in AUC as k increases. Overall, though, the mea-sure is fairly robust to parameter values. The other BM25-based similarity measure OKTF is not shown here, but it shows a similar pattern with regards to making changes to k and b . However, it is more resistant to performance decrease when increasing k .
The sim function used in our classifier does not have to be a standard similarity measure; as suggested in Section 3.1 it could be a feature of the document or the document/query pair. In this section we use such features to predict user preferences, again evaluating by accuracy, AUC, and corre-lation.

Table 3 and Table 4 summarize the classification perfor-mances of document and query-document features respec-tively. Because we tested a large number of features, we only report a select subset; in particular, when a feature can be computed for different document fields (URL, title, body, or full text), we report only the field that gives maximum performance. Table 4: Pearson correlation, accuracy and AUC of query-document features on document fields. Only document fields with maximum values are reported.  X  represents URL,  X  represents title, ? represents body, and  X  represents full text. tn represents term number, tf represents term frequency, tr represents term ratio, sl represents stream length.
 combinations of features, we use the same idea, with two similarity calculations for each feature in the model.
Table 5 and Figure 3 summarize the performance of mod-els using OK and OKTF as individual features, a model using all document and query/document features, a model using all standard similarity measures as features, and fi-nally a model using all of the above.

The AUC of the OK measure with k = 8 . 2 and b = 0 . 6 is 0.639 in the unsupervised setting (see Fig. 2). Training with OK increases its classification performance by 3.3%. Using all document and query/document features in com-bination gives an AUC of 0.719, 27% higher than any in-dividual feature in that set and 9% higher than OK on its own. Using all standard similarity measures together gives an AUC of 0.739, 12% higher than OK but only 3% higher than the document + query/document features. Using all of the above features together further increases AUC by a small amount, and produces classifiers which have accura-cies close to the agreement between participants reported in Section 2.3 (67.8% vs 71%).

These results suggest that both classes of features do quite a good job of capturing user preferences X  X lmost to the ex-tent that users can predict each others X  preferences.
We have presented an experiment on having users judge similarity between documents by expressing a binary pref-erence for one document being more similar to a reference document than another. We compared those preferences
