 A general form of estimating a quantity w  X  X  n from an empirical measurement set X by minimiz-ing a regularized or penalized functional is where I w ( X )  X  X  m expresses the relationship between w and data X , L ( . ) := R m  X  X  + is a loss function, J ( . ) := R n  X  X  + is a regularization term and  X   X  X  is a weight. Positive integers n, m represent the dimensions of the associated Euclidean space s. Varying in specific applications, the loss function L has lots of forms, and the most often used are these induced (A is induced by B, means B is the core part of A) by squared Euclidean norm or squared Hilbertian norms. Empirically, the functional J is often interpreted as smoothing function, model bias or un certainty. Although Equation (1) has been widely used, it is difficult to establish a general mathematically exact relationship between L and J . This directly encumbers the interpretability of paramete rs in the model selection. It would be desirable if we can represen t Equation (1) by a simpler form Obviously, Equation (2) provides a better interpretabilit y for the regularization term in Equation (1) by explicitly expressing the model bias or uncertainty as a v ariable of the relationship functional. In this paper, we introduce a minimax framework and show that fo r a large family of Euclidean norm induced loss functions, an equivalence relationship betwe en Equation (1) and Equation (2) can be established. Moreover, the model bias or uncertainty will b e expressed as distortions associated with certain functional spaces. We will give a series of coro llaries to show that well-studied lasso, group lasso, local coordinate coding, multiple kernel lear ning, etc., are all special cases of this novel framework. As a result, we shall see that various regulariza tion terms associated with lasso, group lasso, etc., can be interpreted as distortions that belong t o different distortion sets. Within this framework, we further investigate a large famil y of distortion sets which can generate a special type of group level sparsity which we call sparse gr ouping representation (SGR). Instead of merely designing one specific regularization term, we giv e sufficient conditions for the distortion sets to generate the SGR. Under these sufficient conditions, a large set of consistent regularization terms can be designed. Compared with the well-known group la sso which uses group distribution information in a supervised learning setting, the SGR is an u nsupervised one and thus essentially different from the group lasso. In a novel fault-tolerance c lassification application, where there appears class or group label noise, we show that the SGR outpe rforms the group lasso. This is not surprising because the class or group label information is u sed as a core part of the group lasso while the group sparsity produced by the SGR is intrinsic, in that t he SGR does not need the class label information as priors. Finally, we also note that the group l evel sparsity is of great interests due to its wide applications in various supervised learning setti ngs.
 In this paper, we will state our results in a classification se tting. In Section 2 we will review some closely related work, and we will introduce the robust minim ax framework in Section 3. In Section 4, we will define the sparse grouping representation and prov e a set of sufficient conditions for generating group level sparsity. An experimental verificat ion on a low resolution face recognition task will be reported in Section 5. In this paper, we will mainly work with the penalized linear r egression problem and we shall review some closely related work here. For penalized linear regres sion, several well-studied regularization procedures are ridge regression or Tikhonov regularizatio n [15], bridge regression [10], lasso [19] and subset selection [5], fused lasso [20], elastic net [27] , group lasso [25], multiple kernel learning [3, 2], local coordinate coding [24], etc. The lasso has at le ast three prominent features to make itself a principled tool among all of these procedures: continuous shrinkage and automatic variable selec-tion at the same time, computational tractability (can be so lved by linear programming methods) as well as inducing sparsity. Recent results show that lasso ca n recover the solution of l 0 regularization under certain regularity conditions [8, 6, 7]. Recent advan ces such as fused lasso [20], elastic net [27], group lasso [25] and local coordinate coding [24] are m otivated by lasso [19]. Two concepts closely related to our work are the elastic net o r grouping effect observed by [27] and the group lasso [25]. The elastic net model hybridizes lasso and ridge regression to preserve some redundancy for the variable selection, and it can be viewed a s a stabilized version of lasso [27] and hence it is still biased. The group lasso can produce group le vel sparsity [25, 2] but it requires the group label information as prior. We shall see that in a novel classification application when there appears class label noise [22, 18, 17, 26], the group lasso fa ils. We will discuss the differences of various regularization procedures in a classification sett ing. We will use the basic schema for the sparse representation classification (SRC) algorithm prop osed in [21], and different regularization procedures will be used to replace the lasso in the SRC.
 The proposed framework reveals a fundamental connection be tween robust linear regression and duced a robust model for least square problem with uncertain data and [23] discussed a robust model for lasso, our results allow for using any positive regulari zation functions and a large family of loss functions. In this section, we will start with taking the loss function L as squared Euclidean norm, and we will generalize the results to other loss functions in section 3. 4. 3.1 Notations and Problem Statement In a general M ( M &gt; 1 )-classes classification setting, we are given a training da taset T = the i th observation. A data (observation) matrix is formed as A = [ x 1 , , x n ] of size p  X  n . Given a test example y , the goal is to determine its class label. 3.2 Distortion Models where w = [ w 1 , w 2 , , w n ] T is a vector of combining coefficients; and  X   X  X  p represents a vector of additive zero-mean noise. We assume a Gaussian model v  X  N (0 ,  X  2 I ) for this additive noise, so a least squares estimator can be used to compute the combin ing coefficients.
 The observed training dataset T may have undergone various noise or distortions. We define th e following two classes of distortion models.
 Definition 1: A random matrix  X  A is called bounded example-wise (or attribute) distortion 1 , , n. where  X  is a positive parameter.
 This distortion model assumes that each observation (signa l) is distorted independently from the other observations, and the distortion has a uniformly uppe r bounded energy ( X  X niformity X  refers to the fact that all the examples have the same bound). BED includes attribute noise defined in [22, 26], and some examples of BED include Gaussian noise and sampling noise in face recogniti on. Definition 2: A random matrix  X  A is called bounded coefficient distortion ( BCD ) with bound f , denoted as BCD ( f ) , if ||  X  Aw || 2  X  f ( w ) ,  X  w  X  X  p , where f ( w )  X  R + .
 The above definition allows for any distortion with or withou t inter-observation dependency. For example, we can take f ( w ) =  X  || w || 2 , and Definition 2 with this f ( w ) means that the maximum eigenvalue of  X  A is upper limited by  X  . This can be easily seen as follows. Denote the maximum eigenvalue of  X  A by  X  max ( X  A ) . Then we have which is a standard result from the singular value decomposi tion (SVD) [12]. That is, the condition of ||  X  Aw || 2  X   X  || w || 2 is equivalent to the condition that the maximum eigenvalue o f  X  A is upper bounded by  X  . In fact, BED is a subset of BCD by using triangular inequality and taking special forms of f ( w ) . We will use D := BCD to represent the distortion model.
 Besides the additive residue  X  generated from fitting models, to account for the above disto rtion models, we shall consider multiplicative noise by extendin g Equation (3) as follows: where  X  A  X  X  represents a possible distortion imposed to the observatio ns. 3.3 Fundamental Theorem of Distortion Now with the above refined linear model that incorporates a di stortion model, we estimate the model parameters w by minimizing the variance of Gaussian residues for the wors t distortions within a permissible distortion set D . Thus our robust model is The above minimax estimation will be used in our robust frame work.
 An advantage of this model is that it considers additive nois e as well as multiplicative one within a class of allowable noise models. As the optimal estimation of the model parameter in Equation (5), w  X  , is derived for the worst distortion in D , w  X  will be insensitive to any deviation from the underlying (unknown) noise-free examples, provided the de viation is limited to the tolerance level given by D . The estimate w  X  thus is applicable to any A +  X  A with  X  A  X  X  . In brief, the robustness of our framework is offered by modeling possible multiplicative noise as well as the consequent insensitivity of the estimated parameter to any deviations (within D ) from the noise-free underlying (unknown) data. Moreover, this model can seamle ssly incorporate either example-wise noise or class noise, or both.
 Equation (5) provides a clear interpretation of the robust m odel. In the following, we will give a theorem to show an equivalence relationship between the rob ust minimax model of Equation (5) and a general form of regularized linear regression procedu re.
 Theorem 1. Equation (5) with distortion set D ( f ) is equivalent to the following generalized regu-larized minimization problem: Sketch of the proof: Fix w = w  X  and establish equality between upper bound and lower bound. In the above we have used the triangle inequality of norms. If y  X  Aw  X  6 = 0 , we define u = ( y  X  Aw  X  ) / || y  X  Aw  X  || 2 . Since max that the expression is also valid if y  X  Aw  X  = 0 .
 Theorem 1 gives an equivalence relationship between genera l regularized least squares problems and the robust regression under certain distortions. It sho uld be noted that Equation (6) involves is known that these two coincide up to a change of the regulari zation coefficient so the following lasso, local coordinate coding, etc., can be derived based o n Theorem 1.
 Corollary 1: l 0 regularized regression is equivalent to taking a distortio n set D ( f l 0 ) where 0 ( w ) = t ( w ) w T , t ( w i ) = 1 /w i for w i 6 = 0 , t ( w i ) = 0 for w i = 0 . 1 ( w ) =  X  || w || 1 .
  X  || w || 2 .
 Corollary 4: Elastic net regression [27] ( l 2 + l 1 ) is equivalent to taking a distortion set D ( f e ) where f e ( w ) =  X  1 || w || 1 +  X  2 || w || 2 2 , with  X  1 &gt; 0 ,  X  2 &gt; 0 .
 Corollary 5: Group lasso [25] (grouped l 1 of l 2 ) is equivalent to taking a distortion set D ( f gl 1 ) Corollary 6: Local coordinate coding [24] is equivalent to taking a disto rtion set D ( f lcc ) where Similar results can be derived for multiple kernel learning [3, 2], overlapped group lasso [16], etc. 3.4 Generalization to Other Loss Functions From the proof of Theorem 1, we can see the Euclidean norm used in Theorem 1 can be generalized to other loss functions too. We only require the loss functio n is a proper norm in a normed vector space. Thus, we have the following Theorem for a general form of Equation (1).
 Theorem 2. Given the relationship function I w ( X ) = y  X  Aw and J X  X  + in a normed vector space, if the loss functional L is a norm, then Equation (1) is equivalent to the following mi nimax estimation with a distortion set D ( J ) : 4.1 Definition of SGR We consider a classification application where class noise i s present. The class noise can be viewed as inter-example distortions. The following novel represe ntation is proposed to deal with such dis-tortions.
 Definition 3. Assume all examples are standardized with zero mean and unit variance. Let  X  ij = x x j be the correlation for any two examples x i , x j  X  T . Given a test example y , w  X  X  n is defined as a sparse grouping representation for y , if both of the following two conditions are satisfied, (a) If w i  X   X  and  X  ij &gt;  X  , then | w i  X  w j | X  0 (when  X   X  1 ) for all i and j . (b) If w i &lt;  X  and  X  ij &gt;  X  , then w j  X  0 (when  X   X  1 ) for all i and j .
 Especially,  X  is the sparsity threshold, and  X  is the grouping threshold.
 This definition requires that if two examples are highly corr elated, then the resulted coefficients tend to be identical. Condition (b) produces sparsity by req uiring that these small coefficients will be automatically thresholded to zero. Condition (a) preser ves grouping effects [27] by selecting all these coefficients which are larger than a certain thresh old. In the following we will provide sufficient conditions for the distortion set D ( J ) to produce this group level sparsity. 4.2 Group Level Sparsity As known, D ( l 1 ) or lasso can only select arbitrarily one example from many id entical candidates [27]. This leads to the sensitivity to the class noise as the e xample lasso chooses may be mislabeled. As a consequence, the sparse representation classification (SRC), a lasso based classification schema [21], is not suitable for applications in the presence of cla ss noise. The group lasso can produce When there exists group label noise or class noise, group lass o will fail because it cannot correctly determine the group. Definition 3 says that the SGR is defined b y example correlations and thus it will not be affected by class noise.
 In the general situation where the examples are not identica l but have high within-class correlations, we give the following theorem to show that the grouping is rob ust in terms of data correlation. From now on, for distortion set D ( f ( w )) , we require that f ( w ) = 0 for w = 0 and we use a special form of f ( w ) , which is a sum of components f j ( w ) , Theorem 3. Assume all examples are standardized. Let  X  ij = x T i x j be the correlation for any two examples. For a given test example y , if both f i 6 = 0 and f j 6 = 0 have first order derivatives, we have
Sketch of the proof: By differentiating || y  X  Aw || 2 2 + P f j with respect to w i and w j respectively, i  X  x T j || 2 , we proved the Theorem 3.
 This theorem is different from the Theorem 1 in [27] in the fol lowing aspects: a) we have no re-strictions on the sign of the w i or w j ; b) we use a family of functions which give us more choices to bound the coefficients. As aforementioned, it is not necessa ry for f i to be the same with f j and we and a monotonous function with very small growth rate would b e enough. As an illustrative example, we can choose f i ( w i ) or f j ( w j ) to be a second order function with with a constant  X  . If the two examples are highly correlated and is sufficiently large, then we can conclude that the difference of the coefficients will be clos e to zero.
 The sparsity implies an automatic thresholding ability wit h which all small estimated coefficients will be shrunk to zero, that is, f ( w ) has to be singular at the point w = 0 [9]. Incorporating this requirement with Theorem 3, we can achieve group level spars ity: if some of the group coefficients are small and automatically thresholded to zero, all other c oefficients within this group will be reset to zero too. This correlation based group level sparsity doe s not require any prior information on the distribution of group labels.
 To make a good estimator, there are still two properties we ha ve to consider: continuity and un-biasedness [9]. In short, to avoid instability, we always re quire the resulted estimator for w be a continuous function; and a sufficient condition for unbiase dness is that f  X  ( | w | ) = 0 when | w | is large. Generally, the requirement of stability is not consi stent with that of sparsity. Smoothness determines the stability and singularity at zero measures t he degree of sparsity. As an extreme ex-ample, l 1 can produce sparsity while l 2 does not because l 1 is singular while l 2 is smooth at zero; at the same time, l 2 is more stable than l 1 . More details regarding these conditions can be found in [1, 9]. 4.3 Sufficient Condition for SGR Based on the above discussion, we can readily construct a spa rse grouping representation based on Equation (5) where we only need to specify a distortion set D ( f  X  ( w )) satisfying the following sufficient conditions: Lemma 1: Sufficient condition for SGR. (b). f  X  j is continuous and singular at zero with respect to w j for all j . (c). f  X  j  X  ( | w j | ) = 0 for large | w j | for all j .
 Proof: Together with Theorem 3, it is easy to be verified.
 As we can see, the regularization term  X l 1 +(1  X   X  ) l 2 2 proposed by [27] satisfies the above condition (a) and (b), but it fails to comply with (c). So, it may become b iased for large | w | . Based on these conditions, we can easily construct regularization t erms f  X  to generate the sparse grouping representation. We will call these f  X  as core functions for producing the SGR. As some concrete examples, we can construct a large family of clipped 1 L q + 2 l 2 2 where 0 &lt; q  X  1 by restricting f i = w i I ( | w i | &lt;  X  ) + c for some constant  X  and c . Also, SCAD [9] satisfies all three conditions so it belongs to f  X  . This gives more theoretic justifications for previous empi rical success of using SCAD. 4.4 Generalization Bounds for Presence of Class Noise We will follow the algorithm given in [21] and merely replace the lasso with the SGR or group lasso. After estimating the (minimax) optimal combining coefficie nt vector w  X  by the SGR or group lasso, we may calculate the distance from the new test data y to the projected point in the subspace spanned by class C i : 1( ) is an indicator function; and similarly A | C A decision rule may be obtained by choosing the class with the minimum distance: Based on these notations, we now have the following generali zation bounds for the SGR in the presence of class noise in the training data.
 Theorem 4. All examples are standardized to be zero mean and unit varian ce. For an arbitrary C k 6 = C i . We assume w is a sparse grouping representation for any test example y and  X  ij &gt;  X  (  X  || y  X  Aw | C y , where where w 0 is a constant and the confidence threshold is defined as  X  = d i ( A | C i )  X  d i ( A | C k ) . Sketch of the proof: Assume y is in class C i . The correctly labeled (mislabeled, respectively) subset denote Aw | C 1 For each k  X  C 1 i , we differentiate with respect to w k and do the same procedure as in proof of Theorem 3. Then summarizing all equalities for C 1 i and repeating the same procedure for each i  X  C 2 i . Finally we subtract the summation of C 2 i from the summation of C 1 i . Use the conditions that w is a sparse grouping representation and  X  ij &gt;  X  , combing Definition 3, so all w k in class C i should be the same as a constant w 0 while others  X  0 . By taking the l 2 -norm for both sides, we This theorem gives an upper bound for the fault-tolerance ag ainst class noise. By this theorem, we can see that the class noise must be smaller than a certain val ue to guarantee a given fault correction confidence level  X  . In this section, we compare several methods on a challenging low-resolution face recognition task (multi-class classification) in the presence of class noise . We use the Yale database [4] which consists of 165 gray scale images of 15 individuals (each person is a cl ass). There are 11 images per subject, one per different facial expression or configuration: cente r-light, w/glasses, happy, left-light, w/no glasses, normal, right-light, sad, sleepy, surprised, and wink. Starting from the orignal 64  X  64 images, all images are down-sampled to have a dimension of 49 . A training/test data set is generated by uniformly selecting 8 images per individual to form the tr aining set, and the rest of the database is used as the test set; repeating this procedure to generate five random split copies of training/test data sets. Five class noise levels are tested. Class noise le vel= p means there are p percent of labels (uniformly drawn from all labels of each class) mislabeled f or each class.
 For SVM, we use the standard implementation of multiple-cla ss (one-vs-all) LibSVM in Mat-labArsenal 1 . For lasso based SRC, we use the CVX software [13, 14] to solve the corresponding convex optimization problems. The group lasso based classi fier is implemented in the same way as the SRC. We use a clipped  X l 1 + (1  X   X  ) l 2 as an illustrative example of the SGR, and the corre-sponding classifier is denoted as SGRC. For lasso, group Lass o and the SGR based classifier, we Figure 1 (b) shows the parameter range of  X  that is appropriate for lasso, group lasso and the SGR based classifier. Figure 1 (a) shows that the SGR based classi fier is more robust than lasso or group lasso based classifier in terms of class noise. These results verify that in a novel application when there exists class noise in the training data, the SGR is more suitable than group lasso for generating group level sparsity. Towards a better understanding of various regularized proc edures in robust linear regression, we introduce a robust minimax framework which considers both a dditive and multiplicative noise or distortions. Within this unified framework, various regula rization terms correspond to different Figure 1: (a) Comparison of SVM, SRC (lasso), SGRC and Group l asso based classifiers on the low resolution Yale face database. At each level of class noise, the error rate is averaged over five copies level are plotted. (b) Illustration of the paths for SRC (las so), SGRC and group lasso.  X  is the weight for regularization term. All data points are averaged over fi ve copies with the same class noise level of 0.2. distortions to the original data matrix. We further investi gate a novel sparse grouping representation (SGR) and prove sufficient conditions for generating such gr oup level sparsity. We also provide a generalization bound for the SGR. In a novel classification a pplication when there exists class noise in the training example, we show that the SGR is more robust th an group lasso. The SCAD and clipped elastic net are special instances of the SGR.
 [1] A. Antoniadis and J. Fan. Regularitation of wavelets app roximations. J. the American Statis-[2] F. Bach. Consistency of the group lasso and multiple kern el learning. Journal of Machine [3] F. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple ker nel learning, conic duality, and [4] P. N. Bellhumer, J. Hespanha, and D. Kriegman. Eigenface s vs. fisherfaces: Recognition using [5] L. Breiman. Heuristics of instability and stabilizatio n in model selection. Ann. Statist. , [6] E. Cand  X  es, J. Romberg, and T. Tao. Stable signal recovery from incom plete and inaccurate [7] E. Cand  X  es and T. Tao. Near-optimal signal recovery from random proj ections: Universal en-[8] D. Donoho. For most large underdetermined systems of lin ear equations the minimum l1 nom [9] J. Fan and R. Li. Variable selection via nonconcave penal ized likelihood and its oracle proper-[10] I. Frank and J. Friedman. A statistical view of some chem ometrics regression tools. Techno-[11] L. El Ghaoui and H. Lebret. Robust solutions to least-sq uares problems with uncertain data. [12] G.H. Golub and C.F. Van Loan. Matrix computations . Johns Hopkins Univ Pr, 1996. [13] M. Grant and S. Boyd. Graph implementations for nonsmoo th convex programs, recent ad-[14] M. Grant and S. Boyd. UCI machine learning repositorycv x: Matlab software for disciplined [15] A. Hoerl and R. Kennard. Ridge regression. Encyclpedia of Statistical Science , 8:129 X 136, [16] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In Pro-[17] J. Maletic and A. Marcus. Data cleansing: Beyond integr ity analysis. In Proceedings of the [18] K. Orr. Data quality and systems theory. Communications of the ACM , 41(2):66 X 71, 1998. [19] R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B , 58:267 X  [20] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Kni ght. Sparsity and smoothness via the [21] J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, and Y. Ma. Ro bust face recognition via sparse [22] X. Wu. Knowledge Acquisition from Databases . Ablex Pulishing Corp, Greenwich, CT, USA, [23] H. Xu, C. Caramanis, and S. Mannor. Robust regression an d lasso. In NIPS , 2008. [24] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using lo cal coordinate coding. In Advances [25] M. Yuan and Y. Lin. Model selection and estimation in reg ression with grouped variables. [26] X. Zhu, X. Wu, and S. Chen. Eliminating class noise in lar ge datasets. In Proceedings of the [27] H. Zou and T. Hastie. Regularization and variable selec tion via the elastic net. J. R. Statist.
