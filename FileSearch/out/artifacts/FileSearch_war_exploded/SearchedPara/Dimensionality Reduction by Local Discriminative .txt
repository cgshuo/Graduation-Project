 Nathan Parrish nparrish@u.washington.edu Maya R. Gupta gupta@ee.washington.edu Dimensionality reduction is the mapping of high-dimensional data into a lower-dimensional space while retaining as much of the information content of the data as possible. As a preprocessing step for super-vised classification algorithms, dimensionality reduc-tion achieves several important goals. It reduces the storage requirements and algorithm complexity by re-ducing the input space of the data. It can improve per-formance of learning algorithms by rejecting spurious or noisy features prior to training and testing. Dimen-sionality reduction can also protect against overfitting by reducing the number of parameters learned by the classifier.
 We present a method for supervised dimensionality re-duction that is based on a local discriminative Gaus-sian (LDG) criterion. The discriminative Gaussian cri-terion is a smooth approximation to the leave-one-out cross-validation error of a quadratic discriminant anal-ysis (QDA) classifier, so it seeks a mapping where a quadratic boundary separates the classes. Because this goal of separation by class may be difficult to achieve globally, our criterion instead operates locally to each training point.
 The considered objective function is non-convex with no analytical solution; however, we present an approxi-mation that is solved via a maximal eigenvalue decom-position. The simplicity of the solution is an advan-tage over other state-of-the-art dimensionality reduc-tion techniques that require iterative solution meth-ods or more complex generalized eigenvalue decompo-sitions.
 We perform experiments for supervised dimensionality reduction, and for dimensionality reduction for trans-fer learning. We show that on datasets with a large number of feature dimensions, other state-of-the-art algorithms are either intractably slow or exhibit nu-merical instability, whereas LDG is able to extract a useful mapping even when the number of features in the original data is in the thousands. We also show that LDG can be easily extended to the transfer learn-ing setting, where the training data is drawn from a different distribution than the test data. Experiments show that LDG is effective in this setting as well. We take as given a set of labeled training data the i th feature vector and class label respectively. We wish to find a matrix B  X  R d  X  l ,l &lt; d such that the reduced-dimensionality feature vectors { B T x i } can be separated according to class. We measure this separa-bility by the performance of a generative classifier. Let p ( x i | y i ) be the likelihood of x i given class y i , estimated from the other n  X  1 training sample pairs. Then the leave-one-out cross-validation error of a maximum a-posteriori (MAP) classifier acting on the mapped fea-tures measures the separation achieved by B :
X where the indicator function I(  X  ) is one if its argument is true and zero otherwise.
 The discontinuity of the indicator function in (1) makes it difficult to minimize. In order to arrive at a smooth, differentiable objective function that approx-imates (1), we substitute a log for the indicator and a sum for the max: In related work, p ( x i | j ) was assumed to be a Gaus-sian mixture model (GMM), and the objective was to learn a parameter vector,  X , of GMM weights, means, and variances that minimized f ( X ) with p ( x i | j,  X ) re-placing p ( B T x i | j ) in (2) (Ma &amp; Chang, 2003). The learned parameters were shown to improve the GMM classification performance over the parameters learned by maximimum likelihood estimation. In that work, (2) is motivated as maximizing the mutual informa-tion between the class labels and the feature vectors. We assume p ( x i | j ) is Gaussian, N ( x i ;  X  i,j ,  X  i,j ever, to reduce the model bias of assuming one Gaus-sian per class, we model p ( x i | j ) as locally Gaussian (Garcia et al., 2010). That is, we estimate the param-eters of the Gaussian for point x i and class j by finding the k nearest class j neighbors, in Euclidean distance, to training point x i and using these points to estimate the Gaussian X  X  maximum likelihood mean and covari-ance. To reduce estimation variance, we model each covariance matrix as a scaled identity  X  i,j =  X  2 i,j where I is the properly sized identity matrix. There-fore, p ( B T x i | j ) = N ( B T x i ; B T  X  i,j ,B T B X  Objective (2) is non-convex with no analytical solu-tion. Gradient-descent or global optimization can be used, but become computationally expensive if the number of classes, training samples, or dimensionality are large. Therefore, we propose a tractable approx-imation that has an analytical solution. The B that minimizes our approximation can be used directly (as we do in our experiments), or as a starting point for a gradient descent approach to minimizing (2).
 We rewrite (2) as and bound (2) from below with Jensen X  X  inequal-ity by replacing the first log term in (3) with P j =1 p ( j ) log( p ( B straint that B T B = I . This constraint simplifies (2) by making the covariance of the Gaussians in the mapped space independent of B . Furthermore, it makes for a unique solution. After taking the log of the Gaussians, we arrive at the LDG objective: B  X  = arg min Despite the approximations, (4) retains an intuitive meaning. The B that minimizes the first term in (4) is the maximum likelihood solution for the correct-class local Gaussians. The second term is composed of m  X  1 different terms, each of which, if minimized individu-ally, will give the maximum likelihood solution for an incorrect-class Gaussian, i.e. a Gaussian distribution trained by the local neighbors of x i coming from a different class. Therefore, (4) can be viewed as a reg-ularized maximum likelihood estimate, where the reg-ularization term attempts to minimize the likelihood of incorrect classes. 2.1. LDG Solution The B that optimizes (4) can be found with one eigen-decomposition. Define Then (4) can be written as Since both V and A are real symmetric matrices, it is straightforward to show that the solution to (5) is to set B  X   X  X  columns to be the l smallest eigenvectors of matrix ( V  X  A ).
 Additionally, we add a cross-validated regularization parameter,  X  , to (4), which we have found in practice can produce a mapping that better separates the data: The solution to (6) is to set B  X   X   X  X  columns to be the l smallest eigenvectors of matrix ( V  X   X A ). Fisher discriminant analysis (FDA) (Fisher, 1936) is a supervised technique that chooses B to maxi-mize the ratio of the between-class covariance S ( b ) to the within-class covariance S ( w ) . The solution is to choose the top eigenvectors of the generalized eigen-decomposition S ( b )  X  =  X S ( w )  X  . FDA has two draw-backs. First, FDA can perform poorly on multi-modal data where no single linear boundary separates the data by class. Second, the between-class covariance matrix is at most rank m  X  1, so FDA can provide at most m  X  1 dimensions.
 Local Fisher discriminant analysis (LFDA) (Sugiyama, 2007) alleviates the drawbacks of FDA. LFDA gen-eralizes FDA by adding a weight based on pairwise sample distances to the between-class and within-class covariance matrices. Thus, LFDA is able to sepa-rate multi-modal data. This change also results in LFDA being able to provide greater than m  X  1 di-mensions. LFDA is solved using the same generalized eigen-decomposition as FDA.
 Neighbourhood components analysis (NCA) (Glober-son et al., 2005) is a dimensionality reduction tech-nique that is based on a smooth approximation to the leave-one-out k-NN error. The dimensionality reduc-tion found by NCA was shown to provide good clas-sification accuracy; however, it suffers from two key drawbacks. First, the optimization requires gradient descent, and can be slow for datasets with a large num-ber of features or training examples. Second, the NCA optimization must be re-run for any desired number of final dimensions. This is in contrast to principal components analysis (PCA), FDA, LFDA, and LDG, where B can be found once for the largest number of final dimensions, and then the top submatrices of B are the optimal solution for fewer dimensions. Finally, there is a large body of work in distance metric learning and feature selection that is related to linear dimensionality reduction. Distance metric learning addresses the problem of how best to de-termine the distance between feature vectors in R d Linear distance metric learning is primarily concerned with finding a positive semi-definite Mahalanobis met-ric M that gives the distance between x i and x ` as p ( x i  X  x ` ) T M ( x i  X  x ` ). Linear dimensionality reduc-tion can be thought of as finding a low rank Maha-lanobis metric M , such that M = BB T . The ap-proaches given in (Globerson &amp; Roweis, 2006; Davis et al., 2007; Weinberger &amp; Saul, 2009) propose convex optimization problems for finding M . These meth-ods suffer from the drawback that rank constraints are non-convex, and thus the M that they find is typically not low rank. However, we can perform dimensional-ity reduction by rewriting the Mahalanobis metric as M = L  X  L T and using a feature selection method on the resulting z i =  X  1 / 2 L T x i as proposed in (Globerson &amp; Roweis, 2006; Davis et al., 2007). We perform experiments to compare LDG to sev-eral different dimensionality reduction methods: PCA, FDA, LFDA, NCA, and information theoretic met-ric learning (ITML) (Davis et al., 2007) with feature selection using the maximum-relevance, minimum re-dundancy criterion (MRMR) (Peng et al., 2005). For the NCA, LFDA, ITML, and MRMR feature selection, we use code provided by the authors. We evaluate the performance of the dimensionality reduction methods via k-NN classification accuracy with k = 3, as was done in (Weinberger &amp; Saul, 2009).
 As a preprocessing step, we standard normalize the training data so that each feature has a mean of zero and standard deviation of one. We choose the num-ber of neighbors used to estimate the local Gaus-sians for the LDG dimensionality reduction by five-fold cross-validation using a local QDA classifier (Gar-cia et al., 2010) on the original data. For LDG,  X   X  { . 2 ,. 4 ,. 6 ,. 8 , 1 } , and we choose whichever  X  min-Image Seg 19 1617 96.5 4 92.5 2 96.2 1 95.0 3 95.6 138 95.4 4641 Ringnorm 20 3000 86.9 9 85.8 5 71.9 1 85.8 6 80.6 23 85.7 9119 Statlog 36 3000 90.1 10 90.1 5 86.6 3 88.3 6 88.0 232 --USPS 256 3000 93.5 24 92.0 10 90.9 5 92.6 7 90.8 4886 --Isolet 617 3000 87.9 94 73.1 15 88.9 18 90.2 21 ----MNIST 784 3000 89.1 55 87.5 13 76.2 10 32.7 34 ----Gisette 5000 3000 95.5 466 96.7 63 51.5 1983 49.8 2313 ----Mutants 5408 200 90.0 2908 64.8 2 52.0 1946 48.0 2003 ----Arcene 10K 70 76.0 578 61.3 15 53.7 39 nc nc ----
Dexter 20K 210 84.0 4365 58.1 2 nc nc nc nc ----imizes the k-NN leave-one-out cross-validation error at dimensionality equal to the number of classes plus five. We have found that, in general, a few more di-mensions than the number of classes present in the data is a good dimensionality at which to choose  X  . In the case of ties, we select the largest value of  X  . MRMR requires that we discretize the ITML features for feature selection, and we do so by thresholding at the mean, as recommended in the authors X  code. We perform experiments on fifteen datasets, and for each we average the accuracy over ten random 70/30 splits of the training and test data (up to a maxi-mum of 3000 training samples). The datasets that we use can be found either at the UCI Machine Learning Repository or the Machine Learning Dataset Reposi-tory. The P53-Mutants dataset contained a large de-gree of class asymmetry. Therefore, we randomly sam-pled 143 of the inactive class samples and discarded the rest in order to make a 50/50 split between inac-tive and active class data (as opposed to the 1% vs 99% split in the original dataset).
 Figure 1 and Table 1 show that for small datasets, LDG is comparable to other state-of-the-art meth-ods. However, LDG provides a clear advantage on the datasets with the largest feature dimensionality. NCA and ITML failed to converge in under three hours per training/test split on a standard 2.8 GHz PC for the datasets marked with  X - X  in Table 1, and results for these datasets are not plotted in Figure 1. Figure 1 also shows that ITML has difficulty with the Ringnorm dataset which has some features that are only noise. LDG also outperforms FDA and LFDA on some of the datasets. FDA can provide dimensionality only up to one fewer than the number of classes, which lim-its its performance on the Ionoshpere and Ringnorm datasets. Furthermore, FDA and LFDA exhibit nu-merical instability in some of the datasets with large feature dimensionality due to the fact that the within-class covariance matrix is underdetermined. Thus, the generalized eigenvalue decomposition that these algo-rithms solve fails to find discriminative dimensions. LFDA returns complex eigenvalues for the Arcene and Dexter datasets, and FDA does the same on the Dex-ter dataset; thus, the LFDA and FDA results are not computable for these datasets.
 In Table 1, we show the average classification accuracy when the dimensionality is chosen by leave-one-out cross-validation. We do this by increasing the dimen-sionality until the cross-validation accuracy decreases by adding another dimension. The run-time numbers measure the mean time it takes, in seconds, for the method to produce the dimensions shown in Figure 1 and to select the best dimensionality. In this section, we apply LDG dimensionality reduc-tion to transfer learning. In transfer learning, we wish to classify test data drawn from some unknown target domain distribution of feature vectors and class labels where we have very few training examples. However, we assume that we have plenty of training examples from a source domain that differs from the target do-main, but is thought to be useful for learning. For example, in our experiments we treat resized MNIST handwritten digits as the source and USPS handwrit-ten digits as the target (see Figure 4).
 Let T = { ( x i ,y i ) } n t i =1 be the target domain training data drawn iid from some unknown joint distribution p main training data drawn iid from unknown joint dis-tribution p S ( x,y ) 6 = p T ( x,y ), with n s &gt;&gt; n The goal in transfer learning is to achieve high clas-sification accuracy in the target domain by training a classifier using both sets of training data. There-fore, one of the goals for dimensionality reduction is to find a B matrix such that the target domain data are separated according to class. However, we now have the added goal that we wish to find a mapping where the source and target domain distributions are similar, i.e. p T ( B T x,y )  X  p S ( B T x,y ). In Figure 2 we show examples of two different one-dimensional spaces that have been mapped from some higher-dimensional space (which is not shown). The left plot is a map-ping in which the target domain data are separated according to class, but the source and target domain distributions are not similar. In the right plot, the tar-get domain data are separated, and additionally, the source domain data distribution is similar to that of the target domain data. Therefore, both the source and target domain data can be used to train a classi-fier for the test data using the right-side mapping. For transfer learning, we weight objective (2) for the target and source domain training data using param-eter  X  , f ( B ) = (1  X   X  ) We estimate the parameters of the Gaussian distribu-tion for target domain point x i using the k nearest source domain training examples. The first term in (7) is the primary transfer term. The denominator in this term finds a B that maximizes the likelihood of the target domain data for a Gaussian distribution trained using the local source domain data, thus find-ing a B that brings the same-class source and target domain data close together. Conversely, the numer-ator in the first term seeks a B that minimizes the likelihood of the different-class source Gaussians, thus ensuring that the mapping is still discriminative. The second term in (7) is the normal LDG objective function for the source domain data only. Thus, if  X  = 0 . 5, (7) is very similar to standard LDG dimen-sionality reduction acting on the pooled source and target domain data. We include this term because if the source and target domain distributions are simi-lar, then we can set  X  = 0 . 5 to train B using as much data as possible. We choose  X  by cross-validating over the target domain training data. In case of ties, we choose the largest value of  X  , thereby defaulting to using as much data as possible. We make the same approximations as in Section 2 to find an analogous approximation to (6) for (7). Of the dimensionality reduction techniques described in Section 3, only ITML has been adapted to the transfer learning scenario (Saenko et al., 2010). Let d
M ( x i ,x ` ) = ( x i  X  x ` ) T M ( x i  X  x ` ), the squared Ma-halanobis distance. The original ITML objective is: For transfer metric learning, the authors propose to use objective function (8), but generate constraints only between examples from different domains, i.e. x i  X  T  X  i and x `  X  S  X  ` . In this way, they find an M that makes distances between examples across the two domains small for same-class data and large for different-class data. We again use MRMR feature se-lection to find a dimensionality reduction matrix from M as described in Section 3. We conduct transfer learning experiments for two dif-ferent classification problems. The first is to classify images according to the category of the object found in the images, a thirty class problem, with datasets from three domains: Amazon product images, images taken with a high-resolution DSLR camera, and im-ages taken with a low-resolution webcam. Examples of the back packs category for these three domains is shown in Figure 3. This dataset was first used by Saenko, et al., and we use the same preprocessing tech-niques as described in (Saenko et al., 2010) to featurize the images, which results in 800 features per image. In the second problem, the two different domains con-sist of the grayscale digit images in the MNIST and USPS datasets. The image features are the raw pixel values, and the only preprocessing we use is to resize the MNIST images to 16 x 16 pixels to match the USPS images. We show examples of images from each domain in Figure 4.
 We compare four dimensionality reduction techniques by their performance using k = 3 nearest-neighbor classification. The first is transfer LDG where we choose  X  from [0 ,. 1 ,. 3 ,. 5] by k-NN cross-validation at dimensionality equal to the number of classes plus five. We compare to pooled PCA and pooled FDA dimen-sionality reduction. These approaches ignore the dif-ference between the domains by pooling the training data from each domain and then performing standard PCA or FDA. Finally, we compare to linear ITML for transfer learning as described in (Saenko et al., 2010) with MRMR feature selection. We also show results for ITML with no dimensionality reduction.
 The source domain training data consists of all the available data in that particular domain, and we stan-dard normalize so that it has mean zero and standard deviation one. The target domain training data con-sists of exactly two examples per class and is standard normalized independently of the source domain train-ing data. We remove any features that exhibit zero variance in the source or target domain.
 Figure 5 plots the accuracy averaged over ten random splits of the target domain test and training data. The results show that LDG is statistically the best or tied for the best at many dimensions in all experiments. Pooled PCA performs well in the datasets where Ama-zon images act as the source, but fails to perform as well as LDG on the other datasets. Pooled FDA per-forms poorly on all of the datasets.
 We do not show results for the best dimensionality chosen by cross-validation, similar to those in Table 1, due to space constraints. We do note that for the dimensions we have plotted, ITML achieves its highest accuracy with no dimensionality reduction, but still does not match the best performance of LDG. We have presented LDG dimensionality reduction, a technique that maps the data to a space where classes are separated locally to each training point. LDG is solved via a simple maximal eigenvalue decompo-sition, and thus scales better than iterative methods and LFDA for large datasets. Furthermore, we have shown that LDG dimensionality reduction can be ap-plied to transfer learning problems with good results. Davis, J. V., Kulis, B., Jain, P., Sra, S., and Dhillon, I. S. Information-theoretic metric learning. In Proc.
International Conference on Machine Learning , pp. 209 X 216, 2007.
 Fisher, R. A. The use of multiple measurements in taxonomic problems. Annals of Human Genetics , 7 (2):179 X 188, 1936.
 Garcia, E. K., Feldman, S., Gupta, M. R., and Srivastava, S. Completely lazy learning. IEEE
Trans. Knowledge and Data Engineering , 22(9):1274  X  1285, 2010.
 Globerson, A. and Roweis, S. Metric learning by col-lapsing classes. In Proc. Advances in Neural Infor-mation Processing Systems 18 , pp. 451 X 458. 2006. Globerson, A., Roweis, S. T., Hinton, G., and
Salakhutdinov, R. Neighbourhood components analysis. In Proc. Advances in Neural Information Processing Systems 17 , pp. 513 X 520. 2005.
 Ma, C. and Chang, E. Comparison of discriminative training methods for speaker verification. In Proc.
IEEE ICASSP 2003 , volume 1, pp. I X 192  X  I X 195 vol.1, Apr. 2003.
 Peng, H., Long, F, and Ding, C. Feature selec-tion based on mutual information criteria of max-dependency, max-relevance, and min-redundancy.
IEEE Trans. Pattern Analysis and Machine Intel-ligence , 27(8):1226  X 1238, Aug. 2005.
 Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In
Proc. Computer Vision ECCV 2010 , volume 6314 of Lecture Notes in Computer Science , pp. 213 X 226. 2010.
 Sugiyama, M. Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis. J. Mach. Learn. Res. , 8:1027 X 1061, May 2007.
 Weinberger, K. Q. and Saul, L. K. Distance metric learning for large margin nearest neighbor classifica-
