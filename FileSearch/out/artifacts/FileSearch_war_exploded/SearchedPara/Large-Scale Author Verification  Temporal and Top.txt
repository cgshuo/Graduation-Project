 The task of author verification is concerned with the ques-tion whether or not someone is the author of a given piece of text. Algorithms that extract writing style features from texts are used to determine how close in style different doc-uments are. Currently, evaluations of author verification algorithms are restricted to small-scale corpora with usually less than one hundred test cases. In this work, we present a methodology to derive a large-scale author verification cor-pus based on Wikipedia Talkpages . We create a corpus based on English Wikipedia which is significantly larger than ex-isting corpora. We investigate two dimensions on this corpus which so far have not received sufficient attention: the influ-ence of topic and the influence of time on author verification accuracy.
 Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms : Experimentation Keywords: authorship verification, plagiarism detection
Plagiarism detection is a field of research whose impor-tance in educational and academic contexts is well-known [5, 12]. Moreover, plagiarism detection has applications in his-torical and forensic research [3, 15]. Here, it is often neces-sary to determine the true author of encountered documents. The plagiarism sub-field of author verification , which we fo-cus on in this work, frames the question of authorship as a classification task: Given a piece of text T , an author A and one or more reference documents known to have been written by A , determine whether A has written T .

Authors unconsciously leave clues in their written text, that can identify them as the author. These clues are called style markers [4]. There are many different possibilities for style markers, e.g. the average number of words in a sen-tence, the frequency of words ending in ing (for English) and the fraction of times a conjunction is followed by a clarifying statement.

To accurately capture an author X  X  writing style it is im-portant to determine which style markers truly capture the style and which are influenced by the topic of the text or the context. Character N-grams are the most common approach to capture style markers. Not only are they language inde-pendent and inexpensive to compute and store, past research has also shown them to be a powerful tool to recognize and verify authors.

A common benchmark for the author verification task is provided by the PAN benchmark series, which started in 2009 and is still ongoing [16, 8]. In 2013 [9], for the first time, multiple languages were evaluated (English, Greek and Spanish) and for each language up to 30 verification problems were provided. For each verification problem, the document to verify, and the author with up to eight short reference documents was provided. Decreasing the amount and size of reference documents has been a trend across the years -while initially, whole books were used as reference documents, currently short extracts of books are used. The documents within one verification problem are matched for genre, topic and date of writing.

While this benchmark allows us to compare several algo-rithms, it is at the same time exceedingly small. This makes it impossible to investigate to what extent currently used style markers are influenced by additional dimensions such as topic and time. In this paper, we address this shortcom-ing and propose a methodology based on Wikipedia Talk-pages (WTP), to automatically gather a large-scale corpus for author verification, which is controlled for topical and temporal effects.

Having created such a corpus from the English Wikipedia 1 we investigate the following research hypotheses: Topic hypothesis When using short and diverse sets of
The corpus is available at http://www.st.ewi.tudelft. nl/~hauff/wikiAuthorVerificationData.html . Temporal hypothesis It is unrealistic to assume that an
Our work makes the following contributions: (i) we in-troduce a corpus creation pipeline based on WTPs which can generate corpora with thousands of test cases in many different languages; (ii) we challenge the topic assumption and show that topic alignment improves the accuracy both for the positive and negative class, and (iii) we experimen-tally show that the writing style of authors changes over time, making two documents from the same author, written a long time apart, more prone to a false classification.
After a short overview of related work, we introduce our corpus creation methodology, then briefly describe the char-acter N-gram based author verification approach we imple-mented. Finally we report the results when investigating the influences of topic and time on the verification accuracy.
Different categories of stylistic markers (or stylometric fea-tures) have been identified in the past as useful to determine the authorship of a document: lexical (e.g. sentence length, number of unique terms), character (e.g. character N-grams or character types), syntactic (e.g. part-of-speech features), semantic (semantic relationship between words or phrases, e.g. clarification or conjunction) or application specific (e.g. greetings in e-mail messages) [20].

A lot of research has focused on the applicability of differ-ent style markers. In [6], 39 text features were compared. It was concluded that word and punctuation mark profiles are strong discriminators between authors, next to character 2-and 3-grams. Stamatatos [20] provides a detailed overview of character attribution methods currently employed, and dis-cusses the usability of different stylistic marker categories. He concludes that character N-grams belong to the most ef-fective features out of all lexical and character style features, while having only minimal computational requirements.
Within the character N-grams technique the most stylisti-cally important features are those N-grams that occur most frequently in a text, indicating an author X  X  preference for specific word endings or conjugations. Stamatatos [19] states that character N-grams are able to capture complicated stylis-tic information on the lexical, syntactic, or structural level and provides examples of lexical ( X  X he X ,  X  to X ,  X  X ha X ), word-class ( X  X ng X ,  X  X d  X ), and punctuation usage ( X . T X ,  X   X  X  X ) in-formation in character 3-grams. In [18], character 4-grams were found to be most effective for author attribution.
Ruseti et al. [17] use the one hundred most common char-acter trigrams to distinguish authors, next to a limited set of additional lexical features, achieving more than 95% accu-racy on the plagiarism and author unmasking task at PAN 2011. Both works, [17, 19], assume that the most frequent N-grams will consist of stylistic markers, rather than N-grams specific to a topic as writing style is assumed to be topic-independent.

Employing machine learning frameworks to automatically select the best features has been proposed, among others in [1] and [11].

Finally recent author verification attempts are also sum-marized in the overviews of the various PAN tasks [16], [8], and [9]. For the author verification task it was found that word-based features are useful in distinguishing between au-thors writing about different subjects. Moreover, most high-scoring submissions used character n-gram features, often combined with other style features.

As stated earlier, the currently used benchmark corpora are rather small with respect to the number of test cases, making investigations into particular dimensions of the prob-lem (such as time or topic) infeasible. At the same time, older corpora contain very long reference documents (entire books), which are not available in many use cases. Recently, Mikros and Perifanos [13] developed an authorship attribu-tion corpus based on tweets. We consider a corpus based on tweets not very suitable for most use cases of author verifi-cation as the writing style on Twitter is heavily biased due to the very strong limitations on allowed text. Finally, we also note the use of the Reuters RCV1&amp;2 corpora (newswire texts) for authorship attribution experiments [7]. These cor-pora however are not controlled for multi-author influences or quotations (as we do). Furthermore, the writing styles of news-wire stories are usually geared towards a particular one as required by the news organization. In comparison, we consider our corpus more in line with use cases that deal with digital forensics.
As previously stated, the trend in authorship research is to move towards evaluating less and shorter texts rather than entire books, and to evaluate algorithms across mul-tiple languages. Therefore we aimed to develop a pipeline that allows us to automatically derive large-scale corpora in different languages, with many contributing authors across a range of topics and contributions by authors across an extended period of time.

The input to our pipeline are WTPs. On WTPs Wikipedia contributors leave comments arguing about Wikipedia page changes. WTP comments are annotated with the contribu-tor X  X  user name or IP address and the time of commenting. We chose WTPs over the authors X  Wikipedia page contribu-tions, as we expect authors to adhere to the Wikipedia writ-ing guidelines when contributing to an existing Wikipedia page. In contrast, we assume that on WTPs, which do not prescribe to a specific writing style, contributors are more likely to write in their own style.
 While in this work we focus on the English portion of Wikipedia, it is evident that such corpora can be built for all languages with sufficient WTP contributions.

We consider each Wikipedia contributor as a potential author in our corpus and gather all WTP comments by the contributor. To retrieve the contribution of an author we compare the difference in a WTP between two revisions. Al-though a revision is created as soon as the WTP is changed (text is added or deleted or reformatted), we only consider those revisions where text is added by a contributor. This approach can lead to some noise (e.g. a contributor may be known under multiple user names or IP addresses, a con-tribution can contain a quote from another author. etc.), however, a manual analysis confirmed the quality of the de-rived data. Two hundred contributions between 2,500 and 10,000 characters were examined by the authors: 85% of the contributions had a single author, 5% contained a quotation of a size less than a third of the contribution, 7% contained a larger quotation, and the last 3% contained content oth-erwise not belonging to the contributing author. Thus, we conclude that our pipeline is of sufficient quality to generate a large-scale corpus for author verification.

For this work we generated a corpus based on a por-tion (40%) of English Wikipedia 2 . Overall, we observed 2 , 891 authors with at least four different comments, each one between 2,500 and 10,000 characters long. We chose this length interval in order to create a dataset which, while much larger and more diverse, is similar in spirit to the lat-est PAN datasets, where reference documents with approx-imately 1,000 words (  X  5,000 characters) are used.
Given the nearly 3 , 000 (for us) valid Wikipedia contrib-utors, it is evident that we can generate a large number of verification problems (or test cases). In this section, we fo-cus on building a balanced test set to evaluate our topical and temporal hypotheses.

Recall that a test case consists of an author (a Wikipedia contributor A ), reference documents (we use a single com-ment by A as reference document) and an unknown piece of text (positive test case: a comment by A , negative test case: a comment by another contributor).

To investigate our topical hypothesis, we require test cases where the reference and to-be verified documents are simi-lar and dissimilar respectively. To investigate the temporal hypothesis, we require test cases where the reference and to-be verified documents have been written within a short and long period of time respectively.

We generate five test sets. In the following list, similar either means similar in topic or similar in time:
To generate test cases with known topical similarity, we determined the semantic similarity between the two Wikipedia article pages that the WTPs from which a pair of com-ments were derived, are attached to. The similarity score (between 0 and 1) was computed with the Wikipedia Miner Toolkit [14]. Comment pairs with a score above 0 were con-sidered similar, comment pairs with a score of 0 as dissimilar.
To generate test cases across different time spans, we sim-ply made use of the timestamps attached to each comment. Comments written more than three years apart are consid-ered dissimilar in time. As similar in time we consider com-ments that have been written less than a week apart in time. We also experimented with the French and German Wikipedia, which yielded similar results. Due to space con-straints, we focus on English Wikipedia in this work.
Having set the similarity thresholds, we now derive the test cases. First, those authors are selected that have both similar and dissimilar WTP comments. For each author a test case is created with two similar posts, and a test case with dissimilar posts.

After these same-author test cases have been generated for each author, for every author two different-author test cases are generated: one for similar time/topic, and one for dissimilar time/topic. The text from the unknown au-thor is selected by taking a random text from the available comments, and verifying that it conforms to the time/topic requirements.

This method yields four test cases for every author: one same-author similar time/topic, one same-author dissimilar time/topic, one different-author similar time/topic, and one different-author dissimilar time/topic. The precise number of generated test cases is presented in Section 4.

Finally, each generated test set is balanced to having a 50% same-author rate by randomly discarding test cases if necessary.
We now briefly describe the character N-gram approach we implemented for our experiments. On a high level, the algorithms works as follows: first, all documents are pre-processed. Then, author profiles are created for the set of known reference documents and for the unknown document, based on the Common N-Grams (CNG) approach[10]. Es-sentially, each profile is a vector of N-grams. The distance between the known author profile and the unknown author profile is calculated according to [21], and finally based on the computed distances a judgement is made whether the unknown author is the same person as the known author.
The pre-processing steps included simple text transforma-tions such as replacing all digits by a special symbol, since the important stylistic information is the use of digits rather than the exact combination of digits as shown in [7]. Previ-ous work [19] concluded that for the CNG method N-grams of length three to five and a profile length of between 1 , 000 and 5 , 000 N-grams usually gives the best results. Our pre-liminary experiments confirmed those results, and we fixed N = 4 for all reported experiments.

The presented algorithm calculates how distinct authors are from each other, using a distance measure for the char-acter N-gram profiles. The average distance of all test cases is taken as threshold for similarity. This forces roughly half of the test cases to be classified as positive (same author) and half as negative (different author).
In total, there are 2359 test cases with a reported topic similarity of 0, and 1924 test cases with a reported similarity &gt; 0. The test case numbers in Table 1 reflect this skew; since all test sets are balanced there are far fewer test cases in the Similar test set than the others. Despite this, even the smallest generated test set is still ten times larger than last year X  X  PAN dataset.

In Table 1 we also report the number of test cases gen-erated for each of the five test sets as well as the accuracy achieved by the CNG algorithm. If our topic hypothesis would hold, then cases where the topic is the same should Table 1: Number of test cases and the accuracy of the CNG algorithm when comparing comments about similar topics to comments about dissimilar topics. A random baseline achieves accuracy 0.5.
 Table 2: The recorded accuracy when comparing comments made within one week to comments made more than three years apart. be ranked as being the same author more often, than cases where the topic is different. As this does not influence the Similar and Different test sets, we would expect the re-ported accuracy not to diverge too much from the baseline accuracy ( All annotated ). In contrast, for the Matching test set we expect a higher reported accuracy than for the baseline, while for the Nonmatching test set we expect a lower reported accuracy than for the baseline.

We observe that the second expectation holds, however, the first expectation is not reflected in the results. The Sim-ilar test set yields a higher accuracy than the baseline, while the Different test set yields a lower accuracy. We thus have to conclude that topic words are indeed also signif-icant style markers that play a role in the author verification task.

The results of the test sets generated according to tem-poral similarity are shown in Table 2. The first observation to make is that the Similar test set performs better than the baseline ( All annotated ) test set. The reported ac-curacies suggest that the writing style changes over time -the accuracy of the Similar test set (0.67) is considerably higher than the accuracy of the Different test set (0.58). A similar observation can be made when comparing Match-ing and Nonmatching . The conclusion we draw from these experiments is that time matters. The writing style changes over time, making two texts written far apart in time more difficult to link to the same author.

In [2] it was already suggested that writing style changes over time. This is confirmed by the our results. However, in [2] it is also suggested that a higher time gap could be a reason for a better discrimination between old and new works. We performed additional experiments and generated an additional test set with a one year time gap. However, the results did not confirm this claim -the results for the one and three year time gap were roughly the same. Due to data sparsity, time gaps larger than three years could not be investigated.
In this work, we proposed a methodology that allows us to generate large-scale corpora for author verification across multiple languages, based on Wikipedia Talkpages. We in-troduced two ways of building test cases to investigate top-ical and temporal hypotheses and reported results for the English portion of Wikipedia.

We investigated the topical influence and found that the topic does indeed influence the verification accuracy. The influence is not simply that similar topics skew the results towards false positives. Rather, similar topics overall (posi-tive and negative) cases were found to increase accuracy of author verification.

We also investigated the influence of time and found that writing style indeed changes over time, by comparing WTP contributions made within a week with WTP contributions made years apart. Author verification is more accurate when comparing texts that have been written within a short pe-riod of time.

In the future, we plan to investigate more languages -with the corpus generation pipeline in place, large-scale corpora for all languages with sufficient WTPs can be created. This will allow us to investigate the influence of the language fam-ily on the author verification accuracy. Furthermore, while in this work we restricted our investigation to the CNG algo-rithm (which is language independent), we will also consider additional language-dependent algorithms.
