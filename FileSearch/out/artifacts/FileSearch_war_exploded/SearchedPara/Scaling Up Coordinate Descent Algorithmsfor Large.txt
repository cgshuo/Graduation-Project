 Chad Scherrer chad.scherrer@pnnl.gov Pacific Northwest National Laboratory Mahantesh Halappanavar mahantesh.halappanavar@pnnl.gov Pacific Northwest National Laboratory Ambuj Tewari ambuj@cs.utexas.edu University of Texas at Austin David Haglin david.haglin@pnnl.gov Pacific Northwest National Laboratory Consider the ` 1 -regularized loss minimization problem where X  X  R n  X  k is the design matrix, w  X  R k is a weight vector to be estimated, and the loss func-tion ` is such that ` ( y,  X  ) is a convex differentiable function for each y . This formulation includes Lasso sion ( ` ( y,t ) = log(1 + exp(  X  yt ))).
 In this context, we consider a coordinate descent (CD) algorithm to be one in which each iteration performs updates to some number of coordinates of w , and each such update requires traversal of only one column of X .
 The goal of the current work is to identify and exploit the available parallelism in this class of algorithms, and to provide an abstract framework that helps to structure the design space.
 Our approach focuses on shared-memory architec-tures, and in particular our experiments use the OpenMP programming model.
 This paper makes several contributions to this field. First, we present GenCD, a Gen eric C oordinate D escent framework for expressing parallel coordinate descent algorithms, and we explore how each step should be performed in order to maximize parallelism. In addition, we introduce two novel special cases of this framework: Thread-Greedy Coordinate Descent and Coloring-Based coordinate descent, and compare their performance to that of a reimplementation of the recent Shotgun algorithm of Bradley et al. (2011), in the context of logistic loss.
 A word about notation: We use bold to indicate vec-tors, and uppercase bold to indicate matrices. We de-note the j th column of X by X j and the i th row of X by x T i . We use e j to denote the vector in R k consist-ing of all zeros except for a one in coordinate j . We assume the problem at hand consists of n samples and k features.

Name Dim Description Step We now present GenCD, a generic coordinate descent framework. Each iteration computes proposed incre-ments to the weights for some subset of the coordi-nates, and then accepts a subset of these proposals, and modifies the weights accordingly. Algorithm 1 gives a high-level description of GenCD.
 Algorithm 1 GenCD while not converged do In the description of GenCD, we refer to a number of different arrays. These are summarized in Table 1. For some algorithms, there is no need to maintain a physical array in memory for each of these. For ex-ample, it might be enough for each thread to have a representation of the proposed increment  X  j for the column j it is currently considering. In such cases the array-oriented presentation serves only to benefit the uniformity of the exposition. 2.1. Step One: Select We begin by selecting J coordinates for considera-tion during this major step in an iteration of GenCD. The selection criteria differs for variations of CD tech-niques.
 There are some special cases at the extremes of this selection step. Sequential algorithms like cyclic CD (CCD) and stochastic CD (SCD) correspond to se-lection of a singleton, while parallel  X  X ull greedy X  CD corresponds to J = { 1 ,  X  X  X  ,k } .
 Between these extremes, Shotgun (Bradley et al., 2011) selects a random subset of a given size. Other obvious choices include selection of a random block of coordinates from some predetermined set of blocks. Though proposals can be computed in parallel, the number of features is typically far greater than the number of available threads. Thus in general, we would like to have a mechanism for scaling the number of proposals according to the degree of available paral-lelism. As the first step of each iteration, we therefore select a subset J of the coordinates for which proposals will be computed. 2.2. Step Two: Propose Given the set J of selected coordinates, the second step computes a proposed increment  X  j for each j  X  J . Note that this step does not actually change the weights;  X  j is simply the increment to w j if it were to be updated . This is critical, as it allows evaluation over all selected coordinates to be performed in parallel without concern for conflicts.
 Some algorithms such as Greedy CD require a way of choosing a strict subset of the proposals to accept. Such cases require computation of a value on which to base this decision. Though it would be natural to compute the objective function for each proposal, we present the more general case where we have a proxy for the objective, on which we can base this decision. This allows for cases where the objective function is relatively expensive but can be quickly approximated. We therefore maintain a vector  X   X  R k , where  X  j is a proxy for the objective function evaluated at w +  X  j e j and update  X  j whenever a new proposal is calculated for j .
 Algorithm 2 shows a generic Propose step.
 Algorithm 2 Propose step for each j  X  J do // parallel 2.3. Step Three: Accept Ideally, we would like to accept as many proposals as possible in order to avoid throwing away work done in the proposal step. Unfortunately, as Bradley et al. (2011) show, correlations among features can lead to divergence if too many coordinates are updated at once. So in general, we must restrict to some sub-set J 0  X  J . We have not yet explored conditions of convergence for our new Thread-Greedy algorithm (see Section 7) but find robust convergence experimentally. In some algorithms (CCD, SCD, and Shotgun), fea-tures are selected in a way that allows all proposals to be accepted. In this case, an implementation can simply bypass the Accept step.
 2.4. Step Four: Update After determining which proposals have been ac-cepted, we must update according to the set J 0 of proposed increments that have been accepted.
 In some cases, the proposals might have been com-puted using an approximation, in which case we might like to perform a more precise calculation for those that are accepted.
 As shown in Algorithm 3, this step updates the weights, the fitted values, and also the derivative of the loss.
 Algorithm 3 Update step for each j  X  J 0 do // parallel Note that each iteration of the for loop can be done in parallel. The w j updates depend only upon the improved  X  j . While the updates to z have the po-tential for collisions if X ij 1 = X ij 2 for some distinct j ,j 2  X  J 0 , this is easily avoided by using atomic mem-ory updates available in OpenMP and other shared memory platforms. As described above, the Propose step calculates a pro-posed increment  X  j for each j  X  J . For a given j , we would ideally like to calculate where F is the smooth part of the objective function, Unfortunately, for a general loss function, there is no closed-form solution for full minimization along a given coordinate. Therefore, one often resorts to one-dimensional numerical optimization.
 Alternatively, approximate minimization along a given coordinate can avoid an expensive line search. We present one such approach that follows Shalev-Shwartz &amp; Tewari (2011).
 First, note that the gradient and Hessian of F are Here ` 0 and ` 00 denote differentiation with respect to the first variable. 3.1. Minimization for squared loss For the special case of squared loss, the exact mini-mizer along a coordinate can be computed in closed from. In this case, ` 00 ( y,t )  X  1, the Hessian H is con-stant, and we have the second-order expansion Along coordinate j , this reduces to As is well known (see, for example, (Yuan &amp; Lin, 2010)), the minimizer of (2) is then given by where  X  is the clipping function Note that this is equivalent to where s is the  X  X oft threshold X  function as described by Shalev-Shwartz &amp; Tewari (2011). 3.2. Bounded-convexity loss More generally, suppose there is some  X  such that  X  X  2 ` ( y,z )  X   X  for all y,z  X  R . This condition holds for squared loss (  X  = 1) and logistic loss (  X  = 1 / 4). Then, let In particular, along any given coordinate j , we have By (4), this upper bound is minimized at Because the quadratic approximation is an upper bound for the function, updating  X  j  X   X   X  is guaran-teed to never increase the objective function. 3.3. A proxy for the objective In cases where J 0 is a strict subset of J , there must be some basis for the choice of which proposals to ac-cept. The quadratic approximation provides a way of quickly approximating the potential effect of updating a single coordinate. For a given weight vector w and coordinate j , we can approximate the decrease in the objective function by 4.1. Algorithms At this point we discuss a number of algorithms that are special cases of the GenCD framework. Our intent is to demonstrate some of the many possible instanti-ations of this approach. We perform convergence and scalability tests for these cases, but do not favor any of these algorithms over the others.
 For current purposes, we focus on the problem of clas-sification, and compare performance across algorithms using logistic loss. All of the algorithms we tested ben-efited from the addition of a line search to improve the weight increments in the Update step. Our approach to this was very simple: For each accepted proposal increment, we perform an additional 500 steps using the quadratic approximation.
 S
HOTGUN is a simple approach to parallel CD intro-duced by Bradley et al. (2011). In terms of GenCD, the Select step of Shotgun chooses a random subset of the columns, and the Accept step accepts every proposal. This makes implementation very simple, because it eliminates the need for computation of a proxy, and allows for fusion of the Propose and Update loops. Algorithm 4 Proposal via approximation for each j  X  J do // parallel Bradley et al. (2011) show that convergence of Shot-gun is guaranteed only if the number of coordinates selected is at most P  X  = k 2  X  , where k is the number of columns of X , and  X  is the spectral radius (  X  max-imal eigenvalue) of X &gt; X . For our experiments, we therefore use P  X  columns for this algorithm.
 Bradley et al. (2011) suggest beginning with a large regularization parameter, and decreasing gradually through time. Since we do not implement this, our results do not apply to Shotgun variations that use this approach.
 In our novel T HREAD -G REEDY algorithm, the Select step chooses a random set of coordinates. Propos-als are calculated in parallel using the approximation described in Equation (7) of Section 3.2, with each thread generating proposals for some subset of the co-ordinates. Each thread then accepts the best of the proposals it has generated.
 We have not yet determined conditions under which this approach can be proven to converge, but our em-pirical results are encouraging.
 Similarly, G REEDY selects all coordinates, and uses the computed proxy values to find the best for each thread. However, rather than accept one proposal for each thread, proxy values are compared across threads, and only the single best is chosen in the Accept step. Dhillon et al. (2011) provide an analysis of convergence for greedy coordinate descent.
 C
OLORING is a novel algorithm introduced in this pa-per. The main idea here is to determine sets of struc-turally independent features, in order to allow safe concurrent updates without requiring any synchro-nization between parallel processes (threads). Color-ing begins with the preprocessing step, where struc-turally independent features are identified via partial distance-2 coloring of a bipartite graph representation of the feature matrix. In the Select step, a random color (or a random feature) is selected. All the fea-tures that have been assigned this color are updated concurrently. Based on the colors, a different number of features could get selected at each iteration. In contrast to Shotgun, conflicting updates are not lost. However, we note that Coloring is suitable only for sparse matrices with sufficient independence in the structure. Details of this algorithm are provided in Appendix A .
 Since C OLORING only allows parallel updates for fea-tures with disjoint support, updating a single color is equivalent to updating each feature of that color in se-quence. Thus convergence propoerties for C OLORING should be very similar to those of cyclic/stochastic co-ordinate descent.
 Table 2 provides a summary of the algorithms we con-sider. 4.2. Implementation We provide a brief overview of software implementa-tions in this section. All the four algorithms are im-plemented in C language using OpenMP as the par-allel programming model Chapman et al. (2007). We use GNU C/C++ compilers with -O3 -fopenmp flags. Work to be done in parallel is distributed among a team of threads using OpenMP parallel for con-struct. We use static scheduling of threads where each thread gets a contiguous block of iterations of the for loop. Given n iterations and p threads, each thread gets a block of n p iterations.
 Threads running in parallel need to synchronize for certain operations. While all the algorithms need to synchronize in the Update step, Algorithm G REEDY also needs to synchronize during the Select step to de-termine the best update. We use different mechanisms to synchronize in our implementations. Concurrent updates to vector z are made using atomic memory operations. We use OpenMP atomic directive for up-dating real numbers and use the x86 hardware opera-tor sync fetch and add for incrementing integers. We use OpenMP critical sections to synchronize threads in the Propose step of G REEDY algorithm. In order to preserve all the updates, we synchronize in the Update step of S HOTGUN . Note that this is different from the S HOTGUN algorithm as proposed in Bradley et al. (2011). Since structurally independent features are updated at a given point of time in the C OLORING algorithm, there is no need for synchronization in the Update step. 4.3. Platform Our experimental platform is an AMD Opteron (Magny-Cours) based system with 48 cores (4 sockets with 12 core processors) and 256 GB of globally ad-dressable memory. Each 12-core processor is a multi-chip module consisting of two 6-core dies with separate memory controllers. Each processor has three levels of caches: 64 KB of L1 (data), 512 KB of L2, and 12 MB of L3. While L1 and L2 are private to each core, L3 is shared between the six cores of a die. Each socket has 64 GB of memory that is globally addressable by all four sockets. The sockets are interconnected using the AMD HyperTransport-3 technology 1 .
 We conduct scaling experiments on this system in pow-ers of two (1, 2, 4, ... , 32). Convergence results pro-vided in Figure 1 are on 32 processors for each input and algorithm. 4.4. Datasets We performed tests on two sets of data. For each, we normalized columns of the feature matrix in order to be consistent with algorithmic assumptions.
 D OROTHEA is the drug discovery data described by Guyon et al. (2004). Here, each example corresponds to a chemical compound, and each feature corresponds to a structural molecular feature.
 The feature matrix is binary, indicating presence or absence of a given molecular feature. There are 800 examples and 100,000 features, with 7.3 nonzeros per feature on average. We estimate P  X  to be about 23. Our coloring resulted in a mean color size of 16. As a response, we are given an indicator of whether a given compound binds to thrombin. There are 78 examples for which this response is positive.
 For the D OROTHEA data set, we used a regularization parameter of  X  = 10  X  4 .
 R
EUTERS is the RCV1-v2/LYRL2004 Reuters text data described by Lewis et al. (2004). In this data, each example corresponds to a training document, and each feature corresponds to a term. Values in the training data correspond to term frequencies, trans-formed using a standard tf-idf normalization.
 The feature matrix consists of 23865 examples and 47237 features. It has 1.7 million nonzeros, or 37.2 nonzeros per feature on average. We estimate P  X  to be about 800. Our coloring resulted in a mean color size of 22.
 Each document in this data is labeled as belonging to some set of  X  X opics X . The most common of these (matching 10786 documents) is CCAT, corresponding to  X  X LL Corporate-Industrial X . Based on this, we used membership in the CCAT topic as a response. For the R EUTERS data set, using a regularization pa-rameter of  X  = 10  X  4 led to an optimal solution of 0, so we reduced it to  X  = 10  X  5 . 5.1. Convergence rates The results of our convergence experiments are sum-marized in Figure 1.
 First, consider D OROTHEA . Here, Figure 1(a) shows that all four algorithms were very close to convergence by the end of 10 minutes. In particular, for T HREAD -G
REEDY after the first 224 seconds, both the Objective function and the number of nonzeros (NNZ) were sta-ble.
 As expected, G REEDY added nonzeros very slowly, while S HOTGUN and C OLORING began by greatly in-creasing NNZ. Interestingly, though this initially put G
REEDY far behind the other algorithms, but by the end of the first ten seconds, it is back on track with S HOTGUN and Coloring.
 At around 200 seconds, there is a sudden increase in NNZ for G REEDY . We are uncertain of the source of this phenomenon.
 Perhaps most striking is that both S HOTGUN and C OL -ORING tend to begin by greatly increasing the number of nonzeros (NNZ). In the case of a very sparse op-timum like that of D OROTHEA with  X  = 10  X  4 , this initial effect is difficult to overcome, and both algo-rithms are at a disadvantage compared with G REEDY and T HREAD -G REEDY . This effect is less dramatic for R
EUTERS , where the optimal solution has more nonze-ros, and we suspect it would disappear entirely for problems with near-dense optima.
 Overall, performance of C OLORING and S HOTGUN were remarkably similar for both data sets. 5.2. Scalability Figure 2 shows scalability across algorthims, in terms of the number of updates per second. G REEDY makes updates relatively slowly, due to the large amount of work preceding an update in a given iteration. While the proposals can be computed in parallel, the threads must synchronize in order to identify the single best coordinate to update. This synchronization, and the subsequent update in serial, reduces parallel efficiency. The T HREAD -G REEDY has no such synchronization, and allows the number of updates to increase with the number of threads. The only concern we foresee is potential divergence if the number of threads is too large.
 scalability. R EUTERS has a similar number of feau-tures per color, but P  X  is much higher (800 vs 23 for D OROTHEA ). This leads to greater scalability for S HOT -GUN in this case, but not for C OLORING . The importance of coordinate descent methods for ` 1 regularized problems was highlighted by Friedman et al. (2007) and Wu &amp; Lange (2008). Their insights led to a renewed interest in coordinate descent meth-ods for such problems. Convergence analysis of and numerical experiments with randomized coordinate or block coordinate descent methods can be found in the work of Shalev-Shwartz &amp; Tewari (2011), Nesterov (2010) and Richt  X arik &amp; Tak  X a X c (2011a). Greedy coordi-nate descent is related to boosting, sparse approxima-tion, and computational geometry. Clarkson (2010) presents a fascinating overview of these connections. Li &amp; Olsher (2009) and Dhillon et al. (2011) apply greedy coordinate descent to ` 1 regularized problems. Recently, a number of authors have looked at par-allel implementations of coordinate descent methods. Bradley et al. (2011) consider the approach of ignoring dependencies, and updating a randomly-chosen sub-set of coordinates at each iteration. They show that convergence is guaranteed up to a number of coordi-nates that depends on the spectral radius of X &gt; Richt  X arik &amp; Tak  X a X c (2011b) present an approach us-ing GPU-accelerated parallel version of greedy and randomized coordinate descent. Yuan &amp; Lin (2010) present an empirical comparison of several methods, including coordinate descent, for solving large scale ` 1 regularized problems. However, the focus is on serial algorithms only. A corresponding empirical study for parallel methods does not exist yet partly because the landscape is still actively being explored. Finally, we note that the idea of using graph coloring to ensure consistency of parallel updates has appeared before (see, for example, (Bertsekas &amp; Tsitsiklis, 1997; Low et al., 2010; Gonzalez et al., 2011)). We presented GenCD, a generic framework for ex-pressing parallel coordinate descent algorithms, and described four variations of algorithms that follow this framework. This framework provides a unified ap-proach to parallel CD, and gives a convenient mech-anism for domain researchers to tailor parallel imple-mentations to the application or data at hand. Still, there are clearly many questions that need to be ad-dressed in future work.
 Our T HREAD -G REEDY approach can easily be extended to one that accepts the best | J 0 | proposals, indepen-dently of which thread proposed to update a given coordinate. This would have additional synchroniza-tion overhead, and it is an open question whether this overhead could be overcome by the improved Accept step.
 Another open question is conditions for convergence of the T HREAD -G REEDY algorithm. A priori, one might suspect is would have the same convergence criteria as S
HOTGUN . But for D OROTHEA , P  X  = 23, yet updating a coordinate for each of 32 threads results in excellent convergence.
 It is natural to consider extending S HOTGUN by parti-tioning the columns of the feature matrix into blocks, and then computing a P  X  b for each block b . Intuitively, this can be considered a kind of  X  X oft X  coloring. It remains to be seen what further connections between these approaches will bring.
 Our C OLORING algorithm relies on a preprocessing step to color the bipartite graph corresponding to the adjacency matrix. Like most coloring heuristics, ours attempts to minimize the total number of colors. But in the current context, fewer colors does not necessar-ily correspond with greater parallelism. Better would be to have a more balanced color distribution, even if this would require a greater number of colors. Bertsekas, D. P. and Tsitsiklis, J. N. Parallel and Dis-tributed Computation: Numerical Methods . Athena Scientific, 1997.
 Bradley, J. K., Kyrola, A., Bickson, D., and Guestrin, C. Parallel Coordinate Descent for L1-Regularized Loss Minimization. In International Conference on Machine Learning , 2011.
 Catalyurek, U., Feo, J., Gebremedhin, A., Halap-panavar, M., and Pothen, A. Graph Coloring Algo-rithms for Multi-core and Massively Multithreaded
Architectures. Accepted for publication by Journal on Parallel Computing Systems and Applications, 2011.
 Chapman, Barbara, Jost, Gabriele, and Pas, Ruud van der. Using OpenMP: Portable Shared Mem-ory Parallel Programming (Scientific and Engineer-ing Computation) . The MIT Press, 2007. ISBN 0262533022, 9780262533027.
 Chung, F. R. K. Spectral Graph Theory . American Mathematical Society, 1997.
 Clarkson, K. L. Coresets, sparse greedy approxima-tion, and the Frank-Wolfe algorithm. ACM Trans-actions on Algorithms , 6(4):1 X 30, August 2010. Dhillon, I. S., Ravikumar, P., and Tewari, A. Near-est neighbor based greedy coordinate descent. In
Advances in Neural Information Processing Systems 24 , 2011.
 Friedman, J., Hastie, T., H  X ofling, H., and Tibshirani, R. Pathwise coordinate optimization. Annals of Applied Statistics , 1(2):302 X 332, 2007.
 Gonzalez, J., Low, Y., Gretton, A., and Guestrin, C.
Parallel gibbs sampling: From colored fields to thin junction trees. Journal of Machine Learning Re-search -Proceedings Track , 15:324 X 332, 2011. Guyon, I., Gunn, S., Ben-Hur, A., and Dror, G. Re-sult analysis of the NIPS 2003 feature selection chal-lenge. Advances in Neural Information Processing Systems , 17:545 X 552, 2004.
 Lewis, D., Yang, Y., Rose, T., and Li, F. RCV1 : A New Benchmark Collection for Text Categorization
Research. Journal of Machine Learning Research , 5: 361 X 397, 2004.
 Li, Y. and Olsher, S. Coordinate Descent Optimization for ` 1 Minimization with Application to Compressed
Sensing ; a Greedy Algorithm Solving the Uncon-strained Problem. Inverse Problems and Imaging , 3:487 X 503, 2009.
 Low, Y., Gonzalez, J., Kyrola, A., Bickson, D., Guestrin, C., and Hellerstein, J. M. Graphlab: A new framework for parallel machine learning. In Proceedings of the Twenty-Sixth Conference on
Uncertainty in Artificial Intelligence , pp. 340 X 349, 2010.
 Nesterov, Y. Efficiency of coordinate descent methods on huge-scale optimization problems. Technical Re-port CORE Discussion Paper #2010/2, Universit  X e Catholique de Louvain, 2010.
 Nikiforov, V. Chromatic number and spectral radius.
Linear Algebra and its Applications , 426(2-3):810 X  814, October 2007.
 Richt  X arik, P. and Tak  X a X c, M. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Arxiv preprint arXiv:1107.2848 , 2011a.
 Richt  X arik, P. and Tak  X a X c, M. Efficient Serial and Par-allel Coordinate Descent Methods for Huge-Scale
Truss Topology Design. In Operations Research Pro-ceedings , pp. 1 X 6, 2011b.
 Shalev-Shwartz, S. and Tewari, A. Stochastic meth-ods for l 1 -regularized loss minimization. Journal of Machine Learning Research , 12:1865 X 1892, 2011. Wu, T. and Lange, K. Coordinate descent algorithms for lasso penalized regression. Annals of Applied Statistics , 2:224 X 244, 2008.
 Yuan, G. and Lin, C. A Comparison of Opti-mization Methods and Software for Large-scale L1-regularized Linear Classification. Journal of Ma-chine Learning Research , 11:3183 X 3234, 2010.
 Zuckerman, D. Linear Degree Extractors and the Inap-proximability of Max Clique and Chromatic Num-
