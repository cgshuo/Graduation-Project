 In this paper, an algorithm called Time Driven Documents-partition (TDD) is proposed to construct an event hierarchy in a text corpus based on a given query. Specifically, assume that a query con-tains only one feature  X  Election . Election is directly related to the events such as 2006 US Midterm Elections Campaign, 2004 US Presidential Election Campaign and 2004 Taiwan Presidential Election Campaign, where these events may further be divided into several smaller events (e.g. the 2006 US Midterm Elections Cam-paign can be broken down into events such as campaign for vote, election results and the resignation of Donald H. Rumsfeld). As such, an event hierarchy is resulted. Our proposed algorithm, TDD, tackles the problem by three major steps: (1) Identify the features that are related to the query according to both the timestamps and the contents of the documents. The features identified are regarded as bursty features; (2) Extract the documents that are highly related to the bursty features based on time; (3) Partition the extracted doc-uments to form events and organize them in a hierarchical structure. To the best of our knowledge, there is little works targeting for con-structing a feature-based event hierarchy for a text corpus. Practi-cally, event hierarchies can assist us to efficiently locate our target information in a text corpus easily. Again, assume that Election is used for a query. Without an event hierarchy, it is very difficult to identify what are the major events related to it, when do these events happened, as well as the features and the news articles that are related to each of these events. We have archived two-year news articles to evaluate the feasibility of TDD. The encouraging results indicated that TDD is practically sound and highly effective. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  clustering ;H.5[ Information Interfaces and Pre-sentation ]: Miscellaneous  X  The work was supported by a grant of RGC, Hong Kong SAR, China (No. 418206).
 Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. Algorithms, Design, Management Hierarchies, Events, Time, Clustering, Text, Retrieval, Presentation
In this information overwhelming century, information becomes ever more pervasive and important. While there are some excel-lent search engines, like Google, to assist us to retrieve our target information by simply providing some keywords, the problem of too much information surrounding us makes it harder and harder to locate the right piece of information efficiently. As an example, consider we want to identify what had happened when the virus SARS (Severe Acute Respiratory Syndrome) broke out in Hong Kong. By specifying the word SARS as the search query, we may obtain a result that is similar to the one as shown in Figure 1 (a) through some existing news search engines such as Factiva figure, it shows the headlines, and the first few lines of the news articles if their contents contain the word SARS .

Yet, the search result in Figure 1 (a) does not capture any of the  X  X ime-dependent information X , meanwhile this information is crit-ical in terms of assisting us to efficiently locate our target. For example, it is very difficult to identify from Figure 1 (a) that what are the major events related to the keyword SARS (e.g. the closure of school, the travel warnings issued by WHO, the resignation of the Hong Kong Secretary of Health, etc), the periods when these events happened, as well as the keywords and the news articles that are related to each of these events. On the other hand, the time re-lated information is often associated with the documents, such as the time when the web page is being updated, the time when a news article is being released or the time when a blog is being written. It is desirable to group the documents in a corpus according to both their contents and their timestamps.

This urges us to think critically whether we can extend the ca-pability of the existing search engines to incorporate more infor-mation, such as including the time dimension. Specifically, we are curious of whether it is possible to have an algorithm that is able to group the retrieved documents into events according to the sim-ilarity of their contents and their timestamps, so as to construct an event hierarchy according to a particular user X  X  query.

Let us continue with our previous example about SARS. The questions that we have raised before can all be answered efficiently by constructing an event hierarchy which is similar to the one shown http://www.factiva.com organized the search results in (a) by using an event hierarchy. in Figure 1 (b). In the figure, the upper block diagram represents when the events happened. The x-axis is the time. The blocks in the diagram represent events. All events in the diagram are related to the keyword SARS . They are arranged in a hierarchical structure, where the links denote their relationships. For instance, Event A and Event B are at the same level, where Event A and Event B re-spectively contain Event C and Event D. There is no direct relation-ship between Event C and Event D as they are not connected with each other. Event C is further broken down into Event E and Event F, whereas the later two events are connected. Similar description applies for the other events. The lower half of Figure 1 (b) is similar to Figure 1 (a) except that it only shows the information of the news articles that are related to a particular event in the upper block dia-gram (Event J for this figure). As such, we can mitigate the problem of information overloading by displaying the information related to a particular event only. It is worth to note that we have preserved all information as we can easily switch from one event to the oth-ers. Last but not least, in Figure 1 (b), every event is associated with a set of keywords. For instance, Event J is associated with the keywords such as School , College and University . Practi-cally, it is very useful to associate a set of keywords with each event because we can identify their relationships efficiently.
Let us present one more example to account for our motiva-tion. For the keyword, Election , it is related to the events such as the 2006 US Midterm Elections Campaign, the 2004 US Presi-dential Election Campaign and the 2004 Taiwan Presidential Elec-tion Campaign. Usually, an event may be further broken down into several sub-events. In this example, the 2006 US Midterm Elec-tions Campaign may be broken down into events such as campaign for vote, election results and the resignation of Donald H. Rums-feld, the former US Secretary of Defense. Furthermore, an event is usually associated with more than one feature. For instance, the 2004 US Presidential Election Campaign is further associated with the features such as President , Campaign , Bush , Senator and Kerry . Eventually, an event hierarchy for the feature Election , such as the one in Figure 2, could be formulated. As a result, we will have a much clearer picture about the relationships among the features.

Formally speaking, given a user query, we propose an algorithm to construct an event hierarchy by grouping the retrieved docu-ments into events according to their timestamps and their contents. A search query is defined as a set of keywords, in which we call them features. An event is defined as an object which consists of the following three components: (1) A set of documents with simi-lar contents; (2) A set of representative features extracted from the documents; and (3) Two timestamps to denote for the event begins and ends.

Conceptually, given a query, our problem can be readily solved by the following straightforward framework: (1) Identify all of the documents that are related to the query; (2) Group the highly related documents together to form events; (3) Arrange the events in an hierarchical structure; (4) For each event, extract a set of features that can represent it. Intuitively, this framework works well, and can be solved by combining some of the existing techniques [1, 2, 4, 7, 9, 12, 16, 20, 21, 17, 27, 28, 30, 31, 32, 33, 18, 34, 36, 37, 38]. Unfortunately, we claim that this framework is ineffective to solve our problem. The reason is because of the imprecise nature of the search results. The search results returned in Step (1) of the straightforward framework are usually contain a sizable number of unrelated documents. These unrelated documents are hard to clean up, making it difficult to group the documents into events so as to construct an appropriate event hierarchy and identify a set of representative features for each event. We will discuss the details in Section 2.

In contrast to the above straightforward approach, we proposed an algorithm called Time Driven Documents-partition (TDD) to solve our problem. Given a query, TDD will: (1) Identify the fea-tures that are related to the query according to the timestamps and contents of the documents in the corpus. These features are re-garded as bursty features; (2) Extract the documents that are highly related to the bursty features based on time; (3) Partition the ex-tracted documents and organize them to construct an event hier-archy. We will show that bursty f eatures are less prune to noise. So our approach starts with discovery the bursty features. We will discuss the details in Section 2.

We have conducted extensive experiments to evaluate TDD X  X  ef-fectiveness by using two-year news articles. We chose news ar-ticles because they are indexed by timestamps and their contents have strong temporal structure. It is easy to evaluate whether TDD is feasible. Nevertheless, TDD can be extended to other areas, such as grouping the retrieved web pages according to their latest modi-fication dates. This will be left for our future work. The rest of the paper is organized as follows  X  Section 2 presents TDD; Section 3 evaluates the effectiveness of TDD; Section 4 briefly discusses some of the major related work; Section 5 summarizes and con-cludes this paper.
Table 1 summarizes the notations that would be used throughout this paper. Let D = { d 1 , d 2 ,... } be a text corpus, where d ument with the timestamp t i . Following the existing direction [7, 30, 31], D , is partitioned into L consecutive and non-overlapping windows according to the timestamps of the documents. Let W be a set containing all the windows. Let F be a set containing all dif-ferent features in D . A feature, f  X  F , is a word in the text corpus. |
F | is the number of different features in D .

The input of our algorithm is a query, Q ,where Q  X  F . The out-put is an event hierarchy (which is similar to Figure 2). We will first describe how we formulate the problem and why it is formu-lated in such a way by presenting some real life examples in Section 2.1, then we will present the implementation details in the sections thereafter. Given a user query, Q , in order to construct an event hierar-Sym. Description D a text corpus
D a set of documents, D  X  D d i a document, d i  X  D t i the timestamp of d i
W a set of windows w a window, w  X  W L number of windows in the text corpus
F a set of all different features in the text corpus f a feature, f  X  F n fw number of documents contains f in w N w the feature count of w Q a user query, Q  X  F
B a set of bursty periods for Q b a bursty period, b  X  B F b a set of features associated to b , F b  X  F D b a set of documents in b that are related to Q , D b  X 
E a set of events e an event, e  X  E chy, the first step arguably is to identify all of the events that are related to Q . A possible approach is that we first retrieve all of the documents that are related to Q , and then cluster the docu-ments into groups so as to formulate events. We argue that this approach is inappropriate . Let us consider for the following sit-uation. There was a news article released from the South China Morning Post (SCMP) 2 with the headline  X  X wo chiefs, two sys-tems of getting the job done X  containe d the feature SARS .Inter-estingly, this article has nothing to do with the virus SARS, s a  X  cute r on 2002/01/05 (a date well before the virus SARS outbreak) and was related to the appraisals of the two chief executives in the two s  X  pecial a From our experiences, when we issue the query Q = { SARS } are always targeting for retrieving the information that is related to the virus (Severe Acute Respiratory Syndrome) but not the place (Special Administrative Regions). Query the information for the later issue will usually involve some other features, such as Hong , Kong or Macau . Even though a document, d  X  D , matches the user query, Q , the user may not be interested in it. Accordingly, if we follow the approach of retrieving the related documents first and A news agent: http://www.scmp.com then identify the events by clustering, we may need to have some other heuristics for filtering the documents that match Q but do not coincide with the users X  interest. This is difficult. Similarly, even though a document which does not match Q , the user may some-times be interested in it (we will explain this later). In this paper, we claim that we should first identify the events, E , that satisfy Q , and then map the related documents back to the events. It is a re-verse of the just mentioned ineffective approach. Yet, if we follow this new direction, a question immediately ensues: How to identify the events that are related to Q in the text corpus without retrieving any of the documents?
From our observations, the emergence of an event is always as-sociated with a burstoffeatures , where some features suddenly appear frequently when the event emerges, and their frequencies drop when the event fades away. For instance, from 2003/01/01 to 2004/12/27 (726 days), the feature Tsunami only appeared 28 times in 23 news articles in SCMP 2 . Yet, from 2004/12/27 to 2004/12/31 (5 days), Tsunami appeared 86 times in 41 news arti-cles. Both the number of news articles and the feature frequency in-creased dramatically in only 5 days. History tells us that there was an undersea earthquake that occurred at 00:58:53 UTC on 2004/12/26 with an epicenter off the west coast of Sumatra, Indonesia. The earthquake triggered a series of devastating tsunami that spread throughout the Indian O cean killing lar ge numbers of people and inundating coastal communities acr oss South and Southeast Asia. This event is usually regarded as the Asian Tsunami. Its emergence can be identified by observing the distribution of Tsunami across the timeline. Hence, by monitoring the feature distributions in D , we can identify whether there is any event occurred. If the fea-ture, f  X  Q , suddenly appears frequently , we can then conclude that some events related to Q emerge. Here, another question im-mediately arises: how to define the phrase  X  X uddenly appears fre-quently X ? We will provide the implementation details in Section 2.2.

Without loss of generality, assume that we know all of the peri-ods where Q suddenly appears frequently. Let B be a set of such periods and D b be a set of documents which satisfies Q and resides in b  X  B . Intuitively, the next process is to construct an event hi-erarchy (e.g. Figure 2) by recursively dividing each D b based on some partitioning algorithms. However, we claim that this process should not be conducted at this moment. Let us consider for the following situation. Assume that Q = { Iraq } . There was an arti-cle release from SCMP 2 on 2003/03/18 titled  X  X  letter to Saddam: Be more like me X . Obviously, even just judging from the title, this article is related to Iraq . But, in the whole article, the feature Iraq is not found. Some documents may be highly related to Q ,buttheir contents do not match Q . So, before formulating the hierarchical structure, we should extract all of the documents that are highly re-lated to each D b . Our problem now is: given a set of documents, D , extract all of the documents that have similar contents.
A simple but ineffective method is to compare the documents in b with D b one by one: for each document in b , see whether it is similar to any of the document in D b . Unfortunately, since the feature distributions are sparse in the text domain, two documents with similar features may not necessarily belong to the same event [6, 29]. As the result obtained by comparing two sets of documents will usually be more reliable than comparing in a document ba-sis [29], we should select a proper subset of documents in b and compare it with D b . Now, another question arises: how to select a proper subset of documents in b ?
At the first glimpse, this question can be answered by the tech-
See http://en.wikipedia.org/wiki/2004 Indian Ocean earthquake. niques used in query refinement [14, 22, 25]: find all of the features that are highly associated with Q , and then for those documents be-long to these features, try to align them to D b ,  X  b  X  B . Unfortu-nately, the existing techniques merely rely on the co-occurrence of features in the text domain, and cannot include the time dimension. Yet, time is important in solving our problem. For instance, the fea-ture Virus is highly related to the feature Bird (Bird Flu  X  H5N1  X  a kind of virus) for some bursty periods. Not all of the bursty periods of Virus are coincident with Bird . It is unlikely that two features have high association for all of their bursty periods.
In this paper, we try to solve the above problem with the help of the bursty patterns of the other features in D . Firstly, we identify the bursty patterns of all features, F ,in D . Then, for each f determine whether any of the bursty periods of f is similar to any of the b  X  B . Finally, we compare the similarity between D the documents that belongs to f by using two novel coefficients: intra-document similarity and inter-document similarity. Their for-mulation is motivated by a newly defined concept called  X  X ap-to X . Details will be discussed in Section 2.3.
 Thus, we can obtain a set of documents, D b , that are related to Q and are released within the period b  X  B . As discussed previ-ously, each D b may further be divided into several events. In this paper, we identify these sub-events by implementing the bisecting K-Means clustering algorithm [29]. Details are discussed in Sec-tion 2.4.
Let f  X  Q . In order to determine which of the windows the fea-ture f  X  X uddenly appears frequently X , we try to compute the  X  X rob-ability of bursty X  for each window w  X  W .Let P ( w , f ; p probability that the feature f is bursty in window w according to p where p e is the probability that f would appear in a time window given that it is not bursty. The intuition is that if the frequency of f appearing in w is significantly higher that the expected probabil-ity p e ,then w is regarded as a bursty period. We compute P according to the cumulative distribution function of the binomial distribution [19]: where N w is the total count of the features appeared in w and n frequency of f appearing in w . P ( w , f ; p e ) is the cumulative distri-bution function of the binomial distribution and p ( k ; N corresponding probab ility mass function.

Figure 4 (a) and (b) respectively show the probability mass func-tion ( p ( k ; N w , p e ) and the cumulative distribution function ( P of the binomial distribution with p e = 0 . 3and N w = 1000 using a gradually removed. We only retain those periods with many related documents. Algorithm 1 computeBaselineProbability ( N ) Input: R = { r 1 , r 2 ,..., r L } Output: The expected probability p e 1: repeat 2: p e = mean ( R ) ; 3:  X  = standardDeviation ( R ) ; 4: for each r i  X  P do 5: if r i &gt; m  X   X  p e then 6: remove r i from R ; 7: end if 8: end for 9: until r i  X  m  X   X  p e ,  X  r i  X  R 10: return p e ; normal approximation. The x-axis represents the number of times f appears in w . Their shape depends on p e only. p ( k ; N be maximum if the probability of f appearing in w equals to p Let r = n fw / N w be the probability of f appearing in w .If r p P ( w , f ) would approach to 0. In contrast, if r p e , P ( w approach to 1. In other words, if the probability of f appearing in w is well below the expect probability, p e , we will not consider f is bursty in w . On the other hand, if the probability is much higher than p e , we conclude that f exhibits some abnormal behavior and hence it is bursty in w .

In Eq. (1), in order to compute p e , a simple yet ineffective ap-proach is to explicitly define a fixed threshold, x , such that if f appears more than x %in w , f is bursty in w . This approach is im-practical as different features have different thresholds. In order to assign different thresholds to diff erent features automatically, we may attempt to rely on the mean probability: if f is distributed evenly over all windows in the text corpus, then the probability of f in any window is: p e =( 1 / L )  X  w  X  W ( n fw / N w ) may not be appropriate. If the features with many bursty periods, such as SARS or Iraq , their mean probabilities will be heavily bi-ased by their bursty p eriods, and these pr obabilitie s cannot be used to model the situations when the features occur by chance. In this paper, we ignore the features with a significantly high frequencies in the windows when computing p e .

Algorithm 1 shows how we compute p e .Let R = { r 1 , r 2 be a sequence where each element, r i , denote for the probability of f appearing in w i ( r i = n fw i / N w i ), where w i is the i th window in W . In lines 2  X  3, we compute the mean of R ( p e ) and its standard deviation. Then, we check whether all r i drops within m standard deviations of the mean of R . We reject those points in R that are out of this range until all r i are less than or equal to this range. In this paper, we set m = 3.

Finally, Algorithm 1 may encounter this problem: p e may be equal to 0. Let us consider for the feature Tsunami .Itsdistribu-tion from 2003/01/01 to 2004/12/31 (730 day) is shown in Figure 5 (b). It only appears in 25 days with a total of 114 times. After completing Algorithm 1, p e would be 0. If p e = 0, then the re-sult of Eq (1) would be undefined. So, we redefine r i by using the Laplacian smoothing [3]. As a result, r i and Eq.(1) will become: r In order to decide whether f is bursty in w , we check where n drops in Figure 4 (b). If n fw drops in Region D, then f is bursty in w as its occurrence is significantly higher than the expected proba-bility, p e .If n fw drops in Region A, then f is not bursty in w as it appears in w is less than the expected probability, p e .
Finally, we claim that user contribution in the bursty periods identification is necessary. Let us take the feature SARS as an exam-ple. Its bursty patterns are shown in Figure 3. The x-axes denote the day in 2003 and the y-axes denote the bursty state (0 or 1). When the bursty state is 0, no important event related to SARS happened. The situation reverses if the bursty state is 1, where some important events emerged. Figure 3 (a) shows all of the bursty periods. The number of bursty periods decreased gradually from Figure 3 (b) to Figure 3 (d). We only retain those periods with many related doc-uments. If the number of bursty periods is reduced, the number of events identified must also be reduced. Hence, we may locate our target information more easily if there exists a threshold to con-trol the number of events to be retrieved. Let  X   X  [ 0 , 1 a threshold. We re-scale Region B into this range. For instance, if  X  = 0 . 8 and Region B is from 0 . 4to1 . 0, then from all w with P ( w , f ; p e )  X  0 . 667, they will be regarded as bursty periods. Our proposed algorithm includes this kind of user contribution, which is not being reported elsewhere.
 Figure 6: Four bursty features and their corresponding docu-ments.
 Let us assume that the bursty periods related to Q are identified. Let B be a set of bursty periods and D b be a set of documents that satisfy Q and reside in b  X  B . In this section, we describe how we identify the features and the documents that are highly related to every b . For the features that are highly related to b , we call them as associated feature, F b ,ofperiod b . Similarly, for the documents that are highly related to b , we call them as associated documents, D ,ofperiod b .

As stated in Section 2.1, we identify F b based on the following steps: (1) For all f  X  F , identify their bursty periods according to the procedure described in the previous section; (2) Let D be a set of documents that resides in one of the bursty periods of f .We conclude that D b can be enlarged by D if D map-to D b ( D
D EFINITION 1. (M AP -T O  X  ) Let D b be a set of documents that satisfies Q and resides in b  X  B. Let D be another set of doc-uments. D b and D may be overlapped. We say that D map-to D (D  X  D b ) if and only if D is also resides in b and is significantly similar to D b ,whereD b  X  D b . Note that D is significantly similar to D b does not imply D is signifantly similar to D b ,i.e. D does not imply D b  X  D.

E XAMPLE 1(M AP -T O ) Figure 6 shows four features: two bursts for Bird and Flu , and one burst for Cold and H5N1 . According to Definition 1, a list of map-to relationships are identified, and are listed in Table 2. For instance, B1 and H1 are highly overlapped, B1  X  H1. Similarly, as H1 and B1 are also highly overlap, H1  X 
B1. Since B1 is a subset of F1, B1  X  F1. In contrast, F1 cannot map to B1 or H1, as it is a superset of both of them.

In Defintion 1, we have to carefully define the phrase  X  X ignifi-cantly similar to X . Given two sets of documents, D b and D ,wede-termine whether they are siginficiantly similar to each other by two components: intra-similarity ( S I )and mapping-similarity ( S first present how they are computed, then explain why they are computed in this way. For the intra-similarity ( S I ): where d is the nearest neighbor of d  X  D ,and C ( d , d ) the similarity between d and d (e.g. cosine coefficient [5]). Eq. (4) computes the average similarity between every pair of the near-est neighbor document in D . This is why S I is termed as intra-similarity . For the mapping-similarity: where d  X  D b is the nearest document with respect to d  X  S (  X  ) and S M (  X  ) differs in the contents of their similarity functions, C (  X  ,  X  ) . S I computes the similarity within the same set of docu-ments, whereas S M computes the similarity between two different sets of documents. Specifically, S M (  X  ) finds the set of the most sim-ilar documents in D that map-to D b . Thisiswhy S M is termed as mapping-similarity .
 Intuitively, if S M ( D ) &lt; S I ( D ) , we would be in favor of grouping D b and D together, and rejecting to group them otherwise. Unfor-tunately, directly comparing S M (  X  ) and S I (  X  ) is inappropriate. Both S (  X  ) and S I (  X  ) are averaged values . An averaged value may easily be affected by outliers. Thus, standard deviations of both compo-nent must be used when we have to conduct the comparison. Fur-thermore, the overlapping areas between D b and D mayalsoaffect the comparison. For instance, if two sets of documents are highly overlapped, they may group together even if S M (  X  ) is a bit smaller than S I (  X  ) , i.e. some relaxation should be allowed in this case. On the other hand, no relaxation should be given if D b  X  D = / 0 capture these ideas, we use a one-tail t -test with H 0 : S H 1 : S M &lt; S I , with the test statistics: where  X  is the standard deviation within S I ( D ) . H 0 would be re-jected ( D should not be mapped to D b )if t 0 &gt; T  X  ,where0 is the significance level.  X  cannot be greater than 0.5 since this problem is a one tailed t -test.  X  must be chosen carefully, as it controls the relaxation of the t -test. The higher the value of tighter is the control.

As we discussed above,  X  should be determined dynamically based on the overlapping area between D b and D . Intuitively, should behave as the pattern shown in Figure 7, where the y -axis is the degree of confidence (  X  )andthe x -axis is the percentage of overlapping between D b and D . Befor we continue, let us define a variable,  X  : Figure 7 captures the idea that if the overlapping area is large ( 1), the required degree of confidence would be small (  X  0). This would result in a more relax situation. If the overlapping area is small (  X  0), a much higher degree of confidence (  X  0 . 5) would be resulted. If the overlapping area is halved (  X  = 0 . 5), the required degree of confidence would be ambiguous (0.25). Logically, the re-lationship between  X  and  X  should not be linear because we should be more certain with the decisions toward both ends. Eventually, we can model the situation described so far by using a cosine func-tion. Mathematically, Since it does not make sense if the degree of confidence is smaller than 0 or larger than 0.5, the cosine function is re-scaled within 0 to 0.5. This is why the two 1/4 are added.
In this section, we describe how to construct an event hierarchy for a particular query, Q .Let E be all of the events that appear in the event hierarchy. An event, e  X  E , is an object which consists of the following three components: (1) A period of two timestamps; (2) A set of representative features; and (3) A set of similar documents. After the previous two steps, we will obtain a set of documents, D ,thatis highly related to Q (some documents D  X  D b match Q directly, and some of them do not match Q but are very similar to the other documents in that match Q in D b ) and resides in b where B is a set of periods when Q suddenly appears frequently. As discussed previously, for each D b , it may further be broken down into several events. Accordingly, for each D b ,weusethebisect-ing K-Means clustering algorithm [29] to partition it, such that the documents with similar contents in D b would be grouped together to form events. Bisecting K-Means is particularly suitable for par-titioning the text corpus and will generate a dendrogram automat-ically. Details of the algorithm could be referred to [29]. Similar to most of the clustering problems, the only issue remained here is how to specify a stopping criteri on for the partitioning process: how many events should be left? Defining a fixed value is unrea-sonable, as it is impossible to predict how many events exists in D in advance.

In this paper, the stopping criterion of the bisecting K-Means depends on the timestamp of the documents in D b .Let t i timestamp of d i . We recursively partition D b until max m ,where m is a predefined threshold. In this paper, we set m This is based on our observations from the dataset that we used. The details of the dataset are described in Section 3. Eventually, each resulting partition is regard as an event. Since a dendrogram is generated automatically from the bisecting K-Means, a hierarchical tree structure which is similar to Figure 2 is obtained naturally.
The representative features of each event are extracted by using the information gain [26]. We extract the top k features for each of the event, e  X  E .This k is set by the users according to their pref-erences, and it will not affect the str ucture of the event hierarchy in any circumstance. We have archived 78,695 news articles from the South China Morning Post from 2003/01/01 to 2004/12/31. All features are stemmed using the Lovins stemmer [15]. Features that appear in more than 75% of the total news articles in a day are categorized as stopwords. Features that appear in less than 8% of the total news articles in a day are categorized as noise. All features are weighted using the tf  X  idf schema [24] whenever necessary. In order to facil-itate the computation, a news article with fewer than 35 different features is removed. Since the news articles arrive everyday, a win-dow, w , is naturally meant one day. Figure 8 and Figure 9 show the event hierarchies for Q 1 = { and Q 2 = { Bush } when  X  = 0 . 9. There are totally 32 and 15 events associated with Q 1 and Q 2 , respectively. Due to the space limit, we only show the events up to the forth level for Q 1 and the second level for Q 2 . For each event, we select the top 10 features to rep-resent it. These features then propagate from the last level to the first level via their parents. Table 3 and Table 4 list the stemmed features for each event in the first level of the hierarchies.
Figure 8 corresponds to the events related to SARS. The first level of the event hierarchy contains four periods. The major event is from 2003/03/28 to 2003/07/02. It further divides into many smaller events. The other three events in the first level all last for only 3 to 4 days. They do not have any further branches. The following is a brief review of these events  X  Event 1 (2003/03/28  X  2003/07/02): SARS breaks out in Mainland China and spreads out to other countries. On 2003/03/27, CDC official presents the first evidence that a virus associated with the upper respiratory in-fections and another virus called corona might be likely the cause of SARS. On 2003/07/02, the aforementioned virus is under con-trol, and WHO (World Health Organization) removes all coun-tries, except Taiwan, from its list of areas with recent local SARS transmission after 20 days passed since the last SARS case was reported and isolated. Event 2 (2003/12/26  X  2003/12/28): The first Christmas after SARS was outbreak. Event 3 (2004/01/11  X  2004/01/14): Guangdong has a suspected case of SARS, which is the first suspected case after SARS was broke out in 2003. Event 4 (2004/03/27  X  2004/03/29): One year after SARS was broke out. Figure 9 corresponds to the events related to the US President George Bush. The patterns between Figure 8 and Figure 9 are very different. Figure 8 has one large major period and a  X  X eep X  hier-archical structure. Figure 9 is flat and each event in the first level lasts for a few days, where most events are related to Iraq War. Features such as iraq , saddam , hussein and weapon are usually associated to the events. Events in late 2004 are mainly related to the 2004 US Presidential Election, where the features associated to him are mostly ker (John Kerry, a presidential candidate), candid (candidate), sen (senate), voter and whit (white house). The fol-lowing is a brief review of these events  X  Event 1 (2003/01/28  X  2003/02/06): President Bush announces that he is ready to attack Iraq even without a UN mandate. Event 2 (2003/02/22): Hans Blix orders Iraq to destroy its Al Samoud 2 missiles by 2003/03/01. Event 3 (2003/03/14  X  2003/03/27): Iraq War begins after Presi-dent Bush delivers an ultimatum to Saddam Hussein to leave the country within 48 hours, but Saddam refuses. Event 4 (2003/04/07  X  2003/04/10): British forces takes c ontrol of Basra and U.S. forces takes control Baghdad. Event 5 (2003/05/29  X  2003/06/01): Weapons of mass destruction (WMD) have not been found, but U.S. secre-tary of state Colin Powell and British prime minister Tony Blair deny that WMD are distorted or exaggerated to justify an attack on Iraq. Event 6 (2003/07/07): Bush administration concedes that ev-idence that Iraq was pursuing a nuclear weapons program, cited in January State of the Union address and elsewhere, was unsubstanti-ated. Event 7 (2003/09/10  X  2003/09/11): Memorial of the Septem-ber 11 Attack. Event 8 (2003/10/05): White House reorganizes its reconstruction efforts in Iraq, placing National Security Adviser Condoleezza Rice in-charge and diminishing the role of the Pen-tagon. Event 9 (2003/10/23  X  2003/10/24): The Madrid Confer-ence, an international donors X  conference of 80 nations to raise funds for the reconstruction of Iraq. Event 10 (2003/11/07): Japan may send a total of about 1,200 troops and civilian staff to Iraq to help rebuild the country. Event 11 (2003/12/17): US President Bush against Taiwan separatists to change the status quo. Event 12 (2004/01/23  X  2004/01/28): The United States asks the UN to in-tercede in the dispute over the elections process in Iraq and report for no WMD has been found in Iraq and that prewar intelligence was  X  X lmost all wrong X . Event 13 (2004/03/07  X  2004/03/08): The Iraqi Governing Council signs interim constitution. Event 14 and Event 15 (2004/10/02  X  2004/10/24 and 2004/10/29  X  2004/11/22): The 2004 United States presidential election.
In general, the events identified are justifiable. The documents in each group share a high degree of similarity (they are discussing the same event), and the features that are associated with the event can represent it. According to our experiences, as well as the evidences, the bursty patterns of some features are very similar, such as SARS and Iraq . SARS burst out from late March to early July, whereas Iraq burst out from late February to early May. Yet, these two fea-tures will never be grouped together according to our experimental results. This is because in Section 2.3, we do not simply consider the bursty patterns of two features. We further implement a t -test based heuristic for determining whether two sets of documents are similar with each others. Furthermore, a feature can be assigned to multiple events. This is different from some of the existing works, such as [7], where a feature can only be assigned to one event.
Although there are advanced systems built for the purpose of browsing information in a text corpus [13, 23, 35], most of them do not include the time dimension. Although some works make use of the time dimension [10, 31], they seldom discuss how their pa-rameters are estimated. Topic detection and tracking (TDT) is the major area that tackles the problem of discovering events from the news articles. Most of them focused on online detection [1, 2, 4, 28]. For the offline ones [32, 38, 37], they did not target organiz-ing the events into hierarchical structures, but aimed at ranking the events in text corpus without human interleave. [9] proposed an al-gorithm for constructing a hierarchical structure for the features in the text corpus by using an infinite-state automaton. Our problem is different from [9] since we are aiming at constructing event hier-archies, but not feature hierarchies. [21, 27, 30, 31] proposed using -test to construct an overview timeline for the features in the text corpus. Our outcome is different from theirs as we construct hierar-chical structures, but their structure is flat. [33] proposes methods for mining knowledge from the query logs of the MSN search en-gine by building a time-series for each query term based on the similarity of the time-series patterns, but did not pay attention to the contents of the web pages. We find the bursty events based on both the time and the content information. [11] proposed a model for searching the features where t he features are used to identify whether a specific message is a response to others. This model is based on analyzing the dynamics of sending-receiving message over some time intervals. Grosz and Sidner [8] proposed a model that organizes documents in a nested structure. From a high-level point of view, all of these works are trying to extract meaningful structures in the data. Yet, their focuses and ours are different. We are neither targeting organizing the documents, nor aiming at detecting the relationships among documents. For the problem of extracting features that are associated to a particular event, it is sim-ilar to that of identifying features that are related to a search query. Query expansion [22], query feedback [25] and query refinement [14] are some popular techniques. Similar to these approaches, our proposed algorithm is based on the co-occurrence of features. Un-like these approaches, we take the dimension of time and the period of events into consideration.
We proposed an algorithm called tim e driven documents-partition (TDD) to construct an event hierarchy in a text corpus based on a user query. An event is an object which consists of: (1) A set of documents with similar contents; (2) A set of representative fea-tures extracted from the documents; and (3) The period of the event. Given a query, TDD will: (1) Identify the features, called bursty features, that are related to the query according to the timestamps and the contents of the documents; (2) Extract the documents that are highly related to the bursty features based on time; (3) Partition the extracted documents and construct an event hierarchy. Event hierarchies can assist us to locate our target information efficiently. Without them, it is very difficult to identify what are the major events that are related to the query, the periods when the events happened, as well as the features and the documents that are re-lated to each of the events. Extensive experiments are conducted to evaluate TDD. The results indicated that it is practically sound and highly effective.
