 Recent statistical evidence has shown that a regression model by incorporating the interactions among the original covariates (fea-tures) can significantly improve the interpretability for biological data. One major challenge is the exponentially expanded feature space when adding high-order feature interactions to the model. To tackle the huge dimensionality, Hierarchical Sparse Models (HSM) are developed by enforcing sparsity under heredity structures in the interactions among the covariates. However, existing methods only consider pairwise interactions, making the discovery of important high-order interactions a non-trivial open problem. In this paper, we propose a Generalized Hierarchical Sparse Model (GHSM) as a generalization of the HSM models to learn arbitrary-order inter-actions. The GHSM applies the ` 1 penalty to all the model coeffi-cients under a constraint that given any covariate, if none of its as-sociated k th-order interactions contribute to the regression model, then neither do its associated higher-order interactions. The re-sulting objective function is non-convex with a challenge lying in the coupled variables appearing in the arbitrary-order hierarchical constraints and we devise an efficient optimization algorithm to di-rectly solve it. Specifically, we decouple the variables in the con-straints via both the GIST and ADMM methods into three subprob-lems, each of which is proved to admit an efficiently analytical so-lution. We evaluate the GHSM method in both synthetic problem and the antigenic sites identification problem for the flu virus data, where we expand the feature space up to the 5th-order interactions. Empirical results demonstrate the effectiveness and efficiency of the proposed method and the learned high-order interactions have meaningful synergistic covariate patterns in the virus antigenicity. High-Order Interaction; Heredity Structure; Hierarchical Sparsity
Fitting a linear regression model to the response based on a num-ber of covariates (features) is a commonly used tool in statistical analysis. However, in numerous situations, a linear model on the covariates may be not sufficiently enough to provide comprehen-sive explanations for the data and to make accurate predictions. For example, in the influenza antigenic sites identification problem, a mutation at an individual antigenic site (covariate) is less determin-istic to change the phenotypic behavior (antigenic change) of the influenza virus. Instead, multiple simultaneous mutations at dif-ferent antigenic sites will significantly enhance the antigenic drift, and strong interactions among the antigenic sites are observed dur-ing the virus evolution.

Actually, recent statistical results have shown that studying the feature interactions in a learning model can significantly enhance its interpretability for the data and improve the prediction accuracy [6, 15, 13]. Generally, the interaction effects are represented as the elementwise product among the covariates and for example, the second-order interaction between two covariates x i and x resented by their elementwise product x i x j . Hence, the inter-actions can encourage capturing the nonlinearity in the data. The interactions among covariates have been found to play an impor-tant role in various areas. For example, strong evidences have been found in [2] that the genetic-environmental interactions have signif-icant effects on conduct disorders, and similar results are reported in [5] that the genetic environmental interactions in serotonin sys-tem are highly correlated with the adolescent depression. More-over, in [19], considering the interaction between the continuance commitment and affective commitment is shown to be effective in predicting the absenteeism. Recently, in the antigenic sites identifi-cation problem [23], interactions among co-evolved antigenic sites are proved to be critical to quantify the impact of multiple simulta-neous mutations.

As the study of the interactions among covariates gains increas-ing attentions, a major challenge is the exponentially expanded fea-ture space. That is, when considering the k th-order interactions among covariates, the number of interactions is O ( d k ) with respect to the d covariates. Such a large number of interactions make the learning model computationally demanding even when d and k are very small. One promising strategy is to exploit sparse structure under this scenario, since only a subset of the covariates and the interactions could be of interest. A simple way is to directly ap-ply the Lasso [21] method by treating all the covariates and the interactions equally, which is referred to the all-pairs Lasso [1, 23]. Furthermore, since the interaction effects are generated from the covariates and the higher-order interaction effects originate from lower-order ones, logical heredity relationship among those effects could be taken into account instead of treating them equally.
In order to make use of the heredity structure, statisticians fa-vor the sparsity which obeys certain logical heredity constraints, referring to the situation that if a set of parameters are estimated as zeros, then the set of its dependent parameters based on some cer-tain heredity relationship should also be set to zeros. Accordingly, a number of Hierarchical Sparse Models (HSM) have been devel-oped. For example, in [15], a convex Lasso-style method named VANISH is proposed by enforcing the strong heredity constraint for the second-order interactions that if a second-order interaction is added to the model, then both the corresponding covariates must be included as well. Consequently, many convex formulations, in-cluding the glinternet [11], GRESH [16], FAMILY [9], and the hi-erarchical sparse model [22], incorporate the strong heredity into the second-order interactions with the heredity structure achieved via the group sparsity [25], where the covariate and interactions restricted by a heredity constraint form a group. Similar consider-ations are discussed by [26, 24]. On the other hand, in contrast to those convex models, the SHIM method [3] adopts a non-convex formulation to achieve the strong heredity by decomposing the co-efficient of each interaction into a product of the coefficients for the covariates. In addition to the strong heredity, there is another type of hierarchical relation, the weak heredity , which introduces a constraint that a pairwise interaction is considered if either of its corresponding covariates was included. Both the strong and weak heredity are investigated in [1] and an efficient algorithm to handle the weak heredity is introduced in [12].

So far, many interests have been focused on exploring the sparse heredity in the interaction model but none of them can deal with general hierarchies, since all of them study only the second-order interactions and their algorithms are particularly designed for the second-order interactions. On the other hand, there have been suf-ficient evidences to indicate that higher-order interactions are more important in many applications. For example, in psychological analysis, the third-order interactions among covariates have been shown to be important [4]. Specifically, in antigenicity analysis of influenza virus, a recent study on proteins of the H3N2 influenza virus shows that more than two of the amino acid positions could mutate simultaneously [17] and biological evidences in [14] also demonstrate that the co-evolved antigenic sites are more likely to be physically close in the 3D structure of the protein.

Unfortunately, due to the difficulty in defining and learning with the high-order heredity, we are unaware of any existing work that can deal with general hierarchies with the order of feature inter-actions larger than two and there is even no formal definition for the arbitrary-order heredity. In this paper, we propose a General-ized Hierarchical Sparse Model (GHSM) to tackle arbitrary-order interactions among features. We first introduce the definition of the arbitrary-order heredity , which makes an assumption that given any covariate, if none of its associated k th-order interaction effects contribute to a learning model, then neither do its associated higher-order interaction effects. Based on this definition, we formulate the GHSM model by applying the ` 1 penalty to all the coefficients under certain hierarchical chain constraints, which guarantee the arbitrary-order heredity. The resulting problem is non-convex and not easy to be optimized since the number of variables in the op-timization problem increases dramatically when the order of inter-actions becomes bigger, which poses a computational challenge. To optimize the objective function, we use the GIST method [7] where the proximal operator is solved by the ADMM method. In the three subproblems of the ADMM method, the first two need to solve quadratic programming problems and the last one is a least square problem with a hierarchical chain constraint. After analysis, we show that all the three subproblems admit efficiently analytical solutions. In the experiments, we evaluate the GHSM method in both synthetic problem and the antigenic sites identification prob-lem in influenza virus data, and empirical results show that the GHSM method can capture meaningful synergistic covariate pat-terns, which can be well explained by biological knowledge.
Throughout this paper, we use regular letters to denote scalars, bold-face and lowercase letters for vectors, and bold-face and up-percase letters for matrices or tensors. Suppose the data matrix for training is denoted by X = ( x 1 ,  X  X  X  , x d )  X  R n  X  d , where n is the number of samples, d is the feature dimensionality, and x the values for the i th covariate in the n data samples. The response vector is y  X  R n . The second-order interaction models [15, 3, 1, 11, 12, 16, 9, 22] commonly consider the following regression model: where denotes the elementwise product between vectors,  X   X  R d with  X  i as its i th element is the coefficient vector for the covari-ates,  X   X  R d  X  d with  X  i,j as its ( i,j ) th element is the coefficient matrix for the pairwise interaction effects, and  X   X  N ( 0 , X  a Gaussian noise vector. In the existing works, two types of hered-ity structure are considered for the second-order interactions, i.e. the strong heredity and the weak heredity, whose definitions are as follows: Based on Eqs. (2) and (3), the HSM methods introduced in [1, 12], named as the strong and weak hierNet, explicitly enforce the heredity structure by adding inequality or symmetry constraints to the Lasso method as where l (  X  ) is a loss function based on Eq. (1),  X  is a regularization parameter that controls the sparsity, N d denotes the set of integers { 1 ,  X  X  X  ,d } , k X k 1 denotes the ` 1 norm of a vector or matrix, and  X  i,  X  denotes the i th row of  X  . The only difference between prob-lems (4) and (5) is the existence of the symmetry constraint on  X  . It is not hard to see that the constraints in problems (4) and (5) can guarantee the strong and weak heredity defined in Eqs. (2) and (3). The strong and weak hierNet methods are representatives of the second-order HSM methods which are explicitly enforced to obey the heredity structure.

In the hierNet models, only the second-order interaction is con-sidered. As discussed in the previous section, higher-order interac-tions are important to model the biological data. To the best of our knowledge, there is no work to even define the high-order interac-tions. In the next section, we will first provide a formal definition of the arbitrary-order heredity and then introduce our method to model the arbitrary-order interactions.
Here we consider up to the K th-order interactions among the covariates ( K d ), and the regression model is formulated as notes a data vector for the k th-order interaction corresponding to  X  i ,  X  X  X  ,i k  X  , an interaction index  X  i 1 ,  X  X  X  ,i k  X  , where i i , is an index to uniquely indicate the interaction among the co-responds to one model coefficient and hence the number of model parameters is reduced from O P K k =1 d k to O P K k =1 d
Based on the regression model in Eq. (6), we will define the arbitrary-order heredity in this section. We first introduce some no-tations. For i 1 &lt;  X  X  X  &lt; i k and j 6 X  { i 1 ,  X  X  X  ,i  X  j and j  X  X  i 1 ,  X  X  X  ,i k  X  are the indices for the interaction effect among covariates i 1 ,  X  X  X  ,i k and j , which adds j into  X  i by preserving the ascending order. Similarly,  X  i 1 ,  X  X  X  ,i fines the index by removing the element j from  X  i 1 ,  X  X  X  ,i j  X  X  i 1 ,  X  X  X  ,i k } .

If we follow the concept of the strong and weak heredity for the second-order case in Eqs. (2) and (3), a straightforward definition for the strong arbitrary-order heredity can be formulated as or equivalently, Similar extension can be made for the weak heredity for arbitrary-order case as well. Unfortunately, the above definition will lead to P considered, which makes the problem intractable to be solved.
So, in order to represent the heredity structure in arbitrary-order case, we propose a more concise and intuitive definition as follows. First we define an ordering of the elements in  X  ( k ) in a way follow-ing the index principle in Table 1. That is, the 1st element in  X  indicates the element with the interaction index  X  1 ,  X  X  X  ,k  X  1 ,k  X  , the 2nd element in  X  ( k ) indicates the one with the interaction index  X  1 ,  X  X  X  ,k  X  1 ,k + 1  X  , and so on. Based on this ordering, we define e i appears in an interaction index of  X  ( k ) , then the corresponding element in e ( k ) i is set to 1 while the rest entries in e Then we introduce the definition for the arbitrary-order heredity.
D EFINITION 1 (A RBITRARY -O RDER H EREDITY ). Given the regression model in Eq. (6), when up to the K th-order interactions among the covariates ( K  X  2 ) are considered, the heredity among the K orders is defined as
In Definition 1, for any covariate i , if none of its associated k th-order ( k &lt; K ) interaction terms contribute to the regression model, Figure 1: A pictorial illustration of the relationship among the strong, then neither do its associated higher-order interaction terms. The arbitrary-order heredity poses constraints on sets of variables as-sociated with each covariate i instead of each individual interac-tion coefficient and hence it only leads to ( K  X  1) d constraints, whose size is much smaller than P K  X  1 k =1 d k , which is the size of constraints induced by the straightforward strong arbitrary-order heredity discussed before, making the learning model have much lower complexity as we will see later.

When K is set to 2 , it is easy to see that the arbitrary-order hered-ity in Definition 1 degenerates to the strong heredity in Eq. (2) and hence the arbitrary-order heredity is a generalization of the strong second-order heredity. Fig. 1 gives an example to explain the rela-tionship among the strong, weak and arbitrary-order heredity where the circle denotes the coefficient of a covariate  X 1 X , ovals denote the coefficients of the interactions involving the covariates  X 1 X , and the red arrows indicate the heredity. From the figure, we see that when K = 2 , the top 2 layers in the arbitrary-order heredity degener-ates to the strong heredity structure by eliminating the redundancy among the model coefficients.
Based on Eq. (6) and Definition 1, we formulate the GHSM model for up to the K th-order interactions as where  X  and  X  are two regularization parameters controlling the sparsity and the decay in the coefficients for interactions of differ-ent orders,  X  denotes the set of parameters {  X  ( k ) } K is a loss function for regression such as the square loss defined as k X k 2 denotes the ` 2 norm of a vector. In problem (7), the con-straints associated with each covariate i have a chain of inequality constraints, which contains ( K  X  1) inequality constraints and there are a total of d chains. It is easy to see that these constraints achieve the arbitrary-order hierarchy in Definition 1.
Problem (7) is non-convex due to the chains of inequality con-straints. Moreover, the variables are coupled in different chains of sociated with the covariates 1 , 2 , 3 respectively, which makes the problem more complex. In the next section, we propose an effi-cient algorithm to solve problem (7).
In this section, we introduce the optimization algorithm to solve problem (7). The main idea is to combine proximal gradient meth-ods and the ADMM. Since problem (7) is non-convex, we adopt the GIST method [7] whose entire algorithm is shown in Algorithm 1. We define r (  X  ) as Then, the proximal operator at the ( t + 1) th iteration in the GIST method solves the following problem: where [  X  ( k ) ] ( t ) denotes the estimation of  X  ( k ) v gradient of L (  X  ( t ) ) with respect to  X  ( k ) at  X  ( t ) Z method by satisfying the following condition: where F (  X  ) = L (  X  ) + r (  X  ) and  X  is a constant in (0 , 1) . Then, the GIST algorithm iteratively solves the proximal problem (8) un-til convergence.
 Algorithm 1 The GIST algorithm for solving problem (7).
 2: repeat 4: repeat 7: until condition (9) is satisifed; 8: t = t + 1 ;
The key problem in Algorithm 1 is to solve the proximal problem (8). Since r (  X  ) is an extended real-value function, problem (8) can be reformulated as where we omit the iterative index t for notational simplicity. Prob-lem (10) is still non-convex due to the chains of inequality con-straints. However, the following theorem shows that problem (10) admits an equivalently convex formulation. 1
T HEOREM 1. Let  X   X  ( k ) = |  X  ( k ) | , where the operator | X | de-notes the elementwise absolute operator on a vector. Then problem (10) is equivalent with the following convex optimization problem: where 1 denotes a column vector of all ones with appropriate size,  X  v than X  operator. The solution of problem (10) can be obtained as  X  ( k ) = sign( v ( k ) )  X   X  ( k ) for k  X  N K , where sign(  X  ) is the ele-mentwise sign operator.

It is easy to find that problem (11) is a quadratic programming problem, hence many off-the-shelf solvers for convex program-ming can be used directly to obtain the optimal solution. Neverthe-less, instead of using these tools, we propose an efficient algorithm to solve problem (11) by taking advantage of the chain structure in the constraints. In problem (11), the variables are coupled together in the chains of inequality constraints. In order to decouple these parameters, we use the ADMM method to solve problem (11) by introducing new variables.
 Then, problem (11) can be reformulated as where p (1) i is the i th element in p (1) and q j,i is the j th element in q . Based on problem (12), we define the augmented Lagrangian function as where  X   X  denotes the set of parameters {  X   X  ( k ) } K and { b i } d i =1 are the Lagrangian multipliers, and  X  1 penalty parameters. Then we need to solve the following problem: The above problem can be solved via the ADMM algorithm pre-sented in algorithm 2, in which three subproblems in steps 4, 5 and 6 need to be solved. In the next two sections, we will show how to solve those subproblems efficiently. Algorithm 2 The ADMM algorithm for solving problem (12).
 2: Set  X  = 0 . 1 and t = 0 ; 3: repeat 9: t = t + 1 ; 10: until Some convergence criterion is satisfied;
In step 4 of Algorithm 2, with fixed P and Q , the problem with respect to  X   X  can be rewritten as which can be easily solved via the following analytical solution:
Given fixed  X   X  , Q and p (1) , the problem with respect to p p ( K ) corresponding to step 5 of Algorithm 2 can be decomposed into K  X  1 separable problems with the problem for p ( k ) lated as where I is an identity matrix with appropriate size, the matrix E Note that H ( k ) is positive definite (PD). Hence, H ible and its inverse can be computed efficiently as [ H is actually taken on a d  X  d matrix I + [ E ( k ) ] &gt; the original d k  X  d k matrix H ( k ) . Then, the optimal solution of problem (15) can be computed as Moreover, given the data, E ( k ) ( I + [ E ( k ) ] &gt; E a constant matrix and so it can be computed only once and stored prior to the model learning. As a consequence, the analytical so-lution in Eq. (16) only takes O ( d d k ) time complexity for each [  X   X  ( k ) ]  X  , which is almost linear with respect to the number of the k th-order interactions.
With fixed  X   X  and p (2) ,  X  X  X  , p ( K ) , the problem w.r.t. Q and p can be formulated as Problem (17) can be decomposed into d independent problems with the i th one formulated as where  X   X  (1) i , a (1) i are the i th elements in  X   X  (1) and  X  k,i , b k,i are the k th elements in  X  i and b i respectively. In problem (18), the chain of inequality constraints still exists. We first rewrite this problem into a more general formulation as where problem (18) is a special case of problem (19) by setting s = ( p  X 
In the following, we generalize our previous results in [8] to show that an efficient solution exists for problem (19). We first introduce two useful lemmas to reveal some interesting properties of problem (19).
 L EMMA 1. In problem (19), the following properties hold: (1) If u 1  X  u 2  X   X  X  X   X  u K , then the optimal solution ( s  X  X  X  ,s  X  K ) is ( u 1 ,u 2 ,  X  X  X  ,u K ) ; (2) If u 1  X  u 2 the optimal solution ( s  X  1 ,s  X  2 ,  X  X  X  ,s  X  K ) is ( u u identical elements u  X  .

L EMMA 2. For any two sets of inputs { ( u 1 ,  X  X  X  ,u l ) , (  X   X  stances of problem (19), if the optimal solutions for them are (  X  u  X  u  X   X   X  u  X  , the optimal solution for the problem defined by the con-catenated sequence ( u 1 ,  X  X  X  ,u l ) ./ ( u l +1 ,  X  X  X  ,u nated weights (  X  1 ,  X  X  X  , X  l ) ./ (  X  l +1 ,  X  X  X  , X  n ) is (  X  u (  X  u  X  ,  X  X  X  ,  X  u  X  ) | n  X  l ; (2) Otherwise, i.e.,  X  u  X  lution for the problem defined by the concatenated sequence is ( u
Lemma 2 implies that we can immediately obtain the solution of problem (19) defined by the input ( u 1 ,  X  X  X  ,u n ) and (  X  if ( u 1 ,  X  X  X  ,u n ) is a concatenation from two sub-sequences and the optimal solutions corresponding to problems defined by the two sub-sequences have solutions with identical values. Therefore, based on Lemma 2, we devise Algorithm 3 to solve problem (19) with its optimality guaranteed by the following theorem.

T HEOREM 2. For problem (19) defined by ( u 1 ,  X  X  X  ,u K ) and (  X  1 ,  X  X  X  , X  K ) , Algorithm 3 finds its optimal solution.
We analyze the time complexity of the whole optimization pro-cedure for solving the GHSM model. We first discuss the time complexity of the inner most Algorithm 3. In Algorithm 3, step 1 only needs to scan the input sequence ( u 1 ,  X  X  X  ,u K ) once and thus it needs O ( K ) time. Although there exist two loops from step 3 to step 14, the maximum number of the concatenation operations in step 8 is M  X  1 . For the concatenation operation, according to Lemma 2, it only needs to compute the weighted average of the Algorithm 3 The algorithm for solving problem (19).
 3: for m = 2 : M do 5: while there are at least two sequences in the stack do 6: Pop the first and second sequences from the stack and denote the 8: Concatenate the two sequences under the second condition in 9: else 10: Push the two sequences back into the stack without any 11: Break; 12: end if 13: end while 14: end for 15: Concatenate the solutions of the sequences in the stack from bottom to entries in two sequences and we just need to record the weighted average and the sum of the weights in each sequence, making each concatenation operation cost O (1) . Since M  X  K , the complexity of Algorithm 3 is O ( K ) .

In Algorithm 2, solving the subproblem in step 4 by using Eq. (14) requires O P K k =1 d k time. The computation of step 5 by using the closed-form solution in Eq. (16) takes O d P K k =2 time. The computation in step 6 needs to execute Algorithm 3 for d times, and hence the time cost is O ( dK ) . Usually, we have K d and hence dK &lt; d P K k =2 d k . So the total time complexity of each iteration in Algorithm 2 is O d P K k =2 d k .

By assuming that Algorithms 1 and 2 need N 1 and N 2 iterations to converge respectively, the total time complexity for solving the GHSM model is O N 1 N 2 d P K k =2 d k , which is almost linear with respect to the total number of interaction effects P our experiments, we empirically find that both N 1 and N 2 when convergence. Hence, the overall algorithm for solving the GHSM model is very efficient. Moreover, according to Sections 4.2 and 4.3, the problems in steps 4, 5 and 6 of the ADMM algorithm can be decomposed into a number of independent problems, which can be parallelized to further improve the efficiency.
In this section, we empirically evaluate the proposed GHSM method and compare with a large number of the state-of-the-art methods for hierarchical sparse modeling. Specifically, the com-petitors include (1) Lasso [21], which is the sparse model using covariates only by applying the ` 1 penalty on the model coeffi-cients; (2) All-Interactions Lasso (AIL-k ), which is the sparse in-teraction model using up to the k th-order interactions based on the ` 1 penalty. We concatenate all the effects together to form a new data matrix and treat it as a Lasso problem; (3) weak hierNet (w-hierNet) [1], which is the HSM method using up to the 2nd-order interaction effects with the weak heredity, i.e., solving the problem in Eq. (5). Its R package  X  X ierNet X  is available in  X  X RAN X ; (4) eWHL [12], which is an efficient implementation for the w-hierNet method proposed in [1]; (5) strong hierNet (s-hierNet) [1], which is the HSM method using up to the 2nd-order interaction ef-fects with the strong heredity, i.e., solving the problem in Eq. (4). Its R package  X  X ierNet X  is available in  X  X RAN X ; (6) FAMILY [9], which is a convex HSM model using up to the 2nd-order interaction effects with the strong heredity, where the sparsity is achieved by using the group lasso. Many methods [11, 16, 9, 22] have similar ideas and we select the FAMILY method as a representative. Its R package  X  X AMILY X  is available in  X  X RAN X ; (7) GHSM-k , which is the proposed GHSM models that can deal with up to the arbitrary k th-order interaction effects.

The experimental platform is a 64-bit machine with 2.2 GHz quad-core Intel Core i7 CPU and 16 GB memory.
In this section, we conduct experiments on synthetic datasets.
To study different orders of interactions, we generate 3 synthetic datasets with the highest order being K = 3 , 4 , 5 respectively. In all the datasets, the number of training samples n is set to 200 and the number of covariates d is 20. Each entry in the data matrix for training, X  X  R n  X  d , is sampled from the standard normal distribu-tion. The k th-order interaction matrix Z ( k )  X  R n  X  ( d erated with its column indices following Table 1. All the columns of matrices X and Z ( k )  X  X  are normalized to have zero mean and unit variance.

In all the 3 datasets, we set the first half of the coefficients with respect to  X  (1) to be 1 and the rest to be 0. For the first dataset with K = 3 , the indices of the non-zero coefficients in the second-order i.e., there are 5 non-zero coefficients in  X  (2) . Similarly, we set the indices of the non-zero coefficients in the third-order interactions as entries in  X  (3) are non-zero. All the non-zero entries in  X   X  (3) are set to 0 . 5 . For the second dataset with K = 4 , we keep the settings for the orders up to the third order as in the case K = 3 and set the indices of the non-zero coefficients in the 4th-order interac-to 4 non-zero coefficients in  X  (4) . All the non-zero entries in  X  are also set to 0.5. For the third dataset with K = 5 , we adopt the same settings for up to the fourth order as in the case K = 4 and set the indices of non-zero coefficients in the 5th-order interactions having values of 0.5. It is easy to check that the above settings of the model coefficients satisfy the arbitrary-order heredity. Finally, the response vector y is constructed as y = P K k =1 Z ( k ) where Z (1) = X and  X  N ( 0 , I / 4) . The statistic of the interac-tion effects used in the synthetic data is given in Table 3.
For all the methods in comparison, we choose their regulariza-ing additional 200 data samples as a validation set. For the GHSM method, there is another regularization parameter  X  , which is se-lected from a set { 1 , 2 , 10 } . To measure the performance of differ-ent methods, we use the sensitivity (Sen.) and the specificity (Spe.) [12], where non-zero entries in the corresponding coefficient vector are treated as positive and zero entries are negative, for each order of interactions to test the recovery performance on the model co-efficients and use the root mean square error (RMSE) on a test set having 200 samples for each setting. For each setting, we repeat each configuration for 10 times and report the average results. Table 3: The statistic of the effects in the synthetic study.  X # X  indicates
The detailed results are shown in Table 2. The compared meth-ods can be divided into 5 groups, i.e., { Lasso } , { AIL-2, w-hierNet, eWHL, s-hierNet, FAMILY, GHSM-2 } , { AIL-3, GHSM-3 } , { AIL-4, GHSM-4 } and { AIL-5, GHSM-5 } , according to the highest or-der of interactions that they can handle. From the results, we have several observations: (1) All the methods in comparison can cor-rectly detect the useful covariates, since all the Sen. values in the column for the 1st-order interactions are 1 X  X . Similar results are also observed for the 2nd-order interaction models; (2) In each syn-thetic dataset, the GHSM method has better recovery performance for different orders of interactions compared with other methods; (3) The prediction performance of each method is usually propor-tional to its capacity. That is, the methods that can learn higher-order interactions will achieve lower RMSE for prediction; (4) In all the three datasets, the GHSM method always has the best predic-tion performance and it significantly outperforms the Lasso and the existing second-order interaction models; (5) In the third synthetic dataset, the AIL-5 method performs even worse than the AIL-4 method. One possible reason is that the AIL method does not capture the sparse heredity structure, making it hardly detect the correct higher-order interactions. One evidence is that the AIL-5 method only recovers 58% and 25% of the correct 4th and 5th or-der interactions, respectively, while our GHSM-5 method achieves 78% and 80%.

The synthetic study demonstrates that as long as significant high-order interaction effects exist in the data, the high-order interaction methods will have better performance compared with the conven-tional second-order interaction methods, and the proposed GHSM method with high-order interactions can learn the sparse heredity structure and accurately detect those interactions, leading to im-proved prediction performance.
In this section, we study the application of the GHSM method on the antigenic sites identification problem.
Seasonal influenza A viruses pose great threats to public health, while the vaccination is the primary way to reduce this risk. An effective vaccination program requires an antigenic match between circulating viruses and vaccine strains to be used, and hence a timely identification of emerging influenza virus antigenic variants is crit-ical to the success of influenza vaccination programs. Recent stud-ies have suggested that multiple interactive antigenic sites muta-tions will significantly enhance the antigenic drift of the influenza viruses to new variants [17, 10, 23]. However, discovering the im-portant interactive patterns among the antigenic sites is not trivial.
In this problem, each site is treated as a covariate of the anti-genic distances which are the responses, hence identifying interac-tive patterns among multiple antigenic sites can be formulated as an interaction model. Here we apply the proposed GHSM method to identify interactive antigenic sites on an influenza H3N2 virus dataset [20, 23]. 2 This dataset collects the results from the hemag-glutination inhibition (HI) assays, which is a matrix recording the reaction values between 192 viruses in rows and a number of test serums in columns. The 192 H3N2 influenza A viruses are col-lected during year 2004 to 2007. These reaction values in the HI matrix describe the virus antigenicities, and the Euclidian differ-ence between the reaction values of two viruses describes the anti-genic distance between them. A large antigenic distance may in-duce an antigenic drift and sometimes cause influenza outbreaks due to viral escape from existing immunity. Therefore, accurately predicting the antigenic distances among viruses is a fundamental task. In this dataset, for each virus, its hemagglutinin (HA) protein sequence, i.e., a sequence of amino acid sites (covariates) that are responsible for antigenic changes, is also collected. The number of amino acid sites in the HA sequence of each virus is d = 329 . By comparing the sites in any two HA sequences, we could obtain a difference vector, in which the unchanged positions have zero val-ues and the mutated positions have integer values between 1 and 5, which is computed via the pattern-induced multi-sequence align-ment (PIMA) scheme [20, 23]. Then, each pairwise difference vector is treated as a data sample in the data matrix X , and each pairwise antigenic distance obtained from the HI reaction values is used as a response value in the target y . So there are a total number of 192 2 = 18336 samples.

Since there are 329 features, the number of interaction effects in-creases drastically with respect to the order. Table 4 gives statistics of the effects in this data. When we consider the 3rd-and 4th-order interactions, the dimension of the entire feature space is around hundreds of millions, which leads to heavy computational demand. Fortunately, in each data sample, we find that only few mutated po-sitions have non-zero values and hence the data matrix X is sparse. As a consequence, a large number of the interaction effects are use-less zero vectors since they are obtained via the product among sparse inputs and we can eliminate them before learning. The num-ber of valid effects for each order is given in Table 4. Here we focus on up to the 4th-order interactions since higher-order interactions cannot bring too much performance improvement.

The entries in X and Z ( k )  X  X  are normalized into [0 , 1] and each entry in y is log-transformed and normalized into [0 , 1] as well. The regularization parameters for different methods are selected from a set { 10  X  5 , 10  X  4 ,  X  X  X  , 10 3 } via the 5-fold cross validation. The parameter  X  in GHSM methods is chosen in the same way as that in the synthetic data. We randomly split the dataset into a training set and a test set by varying the training ratio from 10% to 90% at an interval of 20%. Each setting is repeated for 10 times. We report both the predictive RMSE on the test set and the running time for all the methods. The predictive RMSE X  X  for different methods are presented in Table 5. From the results, we observe that all the interaction mod-els remarkably outperform the Lasso method, implying that incor-porating the feature interactions is important in this dataset. Dif-ferent from the synthetic case, the AIL methods do not show much improvement over the Lasso and this is possibly because the AIL methods can not make use of the sparse heredity structure and the interactions among the antigenic sites are much more complex than the synthetic case. For the second-order interaction models, the strong heredity based methods, i.e., the s-hierNet, FAMILY, and GHSM-2, show better performance than the weak heredity based methods including the w-hierNet and eWHL methods. This could be an evidence that the strong heredity structure is more useful for this problem. Similar to the results in the synthetic case, the GHSM method with higher-order interactions obtain more accurate pre-diction results and the GHSM-4 method performs the best in all settings.

In addition to the RMSE, we also provide visualizations for the prediction results of different methods via the antigenic cartogra-phy, which is an approach to visualize the virus antigenic evolution process on a 2D/3D space by using the antigenic distance among the viruses and has been widely used for virus antigenicity expla-tography is to utilize the multi-dimensional scaling technique to obtain the coordinates of each virus on a 2D/3D space given the antigenic distance among them. We plot the embedding results in Fig. 2 when the training rate is 10% and use the predicted anti-genic distance to reconstruct the cartography of all the viruses. In Fig. 2(a), the cartography using the true antigenic distance among the viruses is plotted in a 2D space, where each circle represents a virus and all the 192 viruses are divided into four different groups according to the year of their appearance. Figs. 2(b)-2(h) show the cartographies of different methods respectively. Since the car-tographies of the AIL methods are similar to that of the Lasso and the eWHL and w-hierNet methods are two solutions of the same model, we omit their cartographies. By comparing with Fig. 2(a), we observe that the Lasso method can hardly reconstruct the anti-genic distances among the viruses. The second-order interaction models including the w-hierNet, s-hierNet, FAMILY and GHSM-2 methods show clearer reconstruction but the viruses in different years are still hard to be distinguished. The group structure in the cartographies of the GHSM-3 and GHSM-4 methods is much bet-ter than others, which can be confirmed by the Pearson correlation coefficients reported in Fig. 2.

One advantage of the GHSM method is that we can identify important high-order interactions based on the model parameters. Specifically, we take the 4th-order interactions learned from the GHSM-4 method for example to see which 4th-order interactions are detected by the algorithm. We sort the magnitude of the co-efficients for the 4th-order interactions in a descending order and then select the top 5 interactions, which are  X  157 , 159 , 242 , 246  X  ,  X  94 , 145 , 189 , 198  X  . It is well-known in the H3N2 virus antigenic-ity analysis that there are 135 important antigenic sites out of the total 329 positions identified as the antibody binding sites, since these sites locate at the surface of the H3N2 virus protein struc-ture and they are more likely to react with the sera. These antibody binding sites are further divided into 5 binding areas A-E accord-ing to their locations. Promisingly, we find that all of these detected positions in the 4th-order interactions belong to the binding areas. More interestingly, when we tag the binding areas for these sites in the selected interactions, we get the following patterns:  X  B, B, respectively, from which we see that the antigenicity of the H3N2 virus is more likely to be controlled by the interactions among the sites in different binding regions simultaneously instead of in the same binding region. This observation is reasonable since multi-ple sites from different binding areas can accurately capture the 3D structure (shape) of the virus. Fig. 3 shows the 3D structure of these binding areas with the sites from the selected interactions. So far, all of the above analysis is considerably useful in influenza vaccine strain selection, and it can significantly reduce the human labor efforts for serological characterization and will increase the probability of correct influenza vaccine candidate selection.
Moreover, we compare the training time of different HSM mod-Table 6. For the 2nd-order HSM, the strong heredity based meth-ods, i.e., the s-hierNet and FAMILY, are computationally much more expensive than the weak heredity based methods such as the w-hierNet and eWHL. The FAMILY method fails to give the solu-tion in reasonable time. Actually, the FAMILY method is compu-tationally intractable even when using only 50% of the samples for training (refer to Table 5). The eWHL method is very efficient since it is specifically designed for the w-hierNet model. The proposed GHSM-2 method, a strong heredity based method, is much more efficient than the s-hierNet and FAMILY methods and comparable with the eWHL algorithm. By increasing the order of interactions, the GHSM-3 and GHSM-4 methods are still very efficient com-pared with the w-hierNet, s-hierNet and FAMILY methods, while it has much better performance than those methods.
In this paper, we proposed a generalized hierarchical sparse mod-el to learn arbitrary-order interactions contained in the data via the proposed arbitrary-order heredity structure. An efficient algorithm was developed by decoupling the variables in the complex con-straint and all the subproblems have efficient analytical solutions. Empirical results show the effectiveness of the proposed method.
When considering high-order interactions, if the data matrix are not sparse like the influenza virus data, the number of high-order in-teractions still increases exponentially with respect to the order and solving the GHSM will become intractable even for small d and K . One possible direction to solve this problem is to conduct dimen-sionality reduction methods before learning the GHSM model via, for example, the feature screening technique. We are also interested in applying the GHSM methods to other biological problems, such as the cancer microarray analysis, to detect important interactions. This work was partially supported by NSF IIS-1250985, NSF IIS-1407939, NIH R01AI116744, NSFC-61305071, NSFC-61473087 and NSF of Jiangsu Province (BK20141340). Figure 3: The 3D structure of the H3N2 virus. The red regions denote [1] J. Bien, J. Taylor, and R. Tibshirani. A lasso for hierarchical [2] R. J. Cadoret, W. R. Yates, G. Woodworth, and M. A.
 [3] N. H. Choi, W. Li, and J. Zhu. Variable selection with the [4] J. F. Dawson and A. W. Richter. Probing three-way [5] T. C. Eley, K. Sugden, A. Corsico, A. M. Gregory, P. Sham, [6] J. Friedman, T. Hastie, and R. Tibshirani. The Elements of [7] P. Gong, C. Zhang, Z. Lu, J. Z. Huang, and J. Ye. A general [8] L. Han and Y. Zhang. Learning tree structure in multi-task [9] A. Haris, D. Witten, and N. Simon. Convex modeling of [10] J.-W. Huang, C.-C. King, and J.-M. Yang. Co-evolution [11] M. Lim and T. Hastie. Learning interactions through [12] Y. Liu, J. Wang, and J. Ye. An efficient algorithm for weak [13] D. C. Montgomery, E. A. Peck, and G. G. Vining.
 [14] D. D. Pollock, W. R. Taylor, and N. Goldman. Coevolving [15] P. Radchenko and G. M. James. Variable selection using [16] Y. She and H. Jiang. Group regularized estimation under [17] A. C.-C. Shih, T.-C. Hsiao, M.-S. Ho, and W.-H. Li. [18] D. J. Smith, A. S. Lapedes, J. C. de Jong, T. M. Bestebroer, [19] M. J. Somers. Organizational commitment, turnover and [20] H. Sun, J. Yang, T. Zhang, L.-P. Long, K. Jia, G. Yang, R. J. [21] R. Tibshirani. Regression shrinkage and selection via the [22] X. Yan and J. Bien. Hierarchical sparse modeling: a choice [23] J. Yang, T. Zhang, and X.-F. Wan. Sequence-based antigenic [24] M. Yuan, V. R. Joseph, and H. Zou. Structured variable [25] M. Yuan and Y. Lin. Model selection and estimation in [26] P. Zhao, G. Rocha, and B. Yu. The composite absolute
