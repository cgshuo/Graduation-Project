 the early stages of NLP research. Meanwhile, this task is a the key techniques needed for automatic text very important for to the practice on information retrieval, machine tr -anslation, dictionary compiling, minorties. Typically, POS tagging and chunking are mo -deled in a pipelined way. However, the M oreover POS tagging er rors cannot be corrected by tag ging and structure of chunk .
 In order to avoid error propagation and make use of chunk information for POS tagging, P OS tagging and chunking can be viewed as a single task: given a raw Kazakh input sentence, t he joint POS tagger considers all possible tagged and sequences, and chooses the overall best output. A major challenge for such a joint system is the large search space faced by the dec o der. To deal with the increased search space, we adopt a recently proposed beam -search exten s ion to shift -reduce joint model [ 5 -8 ] , which enables the model to pack equivalent tagger states, i mproving both speed and accuracy. vides the r ich feature information, leading to large improvements over using either the generative mo d-el or the d iscriminative model in term of POS tagging (the mixing model achieves 89.1 % p recision on the Xingjiang Daily Tagging set , compared to figures of 81.4 % and 86.1 % for the HMM molde and M E M odels). In term of chunking, the p recision is large improvements over using either CRF model or ME model. The remaining part of the paper is organized as follows. Section 2 gi v es a brief introdu c-tion to the foregoing method of POS tagging and chunk and propose the mixing model . Section 3 d e-analyses. Section 6 conclude the paper. 2.1 The Pipelined Method r e ti  X  T , 1  X  i  X  n, and T is the POS tag ging set. A chunk sequence is denoted by d = a chunk ( w b... w e ) whose first word of chunk is w b and last word is w e . The pipelined meth -od treats POS tagging and chunking as two cascaded problems. First, an optimal POS tag ging sequence is determined.
 In a perceptron, the score of a tag sequence is : Where refers to the feature vector and is the corresponding weight vector.
 Then, an optimal chunking is determined based on x and . Similar to POS tagging, the score a chunking sequence is 2.2 The Joint Models In the joint method [ 5 -8 ] , we aim to simultaneously solve the two problems. Under the linear model, the score of a tagged sentence sequence is: For simplicity, we denote where means the concatenation of weights of feature .
 Under the joint model, the weights of POS and chunk features, are simultaneously l -earned. We expect that POS and chunk features can interact each other to determine an optim -al joint result. 2 .3 The M ixing M odel In section 2.1, we know that POS tagging and chunking are modeled in a pipelined way. H -a zakh POS tagging is about 89.3 % , which is much lower than that of English and Chinese . S -i multaneously, J oint approach is that since the shift -reduce model processes an input sentence including look -ahead POS results about 1.1 % decrease in POS tagging performance on the d -state -of -t he -art performance. In order to make up for shortcomings p ipelined method and j oint method , we combine the two approaches. First, before we analyze the sentence, we use the method of segmentation with p e -r ceptron a lgorithm (Yue Zhang and Stephen Clark,20 07 ) to identify sentence chunks (not give a as {B, E, M, S}. The tags B, E, M represent the character being the beginning, end, and mi -ngle -word chunk.
 For example :  X  X  X  X  X  X  X  /E  X  X  X  X  X  X  /M  X  X  X  /M  X  X  X  X  X  /B ( table and chair ).
 tem is composed of four states and four actions . Four states, respectively, comprise one queue , e labeling process .
 The main shift -reduce actions are:  X  SHIFT 1 , which choose the next word from queue to POS tagging , then pushes the  X  SHIFT2, which pushes the first word -POS pair in the linked list onto the stack;  X  REDUCE, which predict the position of word in chunk(start,middle and end) . I f the word  X  TERMINATE, which check the linked list 2 is empty. 3.1 Linear Models for NLP le POS tagging . We assume:  X  Training examples  X  xi,yi  X  i = 1...n  X  A function GEN which enumerates a set of candidates GEN(x) for an input x.  X  A representation  X  mapping each (x,y)  X   X  X  X  to a feature vector  X (x,y)  X  R d .  X  A parameter vector  X  R d .  X  The components GEN,  X  and define a mapping from an input x to an output F(x) thro -W here  X (x,y) X  is the inner product  X  , The learning task is to set the parameter v -a lues using the training examples as evidence. The decoding algorithm is a method for se ar -c hing for the arg max in Eq. 7 . 3.2 The Perceptron Algorithm In the 3.1 section , we assume the is the parameter vector in the model. Each element in u re over the whole sentence y. We calculate the value by supervised learning, using the av -e r a ged perceptron algorithm [ 10 -12 ] , given in Algorithm 1 .
 After review the averaged perceptron algorithm, due to i ts convergence properties have a full description, w e now only consider the problem of decode.
 3.3 D ecode Algorithm In section 3.2, we introduced the averaged p erceptron a lgorithm . Note that the most complex step of the method is finding zi =  X  and this is precisely the decodin rove decode performance.
 Beam Search Algorithm a -nd all chunk of unlabe l ed from the sentence.
 At each stage in the decoding process, existing items from the agenda are progressed by appl -put back on the agenda. The decoding process is terminated when the highest scored sta te it t h m 2 .
 Optimized Beam -Search Algorithm In the averaged beam -search algorithm, choose the B highest scoring state items in an agenda l t. If we choose the value of B is too large, while decoding accur acy has improved, but the decoding speed is greatly reduced . The value of B is too small, the opposite effect. Therefore we designed a dynamic value B to solve the above problems.
 At each step in the decoding process, outputs a set of prediction {&lt; &gt;, m chance by setting a reasonable threshold. Moreover, the closer that the value of B r is to 1, the word have greater possibility is a multi -category words in the word tagging process. He -e disambiguation of multi -category words. For chunking, we summary rule to constrain the se -a rch space of our models due to their high complexity. This section gives a full description of the tagging approach. We first describe the disamb i guat d el. 4 .1 Disambiguation of Multi -Category Words i thin the threshold. For mark words, we regard it as a multi -category words . Next, we will i n t roduce the main process disambiguation of multi -category words by using the information of chunking [13] .
 First of all, we extract the struc t ure of basic chunk from the corpus as a set of rule . For exam ple, we statistical induction the strucure of noun phrase shown in following: (1)  X  X  X  X  X  X  X  X  X  X   X  X  ( Male teachers , n + n) (2)  X  X  X  X  X  X  X   X  X  X  X   X  X  X  X  X  ( table and chair, n + conj + n ) (3)  X  X  X  X   X  X  X  X   X  X  X  ( he and me , pron + conj + pron) (4)  X  X  X  X  X   X  X  X  X  X  ( my book, pron + n) (5)  X  X  X  X  X   X  X  X  ( beautiful flower , adj + n) (6)  X  X  X   X  X  X  X  X  X   X  X  X  X  X  X  X  ( respect senior , v + n ) (7)  X  X  X  X  X  X   X  X   X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X   X  X  X  X   X  ( pleased with hold the flower, adj + adv + n) (8)  X  X  X  X   X  X  X  ( push cart , v + n) (9)  X  X  X   X  X  X  5 ( five flower, num + n) We now assume is a marked word ,  X  is ambiguity tag of and is a tag of curr -e nt ch unk that contains the word . Moreover the , represent the corresponding struc t ure of and . Then find corresponding rule sets, traverse the set to check and wh -e ther or not it was in the set . Next we operate in three steps: 1. If there is only one in the set , we th ink that it is POS tagging of . 2. If neither of them not in the set, we mark the word and the chunk as wrong recognition , t hen to further identify them by ME(Maximum Entropy) model . For the word, we can use the information of chunk from the context. 3. If all of them in the set, the word will tagged by ME model again. For t he two score of s t product as tagging of the word. 4 .2 Features For this paper, we wanted to compare the results of a perceptron model with a generative m o -tic, so we modified the feature set from those papers to explicitly include the next unlabel ed ess correlation between the last word of first chunk and first word of second chunk, therefore we have not choose it as feature. Otherwise the features are basically the same as in those p -ls that we will include in our feature sets . Note that we recognize POS and chunk to choose es set . For POS tagging: (1) If Wi is first word of the chunk and the chunk and previous chunk is more than one wo -(2) If Wi is last word of the chunk and the chunk and next chunk is more than one word, t -(3) I f the chunk is only a word. the POS tagging feature template shown in Fig. 2 . ( c ) . For chunking: We set the feature template in Fig. 2 . ( d ) .
 In th ose feature template , The current word the symbols S0 represent the current word , S1, S2, two words in the incoming linked list 1 , and symbols P1 and P1 re present the top two chun k on the stack. L0, L1, L2 represents following two unlabeled chunk of the current word. w r e -p resents word, c represents structure of chunk , t represents the tag for a word or a chunk . 5.1 Experimental Settings The experimental corpus from annotated Xingjiang Daily(Kazakh language version), including t ence s, 231208 words and 83026 chunks.
 POS tagging result, we evaluate the different tag respectively. Then the evaluation f unction is defined as follows:  X  a = number of correct tagging Then : P(Precision) =  X  b = number of undiscerned tagging R(Recall) =  X  c = number of wrong tagging F -measure = 5 .2 Pipelined Model and Joint Model Performance First of all, we evaluate the performance of our pipelined model and joint model annotator de -s cribed in Section 2. The result of pipelined model shown in Table 1 . For joint model, b ased on ou r preliminary experiments, we both set the beam size to 16 and 32 for the joint annot a t -or, the result shown in Table 2 . C ompare to the Table 1 and Table 2, we can see that joint model leading to large improvements over pipelined model . O nly observe the Table 2 , 32 bea -m size compare to 16 beam size, while the precision improved about 0.6% , the s peed decreas -ed approximately 50%.
 5 .3 Development Results In this paper, we propose the dynamic beam to control the search space. For dynamic beam , w e shoul d set the appropriate threshold to improving the efficiency of decoder. In this experi -ot a tor in diff e rent threshold. The dynamic beam curves of the mixing models are shown in Fig. 3 . From the Fig. 4 . ,we i ntegrated consideration the precision and the decode speed choose 6 as the threshold. In addition, we conduct a large number of experiment which discern multi -ca -te gory words to confirm the threshold of best performance. From the Figure , we obtain the best threshold is 3.5.
 In this experiment, recognition rate of multi -category words is improved greatly in the POS ta m the Table 3 , we contrast it with previous work to know that chunk provides the r ich infor -mation, leading to recognition rate of multi -category words large improvements w hether in p re -cision or F -measure. 5 .4 Final Results and Analysis For show the final result, we contrast previous work with mixing model. First of all, we test the POS tagging and chunking which utilize joint model and mixing model respectively. The w n in Table 4 .
 It is noteworthy that we obtained the first positive result that the mixing model does improve POS tagging about 2.8 % , while, remove 6.1 % improved in multi -category words, otherwise on l y have 1.8 % improved. But for the chunking, this is our mixing model is considered to hav e improved the chunking accuracy over the pipelined tagger about 4. 8 . Therefore we should c o -i n best performance. We proposed a joint POS tagging and chunking mixing model, which achieved a considerable reduction in error rate compared to a baseline two stage system. We used a single linear mod -el for combined POS tagging and chunking , and chose the generalized perceptron algorithm for joint training. And beam search for efficient decoding and propose the dynamic beam to imp -r ove the preformence of decode . Moreover t he search space is reduction greatly and precision h e structural relationship of between the chunk and the word to obtain broader and more in depth feature.
 The joint system takes features only from partal context. There may be additional features that are particularly useful to the joint system. Open features, such as knowledge of numbers and relationships from semantic networks [12] , have been repor ted to improve th e accuracy of seg -an obvious next step is the study of open features in the joint POS tagger and chunk reco -gnition .

