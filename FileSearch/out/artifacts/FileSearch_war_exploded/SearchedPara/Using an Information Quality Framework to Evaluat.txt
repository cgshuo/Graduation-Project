  X  X ow do people feel? X   X  X hat opinions do people have? X  These questions are fre-quently asked by people when making decisions. In the past, public opinion was often hard to gauge because most media, such as news services or advertisements, were simply one-way communication channels. However, with the advent of Web2.0, many online collaboration tools, e.g., weblogs and discussion forums, are being de-veloped to allow Internet users to exchange opinions and share valuable knowledge. information and are forthcoming in presenting their personal viewpoints honestly. users X  opinions posted on the Web have a huge impact on consumer decisions [1]. Many e-commerce websites, such as Amazon 2 , are aware of the word of mouth effect desired information. To alleviate this information overload problem, opinion mining techniques have been devised to extract and summarize meaningful opinions from reviews. 
A major task of opinion mining is to identify sentimental (or bipolar) text units in re-document, depending on the granularity of opinion mining. Generally, supervised approaches like classification, which categorizes text units as sentimental or non-sentimental, can extract the core opinions expressed in reviews efficiently [11, 17]. [2, 7] also identify opinion targets (i.e., product features) and compile summaries of reviews to help users understand the advantages and disadvantages of a product. Obviously, the than those of ordinary people. [11] analyzes opinion discourses to identify opinion hold-ers. Opinion units are then weighted according to the holders X  authority. 
While many opinion mining approaches try to identify and analyze opinions from reviews, few works consider the quality of reviews. As Web 2.0 encourages knowl-edge sharing, there are no constraints on review writing. Consequently, the quality of reviews varies enormously. We observe from Amazon that many reviews simply contain emotional expressions, such as,  X  I love this camera and it is really nice.  X  Such reviews lack constructive expressions and should not be included in the opinion min-ing process. Some websites do consider the quality of reviews, and provide rating systems to rank reviews according votes submitted by users. Figure 1 shows a review rating on Amazon, where 121 out of 128 voters thought the review was helpful. 
Even with a rating system, ratings still suffer from imbalance vote bias, winner cir-cle bias, and early bird bias, which make the system impracticable [14]. Thus, there is an urgent need for quality evaluation mechanisms to help users or opinion mining algorithms identify informative reviews. In this paper, we treat the evaluation of product reviews X  quality as a classification problem and employ a multiclass support vector machine (multiclass SVM) [23] model to categorize reviews. In addition, we adopt a mature information quality framework [8], which has been widely used in many domains over the last twenty years, to define meaningful review features for classification. Experiments demonstrate that the proposed method outperforms state-tors that are critical for compiling informative reviews. 
The remainder of the paper is organized as follows. Section 2 contains a review of related works. In Section 3, we introduce the information quality framework and formance in Section 4. Then, in Section 5, we summarize our conclusions and ave-nues for future research. 2.1 Opinion Mining Opinion extraction and polarity identification are two major tasks in opinion min-ing. Depending on the granularity of the opinion mining approach, an opinion can be a word, a sentence, a paragraph, or even a complete review document. Most approaches rely on a human-composed opinion lexicon. Turney [21] gathered seven positive and seven negative words as an opinion dictionary and proposed the use of pointwise mutual information to calculate the degree of co-occurrence of a word [2] used information retrieval techniques to extract sentiment n-gram features from a set of positive and negative product reviews. Then, based on the features, classifi-cation algorithms can be employed to extract and classify sentiments or opinions in new reviews. Ku et al. [13] dealt with the opinion mining problem in a bottom-up General Inquirer 3 opinion lexicon and combined the translations with the Chinese Network Sentimental Dictionary 4 . The aggregated opinion lexicon determines the polarity of Chinese characters and, by extension, the polarity of Chinese words, sentences, and documents. Hu and Liu [7] observed that opinion sentences usually contain sentiment adjectives; thus, their opinion lexicon contained a set of senti-ment adjectives, and they used WordNet [5, 16] to identify new sentiment adjec-tives found in reviews. Those adjectives were then inserted into the lexicon to expand it recursively. In addition, they extracted sentences containing sentiment adjectives to compose opinion summaries of reviews. Kim and Hovy [11] also used of WordNet to expand a human-composed opinion lexicon. To determine a word X  X  polarity, the word was represented as a set of synonyms defined by WordNet. Next, a Naive Bayes classifier was employed to assign the polarity of the word. Their method also identified opinion holders by selecting name entities close to topic phrases in opinion sentences. In practice, opinion words are context dependent and can belong to any part-of-speech [3]. For example, the word  X  simple  X  has a positive conveys a negative sentiment in  X  The story of this movie is too simple.  X  Ding et al. [3] expanded the opinion lexicon in [7] by adding opinion verbs and nouns. They also considered the context information in sentences for opinion mining. Their ex-periments demonstrated that the accuracy of opinion mining can be improved by using a holistic opinion lexicon. 2.2 Review Quality Evaluation Zhang and Varadarajan [24] collected a set of product reviews on Amazon along with the corresponding helpfulness votes given by users. Based on the votes, the authors defined the utility of a review as the ratio of users (voters) who thought it was helpful and employed SVM regression to approximate the utility of the reviews. The resulting regression function was then used to estimate the utility of new reviews. The authors demonstrated through experiments that the shallow syntactic features of reviews are highly correlated with utility estimation. Kim et al. [12] also employed SVM regres-review features, namely, structural, lexical, syntactical, semantic, and meta-data fea-training examples, e.g., [12] and [24], are affected by the above types of bias and are unreliable. Rather than make predictions based on biased votes, Liu et al. treated review quality evaluation as a classification problem. In addition, they used an expert-composed data set to train an unbiased SVM classifier, which categorized reviews as either high-quality or low-quality. 
The above methods adopt a diverse set of features to evaluate the quality of re-hardly reflect the intrinsic characteristics of reviews. In this paper, we apply an effec-tive framework of information quality to derive information-oriented features for evaluating the quality of reviews. Using the information-oriented features improves the evaluation performance, and resolves important factors in review compositions. 3.1 Definition of Review Quality We regard review quality evaluation as a classification problem and employ an in-formation quality framework to derive informative review features for classification. Five categories of review quality, namely  X  high-quality  X ,  X  medium-quality  X ,  X  low-review quality in [14] and the definition of spam reviews in [9]. A high-quality review must provide complete and timely information about a product. It must also contain a large number of opinions to help readers make purchasing decisions. The content of a medium-quality review is relevant to a product, but it is not informative enough. Al-though such reviews are useful, they hardly persuade readers to make decisions. A too objective to judge the value of the product . A review is considered a duplicate if its content is very similar to a review posted previously. It may be a fake review or a repeat review posted by mistake. Finally, a spam review only provides comments advertisement or a question-answer type of review. 3.2 Classification Models The support vector machine (SVM) is a state-of-the-art machine learning algorithm for classification problems [19]. In this study, we employ two multiclass SVM-based approaches: One-Versus-All SVM and Single-Machine Multiclass SVM. One-Versus-All SVM (OVA SVM): This approach decomposes a multiclass classifier for a quality class, the training reviews in the class are regarded as positive examples, and remaining reviews are consid ered negative examples. To classify a new review, each classifier computes a score indicating the degree of association (or margin) between the review and the corresponding class. Then, the review is assigned to the class with the largest score. We implement the one-versus-all approach with the SVM light binary SVM tool [10]. The RBF kernel is selected because of its superior classification performance. Single-Machine Multiclass SVM (SMM SVM): Rather than combine the results of independent binary classifiers, the single-machine approach constructs a classification function by considering N classes simultaneously [23]. s.t. examples;  X  's are the slack variables for the training examples; and C is a regulari-zation term to control overfitting. The acquired w  X  X  and b  X  X  then assign a class label to a test review x test . cause non-linear kernels are time-consuming for multiclass problems. Moreover, the experiments reported in [6] demonstrate that linear kernels are comparable to non-linear kernels in many complex and large problems. 3.3 Information Quality-Based Learning Features Information quality (IQ) methodology investigates the characteristics of information items and derives item features considered informative from the perspective of infor-mation consumers [22]. In the last twenty years, many IQ frameworks have been developed for various application domains [4]. For instance, Zhu [25] assessed the quality of web pages in terms of an IQ framework to enhance the retrieval perform-ance of information systems. Meanwhile, Wang and Strong [22] developed a two-stage survey to collect features considered important by information consumers, and proposed a hierarchical IQ framework that organized the features along different dimensions. Eppler and Wittig [4] commented that Wang and Strong X  X  framework, shown in Table 1, attempts to strike a balance between theoretical consistency and derive informative review features in terms of the hierarchical framework. Some dimensions are not considered because they are not applicable to product reviews. We use following nine dimensions and fifty features for SVM training and testing. Believability (D1): This dimension is the extent to which an information item (i.e., a whose product ratings are extremely high or low are likely to be radical reviews. We therefore measure the deviation of a review X  X  product rating from the average to assess its believability.  X  The rating deviation of a review ( f Objectivity (D2): This dimension is the extent to which an information item is biased. Apparently, subjective opinions in reviews help readers to make decisions. We therefore apply Hu and Liu X  X  algorithms [7] to extract opinion sentences and measure this dimension in terms of review opinions.  X  The number of opinion sentences ( f  X  The percentage of opinion sentences ( f  X  The percentage of positive sentences ( f  X  The cosine similarity between the tf-idf vectors [15] of a review and the product trusted or highly regarded. Reviews written by authoritative reviewers are certainly influential. We measure this dimension based on the reviewer X  X  publications and the ranking given by e-commerce websites.  X  The number of reviews written by the reviewer ( f  X  The ranking of the reviewer ( f Relevancy (D4): This dimension is the extent to which the content in a review is useful for decision-making. Helpful product reviews should provide a large amount of product information. We consider the following statistics to assess the relevance of a review.  X  The number of the product name ( f  X  The percentage of the product name ( f  X  The number of opinion sentences containing the product name ( f  X  The percentage of opinion sentences containing the product name ( f Timeliness (D5): This dimension is the extent to which the information in a review is timely and updated. Old or duplicate reviews cannot reflect the value of a product in time; thus, the quality of information is low.  X  The degree of duplication of a review ( f  X  The interval (in terms of the number of days) between the current review and Completeness (D6): This dimension is the extent to which the information in a review is complete and covers various aspects of a product. High quality reviews should cover all kinds of product features and specifications.  X  The number of kinds of product features ( f Appropriate Amount of Information (D7): This dimension is the extent to which the volume of information in a review is sufficient for decision-making. The more review will be.  X  The number of product features ( f  X  The average frequency of product features in a review ( f  X  The number of sentences that mention product features in a review ( f Ease of Understanding (D8): A comprehensible review should state opinions about a product directly and clearly; and it should not contain rarely used or misspelled words.  X  The number of misspelled words in a review ( f  X  The average document frequency [15] of review words ( f  X  The position of the first opinion sentence in the review ( f Concise Representation (D9): This dimension represents the conciseness of a review, and complements the dimension of the appropriate amount of information. Including a lot of information may result in a review that is too long.  X  The average length of sentences ( f  X  The average number of sentences ( f 4.1 Data Preprocessing and Annotation We selected the reviews of ten popular digital cameras and ten mp3 players at Ama-date) were collected to construct an eval uation corpus. Two human experts annotated Inconsistent annotations were resolved through discussions between the annotators shown in Table 2. The kappa statistics between the annotators for digital cameras and mp3 players are 0.7253 and 0.7928, respectively, and are good enough to conduct reliable evaluations. 
The evaluations are conducted as follows. First, we examine the performance of our IQ dimensions. Then, the most effective dimension combination is compared method [24]; 2) the lexical features of Kim X  X  method [12], along with the length, method [14]. We use these feature sets for comparison because they have proven effective in review mining tasks. In addition, we assess the performance of the bag-of-words model [15] in which each uniqe term is treated as a feature. The method is also regarded as a baseline system. To convert the selected reviews into IQ-based feature vectors, we first remove stopwords [15] in the reviews and check the remaining terms for misspellings by using the WordNet, Wiktionary 5 , and Google X  X  spell check function 6 . Next, we apply Hu and Liu X  X  algorithm [7] to extract product features, opinion words, opinion sentences, and sentence polarities from the reviews. Then, the extracted entities are manually examined to filter out false alarms. For each compared method, 10-fold cross-validation [15] is adpoted to derive credible results. Macro/micro-average precision, recall, and the F1 score [15] are used as evaluation metrics. However, as each review can only belong to one quality category, the micro-average precision, recall, and F1 scores are equivalent; thus, we only consider the micro-average F1 scores. 4.2 IQ Dimension Evaluations Tables 3 and 4 show the performances of the IQ dimensions using SMM SVM and OVA SVM, respectively. As shown from Table 2, the  X  X pam X  and  X  X uplicate X  catego-huge variation in the macro-averaging performance. In contrast, the micro-average F1 score is insensitive to category sizes and is thus appropriate for evaluating the overall performance of each dimension. We assess the effect of IQ dimensions iteration by performance of the best dimension in the first row of each table. In the i  X  X h iteration (2  X  Next, we examine each remaining dimension combined with the basis and show the Table 3 shows the performance of the top-3 effective dimensions, {objectivity, repu-tation, information}, in SMM SVM. For each row, a one-tail paired t-test is applied to determine whether combining each dimension with the basis improves the system performance significantly. The symbol  X * X  indicates that combining a dimension im-proves the performance significantly, while the symbol  X # X  indicates the opposite. 
It is noteworthy that one IQ dimension (i.e., the appropriate amount of informa-tion) is sufficient for the RBF kernel to achieve superior performances. However, combining this dimension with the other IQ dimensions does not improve the system performance overall. This is because non-linear kernels have a powerful modeling ability so that a few discriminative features and dimensions are sufficient to construct accurate classifiers. Note that  X  X bjectivity X  and  X  X he appropriate amount of informa-tion X  are in the top-3 effective dimensions of the both SVM approaches. This indi-linear kernel and the RBF kernel are {D2, D3, D5, D7, D8} and {D2, D7, D9} respec-tively, which are used in the following comparisons. We observe that the other dimensions (i.e., other than the most effect ive combinations) contain diverse features. For instance, the brand names and product names mentioned in the evaluated reviews vary a great deal, so the features of the  X  X ompleteness X  and  X  X elevance X  dimensions are too sparse to contribute the system performance. Consequently, the performances of employing all IQ dimensions have little difference to those generated by the most effective combinations. 4.3 Comparisons with Other Methods Tables 5 and 6 show the performances of the compared methods. Generally, all the compared methods outperform the baseline method in terms of the micro-average F1 scores. The proposed method achieves the best performance and the improvement over each of the compared methods is statistically significant in terms of the one-tailed paired t-test. Liu X  X  approach [14] is a state-of-the-art method for classifying the quality of reviews. The set of informativeness features is information-oriented; hence its performance is good and comparable. The sets of shallow syntactic features and lexical features are effective in determining a review X  X  helpfulness. Helpfulness is the quality. Therefore, the performances of those features are inferior and are even worse than the performance of the baseline when using the linear kernel. 
The inferior performances of the baseline method highlight the difficulty of evalu-ating the quality of reviews. Since reviews are sentimental and information-oriented, evaluation systems must consider both textual and semantic characteristics of reviews to measure review quality. Our method examines various factors of reviews in detail. The superior evaluation performances indi cate that the derived features and dimen-characteristics. views. We regard a review as an information item and apply a theoretical IQ frame-work to derive representative review features and dimensions. Experiments show that our method can accurately classify reviews in terms of their quality, and that it outper-We will also apply the proposed method to various styles of opinion documents, such as blog entries and forum threads, to assess the quality of the information they provide. Acknowledgments. The authors would like to thank the anonymous reviewers for their valuable comments and suggestions. This work was supported in part by NSC 97-2221-E-002-225-MY2 and JAID S09800251079. 
