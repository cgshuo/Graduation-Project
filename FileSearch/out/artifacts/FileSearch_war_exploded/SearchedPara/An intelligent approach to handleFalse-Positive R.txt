 Institute for Integrated and Intelligent Systems, Grif fi th University, Queensland, Australia 1. Introduction
Radio Frequency Identi fi cation, commonly abbreviated to RFID, is an automated means to wirelessly it is able to capture the information of where and when the tags appear within a certain location. The wide array of applications that could employ this technology range from a shopping cart trolley which tallies up the groceries of the user automatically to a constantly tracking postal service to fi nd exact locations of customers X  parcels. Although the hardware of RFID systems has advanced to the point of technology effectively tracking items seamlessly, the RFID systems still suffer from several af fl ictions that prevent it from becoming widely accepted by the commercial sector.

One of the serious problems that prevents RFID systems from obtaining high levels of commercial different methodologies such as manual cleaning, deferred rule cleansing and fi ltration techniques but no methodology has succeeded in completely eliminating false-positive anomalies thus far. These methodologies suffer from a lack of au tomation, intelligence and advanced analysis to properly correct false-positive readings without inadvertently introducing arti fi cial anomalies from the cleaning process. The most problematic situation occurs with ambiguous readings when false positive anomalies cannot be easily cleaned.
To correct these anomalies, we propose in this paper a concept that utilises an advanced analysis coupled with inte lligent classifying algor ithms to eliminate false-positive readings within the RFID data warehouse. The methodology fi rst evaluates each tag within the data set to discover a suspicious obser-vation and key characteristics used in future processes. The observation along with the characteristics are to be stored or deleted. We have chosen three intelligent classi fi ers to determine whether the readings should be deleted. These are: Bayesian Network, Neural Network and Non-Monotonic Reasoning Logic Engine.

We conducted four different experimentations to fi nd the highest achieving con fi gurations of the techniques and the overall best performing classi fi er. Each of the fi rst three experiments investigated the most effective training algorithm/formula for each of the three classi fi ers. From this experimental evaluation, we found that genetic training algorithms with both 10 and 100 chromosomes performed the best for both the Bayesian and Neural Networks respectively and that the  X  formula allowed the Non-Monotonic Reasoning Logic Engines to obtain the highest cleaning rate. The fourth experiment tested each of the three classi fi ers to determine which cleaned the highest amount of false positive anomalies. We could not compare our approach to any of the prev iously proposed concepts presented in literature because those methods l ack automation or t he ability to co rrect ambiguous observations. From this fi nal experiment, we found that the Non-Monotonic Reasoning classi fi er achieved the highest performing cleaning rate.

The remainder of this paper has been organised as following: Section 2 will examine the background of our concept and Section 3 will provide an analysis of related and previous work already conducted in this fi eld of study. We propose our methodology and scenario for experimentation in Section 4. The details, results and analysis of the experimentation will be given in Section 5 followed directly by our conclusion and future work suggestions in Section 6. 2. Background
RFID systems have been prominent within the research community since 1948 when Harry Stockman published the fi rst academic paper to feature this technology [27]. This is due to the fact that there are massive bene fi ts available in a wide array of commercial applications from the integration RFID systems. Unfortunately, due to problems, including ambiguous false positive data anomalies, the potential uses have only minutely been brought into reality. 2.1. RFID
Radio Frequency Identi fi cation (RFID) is the seamless transmission of identi fi cation tags to readers of data between these objects may be viewed in Fig. 1. The tags fall into three categories: the active in a battery is harnessed to extend only the range of the transmission; and the passive tags in which energy is scavenged from the electro-magnetic pulse generated by the reader to transmit the unique identi fi er. We have chosen to focus on passive tags as they are the most commercially viable being both cheap and having an eternal lifespan due to the lack of a need for battery power.

Unfortunately, the bene fi ts of the passive RFID tag have been limited by the fl aws within the practical implementation of the device. Speci fi cally, there are four issues preventing the wide scale adoption of the system: the data is low level [17]; the incoming data is prone to error [16]; large volumes of data are generated over relatively small amounts of time [24]; and there is an increasingly complex nature of the spatial-temporal readings [29].

Low level data is hazardous to the application in t hat the resu lting observations are meaningless without being put into context. Without higher level event transformations, the data will not be able to be utilised be yond basic operations. The issue of large volume s of data also hinders the applications as the generated tag readings fl ood the system with incoming data. It is reported that Wal*Mart stores, which has integrated RFID into its daily routines, records several TeraBytes of data daily [12]. As the data is being recorded, there are also problems in that the recordings may consist of duplicate, missed and wrong readings which mislead the data analysts. Additionally, when the data is recorded there are certain issues that arise due to the complex nature of the spatial-temporal readings especially when employing readers that are not stationary.

Within the erroneous issue of the tags, the two persistent anomalies are false positive readings in which data is captured where it was not meant to and false negative readings in which data is not captured where it is required to be. False positive readings consist of two main scenarios in which a recording has either been duplicated by mistake [5], or read at a reader where the physical tag has not actually been present [1]. The cause for such occurrences usually amounts to tags being captured outside the reader range due to the environmental setup or where more than one reader is observing the same tag due to overlapping interrogation zones. It has been stated that the fi ltration of these unwanted data records are major fl aws within all RFID applications [12,13]. 2.2. Bayesian networks
A Bayesian Network is a probabilistic classi fi er utilising probabilistic values which are given to different variables to arrive at the most likely conclusion. To fi nd the likelihood of each outcome, the variables of each conclusion are multiplied. The h ighest achieving outcome is then identi fi ed as the likeliest candidate to be the correct conclusion [21]. The equation utilis ed for each conclusion fi nds the probability of the entire case by deriving all the probabilities of observations within the statement. The network is built upon these pr obabilitie s and, as such, needs guidance as to what each of these probabilities into genes of a chromosome and fi nding the optimal con fi guration [15].

The chromosomes are initiated by setting the genes to small random values. After a fi tness test to derive the highest performing chromosomes of the population, a certain amount of this population is destroyed and the resulting fi ttest chromosomes are crossed-over to re-establish the original population number. The cross-over function consists of the genes of the two parents being traded to create children of the parents. Common types of cross-overs include one point, two-point or uniform cross over. Eventually, a mutation occurs on a small percentage of the res ulting population t o avoid local minima issues. 2.3. Neural networks of the brain. It does this by creating a fi xed amount of Neurons which are trained to deliver a certain output when fed various input. The entire process has actually been based on the biological neuron pictured in Fig. 3. Dendrites will receive information which is passed to the cell body whose objective is to pass the information into the axon when certain requirements are met and, thus, to dendrites of other neurons via the synapse connection. The crucial difference between a digital Neuron, shown in Fig. 4 and it X  X  biological counterpart is that there is a computational limit to the amount of hidden units that may be present within a network. Unfortunately, technology has not advanced enough to effectively and ef fi ciently emulate the amount of Neurons the human brain possesses, which is estimated to be between 10 billion and 1 trillion [30].

The Arti fi cial Neural Network consists of three main layers: the Input Layer , Hidden Layer ( s )and the Output Layer [20]. The processes include receiving inputs which are modi fi ed at a central sum area. The Neuron will then apply an activation function such as the hard limiter or sigmoidal functions, which are displayed in Algorithm 1 and Eq. (1) respectively, to derive a value for the output. With regards to training the network, there are several techniques available such as the Back-propagation [4,26] and Genetic Algorithms [14,25] which are both considered as leaders with regards con fi guring Arti fi cial Neural Networks.
When attempting to con fi gure the Neural Network weights, one method that exists is to utilise training algorithms. Two dominant training algorithms that have been proven to excel in network training are the Back-Propagation Algorithm and the Evolutionary Neural Network. Back-Propagation relies on the concept of training the network by propagating error back through the network via modifying the weights after the output has been calculated [26]. The algorithm uses either a predetermined limited amount of iterations or the Root-Mean-Square error threshold of the calculated output as stopping criteria [4]. A fl ow diagram interpretation of how the Back-Propagation Algorithm operated may be viewed in Fig. 5.
The Evolutionary Neural Network training algorithm in contrast to Back-Propagation utilises the theory of genetic evolution to train the network weights. Similar to the genetic algorithm process of training a Bayesian Network, all the weights are added into a chromosome as genes to be manipulated according to the fi ttest output obtained [14]. The weights are initialised as small random numbers which are checked for either obtaining a high enough score or if the amount of generation limit has been reached. In the case that neither of the stopping criteria has been met, the algorithm will examine each chromosome in relation to achieving the correct output. A certain amount of the un fi t chromosomes are then destroyed within the population to be replaced with child chromosomes of two of the fi tter chromosomes. The child chromosomes can be created with different means such as one-point, two-point or uniform crossover [25]. Mutation will then be applied to a certain small percent of the population to assure that the network avoids problems such as network paralysis or local minima [6]. A general fl ow diagram interpretation of the Evolutionary Neural Network training process may viewed in Fig. 6. 2.4. Non-monotonic reasoning
Non-Monotonic R easoning is a distinct logic that operates by having the ability to make various deductions which will then be eliminated until the conclusion has been drawn from various observations. just one over-reaching conclusion. Clausal Defeasible Logic (CDL) has been designed to incorporate the Non-Monotonic Reasoning logic precisely within a computational environment and allows it to be run within various applications [2]. The direct bene fi t to embedding Clausal Defeasible Logic is that applications will be able to deterministically arri ve at intelligent conclusions when given various input parameters.
 An example in which Non-Monotonic Reasoning would decipher the correct solution is in relation to Siberian Huskies barking [18]. It is generally accepted that dogs bark, however, the Siberian Huskies are dogs which do not bark. This has been represented within the logical engine map pictured in Fig. 7 where the rule that the Siberian Huskies do not bark outweighs the rule in which dogs bark. In most cases, the logic declares a general rule which will then be defeated by speci fi c rules that defy the previously declared rule.

To logically interpret this, it must fi rst be stated as a fact that Siberian Huskies are dogs. After this step, we state that Siberian Huskies plausibly do not bark and that this latter rule defeats the former. To properly illustrate any ambiguous pr oblem within the Clausal Defeasib le Logic framework, the Decisive Programming Language (DPL) mus t be utilised [3]. Using the DPL, the Siberian Huskies problem translates into the following statements: Siberian Huskies ( x )  X  Dogs ( x ) .
 R1: Dogs ( x )  X  Bark ( x ) .
 R2: Siberian Huskies ( x )  X  X  X  Bark ( x ) .
 R2 &gt; R1
After this code has been written, it is passed into the CDL compiler which transforms it into an implementable C function which will return the logical conclusions for any given inputs. The aspect that when it determines its conclusion. These levels of ambiguity strength are represented using the following formulae:  X   X  : The strongest of the algorithms as it will only respond to factual information to arrive at a  X   X  : An algorithm in which any conjunction of  X  and  X  are used to reach the conclusion.  X   X  : The algorithm in which ambiguity is propagated to reach its conclusion.  X   X  : The algorithm in which ambiguity will not be allowed to be used to draw its conclusions.  X   X  : The formula in which the disjunction of  X  and  X  are used to draw conclusions. 3. Related work
With regards to previous work, there are three main methodologies used to eliminate the false positive readings within the RFID data set. The fi rst method is the manual cleaning tool in which data sets are cleaned by the user identifying and discarding anomalies [22]. The manual cleaning tool is not speci fi cally designed to be utilised with RFID data sets, but it may be applied to the observational data warehouse. The second methodology is the automated use of rules de fi ned by the user which is applied on incoming data with use of a dynamic smoothing window [16].
 The manual cleaning method is effective but costly in terms of time and effort on the user X  X  part. An automated solution would be more practical particularly for mass amounts of data. The rule-based approach is automated, however has problems when cleaning ambiguous information due to the fact that not all cases, in reality, are solvable with a general rule. While fi ltration, also an automated solution, suffers from the analytical data available which is only incoming data, therefore the scope of data to analyse is very limited. Additionally, false-positive anomaly correction within RFID systems is similar to the Data Mining of Imbalanced Data Sets [8]. However, due to the unique nature of the observational 4. Methodology
To properly illustrate how our methodology performs, we will fi rst review the motivation and archi-tecture of the program. This includes the Feature Set De fi nition, Bayesian Network, Neural Network, Non-Monotonic Reasoning and Loading phases. Next, we will provide details of the ideal scenario to design our system most effectively. Finally, any assumptions we have made to ensure that the algorithms operates correctly will be listed. 4.1. Motivation
We created this system with the intended goal of correcting ambiguous false positive RFID data current related work (such as rule-based soluti ons) lack the intelligence needed to handle ambiguous anomalies. This would allow ourmethodology to be able to counteractthe anomalousdata in an intelligent and automated manner. This results in our methodology overcoming the current problems that exists in superiority over existing methodologies. We chose the three classi fi ers as opposed to other techniques due to the Bayesian Network [10], Neural Network [11] and Non-Monotonic Reasoning [9] have all been able to effectively correct false-negative RFID anomalies in the past. We also chose Bayesian and Neural Networks as probabilistic methods to be compared w ith the deterministic N on-Monotonic a pproach.
A sample of the ambiguous scenario we attempt to correct is shown in Table 1. The table consists of the three vital RFID observational data: the tag X  X  identi fi er-EPC (Electronic Product Code), the reader X  X  identi fi er and the time of the event. As seen in the table, the tag with an EPC of T1 is seen at Location R3, R4, R2, R4, R1 and R1 where the third observation would be fl agged as a suspicious reading. In previous methodologies, this fl agged observation would automatically be deleted due to the proximity of the readers in the second and fourth observations. However, using the  X  X ap Data X  our methodology may make a more intelligent cleaning decision.

The map data, as seen in Fig. 8, is used to determine validity of the observation. Within the fi gure, the lines that house the circle represent walls, the large circles represent the reader range, the letters inside the circle identify the reader and the lines between the circles represent two readers being within proximity of each other. Knowing t his map data, our met hodology provides a highe r intellig ence when Knowing that both locations in the fi rst and fi fth observation are geographically within proximity to the fourth observation which it will discover as a false-positive anomaly due to the second, third, fi fth and sixth readings not being within the proximity of the fourth which in this example scenario, is correct. We believe that by enhancing the rules with our concept will apply much needed intelligence in scenarios where the obvious action will not clean the data set. 4.2. Architecture
As it may be observed in Fig. 9, the design of our system has been broken into three sections: the Feature Set De fi nition Phase ;the Classi fi er Phase ;andthe Modi fi cation Phase . The Feature Set De fi nition Phase is the fi rst process that is conducted within our application in which the raw data is searched and sorted to fi nd suspicious readings and the circumstances surrounding each of these observations. The Classi fi cation is where the system deviates between three different classi fi ers, the Bayesian Network , Neural Network or Non-Monotonic Reasoning Engines . Each classi fi er has one goal which is to determine if the fl agged reading should be deleted or kept within the data set. This decision determined the validity of the observation, it will then pass the decision onto the Modi fi cation Phase which will either delete or keep the value being passed to it. This entire process may be viewed in the high level pseudo-code interpretation found in Algorithm 2.
 4.2.1. Feature set de fi nition phase to discover suspicious readings and investigate key characteristics surrounding the fl agged observation. Initially, this phase breaks the tag readings into Tag Streams designed to analyse the route of one tag. We de fi ne Tag Streams as individually analysed streams for one tag from the mass amount of readings. A take to reach the location, or if the geographical locations of the readers are not within proximity. To determine the geographic validity of the readers the program utilises a table named  X  X ap Data X , which is constructed by the user and re fl ects the geographic layout of all adjacent readers within the static and d . The values of observations a and b are the reading taken two readings or one reading respectively before the suspicious reading x .The c and d readings are the observations which been recorded once and twice after the suspicious reading. From all these observations, the timestamp and the location are all recorded and used in further analysis.
 After the values of a, b, x, c and d with their respective timestamps and locations have been found, the comprised of ten different binary mathematical operations, however additional characteristics maybe be added by the user. Each of the characteristics contain spatial and temporal information regarding the observations before and after the suspicious readings. With regards to the proximity of timestamps, we have utilised the value of half a second. This time value may be altered to better suit the application for which it is designed. The characteristics we discover are as follows:  X  b.loc  X  x.loc  X  c.loc  X  x.loc  X  b.time == x.time  X  c.time == x.time  X  b.loc == x.loc  X  c.loc == x.loc  X  a.loc  X  x.loc  X  d.loc  X  x.loc  X  b.time  X  x.time  X  c.time  X  x.time
The data used in these characteristics are fi ve different values that have two sub-values each. The fi ve values include the observations of a, b, x, c ,and d , which each have the time (time) and location (loc) for each value stored. The relations our methodology uses in analysis include when values are within certain proximity which is represented as  X  , or are equivalent which is represented as == .Itis important to note that the function that states that the two values are within proximity of each other have different meaning between the location and time. With regards to location, the proximity is determined by the  X  X ap Data X  table whereas the proximity with regards to time refers to the time value of the two observations being within the user de fi ned time value of each other. After all these characteristics have been gathered, they are passed onto the various classifying methodologies as inputs to determine whether or not the fl agged item should remain within the data set. 4.2.2. Classi fi er phase  X  bayesian network
The fi rst option that the Classi fi er Phase can utilise is the Bayesian Network. In this example we have considered the Bayesian Network to have ten inputs that correspond to the analytical characteristics found at the end of the Feature Set De fi nition Phase . Using these ten inputs the Bayesian Network will then determine, based on the weights it has obtained through training, whether the fl agged observational reading should be kept within the database, see Eq. (2). This will result in one output known as the keep value which will be set to either true or false. An example of the tabular structure of the network may be viewed in Table 2. This keep value output will be passed to the Modi fi cation Phase at the end of this process at which point the entire application will repeat each time a suspicious reading is encountered. We also set all binary input numbers from 0 and 1 to 0.1 and 0.9 respectively to allow for higher mathematical functions to bene fi t from avoiding to multiply by zero.

We have chosen a Genetic Algorithm to train the Bayesian Network weights based on the various test cases that may arise. The Genetic Algorithm will have the population of the chromosomes to determine the ideal number of chromosomes utilised for training purposes. The mutation rate of the Genetic algorithm being utilised will be 1% for the top 10% chromosomes with regards to fi tness and 5% for all other chromosomes. After the best weight con fi guration has been determined, the network will be utilised to compare it against the Neural Net work and Non-Monotonic R easoning approaches. 4.2.3. Classi fi er phase  X  neural network
An Arti fi cial Neural Network (ANN) is the second option we have chosen for the Classi fi er Phase which utilises weighted neurons to determine the validity of the fl agged value. Like the Bayesian Network, this ANN will use the ten inputs gathered from the Feature Set De fi nition Phase to pass through the network and obtain one output. This network con fi guration has been displayed graphically in Fig. 11. The network shall be comprised of a single hidde n layer with eleven hidden nodes resulting in 121 weights between all the nodes. We speci fi cally wanted to choose more hidden units than inputs and only one layer as we have found th at multi-layered network s do not necessarily enha nce the perfo rmance of the classi fi er.
 We have also set the momentum and learning rates to 0.4 and 0.6 respectively and have utilised a Sigmoidal Activation Function. Additionally, as with the Bayesian Network we shall use the numbers 0.1 and 0.9 rather than the binary numbers of 0 and 1 respectively. Two prominent training algorithms have been utilised to properly con fi gure the Neural Network. The fi rst is the Back-Propagation Algorithm while the second is the Genetic Algorithm which has also been utilised within the Bayesian Network. Both algorithms will use a limited amount of iterations as stopping criteria for the training. The pseudo-coded version of the Back-Propagation and Genetic Algorithms may be viewed in Algorithms 3 and 4 respectively. Additionally, various amounts of chr omosomes will be utilised for the Genetic Algorithm. 4.2.4. Classi fi er phase  X  non-monotonic reasoning The fi nal classi fi er we have utilised within our implemen tation is Non-Monotoni c Reasoning Logic variables obtained from the Feature Set De fi nition Phase . From this, the logic engines determine the correct course of action to either keep the value or not based on the different levels of ambiguity we enforce.

The rules utilised within the logic engine may be examined in Table 3. The four symbols that are used to interact with the values within the rules are the logic AND operator , the negative operator  X  ,the equal operator == and our use of the double arrow  X  to illustrate proximity between the two analysis variables. The process used to fi nd the conclusion from the lookup table created by the logic engine can be seen in pseudo-code in Algorithm 5.

As a default case where neither keep val or  X  keep val are encountered, the logic engine will keep the fl agged reading to avoid arti fi cially introduced false negative observations. Additionally, the order in which they have been written in this document is also the order of priority with regards to fi nding the conclusion. 4.2.5. Modi fi cation phase
After each inte lligent c lassi fi er has determined whether or not to keep or delete the fl agged reading, it will pass it to the Modi fi cation Phase . After the decision has been received, the application will anomalies rather than automatically delete the fl agged items if the user would like to examine the observations prior to modi fi cation. 5. Experimental results and analysis
In order to investigate the applicability of our concepts, we conducted four experiments. The fi rst three were dedicated to fi nding the optimal con fi guration of each classi fi er whereas the last focused on the comparison of the three classi fi ers. In this section, we describe both the scenario, database structure, assumptions and environment in which we conducted these experiments and an analysis of the experiments. Furthermore, we describe the experimental evaluation and present the results of four experiments which we conducted. The fi rst three are designed to determine the highest achieving con fi guration of the Bayesian Network, Neural Network and Non-Monotonic Reasoning classi fi ers. In the fourth experiment, the highest achieving classi fi ers have been compared against each other to fi nd which one achieves the highest cleaning rate. 5.1. Scenario
The ideal scenario which we envisioned for our application would be an enclosed RFID static en-vironment. It is important that such an environment exists as the program requires knowledge of the geographical landscape with regards to the locations of the readers. The motivation for such an applica-tion came from the idea of a building that houses important items which need to be monitored thoroughly with an RFID environment for their speci fi c applications, this software may be utilised for an array of employing RFID systems in this manner [28], we believe that this scenario is particularly realistic to be bene fi cial to similar applications that could be conducted. 5.2. Database structure
To store the observationaldata within a data warehousewe have designed a database structure modelled after the Data Model for RFID Applications (DMRA) [19]. DMRA is currently being used within RFID middleware where it is designed to manage all RFID observational data ef fi ciently. We have chosen only three tables from the many utilised in the appli cation that are relevant to our methodology which houses information regarding the readers (Reader), tags (Object) and interaction between both of them (Observation). The structure of the tables used in this data warehouse is as follows:  X  READER ( ReaderID, Name, Description )  X  OBJECT ( Epc, Name, Description )  X  OBSERVATION ( ReaderID, Value, Timestamp ) Additionally, we have created another table  X  X ap Data X , which stores information vital to our concept. The prime goal of this additional table is to store the readers which are within close proximity of each other. The structure for this additional table is as follows: MAPDATA ( ReaderID1, ReaderID2 ). 5.3. Assumptions
We have two prime assumptions for this application, fi rst, that the environment is static and, secondly, that the locations of the readers are known. This is important as the concept relies heavily on both the mapped data to determine if a reading is suspicious and, subsequently, when it attempts to reason the validity of the observation if it has been fl agged. This would require the application to be run in an RFID-enabled environment where the readers are mounted which would prevent it from changing application before the application has begun to correct anomalies. Additionally, any modi fi cations to the environment or the reader locations would have to be updated within the map data table as well. 5.4. Environment The code for this methodology was written and comp iled in the C++ language utilising Microsoft Visual Studio C++ 6.0. The experiments were run on a Windows XP operating system running Service Pack 3. As outlined earlier, above, there are four main experiments which were conducted using the methodology. The fi rst experiment was designed to test the highest performing Genetic Algorithm when training the Bayesian Network. For this experiment, the amount of chromosomes in the population was manipulated to fi nd the highest performing number. The second experiment was designed to discover which training algorithm of either the Back-Propagation or Genetic Algorithm obtained the highest cleaning rate. For this experiment, the amount of chromosomes were modi fi ed and compared with the back-propagation algorithm to determine the highest achieving algorithm. The third experiment was designed to determine which formulae achieved the highest cleaning rate within the Non-Monotonic Reasoning approach.

We speci fi cally chose only to examine classi fi er techniques as the related work is not comparable due to either it not being able to clean ambiguous data or not using an automated process. The last experiment which was conducted took the highest achieving con fi gurations of each of the classi fi ers and compared were training sets in which 500, 1,000 and 5,000 scenarios were used to train the algorithms and fi nd the classi fi ers.

The second data set was three testing sets in which 1,000, 5,000 and 10,000 randomly chosen scenarios were selected and passed to the application to have the anomalies eliminated. Each of these testing sets contained feature set de fi nitions generated within our sample scenario. After each of the training and testing experiments have been conducted, the average of cleaning rate of the experiments has been derived for each technique and used to identify the highest achieving method. 5.5. Bayesian network experiment
For our fi rst experiment, we conducted an investigation into the optimal amount of chromosomes which are needed to clean the false positive anomalies. To accomplish this, we created three Bayesian Networks that have been con fi gured using a genetic algorithm with 10, 50 and 100 chromosomes. Each network was trained for 10 generations to breed and optimise the con fi guration. With regards to the set of data being used for training, we used 3 different  X  X raining Cases X  comprising 500, 1,000 and 5,000 false-positive anomaly scenarios. After these experiments were completed, the average of the 3 training cases was then extracted and, subsequently, used to determine the amount of chromosomes that are needed to achieve the highest cleaning rate.

The results of this experiment are shown in Fig. 12 with the Amount of Training Cases and Chro-mosomes are graphed against the Percentage of Correctness. It may be observed that the fi rst viewable feature is the con fi guration that used 10 chromosomes to train the network obtained the highest average cleaning rate. As a result, the Bayesian Network using a Genetic Algorithm with 10 chromosomes will be utilised in the fi nal experiment in which all three classi fi ers are compared. The lowest achieving con fi guration using 100 chromosomes tested upon 500 training cases was the Bayesian Network. The highest achieving con fi guration has been found to be the con fi gurations with 10 and 50 chromosomes against 500 and 1,000 training cases respectively. 5.6. Neural network experiment
The second experiment we conducted was in relation to determining the highest performing network con fi guration for a Neural Network to clean anomalous RFID data. To do this, we trained the weights of the networks using the back-propagation (BP) and the genetic algorithms with 10 (GA-10), 50 (GA-50) and 100 (GA-100) chromosomes present. The perform ance of the resulting networks were determined based upon the correctness of the classi fi cation from 3 Training Cases using 500, 1,000 and 5,000 false-positive anomaly scenarios. Each con fi guration had been trained by 10 iterations or generations before the training experiment commenced. The main goal of this experiment was to determine the highest average achieving network trainer, thus the average of the three training cases has also been found.
From the experimental results shown in Fig. 13, we have derived a general observation that the perfor-mance of the network is vastly improved with 50 and 100 chromosomes using the Genetic Algorithm. The highest performing average of the Neural Network has been found to be the Genetic Algorithms when trained with both 50 and 100 chromosomes. As such, we decided to use the Genetic Algorithm with 100 chromosomes as the attempt to clean 500 training cases performed the highest. The lowest performing cleaning algorithm was the Back-Propagation algorithm when attempting to clean 1,000 training cases. 5.7. Non-monotonic reasoning experiment
The main goal of the third experiment was to derive the highest performing Non-Monotonic Reasoning formula from the fi ve different options used in Clausal Defeasible Logic. With this in mind, the  X  (Mu),  X  (Alpha),  X  (Pi),  X  (Beta) and  X  (Delta) formulae were each trained using 3 training cases containing 500, 1,000 and 5,000 false-positive scenarios each. Like the previous two experiments, the averages of each performing algorithm was ascertained and used to determine which of the fi ve formulae would be utilised to proceed onto the fi nal experiment. The results of this experiment are presented in Fig. 14 where the observations have been structured with the Amount of Training Cases and the various Formulae have been graphed against the Percentage of Correctness.
 the  X  and  X  formulae both performed the least cleaning. With regards to the fi nal experimentation, we have chosen the  X  formula as it performed the highest and is the most likely to continue to perform highly. The reasons as to why we rejected the  X  and  X  formulae lie in the fact that the  X  formula is strict in that it only accepts factual information and the  X  formula is connected directly to the  X  . Hence, we determined that the  X  formula would be superior to the other formulae. 5.8. Comparison experiment
The goal of the fi nal experiment was to determine which of the three highest performing classi fi er techniques would clean the highest percentage of a large amount of false-positive RFID anomalies. The three classi fi ers used in this experiment included the Bayesian Network trained by a Genetic Algorithm with 10 chromosomes (BN), the Neural Network trained by Genetic Algorithm with 100 chromosomes (NN) and the  X  of the Non-Monotonic Reasoning Engine (NMR). The classi fi ers were all chosen based upon the high performance found within the fi rst three experiments previously discussed. Both of the Bayesian and Neural Networks had both been trained for 10 generations before these tests were conducted. As opposedto the previous experiments, we determined that three  X  X esting Cases X  containing 1,000, 5,000 and 10,000 randomly chosen false-positi ve scenarios would be utilised to determine the cases has been found from the results.

The results of this experiment is depicted in Fig. 15 where the Amount of Test Cases and Classi fi er has been graphed against the Percentage of Correctness. From these results, it can be seen that the Non-Monotonic Reasoning Engine achieves the highest average cleaning rate among the other classi fi ers. The highest performing classi fi er has also been found to be the Non-Monotonic Reasoning when attempting when attempting to clean 10,000 test cases.

The Non-Monotonic Reasoning Engine outperformed the other classi fi ers in dealing with false-positive data due to the fact that it is a deterministic approach. The Bayesian and Neural Networks, by contrast, rely on a probabilistic nature to train their respective networks. The major drawbacks of this system are that it is speci fi cally tailored for the static RFID cleaning problem, however, we believe that the same concept may be applicable to any static spatio-temporal data enhancement case study. With regards to applying our methodology to other applications where the environment is dynamic, the Feature-Set De fi nition and Non-M onotonic Reasoning will need greater complexity to accommodate the change in anomalies. Although the test cases utilised in experimentation were small in comparison to the immense amount of RFID readings that get recorded in real world systems, we believe our methodology would behave similarly upon larger data sets. 6. Conclusions
In this work we have focused upon an automated and intelligent means to correct false positive RFID readings at a deferred stage of the capture cycle. The rationale behind our choice of creating such a methodology was to create a concept which will target ambiguous observations. We have shown through experimental evaluation that our presented concept obtains a high cleaning rate. Speci fi cally, this work makes the following contributions to the fi eld:  X  We developed a new concept to clean ambiguous false-positive RFID observations.  X  We introduced the use of Non-Monotonic Reasoning, Deferred Bayesian Network, and Arti fi cial  X  We discovered that the Non-Monotonic Reasoning  X  formula performs the best.  X  We identi fi ed the optimum Deferred Bayesian Network con fi guration could be achieved through  X  We found that the use of a Genetic Algorithm with a population of 100 chromosomes is optimal for  X  Through experimental evaluation, we found that the Non-Monotonic Reasoning classi fi er obtained  X  Put forth a hypothesis derived from our results that the Non-Monotonic Reasoning classi fi er suc-
While in this work we concentrated on false positive RFID anomalies, the presented concept is applicable to any ambiguous data. While the Non-Monotonic Reasoningperformed the best and achieved the highest cleaning rate other methods, due to the their nature, would be applicable for other anomalies. Speci fi cally, the Bayesian and Neural Networks would be better suited for an application in which the data is missing, such as false negative anomalies. False-Negative readings need such probabilistic nature to counteract the missing observations which have not been recorded. However, the false-positive anomalies have the problem of gaining too much observations which, in turn, makes a deterministic methodology superior in terms of correcting the anomalies where there is enough evidence to correctly discover the right course of action. Our concept would also bene fi t other applications that rely on the identi fi cation and handling of false-positive anomalies such as physical intrusion detection.
With regards to future work, we believe it would be valuable to investigate effectiveness of other classi fi cation techniques, such as the Support Vector Machine, or to develop dedicated classi fi cation algorithm tailored speci fi cally for false-positive RFID anomalies. Also, we would like to investigate the effectiveness of the data correction process when enhancing both the feature set de fi nition or the DMRA data storage, and how our system would react to Terabytes of data as opposed to the test cases we experimented with. It would also be interesting to see the effectiveness of hybrid technique of probabilistic and deterministic methodologies,such as using the Neural Network then passing the fi ndings to the Non-Monotonic Reasoning logic engines. We also intend to incorporate advanced data mining algorithms that have been utilised in past literature, such as Frequency/Statistical based methodologies, as we believe it would enhance the classi fi er results.
 Acknowledgment
This research is partly sponsored by ARC (Australian Research Council) grant no DP0557303. References
