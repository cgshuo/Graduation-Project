 The problem at hand can be described as follows: Input : An unlabeled corpus of an arbitrary natural language and two arbitrary Output : A YES/NO answer as to whether w 1 and w 2 are morphological vari-Restrictions : We consider only concatenative morphology and assume that
The relevance of the problem is that of stemming as applied in Informa-tion Retrieval (IR). The issues of stemming in IR has been discussed at length elsewhere and need not be repeated here. It suffices to say that, though not uncontroversial, stemming continues to be a feature of modern IR systems for languages like English (e.g Google 1 ), and is likely to be of crucial importance for languages which make more use of morphology (cf. [1]).

The reasons for attacking the problem in an unsupervised manner include advantages in elegance, economy of time and money (no annotated resources required), and the fact that the same t echnology may be used on new languages. The latter two reasons are especially important in the context of resource-scarce languages.

Our proposed unsupervised same-stem decision algorithm proceeds in two phases. In the first phase, a ranked list of salient affixes are extracted from an unlabeled text corpus of a language. In the second phase, an input word pair is aligned to shortlist affixes that could potentially be added to a common stem to alternate between the two. Crucially, this shortlist of affix alternations is analyzed to check whether they form a systematic alternation in the language as a whole (i.e not just in the pair at hand). This analysis depends strongly on the ranked affix list from the first phase.

An outline of the paper is as follows: we start with some notation and basic definitions, with which we describe the theory that is intended to model the assumed behaviour of affixation in natural languages. Then we describe in de-tail and with examples the thinking behind the affix extraction phase, which actually requires only a few lines to define mathematically. Following that, we present our ideas on how to distinguish a sy stematic morphological alternation from a spurious one. This part is the more experimental one but at least it re-quires no guiding, tuning or annotation whatsoever. The algorithm is evaluated against a human gold standard on four languages chosen to span the full width of morphological typology. Finally, we briefly discuss related work, draw some tentative conclusions and hint at future directions. We have chosen to illustrate using suffixes but the method readily generalizes to prefixes as well (and even prefixes and suffixes at the same time). 2.1 A Naive Theory of Affixation Notation and definitions:  X  w,s,b,x,y,...  X   X   X  : lowercase-letter variables range over strings of some  X  sw : s is a terminal segment of the word w i.e there exists a (possibly empty)  X  W,S,...  X   X   X  : capital-letter variables range over sets of words/strings/  X  f W ( s )= |{ w  X  W | sw }| : the (suffix) frequency, i.e the number of words in  X  S W = { s | sw  X  W } : all terminal segments of the words in W  X  uf W ( u )= |{ ( x, y ) | xuy = w  X  W }| : the substring frequency of u , i.e the  X  nf W ( u )= uf W ( u )  X  f W ( u ): the non-final frequency of u , i.e. the substring  X  | X | : is overloaded to denote both the length of a string and the cardinality
Assume we have two sets of random strings over some alphabet  X  :  X  Bases B = { b 1 ,b 2 ,...,b m }  X  Suffixes S = { s 1 ,s 2 ,...,s n } Such that: Arbitrary Character Assumption (ACA): Each character c  X   X  should Note that B and S need not be of the same cardinality and that any string, including the empty string, could end up belonging to both B and S . They need neither to be sampled from the same distribution; pace the requirement, the distributions from which B and S are drawn may differ in how much probability mass is given to strings of different lengths. For instance, it would not be violation if B were drawn from a a distribution favouring strings of length, say, 42 and S from a distribution with a strong bias for short strings.

Next, build a set of affixed words W  X  X  bs | b  X  B, s  X  S } , that is, a large set whose members are concatenations of the form bs for b  X  B, s  X  S , such that: Frequent Flyer Assumption (FFA): The members of S are frequent. For-In other words, if we call s  X  S a true suffix and we call x an arbitrary segment if it neither a true suffix nor the terminal segment of a true suffix, then any true suffix should have much higher frequency than an arbitrary segment of the same length. 2.2 An Algorithm for Affix Extraction The key question is, if words in natural languages are constructed as W explained above, can we recover the segmentation? That is, can we find B and S ,given only W ? The answer is yes, we can partially decide this. To be more specific, we can compute a score Z W such that Z W ( x ) &gt;Z W ( y )if x  X  S and y/  X  S .In general, the converse need not hold, i.e if both x, y  X  S ,orboth x, y /  X  S ,then it may still be that Z W ( x ) &gt;Z W ( y ). This is equivalent to constructing a ranked list of all possible segments, where the true members of S appear at the top, and somewhere down the list the junk, i.e non-members of S , start appearing and fill up the rest of the list. Thus, it is not said where on the list the true-affixes/junk border begins, just that there is a consistent such border. We shall now define three properties that we argue will be enough to put the S -belonging affixes at the top of the list. For a terminal segment s , define: Frequency. The frequency f W ( s )of s (as a terminal segment).
 Curve Drop. The Curve Drop of s is the minimal percentage drop in freqency Random Adjustment. First, for s , define its probability as:
It is appropriate now to show the intuition behind the definitions. There isn X  X  much to comment on frequency, so we X  X l go to curve drop and random adjustment. All examples in this section come from the Brown corpus [2] of one million tokens ( | W | = 47178 and | S W | = 154407).

The curve drop measure is meant to predict when a suffix is well-segmented to the left. Consider a suffix s , in all the words on which it appears, there is a preceding character c . For example, -ing occurs 3258 times, of which it is preceded by t 640 times, of l 329 times, r 317 times, d 258 times, n 249 times and so forth. This contrasts with -ng which occurs 3352 times, of which it is preceded by i 3258 times, by o 35 times, by a 26 times and so on. The reasoning is thus as follows. If s is a true suffix and is well-segmented to the left, then its curve-drop value should be high. Frequent true suffixes that attach to bases whose last character is random should have a close to uniform curve. On the other hand, if the curve drop value is low it means there is a character that suspiciously often precedes s . However, if s weren X  X  a true suffix to begin with, perhaps just a frequent but random character, then we expect it X  X  curve drop value to be high too! To exemplify this, we have C ( ing )  X  0 . 833, C ( ng )  X  0 . 029 and C ( a )  X  0 . 851.

The random adjustment measure it precisely to distinguish what a  X  X requent but random segment X  is, that is, discriminate e.g -a versus -ing as well as -a versus -ng . Now, how does one know whether something is random or not? One approach would be to say the shorter the segment the more random. Although it X  X  possible to get this to work reasonably well in practice, it has some draw-backs. First, it treats all segments of the same length the same, which may be too brutal, e.g should -s be penalized as much as -a ? Second, it might be considered too vulnerable to orthography. For example if a language has an odd trigraph for some phoneme, we are clearly going to introduce an error source. Instead we propose that a segment is random iff it has similar probability in any posi-tion of the word. Instead we propose that a segment is random iff it has similar probability in any position of the word. This avoids the  X  X lat length X -problems but has others, which we think are less harmful. First, we might get sparse data which can either be back-off smoothed or, like here, effectively ignored (where we lack occurence we set the RA to 1.0). Second, phonotactic or orthographic constraints may cause curiousities, e.g. English y is often spelled i when medial as in fly vs. flies .

To put it all together, we propose the characterization of suffixes in terms of the three properties as shown in table 1. The terms high and low are of course idealized, as they are really gradient properties.

As seen from the table, we hold that true suffixes (and only true suffixes) are those which have a high value for all three properties. Therefore, we define our final ranking score, the Z W : S W  X  Q :
The final Z W -score in equation 5 is the one that purports to have the property that Z W ( x ) &gt;Z W ( y )if x  X  S W and y/  X  S W  X  at least if purged (see below). We cannot give a formal proof that languages satisfying ACA and FFA should get a faultless ranking list because this is true only in a heuristic sense. To set bounds on the probability for it to hold is also depends on a lot of factors that are hard, or at least inelegant, to characterize. We hope, however, to have sketched the how the ACA and FFA assumptions are used. 2.3 Affix Extraction Sample Results On the affix extraction part as such, we will only give some impressionistic results rather than a full-scale evaluation. The reason for this is that, although undoubtedly the list has some valid meaning, it is at present unclear to the author what a gold standard should be in every detail in every language. Furthermore, different applications, such as the final objective in this paper, may not require that a context-less choice between two related affixes, e.g -ation and -tion ,be asserted.

For an English bible corpus [3] we get the top 30 plus bottom 3 suffixes as shown in table 2.
The results largely speak for themselves but some comments are in order. A good sign is that the list and its order seems to be largely independent of corpus size (as long as the corpus is not extremely small) but we do get some significant differences between bible English and newspaper English. As is easily seen from the lists, some suffixes are suffixes of each other so one could purge the list in some way to get only the most  X  X ompetitive X  suffixes. For a fuller discussion of purging, other languages and all other matters pertaining to the affix extraction algorithm, the reader is referred to the longer exposition in [4]. Having a list of salient affixes is not sufficient to parse a given word into stem and affix(es). For example, sing happens to end in the most salient suffix yet it is not composed of s and ing because crucially, there is no * s ,* sed etc. Thus to parse a given word we have to look at additional evidence beyond the word itself, such as the existence of other inflections of potentially the same stem as the given word, or further, look at inflections of other stems which potentially share an affix with the given word. This line of thought will be pursued below.

The problem at hand, namely, to decide if two given words w 1 , w 2 share a common stem (in the linguistic sense) is easier than parsing one word. Essentially, there are four interesting kinds of situ ations the same-stem-decider must face: 1. w 1 and w 2 do share the same stem and have a salient affix each, e.g played 2. w 1 and w 2 do share the same stem but one of them has the  X  X ero X  affix, e.g 3. w 1 and w 2 do not share the same stem (linguistically) but do share some 4. w 1 and w 2 do not share the same stem (linguistically) and do not share any Number 4 is trivial to decide in the negative. Number 1 is also easy to affirm using a list of salient affixes, whereas the special case of number 2 requires some care. The real difficulty lies in predicting a negative answer for case number 3 (while, of course, at the same time predicting a positive for cases 1 and 2). We will go for an extended discussion of this matter below.

Consider two words w 1 = xs 1 and w 2 = xs 2 that share some non-empty initial segment x . Except for chance resemblances, which by definition are rare, we would like to say that w 1 and w 2 belong to the same stem iff: 1. s 1 and s 2 are well-segmented salient suffixes in the language, i.e -w and -lt 2. s 2 and s 2 must systematically contrast in the language, that is, there must The key difficulty is to decide, in an unsupervised manner, when something is systematic and when it isn X  X . In order to tackle this, we will propose a heuristic for measuring how much two suffixes contrast. This will give a score between 0 and 1 where it is not clear at which value  X  X ystematic X  begins. We could say that, at this point, the user has to supply a threshold value. However, instead, we devise another heuristic that obvi ates the need for a threshold at all. The resulting system thus supplies a YES/NO answer to the same-stem decising problem without any human interaction. 3.1 Formalizing Same Stem Co-occurence From the word distributions characteristic of natural language corpora, it is surprisingly difficult to come up with a measure of how much a set of suffixes show up on the  X  X ame stems X  that is not such that it favours the inclusion of any simply frequent, rather than truly contrasting, terminal segment. For example, the author has not had much success with standard vector similarity measures. Instead, we propose the following usage of co-occurence statistics. The measure presented is valid for an arbritrary set of suffixes (called P for  X  X aradigm X ) even though the relevance in this paper is for the case where | P | =2.
 First, for each suffix x , define its quotient function H x ( y ): S W  X  [0 , 1] as: where Stems ( x )= { z | zx  X  W } . The formula is conveying the following: We are given a suffix x , and we want to construct a quotient function which is a function from any other suffix to a score between 0 and 1. The score is calculated as: look at all the stems of x , other suffixes y will undoubtedly also occur on some of these stems. For each other suffix y , find the proportion of x :s stems on which y also appears. This proportion will be the quotient associated with y . Two examples of quotient functions (sorted on highest value) are given in table 3.
Now, given a set of affixes P , construct a rank by summing the quotient functions of the members of P : The x = y is just there so that the y :s that are also in P do not get an  X  X xtra X  1.0, since H x ( x )=1 . 0 regardless of the data. The rank is just y sorted on highest V P ( y ).

As an example, take W from the Swedish PAROLE-Corpus [5]. We can com-pare in table 4 the very common paradigm { a, an, as, ans, or, orna, ors, ornas } with the nonsense paradigm { ungen,ig,ar,ts,s,de,ende,er } consisting only of individually frequent suffixes. In table 4, the ranks of the member of P to the left are [0, 1, 2, 4, 6, 8, 22, 31], and for P to the right the ranks are [115044, 127, 17, 28, 4, 10, 100236, 14].

Now, if we can generalize from these cases it seems that we can rank different hypotheses of paradigms (of the same size) by looking at their quotient ranks. If the members of P  X  X urn up high in X  the quotient rank then the members of P tend to turn up on the same stems. There are several issues in formalizing the notion of  X  X urn up high in X . The places in the ranked list alone? Also incorporate the scores? Average place or total sum of places? For now we will just do a simple sum of places in the ranked list, divide by the optimum sum (which depends on |
P | and is 0 + ... + | P | X  1), and take the inverse. This gives a score between 0 and 1 where a high score means the members of P tend to appear on the same stems:
According to the desiderata 1 and 2 in section 3 (p. 329) we finally define an affix-systematicity likelihood score as: As a convention we set Z W ( )=0. 3.2 Escaping Thresholds The VI -score from the last section may be used for a greedy hill-climbing search through the affix set space. For example, we may start with an affix, a one member set, and see whether we can improve the affix score by including an-other member, and perhaps another after that until we can X  X  improve the score anymore. In this process, we may also entertain the possibility of kicking some member out if that improves the score  X  as long as there is no backtracking the search remains polynomial. Formally, define the growing function of a set P of affixes as:
Two growth-examples are shown in table 5, one which attains a perfect 1.0 score and one in which the original member is expelled in a later iteration.
Now, how does this help us work around a threshold for deciding how sys-tematically a pair of suffixes have to co-occur to conflate their stems? Recall the writing convention w 1 = xs 1 and w 2 = xs 2 . Instead of having a threshold we may conjecture that: For example, this predicts that sting and station are not the same stem because neither G  X  ( ing )= { ,e,ed,er,es,ing,s } contains  X  X tion X  nor does G  X  ( ation )= { ate, ated, ating, ation, ations } contain  X  X ng X . From our experience this test is quite powerful. However, there are of course cases where it predicts wrongly, due to the greedy nature of the G  X  -calculation, e.g G  X  ( ing ) does not contain  X  X rs X . Moreover, if one of the affixes is the empty affix, we need a special fix (see below). 3.3 Same-Stem Decision Algorithm We can now put all pieces together to define the full algorithm as shown in table 6.

If one of s 1 ,s 2 is the empty string then step 3 and 4 should be restated as follows (using s to denote the non-empty one of the two). The maximization to: answer YES/NO acccordingly as  X  G  X  ( s ).

The bad news is that the computation of the G  X  :s tends to be slow due to the summing and sorting of typically very long (50 000-ish items) lists. On my standard PC with a Python implementation it typically takes 30 seconds to decide whether two words share the same stem. Several authors, e.g [6,7], have evaluated their stemming algorithms on Informa-tion Retrieval performance. While IR is the undoubtedly the major application area we feel that evaluating on retrieval performance does not answer all relevant questions of stemming performance. For instance, a stemmer may make confla-tions and miss conflations that simply did not affect the test queries. In fact, one may get different best stemmers depending on the test collection. There is also difference as to whether the whole document collection, an abstract of each document or just the query is stemmed.

We find it more instructive to test stemming separately against a stemming gold standard and assess the relevance of stemming for IR by testing the stem-ming gold standard on IR performance. If stemming turns out to be relevant for IR, then researchers should continue to develop stemming algorithms towards the gold standard. In the other case, one wonder whether IR-improving term conflation methods should be called stemmers.

In order to assess the cross-linguistic applicability of our stemming algorithm we have chosen languages spanning spectrum of morphological typology  X  from isolating to highly suffixing  X  Maori, English, Swedish and Kuku Yalanji [8]. As training data we used only the set of words from a bible translation to emphasize the applicability to resource-scarce languages.

For these four languages we devised a stemming gold standard using [12,13] for Maori and [14,15] for Kuku Yalanji, languages not generally known to the author. So as not to let the test set be dominated by too many simple test cases, the selection of test set cases was done as follows: 1. Select a random word w 1 from W for the corresponding language 2. Select a random number i in 0  X  i  X | w 1 | X  1 3. Select a random word w 2 from the subset of words from W \{ w 1 } sharing i 4. Mark the pair w 1 ,w 2 to be of the same stem or not, according to traditional This was repeated until 200 pairs of words for each language had been selected, 100 same-stem and 100 not same-stem. Except for Maori where we could only really find 13 same-stem cases this way, all involving active-passive alternating verbs(describedindetailin[16]).

The evaluation results are shown in table 7. Errors fall into just one major type, in which the algorithm is too cautious to conflate; it is when two words do share the stem but where one of the suffixes is rather uncommon (possibly because it is really composite) and therefore it is not in the grow-set of the other suffix; for example Swedish skap-ade-s (past passive) and skap-are-n-s (agent-noun definite genitive). We also expected false positives in the form of random resemblances involing short words and short affixes; e.g as versus a but no such cases seem to have occurred in the test set in any of the languages.
We have done attempted a comparison with other existing stemmers, mainly because they tend not be aimed at an open set of languages and those which are, are really not fully supervised and we fear we might not do justice to them in setting parametres (see Related Work section). The widely known Porter stem-mer [17] for English scores exactly th e same result for English as our stemmer, which suggests than an unsupervised approach may come very close to explicitly human-informed stemmers. Many other stemmers, however, are superior to ours in the sense that they can stem a single word correctly whereas ours requires a pair of words to make a decision. This is especially relevant when large bodies of data needs to be stem-indexed as it would take quadratic time (in the number of words) in our setting. A full survey of stemming algorithms for specific languages or languages like English has more or less fully been done elsewhere (the technology becoming relatively mature cf. [18,19,6,7,20,21,22,23] and references therein). We will focus instead on unsupervised approaches for a wider class of languages.
 Melucci and Orio [7] present a very elegant unsupervised stemming model. While training does not require any manually annotated data, some architectural choices depending on the language still has to be supplied by a human. If this can be overcome in an easy way, it would be very interesting to test their Baum-Welch training approach versus the explicit heuristics in this paper, especially on a wider scope of languages than given in their paper. The unsupervised stemmer outlined in [6] actually requires a lot of parametres to be tweaked humanly and mainly targets languages with one-slot morphology.

Other systems for unsupervised learning of morphology which do not explic-itly do stemming could easily be transformed into stemmers. Work includes [24,25,26,27,28,29,30,31,32,33,34,35,36,37] and other articles by the same au-thors. All of these systems, however, require some parametre tweaking as it is and perhaps one more if transformed to stemmers, so there is still work want-ing before they can be compared on equal grounds to the stemmer described here. Given that they use essentially the same kind of evidence, it is likely that some of them, especially [38], will reach just as competitive results on the same task.

Of course, we also wish to acknowledge that traditional stemmers output the actual stem, which is one (significant) step further than deciding the same-stem problem for word pairs. We have presented a fully unsupervised human-intervention-free algorithm for stemming for an open class of languages showing very promising accuracy results. Since it does not rely on existing large data collections or other linguistic resources than raw text it is especially attractive for low-density languages. Although poly-nomial in time, it appears rather slow in practice and may not be suitable for stem-ming huge text collections. Future directions include investigating whether there is a speedier shortcut and a better, more systematic, approach to layered morphol-ogy i.e for languages which allow affixes to be stacked.
 The author has benefited much from discussions with Bengt Nordstr  X  om. We also wish to extend special thanks to ASEDA for granting access to electronic versions of the Kuku Yalanji bible texts.

