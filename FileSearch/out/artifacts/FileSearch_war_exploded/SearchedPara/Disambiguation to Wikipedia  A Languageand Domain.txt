 The possibility to understand multilingual content can facilitate better several natural science applications, such as information extraction, information re-trieval, web search, and web mining, where popular systems can provide enough accuracy just for the languages of inter est. Due to the lack of systems and re-sources, the adaptation of one method to other languages often create problems. This problem is even worse with low-reso urce languages. Th erefore, a general method for multilingual processing is very essential.

The ability to identify entities (such as people, locations and organizations) has been placed as an important task a nd has been considered as a valuable preprocessing step for many types of language analysis, including question an-swering, machine translation, and information retrieval. Its goal is the detection of entities and mentions in text, and labelling with one of several categories (PERSON or LOCATION). An entity (such as George W. Bush) can be refered to multiple surface forms (e.g.  X  X eorge Bush X  and  X  X ush X ) and a surface form can refer to multiple entities (e.g. two U.S presidents or the pianist Alan Bush).
We purpose the automatic construction of a large-scale, multilingual system that can recognize and disambiguate enti ties from a text for a general domain. As these texts can be in multiple languages, we target a method that can perform well in a multilingual environment. Also, when the query input is taken from the Bridgeman Art Library 1 , they are short texts which are informally written. They suffer from spelling mistakes, grammatical errors and the usually do not form a complete sentence. Our approach takes advantage of the human knowledge created through Wikipedia, a large-scale, collaborative web-based resource with collective efforts of millions of contributors.

Our work differs from that of previous researchers is that we have focused pri-marily on multilingual aspects as opposed to one single language, observed in all previous works [1 X 4]. We propose a framework in which dictionary and structures are extracted from the online encyclopedia and are employed to derive statistical measures which are then placed in a machine learner for disambiguation. Ours is a unsupervised approach, without requiring any human effort. To the best of our knowledge, it is the first attempt trying to adapt and apply disambiguation to Wikipedia to multiple languages and general domain, which is more difficult than previous wokrs on only English.

In this work, we revisit previous work an d propose two sets of new features: se-mantic relatedness and contextual features, experimented in different languages (English, Italian, Polish). The relatedness features are computed from the set of incoming links, commonness and keyphras eness in different formulations, while contextual features are adapted from pr evious work [1], combined, weighted and averaged with our new relatedness features. Both kinds of features yield im-provement in all three languages, both wh en injected separately and when used in combination together. In addition, we evaluate the contribution of these novel features in different domains (Wikipedia, newswire, query logs).

The structure of the paper is as follows. In Section 2 we discuss previous work on using Wikipedia for entity disambiguation and linking. In Section 3 we present our framework, the knowledge and statistical measures extracted from Wikipedia. In Section 4 we present the adaptation of the traditional D2W to other languages and domains; the experimental setting used to evaluate these methods and the dataset. The results we obtained and future works are discussed in Section 5 and 6. 2.1 Disambiguation to Wikipedia Entity disambiguation refers to the det ection and association of text segments with entities defined in an external repository. Disambiguation to Wikipedia ( D2W ) refers to the task of detecting and linking expressions in text to their referent Wikipedia pages. In the last decade, substantial consideration has been given to the use of encyclopedic knowledge for disambiguation. Using encyclope-dic knowledge for entity disambiguation has been pioneered by [5]. Subsequent works have been intensively exploited Wikipedia link structures and made use of natural annotations as the source for learning [1 X 4].

For example, given a text  X  X ohn McCart hy,  X  X reat man X  of computer science, wins major award. X , a D2W system can detect the text  X  X ohn McCarthy X  and link to the correct Wikipedia page John Mc Carthy (computer scientist) 2 ,in-stead of other John McCarthy who are ambassador, senator or linguist. 2.2 Previous Work One of the shortcoming of previous studies on D2W is that they have mainly focused on only one language. For instance, [5] has focused on linking only named entities in Wikipedia articles. The author first propose some heuristics to define if a Wikipedia title correspond to a named entity or not, then exploits the redirect pages and disambiguation pages to collect other ambiguous names to build the dataset. Overlapping categories between pages with traditional bag-of-words and TF-IDF measures are lastly employed in a kernel function for name disambiguation. This work, however, targets only named entities and uses only a small subset of Wikipedia articles (roughly half of a million). [3] presents a method for named entity disambiguation based on Wikipedia. They first extract resources and context for each entity from the whole Wikipedia collection, then use named entity recogn izer in combination with other heuris-tics to identify named entity boundaries in the articles. Finally, they employ a vector space model which includes context and categories for each entity for the disambiguation process. The approach works very well with high disambigua-tion accuracy. However, note that the us e of many heuristics and named entity recognizer shows one weakness of the method that is difficult to adapt to other languages as well as other content types such as noisy text. [1] proposes a general approach for D2W .First,theyprocessthewhole Wikipedia and collect set of incoming/outgoing links for each page. They employ a statistical method for detecting links by gathering all n-gram in the document and retaining those whose probability exceeds a threshold. For entity disam-biguation they use machine learning with a few features, such as the common-ness of a surface form, its relative relatedness in the surrounding context and the balance of these two features. Howev er, note that they have never tried to adapt this method to other kinds of text or other languages. [4] tries to combines local and global approaches for the D2W task with a set of local features in combination with global features. Based on traditional bag-of-words and TF-IDF measures, semantic relatedness, they implement a global approach for D2W . However, their system makes use of many natural language specific tool, such as named entity recognition, chunking and part-of-speech tagging. Thus, it is very difficult to apply the method to noisy text as well as to adapt to other languages.

Recently [6] adapted the D2W to microblogs in Twitter. To cope with the lack a rich disambiguation context, they proposed two context expansion methods based on tweet authorship and topic-based clustering. Still this work was focused on only English and have to heavily adapt since it makes use of [4] and popular natural language software which are very difficult to find for other languages.
Previous approaches to D2W differ with respect to the following aspects: 1. the corpora they address; 2. the type of the text expression they target to link; 3. the way they define and use the disambiguation context for each entity. For instance, some methods focus on linking only named entities, such as [3, 5]. In [3], the author defines the disambiguation context by using some heuristics such as entities mentioned in the first paragraph and those for which the corresponding pages refer back to the target entity. [1] u tilizes entities which have no ambiguous names as local context and also to com pute semantic relatedness. A different method is observed in [4] where they first train a local disambiguation system and then use the prediction score of that as disambiguation context.
It is worth noting that all of the aforementioned methods have been proposed and tested in only English. In the previous work [7], we have implemented a baseline for D2W in English, tested with a standard dataset and with query blogs. In this work, we revisit previous work and propose two sets of new fea-tures: semantic relatedness and contextual features. The relatedness features are computed from the set of incoming links , commonness and keyphraseness, while contextual features are adapted from pr evious work [1], combined, weighted and averaged with our new relatedness features. Both kinds of features yield im-provement in all three languages, both wh en injected separately and when used in combination together.

An example of disambiguation context is shown in figure 1. The disambigua-tion context of the surface form human may includes other unambiguous surface forms (which is linked to only one target article), which is a subset of commu-nication , linguistics , dialects , Natural languages , spoken , signed ,etc. 3.1 Datasets We evaluate our D2W on seven datasets, in which four are from previous work. The first three datasets, from [4], are constructed using Wikipedia itself. [4] used a sample of 10,000 paragraphs from Wikipedia pages. Mentions in this data set correspond to existing hyperlinks in the Wikipedia text. However, in our work, we derive three datasets from that, samples of 500/1,000/1,500paragraphs, respectively. We prove that a data of only about 500 paragraphs is sufficient for training, other datasets of increa sing size yield similar results.

The fourth dataset, from [1], is a subset of the AQUAINT corpus of newswire text that is annotated to mimic the hyperlink structure in Wikipedia. The fifth dataset is a sample of 500 Italian Wikipedia articles. The sixth dataset is a sample of 500 Polish Wikipedia articles. To evaluate our approach on noisy text, we use the seventh dataset, a collection of 1000 queries. The annotators are asked to link the first five nominal mentions of each co-reference chain to Wikipedia, if possible. In the last dataset, we used a corpus of query logs provided by the Bridgeman Art Library (BAL). Bridgeman Art library contains a large repository of images coming from 8000 collections and representing more than 29.000 artists. From 6-month query logs, we sample 1000 queries containing around 1,556 tokens. Queries are typed by users of the art library, using Bridgeman query language constructions. Each query is a text snippet containing some name of an artist, a painter, a painting, or an art movement. These texts often present spelling errors, capitalization mistakes, abbreviations, and non-standard words. The length of each query varies from 1 to 10 words; in average, there are 3 words per query. Some examples of the queries are shown in table 1.
 3.2 Structures in Wikipedia In this section, we describe the categorization of articles in Wikipedia collection. Category Pages. A category page defines a category in Wikipedia. For exam-ple the article Category:Impressionist painters lists painters of the Impressionist style. As it contains subcategories, one can use it to build the taxonomy between categories or to generalize a speci fic category to its parent level. Disambiguation Pages. A disambiguation page presents the problem of poly-semy, the tendency of a surface form to relate to multiple entities. For example, the surface form tree might refer to a woody plant, a hi erarchical data structure in a graphical form, or a computer scienc e concept. The correct sense depends on the context of the surface form to which we are comparing it to; consider the relatedness of tree to algorithm ,and tree to plant .
 List of Pages. A list of page , as its name indicates, lists all articles related to the specific domain. It is useful when we want to build some domain-dependent applications. For example one can employ the article Lists of painters to take the information of all popular painters and build an entity disambiguator. Redirect Pages. A redirect page exists for each alternative name to refer to an entity in Wikipedia. The name is transformed into a title whose article contains a redirect link to the actual article for that entity. For example, Da Vinci is the surname of Leonardo da Vinci . It is therefore an alternative name for the artist, and consequently the article Da Vinci is just a pointer to the article Leonardo da Vinci .
 Relevant Pages. A relevant page is remained after scanning over the whole Wikipedia collection and excluding category pages, disambiguation pages, list of pages, redirect pages, and pages used to provide help, define template and Wikipedia definitions.
 Unrelevant Pages. Unrelevant pages are those used to define some terms, some templates or provide some help ( Template:Periodic table and Help:Editing for example). 3.3 Parsing Wikipedia Dump As parsing the Wikipedia dump we perform a structure analysis of running text. We use the pages-articles.xml that contains current version of all article pages, templates, and other pages. The parser takes as input the Wikipedia dump file and analyzes the content enclosed in the various XML tags. We use the Wikipedia dump in Jul y 2011. In the first running pa rse, it builds the set of redirection pairs (i.e., one article is a pointer to some actual one), list of pages, disambiguation pages, and set of titles of all articles in that Wikipedia edition.
In the second parse, the system scans over all Wikipedia articles and construct list of links (i.e., surface form and target article for each link, number of times one surface form is linked to the target article), list of incoming and outgoing links of each article. Note that we use the set o f redirection pairs to keep everything related to links only as the actual article and not as the redirected article. For example, in the example above, if the title Da Vinci appears in one link, we will change it to Leonardo da Vinci . This process makes the statistical measures derived from links more accurate.

The third parse focuses mainly on individual pages. It scans over all Wikipedia articles and construct for each article: I D, title, set of links (with surface form and target article), set of categories, set of templates. Note that in all three times of parsing, we exclude the category pages, disambiguation pages, file pages, help pages, list of pages, pages refering to templates and Wikipedia itself. As a result, we keep only the most relevant pages with textual content. 3.4 Extracting Dictionaries and Structures The set of Wikipedia titles and surface forms are preprocessed by casefolding. After scanning over all Wikipedia articles, we construct a set of titles, set of surface forms (i.e., words or phrases that are used to link to Wikipedia arti-cles), set of files (i.e., a Wikipedia arti cle but does not have textual content, File:The Marie Louise X  X  diadem.JPG for example), and set of links (with sur-face form and target article for each link). We use the term surface form to denote the occurence of a mention inside a Wikipedia article and the term target article to denote the target Wikipedia article that surface form linked to.
Table 3 shows the dictionaries we extracted with corresponding size. Table 4 depicts links with corresponding surface forms and target articles derived from the Wikipedia paragraph in figure 1. We use the dictionaries of titles, surface forms, and files to match with the textu al content and detect entity/mention boundaries, whereas set of links are used to compute statistical measures for our learning framework. 3.5 Statistical Measures Following previous work [1, 4], we develop a machine learning approach for dis-ambiguation, based on the links available in Wikipedia articles for training. For every surface form in an article, a Wikipedian has manually selected the correct destination to represent the int ended sense of the anchored text. This provides millions of manually defined ground-truth examples (see table 3) to learn from.

From the dictionaries and structures extracted from Wikipedia, we derive statistical measures. We now describe the primitives for our statistical measures and learning framework. -s : a surface form (anchor text) -t : a target (a link) -W : the entire Wikipedia -t in : pages that link to t ( X  X ncoming links X ) -t out : pages going from t ( X  X utgoing links X ) -count ( s link ): number of pages in which s appears as a link -count ( s ): total number of pages in which s appears -p ( t | s ): number of times s appears as a link to t -p ( s ): number of times s appears as a link -| W | : total number of pages in Wikipedia Keyphraseness. Keyphraseness is the probability of a phrase to be a potential candidate to link to a Wikipedia article. Similarly as [8], to identify important words and phrases in a document, we first extract all word n-grams. For each n-grams a , we compute its probability of being a potential candidate. Commonness. Commonness is the probability of a phrase s to link to a specific Wikipedia article t .
 Relatedness. To measure the similarity between two terms, we consider each term as a representative Wikipedia article. For instance, the term exhibition is represented by the Wikipedia page http : //en.wikipedia.org/wiki/Exhibition . We used the Normalized Google Distance [9], where the similarity judgement is based on term occurrence on web pages. The method was employed in [10] to compute relatedness between Wikipedia articles. In this method, pages that link to both terms suggest relatedness.
 where a and b are the two articles of interest, A and B are the sets of all articles that link to a and b , respectively, and W is the entire Wikipedia. We follow a two-phase approach for disambiguation algorithm: first recognize mentions (i.e. surface forms) in a document with potential candidates, then generate features for each candidate and apply a machine learning approach. 4.1 Disambiguation Candidate Selection The first step is to extract all mentions that can refer to Wikipedia articles, and to construct, for each recognized mention, a set of disambiguation candidates. Fol-lowing previous work [1, 4], we use Wikipedia hyperlinks to perform these steps.
To identify important words and phrases in a document, we first extract all word n-grams. For each n-grams a , we use its keyphraseness X  the probability of being a potential candidate to recognize potential n-grams that are candidates to link to an Wikipedia article. We retain n-grams whose keyphraseness exceeds a certain threshold. Preliminary experiments on a validation set showed that the best performance for mention detection is achieved with keyphraseness =0 . 01.
The next step is to identify potential candidate links. For that we employ commonness X  the probability of an n-gram to link to a specific Wikipedia article. The result is a set of possible mappings ( surfaceform, link i ). For computational efficiency, we use the top 10 candi dates with highest commonness. 4.2 Learning Method As illustrated, disambiguation deals with the problem that the same surface form may refer to one or more entities. The links ( surface form / target article ) derived from Wikipedia provide a dataset of disa mbiguated occurences of entities. Table 5 shows examples of positive and negative examples in the dataset, created for the six separate concepts of the surface form human .

Given a surface form with its potential candidates, the entity disambiguation problem can be cast as a ranking problem. Assuming that an appropriate scoring function score ( s, t i ) is available, the link corresponding to surface form s is defined to be the one with highest score: 4.3 Baseline and Features Baseline. As a baseline we use the commonness, which is the fraction of times the title t is the target page for a surface form s . This single feature is a very reliable indicator of the correct disambiguation [1, 4], and we use it as a baseline in our experiments.
 Relatedness and Contextual Features. As ranking algorithm for disambigua-tion, we use three kinds of features: the commonness of each candidate link, its average relatedness in the surrounding context, and the disambiguation con-text measure. We follow a global approach where the disambiguation context is measured by taking every n-grams that have only one candidate, and thus, is unambiguous. Here we use the term  X  X andidate sense X  to refer to a potential candidate and  X  X ontext article X  as a unambiguous candidate.

The relatedness of each candidate sense i s the weighted average of its related-ness to each context article. To weight c ontext articles, we use the two measure mentioned previously. First, keyphraseness can help to identify how often a sur-face word is used as a link. Second, commonness can help to test how often a surface word is linked to that context article. where c  X  C are the context articles of t . The target with the highest score will be chosen as the final answer.

To balance commonness and relatedness, we need take into account both how frequent a surface form is and how good the context is. To measure disambigua-tion context, [1] used the sum of the weights that were previously assigned for each unambigous surface form. In our approach, we use the weight as defined previously and compute the average weighted of every context article. The data and experiments in this paper are based on a version of Wikipedia that was released in July 2011. For the learning machine we use LibSVM 3 [11] as our disambiguation classifier. The results are shown from tables 6 to 9. First, the results achieved with standard datasets in table 6 prove that ours are competitive to those of state-of-the-art systems [1, 4] and with the AQUAINT dataset it achieves highest performance. To test if a larger dataset is more effective for learning a D2W system, we ex-periment with datasets in increasing size, from 500 to 1,000 and 1,500 Wikipedia paragraphs, respectively. As shown in table 7, more data provide similar accu-racy. It means that only a sufficient amount of data is needed for D2W training.
Table 8 shows our results in three data sets with the baseline, when injected with only the relatedness and when all features are used. In all datasets, the average relatedness proves very effectiv e with consistent improvement across different datasets and languages. Whereas table 9 show the Wikification results in three languages and with query logs. Both Italian and Polish are about 22% of size of English Wikipedia (regarding the number of pages, as seen in table 2), but the decrease in performance is very large for Polish. The reason may lie in the accents and special characters in Polish. Also, we observe that Polish Wikipedia contains much shorter texts with less st ructured information (number of links) and still some information is missing in Polish Wikipedia pages (e.g. file and image information). Lastly, the results with query logs show that a statistical approach is useful for entity recognition and disambiguation on noisy text such as query logs and the exploitation of large-scale resource can compensate for the lack of language context.
 The results show that a statistical approach is useful for semantic disambigua-tion on multiple languages (English, Italian, Polish) and on different genres (Wikipedia, newswire, query logs). Results show that the exploitation of large-scale resource can help for noisy text whereas a language-independent approach is still feasible. Note that we achieve competitive performance with previous works [1, 4] with standard datasets in English. Our approach does not make use of any natural language specific tool, such as named entity recognition, POS tagger and can easily adapt to other languages in the same manner, with very little adaptation.
 Acknowledgements. This work is supported by State Secretariat for Edu-cation, Research and Innovation (SERI) project nr. C11.0128 title  X  X utomatic Web data collection from n on-reactive sources by me ans of normative systems and Semantic Web Technologies X , which is connected to the COST Action IS1004 WEBDATANET (http://webdatanet.eu). We would like to thank Massimo Poe-sio for useful discussions.

