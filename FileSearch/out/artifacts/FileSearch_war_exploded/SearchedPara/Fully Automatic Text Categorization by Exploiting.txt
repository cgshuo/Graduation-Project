 Supervised learning is the dominant approach for Text Categorization (TC) [15] [21]. Its performance depends heavily on the quantity and quality of hand-labeled docu-ments. To reduce the burden of manual labeling, semi-supervised approaches [19] use the knowledge from both labeled and unlabeled data for classifier training. 
This paper proposes FACT approach. Its underlying assumption is that the cate-many real applications with good human-computer interfaces. FACT employs the semantic of the category name and the hidden knowledge of the document set for representative keywords for each category, which serves as the Representative Profile (RP) of the category to initially label a set of documents. Second, the document clus-learning. 
The key of FACT approach is automatic document labeling. Its basic idea derived from following observation: Given the category name, the prerequisite for the human concepts implied by the category name. With the knowledge in mind to read the docu-their memories serves as the intermediate to link the categories and documents EuroWordNet, CoreNet, and HowNet, an intuitive way to simulate human X  X  docu-ment labeling is to use these lexical resources to provide the same functionality as that implied in the category name and their definitions in WordNet are used as a bridge to provide the linkage between the category and the unlabeled documents. Both supervised [15] and semi-supervised [3] [8] [11] TC methods treat category name only as symbolic labels that assume no additional knowledge about them avail-the category name is used to supervise the classifier learning. Then, no manual label-ing efforts is required in FACT. 
FACT is closely related to unsupervised TC approaches. Without labeled docu-ments, they utilize category names [1] or user-specified keywords [20] as the RP for training data building. Since it was hard for users to provide such keywords, [2] uses document clustering together with feature selection to find a set of important words to category. 
Our work also relates to applying WordNet for automatic TC. [4] proposed to util-ize the synonyms in WordNet to improve TC. Several similar techniques are reported to incorporate the synonyms [9], hypernyms [12], hyponyms [13], meronyms and holonyms [16] of words found in the training documents for classifier training. These researches mainly focus on incorporating WordNet to improve the TC model, where the labeled documents are still used. They are different from FACT since we need no labeled data. Given a set of categories C and a set of unlabeled documents D , our FACT consists of four steps: (1) Initial document labeling; (2) Refinement of the initial document label-of FACT. 3.1 Initial Document Labeling sub-steps: 1) Category name understanding; 2) RP generation; 3) Initial document labeling. 1) Category Name Understanding phrase that can be found in WordNet. After the stop-words (conjunctions, preposi-concepts of the category. They will serve as th e seed features to be extended as the RP of corresponding category. 
WordNet organizes English nouns, verbs, adjectives, and adverbs into synonym sets, called synsets. Different relationships are defined to link these synsets. One sim-relevant words as the RP of the category. However, there are generally multiple senses (synsets) for each seed word. The homonyms or polysemes could introduce should be considered. 
Similar to existing WSD methods [7] using the surrounding words in the sentence to select the correct sense, we propose a sense ranking approach for a word w in the name of category c j  X  C by using the contextual words appeared in the names of cate-structure inside). For the example given in Figure 1, spam and private emails are two categories. The words  X  X rivate X  and  X  X mail X  is the contextual words of  X  X pam X . 
Each word sense in WordNet is represented as a synset. It has a gloss that provides a linguistic micro-context for that sense. The senses of word w are ranked according found in the category names in C . words { cw 1 , cw 2 , ..., cw n }. Each word cw i has m i senses { i m the word sense ranks are calculated as follows: 1. For each word sense w r , 1  X  r  X  m , its relatedness with k 3. For each sense w r , its rank is calculated: Rank ( w r )=  X  X  X  where each sense of the contextual word is weighted by its frequency count f r to indi-cate how important the word sense is. common with its contextual words. The real meaning of the words in a category name might cover several senses, i.e., several senses defined in WordNet might be relevant to the concepts implied by the category name. So, different from traditional WSD that selects only one correct sense, we here need to choose several of them. Intuitively, we threshold value is difficult to tune. To handle such a problem, we use a compromised policy, i.e., the higher value of m , the lower value of t is set. 2) RP Generation keywords for each category, which will serve as the RP rp j of corresponding category c  X  C . The basic idea is to use the multiple relations defined in WordNet to extract the semantically related words as extended features for each category. synonyms, hypernyms, hyponyms, meronyms, and holonyms that are identified using are also used as the representative keywords.

For the category name with multiple words: Besides the keywords found by the multiple semantic relations in WordNet, new phrases are also constructed as a part of the rp j by automatic synonym substitution. It means that the synonyms are utilized to substitute corresponding word to construct a new phrase. For example, a category name is  X  X pam detection X , the synonym of  X  X pam X  is  X  X unk e-mail X , then  X  X unk e-mail detection X  is also selected as the ex tended features of corresponding category.
Considering that the more relevant sense should contribute more to the TC model, we use the ranking value of each sense to assign the weight to the extended features from this sense, i.e., the word sense with higher rank value will play a more impor-tant role in identifying the relevant documents as training data. Intuitively, a the weighting policies. 3) Initial Document Labeling Once the rp for each category is constructed, we then apply it for probabilistic docu-ment labeling. The probabilistic labeling is based on the similarity of each document d  X  D with each rp larity. It is based on the Vector Space Model (VSM) of the representations of the rp j s ( d i , c j ) is obtained to indicate similarity between d i and rp j . 
Using the similarity scores between categories and documents, we can automati-category. 
The initial probability value reflects to what extend a document belongs to the category. It is generated only based on the similarity sores between the document and the RP of corresponding category. There are many cases that one document has mul-tiple probability values regarding to different categories. So, for each c j  X  C , assuming mx normalize the possibility value across multi-categories, 3.2 Refinement of the Initial Document Labeling document clustering to adjust the initial probability value of the label. 
The adjustment is conducted by an alignment model based on the Bayesian infer-c'  X  C' , based on the Bayesian probability, we have two components could be obtained with following two equations: 3.3 Training Data Construction and Classifier Building documents, the training data are generated automatically. described as: 1) The top p + % documents of the list are selected as positive samples for c ; 2) The documents at the top p -% of the list from other categories are identified as with higher quality. They need to be tuned carefully for the final classifier building. 
There might be that multiple categories share the same documents as positive sam-value. 
With the constructed training data, classifiers are built by two discriminative meth-ods, i.e., SVM [17] and TSVM [8]. 1) Experiments Setup Three English datasets, i.e, 20-NewsGroup (20NP), Reuters-21578, and WebKB, were used. WebKB data set consists of a collection of web pages gathered from university computer science departments, which are divided into seven categories. Our experi-ment uses four most populous categories: student, faculty, course and project X  X ll together containing 4199 pages. Reuters-21578 is a collection of documents from the Reuters. As many researches [8][15] were conducted, only the most populous 10 Grain, Crude, Trade, Interest, Ship, Wheat and Corn. There are totally 9296 docu-obtain the training and test data. 20NP is a collection of approximately 20000 articles from 20 different UseNet discussion groups. There are 1000 news documents for each NP. For simplicity, we select two of the 4 main categories, i.e., Science (SCI), Com-ries, 4 in Science and 5 in Computing . Since we assume the category name should be specified as normal words in natural language, the abbreviation of the category name For baseline approaches, the given training and test sets are used. 
For each document, the title and body are extracted as a single feature vector. After the stop-words elimination and stemming, we select the traditional tf-idf term weight-ing scheme as our document representation model. In our experiment, the SVM-light package [8] is employed for the implementation of SVM and TSVM, where a linear kernel is used, and the weight C of the slack variables is set to default. 2) Evaluation Criteria measure. For the multi-classification problem, we adopt both document-centric mi-croaveraging F1 measure and category-centric macroaveraging F1. The accuracy, i.e., the proportion of documents with correct labels, is adopted to measure the quality of document labeling and the used training data. 3) Experiment Results We conduct several experiments to tune the parameters of our algorithm. Category name understanding: The compromised policy for selecting the top t % from the ranking result of m senses is given in Figure 2. 
Actually, several experiments on word sense selection have been conducted to tune defined in Fig. 2 have almost no effect on the final classification results. Initial document labeling: We change the percentage pc % ranging from 10-100% to Reuters-21578, its category names have the best quality to represent the content of its documents. However, the category names used in WebKB don X  X  reflect appropriately its document contents. Also, we observe that, for Reuters-21578 and 20NP, the quali-ties of the document labeling decrease monotonously with pc . It is consistent with the initial assumption that smaller pc means more reliable labeled data. But for WebKB, there are biased keywords in the RPs. Refinement of the probabilistic labeling: The k -means algorithm is adopted for document clustering. We use Weka [6] for its implementation. The double of the actual data refinement. 
The value setting of k does not matter much for FACT as long as it is not too small (generally, the value of k should be bigger than the actual number of the categories). the results are very similar to that shown in Figure 3. Training data construction: We investigate the accuracy of the constructed training positive samples for category c j . Then we can decide how many negative samples are selected p -. that, the more negative (or positive) samples is involved in the training data, the lower that of labeled data in Figure 3. are based on empirical experiments which will be explained below. 
We select SVM and TSVM as baseline approaches. For the baseline, the given training and test data from each of the three datasets are used for building and evaluat-ing the classifiers. We implement our FACT approach into two versions, i.e., SVM+WordNet and TSVM+WordNet . SVM(TSVM)+WordNet means that all the data automatically; then SVM (TSVM) classifier is learned; and the test data are used for its evaluation. 
Table 1 illustrates the TC results. We observe that, when SVM+WordNet is used to categorize the Reuters-21578 and 20NP, FACT can achieve more than 90% of F1 performance of the baseline SVM methods. It proves the effectiveness of FACT. 
The unsupervised method given in [1], which uses only the category name to boot-mantic knowledge of the category name defined in WordNet (as an external knowl-edge source) to bridge the categories and documents, as shown in Table 1, the SVM+WordNet classifier outperforms it. The TC classifier obtained by labeling words [2] has similar performance comparing to supervised approach. As mentioned in [1], its reason may be the easier TC task and the weaker NB classifier. Actually, the human intervention in selecting important words for each category might result in the that their TC results have better quality comparing with our FACT using WordNet to generate the representative words automatically. 
For the phenomenon that SVM with inaccurate and insufficient training data could training data is more compact than the training data used for baseline method. Since we select top ranked documents from the initially labeled documents as training data, the similarity computing between the documents and RPs of the category realizes positive and negative examples much wider than the baseline method. Thus, it might phenomenon. 
However, for WebKB, FACT is worse than the baselines. It is because WebKB consists of web pages where many words have no semantic relation with the category name, e.g., Email, date, phone, etc. Then, the limitation of FACT is that it is sensitive ments are semantically consistent to the cat egory name. But considering the fact that for a real application with good human-com puter interface, the category are generally gory labels) with good quality. 
For the baselines, since the knowledge implied in the unlabeled documents is ex-ploited, TSVM classifier outperforms SVM classifier. However, SVM+WordNet outperforms TSVM+WordNet a lot on Reuters-21578 and 20NP. This can be ex-negative and positive documents are separated with a wide margin is decreased a lot comparing that for SVM. 
Regarding training data selection from the initial labeled documents, we conduct results. For 20NP and Reuters-21578, we can see that, when p + &lt; 0.5, with the increase within 1~3, and then decrease slowly. However, for p +  X  0.5, the F1 measure increase monotonously with a . We know that, (1) generally, when more training data are util-labeled documents are utilized, more incorrectly labeled documents might be intro-duced into the training data, which will degrade the categorization results. The above phenomena can be explained by the tradeoff between these two trends of (1) and (2). major role. It causes the enhanced categorization performance with the increase of a . make trend (1) play the principle role. The performance of the classifier grows slowly. However, for WebKB, F1 doesn X  X  have the similar behavior. The reason might be that its category name is not semantically consistent with document contents. 
Figure 5 shows that, when p + is within 0.1~0.2, and a within 1~3, the learned clas-sifier has almost the best performance. It is generally true for the three datasets. Then, ance ranges. The contribution of WordNet: To evaluate the impact of WordNet on the results, additional experiments with SVM were done: Without WordNet , where the RP only contains the words from category names. The results are also shown in Table 1. Since SVM+WordNet outperforms without WordNet significantly. It demonstrates that advantage than applying a simple query only consisting in the name of the category. Also, the contribution of WordNet for WebKB is not as notable as for 20NP and Reuters. It might be due to that WebKB X  X  category names are not consistent with its documents, which make the contribution of generated features from WordNet is not stable (although it affect the results obtained without WordNet as well). This paper proposes FACT for fully automatic TC. With the support of WordNet, the semantics of the category name are utilized for automatic document labeling. The document clustering is employed to reduce the possible biases derived from the cate-gory name and WordNet. The experiments show that, when the given category name has a clear representation of the topics desc ribed in the content of the documents, its performance is very close to the supervised method. Our future work will analyze its gual TC tasks. 
