
Predicting the performance of web queries is useful for several applications such as automatic query reformulation and automatic spell correction. In the web environment, accurate performance prediction is challenging because measures such as clarity that work well on homogeneous TREC-like collections, are not as effective and are often expensive to compute. We present Rank-time Per-formance Prediction (RAPP), an effective and efficient approach for online performance prediction on the web. RAPP uses retrieval scores, and aggregates of the rank-time features used by the document-ranking algorithm to train regressors for query performance predic-tion. On a set of over 12,000 queries sampled from the query logs of a major search engine, RAPP achieves a linear correlation of 0.78 with DCG @ 5, and 0.52 with NDCG @ 5. Analysis of predic-tion accuracy shows that hard queries are easier to identify while easy queries are harder to identify.
 Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Algorithms, Experimentation, Theory Keywords: Performance prediction, Query difficulty, Web search
Query performance prediction is the task of estimating the qual-ity of the results retrieved for a query, using effectiveness measures such as normalized discounted cumulative gain (NDCG). Perfor-mance prediction is useful for various applications such as detect-ing queries with no relevant content, performing selective query ex-pansion, and to merge results in a distributed information retrieval system [5]. However, most post-retrieval query performance pre-dictors are expensive to compute, and not well-suited for the web.
The key idea behind RAPP is to use retrieval scores and fea-tures that are available to the retrieval algorithm during ranking. Our choice of features is based on two observations. First, the retrieval scores of the top-ranked documents are good indicators of document relevance and therefore are good estimators of re-trieval effectiveness. Retrieval score-based features have previ-ously been shown to be effective for classifying TREC queries as easy or hard [4]. Second, Web search engines use retrieval algo-rithms that combine query dependent and query-independent doc-ument features that are designed to capture relevance. The perfor-mance of a query is intimately related to these feature values. Since retrieval scores for different queries may not be directly compara-ble, we use statistical aggregates of the scores. Moreover, statistical aggregates such as maximum, mean, and standard deviation cap-ture different aspects of the quality of search results. For example, our initial analysis showed that retrieval scores for low-performing queries tend to have low mean and high variance.

Figure 1 illustrates RAPP. First, we retrieve the top k documents using the retrieval algorithm.We use the retrieval scores as well as the query-dependent and query-independent features of these top-ranking documents. We then compute statistical aggregates such as mean, maximum, standard deviation, variance, and coefficient of dispersion of these features. Finally, we use these aggregated features and the individual retrieval scores to train a regressor to predict a target performance measure.
To evaluate RAPP, we target the prediction of two performance measures commonly used in web search: DCG@5 1 and NDCG@5 (referred as DCG and NDCG henceforth). We use a set of 12,185 queries, which were obtained as a frequency-weighted random sam-ple from the query logs of a major web search engine. For retrieval we use LambdaRank [2], an effective learning to rank algorithm for the web. For each query in our collection, we create feature vectors as follows. First, we use LambdaRank to assign scores and rank documents on the Web 2 . Our implementation uses several retrieval features such as BM25F-based features, click-based fea-tures, query length, and other query-independent features such as variants of PageRank. For each of these retrieval features we create statistical aggregates as listed in Section 1. Next, we select the top 100 aggregates (referred to as regression features henceforth) that have the highest linear correlation with the target metric on a set of training queries. Some example features include clickboost_max (maximum value of a click-based feature) and score_stdev (stan-dard deviation of LambdaRank scores). Finally, we create a query performance prediction dataset by associating with each query, the performance metric, DCG@5 or NDCG@5 and the regression fea-tures. On this dataset, we conduct 3-fold cross-validation experi-ments to train linear as well as non-linear regressors based on the Random Forest algorithm [6] 3 .

Clarity comparison. We use Clarity [3], a competitive perfor-mance prediction technique, as an experimental baseline. To com-pute Clarity for a query, we use a query model built from the top
Normalized by perfect DCG@5 to scale values to (0,1).
LambdaRank was trained on an entirely different data set.
We use the R package implementation with default parameters. 50 results returned by the search engine. Because Clarity compu-tation is expensive, we calculated Clarity only for a random subset of 600 queries drawn from our original query set. Table 1 shows the results of performance prediction for DCG and NDCG using Clarity as well as selected features used in RAPP. Clarity achieves very low linear correlation with both DCG and NDCG. When com-pared to the performance of features used in RAPP, even the lowest performing individual feature outperforms Clarity. This suggests that while Clarity is a competitive measure in smaller TREC col-lections, it is not a well-suited for the Web.
 Table 1: Clarity comparison: Average  X  the average correlation of
Table 2 shows the prediction accuracy for RAPP in terms of lin-ear correlation and root mean squared error (RMSE). Both pre-dicted DCG and NDCG values achieve a high linear correlation and low RMSE. Also, NDCG prediction is much worse as indi-cated by the low correlation and the higher RMSE values. This is mainly because NDCG is a non-linear metric that is calculated based on the actual number of relevant documents that exist in the collection. Thus NDCG cannot be estimated based on the features of the top-ranked documents alone. Finally, in terms of correla-tion and RMSE, there is little difference in prediction effectiveness between simple linear regression and the non-linear random forest based regression.
 Table 2: RAPP Effectiveness: Corr.  X  Linear correlation measure.
The scatter plot in Figure 2(a) illustrate a strong correlation be-tween the predicted and actual DCG values for one fold of the data. Figure 2(b) shows predicted NDCG values which are not as strongly correlated with the actual values. For DCG, when the ac-tual values are less than 0.2, the predicted values are also less than 0.2 in most cases. On the other hand, when the actual values are greater than 0.4 the predicted values are more spread out. This suggests, DCG prediction is more precise for hard queries than for average , and easy queries. Our preliminary analysis suggests that feature values for hard queries are most consistent (lower values) compared to easy queries.

Similarly, NDCG prediction is highly precise when predicted values are below 0.3. However, prediction effectiveness degrades quickly when predicted values are above 0.4. Thus, for both mea-sures, the high linear correlation and low RMSE values mask the rather poor effectiveness at the extremes.
Feature Importance. Next, we inspect the features used for regression. We consider three subsets: features based on 1) Lamb-daRank scores, 2) Click -based features, and 3) BM25F -based fea-tures. Table 3 shows the prediction effectiveness of the different feature groups for linear regression. For DCG, all feature groups achieve high correlation while for NDCG, click and BM25F fea-tures are substantially lower compared to the combined features. Also, relative feature importance differs for DCG and NDCG. For instance, click features are more important for predicting DCG than LambdaRank score features. The order is reversed for NDCG. Click-based features are strong predictors of user preference [1], and it is no surprise that they correlate well with DCG. However, NDCG being a non-linear metric, is harder to predict with click-based features alone. Also, we hypothesize that since LambdaRank combines several features including click features and is trained to optimize for NDCG, the LambdaRank-based features are better predictors than click-based features. Interestingly, we find that the click features for DCG and LambdaRank features for NDCG are as effective as all the features combined. This suggests that more careful feature selection can reduce run-time computations while retaining prediction effectiveness.
 Table 3: Feature Groups Effectiveness: Corr.  X  Linear correlation
In this paper, we describe RAPP, an effective and efficient Web query performance prediction technique that uses retrieval scores and retrieval features. Large scale evaluation using actual web queries shows that RAPP is effective, and outperforms the state-of-the-art Clarity baseline. Moreover, experimental results suggest that Clarity is not well-suited for the web. While RAPP is a general approach that can be used for different ranking algorithms and to target different measures, the results in this paper are based only on DCG @ 5 and NDCG @ 5 prediction for LambdaRank. We leave investigation of RAPP X  X  utility for other ranking algorithms and performance measures such as MAP as part of future work. This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-0910884. Any opin-ions, findings and conclusions or recommendations expressed here are the authors X  and do not necessarily reflect those of the sponsor.
