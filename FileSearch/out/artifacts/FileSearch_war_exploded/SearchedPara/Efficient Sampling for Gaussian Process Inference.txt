 Gaussian processes (GPs) are used for Bayesian non-parametric estimation of unobserved or latent functions. In regression problems with Gaussian likelihoods, inference in GP models is analytically tractable, while for classification deterministic approximate inference algorithms are widely used [16, 4, 5, 11]. However, in recent applications of GP models in systems biology [1] that require the estimation of ordinary differential equation models [2, 13, 8], the development of deterministic ap-proximations is difficult since the likelihood can be highly complex. Other applications of Gaussian processes where inference is intractable arise in spatio-temporal models and geostatistics and de-terministic approximations have also been developed there [14]. In this paper, we consider Markov chain Monte Carlo (MCMC) algorithms for inference in GP models. An advantage of MCMC over deterministic approximate inference is that it provides an arbitrarily precise approximation to the posterior distribution in the limit of long runs. Another advantage is that the sampling scheme will often not depend on details of the likelihood function, and is therefore very generally applicable. In order to benefit from the advantages of MCMC it is necessary to develop an efficient sampling strategy. This has proved to be particularly difficult in many GP applications, because the posterior distribution describes a highly correlated high-dimensional variable. Thus simple MCMC sampling schemes such as Gibbs sampling can be very inefficient. In this contribution we describe an effi-cient MCMC algorithm for sampling from the posterior process of a GP model which constructs the proposal distributions by utilizing the GP prior. This algorithm uses control variables which are auxiliary function values. At each iteration, the algorithm proposes new values for the control vari-ables and samples the function by drawing from the conditional GP prior. The control variables are highly informative points that provide a low dimensional representation of the function. The control input locations are found by minimizing an objective function. The objective function used is the expected least squares error of reconstructing the function values from the control variables, where the expectation is over the GP prior.
 We demonstrate the proposed MCMC algorithm on regression and classification problems and com-pare it with two Gibbs sampling schemes. We also apply the algorithm to inference in a systems biology model where a set of genes is regulated by a transcription factor protein [8]. This provides an example of a problem with a non-linear and non-factorized likelihood function. evaluated at those inputs. A Gaussian process places a prior on f which is a N -dimensional Gaussian distribution so that p ( f ) = N ( y |  X  , K ) . The mean  X  is typically zero and the covariance matrix K is defined by the kernel function k ( x n , x m ) that depends on parameters  X  . GPs are widely used for supervised learning [11] in which case we have a set of observed pairs ( y i , x i ) , where i = 1 , . . . , N , and we assume a likelihood model p ( y | f ) that depends on parameters  X  . For regression or classi-fication problems, the latent function values are evaluated at the observed inputs and the likelihood modelling latent functions in ordinary differential equations, the above factorization is not applica-ble. Assuming that we have obtained suitable values for the model parameters (  X  ,  X  ) inference over f is done by applying Bayes rule: For regression, where the likelihood is Gaussian, the above posterior is a Gaussian distribution that can be obtained using simple algebra. When the likelihood p ( y | f ) is non-Gaussian, computations become intractable and we need to carry out approximate inference.
 The MCMC algorithm we consider is the general Metropolis-Hastings (MH) algorithm [12]. Sup-pose we wish to sample from the posterior in eq. (1). The MH algorithm forms a Markov chain. We initialize f (0) and we consider a proposal distribution Q ( f ( t +1) | f ( t ) ) that allows us to draw a new state given the current state. The new state is accepted with probability min(1 , A ) where To apply this generic algorithm, we need to choose the proposal distribution Q . For GP models, finding a good proposal distribution is challenging since f is high dimensional and the posterior distribution can be highly correlated.
 To motivate the algorithm presented in section 2.1, we discuss two extreme options for specify-ing the proposal distribution Q . One simple way to choose Q is to set it equal to the GP prior p ( f ) . This gives us an independent MH algorithm [12]. However, sampling from the GP prior is very inefficient as it is unlikely to obtain a sample that will fit the data. Thus the Markov chain will get stuck in the same state for thousands of iterations. On the other hand, sampling from the prior is appealing because any generated sample satisfies the smoothness requirement imposed by the covariance function. Functions drawn from the posterior GP process should satisfy the same smoothness requirement as well.
 The other extreme choice for the proposal, that has been considered in [10], is to apply Gibbs sampling where we iteratively draw samples from each posterior conditional density p ( f i | f  X  i , y ) with f  X  i = f \ f i . However, Gibbs sampling can be extremely slow for densely discretized functions, as in the regression problem of Figure 1, where the posterior GP process is highly correlated. To caused by the conditioning on all remaining latent function values. For the one-dimensional example in Figure 1, Gibbs sampling is practically not applicable. We further study this issue in section 4. A similar algorithm to Gibbs sampling can be expressed by using the sequence of the conditional densities p ( f i | f  X  i ) as a proposal distribution for the MH algorithm 1 . We call this algorithm the Gibbs-like algorithm. This algorithm can exhibit a high acceptance rate, but it is inefficient to sample from highly correlated functions. A simple generalization of the Gibbs-like algorithm that is more appropriate for sampling from smooth functions is to divide the domain of the function into regions and sample the entire function within each region by conditioning on the remaining function regions. Local region sampling iteratively draws each block of functions values f k from to sample from highly correlated functions since the variance of the proposal distribution can be very small close to the boundaries between neighbouring function regions. The description of this algorithm is given in the supplementary material. In the next section we discuss an algorithm using control variables that can efficiently sample from highly correlated functions. 2.1 Sampling using control variables Let f c be a set of M auxiliary function values that are evaluated at inputs X c and drawn from the GP prior. We call f c the control variables and their meaning is analogous to the auxiliary inducing variables used in sparse GP models [15]. To compute the posterior p ( f | y ) based on control variables we use the expression sample from p ( f | y ) in a two-stage manner: firstly sample the control variables from p ( f c | y ) and then generate f from the conditional prior p ( f | f c ) . This scheme can allow us to introduce a MH algorithm, where we need to specify only a proposal distribution q ( f ( t +1) c | f ( t ) c ) , that will mimic sampling from p ( f c | y ) , and always sample f from the conditional prior p ( f | f c ) . The whole proposal distribution takes the form Each proposed sample is accepted with probability min(1 , A ) where A is given by The usefulness of the above sampling scheme stems from the fact that the control variables can form a low-dimensional representation of the function. Assuming that these variables are much fewer than the points in f , the sampling is mainly carried out in the low dimensional space. In section 2.2 we describe how to select the number M of control variables and the inputs X c so as f c becomes highly informative about f . In the remainder of this section we discuss how we set the proposal A suitable choice for q is to use a Gaussian distribution with diagonal or full covariance matrix. The covariance matrix can be adapted during the burn-in phase of MCMC in order to increase the acceptance rate. Although this scheme is general, it has practical limitations. Firstly, tuning a full covariance matrix is time consuming and in our case this adaption process must be car-ried out simultaneously with searching for an appropriate set of control variables. Also, since the terms involving p ( f c ) do not cancel out in the acceptance probability in eq. (5), using a diagonal covariance for the q distribution has the risk of proposing control variables that may not satisfy the GP prior smoothness requirement. To avoid these problems, we define q by utilizing the GP prior. According to eq. (3) a suitable choice for q must mimic the sampling from the posterior p ( f c | y ) . Given that the control points are far apart from each other, Gibbs sampling in the control variables space can be efficient. However, iteratively sampling f c i from the conditional posterior hoods 2 . An attractive alternative is to use a Gibbs-like algorithm where each f c i is drawn from using the MH step. This scheme of sampling the control variables one-at-a-time and resampling f is iterated between different control variables. A complete iteration of the algorithm consists of a full scan over all control variables. The acceptance probability A in eq. (5) becomes the likelihood ratio and the prior smoothness requirement is always satisfied. The iteration between different control variables is illustrated in Figure 1. Although the control variables are sampled one-at-at-time, f can still be drawn with a considerable variance. To clarify this, note that when the control variable f c i changes the effective proposal conditional GP prior given all the control points apart from the current point f c i . This conditional prior can have considerable variance close to f c i and in all regions that are not close to the remaining control variables. As illustrated in Figure 1, the iteration over different control variables allow f to be drawn with a considerable variance everywhere in the input space. 2.2 Selection of the control variables To apply the previous algorithm we need to select the number, M , of the control points and the associated inputs X c . X c must be chosen so that knowledge of f c can determine f with small error. The prediction of f given f c is equal to K f,c K  X  1 c,c f c which is the mean of the conditional prior averaged over any possible value of ( f , f c ) : The quantity inside the trace is the covariance of p ( f | f c ) and thus G ( X c ) is the total variance of this distribution. We can minimize G ( X c ) w.r.t. X c using continuous optimization similarly to the approach in [15]. Note that when G ( X c ) becomes zero, p ( f | f c ) becomes a delta function. To find the number M of control points we minimize G ( X c ) by incrementally adding control vari-ables until the total variance of p ( f | f c ) becomes smaller than a certain percentage of the total vari-ance of the prior p ( f ) . 5% was the threshold used in all our experiments. Then we start the sim-ulation and we observe the acceptance rate of the Markov chain. According to standard heuristics [12] which suggest that desirable acceptance rates of MH algorithms are around 1 / 4 , we require a full iteration of the algorithm (a complete scan over the control variables) to have an acceptance rate larger than 1 / 4 . When for the current set of control inputs X c the chain has a low acceptance rate, it means that the variance of p ( f | f c ) is still too high and we need to add more control points in order to further reduce G ( X c ) . The process of observing the acceptance rate and adding control variables is continued until we reach the desirable acceptance rate.
 When the training inputs X are placed uniformly in the space, and the kernel function is stationary, the minimization of G places X c in a regular grid. In general, the minimization of G places the control inputs close to the clusters of the input data in such a way that the kernel function is taken into account. This suggests that G can also be used for learning inducing variables in sparse GP models in a unsupervised fashion, where the observed outputs y are not involved. We consider two applications where exact inference is intractable due to a non-linear likelihood function: classification and parameter estimation in a differential equation model of gene regulation. Classification : Deterministic inference methods for GP classification are described in [16, 4, 7]. Among these approaches, the Expectation-Propagation (EP) algorithm [9] is found to be the most efficient [6]. Our MCMC implementation confirms these findings since sampling using control variables gave similar classification accuracy to EP.
 Transcriptional regulation : We consider a small biological sub-system where a set of target genes are regulated by one transcription factor (TF) protein. Ordinary differential equations (ODEs) can provide an useful framework for modelling the dynamics in these biological networks [1, 2, 13, 8]. The concentration of the TF and the gene specific kinetic parameters are typically unknown and need to be estimated by making use of a set of observed gene expression levels. We use a GP prior to model the unobserved TF activity, as proposed in [8], and apply full Bayesian inference based on the MCMC algorithm presented previously.
 Barenco et al. [2] introduce a linear ODE model for gene activation from TF. This approach was extended in [13, 8] to account for non-linear models. The general form of the ODE model for transcription regulation with a single TF has the form where the changing level of a gene j  X  X  expression, y j ( t ) , is given by a combination of basal tran-scription rate, B j , sensitivity, S j , to its governing TF X  X  activity, f ( t ) , and the decay rate of the mRNA, D j . The differential equation can be solved for y j ( t ) giving where A j term arises from the initial condition. Due to the non-linearity of the g function that trans-forms the TF, the integral in the above expression is not analytically obtained. However, numerical integration can be used to accurately approximate the integral with a dense grid ( u i ) P i =1 of points in the time axis and evaluating the function at the grid points f p = f ( u p ) . In this case the integral in the above equation can be written P P t p =1 w p g ( f p ) e D j u p where the weights w p arise from the numerical integration method used and, for example, can be given by the composite Simpson rule.
 The TF concentration f ( t ) in the above system of ODEs is a latent function that needs to be esti-mated. Additionally, the kinetic parameters of each gene  X  j = ( B j , D j , S j , A j ) are unknown and also need to be estimated. To infer these quantities we use mRNA measurements (obtained from microarray experiments) of N target genes at T different time steps. Let y jt denote the observed gene expression level of gene j at time t and let y = { y jt } collect together all these observations. Assuming a Gaussian noise for the observed gene expressions the likelihood of our data has the form where each probability density in the above product is a Gaussian with mean given by eq. (7) and f 1  X  p  X  P t denotes the TF values up to time t . Notice that this likelihood is non-Gaussian due to the non-linearity of g . Further, this likelihood does not have a factorized form, as in the regression and classification cases, since an observed gene expression depends on the protein concentration activity in all previous times points. Also note that the discretization of the TF in P time points corresponds to a very dense grid, while the gene expression measurements are sparse, i.e. P T .
 To apply full Bayesian inference in the above model, we need to define prior distributions over all unknown quantities. The protein concentration f is a positive quantity, thus a suitable prior is to consider a GP prior for log f . The kinetic parameters of each gene are all positive scalars. Those parameters are given vague gamma priors. Sampling the GP function is done exactly as described in section 2; we have only to plug in the likelihood from eq. (8) in the MH step. Sampling from the kinetic parameters is carried using Gaussian proposal distributions with diagonal covariance matrices that sample the positive kinetic parameters in the log space. In the first experiment we compare Gibbs sampling ( Gibbs ), sampling using local regions ( region ) (see the supplementary file) and sampling using control variables ( control ) in standard regression problems of varied input dimensions. The performance of the algorithms can be accurately assessed by computing the KL divergences between the exact Gaussian posterior p ( f | y ) and the Gaussians obtained by MCMC. We fix the number of training points to N = 200 and we vary the input di-mension d from 1 to 10 . The training inputs X were chosen randomly inside the unit hypercube [0 , 1] d . Thus, we can study the behavior of the algorithms w.r.t. the amount of correlation in the posterior GP process which depends on how densely the function is sampled. The larger the dimen-sion, the sparser the function is sampled. The outputs Y were chosen by randomly producing a GP then adding noise with variance  X  2 = 0 . 09 . The burn-in period was 10 4 iterations 3 . For a certain dimension d the algorithms were initialized to the same state obtained by randomly drawing from the GP prior. The parameters (  X  2 f , ` 2 ,  X  2 ) were fixed to the values that generated the data. The experimental setup was repeated 10 times so as to obtain confidence intervals. We used thinned samples (by keeping one sample every 10 iterations) to calculate the means and covariances of the 200-dimensional posterior Gaussians. Figure 2(a) shows the KL divergence against the number of MCMC iterations for the 5 -dimensional input dataset. It seems that for 200 training points and 5 dimensions, the function values are still highly correlated and thus Gibbs takes much longer for the KL divergence to drop to zero. Figure 2(b) shows the KL divergence against the input dimension after fixing the number of iterations to be 3  X  10 4 . Clearly Gibbs is very inefficient in low dimen-sions because of the highly correlated posterior. As dimension increases and the functions become sparsely sampled, Gibbs improves and eventually the KL divergences approaches zero. The region algorithm works better than Gibbs but in low dimensions it also suffers from the problem of high correlation. For the control algorithm we observe that the KL divergence is very close to zero for all dimensions. Figure 2(c) shows the increase in the number of control variables used as the input dimension increases. The same plot shows the decrease of the average correlation coefficient of the GP prior as the input dimension increases. This is very intuitive, since one should expect the number of control variables to increase as the function values become more independent.
 Next we consider two GP classification problems for which exact inference is intractable. We used the Wisconsin Breast Cancer (WBC) and the Pima Indians Diabetes (PID) binary classification datasets. The first consists of 683 examples (9 input dimensions) and the second of 768 examples (8 dimensions). 20% of the examples were used for testing in each case. The MCMC samplers were run for 5  X  10 4 iterations (thinned to one sample every five iterations) after a burn-in of 10 4 iterations. The hyperparameters were fixed to those obtained by EP. Figures 3(a) and (b) shows the log-likelihood for MCMC samples on the WBC dataset, for the Gibbs and control algorithms respectively. It can be observed that mixing is far superior for the control algorithm and it has also converged to a much higher likelihood. In Figure 3(c) we compare the test error and the average negative log likelihood in the test data obtained by the two MCMC algorithms with the results from EP. The proposed control algorithm shows similar classification performance to EP, while the Gibbs algorithm performs significantly worse on both datasets.
 In the final two experiments we apply the control algorithm to infer the protein concentration of TFs that activate or repress a set of target genes. The latent function in these problems is always one-dimensional and densely discretized and thus the control algorithm is the only one that can converge to the GP posterior process in a reasonable time.
 We first consider the TF p53 which is a tumour repressor activated during DNA damage. Seven samples of the expression levels of five target genes in three replicas are collected as the raw time course data. The non-linear activation of the protein follows the Michaelis Menten kinetics inspired response [1] that allows saturation effects to be taken into account so as g ( f ( t )) = f ( t )  X  (6) where the Michaelis constant for the jth gene is given by  X  j . Note that since f ( t ) is positive the GP prior is placed on the log f ( t ) . To apply MCMC we discretize f using a grid of P = 121 points. During sampling, 7 control variables were needed to obtain the desirable acceptance rate. Running time was 4 hours for 5  X  10 5 sampling iterations plus 5  X  10 4 burn-in iterations. The first row of Figure 4 summarizes the estimated quantities obtained from MCMC simulation.
 Next we consider the TF LexA in E.Coli that acts as a repressor. In the repression case there is an analogous Michaelis Menten model [1] where the non-linear function g takes the form: g ( f ( t )) = + f ( t ) . Again the GP prior is placed on the log of the TF activity. We applied our method to the same microarray data considered in [13] where mRNA measurements of 14 target genes are collected over six time points. For this dataset, the expression of the 14 genes were available for T = 6 times. The GP function f was discretized using 121 points. The result for the inferred TF profile along with predictions of two target genes are shown in the second row of Figure 4. Our inferred TF profile and reconstructed target gene profiles are similar to those obtained in [13]. However, for certain genes, our model provides a better fit to the gene profile. Gaussian processes allow for inference over latent functions using a Bayesian estimation framework. In this paper, we presented an MCMC algorithm that uses control variables. We showed that this sampling scheme can efficiently deal with highly correlated posterior GP processes. MCMC allows for full Bayesian inference in the transcription factor networks application. An important direction for future research will be scaling the models used to much larger systems of ODEs with multiple in-teracting transcription factors. In such large systems where MCMC can become slow a combination of our method with the fast sampling scheme in [3] could be used to speed up the inference. Acknowledgments This work is funded by EPSRC Grant No EP/F005687/1  X  X aussian Processes for Systems Identifi-cation with Applications in Systems Biology X .

