 1. Introduction
Information retrieval systems (IRSes) are widely used in many applications such as search engines, elec-tronics libraries, e-commerce, electronics news, genomic sequence analysis, etc. ( Kobayashi &amp; Takeda, sion efficiency, scalability, and support for various search techniques (e.g. Boolean, ranked, phrase, and
Moffat, &amp; Ramamohanarao, 1998 ). Compression of inverted files is essential to large-scale IRSes. This is because the total time of transferring a compressed inverted list and subsequently decompressing it is potentially much less than that of transferring an uncompressed inverted list. All the well-known inverted file compression methods trade off between compression ratio and decompression time. Can we develop a method that has both the advantages of compression ratio and fast decompression? 1.1. Inverted file compression and its difficulties An inverted file contains, for each distinct term t in the collection, an inverted list of the form where frequency f t is the total number of documents in which t appears, and id to obtain the answer list ( Frakes &amp; Baeza-Yates, 1992; Witten et al., 1999 ).
 between itself and its predecessor to form a list of d -gaps. For example, the inverted list &lt;5;13,18,
Golomb coding ( Teuhola, 1978 ), batched LLRUN coding ( Fraenkel &amp; Klein, 1985; Moffat &amp; Zobel, 12 X  X  mechanism ( Anh &amp; Moffat, 2005 ), have been proposed for compressing inverted lists by estimates for these d -gaps probability distributions. The more accurately the estimate, the greater the compression can be achieved. In this paper, Golomb coding means the  X  X  X ocal Bernoulli model X  X  described in Witten et al. (1999) .

The document identifiers for any given word are not uniformly distributed, since the documents in the this clustering can be taken into account. Based on the d -gap technique, some coding methods, such as skewed Golomb coding and batched LLRUN coding, can capture clustering of documents via accurate estimates to achieve satisfactory compression performance. However, the estimates in these methods are relatively sophisticated, which require more decompression time, so they are not yet applied in real IRSes.

Recently, Moffat and Stuiver (2000) have proposed interpolative coding. It is independent of the esti-ranges and codes in an interpolative order, superior compression performance can be achieved. However, hibits it from being widely used in real-world IRSes.

Therefore, to solve problems such as the slow response time and the large disk space required in large scale IRSes, a new method that provides high speed decoding and exploits clustering well to achieve good compression should be developed. 1.2. Research goal
In terms of query throughput rates, Trotman (2003) shows that for small inverted lists Golomb coding is recommended, whereas for large inverted lists variable byte coding is recommended. Furthermore, Anh and
Moffat (2005) show that word-aligned  X  X  X arryover-12 X  X  mechanism allows a query throughput rate that is higher than Golomb coding and variable byte coding, regardless of the lengths of the inverted lists.
Although these compression methods provide high query throughput rates, their compression efficiencies need to be improved.

In this paper, we develop a new method based on interpolative compression combined with a d -gap com-to achieve good compression performance. Nevertheless, the decoding speed of this new method is even faster than that of Golomb coding and word-aligned  X  X  X arryover-12 X  X  mechanism.

This paper is organized as follows. In Section 2, we present the interpolative coding that is the most tion 6, we present some possible applications of the unique-order interpolative coding. Finally, Section 7 presents our conclusion. 2. Interpolative coding 2.1. Algorithm description
Moffat and Stuiver (2000) have proposed a compression technique called interpolative coding. It makes compression performance. In this method, the storing order as well as lower bound lo and upper bound encode x in some appropriate manner. The simplest mechanism uses only binary code to encode x in d log 2  X  hi lo  X  1  X e bits. The algorithm is described in Fig. 1 .

This interpolative coding is best illustrated with an example. Consider the inverted list document identifiers on the left-hand side may take on values 1 X 5 and those three on the right-hand side plest implementation of Binary _ Code , the corresponding codewords are 4, 2, 0, 2, 4, 2, and 4 bits long.
Using a centered minimal binary code, the compression efficiency of interpolative coding can be further that a number in the range 1 ... r is to be coded. A simple binary code assigns codewords d log all values 1 through r , and wastes 2 d log 2 r e r codewords. That is, 2 ened by one bit without loss of unique decodability. These minimal codewords are then centered on the encoding range. Numbers at the extremes of the range requiring one bit more for storage than those in the center.
 2.2. Observation and improvement
The major overhead of interpolative coding is that a recursive process is used to calculate the order and range of every document identifier. Although a recursive process can be converted to a non-recursive one and decoding very slow. This is why interpolative coding is not widely used in IRSes.
We observed that the calculation of the order and range for every document identifier can be accelerated by storing partial results in memory. Consider a general inverted list IL the number of documents containing term t , id k &lt; id k +1
N . Using the interpolative coding method in Fig. 1 , for every f the list. Some examples are shown in Table 1 . These triple sequences are useful for interpolative coding to calculate the order and range for each document identifier. For example, consider the inverted list
IL this list can be calculated using f t = 5 triples in Table 1 . The full sequence of triples are ( id (5,3,8), ( id 1 ,1, id 3 2) = (1,1,3), ( id 2 , id 1 +1, id id 4 +1, N ) = (8,8,10). Storing such a table containing a full set of triple sequences in memory is helpful for the coding and decoding processes of interpolative coding. Compared with the method in Fig. 1 , this improved method eliminates need for a stack in the document identifier order and range calculation, saving a large amount of time.

The triples for each f t can easily be represented as a two-dimensional array I _ Triple consisting of f rows and 5 columns. This representation for f t = 5 is shown in Fig. 2 . The first row of the array repre-sents the first triple, and the second row represents the second triple, and so forth. The first column is used to denote the index of the document identifier in the inverted list for the first element of the tri-ple. For example, I _ Triple [3][1] is 2, meaning the first value of No. 3 triple is id are 3 and 1, meaning the third value of No. 3 triple is id practical and convenient, two extra values are used for each inverted list: id in Fig. 3 can be used to generate the corresponding triples for each f
For a sub-inverted list IL [ index ... ( index + k 1)] among id a two-dimensional array I _ Triple . 2.3. Remark shown in the following: for m :  X  1to f t do
However, this improved method still requires large memory space. For example, each triple contains five in an inverted list with f t document identifiers, 20  X  f can reach up to thousands or millions, which means the memory space required for I _ Triple storage is ten thousands or even ten millions of bytes. This makes it impossible using memory to accelerate coding and decoding with interpolative code. Furthermore, using I _ Triple to encode and decode requires extra memory access time, which makes the decoding speed slow. 3. Unique-order interpolative coding
The recursive process makes the decoding of interpolative coding slow. Although using memory to store partial results of the recursive process can accelerate the coding and decoding of interpolative coding, a large amount of memory is required to store the I _ Triple for each f process of all inverted lists no matter how many different values of f fore be reduced, which accelerates the whole process. 3.1. The coding method
This section presents the details of our proposed coding method. Two key decisions are to be made in the coding method. 3.1.1. Decomposition of an inverted list into blocks to take advantage of interpolative coding
In an inverted list IL t =&lt; f t ; id 1 , id 2 , ... , id and all document identifiers are within the range 1 X  N . A group size g is first determined. Then IL into m  X d f t document identifier in each block to be a boundary pointer, the document identifiers between boundary gether can be regarded as a sub-inverted list, and a suitable d -gap compression scheme with high decoding speed can be used for compression. The inner pointers in each block are compressed via interpolative cod-new method allows document identifiers to be stored in a fixed order, hence the name unique-order inter-polative coding. When f t 6 g or m =1or g = 1, no inner pointers are present, and we apply only a d -gap compression scheme. 3.1.2. Choice of a suitable coding method for boundary and residual pointers
Compared with the d -gaps of a traditional d -gap compression scheme, the d -gaps of unique-order inter-polative coding extracted from every group of document identifiers are potentially much larger and may cause a decrease in compression efficiency. Therefore, a suitable coding method is required to encode the boundary pointers to improve compression efficiency. To simplify implementation, the boundary and resid-ual pointers are encoded with the same method.

In this paper, we recommend Golomb coding and r coding for encoding the d -gaps of unique-order also close together, and the d -gaps are small. Other coding methods are not disregarded. We are still looking for a faster and more compact coding method to encode the d -gaps of unique-order interpolative coding.

To improve the compression efficiency of the d -gaps of unique-order interpolative coding, the value g is ity. This approach works the best when the original d -gaps are small. 3.2. Illustration
This unique-order interpolative coding is best illustrated with an example. Given an inverted list are therefore the boundary pointers, the document identifiers 32 and 33 are the residual pointers, and the others are the inner pointers. Let [ id i , id i +1 , ... , id code. Since the two successive boundary pointers must be known before interpolative coding of the inner pointers, the boundary pointer of each block is stored before coding of the inner pointers. Therefore, the inverted list is to be stored as where [8,12,13] and [18,23,28] are in interpolative codes, and 5, 15, 29, 32, 33 in d -gaps. The resulted list is
Next, since there are three document numbers between each pair of boundary pointers, the list can be sim-plified as Finally, the residual pointers 32 (=3 + 29) and 33 (=1 + 32) are obtained by the remaining d -gaps.
Now, consider a general inverted list IL t =&lt; f t ; id coding with group size g = 4, the IL t can be represented as where id 1 , id 5 , id 9 , id 13 are encoded using a d -gap coding method and [ id tation in Section 2) as
We observed that the I _ Triple for [ id i , id i +1 , id adding 4 (which is the value of g ) to the indices of document identifiers in the I _ Triple for [ id use Golomb coding to encode boundary pointers and residual pointers, this new coding method can be shown as the program in Fig. 5 . 3.3. Implementation optimization
This section presents how to use loop unwinding to accelerate the encoding and decoding of unique-ther accelerated. For example, for g = 4, the following program segment in Fig. 5 for i :  X  0to( m 1) do can be converted to for i :  X  0to( m 1) do
In other words, once the group size g has been determined, the I _ Triple accesses in loop can be elimi-nated in unique-order interpolative coding. So the 8 3 = 5 times memory accesses for each document identifier can be avoided, which in turn accelerates the encoding process. By using the same approach, the decoding of unique-order interpolative coding can also be accelerated. 4. Analysis
Give an inverted list IL =&lt; f ; id 1 , id 2 , ... , id size g . Once g is determined, the IL will be divided into m  X d g document identifiers and the last block containing f ( m 1) g document identifiers. The boundary point-c coding, in d -gap manner, and the inner document identifiers will be coded by the interpolative coding. Voorhis, 1975; Golomb, 1966; Mcllroy, 1982; Moffat &amp; Stuiver, 2000 ).

If Golomb coding is used to encode the boundary pointers and residual pointers, then the maximum number of bits required to store these f ( m 1)( g 1) boundary and residual pointers is
If we use c coding to encode these pointers, then the maximum number of bits required is
Based on Eq. (3) , the number of bits required to code the inner pointers (( m 1) groups, ( g 1) document identifiers in each group) is Since and the sum of the logarithms of the ( m 1) individual ranges is maximized when all obtains
Therefore, if Golomb coding is used to encode the boundary and residual pointers, then the maximum number of bits required by the unique-order interpolative coding is at most
Or if we use c coding, it is
Eqs. (9) and (10) can be simplified under the condition that no residual pointers exist. For example, when f =( m 1) g + 1, Eq. (9) can be rewritten as: and some examples of the maximum number of bits required for unique-order interpolative coding are de-rived in Table 2 .

The results in Table 2 showed that when Golomb coding is used to encode boundary pointers, the maxi-mum number of bits required in unique-order interpolative coding has inverse relationship with group size cannot be used. We design an experiment in Section 5 to find a suitable group size g .
The results in Eqs. (9), (10) , and Table 2 can be improved if Eq. (3) can be improved. For example, the maximum number of bits required for interpolative coding to encode an inverted list with three document identifiers ranging from 1 to N is since the middle item requires d log 2  X  N 2  X e bits, and the left and right items require d log where a , b are two positive integers and a + b =( N 1). Since and hence
We replace Eq. (3) with Eq. (15) when group size g = 4, and the maximum number of bits required for the unique-order interpolative coding under the condition that no residual pointers exist is therefore Compared with the figure in Table 2 , a much tighter upper bound is obtained.

To further understand the characteristics of unique-order interpolative coding, we conducted following experiments. We used encoding methods such as Golomb coding, skewed Golomb coding, batched
LLRUN coding, interpolative coding, variable byte coding, Carryover-12 mechanism, unique-order interpolative coding 1 (group size g = 4; boundary pointers and residual pointers by Golomb coding), un-bution and compressed using the eight methods. The Golomb coding performs the best, since it is a min-imum-redundancy code for geometric gap distribution ( Gallager &amp; Van Voorhis, 1975 ). Compared with other methods, unique-order interpolative coding is not suitable for a geometric distribution when the first experiment, both variable byte coding and Carryover-12 mechanism are inefficient in compression. gaps was broken into chunks of 200 contiguous values. The chunks were then placed in groups of five.
In the first three chunks of each group, all gaps were multiplied by a factor of 0.1; whereas in the other are shown in Table 3 (b). Compared with skewed Golomb coding, batched LLRUN coding, and interpola-tive coding, the compression efficiency of Golomb coding is not as good as others, meaning it is unable to exploit clustering well. The compression results of unique-order interpolative coding for a skewed geomet-
Again, both variable byte coding and Carryover-12 mechanism are inefficient in compression for most cases in the second experiment. From Table 3 (b), interpolative coding can even outperform self-entropy. This is minimal binary code to encode every gap after it is converted to a triple. 5. Experiments An experimental information retrieval system was implemented to evaluate the various coding methods.
Experiments were conducted on some real-life document collections, and query processing time and storage requirements for each coding method were measured. 5.1. Document collections and queries
Five document collections were used in the experiments. Their statistics are listed in Table 4 . In this in the collection; and f indicates the number of document identifiers that appear in an inverted file. Collection Bible is the King James version of the Bible, in which each verse is considered as a document. and fourth collections, FBIS (Foreign Broadcast Information Service) and LAT (LA Times), are disk 5 of niques ( Voorhees &amp; Harman, 1997 ). The final collection TREC includes the FBIS and LAT collections.
Since effectiveness of coding methods relies heavily on clustering of documents, inverted files for these files were then used to test the advantages and shortcomings of various coding methods.
We followed the method ( Moffat &amp; Zobel, 1996 ) to evaluate performance with random queries. For each document collection, 1000 documents were randomly selected to generate a query set. A query was gener-ated by selecting words from a word list of a specific document, combined by some randomly generated
Boolean operators ANDs and ORs. To form the document word list, words in the document were case folded, and stop words such as  X  X  X he X  X  and  X  X  X his X  X  were eliminated. For example, a query word list may collection that satisfied the query. The generated queries followed a Zipf-like distribution P 1/ q 1999 ). 5.2. Compression performance of unique-order interpolative coding
In this section, Golomb coding was used to code both boundary pointers and residual pointers. This is due to the fact that the average gap sizes in Table 4 are relatively big, Golomb coding was recommend according to Table 3 (b). The compression result is shown in Table 5 , and the metric used is the average number of bits per document identifier BPI , defined as follows:
For each term t , the cost of using r coding to encode the frequency f presented results.
 Note that for group size g = 4 and g = 8, unique-order interpolative coding achieved good compression.
For a simple implementation, we suggest using g = 4. In the following experiments, Golomb coding was used to code both boundary pointers and residual pointers for unique-order interpolative coding, and group size g was set to 4 unless otherwise stated. 5.3. Compression performance of different coding methods We now compare the effectiveness of the eight coding methods: c coding, Golomb coding, batched
LLRUN coding, skewed Golomb coding, interpolative coding, variable byte coding, Carryover-12 mech-quency f t is calculated and included in the presented results. Moreover, any necessary overheads, such as the complete set of models and model selectors for the batched LLRUN coding, are also calculated and included. However, the cost of storing the parameter b for each inverted list in Golomb coding ( Witten coding can be calculated via stored frequency f t using Witten  X  s approximation. The results are shown in
Table 6 . Notice that: 1. Both variable byte coding and Carryover-12 mechanism are inefficient in compression of inverted files.
 2. For the other coding methods, the compression efficiencies of both c coding and Golomb coding are relatively low because of the simple models they use. 3. The compression efficiencies of batched LLRUN, skewed Golomb, interpolative, and unique order inter-polative coding methods are relatively good. This shows that clustering is a good compression aid. coding, meaning that it does take a good advantage of the clustering property. 5.4. Search performance of different coding methods
The query processing time includes (1) disk access time, (2) decompression time, and (3) document iden-tifiers comparison time. Experiments showed that disk access time and decompression time occupy more than 90% of query processing time. And document identifier comparison time is not a function of the cod-ing method used. Therefore the search performance metric is defined as
All experiments described in this section were run on an Intel P4 2.4GHz PC with 256MB DDR memory running Linux operating system 2.4.12. The hard disk was 40 GB, and the data transfer rate was 25 MB/s.
Intervening processes and disk activities were minimized during experimentation. All decoding mechanisms were written in C , complied with gcc , and optimized as follows: 1. Replaced sub-routines with macros. 2. Careful choice for compiler optimization flags. 3. Implementation used 32-bit integers, as that is the internal register size of the Intel P4 CPU. 4. Implemented the integer logarithm function d log 2  X  i  X e with a lookup table.

Let z be a 256-entry array, and z[k] be d log 2  X  k  X  1  X e where 0 implemented in C as follows (v is the returned value of d log 5. Implemented the integer logarithm function b log 2  X  i  X c also with a lookup table.
The array z is the same as that used in the function d log in C as follows (v is the returned value of b log 2  X  i  X c ): 6. A 256-entry lookup table is used to locate the exact bit location of the first  X  X 1 X  X  bit in a byte. cess of unary codes because no bit-by-bit decoding is required. 7. Access to binary codes with masking and shifting operations, and no bit-by-bit decoding is required.
With these optimizations, decoding of a document identifier only required tens of ns, and no bit-by-bit decoding is required.

Other optimizations included: The Huffman code of batched LLRUN coding was implemented with canonical prefix codes ( Turpin, 1998 ). The canonical prefix codes can be decoded via fast table look-up.
And for the interpolative coding method, recursive process was transformed to non-recursive process, at the cost of an explicit stack ( Tenenbaum et al., 1990 ).

The search performance measurements are shown in Table 7 . Key findings are: 1. Although variable byte coding and Carryover-12 mechanism gave fast decoding, r coding and unique-order interpolative coding achieved higher query throughput rates. This is because the disk access time (AT) of variable byte coding and Carryover-12 mechanism is much higher than that of r coding and unique-order interpolative coding. than that of Carryover-12. This is because a large portion of the d -gaps of frequently used query terms for DBbib is of value 1. Both r coding and unique-order interpolative coding can encode these d -gaps very economically. This also makes the decoding times of r coding and unique-order interpolative coding for these d -gaps very low. 3. Batched LLRUN coding, skewed Golomb coding, and interpolative coding gave better compression rates than Golomb coding. However, their complex decoding mechanisms prohibited them from being used in real-world IRSes. 4. Experimental results showed that r coding, Carryover-12 mechanism, and unique-order interpolative coding were recommended for real-world IRSes. Their query throughput rates were all much higher than that of Golomb coding. 5. To obtain better compression rates, Golomb coding and unique-order interpolative coding use a mini-mal binary code in their codewords. To decode a minimal binary code,  X  X  X oggle point X  X  calculations are required and slow down query evaluation. Rice coding is a variant of Golomb coding where the value b culation required. The disadvantage of this restriction is the slightly worse compression than that of
Golomb coding. If we use Rice coding to encode the boundary and residual pointers in unique-order there is no  X  X  X oggle point X  X  calculation required for unique-order interpolative coding. Table 8 showed that Rice coding allowed query throughput rates of approximately 8% higher than Golomb coding, and unique-order interpolative coding without  X  X  X oggle point X  X  calculation allowed query throughput rates of approximately 30% higher than Golomb coding. Experimental results further showed that the decoding time of unique-order interpolative coding without  X  X  X oggle point X  X  calculation is even less than that of Carryover-12 mechanism.
 6. Experimental results showed that a good coding method must be characterized by both high compression ratio and high decompression rate. The unique-order interpolative coding is such a good method. 6. Other applications
Unique-order interpolative coding, like interpolative coding, can be directly applied to encode strictly quency of the term appearing within that document, giving the inverted list the form: where f t is the number of documents containing term t , id ument i ,1 6 i 6 f t . The within-document frequencies can be compressed in exactly the same manner of compressing document pointers: if there are f t entries in an inverted list and a total of F that term in the collection, the sequence of cumulative sums of the f integer sequence, and all of the existing compression methods are applicable. Because the within-document frequencies are typically small, according to Table 3 (b), unique-order interpolative coding should use c coding to encode within-document frequencies. Table 9 shows the cost, in bits per pointer, of storing the tive coding achieved very good compression, second to only the interpolative coding. Considering also the performance results in Section 5.4 and implementation cost, we conclude that the unique-order inter-polative coding is very suitable for encoding within-document frequencies of inverted lists.
Unique-order interpolative coding can also support a self-indexing strategy with a little additional stor-ses. An inverted list must be completely decompressed in order to be randomly accessed to any document identifier, and the full decompression is expensive. Recently, Moffat presented the skipped inverted lists (using self-indexing strategy) to support random access and reduce the query response time ( Moffat &amp; of each block (called critical document identifier) separately from other document identifiers. Document and the second step is searching in one targeted block. Experimental results show that the self-indexing strategy can improve response time for both Boolean AND queries and ranked queries. The only shortcom-the location of the next critical one.

Compared with the self-indexing strategy by Moffat, the unique-order interpolative coding is more effi-while the number of bits needed to store the inner pointers can be calculated using the value of the two boundary pointers and Eq. (17) .
 where N is the gap of two successive boundary pointers and k  X d log bound of Eq. (12) (in Section 4) expressed in closed form, which can be obtained by simulation and vali-useless in set operations during query processing can be skipped easily. Such a method is called skipped unique-order interpolative coding. The results in Table 10 showed that this method does not require extra that of a Golomb code. This method is also simple, so we suggest that it be widely used in IRSes. 7. Conclusion
This paper proposes a novel coding method, the unique-order interpolative coding, for compressing in-custom designed to suit the clustering property of document identifiers, a property that has been observed and storage-economical IRS.
 Acknowledgement
This work was support by National Science Council, ROC: NSC92-2213-E009-065. A shorter version Nevada.
 References
