 Ahar on Bar -Hillel AHARONBH @ CS . HUJI . AC . IL Daphna Weinshall DAPHNA @ CS . HUJI . AC . IL Jerusalem, Israel 91904 Graph based clustering methods have been widely and suc-cessfully used in man y domains such as computer vision, bioinformatics and exploratory data analysis. This cate gory spans a wide range of algorithms, from classical agglomer -ative methods such as aver age linka ge (Duda et al., 2001), to the recently developed and more sophisticated spectral methods (Shi &amp; Malik, 2000) and stochastic formulations (Blatt et al., 1997; Gdalyahu et al., 2001). The initial rep-resentation in all these methods is a matrix (or graph) of distances between all pairs of datapoints. The computation of this distance matrix is considered a  X  X reprocessing X  step, and typically one uses some norm on the feature space (or a related variant).
 Despite the important dif ferences between the various graph-based clustering algorithms, it is widely ackno wl-edged that clustering performance critically depends on the quality of the distance function used. Often the quality of the distance function is more important then the specifics of the clustering algorithm. In this paper we focus on the question of how to learn a  X  X ood X  distance function, which will lead to impro ved clustering. Our main contrib ution is DistBoost -a novel semi-supervised algorithm for learning distance functions.
 We consider a semi-supervised clustering scenario in which the data is augmented by some sparse side information, in the form of equi valence constraints. Equi valence con-straints are relations between pairs of data points, which indicate whether the points belong to the same cate gory or not. We term a constraint  X  X ositi ve X  when the points are kno wn to be from the same class, and  X  X e gati ve X  oth-erwise. Such constraints carry less information than ex-plicit labels on the original datapoints, since clearly equi v-alence constraints can be obtained from explicit labels but not vice versa. More importantly , it has been suggested that in some cases equi valence constraints are easier to obtain, especially when the database is very lar ge and contains a lar ge number of cate gories without pre-defined names (Hertz et al., 2003).
 In recent years there has been a gro wing interest in semi su-pervised clustering scenarios, leading to two dif ferent (and related) lines of research. In the first, the constraints are incorporated directly into the clustering algorithm, limit-ing the clustering solutions considered to those that com-ply with the given constraints. Examples are the con-strained complete linkage algorithm (Klein et al., 2002), constrained K-means (W agstaf f et al., 2001) and a con-strained EM of a Gaussian mixture (Shental et al., 2003). The second line of research, to which this work belongs, uses the constraints to learn an informati ve distance func-tion (prior to clustering). Most of the work in this area has focused on the learning of Mahalanobis distance functions of the form et al., 2002). In these papers the parametric Mahalanobis metric was used in combination with some suitable para-metric clustering algorithm, such as K-means or EM of a mixture of Gaussians. In contrast, we develop in this paper a method that learns a non-parametric distance function, which can be more naturally used in non-parametric graph based clustering.
 More formally , let denote the original data space, and as-sume that the data is sampled from discrete labels. Our goal is to learn a distance function Our key observ ation is that we can learn such a function, by posing a related binary classification problem over the product space classification techniques. The binary problem is the prob-lem of distinguishing between pairs of points that belong to the same class and pairs of points that belong to dif ferent classes. 2 The training data included a set of equi valence constraints, which can be formally regarded as binary la-bels on points in the same class by and pairs of points belonging to dif fer -ent classes by , we can interpret the classifier X  s mar gin as the required distance function.
 Ha ving reduced distance learning to binary classification with mar gins, we can now attempt to solv e this problem using standard powerful mar gin based classifiers. We have explored both support vector machines (SVM X  s) and boost-ing algorithms. Ho we ver, experiments with several SVM variants and decision trees (C4.5) boosting have led us to recognize that the specific classification problem we are in-terested in has some unique features which require special treatment: 1. The product space binary function we wish to learn 2. In the learning setting we have described abo ve, we These considerations led us to develop the DistBoost algo-rithm, which is our main contrib ution in this paper . Dis-tBoost is a distance learning algorithm which attempts to address the issues discussed abo ve. It learns a distance function which is based on boosting binary classifiers with a confidence interv al in product space, using a weak learner that learns in the original feature space (and not in product space). We suggest a boosting scheme that incorporates un-labeled data points. These unlabeled points pro vide a den-sity prior , and their weights rapidly decay during the boost-ing process. The weak learner we use is based on a con-strained Expectation Maximization (EM) algorithm, which computes a Gaussian mixture model, and hence pro vides a partition of the original space. The constrained EM pro-cedure uses unlabeled data and equi valence constraints to find a Gaussian mixture that complies with them. A weak product space hypothesis is then formed as the equi valence relation of the computed partition.
 We have experimented with DistBoost and conducted sev-eral empirical comparisons of interest. The first is a com-parison of DistBoost to other mar gin based distance func-tions obtained using the more traditional algorithms of SVM and decision tree boosting. Another comparison is between DistBoost and pre viously suggested distance learning algorithms which are based on Mahalanobis met-ric estimation. Finally , clustering using the distance func-tion learnt by DistBoost is compared to pre viously sug-gested methods of incorporating equi valence constraints di-rectly into clustering algorithms. During the comparati ve assessment DistBoost was evaluated with several agglom-erati ve clustering algorithms and with dif ferent amounts of equi valence constraints information. We used several datasets from the UCI repository (Blak e &amp; Merz, 1998), A sample from the MNIST dataset (LeCun et al., 1998), and a dataset of natural images obtained from a commercial im-age CD. In most of our experiments the DistBoost method outperformed its competitors. The DistBoost algorithm builds distance functions based on the weighted majority vote of a set of original space soft partitions. The weak learner X  s task in this frame work is to find plausible partitions of the space, which comply with the given equi valence constraints. In this task, the un-labeled data can be of considerable help, as it allo ws to define a prior on what are  X  X lausible partitions X . In order to incorporate the unlabeled data into the boosting process, we augmented the Adaboost with confidence interv als pre-sented in (Schapire &amp; Singer , 1999). The details of this augmentation are presented in Section 2.1. The details of the weak learner we use are presented in Section 2.2. 2.1. Semi super vised boosting in product space Our boosting scheme is an extension of the Adaboost algo-rithm with confidence interv als (Schapire &amp; Singer , 1999; Schapire et al., 1997) to handle unsupervised data points. As in Adaboost, we use the boosting process to maximize the mar gins of the labeled points. The unlabeled points only pro vide a decaying density prior for the weak learner . The algorithm we use is sketched in Fig. 1. Given a par -tially labeled dataset the algorithm searches for a hypothesis Note that the unlabeled points do not contrib ute to the min-imization objecti ve (1). Rather , at each boosting round the y are given to the weak learner and supply it with some (hopefully useful) information regarding the domain X  s den-sity . The unlabeled points effecti vely constrain the search space during the weak learner estimation, giving priority to hypotheses which both comply with the pairwise con-straints and with the density information. Since the weak learner X  s task becomes harder in later boosting rounds, the boosting algorithm slo wly reduces the weight of the un-labeled points given to the weak learner . This is accom-plished in step 4 of the algorithm (see Fig. 1). In product space there are correspond to all the possible pairs of original points, and the number of weights is therefore update rules for the weight of each unlabeled point are identical, and so all the unlabeled points can share the same weight. Hence the number of updates we effecti vely do in each round is proportional to the number of labeled pairs only . The weight of the unlabeled pairs is guaranteed to Algorithm 1 Boosting with unlabeled data Given Initialize For 1. Train weak learner using distrib ution 2. Get weak hypothesis 3. Choose 5. Normalize: 6. Output the final hypothesis decay at least as fast as the weight of any labeled pair . This immediately follo ws from the update rule in step 4 of the algorithm (Fig. 1), as each unlabeled pair is treated as a labeled pair with maximal mar gin of 1.
 We note in passing that it is possible to incorporate un-labeled data into the boosting process itself, as has been suggested in (d X  X lche Buc et al., 2002; Grandv alet et al., 2001). In this work the mar gin concept was extended to unlabeled data points. The mar gin for such a point is a pos-itive number related to the confidence the hypothesis has in classifying this point. The algorithm then tries to min-imize the total (both labeled and unlabeled) mar gin cost. The problem with this frame work is that a hypothesis can be very certain about the classification of unlabeled points, and hence have low mar gin costs, even when it classifies these points incorrectly . In the semi supervised clustering conte xt the total mar gin cost may be dominated by the mar -gins of unconstrained point pairs, and hence minimizing it doesn X  t necessarily lead to hypotheses that comply with the constraints. Indeed, we have empirically tested some vari-ants of these algorithms and found that the y lead to inferior performance. 2.2. Mixtur es of Gaussians as weak hypotheses The weak learner in DistBoost is based on the constrained EM algorithm presented by (Shental et al., 2003). This al-gorithm learns a mixture of Gaussians over the original data space, using unlabeled data and a set of positi ve and neg-ative constraints. Belo w we briefly revie w the basic algo-rithm, and then sho w how it can be modified to incorporate weights on sample data points. We also describe how to translate the boosting weights from product space points to original data points, and how to extract a product space hy-pothesis from the soft partition found by the EM algorithm. A Gaussian mixture model (GMM) is a parametric statis-tical model which assumes that the data originates from a weighted sum of several Gaussian sources. More formally , a GMM is given by notes the weight of each Gaussian, its respecti ve parame-ters, and denotes the number of Gaussian sources in the GMM. EM is a widely used method for estimating the pa-rameter set of the model ( ) using unlabeled data (Demp-ster et al., 1977). In the constrained EM algorithm equiva-lence constr aints are introduced into the  X  X  X  (Expectation) step, such that the expectation is tak en only over assign-ments which comply with the given constraints (instead of summing over all possible assignments of data points to sources).
 Assume we are given a set of unlabeled i.i.d. sampled points these points . Denote the inde x pairs of positi vely con-strained points by atively constrained points by model contains a set of discrete hidden variables , where the Gaussian source of point den variable . The constrained EM algorithm assumes the follo wing joint distrib ution of the observ ables the hiddens : The algorithm seeks to maximize the data lik elihood, which is the mar ginal distrib ution of (2) with respect to . The equi valence constraints create comple x dependencies between the hidden variables of dif ferent data points. Ho w-ever, the joint distrib ution can be expressed using a Mark ov netw ork, as seen in Fig. 1. In the  X  X  X  step of the algorithm the probabilities a standard inference algorithm to the netw ork. Such in-ference is feasible if the number of negati ve constraints is parameters are then updated based on the computed proba-bilities. The update of the Gaussian parameters can be done in closed form, using rules similar to the standard EM update rules. The update of the cluster weights is more complicated, since these parameters appear in the normalization constant with a gradient descent procedure. The algorithm finds a local maximum of the lik elihood, but the partition found is not guaranteed to satisfy any specific constraint. Ho w-ever, since the boosting procedure increases the weights of points which belong to unsatisfied equi valence constraints, it is most lik ely that any constraint will be satisfied in one or more partitions.
 We have incorporated weights into the constrained EM pro-cedure according to the follo wing semantics: The algo-rithm is presented with a virtual sample of size training point with weight this sample. All the repeated tok ens of the same point are considered to be positi vely constrained, and are therefore assigned to the same source in every evaluation in the  X  X  X  step. In all of our experiments we have set actual sample size.
 While the weak learner accepts a distrib ution over the origi-nal space points, the boosting process described in 2.1 gen-erates a distrib ution over the sample product space in each round. The product space distrib ution is con verted to a dis-trib ution over the sample points by simple mar ginalization. Specifically , denote by weight In each round, the mixture model computed by the con-strained EM is used to build a binary function over the product space and a confidence measure. We first deri ve a partition of the data from the Maximum A Posteriori (MAP) assignment of points. A binary product space hy-pothesis is then defined by giving the value to pairs of points from the same Gaussian source, and to pairs of points from dif ferent sources. This value determines the sign of the hypothesis output. This setting further supports a natural confidence measure -the probability of the pair X  s where + are the hidden variables attached to the two points. The weak hypothesis output is the signed confi-dence measure in be vie wed as a weak  X  X istance function X . We have tried to solv e the distance learning problem over the product space using two more traditional mar gin based classifiers. The first is a support vector machine, that tries to find a linear separator between the data examples in a high dimensional feature space. The second is the Ad-aBoost algorithm, where the weak hypotheses are decision trees learnt using the C4.5 algorithm. Both algorithms had to be slightly adapted to the task of product space learning, and we have empirically tested possible adaptations using data sets from the UCI repository . Specifically , we had to deal with the follo wing technical issues: The clustering performance obtained using these two vari-ants is compared to DistBoost in section 4. The design is-sues mentioned abo ve were decided based on the perfor -mance over the UCI datasets, and the settings remained fix ed for the rest of the experiments. We compared our DistBoost algorithm with other tech-niques for semi-supervised clustering using equi valence constraints. We used both distance learning techniques, including our two simpler variants for learning in product space (SVM and boosting decision trees), and constrained clustering techniques. We begin by introducing our exper -imental setup and the evaluated methods. We then present the results of all these methods on several datasets from the UCI repository , a subset of the MNIST letter recognition dataset, and an animal image database. 4.1. Experimental setup Gathering equi valence constraints: Follo wing (Hertz et al., 2003), we simulated a distrib uted learning scenario, where labels are pro vided by a number of uncoordinated independent teachers. Accordingly , we randomly chose small subsets of data points from the dataset and parti-tioned each of the subsets into equi valence classes. The constraints obtained from all the subsets are gathered and used by the various algorithms.
 The size of each subset in these experiments was chosen to be , where is the number of classes in the data. In each experiment we used subsets, and the amount of partial information was controlled by the constr aint inde x ; this inde x measures the amount of points which participate in at least one constraint. In our experiments we used the number of equi valence constraints thus pro vided typi-cally includes only a small subset of all possible pairs of datapoints, which is Ev aluated Methods: we compared the clustering perfor -mance of the follo wing techniques: 1. Our proposed boosting algorithm ( DistBoost ). 2. Mahalanobis distance learning with Rele vant Compo-3. Mahalanobis distance learning with non-linear opti-4. Mar gin based distance learning using SVM as a prod-5. Mar gin based distance learning using product space 6. Constrained EM of a Gaussian Mixture Model (Con-7. Constrained Complete Linkage (Constrained Com-8. Constrained K-means (COP K-means) (W agstaf f Methods 1-5 compute a distance function, and the y are evaluated by applying a standard agglomerati ve clustering algorithm (W ard) to the distance graph the y induce. Meth-ods 6-8 incorporate equi valence constraints directly into the clustering process.
 All methods were evaluated by clustering the data and mea-suring the score defined as where denotes precision and denotes recall. For the distance learning techniques we also sho w cumulative neighbor purity curv es. Cumulative neighbor purity mea-sures the percentage of correct neighbors up to the -th neighbor , averaged over all the datapoints. In each exper -iment we averaged the results over 50 or more dif ferent equi valence constraint realizations. Both DistBoost and the decision tree boosting algorithms were run for a constant number of boosting iterations the dataset). In each realization all the algorithms were given the exact same equi valence constraints.
 Dimensionality reduction: the constrained LD A algo-rithm Some of the datasets reside in a high dimensional space, which must be reduced in order to perform param-eter estimation from training data. We used two methods for dimensionality reduction: standard Principal Compo-nents Analysis (PCA), and a constrained Linear Discrimi-nant Analysis (LD A) algorithm which is based on equi va-lence constraints.
 Classical LD A (also called FD A, (Fukunaga, 1990)) com-putes projection directions that minimize the within-class scatter and maximize the between-class scatter . More for -mally , given a labeled dataset matrix that maximizes where scatter matrix ( is the data X  s empirical mean) and class scatter matrix ( is the empirical mean of the class).
 Since in our semi-supervised learning scenario we have ac-cess to equi valence constraints instead of labels, we can write down a constrained LD A algorithm. Thus we esti-mate the within class scatter matrix using positi ve equi va-lence constraints instead of labels. Specifically , given a set of positi ve equi valence constraints, we use transiti ve clo-sure over this set to obtain small subsets of points that are kno wn to belong to the same class. Denote these subsets by number of data points these subsets to estimate as follo ws where here denotes the mean of subset . 4.2. Results on UCI datasets We selected several datasets from the UCI data repository and used the experimental setup abo ve to evaluate the var-ious methods. Fig. 2 sho ws clustering score plots for several data sets using Ward X  s agglomerati ve clustering al-gorithm. Clearly DistBoost achie ves significant impro ve-ments over Mahalanobis based distance measures and other product space learning methods. Comparing DistBoost to methods which incorporate constraints directly , clearly the only true competitor of DistBoost is its own weak learner , the constrained EM algorithm. Still, in the vast majority of cases DistBoost gives an additional significant impro ve-ment over the EM. 4.3. Results on the MNIST letter recognition dataset We compared all clustering methods on a subset of the MNIST letter recognition dataset (LeCun et al., 1998). We randomly selected training samples ( from each of the classes). The original data dimension was ! , which was projected by standard PCA to the first prin-cipal dimensions. We then further projected the data using the constrained LD A algorithm to ! dimensions. Cluster -ing and neighbor purity plots are presented on the left side of Fig 3. The clustering performance of the DistBoost al-gorithm is significantly better than the other methods. The cumulati ve purity curv es suggest that this success may be related to the slo wer decay of the neighbor purity scores for DistBoost . 4.4. Results on an Animal image dataset We created an image database which contained images of animals tak en from a commercial image CD, and tried to cluster them based on color features. The clustering task in this case is much harder than in the pre vious applications. The database contained 10 classes with total of 565 images. Fig. 3 sho ws a few examples of images from the database. The original images were hea vily compressed jpg im-ages. The images were represented using Color Coherence Vectors (Pass et al., 1996) (CCV X  s). This representation extends the color histogram representation, by capturing some crude spatial properties of the color distrib ution in an image. Specifically , in a CCV vector each histogram bin is divided into two bins, representing the number of  X  X oher -ent X  and  X  X on-Coherent X  pix els from each color .  X  X oher -ent X  pix els are pix els whose neighborhood contains more than neighbors which have the same color . We repre-sented the images in HSV color space, quantized the im-ages to the CCV of each image -a ! dimensional vector -using In order to reduce the dimension of our data, we first re-mo ved all zero dimensions and then used the first PCA dimensions, follo wed by Constrained LD A to further re-duce the dimension of the data to ! . The cluster -ing results and neighbor purity graphs are presented on the right side of Fig 3. 4 The dif ficulty of the task is well reflected in the low clustering scores of all the methods. Ho we ver, DistBoost still outperforms its competitors, as it did in all pre vious examples. In this paper , we have described DistBoost -a novel al-gorithm which learns distance functions that enhance clus-tering performance using sparse side information. Our ex-tensi ve comparisons sho wed the adv antage of our method over man y competing methods for learning distance func-tions and for clustering using equi valence constraints. An-other application which we have not explored here, is near -est neighbor classification. Nearest neighbor classification also critically depends on the distance function between datapoints; our hope is that distance functions learned from equi valence constraints can also be used for impro ving nearest neighbor classification.
 Bar -Hilel, A., Hertz, T., Shental, N., &amp; Weinshall, D. (2003). Learning distance functions using equi valence relations.
 Blak e, C., &amp; Merz, C. (1998). UCI repository of machine learning databases.
 Blatt, M., Wiseman, S., &amp; Doman y, E. (1997). Data clus-tering using a model granular magnet. Neur al Computa-tion , 9 , 1805 X 1842. d X  X lche Buc, F., Grandv alet, Y., &amp; Ambroise, C. (2002). Semi-supervised mar ginboost.
 Dempster , A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Max-imum lik elihood from incomplete data via the EM algo-rithm. JRSSB , 39 , 1 X 38.
 Duda, R. O., Hart, P. E., &amp; Stork, D. G. (2001). Pattern Classification . John Wiley and Sons Inc.
 Fukunaga, K. (1990). Statistical pattern reco gnition . San
Die go: Academic Press. 2nd edition. Gdalyahu, Y., Weinshall, D., &amp; Werman., M. (2001). Self organization in vision: stochastic clustering for image segmentation, perceptual grouping, and image database organization.
 Grandv alet, Y., d X  X lche Buc, F., &amp; Ambroise, C. (2001). Boosting mixture models for semi supervised learning. Hertz, T., Bar -Hillel, A., Shental, N., &amp; Weinshall, D. (2003). Enhancing image and video retrie val: Learning via equi valence constraints. IEEE Conf . on Computer Vi-sion and Pattern Reco gnition, Madison WI, June 2003 . Klein, D., Kamv ar, S., &amp; Manning, C. (2002). From instance-le vel constraints to space-le vel constraints: Making the most of prior kno wledge in data clustering. LeCun, Y., Bottou, L., Bengio, Y., &amp; Haf fner , P. (1998).
Gradient-based learning applied to document recogni-tion. Proceedings of the IEEE , 86 , 2278 X 2324.
 Pass, G., Zabih, R., &amp; Miller , J. (1996). Comparing images using color coherence vectors. ACM Multimedia (pp. 65 X 73).
 Schapire, R. E., Freund, Y., Bartlett, P., &amp; Lee, W. S. (1997). Boosting the mar gin: a new explanation for the effecti veness of voting methods. Proc. 14th Interna-tional Confer ence on Mac hine Learning (pp. 322 X 330). Mor gan Kaufmann.
 Schapire, R. E., &amp; Singer , Y. (1999). Impro ved boosting using confidence-rated predictions. Mac hine Learning , 37 , 297 X 336.
 Shental, N., Hertz, T., Bar -Hilel, A., &amp; Weinshall, D. (2003). Computing gaussian mixture models with EM using equi valence constraints.
 Shental, N., Hertz, T., Weinshall, D., &amp; Pavel, M. (2002). Adjustment learning and rele vant component analysis. Computer Vision -ECCV .
 Shi, J., &amp; Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Mac hine Intellig ence , 22 , 888 X 905.
 Wagstaf f, K., Cardie, C., Rogers, S., &amp; Schroedl, S. (2001). Constrained K-means clustering with back-ground kno wledge. Proc. 18th International Conf . on Mac hine Learning (pp. 577 X 584). Mor gan Kaufmann, San Francisco, CA.
 Xing, E., Ng, A., Jordan, M., &amp; Russell, S. (2002). Dis-tance metric learnign with application to clustering with side-information. Advances in Neur al Information Pro-
