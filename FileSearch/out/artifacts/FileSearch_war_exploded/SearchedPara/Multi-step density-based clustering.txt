 REGULAR PAPER Stefan Brecheisen  X  Hans-Peter Kriegel  X  Martin Pfeifle Abstract Data mining in large databases of complex objects from scientific, en-gineering or multimedia applications is getting more and more important. In many areas, complex distance measures are first choice but also simpler distance func-tions are available which can be computed much more efficiently. In this paper, we will demonstrate how the paradigm of multi-step query processing which relies on exact as well as on lower-bounding approximated distance functions can be inte-grated into the two density-based clustering algorithms DBSCAN and OPTICS resulting in a considerable efficiency boost. Our approach tries to confine itself to  X  -range queries on the simple distance functions and carries out complex distance computations only at that stage of the clustering algorithm where they are com-pulsory to compute the correct clustering result. Furthermore, we will show how our approach can be used for approximated clustering allowing the user to find an individual trade-off between quality and efficiency. In order to assess the quality of the resulting clusterings, we introduce suitable quality measures which can be used generally for evaluating the quality of approximated partitioning and hier-archical clusterings. In a broad experimental evaluation based on real-world test data sets, we demonstrate that our approach accelerates the generation of exact density-based clusterings by more than one order of magnitude. Furthermore, we show that our approximated clustering approach results in high quality clusterings where the desired quality is scalable with respect to (w.r.t.) the overall number of exact distance computations.
 Keywords Approximated clustering  X  Complex objects  X  Data mining  X  Density-based clustering 1 Introduction In recent years, the research community has given a lot of attention to the clus-tering problem resulting in a large variety of different clustering algorithms [ 13 ]. One important class of clustering algorithms is density-based clustering which can be used for clustering all kinds of metric data and is not confined to vector spaces. Density-based clustering is rather robust concerning outliers [ 9 ] and is very effec-the reachability plot created by the density-based hierarchical clustering algorithm OPTICS [ 2 ] serves as a starting point for an effective data mining tool which helps to visually analyze cluster hierarchies [ 4 ].
 based on  X  -range queries for each database object. Each range query requires a lot of distance calculations, especially when high  X  -values are used. Therefore, these algorithms are only applicable to large collections of complex objects, e.g. trees, point sets, and graphs (cf. Fig. 1 ), if those range queries are supported efficiently. When working with complex objects, the necessary distance calculations are the time-limiting factor. Thus, the ultimate goal is to save as many of these complex distance calculations as possible.
 multi-step query processing paradigm directly into the clustering algorithm rather than using it  X  X nly X  for accelerating range queries. Our clustering approach itself exploits the information provided by simple distance measures lower-bounding complex and expensive exact distance functions. Expensive exact distance com-putations are only performed when the information provided by simple distance computations, which are often based on simple object representations, is not enough to compute the exact clustering. Furthermore, we show how our approach can be used for approximated clustering where the result might be slightly differ-ent from the one we compute based on the exact information. In order to measure the dissimilarity between the resulting clusterings, we introduce suitable quality measures.
 new approach which integrates the multi-step query processing paradigm directly into the clustering algorithms rather than using it independently. As our approach can also be used for generating approximated clusterings, we introduce objective quality measures in Sect. 3 which allow us to assess the quality of approximated clusterings. In Sect. 4 , we present a detailed experimental evaluation showing that the presented approach can accelerate the generation of density-based clusterings on complex objects by more than one order of magnitude. We show that for ap-proximated clustering the achieved quality is scalable w.r.t. the overall runtime. We close this paper, in Sect. 5 , with a short summary and a few remarks on future work. 2 Efficient density-based clustering In this section, we will discuss in detail how we can efficiently compute a flat (DBSCAN) and a hierarchical (OPTICS) density-based (DB) clustering. First, in Sect. 2.1 , we present the basic concepts of density-based clustering along with the two algorithms DBSCAN and OPTICS. Then, in Sect. 2.2 , we look at different ap-proaches presented in the literature for efficiently computing these algorithms. We will explain why the presented algorithms are not suitable for expensive distance computations if we are interested in the exact clustering structure. In Sect. 2.3 ,we will present our new approach which tries to use lower-bounding distance func-tions before computing the expensive exact distances. 2.1 Density-based clustering The key idea of density-based clustering is that for each object of a cluster the neighborhood of a given radius  X  has to contain at least a minimum number MinPts of objects, i.e. the cardinality of the neighborhood has to exceed a given threshold. In the following, we will present the basic definitions of density-based clustering. Definition 1 ( directly density-reachable ) Object p is directly density-reachable from object q w.r.t.  X  and MinPts in a set of objects DB, if p  X  N  X  ( q ) and |
N  X  ( q ) | X  MinPts, where N  X  ( q ) denotes the subset of DB contained in the  X  -neighborhood of q .
 condition holds for an object q , then we call q a core object. Other objects can be directly density-reachable only from core objects.
 Definition 2 ( density-reachable and density-connected ) An object p is density-reachable from an object q w.r.t.  X  and MinPts in a set of objects DB, if there is a chain of objects p 1 ,..., p n , p 1 = q , p n = p , such that p i  X  DB and p i + 1 is directly density-reachable from p i w.r.t.  X  and MinPts. Object p is density-connected to object q w.r.t.  X  and MinPts in a set of objects DB, if there is an object o  X  DB, such that both p and q are density-reachable from o in DB w.r.t.  X  and MinPts.
 does not have to be symmetric. On the other hand, density-connectivity is sym-metric (cf. Fig. 2 ).
 2.1.1 DBSCAN A flat density-based cluster is defined as a set of density-connected objects which is maximal w.r.t. density-reachability. Then the noise is the set of objects not con-tained in any cluster. A cluster contains not only core objects but also objects that do not satisfy the core object condition. These border objects are directly density-reachable from at least one core object of the cluster.
 database, is based on the fact that a cluster is equivalent to the set of all objects in DB which are density-reachable from an arbitrary core object in the cluster (cf. Lemmas1and2in[ 9 ]). The retrieval of density-reachable objects is performed by iteratively collecting directly density-reachable objects. DBSCAN checks the  X  -neighborhood of each point in the database. If the  X  -neighborhood N  X  ( q ) of a point q has more than MinPts elements, q is a so-called core point, and a new cluster C containing the objects in N  X  ( q ) is created. Then, the  X  -neighborhood of all points p in C which have not yet been processed is checked. If N  X  ( p ) contains more than MinPts points, the neighbors of p which are not already contained in C are added to the cluster and their  X  -neighborhood is checked in the next step. This procedure is repeated until no new point can be added to the current cluster C . Then the algorithm continues with a point which has not yet been processed trying to expand a new cluster. 2.1.2 OPTICS While the partitioning density-based clustering algorithm DBSCAN [ 9 ] can only identify a  X  X lat X  clustering, the newer algorithm OPTICS [ 2 ] computes an ordering of the points augmented by additional information, i.e. the reachability-result of OPTICS, i.e. the cluster ordering, is displayed by the so-called reach-ability plots which are 2D-plots generated as follows: the clustered objects are ordered along the x -axis according to the cluster ordering computed by OPTICS An example reachability plot is depicted in Fig. 3 . Valleys in this plot indicate clusters: objects having a small reachability value are closer and thus more similar to their predecessor objects than objects having a higher reachability value. Thus, it is possible to explore interactively the clustering structure, offering additional insights into the distribution and correlation of the data.
 OPTICS algorithm, the core-distance of an object p and the reachability-distance of an object p w.r.t. a predecessor object o .
 Definition 3 ( core-distance ) Let p be an object from a database DB, let N  X  ( p ) be the  X  -neighborhood of p , let MinPts be a natural number and let MinPts-dist ( p ) be the distance of p to its MinPts-th neighbor. Then, the core-distance of p ,de-INFINITY otherwise.
 Definition 4 ( reachability-distance ) Let p and o be objects from a database tween o and p , and let MinPts be a natural number. Then the reachability-max ( core-dist  X , MinPts ( o ), dist ( o , p )) .
 with a reachability-value for each object. Its main data structure is a seedlist , con-taining tuples of points and reachability-distances. The seedlist is organized w.r.t. ascending reachability-distances. Initially the seedlist is empty and all points are marked as not-done .
 cycle. For every point p in the result of the range query, it computes r = order of the seedlist is reestablished. 2.2 Related work DBSCAN and OPTICS determine the local densities by repeated range queries . In this section, we will sketch different approaches from the literature to accel-erate these density-based clustering algorithms and discuss their unsuitability for complex object representations. 2.2.1 Exact clustering In the following we will present some approaches leading to exact density-based clusterings.
 Multi-dimensional index structures. The most common approach to accelerate each of the required single range queries is to use multi-dimensional index struc-tures. For objects modelled by low-, medium-, or high-dimensional feature vectors there exist several specific R-tree [ 12 ] variants. For more detail we refer the inter-ested reader to [ 11 ].
 Metric index structures. In contrast to Fig. 1 a where the objects are modelled by a high-dimensional feature vector, the objects presented in the example of Fig. 1 b X  X  are not modelled by feature vectors. Therefore, we cannot apply the index struc-tures mentioned in the last paragraph. Nevertheless, we can use index structures, such as the M-tree [ 7 ] for efficiently carrying out range queries as long as we have a metric distance function for measuring the similarity between two complex objects. For a detailed survey on metric access methods we refer the reader to [ 6 ]. Multi-step query processing. The main goal of multi-step query processing is to reduce the number of complex and, therefore, time consuming distance calcula-tions in the query process. In order to guarantee that there occur no false drops, the used filter distances have to fulfill a lower-bounding distance criterion. For any two objects p and q , a lower-bounding distance function d f in the filter step has to return a value that is not greater than the exact object distance d o of p and q , to safely filter out all database objects which have a filter distance greater than the current query range because the exact object distance of those objects can-not be less than the query range. Using a multi-step query architecture requires efficient algorithms which actually make use of the filter step. Agrawal, Falout-sos and Swami proposed such an algorithm for range queries [ 1 ] which form the foundation of density-based clustering. For efficiency reasons, it is crucial that d ( p , q ) is considerably faster to evaluate than d der to achieve a high selectivity d f ( p , q ) should be only marginally smaller than d ( p , q ) . forms query intensive KDD algorithms into a representation using the similar-the considered algorithm. The approach was applied to accelerate the cluster-ing algorithms DBSCAN and OPTICS by using an R-tree like index structure. ity queries for mining in metric databases. It was shown that many different data mining algorithms can be accelerated by multiplexing different similarity queries.
 Summary. Multi-dimensional index structures based on R-tree variants and clus-tering based on the similarity join are restricted to vector set data. Furthermore, the main problem of all approaches mentioned above is that distance computa-query object. In order to create, for instance, a reachability plot without loss of information, the authors in [ 2 ] propose to use a very high  X  -value. Therefore, all of the above mentioned approaches lead to O ( | DB | 2 ) exact distance computations for OPTICS. 2.2.2 Approximated clustering Other approaches do not aim at producing the exact hierarchical clustering struc-ture, but an approximated one.
 Sampling. The simplest approach is to use sampling and apply the expensive data mining algorithms to a subset of the dataspace. Typically, if the sample size is large enough, the result of the data mining method on the sample reflects the exact result well.
 Grid-based clustering. Another approach is based on grid cells [ 13 ] to accelerate query processing. In this case, the data space is partitioned into a number of non-overlapping regions or cells which can be used as a filter step for the range queries. All the points in the result set are contained in the cells intersecting the query range. To further improve the performance of the range queries to a constant time complexity, query processing is limited to a constant number of these cells (e.g. the cell covering the query point and the direct neighbor cells) and the refinement step is dropped, thereby trading accuracy for performance.
 Distance mapping. In [ 20 ], five different distance-mapping algorithms were in-troduced to map general metric objects to Euclidean or pseudo-Euclidean spaces in such a way that the distances among the objects are approximately preserved. The approximated data mining algorithm is then performed within the Euclidean space based on rather cheap distance functions. If there already exist selective fil-ters which can efficiently be computed, an additional mapping into a feature space is superfluous, i.e. we can carry out the approximated data mining algorithm di-rectly on the filter information.
 Data bubbles. Finally, there exist efficient approximated versions of hierarchical clustering approaches for non-vector data which are based on Data Bubbles [ 22 ]. These approaches augment suitable representatives with additional aggregated in-formation describing the area around the representatives.
 Summary. All indicated approximated clustering approaches are able to generate efficiently the corresponding clustering structure. The question at issue is: How much quality do they have to pay for their efficiency gain? based clusterings trying to confine itself to simple distance computations lower-bounding the exact distances. Further expensive exact distance computations are postponed as long as possible, and are only carried out at that stage of the algo-rithm where they are compulsory to compute the correct clustering. Furthermore, we will also indicate how to use our algorithm for approximated clustering. 2.3 Accelerated density-based clustering In this section, we will demonstrate how to integrate the multi-step query pro-cessing paradigm into the two density-based clustering algorithms DBSCAN and OPTICS. We discuss in detail our approach for OPTICS and sketch how a simpli-fied version of this extended OPTICS approach can be used for DBSCAN. 2.3.1 Basic idea DBSCAN and OPTICS are both based on numerous  X  -range queries. None of the approaches discussed in the literature can avoid that, we have to compute the exact distance to a given query object q for all objects contained in N  X  ( q ) . Especially for OPTICS, where  X  has to be chosen very high in order to create reachability plots without the loss of information, we have to compute | DB | many exact dis-tance computations for each single range query, even when one of the methods discussed in Sect. 2.2.1 is used. In the case of DBSCAN, typically, the  X  -values are much smaller. Nevertheless, if we apply the traditional multi-step query pro-cessing paradigm with non-selective filters, we also have to compute up to | DB | many exact distance computations.
 depend on the size of the database and the chosen  X  -value but rather on the value of MinPts, which is typically only a small fraction of | DB | ,e.g.MinPts = 5isa suitable value even for large databases [ 2 , 9 ]. Basically, we use MinPts-nearest-neighbor queries instead of  X  -range queries on the exact object representations in order to determine the  X  X ore-properties X  of the objects. Further exact complex distance computations are only carried out at that stage of the algorithms where they are compulsory to compute the correct clustering result. 2.3.2 Extended OPTICS The main idea of our approach is to carry out the range queries based on the lower-bounding filter distances instead of using the expensive exact distances. In order to put our approach into practice, we have to slightly extend the data structure underlying the OPTICS algorithm, i.e. we have to add additional information to the elements stored in the seedlist.
 The extended seedlist. We do not any longer use a single seedlist as in the original OPTICS algorithm (cf. Fig. 4 ) where each list entry consists of a pair (ObjectId, Reachability Value). Instead, we use a list of lists, called Xseedlist, as shown in Fig. 5 . The Xseedlist consists of an ordered object list OL, quite similar to the original seedlist but without any reachability information. The order of the objects o in OL, cf. the horizontal arrow in Fig. 5 , is determined by the first element of each predecessor list PL ( o i ) anchored at o i , cf. the vertical arrows in Fig. 5 . o consists of the following information:  X  Predecessor ID. A processed object o i , l which was already added to the reach-ability plot which is computed from left to right.  X  Predecessor Flag. Aflag F i , l indicating whether we already computed the ex-filter information.  X  Predecessor Distance. PreDist ( o i , o i , l ) is equal to if we already computed the exact object distance d o ( o i , o i , l ) ,elseitisequalto to this extended OPTICS algorithm are maintained. In the following, we will de-scribe the extended OPTICS algorithm trying to minimize the number of exact distance computations.
 Algorithm. The extended OPTICS algorithm exploiting the filter information is depicted in Fig. 6 . The algorithm always takes the first element o 1 from OL. If it is at the first position due to a filter computation, we compute the exact distance d ( o from the first position of PL ( o 1 ) . Furthermore, object o 1 might be removed from the first position of OL. On the other hand, if the filter flag F 1 , 1 indicates that an exact distance computation was already carried out, we add object o 1 to the we carry out the procedure update-Xseedlist( o 1 ).
 Update-Xseedlist. This is the core function of our extended OPTICS algorithm. First, we carry out a range query around the query object q = o 1 basedonthefilter of q by computing the MinPts-nearest neighbors of q as follows: ( obj , flag , dist ) which are organized in ascending order according to dist. For all  X  We walk through SortList  X  ( q ) starting at the first element. We set and reorder SortList  X  ( q ) . This step is repeated until the first MinPts elements of SortList  X  ( q ) are at their final position due to an exact distance computation.
The core-distance of q is equal to the distance if dist MinPts  X   X  holds, else it is set to INFINITY.
 try, if q is a core object and dist j  X   X  holds. If there exists no entry for obj j in ( q , flag both cases the ordering of Fig. 5 has to be maintained.
 Lemma 1 The result of the extended OPTICS algorithm is equivalent to the result of the original one.
 Proof First, the extended OPTICS algorithm computes the correct core-distances by applying a MinPts-nearest neighbor search algorithm. Second, in each cycle the extended and the original OPTICS algorithm add the object o 1 having the minimum reachability-distance, w.r.t. all objects reported in the foregoing steps, to the cluster ordering. For the extended OPTICS algorithm this is true, as we have conditions of Fig. 5 , and due to the lower-bounding filter property.
 objects which are very close to the current query object q according to the filter information, whereas the traditional multi-step query approach would compute exact distance computations for all objects o  X  N filter  X  ( q ) .As  X  has to be chosen very high in order to create reachability plots without loss of information [ 2 ], the traditional approach has to compute | DB | many exact distance computations, even when one of the approaches discussed in Sect. 2.2.1 is used. On the other hand, the number of exact distance computations in our approach does not depend on the size of the database but rather on the value of MinPts, which is only a small compute | DB | X  MinPts,i.e.O ( | DB | ) , exact distance computations if we assume the original OPTICS run. Only when necessary, we carry out further exact distance computations (cf. line (*) in Fig. 6 ). 2.3.3 Extended DBSCAN Our extended DBSCAN algorithm is a simplified version of the extended OPTICS algorithm also using the Xseedlist as its main data structure. We carry out an  X  -range query on the lower-bounding filter distances for an arbitrary database object q which has not yet been processed. Due to the lower-bounding properties of the no core point. Otherwise, we test whether q is a core point as follows. and compute for each visited object o the exact distance d o ( o , q ) until for MinPts elements d o ( o , q )  X   X  holds or until we reach the end. If we reached the end, we certainly know that q is no core point. Otherwise q is a core object initiating a new cluster C .
 are inserted into the Xseedlist (cf. Fig. 5 ). All objects for which we have already cluster as the core-object q . At the beginning of OL, we add the entry ( o , NIL ) , where PL ( o ) = NIL indicates that o certainly belongs to the same cluster as q . for which we did not yet compute d o ( o , q ) are handled as follows: serted into OL and the ordering conditions of Fig. 5 are reestablished.  X  If there already exists an entry for o in OL and, furthermore, PL ( o ) = NIL holds, nothing is done.  X  If there already exists an entry for o in OL and, furthermore, PL ( o ) = NIL Fig. 5 are reestablished.
 OL and, if PL ( o 1 ) = NIL holds, we add o 1 to C , delete o 1 from OL, carry out a range query around o 1 , and try to expand the cluster C .IfPL ( o 1 ) = NIL holds, PL ( o length of PL ( o 1 ) = 1, we delete o 1 from OL. Iteratively, we try to expand the current cluster by examining the first entry of PL ( o 1 ) until OL is empty. Lemma 2 The result of the extended DBSCAN algorithm is equivalent to the re-sult of the original one.
 Proof First, the determination whether an object o is a core object is correct as o  X  N  X  ( o )  X  o  X  N filter  X  ( o ) holds due to the lower-bounding filter property. We test as many elements o  X  N filter  X  ( o ) as necessary to decide whether | N  X  ( o ) | X  MinPts holds. Second, similar to the proof of Lemma 1, we can guarantee that an object o is only added to the current cluster if d o ( o , p )  X   X  holds for an object p which has already been singled out as a core object of the current cluster. 2.3.4 Length-limitation of the predecessor lists In this section, we introduce two approaches for limiting the size of the prede-cessor lists to a constant l max trying to keep the main memory footprint as small as possible. The first approach computes additional exact distances to reduce the length of the object reachability lists, while still computing the exact clustering. On the other hand, the second approach dispenses with additional exact distance computations leading to an approximated clustering. Exact clustering. In the case of OPTICS, for each object o i inOL,westoreallpo-lower-bounding property of d f , we can delete all entries in PL ( o i ) which are lo-cated at positions l &gt; l , if we have already computed the exact distance between o computation might possibly lead to several delete operations in the correspond-ing predecessor list. In order to limit the main memory footprint, we introduce a parameter l max which restricts the allowed number of elements stored in a prede-cessor list. If more than l max elements are contained in the list, we compute the exact distance for the predecessor o i , 1 located at the first position. Such an exact distance computation between o i and o i , 1 usually causes o i , 1 to be moved upward in the list. All elements located behind its new position l are deleted. So if l  X  l max holds, the predecessor list is limited to at most l max entries. Otherwise, we repeat the above procedure.
 ( o we iteratively repeat this limitation procedure.
 Lemma 3 The above length limitation approach does not change the result of the extended DBSCAN and OPTICS algorithms.
 PreDist ( o i , o i , l ) are deleted which are necessary for determining whether an ob-ject is directly density-reachable (cf. Definition 1 ) from a core object of the current cluster. For OPTICS we do not delete any entries which are necessary for comput-ing the minimum reachability distance w.r.t. all already processed objects. Approximated clustering. In our approximated approach, we artificially limit the length of the predecessor lists by discarding all elements which are located at a position higher than l max without computing any additional exact distances. This approach might not produce the same result as the original OPTICS and DBSCAN algorithms as the filter distances do not necessarily have to coincide with the exact distances. Note that if we have a very exact filter, the cutting-off of the predecessor lists will not worsen the quality heavily (cf. Sect. 4.2.2 ). Nevertheless, we need to know how much quality we have to pay for the achieved efficiency gain. 3 Similarity measures for clusterings The similarity measures introduced in this section are suitable for generally mea-suring the quality between partitioning and hierarchical approximated clusterings w.r.t. a given reference clustering. Both partitioning and hierarchical clustering algorithms rely on the notion of a cluster.
 Definition 5 ( cluster ) A cluster C is a non-empty subset of objects from a database DB, i.e. C  X  DB and C = X  .
 Definition 6 ( partitioning clustering ) Let DB be a database of arbitrary objects. { 1 ,..., n }: i = j  X  C partitioning clustering of DB.
 clustering CL p ={ C 1 ,..., C n } that C 1  X  ...  X  C n = DB holds. In contrast to the partitioning structure computed by DBSCAN, OPTICS computes a hierarchi-cal clustering order which can be transformed into a tree structure by means of suitable cluster recognition algorithms [ 2 , 4 , 19 ].
 Definition 7 ( hierarchical clustering ) Let DB be a database of arbitrary objects. A hierarchical clustering is a tree t root where each subtree t represents a cluster C Furthermore, the root node t root represents the complete database, i.e. C t root = DB. Again, we do not demand from the n subtrees t i of t = ( C t ,( t 1 ,..., t n )) that C 3.1 Similarity measure for clusters As outlined in the last section, both partitioning and hierarchical clusterings con-sist of flat clusters. In order to compare flat clusters to each other we need a suit-able distance measure between sets of objects. One possible approach is to use distance measures as used for constructing distance-based hierarchical cluster-ings, e.g. the distance measures used by single-link , average-link or complete-link [ 13 ]. Although these distance measures are used for the construction of hierarchi-cal clusterings, these measures are not suitable when it comes to evaluating the quality of flat clusters. The similarity of two clusters w.r.t. quality solely depends on the number of identical objects contained in both clusters which is reflected by the symmetric set difference .
 Definition 8 ( symmetric set difference ) Let C 1 and C 2 be two clusters of a database DB. Then the symmetric set difference d : 2 DB  X  2 DB  X  X  0 .. 1 ] and the normalized symmetric set difference d norm : 2 DB  X  2 DB  X  X  0 .. 1 ] are defined as follows: Note that ( 2 DB , d ) and ( 2 DB , d norm ) are metric spaces. 3.2 Similarity measure for partitioning clusterings In this section, we will introduce a suitable distance measure between sets of clus-ters. Several approaches for comparing two sets S and T to each other exist in the literature. In [ 8 ] the authors survey the following distance functions: the Hausdorff distance ,the sum of minimal distances ,the (fair-)surjection distance and the link distance . All of these approaches rely on the possibility to match several elements in one set to just one element in the compared set which is questionable when comparing the quality of an approximated clustering to a reference clustering. defining similarity between two partitioning clusterings is based on the minimal weight perfect matching of sets. This well known graph problem can be applied here by building a complete bipartite graph G = ( Cl , Cl , E ) between two clus-terings Cl and Cl . The weight of each edge ( C i , C j )  X  Cl  X  Cl in this graph G is defined by the distance d ( C i , C j ) introduced in the last section between the two clusters C i and C j . A perfect matching is a subset M  X  Cl  X  Cl that connects each cluster C i  X  Cl to exactly one cluster C j  X  Cl and vice versa. A minimal weight perfect matching is a matching having maximum cardinality and a min-imum sum of weights of its edges. Since a perfect matching can only be found for sets of equal cardinality, it is necessary to introduce weights for unmatched clusters when defining a distance measure between clusterings.
 Definition 9 ( minimal matching distance ) Let DB be a database and let dist : 2
DB  X  2 DB  X  R be a distance function between two clusters. Let Cl = {
C |
Cl | X | Cl | .Furthermore,let w : 2 DB  X  R be a weight function for the un-matched clusters, and let  X  be a mapping that assigns C  X  Cl a unique number i  X  X  1 ,..., | Cl |} , denoted by  X ( Cl ) = ( C sible permutations of Cl is called ( Cl ) . Then the minimal matching distance d The weight function w : 2 DB  X  R provides the penalty given to every unassigned cluster of the clustering having larger cardinality. Let us note that this minimal matching distance is a specialization of the netflow distance [ 18 ]whichisshown to be a metric if the distance function dist is a metric and the weight function meets the following conditions for two clusters C , C  X  2 DB : 1. w( C )&gt; 0 2. w( C ) + w( C )  X  dist ( C , C ) the underlying distance function dist for the minimal matching distance. Further-more, the unnormalized symmetric set difference allows us to define a meaningful weight function based on a dummy cluster  X  since the empty set is not included as an element in a clustering (cf. Definition 6 ). We propose to use the following weight function w  X  ( C ) = d ( C ,  X  ) where each unmatched cluster C is penalized with a value equal to its cardinality | C | . Thus the metric character of the mini-mal matching distance is satisfied. Furthermore, large clusters which cannot be matched are penalized more than small clusters which is a desired property for an intuitive quality measure. Based on Definition 9 , we can define our final qual-ity criterion. We compare the costs for transforming an approximated clustering Cl  X  into a reference clustering Cl ref to the costs piling up when transforming Cl  X  first into  X  , i.e. a clustering consisting of no clusters, and then transforming  X  into Cl Definition 10 ( quality measure Q APC ) Let Cl  X  be an approximated partition-ing clustering and Cl ref the corresponding reference clustering. Then, the ap-proximated partitioning clustering quality Q APC ( Cl  X  , Cl ref ) is equal to 100% if Cl  X  = Cl ref = X  ,elseitisdefinedas Note that our quality measure Q APC is between 0% and 100%. If Cl  X  and Cl ref are identical, Q APC ( Cl  X  , Cl ref ) = 100% holds. On the other hand, if the clusterings are not identical and the clusters from Cl  X  and Cl ref have no objects in common, 3.3 Similarity measure for hierarchical clusterings In this section, we present a quality measure for approximated hierarchical cluster-ings. To the best of our knowledge, the only quality measure for an approximated hierarchical clustering was introduced in [ 22 ]. A simple heuristic was applied to ability plot resulting from an approximated OPTICS run. The number of clusters found w.r.t.  X  cut was compared to the maximum number of clusters found in the reachability plot resulting from an exact clustering. This quality measure has two major drawbacks. First, it does not reflect the hierarchical clustering structure, but compares two flat clusterings to each other. Second, the actual elements building up a cluster are not accounted for. Only the number of clusters is used for comput-ing the quality. In the following, we will present a quality measure for hierarchical clusterings which overcomes the two mentioned shortcomings.
 Definition 7 ). In order to define a meaningful quality measure for approximated hierarchical clusterings, we need a suitable distance measure for describing the similarity between two trees t  X  and t ref . Note that each node of the trees reflects a flat cluster, and the complete trees represent the entire hierarchical clusterings. tween two trees is the degree-2 edit distance [ 21 ]. It minimizes the number of edit operations necessary to transform one tree into the other using three basic opera-tions, namely the insertion and deletion of a tree node and the change of a node label. Using these operations, we can define the degree-2 edit distance between two trees.
 Definition 11 ( cost of an edit sequence ) An edit operation e is the insertion, dele-tion or relabeling of a node in a tree t . Each edit operation e is assigned a non-is defined as the sum of the cost of each edit operation, i.e. c ( S ) = c ( e 1 ) + X  X  X + c ( e m ) .
 Definition 12 ( degree-2 edit distance ) The degree-2 edit distance is based on n with degree ( n )  X  2, or of relabelings. Then, the degree-2 edit distance E D 2 between two trees t and t is the minimum cost of all degree-2 edit sequences that sequence transforming t into t } .
 can always be transformed into each other using only degree-2 edit operations. This is true because it is possible to construct any tree using only degree-2 edit operations. As the same is true for the deletion of an entire tree, it is always pos-sible to delete t completely and then build t from scratch resulting in a distance value for this pair of trees. In [ 21 ] the authors presented an algorithm which com-putes the degree-2 edit distance in O ( | t | X | t | X  D ) time, where D denotes the maximum fanout of the trees, and | t | and | t | the number of tree nodes. Furthermore, we propose to use the normalized symmetric set difference d norm as introduced in Definition 8 to weight the relabeling cost. Using the normalized ver-sion allows us to define a well-balanced trade-off between the relabeling cost and the other edit operations, i.e. the insert and delete operations. Based on these costs, we can define our final quality criterion. We compare the costs for transforming an approximated hierarchical clustering Cl  X  modelled by a tree t  X  into a reference clustering Cl ref modelled by a tree t ref , to the costs piling up when transforming t  X  first into an  X  X mpty X  tree t nil and then transforming t nil into t ref . Definition 13 ( quality measure Q AHC ) Let t ref be a tree representing a hierarchi-cal reference clustering Cl ref ,and t nil a tree consisting of no nodes at all, rep-resenting an empty clustering. Furthermore, let t  X  be a tree representing an ap-proximated clustering Cl  X  . Then, the approximated hierarchical clustering qual-ity Q AHC ( Cl  X  , Cl ref ) is equal to As the degree-2 edit distance is a metric [ 21 ], the approximated hierarchical clus-tering quality Q AHC is between 0% and 100%. 4 Evaluation In this section, we present a detailed experimental evaluation which demonstrates the characteristics and benefits of our new approach. 4.1 Settings Test data sets. As test data, we used real-world CAD data represented by 81-dimensional feature vectors [ 17 ] and vector sets where each element consists of seven 6D vectors [ 16 ]. Furthermore, we used graphs [ 15 ] to represent real-world image data. If not otherwise stated, we used 1,000 complex objects from each data set, and we employed the filter and exact object distance functions proposed in [15 X 17]. The used distance functions can be characterized as follows:  X  The exact distance computations on the graphs are very expensive. On the other hand, the used filter is rather selective and can efficiently be computed [ 15 ].  X  The exact distance computations on the feature vectors and vector sets are also very expensive as normalization aspects for the CAD objects are taken into ac-count. We compute 48 times the distance between two 81-dimensional feature vectors, and between two vector sets, in order to determine a normalized dis-tance between two CAD objects [ 16 , 17 ]. As a filter for the feature vectors we use their Euclidean norms [ 10 ] which is not very selective, but can be com-puted very efficiently. The filter used for the vector sets is more selective than the filter for the feature vectors, but also computationally more expensive [ 16 ]. Implementation. The original OPTICS and DBSCAN algorithms, along with their extensions introduced in this paper and the used filter and exact object distances were implemented in Java 1.4. The experiments were run on a workstation with a Xeon 2.4 GHz processor and 2 GB main memory under Linux.
 Parameter setting. As suggested in [ 2 ], for an OPTICS run we used a maximum  X  -parameter in order to create reachability plots containing the complete hierar-chical clustering information. For DBSCAN, we chose an  X  -parameter, based on as possible. Furthermore, if not otherwise stated, the MinPts-parameter is set to 5, and the length of the predecessor lists is not limited.
 Comparison partners. As a comparison partner for extended OPTICS, we chose the full table scan based on the exact distances, because any other approach would include an unnecessary overhead and is not able to reduce the number of the required | DB | 2 exact distance computations. Furthermore, we compared our ex-tended DBSCAN algorithm to the original DBSCAN algorithm based on a full table scan on the exact object distances, and we compared it to a version of DB-SCAN which is based on  X  -range queries efficiently carried out according to the ond comparison partner outperforms a DBSCAN algorithm using  X  -range queries basedonanM-tree[ 7 ] and the DBSCAN algorithm according to [ 3 ]. 4.2 Experiments 4.2.1 Exact clustering In this section, we first investigate the dependency of our approach on the filter quality, the MinPts-parameter, and the maximum allowed length of the predecessor lists. For these tests, we concentrate on the discussion of the over-all number of distance computations. Furthermore, we investigate the influence of the  X  -value in the case of DBSCAN, and, finally, we present the absolute run-times, in order to show that the required overhead of our approach is negligible compared to the saved exact distance computations.
 Dependency on the filter quality. In order to demonstrate the dependency of our approach on the quality of the filters, in a first experiment we utilized artificial  X   X  d o ( o 1 , o 2 ) where  X  is between 0 and 1. Figure 7 a depicts the number of distance computations n dist w.r.t.  X  . In the case of DBSCAN, even rather bad filters, i.e. small values of  X  , help to reduce the number of required distance computations considerably, indicating a possible high speed-up compared to both comparison partners of DBSCAN. For good filters, i.e. values of  X  close to 1, n dist is very small for DBSCAN and OPTICS indicating a possible high speed-up compared to a full table scan based on the exact distances d o .
 Dependency on the MinPts -parameter. Figure 7 b demonstrates the dependency of our approach for a varying MinPts-parameter while using the filters introduced obviously the efficiency of our approach increases with a decreasing MinPts-parameter. Note that even for rather high MinPts-values around 10 = 1%  X | DB | , our approach saves up to one order of magnitude of exact distance computations ters for the vector sets and the graphs. Furthermore, even for the filter of rather low selectivity used by the feature vectors, our approach needs only 1 / 9ofthe maximum number of distance computations in the case of DBSCAN and about 1 / 4 in the case of OPTICS.
 Dependency on the maximum allowed length of the predecessor lists. Figure 7 c depicts how the number of distance computations n dist depends on the available main memory, i.e. the maximum allowed length l max of the predecessor lists. Ob-viously, the higher the value for l max , the less exact distance computations are re-quired. The figure shows that for OPTICS we have an exponential decrease of n dist w.r.t. l max , and for DBSCAN n dist is almost constant w.r.t. changing l max param-eters, indicating that small values of l max are sufficient to reach the best possible runtimes.
 Dependency on the  X  -parameter. Figure 8 shows how the speed-up for DBSCAN between our integrated multi-step query processing approach and the traditional multi-step query processing approach depends on the chosen  X  -parameter. The higher the chosen  X  -parameter, the more our new approach outperforms the tra-ditional one which has to compute the exact distances between o and q for all o queries on the exact distances and computes further distances only if compulsory to compute the exact clustering result.
 Absolute runtimes. Figure 9 presents the absolute run-times of the new extended DBSCAN and OPTICS algorithms which integrate the multi-step query process-ing paradigm compared to the full-table scan on the exact object representations. Furthermore, we also compare our extended DBSCAN to a DBSCAN variant us-ing  X  -range queries based on the traditional multi-step query processing paradigm. Note, that this comparison partner would induce an unnecessary overhead in the case of OPTICS where we have to use very high  X  -parameters in order to detect the complete hierarchical clustering order. In all experiments, our approach was always the most efficient one. For instance, for DBSCAN on the feature vectors, our approach outperforms both comparison partners by an order of magnitude indicating that rather bad filters are already useful for our new extended DBSCAN algorithm. Note that the traditional multi-step query processing approach does not benefit much from non-selective filters even when small  X  -values are used. In the case of OPTICS, the performance of our approach improves with increasing filter quality. For instance, for the graphs we achieve a speed-up factor of more than 30 indicating the suitability of our extended OPTICS algorithm. 4.2.2 Approximated clustering In this section, we carry out experiments where we just cut off the predecessor lists PL ( o ) after the l computations between o and the discarded potential predecessor objects. Note that this approach might lead to an information loss. Figure 10 shows that the maxi-mum number of needed distance calculations only marginally increases for higher l max -values for the graphs and the vector sets indicating that we can cut off the ob-ject reachability lists at small l max -values without a considerable information loss. On the other hand, for the feature vectors we have to compute more exact distance computations the higher the l max -value is. The additionally needed exact distance computations (cf. line (*) in Fig. 6 ) are due to the rather low filter selectivity of the used filter.
 ing the quality measures introduced in Sect. 3 . For extracting the hierarchical tree structure, we used the cluster recognition algorithm presented in [ 4 ]. Figure 11 depicts the quality measures Q APC for DBSCAN and Q AHC for OPTICS for our three test data sets w.r.t. varying l max values. Our quality measures indicate a very high quality for the graphs and the vector sets over the full range of the investi-gated l max values. On the other hand, when using the feature vectors both quality measures Q APC (for DBSCAN) and Q AHC (for OPTICS) increase with increas-ing l max values. These tests not only indicate that we can cut off the predecessor lists at small values of l max without considerably worsening the clustering qual-ity when using selective filters. The tests also demonstrate the suitability of our quality measures Q APC and Q AHC which indicate low quality when filters of low selectivity are combined with small l max values. 5Conclusion In many different application areas, density-based clustering is an effective ap-proach for mining complex data. Unfortunately, the runtime of these data-mining algorithms is rather high, as the distance functions between complex object repre-sentations are often very expensive. In this paper, we showed how to integrate the well-known multi-step query processing paradigm directly into the two density-based clustering algorithms DBSCAN and OPTICS. We replaced the expensive exact  X  -range queries by MinPts-nearest neighbor queries which themselves are based on  X  -range queries on lower-bounding filter distances. Further exact com-where they are compulsory to compute the correct clustering result. Furthermore, we showed how we can use the presented approach for approximated clustering. In order to evaluate the trade-off between the achieved efficiency gain and the quality loss, we introduced suitable quality measures for comparing the parti-tioning and hierarchical approximated clusterings to the exact ones. In a broad experimental evaluation based on real-world test data sets we demonstrated that our new approach leads to a significant speed-up compared to a full-table scan on the exact object representations as well as compared to an approach, where the  X  -range queries are accelerated by means of the traditional multi-step query pro-cessing concept. Furthermore, we showed that for approximated clusterings we can reduce the number of required distance computations even further. Finally, we pointed out that the resulting approximated clustering quality heavily depends on the filter quality demonstrating the suitability of our introduced quality measures. dealing with complex object representations also benefit from a direct integration of the multi-step query processing paradigm.
 References
