 The completeness and certainty of a user X  X  preferences may vary during her preference construction process in a conversational recommender. In order to more effectively support users to uncover their hidden criteria and/or solve preference conflicts, we propose to generate adaptive tradeoff explanations in organization-based recommender interfaces, to be conditional on the user X  X  contextual needs. An experiment shows the adaptive element X  X  higher potential to improve recommendation efficiency, relative to methods without this feature. H5.2 [ Information interfaces and presentation ]: User Interfaces  X  graphical user interfaces ( GUI), user-centered design . Design, Human Factors. Conversational recommenders, explan ation, preference elicitation. Recommenders that involve users in a conversational interaction model to incrementally elicit their preferences are commonly referred as conversational reco mmenders, which are especially usable for solving multi-objective decision problems. For example, while searching for high -risk products (e.g. PCs, digital cameras, cars), users inherently ha ve multiple objectives to reach for targeting at their desired product, like criteria on computers X  price, processor speed, memory, hard capability, etc. Most of recent works on conversational recommenders have emphasized on how to generate critiques in order to allow for effective feedback mechanism during each interaction cycle. As a typical example, the dynamic-critiquing system can propose a set of compound critiques (e.g. Different Manufacturer, Lower Resolution and Cheaper X ) for the user to choose one to improve the current recommended product [13]. The suggested critique is also regarded as a kind of explanation, explaining to users the recommendation opportunity in the remaining dataset [9]. However, the critique is computed purely according to the availability of products in the dataset, without dynamically adapting its generation process to the condition of the user X  X  current preferences (such as whether the user would prefer the suggested critique and whether it can further assist users to complete their requirements or solve preference conflicts). As a matter of fact, according to adaptive decision theory [10], people are usually unable to accurately state their preferences up front especially when they ar e confronted with an unfamiliar product domain or overwhelming information. We call such preference condition as  X  incomplete preferences  X  which can be concretely reflected with un-specified criteria on the product X  X  attributes and will typically appear during a user X  X  beginning interaction cycles. Moreover,  X  conflicting preferences  X  phenomenon will happen when no option is best on all of a user X  X  stated attribute preferences, that she may have probably established initially or in the la ter interaction rounds when being more familiar with the product catalog. Indeed, conflicting has long been recognized as a major source of decision difficulty and many researchers have argued that making tradeoffs between more of one thing and less of anot her is a crucial aspect of high-quality and rational decision maki ng [4,10,11]. However, users have been found often avoiding explicit tradeoffs primarily due to their cognitive or emotional limitation [8]. It is hence meaningful to make recommendations explicitly serve people with different initial prefer ence certainty levels and even tailor to various preference conditions of a single user during her whole preference construction pr ocess. Additionally, it is of critical value to correspondingly explain these recommendations so as to guide users to unders tand the recommending rational and hence complete their preference model for achieving an accurate decision. In recent years, explanations have been increasingly studied while mainly for collaborative filtering based or content-based recommendations [6,14]. Fe w works have actually paid attention to developing explanations to expose tradeoffs (i.e. pros and cons) of recommendations in a conversational recommender, although much earlier works on us er-adaptive systems have indicated the importance of the tradeoff exposure [2,5]. In the following, we first briefly summa rize our previous findings, based on which we will introduce our newly designed adaptive tradeoff explanation interfaces in a prototype recommender. We have previously proposed a method to organize recommendations into categories, so called the organization-based recommender interface empirically proven with two practical benefits [12,3]: one is that it can more effectively build users X  trust in recommendations than the traditional interface where recommendations are listed one by one even each with a  X  X hy X  explanation; and another is that it can more accurately predict critiques that users truly intend to make, compared to other typical critique generation methods. Despite of these benefits, one lim itation of our previous approach as observed from user studies is th at it is lack of adaptability to the variety of user requirement s. For instance, some users commented that they felt helpless about how to revise their preferences when there are c onflicts, since the recommended products including their explana tions seemed not directly corresponding to such situation. Thus, in this paper, we propose an approach to addressing the limitation, which is to integrate  X  X daptive tradeoff explanations X  into our preference-based organization interface. In essence, the adaptive element works by automatically capturing the current user X  X  preference model at a time, analyzing its property (e.g. whether it is  X  X ncomplete X  and/or  X  X aving conflicts X ) and then adjusting the organization algorithm being conditional on the property. For example, when the system observes that the user X  X  preferences are incomplete respecting some un-stated attributes, suggestions will be made and expl ained as  X  X hese products have bigger hard drive capability and larger screen size that you may like X . In another case when ther e are preference conflicts, the explanation for a set of products will be like  X  X hey satisfy your preferences on price and processor speed, but not on memory type X . In order to test the new algorithm X  X  efficiency, we have conducted a simulation to compare it with our previous method and a baseline approach. The tradeoff explanation is inte nded for two purposes: one is to show the tradeoff knowledge th at exists between product attributes (e.g. higher processor sp eed is often correlated to higher price) for the user to get to know their relationship; another is to explain the conflict (if there is) occurring in a user X  X  stated criteria so as for her to make proper weight adjustment. We are still based on the association rule mining to discover the tradeoff relationship, like in our previous organization algorithm. However, the definition of inputs to Apriori (the association rule mining tool [1]) is fundamentally different from the original method. That is they are adaptively changeable conditional on the user X  X  current needs, rather than configured with a uniform definition and format (see our prior publications [3, 12]). In truth, our new algorithm has been developed with the purpose of explicitly tailoring its generation of recommendations and explanations to different user preference conditions such as  X  incomplete preferences  X  or  X  preference conflicts  X . More concretely, in different cond itions, the recommendations along with their explanations will be eith er adapted to stimulate users to uncover hidden needs, or solve conflicting values, or with both objectives. The user X  X  reaction to the displayed items will be reflected in her refined preference model for the system to compute a new set of recommendations in the next cycle. The conversational process can conti nue until the user X  X  preferences are maximally complete and precise, at which time the best matching product should be her targ et choice. Figure 1 shows an example of the algorithm X  X  control flow at a cycle. 
Figure 1.Control flow of the ad aptive organization algorithm A pre-filtering process is first c onducted to retrieve products that exactly match all of the user X  X  attribute preferences. If the retrieval is successful and th e number of matching products exceeds a pre-defined threshold p (e.g. p = 20), we mainly consider this retrieval set for the organization. At this point, if the user X  X  preferences are found  X  incomplete  X , the organization is aimed at suggesting preferences on un-stated attributes (e.g.  X  X hese products satisfy all of your stated preferences. In addition, they have bigger memory that you may like X  where the memory is an un-stated attribute that the user has not specified any criterion). Specifically, we convert all retrieved products into a set of tradeoff vectors. Each vector indicates the corresponding product X  X  tradeoff properties on un-stated attributes, by comparing each of the attribute va lues with its average across all products ( X  better-than-average  X  abbreviated as BTA and noted as  X   X   X , or  X  worse-than-average  X  abbreviated as WTA and noted as  X   X   X ). As an example, one pr oduct can be formalized as {(processor speed,  X  ), (weight,  X  ), (memory,  X  )} meaning that it has higher processor speed, lighter weight but smaller memory relative to these attributes X  average values in the retrieval set. The determining of BTA or WTA property is concretely based on the attribute X  X  default preference. That is, as for each attribute, a default preference is pre-defined (e.g. for  X  X rocessor speed X , it is the higher, the better). These tradeoff vectors are then inputted to the Apriori algorithm to reveal how the attributes are frequently associated between one another, based on which the organization is made. As a sample interface, Figure 2 shows how the set of matching products is displayed in the organization interface, where each category contains products with the same properties on suggested attributes. Thus, it can be seen that the tradeoff explanation here is to disclose what additional benefits along with compromises the user may be interested in, so as to stimulate them to discover hidden needs. 
Figure 2. Explanation of preference suggestions to stimulate 
Figure 3. Explanation of partially satisfied products to solve In another condition when no ava ilable product matches all of the user X  X  stated preferences or the retrieved number is less than the threshold p , a partial satisfaction set will be organized and returned. The goal is to propose di fferent tradeoff directions for the user to decide which attri bute constraint(s) she would be willing to relax, to exchange fo r gains on more important ones. Based on the Multi-Attribute Utility Theory (MAUT) [7], each product is computed with a sc ore representing its weighted satisfying degree with the user X  X  preferences, and the set of products with higher scores is th en returned. Each product in the set is formally converted into a tradeoff vector indicating which attribute preference(s) it matches (i.e.  X  satisfactory  X  noted as  X  ) and which one(s) it does not (i.e.  X  unsatisfactory  X  as  X  ). For example, one product is form alized like {(display size,  X  ), (weight,  X  ), (processor speed,  X  )}, indicating that  X  X his product satisfies your preferences on display size and weight, but not on processor speed X . Therefore, the explanation at this point is to expose the tradeoff relation between conflicting preference values that the user specified, so as to support her to make informed weight adjustment to refine preferences. Please note that although it is also called  X  X radeoff vector X  in this case, the meaning is different from when suggesting preferences: satisfactory against unsatisfactory in accordance with the user X  X  stated preferences, instead of BTA or WTA on un-stated attributes. Again, the association rule mining tool is applied to uncover frequent and recurring patterns among inputted tradeoff vectors. Products are thereafter organized into categories in the form of  X  X hese products satisfy your preferences on ..., but not on ... X  (see Figure 3). If  X  incomplete preferences  X  are additionally observed along with  X  preference conflicts  X , they will be also reflected in the generated category (e.g.  X  X hese products satisfy your preferences on .... In addition, they have bigger hard capability X , where  X  X igger hard capability X  is a suggested preference). Since Apriori normally creates a number of category candidates (each candidate is a discovered association rule), we primarily select ones with higher overall utilities, while the utility is computed differently dependent on the current preference condition. For instance, it is formally calculated as the category X  X  gains versus losses relative to th e user X  X  potential needs, if the purpose is to suggest preferences , but a weighted tradeoff value between conflicting criteria when it is to assist users in solving preference conflicts. In either case, diversity is further involved in order to return categories as dive rse as possible in respect of both their titles (i.e. the discovered ru les) and associated products (please refer to [3] in this regard). As a result, k categories is selected and each category is representative of several products that share the same pattern (e.g. {(price,  X  ), (weight,  X  ), (memory,  X  )}). A pre-designed base of explan ation templates is then used to explain each selected category in a right manner. Specifically, the property (i.e.  X  and  X  ) assigned to each attribute is concretized regarding its actual meaning. For example, if it is  X  X etter-than-average X  regarding one attribute (e.g. processor speed), the explanation will be like  X  X igher processor speed X  as a suggested preference. On the other hand, if it is  X  satisfactory  X  referring to an attribute value satisfying the user X  X  stated criterion (e.g. on price), the explanation will be like  X  X atisfy your preference on price X . Please see more explanation examples (i.e. categories X  titles) in Figures 2 &amp; 3. As noted before, a user X  X  preferen ce construction is in nature an incremental process. After each recommendation cycle (if the user does not quit), the user X  X  preferen ces will be automatically refined according to the action she did, so as for the computation of new items in the next cycle. More cocnretely, default value function and weight will be assigned to an attribute and included in the user X  X  preference model, if she se lected a near-target product with suggestion on this previously unconsidered attribute. Weight adjustment will be performed when there are preference conflicts. The weight of  X  satisfactory  X  attribute(s) appearing in the chosen product will be increased by a certain degree (e.g.  X  ), and the weight of  X  X nsatisfactory X  one(s) will be decreased by  X  (  X  is default set as 0.25). If the weight of an attribute is reduced to 0, it will be removed from the user X  X  preference model (implying that the user is indifferent to it any more). In order to understand whether the integration of the  X  X daptive element X  can experimentally improve the algorithm X  X  recommendation efficiency, i.e. saving users X  interaction cycles in locating their target choice, we performed a simulation to compare the new algorithm with our previous method that is without the adaptive feature [3] (so termed as Non-ADPT), in addition to a standard related appr oach which is also based on the association mining to categorize but without consideration of both user preferences and adaptability (the dynamic-critiquing [9,13], henceforth STD). A PC dataset (with 120 PCs) was used for the offline evaluation. Each product is described by 9 ma in attributes including price, type, manufacturer, processor speed, memory, etc. A similar leave-one-out methodology to [13] was a dopted in our experiment. That is, at each time, we randomly chose one product (called the base item) in the dataset used for two purposes: the product most similar to it was determined as a simulated user X  X  target choice, and the random subset of its a ttribute values was based to generate the user X  X  initial pr eferences. The tested algorithm returns k categories of recommendations during each cycle, and the category with the product cl osest to the  X  X ser X  X  X  ideal preferences (i.e. the base item) is selected. The preference model is refined according to the category X  X  properties, and a new set of recommendations is computed for the next round. The interaction session terminates when the  X  X ser X  X  X  target choice is identified. Each product in the dataset was left out at 10 times and we ran the leave-one-out test for all products in the dataset (so totally 120x10 times of tests on each algorithm). The number of recommendation cycles for finding the target choice was recorded. Figure 4.a shows the average cycles respec tively through the three methods. It can be seen that the adaptive organization algorithm (ADPT) demands the least amount of cycl es, which is reduction of 33% relative to the standard organi zation method (STD) and 12% to our previous approach (Non-ADPT). We further did the comparisons by varying initial preferences X  completeness levels, the number of returned categories (i.e. k ) and the number of products displayed within each category (i.e. q ), respectively. It shows that ADPT won in all of the scenarios. Specifically, when the initial preferences became more incomplete, all of the three algor ithms turned out with increasing session lengths, but ADPT relatively required the least interaction effort (see Figure 4.b). The adaptive algorithm was also stable in obtaining the highest efficiency, especially when less categories were returned ( k = 2 or 4), or less products were shown in each category at a cycle ( q = 2 or 4). On average, it reaches 37% and 39.6% reductions of cycles compared to STD, and 20% and 10.6% to Non-ADPT (see Figures 4.c and 4.d). In this paper, we introduced and evaluated an adaptive element that can explicitly tailor the process of retrieving and explaining recommendations to the condition of user preferences, taking into account of both  X  incomplete preferences  X  and  X  preference conflicts  X  phenomena. Positive experimental results imply its potential practical impacts. Therefore, for the next step, a user study will be carefully designed and performed in order to not only verify the simulation X  X  findings, but also measure users X  decision quality and subjective perceptions when they are in reality interacting with the adap tive tradeoff explanation interface. We believe that with this ad aptive element, a conversational recommender should have higher chance to allow its users to efficiently obtain an accurate and confident decision. [1] Agrawal, R., Imielinski, T., and Swami, A. 1993. Mining [2] Ardissono, L., Goy, A., Petrone, G., Segnan, M., and Torasso, P. [3] Chen, L., and Pu, P. 2007. Pr eference-based organization [4] Frisch, D., and Clemen, R. T. 1994. Beyond expected utility: [5] Jameson, A., Sch X fer, R., S imons, J., and Weis, T. 1995. [6] Herlocker, J. L., Konstan, J. A., and Riedl, J. 2000. Explaining [7] Keeney, R., and Raiffa, H. 1976. Decisions with Multiple [8] Luce, M. F., Payne, J. W., and Bettman, J.R. 1999. Emotional [9] McCarthy, K., Reilly, J., McGinty, L., and Smyth, B. 2004. [10] Payne, J. W., Bettman, J. R., and Johnson, E. J. 1993. The [11] Pu, P., and Chen, L. 2005. Integrating tradeoff support in [12] Pu, P., and Chen, L. 2006. Trust building with explanation [13] Reilly, J., McCarthy, K., McGinty, L., and Smyth, B. 2004. [14] Vig, J., Sen, S., and Riedl, J. 2009. Tagsplanations: explaining 
