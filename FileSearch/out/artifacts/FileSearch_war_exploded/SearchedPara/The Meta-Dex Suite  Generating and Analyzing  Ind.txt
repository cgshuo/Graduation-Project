 Our Meta-dex software suite extracts c ontent and index text from a corpus of PDF files, and generates a meta-index that references entries across an entire domain. We provide tools to analyze the individual and integrated indexes, and visualize entries and books within the meta-index. The suite is scalable to very large data sets. H.5.2 [ Information Interfaces and Presentation ]: User Interfaces  X  evaluation/methodology, prototyping Algorithms, Measurement, Design. Digital collections, digital books, indexes, meta-indexes, visualization, user interfaces. Large-scale digitization projects such as the Gutenberg Project, and Google Books [1] have placed large amounts of text in the public domain. Scholars have long used back-of-book indexes (BoBIs) to find information in print [2]. BoBIs are compiled by professional indexers as a useful summary of content, and as indicators of important concepts. They also offer an alternative to keyword search for locating relevant information [3, 4]. The multiple indexes in a corpus contain a range of domain vocabulary. They reveal knowledge structures and provide entry points for navigation. With this in mind, our Indexer X  X  Legacy project is investigating how BoBIs can add value to search. The Meta-dex suite is the product of this endeavour. To our knowledge no widely-available soft ware exists to create meta-indexes from plain-text and PDF documents. Meta-indexes are generated across several steps (Fig.1). At each step, summary statistics are calcula ted and saved to the database for later analysis. We will demonstrate the build process, and show visualizations of end produc ts. The suite X  X  components are: Extractor . Plain-text files of conten t and index are generated from the book PDF. OCR error filter . An enhanced editor highlights for correction terms that are likely to be OCR errors. Expander . Each entry in an index is 'expanded' into fully-qualified entries from the main heading through each subheading to each individual page reference. Compressor . The large sorted file is 'compressed' back into a tabbed index format, to comp ile references under similar headings. Index terms are generalized to reduce the effects of plurals, gerunds, etc. The entries from each book are tagged with the book's unique ID number. The result is a global domain index that references specific pages in specific books. Analysis tools . During extraction, the number of lines, entries, sub-entries, parts-of-speech, etc. and term frequencies are saved to a database for later analysis. Basic proportions of most-compression ratios, etc. are generated for comparison between domains. Language modeling is also used to gauge the coherence between index and content, both within books and across an entire domain. Visualization tools show entries and terms from personalized searches projected onto the domain meta-index, and will use interactive colour, z ooming, and three-dimensional displays. Suite components can be run locally, but can also be run on the cloud in order to accommodate users without adequate local computing power and storage. Ou r current implementation uses an Amazon EC2 server to read from and write to storage on S3. [1] Coyle, K. 2006. Mass digitization of books. Journal of Academic [2] J X rgensen, C. and Liddy, E. D. 1996. Information access or [3] Abdullah, N. and Gibb, F. 2008. Using a task-based approach in [4] Egan, D.E., Remde, J.R., Gomez, L.M., Landauer, T.K., Eberhardt, 
