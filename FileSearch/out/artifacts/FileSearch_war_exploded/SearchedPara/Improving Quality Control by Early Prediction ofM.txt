 We describe methods for continual prediction of manufac-tured product quality prior to final testing. In our most ex-pansive modeling approach, an estimated final characteristic of a product is updated after each manufacturing operation. Our initial application is for the manufacture of microproces-sors, and we predict final microprocessor speed. Using these predictions, early corrective manufacturing actions may be taken to increase the speed of expected slow wafers (a collec-tion of microprocessors) or reduce the speed of fast wafers. Such predictions may also be used to initiate corrective sup-ply chain management actions. Developing statistical learn-ing models for this task has many complicating factors: (a) a temporally unstable population (b) missing data that is a result of sparsely sampled measurements and (c) relatively few available measurements prior to corrective action op-portunities. In a real manufacturing pilot application, our automated models selected 125 fast wafers in real-time. As predicted, those wafers were significantly faster than aver-age. During manufacture, downstream corrective processing restored 25 nominally unacceptable wafers to normal oper-ation.
 H.2.8 [ Database Management ]: Data Mining Algorithms manufacturing, quality control, prediction
Modern-day instrumented manufacturing is a complex pro-cess, sometimes taking weeks to even months to produce the final product. Starting from the initial crude state, the fi-nal product is produced by the application of hundreds of steps and tools. Typical examples of where we encounter such heavily instrumented operations is the semiconductor industry, the pharmaceutical industry, the (processed) food industry. Given the complexity of these processes and the time to manufacture, it is not surprising that extensive ef-forts have been made to collect data and mine them looking for patterns that can eventually lead to improved productiv-ity [6], [7], [13], [17], [18]. Among the primary roles of data mining in these domains are quality control and the detec-tion of anomalies. When something goes wrong, such as a significant reduction in final product quality, the data are pulled and examined to find probable causes. Many of these industries are extremely sensitive to such mishaps. Even a meager (few percent) drop in quality could cost a corpora-tion millions to even a few billions of dollars. Conversely, a few percent increase in quality can be highly lucrative. From a data collection perspective, tens or even hundreds of thou-sands of measurements are taken and recorded to monitor results at different stages of production. Since, the objective is mostly to monitor quality of production, measurements can be sparsely sampled, typically less than 10%.

In contrast to monitoring production for diagnostic appli-cation, in this paper we consider prediction of final prod-uct quality. In particular, we focus on the semiconductor industry, where we predict the final microprocessor perfor-mance. The challenges we face and the methods we employ are largely applicable to other such domains mentioned be-fore.

Each wafer, which is a collection of chips, has an incre-mental history of activity and measurement accrued during its manufacture. In its purest and most ambitious form, our objective is to predict the final outcome of each wafer in terms of critical functional characteristics. Months may pass before a chip is completed, hence the great interest in min-ing data prior to final testing [9], [1], [5]. Moreover, if such an endeavor were to be successful, it would greatly enhance manufacturing productivity.

While many alternative testing measurements are reason-able to measure the health of a wafer, in our initial appli-cations, we designate a proxy for microprocessor chip speed as the predicted outcome. Thus during manufacture, the
Figure 1: Overview of the applied methodology average speed of the finished product is estimated at a time far from completion.

Using the same data that are recorded to monitor indi-vidual elements of the fab manufacturing process, the final performance of a wafer is estimated. This exercise implic-itly raises, and in part addresses the question of how much power such a set of measurements, designed explicitly for the purposes of monitoring unit and integrated process per-formance, has for this very different prediction application.
Measures of speed are the final critical characteristics used in this paper to measure outcome. A chip running too slow is clearly a negative outcome, as is a chip running too fast, since it may consume too much power. The advantages of ac-curately predicting final performance are manifold. Among the actions that might be taken are as follows:
Predicting final performance based on incomplete mea-surements is a difficult task. It implies having accurate and highly predictive measurements. The benefits can poten-tially be great in improving manufacturing efficiency and yield and the early detection of potentially weak outcomes. From a machine learning perspective, technical difficulties abound: with time-varying populations to inherent insta-bilities of massively missing data, to only a few measure-ments being known before critical steps. To address these difficulties, knowledge-based methods for filling in missing values are developed, specialized sampling techniques are employed, combined learning methods such as linear with boosted trees are invoked, and customized schemes to ad-just and optimize the predictions obtained from the learning Figure 2: Stages of wafer/chip manufacturing. A wafer moves from left to right. methods are deployed. An overview of the applied method-ology is shown in Figure 1.

In real microprocessor production experiments, our auto-mated models selected 125 predicted fast wafers (5 lots) in real-time. Wafers from these selected lots were split for post prediction processing to allow corrective processing and as-sessment of prediction accuracy. These selected wafers were significantly faster than average, as predicted. Of the 5 lots, one lot was fast enough that downstream corrective pro-cessing restored nominally unacceptable wafers to normal operation.
It takes a few months to manufacture a microprocessor, during which a wafer undergoes incremental processing (nom-inally value adding) and measurement (nominally non-value adding) operations. During production, in total, thousands of different measurements are taken, and while some rela-tively small number of measurements are made on at least one wafer in every lot, as few as only 5 to 10% of the wafers may undergo any single measurement. Furthermore, there may be varying degrees of coordination in the selection of lots and wafers between measurements. Thus some lots and wafers may have many measurements while other lots and wafers have only a very few or no measurements beyond the relatively small set of compulsory measurements.

Figure 2 illustrates the progression of a wafer through the line for a mainframe microprocessor. Here, a wafer starts at step 1, where a Pad Oxide operation is performed, and proceeds to increasingly numbered steps. Wafers typically travel in groups of 25, called a lot. Measurement steps monitoring the quality of individual processing steps, or as-sessing the quality of integrated processing progress, follow many processing steps. These measurement steps may be performed on randomly selected lots, with a lot sampling frequency determined by quality control metrics, and most commonly on 2 to 4 randomly selected wafers within each sampled lot. The same wafers may not necessarily be mea-sured on following steps, so that most wafers will have a random collection of measurements, with many of them un-known.

The target outcome for prediction is an electrical test (PSRO) serving as a (inverse) proxy for microprocessor speed. The higher the PSRO the slower the wafer and vice-versa. This test is conducted on all wafers as one of the last set of electrical tests (LT) conducted on test structures built in the wafer kerfs. In an ideal implementation, we would update a prediction of PSRO measured at LT for each wafer after each processing and measurement step.

However, in these initial implementations, we established a limited number of landmarks in the production process at which to provide and update predictions. These land-mark steps are selected based on knowledge of the pro-duction line. While the ideal implementation of continual prediction covers all possibilities, a reasonable alternative is to make the predictions after these critical landmark steps. This coordinates the data collection for all wafers, so that they are synchronized relative to completeness of data, and more amenable to statistical modeling. Engineering knowl-edge also plays an important role in defining the landmarks. From the engineering perspective, landmarks may be se-lected based on the potential actions that may be taken. In our case, we can continue to model and predict after each step, and predictions tend to get more accurate as more steps are completed. However, corrective processing action is only feasible during early stages of manufacture, that is, with less than 50% of steps completed. In Figure 2, we might establish landmarks at step 7 and 14, where predic-tions after step 14 might be useful for customer triage, but no corrective processing action can be taken.

For our primary application, the most critical prediction of final speed was made at a landmark marking the last time for corrective processing action. If a wafer X  X  predicted speed was unacceptably high or low, its progress on the line was halted until an engineering review and response, including tailored remedial downstream processing. The basic unit for sampling is a wafer and its historical record. Depending on the application and manufacturing line operation policies, it may be necessary to predict final mean or median speed by individual wafer or by lot. In our initial implementation, we predicted mean lot speed by averaging the predictions of the individual wafers comprising those lots.
Our application has the following input and output char-acteristics:
Using these input measurements, the objective is to pre-dict the output measure long before it is actually measured. In the ideal application, a variety of engineering and man-agement actions may be initiated based on the continuously updated predictions of final wafer characteristics. Unwar-ranted corrections to the wafers or supply-chain actions may be very costly, in the worst case ruining nominally salable products. This imposes a clear requirement that the predic-tions be made with high precision. Thus, depending on the expected precision, we restrict actions to those wafers that are predicted to be most deviant. In our application these are the estimated fastest and slowest wafers.
The data are all real valued and can be posed in a standard vector format. For any wafer, W(i), the target speed predic-tion, can be made by mapping from the input vector X(i) to the output, Y(i). The complete data for wafers that have completed testing can be readily retrieved from a database. However, the wafers of interest, for which actionable predic-tions are to be made, have not completed even half of the full processing flow. Thus the input data vector for those wafers is highly censored. Any hope of making highly accu-rate predictions with such a data set relies strongly on the stability of the processes occurring downstream from that last data collection step, or an assumption that the down-stream operations have relatively little influence on speed. This results in a standard data presentation with one prac-tical deficiency: Most of the data items are missing. Figure 3 presents fraction of the missing data for each of the mea-surements from a sample of 6435 completed wafers. Approx-imately 90% of the nominally anticipated measurements are missing. The missing values are not consistent for a select set of measurements. Instead, the measurements are randomly sampled, not corresponding to any particular requirement for the feature. Thus the actual recorded data will vary in the number of missing values from wafer to wafer.

The missing values reflect both the measurement sam-pling policies as well as the censored nature of the data. To estimate whether unit and integrated processes are op-erating within specification, sampling of some measurement values is adequate to collect mean values for quality control. That has traditionally been the main goal in sampling the measurements. When the goal is modified to use these same measurements for prediction, the inadequacy of current data collection standards is manifest. With 90% missing, predic-tion is not feasible. How then do we transform an intractable problem due to lack of data to a feasible application with adequate data? Modifying the sampling procedures to full data collection, at least for some key measurements, is a potential long-term strategy. However, for immediate and practical action, the current data samples must be used as is. A related issue, that we do not address further here, is whether the particular physical or chemical measurement designed for optimal quality control of a given unit process is an optimal, or even adequate, measurement for the purposes of prediction.

During manufacturing, wafers in a lot are generally pro-cessed and measured together as a group, explicitly so in batch processing tools, implicitly so in single wafer tools, undergoing the same process essentially simultaneously, in the same tools. We can take advantage of these relationships to improve estimates of missing measurements. Consider the following hierarchy of possibilities for estimating a missing measurement for a wafer.
The simplest idea is to estimate missing measurements by the global measurement mean, using the complete sample. This approach would allow machine learning to function, possibly succeeding when the most predictive measurements are more fully sampled. In our application, over 90% of measurements are missing, and this approach fails to predict accurately.

The second idea is to use the wafer X  X  lot mean. Because the wafers with a lot are generally processed identically, this approach can improve results greatly over using a global mean.

The next idea improves somewhat over the lot mean. In the course of production, some wafers may temporarily be split from their parent lots into child lots. The child lots may undergo single or multiple processes at different times and by different tools. In this case, at the expense of additional record-keeping, the individual child lot means are used for estimating each wafer X  X  missing values, based on each wafer X  X  lot membership at each process, rather than using the full lot means.

The variance of a measurement within a lot is usually much less than between lots. That explains the rationale for using within lot estimates for missing values. Of the three alternatives cited here, in our application, the detailed child-lot option yields the best predictive accuracy for reasons mentioned earlier.

It is also important to note that other machine learning methods for filling in missing values, such as expectation-maximization based methods, were tested and resulted in less accurate predictions than the suggested approach; pos-sibly because they are agnostic to domain specific informa-tion. Moreover, such methods are significantly more compu-tationally expensive, which is undesirable in the anticipated large-scale applications.
In the previous section, we reviewed the sampling of mea-surements. This is inherent in the operation of the fab, and is something that is unlikely to be modified due to time and cost constraints.

In this application, our data set is continually growing due to the manufacture of additional chips. Under the as-sumption that the data are stable and are from the identical population, the complete sample would be used for learning. Once the manufacturing process has stabilized, the physical relationships among the measurements should also stabilize. The largest sample in a high-dimensional feature space is likely best for learning and most representative of the com-plete population.

Here we see competing themes for learning. Depending on the stability of the manufacturing processes, we are pulled in different directions. If the population is stationary, the standard train and test model can be applied on the full sample. However, it is not unusual for the population to be nonstationary in the complex manufacturing environment for semiconductors. Yield or performance enhancing process adjustments may continue over a significant portion of a product X  X  life cycle, while nominally stable processes may evolve within or in some cases temporarily outside of control limits. In these environments, the population acts like a time series, where the most recent data are more valuable that older historical data.

To make predictions and measure performance, a sepa-rate train and test set of prior results are essential. Clearly, lots must be completely separated, given their underlying relations among their wafers. Because results may change over time, and the population is not stationary, independent time-ordered sets are advantageous over randomly sampled wafers or lots. This time-ordering corresponds to the real manufacturing environment, where we look at recently man-ufactured wafers to predict future wafer performance. This application has thousands of wafers to sample, and ample data are present for training and testing. If the populations from these two time periods are very similar, some reason-able percentage of the complete sample could be used for training and testing, for example 70% training and 30% test-ing. However, given the nonstationary nature of data, better results can be achieved by restricting the training data to a window of k days. This reflects the usual time-series ex-pectation  X  for non-periodic data X  that the more recently completed wafers are most indicative of expected results for current wafers that are still progressing. In our case, we use the following constraints on data sampling:
The value of k is typically much smaller than n , perhaps 3 months of data. However, the choice of k must also be verified by testing, and several possibilities are examined. The population may change, and that implies that these values and experiments may be performed periodically to verify previous choices. Yet, we know that even good per-formance on test cases could change over time, so it is wise to have a large test set taken over a longer time-frame that is representative of varying conditions. In particular, we have gone through periods where pessimism is more warranted in predictions, especially when changes are being made to en-hance the manufacturing processes. The expectation is that updates to the manufacturing process are implemented with an eventual return to stability. Thus we adopt an emphasis on recent data for training, and more extensive historical data for testing.

Figure 4 illustrates the evaluation procedure that is used to estimate model predictive performance for the current wafers and to determine sample and model characteristics. In a static environment, one might simply choose those mod-Table 1: Above is the comparison in terms of av-erage R 2 of different state-of-the-art learning meth-ods at different steps in the processing (figure 2) of a wafer based on weeks of daily experimenta-tion. SVM stands for support vector machines [16]. HMML stands for a hidden markov model based method with lasso regression in every state [12]. BTM stands for best of the time series methods us-ing SPSS expert modeler. eling characteristics that minimize error. However, the ap-plication environment is dynamic X  X afers enter and leave the manufacturing line and processes and fab performance may change. Directions in fab and model performance may also change, but not on a daily basis. Therefore some overall knowledge about the trends in model performance must be applied. One reasonable strategy is to make major mod-eling decisions in an experimental phase, and then watch trends over time before making major revisions. However, the estimates for individual wafers are critically important for decisions made on a daily basis. Typically, only wafers with the most extreme predictions will be selected for ac-tions. The procedure of Figure 4 is used for our internal estimates. Actual decisions are made about selecting wafers for revision, and the consequences of those decisions are the ultimate evaluation of predictive performance.

All aspects of this automatic machine learning application are influenced by the requirement to deal effectively with a complex manufacturing environment evolving through peri-ods of relative stability and rapid change. The nature of sampling of data and evaluation is essential to any predic-tive analysis. We have seen how these dynamics influence our sampling and evaluation techniques. Next, we exam-ine our approach to learning. We see that once again, our approaches and techniques to learn from training data are influenced strongly by the need to operate in both periods of relative stability and rapid change.
From a machine learning perspective, the objective is to predict the eventual outcome of product testing, PSRO mea-sured at LT. Given a set of real-valued measurements in-cluding the outcome, regression methods are applicable. We could also view the task as classification, when well defined speed thresholds can be specified. Our early experiments demonstrated far better predictive value for regression anal-ysis than classification. Predicting the continuous PSRO provides a natural ordered ranking of the wafers. The most likely candidates for correction are those with the most ex-treme predictions or those outside a specified normal range.
Using the procedure in Figure 4, different learning meth-ods can be compared, and the one with best results selected. This is a standard approach to selecting learning algorithms in a stationary population when predictive performance is the primary goal. However, the fab population is not sta-tionary, and periods of relative stability and periods of rapid change are both anticipated.

To deal with these changes and also based on experimen-tation over many weeks, two methods were combined and used for modeling:
The results of testing several learning methods are shown in table 1. The ensemble learning method which averages the predictions of boosted trees and linear regression per-forms the best overall. The reported results are R 2 values averaged over weeks of experimentation. R 2 is a standard measure in statistics used to evaluate regression algorithms. the mean squared error of a model M on the test set, while mse (  X  t ) denotes the mean squared error of the training-set target mean on the test set. In our case, M would signify the regression functions learned using the different learning methods while  X  t would signify the mean PSRO computed over the training set. Hence, R 2 values closer to 1 imply that M is much superior to  X  t . Negative R 2 values imply that using M is inferior to using the simple prediction of the training set mean, and are highly suggestive of nonstation-arity in the underlying input output relationships.
The classical linear model is a simplified model that as-sumes a fixed representation. In our experiments, it usually performed worse than the forests. However, in nonstation-ary environments, i.e. fab performance is evolving, the linear method could win. The reason is likely tied to its simplified and restricted perspective that does not overfit the data and is more robust.

The forests, numbering in the hundreds of decision trees, are capable of modeling much more complex functions than the single linear regression model. When the population is stable, the forests will perform much better. When fab behavior is evolving, the results can weaken because the fit to the (stable) training data is too tight.
 The predictions of these two methods can be averaged. This is an effective strategy for dealing with evolving fab dynamics. Combining two or more independent methods is known to often give better results [2], [4], [3]. The methods can be evaluated independently and in combination. In our applications, they are retrained on the data every day, so there is ample opportunity to examine which variation is doing better. Besides the purely empirical evaluation, one may have knowledge of the overall performance of the fab. For example, just looking at the trend in mean speed over several weeks can suggest whether the fab performance is stable or not.

Figure 5 is a overview of a procedure for sampling, learn-ing and evaluating the models induced from the current sam-ple of wafer data.
The overall mission is the early identification of wafers or lots that will be unacceptably fast or slow, and the im-plementation of effective countermeasures. The engineering staff recognizes an acceptable range of speeds for each prod-uct. If our predictions were completely accurate, we could 1. Collect sample S1 of wafers with known completed measurements 2. Collect independent sample. S2, for testing. 3. Learn a prediction model from S1 and evaluate on S2. simply report and act on all wafers predicted outside of that acceptable range. We can see in table 1 that predictions are far from completely accurate using data collected prior to step 7, which is the last opportunity to implement down-stream corrective processing.

Analogous to predictive sales applications where lift is plotted, these predictions can be ordered and ranked. Wafers in the extreme tails of the prediction distribution are usu-ally much more likely to be out of range, and of interest in our application. The test data are used to estimate expected deviations from the mean. Given a specific threshold, for ex-ample all wafers predicted above t , overall deviation of the true values from the mean are measured. Additionally mea-sured are deviation in the correct direction and deviation in the negative direction. A measure of accuracy is provided, where a prediction is scored as correct when it is in the same direction as the true answer, i.e. above or below the mean. The results for selected threshold, t , should surpass a min-imum degree of accuracy for both direction and deviation. An effective threshold must provide highly accurate predic-tions and identify wafers with meaningfully large absolute deviations from the desired range.

The selected wafers will undergo corrective processing to increase or decrease their speed. In general we use correc-tive processing strategies designed to adjust wafers slightly, to move wafers from outside a desired range into the range, rather than trying to move the wafers to the center of the range. Assuming a modest increase in speed for a predicted slow wafer, a mistake in prediction could put make it too fast and actually degrade the wafer yield, a costly expense. However, if the increase in speed maintains the wafer X  X  chips within the upper bound, then the expense is minor. Thus, a more detailed analysis of thresholds for prediction is war-ranted to find an interval where prediction is most accurate. In figure 6, procedures for optimizing the thresholds for de-tecting high or low values based on predictions of the model described in the Section 4.

Although we nominally focus on the early detection and correction of aberrant wafers, other applications of our sys-tem require early detection with high accuracy of  X  X ormal X  7. Compute an accuracy rate: wafers, i.e. not fast, not slow. For example, some machine designs can only use chips with relatively tight power per-formance specifications and customized wafer back-end pro-cessing. Any chips tailored in the back end for that design, not ultimately meeting those tight specifications, may be unusable for another build. In such a case, improving the likelihood that chips tailored for that design will meet those tight specifications can reduce yield loss.

The task of early detection of normal wafers is not merely a trivial complement to the prediction with high accuracy of aberrant wafers: The absence of a prediction of aber-rant wafers does not imply a prediction of normal. While the models we have developed for detecting aberrant wafers have high precision, their recall is limited and the applica-tions exploiting those models are relatively forgiving of false negatives. Thus the early detection of normal wafers is a more difficult and complex problem from detecting fast or slow alone.

One approach is to find an interval for an ordered set of wafer speed estimates, where the true normal occurrence rate is very high. Figure 7 describes a procedure for find-ing an interval for normal wafers. In the absence of this application, wafers would be chosen randomly for back end customization, and a base fraction of chips will fail to meet final specs. Thus for this application, we measure success in terms of the reduction of the number of customized chips failing to meet the spec.

Table 2 summarizes characteristics of wafers from several intervals selected by the method of Figure 7. The reduc-tion in loss shows the reduction in the number of failing chips in a given interval from the default random selection of chips, as a fraction of the number of chips failing with ran-dom selection. For interval 1, it X  X  100% -(2.3% / 15.4%) = 79%. We see clearly the expected tradeoff between fraction of the population selected for customization and the likeli-hood of failing to meet final specs. We anticipate use of our system for normal finding applications to address relatively low volume products. Thus the rate of yield loss can be cut dramatically, by relatively modest (relative to required product volumes) reductions in the population of candidate customization wafers.
 Table 2: Sample results for normal wafers. f o and f are fraction of wafers outside PSRO target range and included in the prediction interval respectively. L r is reduction in loss relative to random.
The concepts presented here have been implemented in a fully automated system that predicts the LT PSRO proxy for final chip microprocessor speed. Data for training, testing, and prediction are extracted from the Fab X  X  data warehouse, which is updated within minutes of any newly completed measurement for a wafer. In our current implementation, samples, decision models, and estimates are updated once a day.

A simple evaluation of predictive model performance on test data sets is an inadequate characterization of overall system performance. Rather, below, we describe two com-prehensive evaluations. Retrospectively using complete his-torical data, we performed a complete simulation of daily resampling, model building and testing. In a smaller, more expensive prospective study, we performed true real-world testing in a manner similar to evaluating the efficacy of a drug versus a placebo. In both studies, the application is for remedial action to a wafer prior to the landmark step. Retrospective Study: In Figure 2, the decision to hold a wafer and commit to corrective downstream processing must be made by the landmark step 7 (LS7). Thus the system will compute predictions using only those measurements col-lected prior to that landmark. Using data from all the wafers that were completed through LT during a two month period, we examined the daily estimation process for each wafer just prior to LS7. Twenty-four lots of approximately 25 wafers were completed during this time period. Of those 24 lots, 3 lots were predicted to be substantially fast and 3 substan-9. Choose the best accuracy such that a minimum of k wafers are covered. tially slow. All 6 of the identified lots had average speed offsets in the predicted direction which is evidence of opera-tionally high accuracy, especially given the potential impact of downstream processes of uncertain impact and stability.
Table 3 is a summary of statistical results from a single day X  X  model of the line. Two independent test set samples were examined using different thresholds as described above. We see that roughly 90% of the wafers predicted to be slow in both test sets were actually slower than average, a highly operationally accurate result. We also see the anticipated tradeoff between the number of wafers exceeding a predicted speed threshold and the accuracy of those predictions, al-though relatively large reductions in the numbers of wafers identified are required for relatively small improvements in accuracy. This model was then applied to (mystery) wafers outside of the train and test sets. The 94% accuracy of the predictions on the mystery wafers was similar to that on the test wafers. Deviations from the mean were larger for the mystery set than the test set. The extent of deviation from the mean is a critical factor in determining whether corrective processing is warranted. In this system, learning and optimizing methods are tailored to identify wafers with extreme deviations, however no explicit controls are intro-duced to assure any minimum absolute deviations.
 Real Time Study: In a second, prospective study, we in-tervened directly in the production process to correct nom-inally fast wafers. A quota of 5 lots, about 125 wafers, was allocated for intervention. We would notify an engineer to hold a predicted fast lot prior to LS7, and then the lot would be split. Half of the lot would continue in the regular fash-ion, i.e. with business as usual processing, and half would be processed in a fashion to introduce a small speed reduction.
From a macro-decision perspective, one of the 5 lots is clearly a too-fast lot and is saved and corrected, while the other 4 lots remain in the normal range when modest correc-tions are applied. At the micro-decision level, the accuracy of predictions in this pilot was less than in the retrospective study. 21 of the 32 wafers identified were faster than target. One likely explanation for the reduction in accuracy is the fact that during the prospective study there was on going active experimentation with downstream processes known to influence PSRO.
We have described a fully functioning system that pre-dicts mean wafer speed prior to final testing. Speed serves as a proxy for estimating overall wafer health during man-ufacture. The advantages of accurate prediction are mani-fold including wafer correction and prioritization for differ-ent customers. Although the current implementation does not accurately predict future performance of all wafers, we have shown promising results for identifying some outliers.
Clearly, this is a difficult prediction problem. The mea-surements are sampled in small quantities and the utility of these measurements is uncertain, especially when applied to individual wafer estimation. Processes may evolve over time as described above, and manufacturing tool perfor-mance may evolve over time reflecting a dynamic mix of products in a multi-purpose fab such as IBM X  X  300mm line.
From a modeling perspective, the nonstationary nature of the manufacturing processes along with overwhelming miss-ing data makes for a complex analysis. Despite all these complications, we have shown that estimation significantly beyond chance is feasible and in some cases reasonable pre-dictions can be made at the wafer and lot level.

It is important to note that the strategies employed here could be adapted to other manufacturing environments men-tioned before, that share similar concepts like distinct man-ufacturing steps and recorded intermediate measurements. Products in these other domains also tend to move in groups through the manufacturing steps and hence, the ideas for fill-ing in missing values could be easily applied. The sampling, learning and adjustment of predictions methodologies de-scribed in this paper to choose faulty products also naturally extend to these other domains. In fact, we have already ex-plored such possibilities with the manufacture of consumer products, snack products and pharmaceuticals, with some initial promise.

There are many opportunities for future improvements in the performance of the system. We anticipate improve-ments in accuracy with applications to increasingly stable manufacturing environments, where a fab is dedicated to a particular product, rather than a potpourri of products as is the case with the IBM fab. Another direction that could lead to further enhancement is by improving the qual-ity of measurements, or by increasing the sampling rate of wafer measurements. Data input for learning, testing and prediction in these implementations was aggregated by wafer. Many unit manufacturing processes exhibit signifi-cant across-wafer non-uniformities. In a related but differ-ent problem of monitoring yield, it was reported that some semiconductor yield models show improvements with spa-tially resolved estimates, e.g. by individual chip or by region [10]. Yield monitoring has been a heavily studied problem in semiconductor literature [15, 11, 19, 8], where defect data is the primary driver in estimating yield, usually of mem-ory chips. In our case however, we had only electrical and physical measurements taken early on in the manufacturing process to estimate microprocessor speed. Moreover, we de-scribed an online system which runs daily in the fab and adapts to changing dynamics as opposed to a static yield model.

From a machine learning perspective, models could be incrementally updated as new measurements are recorded. Specialized algorithms would be needed for incremental learn-ing because not only are new wafers incrementally observed, but also older wafers have additional information. Our cur-rent algorithms make a fresh start every day with the lat-est sample and complete batch learning. Those procedures are adequate when the system is not stressed by time con-straints. Both knowledge from chip-making and possibly improved machine learning techniques could produce a new class of methods for estimating chip performance.
 We would like to thank Ronald Logan, Jonathan K. Winslow and Daniel Poindexter from the IBM fab in east Fishkill, for their guidance in the development of the system based on their domain expertise. We would also like to thank Brian White for software support. [1] C. Apte, S. Weiss, and G. Grout. Predicting defects in [2] X. Bao, L. Bergman, and R. Thompson. Stacking [3] R. Bell, J. Bennett, Y. Koren, and C. Volinsky. The [4] S. Dzeroski and B.  X  Zenko. Is combining classifiers with [5] T. Fountain, T. Dietterich, and B. Sudyka. Mining ic [6] R. Goodwin, R. Miller, E. Tuv, A. Borisov, [7] J. Harding, M. Shahbaz, Srinivas, and A. Kusiak. Data [8] H. Hu. Supervised learning models in sort yield [9] K. B. Irani, J. Cheng, U. M. Fayyad, and Z. Qian. [10] D. Krueger, D. Montgomery, and C. Mastrangelo. [11] N. Kumar, K. Kennedy, K. Gildersleeve, R. Abelson, [12] Y. Liu, J. Kalagnanam, and O. Johnsen. Learning [13] H. Melzner. Statistical modeling and analysis of wafer [14] R. Schapire. The strength of weak learnability. Mach. [15] C. Stapper. Fact and fictions in yield modeling. [16] V. Vapnik. Statistical Learning Theory . Wiley &amp; Sons, [17] C. Weber. Yield learning and the sources of [18] S. Weiss, R. Baseman, F. Tipu, and et al. Rule-based [19] C. Yeh, C. Chen, and K. Chen. Validation and
