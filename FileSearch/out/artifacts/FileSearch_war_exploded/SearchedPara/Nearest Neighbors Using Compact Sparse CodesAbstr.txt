 INRIA, LEAR Project-team, Grenoble, France In the past few years, there has been a huge increase in the amount of user generated data in the web. On the one hand, availability of such big data offers tremendous opportuni-ties for creating new applications. On the other hand, orga-nizing such enormous data, so that it is easily and promptly accessible, is a challenge. In this respect, a core compo-nent in several algorithms that work in accessing such big data is that of approximate nearest neighbors (ANN), in which the goal is to retrieve a subset of a dataset that is most similar to a query. A few examples in computer vision are image retrieval ( Fergus et al. , 2009 ), and object recog-nition ( Nister &amp; Stewenius , 2006 ). Often these applica-tions require accessing billions of data points ( J  X  egou et al. , 2011 ), which is difficult without efficient ANN schemes. In this paper, we introduce a new ANN retrieval algorithm based on the idea of sparse data representations . We as-sume that our high dimensional data inhabit a low dimen-sional structure which can be learned in a data-driven way. Towards this end, we use the idea of Dictionary Learn-ing (DL) ( Aharon et al. , 2006 ). DL learns an overcomplete dictionary from the data space such that each data point is k-sparse in this dictionary. The main idea put forth in this paper is to design compact codes, dubbed Sparse ANN codes (SpANN) using these k-sparse vectors. In our new representation, each data vector is encoded as an integer set corresponding to the indices of the support of its sparse code. Our scheme is related to Min-Hash ( Broder , 1997 ) and can be used for fast data access using an inverted in-dex ( Chum et al. , 2007 ; 2008 ). At the same time, sparsity helps to approximately represent data in very low dimen-sions (about one-fourth to one-ninth of the original data size in our experiments), and thus our storage requirement is low.
 Sparsity has been suggested as a promising method for ANN in the recent past ( Cheng et al. , 2012 ; Zepeda et al. , 2010 ; Cherian et al. , 2012 ). A major difficulty affecting the performance of these methods, as well as our scheme pro-posed above, is that the SpANN codes generated by dic-tionaries learned using traditional DL algorithms are of-ten found to be sensitive to small differences in the data points. Recall that DL is a data partitioning technique that partitions dense regions of the data space into multi-ple non-orthogonal subspaces. DL methods (such as K-SVD ( Aharon et al. , 2006 )) do not impose any constraints on the coherence between the dictionary atoms; large co-herence allow neighboring data points to use different ac-tive sets in their sparse codes, resulting in the retrieval o f only near duplicates. We analyze this issue theoretically and propose a novel DL algorithm, dubbed incoherent DL (IDL), that uses additional incoherence constraints betwe en atoms. We show that our new optimization objective can be minimized efficiently. Our experimental results on bench-mark datasets demonstrate that our IDL formulation makes the sparse codes more robust and improve ANN retrieval accuracy rivaling the state of the art.
 To set the context for our later discussions, we will review in the next section, prior literature on the problem of ANN retrieval. Before we proceed, let us first introduce our no-tations.
 Notations: We use I p = { 1 , 2 , , p } , that is, the first p natural numbers. The notation d a , b stands for the angle between two vectors a and b . We will use I n for the n  X  n identity matrix. It is well-known that when the dimensionality of data increases as high as 20, the performance of traditional ANN algorithms such as k-d trees, R-trees, etc. dete-riorates ( Beyer et al. , 1999 ). Of the several schemes to tackle this difficulty, one particularly effective method i s locality sensitive hashing (LSH) ( Indyk &amp; Motwani , 1998 ) that uses specialized hash functions to map the data to compact binary strings. Typically, the hash family se-lected for LSH is independent of the structural proper-ties of the data; knowledge of which might help gen-erate more compact hash codes. To this end, Spec-tral Hashing ( Weiss et al. , 2009 ) suggests minimizing the sum of Hamming distances between pairs of data points weighted by a Gaussian kernel. In ( Liu et al. , 2011 ; Raginsky &amp; Lazebnik , 2009 ), extensions to SH are pro-vided showing better retrieval performance especially for increasing code lengths. More recently, there have been several machine learning approaches to suggested to ad-dress ANN via LSH ( Kulis &amp; Grauman , 2009 ; Liu et al. , 2012 ; Norouzi &amp; Blei , 2011 ; Strecha et al. , 2012 ). Similar to these methods, ours is also a data dependent method that learns a non-uniform tiling of the data space via learning an overcomplete dictionary. In contrast to these methods, which embeds the data into a binary space, we project the data into a set of integers, resulting in two important bene-fits, (i) while the representational power of LSH based tech-niques is 2 m for an m -bit binary code, it can be shown that using our compositional model, the same representational power can be attained for k-sparse codes using a dictio-nary with only 2 O ( m/k ) atoms, and (ii) the actual number of hash bits can be made relatively independent to the length of the active set (assuming k &lt;&lt; m ) by using integer hash-ing techniques such as Bloom filters ( Bloom , 1970 ) without loss in accuracy.
 A different direction in which the ANN problem has been tackled is using vector quantization. In these meth-ods, data points are approximated by their nearest clus-ter centroids, typically learned using k-means on the train -ing data ( Tuytelaars &amp; Schmid , 2007 ; Winder et al. , 2009 ). The main advantage of such methods is that the cen-troids can be precomputed and queried very fast using inverted files. A drawback is that often a large num-ber of centroids are required for high recall. To cir-cumvent this problem, ( Jegou et al. , 2011 ) proposes prod-uct quantization (PQ) in which the dimensions of the data are represented as a Cartesian product of inde-pendent subspaces, each subspace relatively low dimen-sional. In ( Norouzi &amp; Fleet , 2013 ), orthogonal subspaces are learned which are then combined to reconstruct a query point. In ( Babenko &amp; Lempitsky , 2012 ), an enhancement to PQ is suggested by using only two subspaces, but using an efficient algorithm for computing query distances from the subspaces. While, almost all these methods assume that either the centroids or the subspaces are uncorrelated , we allow the basis to be correlated (via overcompleteness), thereby taking advantage of the redundancy between sub-spaces to generate compact codes. Further, we need to store only the sparse coefficients (instead of the centroids) whic h is significantly compressed.
 Building similarity metrics on sparse codes has been investigated several times in the recent past ( Klenk &amp; Heidemann , 2009 ; Cheng et al. , 2012 ). Unfortunately, the adequacy of these metrics for retrieval problems is not thoroughly investigated. Sparse coding for image retrieval has been suggested in ( Yang et al. , 2009 ; Wang et al. , 2010 ; Boix et al. , 2012 ). The goal of these methods is visual understanding, while we target retrieval at the descriptor level. To take an example, ( Boix et al. , 2012 ) proposes a two level binary encoding scheme for image retrieval, but their final representation does not preserve any relation to the euclidean distance between the image descriptors. A sparse coding framework that approximates the inner-product distance between data points for retrieval is presented in ( Zepeda et al. , 2010 ). Their method is hindered by the instability of sparse codes, for which they provide heuristic solutions. More recently, ( Cherian et al. , 2012 ) proposes a hashing frame-work similar to our approach, but requires solving a robust optimization problem for preserving the locality of sparse codes. In ( Zhu et al. , 2013 ), a graph Laplacian, obtained from a training set, is used to enforce locality, followed by canonical correlation analysis for producing stable and generalizable hash codes. In contrast to these methods, our experiments demonstrate that our incoherent dictionary learning formulation implicitly encourages generalizabi lity in a much simpler setting. To set the stage for further discussions, we will review the basics of dictionary learning and sparse coding in this sec-tion. Later, we will establish a connection between the eu-clidean distances in the original data space and the sparse representations. First, let us formally define data sparsity . Definition 1 (k-sparse) . Given a data point y  X  R d , a mapping S : R d  X  R n for n  X  d , and a loss function L : R d  X  R n  X  R , we define S ( y ) to be k-sparse, if where Supp represents the support function and | . | defines the set cardinality. We will denote a k-sparse representa-tion of a data point y as S k ( y ) .
 Let y  X  R d be a data point and given a dictionary B  X  R d  X  n with b i , i  X  I n as its columns such that x = S in B . Then, finding x can be cast as the following opti-mization problem,
S k ( y ) := argmin It is well-known that solving ( 1 ) is NP-hard, but a sub-optimal solution can be obtained using greedy schemes such as orthogonal matching pursuit (OMP) ( Tropp &amp; Gilbert , 2007 ). Although, ( 1 ) can be solved more optimally using its closest convex relax-ation via the  X  1 norm ( Efron et al. , 2004 ), most of these techniques are computationally expensive, which is a primary concern for problems such as ANN.
 In ( 1 ), we assumed that B is known, which is seldom true in general problems. To address this difficulty, DL techniques have been suggested, in which one learns a dictionary from the data itself by solving a variant of ( 1 ), but with B also as an unknown. Given a training dataset v i , i  X  I N , the DL problem is cast as follows: where x j i denotes the j -th dimension of the coefficient vec-tor x i and  X  is a suitable regularization. The atoms are generally constrained to have unit norm to avoid scaling is-sues. Although ( 2 ) is non-convex, it is convex in either B or x  X  X  (thanks to the  X  1 regularization, instead of  X  0 ) and thus can be solved efficiently to a local minimum using block coordinate descent ( Aharon et al. , 2006 ).
 One property of the dictionary that is important to our scheme is coherence , , defined as: Now, we have all the ingredients necessary to establish a connection between ANN retrieval and sparse coding. Theorem 1 (Distances) . Let y 1 , y 2  X  R d be two zero-mean data points, B  X  R d  X  n be a dictionary with coher-ence , and let k y i  X  Bx i k 2 2  X   X  2 i , for k-sparse vectors x i  X  X  1 , 2 } . Then, k y 1  X  y 2 k 2  X   X  1 +  X  2 + Proof. Assuming spherical balls with centers t i = Bx i , with locus y i , and radii  X  , it can be shown that k y 1  X  y 2 k 2  X  max where the last result is obtained from the definition of t  X  X , followed by applying the triangle inequality and the Cauchy-Schwartz inequality. From Gershgorin X  X  theorem, we have k B k 2 2  X  k B T B k  X  which can be upper-bounded by 1 + ( n  X  1) using the definition of . Substituting this upper-bound instead of k B k 2 in ( 6 ), we have the desired result.
 Building on Theorem 1 , we will now introduce our com-pact data representation for NN retrieval. As is clear from our previous discussions, each atom in the dictionary represents a non-orthogonal subspace. Sparse coding seeks a new subspace that is a sparse linear combi-nation of the atoms that hosts a given data vector. If the data vector is exactly k-sparse in B , then there is a unique com-bination of k subspaces from the dictionary for this data vector as formally stated in the following proposition whic h can be proved directly by invoking a contradiction. Proposition 1. If y = j -th atom in a basis active set A and if &lt; 1 , then OMP will find this active set uniquely.
 This implies that the k atoms can be used as representatives for a subset of the data, thus producing a data partitioning as shown in Figure 1 . Our main idea is to build compact ANN codes using these atoms, dubbed Sparse ANN codes (SpANN).
 Definition 2 (SpANN codes) . Suppose for a data point y , let the k-sparse code be S k ( y ) . Then, the SpANN code SpANN k ( y ) is defined as the set of k indices correspond-ing to the support of S k ( y ) .
 To make our representation clear, let us take a simple ex-ample.
 Example 1. Assume y  X  R 100 and let x = S 3 ( y ) where x  X  R 1000 . Let Supp( x ) = { x 25 , x 49 , x 972 } corre-spond to the non-zero dimensions in x . Then, we have SpANN 3 ( y ) = { 25 , 49 , 972 } .
 There exists several efficient methods for encoding integer sets into binary bits (such as Bloom filters ( Bloom , 1970 )), enabling efficient access using an inverted file. A draw-back our representation is that SpANN codes are invariant to data scaling, as formally stated below.
 Proposition 2. For y  X  R d , SpANN( y ) = SpANN( w y ) , for w  X  R \{ 0 } .
 However, by storing the sparse codes associated with all the data points having the same SpANN code in an in-verted file bucket, the ANN can be found using a linear scan on these sparse codes using Theorem 1 . As observed in ( Gordo &amp; Perronnin , 2011 ), using an asymmetric dis-tance in which the query point is not sparse coded, is em-pirically seen to provide better retrieval performance. It is assumed that each such inverted file bucket will contain only a few data points mapped into it. Note that, we need to store only the sparse codes instead of the original data points, thus enabling compressed storage.
 The following theorem establishes a connection between the SpANN codes and the angle between the corresponding data vectors. Since our codes are invariant to data scaling, we assume the data vectors are unit normalized.
 Theorem 2. For any two zero-mean unit-norm data vec-tors y 1 and y 2 , let x i = S k ( y i ) and T i = SpANN k for i  X  { 1 , 2 } . Suppose k y 1  X  y 2 k 2  X   X  1 +  X  2 where k y i  X  Bx i k 2  X   X  i . Then, for a probability defined on the volume of overlap of two hypershperes with centroids y i , locus t i = Bx i , and radii  X  , P rob ( | T 1  X  T 2 | = k ) mono-tonically increases with decreasing \ y 1 , y 2 . Proof. The proof is a direct extension of ( Gromov , 1987 )[Theorem 1.A].
 Theorem 2 says that as the angle between the data points decreases, there is an increasing probability that their SpANN codes match exactly. From another perspective, the theorem suggests that by using | T 1  X  T 2 | X   X  , for some  X  &lt; k  X  X hat is, to look at neighboring subspaces of y 1 overlaps with at least  X  subspaces of y 2  X  will have a non-zero probability of finding the ANN. Using this intuition, we will rank the SpANN codes of the database points ac-cording to their basis overlap with the SpANN codes of a query point, and will explore the inverted file buckets of only those codes that overlaps by more than  X  indices. To-wards this end, we can in fact use the more standard Jac-card distance ( Jaccard , 1901 ), given by | T 1  X  T 2 | / | T for comparing the SpANN codes, for which efficient hash-ing schemes exist for fast lookup ( Chum &amp; Matas , 2010 ). Algorithm 1 summarizes the steps involved for populating the inverted file and query retrieval using SpANN codes. Algorithm 1 SpANN Indexing and Retrieval 1: Input: B , Y , k ,  X  , H { an inverted index } 2: Output: y  X  3: for all y i  X  Y do 4: Compute x i = S k ( y i ) and T i = SpANN k ( y i ) . 5: T  X  X   X  X  T i } 6: H ( T i )  X  H ( T i )  X  x i { assign a location to x i 7: end for 8: Given a query q , compute T q = SpANN k ( q ) , 9:  X  T  X  Jaccard( T q , h )  X   X ,  X  h  X  X  11: y  X  associated with x  X  .
 The next theorem connects the subspace overlap with the coherence of the dictionary.
 Theorem 3. Let y 1 , y 2 be two zero-mean unit norm data points and let B be a dictionary with coherence . Let T i SpANN k ( y i ) for i  X  X  2 . If T 1 6 = T 2 , then there exists a k 1  X  k  X  &lt; k and T  X  i = SpANN k  X  ( y i ) , such that T and only if there exists b j for which | y T b j | &gt; Proof. Let cos X  = so that the if part: suppose  X  b i such that | b T i y | &gt; then [ b i , y &lt;  X  2 . This means b i is the most correlated dic-tionary atom to y and thus will be selected by OMP in the first step. If not, there must exist another basis b j such that this condition is satisfied. In such a case, b T i b which contradicts the condition that the coherence of the dictionary is . To prove the only if part: assume  X  k  X  : 1  X  k &lt; k such that T 1 = T 2 = T = { i 1 , i 2 , , i k  X  } . With-out loss of generality, let b i to y and thus will be selected by OMP in its first step, from which the result follows.
 Theorem 3 shows that the sensitivity of SpANN codes is related to the coherence of the dictionary. Lower coher-ence implies larger apex angles for the atoms. As a result, their chance to be included in active sets of neighboring data points is higher. Traditional DL schemes do not allow any control on the coherence (empirically, they are seen to cohere by more than 90% for the data used in our ex-periments). This motivates us to build a novel formulation of ( 2 ) that incorporates incoherence constraints. The problem of learning incoherent dictionaries has been dealt with through heuristic approxima-tions ( Yaghoobi et al. , 2010 ) using parametric models, or in a theoretical setting with stochastic assumptions on the dictionary ( Arora et al. , 2013 ). Another class of methods incorporate additional incoherence regularizations of th e form k B T B  X  I k 2 F into the DL objective ( Lin et al. , 2012 ; Ramirez et al. , 2010 ). However, these methods have two shortcomings, namely (i) they do not offer any explicit control over coherence and (ii) empirically, it is often found to be difficult to reduce the coherence to arbitrarily low values. To circumvent these problems, in this paper, we propose a novel DL formulation that incorporates inco-herence explicitly. Before showcasing our formulation, we review a result on the minimum coherence achievable by any overcomplete dictionary.
 Proposition 3. For any dictionary B  X  R d  X  n such that n &gt; d , assuming d = dim ( span ( B )) , the minimum coher-ence min is given by: Proof. See ( Benedetto &amp; Kolesar , 2006 )[Theorem IV.2]. Now, let us reformulate ( 2 ) with constraints on the maxi-mum coherence between the atoms: where B  X  k is the dictionary B with the k -th atom removed, and  X  is a constant; min  X   X   X  1 and controls the allowed dictionary coherence. Recall that k v k  X  corresponds to the maximum of the absolute values of entries in an input vector v . Compared to the traditional DL formulation, the only non-trivial part in ( 8 ) is to solve for B which we con-sider in details next.
 We can rewrite the DL part of ( 8 ) as: As is clear, this DL problem is non-convex, but interest-ingly, is convex in each dictionary atom while keeping the rest of the atoms fixed. This suggests an additional block coordinate descent scheme for each atom separately. The subproblem for solving the atom b k has the following form: With a slight abuse of notation to avoid writing too many refer the dictionary atom under consideration by dropping the subscript k , then we concisely rewrite ( 10 ) as: where the scalar z i = x k i . Further, let  X  y i = y i  X  taking the Lagrangian of ( 11 ) using dual vectors  X  1 ,  X  R n  X  0 , followed by setting its derivative w.r.t. b to zero, we have b in terms of the dual variables as: b = Substituting b in the Lagrangian, we have the following dual for ( 11 ): min where, a = [ I n ,  X  I n ] , and  X  = [  X  T 1 ,  X  T 2 ] T . Introducing A = we can rewrite ( 13 ) more concisely as: The form in ( 14 ) is a convex  X  1 regularized non-negative least squares problem and can be solved efficiently using standard toolboxes ( Kim et al. , 2012 ). The various steps of the IDL algorithm are summarized in Algorithm 2 . Due to the coupling of the dictionary atoms with each other through the incoherence constraints, the dictionary learn ing sub-problem is non-convex and thus convergence to a local minima ( 8 ) is not guaranteed (similar to other approaches such as ( Ramirez et al. , 2010 )). Empirically, it is seen to converge in less than 20 iterations for the inner loop and in about 100 iterations for the outer loop in all our experimen-tal datasets.
 Algorithm 2 IDL Algorithm 1: Input: Y , k = 0 , B k { initial dictionary } 2: Output: B out 3: repeat 4: X  X  min x i k y i  X  B k x i k 2 2 +  X  k x i k 1 ,  X  i  X  X  5: p  X  0 ,  X  B p = B k 6: repeat 7: for all j  X  X  n do 8: b j  X  from ( 14 ) and ( 12 ) 10: end for 11: p  X  p + 1 12: until k  X  B p  X   X  B p  X  1 k F  X   X  14: k  X  k + 1 15: until convergence In this section, we will first introduce our evaluation datasets and benchmark metrics, preceding which, we will be furnishing our results comparing against several state-of-the-art ANN methods.
 Datasets and Evaluation Metric: Our experiments are mainly based on the evaluation protocol of ( Jegou et al. , 2011 ) using two publicly available ANN datasets: (i) 1M SIFT and (ii) 1M GIST descriptors. The first dataset is split into a training set with 100K 128-dimensional SIFT descriptors, a base set of 1M descriptors to be queried, and 10K query descriptors. Of the 100K training set, we use a random sample of 90K descriptors for learn-ing the dictionary and 10K for validation. The GIST dataset consists of 960-dimensional descriptors and a trai n-ing, database, and query step split of 500K, 1M, and 1K respectively. Of the training set, we use 400K descriptors for DL and 100K for validation. We also use the 1B BI-GANN dataset ( J  X  egou et al. , 2011 ) consisting of one billion SIFT descriptors, which we use to evaluate our query time performance. We will use Recall@K for evaluating our al-gorithms (as in ( Jegou et al. , 2011 )), which is defined as the proportion of the queries for which the ground truth neigh-bor is found in the first K retrieved points. For K = 1 , this measure corresponds to the standard precision measure. There are two parameters in our ANN scheme, namely (i) the number of dictionary atoms, and (ii) the dictionary in-coherence, which are important for the performance of our method. In the following, we empirically investigate the sensitivity of these parameters against ANN accuracy. We use the SIFT validation set for these experiments, and use separate 1K descriptors from the validation set as queries. For all the experiments, we used a fixed Jaccard threshold of  X  = 0 . 33 .
 Evaluation of IDL: In Figure 2 , we plot the Recall@1 against an increasing dictionary size and varying maximum coherences with IDL. The plot shows the accuracy for dic-tionary sizes 256, 512, and 1024, and maximum coher-ence ranging from 0.2 to 0.8 at steps of 0.1. As is clear from the plot, increasing coherence results in lower ANN accuracy. We found = 0 . 2 gave the best performance for dictionaries of sizes 256 and 512, while = 0 . 3 per-formed best for 1024 atoms. We use these coherences for the respective dictionaries in the experiments to follow. F or highly overcomplete dictionaries (such as for SIFT with 1024 atoms), the IDL algorithm was found to converge slowly for incoherences closer to the theoretical minimum. This is not unexpected as finding optimal basis directions respecting the incoherences in such settings is difficult. W e also found that for alternative incoherence models such as ( Ramirez et al. , 2010 ), the incoherence could not be de-creased below 0.6 (for SIFT descriptors) even for large reg-ularizations.
 Increasing Active Set Size: Figure 3 plots the Recall@1 for increasing active set size and varying dictionary sizes on our SIFT validation set. If n is the number of atoms in a dictionary, then we use  X  log 2 ( n )  X  bits to encode each inte-ger in the active set to access the inverted file table. For all data points having the same active set, we store M floating point sparse active coefficients for each point, in the disk for the linear scan (note that we do not need to store the original descriptors). In the figure, we plot the recall for M = 16 and M = 32 corresponding to the largest M co-efficients in the sparse codes for the database descriptors. As expected, the plot shows that the recall increases with an increasing SpANN code length and larger values of M . We use M = 32 for our SIFT experiments and M = 64 for GIST.
 Recall against K for SIFT: We now present compar-isons against the state-of-the-art methods. We compare our method to (i) Spectral Hashing (SH) ( Weiss et al. , 2009 ), (ii) Product Quantization using 16 cells (PQ-IVFADC@16) ( Jegou et al. , 2011 ), (iii) PQ with asym-metric linear scan (PQ-ADC), (iv) Shift Invariant Kernel Hashing (SIKH) ( Raginsky &amp; Lazebnik , 2009 ), (v) Itera-tive Quantization (ITQ) ( Gong &amp; Lazebnik , 2011 ), and (vi) Cartesian K-Means (CKMeans-AD). The results for these methods are taken from their respective publications (ex-cept SIKH, which is our implementation). Figure 4(a) compares the Recall@K of SpANN codes generated us-ing 256 atoms against these methods. All the methods (in-cluding ours) used 64 bits for hashing. As is clear from the plot, SpANN codes outperform the next best method (PQ-IVFADC@16) by about 12% at Recall@1, while per-forming competitively for higher values of K . Note that, methods such as PQ-ADC (that show better Recall@1000) uses an exhaustive search on the entire dataset to achieve high recall, while our method uses only about 2% (20,061 out of 1M points) to achieve 83.1% recall, which we be-lieve is a significant gain. In Figure 4(b) , we compare the Recall@K against other sparse ANN methods such as (i) ( Cherian et al. , 2012 ), (ii) using a dictionary learned with the classical DL formulation, and (iii) using an inco-herent dictionary learned using ( Ramirez et al. , 2010 ). All these other methods used 256-atom dictionaries and 64-bit codes. We also compare to other sizes of the dictionaries, demonstrating performance significantly better than other sparse ANN methods.
 Recall against K for GIST: In Figure 4(c) , we show the Recall@K performance for 960D GIST descriptors. Through the validation set, we found a dictionary of size 1024 atoms demonstrated an optimum trade off between sparse coding time and recall accuracy. In the figure, we plot the Recall@K for K varying from 1 to 1000, and com-pare it with other state-of-the-art methods listed above. W e also show the retrieval performance of (i) coherent dictio-nary (DL) learned using the classical DL, and (ii) incoher-ent DL learned using ( Ramirez et al. , 2010 ). All the meth-ods used 64-bit codes, while the DL methods used the same number of atoms. We find that our SpANN codes outper-form the next best method (PQ-IVFADC@8) by about 9% at Recall@1 and by about 21% at Recall@100. We point out that the performance of classical DL was found to be poor (recall@K less than 4%) for all K on this dataset. Plots for all comparison methods are taken from their published works.
 Retrieval Time: We used the BIGANN dataset for this experiment which consists of one-billion SIFT descriptors . For improving query performance on this huge dataset, we initially used K-Means with 1K centroids to partition the dataset, followed by using separate hash tables for each partition to index the SpANN codes. The codes are bi-nary encoded using Bloom filters so that computing Jaccard overlap is reduced to finding Hamming distances. While querying, we first find the K-Means centroid nearest to a given query, later querying the inverted indices in the cor-responding data partition. For K-NN retrieval, we might need to look at multiple such partitions. To further improve the efficiency, we used a query expansion scheme loosely based on the method of ( Chum &amp; Matas , 2010 ) in which we query indices in the order of their probability of being in the active set (found using the training set). Our timing comparisons are based on a single core 2.7 GHz AMD processor with 32GB memory. We used the SPAMS toolbox ( Mairal et al. , 2010 ) for sparse coding. It took a few microseconds on average to sparse code each SIFT descriptor for all our dictionary sizes, while it took less than a millisecond for each GIST descriptor. In Figure 5 , we show the average query time for SpANN codes (  X  64 bits) when the database size increases from 1M to 1000M. Our implementation was primarily in MATLAB with op-timized C++ routines for accessing the inverted file table. The plot shows that our method offers competitive query time when compared to results reported in earlier works such as ( Jegou et al. , 2011 ) on the same dataset. In Table 1 , we show the average number of data points accessed from the inverted table for the two datasets. This paper introduced SpANN codes for efficient approxi-mate nearest neighbors using sparse coding. After explor-ing several theoretical properties of ANN using SpANN codes, we showed that their robustness is tied to the inco-herence of the sparse coding dictionary. Using this obser-vation, we proposed a novel incoherent dictionary learning formulation and an efficient method to solve it. Our exper-iments demonstrated the adequacy of our scheme for effi-cient ANN retrieval. An issue that remains to be addressed is when data is very sparsely distributed in space; as a re-sult, there might not be sufficient overlap between SpANN codes of nearby data points. One way to tackle this prob-lem is to use hierarchical dictionary learning, investigat ions into such a scheme is an interesting future work. The author would like to thank Dr. Julien Mairal, Dr. Vas-silios Morellas, and several anonymous reviewers for use-ful feedback.
 Aharon, M., Elad, M., and Bruckstein, A. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. TSP , 54(11):4311 X 4322, 2006. Arora, S., Ge, R., and Moitra, A. New algorithms for learning incoherent and overcomplete dictionaries. arXiv preprint arXiv:1308.6273 , 2013.
 Babenko, A. and Lempitsky, V. The inverted multi-index. In CVPR , 2012.
 Benedetto, J. J. and Kolesar, J. D. Geometric properties of grassmannian frames for r 2 and r 3. Journal on Ad-vances in Signal Processing , 2006, 2006.
 Beyer, K., Goldstein, J., Ramakrishnan, R., and Shaft, U.
When is nearest neighbor meaningful? Database The-ory , pp. 217 X 235, 1999.
 Bloom, B. H. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM , 13(7): 422 X 426, 1970.
 Boix, X., Roig, G., Leistner, C., and Van Gool, L. Nested sparse quantization for efficient feature coding. In ECCV , pp. 744 X 758. Springer, 2012.
 Broder, A. Z. On the resemblance and containment of doc-uments. In Compression and Complexity of Sequences . IEEE, 1997.
 Cheng, H., Liu, Z., Hou, L., and Yang, J. Sparsity induced similarity measure and its applications. In TCSVT , 2012. Cherian, A., Morellas, V., and Papanikolopoulos, N. Ro-bust sparse hashing. In ICIP . IEEE, 2012.
 Chum, O. and Matas, J. Large-scale discovery of spatially related images. PAMI , 32(2):371 X 377, 2010.
 Chum, O., Philbin, J., Isard, M., and Zisserman, A. Scal-able near identical image and shot detection. In CIVR . ACM, 2007.
 Chum, O., Philbin, J., and Zisserman, A. Near duplicate image detection: Min-Hash and TF-IDF weighting. In BMVC , 2008.
 Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. Lea st angle regression. Annals of Statistics , 32(2):407 X 451, 2004.
 Fergus, R., Weiss, Y., and Torralba, A. Semi-supervised learning in gigantic image collections. In NIPS , 2009. Gong, Y. and Lazebnik, S. Iterative quantization: A pro-crustean approach to learning binary codes. In CVPR , 2011.
 Gordo, A. and Perronnin, F. Asymmetric distances for bi-nary embeddings. In CVPR , 2011.
 Gromov, M. Monotonicity of the volume of intersection of balls. Geometrical Aspects of Functional Analysis , pp. 1 X 4, 1987.
 Indyk, P. and Motwani, R. Approximate nearest neighbors: towards removing the curse of dimensionality. Theory of Computing , pp. 604 X 613, 1998.
 Jaccard, P. Etude comparative de la distribution florale dan s une portion des Alpes et du Jura. Bulletin de la Socit vaudoise des Sciences Naturelles , 37:547 X 579, 1901. Jegou, H, Douze, M, and Schmid, C. Product quantiza-tion for nearest neighbor search. PAMI , 33(1):117 X 128, 2011.
 J  X  egou, H., Tavenard, R., Douze, M., and Amsaleg, L.
Searching in one billion vectors: re-rank with source coding. In ICASSP , pp. 861 X 864, 2011.
 Kim, D., Sra, S., and Dhillon, I. S. A non-monotonic method for large-scale non-negative least squares. Opti-mization Methods and Software , 2012.
 Klenk, S. and Heidemann, G. A sparse coding based simi-larity measure. In DMIN , 2009.
 Kulis, B. and Grauman, K. Kernelized locality-sensitive hashing for scalable image search. In CVPR , 2009. Lin, T., Liu, S., and Zha, H. Incoherent dictionary learning for sparse representation. In ICPR , 2012.
 Liu, W., Wang, J., Kumar, S., and Chang, S. Hashing with graphs. In ICML , 2011.
 Liu, W., Wang, J., Ji, R., Jiang, Y., and Chang, S. Super-vised hashing with kernels. In CVPR , 2012.
 Mairal, J., Bach, F., Ponce, J., and Sapiro, G. Online learn-ing for matrix factorization and sparse coding. JMLR , 11:19 X 60, 2010.
 Nister, D. and Stewenius, H. Scalable recognition with a vocabulary tree. In CVPR , 2006.
 Norouzi, M. and Fleet, D. J. Cartesian k-means. In CVPR , 2013.
 Norouzi, Mohammad and Blei, David M. Minimal loss hashing for compact binary codes. In ICML , 2011. Raginsky, M. and Lazebnik, S. Locality-sensitive binary codes from shift-invariant kernels. In NIPS , 2009. Ramirez, I., Sprechmann, P., and Sapiro, G. Classification and clustering via dictionary learning with structured in-coherence and shared features. In CVPR , 2010.
 Strecha, C., Bronstein, A., Bronstein, M., and Fua, P. LDAHash: Improved matching with smaller descriptors. PAMI , 34(1):66 X 78, 2012.
 Tropp, J. and Gilbert, A. Signal recovery from random measurements via orthogonal matching pursuit. TIT , 53 (12):4655 X 4666, 2007.
 Tuytelaars, T. and Schmid, C. Vector quantizing feature space with a regular lattice. In ICCV , 2007.
 Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., and Gong, Y.
Locality-constrained linear coding for image classifica-tion. In CVPR , 2010.
 Weiss, Y., Torralba, A., and Fergus, R. Spectral hashing. In NIPS , 2009.
 Winder, S., Hua, G., and Brown, M. Picking the best daisy. In CVPR , 2009.
 Yaghoobi, M., Daudet, L., and Davies, M. Structured and incoherent parametric dictionary design. In ICASSP , 2010.
 Yang, J., Yu, K., Gong, Y., and Huang, T. Linear spatial pyramid matching using sparse coding for image classi-fication. In CVPR . IEEE, 2009.
 Zepeda, J., Kijak, E., and Guillemot, C. Approximate near-est neighbors using sparse representations. In ICASSP , 2010.
 Zhu, X., Huang, Z., Cheng, H., Cui, J., and Shen, H. Sparse hashing for fast multimedia search. ACM TIS , 31(2):9,
