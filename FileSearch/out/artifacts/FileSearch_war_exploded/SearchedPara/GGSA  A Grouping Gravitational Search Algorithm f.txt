 1. Introduction
Data clustering is one of the most important and popular data analysis techniques, and refers to the process of grouping a set of data objects into clusters, in which the data of a cluster must have great similarity and the data of different clusters must have high dissimilarity ( Barbakh et al., 2009; Jain, 2010; Berikov, 2014 ).
Clustering has been applied in a wide variety of fi elds such as machine learning, pattern recognition, web mining, and image segmentation ( Everitt et al., 2001 ).

There are many clustering algorithms in the literature. Tradi-tional classi fi cations of clustering algorithms primarily distinguish between hierarchical and partitional ( Everitt et al., 2001; Xu and
Wunsch, 2005 ). Hierarchical clustering algorithms recursively clusters either in an agglomerative (bottom-up) mode or in a divisive (top-down) mode. Agglomerative methods start with each data object in a separate cluster and successively merge the most similar pairs until termination criteria are satis fi ed. Divisive methods start with all the data objects in one cluster and repeatedly divide each cluster into smaller clusters, also until termination criteria are satis fi ed. On the other hand, partitional clustering algorithms that we concerned in this paper, attempt to fi nd all the clusters simultaneously without forming a hierarchical structure. In fact, partitional clustering algorithms initially obtain a set of disjoint clusters and progressively re fi ne it to minimize a prede fi ned criterion function. Their objective is to maximize the intra-cluster connectivity or compactness while minimizing inter-cluster connectivity or separability ( Everitt et al., 2001 ; Xu and Wunsch, 2005 ).

Besides traditional classi fi cations of clustering algorithms, in Chang et al. (2009) , four major types of clustering algorithms based on the clustering criterion adopted by the algorithm are identi fi algorithms based on the idea that neighbor data should share the same cluster. Classical clustering algorithms such as density-based methods ( Ankerst et al., 1999; Ester et al., 1996 ), nearest neighbor methods ( Lu and Fu, 1978 ) and methods like single link agglomerative clustering ( Vorhees, 1985 )belongtothis fi rst group. As pointed out in of arbitrary shapes, but they are not robust when there is small spatial separation between the clusters. The second set of clustering algo-rithms is formed by those approaches which consider intra-clusters variation (intra-clusters points or centers) to form the fi
This category includes algorithms like k -means ( MacQueen, 1967 ), average link agglomerative clustering ( Vorhees, 1985 ), learning-network clustering ( Brugger et al., 2008 ), and model-based clustering approaches ( Mclachlan and Basford, 1988 ). As pointed out in Chang well-separated clusters, but they may fail for more complicated cluster structures. The third category includes a simultaneous row-column clustering known as bi-clustering algorithms ( Madeira and Oliveira, 2004 ). Finally, the fourth group of clustering algorithms includes approaches that optimize different characteristics of the dataset. This 2006 ) and also clustering ensembles approaches ( Hong et al., 2008 ).
From an optimization perspective, clustering problem can be formally considered as a particular kind of NP-hard grouping problem ( Falkenauer, 1998 ). This has stimulated the search for ef fi cient approximate algorithms, including not only the use of heuristics for particular classes of problems, but also the use of general-purpose metaheuristics which solve problems that are believed to be hard by exploring the large solution space and achieve this goal by effectively reducing the size of solution space and exploiting the reduced space ef fi ciently. This class of algo-rithms includes, but is not restricted to, Ant Colony Optimization (ACO) ( Dorigo, 1992 ), Evolutionary Computation (EC) such as
Genetic Algorithms (GAs) ( Holland, 1962; Holland, 1975; Elsayed et al., 2014 ), Particle Swarm Optimization (PSO) ( Kennedy and
Eberhart, 1999; Mazhoud et al., 2013 ), Greedy Randomized Adap-tive Search Procedure (GRASP) ( Feo and Resende, 1989 ), Iterated
Local Search (ILS) ( Stutzle, 1999 ), Variable Neighborhood Search (VNS) ( Mladenovic and Hansen, 1997 ), Simulated Annealing (SA) ( Kirkpatrick et al., 1983 ), and Tabu Search (TS) ( Glover and Laguna, 1997 ).
 Recently, a stochastic population-based metaheuristic, called
Gravitational Search Algorithm (GSA), which is motivated by the laws of gravity and motion has been proposed ( Rashedi et al., 2009; Rafsanjani and Dowlatshahi, 2012 ). Originally, GSA was designed for solving continuous optimization problems and like most metaheuristics, has a fl exible and well-balanced mechanism for enhancing exploration and exploitation abilities. The search strategy of GSA is to move the members of population towards the
K best solutions of population using the laws of gravity and motion, where K is a number between one and the size of the population.

Motivated by the success of the GSA with variant optimization problems ( Rashedi et al., 2011; Han and Chang, 2012; Pal et al., 2013; Dowlatshahi et al., 2014 ), this paper proposes a grouping version of GSA (GGSA) to data clustering. Proposed GGSA differs from the standard GSA in two important aspects. First, a special encoding scheme, called grouping encoding, is used in order to make the relevant structures of clustering problems become parts of solutions. Second, given the encoding, special GSA updating equations are used, suitable for the solutions with grouping encoding. To evaluate the performance of the GGSA to clustering problems, 13 real datasets from the UCI database ( Blake and Merz, 1998 ), which is a well-known database repository, are selected. For a given dataset with D clusters, the GGSA attempts to fi nd the D cluster centers by randomly selecting 75% of instances of the given dataset, which this 75% is called the training set of GGSA. Then, in order to evaluate the performance of the GGSA, the remaining 25% of dataset, which is called the test set, is used to obtain Classi tion Error Percentage (CEP) which is de fi ned as the ratio of number of misclassi fi ed samples in the test dataset and total number of samples in the test dataset. The result obtained by
GGSA on data clustering is compared with the results of several metaheuristics and a wide set of well-known classical classi tion techniques that are also given in De Falco et al. (2007) .
The rest of the paper is organized as follows. In Section 2 , the cluster analysis problem is discussed. Section 3 provides a review of standard GSA. In Section 4 , the proposed GGSA for clustering problems is presented. Section 5 contains the experimental part of the paper, in which the performance of the proposed approach is evaluated. Finally, in Section 6 conclusion is given. 2. The data clustering problem
As brie fl y mentioned, data clustering is the process of grouping a set of data objects into clusters. Mathematically, a data clustering problem is de fi ned as follows. Let O  X  { O 1 , ... , O n data objects (vectors) in a given feature space S . The objective of a data clustering problem is to fi nd an optimal partition of objects O , C  X  { C 1 , ... , C D }, O  X [ D i  X  1 C i , and C i \ C j stands for the i th cluster of partition C , in such a way that the data objects belonging to the same cluster are similar whereas data objects belonging to different clusters are as different as possible, in terms of a distance measurement function.

The measure of distances is one of the key elements when dealing with data clustering problems, since usually the similarity between two different data objects O i and O j is related to a measure of distance in the feature space S . To calculate the distance in clustering problems, different distance functions have been provided. And the most famous one is the Euclidean distance.
Among the objective functions that used in cluster analysis for measuring the quality of resulting clusters, the famous and widely used function is the sum of quadratic error which considers cohesion of clusters in order to evaluate the quality of a given partition data, and de fi ned as follows: f  X  O ; C  X  X   X  D where D is the number of clusters and jj O j Z i jj 2 is the Euclidean distance between a data object O j A C i and the center of cluster i , represented by symbol Z i , to be found by Eq. (2) : Z  X  1 j C objects in the cluster i .

The data clustering process can be categorized into two classes: unsupervised clustering and supervised clustering. In unsuper-vised clustering which can also be named automatic clustering, the training data does not need to specify the number of clusters. However, in supervised clustering the training data does have to specify what to be learned; the number of clusters. The datasets that we tackled in this paper contain the information of clusters. Therefore, the optimization goal is to fi nd the centers of the D clusters by minimizing the objective function, i.e. the sum of distances of the data objects to their cluster centers. In this paper, given by Eq. (3) ,asin( De Falco et al., 2007 ): fit  X  O Train ; C  X  X  1 where D is the number of clusters, O Train is the set of instances which compose the training set, | O Train | is the number of instances of training set, and O Train j is an instance with index j in the training set. 3. The GSA technique
The standard GSA introduced by Rashedi et al. (2009) is inspired by the Newtonian laws of gravity and motion. This swarm optimization technique provides an iterative method that simu-lates object interactions, and moves through a multi-dimensional search space under the in fl uence of gravitation. The effectiveness of the GSA in solving a set of nonlinear benchmark functions has been proven in Rashedi et al. (2009) .

In the basic model of the GSA, which has been originally designed to solve continuous optimization problems, a set of objects (agents) is introduced in the D -dimensional solution space of the problem to fi nd the optimum solution. The position of each agent in GSA demonstrates a candidate solution to the problem; hence each agent is represented by the vector X i in the solution space of the problem. Agents with a higher performance get a greater mass value, because a heavy agent has a large effective attraction radius and hence a great intensity of the attraction.
During the lifetime of GSA, each agent successively adjusts its position X i toward the positions of K best agents of the population.
To describe GSA in more detail, consider a D -dimensional space with s searcher agents in which the position of the i th agent is de fi ned as follows:
X  X  X  x 1 i ; ... ; x d i ; ... ; x D i  X  ; i  X  1 ; 2 ; ... ; where x d i presents the position of the i th agent in the d th dimen-sion. Based on Rashedi et al. (2009) , the mass value of the i th agent is calculated after computing the current population's fi follows: q  X  t  X  X  fit i  X  t  X  worst  X  t  X  best  X  t  X  worst  X  t  X  ;  X  5  X 
M  X  t  X  X  q i  X  t  X   X  s where M i ( t ) and fi t i ( t ) represent the mass value and the are de fi ned as follows for a minimization problem: best  X  t  X  X  Min worst  X  t  X  X  Max
The acceleration of the i th agent by using law of motion is calculated as: a  X  t  X  X  G  X  t  X   X  where: rand j is a uniformly distributed random number in the interval [0,1],
R ( t ) is the Euclidean distance between two agents i and j in a
D -dimensional Euclidean space, 1  X  is a very small value used in order to escape from division by zero error whenever the Euclidean distance between two agents i and j is equal to zero,
Kbest is the set of fi rst K agents with the best fi tness value and biggest mass value, in which K is a function of time, initialized to K initial value at the beginning of the algorithm and its value is decreased with time, and
G initial , and it will be reduced with time toward a fi nal value,
G end , by Eq. (10) : G  X  t  X  X  G  X  G initial ; G end ; t  X  :  X  10  X 
Then, the next velocity of the i th agent is calculated as a fraction of its current velocity added to its acceleration by Eq. (11) and the next position of the i th agent can be calculated using
Eq. (12) : v  X  t  X  1  X  X  rand v d i  X  t  X  X  a d i  X  t  X  X  rand x  X  t  X  1  X  X  x d i  X  t  X  X  v d i  X  t  X  1  X  ;  X  12  X  where rand is a uniformly distributed random number in the interval [0,1].
 The pseudo code of the standard GSA is shown in Algorithm (1) . Algorithm (1). Template of standard Gravitational Search Algorithm.
 Generate the initial population; Evaluate the fi tness value for each agent; Calculate the mass value for each agent; While stopping criteria is not satis fi ed Do Endwhile Output: Best solution found.

In GSA, parameters K and G are two main factors which balance exploitation and exploration. To avoid being trapped into a local optimum, each metaheuristic must use the exploration at begin-ning iterations. In GSA, this point is accomplished by assignment of high values to parameters K and G at the beginning, i.e. the value of K initial and G initial must be high. The high value for parameter K allows an agent to move in the solution space based on the position of more agents and consequently the exploration ability of the algorithm is increased. Also, a high value for parameter G increases the mobility of each agent in the solution space and hence the exploration capability of the algorithm is of the solution space are probably recognized in the early itera-tions of the algorithm. Hence, by lapse of iterations, the explora-tion of GSA must fade out and its exploitation must fade in. This issue is accomplished by reducing the values of parameters K and
G by lapse of iterations. The low value for parameter K causes an agent to move in the solution space based on the position of few agents and consequently the exploitation ability of the algorithm is increased. Also, the low value for parameter G decreases the mobility of each agent in the solution space and hence the exploitation of the algorithm is increased. Therefore, the good regions of the solution space are probably exploited in the ultimate iterations of the algorithm. 4. The proposed Grouping Gravitational Search Algorithm (GGSA)
In this section, an adaption of the structure of the standard GSA for solving clustering problems, called GGSA, is proposed. For this reason, a special encoding scheme is used to take into account the structure of clustering problems. Given the encoding, new updat-ing equations are developed which maintain the major character-istics of the Eqs. (9),(11) and (12) . 4.1. Solution encoding
To design an iterative metaheuristic, a representation scheme is necessary to encode a solution. The representation of a solution must be suitable and relevant to the tackled optimization problem and easy to manipulate by the search operators, so that the time and space complexities of these operators dealing with the representation must be reduced. The representation used in GGSA is the grouping representation ( Falkenauer, 1998 ) which is con-stituted by two different parts: item part and group part . The item part consists of an array of size n ( n is the number of data objects). The group part consists of a permutation of D group (cluster) labels. Each member in the item part can take any of D group tags, indicating that the corresponding item belongs to the cluster of the given tag. Fig. 1 illustrates a grouping encoding used by GGSA for solution C  X  { C 1  X  { O 1 , O 3 }, C 2  X  { O clustering problem with O  X  { O 1 , O 2 , O 3 , O 4 , O 5 Notice that when optimizing a continuous function by standard
GSA, generally each solution is represented with a vector of length D of real numbers ( D is the search dimension), where each value corre-sponds to one variable. In a similar way, when solving a clustering problem with GGSA, one can represent a solution composed of D clusters as a structure whose length is equal to the number of clusters. In other words, in GGSA groups play the role of variables of standard
GSA. Similar to standard GSA in which the location of object in d th dimension represents the value of d th variable, in GGSA it determines the data which belong to d th cluster. Through the paper, the number of clusters in a feasible solution X is represented by D .
The main property of the grouping encoding which encourages us to use it is that grouping encoding has very low redundancy ( Falkenauer, 1998 ). An encoding is said redundant if for one solution of the problem there are several different encodings; in such a case, the mapping between the solution space of problem and the encoding space of algorithm is one-to-many. The redundancy in the encoding of an algorithm enlarges the search space and does not allow the search operators to work properly and therefore reduces the ef fi algorithm ( Falkenauer, 1996; Talbi, 2009 ). For example, the degree of redundancy (i.e. the number of distinct encoding that represents the same solution of the problem) of integer encoding of data clustering problem grows exponentially with the number of clusters. Thus the size of the space which the algorithm has to search is much larger than operators will work with the group part of the grouping encoding, and the standard item part of the encoding merely serving to identify which items actually form which group; because, in data clustering problems it is the clusters which are the meaningful building blocks. 4.2. Updating equations in GGSA
Given the grouping representation, the aim of this section is to with clusters of data instead of scalars. The main characteristic of the rede fi ned equations is that they work in continuous space but their outcome is used in cluster space through a two phase procedure.
To recognize the main search operators of GSA, it is necessary to understand how the algorithm works. Suppose a population of agents is initialized in the solution space of a problem. In the top-down view, when GSA is applied to solve an optimization problem it tries move  X  the agents' positions in the solution space of the problem. Based on Eq. (12) , this movement is accomplished by the typical addition operator, i.e.  X   X   X  , which gets two input parameters, the position of the i th agent and a movement length of the i th agent, and then returns a new position in the solution space as output. The movement length of the i th agent itself is computed by two classes of movement lengths, the Independent Movement Length (IML) and Dependent Movement Length (DML). Note that IML is the movement length which is obtained for each agent without knowing any other positions besides its own on the current iteration. This type of movement length for the i th agent depends only on its previous movement length (or previous velocity), and in fact it is a fraction of the previous movement length of the i th agent. On the other hand, DML is the movement length which is obtained for each agent by considering the position of all members of Kbest set. Eq. (9) shows the calculation of DML for the i th agent in which all members of Kbest try itself depends on several subcomponents such as, the position of the agents in d th dimension, the Euclidean distance between the i th and the j th agents, the mass value of the j th agent, and the value of the way that they be able to work with groups rather than scalars, there are three operators which must be rede fi ned: the linear distance operator (  X   X  ), the Euclidean distance operator, and the movement operator (  X   X   X  ).

In standard GSA, linear distance operator quanti fi es the linear distance between two scalars. This operator must be rede fi that it can quantify the distance between two clusters. There are numerous possibilities for the cluster distance measure. One is the minimum distance between the clusters, i.e. the distance between their two closest members. Another measure is the maximum distance between the clusters, instead of the minimum. Also, there are other measures that represent a compromise between the extremes of minimum and maximum distance between cluster members. One is to represent clusters by the centroid of their members, as the k -means algorithm does, and uses the distance between centroids. In this paper, we use a cluster distance measure known as Jaccard distance . Let C 1 and C 2 be two clusters between C 1 and C 2 is de fi ned as follows: Dist J  X  C 1 ; C 2  X  X  Dist J  X  C 2 ; C 1  X  X  1 j C 1 \ C 2 where Dist J ( C 1 , C 2 ) is the degree of dissimilarity between two groups C 1 and C 2 which determines how far apart two clusters are. This measure is equal to  X  0  X  if we have C 1  X  C 2 , is equal to we have C 1 \ C 2  X   X  , and in general we have 0 r Dist J
To rede fi ne Euclidean distance operator to work with clusters rather than scalars, the main consideration is that, in GGSA, clusters play the role of variables of standard GSA. In other word, similar to standard GSA in which the location of particle in d th dimension represents the value of d th variable, in GGSA it determines the data which belong to d th cluster. Let C  X  { C 1 , ... , C D }and C 0  X  { C 0 two candidate clusterings of data objects. We de fi ne the Euclidean distance between C and C 0 as follows: Euclidean J  X  C ; C '  X  X  Euclidean J  X  C 0 ; C  X  X 
Notice that when computing Dist J (.,.) in above equation, it is reasonable to pair off the clusters properly before distance calculation. To mitigate the negative consequences of arbitrary cluster pairing, we can re-index the clusters of C and C 0 way that the most similar clusters always become paired together. For this reason, a general pairing procedure with time complexity O( n ) which uses the concept of the Maximum Weight bipartite Matching (MWM) is applied ( Husseinzadeh Kashan et al., 2013 ).
The MWM ordering rule starts its work on a complete bipartite similarity graph with two parts of nodes: source nodes and destination nodes, in which each source node is corresponded to one cluster in C and each destination node is corresponded to one group in C 0 . The edge between two nodes is weighted by their degree of similarity obtained by Jaccard coef fi cient , i.e.  X j C \ C j = j C 1 [ C 2 j X  . Let G b be such a weighted complete bipartite similarity graph. A matching in G b is a subset of edges that none of which are incident on any common node. Matching covers a node if it is incident to an edge belonging to matching. In this way, fi nding a matching with the largest sum of edge weights (i.e., maximum weight matching) in G b and re-indexing (reordering) the clusters of C 0 in such a way that both end nodes (clusters) of an edge, which participates in the matching, have a same index, satis fi es our goal of pairing the most similar clusters in C and C 0 ( Husseinzadeh Kashan et al., 2013 ).

Finally, with reshaping Eq. (12) in form of x d i  X  t  X  1  X  x d  X  v d  X  t  X  1  X  ; and substituting operator  X   X  by Dist j , the updating equations in GGSA are introduced as follows: v  X  t  X  1  X  X  rand Dist  X  x d i  X  t  X  1  X  ; x d i  X  t  X  X  v d i  X  t  X  1  X  ;  X  16  X  where d  X  1 ;:::; D is the cluster index, x d i  X  t  X  and x d Dist of solutions X i ( t ) and X j ( t ), and Euclidean J  X  X  X  E  X  implies  X  almost equal  X  and is used in place of  X   X   X  , because it may be impossible to construct the new cluster x d i  X  t  X  1  X  in such a right side of Eq. (16) , we set v d i  X  t  X  1  X  X  1 (because Dist 4.3. Generating a new solution in GGSA
To generate a new solution in GGSA, we will use the idea used in GPSO ( Husseinzadeh Kashan et al., 2013 ). Similar to GPSO, generation of a new solution in GGSA takes two phases. The phase is the inheritance phase which includes deciding about those parts of the solution X i ( t ) that the solution X
During this phase, a number of items may be missed in the solution X i ( t  X  1). The second phase is the reinsertion phase wherein missing items are inserted back to the existing clusters.
In GGSA the inheritance phase is handled implicitly through Eq. tance phase, should be in such a way that its degree of dissim-the degree of similarity between two groups should be almost items shared between x d i  X  t  X  and x d i  X  t  X  1  X  (i.e. n d  X  t  X j ) in such a way that the value of Dist J  X  x d i  X  t  X  1  X  and x d i  X  t  X  1  X  are one of the parts that solution X i from solution X i ( t ). Starting from Eq. (16) , the number of items shared between x d i  X  t  X  and x d i  X  t  X  1  X  is calculated as follows: Dist  X  x d i  X  t  X  1  X  ; x d i  X  t  X  X  X  1 n d i  X  t  X  1  X  ) n d i  X  t  X  1  X  X  1 v d i  X  t  X  1  X  X  x d i  X  t  X  : j  X  17  X 
Since n d i  X  t  X  1  X  must be integer, we set n d i  X  t  X  1  X  X  X  1 v d j x d  X  t  X jc , where : bc denote the integer part.

Algorithm (2) explains the steps of generating a new solution relevant to agent i at iteration t  X  1.

Algorithm (2). Template of New Solution Generator (NSG). //The MWM ordering rule For d  X  1to D do (for all X j  X  t  X  A Kbest ) by MWM pairing procedure; // The inheritance phase
Calculate the value of Euclidian J  X  X i  X  t  X  ; X j  X  t  X  X  (for all X using Eq. (14); For d  X  1to D do using Eq. (13) ; allocate them to new cluster x d i  X  t  X  1  X  ;
Endfor //The reinsertion phase
For each data O j that have not been selected in the inheritance phase do center;
Output: Solution X i ( t  X  1) 5. Experimental results
In this Section, we evaluate the performance of the GGSA on 13 real benchmark datasets from the UCI database ( Blake and Merz, 1998 ), which is a well-known database repository. These 13 benchmark problems which represent examples of data with low, medium and high dimensions are chosen exactly the same as in ( De Falco et al., 2007; Karaboga and Ozturk, 2011; Senthilnath et al., 2011 ), to make a reliable comparison. The structure of this
Section is as follows. First, we describe the characteristics of the 13 selected standard classi fi cation datasets. Then, we present the results obtained from the GGSA for benchmark datasets. Finally, we present the comparison of the GGSA with other four meta-heuristics and other nine classical methods used in the literature ( De Falco et al., 2007 ) and analyze their performance. 5.1. Datasets description
The 13 benchmark classi fi cation datasets used in this paper are well-known and well-used datasets by the machine learning community. The characteristics of these datasets such as the number of instances, the number of features, and the number of classes are summarized in Table 1 . From the each dataset, the 75% of data is randomly selected and is used in training process as a training set, and the remaining 25% of data is used in testing process as a test set. The number of the training and test sets can be found in Table 1 . After training phase, we obtain the cluster centers as extracted knowledge form training set that can be used for classifying the test set. 5.2. Results and comparisons
In this section, the performance of the proposed GGSA is investigated by applying the proposed algorithm to solve different benchmark datasets. The proposed GGSA was implemented in C language and run on a PC with an Intel 2.2 GHz CPU. The parameters of GGSA are fi xed on the following values: the number of objects, i.e. s , is set to 20, the initial value of K , i.e. K G end , is set to 0.5, and the stopping criterion is satis fi iterations. Also, two linear functions are used to reduce the value of parameters G and K with time. Note that all of these parameters are experimentally obtained by testing the GGSA on the different datasets.

The algorithm was tested on a set of 13 well-known benchmark datasets. The computational results generated by the proposed GGSA and other metaheuristics  X  standard version of GSA ( Bahrololoum et al., 2012 ), Arti fi cial Bee Colony (ABC) ( Karaboga and Ozturk, 2011 ), Particle Swarm Intelligence (PSO) ( De Falco et given in Table 2 . For each dataset, we report the Classi Error Percentage (CEP) which is the percentage of misclassi on the test set. The CEP is computed as follows: fi rst, the entire test data is classi fi ed and the number of misclassi fi cations is counted. This is possible because in the test dataset, we know the actual class label of each data instance. Second, this number is divided by total number of instances in the test set, and fi nally to achieve percentage it is multiplied by 100. The CEP is calculated by following equation: CEP  X  100 Number of misclassified instances Total size of test set
Table 2 shows that GGSA outperforms the PSO algorithm and standard GSA in all datasets, and overcomes ABC in 10 datasets, whereas in the case of other three datasets both ABC and GGSA get the same results. Also, GGSA have obtained acceptable results in comparison with FA; just the case of the datasets Cancer, Heart, and Thyroid, FA gets the better results. Moreover, the average CEP for all datasets are 8.90% for GGSA, 11.36% for FA, 11.41% for standard GSA, 13.13% for ABC algorithm, and 15.99% for PSO algorithm. In Table 2 , the ranking of all these metaheuristics based on their average CEP for all datasets is presented. The proposed GGSA is ranked in the fi rst place among fi ve metaheuristics.
In Table 3 , the CEP of GGSA and nine other classical well-known classi fi cation techniques  X  Bayes Net ( Jensen, 1996 ), Multi Layer Perceptron Arti fi cial Neural Network (MLP-ANN) ( Rumelhart et al., 1986 ), Radial Basis Function Arti fi cial Neural Network (RBF-ANN) ( Hassoun, 1995 ), KStar ( Cleary and Trigg, 1995 ), Bagging ( Breiman, 1996 ), MultiBoostAB ( Webb, 2000 ), Naive Bayes Tree (NBTree) ( Kohavi, 1996 ), Ripple Down Rule (Ridor) ( Compton and Jansen, 1988 ) and Voting Feature Interval (VFI) ( Demiroz and Guvenir, 1997 ) that are given in De Falco et al. (2007) are presented.
Parameter values used for any technique are those set as default in Waikato Environment for Knowledge Analysis (WEKA) system ( Witten and Frank, 2000 ). As this table shows, the GGSA gets the best solution in 9 of the datasets among 13 datasets used. To be able to make a good comparison, the ranking of GGSA and all these classical methods based on their average CEP for all datasets is presented in Table 3 . The proposed GGSA is ranked in the place of this ranking.
 Form Table 3 we can observe that, for the E. Coli dataset, the
GGSA with the CEP value 3.11 performs extremely better than all the other classical classi fi ers. Also, based on the results given in
Tables 2 and 3 , the standard GSA and FA perform better than all the other classical classi fi ers, for the E. Coli dataset, with the CEP values 7.31 and 8.54, respectively. As seen in Table 3 , the best classical classi fi cation method for the E. Coli dataset is MLP-ANN with the CEP value of 13.53. As we know, algorithms for the same problem can be based on very different ideas and can solve the problem with dramatically different qualities. Based on the results of Tables 2 and 3 , it is concluded that metaheuristics can better results than classical classi fi cation methods for the E. Coli dataset. The average misclassi fi cation percentage of metaheuristics is 9.40, whereas this value for classical classi fi cation methods is 19.47.

To be able to make a good comparison of metaheuristic classi fi cation algorithms and classical classi fi cation algorithms,
Table 4 is reported. This table shows the average classi fi errors of all datasets obtained by GGSA and 13 other classi methods. The ranking is based on the ascending order of average classi fi cation error percentage.
 To statistically compare the performance differences among GGSA and the other 13 classi fi cation algorithms, we conduct a
Wilcoxon signed-rank test ( Derrac et al., 2011 ). Table 5 shows the resultant p -values when comparing among GGSA and the other 13 classi fi cation algorithms on the 13 benchmark datasets. From the results, it can be seen that GGSA is signi fi cantly better than other 13 classi fi cation algorithms with a level of signi fi cance a  X  0.05.
Also, as the table states, GGSA shows a signi fi cant improvement over 11 classi fi cation algorithms with a level of signi a  X  0.01. 6. Conclusions and future work Adapting the grouping representation, we proposed a Grouping
Gravitational Search Algorithm (GGSA) for data clustering in this paper. The main property of the grouping representation which encourages us to use it is that it has very low redundancy. The updating equations of GGSA are analogous to the standard GSA equations which use the group coef fi cient of dissimilarity in place of the arithmetic subtraction operator. The use of cluster dissim-ilarity measure allows us working with clusters rather than items, since it is the clusters that constitute the underlying building-blocks of the solution in data clustering. One of the main characteristics of the new equations is that they work in contin-uous space but their outcome is used in cluster space through a two phase procedure.

The performance of the proposed algorithm is evaluated in term of misclassi fi cation percentage by several well-known bench-mark datasets. Its performance is compared with the Arti fi Colony, the Particle Swarm Optimization, the standard GSA, the
Fire fl y Algorithm, and nine other well-known classical classi fi tion techniques from the literature. The experimental results con fi rm the effectiveness of the proposed method and show that it can successfully be applied to data clustering.

For future research, the effectiveness of our approach could be examined on grouping problems such as bin packing problem, graph coloring problem, timetabling problem, set K-cover problem, multiple traveling salesmen problem, cell formation problem, etc. References
