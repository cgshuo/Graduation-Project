 There is a very severe speed vs. accuracy tradeoff in stochastic context-free parsing, which can be ex-plained by the grammar factor in the running-time complexity of standard parsing algorithms such as the CYK algorithm (Kasami, 1965; Younger, 1967). That algorithm has complexity O ( n 3 | P | ) , where n is the length in words of the sentence parsed, and | P | is the number of grammar productions. Grammar non-terminals can be split to encode richer dependen-cies in a stochastic model and improve parsing ac-curacy. For example, the parent of the left-hand side (LHS) can be annotated onto the label of the LHS category (Johnson, 1998), hence differentiating, for instance, between expansions of a VP with parent S and parent VP. Such annotations, however, tend to substantially increase the number of grammar pro-ductions as well as the ambiguity of the grammar, thereby significantly slowing down the parsing algo-rithm. In the case of bilexical grammars, where cat-egories in binary grammars are annotated with their lexical heads, the grammar factor contributes an ad-ditional O ( n 2 | V all O ( n 5 | V the number of delexicalized non-terminals (Eisner, 1997). Even with special modifications to the ba-sic CYK algorithm, such as those presented by Eis-ner and Satta (1999), improvements to the stochastic model are obtained at the expense of efficiency.
In addition to the significant cost in efficiency, increasing the non-terminal set impacts parame-ter estimation for the stochastic model. With more productions, much fewer observations per production are available and one is left with the hope that a subsequent smoothing technique can effectively deal with this problem, regardless of the number of non-terminals created. Klein and Manning (2003b) showed that, by making certain linguistically-motivated node label annotations, but avoiding certain other kinds of state splits (mainly lexical annotations) models of relatively high accu-racy can be built without resorting to smoothing. The resulting grammars were small enough to al-low for exhaustive CYK parsing; even so, parsing speed was significantly impacted by the state splits: the test-set parsing time reported was about 3s for average length sentences, with a memory usage of 1GB.

This paper presents an automatic method for de-ciding which state to split in order to create concise and accurate unsmoothed probabilistic context-free grammars (PCFGs) for efficient use in early stages of a multi-stage parsing technique. The method is based on the use of statistical tests to determine if a non-terminal combination is unobserved due to the limited size of the sample ( sampling zero ) or because it is grammatically impossible ( structural zero ). This helps introduce a relatively small number of new non-terminals with little additional parsing overhead. Experimental results show that, using this method, high accuracies can be achieved with orders of magnitude fewer non-terminals than in typically induced PCFGs, leading to substantial speed-ups in parsing. The approach can further be used in combi-nation with an existing reranker to provide competi-tive WSJ parsing results.

The remainder of the paper is structured as fol-lows. Section 2 gives a brief description of PCFG induction from treebanks, including non-terminal label-splitting, factorization, and relative frequency estimation. Section 3 discusses the statistical criteria that we explored to determine structural zeros and thus select non-terminals for the factored PCFG. Fi-nally, Section 4 reports the results of parsing experi-ments using our exhaustive k -best CYK parser with the concise PCFGs induced from the Penn WSJ tree-bank (Marcus et al., 1993). A context-free grammar G = ( V, T, S  X  , P ) , or CFG in short, consists of a set of non-terminal symbols V , a set of terminal symbols T , a start symbol S  X   X  V , and a set of production P of the form: A  X   X  , where A  X  V and  X   X  ( V  X  T )  X  . A PCFG is a CFG with a probability assigned to each production. Thus, the probabilities of the productions expanding a given non-terminal sum to one. 2.1 Smoothing and factorization PCFGs induced from the Penn Treebank have many productions with long sequences of non-terminals on the RHS. Probability estimates of the RHS given the LHS are often smoothed by making a Markov assumption regarding the conditional independence of a category on those more than k categories away (Collins, 1997; Charniak, 2000): Making such a Markov assumption is closely re-lated to grammar transformations required for cer-tain efficient parsing algorithms. For example, the CYK parsing algorithm takes as input a Chomsky Normal Form PCFG, i.e., a grammar where all pro-ductions are of the form X  X  Y Z or X  X  a , where X , Y , and Z are non-terminals and a a ter-a treebank whose trees have been factored so that n -ary productions with n&gt; 2 become sequences of n  X  1 binary productions. Full right-factorization in-volves concatenating the final n  X  1 categories from the RHS of an n -ary production to form a new com-posite non-terminal. For example, the original pro-duction NP  X  DT JJ NN NNS shown in Figure 1(a) is factored into three binary rules, as shown in Fig-ure 1(b). Note that a PCFG induced from such right-factored trees is weakly equivalent to a PCFG in-duced from the original treebank, i.e., it describes the same language.
 From such a factorization, one can make a Markov assumption for estimating the production probabilities by simply recording only the labels of the first k children dominated by the composite fac-tored label. Figure 1 (c), (d), and (e) show right-factored trees of Markov orders 2, 1 and 0 respec-as mentioned above, these factorizations reduce the size of the non-terminal set, which in turn improves CYK efficiency. The efficiency benefit of making a Markov assumption in factorization can be substan-tial, given the reduction of both non-terminals and productions, which improves the grammar constant. With standard right-factorization, as in Figure 1(b), the non-terminal set for the PCFG induced from sec-tions 2-21 of the Penn WSJ Treebank grows from its original size of 72 to 10105, with 23220 produc-tions. With a Markov factorization of orders 2, 1 and 0 we get non-terminal sets of size 2492, 564, and 99, and rule production sets of 11659, 6354, and 3803, respectively.

These reductions in the size of the non-terminal set from the original factored grammar result in an order of magnitude reduction in complexity of the CYK algorithm. One common strategy in statisti-cal parsing is what can be termed an approximate coarse-to-fine approach: a simple PCFG is used to prune the search space to which richer and more complex models are applied subsequently (Char-niak, 2000; Charniak and Johnson, 2005). Produc-ing a  X  X oarse X  chart as efficiently as possible is thus crucial (Charniak et al., 1998; Blaheta and Charniak, 1999), making these factorizations particularly use-ful. 2.2 CYK parser and baselines To illustrate the importance of this reduction in non-terminals for efficient parsing, we will present base-line parsing results for a development set. For these baseline trials, we trained a PCFG on sec-tions 2-21 of the Penn WSJ Treebank (40k sen-tences, 936k words), and evaluated on section 24 (1346 sentences, 32k words). The parser takes as input the weighted k -best POS-tag sequences of a perceptron-trained tagger, using the tagger docu-mented in Hollingshead et al. (2005). The number of tagger candidates k for all trials reported in this paper was 0.2 n , where n is the length of the string. From the weighted k -best list, we derive a condi-tional probability of each tag at position i by taking the sum of the exponential of the weights of all can-didates with that tag at position i (softmax).
The parser is an exhaustive CYK parser that takes advantage of the fact that, with the grammar fac-torization method described, factored non-terminals can only occur as the second child of a binary pro-duction. Since the bulk of the non-terminals result from factorization, this greatly reduces the number of possible combinations given any two cells. When parsing with a parent-annotated grammar, we use a version of the parser that also takes advantage of the partitioning of the non-terminal set, i.e., the fact that any given non-terminal has already its parent indi-cated in its label, precluding combination with any non-terminal that does not have the same parent an-notated.

Table 1 shows baseline results for standard right-factorization and factorization with Markov orders 0-2. Training consists of applying a particular gram-mar factorization to the treebank prior to inducing a PCFG using maximum likelihood (relative fre-quency) estimation. Testing consists of exhaustive CYK parsing of all sentences in the development set (no length limit) with the induced grammar, then de-transforming the maximum likelihood parse back to the original format for evaluation against the refer-ence parse. Evaluation includes the standard PAR-SEVAL measures labeled precision (LP) and labeled recall (LR), plus the harmonic mean (F-measure) of these two scores. We also present a result using parent annotation (Johnson, 1998) with a 2nd-order Markov assumption. Parent annotation occurs prior to treebank factorization. This condition is roughly equivalent to the h = 1 , v = 2 in Klein and Manning
From these results, we can see the large efficiency benefit of the Markov assumption, as the size of the non-terminal and production sets shrink. However, the efficiency gains come at a cost, with the Markov order-0 factored grammar resulting in a loss of a full 8 percentage points of F-measure accuracy. Parent annotation provides a significant accuracy improve-ment over the other baselines, but at a substantial efficiency cost.

Note that the efficiency impact is not a strict func-tion of either the number of non-terminals or pro-ductions. Rather, it has to do with the number of competing non-terminals in cells of the chart. Some grammars may be very large, but less ambiguous in a way that reduces the number of cell entries, so that only a very small fraction of the productions need to be applied for any pair of cells. Parent annotation does just the opposite  X  it increases the number of cell entries for the same span, by creating entries for the same constituent with different parents. Some non-terminal annotations, e.g., splitting POS-tags by annotating their lexical items, result in a large gram-mar, but one where the number of productions that will apply for any pair of cells is greatly reduced.
Ideally, one would obtain the efficiency benefit of the small non-terminal set demonstrated with the Markov order-0 results, while encoding key gram-matical constraints whose absence results in an ac-curacy loss. The method we present attempts to achieve this by using a statistical test to determine structural zeros and modifying the factorization to remove the probability mass assigned to them. The main idea behind our method for detecting structural zeros is to search for events that are in-dividually very frequent but that do not co-occur. For example, consider the Markov order-0 bi-nary rule production in Figure 2. The produc-tion NP  X  NP NP: may be very frequent, as is the NP:  X  CC NN production, but they never co-occur together, because NP does not conjoin with NN in the Penn Treebank. If the counts of two such events a and b , e.g., NP  X  NP NP: and NP:  X  CC NN are very large, but the count of their co-occurrence is zero, then the co-occurrence of a and b can be viewed as a candidate for the list of events that are structurally inadmissible. The probability mass for the co-occurrence of a and b can be removed by replacing the factored non-terminal NP: with NP:CC:NN whenever there is a CC and an NN com-bining to form a factored NP non-terminal.

The expansion of the factored non-terminals is not the only event that we might consider. For exam-ple, a frequent left-most child of the first child of the production, or a common left-corner POS or lexi-cal item, might never occur with certain productions. For example,  X  X BAR  X  IN S X  and  X  X N  X  of X  are both common productions, but they never co-occur. We focus on left-most children and left-corners because of the factorization that we have selected, but the same idea could be applied to other possible state splits.

Different statistical criteria can be used to com-pare the counts of two events with that of their co-occurrence. This section examines several possible criteria that are presented, for ease of exposition, with general sequences of events. For our specific purpose, these sequences of events would be two rule productions. 3.1 Notation This section describes several statistical criteria to determine if a sequence of two events should be viewed as a structural zero. These tests can be gen-eralized to longer and more complex sequences, and to various types of events, e.g., word, word class, or rule production sequences.

Given a corpus C , and a vocabulary  X  , we denote by c be the total number of observations in C . We will denote by  X  a the set { b  X   X  : b 6 = a } . Hence c n  X  c a . Let P( a ) = c a n , and for b  X   X  , let P( a | b ) = 3.2 Mutual information The mutual information between two random vari-ables X and Y is defined as For a particular event sequence of length two ab , this suggests the following statistic: Unfortunately, for c assume, however, that all unobserved sequences are given some count, then when c where K is a constant. Since we need these statistics only for ranking purposes, we can ignore the con-stant factor. 3.3 Log odds ratio Another statistic that, like mutual information, is ill-defined with zeros, is the log odds ratio : Here again, if c assign to all unobserved pairs a small count , when c 3.4 Pearson chi-squared For any i, j  X   X  , define  X   X  chi-squared test of independence is then defined as follows: In the case of interest for us, c simplifies to: 3.5 Log likelihood ratio Pearson X  X  chi-squared statistic assumes a normal or approximately normal distribution, but that assump-tion typically does not hold for the occurrences of rare events (Dunning, 1994). It is then preferable to use the likelihood ratio statistic which allows us to compare the null hypothesis, that P( b ) = P( b | a ) = P( b |  X  a ) = c b n , with the hypothesis that P( b | a ) = is that the context of event a does not change the probability of seeing b . These discrete conditional probabilities follow a binomial distribution, hence the likelihood ratio is where B [ p, x, y ] = p x (1  X  p ) y  X  x ( y cial case where c expression can be simplified as follows: The log-likelihood ratio, denoted by G 2 , is known to be asymptotically X 2 -distributed. In this case, and with the binomial distribution, it has has one degree of freedom, thus the distribution will have asymptotically a mean of one and a standard devia-tion of We experimented with all of these statistics. While they measure different ratios, empirically they seem to produce very similar rankings. For the experiments reported in the next section, we used the log-likelihood ratio because this statistic is well-defined with zeros and is preferable to the Pearson chi-squared when dealing with rare events. We used the log-likelihood ratio statistic G 2 to rank unobserved events ab , where a  X  P and b  X  V . Let V let  X   X  ( V terminal/colon symbol pairs. Suppose we have a fre-quent factored non-terminal X :  X B for X, B  X  V Then, if the set of productions X  X  Y X :  X A with A  X  V o is also frequent, but X  X  Y X :  X B is un-observed, this is a candidate structural zero. Simi-lar splits can be considered with non-factored non-terminals.

There are two state split scenarios we consider in this paper. Scenario 1 is for factored non-terminals, which are always the second child of a binary pro-duction. For use in Equation 7, Scenario 2 is for non-factored non-terminals, which we will split using the leftmost child, the left-corner POS-tag, and the left-corner lexical item, which are easily incorporated into our grammar factorization approach. In this scenario, the non-terminal to be split can be either the left or right child in the binary production. Here we show the counts for the left child case for use in Equation 7: In this case, the possible splits are more compli-cated than just non-terminals as used in factoring. Here, the first possible split is the left child cat-egory, along with an indication of whether it is a unary production. One can further split by in-cluding the left-corner tag, and even further by including the left-corner word. For example, a unary S category might be split as follows: first to S[1:VP] if the single child of the S is a VP; next to S[1:VP:VBD] if the left-corner POS-tag is VBD; finally to S[1:VP:VBD:went] if the VBD verb was  X  X ent X .

Note that, once non-terminals are split by anno-tating such information, the base non-terminals, e.g., S, implicitly encode contexts other than the ones that were split.

Table 2 shows the unobserved rules with the largest G 2 score, along with the ten non-terminals that these productions suggest for inclusion in our non-terminal set. The highest scoring un-observed production is PP  X  IN[that] NP. It re-ceives such a high score because the base production (PP  X  IN NP) is very frequent, and so is  X  X N  X  that X , but they jointly never occur, since  X  X N  X  that X  is a complementizer. This split non-terminal also shows up in the second-highest ranked zero, an SBAR with  X  X hat X  complementizer and an S child that consists of a unary VP. The unary S  X  VP production is very common, but never with a  X  X hat X  complementizer in an SBAR.

Note that the fourth-ranked production uses two split non-terminals. The fifth ranked rule presum-ably does not add much information to aid parsing disambiguation, since the AUX MD tag sequence is with a factored category, ruling out coordination be-tween NN and NP.

Before presenting experimental results, we will mention some practical issues related to the ap-proach described. First, we independently parame-terized the number of factored categories to select and the number of non-factored categories to se-lect. This was done to allow for finer control of the amount of splitting of non-terminals of each type. To choose 100 of each, every non-terminal was as-signed the score of the highest scoring unobserved production within which it occurred. Then the 100 highest scoring non-terminals of each type were added to the base non-terminal list, which originally consisted of the atomic treebank non-terminals and Markov order-0 factored non-terminals.

Once the desired non-terminals are selected, the training corpus is factored, and non-terminals are split if they were among the selected set. Note, how-ever, that some of the information in a selected non-terminal may not be fully available, requiring some number of additional splits. Any non-terminal that is required by a selected non-terminal will be selected itself. For example, suppose that NP:CC:NP was chosen as a factored non-terminal. Then the sec-ond child of any local tree with that non-terminal on the LHS must either be an NP or a factored non-terminal with at least the first child identified as an NP, i.e., NP:NP. If that factored non-terminal was not selected to be in the set, it must be added. The same situation occurs with left-corner tags and words, which may be arbitrarily far below the cate-gory.

After factoring and selective splitting of non-terminals, the resulting treebank corpus is used to train a PCFG. Recall that we use the k -best output of a POS-tagger to parse. For each POS-tag and lexical item pair from the output of the tagger, we reduce the word to lower case and check to see if the com-bination is in the set of split POS-tags, in which case we split the tag, e.g., IN[that].

Figure 3 shows the F-measure accuracy for our trials on the development set versus the number of non-factored splits parameterized for the trial. From this plot, we can see that 500 non-factored splits provides the best F-measure accuracy on the dev set. Presumably, as more than 500 splits are made, sparse data becomes more problematic. Figure 4 shows the development set F-measure accuracy ver-sus the number of words-per-second it takes to parse the development set, for non-factored splits of 0 and 500, at a range of factored split parameterizations. With 0 non-factored splits, efficiency is substantially impacted by increasing the factored splits, whereas it can be seen that with 500 non-factored splits, that impact is much less, so that the best performance is reached with both relatively few factored non-terminal splits, and a relatively small efficiency im-pact. The non-factored splits provide substantial ac-curacy improvements at relatively small efficiency cost.

Table 3 shows the 1-best and reranked 50-best re-sults for the baseline Markov order-2 model, and the best-performing model using factored and non-factored non-terminal splits. We present the effi-ciency of the model in terms of words-per-second over the entire dev set, including the longer strings (maximum length 116 words) 5 . We used the k -best decoding algorithm of Huang and Chiang (2005) with our CYK parser, using on-demand k -best back-pointer calculation. We then trained a MaxEnt reranker on sections 2-21, using the approach out-lined in Charniak and Johnson (2005), via the pub-used the default features that come with that pack-age. The processing time in the table includes the time to parse and rerank. As can be seen from the trials, there is some overhead to these processes, but the time is still dominated by the base parsing.
We present the k -best results to demonstrate the benefits of using a better model, such as the one we have presented, for producing candidates for down-stream processing. Even with severe pruning to only the top 50 candidate parses per string, which re-sults in low oracle and reranked accuracy for the Markov order-2 model, the best-performing model based on structural zeros achieves a relatively high oracle accuracy, and reaches 88.0 and 87.5 percent F-measure accuracy on the dev (f24) and eval (f23) sets respectively. Note that the well-known Char-niak parser (Charniak, 2000; Charniak and Johnson, 2005) uses a Markov order-3 baseline PCFG in the initial pass, with a best-first algorithm that is run past the first parse to populate the chart for use by the richer model. While we have demonstrated ex-haustive parsing efficiency, our model could be used with any of the efficient search best-first approaches documented in the literature, from those used in the Charniak parser (Charniak et al., 1998; Blaheta and 2003a). By using a richer grammar of the sort we present, far fewer edges would be required in the chart to include sufficient quality candidates for the richer model, leading to further downstream savings of processing time. We described a method for creating concise PCFGs by detecting structural zeros. The resulting un-smoothed PCFGs have far higher accuracy than sim-ple induced PCFGs and yet are very efficient to use. While we focused on a small number of simple non-terminal splits that fit the factorization we had se-lected, the technique presented is applicable to a wider range of possible non-terminal annotations, including head or parent annotations. More gener-ally, the ideas and method for determining structural zeros (vs. sampling zeros) can be used in other con-texts for a variety of other learning tasks. This material is based upon work supported by the National Science Foundation under Grant IIS-0447214. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. The first author X  X  work was partially funded by the New York State Office of Science Technology and Academic Research (NYS-TAR).

