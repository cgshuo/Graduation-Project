 H.3 [ Information Storage and Retrieval ]: H.3.1 Con-tent Analysis and Indexing Experimentation Speech retrieval; Spontaneous speech This paper reports on experiments with the first available Czech IR test collection. The collection consists of a con-tinuous stream from automatic transcription of spontaneous speech (see [3] for details) and the task of the IR system is to identify appropriate replay points where the discussion about the queried topic starts. The collection thus lacks clearly defined document boundaries. Moreover, the accu-racy of the transcription is limited (around 35% word error rate), mostly due to the nature of the speech X  X nterviews with Holocaust survivors, which are sometimes emotional, accented, and exhibiting age-related speech impediments. This collection therefore offers an excellent opportunity to explore both effects present in Czech (e.g., morphology) and effects that result from processing spontaneous speech. It was also used in the CL-SR track at the CLEF 2006 evalu-ation campaign ( http://www.clef-campaign.org/ ).
Retrieval from a speech stream with unknown topic bound-aries is an interesting challenge, but that is not our princi-pal focus in these experiments. We therefore transformed the collection into artificially defined set of  X  X ocuments X  by removing all recognized pauses between words and then sliding a 3-minute window over the transcripts with a 1-minute step size. This resulted in a collection of 11,377 overlapping passages, each containing an average of 390 rec-ognized words (denoted as the asr field) and a set of au-tomatically produced Czech translations (using techniques described in [2]) for 20 automatically assigned thesaurus key-words (using techniques described in [4]) (the ak field). Each field was indexed separately, and a unified index ( asr.ak ) was also constructed.

Twenty-nine topics were initially created in English in fields), translated into Czech by a native speaker, and then checked for natural expression by a second native speaker. We performed monolingual experiments with  X  X ong X  queries constructed by concatenating the words from all three topic fields.

A morphological analyser was used to obtain the infor-mation about the lemma (linguistic root form), stem (ap-proximation to that root form using truncation alone) and part-of-speech for each Czech word [1] . Three variants of the collection were indexed, one with only words, one with only lemmas and one with only stems. Part-of-speech tags were used as a basis for stopword removal X  X s we could not find any decent stoplist for Czech, we simply removed all words that were tagged as preposition, conjunction, particle or interjection. In each case, identical processing was done for the queries. We used Lemur to implement a simple tf.idf model with blind feedback (using Lemur X  X  standard parame-ters). Length normalization was not performed because the collection preprocessing resulted in documents with nearly identical lengths.
Relevance assessors identified appropriate start times by interactively searching using manually assigned English the-saurus terms and the same automatically transcribed con-tent, ultimately confirming their decisions by listening to the audio when the automatically produced transcripts were not sufficiently accurate to make a definitive judgment. Table 1 reports the mean Generalized Average Precision (mGAP), which is computed in a manner similar to mean average pre-cision (for details see [3]).

Indexing the ak field, alone or in combination with asr , proved not to be helpful (although the apparent reduction when indexed together is not statistically significant ( p&gt; 0 . 05)). Manual examination of a few ak fields indeed indi-cates a low density of terms that appear as if they match
Figure 1: GAP by topic, asr field, long queries. the content of the passage, but additional analysis will be needed before we can ascribe blame between the transcrip-tion, classification and translation stages in the cascade that produced those keyword assignments. We therefore focus on results obtained using the asr field alone for the remainder of our analysis.

It is apparent that some form of linguistic preprocessing is indeed crucial for Czech. Both lemmatization and stem-ming boosted the performance almost by a factor of two in comparison with the word runs, and a Wilcoxon signed-rank test shows that difference to be statistically significant ( p&lt; 0 . 005). The slight apparent advantage of the lemma run over the stem run is not statistically significant ( p&gt; As Figure 1 shows, substantial variation in GAP is evident across topics. The four topics with the highest GAP values (1225, 1630, 2198, 3014) each contain highly discriminative terms that were correctly transcribed. Topic 1630 exhibits an enormous difference between word matching and match-ing either stems or lemmas, a vivid reminder of how the recall-enhancing effect of linguistic analysis can dominate averaged measures (a similar effect is also apparent for topic 1310). While a few cases of adverse effects from linguis-tic analysis are visible (most notably with topics 1225 and 1181), these effects are generally relatively small. The occa-sional differences between stems and lemmas suggests that combining evidence from both might help in some cases.
Unsuccessful topics generally either asked about abstract concepts without using many discriminative terms (e.g., topic 1288:  X  X trengthening faith during the Holocaust X ), or the discriminative terms for the topic happened to be missing from the collection. For example, topic 3018 contained a sin-gle discriminative term that was simply spelled differently in the ASR lexicon (and consequently in the transcripts). Manually conforming the spelling in the topic to that found in the lexicon would have increased the GAP for that topic (with lemma ) from 0.0026 to 0.1175.

Interestingly, it turned out that every term that we (man-ually) judged to be highly discriminating in our analysis of successful and unsuccessful topics was a named entity (NE). This prompted us to perform a more systematic analysis of the vocabulary coverage for the NEs present in all 29 topics. If we leave out the NEs that are widespread in the collection and thus useless for IR (Jew, Holocaust, Hitler, etc.), there are 42 NEs in the topic set; only 13 of them are present in the ASR lexicon, only 11 of those 13 actually appeared any-where in the transcripts, and only 5 of those 11 substantially contributed to successful IR (or, if we manually conform the spelling in topic 3018, 6 of 12). The overall  X  X uery rare named entity error rate X  for this collection is therefore (42-5)/42=88%, more than double the overall word error rate. Rare NEs are quite naturally not well represented in the materials from which ASR systems are trained; integrating phone-lattice term detection with large-vocabulary recogni-tion offers one promising research direction. Inconsistent spelling is probably the more easily rectified problem; anno-tators of ASR training materials are typically not domain experts, and in some cases valid alternate transliterations (e.g., from Yiddish roots) result in disagreement even among experts. One useful approach would be to adjust the top-ics to conform to the ASR lexicon, thus simulating a similar process an interactive searcher could perform if notified that one of their query terms is outside the known vocabulary.
In addition to the ideas above for dealing with rare terms, another obvious next step would be to optimize our system design to better reflect the task characteristics that moti-vated the design of the mean GAP measure. We have shown that passage retrieval can indeed sometimes get us in the right neighborhood, but overlapping passages may not be the best way of identifying optimal replay start times. An-other question that we need to explore is whether some other retrieval model might be more effective. Finally, extending our work to include on the far larger CLEF 2007 Czech news test collection will allow us to enrich our comparison between lemmas and stems for Czech indexing. This work was supported in part by projects MSMT LC536, GACR 1ET101470416 and NSF IIS-0122466. [1] J. Haji X  c. Disambiguation of Rich Inflection. [2] C. Murray et al . Leveraging Reusability: Cost-effective [3] D. Oard et al . Overview of the CLEF-2006 [4] S. Olsson, D. Oard, and J. Haji X  c. Cross-Language Text
