 where Z = P h and {  X  i } N i =1 ,W = { w ik } ,  X  = {  X  k } are free parameters. For convenience, let us re-parameterise the distribution as follows The partition function can be rewritten as where  X ( h ) = where hidden variables h . factorisation { x assign non-zeros probability to improbably areas.
 its entropy is decomposable, i.e., and thus
X where divergence in Eq. (5) into local terms: Gaussian values.
 (which is 1 ). In our experiment, the cooling is scheduled as follows sample. In our experiments, k = 50 . The log-likelihood of an evidence is where Z ( e ) = P h W ik reads where the domain of the Gaussian is constrained to x  X   X  ( e ) . and finally: ent in the factor posterior: turn.
 following objective function rule, we have: smoothing fashion: sampled Gaussian at time t .
 weighted down. Typically we choose  X  close to 1 , e.g.,  X  = 0 . 9 . by the data likelihood. The data likelihood can be estimated as integration can be carried out using the technique described in the main text. Here we are concerned about the popular Gumbel X  X  distribution. Let us start from the Gumbel density function where  X  is the mode (location) and  X  is the scale parameter. order polynomial around  X  ), we have Renormalising this distribution, e.g., by replacing e  X  2 . 7183 by distribution .
 the m -th category is where  X  l is the location of the l -th utility.
  X  m  X   X  log y m , or y l &gt; y m exp for y m  X  0 . Thus Now, by changing variable under the integration from x m to y m , we have ensure that x 1 &gt; x 2 &gt; ... &gt; x D . This is equivalent to The probability of this is essentially Taylor X  X  expansion The Gaussian approximation has the form where 1 /f 00 (  X  ) is the new variance. i.e.,  X  &lt; x &lt;  X  , the new density reads where Some special cases: Association , 83(403):892 X 901, 1988.
 EJ Gumbel. Statistical of extremes . Columbia University Press, New York, 1958. R.D. Luce. Individual choice behavior . Wiley New York, 1959. pages 105 X 142, 1973.
 85(410):558 X 564, 1990.
 Truyen Tran  X  X  truyen.tran@deakin.edu.au Dinh Phung  X  dinh.phung@deakin.edu.au Svetha Venkatesh  X  svetha.venkatesh@deakin.edu.au
Restricted Boltzmann machines (RBMs) have proved to be a versatile tool for a wide variety of machine learning tasks and as a building block for deep architectures (Hinton and Salakhutdinov, 2006; Salakhutdinov and Hinton, 2009a; Smolensky, 1986). The original proposals mainly handle binary visible and hidden units. Whilst binary hidden units are broadly applicable as feature detectors, non-binary visible data requires different designs. Recent exten-sions to other data types result in type-dependent models: the Gaussian for continuous inputs (Hinton and Salakhutdinov, 2006), Beta for bounded continu-ous inputs (Le Roux et al., 2011), Poisson for count data (Gehler et al., 2006), multinomial for unordered categories (Salakhutdinov and Hinton, 2009b), and or-dinal models for ordered categories (Truyen et al., 2009; Tran et al., 2012).

The Boltzmann distribution permits several types to be jointly modelled, thus making the RBM a good tool for multimodal and complex social survey analy-sis. The work of (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012; Xing et al., 2005) combines con-tinuous (e.g., visual and audio) and discrete modalities (e.g., words). The work of (Tran et al., 2011) extends the idea further to incorporate ordinal and rank data. However, there are conceptual drawbacks: First, con-ditioned on the hidden layer, they are still separate type-specific models; second, handling ordered cate-gories and ranks is not natural; and third, specifying direct correlation between these types remains diffi-cult.

The main thesis of this paper is that many data types can be captured in one unified model. The key observations are that (i) type-specific properties can be modelled using one or several underlying continuous variables , in the spirit of Thurstonian models 1 (Thur-stone, 1927), and (ii) evidences be expressed in the form of one or several inequalities of these underlying variables. For example, a binary visible unit is turned on if the underlying variable is beyond a threshold; and a category is chosen if its utility is the largest among all those of competing categories. The use of underlying variables is desirable when we want to explicitly model the generative mechanism of the data. In psychology and economics, for example, it gives much better in-terpretation on why a particular choice is made given the perceived utilities (B  X ockenholt, 2006). Further, it is natural to model the correlation among type-specific inputs using a covariance structure on the underlying variables.

The inequality observation is interesting in its own right: Instead of learning from assigned values, we learn from the inequality expression of evidences , which can be much more relaxed than the value as-signments. This class of evidences indeed covers a wide range of practical situations, many of which have not been studied in the context of Boltzmann machines, as we shall see throughout the paper.

To this end, we propose a novel class of models called Thurstonian Boltzmann Machine (TBM). The TBM utilises the Gaussian restricted Boltzmann ma-chine (GRBM): The top layer consists of binary hidden units as in standard RBMs; the bottom layer contains a collection of Gaussian variable groups, one per in-put type. The main difference is that TBM does not require valued assignments for the bottom layer but a set of inequalities expressing the constraints imposed by the evidences . Except for a limiting case of point as-signments where the inequalities are strictly equalities, the Gaussian layer is never fully observed. The TBM supports more data types in a unified manner than ever before: For any combination of the point assign-ments, intervals, censored values, binary, unordered categories, multi-categories, ordered categories, (in)-complete ranks with and without ties, all we need to do is to supply relevant subset of inequalities.
We evaluate the proposed model on three appli-cations of very different natures: handwritten digit recognitions, collaborative filtering and complex sur-vey analysis. For the first two applications, the per-formance is competitive against methods designed for those data types. On the last application, we be-lieve we are among the first to propose a scalable and generic machinery for handle those complex data types.
Let x = ( x 1 ,x 2 ,...,x N ) &gt;  X  R N be a vector of input variables. Let h = ( h 1 ,h 2 ,...,h K ) &gt;  X  X  0 , 1 } of hidden factors which are designed to capture the variations in the observations. The input layer and the hidden layer form an undirected bipartite graph, i.e., only cross-layer connections are allowed. The model admits the Boltzmann distribution where Z = P h ising constant and E ( x , h ) is the state energy. The energy is decomposed as where {  X  i } N i =1 ,W = { W ik } ,  X  = {  X  k } are free param-eters and W i  X  denotes the i -th row.
 Given the input x , the posterior has a simple form where W  X  k denotes the k -th column. Similarly, the generative process given the binary factor h is also factorisable where N (  X , 1) is the normal distribution of mean  X  and unit deviation. We now generalise the Gaussian RBM into the Thurstonian Boltzmann Machine (TBM). Denote by e an observed evidence of x . Standard evidences are the point assignment of x to some specific real-valued vector, i.e., x = v . Generalised evidences can be ex-pressed using inequality constraints for some transform matrix A  X  R M  X  N and vectors b , c  X  R M , where  X  denotes element-wise inequali-ties. Thus an evidence can be completely realised by specifying the triple  X  A, b , c  X  . For example, for the point assignment,  X  A = I , b = v , c = v  X  , where I is the identity matrix. In what follows, we will detail other useful popular realisations of these quantities. 3.1. Boxed Constraints
This refers to the case where input variables are in-dependently constrained, i.e., A = I , and thus we need only to specify the pair  X  b , c  X  .
 Censored observations. This refers to situation where we only know the continuous observation be-yond a certain point, i.e., b = e and c = +  X  . For example, in survival analysis, the life expectancy of a person might be observed up to a certain age, and we have no further information afterward.
 Interval observations . When the measurements are imprecise, it may be better to specify the range of possible observations with greater confidence rather than a singe point, i.e., b = e  X   X  and c = e +  X  for some pair ( e ,  X  ). For instance, missile tracking may estimate the position of the target with certain preci-sion.
 Binary observations . A binary observation e i can be thought as a result of clipping x i by a threshold  X  i that is e i = 1 if x i  X   X  i and e i = 0 otherwise. The boundaries in Eq. (3.2) become: Thus, this model offers an alternative 2 to standard bi-nary RBMs of (Smolensky, 1986; Freund and Haussler, 1993).
 Ordinal observations. Denote by e = ( e 1 ,e 2 ,...,e N ) the set of ordinal observations, where each e i is drawn from an ordered set { 1 , 2 ,..,L } . The common assumption is that the ordinal level e i = l is observed given x i  X  [  X  l  X  1 , X  l ] for some thresholds  X   X   X  2  X  ... X  L  X  1 . The boundaries thus read This offers an alternative 3 to the ordinal RBMs of (Truyen et al., 2009). 3.2. Inequality Constraints Categorical observations . This refers to the sit-uation where out of an unordered set of categories, we observe only one category at a time. This can be formulated as follows. Each category is associ-ated with a  X  X tility X  variable. The category l is ob-served (i.e., e i = m ) if it has the largest utility, that is x il  X  max m 6 = l x im . Thus, x il is the upper-threshold for all other utilities. On the other hand, max m 6 = l x im the lower-threshold for x il . This suggests an EM-style procedure: ( i ) fix x il (or treat it as a threshold) and learn the model under the intervals x im  X  x il for all m 6 = l , and ( ii ) fix all categories other than l , learn the model under the interval x il  X  max m 6 = l x im . This of-fers an alternative 4 to the multinomial logit treatment in (Salakhutdinov et al., 2007).

To illustrate the point, suppose there are only four variables z 1 ,z 2 ,z 3 ,z 4 , and z 1 is observed, then we have z 1  X  max { z 2 ,z 3 ,z 4 } . This can be expressed as z 1 z 2  X  0; z 1  X  z 3  X  0 and z 1  X  z 4  X  0. These are equivalent to * Imprecise categorical observations . This gener-alises the categorical case : The observation is a sub-set of a set, where any member of the subset can be a possible observation 5 . For example, when asked to choose the best sport team of interest, a per-son may pick two teams without saying which is the best. For instance, suppose the subset is { z 1 ,z 2 } , then min { z 1 ,z 2 } X  max { z 3 ,z 4 } , which can be expressed as z  X  z 3  X  0; z 2  X  z 3  X  0, z 1  X  z 4  X  0 and z 2  X  z 4  X  0. This translates to the following triple * Rank (with Ties) observations . This generalises the imprecise categorical cases : Here we have a (par-tially) ranked set of categories. Assume that the rank is produced in a stagewise manner as follows: The best category subset is selected out of all categories, the second best is selected out of all categories except for the best one, and so on. Thus, at each stage we have an imprecise categorical setting, but now the utilities of middle categories are constrained from both sides  X  the previous utilities as the upper-bound, and the next utilities as the lower-bound.

As an illustration, suppose there are four variables z ,z 2 ,z 3 ,z 4 and a particular rank (with ties) imposes that min { z 1 ,z 2 }  X  z 3  X  z 4 . This be rewritten as z  X  z 3 ; z 2  X  z 3 ; z 3  X  z 4 , which is equivalent to *
Under the TBM, MCMC-based inference without evidences is simple: we alternate between P ( h | x ) and P ( x | h ). This is efficient because of the factori-sations in Eqs. (2,3). Inference with inequality-based evidence e is, however, much more involved except for the limiting case of point assignments.

Denote by  X  ( e ) = x | b  X  A x  X  c the con-strained domain of x defined by the evidence e . Now we need to specify and sample from the constrained distribution P ( x , h | e ) defined on  X  ( e ). Sampling P ( h | x ) remains unchanged, and in what follows we focus on sampling from P ( x | h , e ). 4.1. Inference under Boxed Constraints
For boxed constraints (Section 3.1), due to the con-ditional independence, we still enjoy the factorisation P ( x | h , e ) = Q i P ( x i | h , e ). We further have where  X (  X  | h ) is the normal cumulative distribution function of P ( x i | h ). Now P ( x i | h , e ) is a trun-cated normal distribution, from which we can sample using the simple rejection method, or more advanced methods such as those in (Robert, 1995). 4.2. Inference under Inequality Constraints
For general inequality constraints (Section 3.2), the input variables are interdependent due to the linear transform A . However, we can specify the conditional distribution P ( x i | x  X  i , h , e ) (here x  X  i = x \ x realising that where A mi 6 = 0 for m = 1 , 2 ,...,M . In other words, x is conditionally box-constrained given other variables.
This suggests a Gibbs procedure by looping through x 1 ,x 2 ,...,x N . With some abuse of nota-tion, let  X  b mi = b m  X  P j 6 = i A mj x j /A mi and  X  c c summarised as x The min and max operators are needed to handle change in inequality direction with the sign of A mi and the join operator is due to multiple constraints.
For more sophisticated Gibbs procedures, we refer to the work in (Geweke, 1991). 4.3. Estimating the Binary Posteriors
We are often interested in the posteriors P ( h | e ), e.g., for further processing. Unlike the stan-dard RBMs, the binary latent variables here are cou-pled through the unknown Gaussians and thus there are no exact solutions unless the evidences are all point assignments. The MCMC-based techniques de-scribed above offer an approximate estimation by av-eraging the samples n h ( s ) o constraints, mean-field offers an alternative approach which may be numerically faster. In particular, the mean-field updates are recursive: where Q k is the probability of the unit k being acti-vated,  X   X  i is the mean of the normal distribution trun-sity function, and  X ( z ) is normal cumulative distribu-tion function. Interested readers are referred to the Supplement 6 for more details. 4.4. Estimating Probability of Evidence
Given the hidden states h we want to estimate the probability that hidden states generate a particular evidence e
For boxed constraints, analytic solution is available since the Gaussian variables are decoupled, i.e., P ( e i h ) =  X ( c i  X   X  i )  X   X ( b i  X   X  i ), where  X  i =  X  i + P For general inequality constraints, however, these vari-ables are coupled by the inequalities. The general strategy is to sample from P ( x | h ) and compute the portion of samples falling into the constrained domain  X  ( e ). For certain classes of inequalities we can approx-imate the Gaussian by appropriate distributions from which the integration has the closed form. In particu-lar, those inequalities imposed by the categorical and rank evidences can be dealt with by using the extreme value distributions . The integration will give the logit form on distribution of categories and Plackett-Luce distribution of ranks. For details, we refer to the Sup-plement.
Learning is based on maximising the evidence like-lihood where P ( h , x ) is defined in Eq. (1). Let Z ( e ) = P log Z . The gradient w.r.t. the mapping parameter reads
The derivation is left to the Supplement. 5.1. Estimating Data Statistics
The data-dependent statistics E P ( x the data-independent statistics E P ( x tractable to compute in general, and thus approxima-tions are needed.
 Data-dependent statistics. Under the box con-straints, the mean-field technique (Section 4.3) can be employed as follows For general cases, sampling methods are applicable. In particular, we maintain one persistent Markov chain (Younes, 1989; Tieleman, 2008) per data instance and estimate the statistics after a very short run. This would explore the space of the data-dependent distri-bution P ( x , h | e ) by alternating between P ( h | x ) and P ( x | h , e ) using techniques described in Sec-tion 4.
 Data-independent statistics. Mean-field distri-butions are not appropriate for exploring the entire state space because they tend to fit into one mode. One practical solution is based on the idea of Hinton X  X  Contrastive Divergence (CD), where we create another Markov chain on-the-fly starting from the latest state of the clamped chain. This chain will be discarded af-ter each parameter update. This is particular useful when the models are instance-specific, e.g., in collabo-rative filtering, it is much cheaper to build one model per user, all share the same parameters. If it is not the case, then we can maintain a moderate set of par-allel chains and collect the samples after a short run at every updating step (Younes, 1989; Tieleman, 2008). 5.2. Learning the Box Boundaries
In the case of boxed constraints, sometimes it is helpful to learn the boundaries  X  b i ,c i  X  themselves. The gradient of the log-likelihood w.r.t. the lower bound-aries reads MCMC procedure running on the data-dependent dis-tribution P ( x , h | e ). Similarly we would have the gradient w.r.t. the upper boundaries:
In this section, we describe applications of the TBM for three realistic domains, namely handwritten digit recognition, collaborative filtering and worldwide sur-vey analysis. Before going to the details, let us first address key implementation issues (see Supplement for more details).

One observed difficulty in training the TBM is that the hidden samples can get stuck in one of the two ends and thus learning cannot progress. The reasons might be the large mapping parameters or the unbounded na-ture of the underlying Gaussian variables, which can saturate the hidden units. We can control the norm of the mapping parameters, either by using the stan-dard ` 2 -norm regularisation, or by rescaling the norm of the parameter vector for each hidden unit. To deal with the non-boundedness of the Gaussian variables, then we can restrict their range, making the model bounded.

Another effective solution is to impose a constraint on the posteriors by adding the regularising term to the log-likelihood, e.g.,  X  ( where  X   X  (0 , 1) is the expected probability that a hid-den unit will turn on given the evidence and  X  &gt; 0 is the regularisation weight. Maximising this quantity is essentially minimising the Kullback-Leibler divergence between the expected posteriors and the true posteri-ors. In our experiments, we found  X   X  (0 . 1 , 0 . 3) and  X   X  (0 . 1 , 1) gave satisfying results.

The main technical issue is that P ( h 1 k | e ) does not have a simple form due to the integration over all the constrained Gaussian variables. Approximation is thus needed. The use of mean-field methods will lead to the simple sigmoid form, but it is only applicable for boxed constraints since it breaks down deterministic constraints among variables (Section 4.3). However, we can estimate the  X  X ean X  truncated Gaussian  X   X  i by averaging the recent samples of the Gaussian variables in the data-dependent phase.

Once these safeguards are in place, learning can greatly benefit from quite large learning rate and small batches as it appears to quickly get the samples out off the local energy traps by significantly distorting the energy landscape. Depending on the problem sizes, we vary the batch sizes in the range [100 , 1000]. 6.1. Probit RBM for Handwritten Digits
We use the name Probit RBM to denote the special case of TBM where the observations are binary (i.e., boxed constraints, see Section 3.1). The threshold  X  i for each visible unit i is chosen so that under the zero mean, the probability of generating a binary evidence equals the empirical probability, i.e., 1  X   X (  X  i ) =  X  e and thus  X  i =  X   X  1 (1  X   X  e i ). Since any mismatch in thresholds can be corrected by shifting the correspond-ing biases, we do not need to update the thresholds further.

We report here the result of the mean-field method for computing data-dependent statistics, which are av-eraged over a random batch of 500 images. For the data-independent statistics, 500 persistent chains are run in parallel with samples collected after every 5 Gibbs steps. The sparsity level  X  is set to 0 . 3 and the sparseness weight  X  is set to 0 . 5. Once the model has been learned, mean-field is used to estimate the hid-den posteriors. Typically this mean-field is quite fast as it converges in a few steps.
 We take the data from MNIST and binarize the im-ages using a mid-intensity threshold. The learned rep-resentation is shown in Figure 2. Most digits are well separated in 2D except for digits 4 and 9. The learned representation can be used for classifications, e.g., by feeding to the multiclass logistic classifier. For 500 hid-den units, the Probit RBM achieves the error rate of 3 . 28%, comparable with those obtained by the RBM trained with CD-1 (3 . 02%), and much better than the raw pixels (8 . 46%). The features discovered by the Probit RBM and RBM with CD-1 are very different (Figure 1), and this is expected because they operate on different input representations. The energy surface learned by the Probit RBM is smooth enough to allow efficient exploration of modes, as shown in Figure 3. 6.2. Rank Evidences for Collaborative
In collaborative filtering, one of the main goals is to produce a personalized ranked list of items. Until very recently, the majority in the area, on the other hand, focused on predicting the ratings, which are then used for ranking items. It can be arguably more efficient to learn a rank model directly instead of going through the intermediate steps.

We build one TBM for ranking with ties (i.e., in-equality constraints, see Section 3.2) per user due to the variation in item choices but all the TBMs share the same parameter set. The handling of ties is nec-essary because during training, many items share the same rating. Unseen items are simply not accounted for in each model: We only need to compare the utili-ties between the items seen by each person. The result is that the models are very sparse and fast to run. For the data-dependent statistics, we maintain one Markov chain per user. Since there is no single model for all data instances, the data-independent statistics can-not be estimated from a small set of Markov chains. Rather we also maintain a data-independent chain per data instance, which can be persistent on their own, or restarted from the data-dependent chains after every Item popularity 0.587 0.560 0.680 0.835 parameter updating step. The latter case, which is re-ported here, is in the spirit of the Hinton X  X  Contrastive Divergence, where the data-independent chain is just a few steps away from the data-dependent chain.
Once the model has been trained, the hidden poste-rior vector  X  h =  X  h 1 ,  X  h 2 ,...,  X  h K , where  X  h k 1 | e u ), is used as the new representation of the tastes of user u . The rank of unseen movies is the mode of the distribution P ( e  X  |  X  h ), where e  X  are the rank-based evidences (see Section 4.4). For fast computation of P ( e  X  |  X  h ), we approximate the Gaussian by a Gumbel distribution, which leads to a simple way of ranking movies using the mean  X  X tility X   X  ui =  X  i + W i  X   X  h u for user u (see the Supplement for more details).

The data used in this experiment is the MovieLens, which contains 1M ratings by approximately 6K users on 4K movies. To encourage diversity in the rank lists, we remove the top 10% most popular movies. We then remove users with less than 30 ratings on the remain-ing movies. The most recently rated 10 movies per user are held out for testing, the next most recent 5 movies are used for tuning hyper-parameters, and the rest for training.

For comparison, we implement a simple baseline us-ing item popularity for ranking, and thus offering a naive non-personalized solution. For personalized al-ternatives, we implement two recent rank-based ma-trix factorisation methods, namely ListRank.MF (Shi et al., 2010) and PMOP (Truyen et al., 2011). Two ranking metrics from the information retrieval liter-ature are used: the ERR (Chapelle et al., 2009) and the NDCG@T (J  X arvelin and Kek  X al  X ainen, 2002). These metrics place more emphasis on the top-ranked items. Table 1 reports the movie ranking results on test sub-set (each user is presented with a ranked list of unseen movies), demonstrating that the TBM is a clear win-ner in all metrics. 6.3. Mixed Evidences for World Attitude
Finally, we demonstrate the TBM on mixed evi-dences. The data is from the survey analysis domain, which mostly consists of multiple questions of different natures such as basic facts (e.g., ages and genders) and opinions (e.g., binary choices, single choices, multi-ple choices, ordinal judgments, preferences and ranks). The standard approach to deal with such heterogene-ity is to perform the so-called  X  X oding X , which converts types into some numerical representations (e.g., ordi-nal scales into stars, ranks into multiple pairwise com-parisons) so that standard processing tools can handle. However, this coding process breaks the structure in the data and thus significant information will be lost. Thus our TBM offers a scalable and generic machin-ery to process the data in its native format and then convert the mixed types into a more homogeneous pos-terior vector.

We use the global attitude survey dataset collected by the PewResearch Centre 7 . The survey was con-ducted on 24 , 717 people from 24 countries during the period of March 17  X  April 21, 2008 on a variety of topics concerning people X  X  life, opinions on issues in their countries and around the world as well as future expectations. There are 52 binary, 124 categorical (of variable category sizes), 3 continuous, 165 ordinal (of variable level sizes) question types.
 Like the case of collaborative filtering, we build one TBM per respondent due to the variation in questions and answers but all the TBMs share the same pa-rameter set. Unanswered/inappropriate questions are ignored. For each respondent, we maintain 2 persis-tent and non-interacting Markov chains for the data-dependent statistics and the data-independent statis-tics, respectively.

Figure 4 shows the 2D distribution of respondents from 24 countries obtained by feeding the posteriors to the t-SNE (van der Maaten and Hinton, 2008) (here no explicit information of countries is used). It is interest-ing to see the cultural/social clustering and gaps be-tween countries as opposed to the geographical distri-bution (e.g., between Indonesia and Egypt, Australia and UK and the relative separation of the China, Pak-istan, Turkey and the US from the rest). To predict the 24 countries, we feed the posteriors into the stan-dard multiclass logistic regression and achieve an error rate of 0 . 49%, suggesting that the TBM has captured the intrastate regularities and separated the interstate variations well.
Latent multivariate Gaussian variables have been widely studied in statistical analysis, initially to model correlated binary data 8 (Ashford and Sowden, 1970; Chib and Greenberg, 1998) then now used for a va-riety of data types such as ordered categories (Kot-tas et al., 2005), unordered categories (Zhang et al., 2008), and the mixture of types (Dunson and Her-ring, 2005) . Learning with the underlying Gaussian model is notoriously difficult for large-scale setting: in-dependent sampling costs cubic time due to the need of inverting the covariance matrix, while MCMC tech-niques such as Gibbs sampling can be very slow if the graph is dense and the interactions between variables are strong. This can be partly overcome by adding one more layer of latent variables as in factor analy-sis (Wedel and Kamakura, 2001; Khan et al., 2010) and probabilistic principle component analysis (Tip-ping and Bishop, 1999). The main difference from our TBM is that those models are directed with continuous factors while ours is undirected with binary factors.
Gaussian RBMs have been used for modelling con-tinuous data such as visual features (Hinton and Salakhutdinov, 2006), where the evidences are the value assignments, and thus a limiting case of our ev-idence system. Some restrictions to the continuous Boltzmann machines have been studied: In (Downs et al., 2000), Gaussian variables are assumed to be non-negative, and in (Yasuda and Tanaka, 2007), continuous variables are bounded. However, we do not make these restrictions on the model but rather placing restrictions during the training phase only. GRBMs that handle ordinal evidences have been stud-ied in (Tran et al., 2012), which is an instance of the boxed-constraints in our TBM.
Since the underlying variables of the TBM are Gaus-sian, various extensions can be made without much difficulty. For example, direct correlations among vari-ables, regardless of their types , can be readily mod-elled by introducing the non-identity covariance ma-trix (Ranzato and Hinton, 2010). This is clearly a good choice for image modelling since nearby pixels are strongly correlated. Another situation is when the input units are associated with their own attributes. Each unit can be extended naturally by adding a lin-ear combination of attributes to the mean structure of the Gaussian.

The additive nature of the mean-structure allows the natural extension to matrix modelling (e.g., see (Truyen et al., 2009; Tran et al., 2012)). That is, we do not distinguish the role of rows and columns, and thus each row and column can be modelled using their own hidden units (the row parameters and columns parameters are different). Conditioned on the row-based hidden units, we return to the standard TBM for column vectors. Inversely, conditioned on the column-based hidden units, we have the TBM for row vectors.
To sum up, we have proposed a generic class of mod-els called Thurstonian Boltzmann machine (TBM) to unify many type-specific modelling problems and gen-eralise them to the general problem of learning from multiple groups of inequalities. Our framework utilises the Gaussian restricted Boltzmann machines, but the Gaussian variables are never observed except for one limiting case. Rather, those variables are subject to in-equality constraints whenever an evidence is observed. Under this representation, the TBM supports a very wide range of evidences, many of which were not possi-ble before in the Boltzmann machine literature, with-out the need to specify type-specific models. In par-ticular, the TBM supports any combination of the point assignments, intervals, censored values, binary, unordered categories, multi-categories, ordered cate-gories, (in)-complete ranks with and without ties.
We demonstrated the TBM on three applications of very different natures, namely handwritten digit recog-nition, collaborative filtering and complex survey anal-ysis. The results are satisfying and the performance is competitive with those obtained by type-specific mod-els.

