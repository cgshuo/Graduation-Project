 Community Question Answering portals like Yahoo! An-swers have recently become a popular method for seeking information online. Users express their information need as questions for which other users generate potential answers. These questions are organized into pre-defined hierarchical categories to facilitate effective answering, hence Question Classification is an important aspect of these systems. In this paper we propose a novel system, CQC, for automat-ically classifying new questions into one of the hierarchical categories. Experiments conducted on large scale real data from Yahoo Answers! show that the proposed techniques are effective and outperform existing methods significantly. H.3.3 [ Information storage and retrieval ]: Information search and retrieval Algorithms, Design, Experimentation, Management Question Classification, Nearest Neighbour, Translation Model
Over the past few years community-based question an-swering (CQA) portals like Naver (www.naver.com), Yahoo! Answers (answers.yahoo.com), Baidu Zhidao (zhidao.baidu.com) and WikiAnswers (wiki.answers.com) have gained a lot of popularity. These portals allow users to both submit ques-tions to be answered and answer questions asked by other user. Due to the open collaborative model of these services, over time a tremendous number of question and answer pairs have been collected, that serves as a valuable source of in-formation.

Most CQA portals organize questions by a predefined set of categories. When submitting a new question, users are required to assign a category label to it. Assigning a correct category label (Question Classification(QC)) plays a very important role in enabling other users to find and answer questions in their area of expertise. These category tags also enhance the future usefulness of the Q&amp;A repository by allowing users to systematically navigate to information of interest. While important, assigning correct category tags is an onerous activity for the user. This is because finding the correct category requires knowledge of the category hier-archy which users might not have. In the following example taken from Yahoo! Answers, the onus is on the user to find the existence and relevance of a category for his question, as all of the listed categories have questions related to impor-tant aspects of the question i.e. tattoos, skin, allergy. Q: How will it effect me if I get a tattoo even though my skin is allergic to alcohol ? 1: [Beauty &amp; Style]  X  [Skin &amp; Body]  X  [Tattoos] 2: [Health]  X  [Diseases &amp; Conditions]  X  [Skin Conditions] 3: [Health]  X  [Diseases &amp; Conditions]  X  [Allergies] 4: [Beauty &amp; Style]  X  [Skin &amp; Bod]  X  [Other-Skin &amp; Body] 5: [Arts &amp; Humanities]  X  [Visual Arts]  X  [Drawing &amp; Illus-tration]
Yahoo! Answers has a 3 level category structure with roughly 26 top-level and 1065 leaf level categories. These categories span a variety of topics such as Internet , Health , Politics and Science . Each question in Yahoo! Answers is associ-ated with one leaf category. Moreover, some categories like [Style]  X  [Skin &amp; Body]  X  [Tattoos] are topical where any sub-set of questions generally show a high degree of similarity to any other subset. On the other hand categories like [Beauty &amp; Style]  X  [Skin &amp; Bod]  X  [Other-Skin &amp; Body] (hence forth referred to as a general class Others ) are quite general and clearly created for sake of completeness. In addition to be-ing hierarchical, category structure of a CQA portal is fairly static. On the contrary, topics discussed by users evolve over time. This results in existing categories becoming less co-herent (more generic) in terms of the topics discussed. This is an important challenge that any QC system has to deal with. CQA portals try to handle this using special cate-gories, Others , meant for classifying questions that don X  X  fit in the hierarchy. But this causes a new problem where the Others category is incorrectly used by users for categorizing questions that would normally fit in an existing more spe-cific category. In Yahoo! Answers we found that &gt; 17% of the top categories in terms of the number of question asked, were of type Others , while they are only 10% of the overall leaf level category count.

At a high level, QC is a text classification task [5]. How-ever, the fact that questions are short makes the task par-ticularly challenging. Existing QC approaches mainly deal with the task of classifying TREC style factual questions into a small number of semantic categories. For example [3] clas-sified questions in to two level hierarchy containing 6 coarse classes (ABBREVIATION, ENTITY, DESCRIPTION, HU-MAN, LOCATION and NUMERIC VALUE) and 50 fine classes. Here, the answer type of the question  X  X hat X  X  the best metro-accessible bar in DC for a first date? X  might be [Location]  X  [Others] . In contrast we are interested in classifying questions into a large number of topical classes ( &gt; 1000), we might, for example, classify the question above into the category [Dining Out]  X  [United States]  X  [Washington, D.C.] .
The QC task is to assign a category tag t user to a new user question q user . For the QC task, we use a nearest neighbor approach to find a subset of most relevant ques-tions ( Q k  X  sim ) and hence related categories for q user then predict t user using Q k  X  sim . Since classification of q is an online task, the classification model learnt from Q should be computationally efficient to train. Motivation for our neighborhood based approach is 2 fold. Firstly, several studies of human perception [4] have argued that represen-tating categories by similarity to prototypes is better than defining them as lists of features in presence of large num-ber of categories describing potentially related objects. Also as compared to a small set of categories, classification on a large-scale hierarchy is known to be very expensive and not up to the mark for many target categories, especially the ones that are related to each other.

Figure 1 outlines the architecture of CQC. It comprises of an offline component and an online component. The offline component processes the CQA archive and creates models to retrieve k questions ( Q k  X  sim ) that are most similar to q Prior work [6] has shown that translation based retrieval models work well for finding Q k  X  sim . So our CQC system uses them to calculate sim ( q,q user ) 1 . The online compo-nent employs a nearest neighbor classifier based on Q k  X  sim and returns the category t user corresponding to q user as de-scribed below. sim ret ( q,q user ) is sim ( q,q user ), as calculated by retrieval models
We use the class labels of the k most similar neighbors ( Q k  X  sim ) according to a similarity function ( sim ( q,q user to predict the class of a user question q user . The classes of these neighbors q  X  Q k  X  sim are weighted using a simi-larity function sim ( q,q user ) that captures the similarity of a neighbor q to q user and a function CatRel ( q,t ) that cap-tures relationship of the neighbor q to the category tag t being scored. Thus, our scoring function is: score ( q user ,t ) =  X  ( t )  X  and our decision function is: Equation 1 indicates that contribution of a neighbor q i to-wards a category t would be high if it is highly similar to q user and also strongly coherent with t . This tackles the coherency related problem mentioned in Section 1. To al-leviate effect of inductive bias,  X  ( . ) weighs each category t by its size relative to the overall size of the collection C as defined by Equation 3. Here parameter  X  controls the effect of category size on the overall score. Based on how we define sim ( q,q user ) we can have several configurations of Equation 2. We propose UNIFORM and RET-SCORE as two possible ways to define sim ( q,q user ). UNIFORM weighs each neighbor equally while in RET-SCORE neighbors are weighted by the same similarity score [ sim ret ( q,q user )] used to retrieve the top k questions Q Formal definitions are given in Equation 4 and Equation 5, respectively. Similarly based on definition of CatRel ( q,t i ) in Equation 6 and Equation 7 henceforth referred to as INDICATOR, CAT-WEIGHTED we have two more configurations of Equa-tion 2.
 In INDICATOR configuration, each neighbor q contributes to the score of only one category based on its category tag in our archive. This makes the standard assumption that each question can belong to only one category and is correctly tagged in our archive. This is not true for many questions in Yahoo! Answers. In fact, as mentioned earlier, categories like Others contain lots of questions that can easily be as-signed to other categories. CAT-WEIGHTED handles these issues by weighing each neighbor by the posterior probability P ( t | q ) using a Naive-Bayes model. P ( t | q ) can be computed as given in Equation 8, where d j is term frequency of word w j in question q and N is the vocabulary size.
 P ( w | t ) can be estimated offline from our CQA archive using Equation 9. P ml ( w | C ) is the probability that the term w is generated from the collection C .

In using a K-NN classifier, the main challenge is in com-ing up with a suitable similarity function sim ret ( q,q user retrieve Q k  X  sim , the set of k most similar questions. In par-ticular, bridging lexical gaps between q user and other ques-tions in the Q&amp;A archives has been a major challenge. We use translation models (TRLM)[6] to solve this problem.
We crawled a dataset of  X  6 million questions and answers from Yahoo! Answers spanning all the leaf level categories. This was the master repository used for all our experiments. Questions in the repository spanned 26 categories at the top level and &gt; 1065 categories at the leaf level. On an average we had  X  6000 questions per category. A large percentage (  X  40%) of the categories in the Yahoo! Answers dataset had fewer than 6000 questions available.

Table 1 shows empirical evaluation results of our retrieval system for finding Q k  X  sim . TRLM outperforms other mod-els in all the measures. In our retrieval experiments we used 228 queries. For each query q user we considered the top 25 results and human annotators were asked to mark if the retrieved questions Q 20  X  sim were relevant to q user .
In this section, we discuss results of our CQC system de-scribed earlier in Section 2.2. For the purpose of training the baseline SVM models, we sampled around 1 million ex-amples spanning all the classes. For a fair comparison, all the models listed in Table 1 used this sampled data as their base corpus ( C ) for retrieval. In addition to this, we selected around 0.1 million questions covering all the categories as our test data. Category tags for the sampled question were used as the ground truth.
We use a flat multi-class classification(F-SVM) and a hier-archical classification(H-SVM) [1] using SVM as our baseline methods. For both the baselines, we have used a linear ker-nel and linear SVM [2], one v/s rest setting. For evaluating QC performance we use the standard evaluations metrics i.e. macro-averaged F 1 ( macro -F 1 ) and micro-averaged F ( micro -F 1 )
Of the four possible configurations, following are the 2 basic configurations of our CQC system in which all the neighbors are weighed equally independent of their similarity UNIFORM + INDICATOR [ U + I ]: Each neighbor gives UNIFORM + CAT-WEIGHTED [ U + C ]: Each neigh-In the remaining 2 configurations, each neighbor q , is weighed proportional to sim ( q,q user ).
 RET-SCORE + INDICATOR [ R + I ]: Each neighbor RET-SCORE + CAT-WEIGHTED [ R + C ]: Each neigh-One instance of each of the above configurations (henceforth denoted with subscript  X  X ase X  2 ) where VSM is used to re-trieve top-k similar questions, is the simplest configuration of our system. These configurations are directly comparable to the two SVM baselines as all of them operate in the same feature space (words in a question).
We use cross validation to estimate a good value of k (number of neighbors i.e | Q k  X  sim | ). Here we experiment with various k values and the value achieving the highest macro -F 1 is selected as the optimal value. We found that choosing k in the range of 10 &lt; k &lt; 15 was good for all con-figuration of our CQC system. Similarly  X  used in Equation 9 was set to 0 . 15 and [0 . 4 , 0 . 65] serves as a good range for  X  . Figure 2: Comparison of basic CQC configurations ( k = 10 ) with SVM baselines. e.g. [ U + I ] Base
Figure 2 compares the performance of SVM baselines against [ U + I ] Base . Parameter values used for [ U + I ]  X  = 0 . 6 and k = 10. We see that our basic CQC config-uration outperforms F-SVM by 38% and H-SVM by 21% in micro -F 1 . Similar behaviour is also observed for macro -F This validates our basic hypothesis that neighborhood based classification strategy helps in the presence of large category hierarchy with several related classes. Next, we compare the remaining 3 basic configurations to [ U + I ] Base for different values of k . As shown in Figure 3, all 3 of them perform bet-ter than [ U + I ] Base . [ R + C ] Base outperforms [ U + I ] in both the measures by  X  7%. Similar relative performance is observed for macro -F 1 . All the configurations give the best performance for k = 10. Performance gradually drops after that due to the introduction of questions from top-ically incoherent categories. Until now we only looked at Figure 3: micro -F 1 comparison of CQC configura-tions for different values of k .
 CQC performance using a VSM model (only questions) for retrieval. Figure 4 outlines micro -F 1 performance of CQC configurations using retrieval models presented in Table 1. Among the CQC configuration classes, [ R + C ] outperforms others for all values of k . [ R + C ] with R=(TRLM) is the best overall configuration and performs  X  12.25% better than [ U + I ] Base and  X  36.65% better than H-SVM.

The key pattern observed across all the results is that improvements in retrieval performance results in improved classification performance of our CQC system. We did sta-tistical significance tests (t-Test) for comparing the perfor-mance (already reported in this section) of important con-figurations like F-SVM v/s H-SVM, H-SVM v/s [ U + I ] Base [ U + I ] Base v/s [ R + C ] Base and [ R + C ] Base v/s [ R + C ] with R=(VSM | OKAPI | TRLM). All these differences are signifi-cant with P-value &lt; 0.001%.
 Figure 4: micro -F 1 comparison of CQC configura-tions with [ U + I ] Base having value of 0 . 538 as base-line.

As mentioned earlier, since classification of q user is an on-line task, it is highly desirable that the classification model learnt from Q k  X  sim take minimal training time. Hence clas-sifiers like SVM are not recommended as its training time Figure 5: Comparison of [ R + C ] Base with O-SVM for different values of k . may prevent us from recommending predicted category la-bels back to user in a timely manner. For the sake of com-pleteness of analysis, we also compare flat multiclass SVM classifier (O-SVM) trained on categories ( t k  X  sim ) covered by Q k  X  sim with our [ R + C ] Base configuration. As shown in Fig-ure 5, for small values of k it performs similar to [ R + C ] As the value of k increases, beyond a point it becomes much worse due to the increase in size of t k  X  sim .
In this paper, we propose a novel QC system to clas-sify a new question into one of the hierarchical categories of a CQA portal. We used translation based models to re-trieve lexically and semantically similar questions. We use a neighborhood based classifier on the retrieved questions to classify new questions. CQC configuration of [ R + C ] with R=[TRLM] is the best overall configuration and per-forms  X  48% better than hierarchical SVM based baseline system. [1] S. Dumais and H. Chen. Hierarchical classification of [2] T. Joachims. Training linear svms in linear time. In [3] X. Li and D. Roth. Learning question classifiers. In [4] E. H. Rosch. Natural categories. Cognitive Psychology , [5] F. Sebastiani. Machine learning in automated text [6] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for
