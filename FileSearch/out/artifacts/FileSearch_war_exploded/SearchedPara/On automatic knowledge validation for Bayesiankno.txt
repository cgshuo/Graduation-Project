 1. Introduction
Typically, the development of a knowledge-base starts with the knowledge acquisition stage, in which the regarding its model of knowledge representation. Once the knowledge base is verified to be free from must infer the same answer as the experts X  expected one associated with the test case.
Originally, the process of knowledge validation only involves evaluating the knowledge base X  X  ability to reach correct conclusions [7,24] . However, the purpose of knowledge validation has gone beyond evalua-has to detect errors in the knowledge base and correct the knowledge base so that all test cases provided [13,21] .

Our focus in this work is on knowledge validation for knowledge bases that handle uncertainty. In partic-ular, we examine Bayesian Knowledge Bases [36] . Briefly, Bayesian Knowledge Bases (BKBs) are a general-ization of the well-known probabilistic model called Bayesian Networks [25] . The main advantages of BKBs over Bayesian Networks (BNs) are the ability to handle information incompleteness and to represent cyclic dependencies among random variables. In addition, a BKB semantically represents uncertain knowledge in pointed out in [37] , encing [5] , adversarial decision making [35] , and data mining [3] .
The study of BKB validation is part of the ongoing PESKI 1 grated environment of intelligent tools for building expert systems whose knowledge bases are modeled in terms of BKBs. Each tool in PESKI [6,15,31,34,30] performs the task of some phase during knowledge engi-according to the given test case set. be generally applicable.
 is to define and detect the contradiction in a test case set that will be used for validation.
Another problem of BKB validation that has also been pointed out in [34,30] is the thrashing phenomenon,
Of course, the knowledge base will end up oscillating between parameter values. The fundamental questions existing approach to BKB validation [34,30] had not identified the root cause of thrashing.
In this work, we provide a formal and rigorous approach to validation for BKBs, in which we analyze mentally changing the structure of the BKB and adjusting parameters in the BKB. Updating the BKB struc-ture prevents thrashing from occurring during parameter adjustment. Our method is guaranteed to properly validate a wide variety of BKBs and test case sets.

This paper is organized as follows: We briefly present some background on BKBs in the following section respectively. 2. Background
In this section, we will review the Bayesian Knowledge-Base model. More specific details on this model as well as discussion on its semantic soundness and applications can be found in [36,37,27] . 2.1. Representing uncertain knowledge with BKBs
Bayesian Knowledge-Bases (abbrev. BKBs) [36] are a probabilistic model that extends Bayesian Networks referred to as a correlation-graph and is formally defined in Definition 2.1 . supports a .
 Notation For each S-node q in a correlation-graph G =( I [ S , E ), we denote Tail ing I-nodes (or parent nodes) to q , i.e., Tail G ( q )={ a 2 I j a ! q 2 E }, and Head instead of ( a , b ) in order to represent the link from a to b in directed graphs.
The parameters of a BKB are given by the conditional probability values associated with its S-nodes. In the weight of q and serves as the actual conditional probability value P ( Head ond, BKBs do not allow two conditional probabilities of the form P ( A = a j I
I are not mutually exclusive. Two sets of I-nodes, I 1 and I node R = v 1 in I 1 and an I-node R = v 2 in I 2 for which v { A =1, B = 1} and { A =0, B =1, C = 0} are mutually exclusive. Two I-node sets which are not mutually each S-node, to ensure that they are consistent probabilistically [36] , any set of S-nodes { q less than or equal to 1. For example, for q 1 with Head G with Head G ( q 2 )={ R = Medium} and Tail G ( q 2 )={ B =1, C = 0}, and q Tail G ( q 3 )={ D = 0}, w ( q 1 )+ w ( q 2 )+ w ( q 3 ) 6 1.
 w is a function from S to [0, 1] such that the following conditions hold:  X  For any S-node q 2 S , Tail G ( q ) contains at most one instantiation of each random variable.  X  For any two distinct S-nodes q 1 , q 2 2 S that support the same I-node, then Tail mutually exclusive. Furthermore, such S-nodes q 1 and q 2  X  For any Q S such that (i) Head G ( q 1 ) and Head G ( q
Tail G ( q 2 ) are not mutually exclusive for all q 1 and q
For example, in the correlation-graph given in Fig. 1 , three S-nodes s sive since they all support the I-node ( C = 1), and the three I-node sets Tail Tail G ( s 8 )={ A =2, B = 0}, and Tail G ( s 9 )={ B = 1} are pairwise mutually exclusive. tional probability rule (CPR) [27] of the form Tail G  X  q  X ) omitted from the CPRs when we concentrate only on the structure of the BKB. This means the correla-tion-graph G =( I [ S , E ) of a BKB can be specified by the set of rules { Tail
As an example, Fig. 2 presents a sample BKB fragment for fresh water aquarium management in terms of in Fig. 3 . The graph structure of our sample BKB is also an example of cyclic dependence among random in Bayesian Networks, but is allowed in our BKB framework. As a final note, BKBs also permit directed cycles in the correlation-graph (see [36] ). 2.2. Reasoning with BKBs
In our BKB setting, we define a state as an I-node set which contains at most one I-node of each random dom variables in the domain for which the BKB represents. For simplicity, from now on, we fix our knowl-edge domain to consist of the random variables R 1 , R 2 , ... , R is a set of I-nodes of the form ( R 1 = v 1 , R 2 = v 2 , ... , R the most probable complete state that exists and contains the given evidence. Information necessary to inference , whose I-node set coincides with the state.

Definition 2.3. Let K =( G , w ) be a BKB with correlation-graph G =( I [ S , E ). A subgraph s =( I
G is called an inference over K if (i) s is acyclic. (ii) ( Well-supported ) " a 2 I 0 , $ q 2 S 0 , q ! a 2 E (iii) ( Well-founded ) " q 2 S 0 , Tail s ( q )= Tail G ( q ). (iv) ( Well-defined ) " q 2 S 0 , Head s ( q )= Head G ( q ). (v) I 0 is a state. Thus, I 0 is referred to as the state of the inference. Furthermore, if I
The well-founded and well-defined properties mean that each conditional probability rule in an inference ditions (i), (ii) and (v) in the definition of inference.
 plete state { A =2, B =0, C =1, D = 0}.
 nodes in the inference, that is, P  X  s  X  X  P ( s ) is also the joint probability of the state of inference s [36] .
Remark. Since a BKB is supposed to represent incomplete knowledge, not every state can be associated with
Reasoning with a BKB can be implemented in numerous ways from A ming [36,27] . The computational complexity of reasoning with BKBs has been shown to be NP-hard [27] . with worst-case polynomial time reasoning as described in [36] . 3. BKB validation The goal of validation for BKBs is to make necessary changes to the given BKB such that the modified the BKB, and output the solution which is the most probable complete state given the evidence provided. needs the answer for random variable C . Suppose the most probable complete state for the evidence ( A =1, B =0) is ( A =1, B =0, C =0, D =1, E = 1), then the answer displayed to the user will be C =0.
The whole solution is displayed to the user only when he or she needs the explanation of why the system disease as a part of his  X  X  X ost probable complete state X  X  of health when he feels chest pains. hand, when the doctor consults with a colleague about the case, the underlying link structure behind his/ answer R = v for random variable R if evidence e is used as a query to the inference engine. For example, a test case for a BKB representing a medical domain may have the set of I-nodes {chest-pain = true, node expected answers ( R 1 = v 1 , ... , R k = v k ) is equivalent to the set of test cases h e , R evidence e .

Fig. 1 . Note that in the case of inference { C =0, s 6 , A =0, s greater than the probability of any incorrect inference of t .
 s is called a correct solution (over K ) for the test case t .
 state relevant to t , and, as a result, there will be no BKB satisfying t . 4. Validation for BKBs by refining parameters to present here is to refine only parameters of a given BKB so that the BKB with updated parameters will condition for a test case set to be contradiction-free.

Before presenting the algorithm, we make another assumption that the initial BKB, as an input for Algo-such a case, there is no way to modify the weights of S-nodes to make the BKB satisfy that test case.
Algorithm 1 ( for validating BKB by refining parameters ) test case in T .  X  Output: ABKB K 0 =( G , w 0 ) and a subset of T that K 0 satisfies. 1. Initialize G 0 as an empty graph, and set Fail  X ; . 2. For i =1to n (a) Find a correct inference C i (over K )of t i with maximal probability. (b) Let 3 G i = G i 1 [ C i . (c) If C i is not a correct solution over K for t i , then 3. For every S-node q in G , set w 0 ( q )= w ( q ). 4. Return K 0 =( G , w 0 ) and T n Fail .
 t bold are values obtained after validating t 1 . The only correct inference C
D 1 of t 1 are depicted in part (a) and part (b) of Fig. 5 , respectively. So G joint probability of these inferences are P ( C 1 ) = .152  X  .750  X  .388  X  .485 .021 and P ( D .750  X  .388  X  .721 .032. A simple comparison ( P ( C 1 )&lt; M for t 1 . Since G 1 does not contain any incorrect inference of t Instead, the weights of all S-nodes but those in G 1 are reduced by a multiplicative factor of is chosen to be 0.005. Once these changes are made, the inference C case t 1 .

Recall that Algorithm 1 adjusts only the weights of S-nodes in the BKB, keeping the BKB structure set of S-nodes to be updated during the validating of a test case t
S-node except for those in the chosen correct inference C remains a correct solution for the test case t i since the weights of S-nodes in C
G , we do not guarantee that test case t i is validated successfully and we place it into the set lemma, for which the proof can be found in Appendix : Lemma 4.1. The BKB K 0 returned by Algorithm 1 satisfies every test case that is not in will be successfully validated, i.e. the set FAIL is empty when the algorithm terminates.
Clearly, there are computational complexity issues with the algorithm above since it is NP-hard in the set? The solution to these questions are provided in the following section. 2.2 ). Thus, for any maximal set Q of S-nodes satisfying the third condition, a resolve the problem. 5. Consistency of test case sets same evidences but with mutually exclusive expected answers. For example, t t it. Throughout this section, we will refer to test case set T ={ t satisfied by a BKB K . This means there exists a correct solution s given correct solutions. Precisely, G T =( T , E T ), where E Lemma 5.1. The graph G T = (T, E T ) is acyclic.
 solution for t i must be greater than that of the correct solution for t ing structures of the correct solutions used in the graph G rect solutions to tie the test cases together.

Let h j be the state of the correct solution s j for test case t h is a complete state, it is mutually exclusive with the answer Ans ( t
Ans ( t i ) 6 2 h j . Thus, the correct solution s j is an incorrect inference of t by definition of incorrect inference. So if we replace the condition defining a link from t by the condition that Evi ( t i ) h j and Ans ( t i ) 6 2 h over the test case set. A collection 4 of complete states { h be referred to as a relevant collection of complete states for the test case set T . over T with respect to C is defined as a graph G T ; C  X  X  T ; E cases, t 1 , ... , t 5 , in both tables is a topological sort of those test cases with respect to G corollary of Lemma 5.1 : complete states for T such that the consistency graph G T ; C we will give this condition a name: complete states for T such that the consistency graph over T w.r.t. C is acyclic. isfying it?
Now suppose the test case set T is consistent. Let C  X f h t , t 2 , ... , t n is a topological order of test cases in T with respect to G of random variable R k , i.e. r k ={ R k = v j v is a value of R of a BKB satisfying the given consistent test case set.

Lemma 5.2. Let K = (G, x ) be a BKB with G = (I [ S, E). If there is a random variable R h n r  X  is well-represented in K, then there exists a BKB K S S 0 ,E E 0 and K 0 satisfies T.
 t we can choose a correct inference C i of t i such that the accumulative correlation-graph G obtain a new BKB which satisfies the given test case set. So to prove Lemma 5.2 , let s state is h i n r  X  , we build each correct inference C i of test case t the I-node ( R  X  = v i ) which is the instantiation of R  X  q supports ( R  X  = v i ) and that Tail G ( q ) is not mutually exclusive with I rect inference C i (in part (b)) is obtained from inference s
Fig. 1 , and complete state h i ={ A =2, B =0, C =1, D = 0}. Correlation-graph G building those inferences C i  X  X . Then, using Algorithm 1 with the specified order t constructed correct inferences C i  X  X , we can assign the weights of S-nodes in G rect solution for test case t i .
 Theorem 5.3. There exists a BKB satisfying the set T of test cases if and only if T is consistent. Theorem 5.4 .
 Theorem 5.4. Deciding consistency of test case sets is NP-complete.
 the corresponding consistency graph is acyclic. To complete the proof for Theorem 5.4 , we will show that the test case set.

Proposition 5.1. For any two test case t i ,t j in T, the link t and only if (a) Evi(t i ) Evi(t j ) [ { Ans(t j ) } , and (b) Ans(t i ) is mutually exclusive with Evi(t j ) [ { Ans(t
Proposition 5.2. For any two test case t i ,t j in T, the link t and only if (c) Evi(t i ) is compatible with Evi(t j ) [ { Ans(t j ) } , and (d) Ans(t i ) 6 2 Evi(t j ) [ { Ans(t j ) } .

G
Similarly, the union of all consistency graphs over T is the graph on T defined as G sists of all links t i ! t j that satisfies the two conditions (c) and (d) in Proposition 5.2 .
G lines are in the consistency graph G T ; C .
 For any consistency graph G T ; C  X  X  T ; E T ; C  X  over T , we have E 0 the two lower graph G 0 T and the upper bound graph G 1 T cases: (a) If G 0 T is cyclic, then T is not consistent. (b) If G 1 T is acyclic, then T is consistent.
 before actually running the validation algorithm. 6. Thrashing in BKB validation thrashing, which has been pointed out in previous work on BKB validation [30,34] . Generally, thrashing such inconsistent inequalities is w ( q 1 )&gt; w ( q 2 )and w ( q oscillate between two states: one of which satisfies w ( q oscillation of the knowledge base may be discovered during validation. However, the phenomenon of knowl-refinement method, but does not take place with another. On the other hand, the occurrence of thrashing means the knowledge base will oscillate regardless of refinement strategy. validation. 6.1. Conditions leading to thrashing
BKBs. By incompleteness, we mean the lack of an inference graph for some state. However, that statement then thrashing will always occur even when a complete BKB is given. In other words, the inconsistency of the test case set is a sufficient condition for the occurrence of thrashing. heart = true X  X , and the second one with evidence  X  X  X hest-pain = false X  X  and expected answer  X  X  X ongestive-neously: w ( s 5 )&gt; w ( s 6 ) and w ( s 5 )&lt; w ( s 6 the existence of parameter assignments for the BKB.
 formally defined as follows: not exist any weight assignment w 0 for S-nodes in K such that the BKB K assignment for a BKB with respect to a given test case set. 6.2. Resolving thrashing resolve thrashing as no BKB can satisfy an inconsistent test case set. has been built up before and represented in the original BKB. The problem here is how to minimally modify the structure of BKB for validation. More formally, suppose a nonnegative function f ( G for the modification of correlation-graph G that results in correlation-graph G case set. We need to find a BKB K 0 =( G 0 , w 0 ) such that K were a polynomial-time deterministic algorithm for finding such a BKB K polynomial time if there exists a satisfying weight assignment for K with respect to T by comparing G weight assignment may be too difficult, determining the minimal modification in the structure of BKB for BKB.

Our task now is to find a BKB K 0 =( G 0 , w 0 ) such that (i) G 0 =( I [ S 0 , E 0 ), where S S 0 and E E 0 , and (ii) K 0 satisfies T .
 each state h t n r  X  , i.e. the state obtained from the complete state h whether a given state h is well-represented in the BKB K .

The idea of our algorithm is to build up an increasing sequence of well-represented subsets of h . We can always assume that the empty state is well-represented, so our sequence will start with the emptyset. Now in its tail), then we obviously get another inference with larger state. The condition Tail maintain the acyclicity of the new graph. If a is an I-node and q is an S-node with Head that if l 1 l 2 are states, then any l 1 -supported I-node is also l of well-represented subsets of h as follows: The following are examples of such a sequence, where the BKB has the correlation-graph given in Fig. 1 . Example 1. Suppose h ={ A =2, B =0, C =1, D = 0}. Then we have Example 2. Suppose h ={ A =0, B =2, C =0, D = 0}. Then we have same. In other words, there is a  X  X  X topping-point X  X  k such that The stopping-point of the sequence in Example 1 is 3, and that of the sequence in Example 2 is 2.
Lemma 6.1. A state h is well-represented in K if and only if there exists k such that I by recording the S-node q which I k 1  X  h  X  -supports the I-node a whenever a is added to I applying the above algorithm to determine if every state h variable R  X  for which such condition holds, we modify K to obtain K 5.2 . Otherwise, we may use another tool, which may involve human interaction, to make the BKB more com-plete so that it contains an inference for each state h t stages. The first stage is structural modification which is to construct G eliminating thrashing, can be done in polynomial time. However, the second one depends on the time com-class, our approach to validation can be performed efficiently. 7. Related work
Approaches and tools for automatic knowledge validation in the literature can be categorized into two knowledge base. Knowledge validation tools in the first group, e.g. SAVES [38] , KVAT [23] and KJ3 [41] , carry out the simplest of automatic jobs. They only run test cases, compare the outputs against the human experts X  answers, and then produce a certification about the accuracy of the knowledge base. The tasks of error detection and knowledge refinement in this case are loaded onto knowledge engineers. Our approach presented here falls into the second group. Although, automatic validation in this group is supposed to locate and eliminate errors from the knowledge base based on the evaluation result, almost no system a methodology for identifying  X  X  X uilty rules X  X  in rule based systems and refining the system by fixing technique.
 specific techniques of knowledge engineering.

There are a couple of related works in refinement of knowledge bases with uncertainty. Ling and Valtorta random variable, i.e. an I-node.

Finally, with regards to Bayesian Network refinement, Jensen [16] discussed a method for tuning condi-tional probabilities (or parameters) in a Bayesian Network so that given a variable A and an evidence e , desired distributions are involved. For example, suppose we want to tune the parameters so that P ( A close to distribution y 1 and P ( A 2 j e 2 ) is close to distribution y
P ( A 2 j e 2 ) y 2 will not damage the earlier P ( A 1 j e 8. Conclusion
We have opened the black-box of fundamental problems, which directly affect the result of validation for BKBs, including the consistency of test case sets and thrashing. The former is motivated by the ques- X  X  X oes there exist a weight assignment for a given BKB so that it makes the BKB satisfy a given test case set? X  X . For the former problem, a key result we have determined is the necessary and sufficient condition consistent, thrashing still possibly happens because of incorrectness or incompleteness in the BKB struc-sary in order to determine whether or not the structure of the knowledge base need to be changed in validation.
 the paper works efficiently and correctly validates 100% of test cases. Appendix A A.1. Proof of Lemma 4.1
Let w 0 ( q ) denote the initial weight of the S-node q and let w is P i  X  s  X  X 
Suppose test case t i is not put into FAIL , we will show that (i) the correct inference C solution for t i immediately after the validating of t i is done, and that (ii) C when the algorithm terminates.

According to the algorithm, if C i is a correct solution of t
P i 1 ( C i )&gt; P i 1 ( s ) for every incorrect inference s of t in this case, C i is still a correct solution of t i when the validating of t that C i is not a correct solution of t i by the time t i maximal value of P i 1 ( D ) for any incorrect inference D of t S-node q in D , we have w i  X  q  X  X  w i 1  X  q  X  P i 1  X  C i wise. Since t i is not put in FAIL , every incorrect inference of t
G . Hence, multiplying the weights of all S-nodes in D gives
The last inequality is due to P i 1 ( D ) 6 M i . Note that the weights of S-nodes in C
P ( C i )= P i 1 ( C i ). Hence P i ( D )&lt; P i ( C i ). This shows C of t i is done.

To show (ii), observe that the S-nodes in C i will be untouched since t of C i is kept the same since then, i.e. P i ( C i )= P i +1 will ever be increased. Therefore, there is no way that an incorrect solution of t be raised after validating of t i .So C i will be a correct solution of t A.2. Proof of Lemma 5.1
Assume otherwise that G T has a cycle t i For any two test cases t i and t j such that t i ! t j 2 E and s j is an incorrect inference of t i .
 Hence This contradiction implies that G T must be acyclic. h A.3. Proof of Lemma 5.2 ence s i =( I i [ S i , E i ) over K , where I i = h i n r
Let ( R N = v i ) be the I-node of R N that appears in h i
For each i  X  1 ; n ,  X  if there exists an S-node q 2 S supporting ( R N = v i ). and Tail exist two distinct S-nodes q , q 0 2 S such that they both support ( R because of the mutual exclusivity.  X  otherwise, let q i be a new S-node, i.e. q i 6 2 S and q Let G 0 =( I [ S 0 , E 0 ), where
Then K 00 =( G 0 , x 00 ) is a BKB, in which x 00 is a function from S Let C i  X  X  I i [f R N  X  v i g[ S i [f q i g ; E 0 i  X  be the subgraph of G 0 in which
Thus C i is a complete inference over the BKB K 00 that contains h ence of the test case t i " i =1,2, ... , n .
 Let G i  X  ence of t i .

Assume otherwise that G i contains an incorrect inference c of t nodes in c , thus h is a complete state such that
Let a be the instantiation of R N in h , and let q be the S-node in c that supports a . Thus q 2 { q
Then q q j for some j 6 i . Then h j h because q must be well-founded in c . It follows that h = h j &lt; i since ( 2 ) and since Ans ( t i ) 2 h i . Thus t or Evi ( t i ) X h j (by ( 1 )). This contradicts ( 2 ).

Hence G i does not contain any incorrect inference of t i
BKB K 00 =( G 0 , x 00 ), with the order of test cases t 1 2a, we get the resulted BKB K 0 =( G 0 , x 0 ) which satisfies all test cases in T . h A.4. Proof of Theorem 5.3
Theorem 5.3 is taken by simply applying Lemma 5.2 for a BKB with correlation-graph G =( I [ S , E ), in as follows:  X  I is the set of all instantiations of all random variables R  X  S ={ q [ a ] j a 2 I }, where q [ a ] denotes a unique S-node corresponding to a .  X  E ={ q [ a ] ! a j a 2 I }.
 Let C  X f h 1 ; ... ; h n g be a relevant collection of complete states for T such that G i  X  1 ; n , let s i =( I i [ S i , E i ) be a subgraph of G , where  X  I i = h i n r N  X  S i ={ q [ a ] j a 2 I i }  X  E i ={ q [ a ] ! a j a 2 I i } that there exists a BKB satisfying all test cases in T . h A.5. Proof of Theorem 5.4 the SAT problem.
 Let U  X  each clause c i is assumed not to contain both literals x
We specify the test case set T ={ t 0 , t 1 , ... , t n } as follows:  X  t 0 = h; , A = yes i ,  X  t i = h e i , A = no i for all i =1, ... , n , where
Here, R k  X  X  are random variables taking values 0 and 1. Each R the I-node R k = 1 (resp. R k = 0) will represent the event that  X  X  x e represents the event that  X  X  X lause c i is false X  X .

Note that any consistency graph over T contains the set of arcs { t any arc between any two nodes in { t 1 , ... , t n }. It follows that a consistency graph G acyclic if and only if it does not contain any link from some test case t where h 0 is the state relevant to t 0 in the collection of complete states C on which G s ( t i ) 6 2 h 0 " i =1, ... , n ,
Hence, T is consistent if and only if there exists a complete state h does not contains any expected answer e i with i 5 0.
 Consider a complete state h 0 ={ A = yes , R 1 = b 1 , ... , R contain any expected answer e i . Intuitively, the fact that h h  X  X . This gives the rise to a satisfying truth assignment k for U , where k ( x which does not contain any expected answer e i with i 5 0:
Thus, T is consistent if and only U is satisfiable, completing the proof. h A.6. Proof of Lemma 6.1
The  X  X  X f X  X  part is obvious, since we have known that every state in the sequence f I So now we are going to prove the  X  X  X nly if X  X  part.

S-nodes in s must be of the form { q 1 , ... , q M }, where q is l -supported by S-node q for some subset l h , then q must coincide with q
I k 1  X  h  X  if and only if it is I k 1  X  h  X  -supported by q as
Since s is acyclic, without loss of generality, assume that a
Then Tail G ( q 1 )= ; and Tail G ( q j ) { a 1 , ... , a
Let k be the stopping-point of the sequence f I k  X  h  X g . We have a f a ; ... ; a j 1 g I k  X  h  X  , then Tail G  X  q j  X  I k  X  h  X  , which implies a we get a j 2 I k  X  h  X  for all j . This shows h  X  I k  X  h  X  . h
References
