 Faculty of computer and information science, University of Ljubljana, Tr X aska, Ljubljana, Slovenia 1. Introduction
The probabilistic radial basis function network classifier (PRBF) is an effective but less-transparent prediction model [1,2]. The PRBF is a special case of the RBF network [3]. It adopts a cluster interpre-a generalization of the Gaussian mixture model [3,4].

In a typical classification problem setting we are concerned with both prediction accuracy and the interpretability of our prediction model. Complex models have potentially higher accuracy but are more difficult to interpret. This can be alleviated either by sacrificing some prediction accuracy for a more transparent model or by using an explanation method that improves the interpretability of the model. A lot of effort has been invested into increasing the interpretability of machine learning models [5 X 9]. Some approaches exploit the essential property of additive classifiers to provide more comprehensible sification model [13 X 15]. The latter are more versatile and facilitate comparison of different types of classifiers. However, such general methods have to treat the model as a black-box and rely on computa-tionally intensive sensitivity analysis.

Of course, general explanation methods can be applied to explain the PRBF X  X  predictions and make it easier to interpret. Furthermore, as we show in this paper, we can exploit the properties of the PRBF classifier to improve the efficiency if the computation. We show how two different general explanation methods [14,15]) can be made more efficient when used with the PRBF, without changing the methods X  effectiveness.
 The remainder of the paper is organized as follows. In the next section, we describe the PRBF. In Section 3, we describe a general one-variable-at-a-time explanation method and how it can be modified for the use with the PRBF. In Section 4 we show how an explanation method which takes into account and real-life data sets are shown in Section 5. Section 6 concludes the paper. 2. The PRBF network classifier
Consider a classification problem with a input random variables X 1 ,...,X a and a discrete target instance x = ( X 1 = x (1) ,...,X a = x ( a ) ) ), where x ( i ) are the input variables X  values.
The corresponding PRBF has a inputs, c outputs, and m hidden units (see Fig. 1). Each output esti-where  X   X &lt; a represents the mean of component j and  X  represents the corresponding a  X  a covariance matrix. The model X  X  adjustable parameter vector consists of the mixing coefficients  X  jk , the component class by considering the components as a common pool [2]. Thus, for each class, the conditional density function p ( x | y k ) is modelled as a mixture model of the form: where f ( x ) are probability density values and  X  jk &gt; 0 are mixing coefficients which satisfy Class probabilities are determined using the Bayes rule where the class prior P k is the observed relative frequency of class y k .

The PRBF is a special case of the RBF network, where the outputs correspond to probability density arate mixtures model [4] can be derived as a special case of PRBF by setting  X  jk = 0 for all classes k , except for the class that the component j belongs to. A PRBF can be trained efficiently using the EM algorithm for likelihood maximization [1,2]. 3. Explaining the PRBF X  X  predictions
General explanation methods can be applied to any classification model which makes them a useful tool both for interpreting models and comparing different types of models. Such methods cannot exploit any model-specific properties thus the key technique is sensitivity analysis  X  changing the inputs of the model and observing changes in the model X  X  output.
 by  X  X iding X  the input value (set of values) and observing how the output changes. As such, the key component of general explanation methods is the expected conditional prediction  X  the prediction where only a subset of the input variables is known. Let Q be a subset of the set of input variables Q  X  S = variables represented in Q : Therefore, p S ( y k | x ) = p ( y k | x ) .

Computing an input feature X  X  contribution by using the expected conditional prediction is common to all such methods. However, the methods differ in how many and which subsets of features they take extreme examples: (a) computing an input feature X  X  contribution by omitting just that feature and (b) desirable results.

Note that in a practical setting the classification function of the model is not known  X  we can only ac-and sampling-based approximations are used. For each of the two explanation methods, we will state the approximation used. The key to the proposed approach is substituting that approximation with a more efficient and exact computation that follows from the definition of the PRBF. 3.1. The one-variable-at-a-time approach
The straightforward characterization of the i -th input variable X  X  importance for the prediction of in-stance x is the difference between the model X  X  prediction for that instance and the model X  X  prediction reveals whether the value contributes towards or against class value y k .

In [14] using the difference in information or difference in log-odds ratios (or weight of evidence [16]) approach from [14]: proximation is proposed, based on perturbing the i -th features values and computing the weighted sum of perturbations, with each element weighted according to the probability of that value [14].
Note that eight of evidence provides an alternative view on information with similar properties (some more favorable, e.g. symmetry). However, the improvement in efficiency proposed in this paper targets p
S \{ i } ( y k | x ) and applies to all three approaches. 3.2. The marginalization approach for PRBF To make the computation more efficient for the PRBF, we use the marginalization property of the Gaussian distribution. Let the joint distribution of a set of random variables be Gaussian with mean  X  and covariance matrix  X  . The joint distribution of each subset is also a Gaussian distribution. The mean  X  0 of the subset X  X  Gaussian is obtained by removing from  X  the components that belong to the variables not in the subset. Covariance matrix  X  0 is obtained by removing the rows and columns of  X  that belong set of variables, we can immediately obtain the distribution of any subset of these variables. mixture of Gaussians: Based on the marginalization property of the Gaussian distribution, it is straightforward to analytically compute p Q ( x | y k ) obtained by excluding the subset S \ Q : not in Q . For the PRBF, we therefore obtain Eq. (5) directly as and use it in Eq. (6). This does not change the effectiveness of the method. In fact, this computation improves on perturbing the i  X  th feature X  X  values as it is not an approximation but an exact computa-tion [17].

Equation (9) is also more efficient than perturbing the values. The latter requires us to compute the weighted average of m &gt; 1 predictions (in practice, m = 1 is not sufficient for a reasonable approxi-mation). The former requires us to first remove the i  X  th component from  X  and appropriate rows and columns from  X  , which can be done in constant time. Then, we compute a single prediction. 4. The all-subsets approach
The main disadvantage of one-variable-at-a-time approaches is that strong interactions between input variables, such as disjunctive concepts and other redundancies will result in unintuitive contributions. Therefore, in certain situations a method that takes such interactions into account is more desirable.
For example, observe the simple disjunction of two binary variables 1  X  1 . Changing either of the two values to 0 would not change the value of the expression and both variables would be assigned a zero importance. Both have to be changed simultaneously to observe a change in the value of the expression and assign a non-zero contribution to the variables, which are clearly important.

A solution was proposed in [15], which we will briefly introduce in order to justify the proposed mod-ification for use with the PRBF. For more details about the explanation method, see [15]. The proposed procedure demands 2 a steps and results in an exponential time complexity: where  X ( Q )( k,x ) = p Q ( y k | x )  X  p  X  ( y k | x ) .

For larger a , the computation of Eq. (10) becomes infeasible, so an alternative formulation was used: which precede the i -th variable in permutation O  X   X  ( a ) .

The conditional expectations in Eq. (11) are difficult to compute, so an additional step was taken to simplify the approximation: where f ( z ) is the probability density function of the vector of input variables and and  X  2 A be the population mean and variance of A , respectively. Taking r random samples from this population gives an unbiased estimator  X   X  0 i ( k,x ) = P r j =1 A  X  N (  X  0 i ,  X  A  X  4.1. Proposed modification
Note that in order to make the transition from Eqs (11) to (12) and facilitate such random sampling, it is assumed that the input variables are independently distributed. On the other hand, for the PRBF we can combine Eqs (11) and (9) to facilitate the same sampling procedure but from a different population produces an unbiased estimator of  X  i ( k,x ) , without the assumption of conditionally independent input variables and does not require an approximation of the conditional predictions.

Furthermore, even if independently distributed input variables are assumed, the approximation based on Eq. (11) is more efficient than the approximation based on Eq. (12). If independently distributed input variables are assumed then both estimators are unbiased estimators of the actual value  X  A =  X  B =  X  ( k,x ) =  X  0 and B O = R  X  A O ,z f ( z ) dz . The population variance  X  2 A can be written as and  X  2 B can be written as We want to show that the approximation based on Eq. (11) is more efficient than the approximation based on Eq. (12). Both require us to compute several samples, where each sample requires two predictions. It is sufficient to show that the proposed approach requires fewer samples to achieve the same expected error.

The approximation error depends only on population variances, so it is sufficient to show that  X  2 B 6  X  R  X  f ( z ) dz = 1 and that the square function is convex. We can use Jensen X  X  inequality to show The above holds for each O  X   X  ( a ) , so constant. That is, if and only if changing the input variable has no effect on the model X  X  prediction. 5. Illustrative examples their visualization using the well-known Titanic data set [18]. The task is to classify the survival of a passenger in the disaster of the Titanic ship.

Figure 2 shows an example of an instance explanation for the prediction of instance x (a first class adult male passenger). The text at the top includes the data set and model names, the class value in values of Eq. (6) for each corresponding input variable) towards (or against) the class value  X  X urvived = yes X . The thinner and lighter bars indicate average contributions of these values across all instances. For the given instance we observe that  X  X ex = male X  speaks strongly against survival and  X  X tatus = first that being male is on average less dangerous and being in the first class is even more beneficial than in the selected case. Note that the same visualization can be used even if some other method for computing the contributions is used instead of WE ( k,x ) . 5.1. Illustrative examples using artificial data sets
The first artificial data set  X  groups  X  is tailored to the PRBF. The data set has two relevant input data set is plotted in Fig. 3(a). We generated 1000 training instances and 1000 testing instances.
Figures 3(b) and 3(c) show how two instance explanations for the same instance from the groups data set but for two different models can be compared. We compared the PRBF and Naive Bayes, which achieve 97% and 35% accuracy, respectively. Comparing the two, we see that the PRBF predicts the instance correctly and the contributions reveal the two important input variables. The explanation for Naive Bayes model X  X  predictions reveals how it incorrectly models the data.

The second data set can be described with a sphere inscribed in the unit cube. The points within the sphere belong to one class and the remaining points to the other (see Fig. 4(a)). The three important input variables correspond to three dimensions and there are two additional input variables, which are irrelevant. Again, we generated 1000 training and 1000 testing instances. The PRBF achieves 87% ac-from the sphere data set. The PRBF correctly predicts the class value. The two coordinates that are far coordinate close to the center contributes against it.

In the first two artificial data sets there are no strong redundancies between input variables, so the all-subsets approach would yield contributions similar to the one-variable-at-a-time approach. To give an illustrative example of where the two methods differ, we use the disjunct data set. The data set has one of the binary input variables is 1.

Again, 1000 training and 1000 testing instances were used and the PRBF model achieves a 100% pre-diction accuracy. Figure 5(a) is the one-variable-at-a-time explanation for an instance from the disjunct value being 1. Figure 5(b) shows how the all-subsets explanation correctly reveals these contributions. For additional examples of the advantages of the all-subsets approach see [19]. 5.2. Application to breast cancer prediction study and a complete description of the data set can be found in [19]. In this study, several machine learning algorithms were evaluated with respect to their performance on the breast cancer recurrence inspection and evaluation showed that oncologists found the explanations useful and agreed with the contributions.

We used the training set from [19] to train a PRBF and we tested its performance on the original 100 independent test instances. The PRBF achieves 73% accuracy, which is higher than the accuracy of the best performing model described in [19] (Naive Bayes, 70%). Using the explanation methods, oncologists are provided with visual support for the predictions of the otherwise nontransparent model.
For the patient in Fig. 6(a) the PRBF predicts a high probability of cancer recurrence (81%). While this prediction turned out to be incorrect as the cancer did not recur, the oncologist would have made the same prediction and agree that chance of recurrence is high due to lympho-vascular invasion (LVI = 1), while a low number of positive lymph nodes (nLymph = 0) and early stage of cancer (stage = 1) Therefore, the most relevant input variables speak against recurrence and the model correctly predicts a low chance of recurrence (6%).

If oncologists chose to replace the underlying Naive Bayes model with the better performing PRBF, the type of visualizations would remain the same and it would not affect the user-experience. This is an advantage of using a general explanation method. 6. Conclusion We have shown how existing general explanation methods can be adapted for more efficient use with PRBF classifiers. The modification facilitates exact computation of conditional predictions and allows for a more efficient computation. The explanations reveal how individual input variables influence the PRBF, thus making it transparent. The general methods allow users to compare different types of models or replace an existing model without having to replace the explanation method, as illustrated on two artificial data sets and a real-world example from oncology. In our further work we will focus on using our methodology on additional real-world problems and improving the visual design of the explanations. We also plan on improving the usability of the proposed explanation methodology by wrapping it into an R package.
 References
