 In many applications, e.g., recommender systems and traffic monitoring, the data comes in the form of a matrix that is only partially observed and low rank. A fundamental data-analysis task for these datasets is matrix completion , where the goal is to accurately infer the entries missing from the matrix. Even when the data satisfies the low-rank assump-tion, classical matrix-completion methods may output com-pletions with significant error  X  in that the reconstructed matrix differs significantly from the true underlying ma-trix. Often, this is due to the fact that the information contained in the observed entries is insufficient. In this work, we address this problem by proposing an active version of matrix completion, where queries can be made to the true underlying matrix. Subsequently, we design Order&amp;Extend , which is the first algorithm to unify a matrix-completion approach and a querying strategy into a single algorithm. Order&amp;Extend is able identify and alleviate insufficient in-formation by judiciously querying a small number of addi-tional entries. In an extensive experimental evaluation on real-world datasets, we demonstrate that our algorithm is efficient and is able to accurately reconstruct the true ma-trix while asking only a small number of queries.
 H.2.8 [ Database Management ]: Database Applications X  data mining matrix completion; recommender systems; active querying
In many applications the data comes in the form of a low-rank matrix. Examples of such applications include rec-ommender systems (where entries of the matrix indicated user preferences over items), network traffic analysis (where c  X  the matrix contains the volumes of traffic among source-destination pairs), and computer vision (image matrices). In many cases, only a small percentage of the matrix entries are observed. For example, the data used in the Netflix prize competition was a matrix of 480K users  X  18K movies, but only 1% of the entries were known.

A common approach for recovering such missing data is called matrix completion . The goal of matrix-completion methods is to accurately infer the values of missing entries, subject to certain assumptions about the complete matrix [2, 3, 5, 7, 8, 9, 19]. For a true matrix T with observed val-ues only in a set of positions  X , matrix-completion methods exploit the information in the observed entries in T (de-noted T  X  ) in order to produce a  X  X ood X  estimate b T of T . In practice, the estimate may differ significantly from the true matrix. In particular, this can happen when the observed entries T  X  are not adequate to provide sufficient information to produce a good estimate.

In many cases, it is possible to address the insufficiency of T  X  by actively obtaining additional observations. For example, in recommender systems, users may be asked to rate certain items; in traffic analysis, additional monitoring points may be installed. These additional observations can lead to an augmented  X  0 such that T  X  0 carries more infor-mation about T and can lead to more accurate estimates b T . In this active setting, the data analyst can become an active participant in data collection by posing queries to T . Of course such active involvement will only be acceptable if the number of queries is small.

In this paper, we present a method for generating a small number of queries so as to ensure that the combination of observed and queried values provides sufficient information for an accurate completion; i.e., the b T estimated using the entries T  X  0 is significantly better than the one estimated using T  X  . We call the problem of generating a small num-ber of queries that guarantee small reconstruction error the ActiveCompletion problem.

The difference between the classical matrix-completion problem and our problem is that in the former, the set of observed entries is fixed and the algorithm needs to find the best completion given these entries. In ActiveCompletion , we are asked to design both a completion and a querying strategy in order to minimize the reconstruction error. On the one hand, this task is more complex than standard ma-trix completion  X  since we have the additional job of design-ing a good querying strategy. On the other hand, having the flexibility to ask some additional entries of T to be revealed should yield lower reconstruction error.
At a high level, ActiveCompletion is related to other recently proposed methods for active matrix completion, e.g., [4]. However, existing approaches identify entries to be queried independently of the method of completion. In contrast, a strength of our algorithm is that it addresses completion and querying in an integrated fashion.

The main contribution of our work is Order&amp;Extend , an al-gorithm that simultaneously minimizes the number of queries to ask and produces an estimate matrix b T that is very close to the true matrix T . The design of Order&amp;Extend is in-spired from recent matrix-completion methods that view the completion process as solving a sequence of (not necessarily linear) systems [10, 11, 13, 16]. We adopt this general view, focusing on a formulation that involves only linear systems. Although existing work uses this insight for simple matrix completion, we go one step further and observe that there is a relationship between the ordering in which systems are solved, and the number of additional queries that need to be posed. Therefore, the first step of Order&amp;Extend focuses on finding a good ordering of the set of linear systems. Inter-estingly, this ordering step relies on the combinatorial prop-erties of the mask graph, a graph that is associated with the positions (but not the values) of observed entries  X .
In the second step, Order&amp;Extend considers the linear sys-tems in the chosen order, and asks queries every time it encounters a problematic linear system Ax = b . A linear system can problematic in two ways: ( a ) when there are not enough equations for the number of unknowns, so that the system does not have a unique solution; ( b ) when solving the system Ax = b is numerically unstable given the specific b involved. Note that, as we explain in the paper, this is not the same as simply saying that A is ill-conditioned; part of our contribution is the design of fast methods for detecting and ameliorating such systems.

Our extensive experiments with datasets from a variety of application domains demonstrate that Order&amp;Extend re-quires significantly fewer queries than any other baseline querying strategy, and compares very favorably to approaches based on well-known matrix completion algorithms. In fact, our experiments indicate that Order&amp;Extend is  X  X lmost op-timal X  as it gives solutions where the number of entries it queries is generally very close to the information-theoretic lower bound for completion.
To the best of our knowledge we are the first to pose the problem of constructing an algorithm equipped simultane-ously with a completion and a querying strategy. However, matrix completion is a long studied problem, and in this section we describe the existing work in this area. Statistical matrix completion: The first methods for matrix completion to be developed were statistical in na-ture [1, 2, 3, 5, 7, 8, 9, 14, 19]. Statistical approaches are typically interested in finding a low-rank completion of the partially observed matrix. These methods assume a random model for the positions of known entries, and formulate the task as an optimization problem. A key characteristic of statistical methods is that they estimate a completion re-gardless of whether the information contained in the visible entries is sufficient for completion. In other words, on any input they output their best estimate, which can have high error. Moreover, statistical methods are not equipped with a querying strategy, nor a mechanism to signal when the information is insufficient.
 Random sampling: Cand  X es and Recht introduced a thresh-old on the number of entries needed for accurate matrix completion [2]. Under the assumption of randomly sampled locations of known entries, they prove that an n 1  X  n 2 ma-trix of rank r should have at least m &gt; Cn 6 5 r log( n ) for their algorithm to succeed with high probability, where n = max( n 1 ,n 2 ). Different authors in the matrix-completion lit-erature develop slightly different thresholds, but all are es-sentially O ( nr log( n )) [9, 15]. We point out that achieving this bound in the real world often requires a significantly large number of samples. For example, adopting the rank r  X  40 of top solutions to the Netflix Challenge, over 151 million entries would need to be queried.
 Structural matrix completion: Recently, a class of ma-trix completion has been proposed, which we call structural . Rather than taking an optimization approach, the meth-ods of structural completion explicitly analyze the informa-tion content of the visible entries and are capable of stat-ing definitively that the observed entries are information-theoretically sufficient for reconstruction [10, 11, 13, 16].
Structural methods are implicitly concerned with the num-ber of possible completions that are consistent with the par-tially observed matrix; this could be infinite, a finite, one, or none. A key observation shared by all structural approaches is that the number of possible completions does not depend on the values of the observed entries, but rather only on their positions. This statement, proved by Kiraly et al. [10], means that in our search for good ordering of linear systems we can work solely with the locations of known entries.
The common characteristic between our method and struc-tural methods is that they also view matrix completion as a task of solving a sequence of (not necessarily linear) systems of equations where the result of one is used as input to an-other. In fact, Meka et al. [13] adopt the same view as ours. However, the key difference between these works and ours is that we are concerned particularly with the active ver-sion of the problem and we need to effectively design both a reconstruction and a querying strategy simultaneously. The active problem: Although active learning has been studied for some time, work in the active matrix completion area has only appeared recently [4, 17]. In both these works, the authors are interested in determining which entries need to be revealed in order to reduce error in matrix reconstruc-tion. Their methods choose to reveal entries with the largest predicted uncertainty based on various measures. Algorith-mically, the difference with our work is that the previous ap-proaches construct a querying strategy independently of the completion algorithm. In fact, they use off-the-shelf matrix completion algorithms for the reconstruction phase, while the strength in our algorithm is precisely its integrated na-ture of querying and completing. These methods appear to have other drawbacks. In the experiments, Chakraborty et al. start with partial matrices where 50-60% of entries are already known  X  far greater than that required by our method. Further, their proposed query strategy does not lead to a significant improvement over pure random query-ing. While Sutherland et al. report low reconstruction error, the main experiments are run over 10  X  10 matrices, provid-ing no evidence that the methods scale. Figure 1: The mask graph G  X  of mask  X  = { (1 , 1), (1 , 2), (1 , 3), (2 , 2), (3 , 1) } .
In this section, we describe our setting and provide the problem definition.
Throughout the paper, we assume the existence of a true matrix T of size n 1  X  n 2 ; T may represent the preferences of n 1 users over n 2 objects, or the measurements obtained in n 1 sites over n 2 attributes. We assume that the entries of T are real numbers ( T ij  X  R ) and that only a subset of these entries  X   X  { ( i,j ) | , 1  X  i  X  n 1 , 1  X  j  X  n 2 } are observed. We refer to the set of positions of known entries  X  as the mask of T . When we are referring to the values of the visible entries in T we will denote that set as T  X  .
 The mask  X  has an associated mask graph, denoted G  X  . The mask graph is a bipartite graph G  X  = ( V 1 ,V 2 ,E ), where V and V 2 correspond to the set of nodes in the left and right parts of the graph, with every node i  X  V 1 representing a row of T and every j  X  V 2 representing a column of T . The edges in G  X  correspond to the positions  X , meaning ( i,j )  X  E  X  X  X  ( i,j )  X   X . An example is shown in Figure 1.
Throughout the paper, we will use b T  X  to denote the esti-mate of T that was computed using T  X  as input. Of course, this estimate depends not only on  X , but also on the al-gorithm A used for completion. Therefore, we denote the estimate obtained by a particular algorithm A on input T  X  as A ( T  X  ) = b T A ,  X  . When the algorithm is unspecified or clear from the context, we will omit it from the subscript.
Finally, we define the reconstruction error of the estimate b T  X  of T as: Recall that for an n 1  X  n 2 matrix X , k X k F (or simply k X k ) is the Frobenius norm defined as k X k F = q P n 1 i =1 P Low effective rank: For the matrix-completion problem to be well-defined, some restrictions need to be placed on T . Here, the only assumption we make for T is that it has low effective rank r ( min( n 1 ,n 2 )). Were T exactly rank r , it would have r non-zero singular values; when T is effectively rank r , it has full rank but its top r singular val-ues are significantly larger in magnitude than the rest. In practice, many matrices obtained through empirical mea-surements are found to have low effective rank.

Rather than stipulating low rank, it is often simpler to postulate that the effective rank r is known. This assump-tion is used in obtaining many theoretical results in the matrix-completion literature [2, 10, 11, 14]. Yet in practice, the important assumption is that of low effective rank. Even if r is unknown but required as input to an algorithm, one could try a several values of r and choose the best perform-ing. For the rest of the paper, we consider r to be known and omit reference to it when it is understood from context. Querying entries: For simplicity of exposition we discuss our problem and algorithms in the context of unlimited ac-cess to all unobserved entries of T . However, our results still apply and our algorithm can still work in the presence of constraints on which entries of T may be queried. Given the above, we define our problem as follows:
Problem 1 (ActiveCompletion). Given an integer r &gt; 0 that corresponds to the effective rank of T and the values of T in certain positions  X  , find a set of additional entries Q to query from T such that for  X  0 =  X   X  Q , RelError ( b T  X  0 ) as well as | Q | are minimized. Note that the above problem definition has two minimiza-tion objectives: (1) the number of queried entries and (2) the reconstruction error. In practice we can only solve for one and impose a constraint on the other. For example, we can impose a query budget b on the number of queries to ask and optimize for the error. Alternatively, one can use error bud-get to control the error of the output, and then minimize for the number of queries to ask. In principle our algorithm can be adjusted to solve any of the two cases. However, since setting a desired b is more intuitive for our active setting, in our experiments we do this and optimize for the error. We will focus on this version of the problem (with the budget on the queries) for the majority of the discussion. The exact case: A special case of ActiveCompletion is when T is exactly rank r, and the maximum allowed error is zero. In this case, the problem asks for the minimum number of queries required to reconstruct b T that is exactly equal to the true matrix T . This can only be guaranteed by ensuring that the information observed in T  X  0 is adequate to restrict the solution space to a single unique completion, which will then necessarily be identical to T .

Intuitively, one expects that the larger the set of observed entries  X , the fewer the number of possible completions of T . In fact, Kiraly et al. [10] make the fundamental observa-tion that under certain assumptions, the number of possible completions of a partially observed matrix does not depend on the values of the visible entries, but only on the positions of these entries. This result implies that the uniqueness of matrices that agree with T  X  is a property of the mask  X  and not of the actual values T  X  .

Critical mask size: The number of degrees of freedom of an n 1  X  n 2 matrix of rank exactly r is r ( n 1 + n 2  X  r ), which we denote  X  ( T,r ). Hence, regardless of the nature of  X , any solution with = 0 must have |  X  0 |  X   X  ( T,r ). We therefore call  X  ( T,r ) the critical mask size as it can be considered as a (rather strict) lower bound on the number of entries that need to be in  X  0 to achieve small reconstruction error.
Empty masks: For the special case of exact rank and = 0, if the input mask  X  is empty, i.e.,  X  =  X  , then ActiveCompletion can be solved optimally as follows: simply query the entries of r rows and r columns of T . This will require  X  ( T,r ) queries, which will construct a mask  X  that determines a unique reconstruction of T . Therefore, when the initial mask  X  =  X  , the ActiveCompletion prob-lem can be solved in polynomial time.
In this section we present our algorithm, Order&amp;Extend , for addressing the ActiveCompletion problem.

The starting point for the design of Order&amp;Extend is the low (effective) rank assumption of T . As it will become clear, this means that the unobserved entries are related to the ob-served entries through a set of linear systems. Thus one ap-proach to matrix completion is to solve a sequence of linear systems. Each system in this sequence uses observed en-tries in T , or entries of T reconstructed by previously solved linear systems to infer more missing entries.

The reconstruction error of such an algorithm depends on the quality of the solutions to these linear systems. As we will show below, each query of T can yield a new equation that can be added to a linear system. Hence, if a linear system has fewer equations than unknowns, a query must be made to add an additional equation to the system. Like-wise, if solving a system is numerically unstable then a query must be made to add an equation that will stabilize it. Cru-cially, the need for such queries depends on the nature of the solutions obtained to linear systems earlier in the or-der. Thus the order in which systems are solved, and the nature of these systems are inter-related. A good ordering will minimize the number of  X  X roblematic X  systems being en-countered. However, problematic systems can appear even in the best-possible order, meaning that good ordering alone is insufficient for accurate reconstruction.

At a high level, Order&amp;Extend operates as follows: first, it finds a promising ordering of the linear systems. Then, it proceeds by solving the linear systems in this order. If a linear system that requires additional information is encoun-tered, the algorithm either strategically queries T or moves the system to the end of the ordering. When all systems have been solved,  X  T is computed and returned. The next subsections describe these steps in detail.
In this section we explain the particular linear systems that the completion algorithm solves, the sequence in which it solves them, and how the ordering in which systems are solved affects the quality of the completion.

For the purposes of this discussion, we assume that T is of rank exactly r . In this case T can be expressed as the prod-uct of two matrices X and Y of sizes n 1  X  r and r  X  n 2 ; that is, T = XY . Furthermore, we assume that any subset of r rows of X , or r columns of Y , is linearly independent. (Later we will describe how Order&amp;Extend addresses the case when these assumptions do not hold  X  i.e., when T is only effec-tively rank-r , or when an r -subset is linearly dependent). To complete T, it suffices to find such factors X and Y . 1 The Sequential completion algorithm: We start by de-scribing an algorithm we call Sequential , which estimates the rows of X and columns of Y . Sequential takes two inputs: (1) an ordering  X  over the set of all rows of X and columns of Y , which we call the reconstruction order , and (2) the partially observed matrix T  X  .

To explain how Sequential works, consider the example in Figure 2, where r = 2, T is on the left and G  X  is on the
Note that X and Y are not uniquely determined; any in-vertible r  X  r matrix W yields new factors XW  X  1 and WY which also multiply to yield T .

Figure 2: An intermediate step of Sequential algorithm. right. The factors X and Y are shown on the side of and above T to convey how their product results in T . The nodes V 1 of G  X  correspond to the rows of X , and nodes V 2 to the columns of Y . In this figure, we illustrate an intermediate step of Sequential , in which the values of the i -th and i rows of X have already been computed. Each entry of T is the inner product of a row of X and a column of Y . Hence we can represent the depicted entries in T by the following linear system: Observe that x i and x i 0 are known, and that the edges ( x ,y j ) and ( x i 0 ,y j ) corresponding to T ij and T i 0 G
 X  . The only unknowns in (2) and (3) are y 1 j and y which leaves us with two equations in two unknowns. As stated above and by assumption, any r -subset of X or Y is linearly independent; hence one can solve uniquely for y and y 2 j and fill in column j of Y .

To generalize the example above, the steps of Sequential can be partitioned in x -and y -steps; at every y -step the algorithm solves a system of the form In this system, y is a vector of r unknowns corresponding to the values of the column of Y we are going to compute; A x is an r  X  r fully-known submatrix of X and t is a vector of r known entries of T which are located on the same column as the column index of y . If A x and t are known, and A full rank, then y can be computed exactly and the algorithm can proceed to the next step.

In the x -steps Sequential evaluates a row of X using an r  X  r already-computed subset of columns of Y , and a set of r entries of T from the row of T corresponding to the current row of X being solved for. Following the same no-tational conventions as above, the corresponding system be-comes A y x = t 0 . For simplicity we will focus our discussion on y -steps; the discussion on x -steps is symmetric. The completion on the mask graph: The execution of Sequential is also captured in the mask graph shown in the right part of Figure 2. In the beginning, no rows or columns have been recovered and all nodes of G  X  are white (unknown). As the algorithm proceeds they become black (known), and this transformation occurs in the order sug-gested by the input reconstruction order  X  . Thus a black node denotes a row of X or column of Y that has been com-puted. In our example, the fact that we can solve for the j -th column of Y (using Equations (2) and (3)) is captured
T
T Figure 3: The direction on the edges imposed by the order-ing of nodes shown in Figure 2. by y j  X  X  two connections to black/known nodes (recall r = 2). For general rank r , the j -th column of Y can be estimated by a linear system, if in the mask graph y j is connected to at least r already computed (black) nodes. This is sym-metric for the i -th row of X and node x i . Intuitively, this transformation of nodes from black to white is reminiscent of an information-propagation process. This analogy was first drawn by Meka et al. [13].
 Incomplete and unstable linear systems: As it has al-ready been discussed in the literature [13], the performance of an algorithm like Sequential is heavily dependent on the input reconstruction order. Meka et al. [13] have discussed methods for finding a good reconstruction order in the spe-cial case where the mask graph has a power-law degree distri-bution. However, even with the best possible reconstruction order Sequential may still encounter linear systems which are either incomplete or unstable . Incomplete linear systems are those for which the vector t has some missing values and therefore the system A x y = t cannot be solved. Unsta-ble linear systems are those in which all the entries in t are known, but the resulting expression A  X  1 x t may be very sen-sitive to small changes in t . These systems raise a numerous problems in the case where the input T is a noisy version of a rank r matrix, i.e., it is a matrix of effective rank r .
In the next two sections we describe how Order&amp;Extend deals with such systems.
First, Order&amp;Extend devises an order that minimizes the number of incomplete systems encountered in the comple-tion process.

Let us consider again the execution of Sequential on the mask graph, and the sequential transformation of the nodes in G  X  = ( V 1 ,V 2 ,E ) from white to black. Recall that in this setting, an incomplete system occurs when the node in G  X  that corresponds to y is connected to less than r black nodes.
Consider an order  X  of the nodes V 1  X  V 2 . This order con-ceptually imposes a direction on the edges of E ; if x is before y in that order, then  X  ( x ) &lt;  X  ( y ), and edge ( x,y ) becomes directed edge ( x  X  y ). Figure 3 shows this transforma-tion for the mask graph in Figure 2 and the order implied there. For fixed  X  , a node becomes black if it has at least r incoming edges, i.e., indegree at least r . In this view, an incomplete system manifests itself by the existence of a node that has indegree less than r . Clearly, if an order  X  guaran-tees that all nodes have r incoming edges, then there are no incomplete systems, and  X  is a perfect reconstruction order .
In practice such perfect orders are very hard to find; in most of the cases they do not exist. The goal of the first step of Order&amp;Extend is to find an order  X  that is as close as possible to a perfect reconstruction order. It does so by constructing an order that minimizes the number of edges that need to be added so that the indegree of any node is r .
To achieve this, the algorithm starts by choosing the node from G  X  = ( V 1 ,V 2 ,E ) with the lowest degree. This node is placed last in  X  , and removed from G  X  = ( V 1 ,V 2 ,E ) along with its incident edges. Of the remaining nodes, the one with minimum degree is placed in the next-to-last position in  X  , and again removed from G  X  = ( V 1 ,V 2 ,E ). This process repeats until all nodes have been assigned a position in  X  .
Next, the algorithm makes an important set of adjust-ments to  X  by examining each node u in the order it occurs in  X  . For a particular u the adjustments can take two forms: 1. if u has degree  X  r : it is repositioned to appear imme-2. if u has degree &gt; r : it is repositioned to appear imme-
These adjustments aim to construct a  X  such that when the implied directionality is added to edges, each node has indegree as close to r as possible. While it is possible to iter-ate this adjustment process to further improve the ordering, in our experiments this showed little benefit.

Once the order  X  is formed as described above, then the in-complete systems can be quickly identified: as Order&amp;Extend traverses the nodes of G  X  in the order implied by  X  , every time it encounters a node u with in-degree less than r , it adds edges so that u  X  X  indegree becomes r ; by definition, the addition of a new edge ( x,u ) corresponds to querying a missing entry T xu of T .
The incomplete systems are easy to identify  X  they cor-respond to nodes in G  X  with degree less than r . However, there are other  X  X roblematic X  systems which do not appear to be incomplete, yet they are unstable . Such systems arise due to noise in the data matrix or to an accumulation of error that happens through the sequential system-solving process. These systems are harder to detect and alleviate. We discuss our methodology for this below.
 Understanding unstable linear systems: Recall that a system A x y = t is unstable if its solution is very sensitive to the noise in t . To be more specific, consider the system A y = t , where A x has full rank and t is fully known. Recall that the solution of this system, y = A  X  1 x t , will be used as part of a subsequent system: A y x = t 0 , where y will become a row of matrix A y . Let A x = U  X  V T be the singular value decomposition of A x with singular values  X  1  X  ...  X   X  Now if there is a  X  j such that  X  j is very small, then the solution to the linear system will be very unstable when the singular vector v j corresponding to  X  j has a large pro-jection on t. This is because in A  X  1 x , the small  X  inverted to a very large 1 / X  j . Thus the inverse operation will cause any component of t that is in the direction of v to be disproportionally-strongly expressed , and any small amount of noise in t to be amplified in y . Thus, unstable systems may be catastrophic for the reconstruction error of Sequential as a single such system may initiate a sequence of unstable systems, which can amplify the overall error.
Unstable vs ill-conditioned systems: It is important to contrast the notion of an unstable system with that of an ill-conditioned system, which is widely used in the literature. Recall, that system A x y = t is ill-conditioned if there exists a vector s and a small perturbation s 0 of s , such that the results of systems A x y = s and A x y 0 = s 0 are significantly different. Thus, whether or not a system is ill-conditioned depends only on A x , and not on its relationship with any target vector t in particular. An ill-conditioned system is also characterized by a large condition number  X  ( A x ) = This way of stating ill-conditioning emphasizes that  X  ( A measures a property of A x and does not depend on t . Con-sequently (as we will document in Section 5.3) the condition number  X  ( A x ) generates too many false positives to be used for identifying unstable systems.
 Identifying unstable systems: To provide a more pre-cise measure of whether a system A x y = t is unstable, we compute the following quantity: We call this quantity the local condition number , which was also discussed by Trefethen and Bau [18]. The local con-dition number is more tailored to our goal as we want to quantify the proneness of a system to error with respect to a particular target vector t . In our experiments, we char-acterize a system A x y = t as unstable if ` ( A x ,t )  X   X  . We call the threshold  X  the stability threshold and in our experi-ments we use  X  = 1. Loosely, one can think of this threshold as a way to control for the error allowed in the entries of reconstructed matrix. Although it is related, the value of this parameter does not directly translate into a bound on the RelError of the overall reconstruction.
 Selecting queries to alleviate unstable systems: One could think of dealing with an unstable system via regular-ization, such as ridge regression (Tikhonov Regularization) which was also suggested by Meka et al. [13]. However, for systems A x y = t , such regularization techniques aim to dampen the contribution of the singular vector that corre-sponds to the smallest singular value, as opposed to boosting the contribution of the singular vectors that are in the di-rection of t . Further, the procedure can be expressed in terms of only A x without taking t into account; as we have discussed this is not a good measure for our approach.
The advantage of our setting is that we can actively query entries from T . Therefore, our way of dealing with this prob-lem is by adding a direction to A x (or as many as are needed until there are r strong ones). We do that by extending our system from A x y = t to A x  X   X  y = t  X  . Of course, in doing so we implicitly shift from looking for an exact solution to the system, to looking for a least-squares solution.
Clearly  X  cannot be an arbitrary vector. It must be an already computed row of X , it should be independent of A and it must boost a direction in A x which is poorly expressed and also in the direction of t . Given the intuition we devel-oped above, we iterate over all previously computed rows of X that are not in A x , and set each row as a candidate  X  . Among all such  X   X  X  we pick  X   X  as the one with the smallest ` ( A x ,t ), and use it to extend A x to A x  X   X  .
Querying T judiciously: Although the above procedure is conceptually clear, it raises a number of practical issues. If the system A x y = t solves for the j -th column of matrix Y , then every time we try a different  X  , which suppose is the already-computed X ( i, :), then the corresponding  X  must be the entry T ij . Since T ij is not necessarily an observed entry, this would require a query even for rows  X  6 =  X  Algorithm 1 The local_condition routine Input: C,A x , X ,t  X  = Random ( T ( i, :) ,T (: ,j ))  X  t = t  X   X  y = D  X  A x  X  t Algorithm 2 The Stabilize routine
Input: A x ,t, X  j : the column of Y being computed for i  X  X  Computed rows of X } do i  X  = arg min i c ( i ),  X   X  = X ( i  X  , :) if c ( i  X  ) &lt;  X  then return null which is clearly a waste of queries since we will only pick one  X  . Therefore, instead of querying the unobserved values of  X  , Order&amp;Extend simply uses random values following the distribution of the values observed in the i -th row and j -th column of T . Once  X   X  is identified, we only query the value of  X  corresponding to row  X   X  and column j .

If there is no  X   X  that leads to a system with local condi-tion number below our threshold, we postpone solving this system by moving the corresponding node of the mask graph to the end of the order  X  .
 Computational speedups: From the computational point of view, the above approach requires computing a matrix inversion per  X  . With a cubic algorithm for matrix inversion, this could induce significant computational cost. However, we observe that this can be done efficiently as all the matrix inversions we need to perform are for matrices that differ only in their last row  X  the one occupied by  X  .

Recall that the least-squares solution of the system A x y = t is y = ( A T x A x )  X  1 A T x t . Now in the extended system
A  X   X  y = Thus,  X  A x T  X  A x can be seen as a rank-one update to A In such a setting the Sherman-Morrison Formula [6] pro-vides a way to efficiently calculate D = (  X  A x T C = ( A T x A x )  X  1 . The details are shown in Algorithm 1. Using the Sherman-Morrison Formula we can find  X  y via matrix multiplication, which requires O ( r 2 ) for at most n = max { n 1 ,n 2 } candidate queries. Since the values of r we encounter in real datasets are small constants (in the range of 5-40), this running time is small. The pseudocode of this process is shown in Algorithm 2. The process of selecting the right entry to query is sum-marized in the Stabilize routine. Observe that Stabilize either returns the entry to be queried, or if there is no en-try that can lead to a stable systems it returns null. In the latter case the system is moved to the end of the order.
Given all the steps we described above we are now ready to summarize Order&amp;Extend in Algorithm 3.
 Algorithm 3 The Order&amp;Extend algorithm Input: T  X  ,r, X  Compute G  X 
Find ordering  X  (as per Section 4.2) for A x y = t (corresponding to the j -th column of Y ) encountered in  X  do return b T = XY Order&amp;Extend constructs the rows of X and columns of Y in the order prescribed by  X   X  the pseudocode shows the construction of columns of Y , but it is symmetric for the rows of X . For every linear system the algorithm encoun-ters, it completes the system if it is incomplete and tries to make it stable if it is unstable. When a complete and stable version of the system is found, the system is solved using least squares. Otherwise, it is moved to the end of  X  . Running time: The running time of Order&amp;Extend con-sists of the time to obtain the initial ordering, which us-ing the algorithm of Matula and Beck [12] is O ( n plus the time to detect and alleviate incomplete and un-stable systems. Recall that for each unstable system we compute an inverse O r 3 and check n candidates O r 3 + r 2 n . Thus the overall running time of our algo-rithm is O ( n 1 + n 2 ) + N  X  ( r 3 + r 2 n ) , where N is the number of unstable system the algorithm encounters. In practice, the closer a matrix is to being of rank exactly r , the smaller the number of error prone systems it encounters and therefore the faster its execution time. 2 Partial completions: If the budget b of allowed queries is not adequate to resolve the incomplete or the unstable sys-tems, then Order&amp;Extend will output b T with only a portion of the entries completed. The entries that remain unrecov-ered are those for which the algorithm claims inability to
Code and information are available at http://cs-people. bu.edu/natalir/matrixComp produce a good estimate. From the practical viewpoint this is extremely useful information as the algorithm is able to inform the data analyst which entries it was not able to re-construct from the observations in T  X  .
In this section we experimentally evaluate the perfor-mance of Order&amp;Extend both in terms of reconstruction error as well as the number of queries it makes. Our experiments show that across all datasets Order&amp;Extend requires very few queries to achieve a very low reconstruction error. All other baselines we compare against require many more queries for the same level of error, or can ever achieve the same level of reconstruction error.
 Datasets: We experiment on the following nine real-world datasets, taken from a variety of applications.

MovieLens : This dataset contains ratings of users for movies as appearing in the MovieLens website. 3 The original dataset has size 6 040  X  3 952 and only 5% of its entries are observed. For our experiments we obtain a denser matrix of size 4 832  X  3 162.

Netflix : This dataset also contains user movie-ratings, but from the Netflix website. The dataset X  X  original size is 480 189  X  17 770 with 1% of observed entries. Again we focus on a submatrix with higher percentage of observe en-tries and size 48 019  X  8 885.

Jester : This dataset corresponds to a collection of user joke ratings obtained for joke recommendation on the Jester website. 4 For our experiments we use the whole dataset with size 23 500  X  100 with 72% of its entries being observed.
Boat : This dataset corresponds to a fully-observed black and white image of size 512  X  512.

Traffic : This is a set of four datasets; each is part of a traffic matrix from a large Internet Service Provider where rows and the columns are source and destination prefixes (i.e., groups of IP addresses), and each entry is the volume of traffic between the corresponding source-destination pair. The largest dataset size 7 371  X  7 430 and 0 . 1% of its entries are observed; we call this TrafficSparse . The other two are fully-observed of sizes 2 016  X  107, and 2 016  X  121; we call these Traffic1 and Traffic2 . 5
Latency : Here we use two datasets consisting of Internet network delay measurements. Rows and columns are hosts, and each entry indicates the minimum ping delay among a particular time window. The datasets are fully-observed and of sizes 116  X  116, and 869  X  19; we call these Latency1 and Latency2 . 5 Baseline algorithms: We compare the performance of our algorithm to two state-of-the-art matrix-completion algo-rithms, OptSpace and LmaFit .

OptSpace : An SVD-based algorithm introduced by Ke-shavan et al.[8]. The algorithm centers around a convex-optimization step that aims to minimize the disagreement of the estimate b T on the initially observed entries T  X  use the original implementation of OptSpace . 6
LmaFit : A popular alternating least-squares method for matrix completion [19]. In our experiments we use the orig-
Source http://www.grouplens.org/node/73 .
Source http://goldberg.berkeley.edu/jester-data/
Source https://www.cs.bu.edu/~crovella/links.html http://web.engr.illinois.edu/~swoh/software/ optspace/code.html x -axis: query budget b ; y -axis: RelError of the completion. inal implementation of this algorithm provided by Wen et. al. 7 , and in particular the version where the rank r is pro-vided, as we observed it to perform best.

As neither OptSpace nor LmaFit are algorithms for active completion, we set up our experiment as follows: first, we run Order&amp;Extend on T  X  0 , which asks a budget of b queries. Before feeding T  X  0 to LmaFit and OptSpace we extend it with b randomly chosen queries. In this way both algorithms query the same number of additional entries. A random dis-tribution of observed entries has been proved to be (asymp-totically) optimal for statistical methods like OptSpace and LmaFit [2, 8, 19]. Therefore, picking randomly distributed b additional entries is the best querying strategy for these algorithms, and we have also verified that experimentally.
For all our experiments, the ground-truth matrix T is known but not fully revealed to the algorithms. The input to the algorithms consists of an initial mask  X  0 , the observed matrix T  X  0 , and a budget b on the number of queries they can ask. Each algorithm A outputs an estimate b T A ,  X  0 Selecting the input mask  X  0 : The initial mask  X  0 , with cardinality m 0 is selected by picking m 0 entries uniformly at random from the ground-truth matrix T . 8 The cardinality m 0 is selected so that m 0 &gt; 0 and m 0 &lt;  X  ( T,r ); usually we chose m 0 to be  X  30  X  50% of  X  ( T,r ). The former constraint guarantees that the input is not trivial, while the latter guar-antees that additional queries are definitely needed. Range for the query budget b : We vary the number of queries, b , an algorithm can issue among a wide range of val-ues. Starting with b &lt;  X  ( T,r )  X  m 0 , we gradually increase http://lmafit.blogs.rice.edu/
We also test other sampling distributions, but the results are the same as the ones we report here and thus omitted. it until we see that the performance of our algorithms stabi-lize (i.e., further queries do not decrease the reconstruction error). Clearly, the smaller the value of b the larger the reconstruction error of the algorithms.
 Reconstruction error: Given a ground-truth matrix T and input T  X  0 , we evaluate the performance of a reconstruc-tion algorithm A , by computing the relative error of b T with respect to T , using the RelError function defined in Equation (1). This measure takes into consideration all en-tries of T , both the observed and the unobserved. The closer b T
A ,  X  is to T the smaller the value of RelError ( b T A ,  X  general, RelError ( b T A ,  X  )  X  [0 ,  X  ) and at perfect recon-struction RelError ( b T A ,  X  ) = 0.

Although our baseline algorithms always produce a full es-timate (i.e., they estimate all missing entries), Order&amp;Extend may produce only partial completions (see Section 4.4 for a discussion in this). In these cases, we assign value 0 to the entries it does not estimate. Experiments with real noisy data: For our first experi-ment, we use datasets for which we know all off the entries. This is true for six out of our nine datasets: Traffic1 , Traf-fic2 , Latency1 , Latency2 , Jester , Boat . Note that Jester is missing 30% of the entries, but we treat them as true zero-values ratings; the remaining datasets are fully known and able to be queried as needed. As these are real datasets they are not exactly low rank, but plotting their singular values reveals that they have low effective rank. By inspecting their singular values, we chose: r = 7 for the Traffic and Latency datasets, r = 10 for Jester and r = 40 for Boat .

Figure 4 shows the results for each dataset. The x -axis is the query budget b ; note that while LmaFit and OptSpace always exhaust this budget, for Order&amp;Extend it is only an upper bound on the number of queries made. The y -axis is query budget b ; y -axis: RelError of the completion. the RelError ( T, b T A ,  X  ). The black vertical line marks the number of queries needed to reach the critical mask size; i.e., it corresponds to budget of (  X  ( T,r )  X  m 0 ). One should interpret this line as a very conservative lower bound on the number of queries that an optimal algorithm would need to achieve errorless reconstruction in the absence of noise.
From the figure, we observe that Order&amp;Extend exhibits the lowest reconstruction error across all datasets. More-over, it does so with a very small number of queries, com-pared to LmaFit and OptSpace ; the latter algorithms achieve errors of approximately the same magnitude in all datasets. On some datasets LmaFit and OptSpace come close to the relative error of Order&amp;Extend though with significantly more queries. For example for the Latency1 dataset, Or-der&amp;Extend achieves error of 0 . 24 with b = 2 K queries; LmaFit needs b = 4 K to exhibit an error of 0 . 33, which is still more than that of Order&amp;Extend . In most datasets, the differences are even more pronounced; e.g., for Traffic2 , Order&amp;Extend achieves a relative error of 0 . 50 with about b = 13 K queries; OptSpace and LmaFit achieve error of more than 0 . 8 even after b = 26 K queries. Such large dif-ferences between Order&amp;Extend and the baselines appear in all datasets, but Boat . For that dataset, Order&amp;Extend is still better, but not as significantly as in other cases  X  likely an indication that the dataset is more noisy. We also point out that the value of b for which the relative error of Or-der&amp;Extend exhibits a significant drop is much closer to the indicated lower bound by the black vertical line. Again this phenomenon is not so evident for Boat probably because this datasets is further away from being low rank.
 Extremely sparse real-world data: For the purpose of experimentation our algorithm needs to have access to all the entries of the ground truth matrix T  X  in order to be able to reveal the values of the queried entries. Unfortunately, the Movielens , Netflix , and TrafficSparse datasets consist mostly of missing entries, therefore we cannot query the majority of them. To be able to experiment with these datasets, we overcome this issue by approximating each dataset with its closest rank r matrix T r . The approximation is obtained by first assigning 0 to all missing entries of the observed T , and then taking the singular value decomposition and setting all but the largest r singular values to zero. This trick grants us the ability to study the special case discussed in Section 3 where the matrix is of exact rank r .
 Using r = 40, the results for these datasets are depicted in Figure 5 with the same axes and vertical line as in Figure 4. Again, we observe a clear dominance of Order&amp;Extend . In this case the differences in the relative error it achieves are much more striking. Moreover, Order&amp;Extend achieves al-most 0 relative error for extremely small number of queries b ; in fact the error of Order&amp;Extend consistently drops to an extremely small value for b very close to the lower bound of the optimal algorithm (as marked by the black vertical line shown in the plot). On the other hand LmaFit and OptSpace are far from exhibiting such a behavior. This signals that Order&amp;Extend devises a querying strategy that is almost op-timal. Interestingly the performance of OptSpace changes dramatically in these cases as compared to the approximate rank datasets. In fact on TrafficSparse and Netflix the error is so high it does not appear on the plot.

Note that the striking superiority of Order&amp;Extend in the case of exact-rank matrices is consistent across all datasets we considered, including others not shown here.
 Figure 6: Recovery process using LmaFit , and Or-der&amp;Extend . Each column is a particular b , increasing from left to right.
 Running times: Though the algorithmic composition is quite different, we give some indicative running times for our algorithm as well as LmaFit and OptSpace . For example, in the Netflix dataset the running times were in the order of 11 000 seconds for LmaFit , 80 000 seconds for Order&amp;Extend , and 200 000 seconds for OptSpace . These numbers indicate that Order&amp;Extend is efficient despite the fact that in addi-tion to matrix completion it also identifies the right queries; the running times of LmaFit and OptSpace simply corre-spond to running a single completion on the extended mask that is randomly formed. Note that these running times are computed using an unoptimized and serial implementation of our algorithm; improvements can be achieved easily e.g., by parallelizing the local condition number computations. Partial completion of Order&amp;Extend : As a final experi-ment, we provide anecdotal evidence that demonstrates the difference in the philosophy behind Order&amp;Extend and other completion algorithms. Figure 6 provides a visual compar-ison of the recovery process of Order&amp;Extend and LmaFit for different values of query budget b . For small values of b , Order&amp;Extend does not have the sufficient information to resolve all incomplete and unstable systems. Therefore the algorithm does not estimate the entries of T corresponding to these systems, which renders the white areas in the two left-most images of Order&amp;Extend . In contrast LmaFit out-puts full estimates, though with significant error. This can be seen by incremental sharpening of the image, compared to the piece-by-piece reconstruction of Order&amp;Extend .
Here we discuss some alternatives we have experimented with, but omitted due to significantly poorer performance. Alternative querying strategies: Order&amp;Extend uses a rather intricate strategy for choosing its queries to T . A natural question is whether a simpler strategy would be suf-ficient. To address this we experimented with versions of Se-quential that considered the same order as Order&amp;Extend but when stuck with a problematic system they queried ei-ther randomly, or with probability proportional (or inversely proportional) to the number of observed entries in a cell X  X  row or column. All these variants were significantly and consistently worse than the results we reported above. Condition number: Instead of detecting unstable systems using the local condition number we also experimented with a modified version of Order&amp;Extend , which characterized a system A x y = t as unstable if its condition number  X  ( A was above a threshold. For values of threshold between 5 and 100 the results were consistently and significantly worse than the results of Order&amp;Extend that we report here, both in terms of queries and in terms of error. Further, there was no threshold of the condition number that would perform comparably to Order&amp;Extend for any dataset.
In this paper we posed the ActiveCompletion problem, an active version of matrix completion, and designed an effi-cient algorithm for solving it. Our algorithm, which we call Order&amp;Extend , approaches this problem by viewing querying and completion as two interrelated tasks and optimizing for both simultaneously. In designing Order&amp;Extend we relied on a view of matrix completion as the solution of a sequence of linear systems, in which the solutions of earlier systems become the inputs for later systems. In this process, recon-struction error depends both on the order in which systems are solved and on the stability of each solved system. There-fore, a key idea of Order&amp;Extend is to find an ordering for the systems in which as many as possible give good estimates of the unobserved entries. However, even in the perfect order problematic systems arise; Order&amp;Extend employs a set of techniques for detecting these systems and alleviating them by querying a small number of additional entries from the true matrix. In a wide set of experiments with real data we demonstrated the efficiency of our algorithm and its superi-ority both in terms of the number of queries it makes, and the error of the reconstructed matrices it outputs. Acknowledgments: This research was supported in part by NSF grants CNS-1018266, CNS-1012910, IIS-1421759, IIS-1218437, CAREER-1253393, IIS-1320542, and IIP-1430145. We also thank the anonymous reviewers for their valuable comments and suggestions. [1] S. Bhojanapalli and P. Jain. Universal Matrix [2] E. J. Cand`es and B. Recht. Exact matrix completion [3] E. J. Cand`es and T. Tao. The power of convex [4] S. Chakraborty, J. Zhou, V. N. Balasubramanian, [5] Y. Chen, S. Bhojanapalli, S. Sanghavi, and R. Ward. [6] G. H. Golub and C. F. V. Loan. Matrix Computations . [7] P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank [8] R. H. Keshavan, A. Montanari, and S. Oh. Matrix [9] R. H. Keshavan, A. Montanari, and S. Oh. Matrix [10] F. J. Kir  X aly, L. Theran, R. Tomioka, and T. Uno. The [11] F. J. Kir  X aly and R. Tomioka. A combinatorial [12] D. W. Matula and L. L. Beck. Smallest-last ordering [13] R. Meka, P. Jain, and I. S. Dhillon. Matrix completion [14] S. Negahban and M. J. Wainwright. Restricted strong [15] B. Recht. A simpler approach to matrix completion. [16] A. Singer and M. Cucuringu. Uniqueness of low-rank [17] D. J. Sutherland, B. P  X oczos, and J. Schneider. Active [18] L. N. Trefethen and D. Bau III. Numerical linear [19] Z. Wen, W. Yin, and Y. Zhang. Solving a low-rank
