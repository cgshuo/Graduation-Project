 We present a novel learning algorithm, DirectRank, which directly and exactly optimizes ranking measures without re-sorting to any upper bounds or approximations. Our ap-proach is essentially an iterative coordinate ascent method. In each iteration, we choose one coordinate and only up-date the corresponding parameter, with all others remain-ing fixed. Since the ranking measure is a stepwise function of a single parameter, we propose a novel line search al-gorithm that can locate the interval with the best ranking measure along this coordinate quite efficiently. In order to stabilize our system in small datasets, we construct a proba-bilistic framework for document-query pairs to maximize the likelihood of the objective permutation of top- X  documents. This iterative procedure ensures convergence. Furthermore, we integrate regression trees as our weak learners in order to consider the correlation between the different features. Experiments on LETOR datasets and two large datasets, Yahoo challenge data and Microsoft 30K web data, show an improvement over state-of-the-art systems.
 I.2 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Performance Learning to rank, supervised learning, direct optimization, ranking measures
Learning-to-rank aims to automatically build a ranking model from training data. Training data consists of queries and documents that are matched with human-labeled rele-vance scores. In testing, the ranking model yields permuta-tions from unseen lists. There are several common ranking measures, including MAP, Precision, MRR and NDCG [21].
Learning-to-rank algorithms can generally be grouped into three categories. The first category is the pointwise ap-proach [13, 16, 32], which assumes that each query-document pair has a numerical or ordinal score. Ranking is formu-lated as a classification or regression problem, in which the rank value of each document is generally computed indepen-dently as an absolute quantity. Methods in this category are fairly similar to conventional machine learning algorithms, and cannot handle pairwise preference and orders.
The second category is the pairwise approach [12, 14, 17, 18, 30], in which the ranked list is decomposed into a set of document pairs. Ranking is treated as a classification problem that can determine which document is better than the other for document pairs. The goal here is to minimize the number of inversions in ranking. For example, Rank-Boost [14] plugs the exponential loss of document pairs into a framework of Adaboost [31]; FRank [35] defines a new bounded loss function called fidelity; RankNet [5] defines a logistic loss for document pairs and uses cross entropy as the loss function, and RankSVM [17, 18] uses SVM to per-form a binary classification on these instances. However, the pairwise approach still ignores the information with re-spect to partial or total orders of retrieved documents. Some recently proposed algorithms, such as LambdaRank [4] and LambdaMART [6], yield good performance. They tackle the problem by defining a smooth approximation to the gradient of the target cost, instead of searching for a smooth approx-imation to the target cost itself. McAllester et al. [22] re-cently proposed a perceptron-like algorithm to directly op-timize loss functions. When applied to ranking problems, the method is a pairwise approach that directly optimizes the ranking measure. Bertsimas et al. [2] models the rank-ing problem by mixed integer programming, which supports many commonly adopted ranking measures. It has shown promising performance on moderate-size data sets, but due to the restriction of current computing ability, this method might be difficult to extend to large data sets.
The third category is the listwise approach. In this ap-proach, the entire ranked list of documents for each query is treated as a training item. Ideally, the listwise meth-ods should directly optimize the ranking measures. How-ever, direct optimization of ranking measures faces a ma-jor difficulty: the ranking measures are non-convex, non-differentiable and discontinuous, due to the fact that the ranking measures are determined by the ranked position of documents rather than an explicit value of each document X  X  ranking score function. Previous studies partially solved this pr oblem by using surrogate functions of ranking measures. These surrogate functions are either not directly related to ranking measures [8, 19, 28, 39], or a continuous and differ-entiable approximation or bounds of ranking measures [9, 11, 20, 26, 34, 37, 40, 41, 42]. Nevertheless, there is still an open question of how to resolve a mismatch between the objective function used in training and the final evaluation criterion used to measure the task performance in testing. The only listwise approach that directly optimizes ranking measure is the coordinate ascent method proposed by Met-zler and Croft [23]. However, it uses heuristics to find a good point and fails to locate the optimal point along each coordinate.

Our work is mainly inspired by the minimum error rate training (MERT) algorithm [24] that is widely used in ma-chine translation. We describe a novel algorithm, Direc-tRank, which trains the ranking model by directly opti-mizing the same ranking measures as used in the testing phase. Our approach adopts the coordinate ascent method, in which one parameter is chosen for tuning, while others are kept unchanged. We observe that when the selected parameter is posed with a small deviation, the ranking mea-sures would not change unless two documents exchange their ranks. Then, it is possible to speed up the optimization of the chosen parameter by only examining those special points that give rise to a change of ranks of any two documents. We call these special points jumping points . Any value for the parameter between two adjacent jumping points corre-sponds to a constant ranking measure. We therefore propose a novel line search algorithm that could efficiently enumerate all jumping points to obtain the optimal interval. Further, to gain better performance in small datasets, in each itera-tion we adopt a probabilistic model to help select a point in the optimal interval such that the likelihood of the objective permutation of documents is maximized.

The time complexity of DirectRank is shown to be related to the number of jumping points. By empirical studies, we show that the number of jumping points is usually so limited that DirectRank can efficiently enumerate them, and thus perform very efficiently. As our framework and algorithms can support arbitrary weak learners (or features), we also apply them into a regression tree based model for the non-linear combination of the features. In each iteration, once a regression tree with respect to the current data distribution is fitted, we call the line search algorithm to assign a suitable weight to maximize the ranking measures. This method proves to be effective in two very large datasets. Last but not least, our algorithms converge into a local coordinatewise optimum, but we empirically verify that our algorithm often finds better solutions than many other systems that use a convex surrogate loss function.

We mainly focus on optimizing the NDCG measure, which is the most commonly used in learning-to-rank tasks, though arbitrary ranking measures can be straightforwardly plugged into our framework. We compare the performance with sev-eral state-of-the-art baselines on small data sets provided by LETOR [27], as well as large datasets, including Yahoo Challenge data [10] and Microsoft 30K web data 1 . http://research.microsoft.com/en-us/projects/mslr/
Suppose that a set of training queries Q s = { q 1 , q 2 ,  X  X  X  q is given, and a set of documents d i = { d i1 , d i2 ,  X  X  X  , d is retrieved for each q i . Let m ( q i ) denote the number of re-trieved documents. We define y i = { y i 1 , y i 2 ,  X  X  X  , y judgment. We define the order r l  X  r l  X  1  X   X  X  X   X  r 2  X  r where  X  means the preference relationship. A T -dimensional feature vector h ( d ij | q i ) = ( h 1 ( d ij | q i ) ,  X  X  X  , h ated for each query-document pair.

The objective of ranking is to construct a ranking func-tion f such that for each query, the retrieved documents can be assigned ranking scores using the function and then be ranked according to the scores. The learning process turns out to be that of optimizing the ranking measure which rep-resents the agreement between the permutation by relevance judgments and the ranking yielded by a ranking function. We first define the ranking function using a linear model, w here the weight vector  X  = (  X  1 ,  X  2 ,  X  X   X  ,  X  T ) is the model parameter. DirectRank can also be applied to non-linear ranking models, introduced in Section 2.4.

In this paper, we use NDCG as the ranking measure. De-normalized sum N ( f , y i , q i ) for each query q i , where Z i is a normalization factor to guarantee N ( f, y the permutation  X  i . Given a training query set Q s , NDCG is denoted by  X  L NDCG ( f ) = 1 n is trimmed at a certain ranking level  X  , so we can change m ( q i ) in Equation (1) to a constant value  X  . Our goal is to search for a model parameter vector  X   X  tha t achieves maxi-mum  X  L NDCG ( f ), i.e.,  X   X  = arg max where T OP  X  ( q i ) is the top  X  documents with respect to the ranking function f . The objective function above is difficult to optimize, as it is non-convex, non-differentiable and discontinuous with respect to  X  , thus we cannot directly use gradient ascent algorithms to optimize.

We resort to an iterative coordinate ascent method. For each iteration, there will be only one coordinate parameter updated, denoted as  X  k , while others stay unchanged. The rationale of this idea is that the ranking function is written as a one-dimensional linear function, Require: Q s = { q i } n i =1 1: for Rep eatedly choose a parameter  X  k do 2: for q i  X  Q s do 3: Call Algorithm 2 for q i 4: // Calculate the NDCG jumping points for q i 5: end for 6: Merge all the jumping points and sort them. 7: Calculate NDCG between the jumping points. 8: end for 9: Get the interval [ L  X  k , R  X  k ] that maximizes NDCG. 10: Pick the coordinates and corresponding intervals that 11: Update the parameter giving the highest likelihood of 12: Repeat 1-11 until convergence.
 second term, we can re-write these two quantities as a ij b , and convert the equation above to, Note that for each document d i,j retrieved by each query q there is a linear function of  X  k . Given an input of  X  k document will get an output score from this linear function. The order of such scores actually reflects the order of the documents which further determines the NDCG value.
This is illustrated in Figure 1, where each of the lines rep-resents a scoring function for a document. At any point of  X  , the rank of the linear function output scores is equiva-lent to the rank of the documents. Note that a slight change of  X  k cannot lead to a jump of NDCG value, unless it is big enough to alter the order of the top- X  documents. Such al-ternation in the order happens only at the point where two lines intersect. We denote the set of such points as jump-ing points . In Figure 1, we have ten lines corresponding to ten documents, which belong to two queries. Intersections ( p 1 , p 2 ,  X  X  X  p 12 ) are jumping points. Theoretically, we can search all the intersections to acquire all possible snapshots of ranked documents. Because any two non-parallel lines will form an intersection, the total number of intersections then but unnecessary, because in real-world applications we are merely interested in the rank of the top- X  documents, the NDCG metric is always truncated to a certain level  X  , and usually  X   X  10. As a result, the jumping point size between top- X  documents is quite limited, and increase less than lin-early as the document size increases. This will be examined by experiments in section 3.3.

Therefore, we can efficiently find all the jumping points on one coordinate. The coordinate ascent algorithm is de-scribed in Algorithm 1. For each coordinate, we exploit such a line search algorithm as following: For each query, we ob-tain all of its jumping points (Line 2  X  5 in Algorithm 1). For instance, in Figure 1, ( p 1 , p 2 ,  X  X  X  , p 5 ) are the jumping of q 2 . Next, the jumping points of all queries are merged and sorted (Line 6). Then, NDCG can be exactly computed, be-cause the objective is a stepwise function. Between any two adjacent jumping points, the top- X  documents of any query stay unchanged, so does the NDCG value (For example, in Figure 1, the NDCG will not change between p 3 and p 10 ).
In section 2.2, we introduce an efficient line search algo-rithm, which can quickly enumerate all intervals with opti-mal NDCG values in each coordinate ascent iteration. In section 2.3, to alleviate the instability on small training datasets, we adopt a probabilistic model to maximize the likelihood of top- X  documents given a set of ranking by human judgment labels. We progress to DirectRank with nonlinear ranking functions, by exploiting regression trees in section 2.4. Finally, in section 2.5, we present a series of theoretical analysis and proofs on the proposed method with respect to time complexity, convergence and consistency.
We again use Figure 1 to illustrate the key ideas in the line search algorithm. Suppose there are only two queries in the training set, q 1 and q 2 , each of which consists of five documents that are denoted as d 1 = { d 11 , d 12 ,  X  X  X  , d d 2 = { d 21 , d 22 ,  X  X  X  , d 25 } respectively. We assign their rele-we intend to maximize NDCG @ 3 , and obtain the TOP 3 ( q on the direction of the k -th coordinate. This means we ap-ply the coordinate ascent method and search the optimal value of the k -th parameter  X  k along the k -th coordinate, while fixing all other parameters.

As shown in Figure 1, ( p 1 , p 2 ,  X  X  X  , p 5 ) are the jumping of q 2 . Table 1 and 2 respectively show the top- X  documents selected between those jumping points, and the changes of rank are noted as bold. Clearly, for a given query, a change of document rank order happens when two linear functions of documents intersect at a jumping point. For example, for q , its linear functions of d 11 and d 12 intersect at p 1 , which causes a switch of rank between d 11 and d 12 in Table 1. Algorithm 2 shows how to precisely compute these points. First, we use selection sort to determine top- X  documents with smallest a ij , which is the slope of the lines in Figure 1. This order is actually the rank when  X  k takes negative infin-ity (Line 2  X  4 in Algorithm 2). Here we just make sure the top- X  documents are ranked by the scores. Next, we repeat the following procedure in order to find the next intersec-tion that leads to a change of top- X  documents (Line 6  X  25). Since the top- X  documents have already been sorted, for each of the top  X  -1 documents, we just calculate their intersection with the candidate right below it (Line 8  X  13). On the other hand, for the candidate at the rank of  X  , all the documents below it have to be scanned, because they might not be sorted, which means any of these documents might be just below the  X  -th candidate (Line 14  X  19). We choose the minimum one from these intersections, which is the next jumping point (Line 21). We update the rank ac-cording to the two intersected documents on this jumping point (Line 23). Standing on the current jumping point, we repeat the procedure above to locate the next jumping point. Such procedure does not terminate until all the jump-ing points are obtained. For example, as for q 1 in Figure 1, its jumping points will generate from left to right as a se-quence: p 1  X  p 2  X  p 3  X  p 4  X  p 5 . The jumping points of q and q 2 are then merged and sorted, and NDCG between them can be easily calculated. The stepwise NDCG values are shown at the bottom of Figure 1.

Note that when calculating the NDCG values between the jumping points, it is completely unnecessary to re-calculate all the training queries. Instead, since at each jumping point there are only a very small number of candidates (usually two) in one query switching their rank order, we can up-date NDCG values in an incremental manner. We store the initial NDCG values for each query when  X  k  X   X  . Whenever a jumping point is found (Line 21), the NDCG of the corresponding query is updated only by the rank switch of the candidates. For example, as shown in Figure 1 and Table 1, when p 2 &lt;  X  k &lt; p 3 , the NDCG of q 1 is 0.889. At p 3 , only d 11 and d 15 are switched between the rank 1 and 2, and the NDCG for q 1 can simply be updated as 0 . 889 + 1 Z
There are two ways to choose the coordinate in each iter-ation. One is cyclic, which repeatedly update every coordi-nate in the cycle. The other is greedy, which within a single iteration, all coordinates are tried ahead, and then Direc-tRank updates the parameter with the maximum NDCG. Require: q , d = { d i } , an d  X  1:  X   X  X  lines for each d i following Formula 5 } 2: S  X   X  X   X  i |  X  i  X   X  , with top- X  smallest slope }  X  top- X  3: S b  X  {  X  i |  X   X  X   X  }  X  the documents out of top- X  4: CurrP  X  X  X  X   X  Current jumping point 5: JumpS  X  X }  X  All jumping points 6: repeat 7: CandS  X  X } 8: for i = 1 to |S  X  | X  1 do 9: p  X  Intersection( S i  X  , S i +1  X  ) 10: if p &gt; CurrP then 11: CandS  X  CandS + { p } 12: end if 13: end for 14: for i = 1 to |S b | do 15: p  X  Intersection( S |S  X  |  X  , S i b ) 16: if p &gt; CurrP then 17: CandS  X  CandS + { p } 18: end if 19: end for 20: if CandS 6 = {} then 21: Currp = min { CandS }  X  get next jumping point 22: JumpS  X  JumpS + { Currp } 23: Update S  X  , S b  X  exchange the lines associated 24: end if 25: until CandS = {} We observe that the greedy approach shows no obvious im-pro vement compared to the cyclic way, and is much slower. So we choose the cyclic pattern for experiments.
Since NDCG is a stepwise function with a single weight  X  , thus there are infinite alternative points in a NDCG-optimal interval. We found that, in small data sets a suitably chosen point improves the stability of our framework, while in big data sets, e.g. Yahoo challenge and Microsoft 30K web, simply taking the middle point of the best interval runs very well. Thus, our system adds the technique in this subsection for small data sets by default, and removes it for big data sets to gain a higher speed.

We exploit a surrogate to choose a point within the inter-val. DirectRank adopts a probabilistic model in which the only parameter is  X  k , and then maximizes the likelihood over an objective permutation. Note that the objective permu-tation is defined merely based on the human-labeled judg-ment scores and fixed during the training. Here we use the Plackett-Luce model [25], although any continuous surrogate of NDCG can be used. The Plackett-Luce model treats the ranking procedure as a random selection sequence without replacement. Similar to [8], we define the log probability of the top- X  documents for a specific permutation as, log P  X  k ( m (  X  i ); q i , f ) = where j and l are the rank indices and  X  (  X , j ) denotes the document of the position j in a permutation  X  . The log probability is continuous, differentiable, and concave [3]. We ma ximize the log likelihood of the objective permutation by the binary search method [1]. This surrogate function opti-mization is still performed within the best-NDCG interval. Therefore, it is basically a supplementary step for the direct NDCG optimization in section 2.2, in order to distinguish the points with the same NDCG.
In this section, we describe how to integrate regression trees into our framework effectively and conveniently. We follow a stage-wise strategy. That is, Algorithm 2 is invoked to tune the optimal weight after each new tree is generated. Here, we use a least-square regression tree, called the MART tree, which is mainly introduced in [15].

MART, one of powerful regression tree based models, aims to construct an ensemble of tree-based weaker learners h tree such that the resulting new scoring function f ( d i ) = P h measure of square loss. Here d i denotes a document, and r is the relevance by human judge of a document in learning-to-rank task. The MART method bypasses the difficulties of combinatory optimization of ranking by regression. When weak learners are rich enough, such as with deeper depth of trees, generated models from MART are indeed capable of approaching the regression objective very well, and hence lead to better performance than other linear models.
However, MART was not designed to optimize the ob-jective, and Friedman [15] suggests the tree X  X  weight  X  t tuned towards specific goals. A similar approach also ap-peared in [43], where another type of regression tree is con-structed and combined with a brute search line search. In that work, no significant improvements have been observed. In our framework, we use MART trees as our weak learners, and in order to enhance the stability, we follow the trick in [43] to restrict the new weight  X  k in range [ a, b ], where we empirically set the hyper-parameters between [0 . 1 , 0 . 5] in our experiments. If the output of Algorithm 2 is beyond the range, we just take the border values.
We give an empirical analysis on the complexity of the line search algorithm (Alg. 2). Line 8  X  13 and Line 14  X  19 show that to locate each jumping point, the line search algorithm goes through all the documents once. For a query with m retrieved documents, the proposed algorithm enumerates all jumping points along one coordinate in O ( m  X  v ), where v is the number of jumping points. v is related to the data distribution, which is hard to be denoted as a function of m . But we can study the empirical relation between them in Table 12. In LETOR, when m equals 100, v/m is around 0.5; when m increases to 1000, this ratio decreases to around 0.062. The runtime of DirectRank is in Table 13. We compare our algorithm to the exhaustive line search. For simplicity, we just consider one query. The latter enu-merates all the intersections and sorts them, then computes ranking measures from left to right incrementally. Its time complexity is O ( m 2 + m 2 log( m 2 ) + m 2 ) = O ( m 2 log( m It is easy to see that the exhaustive algorithm runs faster than DirectRank when v is approximately O (2 m log m ).
According to [38], the principle of empirical risk minimiza-tion is consistent if it provides a sequence of loss functions for which both expected risk and empirical risk converge to the minimal possible value of the expected risk. Apparently directly optimizing ranking performance measures such as NDCG is an example of empirical risk minimization. To make our presentation easier, let  X  X be the space of the fea-ture vectors in which the documents are represented and are typically derived from the query-document pairs. Let  X  Y be the space of the relevance scores each document re-ceives. Thus for any query q that sampled from the query space Q , we have a list X = ( X 1 ,  X  X  X  , X m )  X  X :=  X  of document feature vectors, and a corresponding list Y = ( Y 1 ,  X  X  X  , Y m )  X  X   X   X  Y of document relevance scores. Then the expected NDCG measure can be written as
Following the standard proof of empirical risk minimiza-tion [38], it can be shown that directly optimizing the em-pirical NDCG measure is consistent as stated below. Theorem 1. Denote f  X  n as the ranking function that maxi-mizes the empirical NDCG  X  L NDCG ( f ) over n query-document pairs, and f  X  as the ranking function that maximizes the ex-pected NDCG measure L NDCG ( f ) for a fixed but unknown distribution over the probability space of query-document pairs. Then L NDCG ( f  X  n )  X  L NDCG ( f  X  ) as n  X  X  X  .
The empirical NDCG is non-convex, non-differentiable and discontinuous, but we can prove the convergence of the pro-posed coordinate ascent algorithm as stated below. Theorem 2. The proposed coordinate ascent algorithm con-verges to a set of local coordinatewise maximum solutions. The proofs for both theorems are given in [33].

Even though consistent, DirectRank has a hard time find-ing the ranking function that is the solution of the global maximum of the empirical NDCG measure. The consistency of a family of listwise surrogate functions for the NDCG measure is proved in [29]. Optimizing the empirical concave surrogate objective turns out to be a convex programming problem with considerable computational advantages and the guarantee of the global optimal empirical surrogate ob-jective (but not the optimal empirical NDCG measure), and for which learned ranking functions remain consistent. How-ever, in practice, the size of the training dataset is always limited and finite, the ranking function learned by optimiz-ing listwise surrogate functions do not correspond to even a local coordinatewise maximum solution of empirical NDCG measure, and the ranking function learned by directly opti-mizing the empirical NDCG measure does correspond to a local coordinatewise maximum solution.

The experiments we conducted below show that Direc-tRank not only always reaches a higher NDCG measure on training data than other baselines whose surrogate objec-tives are concave, but also gives a higher NDCG measure on test data. This shows that the local optimal problem of directly optimizing ranking performance measures is less se-rious than the mismatch between the training objective and the test objective introduced by a surrogate objective.
We study the performance of the proposed algorithm in both small and large datasets. We first evaluate DirectRank with a linear ranking function. We try to clarify two is-sues: (1) DirectRank often reaches higher ranking measures on training data than other baselines with surrogate objec-tives. (2) DirectRank proves to be efficient in runtime and stable in handling large datasets. Moreover, we compare the regression-tree version of DirectRank with LambdaMART, the champion of Yahoo Learning-to-rank Challenge.
In this subsection, we restrict the ranking function of Di-rectRank to a linear model and conduct a series of exper-iments on Microsoft LETOR datasets, which are relatively small, as well as two large datasets, including Yahoo Chal-lenge data and Microsoft 30K web data.
The LETOR datasets website released several benchmark datasets, baselines and evaluation tools. We evaluate Direc-tRank on all nine datasets in LETOR. For each datasets, five fold partitions for cross validation and their baseline results have been published.

We compare DirectRank with several state-of-the-art learn-ing to rank algorithms in LETOR, as shown in Figure 2. We train our models on NDCG@5 only and evaluate them on NDCG@1  X  NDCG@10. We repeat our training proce-dure 50 times with different initial parameters. Similar to other LETOR baselines, we choose the iteration with the best MAP of validation sets for testing.

Figure 2 provides the the average results of five folds for different learning to rank algorithms in terms of NDCG at each of the first 10 truncation levels. Since these algorithms perform differently on different datasets, to evaluate the overall performance, we use the concept of winning num-ber W i that is introduced by Liu [21] for each algorithm, which counts the number of how many times the algorithm beats other algorithms over all datasets.
 where n is the number of datasets, k is the indexes of com-pared algorithms. The larger W i is, the better the algorithm performs. Due to the different numbers of baseline algo-rithms in LETOR 3.0 and 4.0, we present winning numbers separately in Table 3 and Table 4. For LETOR 3.0, n = 7 and m = 7; for LETOR 4.0, n = 2 and m = 5.
 From Table 3, we observe that SmoothRank [11] and List-Net [8] are the best two baseline algorithms. And gener-ally speaking, listwise algorithms outperform pairwise algo-rithms, except AdaRank-NDCG. On LETOR 3.0 datasets, DirectRank ranks highest on all NDCG levels. For LETOR 4.0 in Table 4, DirectRank wins except on NDCG@1.
From Figure 2, we can see that out of the nine LETOR datasets, there are six datasets for which DirectRank gen-erally gives the best results, including TD2003, NP2004, TD2004, OHSUMED, MQ2007 and MQ2008.

Compared with other algorithms, DirectRank performs more stably. For example, AdaRank-MAP is the best base-line on HP2004, but it does not fit well in TD2003 and NP2004. SmoothRank generally performs better than other baselines except on TD2003, in which SmoothRank is ranked in the middle. In comparison, DirectRank generally achieves relatively good performance on most of the datasets.
Using Yahoo Challenge Data and Microsoft 30K web data (Table 5), we evaluate DirectRank on accuracy, efficiency and stability. To be consistent, we follow the evaluation ap-proach of Yahoo Challenge, adopting its NDCG formula on both datasets and using NDCG@10 to be the main evalua-tion criterion. Microsoft dataset releases five cross validation folds, so we report the averaged results.

DirectRank randomizes the initial points 20 times, and picks up the iteration with the best NDCG on the valida-tion dataset. The winner of the Yahoo Challenge is Lamb-daMART [6], which combines MART [15] and LambdaRank [4]. Since we use a linear function in DirectRank in this sub-section, to have a fair comparison, we compare it with Lamb-daRank, whose ranking function is also linear. Table 6 shows that DirectRank outperforms LambdaRank on NDCG@10. And Tables 6 and 7 report our algorithm performance on Ya-hoo and Microsoft data. We miss the results of RankBoost in Table 7 because of the 64G memory limitation. We also com-pare DirectRank with other baselines, such as SmoothGrad [20], AdaRank, simple coordinate ascent [23], and Rank-Boost [14]. In addition, we compare the proposed algorithm with consistent-RankCosine [29], whose objective is proved to be a surrogate consistent with NDCG. For LambdaRank, we adopt the result on the testing set reported on Yahoo Challenge Workshop. SmoothGrad X  X  code is from BMRM (http://users.cecs.anu.edu.au/%7Echteo/BMRM.html). The results of consistent-RankCosine are generated from the MAT-LAB program provided by the author of [29]. The rest of the baselines are acquired from RankLib, an open source learning-to-rank package written in Java (http://www.cs. umass.edu/%7Evdang/ranklib.html). We modified the code to adopt the NDCG formula used in Yahoo Challenge. Di-rectRank yields a general best performance. Consistent-RankCosine has a performance close to DirectRank on Ya-hoo data, and DirectRank X  X  advantage looks better on Mi-crosoft data. This might reflect the claim in section 2.5, in which even a large scale of queries is insufficient to guarantee the consistency for those methods with surrogate objectives, so an algorithm whose optimized objective is the exact rank-ing measure is more likely to yield a better performance.
As explained in Section 2.1, the NDCG value is usually truncated by the top- X  documents. Therefore, the pro-posed algorithm performs especially efficiently on optimizing NDCG, where the number of jumping points is very small. Here we also examine the performance of DirectRank on ex-pected reciprocal rank (ERR) and mean average precision (MAP), in which documents are usually not truncated by a certain level. However, as shown in section 2.5, when there are too many jumping points, DirectRank will be slower than a simple brute force search. We have two choices to adapt DirectRank to optimize ERR and MAP. (1) We re-place Algorithm 2 with the brute force search. Table 8 indi-cates a competitive performance compared to LambdaRank and better performance than other baselines on Yahoo Data. Particularly, this work shared the coordinate ascent frame-work with [23], in which it fails to find the exact optimum in each coordinate iteration. This difference might be indicated from the performance of DirecRank (DR) and coordinate as-cent (CA) in Table 8. (2) In order to speed this up, we can also approximate ERR and MAP with a truncation (eg. 10), for which we observe a performance moderately better than coordinate ascent (CA) on Yahoo data.
As tree-based models generally outperform linear models, we compare our system with two state-of-the-art systems, MART and LambdaMART on two large datasets. The max-imum number of trees is set to 1000. In Yahoo data, the number of leaf nodes is set to 10, and more leaves do not contribute to the final performance significantly with respect to the official measure NDCG@10. On Microsoft 30K web data, we adjust the number of leaf nodes as 10, 30 and 50.
Since our DirectRank is constructed based on MART, the consistent improvements in two large datasets verify our mo-tivation. Wu et al. [43] did a similar trial and received no improvement; we conjecture one of the reasons may be that the regression trees in MART are less prone to overfit data than trees in LambdaMART, because we observe that LambdaMART does boost objective measures quite quickly.
Also, DirectRank shows significant superiority to NDCG@1 over Microsoft 30K web data, especially when the number of leaf nodes is quite small, and in other cases (Table 9 2 and 10) DirectRank still performs slightly better. We find the average number of documents per query is greatly different in the two datasets, about 23 in Yahoo dataset and 72 in Microsoft data. Since MART treats all documents equally, more documents may in some sense have a negative influence on the objective NDCG@1; thus, it would be more likely to acquire improvement by adopting an accurate objective. When the number of leaf nodes increases, the two baselines improve significantly in NDCG@1, while our DirectRank is more stable and effective in performance. Moreover, even for a small number of leaf nodes, DirectRank works very well. Finally, MART in [36] gets a higher performance by using a complete binary tree with different depths, and all tree-based algorithms here are implemented in a fair manner by restricting the maximum number of leaf nodes.
In this section, we use linear models as an example to analyze DirectRank from the aspects of running efficiency and stability. We first show that as the document size of each query increases, the total number of jumping points does not increase to the same extent. For example, in the two datasets illustrated by Table 12, each query has about 1000 documents. If we fix the truncation level at 5, and increase the document size from 50 to 1000, the average jumping points per query mildly increase, and converge after the document size is larger than 500. This indicates that a large amount of documents will never appear in the top 5 list, and do not affect the value of NDCG@5.

Illustrated by Table 6, 7 and 11, DirectRank often achieves better training NDCG than other baselines using convex sur-rogates on both small-and large-scale datasets, which sug-gests a local optimum of direct measure is not necessarily worse than the global optimum of convex surrogates. This point is also suggested by McAllester et al. [22]. However, in the LETOR dataset, in order to prevent overfitting, we choose parameters using cross validation data. In the larger datasets, the algorithms in comparison show a consistent performance on both training and testing data.

We implement DirectRank with C++ and run on a single core with a 2.5GHz Opteron. It takes about 10 minutes for a single round, which enumerates all 519 features. As shown in Table 13, given a random starting point, DirectRank con-verges after around 20 rounds and takes 3.3 hours. Smooth-Grad [20] is the fastest, but it does not perform as well as DirectRank in large datasets.

Also, we examine the influence brought by different start-ing points. We run our program for 100 random initial points on the two large datasets, and obtain the standard deviation of NDCG@10, 0.756  X  0.0023 on Yahoo data, and 0.457  X  0.0026 on Microsoft data. It shows that our algo-rithm might not be that sensitive to the initial point, when the data is large enough. In this paper, we propose a novel line search algorithm DirectRank in the coordinate ascent framework to exactly optimize non-smooth ranking measures. In contrast to other listwise methods, DirectRank can exactly optimize any rank-ing measures instead of resorting to upper bounds or approx-imations. DirectRank offers several advantages: efficiency in training, convergence/consistency guarantees, and high accuracy in ranking. Combined with regression-trees, Di-rectRank proves more powerful. Experiments on small-and large-scale datasets show DirectRank generally outperforms several state-of-the-art baseline systems.

The current algorithm allows only one parameter for op-timization per iteration. We are considering combining the Powell algorithm with our line search to realize a fast multi-dimensional optimization. When taking regression trees as weak learners, it is interesting to compare the stage-wise and leveraging strategies, the latter generating all regression trees first and then leveraging their weights. We are also exploring the possibility that the regression trees are con-structed by optimizing ranking measures directly, instead of borrowing from an extra regression model. Finally, since it is now understood that no algorithm that learns a scoring function using a convex surrogate can be consistent with respect to ERR and MAP [7], we will further study the con-sistency of DirectRank on those measures. This research is partially supported by Air Force Office of Scientific Research under grant FA9550-10-1-0335, the Na-tional Science Foundation under grant IIS RI-small 1218863 and a Google research award. [1] M. Bazaraa, H. Sherali and C. Shetty. Nonlinear [2] D. Bertsimas, A. Chang and C. Rudin. Integer [3] S. Boyd and L. Vandenberghe. Convex Optimization. [4] C. Burges, R. Ragno and Q. Le. Learning to Rank with [5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [6] C. Burges. From RankNet to LambdaRank to [7] C. Calauzenes, N. Usunier, P. Gallinari. On the [8] Z. Cao, T. Qin, T. Liu, M. Tsai and H. Li. Learning to [9] S. Chakrabarti, R. Khanna, U. Sawant, and C. [10] O. Chapelle and Y. Chang. Yahoo! Learning to Rank [11] O. Chapelle and M. Wu. Gradient Descent [12] W. Cohen, R. Schapire, and Y. Singer. Learning to [13] K. Crammer and Y. Singer. Pranking with Ranking. [14] Y. Freund, R. Iyer, R. Schapire and Y. Singer. An [15] J. Friedman. Greedy Function Approximation: A [16] E. Harrington. Online Ranking/Collaborative [17] R. Herbrich, T. Graepel, and K. Obermayer. Support [18] T. Joachims. Optimizing Search Engines Using [19] J. Kuo, P. Cheng, and H. Wang. Learning to Rank [20] Q. Le and A. Smola. Direct Optimization of Ranking [21] T. Liu. Learning to Rank for Information Retrieval. [22] D. McAllester, T. Hazan, and J. Keshet. Direct Loss [23] D. Metzler and W. Croft. Linear Feature-Based [24] F. Och. Minimum Error Rate Training in Statistical [25] R. Plackett. The Analysis of Permutations. Applied [26] T. Qin, T. Liu, and H. Li. A General [27] T. Qin, T. Liu, J. Xu, and H. Li. LETOR: A [28] T. Qin, X. Zhang, M. Tsai, D. Wang, T. Liu, and [29] P. Ravikumar, A. Tewari, and E. Yang. On NDCG [30] C. Rudin. The P-Norm Push: A Simple Convex [31] R. Schapire and Y. Freund. Boosting: Foundations [32] A. Shashua and A. Levin. Ranking with Large Margin [33] M. Tan, T. Xia, L. Guo and S. Wang. Direct [34] M. Taylor, J. Guiver, S. Robertson and T. Minka. [35] M. Tsai, T. Liu, Q. Tao, H. Chen and W. Ma. Frank: [36] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin. [37] H. Valizadegan, R. Jin, R. Zhang, and J. Mao. [38] V. Vapnik. Statistical Learning Theory. John Wiley, [39] F. Xia, T. Liu, J. Wang, W. Zhang, and H. Li. [40] J. Xu, T. Liu, M. Lu, H. Li, and W. Ma. Directly [41] J. Xu and H. Li. AdaRank: A Boosting Algorithm for [42] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [43] Q. Wu, C. Burges, K. Svore, and J. Gao. Adapting
