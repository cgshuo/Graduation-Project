
Probabilistic topic modelling, a methodology for reducing high dimensional data vectors to low dimensional represen-tations, has a successful history in exploring and predicting the underlying semantic structure of documents, based on a hierarchical Bayesian analysis of the original texts [1], [2]. Its fundamental idea is that documents can be taken as mixtures of topics, each of which is a probability distribution over words, under the  X  bag-of-words  X  assumption.

Nowadays, topic modelling has been receiving increasing attention in both data mining and machine learning com-munities. A variety of topic models have been developed to analyze the content of documents and the meaning of words. These include models of words only [2], of topic trends over time [3] X  X 6], of word-order with Markov dependencies [7], of words and supervised information, e.g. , authors [8], hierarchical structure of topics) [10], [11], of segments in
Different from previous topic models, this paper presents a novel variant of the Latent Dirichlet Allocation (LDA) model [2], a topic model, called Sequential Latent Dirichlet Allocation (SeqLDA), that explicitly models the underly-ing document structure. Although in this work, we have restricted ourselves to the study of the sequential topic structure of a document, that is how a sub-idea is closely related to the preceding and subsequent segments. The progressive topical dependency is captured using a nested extension of the two-parameter Poisson-Dirichlet process (PDP) [13], [14], based on recent theoretical results in finite discrete spaces [15]. The nested PDP is defined as u i  X  PDP ( a i ,b i , u i  X  1 ) , where a i is a discount parameter, b is a strength parameter, and u i  X  1 is base distribution for u i in a recursive fashion, as those in [16]. The advantage of using the PDP in a nested way is that it allows us to integrate out the real valued parameters, i.e. , the PDP is self-conjugate. We also develop here a collapsed Gibbs sampler for this nested case that provides an efficient and space compact sampler for the PDP.

Considering this sequential structure, we can explore how topics are evolving among, for example, paragraphs in an essay, or chapters in a novel; and detect the rising and falling of a topic in prominence. The evolvement can be estimated by exploring how the topic proportion changes in segments. Obviously, tackling topic modelling together with the topical structures buried in documents provides a solution for going beyond the  X  bag-of-words  X  assumption.

Existing studies about topic evolvements or trends focus mainly on time series, i.e. , topics over time, rather than those hiding inside each document. They explore how topics change, rise and fall, by taking into account timestamps and other information ( e.g. , citations) associated with documents. Blei and Lafferty [3] propose a dynamic topic model (DTM) to capture the topic evolution in document collections that are organized sequentially into several discrete time periods, and then within each period an LDA model is trained on its documents. Wang and McCallum [4] present another topic over time (ToT) model, a non-Markov continuous time topic model. Instead of training an LDA model for documents in each time stamp, ToT assumes that words and timestamps can be jointly generated by latent topics. Indeed, timestamps in ToT are treated as supervised information. Leveraging the citation information, He et al. [17] develop inheritance topic model to understand topic evolution. Sig-nificantly, the difference between these models and our SeqLDA model is that, instead of modelling topic trends in document collections based on documents X  timestamps, we model topic progress within each individual document by capitalizing on the correlations among its segments, i.e. , the underlying sequential topic structure, according to the original document layout. Moreover, compared to [3], the Markov dependencies in our model are put on distributions over topics, rather than distributions over words. In such a
The development of a sequential structural generative model according to the above idea is based on nested PDPs, and models how the sub-idea of a segment is correlated to its preceding and following segments. Specifically, the correla-tion is simulated by the progressive dependency among topic distributions. That is, the j th segment topic distribution  X  i,j is the base distribution of the PDP for drawing the ( j + 1) th segment topic distribution  X  i,j +1 ; for the first segment, we draw its topic distribution  X  i, 1 from the PDP with document topic distribution  X  i, 0 as the base distribution. The strength parameter b and discount parameter a control the variation between the adjacent topic distributions. Figure 1(a) shows the graphical representation of the SeqLDA model. Shaded and unshaded nodes indicate observed and latent variables respectively. An arrow indicates a conditional dependency between variables, and plates indicate repeated sampling.
In terms of a generative process, the SeqLDA model can be also viewed as a probabilistic sampling procedure that describe how words in documents can be generated based on the latent topics. It can be depicted as follows: Step 1 samples the word distribution for topics, and Step 2 samples each document by breaking it up into segments: 1) For each topic k in { 1, . . . , K } 2) For each document i We have assumed the number of topics ( i.e. , the dimension-ality of the Dirichlet distribution) is known and fixed, and the word probabilities are parameterized by a K  X  W matrix  X  = (  X  1 ,...,  X  K ) , and will be estimated though the learning process.  X  i, 0 is sampled from the Dirichlet distribution with prior  X  , and others are sampled from the PDP. Both the Dirichlet distribution and the PDP are conjugate priors for the multinomial distribution, and the PDP is also self-conjugate in a sense. Choosing these conjugate priors makes within each document. Though for simplicity, we fix a and b for each document collection in all our experiments.
In order to use the SeqLDA model, we need to solve the key inference problem which is to compute the posterior distribution of latent variables ( i.e. topic distributions  X  i, 0: J and topic assignment z ) given the inputs ( i.e.  X  ,  X  , a and b ) and observations w . Unfortunately, this posterior distribution cannot be computed directly because of the intractable computation of marginal probabilities. As a consequence, we must appeal to approximate inference techniques, where some of the parameters ( i.e.  X  i, 0: J and  X  in our case) can be marginalized out, rather than explicitly estimated. In topic modeling literature, two standard approximation methods have often been used: variational inference [2] and Gibbs sampling [18]. Here, we pursue an alternative approximating strategy using the latter by taking advantage of the collapsed Gibbs sampler for the PDP [15].
 Gibbs sampling is a special form of Markov chain Monte Carlo (MCMC) simulation which should proceed until the Markov chain has  X  X onverged X  to its stationary state. Al-though, in practice, we run it for a fixed number of iterations. Collapsed Gibbs sampling capitalizes on the conjugacy of priors to compute the conditional posteriors. Thus, it always yields relatively simple algorithms for approximate inference in high-dimensional probability distributions by the stationary behavior of a Markov chain. Note that we use conjugate priors in our model, i.e. Dirichlet prior  X  on  X  i, 0 and  X  on  X  , PDP prior on  X  i,j (PDP is self-conjugate); thus  X  i, 0: J and  X  can be integrated out.

In this section, we derive the collapsed Gibbs sampling algorithm for doing inference in the proposed model. Table II lists all the statistics required in our algorithm. Our PDP sampling is a collapsed version of what is known as the nested Chinese restaurant process (CRP) used as a component of different topic models [11]. The basic theory of the CRP and our collapsed version of it is summarized in Appendix A. The CRP model goes as follows: a Chi-nese restaurant has an infinite number of tables, each of which has infinite seating capacity. Each table serves a B. The Collapsed Gibbs sampler
In each cycle of the Gibbs sampling algorithm, a subset of variables are sampled from their conditional distributions with the values of all the other variables given. In our case, the distributions that we want to sample from is the posterior distribution of topics ( z ), and table counts ( t ), given a collec-tion of documents. Since the full joint posterior distribution is intractable and difficult to sample from, in each cycle of Gibbs sampling we will sample respectively from two conditional distributions: 1) the conditional distribution of topics assignments for all the other words and all the table counts; 2) the conditional distribution of table count ( t i,j,k ) of the current topic given all the other table counts and all the topic assignments.

In our model, documents are indexed by i , segments of each document are indexed by j according to their original layout, and words are indexed by l . Thus, with documents indexed by above method, we can readily yield a Gibbs sampling algorithm for the SeqLDA model as: for each word, the algorithm computes the probability of assigning the current word to topics from the first conditional distribution, while topic assignments of all the other words and table counts are fixed. Then the current word would be assigned to a sampled topic, and this assignment will be stored for being used when the Gibbs sampling cycles through other words. While scanning through the list of words, we should also keep track of the table counts for each segment. For each new topic that the current word is assigned to, the Gibbs sampling algorithm estimates the probabilities of changing the corresponding table count to different values by fixing all the topic assignments and all the other table counts. These probabilities are computed from the second conditional distribution. Then, a new value will be sampled and assigned to the current table count. Note that the values for the table count should be subject to some constraints that we will discuss in detail when we derive the two conditional distributions below.

Consequently, the aforementioned two conditional distri-butions we need to compute are, respectively, where z i,j,l = k indicates the assignment of the l th word in the j th segment of document i to topic k , z 1: I  X  X  z i,j,l } presents all the topic assignments not including the l th word, and t 1: I, 1: J i  X  { t i,j,k } denotes all the table counts except for the current table count t i,j,k . Following the CRP formulation, customers are words, dishes are topics and restaurants are segments in our case. All restaurants share a finite number of dishes, i.e. , K dishes. From Equation (1) and also seen from Equation (6) in the appendix, tables of ( j + 1) th restaurant are customers of j th restaurant in nested CRPs. These counts have to comply with the following u i,k = 1 , we have
When 1 &lt; u i,k  X  j , the conditional probability is
When j &lt; u i,k , it is simplified to where the dash indicates statistics after excluding the current topics assignment z i,j,l .

After sampling the new topic for a word, we need to stochastically sample the table count for this new topic, say k . Although we have summed out the specific seating arrangements ( i.e. different tables and specific table assign-ments) of the customers in the collapsed Gibbs sampler for the PDP, we still need to sample how many tables are serving dish k ( i.e. topic k in our model), given the current number of customers ( i.e. words) eating dish k . The value of t i,j,k should be in the following interval: the current state of topic assignment of each word, the conditional distribution for table count t i,j,k can be obtained by similar arguments, as below.
The collapsed Gibbs sampling algorithm for our proposed model is outlined in Figure 2. We start this algorithm by randomly assigning words to topics in [1 ,...,K ] , and if the total number of customer, n i,j,k + t i,j +1 ,k , is greater than zero, the table count t i,j,k is initialized to 1. Each Gibbs sampler then continues applying Eq. (2), Eq. (3) or Eq. (4) to every word in the document collection; and applying Eq. B. Document modelling
We first follow the standard way in document modelling to evaluate the per-word predicative perplexity of the SeqLDA model and the LDA model. The perplexity of a collection of documents is formally defined as: exp n  X  where w i indicates all words and N i indicates the total number of words in document i respectively. A lower per-plexity over unseen documents means better generalization capability. In our experiments, it is computed based on the held-out method introduced in [8]. In order to calculate the likelihood of each unseen word in SeqLDA, we need to integrate out the sampled distributions ( i.e.  X  and  X  ) and sum over all possible topic assignments. Here, we approximate the integrals using a Gibbs sampler for each sample of assignments z , t .

In our experiments, we run each Gibbs sampler for 2,000 iterations with 1,500 burn-in. After the burn-in period, a total number of 5 samples are drawn at a lag of 100 iterations. These samples are averaged to yield the final trained model. In order to make a scientific comparison, we set hyper-parameters fairly, since they are important for the two models. Instead of using symmetric Dirichlet priors, we employ the moment-match algorithm [20] to estimate  X  from data for LDA. For our SeqLDA model, we empirically choose parameters without optimization as: a = 0 . 2 , b = 10 ,  X  = 0 . 1 . And  X  is set to 200 /W for both models. Note that we seek to optimize the parameter settings for the LDA model, which enables us to draw sound conclusions on SeqLDA X  X  performance.

Figure 3 demonstrates the perplexity comparison for dif-ferent number of topics. The LDA model has been tested on document level (LDA D) and paragraph level (LDA P) separately. We have also run the SeqLDA model with or without being boosted by either LDA D (SeqLDA D) or LDA P (SeqLDA P). The boosting is done by using the topic assignments learnt by the LDA model to initialize the SeqLDA model. As shown in the figure, our SeqLDA model, either with or without boosting, consistently performs for instance, her/his stories in a book or her/his ideas in an essay. Here, we test SeqLDA on the two books with following parameter settings: a = 0 ,  X  = 0 . 5 , k = 20 , b = 25 for  X  X he Prince X , and b = 50 for  X  X he Whale X .

To compare the topics of the SeqLDA and LDA models, we have to solve the problem of topic alignment, since topics learnt in separate runs have no intrinsic alignment. The approach we adopt is to start the SeqLDA X  X  Gibbs sampling with the topic assignments learnt from the LDA model. Figures 5(a) and 6(a) show the confusion matrices between the topic distributions generated by SeqLDA and LDA with Hellinger Distance, where SeqLDA topics run along the X-axis. Most topics are well aligned (with blue on the diagonal and yellow off diagonal), especially those for  X  X he Whale X . For  X  X he Prince X , the major confusion is with topic-0 and 9 yielding some blueish off diagonal.
After aligning the topics, we plot the topic distributions ( i.e. , sub-ideas) as a function of chapter to show how each topic evolves, shown in Figures 5 and 6 respectively. Immediately, we see that the topic evolving patterns over chapters learnt by SeqLDA are much clearer that those learnt by LDA. For example, compare two subfigures in Figure 6, it is hard to find the topic evolvement patterns in Figure 6(b) learnt by LDA; in contrast, we can find the patterns in Figure 6(c), for example, topic-7, which is about men on board ship generally, and topic-12, which is about the speech of old ( X  X hou, X   X  X hee, X   X  X ye, X   X  X ad X ) co-occur together from chapters 15 to 40 and again around chapters 65-70, which is coherent with the book.

Moreover, Figures 7(a) and 7(b) depict the Hellinger distances (also as a function of chapter) between the topic distributions of two consecutive chapters ( i.e. , between chap-ter i and chapter i + 1 ) to measure how smoothly topics evolve through the books. Obviously, the topic evolvement learnt by SeqLDA is much better than that learnt by LDA. SeqLDA always yields smaller Hellinger distances and smaller variance of distances. The big topic shifts found by LDA are also highlighted by SeqLDA, such as Chapter 7 to 10 in Figure 7(a). Evidently, the SeqLDA model has avoided heavy topic drifting, and makes the topic flow between chapters much smoother than LDA does. An immediate and obvious effect is that this can help the reader understand also indicate that the document structure can aids in the statistical text analysis, and structure-aware topic modelling approaches provide a solution for going beyond the  X  X ag-of-words X  assumption.

There are various ways to extend the SeqLDA model which we hope to explore in the future. The model can be applied to conduct document summarisation or document classifications, where sequential structures could play an important role. The two parameters a and b in the PDP can be optimized dynamically for each segment, instead of
The two-parameter Poisson-Dirichlet process (PDP), is a generalization of the Dirichlet process. In regard to SeqLDA, let  X  be a distribution over topics ( i.e. topic
