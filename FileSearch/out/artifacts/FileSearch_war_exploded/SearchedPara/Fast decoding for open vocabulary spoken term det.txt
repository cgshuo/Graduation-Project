 Indexing and retrieval of speech content in vari-ous forms such as broadcast news, customer care data and on-line media has gained a lot of interest for a wide range of applications from market in-telligence gathering, to customer analytics and on-line media search. Spoken term detection (STD) is a key information retrieval technology which aims open vocabulary search over large collections of spoken documents. An approach for solving the out-of-vocabulary (OOV) issues (Saraclar and Sproat, 2004) consists of converting speech into phonetic, syllabic or word-fragment transcripts and represent-ing the query as a sequence of phones, syllables or word-fragments respectively. Popular approaches include subword decoding (Clements et al., 2002; Mamou et al., 2007; Seide et al., 2004; Siohan and Bacchiani, 2005) and representations enhanced with phone confusion probabilities and approximate sim-ilarity measures (Chaudhari and Picheny, 2007). The first step in converting speech to a searchable in-dex involves the use of an ASR system that produces word, word-fragment or phonetic transcripts. In this paper, the LVCSR system is a discriminatively trained speaker-independent recognizer using PLP-derived features and a quinphone acoustic model with approximately 1200 context dependent states and 30000 Gaussians. The acoustic model is trained on 430 hours of audio from the 1996 and 1997 En-glish Broadcast News Speech corpus (LDC97S44, LDC98S71) and the TDT4 Multilingual Broadcast News Speech corpus (LDC2005S11).

The language model used for decoding is a tri-gram model with 84087 words trained on a collec-tion of 335M words from the following data sources: Hub4 Language Model data, EARS BN03 closed captions and GALE Broadcast news and conversa-tions data. A word-fragment language model is built on this same data after tokenizing the text to frag-ments using a fragment inventory of size 21000. A greedy search algorithm assigns the longest possi-ble matching fragment first and iteratively uses the next longest possible fragment until the entire pro-nunciation of the OOV term has been represented by sub-word units.

The speed and accuracy of the decoding are con-trolled using two forms of pruning. The first is the standard likelihood-based beam pruning that is used in many Viterbi decoders. The second is a form of Gaussian shortlisting in which the Gaussians in the acoustic model are clustered into 1024 clusters, each of which is represented by a single Gaussian. When the decoder gets a new observation vector, it computes the likelihood of the observation under all 1024 cluster models and then ranks the clusters by likelihood. Observation likelihoods are then com-puted only for those mixture components belonging to the top maxL1 clusters; for components outside this set a default, low likelihood is used. To illus-trate the trade-offs in speed vs. accuracy that can be achieved by varying the two pruning parame-ters, we sweep through different values for the pa-rameters and measure decoding accuracy, reported as word error rate (WER), and decoding speed, re-ported as times faster than real time (xfRT). For ex-ample, a system that operates at 20xfRT will require one minute of time (measured as elapsed time) to process 20 minutes of speech. Figure 1 illustrates this effect on the NIST 2006 Spoken Term Detec-tion Dev06 test set. The main difficulty with retrieving information from spoken data is the low accuracy of the transcription, particularly on terms of interest such as named en-tities and content words. Generally, the accuracy of a transcript is measured by its word error rate (WER), which is characterized by the number of substitutions, deletions, and insertions with respect to the correct audio transcript. Mamou (Mamou et al., 2007) presented the enhancement in recall and precision by searching on word confusion net-works instead of considering only the 1-best path word transcript. We used this model for searching in-vocabulary queries.

To handle OOV queries, a combination of word and phonetic search was presented by Mamou (Mamou et al., 2007). In this paper, we ex-Apache open source search library written in Java, for indexing and search. When searching for these OOVs in word-fragment indexes, they are repre-sented phonetically (and subsequently using word-fragments) using letter-to-phoneme (L2P) rules. 3.1 Indexing Each transcript is composed of basic units (e.g., word, word-fragment, phones) associated with a be-gin time, duration and posterior probability. An inverted index is used in a Lucene-based indexing scheme. Each occurrence of a unit of indexing u in a transcript D is indexed on its timestamp. If the posterior probability is provided, we store the confi-dence level of the occurrence of u at the time t that is evaluated by its posterior probability P r ( u | t, D ) . Otherwise, we consider its posterior probability to be one. This representation allows the indexing of different types of transcripts into a single index. 3.2 Retrieval Since the vocabulary of the ASR system used to gen-erate the word transcripts is known, we can easily identify IV and OOV parts of the query. We present two different algorithms, namely, exact and fuzzy search on word-fragment transcripts. For search on word-fragment or phonetic transcripts, the query terms are converted to their word-fragment or pho-netic representation.

Candidate lists of each query unit are extracted from the inverted index. For fuzzy search, we re-trieve several fuzzy matches from the inverted in-dex for each unit of the query using the edit distance weighted by the substitution costs provided by the confusion matrix. Only the matches whose weighted edit distance is below a given threshold are returned. We use a dynamic programming algorithm to incor-porate the confusion costs specified in the matrix in the distance computation. Our implementation is fail-fast since the procedure is aborted if it is discov-ered that the minimal cost between the sequences is greater than a certain threshold.

The score of each occurrence aggregates the pos-terior probability of each indexed unit. The occur-rence of each unit is also weighted (user defined weight) according to its type, for example, a higher weight can be assigned to word matches instead of word-fragment or phonetic matches. Given the na-ture of the index, a match for any query term cannot span across two consecutively indexed units. 3.3 Hybrid WordFragment Indexing For the hybrid system we limited the word portion of the ASR system X  X  lexicon to the 21K most fre-quent (frequency greater than 5) words in the acous-tic training data. This resulted in roughly 11M (3.1%) OOV tokens in the hybrid LM training set and 1127(2.5%) OOV tokens in the evaluation set. A relative entropy criterion described in (Siohan and Bacchiani, 2005) based on a 5-gram phone language model was used to identify fragments. We selected 21K fragments to complement the 21K words result-ing in a composite 42K vocabulary. The language model text (11M (3.1%) fragment tokens and 320M word tokens) was tokenized to contain words and word-fragments (for the OOVs) and the resulting hy-brid LM was used in conjunction with the acoustic models described in Section 2. In assessing the match of decoded transcripts with search queries, recognition errors must be accounted for. One method relies on converting both the de-coded transcripts and queries into phonetic represen-tations and modeling the confusion between phones, typically represented as a confusion matrix. In this work, we derive this matrix from broadcast news de-velopment data. In particular, two systems: HMM based automatic speech recognition (ASR) (Chaud-hari and Picheny, 2007) and a neural network based acoustic model (Kingsbury, 2009), are used to ana-lyze the data and the results are compared to produce confusion estimates.

Let X = { x and S the set of context dependent HMM states. Associated with S is a many to one map M from each member s p dle, and end context dependent states to the central phone identity. The ASR system is used to generate a state based alignment of the development data to the training transcripts. This results in a sequence of state labels (classes) { s frame of the input data. Note that the aligned states are collapsed to the phone identity with M , so the frame class labels are given by { c
Corresponding to each frame, we also use the state posteriors derived from the output of a Neu-ral Network acoustic model and the prior probabil-ities computed on the training set. Define X { . . . , x t , . . . } to be the sub-sequence of the input speech frames centered around time index t . The neural network takes X where y is the neural network output and l is the prior probability, both in the log domain. Again, the state labels are mapped using M , so the above pos-terior is interpreted as that for the collapsed phone: The result of both analyses gives the following set of associations:
Each log posterior l where N is a large constant, i ranges over the time index, and j ranges over the context dependent states. From the counts, the confusion matrix entries are computed. The total count for each state is where k is an index over the states. The rows of the above matrix correspond to the ref-erence and the columns to the observations. By nor-malizing the rows, the entries can be interpreted as  X  X robability X  of an observed phone (indicated by the column) given the true phone. The performance of a spoken term detection system is measured using DET curves that plot the trade-off between false alarms (FAs) and misses. This NIST STD 2006 evaluation metric used Actual/Maximum Term Weighted Value (ATWV/MTWV) that allows one to weight FAs and Misses per the needs of the task at hand (NIST, 2006).

Figure 2 illustrates the effect of speed on ATWV on the NIST STD 2006 Dev06 data set using 1107 query terms. As the speed of indexing is increased to many times faster than real time, the WER increases, which in turn decreases the ATWV measure. It can be seen that the use of word-fragments improves the performance on OOV queries thus making the combined search better than simple word search. The primary advantage of using a hybrid decoding scheme over a separate word and fragment based decoding scheme is the speed of transforming the audio into indexable units. The blue line in the fig-ure illustrates that when using a hybrid setup, the same performance can be achieved at speeds twice as fast. For example, with the combined search on two different decodes, an ATWV of 0.1 can be achieved when indexing at a speed 15 times faster than real time, but with a hybrid system, the same performance can be reached at an indexing speed 30 times faster than real time. The ATWV on the hybrid system also degrades gracefully with faster speeds when compared to separate word and word-fragment systems. Preliminary results indicate that fuzzy search on one best output gives the same ATWV performance as exact search (Figure 2) on consen-sus output. Also, a closer look at the retrieval results of OOV terms revealed that many more OOVs are retrieved with the fuzzy search. In this paper, we have presented the effect of rapid decoding on a spoken term detection task. We have demonstrated that hybrid systems perform well and fuzzy search with phone confusion probabilities help in OOV retrieval.

