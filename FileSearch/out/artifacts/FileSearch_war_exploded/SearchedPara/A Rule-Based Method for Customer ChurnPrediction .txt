 Service companies in telecommunications suffer from a loss of valuable customers to competitors; this is known as customer churn. In the last few years, there have been many changes in the telecommunications industry, such as, new ser-vices, technologies and the liberalizations of the market increasing competition. Recently, data mining techniques have b een emerged to tackle the challenging problem of customer churn in telecommunication service fields [1,2,3,4]. As one of the important measures to retain cu stomers, churn prediction has been a concern in telecommunication industry and research.

As one of the important phases in churn prediction process, classification can directly affect the prediction results. Many modelling techniques have been successfully used for the classification, including Decision Tree (DT), Logistic Regression (LR), Artificial Neural Networks (ANN), Naive Bayes (NB) and Sup-port Vector Machine (SVM). However, they still have some limitations. Most of the modelling techniques provide either prediction probability or classification rules. For example, NB, LR [5,6,7], ANN [8,9] and SVM [10,11] can only provide the probability of classification, whereas they do not produce decision rules. DT [12,13,14] can produce rules, however, the probability of classification cannot be obtained from DT model[15].

A number of Genetic Algorithm (GA) based classification models also have been reported in[16,17,18]. These models can be efficient. However, they have high computational complexity. Recentl y, DMEL (Data Mining by Evolutionary Learning) [15], a GA based prediction approach was proposed. This technique not only provides classification rules, but also gives the probability of classifica-tion. It was shown that DMEL is a very promising modelling technique, however, its computational complexity is too high.

This paper proposes a new rule-based classification method named CRL ( C lassification by R ule L earning). CRL can not only classify samples by a set of generated rules, but also it is able to produce the prediction probability. More-over the processing time of this method is reasonable. CRL consists of two main procedures: Firstly, generating rules by applying the heuristic of hill-climbing and pruning unimportant and redundant rules. Secondly, predicting one of the categories to which a test case belongs acco rding to the rules. In the experiments, Lift Curve and AUC, were applied to demonstrate the performance of algorithm.
The rest of this paper is organized as follows: Section 2 introduces the re-lated work. Section 3 describes the details of the proposed algorithm. Section 4 provides the exper imental results and discussion. The conclusion and future work are given in Section 5. Rule-based classification is a well known domain. One of the main characteristics of rule induction is that they are more transparent and understandable than some other training models. A significant amount of research effort has been put into this domain. In a rule-induction process, usually two strategies are applied: general-to-specific (top-down) and specific-to-general (bottom-up). Due to that the strategy of gener al-to-specific is more widely applied in rule learning, this section will introduce several appr oaches by using it, besides examples of rule induction by applying the strategy of specific-to-general can be found in [19,20,21].

Among all the different methods for const ructing the rules, decision tree based algorithms are the most popular [12,13,14]. C4.5 has proved to be one of the most well known among all tree-based classifiers. A tree is constructed using the method of divide-and-conquer. C4.5 starts to search an attribute with the highest information gain, and then the data is partitioned into classes according to this attribute. Each sub-tree represen ts a class, and will be recursively further partitioned using the same policy until some stopping criteria are satisfied, such as the leaf node has been reached etc. The rules can be obtained by traversing each branch of the tree. The details of this algorithm can be found in [12].
CN2 [22] works in an iterative way, each iteration searches one rule that covers (see section 3.1 for the definition) a large number of training cases by making use of beam search method. Entropy and likelihood ratio are used to evaluate the quality and the significance of the rules. Rules are ranked in accordance with the quality of rules. When classifying a test case, CN2 goes through each rule until finding a rule that covers the case, then assigns the class label of that rule to the case.

DMEL is a genetic classification techni que. The DMEL classifier consists of a set of labelled rules which were found by rule induction technique. DMEL starts by generating first  X  order rules, then high  X  order rules can be constructed iteratively by randomly combining lower order rules. Given a sample without labels, the DMEL classifier applies the rules to model the sample and makes the classification decisions. DMEL has been shown to be an efficient classifier. The details of this technique can be found in [15]. Although experimental results from [15] showed that DMEL is efficient at detect-ing churners, there is still improvement that can be done for better classification results. The objective in designing CRL is to improve the classification accu-racy. CRL does so by considering several aspects: Firstly, CRL applies different heuristic methods for constructing classification rules; Secondly, constraints were introduced to avoid bad rules, by applying better strategies. 3.1 Basic Concepts The Rules are of the following form: IF antecedent THEN consequence ,where antecedent describes a conjunction of a certain number of &lt; attribute, interval &gt; pair, and consequence denotes the class label. Another basic concept of rule in-duction is Cover , which expresses the case that a s ample satisfies the antecedent part of the rule. A rule Correctly Covers a sample when it correctly predicts the covered sample. The task of rule induction is to build a model by generating a set of rules, which either do or do not Cover cases. general rule and specif ic rule are two important concepts. Usually, a rule withasmallernumberof &lt; attribute, interval &gt; pairs in the antecedent part is more general than the one having more. The reverse case is more specif ic . most general occurs when the number of antecedent is 1, while most specif ic means the number of antecedent is the total num ber of attributes. N ormally, the most specific rules should not be considered since they are too specific and not able to describe a relative big data space. 3.2 Rule Learning CRL is a general-to-specific strategy based method, so it starts by generating the most general rules, first -order rules, then iterativ ely constructing higher-order rules based on the lower-order rules. Algorithm 1 illustrates the process of learning rules. SET rules is the set of generated rules, k denotes the number of orders, and threshold denotes the maximal number of orders. Algorithm 1. CRL Algorithm(training data) First-order Rules. Algorithm 2 illustrates the process of generating the first-order rules. In order to avoid information loss, all possible combinations of num  X  attribute , num  X  interval and num  X  class respectively denotes the number of attributes, the number of intervals of the current attribute and the number of different categories. Initially, the set of first  X  order  X  rule is empty, rule ijk expresses a rule having the form  X  X F f i = Interval j THEN Class k  X , if Prun-ing (rule ijk ) returns false, then rule ijk can be accepted as a useful rule (Rule pruning is described in section 3.3).
 Algorithm 2. First  X  order  X  rules (training data) High-order Rules. Higher-order rules are iterat ively constructed based on the previous set of lower-order rules. The second-order is grounded on the first  X  order ,the third  X  order is based on the second  X  order . Generally, ( n  X  1)  X  order rules are the base for the n  X  order rules. Algorithm 3 illustrates the pro-cedure of constructing a set of high  X  order rules. Positive lower order rules, which have the positive prediction, and negative lower order rules, which reversely have the negative prediction, can be separately obtained from the lower  X  order  X  rules . consist of all exclusive items extracted from all positive rules. We generate negative-items in the same way. Each po sitive rule becomes more specific by using the hill  X  climbing strategy to add one item into the antecedent part of the current rule. Algorithm 4 describes the heuristic of hill  X  climbing for con-structing one high order rule. It will be discussed in detail. The rule pruning is also an important step for generating high  X  order rules. In order to reduce computation time, we get rid of rules having the same quality. Many methods can be used to assess the quality of rules. They either focus on the coverage or on the accuracy. However, a measure having the ability to make a good trade off between coverage and accuracy is more suited for rule learning. Therefore, Weighted Relative Accuracy(WRA) [23], which can be calculated by Eq. (1) is applied.
 where Num  X  ( a )and Num  X  ( c ) represent the number of data cases that are cov-ered by the antecedent and the conse quence of a rule, respectively. Num  X  ( a, c ) is the number of cases correctly covered by a rule, and Num  X  total denotes the total number of cases in the training set. Line 9 expresses that only rules with exclusive quality can be added.
 Algorithm 3. Higher  X  order  X  rules (a set of lower  X  order  X  rules )
Algorithm 4 illustrates the process of generating one high  X  order rule. Let accuracy  X  list , which is a list consisting of different WRA values, be empty in the beginning. Basically, one piece of high  X  order is based on one piece of lower  X  order rule and a new item , which has been mentioned earlier. If the new item does not occur in the antecedent of the lower  X  order rule, then one  X  high  X  order can be built by combining one  X  lower  X  rule and the new item . Meanwhile, the WRA value of the newly built one  X  high  X  order is stored in the accuracy  X  list . This algorithm returns one of the high  X  order rules with the highest WAR value.
 Algorithm 4. hill  X  climbing ( one  X  lower  X  rule , all  X  low  X  items ) 3.3 Pruning Rules The number of generated rules can be huge , to make the classification effective, we must prune bad rules with insignificant or noisy information. Several criteria are used for pruning rules. Usually,  X  2 statistic test is used to test if there exists strong relative relationship between two attributes. In this paper,  X  2 ,whichcan be calculated by Eq. (2), is applied to decide whether a rule is significant which means all the attributes occurring in ant ecedent part of the rule correlate to the consequence. where O and E are the observed and expected frequency, which can be expressed by Eq. (3): In addition to  X  2 , Support and Confidence are another two essential factors when deciding if a rule should be pruned or not. Support and Confidence can be calculated by Eq. (4).
 Algorithm 5 illustrates the pruning method.  X  is the critical value for  X  2 signif-icance test, and it is set to be 3.84 in this research. minS and minC are two threshold values, which define the minimal value of support and confidence, re-spectively. In this research, the minimum value of support and confidence were set to be 0.01 and 0.5, respectively. A rule can be retained if it satisfies all the threshold values.
 Algorithm 5. Pruning( rule ) 3.4 Classification This section discusses how to classify ca ses based on a number of classification rules. Rules consist of two categories, churn and non  X  churn .Twoprediction models can be built based on all churn rules and all non  X  churn rules, re-spectively. In order to estimate the im portance of each rule, we rank rules in each model based on a set of principles, which are also used in [24] and can be described as follows: 1. If conf idence  X  1 &gt;confidence  X  2, then rule  X  1 has higher priority than 2. If conf idence  X  1= conf idence  X  2and support  X  1 &gt; support  X  2, then rule  X  1 3. If conf idence  X  1= conf idence  X  2, support  X  1= support  X  2and rule  X  1is After ranking, each rule has a position in each prediction model, one is more important than another rule if it has a better position than the other. We define the significance of each rule as follows: where Num  X  rules is the number of rules, position denotes the rule index of the ranked rule set.

Therefore, in a prediction model, the Signif icance  X  level of the most impor-tant rule is 1, while that of the least important one is 1 Num  X  rules . To classify a case, finding all covered rules from the churn model and non  X  churn model, respectively, if the sum Signif icance  X  level of covered rules in churn model is greater than that of non  X  churn , then assign churn to the case as the class label, otherwise, non  X  churn will be assigned. Apart from the mentioned two cases, if the current data case had the same sum Signif icance  X  level for churn model and non  X  churn , the majority class label should be assigned.

In order to know which customers are more likely to churn, CRL scores each customer according to the sum of its Signif icance  X  level . If one is predicted to churn , then this customer is scored the sum of Signif icance  X  level of the churn model. Otherwise, the sum of Signif icance  X  level in non  X  churn model will be used to score that individual. Customers are sorted separately according to the prediction, cases predicted to churn ar e sorted in descending order, while cases predicted to stay ( non  X  churn ) are sorted in ascending order. For both of the sorted lists, customers close to the top are more likely to become churners. Experiments were conducted based on four UCI repository datasets [25] and a telecommunication dataset (six different class distributions) of the Ireland Telecoms[26]. The datasets are described in Table 1. Each of these datasets was equally divided into one training dataset and one testing dataset, they have the same distribution.
 4.1 Evaluation In order to evaluate the classification performance, Lif t Curve measure [27] is used. The Lif t Curve measure is calculated by sco ring each data case, sorting cases according to their scores, calcula ting two parameters that indicate the percentage of samples and the percentage of True Positive, respectively. The two parameters can be calculated by Eq. (6): where Num  X  Samples , Num  X  Total , represent the number of sample data, the total number of testing c ases, respectively. Let Num  X  True  X  Positive and Num  X  Total  X  Positive be the number of samples that have been identified as positive, and the total number of positive cases in the whole testing dataset, respectively.

Occasionally, it is difficult to use lift cur ve to evaluate the detected percentages from different prediction modelling techniques or different feature subsets of data. To overcome this problem, we use the area under Lift Curve (AUC) [28] to evaluate the models. The area under a Lift Curve can be calculated by the following equation: where S 0 is the sum of the ranks of the class 0 (churn) test patterns, n 0 is the number of patterns in the test set which belong to class 0 (churn), and n 1 is the number which belongs to class 1 (nonchurn). The details of AUC can be found in [27,28]. 4.2 Discussion Figure 1 shows the experimental results, which compare the prediction perfor-mance between the new proposed classi fication method and DMEL. Each graph is generated by a different churn rate of training and testing dataset. In or-der to reduce costs, telecommunication operators randomly select(contact) a small amount of customers to offer special consideration or attr active services. The objective is catching relative more potential churners by contacting a rela-tive small amount of customers. In Figure 1, the horizontal axis represents the percentage of contacted customers, wh ile the vertical axis is the percentage of identified churners under the contacted customers. We can clearly see that the results based on CRL are more effective t han DMEL. The lift curves generated by CRL always look better than those of DMEL for all percentages of contacted customers when the churn rates are 1%, 4%, 6% and 8%. When the churn rates are 2% or 10%, the lift curves representing CRL are not always higher than those of DMEL. We still regard CRL as the better algorithm given that it is not appropriate to contact so large a portion of customers when seeking to reduce costs for a telecommunication operator.
 Moreover, the result of AUC measurement is consistent with that of Lift Curve. Figure 2(a) shows that there is a big difference between the AUC values of CRL and DMEL when the churn rates are 1%, 4% and 6%. For the rest of the cases, there still exists obvious differ ence. Figure 2(b) displays the precessing time when using the two algorithms. It shows that the proposed method is quicker than DMEL.

In order to show that CRL can be extende d to other data coll ections, several sets of artificial data were tested. Figure 3 compares the AUC values between the proposed classification algorithm and DMEL on four sets of UCI data. In Figure 3, the horizontal axis denotes different data set, while the vertical axis shows the obtained AUC values based on the two algorithms. It shows that the proposed algorithm produced better classification for each data set. In this paper, we proposed a new rule learning based classification algorithm. The experiments were conducted based on a telecommunication data sets and 4 other data-sets. The Lift Curve and AUC techniques are used to evaluate the performance of the algorithms. The experimental results show that CRL is more efficient than DMEL.

The classification results slightly vary when setting different values for the order to generate rules. Thus, it is nece ssary to find a proper threshold to control the number of high order rules rather than setting the size arbitrarily. In addition, the minimum values of support and confidence should also be considered more in the future since these will impact on the effectiveness of rule pruning.
