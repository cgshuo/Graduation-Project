 same set of random samples to estimate any l in order to choose an  X  X ptimum X  execution path for multi-way joins. and multi-way). While this paper focuses on estimating pair wise l We will compare CRS with normal random projections for approximating l products, and with Cauchy random projections for approximating l 2.1 The Sampling/Sketching Procedure to maintain a permutation mapping on the column IDs. D s = 10 tuples  X  X D (Value). X  (c): Sketches are the first k example, k samples as if we directly sampled the first D ply taking the first D called postings . We denote the postings by P postings P we obtain exactly the same samples as if we directly sampled t he first D which differs pairwise and we do not know beforehand. 2.2 The Estimation Procedure samples from sketches K ( l , l We use  X  u by CRS. For example, in Figure 2, we have D Denote the inner product, squared l 2.3 The Computational Cost number of non-zeros in the i th row, i.e., f sketches. While the conditional sample size D between one pair of data points would be only O ( k We first consider  X  a (  X  u The unconditional variance would be simply as Var (  X  X ) = E  X  Var (  X  X | D to Jensen X  X  inequality). Asymptotically (as k where f We similarly derive the variances for  X  d (2) where we denote d (4) = P D projections [17] are widely used in learning and data mining [2 X 4]. generate a compact representation B = AR  X  R n  X  k . For estimating l that could be metrics for dimension reduction in l Denote v also introduce the notation for the marginal l 4.1 Normal Random Projections the inner product a and the squared l with variances [15, 17] Assuming that the margins m likelihood estimator, denoted by  X  a 4.2 Cauchy Random Projections for Dimension Reduction in l reduction in l where  X  d (1) [14] shows that 4.3 General Stable Random Projections for Dimension Reduct ion in l for dimension reduction in l tail bounds. Of course, CRS can also be applied to approximat ing any l 5.1 Boolean (0/1) Data is not hard to show that the MLE of a is the solution, denoted by  X  a where s The (asymptotic) variance of  X  a 5.2 Real-valued Data (  X  u where  X  u  X  u , m  X  a following two important scenarios, CRS outperforms random projections. 6.1 Boolean (0/1) data u projections, when no marginal information is used. We let f projections, except when f 6.2 Nearly Independent Data Suppose two data points u order), it is easy to show that the variance of CRS is always sm aller: easily by d (2) = m 6.3 Comparing the Computational Efficiency more precisely, O ( P n We estimate all pairwise inner products, l in which CRS does better than random projections.
 sizes for CRS. For random projections, we use the average sam ple size. products and l approximating l and l and are considerably more heavy-tailed than the NSF and NEWS GROUP data [13]. approximating inner products and l There are many applications of l In this case, it is more obvious that adjusting sketch sizes h elps CRS. Dhillon, and Matthias Hein for the datasets.

