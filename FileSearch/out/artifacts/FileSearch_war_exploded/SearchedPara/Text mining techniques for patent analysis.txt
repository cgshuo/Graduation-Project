 1. Introduction
Patent documents contain important research results that are valuable to the industry, business, law, and policy-making communities. If carefully analyzed, they can show technological details and relations, reveal business trends, inspire novel industrial solutions, or help make investment policy ( Campbell, 1983; Jung, 2003 ). In recent years, patent analysis had been recognized as an important task at the government level in some Asian countries. Public institutions in China, Japan, Korea, Singapore, and Taiwan have invested var-ious resources in the training and performing of the task of creating visualized results for ease of various anal-yses ( Liu, 2003 ). For example, the Korean Intellectual Property Office plans to create 120 patent maps for different technology domains in the next 5 years ( Bay, 2003 ).

Patent analysis or mapping requires considerable effort and expertise. For example, Table 1 shows a typical patent analysis scenario which is based on the training materials designed for patent analysts, such as those in
Chen (1999) . As can been seen, these processes require the analysts to have a certain degree of expertise in information retrieval, domain-specific technologies, and business intelligence. This multi-discipline require-ment makes such analysts hard to find or costly to train. In addition, patent documents are often lengthy and rich in technical and legal terminology. To read and analyze them may consume a lot of time even for experts. Automated technologies for assisting analysts in patent processing and analysis are thus in great demand.

A patent document contains dozens of items for analysis; some are structured, meaning they are uniform in semantics and in format across patents such as patent number, filing date, or assignees; some are unstructured, meaning they are free texts of various lengths and contents, such as claims, abstracts, or descriptions of the invention. The visualized results from patent analysis are called patent graphs if they are from the structured data and patent maps if they are from the unstructured texts, although, loosely speaking, patent maps can refer to both cases.

Before their publication, patent documents are given one or more classification codes based on their textual contents for topic-based analysis and retrieval. However, these pre-defined categories may be either too broad or not meet the goal for a particular analysis. Self-developed classification systems are often needed. For example, Table 2 shows part of a patent map created by analyzing 92 patent documents ( Mai, Hwang, Chien,
Wang, &amp; Chen, 2002 ). These patents, issued before February 19, 2002, are the search results of the keyword:  X  X  X arbon nanotube X  X  from the database of USPTO ( United States Patent &amp; Trademark Office ). In Table 2 , manually assigned categories regarding the technology aspects of the patents are listed in rows and those regarding the effect (or function) aspects of the patents are listed in columns. The patent IDs are then assigned to the cells of the technology-effect matrix based on their contents. With this map, patent relations and distri-butions are revealed among these aspects. This information can be used to make decisions about future tech-nology development (such as seeking chances in those sparse cells), inspire novel solutions (such as by understanding how patents are related so as to learn how novel solutions were invented in the past and can be invented in the future), or predict business trends (such as by showing the trend distribution of major competitors in this map).

Depending on the goals, the created map may need to reflect the relations among some machine-identified topics. In such a case, only the relative distance between each topic in the map is relevant, while the absolute position and orientation of each topic does not matter. As an example, Fig. 1 shows a topic map analyzed based on the above 92 carbon nanotube patents, where each circle denotes an identified topic, the size of the circle denotes the number of patents belonging to the topic, and the number in the circle corresponds to the topic ID. Some of the topic titles and their IDs are shown in Table 3 .

Creating and updating such maps requires a lot of human effort. As in the above  X  X  X arbon nanotube X  X  map (called CNT map ), five specialists spent more than one month in analyzing about one hundred patents.
Although it may go unnoticed, such efforts indeed involve some text mining processes such as text segmenta-tion, summary extraction, keyword identification, topic detection, taxonomy generation, term clustering, and document categorization. As shown in Table 1 , this patent analysis scenario is quite similar to the general text mining process commonly discussed in the literature, such as those in Hearst (1999), Losiewicz, Oard, and Kostoff (2000) .

Text mining, like data mining or knowledge discovery ( Fayyad, Piatetsky-Shapiro, Smyth, &amp; Uthurasamy, 1996 ), is often regarded as a process to find implicit, previously unknown, and potentially useful patterns from a large text repository. In practice, the text mining process involves a series of user interactions with the text mining tools to explore the repository to find such patterns. After supplemented with additional information and interpreted by experienced experts, these patterns can become important intelligence for decision-making.
The purpose of this paper is to present a text-mining approach that help automate the patent analysis sce-nario discussed above, based on the patent documents from USPTO. In particular, we propose and implement a text mining method for each technical step in Table 1 . The adopted methodology is first introduced in Sec-tion 2 . The technical details of each method are described in Section 3 . Evaluation of some critical techniques is conducted in Section 4 . In Section 5 , an example of analyzing a set of patents is given. Section 6 discusses the implications and related work. Finally Section 7 concludes this paper. 2. A general methodology
Patent analyses based on structured information such as filing dates, assignees, or citations have been the major approaches in practice and in the literature for years ( Archibugi &amp; Pianta, 1996; Be X  X e X  X arrax &amp; Huot, 1994; Ernst, 1997; Lai &amp; Wu, 2005 ). These structured data can be analyzed by bibliometric methods, data min-ing techniques, or well-established database management tools such as OLAP (On-Line Analytical Processing) modules. Recently, there has been an interest in applying text mining techniques to assist the task of patent analysis and patent mapping ( ACL-2003 Workshop on Patent Corpus Processing, 2003; Fattori, Pedrazzi,
A well-utilization of the full texts in the patent documents may complement the interpretations derived from the bibliometric analysis.

Therefore, based on the patent analysis scenario introduced above, a text mining methodology specialized for full-text patent analysis is proposed and shown in Fig. 2 . First, full patent documents relevant to the analysis purpose are collected. This may involve a repeated process of devising a set of query terms (query formulation), searching a couple of patent databases (collection selection), filtering undesired patents (rele-vance judgment), and downloading patents for local analysis (data crawling). Depending on the analysis pur-pose, the step can be as easy as, for example, fetching all the patents under some IPC (International Patent
Classification) categories and within some year limits, or as hard as searching patent documents relevant to a news story to understand current status of technologies mentioned in the story or searching all the  X  X  X rior arts X  X  that can invalidate competitors X  patents. The later, called technology survey and invalidity search , respectively, are two of the main tracks in patent retrieval evaluation of NTCIR Workshop 3 ( Iwayama, Fujii, Kando, &amp;
Marukawa, 2006 )and4( Fujii, Iwayama, &amp; Kando, 2004 ). Since this issue deserves a monograph, it is beyond the scope of our discussion. Our method assumes that a set of patents has been carefully prepared.
Next, each document in the collection is parsed and segmented. Structured data parsed from the patent documents are saved into a DBMS system for ease of management and unstructured data (the full texts) are segmented into smaller units for summarization. Take the patent documents from the USPTO as exam-ples, the structured data include: filing date, application date, assignees, UPC (US Patent Classification) codes,
IPC codes, and others, while the unstructured segments include: title, abstract, claims, and description of the invention. The description of the invention can be further segmented into field of the invention, background, summary, and detailed description, although some patents may not have all these segments.

In our analysis process, the title and abstract are not the only textual parts used, as is often the case in ana-lyzing scientific publications. Thanks to the consistent format of patent documents, all the summaries of the above-mentioned segments are used. Summarization of the segments (document surrogate) can yield concise representation. It may not only facilitate the sharing and re-use of the analyzed patents among analysts, but it also speedups later automated processing due to less textual data and possibly yields higher effectiveness due to the elimination of less-focused snippets. Although perfect machine-derived summarization is hard to define and achieve, simple methods based on sentence ranking and selection often yield sufficient performance for some text mining tasks. For example, in a patent classification experiment using Na X   X  ve Bayes, KNN, and
SVM as classifiers, Fall, Torcsvari, Benzineb, and Karetka (2003) shows that even using only the first 300 words from the abstract, claims, and description sections, the performance is better than those using the full texts regardless of which classifiers are used.

From the set of document surrogates, keywords and phrases are extracted. The resultant terms are further filtered by a stopword list and by some frequency criteria. The goal is to extract high-quality terms for index-ing and analysis. Yet ideal indexing includes term stemming (morphological analysis) and term clustering so that the terms in various forms corresponding to the similar concepts are associated together in the same set.
This not only reduces the size of the vocabulary for efficient analysis, but also decreases the vocabulary mis-match problem for effective clustering.

With these concise representations in terms and documents, various clustering algorithms can be applied to identify the concepts underlying the collection. These concepts can be further clustered into topics, which in turn can be clustered into categories or domains. Here the distinction among concepts, topics, and categories is not important. As long as human analysts can recognize the knowledge structures underlying the collection, the multi-stage clustering process can stop at any level of granularity.

To show the detected knowledge structures, various forms of representation can be used, such as the folder tree, the 2-D matrix, or the topic map mentioned above. From the folder tree, the results from each of the multi-stage clustering can be directly shown for exploring. From the topic map, several types of maps for visual anal-ysis can be derived by combining other structured data such as time, assignees, number of patents, etc. A general visualization idea is that: for data visualization, occurrence frequencies of dependent variables (such as number of patents or assignees) are plotted against independent variables whose values are arrayed along a (regularly) varying base (such as patent application years). As such, for text-based visualization, the values of the indepen-dent variable become the identified clusters based on text similarities, with the dependent variables being any structured data. All the values of these variables can be filtered, pivoted, or sliced for various analysis purposes.
Examples of the maps derived from the above idea are: (1) Trend map: it can be further divided into 2 sub-types  X  growth map which shows how topics grow in size (number of patents) with time and evolution map which shows how topics evolve (change in size and relation) over time. (2) Query map: showing only those patents satisfying some query conditions in each topic. The sizes of and the similarities among these topics can be re-calculated based on the filtered patents in each cluster to reveal new relationships or patterns among the topics. (3) Aggregation map: showing those aggregated results based on some specified attributes. Exam-ples are the shares of the top-three assignees distributed in each topic. (4) Zooming map: showing the details or overview of the selected part in the map. 3. Technique details
This section presents the details of the proposed techniques in dealing with patent documents. Each tech-nical step in the above methodology is addressed in sequence. The rationales behind these techniques are dis-cussed. Efficiency, effectiveness, robustness, and degree of automation are taken into consideration. For better presenting the proposed techniques, a number of examples are given when necessary. 3.1. Text segmentation
The textual content of a patent document from USPTO is in HTML format and contains title, abstract, claims, and description. The description, the main body of the detailed content, often have sub-sections with titles in uppercase, such as FIELD OF THE INVENTION, BACKGROUND, SUMMARY OF THE
INVENTION, and DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT. Although some patents may have more or fewer such sub-sections or have slightly different title names, most patents do follow this style. Thus a regular expression matcher is devised to extract each of these segments. The method takes advantage of the rule that each sub-section X  X  title is in a single line paragraph separated by two HTML tags:  X  X   X  BR  X  X  BR  X   X  X . After splitting the paragraphs based on these tags, a set of Perl expressions: (/Abstract/i, /Claims/i, /FIELD/i, /BACKGROUND j Art/i, /SUMMARY/i, /DESCRIPTION j EMBODIMENT/i) are used to match the patent segments. An exception is that if FIELD OF THE INVENTION is not detected, the first text paragraph of the BACKGROUND segment is extracted as the FIELD segment. As long as non-relevant single-line paragraphs are properly filtered (those that are too long, too short, and do not contain the above title words), the ratio of false drops (the cases of incorrect detection of the segment titles) can be kept to a minimum. The whole process is quite ad hoc. The details can be varied slightly without affecting the effectiveness very much. We have observed that our colleagues achieved performance similar to ours in this task based on our guidelines, rather than on our source code. 3.2. Text summarization Automatic summarization techniques have been widely explored in recent years ( Document Understanding
Conferences ). They can be mainly divided into two approaches: abstraction and extraction. In abstraction, natural language understanding techniques are applied to analyze sentential semantics and then to generate concise sentences with equivalent semantics. Sophisticated techniques such as sense disambiguity or anaphoric resolution and large human-maintained resources may be applied. In extraction, statistical techniques are applied to rank and select the text snippets as a summary. In our method, the extraction approach is adopted for its relatively low cost and high robustness across technical domains. It is divided into three processing stages: sentence breaking, sentence weighting, and sentence selection and presentation.

Our extraction-based method takes sentences as the smallest units for summarization. As such, each segment is broken up by simply judging a period and question mark as a sentence break. But care has to be taken for exceptions involving floating-point digits, mathematical or chemical expressions (such as  X  X  X .sub.i X  X ), various abbreviations (such as  X  X  Fig. 1  X  X ), or sentences within sentences (such as those containing citations).

Next, each sentence is weighted by the number of keywords, title words, and clue words it contains. Fur-thermore, the position of the paragraph containing the sentence in a segment and the position of the sentence in the containing paragraph are considered as more important information for weighting. (Recall that para-graphs in the USPTO patent documents are separated by the  X  X   X  BR  X  X  BR  X   X  X  HTML tags.) Here the keywords are those maximally repeated patterns extracted by the algorithm to be described. The title words are those non-stopwords that occur in the title of a patent document. As to the clue words, they are a list of about 25 special words that reveal the intent, functions, purposes, or improvements of the patent. These words are prepared by several patent analysts based on their experiences and are listed in Appendix A for reference. In our implementation, the weight of a sentence is calculated as follows: where tf w is the term frequency (occurrence frequency in a segment) of word w which occurs in sentence S , avgtf is the term frequency averaged over all keywords and title words in the segment, and FS and P are the position weights. Heuristically, FS is set to 1.5 if S is the first sentence of any paragraph, and 1 otherwise, while P is set to 2 if sentence S is in the first two paragraphs in a segment, 4 if it is in the last two paragraphs and 1 otherwise. These values for FS and P are specialized for the background segment. Other segments can be further tuned empirically or based on some learning method if training data is available ( Kupiec, Pedersen, &amp; Chen, 1995 ).

Sentences are then sorted by their weights in decreasing order. By giving a desired summary length or ratio, a number of top-ranked sentences are selected. They are combined together in their original order to result in a summary or they are highlighted in the original segment to provide better readability.

Appendix B shows the background segment of a patent with the best sentences underlined. Two sets of summary are highlighted: one from the Microsoft Word X  X  Auto Summarization, which is in italic font; the other from the above method, which is in boldface. Both methods were confined to select the best 3 X 4 sen-tences. As can be seen, Word X  X  summary is more general in introducing the background or current status, while ours directly specifies the motivation of the patent and the problem to be solved. 3.3. Stopwords and stemming Term frequency (TF) and inverse document frequency (IDF) are the two parameters used in filtering terms.
Low TF and DF terms are often removed from the indexing of a collection. However, using them alone does not prevent undesired terms such as function words from being calculated. A list of over 250 stop words from ( van Rijsbergen ) is used in our text processing. After analyzing a set of patent documents, another 200 words chosen by hand were added to the list. They are adverbs mostly. A few are verbs, nouns, and adjectives com-monly seen in a patent document.
 To better match concepts among terms, words are stemmed based on Porter X  X  algorithm ( Porter, 1980 ).
However, the algorithm is so aggressive in removing the word X  X  suffix such that stemmed words become hard to read for analysts. We therefore modify the algorithm to remove only simple plurals and general suffixes (such as regular paste tense). 3.4. Keyword and phrase extraction
In patent analysis, single words alone are often too general in meanings or ambiguous to represent a con-cept. Multi-word phrases can be more specific and desirable. Thus besides important single words, multi-word phrases are also extracted for later use. However, keywords or key-phrases have no lexical boundaries in texts, making them a challenge to identify. Other studies had used statistical approaches based on corpus-wide sta-tistics ( Choueka, 1988 ), while ours applies a simple and fast key term extraction algorithm document by doc-ument. The algorithm works with the help of a stopword list alone. Other resources, such as corpora, lexicons, or dictionaries are not required. As such, it is readily applicable to any knowledge domains without much parameter tuning or resource acquisition.

In this algorithm, the text to be processed is first split into a series of words. The algorithm then repeatedly merges back nearby words based on three simple merging, dropping, and accepting rules. Maximally repeated strings in the text are thus extracted as keyword candidates. By maximally, we mean that either the repeated strings are the longest ones or they occur more often than the longer strings that contain them. For example, a repeated term  X  X  X ublic high school X  X  in a certain document may be extracted without extracting  X  X  X ublic high X  X  or  X  X  X igh school X  X , as they are exact substrings of the longer term. Only when  X  X  X igh school X  X  occurs more often than  X  X  X ublic high school X  X  (in such a case we may say:  X  X  X igh school X  X  subsumes  X  X  X ublic high school X  X ), can  X  X  X igh school X  X  be possibly extracted. The resultant candidates are then subject to a filtering process. A preci-sion-oriented rule may remove candidates containing any stopwords. A recall-oriented rule may only the stopwords from the head and tail of the candidates recursively. Fig. 3 shows the algorithm. The rule for merg-ing, dropping, and accepting terms is implicit expressed in the algorithm. Fig. 4 shows a running example, in which each capital letter denotes a word.

The above algorithm is based on the assumption that a document concentrating on a topic is likely to men-tion a set of strings a number of times. Many natural language documents have this property, including Chi-nese, Japanese, or even melody strings in music ( Tseng, 1999 ). We found that a longest repeated string often is a correct word (or phrase), since its repetition provides evidence for decision on its left and right boundaries.
Similarly, a repeated string that subsumes the others may also be a legal term. The sources of errors mainly come from the inadequate coverage of the stopword list. 3.5. Term association
There are a number of approaches to extract terms relevant to the same topics from the entire document collection. One commonly used heuristic rule is based on term co-occurrence ( Salton, 1989 ). Given a collection of n documents, an inverted term-document structure is first constructed, where each term is denoted in a vector form whose elements are weights of the term in the documents, such as: Tj  X  X  d ities are then computed among all useful term pairs. A typical similarity measure is given by cosine function:
If the weights of the terms were either 1 or 0, denoting the presence or absence of the terms in docu-ments, the similarity becomes a value exactly proportional to the number of documents in which these two terms co-occur. With these pair-wise similarities, terms are clustered with some automatic processes.
However, the above method requires a lot of computations. With m distinct terms in a collection of n docu-terms co-occur in the same document may virtually have no relationship if they are far apart from each other in the text. Calculating their term similarities in this way may turn out to be a waste of computation power.
Therefore, we proposed another method that is far more efficient. The major difference of our method from the above is to limit the terms to be associated to those that co-occur in the same logical segments of a smaller text size, such as a sentence or a paragraph. Association weights are computed in this way for each document and then accumulated over all documents. This changes it into a roughly O( nk average number of selected keywords for association per document and s is the average number of sentences in a document.

Specifically, keywords or key terms extracted from each documents are first sorted in decreasing order of their term frequencies (TF), or TF  X  Term_Length, or other criterion such as TF  X  IDF (Inverse Document
Frequency) if the entire collection statistics are known in advance. Then the first k terms are selected for asso-ciation analysis. A modified Dice coefficient was chosen to measure association weights as: where S i denotes the number of sentences (or paragraphs) in document i and S  X  T number of sentences (paragraphs) in which term T j occurs. Thus the first term is simply the Dice coefficient similarity. The second term ln(1.72 + S i ), where ln is the natural logarithm, is used to compensate for the weights of those terms in longer documents so that weights in documents of different length have similar range of values. This is because longer documents tend to yield weaker Dice coefficients than those generated from the shorter ones. Therefore, in a collection where relatively long documents may occur, the long ones should better be segmented into several shorter ones to prevent the term ln(1.72 + S
Association weights larger than a threshold (1.0 in our implementation) are then accumulated over all the doc-uments in the following manner: where df k is the document frequency of term k and w k is the width of k (i.e., number of constituent words). To build a term relation structure for later use, terms associated with the same one are sorted in decreasing order of their similarities. In our implementation, about 10 X 30 terms per document were selected for analysis and at most 64 co-occurred terms for each keyword were kept in this term relation structure.

Computation of the similarities among all term pairs can be carried out as the inverted index file for the entire collection is constructed. Weights of term pairs from each document are calculated and accumulated just like the index terms accumulating their document frequencies and postings ( Frakes &amp; Baeza-Yates, 1992 ). In this way, a global term relation structure can be obtained efficiently. As an example, for the 381,375 documents in the NTCIR-4 Chinese collection (469 MB of texts), it takes only 133 min on a notebook computer with a 1.7 GHz CPU, 512 Mega RAM, and 4500 RPM hard disk for indexing, keyword extraction, and term association computation ( Tseng, Juang, &amp; Chen, 2004 ). Compared to traditional approaches, this method improves the efficiency drastically while maintaining sufficient effectiveness.

However, in the light of ideal indexing, most of the term pairs obtained do not exhibit synonymous or near-synonymous relationship. They are only related to the same specific topics or events mentioned in the collection. To obtain closer relations among terms, they are further clustered based on their associated words.
In our implementation, additional similarities among terms are calculated based on how many common asso-ciated terms are shared. The complete-link clustering algorithm is then used with a strict threshold to yield small clusters having high intra-similarities. As a result, terms which do not co-occur in any documents may be clustered together. It is noted that although Latent Semantic Indexing (LSI) ( Deerwester, Dumais,
Furnas, Landauer, &amp; Harshman, 1990 ) exhibits this similar advantage, its time complexity is higher than the method presented here. 3.6. Topic clustering
Clustering is a powerful technique to detect topics and their relations in a collection. Various clustering methods have been proposed for years, such as HAC (Hierarchical Agglomerative Clustering) ( Jain, Murthy, &amp; Flynn, 1999 ), MDS (Multi-dimensional Scaling) ( Kruskal, 1977 ), and SOM (Self-organization Map) ( Kohonen, 1997 ). Many of their computer implementations either in standalone programs or in integrated packages can be found and downloaded from the Web for free (such as those in Karypis; Frank, Hall, &amp;Trigg,Trig ). Use of these tools requires a specification of how similarities between items are computed. 3.6.1. Document clustering
Specifically, in document clustering, each document is processed into a vector form, such as d relies on (1) how terms are selected; (2) how they are weighted; (3) and how similarities are measured. From the experience of topic-based retrieval, term selection affects the performance most among these three factors; the other two are second ( Tseng et al., 2004 ). In our implementation, term selection is based on the extracted key terms, associated terms, and TF or IDF filtering. Terms whose TF in a document is lower than a threshold are removed from that document. Terms whose DF is lower than a threshold or higher than another threshold are also removed. After this basic filtering for outliers, depending on how much information is to be used in the clustering, all the indexed terms can be retained; or only those extracted key terms are kept; or most strictly only those terms in the term relation structure are selected. Although intuitively the last choice may result in the highest-quality (and smallest) set of terms, topic-based retrieval experiments showed that long queries (having long description of information need including seemingly noisy terms) often lead to better perfor-mance than short queries ( Fang, Tao, &amp; Zhai, 2004 ). In text categorization tasks, excessive term selection and reduction may also hurt effectiveness in some real-world test collections ( Bekkerman, El-Yaniv, Winter, more or less, a try-and-error process. This was also observed in text visualization studies, such as ( Booker et al., 1999 ) where parameters are provided for term selection. Although there are performance indices such indices do not directly correspond to the quality of the clusters perceived by humans. From the analyst X  X  point of view, the interpretability of the resulting clusters seems to be more associated with its quality. Therefore, to reduce un-expected efforts in tuning the clustering results, manual verification of selected terms is always rec-ommended whenever it is possible.

As to the weighting of the selected terms, TF  X  IDF is used, which is combined with the cosine function to yield a similarity measure. This choice has a simple reason: the measure ranges from 0 to 1, a range compatible with those required by the mapping tools to be used. Other similarity measures, such as pivoted normalization ( Singhal, Buckley, &amp; Mitra, 1996 ) or OKAPI BM25 (a probability model) ( Robertson &amp; Walker, 1994 ), that perform better in other information retrieval tasks simply do not fit since their similarities are beyond this range and do not reflect the Euclidean distances among clusters. 3.6.2. Term clustering followed by document categorization
Another way for topic clustering is to perform term clustering followed by document categorization. Pre-vious work used the term-document index for term clustering ( Noyons &amp; van Raan, 1998b ). In our implemen-tation, terms are clustered based on their co-occurred terms described above. Using all the terms (clustered and their co-occurred) as training data, documents can then be classified into these clusters based on some classification method, such as KNN ( K -Nearest Neighbor). This approach has the advantage that it allows large volume of documents to be efficiently clustered. 3.6.3. Multi-stage clustering
In either document clustering or term clustering, multi-stage clustering can be applied to gradually identify the knowledge structures from concepts to topics and from topics to categories, or vice versa. Agglomera-tively, the clusters which result from a previous stage are considered as super-documents in the current stage and clustering takes place as the same in the previous stage. Or, divisively, the collection is recursively parti-tioned into smaller sub-domains based on sampled documents and selected features. Then fine-grained clus-tering takes place in each sub-domain. Single-step clustering often leads to skewed document distributions among clusters. Therefore multi-stage clustering is applied in our analysis. 3.6.4. Cluster title generation
One important step to help analysts interpret the clustering results is to generate a summary title for each cluster. Again, this could be a summarization task. We adopt the extraction-based statistical approaches because of their robustness. One commonly used method is to select the most frequent terms in the cluster ( Noyons &amp; van Raan, 1998a; Yang, Ault, Pierce, &amp; Lattimer, 2000 ). That is, terms are sorted by their total frequency in cluster (TFC) in decreasing order. Then the top k terms are selected as the cluster title. Here the TFC of a term in a cluster is defined as the sum of this term X  X  TF in those documents belonging to the cluster.
While this method is simple, it runs the risk of selecting those frequent terms across clusters, making the clusters indistinguishable based on their titles. A remedy to this problem is the introduction of the correlation coefficient method, which computes the relatedness of term T with respect to category or cluster C according to the formula ( Ng, Goh, &amp; Low, 1997 ): where TP (True Positive), FP (False Positive), FN (False Negative), and TN (True Negative) denote the num-ber of documents that belong or not belong to C while containing or not containing T , respectively, as shown in Table 4 .

The correlation method is effective for large number of clusters having short documents in them. But it tends to select specific terms that are not generic enough for clusters having a few long documents, because it does not take TF into account. Therefore, we choose only those terms whose document frequency in a clus-ter exceeds half of the number of documents in that cluster for the correlation method in our implementation. We denote this method as CC 0 : 5 . Another remedy is to multiply the correlation coefficient with the TFC (i.e.,
CC  X  TFC) to rank the terms for title generation. 3.6.5. Mapping cluster titles to categories
The cluster titles generated by the above approach may not be topic-indicative enough to well summarize the contents of the clusters. One might need to map the identified clusters into some predefined categories for ease of interpretations or for supporting other data mining tasks. If the categories have existing data for train-ing, this mapping can be recast into a standard text categorization problem, to which many solutions can be applied ( Tseng &amp; Juang, 2003; Yang &amp; Liu, 1999 ).

Another need arises in that there is no suitable classification system at hand, but some generic labels are still desired for quick interpretations. For example, if documents in a cluster were talking about tables, chairs, and beds, then a title labeled  X  X urniture X  would be perfect for this cluster, especially when this hypernym does not occur in this cluster. This case is often solved by human experts, such as those in Lai and Wu (2005), Glenis-son, Glanzel, Janssens, and De Moor (2005) , where cluster titles are given manually. Below we propose an automatic solution by use of an extra resource, i.e., WordNet.
 WordNet is a digital lexical reference system developed by the Cognitive Science Laboratory at Princeton
University ( WordNet: a lexical database for the English language ). English nouns, verbs, adjectives and adverbs are organized into synonym sets. Different relations, such as hypernym, hyponym, meronym, or hol-onym, are defined to link the synonym sets. With these structures, one can look up in WordNet all the hyper-nyms of a set of given terms and then choose the best among them with some heuristic rules. Since the hypernyms were organized hierarchically, the higher the level is, the more generic the hypernyms are. To main-tain the specificity of the set of terms while revealing their general topics, the heuristics have to choose as low-level common hypernyms as possible. When there are multiple choices, ranks should be given to order the hypernyms in priority.

In our implementation, we look up for each given term all its hypernyms alone the path up to the root in the hierarchical tree. The number of occurrence ( f ) and the depth in the hierarchy ( d ) of an encountered hypernym are recorded. With the root being given a depth value of 0, a weight proportional to the normalized f and d is calculated for each hypernym as follows: where nt is the number of given terms to normalize the occurrence f to a value ranges from 0 to 1 and c (0.125 in our implementation) is a constant to control the steepness of the sigmoid function 1 =  X  1  X  exp  X  c d  X  X  whose value approaches 1 ( 1) for large positive (negative) d . Since the depth d only takes on non-negative value, the actual range of the sigmoid is from 0.5 to 1. It is thus subtracted with 0.5 and then multiplied by 2 to map the value of d into the normalized range: 0 to 1. Note that a term having no hypernym or not in WordNet is omitted from being counted in nt . Also note that a term can have multiple hypernyms and thus multiple paths to the root. A hypernym is counted only once for each given term, no matter how many times the multiple paths of this term pass this hypernym. This weight is finally used to sort the hypernyms in decreas-ing order to suggest priority.

Back to the previous example where the three terms were given: table, chair, and bed, their hypernym:  X  X  X ur-niture X  X  did result from the above calculation with a highest weight 0.3584 based on WordNet version 1.6. 3.7. Topic mapping
To represent the detected knowledge structures, two techniques are mainly used: HAC and MDS. Based on the pre-calculated similarities between each topic, the HAC method organizes the topics in a hierarchical way.
This creates a structure that is readily available to the folder tree representation. In order to get higher intra-cluster similarities, the traditional complete-link clustering algorithm is chosen and developed as our HAC method. Similarly, from the pairwise similarities, the MDS technique computes the coordinates of each topic in specified dimensions of Euclidean space, which are usually 2 or 3 for ease of visual interpretation. With these coordinates, a topic map can be created by a plotting tool. We use the MDS program in the RuG/ L04 package ( Kleiweg (Software for Dialectometrics &amp; Cartography) ) for coordinate computation and the
GD module in Perl for plotting. A circle is used to denote a topic. The size of the circle is designed to reflect the number of patents in it. Before plotting, the topics are further clustered by the complete-link algorithm with a specified threshold to divide them into several groups. Topics belonging to the same groups are plotted with the same colors so as to make the map more informative.

More advanced topic maps such as those introduced in Section 2 can be created by combining the struc-tured information from the patents. For examples, by use of the patents from different time spans based on their filing or application dates, we may draw a series of growth maps or evolution maps. Specifically, patents not within the specified dates are removed from those clusters. The similarities among the resultant clusters are re-calculated and mapped based on the reduced number of patents. As long as the number of pat-ents does not change largely, the change in the map should be minimal. With a series of such slightly changed maps complied in an animated file format for visualization, one can trace the trend patterns in an intuitive way.
 To facilitate such use of structured information, a database schema was designed. An example is shown in
Fig. 5 . Retrieving such information for use in various types of topic maps can be made easy and efficient through SQL queries. 4. Technique evaluation
This section presents the evaluation of important techniques discussed above. We believe that an effective text mining process relies on effective component techniques. Thus knowing their performance is important to gain confidence on the final text mining results.

Two patent collections were used in the evaluation: the major one is the 92 CNT patents introduced in the beginning; the other is the 612 patents from NSC (National Science Council), which will be discussed in more details in the next section. 4.1. Text segmentation
To evaluate the effectiveness of text segmentation and summarization, the full text of each patent was parsed into six segments from which a document set for each segment is created. Therefore, each set contains exactly the same number of documents as the original collection. Besides these six segment sets, we also created two other sets for later comparison: the segment extract set and the full-text set. Each is named with an abbreviation as follows: 1. abs : corresponds to the  X  X bstract X  section of each patent. 2. abs : corresponds to the segment of FIELD OF THE INVENTION. 3. task : corresponds to the BACKGROUND OF THE INVENTION. 4. sum : corresponds to the SUMMARY OF THE INVENTION. 5. fea : DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT.
 6. cla : corresponds to the Claims section of each patent. 7. seg _ ext : corresponds to the top-ranked sentences of each document from the sets: abs, app, task, sum, 8. full : corresponds to all the documents from the sets: abs, app, task, sum, and fea.

Each document in the segment extract set (seg_ext in the above) is the concatenation of the machine-generated summaries from each of the five segments (not including the Claims segment) in the same patent. The best s sentences are selected as the summary from each segment, where s is 6 in our implementation. Similarly, each document in the full set is the concatenation of the un-summarized texts from each of the five segments.
To show the segmentation results, among 6  X  92 = 522 segments in the CNT collection, only 9 do not con-tain any text, an empty rate of 1.63%. These include one empty document in the  X  X pp X  set, two in the  X  X ask X  set, five in the  X  X um X  set, and one in the  X  X ea X  set. Among these nine empty documents, five are due to the lack of such sections in the original patent documents, three are due to the use of special section titles, and one is due to the erroneous spelling of the section title. The other two segments, the Abstract and Claims sections, always lead to correct extraction. 4.2. Text summarization
The use of segment summaries as document surrogates is an important feature in our approach. Knowing whether they are useful for subsequent processing and analysis is a key question to be answered. However, evaluation of automated summaries is not an easy task. Two main approaches are commonly applied: intrinsic and extrinsic ( Mani, 2001 ). In intrinsic evaluation, manually prepared answers or evaluation criteria are com-pared with those which are machine generated. In extrinsic evaluation, automated summaries are evaluated based on their performance or influence on other tasks. We adopt the extrinsic approach since it is obviously suitable for our purpose.

In manual creation of a technology-effect matrix or a patent map for analysis, it is helpful to quickly spot the keywords that can be used for classifying the patents in the map. Once the keywords or category features are found, patents can usually be classified without reading all the texts. Thus a summary that retains as many important category features as possible is preferable. Our evaluation design therefore is to reveal whether the segment extract set (the seg_ext in the above) contains enough such features, compared to the other seven doc-ument sets.
 In the following, the method for selecting category features for classification is introduced. By using the
CNT patent map as our experimental data, the segments where important features occur are recorded for each patent and such occurrences are accumulated over all patents. The location distributions of important features among these segments are then compared. 4.2.1. Feature selection
Selecting the best category features for document categorization has been studied in the fields of machine learning and information retrieval. Yang and Pedersen (1997) compared five different methods. They found that Chi-square is among the best that lead to highest performance. The Chi-square method computes the relatedness of term T with respect to category C as: which is exactly the square of the correlation coefficient introduced previously. However, as pointed out by Ng et al. (1997) , the correlation coefficient selects exactly those terms that are highly indicative of membership in a category, whereas the Chi-square method will not only pick out this set of terms but also those terms that are indicative of non-membership in that category. This is especially true when the selected terms are small in number. As an example, in a small real-world collection of 116 documents with only two exclusive categories: construction vs. non-construction in civil engineering tasks, some of the best and worst terms that are com-puted by Chi-square and correlation coefficient are shown in Table 5 . As can be seen, due to the square nature, the chi-square weights negatively related terms as highly as positive ones. (For the term:  X  X ngineering X , the square of 0.7880 is 0.6210.) Therefore, instead of Chi-square, the correlation coefficient is used as our feature selection method. 4.2.2. Experiment results
In the technology-effect matrix of the CNT map, there are nine leaf-categories in the technology taxonomy and 21 leaf-categories in the effect taxonomy. Based on the correlation coefficient method, N ( N = 50) top-ranked features for each of the leaf-category in each document set were extracted. The number of sets in which such a feature occurs, denoted sc for set count , was calculated. The features (less than 50 8  X  400) were then ranked by sc in descending order.
 An example for some ranked features in the category FED (Field Emission Display) is shown in Table 6 .
The fifth row shows that the feature  X  X  X lectron X  X  occurs in 5 document sets. It occurs in 27 documents in the  X  X bs X  set and has a correlation coefficient of 0.31 in that set. The first column titled rel denotes whether the term is a relevant category feature or not. This is judged by one of the experts who participated in the creation of the CNT map. It should be noted that such a judgment is quite subjective and sometimes contradictive so that good features (such as  X  X  X isplay X  X  and  X  X  X athode X  X  in this example) that are effective in discriminating the categories of the analyzed documents may not be judged as relevant. This is often observed in feature selection studies as statistically important terms are hard to identify manually. (For example, the three terms:  X  X  X s X  X ,  X  X  X ts X  X , and  X  X  X oss X  X  solely can achieve 93.5% classification accuracy for the largest category  X  X  X arn X  X  in the Ruet-ers-21578 test collection ( Bekkerman et al., 2001 ).) Also note that, from Table 6 , the correlation coefficients in each segment set correlate to the set counts of the ordered features: the larger the set count, the larger the cor-relation coefficient in each segment set.

The M best features ranked by segment count were then selected, and are called important category features (ICFs). Their occurrences in each document set for each category were counted and averaged as: where MBTC( s , c ) stands for  X  X  X -Best Term Coverage of set s for category c  X  X . Part of the results for M  X  30 are shown in Table 7 . As the first row shows (the FED category), among the 30 ICFs, 16 occur in the  X  X bs X  set, covering 53% of them, and 21 in the  X  X eg_ext X  set, covering 70% of the 30 terms, which reveals that most ICFs can be found in the best six sentences from each segment. The 30 ICFs were further checked by one of the
CNT map creators. The results are shown in Table 8 . The first row shows that among 7 relevant features, 6 of them occur in the  X  X bs X  set, covering 87.5% of them.

The term-covering percentage (of the best terms and the relevant terms) of each set was accumulated and averaged over all categories with respect to the technology taxonomy and the effect taxonomy, respectively, as follows: where j Cat j denotes the number of categories in the taxonomy. The results are shown in Table 9 . The second column denotes the number of categories ( nc ) in that taxonomy and the third column denotes the average number of terms in each category ( nt ) for calculating the term-covering average. The rows with a star in the first column denote that the average is calculated from the human-judged relevant terms. As the bold-faced data show, most machine-derived ICFs occur in the segment extracts, while most human-judged ICFs occur in the abstract section.

To see if important sets change when the number of ICFs changes, we varied the number M for additional values, i.e., 10 and 50, and calculated the averaged term-covering rates again. The results in Fig. 6 show that  X  X bs X ,  X  X um X  and  X  X eg_ext X  are important sets. The  X  X ull X  set becomes important only when more terms are included for consideration. 4.2.3. Findings
From Fig. 6 and Table 9 , it can be seen that: (1) Most ICFs ranked by correlation coefficient occur in the segment extracts, the Abstract section, and the SUMMARY OF THE INVENTION section. (2) Most ICFs selected by humans occur in the Abstract section or the Claims section. (3) The segment extracts lead to more top-ranked ICFs than the full texts, regardless whether the category features are selected manually or auto-matically ( Tseng, Juang, Wang, &amp; Lin, 2005 ). 4.3. Key term extraction and association
Identifying multi-word phrases in an English document is somewhat like identifying words in a Chinese text, because both have no lexical delimiters. As such, we applied the key term extraction algorithm to the patent sets as well as some Chinese news documents to show its effectiveness.

From the segment extract set (seg_ext) of the CNT patents, 942 multi-word phrases were extracted. They all occur at least twice in a document. Among them, only 132 occur in at least two documents. Most phrases are composed of two words (86 terms), and 31 terms are three-word phrases. An assessment of these 132 terms shows that there are 3 adjective phrases such as  X  X  X fficient nonlinear X  X , 15 short clauses such as  X  X  X lter the poros-ity X  X  and  X  X  X esistance to fluid X  X , and all others are correct noun phrases, yielding an error rate of 18/ 132 = 13.64%. To reduce the error rate, a stricter filtering rule such as removing those terms having stop words in them can be applied.

From a small collection of 100 Taiwan news articles, an average of 33 keywords (words that occur at least twice) in an article were extracted, in which an average of 11 terms (or 33%) of them were new to a lexicon of 123,226 terms. The total distinct 954 new terms contains 79 illegal words, an error rate of 8.3%. Compared to the total distinct 2197 extracted keywords, the error rate is only 3.6%. The longest new terms contain nine characters, while the shortest ones contain two characters.

The evaluation of the term association requires more manual efforts. To show the effectiveness in a large scale, a set of Chinese documents was used so that we can evaluate the quality of the co-words in a confident way.

In our experiment, 30 topics (single-term queries) were selected from the index terms of 25,230 Chinese news articles, from which term association was analyzed. Five assessors (all majored in library science) were invited for relevance judgment. For each topic, its top N ( N = 50) co-words were examined. Users were asked if they thought the relationship between a topic and each of its co-words was relevant enough. If they are not sure (mostly due to lack of domain knowledge), they are advised in advance to retrieve those documents that may explain their associations. The results show that in terms of percentage of relatedness, 69% co-words were judged relevant to the topic terms in averege ( Tseng, 2002 ). In another similar experiment with a much larger collection (154,720 documents), the percentage of relatedness increases to 78% ( Ye, 2004 ). As can be seen, the more the documents for analysis, the better the effectiveness of the term association, a phenomenon that is commonly observed in robust machine learning algorithms.

The extracted keywords and their associated terms have a number of direct applications. They can be used in query expansion to improve retrieval performance ( Tseng et al., 2004 ) or they can be suggested to analysts for prior art search or for exploring the knowledge structure underlying the collection. Fig. 7 demonstrates an application example, where a set of subsumed terms was suggested in response to the query  X  X  X ens array X  X  such that the sub-topics in the collection were revealed. Clicking on one of the suggested terms, a map would be displayed to show the relations of the term with other topics.
 4.4. Cluster title generation
For comparison, we used three methods to rank cluster terms for title generation, namely the modified Cor-relation Coefficient (CC 0.5 ), Term Frequency in Cluster (TFC), and multiplication of these two: CC  X  TFC.
These ranking methods were applied to three sets of clustering results. The first is the first-stage document clustering from the CNT patents. The second is the second-stage document clustering from the CNT patents.
The third is the third-stage term clustering from clustering the keywords extracted from NSC patents based on their co-words. For each cluster, at most 5 best terms were selected as its title. Two master students majored in library science then compared the relative quality of these terms under the same clusters. For each ranking method, the number of cases where it has the best title quality over the other methods was counted. Multiple best choices for a cluster are allowed and those cluster titles that are hard to assess can be omitted from being considered. The ranking methods are coded in such a way that the assessors do not know which method is used to generate the examined titles. The results are shown in Table 10 . Note that hierarchical clustering struc-tures were shown to the assessors. They were free to examine whatever clusters or sub-clusters they were inter-ested in. This is why the numbers of examined clusters differ between them.

In spite of this difference, this preliminary experiment shows that titles generated by CC favorable by one assessor, while those generated by TFC are not by either. This is somewhat surprised to know, since most past studies use TFC or a variation of it to generate cluster titles.
 4.5. Mapping cluster titles to categories
The proposed title mapping algorithm is applied to two real-world cluster sets: the first is a term cluster set, the second is a document cluster set, both are the final-stage clustering results from the NSC patents to be discussed in the next section. The first set has 10 clusters and the second has 6. Their cluster titles are shown in the second column of Table 11 .

The proposed method is compared to a similar tool called InfoMap ( Information Mapping Project ) which is developed by the Computational Semantics Laboratory at Stanford University. This online tool finds a set of taxonomic classes for a list of given words. It seems that WordNet is also used as its refer-ence system, since the output classes are mostly WordNet X  X  terms. An agent program is written to send the title words to InfoMap and collect the results that it returns. Only the top-three candidates from both methods are compared. They are listed in the last two columns in Table 11 , with their weights appended.

The reasonable classes are marked in boldface in the table. As can be seen, the two methods perform sim-ilarly. Both achieve a level of 50% accuracy in either set. 5. Application example 5.1. The NSC patent set One of the objectives of this work is to help analyze the US patents whose assignee is National Science
Council (NSC). NSC is the major government agency that sponsors research activities in Taiwan. Institutes, universities, or research centers, public or private, can apply for research fundings from NSC. Once the research results yield any US patents, the intellectual property rights belong to NSC. In other words, NSC becomes the assignee of the patents. However, this policy has been changed since year 2000. NSC no longer insisted on the ownership of the rights. The applicants own the management rights of these intellectual properties.

Due to this background, these documents constitute a knowledge-diversified collection with relatively long texts (about 2000 words per document) describing various advanced technical details. Analysis of the topics in this collection becomes a non-trivial task as very few analysts know of such diversified technologies and fields.
Although each patent has pre-assigned International Patent Classification (IPC) or US Patent Classification (UPC) codes, many of these labels are either too general or too specific to fit the intended knowledge struc-tures for topic interpretation. Therefore, using them alone does not meet the requirement of the analysis. As such, organizing the collection by text mining techniques becomes an important method to help humans understand this collection.

The NSC collection was created by searching the USPTO X  X  web site using  X  X  X ational Science Council X  X  as the search term limited to the assignee field. A total of 612 patents were downloaded on 2005/06/15. 5.2. Text mining processing
The 612 patents were parsed, segmented, and summarized. Among the 6 empty segments resulted, yielding an empty rate of 2.15%. The full texts of these patents take up 23.9 MB.
After summarization with at most 6 sentences for each of the 5 segments (the Claims segment is excluded from the task of topic analysis), the size becomes 2.93 MB, a compression ratio of 87.74%. In spite of this, the lost important terms are few. From the full text set, a total of 20,072 keywords (terms occur at least twice in a document) were extracted. Among them, 19,343 can be found from the 5-segment summary set. Most of the 20,072 keywords occur in one document. Only 2714 of them occur in at least two and have at least one co-occurred terms associated with them.

These 2714 terms were then used to index the document surrogates. With these concise representations in documents and terms, a set of topic maps were generated.
 5.3. Topic mapping
The terms and their co-occurred words alone can be used to generate a topic map for the collection. The above 2714 terms were clustered by a complete-link method, based on how many co-occurred terms they share, into 353 small-size clusters. These clusters were then repeatedly clustered, again based on the common co-occurred terms, into 101 medium-size clusters, then into 33 large-size clusters, and finally into 10 topic clus-is gradually set to a higher value (from 1, 2, 4, to 6) to reflect the need for the clusters to be more and more general in topics in each successive stage. The reason that we performed this multi-stage clustering is due to the fact that even we set a lowest threshold, we could not obtain as low as 10 clusters in a single step for reasonable visual interpretation. The 353 small-size clusters are actually obtained with the similarity threshold set to 0.0. In other words, among the 2714 terms, no two terms in different small-size clusters share the same co-words.
The 33 large-size clusters were mapped by the MDS method as shown in Fig. 8 , in which clusters belonging to the same topics were painted with the same colors. Note the clusters in grey color were not grouped with the others, they form individual topics on their own, not counted in the above 10 topics. Table 12 lists part of these clusters and their topic titles. Topic 1 includes clusters with IDs: 131, 473, 6, and 61, which correspond to the red circles 1 located in the left-upper corner in the map. As can be seen from the title words, this topic is about chemical compounds, which can be generated from our WordNet-lookup algorithm automatically. Other clus-ter X  X  categories are shown in Table 11 , where only 50% of them are reasonable. If manual labeling is allowed, some topics can be easily labeled from the cluster titles. An example is topic 7, where  X  X  X iology X  X  seems to be a reasonable category name from its title words.

Note the above clustering did not involve any information from the documents themselves, except for the initial stage where the full texts are used to extract key terms and their co-words. As another way for topic analysis, the 612 NSC patents were clustered based on their summary surrogates. They were first clustered into 91 topics, which in turn were grouped in 21 sub-domains, from which 6 major domains were found. Fig. 9 shows the relative closeness among these 21 sub-domains. Table 13 lists the hierarchical results. Again, their machine-derived cluster categories are shown in Table 11 , with 50% accuracy. For manual labeling, we tagged each major domain in Fig. 9 by reading their title words in Table 13 based on the academic divisions to be discussed later. With some basic knowledge in science and technology, each domain was labeled without dif-ficulty, except domain 3 whose title words are more diversified. It was thus labeled Generality for general topics.

More types of advanced analysis can be obtained by combining the topic maps with the patents X  structured information. Their implementation involves techniques from the fields of database management and user interface. The details of which are beyond the scope of this discussion. 5.4. Topic analysis comparison
The topic distribution of the NSC patents had actually been analyzed by a subsidiary center of NSC: Sci-ence and Technology Information Center (STIC). STIC analyzed this patent set first by IPC. The results in the form of the topic tree are shown in Table 14 , where major categories are shown in boldface and with their IPC descriptions. Those minor categories are listed together with their numbers of patents shown in the parenthe-ses. As can be seen, these patents spread over a large range of IPC categories and many of such categories are either ambiguous or not at the detailed-enough level or abstract-enough level that is needed by an analyst.
Further breakdown of those ambiguous categories yields similarly divergent and skewed distribution. This makes them hard for further analysis.

As an alternative, STIC used the division information of these patents to show their topic distribution, as is shown in Table 15 . This academic division information comes from the actual divisions of NSC where the patents were managed. In each division, the patent trend (distribution of numbers of patents over years) and the top-performing institutes (those apply for most patents) were then analyzed by STIC, where the institute information again comes from NSC.

The division-wise analysis is quite independent of each other. As more interactions are involved in nowa-days researches, inter-disciplinary relations are interesting to monitor. To reflect this need, the text mining approach was applied, because it relates patents not only by predefined classification, but also by the content they share. As Fig. 9 shows the relationship among the clusters, Figs. 10 and 11 further reveal the IPC and division distributions in each cluster, respectively.

In Fig. 10 , the IPC categories now become disambiguated when considering the other co-occurred catego-ries in the same cluster. For example, in Cluster 1, A61 co-occurred with C08 and C07 such that the patents in this cluster should be more about medical chemistry, while less about hygiene. In Cluster 6 it co-occurred with
C12 such that the patents should be more about biomedical chemistry or biomedicine, while less about beer, wine, etc. Fig. 11 supports these observations in all these six clusters.

Comparing among Fig. 9 , Tables 14 and 15 , these three classification systems provide different facets to understand the topic distribution of the patent set. Each may reveal some insights if we can interpret it.
The IPC system results in divergent and skewed distributions which make it hard for further analysis (such as trend analysis). The division classification is the most familiar one to the NSC analysts, but it lacks inter-disciplinary information. As to the text mining approach, it dynamically glues related IPC categories together based on the patent contents to disambiguate their vagueness. This makes future analysis possible even when the division information is absent, as may be the case in later published patents to which NSC no longer claims their right. 6. Discussions
This section discusses the findings, experiences, and implications from the above application examples and evaluation results. Related studies are also reviewed, of which some inspire our solutions and some compen-sate our work.

The patent sections appear quite regular and each has its own functions in describing the innovations. This regularity makes the segment matcher a simple case to design. Our ad hoc implementation shows only a frac-tion of segments (less than 3%) fails to be extracted. Most failure cases are due to the omission of the corre-sponding sections.

The purpose of summarization is to facilitate people X  X  understanding of a text quickly or easily. In the pat-ent analysis scenario for patent map generation, the tasks include quickly spotting the topic-relevant sections and in those spotted sections quickly focusing on the topic-indicative terms for classification. Our work through document segmentation and summarization has provided strong evidence to help solve these tasks.
Specifically, our results show that the Abstract, Summary, and summaries from each section in a patent doc-ument are the most topic-relevant sections, regardless of whether the classification is for technological aspects or functional aspects. The implication is that if one would like to quickly determine a patent X  X  category based on only a few terms in a quick pace, one should first read the Abstract section or the Summary section. Or alternatively, one could first browse the  X  X  X egment extracts X  X  generated automatically.

However, our heuristic-based method needs further improvement to yield summaries suitable for direct manual analysis, especially for the  X  X  X ummary of the invention X  X  and  X  X  X etailed description X  X  sections. Although  X  X  X roblems to be solved X  X  extracted from the background section seem feasible, as shown in our example, auto-mated summarization of  X  X  X he solution methods X  X  is still a challenge. In addition, the claims in patent docu-ments are written in a special style that is not easy to read. It is thus also an important part that needs further processing. Recent studies have shown promising in transforming, although not summarizing, original long claims into short sentences to increase readability ( Sheremetyeva, 2003; Shinmori, Okumura, Marukawa, &amp; Iwayama, 2003 ).

An efficient key term extraction algorithm is presented and used in the text mining process. Although Fagan (1989) and Smadja (1993) had presented similar phrase extraction algorithms without using machine-readable dictionaries, their methods use corpus-wide statistical information such as document frequency of a term or a pair of terms. As such, their results and effectiveness depend on the corpus used. In contrast, ours can be con-sidered as rule-based. It uses only three simple rules to respectively drop, merge, and accept the sub-phrase units during a merging back process based only on their term frequencies within a document. The accepted repeated patterns are then filtered by a stopword list. As such, its effectiveness depends on whether our assumption (i.e.,  X  X  X f a document focuses on a topic, it is likely to mention a set of strings repeatedly X  X ) holds for the document language and the domain we are dealing with. For languages like Chinese or for domains like the technical patents, our method works well in terms of non-dictionary topical terms that can be extracted.

The co-word analyses based on document-wide co-occurrence or latent semantic indexing require enor-mous computation for large collections. Therefore, an analysis based on snippet-wide co-occurrence is devised. Although such an idea has been proposed previously, such as those in Brown, Della Pietra, De Souza,
Mercer, and Lai (1992) and Schutze (1993) , the difference between ours and others lies in the refinement. As our experiments showed ( Tseng, 2002 ), a na X   X  ve implementation of the idea leads to only 48% of co-occurred terms judged as relevant, while a refined implementation leads to 69%. It is interesting to note that, as with many robust machine learning algorithms, the more the documents for analysis, the better the results from our association computation. Furthermore, through the clustering of terms based on their co-words, terms not occurring in the same documents can be associated.

The term suggestion mechanism based on the extracted keywords and their co-words has been mentioned by Larkey (1999) . At the request of the US patent office, Larkey provided such a search aid with the Inquery system which uses WordNet to help extract phrases and uses section-level co-occurrence to extract co-words.
As Fujii et al. (2004) showed in the task of invalidity search, the machine-generated results from 30 systems do not cover all the relevant patents, manual searches contribute as many relevant ones. These evidences show that hybrid approaches like term suggestion would be of great help to prior art search.

A multi-stage clustering approach is used to mine the knowledge structures and transform them from con-cepts to topics and in turn from topics to categories. Three ranking functions to select the cluster titles are compared. Preliminary evaluation shows that the ranking method commonly used in past studies does not yield favorable results when compared to the others.

Cluster labeling is important for ease of human analysis. Uchida, Mano, and Yukawa (2004) showed that although their clusters coincided with human X  X  for a high percentage, their method failed to produce good cluster labels because human labels are mostly compound nouns, while theirs are mostly single words due to the difficulties of determining compound words. Their work reminded us the importance of phrase extrac-tion and motivated us to map the cluster titles to more generic labels.

In the attempt to produce generic cluster labels, a hypernym search algorithm based on WordNet is devised. Real-case evaluation shows that only 50% cluster labels lead to reasonable categories. However, this is due to the limit of WordNet in this case. WordNet does not cover all the terms extracted from the NSC patents. Also WordNet X  X  hypernym structures may not reflect the knowledge domains desired for analyzing the NSC patents. If a suitable classification system can be found, the hypernym search algorithm may lead to more desirable results. Because the algorithm uses only the depth and occurrence information of the hyper-nyms, it is general enough to be applicable to any hierarchical systems.

The clustering results are plotted on 2-dimensional topic maps with the MDS method. This way of visual-izing the results has been used in scientometrics for technology watch or scientific policy decision. However, insights are still hard to derive from these maps alone. After years of mapping scientific domains, Noyons and
Buter commented that such an overview (via the visualized maps) can be used to find information if you are able to interpret it. In a science and technology foresight analysis based on publication citations ( The 8th Sci-ence &amp; Technology Foresight Survey, 2004 ), questionnaires were gathered from 37 experts in 38 domains for comments and feedback. Some experts confirmed that these maps and topics analyses are valuable, whereas some could not see the practical use of them because of their uninterpretability or improper labels and orga-nization of the topical domains.

In our case, the topic maps created from the NSC patents were presented to three analysts for their com-ments. One common reaction is that without reading any patents, they are able to have a multilayered over-view of them, which is a great relief. Although from IPC or UPC codes, similar overviews can be generated, as has been shown, these pre-defined categories do not meet the domain structures desired by the analysts. A second reaction is that if trend types can be classified and marked on the maps for each cluster, spotting and tracing the trend patterns on the maps can be easier. The suggested types include quantity-based trends such as changing ratios of number of patents in a cluster and quality-based trends such as changing ratios of number of citations to the patents in a cluster. Another valuable comment suggests replacing each circle on the map with a pie chart in order to show the distribution of different assignees or countries for comparative stud-ies and competitive analyses. This would allow them to spot which domains are dominated by which institutes or countries, for example. Analytical information like this could be very useful for policy making. In short, by combining structured information in a friendly interface for various types of interactive analysis and visual-ization, the topic maps could lead to valuable insights for analysts.

In recent years, SOM has also been applied to patent analysis. The variation WEBSOM method has been tested on nearly 7 millions of patent abstracts ( Lagus, Kaski, &amp; Kohonen, 2004 ). Nevertheless, Lamirel, Shadi
Al Shehabi, Hoffmann, and Francois (2003) pointed out that this method only provides the analyst with gen-eral overview of the topics covered by the patents. They then proposed a MultiSOM model that introduces the concepts of viewpoints and dynamics to assist information analysis based on its multi-map display and its inter-map communication process. Specifically, each viewpoint corresponds to a map which is created based on a specific section of the patent. Inter-map communications are activated through the same set of patents in different maps. The advantages of the MultiSOM method include reducing the noise which is inevitably gen-erated in an overall classification approach while increasing the flexibility and granularity of the analyses.
With this potential usefulness, we started to try from the basic SOM by use of the tool developed by Kle-iweg (Extended Kohonen maps) . We used the default setting of the tool and varied the map size from 8  X  8, 10  X  10, 16  X  16, to 34  X  34. Unfortunately, all these maps could not lead us to a meaningful result for this
NSC patent set. The resulting SOMs do not show desired clustering when each patent was labeled with the academic divisions introduced above. Most divisions scatter all over the map. Even the most unique Biotech-nology has outliers in different locations. In the end, no further variation of SOM was tried.

In another attempt to compare our results with others X , we also have tried to cluster these NSC patents based on their co-cited and co-citing intensity. But only 123 (among 612) patents are co-cited by others result-ing in 99 co-cited pairs and only 175 (among 612) patents co-cite others resulting in 143 co-citing pairs. Such sparseness may lead to biased analysis. Therefore, citation analysis is not suitable in this case.
In short, we have tried to compare our approach with others, but this effort has not yet led to meaningful results. 7. Conclusions and future work
This paper describes a series of mining techniques that conforms to the analytical process adopted by and used to train patent analysts (at least in Taiwan, see Chen, 1999 ). The automation of the whole process not only helps create final patent maps for topic analysis, but also facilitates other patent analysis tasks because each step in this process has its own application. For example, after segmentation, more effective classification can be achieved by combining individual segment matching rather than by whole patent matching ( Kim,
Huang, Jung, &amp; Choi, 2005 ). After abstraction, the topics, functions, or technologies revealed in each patent can be more easily accessed and shared by the analysts. After keyword extraction and co-word analysis, rel-evant terms can be suggested to users in prior art search to relieve the human burden in devising domain-spe-cific search terms. After clustering, the patent collection is organized in a way that may complement the IPC or
UPC partition of the collection for analytic purpose. And finally, visualization is a way to combine all these results to suggest patterns, relations, or trends.

In the design of these techniques, we have proposed a rigorous approach to verify the usefulness of segment extracts as the document surrogates, a dictionary-free keyphrase extraction algorithm, an efficient co-word analysis method, and an automatic procedure to produce generic cluster titles. Evaluation of these techniques has shown some success.

Nevertheless, more patent-related text mining issues can be studies. Unlike scientific publications, the reg-ular style of the patent documents deserves more attention. So far we only use the summaries of the segments for document surrogates. The functions of each segment have yet to be explored. For example, one may use only the background segment for domain analysis, since it covers the domain background of a patent, not the details of it. Moreover, one may extract the problem to be solved from the background segment and major solution methods from the summary of the invention with more sophisticated summarization techniques.
By clustering the problems and solutions individually like the patent mapping task in the NTCIR Workshop 4, an analytic map similar to the technology-effect matrix can be created for a domain of patents. Our future work may explore this direction in using the patent segments.
 Acknowledgements
This work is supported in part by NSC under the grants numbered: NSC 93-2213-E-030-007-and NSC 94-2524-S-003-014-.

Appendix A. The clue words for patent summarization are listed below. These words are mainly for the back-ground segment.
 Advantage Difficult Improved Overhead Shorten Avoid Effectiveness Increase Performance Simplify Cost Efficiency Issue Problem Suffer Costly Goal Limit Reduced Superior Decrease Important Needed Resolve Weakness Appendix B The background segment of a US patent and its summaries. Those best sentences selected by Microsoft Word are in italic font, while those selected by our method is in boldface.
 very large databases.
 systems. Relational database management systems are often used in so-called  X  X  X ata warehouse X  X  applications where enormous amounts of data are stored and processed. In recent years, several trends have converged to create a new class of data warehousing applications known as data mining applications. Data mining is the process of identifying and interpreting patterns in databases, and can be generalized into three stages. data warehouse implementations start with a focused application in a specific functional area of the busi-ness. These applications usually focus on reporting historical snap shots of business information that was previously difficult or impossible to access. Examples include Sales Revenue Reporting, Production Reporting and Inventory Reporting to name a few.
 end-users gain previously unseen views of their business, they quickly seek to understand why certain events occurred ; for example a decline in sales revenue. After discovering a reported decline in sales, data warehouse users will then obviously ask,  X  X  X hy did sales go down? X  X  Learning the answer to this question typically involves probing the database through an iterative series of ad hoc or multidimensional queries until the root cause of the condition is discovered. Examples include Sales Analysis, Inventory Analysis or Production Analysis.
 become more sophisticated, they begin to extend their analysis to include prediction of unknown events.
For example,  X  X  X hich end-users are likely to buy a particular product X  X , or  X  X  X ho is at risk of leaving for the competition? X  X  It is difficult for humans to see or interpret subtle relationships in data, hence as data warehouse users evolve to sophisticated predictive analysis they soon reach the limits of traditional query and reporting tools. Data mining helps end-users break through these limitations by leveraging intelligent software tools to shift some of the analysis burden from the human to the machine, enabling the discovery of relationships that were previously unknown.

Most of these technologies, however, are used in a desktop environment where little data is captured and maintained. Therefore, most data mining tools are used to analyze small data samples, which were gath-ered from various sources into proprietary data structures or flat files. On the other hand, organizations are beginning to amass very large databases and end-users are asking more complex questions requiring access to these large databases. References analytical techniques used in data mining are algorithmic-based rather than data-driven, and as such, there are currently little synergy between data mining and data warehouses. Moreover, from a usability perspec-tive, traditional data mining techniques are too complex for use by database administrators and applica-tion programmers, and are too difficult to change for a different industry or a different customer. Cluster analysis finds groupings in the data, and identifies homogenous ones of the groupings as clusters.
If the database is large, then the cluster analysis must be scalable, so that it can be completed within a practical time limit.
 tions and the execution times required . Often, the solution to finding clusters from massive amounts of detailed data has been addressed by data reduction or sampling, because of the inability to handle large vol-umes of data . However, data reduction or sampling results in the potential loss of information. and that allow non-statisticians to benefit from advanced mathematical techniques available in a relational environment .

