 1. Introduction
In information retrieval, the data fusion technique has been investigated by many researchers and quite a few data fusion chain-based methods [5,21] , the linear combination method [3,26,27,29] , the multiple criteria approach [8] , and others an effective technique for obtaining better results.

Data fusion methods for information retrieval can be divided into two categories: score-based methods and rank-based methods. These two different types of methods apply to different situations. If some component systems only provide a ranked list of documents as the result of a query, then rank-based methods can be applied. If each component system provides scores for all retrieved documents, then score-based methods can be applied. In those aforementioned methods, CombSum [9,10] , CombMNZ [9,10] , the correlation method [30] , the linear combination method [3,26,27,29] are score-based methods, while Condorcet fusion [19] , Markov chain-based methods [5,21] , and the multiple criteria approach [8] are rank-based methods.
Among those score-based data fusion methods, the linear combination method is very flexible since different weights can be used for different component systems. It is especially beneficial when component systems involved vary considerably in perfor-that needs to be carefully investigated.
 retrieval systems are evaluated over a group of training queries using the same document collection. If system a 's average method. Some alternatives to the simple performance level weighting scheme were investigated in [29] . On the other hand, both system performance and dissimilarities between systems were considered in [30] to determine all systems' weights. Although the linear combination method with these weighting schemes usually outperforms CombSum and CombMNZ improvement since all these weighting schemes are heuristic.

In this paper we discuss a new approach, multiple linear regression, to training system weights for data fusion. Our experi-ments demonstrate that this approach is more effective than other approaches. To our knowledge, multiple linear regression has not been used for such a purpose before, though it has been used in various other tasks in information retrieval.
Another issue is how to obtain reliable scores for those retrieved documents. Sometimes scores are provided by component approaches and found that the logistic regression model and the cubic regression model could provide reliable scores for the linear combination methods. The binary logistic model has been investigated in the distributed information retrieval context [4,20] , in which no overlap or partial overlap exists between the document collections for component retrieval systems.
The rest of this paper is organized as follows: in Section 2 we review some related work on data fusion. Then in Section 3 we presents the experimental settings and results for evaluating a group of methods of normalizing scores and generating scores from ranking information. Section 6 is the conclusion. 2. Related work databases [7] , and many others. However, how to further improve retrieval performance is a demanding issue.
In these days, people find that the data fusion technology is very useful for performance improvement in various kinds of applications such as image [31] , multiple sensor systems [13] , databases [32] , information retrieval [11,27] , and so on.
In the following we mainly review some previous work on the linear combination method and score normalization methods for data fusion in information retrieval.

It was probably Thompson who reported the earliest work on the linear combination data fusion method in [24] . The linear combination method was used to fuse results submitted to TREC 1. It was found that the combined results, weighted by perfor-mance level, performed no better than a combination using uniform weights (CombSum). Weighted Borda count [2] is a variation of performance level weighting, in which all ranked documents are assigned scores corresponding to their respective Borda counts, and then the linear combination method with the performance level weighting is used for fusion. Bartell and Cottrell [3] used conjugate gradient, a numerical optimisation method, to find good weights of different systems.
Because the method is time-consuming, only 2 to 3 component systems and the top 15 documents returned from each system for a given query were used in their investigation.

Vogt and Cottrell [26,27] analysed the performance of the linear combination method by linear regression. Note that in their work, the linear regression technique is used for a different purpose (performance prediction) from the work reported in this paper (weights assignment). In their experiments, they used all possible pairs of 61 systems submitted to the TREC 5 ad-hoc track. Another numerical optimisation method, the golden section search, was used to search for best system weights. Due to the nature of the golden section search, only two component systems can be involved for each fusion case. their experiment shows that, using the power function of performance with a power value between 2 and 6 can lead to slightly better fusion results than performance level weighting. On the other hand, in Wu and McClean's work [30] , both system perfor-mance and dissimilarities between systems were considered. In their weighting scheme, any system's weight is a compound weight that is the product of its performance weight and its dissimilarity weight. Here the performance weight is also defined as the average performance of the system over a group of training queries as the performance level weighting scheme does, while the dissimilarity weight is defined as the average dissimilarity between the system in question and all other component systems over a group of training queries.
 quite often data fusion methods such as the averaging-score method (CombSum) and the linear combination method can bring improvements in effectiveness and accordingly what the favourable conditions are for data fusion algorithms to achieve better results.
A related problem is how to obtain reliable scores for retrieved documents. This problem is not trivial by any means. Some-times component information retrieval systems provide scores for their retrieved documents. However, those raw scores from different retrieval systems may not be comparable. Some kind of normalization is required for those raw scores. One common linear score normalization method is: for any list of scores (associated with a ranked list of documents) for a given topic or query, we map the highest score into 1, the lowest score into 0, and any other scores into a value between 0 and 1 by using the formula of raw _ s . This normalization method, referred to as 0  X  [15] and others in their experiments.

Two other linear score normalization methods, SUM and ZMUV (Zero-Mean and Unit-Variance), were investigated by Montague scores is mapped to 0 and their variance to 1. ZMUV is known as z-score in statistics.

If only a ranked list of documents is provided without any scoring information, then we cannot use the linear combination method directly. A common way of dealing with this is to assign a given score to documents at a certain rank. For example, corresponding scores based on their rank positions and CombSum or the linear combination method can be applied accordingly.
Scores can also be converted from ranking information with some training data. In MAPFuse [16] , a document at rank t is documents from each component system, the posterior probabilities of documents at each given rank are observed and averaged [23] , scores are generated using a mixture of normalized raw scores ( s system) and ranking-related relevance scores ( s p , from ranking-related posterior probabilities). For s normalization method is used to normalize raw scores. For obtaining s documents at each given rank, SegFuse divides the whole lists of documents into segments of different size ( size
In each segment, documents are assigned equal relevance scores. The final score of a document is given by s process, the aforementioned three methods just use the same method as CombSum does.
 relevant documents and a normal distribution for the set of relevant documents were used, and then a mixture model was investigated in [4,20] in the environment of distributed information retrieval, in which the merged results are retrieved from remains the same sub-problem in both situations.

Although there is quite a large amount of work done on the data fusion issue in information retrieval, how to assign suitable weights for the linear combination method is still an open question. On the one hand, some methods such as performance level hand, some numerical optimisation methods such as golden section search and conjugate gradient are able to find very effective weights if enough time is taken. However, one significant problem with these optimisation methods is they are very time-consuming. It is almost impossible to use them in practice when a relatively large number of component systems are involved or in a dynamic situation when weights need to be recalculated quite frequently. The proposed method in this paper is much methods. The time needed for this approach should be affordable for many applications. 3. The linear combination method
Suppose we have n information retrieval systems ir 1 , ir comprises a ranked list of documents with associated scores. The linear combination method uses the following formula to calculate score for every document d :
Here s i ( d , q ) is the normalized score of document d in result r score of d for q . All the documents can be ranked according to their calculated scores.

If we have no knowledge of the component systems, then an equal weight for every component system is a good policy. If we have some knowledge of those retrieval systems (e.g., evaluating performance by using some training data), then we are able to take a more profitable policy: good systems are assigned heavy weights and poor systems are assigned light weights [24,29] .On the other hand, if some systems produce more similar results than the others, then those systems should be assigned lighter we can obtain appropriate weights for all component systems in an efficient manner.

Suppose there are m queries, n information retrieval systems, and r documents in a document collection D . For each query q information retrieval systems provide relevance scores to all the documents in the collection, therefore, we have ( s for i =(1,2, ... , m ), k =(1,2, ... , r ). Here s jk i stands for the score assigned by retrieval system ir Now we want to estimate by a linear combination of scores from all component systems. We use the least squares estimates here, which for the values  X   X  0 ,  X   X  1 ,  X   X  2 , ... , and  X   X  n for which the quantity of this form or can be approximated closely by linear equations. numerical constants that can be determined from observed data.

In the least squares sense the coefficients obtained by multiple linear regression can bring us the optimum fusion results by the linear combination method, since they can be used to make the most accurate estimation of the relevance scores of all the documents to all the queries as a whole. In a sense this technique for fusion performance improvement is general since it is should work well for any reasonably defined ranking-based metrics such as average precision or recall-level precision, since more accurately estimated scores for all the documents are able for us to obtain better rankings of those documents.
In the linear combination method, those multiple linear regression coefficients, ir , not affected if a constant  X  0 is added to all the scores.

Let us take an example to illustrate how to obtain the multiple regression coefficients from some data. Suppose we have a col-score to each document in the collection. Binary relevance judgment is used. The scoring information and relevance judgment of all documents are shown as follows.

By setting ir 1 , ir 2 , and ir 3 as independent variables, and relevance as the dependent variable, we use R package to obtain the multiple linear regression coefficients. For this example, we obtain for ir 1 , ir 2 , and ir 3 , respectively.
 One related issue that also needs to be considered for using the linear combination method is how to obtain reliable scores. vance of that document to the query.

In this research, we mainly use the binary logistic regression model [4] to estimate the relation between ranking position of documents and their probabilities of being relevant, because we consider it to be a good and reliable method. However, some other score normalization methods are also investigated and some experiments are conducted to compare them. See Section 5 for more details.
The logistic model can be expressed by the following equation ranks seems to be more significant between the ranks 20 and 30 than between 990 and 1000. These last ranks contain such a small number of relevant documents that it might be appropriate to ignore the difference between 990 and 1000. Therefore, we have logistic function is still referred to as the logistic function in the rest of this paper.

Suppose we have a group of 6 results from an information retrieval system, all of which are evaluated by human judgment. result 1 ={1, 1, 0, 0, 1, 0, 0, 0 }, result 2 ={0, 1, 0, 0, 0, 1, 0, 0 }, result {1, 1, 1, 0, 0, 0, 1, 0 }, result 6 ={1, 0, 0, 1, 0, 0, 0, 0 }. Here in each result there are 8 documents. irrelevant. We can run binary logistic regression by using ln(rank) as the independent variable and score (probability of relevance) as the dependent variable. Then we can obtain the values of the coefficients a =1.512, and b = 4. Data fusion experiments summarized in Table 1 .

We only use those runs that include 1000 documents for every query. In each year group, a few runs include fewer than 1000 documents for some queries. Removing those runs with fewer documents provides us a homogeneous environment for the investigation.

In some runs, the raw scores assigned to documents are not reasonable. For example, in anu5aut1, anu5aut2, ibmgd2, and ibmge2 in TREC 5 and cirtrc82 in TREC 8, the same scores are assigned to all 1000 documents for some queries; in uwgcx0 of
TREC 5, all scores are located in each of the three narrow value intervals: (10,100 harris1 of TREC 6, all scores are in a very narrow range of (99,999 information at all. In tnout9t2lc50 of TREC 9, the first one or several top-ranked documents may have scores as high as over
The binary logistic model works like this: for all the runs submitted in each year, we divide them into two groups, odd-numbered query group and even-numbered query group. All the runs in one group are put together as one super-run. Then model. The coefficients we obtain are shown in Table 2 .

Note that in any group, for any query and any result (run), the score assigned to any document is only decided by the ranking position of the document. For example, for the odd-numbered query group in TREC 7, we obtain a =1.209 and b = follows from Eq. (4) that
Therefore, probability (1)=0.7717, probability (2)=0.6655, all 50 queries in each year group, we divided them into two groups again: odd-numbered queries and even-numbered queries.
First we used odd-numbered queries for training and even-numbered queries for testing data fusion methods; then we exchanged their positions by using even-numbered queries to train system weights and odd-numbered queries to test data fusion methods.

The file for multiple linear regression is a p row by ( n +1) column table. p is the total number of documents (duplicates are ement s ij (1  X  j  X  n ) represents the score that document d given query, if document d i occurs in at least one of the component results, then d does not occur in result r j , then a score of 0 is assigned to s nent systems.

Apart from the linear combination method with the trained weights by multiple Regression (referred to as LCR), CombSum [9,10] , CombMNZ [9,10] , PosFuse [16] , MAPFuse [16] , SegFuse [23] , the linear combination method with performance level weighting (referred to as LCP [3,24] ), and the linear combination method with performance square weighting (referred to as
LCP2, [29] ) are also involved in the experiment. For LCP and LCP2, we also divide all queries into two groups: odd-numbered queries and even-numbered queries. A run's performance measured by mean average precision on odd-numbered queries ( MAP ( odd )) is used for the weighting of even-numbered group and vice versa. That is, for any combination we let Similarly, the same methodology is used in PosFuse, MAPFuse, and SegFuse.

We investigated the situation of 3  X  32 component systems. For each given number (3 precision at 5, 10, 15, 20, 30, and 100 document level, were used for performance evaluation.
 presented due to space limitation and similarity to the figures presented.

From Fig. 2 , we can see that LCR is the best in all year groups. If we consider the average of all the combinations (5 year groups*18 different numbers of component systems*200 combinations for each given number of component systems*50 queries), then LCR outperforms LCP2, LCP, SegFuse, MAPFuse, PosFuse, CombSum, CombMNZ, and the best component systems by 5.81%, 9.83%, 11.75%, 13.70%, 16.27%, 17.79%, 17.96%, and 12.35%, respectively. Two-tailed T test (see Table 3 ) shows that the difference between LCR and the others are very highly significant ( p value Considering the difficulty of beating the best component systems, the average improvement of 12.35% is a very good result.
More importantly, such an improvement is consistent across all metrics and all year groups. On average, LCP and LCP2 also out-are used for evaluation. When 3 or 4 component systems are fused, LCP2 is almost as good as LCR. However, when more than 4 component systems are fused, LCP2 is not as good as LCR and the difference between LCP2 and LCR becomes greater and greater. SegFuse is marginally better than the best component system (by 0.56%), while the other four methods MAPFuse, PosFuse, Comb-
Sum, and CombMNZ are not as good as the best component system. See below for more discussion of them when recall-level precision and precision at 10 document level are used for retrieval evaluation.

When using the metric of recall-level precision (see Fig. 3 ), the improvement rates of LCR over other methods are: 5.03% (LCP2), 8.22% (LCP), 8.75% (the best system), 10.02% (SegFuse), 10.34% (MAPFuse), 12.80% (PosFuse), 14.40% (CombSum), and 14.51% (CombMNZ). On average, LCP and LCP2 perform slightly better than the best component system by 1.80% and 4.77%, respectively. However, LCP is not as good as the best component system when 15 or more systems are fused, LCP2 is not as good as the best component system when 28 or 32 systems are fused. All other fusion methods are not as good as the best component system on average.

If considering precision at 10 document level (see Fig. 4 ), then the improvement rate of LCR over other methods are: 6.88% (LCP2), 9.76% (LCP), 5.10% (the best system), 10.38% (SegFuse), 14.85% (MAPFuse), 11.70% (PosFuse), 14.46% (CombSum), and 13.99% (CombMNZ). Apart from LCR, all other fusion methods are worse than the best component system on average, though LCP and LCP2 perform a little better than the component system when a few component systems are fused. tion of data fusion methods to the best component system and to all component systems. We calculated Pearson product correlation coefficients of performances between data fusion methods and the best component system, and coefficients between average of all component systems. This confirms that LCR is the best fusion method from a different angle. document level, etc.,) at the same time, we have a few further observations. 1. We can see that LCR consistently outperforms all other methods and the best component system. When the number of com-ponent systems increases, the difference in performance between LCR and all other data fusion methods increases accordingly. When 32 systems are fused, LCR outperforms other data fusion methods by very large margins: 3. Compared with the best component system, LCP and LCP2 perform better when a small number of component systems are
LCP and LCP2, while precision at 10 document level favours the best component system. 4. CombSum and CombMNZ perform badly compared with all the linear combination data fusion methods and the best compo-nent system. The result is understandable since no training is needed for both of them. It also demonstrates that treating all component systems equally is not a good fusion policy when there are a few poorly performing component systems. no matter which fusion method we use. Generally speaking, the more component systems we fuse, the better the fusion result we can expect. This can be seen from all those curves that are increasing with the number of component systems in all figures and tables. component systems are very poor. It can bring effectiveness improvements steadily for different groups of component systems no matter which rank-based metric is used.
 nations in each setting, we choose those that include the best component result and observe the data fusion results of those sub-groups.  X  ETHme1  X  ,  X  CLAUG  X  ,  X  CLARIT98COMB  X  , and  X  orc199man (11), 44(12), 51(13), 50(14), 60(15), 41(16), 70(20), 97(24), 104(28), and 120(32) combinations. The figures in parentheses are the total number of component results that are involved in the data fusion experiment. For TREC 6, the figures are 14, 22, of the LCR data fusion method.

From Fig. 5 , we can see that LCR steadily outperform the best component result in all four year groups. When 3 component results are fused, the improvement rates of LCR over the best component result in each year group are 5.50%, 3.04%, 4.92%, and 7.51%, respectively; when 32 component results are fused, the improvement rates are 12.07% (TREC 5), 35.20% (TREC 6),
Originally, the improvement rates for 3 component results are 6.75% (TREC 5), 6.73% (20.44%), 6.30% (TREC 7), and 5.25% (TREC 8); the improvement rates for 32 component results are 18.01% (TREC 5), 20.44% (TREC 6), 23.06% (TREC7), and 19.03% (TREC 8). It suggests that LCR is still very effective when top component results are involved for data fusion. 5. Score normalization methods
Score normalization methods definitely have impact on the performance of the data fused methods. In Section 4 , we have demonstrated that using multiple regression to obtain system weights is an effective technique for the linear combination data we investigate and evaluate some more methods of normalizing raw scores and generating scores from ranking information.
One way of evaluating those methods is: for every document involved, we compare its estimated score (obtained from a given normalization method) with the judged score. The Euclidean distance can be used for this purpose. For all the runs selected in each year group, we calculate the Euclidean distance of every resultant list for a given query. That is,
Apart from the binary logistic model, we also include the cubic model, 0 ization, 0  X  1 Borda, and 0.02  X  0.6 Borda. We shall explain them one by one. The cubic model uses the equation involved. In fact, this method can be improved for the TREC scenario since top-ranked documents are not always relevant while bottom-ranked documents are not always irrelevant. Instead of using 1 and 0 for the upper and lower limits, we use a and b as 0  X  1 linear normalization or 0.02  X  0.6 linear normalization to further normalize them. They are referred to as 0 0.02
From Table 6 , we can see that 0  X  1 Borda is the worst with a distance of between 17 and 18 in all 4 year groups. 0.02 looks better than 0  X  1 Borda with a difference of between 11 and 12 in all year groups. 0.02 0  X  in both cases are very close. However, it should be noted that the Euclidean distance is not 100% reliable for determining the goodness of any score normalization method. For all the scores involved, if we multiply all of them by a fixed number, then the fused results will not be changed since only the ranking of those documents matters. Therefore, 0 and 0  X  0.6 linear normalization should be equally good, and the difference between 0 normalization should be very small though the difference between them may look greater by the Euclidean distance. It is the same for 0  X  1 Borda and 0.02  X  0.6 Borda.

In order to have a closer look at the relation between relevance scores and ranks, we present the observed score curve and therefore not presented.

In Fig. 6 , the observed curve is the one with many twists. From Fig. 6 , we can see that 0 would be approximately the best upper and lower limits for TREC 5 data. The Euclidean distance for 0.01 other methods. 0  X  1 linear score normalization looks much better than 0 linear score normalization method, we can obtain even better results. For TREC 5 data, the distance for 0.02 the distance for 0.01  X  0.4 Linear, whose curve is just a little over the observed one, is 6.19. However, even 0.01 model. We hypothesize that it is mainly caused by the abnormal score distributions in some of the runs submitted. Finally, the logistic model and the cubic model are the two best. Both of them fit the observed curve well.
 We carried out some data fusion experiments to compare those score normalization (generating) methods. Firstly, we tested
CombSum by using different score normalization (generating) methods. As before, for each given number (3 tions were randomly selected and tested. Fig. 7 shows the results with TREC 6 data. It can be seen that Borda normalization methods (0  X  1 Borda and 0.01  X  0.4 Borda) are the worst, linear score normalizations (0 middle, and the cubic model and the logistic model are the best two. The difference between the cubic/logistic model and Borda is about 10%, and the difference between the cubic/logistic model and the linear score normalization model is about 3%. Similar differences are also observed when other metrics are used.

Secondly, we used both logistic model and cubic model for the linear combination data fusion method. For both models, the weights for all component systems are trained by using 25 queries and tested by using the other 25 queries. The methodologies both metrics MAP and P10.
 bination data fusion method. 6. Conclusions
In this paper we have presented a new fusion method that uses the multiple linear regression to obtain suitable weights for the linear combination method. The weights obtained in this way is optimum in the least squares sense of estimating relevance it is a very good data fusion method. It outperforms other major data fusion methods such as CombSum, CombMNZ, PosFuse,
MAPFuse, SegFuse, the linear combination method with performance level weighting or performance square weighting, and the best component system, by large margins. We have also investigated methods of generating scores from ranking information and raw score normalization methods, and a comparison has been made between a group of such methods. We find that the bi-nary logistic model and the cubic model are better than the 0 retrieval systems.

References
