 variable s v  X  X  V is linearly related to the hidden state h t  X  X  H with additive noise  X  by dynamics of the continuous hidden state h The switch s (aSLDS), and defines the model The standard SLDS[4] considers only switch transitions p ( s simply denotes the prior p ( s the filtered estimate p ( h Our approach to approximate p ( h posterior p ( h detailed description of the method, including pseudocode, is given in [7]. 2.1 Forward Pass (Filtering) recursion for p ( s loss of generality, we may decompose the filtered posterior a s The exact representation of p ( h proximate this with a smaller I -component mixture where p ( h find a recursion for these parameters, consider Evaluating p ( h We find p ( h and then conditioning on v Evaluating p ( s The mixture weight in (4) can be found from the decomposition The first factor in (6), p ( v given in (5). The last two factors p ( i Finally, p ( s where hi transition p ( s Closing the recursion We are now in a position to calculate (4). For each setting of t he variable s of
I  X  S Gaussians which we numerically collapse back to I Gaussians to form recursion for p ( h where all terms have been computed during the recursion for p ( h The likelihood p ( v 2.2 Backward Pass (Smoothing) forward and presented in [7]. We approximate the smoothed po sterior p ( h with mean g ( s useful starting point for a recursion is: The term p ( h The recursion therefore requires p ( h The difficulty here is that the functional form of p ( s in imate the non-Gaussian p ( h previous backward recursion. Under this assumption, the re cursion becomes p ( h t , s t | v 1: T )  X  X Evaluating h p ( h pute. First we find p ( h which itself can be found from a forward dynamics from the filt ered estimate p ( h statistics for the marginal p ( h extra information about h and cross-variance between h h h Given the statistics of (11), we may now condition on h so effectively constitutes a reversal of the dynamics, where  X  X  X  A ( s conditioning. Averaging the above reversed dynamics over p ( h  X  These equations directly mirror the standard RTS backward p ass[1]. Evaluating p ( s The main departure of EC from previous methods is in treating the term The term p ( s Here p ( s (7). In (13), p ( h Computing the average of (13) with respect to p ( h value of the averaging distribution p ( h as sampling from the Gaussian p ( h Closing the Recursion write the smoothed estimate in the form p ( h smoothed posterior p ( s 2.3 Relation to other methods approximation p ( h term in (14) by Since p ( s in the experiments.
 independence assumption in the Backward Pass, p ( h is in contradiction to (5) which states that the approximati on to p ( h on s required in EC.
 treating p ( v In [12] a variational method approximates the joint distrib ution p ( h marginal inference p ( h directly approximate the marginal.
 therefore more able to represent the posterior. the visible observations (but not any of the hidden states h and p ( s that the original sample states s posterior smoothed estimates arg max Figure 2: The number of errors in estimating p ( s numerical approximation of p ( s conclusions for experiments with an aSLDS[7]. the observed signal v rupted by various levels of Gaussian noise.
 The dynamics of the clean signal is modeled by a switching AR p rocess where s and  X  h tractable and straightforward. For the case  X  2 ( s this we define h and we set A ( s The hidden covariance matrix  X  to the innovation variance. To extract the first component of h variance is given by  X  covariance, a process called Gain Adaptation [16]. 4.1 Training &amp; Evaluation ( training procedure used with the SAR-HMM.
 ing trained SAR-HMM, i.e., the AR coefficients c transition matrix A ( s ) and the same discrete transition distribution p ( s thousand time-steps. facilitate the practical application of dynamic hybrid net works. Acknowledgements References
