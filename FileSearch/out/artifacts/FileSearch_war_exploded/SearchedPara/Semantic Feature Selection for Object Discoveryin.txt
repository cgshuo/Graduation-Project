 With the advances of remote sensing technology and the increases of the public interest, the remote-sensing imagery has been drawing the attention of peo-ple beyond the traditional scientific us er community. Large collections of High-Resolution Remote-Sensing (HRRS) images are becoming available to the public, from satellite images to aerial photos. However, it remains a challenging task to identify objects in HRRS images. While HRRS images share some common fea-tures with traditional images, they possess some special characteristics which make the object discovery more complex and motivate our research work. Motivating Examples. Users are interested in different types of objects on Earth as well as groups of objects with various spatial relationships. For ex-ample, consider Emergency Response O fficers who are trying to find shelters to accommodate a large number of people. How ever, shelters are not distinguishable in Remote Sensing (RS) images. Instead, the officers could search for baseball fields, because most probably, a basebal l field is connected to a school and the school could be used as a temporary shelter in emergency. In addition, qualified shelter should not be far away from wat er source. Therefore, the query might be  X  select all the baseball fields in Newark within 1 mile from any water body  X . Another interesting application domain would be urban planning. With HRRS image retrieval, we may have the task to find out  X  the disinvestment area in Hudson county industrial area  X . This task indicates that we need to identify the industrial areas with a lot of empty lots. While traditional Content Based Image Retrieval (CBIR) techniques discover ob jects such as buildings and water bod-ies, these two examples demonstrate that one need to discover semantic objects such as schools and urban areas from RS or HRRS images.

Based on the above observation, we categorize the target objects that can be recognized in RS or HRRS images into thr ee concept levels: (1) Basic Terrain Types; (2) Individual Objects; and (3) Composite Objects. The first concept level is to distinguish the basic terrain type of the area covered by the images. There are several basic ground layouts: bare land, mountain, water, residential area, forests, etc. The second type of objects are individual objects that are recognizable in images, such as individual buildings, road segments, road in-tersections, cars, etc. Objects in the th ird concept level are composite objects. Composite objects are objects that consist of several individual objects that form a new semantics concept. For example, parks, airports, and baseball fields are all composite objects. In the motivating exa mples, both shelter and disinvestment area are composite objects. As one can notice, the spatial relationships among objects play a critical role in identifying composite objects and interpreting the semantics of HRRS images.

Despite the vast amount of expert effort, it is well known that the performance of CBIR is limited by the gap between low-level features and high-level semantic concepts. Recently, researchers proposed sev eral statistical models [6,1,12,2,3,9] for analyzing the statistical relation s between visual features and keywords. These methods can discover some hidde n semantics of images. However, these methods annotate scenery images accord ing to the individual objects X  presence in each image. Spatial relations among objects are not taken into considera-tion. Those spatial relationships are critical and cannot be ignored in HRRS images. Hence, in HRRS images, users pay more attention on composite ob-jects than on individual objects. This suggests that we have to examine the spatial relationships among objects when we try to identify objects in HRRS images.

In this paper, we investigate the problem of automatically annotating images using relevance-based statistical model on HRRS images. Specifically, we exploit a hyperclique pattern discovery metho d [13] to create new semantic features and feed them into the relevance-based sta tistical learning model. Hyperclique patterns have the ability to capture a strong connection between the overall sim-ilarity of a set of objects and can be naturally extended to identify co-existing objects in HRRS images. Traditionally, by using a training set of annotated im-ages, the relevance-model can learn the joint distribution of the blobs and words. Here, the blobs are image segments acquired directly from image segmentation procedure. Our approach extends the meaning of blobs by identifying the co-existing objects/segments as new blobs. The proposed approach has been tested using the USGIS high-resolution orthology aerial images. Our experimental re-sults show that, with new semantic features as starting points, the performance of learning model can be improved acco rding to several external criteria. In this section, we describe some domain challenges for object discovery in HRRS images as follows.  X  First, it is nontrivial to perform feature selection for image retrieval in HRRS  X  Also, HRRS images usually lack salient regions and carry a lot of noise [4].  X  Finally, another challenge faced by the HRRS image annotation is the impor-In this section, we introduce a method for O bject dis C overy with semanti C feat U re s E lection (OCCUE). Figure 1 shows an overview of the OCCUE method. A detailed discussion of each step of OCCUE is given in the following subsections. 3.1 Image Segmentation Image segmentation divides an image into separated regions. In a large-scale HRRS image database, the images naturally belong to different semantic clus-ters. For example, most of HRRS images can be categorized into four main semantic clusters at the land cover level including grass, water, residence and agriculture [10]. These land-cover level semantic clusters can also be divided into semantic subclusters at an object level. For these subclusters, the distinguishing primitive features are different. Moreo ver, the objects in each land-cover clus-ter are very different. For example, the objects in urban areas are usually road segments, single house roofs, or small vegetated areas. In contrast, woods and grass are dominant in suburban areas. Likewise, different composite objects also appear in different land-cover clusters. For instance, a park is always a large contiguous vegetated area. This different scale distinguishes parks from gardens. In OCCUE, we exploit a two-step approach to increase segmentation reliability. Our two-step segmentation approach satisfies the uniqueness of RS images by segmenting images at the land-cover level first and then dividing images further into individual objects or components of an individual object.

Another major advantage of using two-step image segmentation approach is that this segmentation approach can reflect the hierarchies that exist in the structure of the real-world objects which we are detecting. By abstracting houses, buildings, roads and other objects, people can identify residential areas and the aggregation of several residential areas yields a town. This hierarchy is obviously determined by scale.

In OCCUE, we apply the texture-based algorithms proposed by [4] to segment image at the land cover level. This segmenting method consists of three major steps: (i) hierarchical splitting that recursively splits the original image into children blocks by comparing texture features of blocks, (ii) optimizing, which adjusts the splitting result, if the results of the reduced resolution images have dramatically reduced segments, (iii) merging, in which the adjacent regions with similar texture are merged until a stopping criterion is satisfied.

After the land-cover level segmentation, images are segmented into small re-gions using eCognition along with different input parameters according to land-cover type [5]. Each segment is represente d by the traditional f eatures, e.g. col-ors, textures and shapes, as well as the geometric features. eCognition utilizes a bottom up-region-merging technique starting with one-pixel. In subsequent steps, smaller image segments are merged into bigger ones [5]. We believe that this is one of the easy-to-use and reliable segmentation tools for HRRS images, given the characteristics of the HRRS images: 1)with salt and pepper noises; 2) affected by the atmosphere and the reflective conditions.

The following extracted features represent major visual properties of each image segment.  X  Layer Values are features concerning the pixel channel values of an im- X  Shape Features include area (measured by pixel), length/width ratio which  X  Texture Features evaluate the texture of an image segment based on the  X  Position Features refer to the positions of segments within an image. 3.2 Fuzzy Classification After we segment the images into relatively homogeneous regions, the next step is to group similar image segments into a reasonable number of classes, referred as blob tokens in [12]. Segments in each class are similar even though they are not spatially connected. In the literature [12], unsupervised classification algorithms is employed using the primitive features or weighted features. Using the weighted features would successfully reduce the dimensionality compared with using all primitive features as clustering algorithm input. However, we used supervised classification method that is efficient in grouping image segments into semantic meaningful blobs.

Specifically, fuzzy logic based supervis ed classification is applied to generate blobs. Starting with an empty class hierar chy, we manually insert sample classes and using the features description as definition of a certain class. While nearest neighbor and membership functions are used to translate feature values of arbi-trary range into a value between 0 (no membership) and 1 (full membership), logical operators summarize these return values under an overall class evaluation value between 0 and 1. The advantage s of fuzzy classification are [5]  X  Translating feature values into fuzzy va lues standardizes features and allows  X  It enables the formulation of complex feature descriptions by means of logical
Finally, fuzzy classification also helps to merge the neighboring segments that belong to the same class and get a new semantic meaningful image blob which truly represents the feature and not just a part of it. 3.3 Hyperclique Patterns In this paper, hyperclique patterns [13,14] are what we used for capturing co-existence of spatial objects. The concept of hyperclique patterns is based on frequent itemsets. In this subsection, we first briefly review the concepts on frequent itemsets, then describe t he concept of hyperclique patterns.
Let I = { i 1 ,i 2 , ..., i m } be a set of items. Each transaction T in database D is a subset of I .Wecall X  X  I an itemset. The support of Xsupp ( X ) is the fraction of transactions containing X .If supp ( X ) is no less than a user-specified minimum support, X is called a frequent itemset. The confidence of association rule X 1  X  X 2 is defined as conf ( X 1 likelihood that the presence of a subset X 1  X  X implies the presence of the other items X 2 = X  X  X 1 .

If the minimum support threshold is low, we may extract too many spuri-ous patterns involving items with substantially different support levels, such as (caviar, milk). If the minimum support threshold is high, we may miss many interesting patterns occurring at low levels of support, such as (caviar, vodka). To measure the overall affinity among items within an itemset, the h-confidence was proposed in [13]. Formally, the h-confidence of an itemset P = { i 1 ,i 2 , ...i m } is defined as hconf ( P )= min k { conf ( i k  X  P  X  i k ) } . Given a set of items I and a minimum h-confidence threshold h c ,anitemset P  X  I is a hyperclique pattern if and only if hconf ( P )  X  h c . A hyperclique pattern P can be interpreted as that the presence of any item i  X  P in a transaction implies the presence of all other items P  X  X  i } in the same transaction with probability at least h c . This suggests that h-confidence is useful for capturing patterns containing items which are strongly related with each other. A hyperclique pattern is a maximal hyperclique pattern if no superset of this pattern is a hyperclique pattern. 3.4 Converting Spatial Relationship into Feature Representation Approaches for modelling spatial relationships can be grouped into three cate-gories: graph-based approaches, rule based approaches, and mathematical logic using 2D strings as the projections of the spatial relationships. However, none of this can be used as input for statistical Cross Relevance Model (CRM). In addi-tion, we concentrate on the presence of the objects in the image rather than the complex geometric or topological spatial relationships. For example, consider a golf course, we are interested in the appearance of the well textured grass-land, sand, non-rectangle water-body in a relatively small region. Whether the sand is left or right to the water-body is not important. In OCCUE, we apply hyperclique pattern discovery algorithm [13] to detect co-existing objects. Example 2.1. After segmentation, images are represented by the blob ID as shown in Table 1, let us consider a pattern X= { b 3 ,b 7 ,b 24 } , which implies that blob (#3 roof type II, #7 shade type II , #24 grass type IV) usually appears together. We have supp ( b 3 ) = 82% , supp ( b 7 ) = 91% , supp ( b 24 ) = 73% ,and b  X  b nition of hyperclique pattern , pattern { b 3 ,b 7 ,b 24 } is a hyperclique pattern at the threshold 0.6. Therefore, we treat the set of these three blobs as a new seman-tic feature. We treated these newly discovered hyperclique pattern as new blobs in additional to the existing blobs. Meanwhile, the original blobs #3, #7, and #24 are deleted from the original table. Table 1 will be converted to Table2. The new blobs are represented using 3 digits number in order to distinguish from the orig-inal blobs. We convert the spatial relationship into a measurable representation, so that we can apply statistical model in the next step . 3.5 A Model of Image Annotation Suppose we are given an un-annotated image in image collection I X  X  .Wehave the object representation of that image I = { o 1 ...o m } , and want to automati-cally select a set of words { w 1 ...w n } that reflect the content of the image. The general approach is widely accepted by statistical modelling approach. Assume that for each image I there exists some underlying probability distri-bution P(  X | I). We refer to this distribution as the relevance model of I [8,7]. The relevance model can be thought of as an urn that contains all possible objects that could appear in image I as well as all words that could appear in the an-notation of I . We assume that the observed image representation { o 1 ...o m } is the result of m random samples from P(  X | I).

In order to annotate an image with the top relevance words, we need to know the probability of observing any given word w when sampling from P(  X | I). Therefore, we need to estimate the probability P(w | I) for every word w in the vocabulary. Given that P(  X | I) itself is unknown, the probability of drawing the word w can be approximated by training set T of annotated images.

Assuming that observing w and blobs are mutually independent for any given image, and identically distributed according to the underlying distribu-tion P(  X | J). This assumption guarantees we can rewrite equation (2) as follows:
We assume the prior probability P ( J ) follows uniform over all images in train-ing set mathcalT . We follow [6] and use smoothed maximum likelihood estimates for the probabilities in equation (3). The estimations of the probabilities of blob and word given image J are obtained by: Here, Num ( w, J )and Num ( o, J ) represents the actual number of times the word w or blob o occurs in the annotation of image J . Num ( w, T )and Num ( o, T ) is the total number of times w or o occurs in all annotation in the training set T . | J | denotes for the aggregate count of all words and blobs appearing in image J, and | T | denotes the total size of the training set. The smoothing parameter  X 
J and  X  J determine the interpolation degree between the maximum likelihood estimates and the background probabilities. Due to the different occurrence pat-terns between words (Zipfian distribution) and blobs (uniform distribution) in images, we separate the two smoothing parameter as  X  J and  X  J .

Finally, Equation (1) -(5) provide the mechanism for approximating the prob-ability distribution P ( w | I ) for an underlying image I. We annotate images by first estimating the probability distribution P ( w | I ) and then select the highest ranking n words for the image. In this section, we present experiments on real-world data sets to evaluate the performance of object discovery with seman tic feature selection. Specifically, we show: (1) an example set of identified semantic spatial features, (2) a performance comparison between the OCCUE model and a state-of-the-art Cross-media Rel-evance Model (CRM) model [6]. 4.1 The Experimental Setup Experimental Data Sets. Since our focus in this paper is on HRRS images rather than regular scenery images, we will not adopt the popular image dataset Corel, which is considered as a benchmark for evaluating the performance of image retrieval algorithms. Instead, we use the high resolution orthoimagery of the major metropolitan areas. This data set is distributed by United States Geological Survey (USGS -http://www.usgs.gov/). The imagery is available as Universal Transverse M ercator (UTM) pr ojection and referenced to North American Datum of 1983. For example, the New Jersey orthoimagery is available as New Jersey State Plane NAD83. The file format is Georeferenced Tagged Image File Format(GeoTIFF).
 Data Preprocessing. We downloaded the images of 1-foot resolution in the New York metro area and Springfield MA. Each raw image is about 80MB, which is then be processed using the Remote Sensing Exploitation Platform (ENVI -http://www.ittvis.com/envi/). Images with blurred scene or with no major interesting objects, such as sq uare miles of woods, are discarded. For images that contain objects we are interested in, we grid the image into small pieces (2048  X  2048 pixels). Finally, we have 800 images in our experimental data set and there are 32 features: 10 color features, 10 shape features and 12 texture features. Keywords. The keywords used to annotate the semantics of the HRRS images are also different from the traditional scenery images. First of all, they are not attainable directly from the data seta as those of Corel images. Rather, it is manually assigned by domain experts. Th ese keywords can be divided into three groups: keywords regard landcover, individual objects, and composite objects. Validation. In our experiments, we divided the data set into 10 subsets with equal number of images. We performed 10-cross validation. For each experiment, 8 randomly selected sub-dataset are used as training set, a validation set of 80 images and a test set of 80 images. The validation set is used to select the model parameters. Every images in the da ta set is segmented into comparatively uniform regions. The number of segments in each image, and the size of each segment (measured by the number of pixels) are empirically selected using the training and validating sets.
 Blobs. A fuzzy classification algorithm is applied to generate image blobs. In our experiment, we generated 30 image blobs. Table 3 shows some examples of image blobs. Also, Figure 2 shows a sample image and its blob representation. Spatial Semantic Features. All images with identified image blobs are used to identify the co-occurrence of image blobs. Specifically, we exploited a hyperclique pattern discovery method to find complex objects that consist of co-existing image blobs, which usually form a unique high-level semantic concept and are treated as spatial semantic features. For instance, Table 4 shows some example semantic features. 4.2 Results of Composite Object Annotation To evaluate the annotation performance, we apply some external metrics includ-ing Precision, Recall, and F -measure. Specifically, we judge the relevance of the retrieved images by looking at the manual annotations of the images. A Recall measure is defined as the number of the correctly retrieved images divided by the number of relevant images in the test data set. The P recision measure is defined as the number of correctly retrieved images divided by the number of retrieved images. In order to make a ba lance between the recall and precision Parameter Selection. The hyperclique pattern discovery algorithm has two parameters: support and h-confidence. We examine the impact of these two parameters on the performance of object annotation. The minimum support and the h-confidence thresholds would affect object discovery. For example, the set of blobs (1, 2, 5, 9, 10) can be identified as co-existing objects with minimum support 0.05 and h-confidence 0.4, while it could not be identified when we change the minimum support to 0.15. Figure 3 shows the F-measure values with the change of minimum support and h-confidence thresholds. As can be seen, the F-measure values vary at different support and h-confidence thresholds. However, we can observe a general trend is that the F-measure values increase with the increase of H-confidence. Also, the maximum F-measure value is achieved when the support threshold is relatively high. This is reasonable, since a relatively high support threshold can guarantee statistical significance and provide a better coverage of objects. For this reason, in our experiments, we set relatively high support and h-confidence thresholds. AModelComparison. We compared the annotation performance of the two models, the CRM model and the OCCUE model. We annotate each test image with 1 word from the land-cover level, 3 words from the composite object level. Table 5 shows the comparison results. In the table, we can observe that, for both land-cover level and composite-obj ect level, the performance of OCCUE is much better than that of CRM in terms of P recision, Recall, an dF-measure.For instance, for the composite-object leve l, the F-measure value is improved from 0.2274 (CRM) to 0.4119 (OCCUE). This improvement is quite significant. In this paper, we proposed a semantic fea ture selection method for improving the performance of object discovery in H igh-Resolution Remote-Sensing (HRRS) images. Specifically, we exploited a hyperclique pattern discovery technique to capture groups of co-existing individual objects, which usually form high-level semantic concepts. We treated these g roups of co-existing objects as new se-mantic features and feed them into the l earning model. As demonstrated by our experimental results, with new semantic feature sets, the learning performance can be significantly improved.

There are several potential directions for future research. First, we propose to adapt Spatial Auto-Regression (SAR) m odel [11] for object discovery in HRRS images. The SAR model has the ability in measuring spatial dependency, and thus is expected to have a better predi ction accuracy for s patial data. Second, we plan to organize the identified semanti c features as a concept hierarchy for the better understanding of new discovered high-level objects.

