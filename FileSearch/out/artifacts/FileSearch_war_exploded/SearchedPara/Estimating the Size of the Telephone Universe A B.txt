 Mark-recapture models have for many years been used to es-timate the unknown sizes of animal and bird populations. In this article we adapt a finite mixture mark-recapture model in order to estimate the number of active telephone lines in the USA. The idea is to use the calling patterns of lines that are observed on the long distance network to estimate the number of lines that do not appear on the network. We present a Bayesian approach and use Markov chain Monte Carlo methods to obtain inference from the posterior dis-tributions of the model parameters. At the state level, our results are in fairly good agreement with recent published reports on line counts. For lines that are easily classified as business or residence, the estimates have low variance. When the classification is unknown, the variability increases considerably. Results are insensitive to changes in the prior distributions. We discuss the significant computational and data mining challenges caused by the scale of the data, ap-proximately 350 million call-detail records per day observed over a number of weeks.
 G.3 [ Probability and Statistics ]: Probabilistic algorithms (including Monte Carlo), statistical computing Algorithms Bayesian inference; massive datasets; population size esti-mation; telecommunications
On a typical weekday, the AT&amp;T network carries approx-imately 350 million telephone calls. This traffic includes Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00. standard long distance calls within the US, international calls, calls to toll-free numbers, collect calls, and calls made using pre-paid cards. Both the originating and dialed num-bers may belong to wireless telephones. A fundamental question is the following: how much of the overall total does this traffic represent? More specifically, how many active telephone wirelines (i.e. fixed landlines as opposed to wire-less) do not appear on the AT&amp;T network in a given day, week, or some other fixed period of time? An answer to this question is of significant interest because it can be used to determine wireline share in a current market, or potential line share in a new market.
 There are a number of simple approaches to the problem. Perhaps the simplest of these is to look at existing published reports of wireline counts. The Federal Communications Commission (FCC), publishes reports on many aspects of the telecommunications industry. The reports on statistical trends in telephony [2] cover wireline and wireless counts, payphone data, local and long distance revenues, minutes of use, call durations, and so on. These reports rely, for the most part, on data supplied by the various companies in the industry. For a number of reasons, these data may be incom-plete. As an example, small carriers with less than 10,000 lines in a state are not required to file their line counts with the FCC (page 7-1 of [2]). Additionally, it takes a consider-able amount of time to gather and process all the data for these reports, so the reports will typically not contain any information pertaining to the most recent months.
The motivation behind the work in this article is to esti-mate the whole universe based on a partial, current, view of that universe. Expressed as a question: is it possible to infer information about the whole space using only one view (our own) of that space, without relying on any external data?
Mark-recapture models have a long history in the statis-tics literature, and particularly in biometry, where they have been used successfully for estimation of animal population sizes and survival rates. A number of comprehensive reviews of work in this area [19, 17, 1] are available. For estimating a fixed population size, the idea is to sample the population a number of times and record the degree to which the same in-dividuals are observed. This information is used to infer the number of individuals that were unseen, and hence provides an estimate of the total population size. Mark-recapture has also been used in applications outside of biometry. For ex-ample a recapture model, different to the model described here, was recently used to estimate the size of the World Wide Web [5].
Section 2 is a brief outline of mark-recapture models for population size, and it describes how the technique can be applied to the telephone line problem. Section 3 is a de-scription of the call detail data used in the analysis, with particular emphasis on the software tools required for ma-nipulating massive datasets of this type. Section 4 discusses the model fitting process and Section 5 presents the results. Section 6 describes possible extensions and ideas for further work.
Suppose that a population consists of N (unknown) in-dividuals. A sample of n 1 individuals is captured, marked and released. Some time later a second sample of size n 2 caught, of which m have been marked from the first sample. An intuitive estimate of N can be obtained by assuming that the ratio m/n 2 should equal n 1 /N , and the resulting estimator is: This is the simplest example of a recapture estimate of a population size, and is known as the Lincoln-Petersen es-timator [12, 15]. It was used in the early 20th century for wildlife populations, although a recapture ratio was also used earlier by Laplace to estimate the population of France. If n 1 and n 2 are regarded as fixed, then m has a hypergeo-metric distribution and it can be shown that (1) is a maxi-mum likelihood estimate (MLE) of N . The Lincoln-Petersen model can be extended to allow for more than two sampling occasions, as described for example in [19], and multiple re-captures typically lead to greater precision in the estimate of N .

Two important assumptions of this model are: (1) N is closed (no additions or removals) over the period in which samples are taken, and (2) all individuals are equally likely to be captured in each sample. Estimates are fairly robust to mild violations of the first assumption, but variance in cap-ture probability among individuals (capture heterogeneity) can be a serious problem because it causes negative bias in the estimates of N . Various solutions to heterogeneity have been proposed [14, 16].

Maximum likelihood methods have typically been used to fit most of these models and to obtain parameter estimates, but advances in computing power within the last decade have brought a number of Bayesian methods to the fore [13, 10, 18].
We cast the telephony application in the recapture frame-work as follows: every telephone line (represented by a 10 digit telephone number) corresponds to an individual that may or may not be observed at each sampling occasion. For this exercise a sampling occasion will be a week of usage on the network. 1 Thus the first week in which a number is observed is the initial  X  X apture and marking X , and any subsequent weeks in which the number has usage will cor-respond to the  X  X ecaptures X . As long as the time period between the first and last weeks of the experiment is not too long, it is reasonable to assume that the population is
Day-to-day variability can be quite large, particularly at weekends, so we prefer to use a full week rather than a single day as our sampling unit. closed. However, the assumption of equal catchability in this application is violated. For example, it is clear that a busy business line has a greater probability of generating network usage in a given week than a seldom-used residen-tial line. In order to deal with this heterogeneity we employ a finite mixture model, developed by Pledger [16], with the following properties: 1. Each individual belongs to one of A groups with prob-2. Individuals in group a are captured with probability Instead of assuming that every line is equally likely to gener-ate usage in one week, we let each line belong to a group in which the probability of usage is the same for each member, but the probability varies among the groups. The value of A needs to be selected, and this will be discussed later.
If we have k sampling occasions (weeks in this case), then the probability of a line being observed j times ( j  X  k ) is Let f j be the number of lines that are observed j times ( j = 1 , . . . , k ). The overall likelihood (omitting the constant) for this model can then be written as
L ( N,  X  ,  X  )  X  N ! where r is the total number of distinct lines observed over the k samples, and so f 0 = N  X  r . The model has 2 A independent parameters and k minimal sufficient statistics f , . . . , f k , so we require 2 A  X  k in order to fit the model.
Pledger [16] maximized the log-likelihood using the EM algorithm. In this article, we employ a Bayesian approach using standard Markov chain Monte Carlo (MCMC) meth-ods to obtain samples from the joint posterior distribution of all the model parameters.
Unlike many population estimation problems, where data are often scarce, the volume of data in this application is very large. Collection and management of the call detail is itself a computational challenge. Fortunately a number of tools have been constructed specifically for efficient manip-ulation and analysis of these data.

The Hancock programming language [3] was developed for computing  X  X ignature X  information from large streams of transactional data. Hancock is C-based, and the code is highly optimized for trawling through the vast volumes of traffic on the long distance network. For this study, a small Hancock program was custom-written to return a list of the distinct telephone numbers observed on the network in each of the weeks of the study. Excluding toll-free and international numbers, the network currently observes ap-proximately 180 million distinct telephone numbers (both originating and dialed) each week. Storage of these numbers in flat ASCII files is time consuming and highly inefficient in terms of I/O, so the Sfio compression tool [11, 6] was used to store these weekly lists. Use of Sfio plus the standard Unix tool gzip allows an ASCII file of 180 million numbers to be reduced in size by a factor of 26. gzip alone compresses the same file by a factor of 4.5.

For this study we chose to use k = 5 weeks, specifically 05/03/2003 through 06/06/2003, since it was felt that 5 weeks was short enough not to violate the assumption of a fixed N , but was also long enough to yield a viable sam-ple of data. Once the lists of distinct telephone numbers for each week were extracted, the next task was to remove all numbers that belong to cellular, or wireless, telephones. Since the object was to estimate the number of active wire-lines in the US, wireless traffic needed to be excluded. The growth in cellular traffic over the last couple of years has been dramatic, so it could not be ignored. The Local Ex-change Routing Guide (LERG), published monthly by Tel-cordia Corporation, lists the owners of line ranges, so this was used to identify and exclude all the numbers that be-longed to wireless companies. 2
After the wireless numbers were removed, we were left with approximately 130 million distinct wirelines each week. These lines were first divided up by state, based on area code, and then further divided into 3 categories within each state: residential, business, or unknown. Classification of lines into these categories is based on a number of criteria, including the time of day when most usage occurs. Clearly a business line is more likely to peak during office hours. Lines for which the criteria are highly indecisive are placed in the unknown category. This classification is, of course, not 100% accurate, but it serves as a useful way to split the data a priori .

Once the lines were divided up by state and category, the next task was to determine the capture history of each telephone line. For each observed line i , there exists a k -vector  X  i of zeroes and ones which represents the recapture history of that line over the k weeks of the study. So for k = 5, the vector indicates that telephone line i was not observed (i.e. had no usage on the network) in week 1, was first observed in week 2, was seen again in week 3, went unseen in week 4, and was observed again in the final week of the study. Each line can possess exactly one of 2 k  X  1 possible vectors, since the all-zero case is unobserved.

Calculation of the  X  i was computationally intense because it required simultaneous scanning of all k = 5 lists, each of which contained approximately 130 million telephone num-bers. Another software tool, VennPivot , was useful for this purpose. VennPivot was written to construct Venn diagrams for lists of telephone numbers stored in the Sfio format. For example, given two lists of numbers, it determines which numbers are common to both and which appear in only one or the other. Therefore, given 5 lists corresponding to the 5 weeks of the study, VennPivot returned the counts for each possible capture history vector  X  . In other words, for each of the 2 5  X  1 = 31 possible capture histories, a count of the number of telephone lines with that specific history was obtained. Once these counts were known, it was trivial
In November 2003, the FCC introduced wireless portabil-ity, whereby a wireless number can be  X  X orted X  to a landline telephone, and vice versa. This complicates the identifica-tion of wireless numbers, but is not an issue here because the period under study was prior to November 2003. to calculate the f j , the number of lines observed exactly j times ( j = 1 , . . . , 5). The f j , in turn, are the data points which enter the likelihood (2).
Once the sufficient statistics f j were extracted from the data, we were ready to proceed with the Bayesian analysis. Note that the choice of k = 5 weeks restricted us to set A = 2, a simple dichotomy of the lines into two usage groups. To have A &gt; 2, we would need more than 5 weeks of data, or else some highly informative prior information. This restriction was mitigated, however, by our subdivision of the lines into the residential, business and unknown categories. We fit a model with A = 2 to each of these three groups, so in actuality we allowed for six different homogeneous capture probability groups. We did this separately for each state. As a result, we obtained an estimate of N for every state and category combination; these were then summed over category to determine the line estimate at the state level, and, in turn, summed over state to calculate the nationwide estimate.

We chose to place Uniform[0,1] (flat) prior distributions on all the capture (  X  i ) and group membership (  X  i ) probabilities. For N , we chose the uninformative Jeffrey X  X  prior [9] which was also used, for example, in [7]. This choice means that the posterior full conditional distribution of N , p ( N |  X  ,  X  , r ), is a standard Negative Binomial distribution with N &gt; = r , and we therefore used a Gibbs sampling algo-rithm to generate samples from it. The posterior conditional distributions of the  X  i and  X  i do not have standard forms, so direct sampling from them is not possible. For these cases we adopted the more general Metropolis-Hastings approach, similar to that in [18], to obtain the requisite samples from the posterior distributions.

The MCMC simulations were performed using a C pro-gram custom-written for the purpose. In each case we ran the chain for 200,000 iterations with an initial  X  X urn-in X  of 10,000, storing every 20th point from the output stream. This resulted in a sample of size 10,000 from the joint pos-terior distribution of N ,  X  and  X  . We used standard diag-nostic tools for MCMC (e.g. [8]) as well as time series plots of the componentwise posterior samples to verify adequate convergence and mixing of the Markov chain. Once all the posterior samples for each state and classification pair were obtained, they were aggregated first to the state and then to the national level.
Figure 1 is a graphical representation of the raw data f j the number of lines seen j times over the 5 weeks, split by the classification of residence, business, or unknown. The total number of distinct lines, r , observed over the 5 weeks was just over 181 million.

It is clear from Figure 1 that the usage pattern of lines in the unknown group is markedly different from the other two groups. In this group, more lines were seen in only one Figure 1: Counts, f j , of wirelines observed in the 5 weeks from 05/03/2003 through 06/06/2003. The lines are split into the residence, business and un-known categories. week (out of five) than were seen in two or more weeks. For residence and business, the number of lines seen every week was more than double the number seen in any lesser number of weeks. This is not surprising since the residence/business classification is in part a function of the observed usage, so for lines with less usage, the classification is harder and these lines are more likely to end up in the unknown bucket.
Figure 2 shows the posterior distribution of N summed across all states and classification groups. It represents the distribution of the total number of active wireline telephones in the US in May 2003. The posterior mean, a point esti-mate, is 200,597,000 and 99% of the mass falls between 199 million and 203 million lines. The distribution is slightly skewed to the right. Since 181 million distinct lines were ac-tually seen over the five weeks of observation, we conclude that roughly 10% of the universe went unobserved during that time.

Table 1 shows detailed results for the state of New Jer-sey. The estimates of N are given in thousands, and in each case the number in parentheses is the number of distinct lines actually observed over the five weeks (denoted by r in equation 2). For residence, the posterior mean of N is only 33,000 lines larger than r . In other words, repeated observa-tion of the same lines in this case leads us to an estimate of total size that is only about 1% larger than the number that were observed. This explains why the standard deviation of this estimate is so small. Since r must be a lower bound for N , and r is so close to N , there is very little uncertainty in the lower quantiles of the posterior distribution for N . For business lines, a similar result is evident, although there is slightly more variability in the distribution.

In the case of the unknowns, the results are very differ-ent. Here, the point estimate of N is more than double the number of lines that were observed, and the posterior standard deviation is ten times larger than that for known Figure 2: Posterior distribution of N , the total num-ber of active wirelines in the US in May 2003. The mean of the distribution is 200,597,000, just over 200 million lines. residential lines. The inference is that a large number of lines that would be classified as unknowns were unobserved during the five weeks. This is intuitively sensible because unknowns are typically low usage lines. Although the un-knowns form a small part of the total number of lines in New Jersey, it is clear that most of the variability is associated with them.

Table 1 also illustrates how the two homogeneous capture groups vary according to the residence/business classifica-tion. For residential numbers, there is one group whose es-timated probability of usage in a given week is  X  1 = 0 . 461. The second group has a much higher probability  X  2 = 0 . 962. The latter group can be thought of as the almost  X  X veryday X  users. Approximately one quarter of the lines belong to the lower usage group (  X  1 = 0 . 246), so by implication nearly three quarters belong to the higher usage group. This helps explain why the estimate of N is only marginally larger than r . For business, the results are similar, although in this case roughly 40% of the lines are assigned to the lower usage group, which has  X  1 = 0 . 339. For the unknowns, the cap-ture group results are very different, as we would expect. Here, nearly 95% of all the lines are assigned to a group with very low probability of usage in a week (  X  1 = 0 . 089).
For the purposes of illustration, we focused on New Jer-sey in Table 1. The trends are similar in other states, al-though obviously the aggregated point estimates  X  N vary widely among the states. Table 2 shows the point estimates from the model (the posterior means) at the state level and the corresponding line counts from an FCC report [2] for the beginning of 2002. 3 . Considering that the data used in our model were from May 2003, more than one year later, the agreement is good. In most cases our point estimates
Line counts for the beginning of 2002 were the most recent data at the state level that we were able to find from this source. See Table 7.2 of the FCC report. Table 1: Results from the model for New Jersey.
 The point estimates of N (posterior means) are given in thousands, as are their standard deviations. In each case the number in parentheses is the number of distinct lines ( r ) observed over the five weeks.  X  2 is not listed since it is equal to 1  X   X  1 .
 are a few percentage points higher than the counts from the FCC report. It should also be noted that the FCC counts used here exclude lines belonging to some competitive local exchange carriers (page 7-1 of [2]).
As a result of this analysis, we have obtained point es-timates of the number of fixed wirelines in each US state in May 2003. Recall that we chose uniform prior distribu-tions for all the  X  i and  X  i . We repeated the analysis us-ing Beta(0.5,0.5) priors, which place higher a priori mass on high and low values. The results were almost identical to before. The sheer volume of data behind the likelihood appears to wash away the effects of the priors in this appli-cation.

We chose to use k = 5 weeks of data and set A = 2 homogeneous capture groups in the model. With k = 5 we cannot have A &gt; 2. As discussed earlier, however, our a priori division of lines into the residence, business and unknown categories actually provides us with six groups, two within each of the three classifications. Our choice to use A = 2 was also influenced in part by our own initial experiments and the observations of other authors [16], who found that a dichotomy of individuals was often sufficient to correct for bias induced by heterogeneity. Additional groups tended to have little effect on the estimate of N .
Use of a longer time period, say 10 weeks, would nonethe-less still be worthy of investigation. In this case a number of models with different values of A could be fit, and a model selection criterion (e.g. a likelihood ratio test) could be used to pick the best fitting model, and hence the  X  X est X  number of capture groups for the data. This is the approach used in [16]. The scale of the data in this application needs to be borne in mind; the addition of each new week creates signifi-cant computational overhead, particularly in the calculation of the  X  i , the individual capture histories. Each new week doubles the number of possible capture histories.
The mixture model we chose allows for capture hetero-geneity across individuals but not across time. We assume that a given individual has the same probability of obser-vation at each sampling occasion (ie. in each week), which is a reasonable assumption here, particularly over a small number of weeks. However, the model could be extended to allow for capture probability to vary with time, as well as across capture groups. This would, however, add extra parameters to the model and we do not view it as necessary for these data.

There is one rather subtle difference between this applica-tion and the wildlife applications. Here, the sampling occa-sions are contiguous weeks of telephone usage. The end of one sampling occasion and the beginning of the next is some-what artificial, decided simply by date. In the wildlife con-text, the population is visited, animals are caught, marked and released back into the population to mingle. The ex-perimenter leaves and returns at some later time for the next visit. In this case the sampling occasions are clearly defined as the visits by the experimenter. The use of the model, however, remains justified as long as the associated assumptions regarding capture probability at each sampling occasion (regardless of how it is defined) remain valid.
Lastly, the question of undetectable lines should be ad-dressed. Any line which never makes or receives a long dis-tance call (including toll-free calls) would have probability zero of being observed on the long distance network. Such lines have to be excluded as part of the total population size N which is estimable using the current model. Lines with a very small, but non-zero, probability of usage are still legitimately included, provided the mixture model has enough groups. For this analysis, we have assumed that the size of the true zero probability group is too small to cause any real concern. There has, however, been some recent progress [4] on ways to account for an undetectable segment of the population, and such methods may be useful in future applications.
In this article we presented a mark-recapture model, typ-ically used for wildlife population estimation, and applied it to usage data in order to estimate the number of fixed telephone lines in the US. With the aid of some fast com-putational software and efficient storage mechanisms, we were able to sort through approximately 9 billion call de-tail records, construct capture histories, and apply a Markov chain Monte Carlo Bayesian procedure to the resulting data. The results are not inconsistent with existing published re-ports. There is potential for future work in this and related areas using these methods. The author would like to thank Deepak Agarwal, Corinna Cortes, Kathleen Fisher, Anne Rogers, and Daryl Pregibon for their helpful suggestions and comments on this work. [1] S. T. Buckland, I. B. J. Goudie, and D. L. Borchers. [2] Federal Communications Commission. Report of the [3] C. Cortes, K. Fisher, D. Pregibon, A. Rogers, and [4] C. Q. Da-Silva, J. Rodrigues, J. G. Leite, and L. A. [5] A. Dobra and S. E. Fienberg. How large is the World [6] G. S. Fowler, D. G. Korn, and K. P. Vo. Extended [7] E. I. George and C. P. Robert. Capture-recapture [8] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, [9] H. S. Jeffreys. Theory of Probability . Oxford [10] R. King and S. P. Brooks. On the Bayesian analysis of [11] D. G. Korn and K. P. Vo. SFIO -a Safe/Fast [12] F. C. Lincoln. Calculating waterfowl abundance on [13] D. Madigan and J. C. York. Bayesian methods for [14] J. L. Norris and K. H. Pollock. Nonparametric MLE [15] C. G. J. Petersen. The yearly immigration of young [16] S. Pledger. Unified maximum likelihood estimates for [17] K. H. Pollock. Modeling capture, recapture, and [18] D. Poole. Bayesian estimation of survival from [19] G. A. F. Seber. The estimation of animal abundance
