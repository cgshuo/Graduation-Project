 Online recommendation communities, like Yelp, are valuable information sources for people. Yet, we assert, review com-munities have their own dynamics behind the social interac-tions therein. In this work, we study the Yelp review votes of useful , funny , and/or cool to understand these social per-ceptions of the review. We examine the relationship between these social signals and the emotional valence of the review itself (text and rating). We aim to understand the commu-nity X  X  perception of each of these signaling contributions. We construct a conditional inference tree of social signals from 230,000 Yelp reviews to study how social signals shape the deviance in review rating from the mean rating, an indicator of the overall business rating on Yelp. We find two effects of social signals. First, reviews voted as useful and funny are associated with lower user ratings and relatively negative tone in the review text. Second, reviews voted as cool tend to have a relatively positive tone and higher ratings. Our findings open a research direction for further understand-ing of perceptions of social signals and have implications for design of recommendation systems.
 H.4 [ Information Systems Applications ]: Miscellaneous; D.2.8 [ Software Engineering ]: Metrics X  complexity mea-sures, performance measures Yelp; Online Reviews; Social Signals; Votes; Social Feedback; Conditional Inference Trees; Funny, Cool, Useful Votes
Online recommendation sites are important resources that enable sharing opinions among people. These communities enable members (users) to share their experiences with prod-ucts, services and activities in the form of reviews and rat-ings, that may otherwise be difficult to find before receipt of a service or product. Consumers perceive online recommen-dation communities as unbiased sources of information as compared to business listing websites [10]. The presence of reviews for businesses has been shown to improve customer perception of the usefulness and social presence of the web-site [19].

Reviews have the potential to attract user visits, increase the time spent on the site, and create a sense of commu-nity among frequent users. However, as the availability of online reviews becomes widespread, the effectiveness of rec-ommendation communities shifts from the mere presence of customer reviews to how community members evaluate and use the reviews.

To promote engagement between the minority of content creators and the majority of content consumers, many on-line communities provide users with voting and feedback sys-tems. These systems serve an important purpose X  X hey en-able users to generate social signals in the community e.g. Facebook X  X  likes , Twitter X  X  favorites and retweets and Ama-zon X  X  helpful review votes . In Yelp, each review can be voted as useful , cool and/or funny (in any combination) which pro-vides three social signals that describe the different ways in which users perceive online reviews written by others.
Yelp indexed over one million recommendations from 100 million unique visitors in the month of January 2013 1 . Yelp members, or  X  X elpers X  add reviews as text with an integer rating score on a scale of one to five. The Yelpers can then vote on the review as one or more of useful, cool and funny. While the common perception is that these votes are prox-ies for quality of Yelp reviews, our understanding of each of these social signals is limited. In this work, we ask the question: What do these social signals for a review indicate about that review? Specifically, we ask two questions: 1. Are the social signals of a review indicative of how 2. Are the social signals of a review indicative of the po-We answer these questions by modeling the review X  X  at-tributes (numeric rating and tone of the text) as functions of the social signals which that review receives. We construct a conditional inference tree of social signals, and use review attributes as the dependent variables. We study a large-scale Yelp dataset containing 230K reviews and their social sig-nals.

In this paper, our hypothesis is that these voting signals have specific implications towards the reviews themselves which may not match the signal X  X  label. To unravel such im-plications, we take a closer look at the relationship between these signals and the review content that is evaluated. We demonstrate two surprising observations, given that social signals for a review in Yelp are based on the review text, and have no relationship with the business that is under re-view. First, we find that social signals are associated with reviews whose ratings deviate from the mean rating. Sec-ond, we find that useful and funny signals are associated with reviews having a higher negative tone or lower positive tone, while cool signals are associated with reviews having a higher positive tone.

In the remainder of paper, we summarize related work, describe the data we obtained, and explain our statistical findings. We conclude our paper by discussing the results and describing some of the implications we can draw from this work. Finally we point out some of the limitations of this work and summarize possible future directions.
This paper on one hand, investigates the dynamics of be-havior on online review site, Yelp, and on the other hand is focused on differentiating between various social signals communicated by users. In an effort to understand what has been done in this scope and what is missing, we summarize the relevant literature on perceptions of online reviews and the prior studies on understanding helpful votes.
Understanding people X  X  perceptions of web based informa-tion systems, or social media sites is a complex phenomenon, requiring a series of systematic studies. There has been a sig-nificant amount of work on how members of online commu-nities perceive content. Prior work has shown that attributes of review text, reviewer and social context can shape user response to reviews [2, 12, 13, 21, 27]. Lu et al. [21] use a latent topic approach to extract rated quality aspects (cor-responding to concepts such as price or shipping) from com-ments on eBay. Harper et al. [16] studied the helpfulness of answers on the Yahoo Answers site and the influence of variables such as the type of answer and the topic domain of the question. Siersdorfer et al. [25] looked at prediction of community ratings for comments and discussions of video content on YouTube. Another study on YouTube [28] pre-dicted the genre categorization of YouTube videos based on people X  X  interactions around media content without recourse to meta-data.

A body of work studied how members of online communi-ties use review content. For example, studies have examined the role of expert reviews [6], the role of online recommenda-tion systems [3, 5, 15], and the positive effect of social signals (and feedback mechanisms) on buyer trust [22]. Clemons et al. [7] found that positive ratings can positively influence the growth of product sales, and Chen et al. [6] found that the quality of the review as measured by helpfulness votes also positively influences sales.

A primary objective of online communities is that of user engagement. General aspects of online user engagement have been discussed in detail in prior work [20, 29]. A common way to study user engagement is through online behavior metrics that assess the extent of users X  engagement with content on the site. For instance, Peterson and Carrabis [23] describe engagement metrics that are related to consump-tion time on the site, users X  return to the site or subscription to the site X  X  feed. A study of web content by Amento et al. [1] investigated the utility of various metrics in estimating the quality of web documents. Further, Wu et al. [27] study the temporal development of product ratings and their helpful-ness, and dependencies on factors such number of reviews and the effort required (writing review versus just assigning a rating).
To encourage both creators of content and the readers to engage with the site, many online communities provide users with a feedback system. Facebook likes, twitter X  X  fa-vorites or retweets, Amazon X  X  helpful votes on reviews and Yelp X  X  useful, cool and funny votes are all examples of such systems. Prior research suggests that perceived attributes of the review text, reviewer and social context may all shape consumer response to reviews [2, 12, 13, 21, 27].

There is a body of work on analyzing product reviews and postings in forums. Lu et al. [21] use a latent topic approach to extract rated quality aspects (corresponding to concepts such as  X  X rice X  or  X  X hipping X ) from comments in ebay. The temporal development of product ratings and their helpful-ness and dependencies on factors such number of reviews or effort required (writing review vs. just assigning a rating) are studied [27]. The helpfulness of answers on the Yahoo! Answers site and the influence of variables such as required type of answer, topic domain of the question is manually analyzed [16]. Siersdorfer et al. [25] focused on community ratings for comments and discussions rather than product ratings on Youtube. Another study on Amazon reviews [8] studied the helpfulness scores on Amazon site and found that the helpfulness scores are not only dependent on the content of the review but also on other reviews posted for the product.

While there has been studies on understanding and pre-dicting helpful votes, we don X  X  know much about other useful feedback mechanisms than can encourage engagement. Yelp provides users with two new voting concepts, funny and cool. Not only it is important to know what users in general per-ceive as funny or cool , but also better understandings of how social feedback system relates to conventional text analysis such as sentiment can give us insights in building more ef-fective recommender systems.
Founded in 2004, Yelp is a large online recommendation community that is also a user-maintained business and ser-vice directory to help people find local businesses. According to their website 2 , Yelp had an average of about 102 million monthly unique visitors in 2013 and the community mem-bers have written over 39 million reviews. Members of Yelp make millions of contributions to the community a month. Many regular Yelpers, maintain profiles with pictures, their interests, favorite places, friends and location. Yelpers can friend or follow other people on site, as well as the aforemen-tioned reviewing and voting features. We acquired our data
Variable Description Distribution business stars The average number of Yelp stars for the business. review cool votes  X  The number of review X  X  cool votes. review funny votes  X  The number of review X  X  funny votes. review useful votes  X  The number of review X  X  useful votes. review polarity Polarity score of review text, in range of (-1,1). review subjectivity Subjectivity score of review text, in range of (0,1). and blue lines identify mean and median of the distribution respectively. from the Yelp Dataset Challenge 3 which consists of a sam-ple of Yelp data from the Greater Phoenix, AZ metropoli-tan area. It includes 11,537 businesses , 43,873 reviewers and 229,907 reviews . The data spans from 2005 to 2013. Table 1 summarizes the description and distributions of the numer-ical variables used in this paper.

As in any online recommendation community, Yelp mem-bers contribute reviews for businesses. A review consists of a stars rating and a review text. In order to understand the effect of review text, we process the text for each review to quantify the subjectivity (between 0 and 1) and polarity (be-tween -1 and 1) of the text using the Pattern Toolkit 4 . We use the funny , cool and useful votes counts as measures of review X  X  social feedback.

We construct variables for our model as follows. Our de-pendent variable is a binary attribute that denotes whether a review X  X  rating was larger than the overall rating for that business (across all reviews). This measures the difference between review X  X  rating and the average rating on Yelp for that business. We use the difference between these two rat-ings, as opposed to the actual review X  X  rating to control for the average rating of the business being reviewed.

We use the funny , cool and useful votes as predictors of the rating deviance (defined as the difference between the review rating and the average rating for that business). Instead of using numerical values of votes for funny , cool and useful votes, we define a binary variable for each type of signal. The binary variable is based on whether there were more than three votes on the signal. We chose three as the threshold to control for noise and weak community feedback. We checked the results with thresholds of 0, 1, 2, 4 and 5, and found the results to be similar.
In this paper, we wish to understand the relationship be-tween each social feedback signal and the review X  X  rating, controlling for the rating of the business. We first perform a group comparison between funny and not-funny reviews 5 ,
We consider a review to be funny if it has at least 4 funny votes (see Section 3). cool and not-cool reviews and useful and not-useful reviews. We use a Chi-Square (  X  2 ) test with the null hypothesis that both groups of funny reviews and not-funny voted reviews come from the same distribution. The test rejects the null hypothesis, indicating that they are not coming from the same distribution, with p -value smaller than 2 . 2  X  10 We repeated this test on cool and useful votes and both tests rejected the null hypothesis with p -value smaller than 2 . 2  X  10  X  16 . Therefore, we proceed with analyzing the inter-action between these social signals and understanding how they might impact the deviance in the rating.

We use conditional inference trees [17] to study the role of social feedback on deviation in a review X  X  rating. This method is particularly useful in the case of this study, be-cause the trees not only tell us the extent to which each variable is important, but also help us to better interpret the results. The common splitting criterion used in con-ditional inference trees is the Gini Index. The Gini Index checks for the  X  X urity X  of resulting children nodes in the tree. For a given node t with estimated class probabilities p ( j | t ), j = 1 ,...,J [4], the node impurity i ( t ) is given by: The most favorable split is the one that reduces the node or equivalently the tree impurity. The most favorable split is identified by search. Adopting the Gini diversity index, i ( t ) takes the form: The Gini index, defined as a function  X  ( p 1 ,...,p p ,...,p J , is a quadratic polynomial with nonnegative co-efficients. In conditional inference trees, the node split is selected based on how good the association is. The result-ing node should have a higher association with the observed value of the dependent variable. The conditional inference tree uses a  X  2 test to test the association. Therefore, it not only removes the bias due to categories but also chooses those variables that are informative.

The key to this algorithm is based on the separation of variable selection and the splitting procedure. The recursive binary partitioning is as follows. The response Y comes from sample space Y , which may be multivariate. The m dimen-sional covariate vector X = ( X 1 ,...,X m ) is taken from a sample space X = X 1  X  X  X  X  X  X m . Both the response vari-ables and the dependent variable may be measured at any arbitrary scale. The conditional distribution of the response variable given the covariates depends on the function of co-variates:
For a given learning sample of n iid observations a generic algorithm can be formulated using nonnegative integer of case weights w = ( w 1 ,w 2 ,...,w n ). Each node of a tree is represented by a vector of case weights having non-zero el-ements when the corresponding observations are significant for that node, and zero otherwise. The generic algorithm is: 1. For case weights w , we test the global null hypothesis 2. Set A  X  X j is chosen to split X j , into two disjoint sets. 3. We repeat the steps 1 and 2 with modified case weights The separation of variable selection and splitting procedures is essential for the development of trees with no tendency toward covariates with many possible splits [17].
Figure 1 visualizes the conditional inference tree for our dependent variable (the difference in review X  X  rating and the business X  X  rating). The terminal nodes show different distri-butions of the dependent variable (the deviance from mean rating) in each case. Also, we can see that on the right side of the tree where the number of funny votes is greater than three, there is higher density of negative deviance from the mean. This means the reviews written for the business with more funny votes are usually rated lower than the mean business rating. In other words, the funny votes are usually given to the more critical reviews that have lower ratings. The negative tendency of the ratings compared to the collec-tive business rating shows that the concept of funny is per-ceived negatively by the Yelp users. Table 2 summarizes the mean, standard deviation and standard error of sentiment polarity for reviews considered funny or not-funny . We can see that the funny reviews are  X  0 . 39 units more negative than the not-funny ones.
We find that when the funny votes are high but the cool votes are low (Figure 1), we see the most negative deviance in the ratings from the mean business X  X  rating. When the useful votes are low as well, the likelihood of having a nega-tive deviance rating is equal to 0 . 5 but when the useful votes are high, the negative deviance likelihood is as high as 0 . 6, meaning that 60% of the reviews with higher funny votes, lower cool votes and higher useful votes are rated lower than the collective business X  X  rating. We show an example of parts of a review that was voted funny and useful but did not re-ceive many cool votes: While funny votes are given to more critical reviews (lower ratings), the cool votes seem to enforce a positive percep-tion on the users. On the other hand the useful votes when accompanied by funny votes, seem to be related to lower rat-ings. We see that in the case of high funny votes and high cool votes the likelihood of a review rating being smaller than the collective business review is 0 . 3, see Figure 1.
When the funny votes are low and cool votes are high, we see the highest likelihood of positive change in ratings. In the case that useful votes are low, the likelihood of pos-itive change is the highest among all, i.e. 0 . 8. When the useful votes are high, the likelihood is slightly lower but still close to 0 . 8. This shows the positive perception of cool votes and negative perception of funny votes. An example review, where the cool votes were high and useful and funny votes were low is provided below: Table 2: Mean, Standard Deviation and Standard Error values of polarity for reviews voted with each of the social feedback signals. We can see the mean polarity for funny reviews is around 0 . 4 less than not-funny ones. The cool reviews are more positive in 0 . 25 units in their polarity and the useful votes are 0 . 17 less positive than the not-useful ones.

Table 2 shows that the difference among the sentiment polarity of cool reviews and the ones without cool votes is substantial X  X ndicating that both the ratings and polarity of the reviews are higher among the ones with cool votes. The useful votes are also slightly negatively perceived. When the funny votes are low and the cool votes and useful votes are low as well, we have the most common case happening ( n = 185214). In this case, the chance of negative change is 0 . 3 and the chance of positive deviance is 0 . 7. We can see from Figure 1 that in the case of low funny and cool votes but higher useful votes, the chances of negative change increases to 0 . 4, indicating that the perception of useful signal on Yelp is negative. The polarity of the sentiments on useful reviews is around 0 . 17 units less than the ones without useful votes, which emphasizes on the negative tone of useful reviews.
In this paper, we have shown how different social signals, used as feedback votes, demonstrates the differences of re-view ratings and their sentiments. We started with a limited understanding of what users perceive as cool , funny or useful and took a first step towards understanding the relative re-lationship of each of these social signals with respect to the changes in the review ratings. We were able to point out sig-nificant differences in review rating patterns and emotional valence of the review text across these social signals. We summarize our findings and discuss implications below.
Our results show that the funny votes are related to higher likelihood of negative changes in the ratings. This means that the population of Yelp users perceive those reviews as funny that are lower in ratings. This can be explained in two ways. First, it might be that the general audience on the review sites such as Yelp, enjoy reading the reviews with sarcastic tone or those reviews that criticize the business with some humor. The second explanation goes back to the perception of the reader and the votes. It could be that the users find the lower rated reviews and those with negative tone more funny or humorous. Either way, the fact that the higher funny votes are related to the lower rated reviews illu-minates the opinion of the general Yelp user of what is con-sidered funny. Further, it X  X  worth mentioning that negativity of funny reviews can help with crowd-generated content that may be identified as sarcasm. Previous research has inves-tigated ways to detect sarcasm through natural language models [9, 26].

The same pattern that is observable with funny votes also applies to useful votes, but in a smaller scale. Our results show that the reviews that are rated as useful are on aver-age more likely to be rated lower than those without useful votes. This finding suggests that users find the more crit-ical reviews more useful. While, useful reviews are usually perceived as high quality reviews on review sites, here our findings emphasizes its relationship with lower ratings and higher negative sentiment.

Unlike the perceptions of funny and useful on the reviews, we find that reviews that are perceived as cool are more likely to be highly rated. This suggests that the cool per-ception is usually tied with the higher rated reviews of busi-nesses. The lower rated reviews are less likely to be perceived as cool. We also find that the level of polarity in the review is higher in positivity among cool reviews, which makes the argument of positive perception of cool reviews stronger.
Online recommendation communities rely on member con-tributions to index and serve recommendations [11, 14, 18]. The findings of our research suggest that there is deeper meanings and interaction forms than the generic votes. The specific community of Yelp, for example, judges and commu-nicates meaning through its own interpretation of the signal labels, which is congruent to similar finding in the multime-dia community [24]. Future work can elaborate on the use of these labels and their corresponding social perceptions in other communities. For example, what do likes on Facebook mean in different contexts and by different people? Is it some form of social confirmation, do users support the content of the post? Do they find it cool or funny? Deeper understand-ings of forms of user feedbacks can enable us with better recommendation and social network design.

This study is based on Yelp and further work is needed to ascertain whether our findings apply to other review com-munities. For example the negative tone in funny reviews might not be appreciated in other types of communities. Further, our data was limited to certain variables and the statistical methods we used examine only a small segment of behavior on Yelp. A possible direction for future work is to look closely into the language that is used in funny, cool and useful reviews. For example, are there certain phrases or types of writing that contribute to funny votes? Another possible research direction is to complement this work with a qualitative study. One could find what users perceive as cool or funny by interviewing Yelp users.
Online recommendation communities such as Yelp enable people to find right businesses and services, and enable busi-nesses to find new customers. Hence, it is critical for such communities to maximize social interactions around reviews. Yelp, as a pioneer in online review communities, has pro-vided users with several ways to evaluate content. In this paper, we took a first step to better understand the under-lying review patterns associated with each of these social signals. We found that useful and funny signals are mostly associated with lower ratings and more negative sentiment, while cool signal is associated with higher ratings and posi-tive sentiment. [1] B. Amento, L. Terveen, and W. Hill. Does  X  X uthority X  [2] N. Archak, A. Ghose, and P. G. Ipeirotis. Show me [3] J. Y. Bakos. Reducing buyer search costs: implications [4] L. Breiman. Classification and regression trees . CRC [5] P.-Y. Chen, S.-y. Wu, and J. Yoon. The impact of [6] Y. Chen and J. Xie. Online consumer review: [7] E. K. Clemons, G. G. Gao, and L. M. Hitt. When [8] C. Danescu-Niculescu-Mizil, G. Kossinets, [9] D. Davidov, O. Tsur, and A. Rappoport.
 [10] C. Dellarocas. The digitization of word of mouth: [11] K. E. Finn, A. J. Sellen, and S. B. Wilbur.
 [12] C. Forman, A. Ghose, and B. Wiesenfeld. Examining [13] A. Ghose, P. G. Ipeirotis, and A. Sundararajan. [14] A. Girgensohn and A. Lee. Making web sites be places [15] U. Gretzel and D. R. Fesenmaier. Persuasion in [16] F. M. Harper, D. Raban, S. Rafaeli, and J. A. [17] T. Hothorn, K. Hornik, and A. Zeileis. Unbiased [18] J. Koh, Y.-G. Kim, B. Butler, and G.-W. Bock. [19] N. Kumar and I. Benbasat. Research note: the [20] J. Lehmann, M. Lalmas, E. Yom-Tov, and G. Dupret. [21] Y. Lu, C. Zhai, and N. Sundaresan. Rated aspect [22] P. A. Pavlou and A. Dimoka. The nature and role of [23] E. T. Peterson and J. Carrabis. Measuring the [24] D. A. Shamma, R. Shaw, P. L. Shafton, and Y. Liu. [25] S. Siersdorfer, S. Chelaru, W. Nejdl, and J. San Pedro. [26] O. Tsur, D. Davidov, and A. Rappoport. Icwsm-a [27] F. Wu and B. A. Huberman. How public opinion [28] J. Yew, D. A. Shamma, and E. F. Churchill. Knowing [29] E. Yom-Tov, M. Lalmas, R. Baeza-Yates, G. Dupret,
