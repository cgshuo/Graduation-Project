 Abstract The paper describes a corpus of texts produced by non-native speakers of Czech. We discuss its annotation scheme, consisting of three interlinked tiers, designed to handle a wide range of error types present in the input. Each tier corrects different types of errors; links between the tiers allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified. The annotation scheme is tested on a data set including approx. 175,000 words with fair inter-annotator agreement results. We also explore the possibility of applying automated linguistic annotation tools (taggers, spell checkers and grammar checkers) to the learner text to support or even substitute manual annotation. Keywords Learner corpus Error annotation Second language acquisition Czech 1 Introduction Learner corpora, i.e. electronic collections of texts produced by non-native speakers, are a rich source of information about specific features of learners X  language. They can be annotated in the usual ways, common in other types of corpora, i.e. by metadata, morphosyntactic categories and syntactic structure, but their most interesting aspect is examples of deviant use, which can be identified, corrected and classified. Annotation of this kind is a challenging task, even more so for a language such as Czech, with its rich inflection, derivation, agreement, and a largely information-structure-driven constituent order.

Following an overview of some learner corpora (Sect. 2 ) we present a learner corpus of Czech, consisting of approx. 2 million running words, compiled from texts written by students of Czech as a second or foreign language at all levels of proficiency (Sect. 3 ). We discuss the corpus annotation scheme, consisting of three interlinked tiers, designed to cope with a wide range of error types present in the input (Sect. 4 ). Tier 0 represents the transcribed input, Tier 1 corrects non-words and Tier 2 the remaining error types; links between the tiers allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified according to a taxonomy. Annotation of this kind is supplemented by a formal classification, e.g. an error in morphology can also be specified as being manifested by a missing diacritic or a wrong consonant change.

The annotation scheme was tested in two rounds, each time on a doubly-annotated sample X  X irst on a pilot annotation of approx. 10,000 words and later on a large data set including approx. 175,234 words, both with fair inter-annotator agreement results, calculated as several inter-annotator agreement measures (Sect. 5 ).

To assist the annotator and to supply additional information about deviations from the standard, we aim at a synergy of manual and automatic annotation, deriving information from the original input and from the manual annotation (Sect. 6 ). Some methods interact with the annotator (e.g. a spell checker within the annotation editor marks potentially incorrect forms), or use results of manual annotation, including an automatic check for consistency and compliance with the annotation guidelines. After approval by the annotator X  X  supervisor, some error tags are specified in more detail and more tags are added automatically.

To assist the annotator even further, we also experiment with fully automatic methods (Sect. 7.1 ). We perform automatic emendation by a mildly context-sensitive spell checker and plan to use a grammar checker or a stochastic model to identify error types. A tagger trained on native speakers X  language is used at the tier of corrected text to supply morphosyntactic categories, but we explore options of applying general-purpose tools to the original text (see, e.g. Van Rooy and Scha  X  fer 2003 ; Dickinson 2010 ). 2 Learner corpora A learner corpus, also called interlanguage or L2 corpus, is a computerised textual database of language as produced by foreign/second language (L2) learners (Leech 1998 , p. xiv). It is a very useful resource in the research of second language acquisition (SLA) and foreign language teaching (FLT). It serves as a repository of authentic data about a specific variety of natural language (Granger 2003b ), namely the learner language, or interlanguage (IL). 1
Learner corpora can be used to compare non-native and native speakers X  language, or interlanguage varieties. They can be studied on the background of traditional native language corpora, which helps to track various deviations from standard usage in the language of non-native speakers, such as frequency patterns X  cases of overuse or underuse or  X  X  X oreign-soundingness, X  X  in comparison with the language of native speakers. Recent studies have focused primarily on the frequency of use of separate language elements (Ringbom 1998 ), collocations and prefabs (Nesselhauf 2005 ), lexical analysis and phrasal use (de Cock 2003 ), etc.
An error-tagged corpus can be subjected to computer-aided error analysis (CEA), which is not restricted to errors seen as a deficiency, but understood as a means to explore the target language and to test hypotheses about the functioning of L2 grammar. CEA also helps to observe meaningful use of non-standard structures of IL. Recent studies focus on lexical errors (Len  X  ko-Szyman  X  ska 2004 ), wrong use of verbal tenses (Granger 1999 ) or phrasal verbs (Waibel 2008 ).

Learner corpora can differ in many ways (for more details see, e.g. Granger 2008 , p. 260):  X  Medium: Learner corpora can capture written or spoken texts, the latter much  X  First language (L1): The data can come from learners with the same L1 or with  X  Target language (L2): Most learner corpora cover the language of learners of  X  Proficiency in target language: Some corpora gather texts of students at the same  X  Cross-sectional/developmental data: Most L2 corpora are cross-sectional,  X  Annotation: Many learner corpora contain only raw data, some contain emendations
Table 1 presents an overview of some currently available learner corpora. For more for a more exhaustive list see https://www.uclouvain.be/en-cecl-lcworld.html . 3 A learner corpus of Czech The learner corpus of Czech as a Second Language ( CzeSL ) is built as a part of a larger project, the Acquisition Corpora of Czech ( AKCES ), a research programme pursued at Charles University in Prague since 2005 (S  X  ebesta 2010 ; Hana et al. 2010 ; S  X  tindlova  X  et al. 2012b , c ; Hana et al. 2012 ). All of the included corpora (see below) are collected and built under similar conditions, allowing for a wide range of linguistic comparisons. In addition to CzeSL, it includes the following corpora:  X  SCHOLA 2010 and EDUCO  X  X ecordings and transcripts of classes from Czech  X  SKRIPT  X  X ritten texts of Czech students (about 600,000 words so far, in  X  IUVENT  X  X poken corpus of native young Czechs X  language (planned) Table 2 summarizes the basic properties of the CzeSL corpus. It is focused on four main groups of learners of Czech:  X  Native speakers of other Slavic languages,  X  Native speakers of other Indo-European languages,  X  Native speakers of distant non-Indo-European languages, and  X  Young speakers of Czech with Romani background. 2 The data collected include: 1. Written texts, produced during all range of situations throughout the language-2. Spoken data. 3. Bachelors X  and Masters X  theses, written in Czech by non-native students.
The data cover all language levels according to the Common European Framework of Reference for Languages (CEFR), from real beginners (A1 level) to advanced learners (level B2 and higher), with a balanced mix of levels as much as possible.
Each text is equipped with metadata records, some of them relate to the respondent (including sociological data about the learner, such as age, gender, and language background X  X he first language, proficiency level in Czech, knowledge of other languages, duration and conditions of language acquisition), while other specify the character of the text and circumstances of its production (availability of reference tools, type of elicitation, temporal and size restrictions etc.).
The intended use of the Czech learner corpus is mainly pedagogical. It will be used in the education of teachers of Czech as a foreign language, it will serve as a source of examples for particular phenomena or of complete authentic texts that can be used both in the classroom and in the production of educational tools, and will help to tailor instructions and teaching materials to specific groups of learners (e.g. groups with different native languages or groups of different ages). Moreover, we expect CzeSL to become a resource for an extensive research of Czech as a second language and the second language acquisition in general (S  X  tindlova  X  2011 ).
The corpus is searchable online. 3 Queries can refer to transcripts (for privacy reasons, scans of handwritten text are not publicly accessible), error annotation, morphological tags and lemmas. 4 Error annotation of CzeSL 4.1 Annotation of learner corpora In the context of second/foreign language acquisition, the learners X  language is seen as an independent system, which should be analysed in its entirety, with incorrect structures as an important part. Texts produced by non-native speakers can be annotated in two different ways:  X  Linguistic mark-up (e.g. part-of-speech tagging, morphological or syntactic
Investigating learners X  language is easier when deviant forms are annotated at least by their correct counterparts, or, even better, by tags making the nature of the error explicit. 4 Although learner corpora tagged this way exist, the two decades of research in this field have shown that designing a tagset for the annotation of errors is a task highly sensitive to the intended use of the corpus and the results are not easily transferable from one language to another. 4.2 Annotation scheme as a compromise Building an error-annotated learner corpus of Czech is a challenging task. Czech, at least in comparison to languages of the existing annotated learner corpora, has a more complex morphology and a less rigid word order, which opens annotation issues that have not been addressed before. 5 Moreover, although the annotation scheme should be sufficiently informative and extensible, it should also be manageable and easily applicable, i.e. not too extensive. The resulting scheme and error typology is a compromise between the limitations of the annotation process interpretation, word order or style, do not have straightforward solutions:
Interference: Being no experts in L2 acquisition, the annotators cannot be expected to spot cases of linguistic interference of L1 or some other language known to the learner. A sentence such as Tokio je pe  X  kny  X  hrad  X  X okio is a nice castle X  is grammatically correct, but its author, a native speaker of Russian, was misled by  X  X alse friends X  and assumed hrad  X  X astle X  as the Czech equivalent of Russian gorod  X  X own, city X .

Interpretation: For some types of errors, the problem is to define the limits of interpretation. The clause kdyby citila na tebe zlobna is grammatically incorrect, yet roughly understandable as  X  X f she felt angry at you X . In such cases the task of the annotator is interpretation rather than correction. The clause can be rewritten as zlobila  X  X f she were angry at you X ; the former being less natural but closer to the original. It is difficult to provide clear guidelines.

Word order: Czech constituent order reflects information structure. It may be hard to decide (even in a context) whether an error is present. The sentence R X dio je taky na skr  X   X ni  X  X  radio is also on the wardrobe X  suggests that there are at least two radios in the room, although the more likely interpretation is that among other things which happen to sit on the wardrobe, there is also a radio. The latter interpretation requires a different word order: Na skr  X   X ni je taky r X dio .
Style: Students often use colloquial expressions, usually without being aware of their status and the appropriate context for their use. Even though these expressions might be grammatical, we emend them with their standard counterparts under the rationale that the intention of the student was to use a register that is perceived as unmarked.
Our error annotation is primarily concerned with the acceptability of the grammatical and lexical aspects of the learner X  X  language in a narrow sense. However, we anticipate that future projects would annotate the corpus with less formal properties of speech, such as the degree of achievement of a communicative goal. 4.3 Multi-tier annotation The optimal error annotation strategy is determined both by the goals and resources of the project and by the type of the language. A single-tier scheme could be used for a specific narrowly defined purpose, such as investigation of morphological properties of the learner language. However, in our case, to apply the single-tier scheme would be problematic. First of all, our corpus should be open to multiple research goals. Thus, a restricted set of linguistic phenomena or a single tier of analysis is not satisfactory. As a result, it is necessary to register successive emendations and to maintain links between the original and the emended forms even when the word order changes or in cases of dropped or added expressions. Another reason is the need to annotate errors spanning multiple forms, often in discontinuous positions.

In the ideal case, the annotator should be free to use an arbitrary number of tiers to suit the needs of successive emendations, choosing from a set of linguistically motivated tiers or introducing annotation tiers ad hoc. On the other hand, the annotator should not be burdened with theoretical dilemmas and the result should be as consistent as possible, which somewhat disqualifies a scheme using a flexible number of tiers. This is why we adopted a compromise solution with two tiers of annotation, distinguished by formal but linguistically founded criteria to make the annotator X  X  decisions easy. Thus the scheme consists of three interconnected tiers X  see Fig. 1 , glossed in (1):  X  Tier 0 X  X nonymised transcript of the hand-written original with some properties  X  Tier 1 X  X orms incorrect in isolation are fixed. The result is a string consisting of  X  Tier 2 X  X andles all other types of errors (valency, agreement, word order, etc.)
The correspondences between successively emended forms are explicitly expressed. Nodes at neighbouring tiers are usually linked 1:1, but words can be joined ( kdy by as in Fig. 1 ) or split, deleted or added. These relations can interlink any number of potentially non-contiguous words across the neighbouring tiers. Multiple words can thus be identified as a single unit, while any of the participating word forms can retain their 1:1 links with their counterparts at other tiers.
Whenever a word form is emended, the type of error can be specified as a label at the link connecting the incorrect form at a lower tier with its emended form at a higher tier (such as incorInfl or incorBase for morphological errors in inflectional endings and stems, stylColl as a stylistic marker, wbdOther as a word boundary error, and agr as an error in agreement).

Each node may be assigned information in addition to the form of the word, such as lemma, morphosyntactic category or syntactic function.
 Manual annotation is supported by the purpose-built annotation tool feat 6 and followed by automatic post-processing (see Sect. 6 ). 4.4 Error categorisation A typical learner of Czech makes errors all along the hierarchy of theoretically motivated linguistic levels, from graphemics to discourse structure. For practical reasons we emend the input conservatively to arrive at a coherent and well-formed result, without any ambition to produce a stylistically optimal solution, refraining from too loose interpretation. Where a part of the input is not comprehensible, it is marked as such and left without emendation. The taxonomy of errors is based on linguistic categories, complemented by a classification of superficial alternations of the source text, such as missing, redundant, faulty or incorrectly ordered element. 4.4.1 Errors at Tier 1 Errors in individual word forms, treated at Tier 1, include misspellings (also diacritics and capitalisation), misplaced word boundaries but also errors in inflectional and derivational morphology and unknown stems X  X ade-up or foreign words. Except for misspellings, all these errors are annotated manually. The result of emendation is the closest correct form, which can be further modified at Tier 2 according to context, e.g. due to an error in agreement or semantic incompatibility of the lexeme. See Table 3 for a list of errors manually annotated at Tier 1. The last three error types ( stylColl , stylOther and problem ) are used also at Tier 2.
The rule of  X  X  X orrect forms only X  X  at Tier 1 has a few exceptions: a faulty form is retained if no correct form could be used in the context or if the annotator cannot decipher the author X  X  intention. On the other hand, a correct form may be replaced by another correct form if the author clearly misspelled the latter, creating an unintended homograph with another form. 4.4.2 Errors at Tier 2 Emendations at Tier 2 concern errors in agreement, valency, analytical forms, pronominal reference, negative concord, the choice of aspect, tense, lexical item or idiom, and also in word order. For the agreement, valency, analytical forms, pronominal reference and negative concord cases, there is usually a correct form, which determines some properties (morphological categories) of the faulty form. Table 4 gives a list of error types manually annotated at Tier 2. The automatically identified errors include word order errors and subtypes of the analytical forms error vbx . 5 Evaluation of the error mark-up There is no widely accepted metric evaluating the consistency of annotation of learner corpora. In the current annotation practice of non-native speakers X  corpora, it is common to have ill-formed texts tagged by a single annotator, despite problems in reliability and evaluation. A general shift towards multiple annotation of learner corpora is imminent.

The issue of singly annotated learner texts, used as application training data, was raised for the first time by Tetreault and Chodorow ( 2008 ), who investigated native-speakers X  classification of prepositions usage. They concluded that two native annotators performing the task of tagging errors in prepositions on the same text reach at best an agreement level on the border between moderate and substantial (their kappa value was j = 0.63 X  X he metric is explained in Sect. 5.1 below). Rozovskaya and Roth ( 2010 ) also report low inter-annotator agreement ( j = 0.16 -0.40) for the task of classifying sentences written by ESL learners. Meurers ( 2009 ) also discusses the issue of verification of error annotation validity, viewing the lack of studies investigating inter-annotator agreement in the manual annotation of non-native speakers texts as a serious barrier for the development of annotation tools. 5.1 Inter-annotator agreement (IAA) The manual annotation of CzeSL was evaluated using the metric j (kappa, Cohen 1960 ), the standard measure of inter-annotator agreement, especially for tagged corpora. The values of j are within the interval [ -1, 1], where j = 1 means perfect agreement, j = 0 agreement equal to chance, and j =-1  X  X  X erfect X  X  disagreement.

The problem is to determine which error tags in one annotation correspond to which error tags in the other and how their scopes align. Tier 0, the original text, is shared by both annotations. However, annotators might use a different target hypothesis, and thus the higher tiers can differ. Moreover, they often differ not only in the shape of tokens but also in their number. Because of this, we project error tags to Tier 0 tokens and then calculate differences relative to that tier. When there are multiple tokens on Tier 0 corresponding to a token on the relevant tier, we project the tag on the first Tier 0 token only. 7
Table 5 summarizes the distribution of selected error tags for a pilot sample and for all doubly annotated texts available at the time of the evaluation. The first column gives the error tag; some tags (marked by an asterisk) are used only in the evaluation as a more general error category. 8 The column headed by  X  X vg tags X  gives the number of times the tag was used by an average annotator (calculated simply as the total for the two annotators divided by two).
 5.2 A pilot annotation Early in the project, we calculated IAA on a pilot sample. It consisted of 67 texts totalling 9848 tokens, most of them written by native speakers of Russian; the texts sample was corrected and assigned error tags according to the error taxonomy presented above in Sect. 4.4 by 14 annotators. They were split into two groups: Annotators A and Annotators B. Each group annotated the whole sample independently. On average each annotator processed 1,475 words in 11 texts. The annotator agreement is reported in Table 5 . 5.3 Full corpus Using the feedback gained from the pilot experiment we improved the annotation manual and the training of annotators. In a few cases we also slightly modified the error taxonomy. A substantially larger subset of the transcribed texts was annotated by 31 annotators in three groups specializing on Slavic, non-Slavic and Roma learners. 9 The evaluation was extended to all usable texts doubly-annotated so far, i.e. to 1,396 texts totalling 175,234 words.

As a result, the reliability of the annotation has generally improved X  X ee IAA for the whole doubly-annotated part of the corpus in Table 5 . At the same time we are aware that if two annotations differ, it does not necessarily mean one of them is wrong. Language, especially the language of non-native learners, is fuzzy and ambiguous and we do not intend to cover up this fact by providing instructions aimed solely at high IAA just for the sake of it.

For example, one annotator might perceive the word checkni in checkni moje str X nky  X  X heck my site X  to be a clearly non-Czech word (annotating it as fwNc ), while another would consider it as a colloquial form (annotating it as stylColl ). In such cases, the annotation manual might instruct the annotator to prefer a certain tag. However, even though this would lead to a higher IAA, it would conceal the fact that these expressions are perceived differently by different native speakers.
The table shows that on T1 the annotators tend to agree in the domain categories incor* and wbd* , i.e. for incorrect morphology and for improper word boundaries ( j &gt; 0.8 and j &gt; 0.6, respectively). IAA was lower ( j &lt; 0.4) for categories with a fuzzy interpretation, where a target hypothesis is difficult to establish, such as fw* category and its subcategories, used to tag attempts to coin a new Czech lexeme ( fwFab ), or foreign/unidentified strings of words ( fwNc ). Even the choice between the two subcategories was problematic as can be seen from Table 6 .

At T2 the annotators agree in agreement errors ( agr , j &gt; 0.6) and errors in expressing syntactic dependency ( dep , j * 0.6), and also in the well-defined category of errors in reflexive expressions ( rflx , j * 0.4). However, pronominal references ( ref ), secondary (consequent) errors ( sec ) and X  X urprisingly X  X lso errors in analytical verb forms/complex predicates ( vbx ) and negation ( neg ) show a very low level of IAA, even though they are identifiable by formal linguistic criteria. In all these four cases, the distribution of tags and the annotators X  feedback suggest that the annotation manual fails to provide enough guidance and formal criteria in distinguishing between the error types ref versus agr and ref versus dep (in either case the disagreement represents 19 % of all the inconsistent uses of the tag ref ).
IAA in the distribution of tags for usage and especially lexical errors is lower ( j &lt; 0.4). The usage of these tags is highly dependent on the annotator X  X  judgment, and the results are low as expected. An analysis has revealed that the tag lex has a systematic distribution: if the original lexeme and its  X  X deal X  emendation differ in their meaning distinctly, the annotators agree in their emendations in most of the cases (2); if the lexemes show semantic proximity, the annotators highly disagree in the emendation and therefore also in the consequent annotation (3).
 Tables 6 and 7 present a confusion matrix for T1 and T2 error tags, respectively. The  X ? X  column/row covers cases when there were multiple tags provided by either annotator and they did not include the relevant tag (so we know that the annotators disagreed, but we cannot say which tags correspond to which). Note that the totals might be larger than the sums of the respective row or column as the table shows counts for selected tags only. Thus we can see, for example, that in 8,989 cases the annotators agreed on the incorBase tag, but in 400 cases Annotator B used the incorInfl tag instead, far less common were cases when Annotator B assumed the error to be one of the fw* tags, finally in 574 cases Annotator B used multiple tags, but none of them was incorBase (so we cannot say which one of those corresponds to A X  X  incorBase tag).

From these tables, we can see that the annotators most commonly confused the following tags:  X  incorBase (error in stem) for incorInfl (error in inflection)  X  fwNc (foreign word) for incorBase (error in stem)  X  agr (agreement error) for dep (valency error), less frequently for lex (lexicon 5.4 Error tags depend on emendation Analysis of the tagged data (see Table 8 ) shows that the disagreement in using error tags is not necessarily caused by an annotator X  X  fault, but could rather be dependent on the choice of the emended form (the target hypothesis). For example, while agr has an overall agreement of 0.69, it is 0.82 for identical emendations, but only 0.24 if the target (T2) hypotheses are different. The situation of other tags is similar. See (4) for an example. However, sometimes annotators arrived to identical emendations, but still In some cases, this is manifested by different emendations on the lower tier, i.e. T1. For example, consider the expression in (5): both annotators corrected the non-existent word tezki to te  X  z  X  k X   X  X ifficult X , but they differ in their interpretation of the  X  X ifficult X  ( y and i have the same pronunciation in Czech), correcting it to the official te  X  z  X  k X  on the next layer. A2 interpreted tezky as simply as incorrect form, and between them without knowing more about the language of the speaker.
 In all these cases, tagging is correct vis-a ` -vis the selected emendation. Currently, we investigate the impact of emendation on error annotation at the individual tiers, but we can already support the requirement of explicit target interpretation in the annotation scheme (Lu  X  deling 2008 ). The scheme can thus be verified by the calculation of IAA in the distribution of the tags, depending on the final hypothesis (cf, i.a., Meurers 2009 ). 5.5 Outline of the possible causes of the annotators disagreement We can identify the following causes of the annotators X  disagreements: 1. Invalid or imprecise annotation scheme: Generally, the annotators X  disagreement 2. Insufficient screening and training of the annotators: The level of screening and 3. Different target hypotheses: Some annotations require a considerable amount of 6 Automatic extension of manual annotation So far, the annotation is largely a manual enterprise, quite demanding in terms of annotators X  time and expertise. We aim to shift much of the burden to automatic tools, either by aiding human annotators and/or following up on their work, or by processing the texts from scratch, without any human involvement. 10 Let us explore the former option first.

Manually emended and error-annotated text can be assigned additional information by automatic tools in the following two ways: 1. As far as the emended text approximates standard language, at least in 2. Some manually assigned error tags can be specified in more detail using formal
The tools for extending manual annotation can also be used to check its quality of manual annotation, especially to identify tags that are probably missing or incorrect (see Sect. 6.3 ). 6.1 Automatic addition of linguistic information Emended sentences at T2 can be tagged with morphosyntactic categories and Each word is assigned a lemma and a tag from a standard morphological tagset Hajic  X  ( 2004 ). Applying standard methods to T1, consisting of forms which may be correct only in isolation and which may also be wrongly ordered, can produce unreliable results. 11 Instead of a fully disambiguated tag and lemma, T1 is tagged using potentially ambiguous morphological analysis of isolated forms in combina-tion with the tag and lemma assigned at T2 as follows:  X  If the forms at both tiers are identical, the tag and lemma assigned at T2 is used.  X  If the forms are different, but their lemmas are identical, then that lemma and the  X  If the T1 form X  X  lemma is different from the lemma at T2, the T1 form receives all 6.2 Automatic extension and modification of error annotation Some error types can be detected automatically. This is especially true about formal errors at T1, identifiable by a simple comparison of the corresponding forms at T0 and T1. Errors at T2 are more difficult to classify automatically, thus only a limited number of phenomena are tagged this way.

The manually assigned T1 tags include the following three types of errors: wrong form ( incor ), incorrect word boundaries ( wbd ), and neologism or foreign word ( fw ). The automatically assigned  X  X ormal X  errors complement these manual tags as an additional dimension of annotation. For example, *chrozba / hrozba  X  X hreat X  is manually annotated as incorBase (the h/ch error is in the stem), and *kaz  X  d X cho / kaz  X  d X ho  X  X very masc . sg . gen / acc  X  X s incorInfl (the h/ch error is in the  X ho ending). However, in both cases, the correct h is incorrectly devoiced, thus the h/ch error is annotated as formVcd1 . 12
The formal T1 error tags express the way in which a T1 form differs from the original incorrect T0 form. Most of these tags (such as  X  X  X issing character X  X ,  X  X  X witch error X  X  or even  X  X  X rror in diacritics X  X ) only identify surface manifestations. However, assimilation or palatalisation. It is the possibility of their automatic detection that puts them in the same class with the truly formal error types.

Table 9 provides examples of some currently handled automatically assigned errors on T1. Some errors affect only spelling with no change in pronunciation affect pronunciation (vowel quantity, e epenthesis). Some errors might affect pronunciation in some contexts, but not others (writing i/y , the c/k substitution).
Most of the T2 error tags are assigned manually, because the variability of incorrect structures is too high to allow for reliable automatic error tagging. Thus, only limited amount of information is added automatically:  X  The reflexivity error tag ( rflx ) is added if another type of error concerns a  X  Manually assigned error tags for compound verb forms ( vbx ) are sub-divided as  X  Tags marking deleted and inserted words are added ( odd , miss ).  X  Word order corrections are tagged ( wo ). The annotator reorders the words as 6.3 Automatic annotation checking The system designed for automatic error tagging is also used for evaluating the quality of manual annotation, checking the result for tags that are probably missing or incorrect. For example, if a T0 form is not known to the morphological analyser, it is likely to be an incorrect word which should be emended. Also, if a word was emended and the change affects pronunciation, but no error tag was assigned, an incorBase or incorInfl error tag is probably missing. This approach cannot find all problems in emendation and error annotation, but provides a good approximate measure of the quality of annotation and draws the annotator X  X  attention to potential errors. 7 Fully automatic annotation Despite the benefits of annotators X  insight and judgment, manual annotation, or even manual annotation supplemented by automatic annotation, is tedious and costly. On the other hand, automatic tools are more error-prone and cannot produce the sort of sophisticated annotation envisaged in the present project. Aware of these pros and cons, we explore how far we can get without manual annotation. Due to the lack of methods targeting learner texts, we confronted some  X  X ative Czech X  tools (two taggers and a spell checker) with ill-formed input. 7.1 Automatic emendation One of the options to (partially) automate the task of emendation is to use a proofreading tool X  X  spell checker or a grammar checker. So far, we have experimented with Korektor (Richter 2010 ), a spell checker that has some functionalities of a grammar checker, using a combination of lexicon, morphology and a syntax model. 13
The tool was tested on a subset of the pilot set of annotated texts (see Sect. 5.2 ), produced by learners at intermediate or higher levels of proficiency, yet among the total 9,372 tokens (7,995 tokens excluding punctuation) 918 (10 %) were not recognised by the morphological analyser included in a Czech POS tagger (see Morc  X  e in Spoustova  X  et al. 2007 ). Even more forms were judged as faulty by the annotators: 1,189 (13 %) were corrected in the same way by both annotators at T1 and 1,519 (16 %) at T2.

Results of the spell checker were compared with those of the morphological analyser and with forms at T1 and T2, provided both annotators were in agreement. The spell checker was run in three (batch) modes: (i)  X  X  X utocorrect X  X  (as test the hypothesis that diacritics is a frequent source of errors.

Although the morphological analyser includes a guesser, it makes no attempt to correct an unknown word form, only guesses its morphosyntactic tag and lemma. The spell checker is deemed to be successful for a given form if the morphological analyser treats it as unknown and the spell checker suggests a correction, or if the analyser treats the form as known and the spell checker leaves it intact.

Table 10 shows figures for the morphological analyser. The rows give results for the three modes: autocorrect (i), diacritics for  X  X  X emove-diacritics X  X  followed by  X  X  X iacritics X  X  (ii), and autocorrect ? diacritics for the full sequence (iii). The column  X  X  X orrected X  X  gives the counts for forms corrected by the spell checker run in the relevant mode. The column  X  X  X nknown X  X  gives the number of cases where the morphological analyser happens to flag a form corrected by the spell checker as unknown. The results of the analyser are assumed as truth for the purpose of calculating precision ( X  X  X nknown X  X / X  X  X orrected X  X ) and recall ( X  X  X nknown X  X /918, the latter figure representing all forms unknown to analyser).

Precision is not really a fair measure here, because the analyser never flags forms which are correct in isolation but faulty in a context, while the spell checker often manages to use local context to replace a form X with an orthographically close but morphosyntactically quite different for Y: podl X  ? podle, jej X ch ? jejich, z  X  it ? z  X   X t, lib X  ? l X b X , ze ? z  X  e, div X  ? d X v X , drahy ? drahy  X  , mel ? me  X  l, j X ch ? jich, c  X  ine ? c  X   X   X  ne  X  . Interestingly, diacritics seem to represent a substantial share of problems in learners X  writings, and the preprocessing of the input by the diacritics remover and assigner (iii) means a significant improvement.

Corrections made by the annotators can be compared verbatim with those proposed by the spell checker. The spell checker scores whenever the form proposed by the relevant mode matches the form at T1 or T2, respectively. The two annotators must agree about the corrected form, only then it is seen as fit for comparison.
At T1 the total number of corrections (1,189) is higher than the number of forms unknown to the morphological analyser (918) because the annotators correct also misspellings which look like homographs with an existing form. Such faulty forms are never detected by the morphological analyser. As a result, recall of the spell checker is lower when its performance is compared with T1 than when with the results of the morphological analyser. Precision stays roughly the same as in the previous comparison because in one aspect T1 is similar to the analyser: it still largely abstracts from context. E.g. annotators are instructed to leave errors due to missed grammatical concord for T2. The data are shown in Table 11  X -the column  X  X  X orrected X  X  is identical to that in Table 10 , but the  X  X  X rong X  X  column shows the number of cases where the two annotators agree about an emended form, identical to the suggestion of the spell checker.

It is interesting to investigate cases where the spell checker does not agree with the annotators, but both the spell checker and the annotators indicate an error (170 such cases at T1 for the autocorrect ? diacritics mode). In some of these cases, the simple autocorrect mode without the diacritics component fares better (in 30 cases out of 170). It seems that removing and reassigning diacritics takes the spell-checker too far (Table 12 ). In some cases the T1 and T2 versions differ and none of the methods matches the contextually correct version of T2 ( pl X z  X  , l X pe ).

In 150 cases the spell checker suggests a correction when T1 prefers the original, but in 37 cases the spell checker agrees with an annotator at T2 (in 16 cases with both), which means that the real precision is higher. The rest of the cases are mostly inflectional issues, often due to misassigned diacritics, but also quite a few errors in the annotation (shared by both annotators).

T2 is problematic for evaluation in its own right. Some error types handled here are due to wrong word order, style, phraseology and a few other that go beyond simple spell checking, even in a broader sense of some degree of contextual sensitivity. The figures in Table 13 , otherwise similar to Table 11 , should be interpreted accordingly.

The two-stage annotation scheme suggests the option to distinguish corrections of forms that are wrong in any context, from those that could be correct in isolation, or in a different context, i.e. to test the grammar-checking capabilities of the spell checker. However, Korektor does not quite match the annotation scheme. It is only possible to find a few individual cases of successful corrections of missed agreement or case government (in the order of tens). Again, as in all the previous cases, the mode combining diacritics remover, assigner and proofreader is the best scenario.
The results seem to justify the option to integrate the spell checker into the annotation workflow, even though its suggestions may not quite match the two distinct tiers without tuning to the specific task and annotation scheme. We have already applied Korektor in the autocorrect mode to all transcribed texts in CzeSL , including the texts without any manual annotation. 14 7.2 Automatic error tagging For an experiment in automatic tagging we used two taggers, based on different concepts: Morc  X  e (Votrubec 2006 ) uses a morphological analyser, preferring lexical and morphological diagnostics over syntactic context, while TnT (Brants 2000 ) has the opposite strategy, relying on a lexicon extracted from training data. Both taggers were trained on the same tagset and include a method to handle unknown words. Because of the different strategies the taggers use to tag correct input, they respond differently to various types of deviations. A mutual comparison of their results is thus as interesting as their evaluation against gold standard, which X  X n the case of ill-formed input X  X s a difficult concept anyway.

Identifying all errors would involve comparing manual annotations at T2 form-by-form with the original text at T0. In the current absence of such data, we used data obtained from the easier task of comparing T0 to T1, where all erroneous forms are emended to a closest correct version, disregarding context.

Table 14 presents data extracted from a sample of 93 texts including 12,681 word tokens, with 1,323 tokens (8.9 %) identified as ill-formed by the morphological analyser. The two taggers agreed on the same tag in 405 cases, i.e. in 28.8 % of the total of ill-formed tokens, and disagreed in 918 cases (71.2 %). The figures are additionally split by 12 morphological categories constituting the tag. Column 1 (T0m 9 T0t) shows in which categories the two taggers disagree at T0 for the 918 tokens, where their tags do not match at least in one category. Agreement is significantly lower between categories largely determined by syntactic context (POS, Gender, Number, Case) as opposed to those determined lexically. Columns 2 (T0m 9 T1) and 3 (T0t 9 T1) show agreement rates of tags assigned by Morc  X  e and TnT , respectively, to all tokens at T0 15 in comparison with tags assigned by Morc  X  e to the corresponding tokens at T1. 16 Morc  X  e shows better results overall and in most categories. Columns 4 and 5 show agreement rates for an ill-formed subset of the sample used in Columns 2 and 3. Interestingly, TnT shows significantly better results, except in the categories of Person and Tense.
 The difference between the two taggers is also reflected in the share of different POS categories assigned to ill-formed words. Table 15 shows that Morc  X  e has a more even distribution, but strongly disprefers all verbal categories.

To sum up, the comparison of the two taggers confirms the assumption that the differences in their strategies will have a significant effect on the interpretation of faulty forms. A more general observation concerns the comparison of the success rate of the two taggers on the ill-formed input: TnT loses ground in a context with many errors but outperforms Morc  X  e on faulty forms, while Morc  X  e strongly disprefers verbs and works better in general. 8 Conclusion We described a corpus of Czech texts produced by non-native learners of Czech, focussing on error annotation. Results of its evaluation show fair inter-annotator agreement. We also explored and implemented some options of partially or even fully automating the annotation of learner texts.

It is no simple task to design an annotation scheme for a learner corpus and to maintain consistency in the annotated texts, both in a way that would reflect most demands of the corpus users. One of the main reasons is that annotating learner texts tends to be a highly specific enterprise, and even seemingly similar projects do not offer enough guidance X  X olutions are often too specific to a language or to the project concept and user requirements. On the other hand, annotation itself is quite rewarding due to the plentiful feedback from the annotators about all aspects of the task and, of course, about the learners X  interlanguage.

More specifically, our experience shows that the rules for tagging morphosyn-tactic errors are relatively easy to formalise and it is thus possible to obtain a high inter-annotator agreement for them. However, we were unable to obtain a similarly robust annotation of semantic errors, which are much more dependent on subjective judgement. It is even unclear whether it is desirable to aim to standardize their annotation. Obviously, we should aim to prevent and correct differences that are clear mistakes. Some of it can be done by a better selection of annotators, some by clearer instructions and some by providing better tools to annotators. For example, we have seen less errors in the annotation of incorBase and incorInfl errors after integrating a spell-checker into the annotation tool X  X ome of these annotation errors were simply due to annotators overlooking the incorrect word.

The pilot study, where two POS taggers and a spell checker were applied to ill-formed input, confirmed the viability of a partially or even fully automatic annotation as an alternative to manual-only annotation, especially when the demand for large data is higher than concerns about the error rate. It remains to be seen to what extent the comparison of results of multiple taggers, based on different tagging strategies, can lead to usable interpretations of faulty forms.
 References
 Abstract The paper describes a corpus of texts produced by non-native speakers of Czech. We discuss its annotation scheme, consisting of three interlinked tiers, designed to handle a wide range of error types present in the input. Each tier corrects different types of errors; links between the tiers allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified. The annotation scheme is tested on a data set including approx. 175,000 words with fair inter-annotator agreement results. We also explore the possibility of applying automated linguistic annotation tools (taggers, spell checkers and grammar checkers) to the learner text to support or even substitute manual annotation. Keywords Learner corpus Error annotation Second language acquisition Czech 1 Introduction Learner corpora, i.e. electronic collections of texts produced by non-native speakers, are a rich source of information about specific features of learners X  language. They can be annotated in the usual ways, common in other types of corpora, i.e. by metadata, morphosyntactic categories and syntactic structure, but their most interesting aspect is examples of deviant use, which can be identified, corrected and classified. Annotation of this kind is a challenging task, even more so for a language such as Czech, with its rich inflection, derivation, agreement, and a largely information-structure-driven constituent order.

Following an overview of some learner corpora (Sect. 2 ) we present a learner corpus of Czech, consisting of approx. 2 million running words, compiled from texts written by students of Czech as a second or foreign language at all levels of proficiency (Sect. 3 ). We discuss the corpus annotation scheme, consisting of three interlinked tiers, designed to cope with a wide range of error types present in the input (Sect. 4 ). Tier 0 represents the transcribed input, Tier 1 corrects non-words and Tier 2 the remaining error types; links between the tiers allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified according to a taxonomy. Annotation of this kind is supplemented by a formal classification, e.g. an error in morphology can also be specified as being manifested by a missing diacritic or a wrong consonant change.

The annotation scheme was tested in two rounds, each time on a doubly-annotated sample X  X irst on a pilot annotation of approx. 10,000 words and later on a large data set including approx. 175,234 words, both with fair inter-annotator agreement results, calculated as several inter-annotator agreement measures (Sect. 5 ).

To assist the annotator and to supply additional information about deviations from the standard, we aim at a synergy of manual and automatic annotation, deriving information from the original input and from the manual annotation (Sect. 6 ). Some methods interact with the annotator (e.g. a spell checker within the annotation editor marks potentially incorrect forms), or use results of manual annotation, including an automatic check for consistency and compliance with the annotation guidelines. After approval by the annotator X  X  supervisor, some error tags are specified in more detail and more tags are added automatically.

To assist the annotator even further, we also experiment with fully automatic methods (Sect. 7.1 ). We perform automatic emendation by a mildly context-sensitive spell checker and plan to use a grammar checker or a stochastic model to identify error types. A tagger trained on native speakers X  language is used at the tier of corrected text to supply morphosyntactic categories, but we explore options of applying general-purpose tools to the original text (see, e.g. Van Rooy and Scha  X  fer 2003 ; Dickinson 2010 ). 2 Learner corpora A learner corpus, also called interlanguage or L2 corpus, is a computerised textual database of language as produced by foreign/second language (L2) learners (Leech 1998 , p. xiv). It is a very useful resource in the research of second language acquisition (SLA) and foreign language teaching (FLT). It serves as a repository of authentic data about a specific variety of natural language (Granger 2003b ), namely the learner language, or interlanguage (IL). 1
Learner corpora can be used to compare non-native and native speakers X  language, or interlanguage varieties. They can be studied on the background of traditional native language corpora, which helps to track various deviations from standard usage in the language of non-native speakers, such as frequency patterns X  cases of overuse or underuse or  X  X  X oreign-soundingness, X  X  in comparison with the language of native speakers. Recent studies have focused primarily on the frequency of use of separate language elements (Ringbom 1998 ), collocations and prefabs (Nesselhauf 2005 ), lexical analysis and phrasal use (de Cock 2003 ), etc.
An error-tagged corpus can be subjected to computer-aided error analysis (CEA), which is not restricted to errors seen as a deficiency, but understood as a means to explore the target language and to test hypotheses about the functioning of L2 grammar. CEA also helps to observe meaningful use of non-standard structures of IL. Recent studies focus on lexical errors (Len  X  ko-Szyman  X  ska 2004 ), wrong use of verbal tenses (Granger 1999 ) or phrasal verbs (Waibel 2008 ).

Learner corpora can differ in many ways (for more details see, e.g. Granger 2008 , p. 260):  X  Medium: Learner corpora can capture written or spoken texts, the latter much  X  First language (L1): The data can come from learners with the same L1 or with  X  Target language (L2): Most learner corpora cover the language of learners of  X  Proficiency in target language: Some corpora gather texts of students at the same  X  Cross-sectional/developmental data: Most L2 corpora are cross-sectional,  X  Annotation: Many learner corpora contain only raw data, some contain emendations
Table 1 presents an overview of some currently available learner corpora. For more for a more exhaustive list see https://www.uclouvain.be/en-cecl-lcworld.html . 3 A learner corpus of Czech The learner corpus of Czech as a Second Language ( CzeSL ) is built as a part of a larger project, the Acquisition Corpora of Czech ( AKCES ), a research programme pursued at Charles University in Prague since 2005 (S  X  ebesta 2010 ; Hana et al. 2010 ; S  X  tindlova  X  et al. 2012b , c ; Hana et al. 2012 ). All of the included corpora (see below) are collected and built under similar conditions, allowing for a wide range of linguistic comparisons. In addition to CzeSL, it includes the following corpora:  X  SCHOLA 2010 and EDUCO  X  X ecordings and transcripts of classes from Czech  X  SKRIPT  X  X ritten texts of Czech students (about 600,000 words so far, in  X  IUVENT  X  X poken corpus of native young Czechs X  language (planned) Table 2 summarizes the basic properties of the CzeSL corpus. It is focused on four main groups of learners of Czech:  X  Native speakers of other Slavic languages,  X  Native speakers of other Indo-European languages,  X  Native speakers of distant non-Indo-European languages, and  X  Young speakers of Czech with Romani background. 2 The data collected include: 1. Written texts, produced during all range of situations throughout the language-2. Spoken data. 3. Bachelors X  and Masters X  theses, written in Czech by non-native students.
The data cover all language levels according to the Common European Framework of Reference for Languages (CEFR), from real beginners (A1 level) to advanced learners (level B2 and higher), with a balanced mix of levels as much as possible.
Each text is equipped with metadata records, some of them relate to the respondent (including sociological data about the learner, such as age, gender, and language background X  X he first language, proficiency level in Czech, knowledge of other languages, duration and conditions of language acquisition), while other specify the character of the text and circumstances of its production (availability of reference tools, type of elicitation, temporal and size restrictions etc.).
The intended use of the Czech learner corpus is mainly pedagogical. It will be used in the education of teachers of Czech as a foreign language, it will serve as a source of examples for particular phenomena or of complete authentic texts that can be used both in the classroom and in the production of educational tools, and will help to tailor instructions and teaching materials to specific groups of learners (e.g. groups with different native languages or groups of different ages). Moreover, we expect CzeSL to become a resource for an extensive research of Czech as a second language and the second language acquisition in general (S  X  tindlova  X  2011 ).
The corpus is searchable online. 3 Queries can refer to transcripts (for privacy reasons, scans of handwritten text are not publicly accessible), error annotation, morphological tags and lemmas. 4 Error annotation of CzeSL 4.1 Annotation of learner corpora In the context of second/foreign language acquisition, the learners X  language is seen as an independent system, which should be analysed in its entirety, with incorrect structures as an important part. Texts produced by non-native speakers can be annotated in two different ways:  X  Linguistic mark-up (e.g. part-of-speech tagging, morphological or syntactic
Investigating learners X  language is easier when deviant forms are annotated at least by their correct counterparts, or, even better, by tags making the nature of the error explicit. 4 Although learner corpora tagged this way exist, the two decades of research in this field have shown that designing a tagset for the annotation of errors is a task highly sensitive to the intended use of the corpus and the results are not easily transferable from one language to another. 4.2 Annotation scheme as a compromise Building an error-annotated learner corpus of Czech is a challenging task. Czech, at least in comparison to languages of the existing annotated learner corpora, has a more complex morphology and a less rigid word order, which opens annotation issues that have not been addressed before. 5 Moreover, although the annotation scheme should be sufficiently informative and extensible, it should also be manageable and easily applicable, i.e. not too extensive. The resulting scheme and error typology is a compromise between the limitations of the annotation process interpretation, word order or style, do not have straightforward solutions:
Interference: Being no experts in L2 acquisition, the annotators cannot be expected to spot cases of linguistic interference of L1 or some other language known to the learner. A sentence such as Tokio je pe  X  kny  X  hrad  X  X okio is a nice castle X  is grammatically correct, but its author, a native speaker of Russian, was misled by  X  X alse friends X  and assumed hrad  X  X astle X  as the Czech equivalent of Russian gorod  X  X own, city X .

Interpretation: For some types of errors, the problem is to define the limits of interpretation. The clause kdyby citila na tebe zlobna is grammatically incorrect, yet roughly understandable as  X  X f she felt angry at you X . In such cases the task of the annotator is interpretation rather than correction. The clause can be rewritten as zlobila  X  X f she were angry at you X ; the former being less natural but closer to the original. It is difficult to provide clear guidelines.

Word order: Czech constituent order reflects information structure. It may be hard to decide (even in a context) whether an error is present. The sentence R X dio je taky na skr  X   X ni  X  X  radio is also on the wardrobe X  suggests that there are at least two radios in the room, although the more likely interpretation is that among other things which happen to sit on the wardrobe, there is also a radio. The latter interpretation requires a different word order: Na skr  X   X ni je taky r X dio .
Style: Students often use colloquial expressions, usually without being aware of their status and the appropriate context for their use. Even though these expressions might be grammatical, we emend them with their standard counterparts under the rationale that the intention of the student was to use a register that is perceived as unmarked.
Our error annotation is primarily concerned with the acceptability of the grammatical and lexical aspects of the learner X  X  language in a narrow sense. However, we anticipate that future projects would annotate the corpus with less formal properties of speech, such as the degree of achievement of a communicative goal. 4.3 Multi-tier annotation The optimal error annotation strategy is determined both by the goals and resources of the project and by the type of the language. A single-tier scheme could be used for a specific narrowly defined purpose, such as investigation of morphological properties of the learner language. However, in our case, to apply the single-tier scheme would be problematic. First of all, our corpus should be open to multiple research goals. Thus, a restricted set of linguistic phenomena or a single tier of analysis is not satisfactory. As a result, it is necessary to register successive emendations and to maintain links between the original and the emended forms even when the word order changes or in cases of dropped or added expressions. Another reason is the need to annotate errors spanning multiple forms, often in discontinuous positions.

In the ideal case, the annotator should be free to use an arbitrary number of tiers to suit the needs of successive emendations, choosing from a set of linguistically motivated tiers or introducing annotation tiers ad hoc. On the other hand, the annotator should not be burdened with theoretical dilemmas and the result should be as consistent as possible, which somewhat disqualifies a scheme using a flexible number of tiers. This is why we adopted a compromise solution with two tiers of annotation, distinguished by formal but linguistically founded criteria to make the annotator X  X  decisions easy. Thus the scheme consists of three interconnected tiers X  see Fig. 1 , glossed in (1):  X  Tier 0 X  X nonymised transcript of the hand-written original with some properties  X  Tier 1 X  X orms incorrect in isolation are fixed. The result is a string consisting of  X  Tier 2 X  X andles all other types of errors (valency, agreement, word order, etc.)
The correspondences between successively emended forms are explicitly expressed. Nodes at neighbouring tiers are usually linked 1:1, but words can be joined ( kdy by as in Fig. 1 ) or split, deleted or added. These relations can interlink any number of potentially non-contiguous words across the neighbouring tiers. Multiple words can thus be identified as a single unit, while any of the participating word forms can retain their 1:1 links with their counterparts at other tiers.
Whenever a word form is emended, the type of error can be specified as a label at the link connecting the incorrect form at a lower tier with its emended form at a higher tier (such as incorInfl or incorBase for morphological errors in inflectional endings and stems, stylColl as a stylistic marker, wbdOther as a word boundary error, and agr as an error in agreement).

Each node may be assigned information in addition to the form of the word, such as lemma, morphosyntactic category or syntactic function.
 Manual annotation is supported by the purpose-built annotation tool feat 6 and followed by automatic post-processing (see Sect. 6 ). 4.4 Error categorisation A typical learner of Czech makes errors all along the hierarchy of theoretically motivated linguistic levels, from graphemics to discourse structure. For practical reasons we emend the input conservatively to arrive at a coherent and well-formed result, without any ambition to produce a stylistically optimal solution, refraining from too loose interpretation. Where a part of the input is not comprehensible, it is marked as such and left without emendation. The taxonomy of errors is based on linguistic categories, complemented by a classification of superficial alternations of the source text, such as missing, redundant, faulty or incorrectly ordered element. 4.4.1 Errors at Tier 1 Errors in individual word forms, treated at Tier 1, include misspellings (also diacritics and capitalisation), misplaced word boundaries but also errors in inflectional and derivational morphology and unknown stems X  X ade-up or foreign words. Except for misspellings, all these errors are annotated manually. The result of emendation is the closest correct form, which can be further modified at Tier 2 according to context, e.g. due to an error in agreement or semantic incompatibility of the lexeme. See Table 3 for a list of errors manually annotated at Tier 1. The last three error types ( stylColl , stylOther and problem ) are used also at Tier 2.
The rule of  X  X  X orrect forms only X  X  at Tier 1 has a few exceptions: a faulty form is retained if no correct form could be used in the context or if the annotator cannot decipher the author X  X  intention. On the other hand, a correct form may be replaced by another correct form if the author clearly misspelled the latter, creating an unintended homograph with another form. 4.4.2 Errors at Tier 2 Emendations at Tier 2 concern errors in agreement, valency, analytical forms, pronominal reference, negative concord, the choice of aspect, tense, lexical item or idiom, and also in word order. For the agreement, valency, analytical forms, pronominal reference and negative concord cases, there is usually a correct form, which determines some properties (morphological categories) of the faulty form. Table 4 gives a list of error types manually annotated at Tier 2. The automatically identified errors include word order errors and subtypes of the analytical forms error vbx . 5 Evaluation of the error mark-up There is no widely accepted metric evaluating the consistency of annotation of learner corpora. In the current annotation practice of non-native speakers X  corpora, it is common to have ill-formed texts tagged by a single annotator, despite problems in reliability and evaluation. A general shift towards multiple annotation of learner corpora is imminent.

The issue of singly annotated learner texts, used as application training data, was raised for the first time by Tetreault and Chodorow ( 2008 ), who investigated native-speakers X  classification of prepositions usage. They concluded that two native annotators performing the task of tagging errors in prepositions on the same text reach at best an agreement level on the border between moderate and substantial (their kappa value was j = 0.63 X  X he metric is explained in Sect. 5.1 below). Rozovskaya and Roth ( 2010 ) also report low inter-annotator agreement ( j = 0.16 -0.40) for the task of classifying sentences written by ESL learners. Meurers ( 2009 ) also discusses the issue of verification of error annotation validity, viewing the lack of studies investigating inter-annotator agreement in the manual annotation of non-native speakers texts as a serious barrier for the development of annotation tools. 5.1 Inter-annotator agreement (IAA) The manual annotation of CzeSL was evaluated using the metric j (kappa, Cohen 1960 ), the standard measure of inter-annotator agreement, especially for tagged corpora. The values of j are within the interval [ -1, 1], where j = 1 means perfect agreement, j = 0 agreement equal to chance, and j =-1  X  X  X erfect X  X  disagreement.

The problem is to determine which error tags in one annotation correspond to which error tags in the other and how their scopes align. Tier 0, the original text, is shared by both annotations. However, annotators might use a different target hypothesis, and thus the higher tiers can differ. Moreover, they often differ not only in the shape of tokens but also in their number. Because of this, we project error tags to Tier 0 tokens and then calculate differences relative to that tier. When there are multiple tokens on Tier 0 corresponding to a token on the relevant tier, we project the tag on the first Tier 0 token only. 7
Table 5 summarizes the distribution of selected error tags for a pilot sample and for all doubly annotated texts available at the time of the evaluation. The first column gives the error tag; some tags (marked by an asterisk) are used only in the evaluation as a more general error category. 8 The column headed by  X  X vg tags X  gives the number of times the tag was used by an average annotator (calculated simply as the total for the two annotators divided by two).
 5.2 A pilot annotation Early in the project, we calculated IAA on a pilot sample. It consisted of 67 texts totalling 9848 tokens, most of them written by native speakers of Russian; the texts sample was corrected and assigned error tags according to the error taxonomy presented above in Sect. 4.4 by 14 annotators. They were split into two groups: Annotators A and Annotators B. Each group annotated the whole sample independently. On average each annotator processed 1,475 words in 11 texts. The annotator agreement is reported in Table 5 . 5.3 Full corpus Using the feedback gained from the pilot experiment we improved the annotation manual and the training of annotators. In a few cases we also slightly modified the error taxonomy. A substantially larger subset of the transcribed texts was annotated by 31 annotators in three groups specializing on Slavic, non-Slavic and Roma learners. 9 The evaluation was extended to all usable texts doubly-annotated so far, i.e. to 1,396 texts totalling 175,234 words.

As a result, the reliability of the annotation has generally improved X  X ee IAA for the whole doubly-annotated part of the corpus in Table 5 . At the same time we are aware that if two annotations differ, it does not necessarily mean one of them is wrong. Language, especially the language of non-native learners, is fuzzy and ambiguous and we do not intend to cover up this fact by providing instructions aimed solely at high IAA just for the sake of it.

For example, one annotator might perceive the word checkni in checkni moje str X nky  X  X heck my site X  to be a clearly non-Czech word (annotating it as fwNc ), while another would consider it as a colloquial form (annotating it as stylColl ). In such cases, the annotation manual might instruct the annotator to prefer a certain tag. However, even though this would lead to a higher IAA, it would conceal the fact that these expressions are perceived differently by different native speakers.
The table shows that on T1 the annotators tend to agree in the domain categories incor* and wbd* , i.e. for incorrect morphology and for improper word boundaries ( j &gt; 0.8 and j &gt; 0.6, respectively). IAA was lower ( j &lt; 0.4) for categories with a fuzzy interpretation, where a target hypothesis is difficult to establish, such as fw* category and its subcategories, used to tag attempts to coin a new Czech lexeme ( fwFab ), or foreign/unidentified strings of words ( fwNc ). Even the choice between the two subcategories was problematic as can be seen from Table 6 .

At T2 the annotators agree in agreement errors ( agr , j &gt; 0.6) and errors in expressing syntactic dependency ( dep , j * 0.6), and also in the well-defined category of errors in reflexive expressions ( rflx , j * 0.4). However, pronominal references ( ref ), secondary (consequent) errors ( sec ) and X  X urprisingly X  X lso errors in analytical verb forms/complex predicates ( vbx ) and negation ( neg ) show a very low level of IAA, even though they are identifiable by formal linguistic criteria. In all these four cases, the distribution of tags and the annotators X  feedback suggest that the annotation manual fails to provide enough guidance and formal criteria in distinguishing between the error types ref versus agr and ref versus dep (in either case the disagreement represents 19 % of all the inconsistent uses of the tag ref ).
IAA in the distribution of tags for usage and especially lexical errors is lower ( j &lt; 0.4). The usage of these tags is highly dependent on the annotator X  X  judgment, and the results are low as expected. An analysis has revealed that the tag lex has a systematic distribution: if the original lexeme and its  X  X deal X  emendation differ in their meaning distinctly, the annotators agree in their emendations in most of the cases (2); if the lexemes show semantic proximity, the annotators highly disagree in the emendation and therefore also in the consequent annotation (3).
 Tables 6 and 7 present a confusion matrix for T1 and T2 error tags, respectively. The  X ? X  column/row covers cases when there were multiple tags provided by either annotator and they did not include the relevant tag (so we know that the annotators disagreed, but we cannot say which tags correspond to which). Note that the totals might be larger than the sums of the respective row or column as the table shows counts for selected tags only. Thus we can see, for example, that in 8,989 cases the annotators agreed on the incorBase tag, but in 400 cases Annotator B used the incorInfl tag instead, far less common were cases when Annotator B assumed the error to be one of the fw* tags, finally in 574 cases Annotator B used multiple tags, but none of them was incorBase (so we cannot say which one of those corresponds to A X  X  incorBase tag).

From these tables, we can see that the annotators most commonly confused the following tags:  X  incorBase (error in stem) for incorInfl (error in inflection)  X  fwNc (foreign word) for incorBase (error in stem)  X  agr (agreement error) for dep (valency error), less frequently for lex (lexicon 5.4 Error tags depend on emendation Analysis of the tagged data (see Table 8 ) shows that the disagreement in using error tags is not necessarily caused by an annotator X  X  fault, but could rather be dependent on the choice of the emended form (the target hypothesis). For example, while agr has an overall agreement of 0.69, it is 0.82 for identical emendations, but only 0.24 if the target (T2) hypotheses are different. The situation of other tags is similar. See (4) for an example. However, sometimes annotators arrived to identical emendations, but still In some cases, this is manifested by different emendations on the lower tier, i.e. T1. For example, consider the expression in (5): both annotators corrected the non-existent word tezki to te  X  z  X  k X   X  X ifficult X , but they differ in their interpretation of the  X  X ifficult X  ( y and i have the same pronunciation in Czech), correcting it to the official te  X  z  X  k X  on the next layer. A2 interpreted tezky as simply as incorrect form, and between them without knowing more about the language of the speaker.
 In all these cases, tagging is correct vis-a ` -vis the selected emendation. Currently, we investigate the impact of emendation on error annotation at the individual tiers, but we can already support the requirement of explicit target interpretation in the annotation scheme (Lu  X  deling 2008 ). The scheme can thus be verified by the calculation of IAA in the distribution of the tags, depending on the final hypothesis (cf, i.a., Meurers 2009 ). 5.5 Outline of the possible causes of the annotators disagreement We can identify the following causes of the annotators X  disagreements: 1. Invalid or imprecise annotation scheme: Generally, the annotators X  disagreement 2. Insufficient screening and training of the annotators: The level of screening and 3. Different target hypotheses: Some annotations require a considerable amount of 6 Automatic extension of manual annotation So far, the annotation is largely a manual enterprise, quite demanding in terms of annotators X  time and expertise. We aim to shift much of the burden to automatic tools, either by aiding human annotators and/or following up on their work, or by processing the texts from scratch, without any human involvement. 10 Let us explore the former option first.

Manually emended and error-annotated text can be assigned additional information by automatic tools in the following two ways: 1. As far as the emended text approximates standard language, at least in 2. Some manually assigned error tags can be specified in more detail using formal
The tools for extending manual annotation can also be used to check its quality of manual annotation, especially to identify tags that are probably missing or incorrect (see Sect. 6.3 ). 6.1 Automatic addition of linguistic information Emended sentences at T2 can be tagged with morphosyntactic categories and Each word is assigned a lemma and a tag from a standard morphological tagset Hajic  X  ( 2004 ). Applying standard methods to T1, consisting of forms which may be correct only in isolation and which may also be wrongly ordered, can produce unreliable results. 11 Instead of a fully disambiguated tag and lemma, T1 is tagged using potentially ambiguous morphological analysis of isolated forms in combina-tion with the tag and lemma assigned at T2 as follows:  X  If the forms at both tiers are identical, the tag and lemma assigned at T2 is used.  X  If the forms are different, but their lemmas are identical, then that lemma and the  X  If the T1 form X  X  lemma is different from the lemma at T2, the T1 form receives all 6.2 Automatic extension and modification of error annotation Some error types can be detected automatically. This is especially true about formal errors at T1, identifiable by a simple comparison of the corresponding forms at T0 and T1. Errors at T2 are more difficult to classify automatically, thus only a limited number of phenomena are tagged this way.

The manually assigned T1 tags include the following three types of errors: wrong form ( incor ), incorrect word boundaries ( wbd ), and neologism or foreign word ( fw ). The automatically assigned  X  X ormal X  errors complement these manual tags as an additional dimension of annotation. For example, *chrozba / hrozba  X  X hreat X  is manually annotated as incorBase (the h/ch error is in the stem), and *kaz  X  d X cho / kaz  X  d X ho  X  X very masc . sg . gen / acc  X  X s incorInfl (the h/ch error is in the  X ho ending). However, in both cases, the correct h is incorrectly devoiced, thus the h/ch error is annotated as formVcd1 . 12
The formal T1 error tags express the way in which a T1 form differs from the original incorrect T0 form. Most of these tags (such as  X  X  X issing character X  X ,  X  X  X witch error X  X  or even  X  X  X rror in diacritics X  X ) only identify surface manifestations. However, assimilation or palatalisation. It is the possibility of their automatic detection that puts them in the same class with the truly formal error types.

Table 9 provides examples of some currently handled automatically assigned errors on T1. Some errors affect only spelling with no change in pronunciation affect pronunciation (vowel quantity, e epenthesis). Some errors might affect pronunciation in some contexts, but not others (writing i/y , the c/k substitution).
Most of the T2 error tags are assigned manually, because the variability of incorrect structures is too high to allow for reliable automatic error tagging. Thus, only limited amount of information is added automatically:  X  The reflexivity error tag ( rflx ) is added if another type of error concerns a  X  Manually assigned error tags for compound verb forms ( vbx ) are sub-divided as  X  Tags marking deleted and inserted words are added ( odd , miss ).  X  Word order corrections are tagged ( wo ). The annotator reorders the words as 6.3 Automatic annotation checking The system designed for automatic error tagging is also used for evaluating the quality of manual annotation, checking the result for tags that are probably missing or incorrect. For example, if a T0 form is not known to the morphological analyser, it is likely to be an incorrect word which should be emended. Also, if a word was emended and the change affects pronunciation, but no error tag was assigned, an incorBase or incorInfl error tag is probably missing. This approach cannot find all problems in emendation and error annotation, but provides a good approximate measure of the quality of annotation and draws the annotator X  X  attention to potential errors. 7 Fully automatic annotation Despite the benefits of annotators X  insight and judgment, manual annotation, or even manual annotation supplemented by automatic annotation, is tedious and costly. On the other hand, automatic tools are more error-prone and cannot produce the sort of sophisticated annotation envisaged in the present project. Aware of these pros and cons, we explore how far we can get without manual annotation. Due to the lack of methods targeting learner texts, we confronted some  X  X ative Czech X  tools (two taggers and a spell checker) with ill-formed input. 7.1 Automatic emendation One of the options to (partially) automate the task of emendation is to use a proofreading tool X  X  spell checker or a grammar checker. So far, we have experimented with Korektor (Richter 2010 ), a spell checker that has some functionalities of a grammar checker, using a combination of lexicon, morphology and a syntax model. 13
The tool was tested on a subset of the pilot set of annotated texts (see Sect. 5.2 ), produced by learners at intermediate or higher levels of proficiency, yet among the total 9,372 tokens (7,995 tokens excluding punctuation) 918 (10 %) were not recognised by the morphological analyser included in a Czech POS tagger (see Morc  X  e in Spoustova  X  et al. 2007 ). Even more forms were judged as faulty by the annotators: 1,189 (13 %) were corrected in the same way by both annotators at T1 and 1,519 (16 %) at T2.

Results of the spell checker were compared with those of the morphological analyser and with forms at T1 and T2, provided both annotators were in agreement. The spell checker was run in three (batch) modes: (i)  X  X  X utocorrect X  X  (as test the hypothesis that diacritics is a frequent source of errors.

Although the morphological analyser includes a guesser, it makes no attempt to correct an unknown word form, only guesses its morphosyntactic tag and lemma. The spell checker is deemed to be successful for a given form if the morphological analyser treats it as unknown and the spell checker suggests a correction, or if the analyser treats the form as known and the spell checker leaves it intact.

Table 10 shows figures for the morphological analyser. The rows give results for the three modes: autocorrect (i), diacritics for  X  X  X emove-diacritics X  X  followed by  X  X  X iacritics X  X  (ii), and autocorrect ? diacritics for the full sequence (iii). The column  X  X  X orrected X  X  gives the counts for forms corrected by the spell checker run in the relevant mode. The column  X  X  X nknown X  X  gives the number of cases where the morphological analyser happens to flag a form corrected by the spell checker as unknown. The results of the analyser are assumed as truth for the purpose of calculating precision ( X  X  X nknown X  X / X  X  X orrected X  X ) and recall ( X  X  X nknown X  X /918, the latter figure representing all forms unknown to analyser).

Precision is not really a fair measure here, because the analyser never flags forms which are correct in isolation but faulty in a context, while the spell checker often manages to use local context to replace a form X with an orthographically close but morphosyntactically quite different for Y: podl X  ? podle, jej X ch ? jejich, z  X  it ? z  X   X t, lib X  ? l X b X , ze ? z  X  e, div X  ? d X v X , drahy ? drahy  X  , mel ? me  X  l, j X ch ? jich, c  X  ine ? c  X   X   X  ne  X  . Interestingly, diacritics seem to represent a substantial share of problems in learners X  writings, and the preprocessing of the input by the diacritics remover and assigner (iii) means a significant improvement.

Corrections made by the annotators can be compared verbatim with those proposed by the spell checker. The spell checker scores whenever the form proposed by the relevant mode matches the form at T1 or T2, respectively. The two annotators must agree about the corrected form, only then it is seen as fit for comparison.
At T1 the total number of corrections (1,189) is higher than the number of forms unknown to the morphological analyser (918) because the annotators correct also misspellings which look like homographs with an existing form. Such faulty forms are never detected by the morphological analyser. As a result, recall of the spell checker is lower when its performance is compared with T1 than when with the results of the morphological analyser. Precision stays roughly the same as in the previous comparison because in one aspect T1 is similar to the analyser: it still largely abstracts from context. E.g. annotators are instructed to leave errors due to missed grammatical concord for T2. The data are shown in Table 11  X -the column  X  X  X orrected X  X  is identical to that in Table 10 , but the  X  X  X rong X  X  column shows the number of cases where the two annotators agree about an emended form, identical to the suggestion of the spell checker.

It is interesting to investigate cases where the spell checker does not agree with the annotators, but both the spell checker and the annotators indicate an error (170 such cases at T1 for the autocorrect ? diacritics mode). In some of these cases, the simple autocorrect mode without the diacritics component fares better (in 30 cases out of 170). It seems that removing and reassigning diacritics takes the spell-checker too far (Table 12 ). In some cases the T1 and T2 versions differ and none of the methods matches the contextually correct version of T2 ( pl X z  X  , l X pe ).

In 150 cases the spell checker suggests a correction when T1 prefers the original, but in 37 cases the spell checker agrees with an annotator at T2 (in 16 cases with both), which means that the real precision is higher. The rest of the cases are mostly inflectional issues, often due to misassigned diacritics, but also quite a few errors in the annotation (shared by both annotators).

T2 is problematic for evaluation in its own right. Some error types handled here are due to wrong word order, style, phraseology and a few other that go beyond simple spell checking, even in a broader sense of some degree of contextual sensitivity. The figures in Table 13 , otherwise similar to Table 11 , should be interpreted accordingly.

The two-stage annotation scheme suggests the option to distinguish corrections of forms that are wrong in any context, from those that could be correct in isolation, or in a different context, i.e. to test the grammar-checking capabilities of the spell checker. However, Korektor does not quite match the annotation scheme. It is only possible to find a few individual cases of successful corrections of missed agreement or case government (in the order of tens). Again, as in all the previous cases, the mode combining diacritics remover, assigner and proofreader is the best scenario.
The results seem to justify the option to integrate the spell checker into the annotation workflow, even though its suggestions may not quite match the two distinct tiers without tuning to the specific task and annotation scheme. We have already applied Korektor in the autocorrect mode to all transcribed texts in CzeSL , including the texts without any manual annotation. 14 7.2 Automatic error tagging For an experiment in automatic tagging we used two taggers, based on different concepts: Morc  X  e (Votrubec 2006 ) uses a morphological analyser, preferring lexical and morphological diagnostics over syntactic context, while TnT (Brants 2000 ) has the opposite strategy, relying on a lexicon extracted from training data. Both taggers were trained on the same tagset and include a method to handle unknown words. Because of the different strategies the taggers use to tag correct input, they respond differently to various types of deviations. A mutual comparison of their results is thus as interesting as their evaluation against gold standard, which X  X n the case of ill-formed input X  X s a difficult concept anyway.

Identifying all errors would involve comparing manual annotations at T2 form-by-form with the original text at T0. In the current absence of such data, we used data obtained from the easier task of comparing T0 to T1, where all erroneous forms are emended to a closest correct version, disregarding context.

Table 14 presents data extracted from a sample of 93 texts including 12,681 word tokens, with 1,323 tokens (8.9 %) identified as ill-formed by the morphological analyser. The two taggers agreed on the same tag in 405 cases, i.e. in 28.8 % of the total of ill-formed tokens, and disagreed in 918 cases (71.2 %). The figures are additionally split by 12 morphological categories constituting the tag. Column 1 (T0m 9 T0t) shows in which categories the two taggers disagree at T0 for the 918 tokens, where their tags do not match at least in one category. Agreement is significantly lower between categories largely determined by syntactic context (POS, Gender, Number, Case) as opposed to those determined lexically. Columns 2 (T0m 9 T1) and 3 (T0t 9 T1) show agreement rates of tags assigned by Morc  X  e and TnT , respectively, to all tokens at T0 15 in comparison with tags assigned by Morc  X  e to the corresponding tokens at T1. 16 Morc  X  e shows better results overall and in most categories. Columns 4 and 5 show agreement rates for an ill-formed subset of the sample used in Columns 2 and 3. Interestingly, TnT shows significantly better results, except in the categories of Person and Tense.
 The difference between the two taggers is also reflected in the share of different POS categories assigned to ill-formed words. Table 15 shows that Morc  X  e has a more even distribution, but strongly disprefers all verbal categories.

To sum up, the comparison of the two taggers confirms the assumption that the differences in their strategies will have a significant effect on the interpretation of faulty forms. A more general observation concerns the comparison of the success rate of the two taggers on the ill-formed input: TnT loses ground in a context with many errors but outperforms Morc  X  e on faulty forms, while Morc  X  e strongly disprefers verbs and works better in general. 8 Conclusion We described a corpus of Czech texts produced by non-native learners of Czech, focussing on error annotation. Results of its evaluation show fair inter-annotator agreement. We also explored and implemented some options of partially or even fully automating the annotation of learner texts.

It is no simple task to design an annotation scheme for a learner corpus and to maintain consistency in the annotated texts, both in a way that would reflect most demands of the corpus users. One of the main reasons is that annotating learner texts tends to be a highly specific enterprise, and even seemingly similar projects do not offer enough guidance X  X olutions are often too specific to a language or to the project concept and user requirements. On the other hand, annotation itself is quite rewarding due to the plentiful feedback from the annotators about all aspects of the task and, of course, about the learners X  interlanguage.

More specifically, our experience shows that the rules for tagging morphosyn-tactic errors are relatively easy to formalise and it is thus possible to obtain a high inter-annotator agreement for them. However, we were unable to obtain a similarly robust annotation of semantic errors, which are much more dependent on subjective judgement. It is even unclear whether it is desirable to aim to standardize their annotation. Obviously, we should aim to prevent and correct differences that are clear mistakes. Some of it can be done by a better selection of annotators, some by clearer instructions and some by providing better tools to annotators. For example, we have seen less errors in the annotation of incorBase and incorInfl errors after integrating a spell-checker into the annotation tool X  X ome of these annotation errors were simply due to annotators overlooking the incorrect word.

The pilot study, where two POS taggers and a spell checker were applied to ill-formed input, confirmed the viability of a partially or even fully automatic annotation as an alternative to manual-only annotation, especially when the demand for large data is higher than concerns about the error rate. It remains to be seen to what extent the comparison of results of multiple taggers, based on different tagging strategies, can lead to usable interpretations of faulty forms.
 References
