 Ira Assent  X  Ralph Krieger  X  Boris Glavic  X  Thomas Seidl Abstract Many environmental, scientific, technical or medical database applications require effective and efficient mining of time series, sequences or trajectories of measure-ments taken at different time points and positions forming large temporal or spatial databases. Particularly the analysis of concurrent and multidimensional sequences poses new challenges in finding clusters of arbitrary length and varying number of attributes. We present a novel algorithm capable of finding parallel clusters in different subspaces and demonstrate our improvements.
 Keywords Data mining  X  Clustering  X  Spatial and temporal data  X  Multidimensional sequences 1 Introduction Environmental sensors produce data streams at successive time points which are often archived for further analysis. Applications like stock market analysis or weather stations gather ordered sequences of different values in large temporal databases. Weather stations for example use multiple sensors to measure, e.g., barometric pressure, temperature, humidity, rainfall. Many other scientific research fields like observatories and seismogra-phic stations archive similar spatial or spatial-temporal sequences.
 As one application example, we focus on hydrological data. In a current project of the European Union on renaturation of rivers, the structural quality of river segments is analyzed. For a spatial database of German rivers, about 120.000 one-hundred-meter segments were evaluated according to 19 different structural criteria, e.g., quality of the riverbed [ 20 ]. They were mapped to quality categories, where a value of  X  X ne X  indicates perfect quality, while a value of  X  X even X  indicates most severe damages. Figure 1 illustrates 8 of the 19 attributes of a sample river segment in GIS (geographic information system) representation. The sequence order of the segments is given by the flowing direction of the rivers.

As the project aims at a quality improvement over the next decades, packages of measures have been suggested for different structural damages. They have been formalized in rules specifying the attribute value constraints and the attributes influenced positively by execution of the respective measure. An example constraint might be that a certain segment has good quality (categories one to three) riverbed and riverbanks and poor river bending (categories five to seven). This could be improved by measures like adding deadwood to positively influence river bending.

Finding and analyzing these patterns helps hydrologists summarize the overall state of rivers, give compact representations of typical situations and review the extent to which these situations are covered by measures envisioned. They can identify those patterns which are problematic, i.e., have low quality ratings, but are not yet covered by measures. In a follow-up step, these measures are annotated by time and cost information. This is used to generate an overview over the state of rivers as it might be in the near future if the measures are put into action.

From a computer science point of view, finding the intrinsic structure of these multidi-mensional sequences is a twofold task:  X  detect frequent patterns within sequences for all possible subsequence lengths (note that  X  then detect parallel occurrences of these patterns.

Patterns are ranges of values (which correspond to several categories of river quality structure) found in several (sub-)sequences. Pattern analysis has to take into account that the data is subjective and fuzzy, because structural quality of rivers was mapped by different individuals. Our approach is based on weighting by kernel densities in a density-based cluste-ring approach. This effectively detects fuzzy sequence patterns. These patterns are clustered efficiently for arbitrary lengths using monotonicity properties. We transform these sequence pattern clusters into a cluster position space such that mining parallel patterns can be reduced to efficient FP-tree frequent itemset mining.

This paper is organized as follows: we review related work in Sect. 2 . Basic definitions in Sect. 3 lay a sound foundation for the proposed clustering method in Sect. 4 .Wedes-cribe and disucss our algorithmic concept in Sect. 5 . The experimental evaluation in Sect. 6 demonstrates the effectiveness of our approach on real world and synthetic datasets. Efficiency is shown and parametrization is evaluated. We conclude our paper in Sect. 7 , summarizing our results and anticipating future work. 2 Related work Numerous clustering methods have been proposed in the literature, including partitioning clustering, e.g., the well-known k-means algorithm [ 21 ]. These algorithms require the spe-cification of the number of clusters to be found and can only detect convex cluster regions. Categorical clustering methods work well for categorical data where the notion of neighbo-rhood is not meaningful [ 12 , 23 ].

Density-based algorithms use a function to determine the density of the neighborhood of Density-based clustering is robust to noise since it clusters only those points or sequences above some noise threshold as discussed in [ 2 , 7 ]. Moreover, it naturally incorporates neigh-boring objects into its cluster definition.

The analysis of sequence data has recently gained a lot of attention. Recordings of data at successive time points have been studied in time series mining; e.g. [ 9 , 18 ]. Most of these approaches, however, aim at finding patterns of values which do not have to directly follow one another, but may have other values in between, as in sequential frequent itemset mining [ 1 , 3 ].

Motif mining searches those patterns which have the highest count of similar subsequences [ 6 , 22 ]. Matching within some range is used to determine the frequency. However, neighbors are not weighted and the range is fixed for all sequences. Moreover, parallel patterns are not discussed since the application targeted is one-dimensional time series. While noise is removed in motif discovery as well, we found density-based clustering to be more useful in handling fuzzy data, because density-based clusters automatically adapt to different ranges of fuzziness. 3 Cluster model In this section we formalize our notion of patterns in subsequences. We define a suitable cluster notion which reflects that our database consists of ordinal-valued sequences with some degree of noise.
Density-based clustering, which tries to separate dense areas (clusters) from sparse ones (noise) reflects the requirements of applications such as the river data scenario in that arbitrary cluster shapes may be found, the number of clusters does not need to be fixed a priori and noise is labeled as such, i.e., it does not have to be assigned to any cluster [ 8 , 16 ]. 3.1 Subsequence patterns and clusters in one attribute As noted before, it is crucial to determine subsequence clusters of arbitrary sequence lengths. We define sequence patterns of arbitrary length in single attributes and subsequently model parallelism between these clusters.
 Definition 3.1 Sequence pattern:  X  A tupel S = ( s 1 ,..., s k ) of k subsequent values at positions 1 through k is called a  X  A database DB is a set of sequences { S 1 ,..., S z } .  X  We denote a subsequence of S from position i to j by S [ i , j ]= ( s i ,..., s j ) .  X  Whenever we are not interested in the concrete positions of a sequence, but merely in its  X  A pattern P occurs in a database if there is a sequence S  X  DB and a position i  X  N with  X  X he support of a pattern P is the number of its occurrences in the sequences of the
When searching for prevailing patterns, it is important to notice that merely counting of sequence patterns is not sufficient in many scenarios. It is crucial to account for two factors: first, mapping of river structures may be blurred by people X  X  subjective decisions on unclear category boundaries. Second, measures and their constraints may be applicable over several categories and cannot always be fitted exactly to these categorical boundaries. Moreover, small deviations in few segments may be tolerable for the application of a certain measure if this results in longer river courses treatable by a single measure. Hydrologists are thus interested in including  X  X imilar X  sequences in frequency notions.
 Density-based clustering tries to separate dense areas (clusters) from sparse ones (noise). The density of a pattern is determined by evaluating its neighborhood according to some distance function.

Any L p -Norm L p ( Q , Q ) = p k i = 1 ( q i  X  q i ) p between patterns Q and Q can be used, yet the Manhattan norm ( L 1 ) has shown to work well in preliminary experiments. We use a weighting function to ensure that with greater distance to the pattern evaluated, the influence of neighbors decreases. Any series of monotonously decreasing values can be used as a weighting function, since distances between nominal sequences are always discrete. All kernel-estimators known from statistics [ 14 ] are constantly falling functions. Experiments have shown that weighting functions based on Gaussian kernels ( W P  X  ( Q ) = exp (  X  dist ( P , Q ) 2 / 2  X  2 ) ) perform well in many applications. Using weighting functions sequences to be included in the density evaluation. Whenever the weights assigned drop below a certain significance threshold  X  , these sequences should not be considered in the density estimation. For example, Gaussian kernels assign (possibly very small) density values to all patterns in the database, which can be cut off below some tiny value, such as  X  = 0 . 01 or less. This way, excess density computations can be avoided. Definition 3.2 Neighborhood and density:  X  X he  X  -neighborhood of a pattern in one attribute P , N  X  ( P ) , is defined as the set of all  X  X he density of P is the weighted sum of patterns in the neighborhood  X  P is dense with respect to a density threshold  X  iff density ( P )  X   X  .

We are now ready to formalize our cluster notion. As mentioned before, clusters should consist of similar, dense sequences of the same length. Sequences within one cluster should therefore be within certain parameterizable boundaries.
 Definition 3.3 Cluster:
Aset C ={ P 1 ,..., P m } of m patterns P i is a cluster of length k with respect to a density threshold  X  and a compactness parameter  X  iff:  X  for all patterns Q of length k not in C : C  X  X  Q } is not a cluster ( Maximality )  X  for all patterns P i : density ( P i )  X   X  ( Density )  X  for any patterns P i , P j  X  C there is a chain of patterns ( Q 1 ,..., Q v )  X  C such that Put informally, we are thus looking for clusters which are as large as possible (maximality), whose elements are all dense (density) and at most  X  apart from each other (compactness). We s e t  X  to one in categorical settings.
 Example Figure 2 illustrates the definition of density and gives an example for a density calculation of pattern P = 1 , 2 . The upper part visualizes two sequences S and T with two exemplary attributes, the riverbed and the bank. In the lower part of the figure the density-value for a pattern 1 , 2 is calculated. We assume a significance threshold of  X  = 0 . 2. The Gaussian weighting function drops below  X  = 0 . 2 for sequences having a distance higher than 1 . 8 from the point evaluated. Thus in our ordinal setting only sequences with a distances In our example the  X  -neighborhood of pattern 1 , 2 contains the sequence 1 , 2 itself starting 2  X  1 + 4  X  0 . 6 = 4 . 4. For a density-threshold of e.g.  X  = 3 the pattern 1 , 2 is considered dense. 3.2 Multiclusters in parallel patterns Single sequence patterns give insight into the inherent structure in individual attributes. In multidimensional sequence databases these patterns have to be extended to parallel patterns. River measures may affect several structural properties, e.g. the river bank on the left and the right as well as the river bending. Similarly, constraints are often formulated for several attributes as well. Likewise, in other applications, situations which require specific measures are typically described via several sensor values.

We define multiclusters as frequent parallel occurrences of single attribute clusters. Fre-quency is measured in terms of simultaneous occurrences, i.e. the number of positions in any sequence, where clusters are detected in different attributes. We formalize our notion of parallel clusters as follows: Definition 3.4 Multicluster :
A set of sequence clusters C 1 ,..., C n of length k from n different attributes is a multi-cluster MC iff:  X  C 1 ,..., C n are parallel at some position i,  X  C 1 ,..., C n occur frequently together,
Put informally, we are thus looking for those positions which show a pattern contained in each of the clusters (Parallelism), which have a count equal to or greater than the threshold given (Frequency).

The frequency threshold  X  reflects the number of positions where the multicluster is detec-ted. It can be set in relation to the database size, i.e., the overall number of multidimensional Cluster per attribute ( = 3; = 1) C 1 = { 1,2 , 1,3 } in Riverbed C 2 = { ,4 } in Bank Multicluster ( =2) MC = {C 1 ,C 2 } (parallel three times) sequence segments (see Sect. 6 ). Hence,  X  corresponds to the relative frequency of a multiclus-ter (the support) and is a key parameter depending on the application. In general, increasing  X  decreases the number of identified multiclusters, as more occurrences of the pattern are required. Multiclusters detected using a higher  X  are far more typical for the data set. Our experiments suggest that an initial setting of about 1% of the data set serves as a good starting point for analysis. As more or less patterns are desired, the threshold is adapted accordingly. Example In Fig. 3 we present a multicluster in two attributes. The multicluster MC consists of two clusters C 1 ={ 1 , 2 , 1 , 3 } in the riverbed attribute and C 2 ={ 3 , 4 } in the bank of  X  = 2, MC is a multicluster. 4 Efficient cluster mining To detect clusters of arbitrary length, a naive approach might be to simply re-run a density-based clustering algorithm for each length value to detect all possible clusterings. Obviously, this leaves room for efficiency improvement. 4.1 Monotonicity We avoid excess clustering steps by exploiting monotonicity properties of clusters. We show that patterns which are not dense, cannot be part of longer dense patterns. We may safely prune them from consideration in longer pattern clustering.
We formalize this monotonicity first for patterns and then prove that this property holds for clusters themselves.
 Theorem 4.1 Density monotonicity : For any two patterns P , Q of length k and their respective prefix/suffix P , Q of length k  X  1 holds: (1) Q  X  N  X  ( P )  X  Q  X  N  X  ( P ) (2) density ( P )  X  density ( P )
For (1) we note that an L p norm, p  X  1 is the p-root of sum of absolute differences in sequence values. This means that a reduced sum of k  X  1 of these differences is necessarily smaller than or at most equal to a sum of all k differences.
 Proof For a prefix/suffix Q , P of Q , P we first show: dist ( P , Q )  X  dist ( P , Q ) . Dropping the first or last summand in our distance sum for Q , P , we obtain for the prefixes or suffixes Q , P :
Thus the distance between two patterns is greater than the distance between the respective prefix or suffix. Using this fact we can prove (1):
Part (2) is proven using a similar argument: the density is defined as a weighted sum of the support of patterns. Their shorter counterparts, prefix or suffix, are assigned smaller distance values (part 1) and therefore larger weights.
Since density and neighborship are monotonous, we can conclude that clusters are mono-tonous as well: Theorem 4.2 Cluster monotonicity : For any cluster C of length k, there is a cluster C of length k  X  1 such that for any pattern P and its prefix/suffix P holds: Proof For any P in C ,wehavethat P is dense (Lemma 4.1 ), and since the distance of shorter patterns is smaller (Proof of Lemma 4.1 ), there exists a chain of prefix/suffix patterns which guarantees compactness. This means that all prefixes/suffixes are part of a cluster (which might contain additional patterns due to the third requirement, completeness).

Summing up, we know that any pattern or cluster which does not satisfy the density criterion, can be safely pruned from the search for longer patterns or clusters. 4.2 Indexing subsequence patterns and clusters neighborhood of a subsequence. We introduce an index structure for patterns which supports neighborhood queries and density estimators.

Since existing index structures do not work for patterns of different length, the hierarchical index structure illustrated in Fig. 4 was developed. The index structure is tailored to the MC algorithm in that the bottom-up approach is supported: queried patterns always grow in length.

The index is constructed by scanning once over each sequence. Each pattern of a fixed starting length determined is added to the index structure. A pattern is represented by a path of labeled nodes from the root to a leaf. To later determine the density value of a pattern the support is annotated at each corresponding leaf node. Further on the index structure stores is stored at each leaf node (Fig. 4 ). The MC algorithm uses this flag to efficiently calculate the transitive closure (compactness in Definition 3.3 ) for a dense pattern. The density value for a pattern can be calculated by summing up and weighting the support of all patterns in the corresponding neighborhood. The index structure efficiently supports neighborhood queries by selecting the appropriate node ranges while descending the tree.
 Example In Fig. 4 we assume a parameter setting of  X  = 0 . 2 with a weight of 1 for patterns having distance zero and a weight of 0 . 3 for patterns having distance one. The density value the node range ( X 1 X   X   X 2 X ) must be considered. When processing node  X 2 X , for instance, all patterns containing this node have a distance of at least one (distance from 1 , 3 to 2 ,  X  is greater than or equal to one). Since the maximal distance according to  X  is bounded by 1, the only pattern which must be considered below node  X 2 X  is the pattern 2 , 3 .Inthesameway all other patterns within the specified range can be determined (in this example 1 , 2 , 1 , 3 and 2 , 3 ). Finally the density value for the pattern 1 , 3 can be calculated by summing up the support of the leafs for each weighted subsequence: 4.3 Clusters of arbitrary length To identify clusters of arbitrary length the MC algorithm works bottom-up by first identifying short clusters and then successively searching for longer clusters. This section proposes a method to generate new longer cluster candidates by combining appropriate shorter clusters.
The MC algorithm elongates the investigated patterns by one in each step. Thus the index structure must be able to calculate the position list and support for constantly growing patterns. This is achieved by the algorithm: the position list for a pattern P of length k can be calculated prefix starts at exactly its starting position and its suffix ends where P ends. This means that no extra database scans are necessary to determine its occurrence X  X e simply take a look at all the starting positions of its prefix and determine its intersection with its suffix shifted by one. Since both are shorter by one, they must have been processed earlier.
 Example Figure 4 illustrates this algorithm for the pattern P = 3 , 2 , 1 . Its prefix is the pattern 3 , 2 , its suffix 2 , 1 . We can compute the positions of pattern P simply by shifting prefix 3 , 2 occurs in positions T 3 , T 6 , while its suffix 2 , 1 is found at starting positions S , S 7 , T 1 , T 4 . Any sequence elongated by one which contains this suffix has to start one {
S illustration of T .

As we can see, patterns are efficiently extended on the fly when a longer pattern is accessed for the first time. 5 Multicluster algorithm Our Multicluster algorithm exploits both monotonicity properties and density computation on the index. Working in two steps, each monotonicity property on subsequence patterns and clusters are used. The first step searches for clusters of arbitrary length while the second step combines parallel clusters. 5.1 Step one: subsequence clustering The MC algorithm is presented in Fig. 5 . First, the index structure is created using a parameter length start (can be set to one to mine all patterns where desired). After the construction step the index structure contains one entry for each pattern of length length start .Nextthe first cluster candidate is generated. The first candidate contains all patterns, since all of them might be dense and hence could belong to a cluster. A depth first search on the index structure efficiently retrieves all different patterns stored in the database (method queryAllEntries ).
Next the clustering loop starts which discovers all clusters from length start to length max satisfy the density property. Each unclassified dense pattern is then expanded to a cluster by the method ExpandToCluster . This method creates a new cluster and assigns each dense pattern within the transitive closure (w.r.t.  X  ) to this new cluster.

Neighborhood queries are necessary to determine the density value (method isDense ) and the transitive closure of a subsequence (method ExpandToCluster ). Since all patterns of shorter length are no longer necessary after each clustering step the position lists and flags stored for these patterns can then be released (method prune ).

After a set of clusters is discovered for a specific length the cluster set is stored in the result set clust Set Result and new cluster candidates ( candi dateSet ) are determined by using the This step utilizes the monotonicity property of subsequence patterns in Lemma 4.2 : only subsequence patterns whose prefix and suffix are member of an already discovered cluster (and hence are dense) must be considered as members of a new cluster candidate.
The algorithm to calculate the corresponding cluster candidates is based on the discovered clusters of the previous step (Fig. 6 ). The method CreateCandidateCluster loops over all is extracted from each pattern and all patterns which start with the extracted suffix are queried from the index ( prefixQuery ). Each queried pattern which satisfies the density property is then used to create an elongated pattern by concatenating the appropriate prefix and suffix. These queries are also supported by the proposed index structure. The elongated patterns are finally assigned to a new cluster candidate.
 Example Consider a cluster containing only one pattern P = 5 , 1 , 3 , 7 . This cluster of length 4 is extended to a cluster candidate of length 5 in the following way: First a query using the suffix of P , 1 , 3 , 7 , is performed. We assume that the intersection with all possible whether they are dense or not. This can be achieved by simply evaluating the annotated cluster flag (a dense pattern must belong to a cluster). Let X  X  assume 1 , 3 , 7 , 9 is dense and 1 , 3 , 7 , 3 is not dense. In this case the two patterns 5 , 1 , 3 , 7 and 1 , 3 , 7 , 9 are used to candidate and returned to the MC clustering algorithm. 5.2 Step two: multiclustering Having discovered dense patterns and combined them to clusters, we identify those clusters which are parallel in the dataset (see Definition 3.4 ). Since for any cluster of length k all corresponding clusters of length k  X  1 have previously been detected, we use the monotonicity property presented in Lemma 4.2 .

An important property of multiclusters is that a subsequence of a specific length may belong to no more than one cluster. Hence any position in a sequence is the starting point for at most one density based cluster of a fixed length. This property is used to transform transformation it is possible to discover multiclusters by efficient frequent itemset mining techniques.

We use the following representation to apply the frequent itemset mining: clusters starting at the same position in different attributes are combined to one itemset. The clusters are identified by their cluster-ids. A frequent itemset contains often occurring multiclusters in which each cluster belongs to a different attribute.
 Example Figure 7 illustrates the transformation process. In the upper part, three attributes of sequence S are shown, with clusters highlighted in gray colors. Assumed are the following are transformed according to their positions as follows: Position 1 in the sequence S would clusters, position three only cluster C 1 and so on (lower part of Fig. 7 ). We can therefore immediately derive the frequency of itemsets from the cluster information: the itemset { C 1 } occurs six times, { C 2 } four times, ... , and finally { C 1 , C 2 , C 3 } two times.
We use the Frequent Pattern (FP) growth algorithm proposed by Han et al. [ 13 ]forthe extraction of frequent itemsets. The tree representation is very suitable for our task since database scans are avoided. Recall that the FP-tree builds a path annotated by support values for all frequent items. To obtain frequent itemsets conditional FP-trees are constructed iterati-vely for each frequent item. As mentioned above we use the following representation to apply the FP-tree: one dimensional subsequence clusters starting at a certain position are itemsets patterns w.r.t.  X  . A frequent itemset contains often occurring correlated clusters in which each subsequence cluster belongs to a different dimension. Thus the result of the FP-tree algorithm contains multiclusters as described in Definition 3.4 . In order to find multiclusters of any length the FP-tree algorithm is started for all different lengths for which clusters have been found. Since the FP-tree works extremely fast, clustering arbitrary lengths is efficient. 5.3 Analytical evaluation Subspace clustering is a highly complex task, as the number of subspaces is exponential in the number of attributes. Further on, detecting multiclusters of arbitrary length is quadratic in the sequence length. Consequently, multicluster detection is a challenging problem. To ensure efficiency, our algorithmic approach therefore bears on each of these aspects.
First, concerning arbitrary lengths of possible patterns, our index structure avoids re-building potentially frequent patterns from scratch. Only frequent patterns, not the actual sequences nor the infrequent patterns, are actually processed when elongating clus-ters. For each of these patterns, compact representations are stored and position lists are kept on hard disk to reduce main memory usage. Second, only subspaces which contain clusters in single attributes are mined for multiclusters. This greatly reduces the number of poten-tial combinations. Moreover, by mapping sequences to cluster ids, the FP-growth algorithm allows for fast detection of multiclusters. Memory requirements could be further reduced by applying a secondary storage extension of the FP-tree method as suggested e.g. in [ 11 ].
We validate this analysis in the experiments which show that our algorithm scales almost linearly in terms of both length and numbers of attributes on different data sets. 6 Experimental evaluation We ran experiments on synthetic and real world data to evaluate the quality and performance. Synthetic data is used to evaluate different cluster parameters and to develop guidelines and heuristics for supporting users in setting up parameters. It also evidences the algorithm X  X  scalability as well as its quality in detecting all generated clusters. Real world data demons-trates the usefulness of the results for domain experts. Our implementation is in Java, and experiments were run on Pentium 4 machines with 2.4 Ghz and 1 GB main memory. 6.1 Parameter setup Several parameters can be used to tune our algorithm to domain specific needs. We therefore develop appropriate guidelines and heuristics for choosing reasonable parameter settings. To study the effect of different parameter setups on the clustering result we generated dif-ferent synthetic data sets. For each data set we varied the number of sequence clusters and multiclusters contained in the data.

As suggested in Sect. 3 we use a Gaussian kernel as a weighting function. Thus the first parameter to set is the value for  X  . We propose using a density plot to determine the effect of different  X  values on the density for length two patterns. The data set used to demonstrate this heuristic is eight dimensional and contains four multiclusters. Figure 8 illustrates the density plot for one attribute of the synthetic dataset using  X  = 0 . 75, 0 . 9, and 1 . 0, respectively. Four separate clusters can be seen, which mainly consist of patterns of similar values (on the diagonal). Decreasing the value of  X  corresponds to splitting the rightmost cluster around coordinates (16,16) into two separate clusters (leftmost illustration). As this would result in two clusters which would be a lot less frequent and distinct from the remaining three clusters, splitting is considered inappropriate and a higher value for  X  should be chosen. On the other hand, further increasing the value of  X  corresponds to merging the two clusters at the center into a single cluster (rightmost illustration). This would create a larger and more frequent cluster than the remaining two. Hence, a lower value for  X  is needed to separate these clusters.
These plots, as well as further experiments, suggests a choice for  X  of about one. This allows separation of clusters as well as aggregation of similar values. A choice of one for  X  also reflects the fact that a deviation in one value is generally deemed tolerable in many ordinal settings. An appropriate minimum density value can also be derived from the density plot. The ordinate value which separates the clusters from the  X  X oise floor X  can be visually determined and extrapolated to longer patterns. This leads to appropriate values for the density threshold  X  (e.g., for this dataset  X  = 10). Note that the density plot can be created easily during the initialization of the index structure.

The remaining parameters can be determined in a straightforward manner.  X  is set to very small values (0.01 or less) to cut off insignificant density values. Similar patterns should be clustered together which leads to a compactness threshold of one (  X  = 1) in ordinal settings like the river dataset. Finally, experts are only interested in multiclusters which cover a significant part of the dataset (e.g., one percent minimum frequency). Figure 9 summarizes our heuristics and parameter settings for the experiments.

Using this parameter setting the MC algorithm indeed finds all multiclusters hidden in the synthetic dataset. This first experiment demonstrates the effectiveness of the MC algorithm and indicates that the developed heuristics help users find reasonable parameters. 6.2 Synthetic data As mentioned in Sect. 4.1 , a naive approach for detecting clusters of arbitrary length would be to re-run a density-based subspace clustering algorithm for all possible lengths. We compare the performance and the results of the MC algorithm with SUBCLU [ 16 ], an extension of the density-based DBSCAN [ 8 ] algorithm to subspace clustering. Since SUBCLU was not developed to cluster subsequences we had to extend the original implementation by the density notion presented in Sect. 3 .

To evaluate the scalability of our MC algorithm in comparison with existing approaches, we varied the length of the data sequences and the number of sequences contained in the data base. Additionally we investigated the influence of the number of different labels per sequence domain on the performance of the MC algorithm. For this purpose we implemented a data generator which creates density connected clusters in each sequence. The data generator gets the number of clusters, and the size and length of the data base as input parameters. To parameterize the position and size of a cluster, a starting sequence and the number of pattern belonging to the cluster is specified for each cluster. To generate multiclusters some of these clusters are correlated to parallel multiclusters. All values not belonging to a sequence cluster are uniformly distributed based on the size of the corresponding domain to generate noise.
In our first experiment we generated a data set consisting of eight attributes with twenty different labels. We hid three to six clusters of length five to ten in each sequence. Each hidden multicluster has a minimum frequency of one percent and consists of three to six patterns. Figure 10 illustrates the runtime of both algorithms. Note that MC is faster by an order of magnitude. The runtime of both algorithms depends on the number of subsequences belonging to a cluster. Since longer subsequences are less often dense the time to investigate longer multiclusters is nearly constant. As far as the quality of the result is concerned, both algorithms discovered the main patterns of the six multiclusters hidden in the database.
In our second experiment we used the same hidden clusters as in our first experiment but varied the number of sequences contained in the data base. Figure 11 shows the result for four different sequence lengths (from 30,000 to 150,000). Even in 48 attributes the MC algorithm is capable to find multiclusters in reasonable time. Once again, we have only slightly more than linear behavior for increasing number of sequences.

To evaluate the scalability in terms of sequence lengths we again used the eight-dimensional data set from our first experiment and recorded the time for sequence lengths from 100,000 to 500,000. Additionally three different domains were used, depicted in Fig. 12 as separate lines for 10, 20 and 40 ordinal values per sequence attribute. In order to keep the experiments comparable we adjusted  X  using our density plot heuristics from 0 . 6 , 1 . 2 , 2 . 4, respectively. The MC algorithm shows linear behavior for 10 labels and slightly superli-near behavior for 20 and 40 labels. Even the largest setup with 40 different categories and 500 , 000 sequence segments takes less than 2 , 800 s. This means that our algorithm is capable of handling very large databases with large domains. 6.3 Real world data world datasets. Our analysis mainly focuses on the flowing water renaturation project of the European Union mentioned before. Additionally, we investigate the effectiveness of the MC algorithm on temporal data sets, using data from the  X  X ational Fire Danger Rating Retrieval System X . 6.3.1 River data The river data set consists of 120,000 river segments describing the structural properties of a river, such as the structure of the riverbed. Experts working on renaturing rivers are interested in finding co-occurring clusters to determine those subsequence patterns which occur repeatedly in the river database and which allow to derive broad potential improvement through measures designated. As a final result, experts should be capable of summarizing river segments into measure packages, ranking them according to the quality enhancements expected. This can then be used to create supra-regional schedules for political decision-making.

For the river dataset a value for  X  between 0 . 7and0 . 85 has shaped up as a reasonable parameter. The MC algorithm identifies more than a thousand clusters of length four by using avaluefor  X  of 0 . 7 with a corresponding value for  X  . Many of those clusters do not show when mining clusters of length five. By using a higher value for  X  more patterns are considered similar and cluster count drops between lengths 5 and 6. For this dataset experiments have shown that independent of the  X  value, multiclusters in many attributes can be found only at length 4 X 6. Beyond this length, parallel patterns are rare. This an interesting result for the hydrologists studying this data. They find that they should develop sensible packages of measures in this range and estimate costs for packages of about 500 m river improvement.
Figure 13 illustrates the runtime of the MC algorithm for phase one (searching for clusters of arbitrary length) and for phase two (searching for multiclusters) separately as well as for both. As we can see, the time requirements for mining all clusters of arbitrary length are distributed rather evenly between the two phases. The total time for mining multiclusters of arbitrary pattern length demonstrates the efficiency of our approach. Note that with increasing maximal length, few additional dense patterns are detected such that the increase in time consumption slows down. The steepest ascent is for lengths of up to 6, which corresponds to our result findings.
 Similar to the synthetic dataset, we also applied SUBCLU on this real world dataset. However, even the first iteration of SUBCLU for multiclusters of length 10 did not finished after 10 h. One reason why MC works extremely faster on the investigated dataset than SUBCLU are the time consuming neighborhood queries on the nineteen-dimensional data points. Even the use of index structures like the R-Tree does not speed up these neighbo-rhood queries in these high dimensionalities. Another reason is the efficient combination of dimensions using an FP-tree as done by MC.

For hydrologists to see how the detected multiclusters are distributed and check in which areas the designed measures are applicable or where additional measure engineering is required, we visualize multiclusters in a geographical information system. An example for a cluster visualization is given in Fig. 14 . Those river segments which are part of the cluster are marked by dark lines. The corresponding cluster summary is visualized on the left side. It indicates the cluster length of five river sections (top X  X ottom) as well as the cluster range of ten out of nineteen attributes (left X  X ight).

Additional tools for hydrologists give detailed information beyond this summary. Experts may browse clusters for an overview of the attributes contained in clusters, their value dis-tributions as well as their occurrence in the rivers. Moreover, individual attributes and river values may be checked for containment in clusters. By joining this information with the packages of measures designed, field experts can easily identify areas which are not yet met by measures.

Annotating the measures with cost and duration information, political decision making is supported. Hydrologists used the information derived from these clusters to build a decision support system [ 4 ] that gives concise summaries as well as detailed inspection of the state of the rivers as it is now as well as a prognosis for future development depending on the packages of measures chosen. 6.3.2 Weather data The second real world dataset is retrieved from the  X  X ational Fire Danger Rating Retrieval System X  which provides senor data from various weather stations. We use hourly weather variables from 1998 to 2005 about temperature, relative humidity, wind speed and direction, weather status etc. Overall the dataset contains fifteen variables measured at 27 , 048 time points [ 10 ]. This weather dataset contains a mixture of real valued and categorical variables. The continuous attributes are transformed into ordered categories using the transformation technique presented in [ 19 ]. After normalization we used ten symbols for the quantization.
The weather dataset allows us to determine the quality of the MC algorithm. Based on the weather variables the  X  X ational Fire Danger Rating System X  calculates some indices like the  X  X gnition Component X  which relates the probability that a fire that requires suppression action will result if a firebrand is introduced into a fine fuel complex. These indices are influenced by many variables measured in the rating system. They form natural co-occurring patterns in the data which we use to demonstrate that the MC algorithm does find these multiclusters. all measured sensor data. Figure 15 (right side) illustrates an example cluster which nearly contains all attributes. The cluster overview (Fig. 15 left side) shows many clusters containing nearly all attributes (13 and 14) with a length of up to 7. Thus the MC algorithm indeed finds the intrinsic structure in a real valued dataset. 7 Conclusion and further work Domain experts are supported in their need for pattern detection in sequence databases such as river quality records. The information derived using our approach is incorporated in a decision support system devised by hydrologists. Future work will concentrate on integrating GIS information to handle non-sequence spatial information as well and to extend the model to graph structures [ 17 ]. References Author Biographies
