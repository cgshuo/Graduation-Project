 The computation of covariance and correlation matrices are critical to man y data mining applications and pro cesses. Un-fortunately the classical covariance and correlation matrices are very sensitiv e to outliers. Robust metho ds, suc h as QC and the Maronna metho d, have been prop osed. However, existing algorithms for QC only give acceptable performance when the dimensionalit y of the matrix is in the hundreds; and the Maronna metho d is rarely used in practice because of its high computational cost.

In this pap er, we dev elop parallel algorithms for both QC and the Maronna metho d. We evaluate these parallel algo-rithms using a real data set of the gene expression of over 6,000 genes, giving rise to a matrix of over 18 million en-tries. In our exp erimen tal evaluation, we explore scalabilit y in dimensionalit y and in the num ber of pro cessors. We also compare the parallel beha viours of the two metho ds. Af-ter thorough exp erimen tation, we conclude that for man y data mining applications, both QC and Maronna are viable options. Less robust, but faster, QC is the recommended choice for small parallel platforms. On the other hand, the Maronna metho d is the recommended choice when a high degree of robustness is required, or when the parallel plat-form features a high num ber of pro cessors.
 Categories and Subject Descriptors: H.2.8 [Database Managemen t]: Database Applications -Data Mining General Terms: Algorithms, Performance, Exp erimen ta-tion Keyw ords: parallel, robust, correlation, covariance, Maronna
Giv en n samples of v variables, the correlation between two variables measures the strength of the linear relationship between the two variables. Giv en two columns of samples of length n , the covarianc e between X i and X j is: where i = Ave ( x i ) and j = Ave ( x j ) are the means. A covarianc e matrix measures the relation between all pairs of variables. The corr elation of two variables is the normalized value of the covariance of the two variables and is related to the covariance as follo ws: where i and j are the standard deviations of X i and X j .
The computation of covariance and correlation matrices is critical to man y data mining operations and pro cesses. For example, in exploratory data analysis, it is typical to determine whic h variables are highly correlated. Moreo ver, covariance and correlation matrices are used as the basis for principal comp onen ts analysis, for man ual or automatic dimensionalit y reduction, and for variable selection. They are also the basis for detecting multidimensional outliers through computation of Mahalanobis distances. Figure 1: Advantage of Maronna over QC and the classical Pearson Correlation.

Unfortunately , the classical covariance and correlation ma-trices are very sensitiv e to the presence of multidimensional outliers. The example sho wn in Figure 1 illustrates the prob-lem. If the data were perfectly clean, the classical Pear-son correlation coecien t would be 0.96. However, a small percen tage of outliers (in this case, around 10%) was suf-cien t to create disaster for the classical coecien t, as it drops to 0.08. To impro ve on the robustness of covariance and correlation, man y metho ds have been prop osed to deal with \dirt y" large databases. While Section 1.1 will give a more detailed discussion on related work, two state-of-the-art metho ds are Quadran t Correlation (QC) [1] and the Maronna metho d [5]. For the given example, the cor-relation based on QC drops from 0.98 when the data were perfectly clean, to 0.60 with a small percen tage of outliers. The Maronna metho d is even more robust, as the value only changes sligh tly from 0.96 to 0.90. In Section 1.1, we will ex-plain in more precise mathematical terms why the Maronna metho d is more robust than QC.

However, the problem for QC and the Maronna metho d is that they are computationally exp ensiv e, particularly when the size of the matrix (i.e., v v ) is large. Thus, the problem we tackle in this pap er is: How to compute high dimensional robust covarianc e and corr elation matric es?
The approac h we explore in this pap er is based on paral-lelization. This is motiv ated by the fact that multi-pro cessor compute clusters have become inexp ensiv e in the past decade, to the exten t that even a small organization (e.g., a medical researc h lab oratory) can nd suc h a cluster a ordable. For the algorithms we dev elop here, the target architecture is a compute cluster consisting of commo dity pro cessors running MPI/LAM, a public domain version of MPI. MPI (Message Passing Interface) is a standardized comm unication library for distributed memory mac hines and mak es the programs easy to port to a variet y of parallel mac hines [2]. This pap er mak es the follo wing con tributions:
The robustness of an estimate can be measured by its breakdo wn point -the maxim um fraction of con tamination the estimate can tolerate. There has been considerable em-phasis on obtaining positiv e de nite, ane equiv arian t esti-mators with the highest breakdo wn point of one-half. How-ever, all kno wn ane equiv arian t high-breakdo wn point es-timates are solutions to a highly non-con vex optimization problems and as suc h do not scale up to the large databases whic h are commonplace in data mining applications.
The\F ast MCD" (FMCD) metho d that has recen tly been prop osed is much more e ectiv e than naiv e subsampling for minimizing the objectiv e function of the MCD [7]. But FMCD still requires substan tial running times for large v , and it no longer retains a high breakdo wn point with high probabilit y when n is large.

Muc h faster estimates with high breakdo wn points can be computed if one is willing to drop the requiremen t of ane equiv ariance of the resulting covariance matrix. Examples include classical rank based metho ds, suc h as the Spear-man's and Kendall's ; metho ds based on 1-D \Hub erized" data; and bivariate outlier resistan t metho ds (see [3]). Re-cen tly, the latter two strategies have been com bined to give new pairwise metho ds, suc h as QC, that preserv e positiv e de niteness with a computational complexit y of O ( nv 2 ) [4]. However, these pairwise metho ds are not ane equiv arian t and may be upset by two-dimensional structural outliers. In con trast, the Maronna metho d is positiv e de nite and ane equiv arian t, and is thus more robust than QC. The problem, of course, is that the extra robustness requires a lot more computational e ort.
In this section, we sho w a real-life application whic h re-quires the computation of a high-dimensional robust covari-ance matrix. This application arises from our strong ties with the cardio vascular researc h lab oratory at the St Paul's Hospital in Vancouv er (www.icapture.ub c.ca). Rheumatic valves in the heart cause heart failures, and represen t one of the most common reasons for heart transplan ts. To under-stand how rheumatic valves are formed, researc hers at the hospital collect gene expression data (i.e., using microarra y technologies) for a num ber of rheumatic valves and normal valves. Speci cally , for eac h sample/v alve, eac h gene is as-sociated with a non-negativ e coun t represen ting the num ber of times the gene has expressed itself in the valve. Based on these coun ts, we compute the covariance matrix.

These matrices are useful for a variet y of reasons. One usage is that for any given gene G , we can nd a rank ed list of genes whic h are the most positiv ely or negativ ely corre-lated with G . Another use for the correlation matrix is to form a dissimilarit y function for clustering a given collection of genes. Dendrograms of this kind help medical researc hers to iden tify the biological path ways that are hea vily involved in pro ducing rheumatic valves.

There are, however, a num ber of problems in computing the covariance matrix. First, even though microarra y tech-nologies have impro ved dramatically in recen t years, gene expression data are noisy (i.e., con tain man y outliers). Thus, robust metho ds for computing the covariance matrix are valuable. Second, as usually the case for man y biomedical applications, n , the num ber of samples, may not be large. In our case eac h sample corresp onds to a heart valve, and it tak es a long time to collect even 10 rheumatic values from heart transplan t patien ts. Minimizing the negativ e impact of noise is all the more imp ortan t because of the small num-ber of samples. Last but not least, the dimensionalit y of the matrix is very high. In this pap er, we exp erimen t with a data set of 6068 expressed genes. This corresp onds to a 6068 6068 matrix, with over 18 million entries. This magnitude far exceeds the capabilit y of state-of-the-art al-gorithms for computing robust covariance matrices. In fact, we recen tly receiv ed a new version of the data set with over 12,000 expressed genes. Thus, there is an urgen t need to dev elop algorithms for computing high dimensional robust covariance and correlation matrices.

The Maronna and Quadran t Correlation (QC) metho ds tak e as input a n v matrix X with v variables and n cases, and compute as output a v v matrix, whic h is either the co-variance or correlation matrix. In general, both algorithms perform the follo wing steps: 1. Calculate the median/MAD for eac h variable in X , 2. Compute the pairwise covariance/correlation for X . 3. Restore matrix positiv e de niteness, if required.
Giv en space limitations, we mainly concen trate on step two, the covariance/correlations computation. The Maronna metho d is discussed in Section 3.1, while QC is discussed in Section 3.2. Step one is covered brie y in Section 3.3. Step three, restoring the positiv e de niteness of the covariance and correlation matrices, may or may not be necessary de-pending on the applications. Thus, for space limitations, we omit any details here on step three. A description of the main portion of the parallel version of Maronna is sho wn in Figure 2. Eac h of the O ( v 2 ) pairwise calculations can be computed indep enden tly. Eac h inde-penden t computation for a given v i and v j is iterativ e and con verges at di eren t rates. The inner sequen tial part of eac h computation, depicted in Figure 2, does the follo wing.
First, the values involved in the iteration are initialized in step 3 and 4. is a vector of length two, and is initialized with the median of the data variables involved in this cor-relation calculation. is a 2 x 2 matrix that will hold the estimate values for the correlation upon con vergence. It is initialized as a diagonal matrix holding the MAD of the cor-relation variables in the diagonal. After initialization, the algorithm rep eats the follo wing pro cess. The Mahalanobis distance is used to measure the distance between the samples of the pair of variables in step 10. The Mahalanobis distance measures the distance between a data point and the cen troid of all the data points. A weigh t function is then applied to the distance values in step 12 to decrease the in uence of outliers in the data. Our weigh t function uses Hub er's score function as the robust M-estimate to score the in uence of the sample points to the median and variance. The weigh t function gives weigh ts between zero and one that are applied to the data. The weigh t function will weigh normal data variables near one and down-w eigh the outlier values with weigh ts closer to zero.

The weigh ted data is used to calculate new values for and for the next iteration in steps 14 and 15. The loop con tinues until the change in covariance from one step to another is within the desired tolerance. The algorithm is kno wn to con verge, but the rate varies dep ending on the input.

The sequen tial version of Maronna uses a single pro cessor to perform all of the pairwise computations. The indep en-dence of these O ( v 2 ) pairwise computations mak es Maronna an excellen t candidate for parallelization. The parallel ver-sion divides the pairwise computations into p groups, one for eac h pro cessor. The challenge in computing the covari-ances ecien tly is to ensure that the work is distributed and equally shared among the pro cessors. The num ber of itera-tions varies signi can tly and can poten tially slow down the computation while some pro cessors wait for others to nish. As well, care must be tak en in the distribution and gather-ing of the results since, for large problem sizes, there are a large num ber of pairs to be distributed. The exp erimen ts in Section 4 sho w that Maronna can achiev e signi can t speed-up on large problem sizes and can e ectiv ely use a large num ber of pro cessors.
Figure 3 describ es parallel QC. The ma jor computation in QC is a large matrix multiplication. In the algorithm, we represen t the matrices in column-ma jor order, where the columns are variables. Thus X [ i ] refers to the ith column (variable). After calculating the median and MAD for all the variables, the algorithm creates a temp orary matrix to hold the normalized values: The X matrix is then used to create a matrix Y of all 1's, -1's, and near zero values by applying a function, , that is similar to the sign function, to all the elemen ts in X . Our sign function cuts o the values within c of zero and assigns them to the value x c . Our choice for c in the code was 0.00001. In actualit y, by our function, we are using a Hub erized estimator, whic h in the limiting case is Quadran t Correlation [1]. The limiting case here would be to use the sign function in the place of .
 In the next step, the algorithm calculates the follo wing equation to ll in eac h entry of the correlation matrix: The computationally exp ensiv e part of the calculation is the part where the numerator is calculated using a matrix mul-tiplication between Y and its transp ose in steps 4 and 6 of Figure 3. The operations involved are appro ximately O ( v 3 ). The denominator is the geometric mean of the average num-ber of nonzero elemen ts for a pair of columns i and j . This part of the calculation tak es O ( v 2 ) time and is set up in step 9. The equation nishes in step 12 where the denominator divides the numerator. Again, this division occurs for every elemen t in the matrix. Thus, step 12 requires O ( v 2 ) time.
The parallelization of QC is complicated by the num ber of di eren t types of vector operations that it performs. These operations and a matrix multiply need to be performed on matrices and vectors that are distributed across the p pro ces-sors. Rather than create our own vector and matrix library we implemen ted QC using the PLAP ACK library [8]. PLA-PACK is a well-kno wn parallel numerical library from the Univ ersit y of Texas at Austin that pro vides a variet y of vec-tor and matrix operations. The library is used to construct a pro cessor mesh and partition the linear algebra objects, vectors and matrices, into blo cks that are distributed to the pro cessors. Once the objects are distributed the operations can be done in parallel with eac h pro cessor working on their pieces of the distributed objects. There is some comm unica-tion between the pro cessors during this computation when the values residing on other pro cessors are needed, so the pro cessors do not work indep enden tly.

The dicult y in implemen ting QC using PLAP ACK was to determine the blo ck size and distribution patterns to avoid undue comm unication between the pro cessors neces-sary to perform the various vector and matrix operations. It is possible to reduce the comm unication by replicating the matrix in eac h pro cessor. However, this is imp ossible for larger problem sizes. Memory size was an issue on the problem sizes that we exp erimen ted with in this pap er.
Maronna and QC use the median and MAD values for eac h variable (i.e., column). MAD, whic h stands for the median absolute deviation, measures the deviation of the data from the median and is a more robust measure than the standard deviation. The MAD value of a variable can be directly calculated from the median using the form ula below.
 The constan t : 6745 app earing in the form ula is the inverse of the third quartile of the normal distribution. The numerator alone underestimates the standard deviation and dividing by : 6745 leads to a better estimate [6]. It is possible, with sligh t mo di cations to the median nding algorithm, to directly calculate the MAD values for use by Maronna and QC.
Parallel median nding algorithms are well-kno wn. We choose not to implemen t a parallel median algorithm partly because our focus is high dimensional data where the for-mer case is more imp ortan t, and partly because median-nding time is dominated by the correlation/co variance cal-culations.
We evaluated the parallel Maronna and QC metho ds in two di eren t cluster environmen ts: (a) a small platform: a collection of eigh t 500MHz Pentium-3 pro cessors running Lin ux using MPI-LAM on a 100Mbps LAN; and (b) a grid platform: WestGRID, a compute cluster consisting of 504 dual pro cessor 3 GHz Xeon pro cessors running Lin ux with 2 GB of RAM on Gigabit Ethernet (www.w estgrid.ca). The ma jorit y of our exp erimen ts were performed on the Pentium-3 system whic h pro vided a dedicated and con trolled environmen t to evaluate and test di eren t versions of the program. The WestGRID facilit y was used to evaluate the scalabilit y of the two metho ds for large num bers of pro ces-sors and increasing problem sizes.

We exp erimen ted with sev eral real data sets. The results rep orted below are based on the gene expression levels of 6,068 genes on rheumatic and normal heart valves, as sum-marized in Section 2. We rep eated eac h exp erimen t ten times and rep ort the best results because these more closely represen t what performance would be in an ideal setting. Figure 4: Maronna Performance for large problem size (6000 variables)
Figure 4 sho ws the total time (wall clock time) tak en for the Maronna metho d using the small platform. As exp ected, the total time decreased as we used more pro cessors. On 8 pro cessors the wall clock time was about 400 seconds, rep-resen ting a speed-up of 4.5 out of 8.

In Figure 4 eac h bar is divided into the ma jor time com-ponen ts that mak e up the total time. The most dominan t comp onen ts in Figure 4 are the correlation comp onen t, the I/O, and comm unication comp onen t. The other comp onen ts are so small they do not app ear on the chart. A closer exam-ination of the correlation comp onen t with varying num ber of pro cessors and problem sizes is given in Figure 5.
Figure 5 sho ws that the main computational part of the metho d, calculating the correlation, achiev ed good speed-up over all problem sizes and mac hine sizes. Its overall speed-up was around 5.5 on 8 pro cessors.

The comm unication comp onen t included the time needed to distribute the pairwise correlations to the pro cessors and the time to gather the results bac k to the manager pro ces-sor. It increased with both problem size and num ber of pro cessors.

Apart from the correlation and the comm unication com-ponen ts, the remaining comp onen t included the median/MAD calculation, matrix ll time, I/O time and miscellaneous other operations to initialize and manage memory .
The matrix ll time was the time required to cop y the results from the message bu ers into the nal result matrix. We could have eliminated much of this time by gathering the result directly into the matrix. In general, the time was small and constan t. It did increase the time substan tially when memory constrain ts resulted in page faults. This ex-plains the relativ ely large time on one and two pro cessors, 41 seconds and 22 seconds resp ectiv ely. These page faults did sligh tly in ate the apparen t speed-up in Figure 4.
The I/O time remained relativ ely constan t for a xed pro-gram size. The program used a manager-w ork er organiza-tion where one pro cessor, the manager, read in the matrix, distributed the matrix to the work er pro cessors, gathered the result and wrote it to disk. The time to write the v v matrix to disk was the ma jor portion of the I/O time. Next we turn our atten tion to the parallel QC algorithm. QC calculated its correlation using sev eral vector operations and matrix multiplication. The performance of QC for a large problem is sho wn in Figure 6.

On 8 pro cessors the wall clock time was 105 seconds, whic h was a speed-up of only 1.6 out of 8. Notice that QC executed faster than the Maronna metho d on the same prob-lem. Again, the bars in Figure 4 sho w the ma jor time com-ponen ts that made up the total time. A clear observ ation is that QC was not able to use the pro cessors as e ectiv ely as the Maronna metho d, and at 8 pro cessors there was little to be gained by adding more pro cessors. It is eviden t from the Figure 6: QC Performance for large problem size (6000 variables) gure that the correlation calculation was dominated by the the comm unication time and I/O time. At 8 pro cessors the correlation time was only taking 5 seconds, a fraction of the overall execution time of 105 seconds. QC's correlation per-formance looks very similar to that of Maronna in Figure 5, except that QC's time range extends to 45 seconds instead of Maronna's 1800 seconds.

Except for the small problems sizes the speed-up was bet-ter than exp ected, in some cases sup erlinear. The reason for the sup erlinear speed-up related to how the PLAP ACK library distributed matrix blo cks to pro cessors. It assumed a mesh formation and in these exp erimen ts attempted, as best possible, to arrange the pro cessors in a square mesh. This distribution a ected the performance, making it more dicult to determine exact speed-up num bers.
The distribution of blo cks to pro cessors also a ected com-munication. The comm unication comp onen t for QC is sho wn in Figure 7. The gather portion of QC used a PLAP ACK primitiv e call to assem ble the distributed matrix into a con-tinuous bu er on one pro cessor. The time sho wn for sequen-tial comm unication was actually a memory-to-memory cop y. The large cop y times were due to page faults and were not presen t when executed on a cluster with more memory .
From the previous gures, it is clear that the Maronna metho d was more amenable to parallelization than QC. The question to be answ ered is when the speed-up will stop for the Maronna metho d. To answ er this question, we ran the parallel Maronna and QC on the WestGRID cluster using up to 128 pro cessors on the gene data set. There was some variation in the exp erimen ts. Most of the variation came in the category of I/O time, and times varied by as much as 170 seconds for Maronna and 30 seconds for QC. Eac h exp erimen t was run ten times with = 10 7 and we used the smallest rep eatable value. Although the pro cessors were not shared, the net work and le system were shared, resulted in varying times. The smallest value best re ected what would be possible on a dedicated mac hine.
The speedup for QC and the Maronna metho d are listed in Figure 8. The data points are lab elled with the total run time values for Maronna and for QC with eigh t or more pro cessors. As an example, the Maronna metho d using 128 pro cessors took 15 : 5 seconds and had a speedup of 24 : 1. For the Maronna metho d, the total run times con tinued to de-crease as we added pro cessors. The times decreased from 374 : 7 seconds on a single mac hine to 15 : 5 seconds with 128 pro cessors. We were surprised at how well Maronna per-formed on a large cluster. One may exp ect to have satu-rated the manager pro cessor since it is the only one allo cat-ing and distributing tasks. The fact that this did not occur, even when the total execution time was 15.5 seconds, sug-gests that the Maronna metho d will con tinue to scale well to larger problems and pro cessor sizes. For a problem size of 6068, at 128 pro cessors, there is little to be gained by adding more pro cessors.

As exp ected from previous discussions, the speed-up curv e for QC in Figure 8 sho ws that QC quic kly reac hed the point that it can not e ectiv ely use more pro cessors. The times decreased for up to 16 pro cessors, where the running time was 14.2 seconds compared to Maronna's 37.2, but comm u-nication overhead began to dominate and the times began to increase. The good news is that QC executed quic kly on large problems and was able to exploit a small degree of parallelism. We see that QC's running time was dramati-cally smaller than Maronna's until the overhead became a problem for QC. One may be able to use more pro cessors to solv e larger problems with QC. However, the comm unication overheads in QC were more signi can t than Maronna's.
This pap er has sho wn that robust metho ds for calculat-ing high dimensional correlation and covariance matrices are feasible when implemen ted in parallel. These metho ds now mak e it possible to not only solv e for large correlation and covariance matrices in a timely fashion, but also compute them with a more robust approac h.

Our exp erimen ts were performed on a real dataset with 6068 variables represen ting the expression levels of genes in 20 patien ts. The results sho w that QC scales well for up to 8 pro cessors. Maronna is able to scale up much further beyond since the computation portion requires no comm unication between pro cessors. This helps Maronna to achiev e speed-up on more than 8 pro cessors, up to 128 as can be seen from the WestGRID results. QC is still faster, but Maronna is more robust and scalable to more pro cessors.

The QC and Maronna algorithms are good for solving dif-feren t types of problems. We have created a recip e in Figure 9 to suggest whic h algorithm to use based on the given re-sources and needs. With low dimensional data, the Maronna metho d gives robust results and is not computationally ex-pensiv e. With high dimensional data however, the choice dep ends on the application's need for robustness. If only a mo derate degree of robustness is necessary , and resources are limited to small clusters, then QC works best. If a large cluster is available, either QC or Maronna works well. On the other hand, for very robust applications, the purp ose of the calculation may be considered. If the covariance matrix is needed for other calculations, it is best to use the Maronna metho d because of its higher qualit y results. If the output is intended only for preliminary exploration or visualization, then QC may be chosen for its performance.
 Figure 9: Recip e for Choosing a Parallel Robust Correlation/Co variance Algorithm
