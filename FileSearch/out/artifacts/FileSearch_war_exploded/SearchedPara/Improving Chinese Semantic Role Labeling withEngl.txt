 Semantic Role Labeling (SRL) aims to recognize the arguments for a given predicate and assign semantic roles to them. Figure 1 shows an example of SRL. John is the agent of the predicate married denoting that he is the man who got married, his neighbor is the patient of married denoting who John married. Both of the agent and patient are the sematic roles of married .
 schema to tag the semantic roles. According to this tagging schema, argument identi-fication consists of tagging all tokens of a sentence with IOBES tags (Inside, Outside, Begin, End, Single) relative to a given predicate. Figure 1 shows an example. mantically annotated corpora. For the Chinese dataset English Proposition Bank (CPB) [1], it contains over 80,000 verb instances for 11,000 verb types. However, it is stil-l not enough using only CPB to solve the whole Chinese SRL task. For the English standard benchmark dataset in English Proposition Bank (EPB) [2], it contains near-ly 100 thousand annotated sentences. Given that manually annotating SRL corpus is labor-intensive and expensive, how to improve the monolingual SRL performance with merging different language resources is thus an important issue deserving to explore. recurrent neural network (RNN) with bidirectional long-short-term memory (LSTM), aiming at improving the Chinese SRL performance using English semantic role labeled corpus. The main points are as follows: By representation learning, Chinese SRL corpus and English SRL corpus are mapped into an uniform semantic representation space. This makes it possible to merge the corpora of two languages and train a single SRL model across languages. On the basis of the cross language SRL model, we further train the SRL model specific to Chinese. Our approach requires neither parallel SRL corpus nor machine translation of the corpus. Experiments show that our approach outperforms current state-of-art systems for Chinese SRL task. SRL task was firstly proposed in the work of Jurafsky et al. (2002) [3] and a large body of work has been devoted to this task since then. Traditional SRL approaches normally use a lot of handcrafted features. Koomen et al. (2005) [8] get the best performance among all traditional approaches on English SRL task, they used different parse tree information with lots of traditional features. Most Chinese SRL work adopted similar strategies, although using a much smaller training corpus. Xue &amp; Palmer (2005) [10] and Xue (2008) [11] stands for first through and systematic Chinese SRL research. Sun et al. (2009) [12] performed Chinese SRL with shallow parsing, which took partial pars-es as inputs. Yang and Zong (2014) [13] proposed multipredicate SRL, which showed improvements both on English and Chinese Proposition Bank. Recently, to reduce the heavy burden of feature engineering, deep learning models like CNNs and RNNs have been introduced into SRL task. Collobert and Weston (2008) [9] proposed a Convolu-tional Neural Network (CNN) on English. For Chinese SRL, Wang (2015) [14] used bidirectional LSTM and outperformed previous traditional models. Different from pre-vious work, we focus on how to use English semantic role labeled corpora to improve the performance of Chinese SRL. 3.1 Basic Idea Generally, semantics is believed to be more language general than syntax. Especially in SRL corpus, many sematic roles are same or have similar meanings across languages. words and word order are different between Chinese and English, the semantic roles such as ARG0, and call the roles which only appear in the Chinese SRL corpus as non-common semantic roles, such as ARGM-CND 1 . Table 1 shows that over 60% types of Chinese roles can be found in English and the total number of common roles accounts number of common roles account for 88.54% on whole EPB. Intuitively, adding EPB to train set is helpful for improving the performance of Chinese common role labeling. between the linguistic structure and syntax of Chinese and English corpus. To cross this gap, we propose to project these corpus into a same vector space by means of bilin-gual embedding. Progress in bilingual representation learning [4, 7] shows that words in different languages can be projected into the same vector space as distributed vector embeddings. Moreover, these word embeddings are shown to have the ability of captur-ing semantic coherence across languages [5, 6]. With bilingual embedding, words with similar meanings in different languages are projected into close position in the shared vector space.
 First, we merge and randomly shuffle CPB and EPB, keep the common semantic roles and remove the non-common semantic roles. We use bilingual parallel corpus to learn bilingual word embedding. Using these merged corpus and bilingual embedding, we train an RNN model for all common semantic roles. We ignore the Chinese non-common roles in first pass because these roles are rare and language-specific. Second, for CPB only, we learn both Chinese common roles and Chinese non-common roles together, using the same RNN model in the first pass. We utilize the parameters (including the model in second pass. Our approach can be illustrated in Figure 3. 3.2 Bilingual Word Representation In our approach, we utilize bilingual compositional vector model (BiCVM) [6] to learn our bilingual word embedding. BiCVM learns to assign similar embeddings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring whole sentence and BiCVM can catch more semantic knowledge on sentence level across aligned parallel sentences between two languages. 3.3 Basic SRL Model Given a sentence s , we first compute its representation sequence z . Here z t =  X  ( Wa t ) denotes the representation of the t -th word in s . a t is the feature embedding of current word t which concatenates bilingual embeddings(word t , word t  X  1 , word t +1 and the predicate), POS tag embeddings (word t , word t  X  1 , word t +1 ) and distant feature(the sigmoid function.
 z from both forward and backward directions. We use the bidirectional LSTM because it is a sequence labeling model which can easily catch semantic information and works well in monolingual SRL task (Wang ,2015). We can compute LSTM layer at each word t as follows: Where C t is the memory cell of position t , e C t computes the candidate value for C t , h tiplication. For the t -th word, we get both a forward hidden state hidden state ed together into a merged hidden state hid t = [ output corresponds to the score of a certain semantic role label in IOBES schema. 3.4 Training Criteria Given training examples: y t = k means the t -th word has the k -th semantic role label in IOBES scheme. We can define the score of i -th sentence as follows: where N i is the length of the i -th sentence, o ty ( i ) in the whole network.
 ample, the log likelihood is: where y 0 ranges from all the valid paths of tags.
 4.1 Experiment Settings To comparison with previous work, we conduct experiments on the standard benchmark dataset CPB, follow the same data setting as previous work [11, 14]. For English dataset, we use the training set of CoNLL-2005 dataset(based on EPB), the same data setting as Li and Chang (2015) [15]. For training the bilingual word embedding, we use PKU bilingual corpus 2 .
 word embedding is 50; the dimension of POS tag embedding is 20; the dimension of distant feature embedding is 20; n 1 is 200; n 2 is 100; the number of bidirectional L-STM layer is 1; the learning rate in both first pass and second pass is 10  X  3 ; the hyper-parameter  X  in the objective function is 10  X  3 ; Using early stop strategy to get the best result on development set, the training epochs in the first pass is set to 12, the training epochs in the second pass is set to 6. 4.2 SRL Results Table 2 shows the Chinese SRL results on CPB. one-pass denotes training a bidirec-tional LSTM model on CPB for both Chinese common roles and non-common roles, two-pass denotes our two-pass approach described in Section 3.1. random denotes that the word embedding is randomized initialized, BiCVM in Table 2 denotes that we use the bilingual word embedding described in Section 3.2 instead of randomized initializa-tion. We don X  X  do experiment with monolingual word embedding because it is beyond our focus in this paper. EPB in Table 2 denotes that we use EPB in the first pass training during the two-pass approach.
 using bilingual embedding has a slight improvement. While compared with one-pass training approach, our two-pass approach improves a lot and establishes a new state-of-the-art result in Chinese SRL with 78.39.
 common roles. From Figure 4, EPB is helpful for most Chinese roles, especially none of common role X  X  performance decreases after adding EPB. This proves that our strategy of using EPB is successful, our approach exactly capture the common semantic role information between Chinese and English. The performance on Chinese non-common roles is inconsistent, because these roles are language-specific and EPB can X  X  definitely improve the performance of these roles. 4.3 Translation Equivalent Regularizer Furthermore, we try to make the embedding of the words, which has translation equiv-pass training. For each word t (either Chinese or English) in merging corpus, we de-is the embedding of the word t , x j is the embedding of the word j which has transla-tion equivalent with word t , a tj is the translation probability 3 from word t to word j . However, after adding the regularizer, the result gets 78.31 on CPB test set, doesn X  X  out-perform the best result we gets before. This is possibly because translation equivalence regularizer leads to overfitting in the first pass training. In this paper, we introduce a two-pass approach with bidirectional LSTM, using EPB to improve the performance on Chinese SRL. Our approach doesn X  X  need any parallel annotated SRL corpus, heavy job of feature engineering. And our approach can apply to other languages. Our approach achieves the state-of-the-art results on the Chinese SRL task. In future work, we plan to project different language sentences into same semantic space in a better way.
 Acknowledgments. This work is supported by National Key Basic Research Pro-gram of China (2014CB340504) and National Natural Science Foundation of China (61273318).

