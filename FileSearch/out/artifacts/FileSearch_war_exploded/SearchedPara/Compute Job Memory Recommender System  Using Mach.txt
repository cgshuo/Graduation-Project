 This paper presents a machine learning approach to predict the amount of compute memory needed by jobs which are submitted to Load Sharing Facility (LSF  X  ) with a high level of accuracy. LSF  X  is the compute resource manager and job scheduler for Qualcomm chip design process. It schedules the jobs based on available resources: CPU, memory, storage, and software license s. Memory is one of the key resources and its proper utilization leads to a substantial improvement in saving machine resources which in turn results in a significant reduction in overall job pending time. In addition, efficien t memory utilization helps to reduce the operations cost by decreasing the number of servers needed for the end-to-end design process. In this paper, we explored a suite of statistical and machine learning techniques to devel op a Compute Memory Recommender System for the Qualcomm chip design process with over 90% accuracy in predicting the amount of memory a job needs. Moreover, it demonstrates the potential to significantly reduce job pending time. Memory Utilization; Compute Memory Recommender System; Statistical and Machine Learning Techniques; Grid Computing In a large scale semiconductor fi rm like Qualcomm, compute and license environment provides the infrastructure and resources t o enable the full cycle of the chi p design process. These resourc es include hardware infrastructure (CPU, storage, memory), softwar e licenses, and etc. The amount of compute resources needed by an end-to-end chip design process is substantially dependent on th e design complexity through the entire design cycle and specifica lly prior to the final stage of it where the design is shipped for manufacturing, also known as  X  X apeout X . A key challenge in any large scale semiconductor company is to effectively utilize the compute resources and optimally allocat e them to parallel chip design pro cesses. Effective utilization o f compute resources yields a significant impact on setting the co st of the end-user devices which helps improving company X  X  position in today X  X  com petitive market. Machine memory, as one of the key compute resources of any job execution, needs to be utilized effectively to improve the efficiency of the design process and prevents any major slowdow n or halt in the design flow. Over-requesting memory by a job causes other jobs to go into a pending state since they are usi ng shared pools of resources. On the other hand, under-requesting memory by a job may lead to forceful exit of the job due to lac k of required memory to successfully finish the job. Hence an effective and asymmetric memor y allocation methodology needs to be adopted to ensure that all the jobs are receiving their n eeded memory requirement for proper execution. In this paper, we explored sever al different models from classi c statistical methods to more modern machine learning techniques to develop a memory recommender system based on the historical job execution data on a grid computing platform. Our model is capable of handling large volume of data for training and prediction and can be easily ported to a cloud computing architecture platform. Integrated circuit design, or IC design, is a subset of electro nics engineering, which consists of lo gic and circuit design techniq ues required to develop and build an IC [1]. The initial chip desig n process begins with system-level design and architecture planning. This step is where a chip functionality and design is decided. Upon agreement of a system design, the engineers implement the functional models in a hardware description language like Verilog, System Verilog, or VHDL. The next step in the IC design process, physical design phase, is to map the design into actual geometric re presentations of all electronics devices, such as capacitors, re sistors, logic gates, and transi stors that will go on the chip. Since errors are expensive, time consuming and hard to spot, extensive testing needs to be done to make sure the mapping to transistors was done correctly and the manufacturing rules were followed faithfully. At the final stag e of the design cycle for ICs, Tapeout, the artwork for the silicon photomask of the circuit is sent to be manufactured. After the chip is manufactured and returned back to the design company, massive testing is done to make sure all parts are working according to the specifications. Figure 1 shows the high level chip design process. One of the main differences betw een chip and software design is the high cost of fixing design errors. Errors that are not corr ected by the time silicon masks are manufactured usually require re-spins of some or all of the masks associated with significant capital expense and opportunity costs, frequently exceeding $10 million. As a result, much of the discipline of chip design is involved with early and frequent verification of the chip as it goes through the design process. The complexity of modern chip design, as well as ma rket pressure to produce designs rapidly, has led to the extensive use of El ectronic Design Automation (EDA) software for design, verificati on andp simulation of different steps of the process. This entails util izing massive amount of comput e resources such as hardware, operating software, and EDA software licenses for simulation, design, and testing stages ne eded for a full cycle of a chip design. To have a successful and cos t efficient design process, availa ble compute resources such as CPU, memory, storage and automati on tools and licenses have to be optimally utilized. IBM Load Sharing Facility (LSF  X  ) is a workload management platform and job scheduler for distributed high performance computing environments. It can be used to execute batch jobs on networked UNIX and Windows sy stems on many different architectures. It provides a comprehensive set of policy-driven scheduling features that enable u sers to take full advantage of the available compute infrastructu re resources needed for successfu l job execution [2][10]. In a large scale design process w ith massive amounts of compute resources, there is a need for a scalable, fault-tolerant, and cost efficient computing power, such as that provided by a grid environment. Grid environments consist of a collection of clustered computers which form a heterogeneous grid computing foundation for a full cycle of design process. The grid environment can be thought of as a distributed system with non-interactive workloads and the ability to perform different task s and applications [2][12]. The ability to handle scheduling of massive number of jobs on a heterogeneous network of resources make LSF  X  a viable choice as a workload manager to be utilized by chip design process in many semiconductor companies including Qualcomm Incorporation. Compute infrastructure and resources used in the chip design process make up a big portion of the total cost of the chip. An inefficient allocation of resources can cause a significant inc rease in job pending time which leads to inevitable delays in Tapeout s. Furthermore, it can reduce the productivity of the engineers wh ile they wait a longer time for job completion. For these reasons, sustainable solutions for optimal utilization of compute resour ces are of utmost importance in a competitive market. Using manual or trial and error methods to allocate compute resources needed for jobs are not regarded to be effective. In addition, due to the volume and complexity of the submitted job s in a chip design cycle, it is very difficult, inefficient, and at times impossible for users to accurately predict the needed compute resources for successful job executions based on their experien ce. Hence, a need for machine learni ng intelligence emerges to effectively predict the resources needed in the chip design process. In a grid environment, workload scheduling is done at global an d local levels to increase efficiency and scalability. Job submis sion scripts are responsible for requesting computing resources at t he local level and LSF  X  scheduler is responsible for communicating with all job submission scripts to manage the workload at the g rid level. Hence, smart resource requesting mechanisms by job submission scripts enable efficient resource allocation by the LSF  X  scheduler. Every job submissions script is configured for a specific type of job and includes the job paramet ers and paths, files and formats, selected EDA software products, and required compute resources. Job submission scripts are generally developed and maintained within design teams and are specific to each team X  X  requirement s. Compute resource requirements for each job is then specified by the team lead or the engineer based on their knowledge of past jobs and priority of the submitted job. In general, engineers tend to over-request the amount of comput e resources including memory in order to make sure their jobs execute successfully. This phenomena can result in a substantia l over-request of memory requireme nts once they get accumulated over all the jobs submitted in a grid environment. Figure 2 sho ws that 87% of a specific type of EDA jobs (physical design) for a three month period actually consumed 1MB to 32GB memory, while the users requested 1MB to 32GB of memory for only 17% of the jobs. Figure 3 illustrates the same point from another perspective, memory by run time. It shows 36% of jobs actually need more than 1 GB-Hour memory, in comparison to 81% of jobs requested more than 1 GB-Hour memory. These statistics exhibits great amount of memory over request. The main consequence of this over-request is that jobs have to wait for longer times in a pe nding state for memory resources to beco me available which results in longer design cycles. It also im pacts engineer X  X  efficiency as they have to wait longer for their jobs to complete which contribute s to higher labor cost. Moreover, to a ccommodate for the peak reques t of memory resources, servers with larger memory blocks have to be purchased which leads to signi ficant increase in operations cost, potentially resulting in higher price point for end-user devices, or lowe r profit margins. These statistics dem onstrates the huge potential for improvemen t in the memory allocation efficiency within Qualcomm X  X  grid environment. An initiative was launched to explore this potenti al with the goal of closing the gap between the amount of memory requested by a job and the actual memory a job utilizes after i t completes. A set of historica l data on job submission was collected and an entire suite of techniques from statistical an d machine learning paradigms were explored to build an efficient and accurate memory recommender system which includes dynamic user and management level dashboards, memory violation alerting system, and memory recommender engine. In the next two sections we explain the content of the data and th e development of our r ecommender system. At the start of the project, our approach was to build a predic tive model that would consid er the phase and week of the chip design life cycle, the intensity of the chip design analysis and testi ng at that point in time, plus the EDA software being used at that ph ase. As shown in Figure 1, major chips follow this design flow. However, each chip is unique in the pace of its project plan an d amount of required software licenses and compute resources. Within the available real-time data, the knowledge of how much of each resource is needed for the individual chip design was not readily available. To better understand the basic patterns in the data we used descriptive statistics, machine learning, and visualization met hods to mine the raw data. Some of our early findings were:  X  Some design centers consume m ore memory than others for  X  Some contracting firm X  X  tem porary workers wasted more  X  There are different patterns of memory requested by  X  Work experience reflected by e ngineer title correlates with  X  The task influenced the amount of needed memory  X  Individual user patterns are distinct  X  The number of processors influence memory needs for a job  X  The memory used in prior jobs is a good guess for The last four points from above became crucial features (variables) for our modeling efforts. These early findings confirmed our understanding of the behavior of a chip designer and test engineer. An engineer typically executes one design t ask or testing task for several hours up to several days. Their wo rk will require the executions of computer jobs using only a few EDA tools out of ma ny available. We were able to exploit fields in the job submission scripts captured in the database that indicated the chip project under development, the phase of effort , (i.e., physical design vs phy sical implementation), the EDA software used, and the specific task (i.e., timing analysis vs floor-planning) for every engineer. Capturing this behavior was an essential factor in developing o ur compute memory prediction model. Figure 4 demonstrates the unique behavior for seven engineers during the same month for the same project. Some physical design engineers carried out a ll three tasks: chip timing, parasitic extraction and physical verification, some only one or two of these tasks. In addition , one can see, the average number of memory in gigabytes (GB) varied by task and engineer. We also considered the number of compute processors requested for each job, the user X  X  locatio n, employment status (employee vs contractor), and engineer X  X  department as possible predictors. In the end, the engineer X  X  department, title, and employment statu s did not become part of the predictive model. The number of compute processors needed for the job did become an important variable in the model. Finally, a key behavior captured in the model was the memory needed by past similar jobs. Mem ory used by past computer jobs , project, phase, task, and number of needed processors submitted by an engineer became the main variables in our predictive mode l. One of the main business requirements was to build a predictive model which substantially minimizes compute memory waste over current method driven by engineering teams based on their project needs. In addition, the model should be robust enough t o predict the amount of memory a job needs with high accuracy consistently over varying chip projects and time frames. The model and its subsequent implementation should be easy to understand, deploy and maintain and must be able to handle the data volume of more than 1.4 billion computer jobs executed by Qualcomm every year. Another important business re quirement was to limit under-prediction below a certain threshold. If a job is not assigned enough memory for its completion, the LSF  X  scheduler can be configured to gracefully grant more memory blocks to that job i f the machine has enough available memory resources at run time. But if the machine is running out of memory resources, the job is terminated if it does not have enough memory reserved for its execution a priori. Job interruption carries an extremely exorb itant cost due to the high potential of delaying chip design. In order to fulfill our business challenge with the given requirements, we explored an entire suite of models from classi cal statistics to more modern machine learning paradigms. Below we discuss the pros and cons of some of the best performing models we explored for this problem. In the next sections, we will exp lain the model selection process considering the tradeoffs between parsimony versus accuracy of different models. Here is the list of all models we explored: 1. Neural Network (NN): We used a multi-layer perceptron 2. Autoregressive Integrated Moving Average (ARIMA): 3. Kalman Filter (KF): This time series analysis technique 4. Multivariate Adaptive Regression Spline (MARS X ) 2 : This 5. Linear Regression (LR): Least squares linear regression 6. Classification and Regression Trees (CART  X ) 3 : Decision 7. Chi-square Automatic Interaction Detector (CHAID): To perform our experiments, we used the R statistical software and its packages and SPM Salford Predictive Modeler  X  compared all the models we explored to the existing solution, which is to allocate memory based on user X  X  request, specified in submission script. The results i n this paper are based on the training data set of size over 0.5 million records which are collected for July 1 to Sep 30, 2015 time frame. The trained models were then used to predict the amount of memory needed on the testing set collected for Oct 1, 2015 for one major comp ute grid on all tasks for one specifi c phase, Physical Design. Tabl e 1 captures the statistical metrics to compare different models us ing the test set. Training time for each model is also reported in the last column of this table. As described in business requirements section, our major challenge was to make sure we limit under-prediction below a selected threshold due to severe consequence of under-allocatio n of memory slots to a job. Since currently about 5% of the jobs are submitted with memory request less than what actually is needed by those jobs, it is reasonable to set the threshold for under-prediction to be 5%. For this purpose, we build a 95% statistic al upper prediction interval for ou r linear regression models usin g (1), where  X  is the data set size,  X  is the design matrix,  X  is the model standard deviation, and  X   X   X 2 X ,0.05  X  is the upper 95% quantile from t-distribution with  X 2 X  degrees of freedom [8]. Requested by User 12% 84.3 NA 5% NA NN 88% 34.7 9330 58% 33 AR(3) 92% 32.6 9148 18% 8.5 AR(11) 93% 32.4 9152 16% 9.5 KF 94% 32.0 9118 49% 15 MARS  X  92% 32.5 9158 54% 3 LR(3)+UPI 95 93% 32.6 9149 5% 1.1 LR(11)+UPI 95 93% 32.5 9156 5% 1.2 CART  X  87% 35.2 9366 26% 5.5 CHAID 90% 33.8 9260 17% 6 We added the calculated prediction interval to our predicted values generated by the linear regression model to statisticall y guarantee that 95% of the jobs will be allocated memory more than what they actually need using our recommendations. Percentage of jobs with under-pre dicted amount of memory for each model is also reported for comparison. In addition to minimizing the number of jobs which over-request memory, the recommender system must also minimize the amount of memory over-requested by long-running jobs. Once a job with a large run time acquires the memo ry amount it asks for, it wil l hold on to this amount until the job finishes. There are scenar ios where a model results in smaller number of jobs with memory over-request compared to another model, but the jobs in the fir st model which over-requested memory might run for a longer time. To make sure we account for this situation we used a second set of metrics to compare different models as shown in Table 2. These measures evaluate the ability of each memory recommender model to reduce wasted memory (GB) times the hours of the jobs X  run time. We evaluated our models X  behavior over different data sets over time. Average amount of over-prediction (GB) and over-prediction by run time (GB x Hour) are shown in Table 2. Average am ount of under-prediction (GB) and under-prediction by run time (GB x Hr) is also recorded in this table so we can compare the behavior of different models in ter ms of under-prediction, specifically due to its potential sever consequences as desc ribed in section 5. As Table 1 illustrates KF model shows the best R-SQ and the lowest RMSE. On the other hand, AR, MARS X  and LR+UPI 95 model performances are also close to that of KF in these regard s. The disqualifying factor for using KF and MARS X  models is their high percentage of under-predicted jobs which violates ou r business constraints. AR models s how significantly better resul t on their percentage of under-predicted jobs, but they are still above the threshold of currently u sed method which is requested by user. NN, CART X  and CHAID model performances are not up to par with other models. Table 2 shows the average under and over prediction per user fo r both memory (GB) and memory by run time (GB x Hr) which are consistent with the model performance results of Table 1. All models significantly improve the amount of over prediction over the current model. The KF model results in the least amount of over-prediction trading off larger amounts of under-prediction compared to AR and LR models. LR(11)+UPI 95 shows the lowest values on average under-prediction both for memory (GB) and memory by run time (GB x Hr), but it shows an slight increase i n AIC/BIC values compared to LR(3)+UPI 95 model as shown in Table 1. Considering all the trade-offs o f model performance, parsimony, fulfillment of business constraints, and maintainability, overa ll we determined the LR(3)+UPI 95 model to be the most suitable solution for our memory recommender engine. Figure 5 shows the distribution of requested versus actual and predicted versus ac tual memory using LR(3)+ UPI 95 model for  X  X hysical Design X  phase on one compute grid which represents about 13% of all jobs submitted using EDA software in Qualcomm. As it can be seen in this figure, the predicted versus actual plot shows a significa nt performance improvement over the r equested versus actual plot. The correlation of determination (R-squared) of the requested t o actual memory was a dismal 12%, whereas the R-squared of the predicted to actual memory was an impressive 93%. Requested by User 365.6 1948.3 13.1 108.0 NN 138.7 1071.4 45.5 399.6 AR(3) 65.7 759.7 16.9 194.4 AR(11) 62.0 681.8 14.3 183.6 KF 47.4 642.8 42.9 324.0 MARS  X  64.2 818.2 36.4 291.6 LR(3)+UPI 95 66.8 769.5 14.3 109.1 LR(11)+UPI 95 65.7 701.3 13.6 108.5 CART  X  135.0 1032.4 41.6 388.8 CHAID 127.7 993.5 40.3 378.0 Besides memory requirements, an other constraint which a job scheduler like LSF  X  needs to consider is the amount of CPU resources on a machine at the time of scheduling a job. Since CPU resources are associated wit h blocks of memory of size 4 or 16 GB, a useful metric to compa re performance of a model is the histogram of the mismatch between predicted and actual memory built by discretizing the range of memory into bins similar to a confusion matrix. Figure 6 dem onstrates the improvement in histogram of mismatch of memory bins between what a job requests versus what it actuall y uses when we use LR(3)+UPI prediction model. The figure cl early points out the substantial improvement in terms of matching the predicted and actually use d memory bins of size 16GB. Furthermore, to investigate the average performance of our mode l over time, we executed a comprehensive experiment which simulates the use of our model on a rolling window of 50 days. For the first day of the experiment, we built a model based on the data of last three months and applied the model to predict the amount of memory needed for all t he jobs ran on that day for  X  X hysical Design X  task. Then we advanced our training set forward by one day, meaning we included the data of the actual memory used in that day in the training set. We re-modeled usin g this incremental change in our training set and applied the mod el to predict the amount of memory needed for the jobs ran on the second day and so on. Figure 7 illustrates the result of this comprehensive experiment. It compares the percentage of correct bins on the diagonal of confusion matrix of our model (shown in blue) versus the same value when the memory is requested by the user (shown in red). It can be se en that the percentage of entr ies on the diagonal of the confusion matrix improved from 15% to 91% on average by using our predictive model. In the next section we describe the alerting system we develope d based on the above predict ive model using Splunk X  5 software. Splunk  X  is a software developed by the Splunk Corporation for searching, monitoring, and analyzing machine-generated big data , via a web-style interface. Splunk  X  captures indexes and correlates real-time data in a searchable repository from which it can generate graphs, reports, alerts, dashboards and visualizat ions [9][11].
 We developed our alerting system through the Splunk X  alerting application in two parts: model training and alert triggering. Model training is setup as a cron job using Splunk X  scheduler which triggers every day at a specific time. This time is diffe rent for every grid depending on geog raphical location of that grid. We will be able to capture engineers X  daily behavior of job submis sion to different projects and design phases. Once the model trainin g completes, the linear regression model coefficients and upper prediction interval values for every combination of user, proje ct, phase, task, and number of proce ssors is populated into a Splun k X  index for that day. The alerting application then gets triggere d which queries this index to predict the amount of needed memory for every submitted job by every user and returns back the top 10% violators, users with highest mismatch between their requested and actually used memory. Those users receive an automated email with a report of the jobs they submitted and th e amount of memory they requested and used along with our recommendation. They also receive a Splunk X  dashboard which visualizes their memory usage and our recommendations over the jobs they submitted on that day. Besides user dashboards, a suite of management dashboards were developed which aggregates and summarizes the memory usage at different hierarchical levels. Figures 8 and 9 illustrate an ex ample of such dashboards. The amount of used, requested and recommended memory in terabytes (TB), and memory by run time (TB x Hour), aggregated by project are shown in these tables. Figures 10 and 11 show t he same statistics aggregated a t user level shown for top 10 violators. To preserve confidential ity, we substituted project and user n ames with generic names. It ca n greatly improve the memory a llocation efficiency. Daily dashboards of these sorts are helping management team with tracking problematic areas from me mory allocation efficiency standpoint leading to substantia lly optimizing operational cost . In order to integrate our recommender system as part of the chi p design process with minimal d isruption to this process, we decided to develop and deploy this system in three phases. Full development of this system was done using Python SciPy library in conjunction with the Splunk X  alerting and dashboarding application. Here are the deployment phases of our recommender system: Phase 1. We developed a comprehensive monitoring system to detect the behavior of individual engineers in terms of their memory requests. For this purpose, we set up an informational alerting mechanism to alert the top 10% worst violators. Alerts were sent through the Splunk X  alerting application as explained in previous section. The alerts are originally targeted for eng ineers who submit jobs in  X  X hysical Design X  phase on two major grids, but are intended to be expanded to other grids and to other functional areas in third phase. Besides the alerting system, w e provided visualization dashboards for both engineers and management teams through t he Splunk X  dashboarding application. Phase 2. In this phase we develop a memory recommender engine using Python Sci-Py libray and de ploy this engine as part of th e EDA flow for  X  X hysical Design  X  tasks to substitute the user-specified requests for memory with the automated recommendations generated by th is engine. We provide users with the capability of overwriting our recommendations with their ow n memory requests if they choose to do such. However, they will b e alerted if they have over-requested memory greatly. Phase 3. In the last phase we will expand deployment of our memory recommender system (in cluding the automation engine and monitoring and alerting system) to other design phases besides  X  X hysical Design X . Submitted jobs compete with each other to acquire their request ed compute resources such as pro cessors and memory. They are dispatched for execution by LSF  X  scheduler which evaluates the jobs X  priority and resource need s. When jobs are highly over-requesting memory, they have to wait longer to acquire their requested amount of memory to be executed. The situation gets even worse when a job with a long run time over-requests memory. Since run time of jobs a re not known a priori, the scheduler cannot plan ahead for these types of jobs. Once these jobs get launched, they cause ot her jobs to wait in a pending state for a longer time before the scheduler is able to dispatch them for execution. To evaluate the effect of using our prediction model on reducing job pending time, i.e., the time computer jobs wait to be dispatched for execution, we need to simulate LSF  X  behavior. IBM has developed a simulator for LSF  X  for their internal use, but do not provide it to external customers. For this reason, w e developed an in-house basic version of the LSF  X  scheduler and compared our model performance on job pending time reduction with status quo situation on a suite of real jobs. We imitated similar schedulin g behavior of the real LSF scheduler with less number of re sources in our basic simulator. This simulator composed of two servers parameterized to be of different sizes (128GB and 256 GB) and three queues with different priorities: high, mediu m and low. The job scheduling method is FIFO in each queue with highest probability of scheduling a job from high priorit y queue versus medium priorit y queue and so on. We filled up the queues in our simulator using real jobs submitted for the  X  X hysical Design X  phase on our majo r grid. Table 3 compares the effect of using our memory recommender engine on job pending time versus what is currently requested by user using our basi c LSF simulator. Note that thes e simulations are done using a mock-up version of the LSF scheduler on much limited number of servers and considers memory as the only compute resource needed for job submission. The experiment clearly demonstrates the high potential for improvement in job pending time, however the actual improvement percentages should be looked at with caution and shall not be considered as standard. Table 3. Average Pending Time Reduction for Physical Design 128 GB 43 min 28 min 34% 256 GB 47 min 27 min 27% There are multiple benefits to Qualcomm if compute memory is optimally resourced. The first benefit is the reduced annual c ost of compute hosts with high quantities of installed memory. Whenever possible, it is desirable to purchase cheaper computer s with 16GB memory and four processors, rather than computers with high level of memory resour ces. It is estimated that the annual savings could be in the millions of dollars. This predi ctive model could also be leveraged to minimize compute costs for job s that could be run in a cloud environment. As demonstrated by simulations discussed in the preceding section, pending time can potentially be reduced 27% to 34% for a subset of the jobs. Thus, another benefit of improved memory allocation utilization is the potential for the reduced wait ti me of jobs. Reducing pending time wi ll greatly improve the throughpu t of compute jobs, thus enabling more chip design and test jobs t o be executed resulting in a better quality for the chip. This re duces the wait time for an engineer to get results back and improves the engineer X  X  productivity. Finally , this new system can contribut e by helping to bring the chip to market faster, which is an esse ntial advantage in today X  X  competitive markets. Qualcomm places a high value on all th ese contributions. This Compute Memory Recommender project showed that the computer memory needs could be predicted with an acceptable level of accuracy and be le veraged to improve throughput efficiency. This is in addition to the benefits achieved by reduction in computer resource cost. We confirmed that cleverl y constructed simple predictive m odels, like linear regression, c ould provide the solid business value, and is easier to deploy than more sophisticated machine learning a nd statistical models like neur al nets, Kalman filters, and ARIMA. The main advantage of using a linear regression model is that it is simple, parsimonious, a g ood predictor, easy to understa nd, and easy to implement. Another lesson learned, (or rather re-enforced) is to take nove l approaches to defining the problem, rather than tackling a challenge with an expected  X  X hip-design life cycle X  approach. We found we could use a recomme nder model approach that recognized the unique tasks and cadences of each engineer X  X  work, along with their immediate past tasks to predict their ne xt computer job X  X  needs. Thinking more like a  X  X ocial media X  modeler lead us to construct many simple individual models. An alternate  X  X ife cycle X  approach which was not feasible with our available data would have been to create a model that predicted a standard memory need for a chip project in the N th week of the design cycle for a given task. With the high level of accuracy and flexibility which our recommender model provides, a  X  X ife cycle  X  model is not necessary. The implementation plan for our Compute Memory Recommender system is to deploy it across all chip design phase s in 2016. At the time of writing this paper, we demonstrated th e ability to create predictive models with R-squared of over 90% for the  X  X hysical Design X  phase. Th e tasks done in  X  X hysical Design  X  phase are some of the most compute intensive jobs in semiconductor development. We believe we will see similar good results with other design phases. In the course of the analysis a nd model development, we found we needed to take a  X  X ecommender X  model approach that predicts the needs of compute memory for jobs which run the next day for an engineer doing a specific task for a given project. We test ed many statistical models and machine learning techniques which produced similar results. In order to provide a simple, understandable model that could be readily implemented and updated, we chose a linear regression model with lagged variabl es capturing prior usage by the individual engineer. An upper 95% statistical prediction interval was added to our model recommendation to ensure jobs do not under-request memory more than 5% of the time based on the business requirements. The implementation of the recomm ender model used the Splunk X  data repository and software. Predictive models were written i n Python code, and deployed in conjunction with the Splunk X  application. This system is run daily to train and build uniqu e prediction models for each user by project, EDA software, number of processors, and comput e grid. The Splunk X  alerting system contacts the engineers with the greatest mismatch of requested and actual compute memory usage on a daily basis. Our long term goal is to incorporate our compute memory recommendations from the model into job submission scripts for all design phases. This would make the job resource requests at runtime efficient. Having these capabilities will not only sav e Qualcomm on the cost of computer hardware, but improve job turn-around time thus saving engineering time. Potentially, th e greatest contribution of improvi ng the utilization of all these resources, is expediting the semiconductor X  X  time to market. [1] Lavagno, L., Martin, G., Sche ffer, L., 2006. Electronic [2] Goering, Richard, March 8, 1999. Load Sharing Brings [3] Haykin, S., 2007. Neural Ne tworks, a Comprehensive [4] Shumway, R., Stpffer, D., 2011. Time Series Analysis and Its [5] Simon, D., 2006. Optim al State Es timation: Kalman, H [6] Hastie T., Tibshirani R., an d Friedman J.H., 2009. The [7] Kass, G. V., 1980. An Expl oratory Technique for [8] Sheather S., 2009. A Modern Approach to Regression with [9] Carasso, D. 2012. Exploring Splunk: Search Processing [10] Zhou, S., Zheng, X., Wang, J., and Delisle, P. 1994. Utopia: [11] Zadrozny, P., Kodali, R., 2013 . Big Data Analytics using [12] Foster, I., Zhao, Y., Raicu, I., Lu S., 2008. Cloud Computing [13] Grewal, M.S., Andrews A.P., 20 14. Kalman Filtering Theory [14] Friedman, J. H. 1991. Multiva riate Adaptive Regression [15] Widrow B., Rumelhard D.E., and Lehr M.A., Neural [16] Loh W.Y., 2011 . Classification and regr ession trees. WIREs 
