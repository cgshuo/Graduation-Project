 ORIGINAL PAPER Gheith A. Abandah  X  Fuad T. Jamour  X  Esam A. Qaralleh Abstract The Arabic alphabet is used in around 27 languages, including Arabic, Persian, Kurdish, Urdu, and Jawi. Many researchers have developed systems for recog-nizing cursive handwritten Arabic words, using both holistic and segmentation-based approaches. This paper introduces a system that achieves high accuracy using efficient segmenta-tion, feature extraction, and recurrent neural network (RNN). We describe a robust rule-based segmentation algorithm that uses special feature points identified in the word skeleton to segment the cursive words into graphemes. We show that careful selection from a wide range of features extracted dur-ing and after the segmentation stage produces a feature set that significantly reduces the label error. We demonstrate that using same RNN recognition engine, the segmenta-tion approach with efficient feature extraction gives better results than a holistic approach that extracts features from raw pixels. We evaluated this segmentation approach against an improved version of the holistic system MDLSTM that won the ICDAR 2009 Arabic handwritten word recognition competition. On the IfN/ENIT database of handwritten Ara-bic words, the segmentation approach reduces the average label error by 18.5%, the sequence error by 22.3%, and the execution time by 31%, relative to MDLSTM. This approach also has the best published accuracies on two IfN/ENIT test sets.
 Keywords Optical character recognition  X  Handwritten Arabic words  X  Segmentation  X  Feature evaluation and selection  X  Recurrent neural networks 1 Introduction Offline cursive handwriting recognition is a hard problem for which no satisfactory general solutions are yet available. Major challenges include the overlap and interconnection of neighboring characters, the huge variability in both quality and style of human handwriting, and the similarities of dis-tinct character shapes [ 9 , 33 ].

Arabic is the native language of more than 440million people, and its alphabet is used in around 27 languages, includingArabic,Persian,Kurdish,Urdu,andJawi[ 34 ].Ara-bic is always cursive in print and in handwriting. Despite decades of research, there is still a lack of accurate Arabic handwriting recognition systems [ 44 ].

Arabic handwritten script recognizers can be divided into holistic and segmentation approaches. Holistic approaches process the words as a whole without segmentation into smaller components. Segmentation approaches first segment the words into characters or strokes, then pass the segments to the recognizer [ 38 ].

Developing effective segmentation algorithms for cursive script is difficult and requires considerable expert language knowledge. However, such algorithms generally lead to a substantial gain in recognition accuracy, since they provide richer feature extraction and allow the recognition of open vocabulary [ 35 ].
Holistic approaches have so far proved more successful in limited-vocabulary handwriting recognition [ 44 ]. These approaches build on advances in other areas such as speech recognition and recurrent neural networks (RNNs) [ 14 , 28 ].
This paper presents an efficient system for recognizing handwritten Arabic words that combines efficient segmenta-tion, feature selection, and RNNs to achieve state-of-the-art accuracy. The main contributions are as follows:  X  A novel rule-based segmentation algorithm that segments cursivewordsintographemesbycollectingspecialfeature points from the word skeleton.  X  A selection of an efficient subset of features for recogniz-ing handwritten Arabic words through an evaluation of wide range of feature extraction techniques.  X  A demonstration that the segmentation approach with effi-cient feature extraction gives better label and sequence error rates than a holistic approach that extracts features from the raw pixels, using the same recurrent neural net-work recognition engine.
 This paper is organized as follows: The rest of this section reviews the Arabic writing system, gives an overview of related work in Arabic handwriting recognition, and provides an overview of our system X  X  processing stages. Section 2 describes the sub-word and grapheme segmentation stages. Section 3 describes the feature extraction stage and the algo-rithm used in selecting features. Section 4 describes the RNN used in the character sequence recognition stage. Section 5 describes the algorithms used in the word matching stage. Section 6 presents the experimental results, while Sect. 7 compares our system with the best alternatives, including a detailed comparison with the record holder for the ICDAR offline Arabic handwriting recognition competitions [ 44 ]. Finally, Sect. 8 provides conclusions and plans for future work. 1.1 Overview of Arabic writing Arabic is written from right to left and is always cursive. The Arabic alphabet has 28 basic letters [ 3 ]. Each letter has multiple forms depending on its position in the word. Each letter is drawn in an isolated form when it is written alone, and is drawn in up to three other forms when it is written connected to other letters in the word. For example, the letter Ain has four forms: isolated ( ) , initial ( ), medial ( ), and final ( ) .

Within a word, every letter can connect from the right with the previous letter. The horizontal level where letters are connected is the baseline . There are six letters that do not connect from the left with the next letter. These letters only have the isolated and final forms. When one of these six letters is present in a word, the word is broken into sub-words , often called parts of Arabic words (PAWs). For example, the word  X  X rabic X  ( of the initial Ain ( ) and final, left-disconnecting, Reh and the second PAW consists of the initial Beh ( ), medial Ye h ( ), and final Teh Marbuta ( ). 1.2 Related work Many researchers have developed algorithms to segment handwritten Arabic words into smaller components [ 6 , 32 , 37 , 45 , 55 , 59 ]. A review of several relevant segmenta-tion algorithms is available in Abandah and Jamour [ 1 ]. Segmentation-based papers generally report the segmenta-tion accuracy and do not report overall recognition accuracy or report accuracies that are lower than the holistic systems.
Wshah et al. [ 59 ] use the skeleton and the boundary of the word to do over/under segmentation. The output of the algo-rithm is graphemes that represent anything between one fifth of a letter and three connected letters. They report that their algorithm segments 93% of the test samples into segments that each has at most one letter.

Kundu et al. [ 32 ] developed a complete system for the recognition of unconstrained handwritten Arabic words using over-segmentation of characters and variable duration hidden Markov model. Their segmentation algorithm trans-lates the 2-D image into 1-D sequence of sub-character sym-bols. They report recognition accuracies of only 66 and 60% on IfN/ENIT database test sets d and e [ 48 ], respectively.
Furthermore, many researchers have experimented with a wide range of feature extraction techniques to recog-nize handwritten Arabic text. Some examples can be found in Amin [ 8 ], El Abed and M X rgner [ 15 ], El-Hajj et al. [ 16 ], Lorigo and Govindaraju [ 38 ], Safabakhsh and Adibi [ 54 ]. Feature selection is often used for excluding irrelevant and redundant features. When successful, it can greatly reduce system complexity and processing time as well as improving recognition accuracy [ 29 ].
 M X rgner, El Abed, and Pechwitz have organized a series of Arabic handwriting recognition competitions [ 40  X  44 ]. The purpose of these competitions is to advance the research and development of Arabic handwritten word recognition sys-tems. These competitions use the IfN/ENIT database and have had excellent participations from the research leaders in the area. The participations have shown remarkable progress over seven years. In the following paragraphs, we review a selection of systems that have participated in these compe-titions and achieved top results. Moreover, the performance of these systems is summarized in Sect. 7.1 .

The UOB system developed by Al-Hajj et al. [ 5 ]won the first competition in ICDAR 2005. This system estimates the word baseline and extracts robust language independent features using a sliding window, without character segmen-tation. The system uses a pure HMM recognizer originally developed for speech recognition.
 The competitions organizers also participated with their ARAB-IFN system in ICDAR 2005. This system got the second place and is described in Pechwitz et al. [ 47 ]. The authors performed a thorough analysis of the IfN/ENIT data-base. They experimented with advanced techniques to esti-mate the word baseline and top line and used these estima-tions to normalize the word. The normalization goal is to make the feature extraction more robust against size, slant, skew, and line width variations of a word. They used two feature extraction methods that have similar performance. The first method extracts pixel features using a sliding win-dow with three columns, and the second method extracts skeleton direction features in five zones using overlapping frames.

The Siemens system submitted by Alary et al. was the winner of the ICDAR 2007 competition. This system was adapted for Arabic script from the standard HMM-based Latin script word recognizer that is widely in use within Siemens AG for postal automation projects [ 56 ].
The MDLSTM system developed by Graves was the win-ner of the ICDAR 2009 competition. This system uses a hier-archy of multidimensional RNNs [ 28 ]. It is further described in Sect. 7.2 . This system holds the record for recognition accuracy on the main competition test set, outperforming the winners of both ICFHR 2010 and ICDAR 2011 competitions.
The UPV-PRHLT system developed by Alkhoury et al. [ 7 ] was the winner of the ICFHR 2010 competition. This system uses windows of raw, binary image pixels, which are directly fed into embedded Bernoulli HMMs. The authors found that best results are obtained with a nine-column window and when the extracted window is repositioned to align its center of mass with the window center.

The RWTH-OCR system developed by Dreuw et al. [ 14 ] was the winner of the ICDAR 2011 competition. This system uses sliding window for HMM-based handwriting recogni-tion. Discriminative training based on the maximum mutual informationcriterionisusedtotrainwriterindependenthand-writing models. Writer adaptation is used to improve accu-racy through an unsupervised confidence-based discrimina-tive training on a word and frame level within a two-pass decoding process.

The majority of these recognition engines use HMMs to build letter, PAW, or word models. The engines that build PAW or word models suffer from limitations in recogniz-ing PAWs or words that are not represented in the training set; they are generally limited-vocabulary recognizers. More-over, they generally rely on sliding windows to extract fea-tures from the word image. Such windows limit the variety of features that can be extracted. The size of these windows often requires tuning according to the writing style, which significantly affects the recognition accuracy as reported in Alkhoury et al. [ 7 ].

Our system segments words to elementary graphemes to eliminate the problems associated with building PAW and word models; thus, it may support open vocabulary. It also eliminates the need to tune the feature extractor for differ-ent writing styles without sacrificing the recognition accu-racy. The use of an RNN with bi-directional long short-term memory (BLSTM) enables high letter recognition rates. This accuracy is possible even with difficult letters because of BLSTM X  X  ability to exploit context information. The use of connectionist temporal classification (CTC) output layer facilitates training without the need to manually segment and annotate the letters in each word. 1.3 System overview Figure 1 summarizes the five main stages of our approach to recognizinghandwrittenArabicwords:(i)sub-wordsegmen-tation, (ii) grapheme segmentation, (iii) feature extraction, (iv) sequence transcription, and (v) word matching. Unlike most of the available recognition systems, the sequence tran-scription stage does not use a language model. Thus, word matching in our system is optional, and words can be added or removed from the dictionary to enhance the word recog-nition accuracy without the need to re-train the system. An earlier version of this approach, based on modifying the OCR system  X  X esseract X  [ 58 ], is described in Abandah and Jamour [ 1 ]. Tesseract was chosen then because it is an accurate and open-source system.
 In this paper, we only consider isolated word recognition. When the system is used to recognize complete documents, line and word segmentation must be added as preprocessing stages.

We now describe each of the five recognition stages in detail. 2 Segmentation The input word is segmented into graphemes in two stages; The word is first segmented into sub-words, then the sub-words are segmented into graphemes. 2.1 Sub-word segmentation In this stage, the baseline is estimated first; then the main and secondary bodies are identified, the main bodies of the sub-words are extracted, and the secondary bodies are assigned to their respective main bodies to yield the sub-words.
We estimate the word baseline using the horizontal pro-jection histogram method [ 39 ]. The row that contains the maximum number of black pixels is the baseline. This sim-ple method is sufficient for our purposes. We only use the baseline estimation in recognizing secondary bodies and extracting some configuration features, as described below.

We use a contour-based connected components extrac-tion algorithm to identify the components of the image and its secondary bodies [ 11 ]. A component is classified as a sec-ondary body when one of the following conditions applies: (i) it is very small compared with other components in the same image, (ii) it is relatively small and far from the baseline, or (iii) it is a vertical line with a relatively large component below it. The light bodies in Fig. 2 a are examples of identified secondary bodies and are labeled s1 through s8. Furthermore, secondary bodies that are close to each other and are similar in size are considered one secondary body group , e.g., s8. As explained below, this grouping is important in grapheme separation.
Components that are not secondary bodies are the main bodies of the sub-words. Every main body is extracted with its secondary bodies as one sub-word and is passed to the next stage. Secondary bodies are examined from right to left. For every secondary body, the algorithm assigns it to a main body according the the following rules (first that applies): (i) the main body that is above its midpoint, e.g., s1, (ii) below its midpoint, e.g., s4, (iii) above its left endpoint, e.g., s8, or (vi) the rightmost main body in the word, e.g., s5. In Fig. 2 b, the upper word contains three sub-words (labeled w1 through w3), and the lower word contains four sub-words (labeled w4 through w7). Additionally, any overlap between adjacent sub-words is eliminated by separating them horizontally. 2.2 Grapheme segmentation We now describe the two steps of grapheme segmentation: feature points detection and grapheme separation. 2.2.1 Feature point detection In this step, morphological features are detected in the skele-tons of the sub-words. The secondary bodies are temporarily removed before using Deutsch X  X  thinning algorithm to get the skeleton of the sub-word X  X  main body [ 12 ]. This algorithm is robust and works well with handwritten Arabic script. How-ever, similar to other algorithms, it sometimes removes fine features such as Seen teeth ( ) [ 3 ].

The feature points detected are end points , branch points , and cross points . They are detected by examining the eight neighbors of every skeleton pixel. An end point has one black neighbor, a branch point has three, and a cross point has four. Our algorithm identifies continuities to detect edge points. A continuity is a continuous string of black pixels in the skeleton that connects two feature points. If the starting point is the ending point, then the continuity is a loop.
The edge points are points where the direction of the con-tinuity changes and are detected using the polygonal approx-imation of the skeleton. The algorithm used to find the polyg-onal approximation is due to Douglas and Peucker [ 13 ].
Each vertex in the polygon that is not an end point, branch point, or cross point is considered an edge point. For each edge point, the bisector angle is found. This is the angle between the bisector line of the edge and the horizontal line. For more details and examples, refer to Ref. Abandah and Jamour [ 1 ]. Figure 3 shows example three sub-words with their feature points identified. The edge point marked with BA  X  0 X  is an example edge point with a horizontal bisector.
Once the feature points are detected, a continuity that con-tains them is split into as many child continuities as needed so that each new continuity connects two feature points and that each has two ends. The orientation of the continuity and the attributes of its ends are used to find the segmentation points, as explained in the following subsection. 2.2.2 Grapheme separation Continuities that have the following properties are segmented to split the sub-word into graphemes. Figure 3 shows three continuities that are segmented at the shown points and marked C1, C2, and C3. Other example continuities that are not segmented are marked N1 through N5. 1. Low slope: the orientation of the continuity should be 2. If the right end is an edge, its bisector angle should be 3. The left end is not an end point. N3 and N4 are not seg-4. If the left end is an edge, its bisector angle should be 5. It is not totally covered from above or from below. This The angle ranges above are selected to cover the various cases of Arabic sub-word configurations. For example, the sub-word ( ) contains two letters: the right letter is initial Noon ( ) that has an edge with bisector angle of 135 X , and the left letter is final Alef ( ) that has an edge with bisector angle of 45 X . The horizontal continuity connecting the two edge points in this sub-word satisfies the five properties above and is segmented.

Each continuity that should be segmented is then exam-inedtofindthebestcutpointinit.Thecutpointissearchedfor starting from the first left quarter of the continuity. The first point in a horizontal small segment of the continuity is the cut point. If the cut point happens to cut a letter (by inspecting the stroke width at this point), the cut point is shifted to the point where the stroke width is minimum. If the continuity has no horizontal segments, its midpoint is selected as the cut point. This method enhances the accuracy of secondary body assignment.

Finally, the secondary bodies are reassigned to the seg-mented graphemes using the assignment method described in Sect. 2.1 . The entire secondary body group is assigned to one grapheme because such group belongs to one letter. The segmented main bodies and their respective secondary bodies are then passed to the feature extraction stage. 3 Feature extraction and selection As the above segmentation algorithm identifies sub-words, secondary and main bodies, and feature points, it collects features of the identified and segmented bodies. 3.1 Feature extraction Additional features are extracted in the feature extraction stage. We started with extracting a total of 103 features that are listed in Table 1 . These features can be grouped into six classes: statistical, configuration, skeleton, boundary, elliptic Fourier descriptor, and directional features and are described inthefollowingparagraphs.Moredetailsabouttheextraction of these features can be found in [ 3 , 4 ]. 3.1.1 Statistical features These 15 features are extracted from the object X  X  binary image. The area is calculated from the binary image using A = are also used to derive the scale invariant feature: width to height ratio W / H . The fraction of black pixels in each of the four quadrants is UR / A , UL / A , LL / A , and LR / A .
The object X  X  center of mass ( x , y ) is used in computing the normalized central moments of order ( u + v) by  X  u ,v In this work, we only generated the top two most relevant moments:  X  2 , 0 and  X  0 , 2 [ 4 ].

The normalized center of mass ( x N , y N ) is calculated
The orientation  X  of an elongated object is the orientation of the elongation axis with respect to the horizon [ 3 ]. 3.1.2 Configuration features These 10 features are extracted from the configuration of the object and its surroundings. Relative to the word X  X  baseline, we extract three features: the fraction of pixels above the baseline U / A , the distance from the center of mass to the baseline D y , and the distance from the top black pixel to the baseline D To p .

The Loops feature is the number of closed loops in the object. The Fo r m feature is a categorical feature of the posi-tion of the object in its sub-word.

The remaining five features are related to the object type and its secondaries. The binary feature Is _ Sec is true when the object is a secondary object. For a main object, the num-ber of its secondary objects is S . S a and S b specify for a main object the number of secondary bodies above it and below it, respectively. The secondary configuration Sec _ Conf fea-ture specifies one of four main object types: object without secondaries, with secondaries above it, below it, and with secondaries above and below it. 3.1.3 Skeleton features These four features are extracted from the object X  X  skeleton. Branchs is the number of branch points in the object X  X  skele-ton, and Ends is the number of end points. The features E and E 2 are edge points experimental features computed as the sum of moments of the bisection angles of the body X  X  edge points using E 1 = i x i y i | BA i | and E 2 = i x i y i respectively. 3.1.4 Boundary features These four features are extracted from the object X  X  bound-ary. Figure 4 shows an example handwritten letter and its boundary. For the outer boundary pixels ( x ( t ), y ( t )), 1 , 2 ,..., m , the number of boundary pixels is m . The Free-man chain code is used to compactly encode the boundary pixels [ 17 ]. The direction from every boundary pixel to the next boundary pixel is put in the chain. The direction codes f ( t )  X  X  0 , 1 ,..., 7 } are used such that right is 0, up-right is 1, up is 2, etc.
 The perimeter length is T = m t = 1 L ( f ( t )) where L ( f ( t )) = 1for f ( t ) even and ter to diagonal ratio is T / 2 D = ( T / 2 )/ compactness ratio  X  = T 2 / 4  X  A . 3.1.5 Elliptic Fourier descriptors The outer boundary is a piece-wise linear closed curve and is used in extracting the elliptic Fourier descriptors (EFD) [ 31 ]. The four descriptors of order n are found by a b c d where  X  = 2 n  X  t y i = y ( i )  X  y ( i  X  1 ), t i = x 2 i + y 2 i , (5) t = We extract a set of 26 EFDs comprising a 0 , c 0 , and a , b 3.1.6 Directional features The directional features are extracted from the chain codes of the object X  X  boundaries [ 36 ]. Only four of the eight directions are relevant as the last four directions are mirror images of the first four. The four directional features D d for d = 0 , are defined as D d = t C d ( f ( t )) where C d ( f ( t )) = ( f ( t ) mod 4 ) = d and 0 otherwise. Figure 4 shows four images for the boundary pixels of directions codes d = 0, 1, 2, and 3, respectively. In other words, the feature D d is the number of pixels of direction code d .

To exploit the topological distribution of the directional features, the object is split into regions of R rows and C columns. Then, the four directional features are found for each region separately to get features D R , C r , c , R  X  C split, there are 4 RC directional features where r = 0 , 1 ,..., R  X  1, c = 0 , 1 ,..., C  X  1, and d = 0 , 1 , 2 , els in Row 0, Column 1 of an image split into 2 rows and 3 columns.

We extracted features from three splits 1  X  1, 2  X  2, and 2  X  3, totaling 4  X  ( 1  X  1 + 2  X  2 + 2  X  3 ) = 44 features. 3.2 Feature selection Feature selection is important in many pattern recognition problems for excluding irrelevant and redundant features. It typically decreases system complexity and processing time and often improves recognition accuracy [ 29 ].

We tested several feature selection techniques ranging in complexity from simply picking the best individual features to evolutionary optimization algorithms [ 4 ]. We concluded that the minimal-redundancy-maximal-relevance (mRMR) technique [ 50 ] offers the best compromise between accu-racy and speed, and therefore we use this technique in this paper.

In the mRMR algorithm, the subset S of m best features is grown iteratively from the complete set of features X using forward search algorithm. The following criterion is used to add the x j feature to the previous subset of m  X  features: arg max This algorithm maximizes the difference between the mutual information of the feature x j and class  X  ( relevance ) and the mean of the mutual information values between feature x j and previously selected features x i  X  S m  X  1 ( redundancy ). Where the mutual information for two random variables x and y is found in terms of their probabilistic density functions p ( x ) , p ( y ) , and p ( x , y ) as I ( x ; y ) = p ( x , y ) log As described in Sect. 6.3 . we use a feature subset of length m = 30. For each object i ,the m selected features are assem-bled in a feature vector x i 1 , x i 2 ,..., x im .
Using feature statistics from the training samples, the fea-ture vector is normalized to zero mean and unit standard deviation using  X  x ik = ( x ik  X  x k )/ X  k ,for k = 1 , 2 The normalized vector is then passed to the sequence tran-scription stage. 4 Sequence transcription The sequence transcription stage maps from sequences of feature vectors to sequences of recognized characters. Our sequence transcription is carried out using a RNN with the BLSTM [ 27 ]. The CTC [ 23 ] output layer is used to determine a probability distribution over all possible char-acter sequences, given a particular feature sequence. A list of the most probable output sequences are then selected and passed along to the final word matching stage of recognition.

The combination of BLSTM and CTC has been success-fully applied to both online and offline handwriting recogni-tion in the past [ 26 ]. Our experiments on BLSTM X  X TC were carried out with the open source software library RNNLIB [ 19 ]. 4.1 Bi-directional long short-term memory Recurrent neural networks (RNNs) exploit the sequence con-text through cyclic connections in the hidden layer [ 20 ]. Given an input sequence ( x 1 ,..., x T ) , a traditional RNN computes the hidden vector sequence ( h 1 ,..., h T ) and the output sequence ( y 1 ,..., y T ) by iterating the following equations from t = 1to T : h = H ( W y = Y W where W xh is the input-hidden weight matrix, W hh is the hidden-hidden weight matrix, W hy is the hidden-output weight matrix, b h and b y are bias terms, H is the hidden layer function and Y is the output layer function.
In traditional RNNs, H is usually element-wise applica-tion of the tanh or logistic sigmoid  X ( x ) = 1 /( 1 + exp functions. However, the long short-term memory (LSTM) architecture is better at finding and exploiting long range context information [ 18 , 30 ]. The LSTM blocks replace the nonlinear units in the hidden layer of traditional RNNs [ 30 ]. Figure 5 shows an LSTM block which consists of a core state cell and three gates. The input gate controls storing into the state cell and allows holding information for long periods of time. The forget gate affects the internal state, and the output gate controls the output activation function.

For the version of LSTM used in this paper, H is imple-mented by the following composite function:  X   X  s  X  h where  X  ,  X  ,  X  , and s are respectively the input gate , forget gate , output gate , and state vectors, all of which are the same size as the hidden vector h . The weight matrix subscripts have the obvious meaning, for example, W h  X  is the hidden-input weight matrices from the state to gate vectors are diagonal, so element m in each gate vector only receives input from element m of the state vector. The bias terms (which are added to  X  ,  X  , s , and  X  ) have been omitted for clarity.
A bi-directional RNN [ 57 ] computes the forward hidden sequence ( (  X   X  h 1 ,..., first iterating the backward layer from t = T to 1:  X   X  h then iterating the forward and output layers from t = 1to T :  X   X  h y = Y W  X   X  The advantage of using bi-directional RNNs (BRNNs) is that the output layer is able to make use of both past and future context at each point along the sequence. Combing BRNNs with LSTM gives bi-directional LSTM (BLSTM; [ 27 ]), the RNN architecture used throughout this work. 4.2 Subsampling layers Using multiple bi-directional RNN hidden layers can lead to improved performance, compared with a single-level archi-tecture. However, it can lead to a very large number of con-nection weights between the forward and backward layers in successive levels, which may increase overfitting as well as computationalcost.Onewaytoreducethenumberofweights is to separate the levels with feedforward subsampling lay-ers . Figure 6 shows one subsampling layer between the two hidden layers of a neural network.

The two layers in each level feed forward to a subsampling layer, which then feeds forward to the two layers in the next level up. The total number of inter-level weights can there-fore be controlled by adjusting the sizes of the subsampling layers. For the experiments in this paper, all the subsampling layers consist of standard summation units with tanh activa-tion functions.

The RNN topology (specifically the number of hidden layers, and the number of LSTM cells in each hidden layer)wasoptimizedthroughexperimentationasdescribedin Sect. 6.4 . 4.3 Connectionist temporal classification Connectionist temporal classification (CTC) is an output layer designed for sequence transcription with RNNs [ 23 ]. It trains the network to predict a conditional probability dis-tribution over all possible output transcriptions, or label-ings , given the complete input sequence. It therefore does not require pre-segmented training data.

A CTC output layer contains one more unit than there are elements in the alphabet L of labels for the task. The first |
L | outputs estimate the probabilities of observing the corre-spondinglabels at that time, andtheextraoutput estimates the probability of observing a  X  X lank X , or no label. For a length T input sequence x , the complete sequence of CTC outputs therefore defines a probability distribution over the set L length T sequences over the alphabet L = L  X  X  blank } .We refer to the elements of L T as paths . Since the probabilities of the labels at each time step are conditionally independent given x , the conditional probability of a path  X   X  L T is given by p ( X  | x ) = where y t k is the activation of output unit k at time t . Paths are mapped onto labelings l  X  L  X  T by an operator B that removes first the repeated labels, then the blanks.The conditional probability of some labeling l  X  L  X  T is the sum of the probabilities of all paths corresponding to it: p ( l | x ) = This  X  X ollapsing together X  of different paths onto the same labeling is what allows CTC to use unsegmented data, because it means that the network only requires the order of the labels to define its distribution, and not their alignment with the input sequence.

For a training set S , consisting of pairs of input and tar-get sequences ( x , z ) , the CTC objective function O is the negative log probability of the network correctly labeling all of S: O = X  This function can be efficiently differentiated with respect to the network outputs using the CTC forward X  X ackward algo-rithm [ 23 ]. Back propagation through time [ 53 ] can then be used to find the derivatives of O with respect to the weights, and the network can be trained with gradient descent. 4.4 Error measures A common error measure for sequence transcription tasks is tions, deletions, and substitutions on the test set, divided by the total number of target labels in the test set, and multiplied by 100.

For complete word recognition, however, a more appro-priate error measure is the word, or sequence, error rate , which is simply the fraction of test set sequences that were correctly transcribed. 5 Word matching When a dictionary is available, a word matching algorithm is needed. Word matching can greatly improve accuracy since many transcription mistakes that result in non-dictionary words are corrected. The example in Fig. 1 shows how word matching corrects the miss-transcribed last letter from Lam
The two algorithms we use select the k most probable dic-tionary words using the RNN output of the sequence tran-scription stage. If the dictionary is a set of words S , and the output sequences are l  X  L  X  T , then the word matching stage outputs the k words s  X  S that have lowest values of the cost function D ( s , L ) , where L is the set of the l most probable output sequences (highest p ( l | x ) ). This function estimates the average distances between the dictionary word and the output sequences. The following subsections describe the two word matching algorithms used. 5.1 Weighted edit distance The weighted edit distance algorithm WED is based on the edit distance between two sequences ed ( l , s ) [ 52 ] and is found through the summation D ( s , L ) = The standard edit distance is defined as the minimum total number of changes, insertions, and deletions required to change pattern l to pattern s . However, in WED , changes take values between 0 and 1 that represent how dissimilar the two interchanged letters are. For this purpose, we cre-ated a look-up table based on the Arabic letter shapes. For example, as the letters Te h and Theh have similar shapes, the table gives low change value for these two letters. More detail on this algorithm is in [ 2 ]. 5.2 RNNLIB word matching The RNNLIB has a word matching algorithm that is inte-grated in the CTC output layer. This algorithm is the CTC token passing algorithm described in [ 21 , 24 ]. Through this algorithm, the CTC labeling is constrained to only the sequences of the complete dictionary words. For any word that has variant spellings, this algorithm sums the probabili-ties of all the word X  X  variants to find the word probability. 6 Experimental results This section describes the experiments carried out to tune the recognition system for efficient results. 6.1 Samples Our experiments are based on the IfN/ENIT database of handwritten Arabic words [ 48 ]. This database is used by more than 110 research groups in about 35 countries [ 44 ]. The database version used is v2.0p1e and consists of 32,492 Arabic words handwritten by more than 1,000 writers. These words are 937 Tunisian town/village names. This database is divided into five training sets and two test sets. The numbers of names, PAWs, and characters in each of the sets are shown in Table 2 .

The two test sets were publicly unavailable and were only used in competitions. Therefore, we use the five training sets for training, validation, and testing as described below. How-ever, the two test sets were recently released, and we were able to use them in some tests as shown in Table 5 .
In the rest of this paper, we show results of experiments carried out by training the system using some of these sets and testing using some other set. We refer to each experiment by its training sets followed by its test set, separated by a hyphen. For example, abcd-e experiment indicates that the training sets are a through d and the test set is e . Ten percents of the training samples are randomly held out for validation in the RNN training. 6.2 Segmentation algorithm evaluation We have evaluated the segmentation algorithm through inspection. Figure 7 shows four output examples of this algo-rithm. The four examples a through d consist of 7, 8, 8, and 7 letters, respectively. Most letters are segmented into one grapheme, but some are over-segmented into two or three. The over-segmented letters in these examples are: in words b and c, in word d, and in word d. It is the responsibility of the transcriber in the following stage to map one or more graphemes into their corresponding letters. Note that there are also some cases (not shown) where multiple letters in a vertical ligature are segmented into one grapheme.
Table 3 summarizestheevaluationof107samplesselected from the IfN/ENIT database. The segmentation algorithm produces the expected graphemes accurately in more than 96% of the samples. Only one sample is under-segmented and three samples suffer over-segmentation. This evaluation was done by human inspection at an early stage of the sys-tem development to ensure that the segmentation algorithm is accurate. As illustrated in Sect. 7 , the overall system eval-uation on tens of thousands of samples validates the results of this early evaluation.
 6.3 Selecting features We used the mRMR feature selection tool developed by H. Peng to select the best subsets of features for recog-nizing the segmented objects [ 49 ]. We extracted 103 fea-tures from 17,943 objects. These sample objects belong to 1,696 randomly selected word images of sets a and b : 1,233 words from set a and 463 words from set b .The features along with their grapheme codes were presented to the mRMR tool. This tool discretizes continuous fea-tures using their means and standard deviations at thresh-olds  X  x k  X  nk  X  k , where n = 0 , 1 , 2 ,... and we selected k = 0 . 2. Table 4 lists the mRMR output of the 103 features ordered according to their mRMR scores (where  X  X core X  is  X  X elevancy minus redundancy X  of the feature as computed by Eq. 6 ).

It is interesting to note that the top two features are the configuration features: Is _ Sec that distinguishes a sec-ondary object from a main object and Fo r m that specifies the object X  X  position in the sub-word. Note also that the subset of m = 30 includes only three statistical features, five configuration features, two boundary features, seven EFDs, and 10 directional features mainly of the 2  X  2-region split.

To find how many features are needed to achieve good recognition accuracy, we find the label error as a function of the number of features used. In each experiment, we use best m mRMR features; for m from 5 to 103 in varying steps. The results are shown in Fig. 8 . In each experiment, we train the 3S RNN (described the next subsection) using the m features from sets a through d and find the label error of the test set e ( abcd-e experiments).

The curve in this figure shows that the label error decreases rapidly as m increases from 5 to 20 and does not improve for m &gt; 30. Therefore, we adopt the top 30 features only in our system. The remaining 73 features are not used in the final system as their extraction does not provide added accuracy.

Set e is used here for testing because it is the hard-est; it has the highest label error among the five sets. However, the other sets demonstrate similar curve shapes but with lower label error and curve knees at smaller m values. 6.3.1 Simple feature sets The set of 30 features used in our final system requires mul-tiple feature extraction techniques. We also performed four more experiments to test simpler feature sets; their results are shown as four points in Fig. 8 . The four sets and their label errors are: EFD is the 26 elliptical Fourier descriptor features, 28%. Dir is the 44 directional features, 32%.
 EFD+6 is the 26 EFDs and six selected features, 22%.
Dir+6 is the 44 directional features and six selected fea-The six selected features are: A , W , H , Is _ Sec , Fo r m , and D y .Thesefeaturesareselectedbecausetheyarereadilyavail-able from the segmentation process and have relatively high mRMR score.
 The homogeneous sets of EFD or Dir have low accuracy. However, complementing them with the six selected features produces simple feature sets with high accuracy. Note that the best label error achieved from the entire 103 features is 21%. 6.4 RNN tuning We carried out two sets of experiments: the first set for select-ing the number of layers and the second set for selecting the size of each layer. Similar to the feature selection exper-iments, we used abcd -e experiments. As neural network training involves some randomness, each configuration was repeated four times to get the average performance.
To select the number of layers, we used the following configurations: 1 One hidden layer of size 100 2 Two hidden layers of sizes 60 and 180 2S Two hidden layers of sizes 60 and 180 with sub-sampling 3 Three hidden layers of sizes 40, 80, and 180 3S Three hidden layers of sizes 40, 80, and 180 with two 4 Four hidden layers of sizes 40, 80, 120, and 180 These layer sizes are the default sizes that are found in the RNNLIB library X  X  configuration files. Figure 9 shows the results of testing these six configurations. These results show that the accuracy improves with more layers and with using sub-sampling layers. However, the accuracy does not increase when increasing the number of layers from three to four. Therefore, we adopt the topology of three layers with two sub-sampling layers.

In order to achieve higher accuracy, we use layer sizes larger than those used in the default 3S configuration [ 51 ]. The selected tuned neural network size: three hidden layers of sizes 100, 100 and 360 with two sub-sampling layers of sizes 120 and 180. This configuration achieves a label error of 19.8%, whereas the label error of the 3S configuration is 21.0%. The tuned RNN size is used in the following com-parison. 7 Comparison This section compares our results with the results of other best systems. We also present a detailed comparison with the MDLSTM system by Graves and Schmidhuber which has the best accuracy on test set f [ 21 ]. 7.1 Arabic handwriting recognition competitions Table 5 summarizes the results of the best systems that have participated in the series of Arabic handwriting recognition competitions. We also include here the results of: Our earlier system JU-OCR that has participated in ICDAR 2011, the recently published system AUC, an improved version of the systemthathaswonICDAR2009competition(MDLSTM2), and the system described in this paper JU-OCR2. The table shows the word recognition accuracy of four test sets ( d , e , f , and s ) and the training sets used. For test sets d and e ,the table also shows the percentage of correct result within the first five results and within the first 10 results, when avail-able. The UOB, Siemens, UPV-PRHLT, RWTH-OCR, and AUC systems X  results for these two test sets are the published results [ 7 , 10 , 14 ].

In the first competition (ICDAR 2005), set e was unknown to the participants and was used as the test set. The UOB system scored 75.93% on this set. In later competitions, set e was made available to the participants and new test sets (sets f and s ) were used.

Our earlier system JU-OCR has relatively low accu-racy. Although it uses the grapheme segmentation algorithm described in this paper, but it uses inferior classifier (ran-dom forests) and less efficient feature set and word matching algorithm [ 44 ].

The last two rows in Table 5 summarize the word recogni-tion accuracy of our experiments conducted to evaluate JU-OCR2 against MDLSTM2. These results illustrate that the two systems hold the best published accuracies on the four test sets. JU-OCR2 has the highest accuracy on set d and s at 98.96 and 84.80%, respectively. MDLSTM has the highest accuracy on sets e and f at 94.76 and 94.13%, respectively. The JU-OCR2 results are using WED word matching. How-ever, the abc-d result is using the RNNLIB word matching because it has the best accuracy here (98.96 vs. 98.75%). We didn X  X  expect that JU-OCR2 scores lower than MDL-STM2 on set e because, as shown below, the former sys-tem has lower label and sequence error rates. However, it seems that MDLSTM2 benefits more from the integrated word matching stage and achieves higher accuracy on set e .
The following subsections describe the MDLSTM system and provide a detailed comparison between it and JU-OCR2. 7.2 Multidimensional LSTM (MDLSTM) A system similar to the RNN employed in this work won the Arabic, Farsi and French offline handwriting recognition competitions at ICDAR 2009. This system also employed a CTC output layer together with the LSTM network archi-tecture to transcribe handwritten text. The main difference is that the network used for ICDAR 2009 extracted input fea-tures directly from the raw pixel data of the images, using a hierarchy of network layers. Because images are two dimen-sional, this necessitated the use of multidimensional LSTM (MDLSTM; [ 25 ]). MDLSTM differs from ordinary BLSTM in that there are four distinct hidden layers instead of two, and each of these layers receives information from two previous states instead of one. The complete MDLSTM recognition system is described in detail in [ 22 ].

The advantage of using raw pixels is that the system can be easily adapted to new languages. However, as the results in the next section demonstrate, expertly chosen input features from the segmented sub-words can give better performance. 7.3 Detailed comparison Figure 10 showsthelabelerroroffivetestsetsandtheaverage label error on MDLSTM and JU-OCR2. The figure demon-strates that JU-OCR2 achieves better label error; its average label error is 8.63% and the MDLSTM average label error is 10.39%. Relative to MDLSTM, JU-OCR2 average label error is 16.9% lower. These results are obtained using the open-source RNNLIB version that was used by MDLSTM in ICDAR 2009 competition [ 19 ].

We have also repeated the above experiments using a new version of RNNLIB, and their results are shown in Fig. 11 . The new version has slightly better performance as it has someminorbugssolved.However,theresultsofbothsystems shown in Fig. 11 are much better than the results in Fig. 10 . The main reason for this improvement is that both systems were trained with the weight noise option. Injecting weight noise is used to improve the convergence and generalization abilities of RNNs [ 46 ]. We have noticed that best results are achieved when we first train the system without this option then retrain it with this option starting from the weights of the last best epoch. JU-OCR2 achieves better label error for all sets; its average label error is 5.55% and the MDLSTM2 average label error is 6.81%. Relative to MDLSTM2, JU-OCR2 average label error is 18.5% lower.

Figure 12 shows the sequence error of the five test sets on the two systems using the experiments described above. Consistent with the label error results, JU-OCR2 achieves better sequence error for all sets; its average sequence error is 24.28% and the MDLSTM2 average sequence error is 31.26%. Relative to MDLSTM2, JU-OCR2 average sequence error is 22.3% lower.
 Moreover, although MDLSTM is the fastest system in ICDAR 2009 [ 42 ], JU-OCR2 is faster. The JU-OCR2 X  X  aver-age time to recognize one image in the abcd-e experiment is 147 versus 213ms for MDLSTM2 (31% advantage). As MDLSTM2 processes the entire image pixels in the RNN sequence transcription, it takes 137ms in sequence transcrip-tion and word matching versus only 73ms in JU-OCR2. The remaining 76ms of MDLSTM2 are spent preparing the input files from the word images, whereas, the remaining 74ms of JU-OCR2 are spent in the segmentation and feature extrac-tion stages. These experiments were carried out on Ubuntu 12.04 computer with Intel Core 2 Quad CPU Q9550 running at 2.83GHz and equipped with 2GB memory. 8 Conclusion We have described the JU-OCR2 system for recognizing handwritten Arabic words. This system segments the cur-sive words into graphemes; most of the letters are segmented into one grapheme each, some letters are over-segmented into multiple graphemes, and some vertical letter ligatures are under-segmented into one composite grapheme each. A extracted from the segmented bodies. The feature vector is passed to an RNN sequence transcriber that features BLSTM to exploit grapheme contexts and achieve high recognition accuracy.

The transcriber maps the feature vectors of the segmented bodies directly into letters and is able to achieve a low label error rate (5.55% on average for the IfN/ENIT database), without the use of a dictionary. This suggests that the sys-tem may be suitable for unlimited vocabulary recognition. The high label accuracy is made possible by the robust seg-mentation algorithm, the careful feature selection, and the well-tuned RNN.

Although the best systems in recent Arabic handwriting recognition competitions have used holistic approaches, we have demonstrated that our segmentation-based approach gives lower label and sequence error rates and has best pub-lished accuracy on some test sets. The MDLSTM system that holds the record accuracy in these competitions on test set f uses the same RNN classifier as JU-OCR2. The main difference between the two systems is the feature extraction approaches. Whereas MDLSTM extracts features from the raw pixels, JU-OCR2 extracts features from the segmented bodies.
 We evaluated JU-OCR2 against an improved version of MDLSTM. Compared with MDLSTM2, JU-OCR2 reduces the label error, sequence error, and execution time by 18.5, 22.3, and 31%, respectively.

We have inspected 100 randomly selected, miss-classified samples to find what went wrong. As Table 6 shows, we clas-sify these samples according to the problem source into four types: transcription problem, bad hand writing, segmentation problem, and feature extraction problem (ordered from most to least frequent). The table shows eight example samples, two of each type with explanations. We think that many of these cases can be solved by improving the segmentation, feature extraction, and transcription stages.

As part of our future work plans, we intend to improve the system to solve these problems. We also plan to test our system on complete documents with unlimited vocabulary. We also intend to add some pre-processing stages that have been proven to enhance the recognition accuracy [ 7 , 10 , 14 ]. References
