 ORIGINAL PAPER Oleg Golubitsky  X  Stephen M. Watt Abstract We study online classification of isolated hand-written symbols using distance measures on spaces of curves. We compare three distance-based measures on a vector space representation of curves to elastic matching and ensembles of SVM. We consider the Euclidean and Manhattan distances and the distance to the convex hull of nearest neighbors. We show experimentally that of all these methods the distance to the convex hull of nearest neighbors yields the best classi-fication accuracy of about 97.5%. Any of the above distance measures can be used to find the nearest neighbors and prune totally irrelevant classes, but the Manhattan distance is pref-erable for this because it admits a very efficient implemen-tation. We use the first few Legendre-Sobolev coefficients of the coordinate functions to represent the symbol curves in a finite-dimensional vector space and choose the opti-mal dimension and number of bits per coefficient by cross-validation. We discuss an implementation of the proposed classification scheme that will allow classification of a sam-ple among hundreds of classes in a setting with strict time and storage limitations. 1 Introduction Distance measures play a central role in most pattern recogni-tion problems. The choice of the distance measure affects the accuracy, as well as the time and space complexity of the clas-sification algorithms. The latter aspect is particularly impor-tant for online handwriting recognition, since it is desirable to perform this task in real time and on inexpensive hardware.
We are particularly interested in developing methods for recognition of handwritten mathematics suitable for imple-mentation on mobile devices. Such devices have already become widespread and provide an excellent opportunity for communicating mathematics. Developers of computer alge-bra systems are already actively targeting mobile applica-tions [ 2 ]. Unfortunately, however, the existing software for recognizing handwritten mathematics does not cover a suf-ficiently wide range of subjects and/or is not sufficiently fast and accurate, and therefore, is hardly ever used in mobile user interfaces.

The problem of recognition of handwritten mathemat-ics is substantially different from that of handwritten text. The mathematical alphabet is larger than that of European languages by an order of magnitude. Mathematical expres-sions have a two-dimensional layout. Moreover, there is no fixed vocabulary of formulae, and frequencies of various subformulae strongly depend on the area of mathematics [ 25 ]. On the other hand, mathematical expressions are often hand-printed, rather than being handwritten, which means mathematical symbols are generally better segmented. These properties render the methods developed for handwritten natural language recognition ineffective in a mathemat-ical context. They also suggest that recognition of iso-lated handwritten mathematical symbols is a central and well-posed problem. Of course, there are other important issues, such as character segmentation and rotation/slant correction. These problems are essentially context-depen-dent, and mainly for this reason are beyond the scope of this paper. As mentioned previously, exact repeated con-texts are rare in mathematical expressions, which means that contextual information is mostly statistical. One possible approach to incorporate such information into the classifier, via confidence measures, has been proposed by Golubitsky and Watt [ 8 ]. The rotation/slant correction problem has been recently addressed by Golubitsky et al. [ 9 ]. The objective of this paper is to compare the accuracy and computational complexity of various distance measures and choose the most suitable one for the purpose of recognition of handwritten mathematical symbols. The distance measures considered in this paper are briefly described in the next paragraph.
Among the distance measures used for classifying hand-written mathematical symbols, the elastic matching distance [ 22 ] is known to be one of the most accurate. This distance, however, has several drawbacks.

First, the size and position of the symbol must be normal-ized before the distance is computed. This means that the computation cannot begin until the entire symbol curve is traced. Directional features, e.g., approximations of the first and second derivatives at each point, are translation-invari-ant, but size normalization is still required. It is quite difficult to construct local scale-and position-invariant features that would be resistant to noise, as they would involve higher-order derivatives or their ratios. Perhaps for this reason, to our knowledge, scale-and position-invariant local features have not been used in online character recognition. The Legendre-Sobolev coefficients of the coordinate functions, which we use as features in this paper, are not translation-or scale-invariant, but can be instantly normalized with respect to these transformations as discussed in Sect. 2 .
Second, the elastic matching algorithm is sensitive to the number of points sampled from the curve, and there-fore, is device-dependent, which means that, in practice, the points must be re-sampled. Also, the complexity of the elastic matching algorithm is quadratic in the number of points. The number of re-sampled points is an arbitrary parameter which must be tuned experimentally. There exist modifications of the elastic matching algorithm of subquadratic complexity (also considered in this paper), which involve additional arbi-traryparameters.However,evenwithallparameterscarefully chosen, the elastic matching distance remains several times slower than the Euclidean distance (described in the next paragraph).

Third, and more importantly, the elastic matching distance is not a metric, as it is not symmetric and does not satisfy the triangle inequality. Therefore, it is difficult to use this distance in conjunction with other methods that rely on the properties of the vector space. We will show that faster and more accurate distance measures can be obtained by utilizing such properties.

One useful metric distance is the integral of the Euclidean distance between curves. This can be computed directly, from the point sequences. However, there is a much more efficient and device-independent way, based on the representation of the curve by the coefficients of the truncated series of its coor-dinate functions, with respect to an orthogonal functional basis [ 1 ]. There are actually many different Euclidean dis-tance measures in the space of parametric curves, which are induced by various functional inner products. Among them, Chebyshev, Legendre, and Legendre-Sobolev inner products are widely used in signal processing [ 15 ]. It has been shown [ 4 ] that the Legendre-Sobolev distance provides the most accurate comparison of handwritten symbol curves, among the Euclidean distance measures.

The Legendre-Sobolev coefficients can be computed by an on-line algorithm [ 3 ], as the curve is written, and allow fast normalization of the size of the symbol after the pen is lifted. Moreover, as we shall see, only a few coefficients (10 X 12) and a few bits per coefficient (7 X 8) yield a suffi-ciently accurate representation of the curve. It turns out that, once the Legendre-Sobolev coefficient vectors are computed, it does not matter much which distance is used to com-pare them. The Manhattan distance is especially attractive, because it allows for an efficient implementation in about 50 machine instructions, or 20 nanoseconds on a modern PC.

The accuracy of nearest neighbor classification based on the Euclidean distance measures is lower than that of elastic matching. However, a refined method, based on the distance to the convex hull of several nearest neighbors [ 7 , 24 ], per-forms significantly better than elastic matching. This method effectively takes advantage of the local convexity [ 14 ]ofthe curve classes (see Sect. 4.4 ). The distance to the convex hull of a few points can be computed fast [ 7 , 18 ]. More impor-tantly, this needs to be done only for a few most relevant clas-ses, which can be reliably determined using nearest neighbor ranking with Manhattan distance.

The choice of the parameterization of the curve is also known to significantly affect the classification accuracy. It has been shown [ 4 ] that, for any of the above distance mea-sures, parameterization by the Euclidean arc length yields best results when compared to parameterization by time or affine arc length.

To summarize the main results, among the distance mea-sures we have considered, a combination of Manhattan and Euclidean distances gives the best performance in terms of classification accuracy, time, and space complexity. The best curve representation is as vectors of Legendre-Sobolev coef-ficients of the coordinates, as functions of arc length. The Euclidean distance to the convex hulls of nearest neighbor classes (representing curves as described) gives the best clas-sification accuracy. The Manhattan distance to select candi-date classes for consideration (using Euclidean distance to convex hulls, as described) gives the fastest classification.
This paper is organized as follows. In Sect. 2 , we introduce the representation of a parametric curve by the Legendre-Sobolev coefficient vector and show how to compute this vector on-line, as the curve is written. In Sect. 3 , we describe the dataset used in our experiments. In Sect. 4 , we present the results of comparison of various distance measures by cross-validation and choose the optimal distance measure, taking into account classification accuracy, time, and space complexity. Section 6 concludes the paper. 2 Representation of parametric curves Mostexistingmethodsofcurveclassificationarebasedonthe representation of a parametric curve by a sequence of points. This representation is readily available from the digital pen, which samples points from a curve with a certain frequency and outputs the sequence of their coordinates in real time. We disregard the additional information about the curve such as, for example, pen tip pressure or angle, in order to remain device-independent. Since the sampling frequency and res-olution also depend on the device, the sequence must be size-normalized and resampled, in order to obtain a device-independent representation. Because the size and length of the curve are unknown until the entire curve is traced, these operations must be postponed until pen-up. Thus, the time of computing a device-invariant representation of the curve by a sequence of points inevitably depends on the device (and on the curve). Therefore, we propose to use another representation, which is also device-independent and can be computed mostly on-line, as the curve is written, with a small constant-time overhead after pen-up.

Let x ( X ) and y ( X ), X   X  X  0 , L ] , be the coordinate functions of a parametric curve. We will represent the curve by the first d + 1 Legendre-Sobolev coefficients of the coordinate func-tions. Note that the idea is very similar to that of Fourier coefficients, the only difference being the choice of the func-tional inner product. The Legendre-Sobolev inner product is a natural choice, for the following reasons: 1. Chebyshev, Legendre, and Legendre-Sobolev series 2. Legendre and Legendre-Sobolev inner products are the 3. Classification accuracy improves when we replace
The Legendre-Sobolev inner product of two functions f ( X ) and g ( X ) on [ 0 , L ] is defined as f ( X ), g ( X ) = L where  X  is a real parameter (which we assume to be fixed). Consider the orthonormal polynomial basis B 0 ( X ), B 1 ( X ), B ( X ),... , with deg B mined by the inner product and can be obtained through Gram-Schmidt orthonormalization of the monomial basis 1 , X , X  2 ,... . Then any function satisfying suitable smooth-ness conditions can be represented by its infinite Legendre-Sobolev series f ( X ) = where the Legendre-Sobolev coefficients f i are given by the inner products f = f ( X ), B Then the truncated Legendre-Sobolev series of order d  X  f ( X ) = is a polynomial of degree at most d , which can be thought of as the projection of the function f ( X ) on the subspace of polynomials of degree at most d .

If the function is sufficiently smooth and has a small num-ber of extrema, then it can be well approximated by a polyno-mial of a low degree, and so the projection on a low-dimen-sional polynomial subspace will be a close approximation to the function. The coordinate functions of symbol curves, as functions of time, do look smooth (even where the curve has cusps or corners) and have few extrema (at most 10 X 15). It is not surprising therefore, that they can be accurately approx-imated by truncated series of order about 10, to the extent that the approximation error is unnoticeable to a human eye. When the curve is parameterized by arc length, the coor-dinate functions are no longer smooth, and the approxima-tion will tend to  X  X ut X  cusps and corners; however, truncation order 10 X 15 still appears to be sufficient to approximate most symbol curves so that they can be recognized unambiguously by a human reader [ 3 , 4 ]. Multi-stroke symbols can also be accurately approximated by the truncated Legendre-Sobolev series, after consecutive strokes are joined [ 5 ].
Let x 0 , x 1 ,..., x d be the first d + 1 Legendre-Sobolev coefficients of x ( X ) , and similarly for y ( X ) . Since B 1 (for any inner product), point ( x 0 , y 0 ) can be thought of as the curve X  X  center, and therefore, we can normalize the curve X  X  position by simply omitting these coefficients. To complete normalization, we consider size as any non-zero linear dimension of the symbol, such as width, height, length, diameter, or perimeter. In our context, this definition of size up to a constant factor is sufficient. The norm of the vector formed by the remaining coefficients, ( x is proportional to the size of the symbol and therefore, we can normalize size by normalizing this vector. The result-ing normalized vector represents the curve by a point in a 2 d -dimensional vector space and will be used for classifica-tion.

We next turn our attention to a fast method for computing the Legendre-Sobolev coefficients [ 3 , 4 ]. As the curve is writ-ten and a sequence of points is sampled from it, accumulate the moment integrals m ( x , L ) = L m ( y , L ) = L As pointed out by Golubitsky and Watt [ 3 ], general-purpose numerical integration schemes, such as the trapezoidal rule, failtoapproximatehigher-ordermomentswithsufficientpre-cision. Instead, on each subinterval, we compute the integral of  X  i exactly and multiply the result by average value of the coordinate function at the end points; see [ 3 , Section 3] for details.

Consider the Legendre-Sobolev inner product on [ 0 , 1 ] and apply integration by parts: f ( X ), B We observe that since B i ( X ) are polynomials, the Legendre-Sobolev coefficients of f ( X ) can be obtained as linear com-binations of the moments of f ( X ) and values of f ( X ) at  X  = 0 and  X  = 1. The matrix of the linear transforma-tion from moments to the Legendre-Sobolev coefficients can be computed in advance. For the particular case of  X  = 0, which corresponds to the Legendre inner product, the explicit expressions for its entries are given by Golubitsky and Watt [ 3 , Section 6].

If f ( X ) is defined on an interval [ 0 , L ] , rather than we can first apply a linear substitution  X   X   X / L to the moments, i.e., divide m i ( x , L ) and m i ( y , L ) by L both the substitution and the linear transformation require a number of operations that depend only on the truncation order. As we have seen earlier, the truncation order has a small value, intrinsic to the character set (in our experiments, we try different values from 6 to 12) so methods with qua-dratic complexity are perfectly acceptable. A thorough analy-sis of the approximation accuracy is carried out by Char and Watt, Golubitsky and Watt [ 1 , 3 ]. For an illustration of the convergence, please refer to Fig. 1 . Thus, the time spent to compute the representation after pen-up can be neglected, even on the most inexpensive hardware. The remaining chal-lenge is therefore, to classify the curve fast among several hundreds of classes.

Note that the truncated Legendre-Sobolev coefficient vec-tor contains nearly complete information about most char-acters, because a human can easily recognize the character by looking at its approximation. Arguably, since there is no substantial loss of information at the representation stage, classification algorithms based on this representation should perform well. Another important property of the representa-tion is that, mathematically, it is just a linear projection of the parametric curve onto a finite-dimensional subspace of truncated Legendre-Sobolev series. Under projections, the images of convex sets remain convex and, as is shown in Sect. 4.4 , convexity is a useful property in our problem. Ulti-mately, a numerical representation that preserves essential information about the characters and allows high classifica-tion accuracy should give a new insight into the processes of handwriting recognition and generation as performed by humans. 3 Experimental setting For our experiments, we have assembled a dataset of 50,703 handwritten mathematical symbols from 242 symbol clas-ses. A total of 26,139 samples were collected at the Ontario Research Centre for Computer Algebra, 14,802 samples (of digits) were borrowed from the UNIPEN handwriting data-base [ 10 ], and 9,762 samples from the LaViola database [ 16 ], which includes digits, Latin letters, and a few common mathematical symbols. The data were converted to a uniform InkML format [ 11 ] and stored in a single file. For each symbol, the number of strokes and the x and y coordinates of the sample points are available; for some symbols, we also have information about the timing, pen pressure, pen-up strokes, and context (a MathML description of the formula containing the symbol), which was disregarded in the present experiments. The classes (chosen according to the MathML labelling standard, when possible), with the number of sam-ples available for each class, are listed in Tables 1 and 2 .
All samples were visually inspected in order to correct accidental labelling errors. Manual relabeling was necessary in order to obtain more accurate error rates for the classi-fiers. Indeed, for samples that are hardly recognizable by a human as members of the assigned class, the expected out-put of classification is essentially undefined, therefore such samples do not provide any valuable information and only increase the variance in the resulting error rates. Thus, sam-ples that appeared totally unrecognizable were discarded. Samples that belong to more than one class, as determined by visual inspection, were attributed with all applicable labels (see Fig. 2 for examples). Some classes were so similar (i.e., had no examples of symbols that certainly belong to some but not all of them) that it was decided not to attempt to distinguish among them. Examples of such groups of simi-lar classes include  X  X apital O, little o, omicron, zero X , capi-tal Greek letters with Latin analogs, and certain calligraphic letters without clearly pronounced calligraphic features. All samples of such classes were labeled with the same, most commonly used, label. Whenever a class contained at least onesymbolthatisclearlyamemberofthis,andnoother,clas-ses, we retained the label. As a result, we obtained 38,493 samples belonging to a single class, 10,224 samples to 2 clas-ses, 1,954 samples to 3 classes, 19 samples to 4 classes, and 13 samples to 5 classes.
 We also included the number of strokes in the class labels. This decision was based on an experiment described by Char and Watt [ 5 ], where including the number of strokes in the class label yielded better classification results than using it as an entry in the feature vector. This is the case despite the fact that the number of classes raises to 378, as we include the number of strokes in the label. The reason why the num-ber of strokes is not a good candidate for a coordinate in the feature vector is that, unlike the other features we consider, it violates the property of convexity, which we discuss below in Sect. 4.4 .

We furthermore noticed that some symbols could be writ-ten in more than one way. It is reasonable to assume that a symbol can be classified correctly only if sufficiently many instances of this symbol form are available in the database. Therefore, we discarded all symbol forms which had fewer than 9 instances. However, we do not use allomorph labels in the classification. The classes of small symbols  X  dot  X  and  X  comma  X , as well as class  X  rpar  X  (closing parenthe-sis) that has a large overlap with the class of commas, were not considered. Arguably, small symbols should be classi-fied separately, using a method that takes into account their relative size. All methods discussed in this paper apply size normalization before classification. The quoted results apply to the dataset described.

The normalized Legendre-Sobolev coefficient vectors were pre-computed for all samples, with the value of the Legendre-Sobolev parameter  X  set to 1 / 8 (it has been deter-mined earlier [ 7 ] that this value yields good results for various classification methods).

In order to evaluate the performance of various classifica-tion methods, we used a 10-fold cross-validation. The entire dataset of 50,703 symbols was randomly partitioned into 10 subsets, preserving the proportions of class sizes. Then, sam-ples from each subset were classified, using the samples from the other 9 subsets as training data. A sample was considered to be classified correctly if the composite label attributed to it by the classification algorithm overlapped with the ground truth composite label. For example, if the sample is labeled  X  4  X  and the classifier outputs  X  4 or y  X , the sample is cor-rectly classified. We allow such ambiguous output from the classifier, because in many cases there is no meaningful way to assign a single label to a symbol. For example, a human cannot decide  X  4  X  X r X  y  X  for elements of  X  4 or y  X  and the ambiguity can only be resolved by taking into account con-textual information. Then, the percentage of mis-classified samples was calculated. Note that each mis-classified sym-bol contributes about 0.002% to the overall error rate. Our experiments were performed on a PC with a 2.4-GHz Intel Dual Core processor, 2GB of RAM, running Ubuntu Linux 8.04. The algorithms were implemented in C++. The distances to the convex hulls were computed with the help of the linear algebra library Lapack++ , release 2.5.3. 4 Classification In this section, we present the results of nearest neighbor classification using the following distance measures:  X  elastic matching distance,  X  Euclidean distance on Legendre-Sobolev coefficient  X  Manhattan distance on Legendre-Sobolev coefficient  X  Euclidean distance to the convex hull of several nearest We compare the cost and accuracy these measures and pres-ent a combination that is efficient in both time and space. Our goal is to identify a classification method that is as fast, compact and accurate as possible in principle. We recog-nize that there are trade-offs between these objectives, so for concreteness we take our reference mobile processor as one with a clock of 100MHz for 32 bit operations and at most 1 megabyte of memory available for use by recognition. 4.1 Elastic matching The square elastic matching distance [ 19  X  22 ] from a point sequence A = a 1 ,..., a n to point sequence B = b 1 ,..., (of equal length) is defined as the minimum over all non-decreasing functions r :{ 1 ,..., n } X  X  1 ,..., n } of the square Euclidean distance It can be computed in O ( n 2 ) operations using a dynamic programming algorithm [ 22 ]. A sliding window version of elastic matching [ 13 , 23 ] is often used, in which a param-eter q , 1  X  q  X  n , is introduced, and the values r ( i ) restricted to the interval [ i  X  q , i + q ] , for each i With this restriction, the distance can be computed in O ( operations.

It has been established earlier [ 4 ] that the elastic match-ing distance yields best results when the points a 1 ,..., are sampled from the curve so that the arc lengths between the adjacent points are equal. It has been also established [ 4 , Table 6] that 1-nearest neighbor (NN) classification yields better results than 3-NN or 5-NN when many small clas-ses (with about 10 samples per class) are present, which is our case. This applies to elastic matching and euclidean dis-tance-based classification, therefore, we restrict our analysis to 1-NN in what follows. By cross-validation, we determined that the lowest error rate of 1-nearest neighbor classification, equal to 3.0%, is obtained for n = 30 and q = 2. The error rates for other values of n and q are shown in Table 3 . Observe that the optimal value of q is proportional to n and approxi-mately equal to n / 15. There is no improvement observed for n &gt; 30.

The algorithm for computing the elastic matching distance withtheslidingwindow,inthespecialcasesofsmallvaluesof q , admits an efficient implementation with no dynamic mem-ory allocation and completely inlined code. For example, the computation of a single distance for n = 20 and q = 1 takes 0.25 microseconds on our experimental PC. Assuming that the maximal affordable delay is 100 milliseconds and our reference mobile processor is up to 50 times slower than our experimental PC, we obtain that about 8,000 models can be processed. We conclude that the elastic matching distance is not accurate enough and not fast enough for our purposes and proceed to the study of alternatives. 4.2 Euclidean distance In this and the following sections, we consider the distance measures that are perhaps among the fastest possible. As we shall see, these distances are less accurate than the elastic matching distance. However, they are perfectly suitable for selecting the top T candidate classes, as well as finding the nearest neighbors in each class. Then, the distance to the con-vex hull of these neighbors will be used to select from among the top T .

Using the usual formula for the square Euclidean distance, we obtain an algorithm that computes the distance in 3 D  X  arithmetic operations, or 0 . 12 microseconds (for dimension D = 24, with floating point arithmetic). This is twice faster than elastic matching, but still not fast enough, as it will allow to process about 16,000 samples on our reference mobile processor (see end of previous section for the calculations). Therefore, a faster distance is needed.

A significant improvement can be achieved by using inte-ger arithmetic. This is particularly useful on mobile devices, many of which do not support floating point instructions. It turns out that Legendre-Sobolev coefficients can be com-puted with a very low precision, without compromising the correct retrieval rates. As Table 5 suggests, 7 bits per coeffi-cient are enough. However, the computation of the squares in the Euclidean distance requires each coefficient to be stored in a separate machine word, which makes it difficult to take full advantage of the low precision. For this reason, we con-sider the Manhattan distance as an alternative. 4.3 Manhattan distance The Manhattan distance ,or L 1 distance, between two coef-ficient vectors a and b is the sum of the lengths of the pro-jections of a  X  b on the coordinate axes: || a  X  b || 1 = We show how to compute the Manhattan distance quickly by operating on several coordinates at once in a machine word. Here we give the details for 32-bit words and simply note that the same approach can be applied with words of other sizes.

In our setting, we can assume that the number of coeffi-cients D = 4 m is a multiple of 4 and that each coefficient is an integer number between  X  63 and 63, inclusive. Then D coefficients can be stored in m 32-bit words as follows. For k = 0 , 1 , 2 , 3, use bits 8 k ,..., 8 k + 6 to store a coeffi-cientusingones X  X omplementbinaryrepresentation,inwhich negative numbers are represented by the bitwise complement of their absolute values. Bits 8 k + 7areassumedtobezeroes.
With this representation, the Manhattan distance can be efficiently computed as follows. Consider word 0, in which coefficients for i = 0 , 1 , 2 , 3 are stored. Assume that all four of a i , i = 0 , 1 , 2 , 3 are stored in a 32-bit word, as described previously, as are  X  b i , and let a and b denote these two words, respectively. Note that the negations  X  b i can be pre-computed in advance. Then the word containing | a i  X  b i for i = 0 , 1 , 2 , 3, can be computed as shown in Fig. 3 .Note that the resulting absolute values again occupy at most 7 bits in each byte. To accumulate their sum, we can first compute w = (w + (w # 8 )) &amp; 0x00FF00FF for each word, then add the m words and, finally, apply w = (w + ((w # 16 )) &amp; 0xFFFF to the result. The total number of instructions is 6 m + 3 m ( m  X  1 ) + 3 = 10 m + 2.

In comparison, the Euclidean distance requires 3 D  X  1 = 12 m  X  1 operations. The timings for the Euclidean and Man-hattan distances on 32-bit and 64-bit machines are shown in Table 4 . Note that the real speed-up is much greater than 12 / 10: the Manhattan distance turns out to be about 3 times faster on a 32-bit machine and about 4 X 5 times faster on a 64-bit machine. This speed-up is largely due to the effects of packing, which ensures that the computation of the Manhattan distance proceeds in machine registers, while the Euclidean distance requires retrieval of data from memory. The timings suggest that, on our reference mobile processor, we can compute the Manhattan distance from a test sample to more than 80,000 training samples, for dimension 24, in under 100 milliseconds. This rough estimate suggests that, with the proposed methods, recognition speed is not an issue even on mobile devices, and the main concern becomes the memory required to store the samples. We propose various methods for compact representation in Sect. 4.4 .

As Table 5 shows, the error rates of nearest neighbor clas-sification with Manhattan distance are very similar to those with the Euclidean distance. These error rates are higher by about 0.5% than the best rate for the elastic matching distance, but the accuracy is sufficient to determine the top 3 classes. In order to break the ties among these most rel-evant classes, we will employ a more accurate distance, as described in the next paragraph. 4.4 Convex hull of nearest neighbors It has been observed earlier [ 5  X  7 ] that classes of handwrit-ten symbol curves satisfy the following property of convex-ity . Consider two curves that belong to the same class. If we gradually morph one curve into the other, using a linear homotopy, then the intermediate curves are expected to belong to the same class as well, as shown in Fig. 4 .
This means that, whenever two points belong to a curve class in the infinite-dimensional space of parametric curves, the entire straight line segment connecting these points is contained in this class. In other words, curve classes are con-vex sets. This property may fail to hold if the two curves are different allomorphs of the same symbol; such counter-examples can be found in [ 7 ]. However, it is safe to assume that, if the two curves from the same class are sufficiently close to each other (e.g., in terms of the Euclidean distance), then linear homotopy between them will always produce sim-ilar curves.

The fact that the property of convexity holds only locally for the curve classes has implications for the choice of the classification method. Indeed, if the classes were globally convex, then they would be linearly separable (provided that the classes do not overlap). For such classes, classifiers based on linear support vector machines are a natural choice [ 5 , 6 ]. On the other hand, for classes that do not exhibit any con-vexity, linear support vector machines get outperformed by distance-basednearestneighborclassification.Thedisadvan-tage of the latter family of methods is that they essentially bypass the learning stage and require very large training sets. The distance to the convex hull of nearest neighbors [ 7 , 24 ] can be viewed as a combination of the two approaches, which is particularly suitable for classifying among locally convex classes.

When the number of points is less than the dimension of the vector space their convex hull is a simplex (provided that the points are in generic position). The distance from a point to a simplex can be computed by an efficient algorithm [ 7 , 18 ] presented in Fig. 5 .

It has been shown experimentally [ 7 ] that, for the purpose of curve classification, the optimal number of nearest neigh-bors in the convex hull is indeed smaller than the dimension. Given that the number of neighbors is small and the classes are locally convex, it is unlikely that convex hulls of near-est neighbors from different classes will overlap (unless the classes themselves overlap, which is again unlikely in the presence of multiple labels). An exact measurement of the volume of these overlaps, however, would require rather sophisticated computations, which are beyond the scope of this paper.

It has been also shown [ 7 ] that the distance to the con-vex hull of nearest neighbors successfully breaks the ties among the top few classes produced by a support vector machine classifier. We present the results of experiments, which demonstrate that the pre-classifier based on support vector machines can be replaced by the nearest neighbor classification with Manhattan distance. The latter method is advantageous in three respects: it does not require con-vexity of classes, and hence allomorph labels (which are very tedious to obtain), is faster, and requires less stor-age.

To summarize our combined classification method, we list all the steps and parameters involved in Fig. 6 . In order to reduce the time complexity and space requirements, one can use a certain subset of training samples in steps 2 and 3. Intu-itively, the samples on the boundary of classes play a more important role than those in the interior. The significance S of a training sample X from a class C can be estimated by computing the number of samples from the other classes, for which X is the nearest neighbor, among the samples from C . It turns out that discarding the samples of low signifi-cance does not diminish classification accuracy and allows to reduce the size of the training set almost by a factor of 2.

Figure 8 shows the error rates of the combined classifica-tion method for various values of the parameters D , b , and S , together with the corresponding storage requirements. It has been established earlier [ 7 ] that the value of k = 11 yields best results for the distance to the convex hull of nearest neighbors. The error rates also reach their minimum for T = 10 (3.9% for T = 1, 2.6% for T = 5, 2.5% for T  X  10). The reason why we need T higher than 3, even though the top-3 error rate for the Manhattan distance is close to zero, is because, in the presence of overlaps, it is sometimes beneficial to leave among the top T classes more than one intersection of the correct class with a neighboring class.

Note that the above rates were computed in the presence of multi-class labels. That is, a test symbol was considered to be correctly classified, if its multi-class label has at least one class in common with the multi-class label returned by the classifier. In the setting where we are going to apply this classifier, such ambiguities will be resolved by tak-ing into account statistical contextual information. However, it could possibly be used in other settings, where a sin-gle class is needed on the output. In that case, when mul-tiple classes are returned, we can choose the correct one at random, with the probabilities weighted by the corre-sponding numbers of samples per class. The resulting error rates, for test symbols labeled with their original single-class labels as presented to the writers, are shown in Table 6 (the parameters are the same as the ones that yielded 2.5% error rate for multi-class labels). The high top-1 error rate of about 20% is not very surprising, given that the classifi-cation is based on inherently ambiguous multi-class labels and a random choice to disambiguate them, and that about 20% of all samples are labeled with at least two clas-ses.

We examined the mis-classified symbols in order to characterize them. We found that in about 70% of the  X  X is-classified X  symbols, the symbol was unrecognizable out of context, even by a person. These could be viewed as label-lingerrors X  X hatthesymbolshouldhavehadmultiplelabels. Examples of these are shown in Fig. 7 (a) X (o).

About 30% of of the misclassified symbols (30% of 2.5% = 0.75% overall) were genuine mis-classifications. Some of these were attributable to deficiencies in the training set, oth-ers would be correctly classified if baseline information was provided, some appear to arise when significant parts of the symbol are retraced, and others appear to be well formed symbols that were mis-identified. Typical examples of these are shown in Fig. 7 (p) X (y).

The time needed to compute the distances to the convex hull of 11 points 10 times can be neglected (for any device). Therefore,onamodernPCoramorepowerfulmobiledevice, the entire available dataset can be easily used, with the max-imal dimension (24) and 7 bits per coefficient (there is no improvement for b &gt; 7 when the dimension equals 24). For an inexpensive mobile device, on which storage is pre-cious, we can use Fig. 8 to choose the optimal values of D and S , given the desired error rate and storage limitations. In order to be able to compute the Manhattan distance fast, one has to leave 1 empty bit for the carries, by reserving 1 byte but actually using 7 bits per coefficient. Taking into account this detail, a convenient choice of the parameters for a mobile device could be D = 20 , b = 7, and S = 4, which yields an error rate of 2 . 8% with less than 30,000 samples, requiring less than 600KB of storage. Experiments with fast compres-sion utilities suggest that the storage can be further reduced to about 450KB. With these settings, classification is expected to take less than 50 milliseconds on our reference mobile processor.

Even though it is natural to ignore classes with few samples to improve classification accuracy, this approach presents a practical concern: in a real system, it may often happen that a user enters a new symbol which is not in the dataset. In this case, the system may have to ask the user to provide additional samples, in order to be able to reliably identify members of the new class. This may be inconvenient to the user; however, since a particular writer is likely to use a particular allomorph of each symbol most of the time, it will only be necessary for the writer to provide about 10 samples of the new symbol, in order for the system to learn it well, so the burden does not appear to be too high. 5 Comparison with linear support vector machines As mentioned earlier, symbol classes are linearly separable, but only to some extent. In fact, linear separability is consis-tently violated by the presence of allomorphs, i.e., different ways of writing the same symbol. There is no reason why linear homotopy between two different allomorphs should stay within the same class and, indeed, as Fig. 9 illustrates, it often does not.

Linear support vector machines, naturally, will fail to distinguish between classes that are not linearly separable. One certain, albeit tedious, way to achieve linear separabil-ity is to label the allomorphs and use linear SVM to clas-sify among the allomorph classes, rather than the original ones. This inevitably increases the number of classes and decreases the number of available training samples per class, but the classes become simpler. There are two reasons that influenced our choice of the distance to the convex hull of nearest neighbors in favor of an ensemble linear SVM: the latter classification method is based on pairwise classi-fiers and therefore its complexity grows quadratically in the number of classes (this is also the reason why we found ensembles of non-linear SVM impractical); the former also shows slightly lower classification rates (see Table 7 for comparison). Nevertheless, we do consider linear SVM as a competitive approach, which could be combined with the distance-based methods, for example, via measures of con-fidence, which are available in both cases [ 8 ]. For a detailed description of experiments with linear SVM, please refer to reference [ 7 ]. 6 Conclusion The analysis of various distance measures carried out in this paper suggests that a careful assessment of the quality of a distance measure cannot be based solely on the measurement oferrorratesproducedbythecorrespondingnearestneighbor classifier. In the case of handwritten symbol recognition, the elastic matching distance has been shown to yield a notice-ably lower nearest neighbor classification error rate than the Euclidean distance in the space of curves. Yet we have shown that a more accurate classifier can be built using the vector-space-based representation, by working in a space with more structure and using its additional properties. By integrating a very fast pre-classifier based on the Manhattan distance with a more accurate tie-breaker based on the Euclidean distance to the convex hull of nearest neighbors, we have obtained a combined method that is clearly superior to elastic matching in terms of classification accuracy, time and space complex-ity. We have shown that this method allows classification of a test sample among several hundred classes with a high accu-racy and is suitable for implementation in systems with very strict speed and memory requirements.
 References
