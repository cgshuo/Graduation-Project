 In machine learning, traditional classification only has a single feature space and a label space [ 7 , 17 ]. An example would be assigning a given email into spam or non-spam classes [ 1 ] or assigning a part of speech to each word in a input sentence contain multiple diverse label sets for the training set and it is natural to solve these problems by the joint classification with heterogeneous labels. For example, biologists usually have to label the gene expression images with developmental stages [ 6 ] and anatomical terms annotation of gene expression patterns [ 20 ]. Each image is considered as a data instance, and each data instance must be assigned a stage term and one or more anatomical terms. Traditional classification only has one label set, like spam or non-spam classes. Joint classification, however, includes several different label sets, just like parse tree and syntactic structure. classification with heterogeneous labels. (a) Traditional Classification Random walk is a popular algorithm and has been used in many fields, like economics, biology and computer science. It explains the observed behaviors of many processes and is widely used in classification problems [ 8 , 10 ]. A random walk on a graph can make use of neighbors X  information to learn the correct labels in an iterative process. Therefore, Cai etal .[ 2 ] proposed an graph-based semi-supervised algorithm called preferential random walk (PRW) to solve the JCHL problem. PRW combines the information of both data features with het-erogeneous labels and the standard random walk by using data features, and the transition probability matrices of both algorithms are stable.
 Recently, another graph-based semi-supervised algorithm was proposed, called dynamic label propagation (DLP) [ 13 ], which incorporates the label corre-lations and instance similarities into a new way of performing label propagation. It was developed on the basis of label propagation (LP) [ 19 ] which assumes that nodes connected by edges of large similarity tend to have the same label through information which is propagated within the graph. And it is consistent with random walk. DLP updates the similarity measures dynamically by fus-ing multi-label/multi-class information. Experimental results have shown that this algorithm is more competitive than those algorithms without the dynamic updating process.
 In the light of these previous dicussions, we got inspiration and proposed a graph-based semi-supervised algorithm called RWDLP to solve JCHL problem. In RWDLP, we also build different relations to link heterogeneous labels by a markov chain random walk, and we employ the dynamic updating process to update the transition probability matrix in each iteration. We develop a new simple way to update the transition probability matrix of random walk, by which the data features and heterogeneous labels could be merged more effectively and produce a better performance. As demonstrated in our experiments, the random walk with dynamic label propagation can successfully deal with JCHL problems. ference between traditional classification and joint classification, and then we give a brief review of PRW. In Sect. 3 , we develop the proposed algorithm. In Sect. 4 , we show and discuss the experimental results. In Sect. 5 , we give some concluding remarks. 2.1 Traditional Classification and Joint Classification Traditional classification has one data feature set and one label set on its input-output mapping functions. In this paper, we study joint classification with het-erogeneous lables that has a feature set and multiple diverse label sets. There have been a variety of algorithms proposed to study heterogeneous labels. For example, Jin etal .[ 5 ] studied a learning with several views corresponding to dif-ferent set of class labels. These label sets have a close relation although they are different in the number of labels. However, there are not many studies in joint classification. The main bottleneck of JCHL problem is to deal with multiple tasks in the same time. The solution is to study a new algorithm that can han-del the heterogeneous labels, and another idea is to combine the heterogeneous labels into a unified formulation.
 about JCHL problem that combines the heterogeneous labels into an unified formulation. We consider data instances and heterogeneous labels as nodes in a graph, and regard the affinity of data-to-data, label-to-label and data-to-label as edges. We call the graph as Mix-Relevance Graph (MRG). And then the algorithm imagines a random walker which starts from a node (instance) with a known label, and steps to its neighbor nodes with a specific probability given in the transition probability matrix. 2.2 The Construction of MRG In this subsection, we briefly review the Mix-Relevance Graph in PRW. Through-out this article, we denote a vector as a bold lowercase character x and a matrix as a bold uppercase character X . If there X  X  no special note, all vectors are column vectors. Specifically, the i -th column vectors of a matrix X are denoted as x Let [ N : M ]( M&gt;N ) denote a set of integers in the range of N to M inclusively, row and j -th column of a matrix M . Given a dataset X , there are N data instances and each data instance has M dimensional features, denoted as X  X  R N  X  M . On the other hand, there are the number of labels in each space is different, denoted as Y R . For simplicity, we write Y =[ Y 1 , Y 2 , ..., Y K ] We will discuss how to contruct the MRG in the rest of this section.
 In dataset X , the number of instances is N . The similarity between x x in the object feature space can be measured by the affinity M i  X  [1 : N ]and j  X  [1 : N ]. The affinity M X ( i, j ) can be calculated based on the norm of the difference between their feature vectors x equal to j . In our algorithm, we employ the Gaussian kernel to compute this affinity.
 where x i  X  x j 2 is the Euclidean distance between the i -th object and the j -th object of dataset X . The parameter  X  is regarded as a positive number to control the linkage in the manifold. From M X , we can construct the transition probability matrix of data instances.
 where d X i = j M X ( i, j ), d Y i = j Y ( i, j )and  X  1 G =( V X ,E X ) can be induced, where V X = X and E X  X  V X clear that S X is symmetric and non-negative, therefore G positively weighted. Since G X is constructed with the affinity of data points, it is usually called as data graph, such as the left subgraph in Fig. 2 .Mostofthe existing graph-based semi-supervised learning methods [ 10 , 16 ] only make use of the data graph, while RWDLP use both data graph and label graphs.
 Now we have the data graph G X , and then we should build the label graphs. Take label set Y p as example, each data instance x i belongs to one of Q Y = y p 1 , ..., y p Q p represented by y p j  X  X  0 , 1 } Q p label sets, we should build label graph for each label set, and the structure of each label set maybe different. Generally, classification task will be divided into single label task and multiple label task by the number of label on each data point. For this reason, we proposed two strategies to calculate the correlation between labels. Firstly, we compute the affinity of a single label task Y follow: where y p i  X  y p j label of Y p . Secondly, for a multiple label task Y q , we compute the affinity by cosine similarity as follows: where y q i means absolute value of the i -th label of Y q we can construct the transition probability matrix of label sets.
 where Y k  X  Y , d Y k i = j Y k ( i, j )and  X  2  X  [0 , 1]. Now, we can also induce label graphs G k =( V k ,E k )from M Y k , where V k = Y k just like the right subgraph in Fig. 2 . The MRG is mainly composed of data graph and label graph. We have constructed data subgraph and label subgraphs, while these subgraphs are not connected yet. Obviously, the subgraph G ( V
X ,V Y p ,E XY p ) connects G X and G Y p , whose adjacency matrix is Y p  X  [1 : K ]. Moreover, the subgraph G G G where y p i means absolute value of the i -th label of Y p construct the transition probability matrix.
 posed in the past, but most of them only use information conveyed by G PRW fuses data instances and heterogeneous labels information encoded in G . Motivated by PRW [ 2 ]andDLP[ 13 ], we plan to further develop preferential random walk and dynamic label propagation to measure the relevance among labeled data points and unlabeled data points. Standard random walk on a graph G is usually described as a Markov process with transition probability matrix P = D  X  1 G , where D = diag ( d and d i = j G ( i, j ) are the degree of vertes i . It is clear that P j P ( i, j )=1.If G is symmetric, the graph is undirected. If G is asymmetric, the graph is directed and d i is the out degree of vertex i .Let Y ution of the random walker at time t . The steady state can be computed by It can be seen that the steady state of a standard random walk is just determined by the graph itself. In order to use label information, we propose the RWDLP as follow: where S is the transition probability matrix constructed in last section, Y [ y , y 2 , ..., y K ] contains the given labels of training objects and the unlabels of testing objects at first [ 15 ], and 0  X   X   X  1 specifies the importance of initial label information of a data instance, which can affect the ranking of the resulting label. In each iteration, we will update the transition probability matrix by a new label distribution matrix Y t +1 , as follow: where  X  is the importance of the new label information.
 The steady probability distribution Y can be solved by the iterative method. The overall algorithm is shown in Algorithm 1.
 After solving Y by using Algorithm 1, we predict the heterogenerous labels of data instance in multiple diverse label sets by different methods. For single-y ( i ) is the maximum probability of a label set: where y p i is the i -th row vector of matrix Y p ,and Y p means the output of p -th task. For multi-label task, we compute a threshold value  X  [ 14 ] to get the multiple labels y q ( i )of i -th data instance. where y q i is the row vector of matrix Y q .
 Though we joint classifications with heterogenerous labels, we also compare RWDLP X  X  performance with the state-of-the-art classification algorithms and show the performance in this section. 4.1 Data Set To develop and test our method, we use the Berkeley Drosophila Genome Project (BDGP) gene expression pattern dataset. Recently, a lot of research works have been experienced in the raw data from BDGP. This dataset is widely used to develop and test anatomical annotation methods for Drosophila gene expression pattern images.
 sider three views of images including lateral, dorsal and ventral images in our experiment, because the number of images from other views are not enough. All the images from BDGP have been pre-processed, including aligning and resizing to 128  X  320 gray images. The SIFT features are extracted from the gray images, and the codebook is made by K -means. The number of clusters is set to 1000, 500 and 250 for lateral, dorsal and ventral images, then we concatenate on the three vectors in one bag. To be specific, let x l  X  R 1000 , x d the bag-of-words vector for images in a bag from lateral, dorsal and ventral view. Therefore, an image bag can be represented as x =[ x l ; x ranges, i.e. stages 1 X 3, 4 X 6, 7 X 8, 9 X 10, 11 X 12, 13 X 16 and 17, in the BDGP data-base [ 9 ]. And the total number of anatomical terms is 303, i.e. foregutAISN , maternal and so on. We ignore the stage 1 X 3 and 17 data since the number of anatomical terms is too small. For the same reason, we select 79 anatomical terms in our experiment. At last, there are two classification tasks in a dataset, stage classification and anatomical terms classification. Obviously, stage classifi-cation is a single-label/multi-class problem, which has 5 lables, and anatomical terms classification is a multi-label/multi-class problem, which has 79 lables. We use 5-folds cross-validation and report the average performance of the 5 tails. We use 1-fold for traing and use the remaining 4-folds for testing to imitate actual scenarios in which the known label samples are far less than the unknown label samples. In our experiment, we initialize the testing label with K NN method, because it is simple and intuitive. To be specific, we set k =10 of KNN and it doesn X  X  matter what it is assigned in our method. There are 5 parameters in our method and it isn X  X  sensitive in certain ranges with good performances.  X  1 , X  2 , X  3 , which control the jumping among three subgraphs, can-not affect the result much if they are assigned in the range of (0.05,0.5).  X  is the importance of initial label information of a data instance and it should be assigned in the range of (0.5,0.9).  X  is the importance of the new label infor-mation and it should be assigned in the range of (0.5,0.9). Since we handle two classifications in the same time, we use accuracy for stage classification to measure the performance between our proposed method and other state-of-the-art methods, and use macro precision, macro recall, macro F1, micro precision, micro recall and micro F1 for anatomical terms classification. 4.2 Compared Methods Because stage classification is a single-label/multi-class problem, we choose two state-of-the-art methods, SVM and K NN, to compare with our proposed algo-rithm. The support vector machine (SVM) algorithm constructed by Chang and Lin [ 3 ] is one of the most popular methods of single-label problem. We use radial basis function (RBF) kernel and the optimal parameter values for C =1and  X  =0 . 9. This k -nearest neighbor ( K NN) method is an unsupervised learning, while SVM is supervised and our method is semi-supervised. We set k = 50, how-ever, it doesn X  X  affect the result much. We also predict the anatomical terms for the Drosophila gene expression patterns, which is a multi-label/multi-class prob-lem. It has a different label set compared with stage classification and both of the sets are heterogenerous. Traditionally, the precision, recall and F1 score are the measure of classification performance. But for the multi-label/multi-class classi-fication now, macro and micro average of precision, recall and F1 score are used and suggested by Tomancak, P. etal .[ 11 ]. In our experiment, we compare the result of our proposed algorithm with three state-of-the-art multi-label/multi-class methods: local shared subspace(LS) [ 4 ], harmonic function (HF) [ 20 ]and ML-K NN [ 18 ] which is used to do the initialization. LS and HF are proposed to solve the multi-label annotation problem. For these methods, we used the published codes posted on the corresponding author X  X  websites. 4.3 Performance Comparison We use the average classification accuracy of 5-folds cross-validation to assess Our method exhibits the better result on the average prediction accuracy, and is better than other state-of-the-art methods on 4 stages. The average classification accuracy of our method is 85 . 01 %, while the value of SVM is 80 . 69 %, and K NN tion results. We can see that our algorithm is better than the other methods on all metrics. From Tables 1 and 2 , we can see that our method will have better per-formance with less labeled data instances, while other algorithms must be given more labeled data instances. On the one hand, our proposed algorithm can joint stage classification and anatomical term classification simultaneously, and the former is single-labe/multi-class classification and the latter is multi-label/multi-class classification. On the other hand, we dynamically update the transition probability matrix in iterative process. As shown in the result, when one work is short of information to do classification, we can use the label information of other works to make the decision, and make good use of the limit label information from the data instances. In this paper, we have proposed a random walk with dynamic label propaga-tion method (RWDLP), which dynamically updates label information by itera-tion. The experimental results have demonstrated that the proposed algorithm is effective. In the future, we would like to handle more similar tasks. Besides, our method can deal with multi-label classification or multi-instance multi-label classification and just do some improvements.

