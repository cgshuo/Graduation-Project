 Active learning, as an effective learning paradigm to reduce the labeling cost in supervised settings, has been intensivel y studied in recent years. In most tradi-tional active learning studies, the learner usually regards the specific examples directly as queries, and requests the co rresponding labels from the oracle. For instance, given a diabetes patient datas et, the learner usually presents the en-tire patient example, such as [ ID = 7354288 ,name = John, age =65 ,gender = male, weight = 230 , blood  X  type = AB, blood  X  pressure = 160 / 90 , temperature =98 ,  X  X  X  ] (with all the features), to the oracle, and requests the corresponding label whether this patient has diabetes or not. However, in this case, many fea-tures (such as ID, name, blood-type, and so on) might be irrelevant to diabetes diagnosis. Not only could queries like this confuse the oracle, but each answer responded from the oracle is also applicable for only one specific example.
In many real-world active learning applications, the oracles are often human experts, thus they are usually capable of answering more general queries. For instance, given the same diabetes patient dataset, the learner could ask a gen-eralized query, such as  X  X re men over age 60, weighted between 220 and 240 pounds, likely to have diabetes? X , where only three relevant features (gender, age and weight) are provided. Such gener alized query can often represent a set of specific examples, thus the answer for the query is also applicable to all these examples. For instance, the answer to the above generalized query is applicable for all men over age 60 and weighted between 220 and 240 pounds. This allows the active learner to improve learn ing more effectively and efficiently.
However, although the oracles are indeed capable of answering such general-ized queries in many applications, the co st (effort) is often hi gher. For instance, it is relatively easy (i.e., with low cost) to diagnose whether one specific patient has diabetes or not, with all necessary information provided. However, it is of-ten more difficult (i.e., with higher cost ) to provide accurate diabetes diagnoses (accurate probability) for all men over age 60 and weighted between 220 and 240 pounds. In real-world situation, more domain expertise is usually required for the oracles to answer such generalized queries well, thus the cost for asking generalized queries is often more expens ive. Consequently, it yields a trade-off in active learning: on one hand, asking generalized queries can speed up the leaning, but usually with high cost; on the other hand, asking specific queries is much cheaper (with low cost), but the lea rning process might be slowed down.
In this paper, we apply a cost-sensitive framework to study generalized queries in active learning. More specifically, we assume that the querying cost is known to be non-uniform, and ask generalized queries in the following two scenarios:  X  Scenario 1 (Balancing Acc./Cost Trade-off): We consider only query- X  Scenario 2 (Minimizing Total Cost): In addition to querying cost, we In particular, we propose a novel method to, first construct generalized queries according to two objective functions in the above two scenarios, and then up-date the training data and the learning model accordingly. Empirical study in a variety of settings shows that, the proposed methods can indeed outperform the existing active learning algorithms in simultaneously maximizing the predictive performance and minimizing the querying cost. All of the active learning studies make assumptions. Specifically, most of the previous works assume that the oracles ca n only answer specific queries, and the costs for asking these queries are uniform. Thus, most active learning algorithms (such as [7,11,12,2,3,9]) are designed to achieve as high as possible predictive accuracy by asking a cer tain amount of queries. [4] relaxes the assumption of asking speci fic queries, and proposes active lean-ing with generalized queries. However, it assumes that the oracles can answer these generalized queries as easily as th e specific ones. That is, the costs for asking all the queries are still the same, regardless of the queries being specific or generalized. [8,6,10] relax the assumption of uniform cost, and study active learning in cost-sensitive framework. However, they limit their research in specific queries, and only consider that the costs for asking those specific ones are different.
In this paper, we study generalized queries with cost in active learning. Specif-ically, we assume that the oracles can a nswer both specific and generalized queries, but with different cost. This assumption is more flexible, more gen-eral, and more applicable to the real-world applications. Under this assumption, considering uniform cost fo r generalized queries (su ch as [4]) and considering non-uniform costs for specific queries (such as [8,6,10]) can both be regarded as special cases. Table 1 illustrates the different assumptions in active learning studies. As far as we know, this is the first time to propose this more general assumption and design corresponding learning algorithms for active learning. In this section, we design active learning algorithm to ask generalized queries. Roughly speaking, the active learning process can be broken into the following two steps in each learning iteration:  X  Step 1: Based on the current training and unlabeled datasets, the learner  X  Step 2: After obtaining the answer of the generalized query, the learner We will discuss each step in detail in the following subsections. 3.1 Constructing Generalized Queries In each learning iteration, constructing the generalized queries can be regarded as searching the optimal query in the query space, according to the given objective function. We propose two objective functions for the previous two scenarios, and design an efficient searching strategy to reduce the computation complexity. Balancing Acc./Cost Trade-off. In scenario 1, we only consider querying cost, and still use accuracy to measure the predictive performance of the learning model, thus the learning algorithm is required to balance the trade-off between the predictive accuracy and the querying cost. We therefore design an objective function to choose query that yields m aximum ratio of accuracy improvement to querying cost in each iteration.

More formally, Equation 1 shows the objective function for searching query in iteration t ,where q t denotes the optimal query, Q t denotes the entire query space, C
Q ( q ) denotes the querying cost for the current candidate query q ,  X Acc t ( q ) denotes the accuracy improvement produced by q , which can also be presented by subtracting the a ccuracy in iteration t  X  1 (denoted by Acc t  X  1 )fromthe accuracy in iteration t (denoted by Acc t ( q )). 2
We can see from Equation 1 that, estimating  X Acc t ( q ) /C Q ( q )isrequiredto evaluate the candidate query q . As we assume that the querying cost C Q ( q )is known, we only need separately estimate the accuracies before and after asking q (i.e., Acc t  X  1 and Acc t ( q )).

Estimating Acc t  X  1 is rather easy. We simply apply cross-validation or leave-one-out to the current training data, and obtain the desired average accuracy. However, estimating Acc t ( q ) is a bit difficult. Note that, if we know the answer of q , the training data could be updated by using exactly the same strategy we will describe in Section 3.2 ( Updataing Learning Model ), and Acc t ( q )thus could be easily estimated on the updated training data. However, the answer of q is still unknown in the current stage, thus here, we apply a simple strategy to optimistically estimate this answer, and then evaluate q accordingly.
Specifically, we first assume that the label of q is certainly 1. 3 Thus, we update the training data (using the same method as in Section 3.2), and estimate Acc t ( q ) accordingly. Then, we assume that the label of q is certainly 0, and again update the training data and estimate Acc t ( q ) in the same way. We compare these two estimates of Acc t ( q ), and optimistically choose the better (higher) one as the final estimate.
 Minimizing Total Cost. In Scenario 2, we consider both the querying and misclassification costs, and require the learning algorithm to achieve minimum total cost in the learning process.

However, calculating this total cost of querying and misclassification is a bit tricky. In real-world applications, the learning model constructed on the current training data is often used for the future prediction, thus the  X  X rue X  misclassifi-cation cost should also be ca lculated according to the future predicted examples. We assume that the rough size of such  X  X o-be-predicted X  data is known in this pa-per, due to the following reason. In reality, the quantity of such  X  X o-be-predicted X  data directly affects the quantity of resource (effort, cost) that should be spent in constructing the learning model. For instance, if the model would be used for only few times and on only limited unimportant data, it might not be worth to spend much resource on model construction; on the other hand, if the model is expected to be extensively used on a la rge amount of important data, it would be even more beneficial to improve the model performance by spending more resource. In many such real-world situations, in order to determine how much resource should be spent in constructing the model, it is indeed known (or could be estimated) that how extensively the model would be used in the future (i.e., the rough quantity of the to-be-predicted data).

It is exactly the same case in our current scenario of generalized queries. More specifically, if the current learning model will only  X  X lay a small role X  (i.e., make predictions on only few examples) in the future, it may not worth paying high querying cost to construct a high-performance model. On the other hand, if a large number of examples need to be predicted, it would be indeed worthwhile to acquire more generalized queries (at the expense of high querying cost), such that an accurate model with low misclassification cost could be constructed.
This indicates that, the number of  X  X o-be-predicted X  examples is crucial in minimizing total cost. Therefore, we formali zed the total cost after t iterations (denoted by C t T )inEquation2,where C i Q denotes the querying cost in the i th iteration, C t M denotes the misclassification cost after t iterations, which further can be calculated as the product of the average misclassification cost 4 after t iterations (denoted by AvgC t M ) and the number of future predicted examples (denoted by n ).

To obtain the minimum total cost for the learning model, we greedily choose the query that maximumly reduces the t otal cost in each learning iteration. More formally, Equation 3 shows the objective function for searching query in iteration t , where all notations keep same as above. In the current setting, we assume that C t Q and n are both known, thus we need estimate AvgC t  X  1 M and AvgC t M ( q ) separately, according to Equation 3. We again adopt the similar strategy as in the previous subsection. Specifically, AvgC t  X  1 M could be directly estimated by cross-validation or leave-one-out on the original training set, and AvgC t M ( q ) can be optimistically estimated by assuming the label of q is certainly 0 and 1 respectivel y (see Section 3.1 for details). Searching Strategy. Given the above two objective functions for two scenarios, the learner is required to search the query space and find the optimal one in each iteration.

In most traditional active learning st udies, each unlabeled example is directly regarded as a candidate query. Thus, in each iteration, the query space simply contains all the current unlabeled examples, and exhaustive search is usually ap-plied directly. However, when asking gener alized queries, each unlabeled example can generate a set of candidate generali zed queries, due to the existence of the don X  X -care features. For instan ce, given a specific example with d features, there exist with two don X  X -care features, and so on. Thus, altogether 2 d corresponding gener-alized queries could be cons tructed from each specific example. Therefore, given an unlabeled dataset with l examples, the entire query space would be 2 d l .This query space is thus quite large (grows exponentially to the feature dimension), and it is unrealistic to exhaustively evaluate every candidate. Instead, we apply greedy search to find the optimal query in each iteration.

Specifically, for each unlabeled example (with d features), we first construct all the generalized queries with only one don X  X -care feature (i.e., and choose the best as the current candidate. Then, based only on this candidate, we continue to construct all the generalized queries wit h two don X  X -care features (i.e., greedily increase the number of don X  X -car e features in the query, until no better query can be generated. The last genera lized query thus is regarded as the best for the current unlabeled example. We conduct the same procedure on all the unlabeled examples, thus we can find th e optimal generalized query based on the whole unlabeled set.

With such greedy search strategy, the c omputation complexity of searching is thus O ( d 2 ) with respect to the feature dimension d . This indicates an exponential improvement over the complexity of the original exhaustive search  X  (2 d ). Note that, it is true that such local greedy search cannot guarantee finding the true optimal generalized query in the entire query space, but the empirical study (see Section 4) will show it still works effectively in most cases. 3.2 Updating Learning Model After finding the optimal query in each iteration, the learner will request the corresponding label from the oracle, and update the learning model accordingly. However, the generalized queries often contain don X  X -care features ,andthelabels for such generalized queries are also likely to be uncertain . In this subsection, we study how to update the learning model by appropriately handling such don X  X -care features and uncer tain answers in the queries.

Roughly speaking, we consider the don X  X -care features as missing values, and handle the uncertain labels by taking partial examples in the learning process. More specifically, we simply treat the gener alized queries with don X  X -care features as specific ones with missing values. As many learning algorithms (such as decision tree based algorithms, most generative models, and so on) have their own mech-anisms to naturally handle missing values, this simple strategy can be widely ap-plied. In terms of the uncertain labels of the queries, we handle them by taking par-tial examples in the learning process. For i nstance, given a quer y with an uncertain label (such as, 90% probability as 1 and 10% probability as 0), the learning algo-rithm simply takes 0 . 9 part of the example as certainly 1 and 0 . 1 part as certainly 0. Taking partial examples into learning is often implemented by re-weighting ex-amples, which is also applicable to many popular learning algorithms.
This simple strategy can elegantly update the learning model. However, a pit-fall of the strategy also occurs. When updating the learning model, the current strategy always regards one generalized query as only one specific example (with missing values). This might significantl y degrade the power of the generalized queries. On the other hand, if one gener alized query is regarded as too many specific examples, it might also overwhelm the original training data. There-fore, here we regard each generalized query as n (same) examples (with missing values), where n is suggested to be half of the initial training set size by the empirical study.

So far, we have proposed a novel method to construct the generalized query and update the learning model in each active learning iteration. In particular, we have designed two objective functions t o balance the accuracy/cost trade-off and minimize the total cost of misclassification and querying. In the following section, we will conduct experiments on real-world datasets, to empirically study the performance of the proposed algorithms. In this section, we empirically study the p erformance of the proposed algorithms on 15 real-world datasets from the UCI Machine Learning Repository [1], and compare them with the existing active learning algorithms. 4.1 Experimental Configurations We compare the proposed algorithms with the traditional pool-based active learning (with uncertain sampling) [7] (denoted by  X  X ool X ) and the active learn-ing with generalized queries [4] (denot ed by  X  X GQ X ).  X  X ool X  and  X  X GQ X  repre-sent two special cases for querying cost :  X  X ool X  only asks specific queries (with low querying cost), but cannot take advantage of the generalized queries to im-prove the predictive performance; on the other hand,  X  X GQ X  tends to ask as general as possible queries to promptly improve the predictive performance, but with the expense of high querying cost. W e expect that the proposed algorithms (for the two scenarios) can simultaneously maximize the predictive performance and minimize the querying cost, thus outperforming  X  X ool X  and  X  X GQ X .
All of the 15 UCI datasets have binary class and no missing values. Infor-mation on these datasets is tabulated in Table 2. Each whole dataset is first split randomly into three disjoint subsets: the training set, the unlabeled set, and the test set. The test set is always 25% of the whole dataset. To make sure that active learning can possibly show improvement when the unlabeled data are labeled and included into the training set, we choose a small training set for each dataset such that the  X  X aximum reduction X  of the error rate 5 is large enough (greater than 10%). The training sizes of the 15 UCI datasets range from 1/200 to 1/5 of the whole datasets, also listed in Table 2. The unlabeled set is the whole dataset taking away the test set and the training set.

In our experiments, we set the querying cost ( C Q ) for any specific query as 1, and study the following three cost settings for generalized queries with r don X  X -care features, as follows:  X  C Q =1+0 . 5  X  r : This setting represents a linear growth of C Q with respect  X  C Q =1+0 . 05  X  r : This setting also represents a linear growth of C Q with  X  C Q =1+0 . 5  X  r 2 : This setting represents a non-linear growth of C Q with Note that, these settings of querying cost are only used here for empirically study, any other types of querying cost could be easily applied without changing the algorithms.

As for all the 15 UCI datasets, we have neither true target functions nor human oracles to answer the generalized queries, we simulate the target functions by constructing learning models on the entire datasets in the experiments. The simulated target function regards each generalized query as a specific example with missing values, and provides the posterior class probability as the answer to the learner. The experiment is repeat ed 10 times on each dataset (i.e., each dataset is randomly split 10 times), and the experimental res ults are recorded. 4.2 Results for Balancing Acc./Cost Trade-Off In Scenario 1, we use accuracy to measure the performance of the learning model. Thus, we use an ensemble of bagged decision trees (implemented in Weka [5]) as the learning algorithm in the experiment. Any other learning algorithms can also be implemented in real-world applications.

Figure 1 demonstrates the performance of the proposed algorithm considering only querying cost (denoted by  X  X GQ-Q C X ; see Section 3.1 ), compared with  X  X ool X  and  X  X GQ X  on a typical UCI dataset  X  X reast-cancer X . We can see from the subfigures of Figure 1 that, with all the three querying cost settings,  X  X GQ-QC X  can always effectively increase the pr edictive accuracy of the learning model with low querying cost, and outperform  X  X ool X  and  X  X GQ X . More specifically, in the case that ( C Q =1+0 . 5  X  r ),  X  X GQ-QC X  significantly outperforms both  X  X ool X  and  X  X GQ X  during the entire learning process. In the case that ( C Q = 1+0 . 05  X  r ), although  X  X GQ-QC X  still outperforms the other two algorithm, it performs similarly to  X  X GQ X . As the cost of asking generalized queries is rather low in this case,  X  X GQ-QC X  tends to discover as more as possible don X  X -care features in the queries, thus produc ing similar predictive performance as  X  X GQ X . In the case that ( C Q =1+0 . 5  X  r 2 ),  X  X GQ-QC X  still significantly outperforms the other algorithms. Note that, In this case, the cost of asking generalized queries is relatively high (i.e., grows quadratically with the number of don X  X -care feature), thus  X  X GQ X  tends to discover as few as possible don X  X -care features, and consequently behaves similarly to  X  X ool X .
To quantitatively compare the learning curves, we measure the actual values of the accuracies in 10 equal-distance po ints on the x-axis. The 10 accuracies of one curve are compared with the 10 accuracies of another using the two-tailed, paired t-test with 95% confidence level. The t-test results on all the 15 UCI datasets with all the three querying cost settings are summarized in Table 3. Each entry in the table, w/t/l , means that the algorithm in the corresponding column wins on w ,tieson t ,andloseson l datasets, compared with the algorithm in the corresponding row. We can observe the similar phenomena from Table 3 that,  X  X GQ-QC X  significantly outperforms  X  X GQ X  when the querying cost is relatively high ( C Q =1+0 . 5  X  r 2 and C Q =1+0 . 5  X  r ), and significantly outperforms  X  X ool X  when the querying cost is relatively low ( C Q =1+0 . 05  X  r ). 4.3 Results for Minimizing Total Cost In Scenario 2, we use total cost to measure the performance of the learning model. Thus, we use a cost-sensitive algorithm CostSensitiveClassifier based on an ensemble of bagged decision trees (implemented in Weka [5]) as the learning algorithm in the experiments. In addition, we set the false negative (FN) and false positive (FP) costs as 2 and 10 respectively, and we set the number of the future predicted examples as 1000. Still, an y other settings can be easily applied without changing the algorithm.

Figure 2 demonstrates the performance of the proposed algorithm consider-ing total cost (denoted by  X  X GQ-TC X ), compared with  X  X ool X  and  X  X GQ X  on the same UCI dataset  X  X reast-cancer X . We can see from Figure 2 that  X  X GQ-TC X  effectively decreases the total cost of the learning model, and significantly outperforms  X  X ool X  and  X  X GQ X  with most querying cost settings. More specif-ically, we can discover the similar pattern between  X  X GQ-TC X  and  X  X GQ X  as in the previous subsection: When the querying cost is relatively low (such as C
Q =1+0 . 05 querying cost is relatively high (such as C Q =1+0 . 5  X  r 2 ),  X  X GQ-TC X  often significantly outperforms  X  X GQ X .

The t-test results on the 15 UCI datasets are summarized in Table 4. It clearly shows that,  X  X GQ-TC X  performs significantly better than  X  X GQ X  on most (or even all) tested datasets, when the querying cost is relatively high ( C
Q =1+0 . 5 TC X  still wins (or at least ties) on a majori ty of tested datasets, especially when the querying cost is relatively low ( C Q =1+0 . 05  X  r ). These experimental results clearly indicate that  X  X GQ-TC X  can indeed significantly decrea se the total cost, and outperforms  X  X GQ X  and  X  X ool X .
 4.4 Approximate Probabilistic Answers In the previous experiments, we have assumed that the oracle is always capable of providing accurate probabilistic answers for the generalized queries. However, in real-world situations, it is more common that only  X  X pproximate probabilistic answers X  are provided (especially when the oracles are human experts). We spec-ulate that small perturbations in the probabilistic answers will not dramatically affect the performance of the proposed al gorithms. This is because small pertur-bations in label probabilities only represent light noises. These light noises could be cancelled out in the successive updates of the training set. With a robust base learning algorithm (such as the bagged decision trees), such small noises would be insensitive. In this subsection, we study this issue experimentally.
To simulate the approximate probabilistic answer, we first calculate the exact accurate probabilistic answer from the target model, and then randomly alter it with up to 20% noise. Figure 3 demonstrates the performance the proposed algorithms with such approximate probabilistic labels (denoted by  X  X GQ-AC (appr) X  and  X  X GQ-TC (appr) X ), compared with  X  X GQ-AC X  and  X  X GQ-TC X , with the setting ( C Q =1+0 . 5  X  r ) and on the typical data ( X  X reast-cancer X ).
We can clearly see from these figures that, when only the approximate prob-abilistic answers are provided by the oracle, the performance of the proposed algorithms are not significantly affected . The similar experim ental results can be shown with other settings and on other datasets. This indicates that, the proposed algorithms are rather robust with such more realistic approximate probabilistic answers, thus can be directly deployed in real-world applications. In this paper, we assume that the oracles are capable of answering general-ized queries with non-uniform costs, and study active learning with generalized queries in cost-sensitive framework. In particular, we design two objective func-tions to choose generalized queries in the learning process, so as to either balance the accuracy/cost trade-off or minimize the total cost of misclassification and querying. The empirical study verifies the superiority of the proposed methods over the existing active learning algorithms.

