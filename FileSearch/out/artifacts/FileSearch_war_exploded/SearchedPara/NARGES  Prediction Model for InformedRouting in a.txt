 Rapid increases in the number of Internet users and services have prompted re-searchers within academia and industry to contemplate smart ways of supporting applications with the required Quality of Service (QoS). Service availability is a crucial part of QoS and the network infrastructure must be designed so as to provide high availability to meet QoS. The target of 99.999% availability permits five minutes of downtime per year [1].

There are certain QoS parameters including packet delay, jitter and loss, which may be used as decision factors for online routing management. Although consid-erable efforts have been placed on the I nternet to assure QoS within autonomous systems, the dominant best-effort communications policy does not provide suf-ficient guarantee without abrupt change in the protocols [2] for the Internet. Estimation and forecasting of the end-to-end delay, jitter and packet-loss are essential tasks in network routing management for detecting anomalies.
A considerable amount of research has b een done to forecast time-series with  X  X umerical X  methods. However, while dealing with large online datasets, these methods are time consuming and may not be efficient for real-time application such as multimedia streaming. Zadeh [3] suggests a transition from  X  X omputing with numbers X  to the manipulation of the  X  X uman perceptions. X  Consequently, research projects [4,5] have started focusing on approximation of time-series with non-numerical methods. In this way, the ti me cost for trivial forecasting accuracy in the numerical methods may be avoided.

By prioritising Internet traffic more efficiently, QoS functions can address performance issues related to emerging Internet applications such as real X  X ime voice and video streaming. An effective routing mechanism and its management are crucial to satisfactorily support diverse services in such networks. Routing tables, as the maps in packet delivery throughout the network, are dynamic and get updated by network state X  X ased events. Typical network events include node failure, link failure and congestion. However, a major issue with current routing mechanisms is that they generate routing tables that do not reflect the real X  X ime state of the network and ignore factors like local congestion.
 Packet-loss in the Internet mainly o ccurs due to congestion in the links [6]. Real X  X ime multimedia applications are sensitive to packet-loss, and packet re-transmission is not an acceptable solutio n with these sorts of application. Pre-dicting packet-loss with a certainty from network dynamics of past transmissions is crucial knowledge to inform the next g eneration of smart routers with bet-ter decision factors. We propose a data mining model for classification of links that have a high probability of packet-loss. The model is originally intended to contribute to making informed decisions within smart edge routers where the quality of transmissions should be controlled and is primarily determined by the level of packet-loss.

The current work extends our previous work [7]. The delay and jitter provided by a historical symbolic delay approximation model, called HDAX, is used within the proposed data mining model to pred ict the average number of packets lost in a link. The experiments with HDAX show better accuracy in forecasting the delay and jitter time X  X eries as well as a reduction in the time cost of the forecasting method. To make the forecasting faster, we changed the perception X  based function to a deterministic mapping function to avoid the time-cost of fuzzification and defuzzification.

We propose an informed decision-making model for routing management in a communications network, called NARGES as shown in the Fig. 1. The basic idea of our proposed model is to predict packet-loss in a network node by ap-proximating the trends and values of the delay according to observed patterns. As shown in Figure 1, the model estimates the current trend and value assigned to the node based on the most two recent trends and values. The approximated current values of delay and jitter are then fed into a multilayer perceptron for predicting the packet-loss.
 To evaluate the proposed loss prediction model, we used heterogeneous data. The data is categorised into ten categories in terms of the end-to-end network path and the network queuing policy. Accordingly, we ran a series of indepen-dent experiments with these different categories of datasets. The accuracy of the results, the speed of the algorithms and the cross-correlation of the forecasts of HDAX and predictions of the proposed data mining model are compared to the results from autoregressive moving average (ARMA) model.

The rest of this paper is organised as fo llows. Section 2 describes related work in predicting performance traces. In s ection 3 we describe the forecasting and predictive module of the proposed model. In section 4 the evaluation procedure is explained and later in section 5 we pr esent results of our experiments on applying our model. Section 6 concludes the paper. This research addresses the issue of defining a prediction model based on a symbolic time-series forecasting model. T he model uses historical frequencies of observed patterns in adjacent time-st amped windows within a time X  X eries.
Originally, the research project defined to produce an  X  X nformed X  [8] data mining model to be used in a smart network router for online routing manage-ment. We used a Hybrid model framework as suggested in [9]. In [10] and [11], they present ideas for the definition of a perception based function.
Packet delay and jitter show a temporal dependency with the packet-loss in the Internet [12,13]. Consequently, the r esearch projects have studied the delay, jitter, packet-loss and other performance traces in the network so as to predict network anomalies. If packet n is lost, packet n + 1 is likely to be lost. This can lead the network to a  X  X ursty X  packet-loss in a real X  X ime communications network and may degrade the QoS and the effectiveness of recovery mechanisms.
A quantitative study of delay and loss correlation patterns from off-line anal-ysis of measurement data from the Internet has been done by [14], although it did not consider real X  X ime prediction of packet-loss from the jitter data of online communications [6].

The Rocha-Mier et al. model [9] suggests the measurement and modelling of sequences of network variables based on data network statistics. They have created a useful network scenario using OPNET Modeller. Although real network data variables could be derived from the data logs by the use of intelligent agents or manually by system administrators, there may be violation in accessing data throughout the Internet. Therefore, they adopted the modeller to study various levels of the network traffic load as well as types of services and applications.
The motivation of our work is to take a perception X  X ased approach inspired by [10] embedded in a machine learning framework to predict network variables similarly to [9]. We validate our work using a combination of historical network traffic data and simulated data. Specifically our experiments used traces from network traffic archives generated by Napoli University  X  X ederico II X  [15] to test the impact of various network congestion levels, from  X  X uiet X  to a network with  X  X ursty X  packet-loss, on the proposed model. Simulated offline traces generated by OPNET Modeller, were used to test the impact of different queuing policies on the proposed model in longer experiments. This is a standard approach in the networking domain. The NARGES model, as presented in Figure 1, is a hybrid model that predicts the packet-loss at time t + 1 based on the approximated values of delay and jitter at time t . The delay and jitter forecasts at time t , are approximations of the current values of the time-series forecasted by HDAX model according to the historical trends and values of the corresponding time-series in the previous  X  X wo X  periods, i.e. t  X  1and t  X  2 . The following sections describe the two modules: HDAX and a multilayer perceptron, as the two constitutive parts of NARGES model. 3.1 Forecasting Module: HDAX In this section, we briefly describe our approach to forecasting time-series values from previously observed patterns of del ay and jitter. We use a mapping function for the definition of the patterns for time-series approximation, which is different from what we presented in our previous work [7].
 The HDAX algorithm is defined based on the model definition suggested by Tresp [16]. where f is either known or approximated sufficiently well by a function ap-proximator and t is the zero-mean noise with probability density P ( ), which represents dynamics that are not modelled. Let y t be the value of the discrete time-series at time t and Y t the trend of two consecutive values at times t  X  1and t . We make the underlying HDAX model of the time series with order N =3as We represent possible future trends of the QoS time series y t of delay and jitter values at time intervals t  X  1and t  X  2 with categorical terms from the where these symbols are defined in Table 1. The basic trends in Table 1 are defined with linguistic variables based on a deterministic  X  X apping function. X  Within the mapping function, each of the categorical terms maps an interval on the domain of real numbers to a linguistic representative.

We define  X  previous-current-next  X  patterns (also called the i  X  j  X  k patterns) with a combination of three consecutive trends, at times t  X  2, t  X  1and t as shown in Figure 2. Our approach consists of two phases: training and simulation. The max in Table 1 is the maximum value of a time-series, shown later with the dashed line in Fig. 2.
 HDAX Training. Figure 2 shows a  X  X liding window X  that moves over the training data and makes patterns consisting of three consecutive trends, i , j and k ,attimes t  X  2, t  X  1and t (previous, current and next) together with their frequency in the look X  X p table. The look X  X p table is then used in the simulation phase to approximate the next trend, at time t , and the associated delay/jitter value from the current and previously observed trend patterns, where i , j and k are the respective indices for previous, current and next trends (0  X  i,j,k  X  5) and F ( i,j,k ) is their respective frequency. From the look X  X p table, the probability of i  X  j  X  k patterns is estimated as where i , j and k are the indices for the respective patterns at times t  X  1, t  X  2 and t ,and N k is the number of total observations for all i  X  j  X  k patterns. HDAX Simulation. Based on the most frequently observed patterns in the last two consecutive trends at time t  X  1and t  X  2, the HDAX algorithm uses the estimated conditional probability to approximate the trend at time t .The Y t is a trend value at time t in the time-series of trends, defined based on the last two trends seen at times t  X  1and t  X  2. Formally speaking, we estimate the P k ( i,j ) as follows where i and j are the indices of the observed trends at times t  X  1and t  X  2, respectively. The trend at time t , with the index k , may take six possible values from the alphabets in Eq. 3. Based on this, the look-up table is used to forecast the next trend and value of the time-series based on the trend with highest frequency in this table: With the trend k is estimated, the y t can be approximated based on time step-size between y t  X  1 and y t and the slope of the line at y t  X  1 . 3.2 Predictive Module: Multi-layer Perceptron The predictive module calculates the average packet-loss. As described above, the outputs of HDAX are approximations of the values of the network traces at time t . They are used within a multilayer perceptron (MLP) to get better precision for real-time pack et-loss prediction at time t +1.

The MLP is a feed-forward network with back-propagation learning rule and one hidden layer. As shown in Fig. 1, the MLP has two input layer nodes and forecasts delay and jitter. It is designed and implemented using the MATLAB neural network toolbox.

Optimum parameter values for number of hidden layer neurons, learning rate and momentum was defined empirically by using a training data-set and choosing the parameter values with the highest te st accuracy. The network was tested with between 1 and 100 neurons in the hidden layer. Based on these experiments, ten hidden layer neuron were chosen. The section describes experiments for ev aluating the accuracy, speed and qual-ity of the output of our model and ARMA. We implemented ARMA algorithms based on [17]. The results of HDAX and NARGES are compared with ARMA results. In the experiments, the accuracy of algorithms was compared using nor-malised root mean square error (NRMSE). Performance was compared using the run time. Prediction quality was evaluated with the normalised correlation coefficient with MATLAB X  X  cro ss-correlation function. 4.1 Datasets As mentioned above, the data consists of three QoS traces of delay, jitter and packet-loss. For each experiment three sets of data are considered: the original data generated by D-ITG or OPNET, the output of HDAX (or NARGES) and the output of ARMA. Each dataset is divided into training and test datasets (25% and 75% respectively).

The results were categorised accordin g to datasets used for the experiments in two ways: (i) according to the end-to-end path and (ii) according to the queuing policy used. Forty-six dataset s were used for computing results. Each dataset includes time-stamped traces of delay, jitter and loss values. There are 36 datasets from D-ITG with an average 3000 values in each of delay, jitter and packet-loss time-series. There are als o 10 datasets generat ed by OPNET, 9 with 48000 values and one with 1,000,000 values.
 The data generated by D-ITG are gathered in two ways: an archive from the University of Napoli [15] and data probed over a University network test-bed. Samples were obtained by sending probe packets with a packet rate of 100 packets per seconds. The measurement unit of the delay and jitter is either milliseconds or nanoseconds while for packet-loss the value represents the average number of lost packets in the window of the times. The network test-bed was formed by two nodes on the University LAN: a laptop with 1.70 GHz processor, IntelPro 2200BG network connection and a node o n a HPC Linux Cluster running Red Hat Enterprise Linux 6 (64bit) with processor rating between 3.06-3.46GHz and a gigabit inter-node connection. An end-to-end path (e2eP) definition is considered for the datasets obtained from D-ITG as shown in Table 2.

We used OPNET datasets to study the effect of different packet transmission policies, longer experiments and network congestion states on the performance of the implemented data mining model. The selection of service discipline in the routers (FIFO, WFQ and PQ) can affect VoIP applications and link congestion. Thus, the performance of the prediction model is evaluated using these datasets. Simulations ran on a laptop with 1.70 GHz processor with IntelPro 2200BG network connection and OPNET Modeler 15.0.A. Three methods were used analysis: accuracy, speed and correlation analysis for the proposed models versus ARMA. Friedman X  X  test [18] is used to rank algo-rithms and to test the hypothesis of the similarity of the algorithms based on Holm X  X  test. The p  X  value is used to reject or accept the above  X  X ull hypothesis. X  5.1 Model Performance The accuracy and speed of NARGES model and its HDAX subcomponent was examined by comparing the NRMSE betw een the model outputs and the original traces. Figure 3, shows the results of HDAX for delay and jitter traces as well as the NARGES results over packet -loss traces in a top down order.
The average precision of the forecasted delay and jitter time-series of HDAX and ARMA as well as the NARGES precision for predicted packet-loss are shown within column (a) in Figure 3. Column ( b) shows a comparison between the speed of the models with ARMA. In terms of sim ilarity measurement between the orig-inal datasets and the output by HDAX, NARGES and ARMA, the maximum similarity of normalised cross-correlations was used for correlation analysis be-tween the time-series. Column (c) in Figu re 3 shows the respective correlation coefficient between the output of th e models and the original data.
The predicted average packet-loss f or NARGES was compared to ARMA. As the Fig. 3 shows, NARGES predicts more precisely than ARMA with OPNET datasets but is slower. This is because NARGES has more modules and processes more data than ARMA to predict the final packet-loss values. The training time of the MLP module accounts for the longer time taken to run our model. 5.2 Model Ranking Friedman test was run and the stored statistic used to calculate the Holm X  X  test p  X  value , which is a decision factor for reject ing or accepting the null hypothesis. It also calculates the average ranking of the algorithms used in each test.
Friedman X  X  test is a non-parametric equivalent to the parametric repeated measures ANOVA test. It computes the ranking of the measured outputs for an algorithm with other algorithms, assigning the best of them the ranking 1 and the worst the ranking k . According to the null hypothesis, it is supposed that the results of the algorithms are equivalent and the rankings are also similar.
A similarity test between the precision , performance (speed) and the corre-lation of the output of the models with the original test data is performed via nonparametric Holm tests. The tests were conducted over the results collected from the runs of HDAX, ARMA and NARGES models with the three traces of delay, jitter and loss within each datasets. We have done this to rank the algo-rithms and test the similarity hypothesis between HDAX and NARGES models and ARMA. The Holm tests are testing the similarity of algorithms accuracy, speed and correlation coefficient for dela y, jitter and packet-loss time-series. In Table 3, the algorithm shown in each row works significantly better than ARMA if their corresponding average ranks differ by at least the critical differ-ence, which are the ones with p  X  values  X  0 . 05. In Table 3, the algorithm name showed in each row are taken as the better when the null hypothesis is rejected.
The Holm X  X  tests shows that HDAX ranking is better than ARMA for the precision of the results and the speed of the algorithms whereas ARMA ranking is better than HDAX for cross-correlati on between the forecasted and original time-series. Moreover, according to the p  X  value s, HDAX forecasts significantly better and faster than ARMA for delay traces while ARMA has more correlated outputs in comparison to the original delay data in the 36 runs for the D-ITG datasets in section 4.1. Two algorithms, HDAX and ARMA, perform the same in forecasting jitter values because Holm X  X  t ests accept all null hypothesis. For the whole model outputs accuracy and quality of the predictions, NARGES predicts the packet-loss the same as ARMA does, although NARGES ranks higher. In terms of the speed of the models, the p  X  value of the ARMA speed is less than the significance level (  X  =0 . 05).
 The Holm X  X  test for OPNET datasets, explained in section 4.1, shows that HDAX forecasts significantly more accu rate than ARMA over the delay datasets and has a better quality in terms of the c ross-correlation between its forecasts and the original data. HDAX is faster over jitter datasets while ARMA forecasts faster over delay datasets. Unlike the tests with D-ITG datasets, the NARGES outputs with OPNET datasets show significantly better precision and quality of predictions. It means that our model shows better precision and prediction compared to ARMA in longer experiments. In terms of the speed of the models, the p  X  value of the ARMA speed is less than the significance level (  X  =0 . 05).
Currently network routers must send information between routers to inform about the peer status. The results in this paper demonstrate, in a simulated setting at least, that a data mining agent can predict the peer status to reduce or perhaps eliminate the unnecessary network data transmission overhead and the time required for sending and receiving repeated packets. This paper presented a packet-loss prediction model based on our earlier work [7]. We designed and implemented a hybrid data mining model, NARGES, which is using the forecasted current values of delay and jitter, to predict the future packet-loss rate assigned to a network node. We used a non-numerical approach to predict packet-loss in multimedia streams by observing the delay and jit-ter time-series. The Model is validated with heterogeneous QoS traces and the results show that the quality and the precision of the proposed model is signifi-cantly better than AMRA. However, NARGES was slower than ARMA because it has to process more inputs. As can be seen from the competing speed of HDAX module, it is the training time of the MLP module that degrades the speed of the model. In Table 3, the significant difference between the p  X  value of the Holm X  X  tests for the D X  X TG and the longer OPNET datasets implies that our model can work faster in a real X  X ime network experiment.

