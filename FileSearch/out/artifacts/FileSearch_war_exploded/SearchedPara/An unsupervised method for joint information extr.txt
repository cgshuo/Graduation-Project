 1. Introduction
With the rapid growth of Internet technology which banishes the geographical barrier between customers and vendors, online shopping is becoming increasingly common. Vendors set up their own Web sites providing basic information for a product and typically highlighting certain important product features for marketing purpose. Customers can shop over the Internet by browsing different vendor Web sites to learn more about their desired products. For example, Fig. 1 depicts a Web page collected from a vendor Web site 1 showing information about a digital camera. This page contains several blocks of information. In the uppermost block next to the photo, it provides a list of special or outstanding features of the camera. For instance, one major product feature is  X  X 12.8-megapixel X . In the table below the photo, there are some basic product features. Though online shopping brings much convenience to customers, information conveyed by a single vendor may be biased.
For example, Fig. 2 depicts a Web page collected from a Web site scribes some characteristics about the same digital camera shown in Fig. 1. Although the two pages refer to the same product and share some common product features, they also convey some different product features in the content. For example,  X  X 2.5 in.
LCD Screen Size X  and  X  X 12.8-megapixel X  are common product features found in both vendor sites. On the other hand, the product vendors manually to obtain some commonly found important product features. From the vendor X  X  point of view, it is beneficial to be aware of the important product features displayed in other vendors X  site because it is advantageous to know how com-petitors view the products. Obviously, manually browsing the product information from a large number of different sites is time-consuming and tedious. This problem raises the need for information extraction which aims at automatically extracting precise and useful text fragments from textual documents on different sites. Such extraction methods can be applied to extract the product features from different vendor Web sites. Apart from information extraction, there is also a need to conduct mining in order to achieve the goal of the problem. For example, we wish to obtain product features which are important. Both tasks of information extraction and mining from multiple sites can be done in a reinforcement manner.

One common assumption of existing approaches to information extraction from Web pages is that the information con-tained in each page in different sites is considered independently. Hence the information extraction task is conducted sep-arately without any interaction among different sites. For example, consider the Web pages shown in Figs. 1 and 2 . These
Web pages are collected from two different Web sites and they contain the product feature information for the same digital camera. To extract the product features, existing methods treat these pages separately. Considering these two pages jointly, however, has mutual benefit since they contain useful clues which allow each extraction task to help the other. For instance, one product feature extracted from Fig. 1 is  X  X 12.8-megapixel X . Such discovered product feature strengthens the confidence in format of Fig. 2 , we can then infer that some product features are organized in a table format and one can extract more prod-can again guide the extraction in Fig. 1 . Consequently, the extraction process can interact with each other conducting extrac-tion and mining collectively.

Another shortcoming of many existing information extraction learning techniques is the need for manually provided training examples. A substantial amount of human effort is required to prepare a large set of training examples in advance.
This task is obviously tedious and time-consuming, and requires a high level of expertise. Moreover, owing to the difference between the layout format of the Web pages, a model trained for a particular Web site is not able to extract information from
Web pages collected from other Web sites. To extract information from other Web sites, one requires a separate human ef-fort to prepare another set of training examples.

We propose an unsupervised learning approach which can jointly extract information and conduct feature mining from a set of Web pages across different sites. One characteristic of our approach is that it allows tight interactions between the tasks of information extraction and feature mining. The decisions involved in information extraction and feature mining can be done in a coherent manner assigning solutions satisfying the quality of both tasks and at the same time eliminating potential conflicts.

Our framework is designed based on an undirected graphical model namely Conditional Random Fields (CRF) [29]. CRF is a probabilistic model which can model the interdependence between the neighbouring text fragments within the same Web page, as well as text fragments in different Web pages. We formulate the problem as a label assignment problem on this graphical model. Multiple Web pages can be considered using a unified model and the information can be extracted by label-ing the tokens collectively in a single framework. The graphical model is automatically constructed by analyzing several clues from the Web documents. One useful clue is the layout format information of the Web pages. For example, the product the product features are organized in a list format and in the same block. Such layout format information can help identify the text fragments to be extracted. We also consider the DOM represent the structure of the Web page and provide very useful information for product feature extraction. Our approach also allows easy incorporation of external knowledge. Sometimes, users already have some prior knowledge of the domain from which the information is extracted, or knowledge about the mining tasks. For example, in the important product feature mining application, customer reviews can be treated as a form of prior knowledge in the modeling and learning process. We derive a set of features related to the layout format, the prior knowledge and the DOM structure in our framework. An expectation X  X ax-imization (EM) based adaptive learning method is designed for parameter estimation in the model.

To demonstrate the robustness and efficacy of our framework, we apply our framework to two different applications, namely, important product feature mining from vendor Web sites and hot item feature mining from auction Web sites.
For each application, we have conducted extensive experiments using a number of real-world Web sites in two different do-mains. The results are very encouraging demonstrating the effectiveness of our framework.

Wong et al. developed a framework for feature mining [47,48] . The technique proposed in this paper is more robust capa-ble of dealing with more sophisticated interactions between different sites. This framework is more general and can be easily applied to different applications. Moreover, we have conducted a more complete set of experiments to evaluate our framework.

The remaining content of this paper is organized as follows. Section 2 introduces two applications of our framework. Sec-tion 3 presents some related work. Section 4 describes the modeling and the learning algorithm employed in our framework.
Section 5 introduces the application of our framework to a task, namely, important feature mining, and presents the exper-imental results. Section 6 describes the application of our framework for tackling another task, namely, hot item feature min-ing from different auction Web sites, and present the corresponding experimental results. We draw conclusions and discuss some potential extensions of our framework in Section 7. 2. Applications of our framework
We apply our framework to two applications in this paper. The first application is important product feature mining which aims at extracting product features and discovering important product features for a particular product from different vendor Web sites. Important product features refer to the product features that are either located in the foremost viewable vendor Web sites. Foremost viewable position refers to the position of the Web page which can be seen by users without scrolling. For marketing purpose, vendors are likely to place in a noticeable position, or display in some perceivable format, the product features to which most customers pay attention. The layout format of the product features can therefore be an evidence for the determination of important product features. Customer reviews of products in online discussion forums can also be treated as prior knowledge for discovering important product features since they may contain clues reflecting the customers X  needs. For example, Table 1 shows sample Web pages of customer reviews collected from an online discussion forum. This review mentions some important product features such as the LCD size of the same digital camera model shown in Fig. 1 . Such reviews can be collected automatically by information extraction wrappers [9,18] . The important product fea-tures discovered can be utilized in other intelligent tasks. Specifically, we make use of such information for mining similar products. This can reduce the human effort in analyzing the characteristics of different products.

We use a running example to illustrate our first application. It considers a set of Web pages collected from different ven-dor sites. Figs. 1 and 2 are two samples taken from the set of vendor sites. Some texts of customer reviews are collected such as Table 1 and they are used as prior knowledge. Our approach can automatically extract a list of product features and the lution Capacity (446 images/512 MB) X ,  X  X  X CD Size (3:2 CMOS) X , etc. from Fig. 2 . At the same time, it can also identify that  X  X  X CD Screen Size (2.5 in.) X  is relatively more important via our learning framework from a set of Web sites. One of the evi-dences is related to the fact that the product feature  X  X  X CD size X  is repeatedly mentioned in many Web sites. This LCD size feature is also of interest to customers as exemplified in Table 1 . The set of product features discovered is very useful for customers and vendors to understand more about the characteristics of the product. In our application, we make use of these discovered important product features to conduct mining for similar product of digital cameras. One advantage of using important product features is that the similarity is less likely to be affected by  X  X  X ninformative X  product features such as  X  X  X tandard Resolution Capacity X .

We apply our framework to build the second application, namely, hot item feature mining from auction Web sites. Hot items refer to the items that have a number of bids from potential buyers in the auction sites. Hot item feature mining aims at discovering the product features that most hot items possess. The features discovered can help sellers to decide the start-this application are presented in Section 6. 3. Related work
Various approaches have been proposed to extract information from semi-structured documents such as Web pages [1,5,10,18,25,28,40,42] . Information extraction wrapper is one of the promising techniques for extracting precise text frag-ments from Web documents [27]. A wrapper is normally composed of a set of extraction rules. Recently, wrapper induction methods have been proposed to automatically learn extraction rules from a set of training examples [8,36] . One major short-coming of the existing wrapper induction techniques is that the learned wrapper can only extract information from the same information source where the training examples are collected. For example, if the extraction rules are learned from the training examples collected from a particular Web site, the learned extraction rules cannot be applied to extract information from other sites. Flesca et al. proposed a method to classify Web pages based on HTML tag structure and choose appropriate learned wrappers to extract information [17]. New wrappers can then be learned for the unclassified Web pages. Their meth-od can only partially solve the problem. Moreover, the extraction rules learned can only extract the information specified in the training examples. For instance, if we just annotate the start time, end time, location and speaker in the training example in the seminar announcement domain, the learned wrapper can only extract these four attributes. Other useful information such as the title of the seminar will be lost.

Existing supervised wrapper learning approaches require human effort to prepare labeled examples for training wrap-pers. Several techniques have been proposed to reduce the need for training examples. Embley et al. proposed an approach to extracting information from Web pages by making use of a predefined ontological model of the domain [14]. Though their approach does not require training examples, manual effort and domain knowledge are still needed to define the domain ontology. Wong and Lam proposed a method for reducing the human effort by adapting the extraction knowledge from one information source to other previously unseen information sources [44]. However, the human work in preparing training examples cannot be completely eliminated. Various techniques have been developed for fully automatic information extrac-tion from Web pages without the use any training examples. IEPAD is a system aiming at extracting information by recog-nizing the repeated patterns inside the Web pages [4]. A system known as MDR can discover the data region in a Web page by making use of the repeated pattern in HTML tag trees [30]. Heuristics are then applied to extract useful information from the data region. However, both IEPAD and MDR assume that the input Web pages contain multiple records and repeated patterns.
 ROADRUNNER is another system making use of repeated patterns for information extraction [11]. The idea of ROADRUN-
NER is that Web pages of some Web sites are generated by an automatic Web page generation program. Their layout format are similar although the content of the Web pages is different. It exploits such evidence and recognizes the repeated patterns appeared in the Web pages. Hedley et al. proposed a two-phase sampling method to extract information from hidden Web documents whose data are stored in back-end databases [22]. The idea is to supply queries to search engines to sample a set of Web pages of a site. The templates which generate the Web pages are detected from repeated patterns and the query-re-lated data are extracted. One common limitation of the two approaches is that the Web pages are required to be generated from same template or have similar layout format. This may not be true in Web pages collected from different sources. Cres-cenzi et al. partially solve this problem by developing an approach to clustering Web pages based on their structures [12].
Their approach, however, focuses on the Web pages originating from the same site and information extraction from different sources is not applicable. Chuang et al. proposed a method to synchronize the data extracted by wrappers from multiple sources [7]. The idea is to make use of unsupervised wrapper learning techniques to extract data from different Web pages.
Since the data extracted may contain inconsistent information, tion concerning the segmentation of the text fragments from different sources by means of encoding techniques.
TextRunner is a system aiming at automatically extracting relation from Web pages on the Internet [2]. Grenager et al. applied hidden Markov model and exploited prior knowledge to extract information in an unsupervised manner [21]. How-ever, the quality of the extracted data in fully automatic mode is unlikely to be suitable for subsequent data mining tasks.
Another system called Armadillo has been developed aiming at extracting information from different sources without train-ing examples [6]. However, the method proposed relies on the seeds extracted from structured source or user-defined lexicons.

One common limitation of the existing information extraction approaches is that they can only extract information, but cannot conduct any mining task [16,24] . To discover new pattern or knowledge, a separate mining process is required to be conducted. The error encountered in the extraction step is unavoidably accumulated in the mining step. Recently, various techniques for collectively extracting information and conducting data mining have been proposed [32]. For example, Well-ner et al. proposed an approach for extracting different fields in citation and solving the citation matching problem using
Conditional Random Fields (CRF) [43]. McCallum and Wellner also proposed an approach for extracting proper nouns and linking the extracted proper nouns using a single model [33]. Culotta et al. developed a method exploiting CRF to extract entities from text and discover their relations [13]. Bunescu and Mooney proposed the use of relational Markov networks collectively to extract information from documents [3]. One major difference between these methods and our approach is that our approach is an unsupervised method and does not require any training examples.

Some existing methods related to product feature mining and auction Web site mining have been developed. Probst et al. [38] proposed a semi-supervised algorithm to extract attribute value pairs from text description. Their approach aims at han-dling free text descriptions by making use of natural language processing techniques. Hence, it cannot be applied to Web documents which are composed of a mix of HTML tags and free texts. Morinaga et al. proposed an approach to mining prod-uct reputation from online customer reviews [34]. Their idea is to determine the polarity of the sentences describing the opinion of the customers by making use of a set of syntactic and linguistic rules. However, one limitation of their approach is that it cannot extract the product features of products. Hu and Liu [23,31] proposed a system for summarizing customer reviews on a product posted on Web sites. They aim at classifying sentences with subjective orientation by making use of the subjective words such as  X  X  X ood X ,  X  X  X refect X . Popescu and Etzioni [37] also investigated this research problem. They first made use of the extraction system called KnowItAll [15] to extract the explicit features of the product. Next the extracted explicit features are utilized to identify the opinion or orientation of the reviews. Both of these methods apply linguistic techniques and focus on the sentences which are largely grammatical. Moreover, existing opinion mining techniques fail to utilize the collected opinions to predict automatically the importance of the product features.

Regarding the problem of mining from auction Web sites, Ghani and Simmons proposed an approach for predicting the
They consider different kinds of features including seller features, item features, auction features and temporal features in their approach. Machine learning techniques are then employed to predict the end-price of the items. Wong and Lam at-tempted to extract and summarize the product feature and the associated feature values of the hot items from multiple auc-tion Web sites [46]. The idea of their work is to extract the product feature and the associated values using hidden Markov models. Next a graph mincut algorithm is employed to identify the hot items in the auction sites by considering the ex-tracted data. All of these existing methods suffer from one major shortcoming in that the tasks of extraction and mining are conducted without interaction. Error made in the extraction process is likely to be accumulated in the mining task. 4. Model description and learning approach 4.1. Problem definition Consider a set of N Web pages denoted as P  X f P 1 ; P 2 ; ... ; P noted as D i 2 D . A DOM structure is an ordered tree structure containing two types of nodes. The first type of nodes are tag nodes that contain the presentation structure and layout format of HTML documents. The second type of nodes are text nodes that contain the text fragments to be displayed in the browser. As a result, a set of text fragments represented by
X  X f X i 1 ; X i 2 ; ... ; X i F i g can be collected based on the text nodes in D
Each text fragment X i j can be considered as a sequence of tokens represented by a sequence of observable variables  X  X the tuple  X  X i j ; k ; Y i j ; k  X  . Given a particular domain, we define R to be the prior knowledge about the domain. 4.1.1. Information extraction problem
Suppose the Web page shown in Fig. 1 and the corresponding DOM structure are denoted by P tively. The Web page can be broken down into a set of F i example,  X  X  X anon EOS 5D Digital Camera X  and  X  X 12.8-megapixel X  are samples of text fragments in Fig. 1 . The k th token in the sequence X i j , where 1 6 j 6 F i , is represented as X i the predefined values. In the important product feature mining application, each token can be labeled with one of the three as product feature value . The prior knowledge of the domain is represented by a set of observable variables denoted by R . For example, customer reviews can be treated as some prior knowledge in the important product feature mining application be-cause they may mention some product features of a digital camera. The terms contained in the customer review can be rep-resented by R .

The goal of information extraction is to predict the labels of tokens based on the observation. In principle, we aim at determining Y such that the probability P  X  Y  X  Y j X ; R ; D  X  is maximum, where X , Y , refer to the observations and labels of tokens from the Web pages in P ; D denotes the set of DOM structures; R refers to the prior knowledge of the domain. 4.1.2. Feature mining problem
Let V be a set of unobserved variables related to some non-trivial knowledge about a domain. Suppose the values of V depend on the observations and the labels of all the tokens  X  X and  X  X  X egapixel gross 13.3 X  are about the product feature values. Each sequence can be associated with an unobservable var-iable in V showing whether the sequence is related to important product features. The variables in V are interdependent since similar product features are likely to have similar importance. They are also interdependent on the observations and the labels of the tokens because normally the important product features are mentioned by many different Web sites, positioned in a perceivable area, or displayed with some special layout format such as bold text. For example,  X  X 12.8-mega-pixel X  and  X  X 2.5 in. LCD screen X  are listed on the topmost portion next to the camera photo in Fig. 1 .

We define the feature mining problem as determining the values of V such that the probability P  X  V  X  V j X ; Y  X  is maximized. 4.1.3. Joint information extraction and feature mining problem
Intuitively, the information extraction and feature mining problems can be tackled in separate steps. However, since the values for both Y and V are unknown, the prediction of Y in the information extraction problem may contain errors and these errors will be accumulated in the prediction of V in the feature mining problem. As a result, we define the joint infor-mation extraction and feature mining problem as below.

The objective of joint information extraction and feature mining is to find the values for both Y and V such that the joint probability P  X  V  X  V ; Y  X  Y j X ; R ; D  X  is maximized. The joint information extraction and feature mining problem can solve the two problems together and obtain a solution satisfying both tasks. In this paper, we propose a probabilistic undirected graphical model to address this problem. 4.2. Modeling via undirected graph
In our framework, we formulate the tasks of joint information extraction and feature mining as a single graph labeling problem using Conditional Random Fields (CRF). CRF is a discriminative framework based on undirected graphical model [29]. One advantage of CRF is that it can model the interdependence between entities, without the need to know their actual causality. Another advantage is that unlike the Naive Bayesian model or other generative models which assume that the fea-tures are independent, CRF allows the use of a number of overlapping or dependent features. Moreover, much of the liter-ature also shows that discriminative models can achieve promising performance in practice.

Each node in the graph represents a variable and each edge represents the interdependence between the connected vari-ables. Consider our first application of important product feature mining from different vendor Web sites. Fig. 3 shows a sim-plified CRF model automatically constructed given a set of Web pages concerned with the same product. The size of the graph is much larger when dealing with real data. There are two kinds of nodes. The shaded nodes represent observable vari-ables while the unshaded nodes represent unobservable variables. In page P  X  X ; Y i j  X  . In the sequence, each Y i j ; k is connected to Y each token, the labels of the neighbouring tokens, the observation of the sequence, and the DOM structure of the page are interdependent. For presentation clarity, we introduce another set of unobservable variables denoted by W to the set of mentioned product features in the Web page P are the mentioned product features. Obviously, the mentioned product features are interdependent on the observation of the sequences and labels of the tokens. The important product features are represented by the set of unobservable variables de-noted by V . It is interdependent on the mentioned product features, as well as the observation of the Web pages because normally the important product features are mentioned by many different Web sites, positioned in a perceivable area, or displayed with some special layout format such as bold text. The prior knowledge R of the domain is connected to all the
Y and X i j since they are interdependent. For example, the customer reviews can be treated as some prior knowledge in the important product feature mining application because they may mention some product features of a digital camera. As a result, R can be a set of random variables representing the occurrence of a word which exists in some customer reviews.
It is connected to all the Y i j ; k and X i since they are interdependent. Fig. 3 also shows another sequence  X  X the same page P i , two different sequences,  X  X i 0 p ; Y
Once the undirected graph is constructed, the conditional probability of a particular configuration of the hidden variables, given the values of all the observed variables, can be written as follows: C  X  x ; y  X  . Z  X  x  X  is called the partition function defined as We define the clique potential as a linear exponential function as follows: wise. Hence, Eq. (1) can be written as follows:
Given the set of c i , one can find the optimal labeling of the unobserved variables of the graph via conducting inference. The graph typically consists of a large number of combinations for the labels of all the unobservable variables. Hence, direct com-putation of the probability of a particular labeling of the unobservable variables is infeasible. The inference can be computed by a message passing algorithm, known as the sum X  X roduct algorithm, by transforming the graph into junction tree or factor graph [26]. In this paper, we employ the factor graph approach to infer the values of the hidden variables.
Given a factor graph without cycle, each node can be treated as the root of a tree. The depth d of a node is defined as the maximum number of edges for a message passed from the node to the furthest leaf node. The depth d defined as the maximum depth of any message. Exact inference can be conducted in a single pass of messages and hence d max iteration is required. For a factor graph with cycles, the algorithm can achieve approximate inference and converge in few iterations in practice [35,41] . Suppose each variable node in the factor graph has M states, the computation of a mes-number of variable nodes connected to f . The computation of the message, denoted by l iable node t involves summation of terms of N  X  x  X  1 variables, each of which can take M different states. Each term in the merate all the combinations of the observable variables. As a result, inference of our framework can be conducted efficiently.
In addition, since the computation of the message of each node only considers information from the neighbouring nodes, the computation of different nodes can be done in parallel. We can apply distributed computation to accelerate the inference. By finding the configuration of the hidden variables achieving the highest conditional probability stated in Eq. (1), the desired important product feature and the product feature values can then be discovered. 4.3. Incorporating prior knowledge
As described before, prior knowledge is helpful for discovering important information in the domain. For example, in the important product feature mining application, important features are normally placed in foremost viewable position or dis-played in some special formats such as bold text. Recall that CRF is characterized by a set of binary features and the asso-ciated weights in Eq. (4). We can then easily incorporate prior knowledge by choosing the initial value of some weights before invoking our EM-based voted perceptron algorithm described in Section 4.4. For instance, suppose we know that the term  X  X  X esolution X  is likely to belong to the product feature of a digital camera. We can set a larger initial value for c if f duce a set of such feature functions about the relationship between tokens, prior knowledge, and labels in our framework.
There are two types of prior knowledge used in our framework. The first type is derived from a set of customer reviews about the same domain collected from online discussion forums. Consider a term denoted by e appearing in the texts from
This function equals zero otherwise. For example, the review in Table 1 consists of the term  X  X  X CD X . Then we design one bin-The initial weights for these associated feature functions are then set to a higher value.

Another type of prior knowledge is related to the layout format of the Web pages. Similarly, we define a set of feature functions capturing the relationship between tokens, layout formats, and labels. For example, we define a function that equals one if the token is in some special layout format such as bold and colored, and is labeled as an important product feature. It equals zero otherwise. The initial weights for such kind of feature functions are set to a higher value. 4.4. Unsupervised learning algorithm
Recall that our approach is an unsupervised learning method. The actual labels of the unobservable variables are not known. We cannot apply existing CRF learning algorithms which can estimate the value of the weights c each f i in Eq. (4) [29,39] . To tackle this problem, we develop an expectation X  X aximization (EM) based voted perceptron algo-example, the predicted label for the j th training example in the k th iteration, and the weight for the i th feature function able variables. In the M-step, we employ the voted perceptron algorithm augmented with the following weight updating function: where q denotes the learning rate of the algorithm. The rationale of our learning approach is described below in detail.
Suppose we have a set of training examples denoted by Tra for which the actual labels of the variables are known. We define the log likelihood function as follows: employed in the learning of CRF. Its objective is to find the set of c convex and achieves maximum when the following condition holds:
Therefore, one can obtain the set of c i achieving the maximum of Eq. (6) by using iterative methods such as conjugate gra-dient methods or voted perceptron algorithm. In particular, Fig. 5 shows the outline of voted perceptron for learning the parameters. In essence, the voted perceptron algorithm estimates the weight by iteratively minimizing the following expression:
Compared with the algorithm stated in Fig. 5 , our EM-based voted perceptron algorithm estimates the weight by iteratively diminishing the following expression: and can result in a local optimal solution depending on the initial set of parameters. We tackle this problem by incorporating the prior knowledge when choosing the initial parameters as described in Section 4.3. 4.5. Using discovered important product features for similar product mining
The output of the important product feature mining contains a set of discovered important product features and the asso-ciated product feature values for a particular product. These important product features can be further utilized to build more applications. We demonstrate the usage of such features in mining similar products.

Suppose there are two products, namely Prod A and Prod B . We denote that f product features of Prod A and the associated feature value, respectively, where 1 of discovered important features for Prod A . Similarly, we denote f of Prod B and the associated feature value, respectively, where 1 tant features for Prod B . We define the similarity, namely, sim  X  Prod where /  X  f m  X  Prod U  X  ; f n  X  Prod V  X  X  is the product feature similarity function between the two product features f f metric. Recall that the product features and the associated values can be treated as text fragments in the Web pages. Both / and w are defined as the normalized token level edit distance between two text fragments described in [45]. We give a brief review on the normalized token level edit distance in this paper. Consider two text fragments, each of which can be treated a sequence of tokens. We first compute the character level edit distance between each pair of tokens from the two text frag-distance between two tokens is normalized to the range between 0 and 1. Next, we compute the token level edit distance between two text fragments, with the cost of insertion and deletion of a token equal to one, and the cost of modification of a token equal to the character level edit distance between the tokens. The normalized token level edit distance is then com-puted by dividing the token level edit distance obtained by the largest number of tokens among the two text fragments. Both of the character level and token level edit distance can be computed efficiently by dynamic programming. 5. Application and evaluation: important product feature and similar product mining
We have conducted extensive experiments to evaluate the effectiveness of our framework for important product feature mining as well as similar product mining. Two different domains, namely, the digital camera domain, and the MP3 player domain, were investigated. In each domain, we randomly chose ten different products. For a particular product, we collected
Web pages describing the product from different vendor Web sites as depicted in Table 2 . These vendor Web sites were also randomly chosen by querying Web search engines. Table 3 shows the products used in the experiment. The products labeled
D1 X  X 10 were collected for the digital camera domain and the products labeled M1 X  X 10 were collected for the MP3 player domain.

We manually annotated important product features on the Web pages. These annotations are treated as the gold standard for evaluation purposes. The annotation was done as follows: for each Web page, a human expert first identified the product feature and the product feature values that are highlighted, for example, by special font styles or location in a perceivable area, by the Web designers. For example,  X  X 12.8-megapixel X  and  X  X 2.5 in. LCD Screen Size X  are samples of the annotated prod-uct feature values in Fig. 1 . The product features represented by the annotated text becomes the base set of important prod-uct features. For example,  X  X  X esolution X  and  X  X  X CD Screen X  are the two important product features represented by  X  X 12.8-megapixel X  and  X  X 2.5 in. LCD Screen Size X , respectively. Next, in all Web pages, the product features and the product feature values that are directly related to the product features in the base set, together with the product features in the base set, were collected and labeled as important product features. For instance,  X  X  X CD Screen Size (inches) 2.5 in. X  shown in Fig. 2 is considered as one of the important product features.

Two sets of experiments have been conducted. In these experiments, important product features of each product are dis-covered from the Web pages about the same product on different vendor sites. For example, all the Web pages from all the vendor sites about the digital camera  X  X  X anon EOS 20D X  are considered. The first set of experiments makes use of our ap-proach to the discovery task. We utilize the customer reviews automatically collected from online discussion forum knowledge. The customer reviews are only processed by stopword removal. We then apply our approach to the Web pages in a domain to extract important product features. The Web pages are first segmented into text fragments by consideration of the the average number of tokens per text fragment is about 20. As a result the graph constructed consists of a few thousand nodes. The second set of experiments makes use of an existing unsupervised Web information extraction system known as ROADRUNNER 6 [11]. ROADRUNNER makes use of the regularity of Web pages with similar layout format to extract information. In this set of experiments, Web pages originating from the same Web site are therefore grouped together and fed to ROADRUN-
NER. On the basis of the regularity of the Web pages, ROADRUNNER can then extract the data contained. In contrast with our approach to handling pages across different sites jointly, ROADRUNNER can only extract information from Web pages originat-ing from each site independently. Since ROADRUNNER does not calculate the degree of importance of the extracted product features, all the data extracted are regarded as important features. Notice that in both sets of experiments, there is no human intervention involved.

We adopt two metrics, namely, precision and recall, to measure the performance. Precision is defined as the number of important product features correctly extracted divided by the total number of extracted important product features. Recall is defined as the number of important product features correctly extracted divided by the total number of actual important product features. Only exact matched product features are considered as correct extraction.
 Table 4 shows the performance of our approach in the digital camera domain. The first column shows the product label.
Our approach obtains average recall and precision of 81.9% and 81.2%, respectively. The performance of our approach is sig-nificantly better than that of ROADRUNNER, which obtains average recall and precision of 21.0% and 22.4%, respectively.
ROADRUNNER fails to effectively discover important features because, in many cases, it can only extract a large block of texts such as text in tabular format from the Web pages. For example, ROADRUNNER mainly extracts data from the tabular data under the product pictures, while those important product features located on the right-hand side of the product picture cannot be extracted. It leads to a degradation in the performance of mining important product features.

Table 5 presents the important product feature mining performance in the MP3 player domain. The results are similar to those of the digital camera domain, with average recall and precision of 71.1% and 69.7%, respectively, for our approach. ROADRUNNER fails to produce good results. The average recall and precision are about 20.7% and 16.5%, respectively.
We further utilized the extracted important features to conduct similar product mining. For each product, two methods were employed to find the similarity between the other products. The first method is to employ our approach as described in
Section 4.5. The second method is based on ROADRUNNER. The text extracted by ROADRUNNER is treated as a bag of words describing each product. The term frequency X  X nverse document frequency ( tf X  X df ) for each term is computed. The similar-camera domain. The first column shows the product label with which other products are compared. The second column is divided into two sub-rows. The first sub-row shows the five most similar products arranged in descending order of the sim-ilarity measure by means of our similar product mining approach. The second sub-row shows the five most similar products arranged in descending order of the similarity measure by means of the ROADRUNNER approach. We conducted a qualitative investigation on the results between our approach and the ROADRUNNER approach. It can be observed that our approach provides better relationship finding among the products because the similarities between the products are based on the important product features or the major characteristics of the products. For example, both D1 and D8 possess important product features such as  X  X  X LR camera type X ,  X  X  X SO 1600 X , and  X  X 18 X 55 mm focal length X . However, the ROADRUNNER ap-proach considers that D5 is more similar to D1 although the camera types of D1 and D5 are different and D5 does not contain the important feature  X  X  X SO 1600 X . Table 7 shows the results of similar product mining in the MP3 player domain. A similar trend is observed. For example, our approach discovers that M1 and M2 are the most similar and both of them possess some important product features such as  X  X 20 GB Harddisk store X ,  X  X  X uild-in LCD Display X . The ROADRUNNER approach, however, considers that M4 is more similar to M1 although M4 contains  X  X 1 GB MB memory storage X . This is because the fact that M1 and M4 contain a number of common, but less important product features such as  X  X  X SB connector X  and  X  X  X echargeable battery X . 6. Application and evaluation: hot item feature mining 6.1. Description
Online auction Web sites have a large number of sellers and potential buyers with tremendous number of items from different categories listed for bids at any time. These sites are fast-changing, highly dynamic and complex. For example, a digital camera may receive a large number of bids ranging from few US dollars to a few hundred US dollars in just 1 or 2 days. The mutual influences of the items can be seen from the fact that an item being sold may be seriously affected if an-to digest the huge amount of continuously changing information. For example, when a seller intends to place an item for bidding, he/she is required to set a start bidding price. Some sellers may set the start bidding price with their subjective sold may be very slim, or the start bidding price being set too low and hence the return may decrease. Some other sellers may manually analyze the items currently listed for bidding and their prices before setting the start bidding price. This manual process for analyzing such a vast amount of information is, however, tedious. Besides the sellers, it is also beneficial for a potential buyer to obtain up-to-date, detailed and accurate information to assist the decision. For example, before bidding for a particular item, the potential buyer may study the description of the item, and other similar items listed for bidding.
After certain investigation, he/she can then decide on an amount of money for the bid. Owing to the highly dynamic and fast-he/she may either lose the opportunity to buy the items, or need to pay a higher cost.

Hot item feature mining from different auction Web sites aims at automatically extracting the product feature and the associated values of the items listed for bidding, and discovering the hot item features. A hot item refers to the item which has a high number of bids from the potential buyers, or the item which only has a small number of bids but is very similar to other hot items. Hot item features refer to the features that most of the hot items contain. Since the items listed for bidding in the auction sites have mutual influence, the number of bids for one item is affected by other items. For instance, it is common that one item receives a lot of bids from the potential buyers, but another similar item receives zero bids owing to the higher asking bidding price. Both of these two items actually should be regarded as hot items. Therefore, the number of bids cannot be the only judge of whether an item is hot or not.

The diversified format of the descriptions can range from regular format such as tables to unstructured free texts, making the extraction task difficult. For example, Figs. 6 and 7 depict two Web pages collected from www.ebay.com . These two Web pages are about the auction of digital cameras, but the descriptions provided by the sellers are very different in layout format.

We apply our framework to the hot item feature mining problem. The graphical model for this application is similar to the one shown in Fig. 3 . In essence, the j th text fragment is represented by  X  X to one if the underlying token is  X  X  X esolution X  and labeled with  X  X  X eature value X , and is equal to zero otherwise, we design an tion equals one if an item receiving at least one bid from the potential buyer contains the underlying hot feature. This func-tion equals zero otherwise. Inference is then conducted to extract and mine the hot item features from different Web pages. 6.2. Experimental results
We have conducted experiments on two different domains, namely, the digital camera domain and the MP3 player do-main to evaluate our approach for hot item feature mining. In each domain, we collected 50 Web pages from each of the three real-world auction Web sites, namely, www.ebay.com , www.auctions.yahoo.com , and www.ubid.com. Each Web page contains an item listed for bidding whose remaining bidding period is less than an hour. We have conducted two sets of experiments. The first set of experiment applies our framework to discover the hot item features from the Web sites. The hot item features are extracted from the collection of Web pages describing different products of the domain jointly. The second set of experiments makes use of ROADRUNNER for the extraction of the hot item features. The hot item features are independently extracted from each Web page by ROADRUNNER.

Table 8 shows the experimental results in the digital camera domain. The format of this table is similar to Table 4 . Our approach achieves average recall and precision of 75.4% and 75.6%, respectively. The performance of our approach is better than that of ROADRUNNER, which obtains average recall and precision of 11.0% and 13.7%, respectively. The reason for the less satisfactory performance of ROADRUNNER is that the layout format of the Web pages is very different and ROADRUN-
NER cannot make use of the repeated patterns in the Web pages for extraction. Table 9 shows the performance in the MP3 player domain. A similar trend is observed. Our approach achieves average recall and precision of 64.6% and 67.1%, respec-tively, whereas ROADRUNNER has average recall and precision of 17.4% and 17.2%, respectively.

Table 10 depicts some hot item features discovered in each domain. It can be observed that the hot item features discov-ered are quite useful and informative. Moreover, they can discriminate the hot items from other items that attract little bid-dings from potential buyers. For example, in both domain, potential buyers normally prefer brand new items instead of the used ones. Most of the items that receive at least one bid are brand new. Therefore, if a seller want to sell an used item, he/ the MP3 domain, it can be found that storage size of the MP3 player is a discriminative feature compared with other features.
Players with large volume of storage, such as storage over 20 GB, are more popular. Only a small amount of MP3 players with small storage can eventually be sold on the auction sites. Moreover, voice recorder, which is normally considered as an add-on feature of a MP3 player, is found to be contained in most of the hot items. Such information is very important to both potential buyers and sellers since they can make use of it to set the prices. We also manually investigate the items listed for bidding in the auction sites. About 70% of the items receiving at least one bid from the potential buyers contain more than three of the discovered hot item features. 7. Conclusions and future work
We have developed a framework which can jointly extract information and conduct mining from multiple Web pages across different sites in a unified model. Our approach allows tight interaction between the tasks of extraction and mining.
Our framework is based on the undirected graphical model called Conditional Random Fields (CRF). One characteristic of our model is that it can model the interdependence between neighbouring text fragments within a single Web page, as well as text fragments from different Web pages. This leads to an advantage that information from multiple pages can be considered in a collective manner. Another characteristic is that it allows us to conduct feature mining across different Web sites simul-taneously. Prior knowledge can be easily incorporated into our framework to guide the extraction and mining tasks. We have applied our framework in two applications, namely, important product feature mining from vendor sites, and hot item fea-ture mining from auction sites to demonstrate the efficacy of our framework. Extensive experiments on real-world Web sites have been conducted to evaluate the effectiveness of our approach.

We intend to extend our framework in several directions. One possible direction is to apply our framework to other appli-cations such as mining opinions of the important product features from customer reviews. This can help vendors understand the need of customers and hence accomplish customer relationship management. Existing works on opinion mining mainly consider the content of the customer reviews alone. However, we aim at jointly considering both Web pages from vendors and customer reviews in a coherent manner, since they are likely to be interdependent. Since customer reviews are normally written in largely grammatical free texts, natural language processing (NLP) techniques are required to achieve the task. An-other possible direction is to automatically discover the domain ontology on Web sites. Recently, semantic Web, which is viewed as the next generation of Web, has become an active research area [49]. Ontology is an essential component in semantic Web because it contains domain knowledge. Normally, an ontology is manually constructed by human experts.
We intend to employ our approach to jointly extracting the information from multiple Web pages and constructing the ontology for the major features.
 Acknowledgements The work described in this paper is substantially supported by Grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Nos.: CUHK4193/04E and CUHK4128/07) and the Direct Grant of the Faculty of
Engineering, CUHK (Project Codes: 2050363 and 2050391). This work is also affiliated with the Microsoft-CUHK Joint Lab-oratory for Human-centric Computing and Interface Technologies.

References
