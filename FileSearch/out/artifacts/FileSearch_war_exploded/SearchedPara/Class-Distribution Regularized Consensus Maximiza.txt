 In data mining applications such as crowdsourcing and privacy-preserving data mining, one may wish to obtain consol-idated predictions out of multiple models without access to features of the data. Besides, multiple models usually carry complementary predictive information, model combi-nation can potentially provide more robust and accurate predictions by correcting independent errors from individ-ual models. Various methods have been proposed to com-bine predictions such that the final predictions are maxi-mally agreed upon by multiple base models. Though this maximum consensus principle has been shown to be suc-cessful, simply maximizing consensus can lead to less dis-criminative predictions and overfit the inevitable noise due to imperfect base models. We argue that proper regulariza-tion for model combination approaches is needed to allevi-ate such overfitting e ff ect. Specifically, we analyze the hy-pothesis spaces of several model combination methods and identify the trade-o ff between model consensus and general-ization ability. We propose a novel model called R egularized C onsensus M aximization (RCM), which is formulated as an optimization problem to combine the maximum consensus and large margin principles. We theoretically show that RCM has a smaller upper bound on generalization error compared to the version without regularization. Experi-ments show that the proposed algorithm outperforms a wide spectrum of state-of-the-art model combination methods on 11 tasks.
 H.2.8 [ Database Applications ]: Data Mining Ensemble; Large Margin; Generalization Error
Combining multiple supervised and unsupervised models can be desirable and beneficial, or sometimes even a must. For example, in crowdsourcing, privacy-preserving data min-ing or big data applications, there could be only predictions from multiple models available, with raw features of the data being withheld or discarded. One has to merge the output of these models to obtain the final classification or clustering results. On the one hand, there are various new consensus-based solutions, such as those proposed in [16, 34, 30, 13, 18, 27, 37]. One common idea that these algorithms share is to learn a model that has highest prediction consensus among base models. On the other hand, simple model com-bination algorithms, such as majority voting [15], that do not pursue model consensus are portrayed as baselines infe-rior to the algorithms seeking consensus. These comparisons give the illusion that the more consensus one can achieve, the more likely the consolidated predictions will be accurate. One might ask: are the consolidated predictions that achieve maximal consensus the best choice? Could these consensus-based methods overfit the noisy and limited observed data, leading to results inconsistent with the true data distribu-tion? After all, the goal of classification/clustering is to produce discriminative predictions [4, 29].
 In this paper, we study the above questions based on the Consensus Maximization framework [16] (CM for short in the sequel), due to its generality and e ff ectiveness. We first present a running example of CM in Table 1 to demonstrate that solely maximizing the consensus can lead to undesir-able results. Suppose we have 5 instances in 2 classes, whose ground truth labels are shown in the first column of the ta-ble. There are 2 supervised models ( M 1 and M 2 )and2 unsupervised model ( M 3 and M 4 ). A supervised (resp. un-supervised) model predicts the class (resp. cluster) labels of all instances. The predictions from a model are shown under the header with the model X  X  name. Note that neither the correspondence between class labels and cluster labels, nor the correspondence between cluster labels from di ff erent clustering models is known. We describe the details of CM later and for the moment, one can think of CM as a black box that consolidates the predictions of base models and outputs predictive posteriors p ( y =1 | x )thatachievemaxi-mal consensus among base models. For majority voting (MV for short in the sequel), it simply averages the predictions from supervised models (predictions of unsupervised models cannot be used by MV because the correspondence between classes and cluster labels is unknown). The consolidated predictions produced by CM and MV are shown in the last two columns of the table.

From this running example, one can see that CM makes more correct predictions than MV does. However, the pos-teriors p ( y =1 | x )producedbyCMtendtobecloserto the decision boundary and the margins between p ( y =1 | x ) and p ( y =0 | x )arequitesmall. Wehavetwoobservations. First, according to the margin-based generalization error analysis [24], a smaller margin of posterior class distribu-tions between di ff erent classes leads to a higher empirical margin risk ,whichcontributestotheoverallgeneralization error. If one can produce consolidated predictions with a large posterior margin, a tighter upper bound on the gen-eralization error can be obtained. Second, if the hypothesis space for a model combination algorithm has large capacity (measured by VC-dimension, growth function or covering number, etc.), then the upper bound of generalization error is also higher. One may incorporate certain relevant prior knowledge of the data to shrink the size of the hypothesis space. For example, for multi-class single-label classifica-tion, desirable consensus predictions should be discrimina-tive in the sense that an instance belongs to one and only one class. Our goal is to reduce empirical margin risk and the capacity of the hypothesis space of model combination methods such as CM, and obtain a smaller upper bound on the generalization error.

We propose a family of regularization objectives over class distribution to reduce generalization errors. As a solid in-stance, we add regularization objectives to CM to obtain Regularized Consensus Maximization (RCM). In terms of al-gorithmic e ff ectiveness, though the regularization introduces many tuning parameters and makes the optimization prob-lem not jointly convex, we develop a simple yet e ffi cient ap-proximation to the regularization term without introducing additional parameters. An alternative optimization proce-dure is developed to find a local minimum with reasonably good empirical results. In terms of theoretical e ff ectiveness of learning, we give a detailed analysis of the algorithm and formally prove that, comparing to the original version, RCM achieves a smaller upper bound on generalization error. In summary, we make the following contributions:
In this section, we recapitulate some basic concepts used in the CM framework, which is followed by an analysis of why it tends to overfit the data.

CM combines the output of multiple supervised and un-supervised base models (as shown in Table 1). Let the set of instances be D = { x 1 ,..., x n } ,eachofwhichbelongs to one of c classes { 1 ,...,c } .Supposewehave m models { M 1 ,...,M m } .Withoutlossofgenerality,weassumethat the first 0  X  r  X  m models are supervised models, and the rest are unsupervised models. A supervised model predicts the class label of each instance, while an unsupervised model predicts the cluster label and partitions D into c clusters. Given the predictions of m models, without access to the data D ,CMcomputesconsolidatedpredictionstoachieve maximal consensus among base models.

We use the example in Table 1 to demonstrate how CM constructs a bipartite graph and obtains consensus predic-tions. The bipartite graph contains group nodes and in-stance nodes, where a group node represent a class or clus-ter from a supervised or unsupervised model, and an in-stance node represent a data instance. Therefore, there are c  X  m group nodes and n instance nodes. We number the group nodes such that class/cluster  X  from the j -th model is labeled as the (( j  X  1)  X  c +  X  )-th group. The bipartite graph constructed using the toy example is shown in Fig-ure 1. An instance node is connected to a group node i ff the corresponding instance is classified or clustered into the corresponding class or cluster of a model. A group node of a supervised model is also connected to an external node rep-resenting its ground truth label (the left-most nodes in the figure). As x 1 is predicted to have label 1 by M 1 ,instance node x 1 is linked to the group nodes g 1 representing class 1 of M 1 .Similarly, x 5 is linked to group node g 8 to indicate that the instance belongs to the second group ( R 1 )by M
CM consolidates these base models into a single model whose predictions achieve maximal consensus among the predictions of base models. Formally, we denote the mem-bership distribution of an instance node for x i by a row probabilistic vector u i =[ u i 1 ,...,u i c ]. Similarly, the mem-bership distribution of a group node is given by a row proba-tively denoted by two matrices: U =[ u 1  X  ,..., u n  X  ]  X  Q =[ q 1  X  ,..., q v  X  ]  X  where v = c  X  m .CMseekssmooth probability distributions U and Q ,suchthat any connected nodes in the graph have similar probabilistic distributions. CM solves the following optimization problem: where a i j =1ifthe i -th instance node is connected to the j -th group nodes, and is equal to 0 otherwise. b j indicates if the j -th group node is from a classification model ( b j or a clustering model ( b j =0).  X y j is a vector indicating the class label of the j -th group node from a classification model. For example,  X y 1 =[1 , 0] and  X y 2 =[0 , 1] in the above running example.  X y j is an all zero vector if the corresponding group node is from a clustering model.

We denote the objective by L ( U, Q )andisdefinedas con-sensus loss .Itmeasuresthelevelofdisagreementbetween the consolidated predictions and the outputs of base mod-els. By minimizing L ( U, Q ), consensus among base models is maximized. The second term accounts for the initial pre-dictions of supervised models, and does not play a role if all base models are unsupervised. The whole optimization is solved via block coordinate descent: Here A =( a i j ) n  X  v , D v =diag( A  X   X  n )and D n =diag( A  X  ), K v =diag(  X  Y  X  c ). Here k is an all one column vector of length k .Uponconvergence,thefinalposterior distributions are given in the rows of U and one can use Bayes X  optimal decision rule to decide the most likely label.
The hypothesis space of a learning algorithm is the set of all feasible solutions of the algorithm. A larger hypothe-sis space has more expressive power comparing to a smaller one, leading to less training errors. However, models with alargerhypothesisspaceismorecomplicatedandcanlead to less generalization ability and more predicting errors [29]. Therefore, one needs to trade-o ff between minimizing train-ing error and model complexity. We compare the hypothe-sis spaces of two model combination methods, MV and CM, leading to some insights into the overfitting issue of CM.
Suppose there are m base models (for ease of presentation, assume all models are supervised models). For each instance and each class, a model outputs 1 (a vote) or 0 (no vote). Amodelcombinationmethodisafunction f that maps predictions of base models to posterior distributions on the probabilistic simplex:
S = { p  X  R  X  | p = where we abuse the notation X ,suchthat X is the collection of base model predictions. e  X  is the standard basis having 1inits  X  -th position and 0 anywhere else, representing the distribution of class  X  ,  X   X  is the probability that an instance belongs to class  X  .When c =3,anexampleof2-simplexis shown in Figure 2(a). Various model combination methods can be seen as ways of searching a suitable mapping f in the hypothesis space F of all such mappings, to optimize certain objectives. Existing methods di ff er in their hypoth-esis spaces F and the way they searches, but the capacity of the hypothesis space is directly related to the generaliza-tion ability of a method. Note that the domain of all model combination methods are the same, so the capacities of their hypothesis spaces are completely determined by the images of the maps f ( X )  X  S .

Majority voting simply sums up the number of votes for each class and assigns an instance to the class having the most votes. Formally, given the output of r models for majority voting is made based on the the vector: Note that majority voting maps the predictions of base mod-els to rational vectors on the simplex, with denominators equal to the number of models. For example, if an instance receives two votes for class 1, one votes for class 2 and 0 vote for class 3, from a total of three classifiers, then the output of f is [2 / 3 , 1 / 3 , 0], shown as the square with an arrow in Figure 2(b).

CM maps predictions of base models of an instance to aposteriordistributionin S ,andtheimageofthemapis the whole simplex S .Therelaxationfromrationalvectors to real vectors allows a larger hypothesis space such that CM can find an f to attain low consensus loss (see Sec-tion 5.2). However, it also allows CM to pick an f that outputs predictions close to uniform distribution with small margin (like the diamond in Figure 2(c)), leading to higher empirical margin risk (see Section 4) It is verified in Sec-tion 5 that CM does tend to output predictions that have small consensus losses and small margins. Here we define  X  X verfitting X  in model combination in a vague sense, and de-fer the formal analysis to Section 4.
 Definition 1 (Overfitting in model combination). Amodelcombinationmethodconsolidatespredictionsofbase models to achieve a high degree of model consensus but with higher generalization error upper bound. (c) Hypothesis space of CM (d) Hypothesis space of
According to the above analysis, if we adopt a reason-ably small but rich enough hypothesis space for CM, then we could avoid over-fitting and achieve better performance. How can we specify a suitable hypothesis space for CM? Note that the predictions lying near the corners of the sim-plex (shadows in Figure 2(d)) have a more dominating com-ponent  X   X  0 for some class  X  0 .Ontheonehand,whenthe di ff erence between p ( y =  X  0 | x )andanyother p ( y =  X  | x )is larger, the prediction is more discriminative to reflect the true class distribution. On the other hand, if the number of dominating entries in p ( y | x )isgreaterthan1,thenthose dominating classes are correlated since they co-occur, con-flicting the multi-class distribution assumption. This ob-servation indicates that when searching for solutions in the hypothesis space, CM should penalize solutions that lie too far away from any corner of the simplex and encourage so-lutions that lie close to the corners. For CM to reduce the penalized consensus loss L ( U, Q ), it must move its predic-tions towards one of the corners on the simplex, as shown by the arrows in Figure 2(d). The above intuition suggests that the consolidated predictions should exhibit some sort of independence between classes, given the problem is a multi-class single label problem.

Specifically, recall that U is the consolidated prediction, with the  X   X  th column being the posterior probabilities p ( y =  X  | x ), we can compute the empirical class correlation matrix  X  = U  X  U .Wewantthematrix  X  tobeclosetoa c  X  c matrix D ,whichrepresentstheidealclasscorrelations. Forexam-ple, to enforce independence between classes in multi-class classification problems, we can set the diagonal elements of D to a positive number whose scale is comparable to the empirical correlations, and set the o ff -diagonal elements to apositivenumbermuchsmallerthanthediagonalelements.
By adopting the Frobenius norm, we obtain the following regularization term or by adopting the relative entropy [5]
Adding any of the above regularization terms to the objec-tive of CM, we obtain the following optimization problem: min s.t. u i  X   X  0 ,  X  u i  X  1 =1 ,i =1 ,...,n where  X  =  X  F or  X  E .Theparameter  X  controls the trade-o ff between model consensus and class independence. We will see that the regularization helps reduce the capacity of hypothesis space and also the empirical margin risk.
Our plan for solving the optimization problem Eq.(7) is to first ignore the constraints that u i and q j are probabil-ity distributions and solve the unconstrained optimization problem using gradient descent, then we address the prob-abilistic constraints on u i and q j in the next section. The gradient descent steps for the first two terms in the above objective function are given in Eq.(1) and Eq.(2), the gra-dients of the regularization term  X  with respect to column u j are as follows: Thus a gradient descent step for the regularization term with respect to column u j are: where  X  indicates the assignment of an updated u j to itself.  X  is the learning rate in the t -th iteration with  X  t =  X  0 and  X  0 is the initial learning rate. We let the trade-o ff pa-rameter  X  in the RCM objective be absorbed in  X  0 .Eq.(7) and Eq.(8) have a quite intuitive meaning: for each column u i representing the i -th class, depending on whether the empirical class correlation  X  ij exceeds the ideal class corre-lation D ij , u j is moved away from (  X  ij &gt;D ij )ortowards (  X  ij &lt;D ij ) u i ,andtheamountofdisplacementispro-portional to the distance between the empirical and ideal class correlation. In practice, it is not easy to specify the ideal class correlation matrix D ,andthescalingparameters  X  ij =  X  ij  X  D ij (or 1 + log choice of D .Simplysettingallthe  X  ij to be 1 will actually hurt the performance, as we ignore the information about the class correlations.

We propose an approximation of Eq.(7) and Eq.(8) to avoid specifying the parameters D and to maintain the e ff ect of the regularization, namely, a large margin between class distributions. Note that in Eq.(8), for i  X  = j , D ij should be some small number and if  X  ij  X  D ij ,thescalingparameter the same as D ij ,1+log  X  ij D to this observation, when computing the gradient for the column u j ,wecanset  X  ij as follows: The resulting regularization term is where Eq.(7) and Eq.(8) become So far we have specified all necessary gradient descent steps for RCM. Nonetheless, the original CM gradient descent steps involve the rows of the matrices U and Q ,whileto minimize the regularization term  X  , one has to work with the columns of U .Itisnon-trivialtoderivegradientdescent steps involving both rows and columns of a matrix. We adopt an alternative optimization procedure that first mini-mizes the consensus loss L ( U, Q )throughEq.(1)andEq.(2), then minimizes  X  A through Eq.(12). These two steps are alternatively repeated until it converges.
The converted unconstrained optimization problem ignores the constraints: Although Eq.(1) and (2) maintain rows of U and Q as proba-bility distributions, Eq.(12) might bring any entry of U to be greater than 1 or less than 0, and a row in U or Q might not sum up to 1. We propose to perform probabilistic projection for all u i after all gradient descent steps in each iteration. More formally, the following optimization problem finds v , the projection of u i onto the probabilistic simplex The optimal solution v  X  serves as the new u i for the next iteration, with the probabilistic constraints satisfied. An ef-ficient algorithm (in O ( cn )) with implementation to solve the above problem can be found in [12]. The complete algo-rithm is described in Algorithm 1.
 Algorithm 1 Regularized Consensus Maximization (RCM) 3: for t =1  X  MaxIterNum do 7: for j =1  X  c do 10: end for 12: end for
In this section, we prove that, compared to CM, the pro-posed regularization leads to a smaller upper bound on gen-eralization error. The generalization error bound consists of two terms: the empirical margin risk on training data and a term measuring the capacity of the hypothesis space explored by a learning algorithm. Regarding the empirical margin risk, we first define the multi-class margin [24].
Definition 2 (Canonical Function). Given a func-tion f  X  F that maps predictions of base models to pos-terior distribution (see Section 2.2). For the instance x , ity that x belongs to class  X  ,accordingto f .Let M 1 be the smallest index  X  such that f  X  ( x )=max k f k ( x ) ,and M the smallest index  X  such that f  X  ( x )=max k  X  = M 1 f k canonical function  X  f : X  X  [  X  1 , 1] c ,withthe  X  -th compo-nent being: M 1 is the label selected by Bayes decision rule and M 2 is the closest runner-up.  X  f  X  measures how far away the selected label is from the other competitors. Based on the canonical function, we define the multi-class empirical margin risk
Definition 3 (Empirical Margin Risk). For  X  &gt; 0 and training set s = { x i , y i } m i =1 ,theempiricalmarginrisk R ( f ) of the function f is
R  X  s ( f )= 1 where y i  X  is the  X  -th component of the true label vector y
Next we define necessary concepts to measure the capacity of hypothesis spaces.

Definition 4 (Supremum Metric for functions). [24, 3] Suppose F is the collection of functions mapping from X to S ,and s = { x i } m i =1  X  X is a given set of in-stances. Define the metric (distance measure) for functions d (  X  ,  X  ): F  X  F  X  [0 , +  X  ) on s by Note that the metric such defined depends on the set of instances s .

Definition 5 (Covering number). Let ( F ,d s ) be the space of functions equipped with the supremum metric, where s  X  X afinitesetofinstances. Define B s ( f, r ) the closed ball centered at f with radius r : The covering number N (  X  , H ,d s ) of a set H  X  F is defined as The set T is called an  X  -cover of the subset H . The following bound on generalization error for multi-class classification is given in [24]:
Theorem 1. Let F be a set of functions from X to S and  X  F be the set of canonical functions  X  f .Let s be a learning set of size m drawn iid. from a probability distribution P . Let 0 &lt;  X  &lt; 1 .Withprobability 1  X   X  , 0 f  X  F , where  X  F  X  = {  X   X   X   X  f :  X  f  X   X  F} where  X   X  is the truncation function applied to each of the c components of  X  f Given the bound in Eq.(19), we want to prove that both terms in the bound for the regularized CM are smaller than those for the original CM, and obtain the following theorem:
Theorem 2. RCM has a smaller upper bound on gener-alization error compared with that of CM.
 The above theorem is proved in two steps in the following two lemmas.

Lemma 1. RCM achieves a lower empirical margin risk if we use  X  E as our regularization term and the matrix D is such set that the scaling parameters  X  ij =  X  ji and  X  ii
Proof. Given training data s ,0 &lt;  X  &lt; 1, 1  X  R  X  s ( f )is the proportion of correctly cl assified instances with margin greater than  X  .Suppose f is the prediction function found by CM and  X  f is that found by RCM. In other words,  X  f is obtained by applying Eq.(8) to f .Notethat R  X  s (  X  f )  X  R ( f )  X  X  X  1  X  R  X  s (  X  f )  X  1  X  R  X  s ( f ), we need to prove, for any correctly classified instance with margin greater than  X  , its margin under  X  f is not smaller than that under f . of f and  X  f at some point x that is correctly classified with margin larger than  X  (we ignore the arguments of f and  X  f ). Assume 1 = argmax  X  f  X  and 2 = argmax  X   X  =1 f  X  .Then y  X   X  f 1  X   X  .But y 1 =1,so  X  f 1 = f 1  X  f 2  X   X  .The gradients Eq.(8) at x be Assume that proper values are set to matrix D ,suchthat  X  ii = D ii but  X  ij  X  D ij for i  X  = j .Thenthegradientsare where  X  ii  X   X  ij ,i  X  = j .Thatis,foragiven j , f i has a much larger weight than f j in g j for i  X  = j .If  X  ij =  X  ji f &gt;f 2 ,wehave g 2 &gt;g 1 ,  X   X  f 1 =  X  f 1  X   X  f 2 =( f 1  X  g 1 )  X  ( f 2  X  g 2 )=  X  f
Lemma 2. The hypothesis space of RCM has smaller cov-ering number than the hypothesis space of CM.

Proof. Let  X  F  X  = {  X   X   X   X  f :  X  f  X   X  F} and  X   X  F  X  = {  X   X   X   X   X  f :  X  f  X   X   X  F} where F is the collection of functions f : X  X  S and  X  F are their large margin version as defined in Lemma 1,  X  f is the canonical function and  X   X  is the truncation function Eq.(21). Then  X   X  F  X   X   X  F since for any f  X  F ,itslargemarginversion  X  f  X  F ,thuswehave  X   X  F  X   X  F .Aftertruncation,  X   X  F  X   X   X  F  X  .

Given any training data s of size 2 m ,any  X  / 2-cover of  X  F  X  is also a  X  / 2-cover of  X   X  F  X  .Thereforebydefinition Eq.(18),
N (  X  / 2 ,  X  F  X  ,s )=inf {| T |}  X  inf {| T  X  |} = N (  X  / 2 ,  X  where T  X  {  X  / 2-covers of  X  F  X  } and T  X   X  {  X  / 2-covers of  X  By the definition Eq.(20), we conclude that
In this section, we first summarize the experimental set-tings, including evaluation benchmarks and model combina-tion baselines. Then we demonstrate how CM overfits the data and how the proposed RCM resolves the issue.
Benchmarks Amodelconsolidationmethodconsol-idates the predictions of multiple supervised and/or unsu-pervised models to come up with improved predictive per-formance. Therefore, to evaluate the performance, we need the predictions from multiple base models for the datasets, whose information are summarized in Table 3. The dataset 1 contains 11 text classification tasks. Each task contains the predictions given by the output of 2 classification and 2 clus-tering models. For details of how they processed the data, please refer to [16].

We compare RCM with CM in order to verify the e ff ec-tiveness of the large margin constraint. CM and RCM share most of the parameters such as number of iterations, impor-tance of supervised models, etc.. For the shared parameters, we adopt the parameter settings of CM [16]. In addition, we set the initial learning rate  X  0 to be 0.1. We also compare RCM with other state-of-the-art cluster ensemble methods: available at http://www.cse.buffalo.edu/~jing/ MCLA [27], HBGF [27], SNNMF [18], BCE [30] ECMC [37]. MCLA and HBGF are graph partition based approaches, which use spectral clustering [23, 11] to partition the bipar-tite or hyper graph constructed from the predictions of base models. There is no parameter to tune for these two meth-ods. SNNMF is a matrix factorization based method, which derives clustering of instances using the similarity matrix constructed from base models X  predictions. We run SNNMF to its convergence to obtain the final predictions. BCE is a Bayesian approach to consensus maximization. We set its parameters as follows: LDA parameters  X  =0 . 5 ,  X  =0 . 1, number of iterations for Gibbs sampling is set to 50,000, the topic distributions of the words in documents are randomly initialized. We observe that performance the Gibbs sam-pling for BCE is sensitive to the initialization of the param-eters and unstable, we run the BCE for 10 times and report its best performance. We also implemented BCE using vari-ational inference, but the procedure did not converge after long runs, so we do not report the corresponding results. ECMC is a matrix factorization method with a de-noising step, we adopt the implementations of robust PCA and ma-trix completion packages 2 ,with d 0 =0 . 4 ,d 1 =0 . 6andother parameters being the default values (see [37] for details).
In Section 2.1 and 2.2 we theoretically showed that, CM produces predictions that minimize the consensus loss but overfit the data, and therefore might not generalize well, and in Section 3.1, we proposed RCM to solve the issues. By comparing CM and RCM in consensus loss, prediction margins and accuracy (next section), we verify that CM does have the overfitting issue and RCM can e ff ectively mitigate overfitting.

On the one hand, one can see from Figure 3 that CM has alowerconsensuslossthanRCMdoesacrossalldatasets. http://perception.csl.illinois.edu/matrix-rank/ sample_code.html This is because CM solely minimizes the consensus loss while RCM minimizes a regularized consensus loss and has a smaller hypothesis space. On the other hand, we use entropy of u i ( h the higher the entropy, the smaller the margin u i has and the less discriminative u i is. We show the averaged entropy the figures, we can see that the entropy is higher in the pre-dictions of CM across all datasets except on the dblp dataset. (the result on the cora1 dataset is not shown due to the scale). Therefore on average, the predictions of CM have smaller margins than those of RCM. Since margin is used as an indicator of generalization performance of a learning algorithm [4], CM might overfit the data while RCM should improve the generalization ability and accuracy of CM.
In Table 4, we compare the accuracies of RCM and the baselines on 11 text classification tasks. From the table, we can see that BCE is very unstable and there are two main reasons for this. First, similar to LDA, BCE needs a lot of observed data to infer the consolidated labels, yet usu-ally we have only a couple of base models. Second, Gibbs sampling is too sensitive to initial conditions while varia-tional inference does not converge given only a handful of data. ECMC and SNNMF sometimes give reasonable per-formance, such as ECMC on the cora4 task. However, their optimization are also sensitive to initialization, and their so-lutions are unstable. Both MCLA and HBGF in general have better performance than ECMC and SNNMF, though they are still outperformed by both CM and RCM.

The comparison between CM and RCM is more interest-ing. Using the proposed regularization over the class distri-butions, RCM controls the size of its hypothesis space and focuses on the more discriminative predictions. As we can see from the table, RCM outperforms CM on 10 out of 11 datasets. These evidences, together with the comparisons of consensus loss and entropy in Section 5.2, clearly demon-strate that CM overfits the data to produce highly consensus predictions, while RCM is able to trade-o ff between two ob-jectives and achieves better accuracy.

Statistical significance of the results We verify that the improvements brought by the proposed method is sta-tistically significant. According to [10], one can compare the e ff ectiveness of di ff erent algorithms based on their per-formance on multiple datasets. Since among all baselines, CM has the closest performance to RCM, we compare these two methods using the Wilcoxon signed-ranks test. For the details of how to carry out the test, please refer to [10]. The test shows that RCM is statistically significantly superior to CM with  X  =0 . 05, where  X  is the probability that RCM is not better than CM.
For each of the text classification tasks, we record the ac-curacy at the end of each iteration of RCM. In Figure 5, we plot the accuracies against the number of iterations. From the figure, we can see that, except for the news3 and dblp tasks, RCM converges to some fixed accuracies. Even for those two exceptions where there are some zigzag X  X  at the tails of the curves, we notice that the lowest accuracies ob-tained after the 25th iteration are at least the same as the best baseline (CM in both cases). Therefore, we conclude that given a big enough number of iterations, the algorithm performs better than or comparable with the baselines.
Graph partition based methods [27, 14]. In the pio-neering work of [27], they presented three methods for clus-ter ensemble. For example, HGPA in [27] constructs a hy-pergraph consisting of membership indicators from cluster-ing models as hyperedges, then a hypergraph partition algo-rithm partitions the hypergraph. Another method in [27], MCLA, partitions the hypergraph into k subgraphs, each of which consists of membership indicators from clustering models. These subgraphs are then used to calculate the as-sociation strength of an instance with subgraphs. HBGF is proposed in [14]. HBGF constructs the same bipartite graph as that in [16], then it partitions the graph using spectral clustering or METIS. Since the graph contains both group nodes and instance nodes (See Section 2.1), the results is a partition of both types of nodes. The partition of instance nodes is taken as the aggregated clustering. All these meth-ods do not take discriminative constraints into account in their optimization formula.

Matrix factorization methods [18, 19]. They solve the consensus clustering problem through symmetric non-negative matrix factorization (SNNMF). Using the predic-tions of base models, a similarity matrix is derived and factorized into orthonormal cluster membership indicators. The indicator matrices play the role of the consolidated pre-dictions in CM. The orthonormality constraint on the in-dicators acts as the large margin regularization. Note that the orthonormality and the non-negative constraint are more restrictive than the large margin regularization proposed in this paper, as the consolidated predictions can have only one entry as 1, with all the other entries being 0. We com-pare the proposed algorithm with the SNNMF formulation in Section 5.

Probabilistic methods [30, 1, 32, 20]. In [30], they solve the consensus clustering problem using a LDA like model. Predictions from base models are treated as documents in LDA, and the consolidated predictions as the latent topics of the documents. The method di ff ers from LDA in that di ff erent models are assumed to have di ff erent topic-word distributions. In [1], they extend the above method in order to combine both supervised and unsupervised predictions in a transfer learning setting, which is di ff erent from the problem we are addressing here. In [32], they propose a non-parametric Bayesian method to select the number of clusters in consensus clustering. This algorithm is best employed as a parameter selection step before applying the method proposed in the paper.

In [2], they propose to combine supervised and unsuper-vised models as a way of knowledge transfer, where unsu-pervised models in the target domain serve as additional constraints while supervised models provide initial labeling. It might be important to give di ff erent weights to di ff erent base models, which might have di ff erent importance for the clustering task. In [34], they improve the performance of CM via functional space sampling. They impose weights on base models, where the weights are learned iteratively while seeking consensus results. In [18], weighted consensus ensemble is formulated as a sparse learning problem where the most important base models can be selected for con-solidation. In [13], they provide a general framework for cluster ensemble called  X  X eneralized Weighted Cluster Ag-gregation X . Their goal is to find an a ffi nity matrix that is close to all other a ffi nity matrices derived from predictions of base models. Note that the proposed large margin formu-lation can be adopted by the above algorithms as a building block, and thus is not directly comparable.
 Combining structural predictions has also been studied. For example, in multilabel classification, label correlations provide important information to achieve better classifica-tion performance. In [35], they propose a novel consensus classification algorithm to combine multi-label predictions from multiple base models, taking both label correlation and model consensus into account. Learning to rank is an impor-tant research problem in information retrieval, and recently, aggregating multiple ranking results is attracting more and more attention, due to its potential to improve ranking per-formance over single ranking model. The oldest ranking aggregation method called  X  X orda Couting X , which can be traced back to 1770. The modern statistical ranking ag-gregation started with the Bradley-Terry (BT) model [7, 6], which infers the underlying ranking via maximum likeli-hood estimation. There are also many extensions of the BT model. For example, in [9], they extended the BT model by adding and learning weights on the base ranking models. In [33], they propose an online Bayesian learning algorithm for the BT model. In [25], they study the theoretical aspect of combining multiple ranking results into one ranking list. They present conditions under which certain popular rank-ing aggregation algorithms converge. Aggregating multiple ranking results has also found its place in gaming, such as XBox platform [17] In [31], they propose a Bayesian model to aggregate multiple visual trackers X  output for reliable vi-sual tracking. The proposed method in the paper focuses on combining flat predictions instead of structural predictions.
This work is also related to crowdsourcing, which aims at design mechanisms and algorithms to collect useful informa-tion from massive human population. Aggregating the data collected from multiple human beings is similar to combining the predictions given by multiple base models considered in this paper. In [26], they use crowdsourcing to obtain cheap annotations for NLP tasks, such as a ff ect recognition, word similarity, etc. In [22], they propose to carry out multiple related crowdsourcing tasks simultaneously to alleviate data sparsity for a single crowdsourcing task. In [36], they pro-pose to actively select annotator-instance pairs for human labeling. The idea is that by identifying the most uncertain instance and the corresponding most reliable annotator for the instance, one can learn underlying labels of the instances more e ff ectively.

The algorithm proposed here is motivated by the maxi-mum margin principle widely adopted in previous works [38, 28, 39]. In these works, to encourage discriminability, mod-els are trained with the constraint that the prediction of a la-beled instance should be closer to its true label than to other labels by some distance. However, these works focus on su-pervised learning, which is quite a di ff erent setting from the unsupervised setting here. There are a few works addressing overfitting in clustering [8, 21]. In [8], they analyze the issue from a learning theory perspective, and propose a general algorithm called  X  X earest neighbor clustering X  to restrict the hypothesis space and avoid overfitting. The algorithm learns the similarity matrix for better spectral clustering results, and might be used to construct the a ffi nity matrix in CM. These algorithms focus on clustering and do not address the model combination problem directly.
In this paper, we consider the overfitting issue in consen-sus maximization for model combination. The problem is analyzed by inspecting the hypothesis space and margin-based generalization error of CM. To solve the problem, we develop a model called class-distribution regularized CM that trades o ff two objectives, namely, consensus among base models and margins in predictions. The resulting optimiza-tion problem is challenging to solve since it involves many regularization parameters and is not jointly convex. We propose a simple and e ffi cient approximation of the origi-nal problem, which can be solved using gradient descent. In the experiments, we compared the proposed method with CM and other baselines on 11 datasets, demonstrating the improvement due to the large margin regularization. Acknowledgements
This work is supported in part by China 973 Fundamen-tal R&amp;D Program (No.2014CB340304), NSF grants (CNS-1115234, DBI-0960443, OISE-1129076, and IIS-1319973) and Huawei grant. [1] Acharya Ayan, Hruschka Eduardo, R., Ghosh Joydeep, [2] Acharya Ayan, R. Hruschka Eduardo, Ghosh Joydeep, [3] P. L. Bartlett. The sample complexity of pattern [4] Asa Ben-Hur, David Horn, Hava T. Siegelmann, and [5] Stephen Boyd and Lieven Vandenberghe. Convex [6] Ralph Allan Bradley. Rank analysis of incomplete [7] Ralph Allan Bradley and Milton E. Terry. Rank [8] S  X  ebastien Bubeck and Ulrike von Luxburg. Nearest [9] Xi Chen, Paul N. Bennett, Kevyn Collins-Thompson, [10] Janez Dem X  sar. Statistical comparisons of classifiers [11] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. [12] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and [13] Wang Fei, Wang Xin, and Li Tao. Generalized cluster [14] Xiaoli Zhang Fern and Carla E. Brodley. Solving [15] Jing Gao, Wei Fan, Deepak Turaga, Olivier [16] Jing Gao, Feng Liang, Wei Fan, Yizhou Sun, and [17] Ralf Herbrich, Tom Minka, and Thore Graepel. [18] Tao Li and Chris Ding. Weighted Consensus [19] Tao Li, Chris Ding, and Michael I. Jordan. Solving [20] Xudong Ma, Ping Luo, Fuzhen Zhuang, Qing He, [21] Meila Marina and Shortreed Susan. Regularized [22] Kaixiang Mo, Erheng Zhong, and Qiang Yang.
 [23] Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On [24] H. Paugam-Moisy, A. Elissee ff , and Y. Guermeur. [25] Arun Rajkumar and Shivani Agarwal. A statistical [26] Rion Snow, Brendan O X  X onnor, Daniel Jurafsky, and [27] Alexander Strehl and Joydeep Ghosh. Cluster [28] Ben Taskar, Carlos Guestrin, and Daphne Koller. [29] Vladimir Vapnik. Statistical learning theory .Wiley, [30] Hongjun Wang, Hanhuai Shan, and Arindam [31] Naiyan Wang and Dit-Yan Yeung. Ensemble-based [32] Pu Wang, Carlotta Domeniconi, and [33] Ruby C. Weng and Chih-Jen Lin. A bayesian [34] Sihong Xie, Wei Fan, and Philip S. Yu. An iterative [35] Sihong Xie, Xiangnan Kong, Jing Gao, Wei Fan, and [36] Yan Yan, Romer Rosales, Glenn Fung, and Jennifer [37] Jinfeng Yi, Tianbao Yang, Rong Jin, A.K. Jain, and [38] Yi Zhang and Je ff Schneider. Maximum margin [39] Jun Zhu, Amr Ahmed, and Eric P. Xing. Medlda:
