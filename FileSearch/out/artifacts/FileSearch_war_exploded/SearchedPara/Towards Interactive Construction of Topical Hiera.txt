 Automatic construction of user-desired topical hierarchies over large volumes of text data is a highly desirable but chal-lenging task. This study proposes to give users freedom to construct topical hierarchies via interactive operations such as expanding a branch and merging several branches. Exist-ing hierarchical topic modeling techniques are inadequate for this purpose because (1) they cannot consistently preserve the topics when the hierarchy structure is modified; and (2) the slow inference prevents swift response to user requests. In this study, we propose a novel method, called STROD, that allows efficient and consistent modification of topic hi-erarchies, based on a recursive generative model and a scal-able tensor decomposition inference algorithm with theoreti-cal performance guarantee. Empirical evaluation shows that STROD reduces the runtime of construction by several or-ders of magnitude, while generating consistent and quality hierarchies.
 I.7 [ Computing Methodologies ]: Document and Text Processing; H.2.8 [ Database Applications ]: Data Mining Topic Modeling, Ontology Learning, Interactive Data Ex-ploration, Tensor Decomposition
Constructing a topic hierarchy for large text collection, such as business documents, news articles, social media mes-sages, and research publications, is helpful for information workers, data analysts and researchers to summarize and navigate them in multiple granularity efficiently. While ex-isting hierarchical topic models can be used to produce such hierarchies as an exploration tool, they still require human curation ( e.g. , modify the structure and label the topics) to meet the quality requirement for reliable exploitation. The c  X  manual work for curation is very expensive. This work fo-cuses on helping with the structure modification task.
The nature of this task is interactive and iterative. On one hand, people use a topic model to explore a dataset when the topics are unknown a priori . Thus it is hard to determine the best shape of the hierarchy upfront. On the other hand, as they see the results (inferred topics even with imperfect structure), people have ideas about a more desirable struc-ture, e.g. , one topic should be expanded, or multiple topics should be merged. Then they may want to modify part of the hierarchy but preserve other parts that already look good to be labeled. Some modification, such as expanding a topic, is again exploratory and needs help from the ma-chine. It takes multiple iterations of human investigation and algorithm run to finish the construction.

To enable interactive construction of the topic hierarchy, i.e. , allowing users to modify the structure on the go, the sys-tem needs to satisfy two conditions: efficiency and consis-tency . Efficiency is necessary for users to see results quickly and react before they lose the context. Consistency is neces-sary for confusion-free modification, and has two-fold mean-ings: when people want to modify certain parts of the hi-erarchy, the remaining parts should be preserved after each run (single-run consistency); and a system should output un-differentiated results given identical input in multiple runs (multi-run consistency).
 Limitation of prior work. Most existing hierarchical topic modeling techniques [10, 17, 20, 14, 2] are based on the extensions of latent Dirichlet allocation (LDA), and are not designed for interactive construction of the hierarchy. First, the inference algorithms for these models are expen-sive, demanding hundreds or thousands of passes of data. Second, an inference algorithm generates one hierarchy for one corpus in one run of the algorithm. Running the in-ference algorithm with a slightly modified hierarchy struc-ture does not guarantee preservation of topics in untouched branches. Rerunning the inference algorithm with the same input may result in very different results. Therefore, both single-run and multi-run consistency conditions are violated if we use them for interaction.
 Our solution. We consider a strategy of top-down, pro-gressive construction of a topical hierarchy, instead of in-ferring a complex hierarchical model all at once. Thus the construction can be done via a series of interactive opera-tions, such as expanding a topic, collapsing a topic, merging topics and removing topics. Efficient and consistent algo-rithms can then be designed for each operation. Users can see the results after each operation, and decide what opera-tion to take next. This strategy has several benefits: users can easily control the complexity of the hierarchy; users can see intermediate results and curate the hierarchy in early stages; and it is easier for curators to focus on one simple operation a time.

To support these interactive operations in an efficient and consistent manner, we resort to moment-based inference. Simply put, moment-based inference compresses the original data by collecting important statistics from the documents, e.g. , term co-occurrences, and uses these statistics to infer topics. For one advantage, the inference based on the com-pressed information avoids the expensive, numerous passes of the data. For another advantage, the compression reduces randomness in the data by aggregation. With careful choice of the statistics and the inference method, we can uncover the topics with theoretical guarantee. Modifications to the hierarchy can be supported by manipulating the moments.
We establish a top-down hierarchy construction frame-work STROD based on these ideas. To the best of our knowledge, it is the first framework towards interactive top-ical hierarchy construction. The following summarizes our main contributions:  X  We propose a new hierarchical topic model such that the modification operations mentioned above can be achieved by several atomic operators to the model.  X  We develop a scalable tensor-based recursive orthogonal decomposition (STROD) method for efficient and consis-tent construction.  X  Our experiments demonstrate that our method is several orders of magnitude more efficient than the alternatives, while generating consistent, quality topic hierarchy that is comprehensible to users.
Statistical topic modeling techniques model a document as a mixture of multiple topics, while every topic is mod-eled as a distribution over terms. Two important models are probabilistic latent semantic analysis (PLSA) [13] and its Bayesian extension latent Dirichlet allocation (LDA) [5]. They model the generative processes of each term from each document in a corpus, and then infer the unknown distribu-tions that best explain the observed documents.
 Hierarchical topic models follow the same generative spirit. Instead of having a pool of flat topics, these models assume an internal hierarchical structure of the topics. Different models use different generative processes to simulate this hierarchical structure, such as nested Chinese Restaurant Process [10], Pachinko Allocation [17], hierarchical Pachinko Allocation [20], recursive Chinese Restaurant Process [14], and nested Chinese Restaurant Franchise [2]. When these models are applied to constructing a topical hierarchy, the entire hierarchy is inferred all at once from the corpus.
The main inference methods for these topic models can be divided into two categories: MCMC sampling [11] and variational inference [5]. They are essentially approxima-tion of the Maximum Likelihood (ML) principle (including its Bayesian version maximum a posterior ): Find the best parameters that maximize the joint probability specified by a model. There has been a substantial amount of work on speeding up LDA inference, e.g. , by leveraging sparsity [22, 30, 16] and parallelization [21, 24, 31], or online learning mechanism [1, 12, 8]. Few of these ideas have been adopted by the hierarchical topic model studies.

These inference methods have no theoretical guarantee of convergence within a bounded number of iterations, and are nondeterministic either due to the sampling or the random initialization. Recently, a new inference method for LDA has been proposed based on a method of moments , rather than ML. It is found to have provable error bound and con-vergence properties in theory [3].

All of the hierarchical topic models follow the bag-of-words assumption, while some other extensions of LDA have been developed to model sequential n-grams to achieve bet-ter interpretability [26, 29, 18]. No one has integrated them in a hierarchical topic model. The efficiency and consistency issues will become more challenging in an integrated model. A practical approach is to decouple the topic modeling part and the phrase mining part. Blei and Lafferty [4] have pro-posed to use a statistical test to find topical phrases, which is time-consuming. A much less expensive heuristic is studied in recent work [6] and shown to be effective.

There are a few alternative approaches to constructing a topical hierarchy. Pujara and Skomoroch [23] proposed to first run LDA on the entire corpus, and then split the corpus heuristically according to the results and run LDA on each split corpus individually. CATHY [28] is a recursive topical phrase mining framework for short, content-representative text. It also decouples phrase mining and topic discovery for efficiency purpose. Though it is not designed for generic text, it bears some similarity with this work such as top-down recursion and compression of documents.

After the hierarchy is constructed from a corpus, people can label these topics and derive topic distributions for each document [25]. Those are not the subject of this paper. Broadly speaking, this work is also related to: hierarchical clustering of documents [9], queries [19], keywords [28] etc. ; and ontology learning [15], which mines subsumption ( X  X s-a X ) relationships from text.
Given a corpus, our goal is to construct a user-desired topical hierarchy, i.e. , a tree of topics, where each child topic is about a more specific theme within the parent topic.
For easy interaction, the topics need to be visualized in user-friendly forms. Unigrams are often ambiguous, espe-cially across fine-grained topics [27]. We choose to enhance the topic representation with ranked phrases. The rank-ing should reflect both their popularity and discriminating power for a topic. For example, the top ranked phrases for the database topic can be:  X  X atabase systems X ,  X  X uery pro-cessing X ,  X  X oncurrency control X  . . . . A phrase can appear in multiple topics, though it will have various ranks in them. Formally, the input data are a corpus of D documents. The d -th document can be segmented into a sequence of l tokens. All the unique tokens in this corpus are indexed using a vocabulary of V terms. And w d,j  X  [ V ] ,j = 1 ,...,l represents the index of the j -th token in document d . A topic t is defined by a probability distribution over terms  X   X 
V  X  1 , and an ordered list of phrases P t = { P t, 1 ,P t, 2 where P t,i is the phrase ranked at i -th position for topic t .
A topical hierarchy is defined as a tree T in which each node is a topic. Every non-leaf topic t has C t child topics. We assume C t is bounded by a small number K , such as 10, because the topical hierarchy is intended for human to efficiently browse the subtopics for each topic. The number K is named the width of the tree T .

A topical hierarchy for a corpus is constructed via a series of user operations. An operation transforms one topic hier-archy T to another T 0 . A top-down construction framework supports the following user operations. 1. Expand  X  for an arbitrary topic t in T , grow a subtree rooted at t . 2. Collapse  X  for an arbitrary topic t in T , remove all its descendant topics. 3. Split  X  for an arbitrary topic t in T , split it into k topics. 4. Remove  X  for an arbitrary set of topics t 1 ,...,t n in T , delete these topics. 5. Merge  X  for an arbitrary set of topics t 1 ,...,t merge these topics as a new topic, whose parent is the least common ancestor of them, and whose children are the union of the children of all merged topics. 6. Move  X  for an arbitrary topic t in T , move the subtree rooted at t to be under a different parent topic t 0 . Figure 1 demonstrates these operations. In these operations, only a few topics are affected, so users can consistently mod-ify the hierarchy and control the change.

For convenience, we index a topic using the top-down path from root to this topic. The root topic is indexed as o . Every non-root topic t is recursively indexed by  X  t  X   X  t , where  X  is the path index of its parent topic, and  X  t  X  [ C index of t among its siblings. For example, topic 2 in the  X  X erge X  example of Figure 1 is indexed as o  X  2, and topic 3 in the same tree is indexed as o  X  1  X  1. The level h t of a topic t is defined to be its distance to the root. So root topic is in level 0, and topic o  X  1  X  1 is in level 2. The height H of a tree is defined to be the maximal level over all the topics in the tree. Clearly, the total number T of topics
We develop a Scalable Tensor Recursive Orthogonal De-composition (STROD) framework for interactive topical hi-erarchy construction. In Section 4.1, we propose a new hier-archical topic model, and introduce how the user operations can be achieved by atomic manipulations to the model. In Section 4.2, we present our tensor-based algorithms support-ing these operations. Section 4.3 introduces topical phrase mining and ranking based on the inferred model parameters.
Generative hierarchical topic modeling assumes the docu-ments are generated from a latent variable model, and then infers the model parameters from observed documents to re-cover the topics. Distinct from prior work, we do not infer a hierarchy for one corpus only once. Instead, we allow users to perform consistent modification to the hierarchy. There-fore, we need a model that is convenient for manipulation and supports all the user operations introduced in Section 3.
We first introduce our generative model when the hierar-chy structure is fixed, and then discuss atomic operators to manipulate the model structure.
In this subsection we assume the topic hierarchy structure is fixed. Its height is H , and there are  X  leaf nodes and T  X   X  non-leaf nodes. For ease of explanation we assume all leaf nodes are on the level of H .

Every leaf topic node t ( C t = 0) has a multinomial dis-tribution  X  t = p ( w =  X | t ) over terms. Every document d paired with a non-leaf node t ( C t &gt; 0) has a multinomial distribution  X  d,t = p ( w =  X | d,t ) over t  X  X  child topics: t  X  1 through t  X  C t .  X  d,t represents the content bias of docu-ment d towards t  X  X  subtopics. For the  X  X erge X  example in Figure 1, before merge, there are 3 non-leaf topics: o,o  X  1 and o  X  2. So a document d is associated with 3 multino-mial distributions over topics:  X  d,o over its 2 children,  X  over its 3 children, and  X  d,o  X  2 over its 2 children. Each multinomial distribution  X  d,t is generated from a Dirichlet towards z -th child of topic t , and  X  t = P C t z =1  X  t  X  z
To generate a token w d,j , we first sample a path from the root to a leaf node o  X  z 1 d,j  X  z 2 d,j  X   X  X  X   X  z The nodes along the path are sampled one by one, start-ing from the root. Each time one child z i d,j is selected from all children of o  X  z 1 d,j  X  X  X  X  X  X  z i  X  1 d,j , from the multinomial  X  generated from its multinomial distribution  X  o  X  z 1
The whole generative process is: 1. For each leaf node t in T , generate its distribution over terms  X  t  X  Dir (  X  ); 2. For each document d  X  [ D ]: (a) For each non-leaf node t in T , draw a multinomial dis-
Figure 2: Latent Dirichlet Allocation with Topic Tree (b) For each token index j  X  [ l d ] of document d :
Its graphical representation is Figure 2. Table 1 collects the notations.

For every non-leaf topic node, we can derive a term distri-bution by marginalizing their children X  X  term distributions: So in our model, the term distribution  X  t for an internal node in the topic hierarchy can be calculated as a mixture of its children X  X  term distributions. The Dirichlet prior  X  determines the mixing weight.

When the structure T is fixed, we need to infer its param-eters  X  ( T ) and  X  ( T ) from a given corpus. When the height of the hierarchy H = 1, our model reduces to the flat LDA model.
The main advantage of this model is that it can be con-sistently manipulated to accommodate user operations.
Proposition 1. The following atomic manipulation op-erators are sufficient in order to compose all the user oper-ations introduced in Section 3:  X  EXP ( t,k ) . Discover k subtopics of a leaf topic t .  X  MER ( t 1 ,t 2 ) . Merge two topics t 1 and t 2 into a new topic t 3 under their least common ancestor t .  X  MOV ( t 1 ,t 2 ) . Move the subtree rooted at topic t under t 2 .
 The following are examples about how to use these manipu-lation operators to compose the user operations in Figure 1.  X   X  X ollapse X   X  applying MER ( o,o  X  1) three times.  X   X  X plit X   X  EXP ( o  X  2 , 2) followed by MER ( o,o  X  2).  X   X  X emove X   X  MER ( o  X  2 ,o  X  2  X  1) followed by MER ( o,o  X  2).

Implementation of these atomic operators needs to follow the consistency requirement. 1. Single-run consistency  X  suppose the topical hierarchy T is altered into T 2 after a user operation, certain nodes are not affected. For example, in the  X  X erge X  operation in Figure 1, node 0,1,2,3,5,7 are not touched. The con-sistency condition requires that, if we restart step 2-(b) whenever we reach an affected node in step 2-(b)-ii, T 1 and T 2 are equivalent generative models, i.e. , generate the same documents in expectation. By this definition, we have the following proposition.
 consistent if and only if i) for each unaffected leaf node 2. Multi-run consistency  X  with identical input across mul-tiple runs, one operator should output nearly identical (undifferentiated to human)  X  and  X  .

Section 4.2 presents a moment-based method to compute these operators efficiently and consistently.
In statistics, the  X  -th order population moment of a ran-dom variable is the expectation of its  X  -th power. In our problem, the random variable is a token w d,j in a docu-ment d . The  X  -th population moment is the expected co-occurrence of terms in  X  token positions. They are related to the model parameters  X  and  X  . The method of moments collects empirical moments from the corpus, and estimate  X  and  X  by fitting the empirical moments with theoretical mo-ments. As a computational advantage, it only relies on the term co-occurrence statistics. The statistics contain impor-tant information compressed from the full data, and require only a few scans of the data to collect.

To compute our three atomic operators, we generalize the notion of population moments. We consider the population moments conditioned on a topic t . The first order condi-tional moment E 1 ( t ) is a vector in R V . Component x is the expectation of 1 w = x given that w is drawn from topic t  X  X  descendant.

The second order moment E 2 ( t )  X  R V  X  V is a V  X  V ten-sor (hence, a matrix), storing the expectation of the co-occurrences of two terms w 1 and w 2 given that they are both drawn from topic t  X  X  descendants. Integrating over the document-topic distribution  X  , we have:
E 2 ( t ) = p ( w 1 =  X  ,w 2 =  X | t,t, X  ) (3) = X The operator  X  denotes an outer product between tensors: if Likewise, we can derive the third order moment E 3 ( t )  X  R
V  X  V  X  V (a V  X  V  X  V tensor) as the expectation of co-occurrences of three terms w 1 ,w 2 and w 3 given that they are all drawn from topic t  X  X  descendants:
Equations (2) X (4) characterize the theoretical conditional moments for topic t using model parameters associated with t  X  X  children. The empirical conditional moments can be es-timated from data and parameters of t  X  X  ancestors.
For topic t , we estimate the empirical  X  X opical X  count of term x in document d as: Recall that  X  t is t  X  X  parent. c d,x ( t ) can be recursively com-puted through c d,x (  X  t ) and the boundary is c d,x ( o ) = c i.e. , the total counts of term x in document d .

Then we can estimate empirical conditional moments us-ing these empirical topical counts: where l d ( t ) = P V x =1 c i,x ( t ). These enable fast estimation of empirical moments by passing data once.

The following three subsections discuss the computation of the three atomic operators EXP , MER and MOV with the method of moments.
EXP ( t,k ) should find k subtopics under topic t , without changing any existing model parameters. So we need an al-gorithm that returns (  X  t  X  z , X  t  X  z ) ,z  X  [ k ], with P  X  . By recursion, we note that only  X  o needs to be set by a user. It controls the degree of topical purity of documents. When  X  o  X  X  X  , each document is only about one leaf topic.
We employ the method of moments. In Equations (2) X  (4), we replace the left hand side with the empirical condi-tional moments estimated from the data. The right hand side is theoretical moments with  X  t  X  z , X  t  X  z ,z  X  [ k ] as un-known variables. Solving these equations yields a solution of the acquired model parameters. The following theorem by Anandkumar et al. [3] suggests that we only need to use up to 3rd order moments to find the solution.
 Theorem 1. Assume M 2 and M 3 are defined as: where  X  z &gt; 0 ,v z  X  X  are linearly independent, and k v When M 2 and M 3 are given, v z and  X  z in Equation (7) can be uniquely solved in polynomial time.
 To write Equations (2) X (4) in this form, we define: M 2 ( t ) = (  X  t + 1) E 2 ( t )  X   X  t E 1 ( t )  X  2 (8) U 1 ( t ) = E 2 ( t )  X  E 1 ( t ) ,
M 3 ( t ) = (  X  t + 1)(  X  t + 2) where  X ( A,a,b,c ) permutes the modes of tensor A , such So they fit Equation (7) nicely, and intuitively. If we decom-pose M 2 ( t ) and M 3 ( t ), the z -th component is determined by the child X  X  term distribution  X  t  X  z , and its weight is which is equal to p ( t  X  z | t ).
 M 2 ( t ) is a dense V  X  V matrix, and M 3 ( t ) is a dense V  X  V  X  V tensor. Direct application of the tensor decomposition algorithm in [3] is challenging due to the creation of these huge dense tensors. Therefore, we design a more scalable algorithm. The idea is to bypass the creation of M 2 ( t ) and M 3 ( t ) and utilize the sparsity and decoupled decomposition of the moments. We go over Algorithm 1 to explain it.
Line 1.1 collects the empirical moments with one scan of the data.

Lines 1.2 to 1.6 project the large tensor M 3  X  R V  X  V  X  V into a smaller tensor e T  X  R k  X  k  X  k . e T is not only of smaller size, but also can be decomposed into an orthogonal form: e R . This is assured by the whitening matrix W calculated in Line 1.5, which satisfies W T M 2 W = I . This part contains two major tricks: 1. When calculating W , the straightforward computation requires spectral decomposition of M 2 . We avoid explicit creation of M 2 , but achieve the equivalent spectral de-composition. We first perform spectral decomposition for
E 2 ( t ) = U  X  1 U T , where U  X  R V  X  k is the matrix of k eigenvectors, and  X  1  X  R k  X  k is the diagonal eigenvalue matrix. The k column vectors of U form an orthonor-mal basis of the column space of E 2 ( t ). E 1 ( t ) X  X  repre-sentation in this basis is M 1 = U T E 1 ( t ). According to Equation (8), M 2 can now be written as:
So a second spectral decomposition can be performed on M 0 2 = (  X  t + 1) X  1  X   X  t M 1  X  M 1 , as M 0 2 = U
Then we have UU 0  X ( UU 0 ) T as M 2  X  X  spectral decompo-sition. The space requirement is reduced from V 2 m = k E 2 ( t ) k 0 V 2 , because only term pairs ever co-occurring in one document contribute to non-zero ele-ments of E 2 ( t ). The time for spectral decomposition is reduced from O ( V 2 K ) to O ( mK ). 2. The straightforward computation of the tensor product T = M 3 ( t )( W,W,W ) using explicit M 3 ( t ) and W requires
O ( V 3 ) space and O ( V 3 K + L  X  l 2 ) time, where  X  l is the max-imal document length. We decouple M 3 ( t ) as a summa-tion of multiple tensors, such that the product between each tensor and W is in a decomposable form: either ( v  X  v  X  v )( W,W,W ) or ( v  X  B )( W,W,W ), which can be computed as easily as ( W T v )  X  3 or ( W T v )  X  ( W T BW ).  X   X  t (  X  t + 1) ] where s d ( t ) = 1 l column of W T . U 2 ( W,W,W ) and U 3 ( W,W,W ) can be obtained by permuting U 1 ( W,W,W ) X  X  modes. The reno-vated procedure needs only one pass of data in O ( LK 2 ) time.

Lines 1.7 to 1.14 perform orthogonal decomposition of e T via a power iteration method. The orthonormal eigenpairs ( f  X  z , e v z ) are found one by one. To find one such pair, the algo-rithm randomly starts with a unit-norm vector v , runs power iteration (Line 1.11) for n times, and records the candidate eigenpair. This process further repeats by N times, starting from various unit-norm vectors. Line 1.12 picks the eigen-pair with the largest eigenvalue. After an eigenpair is found, the tensor e T is deflated by the found component (Line 1.14), and the same power iteration is applied to it to find the next eigenpair. After all the k orthonormal eigenpairs ( f  X  z found, they can be used to uniquely determine the k target components (  X  t  X  z , X  t  X  z ) (Line 1.13).

Line 1.15 computes the empirical topical counts for the k inferred child topics. It requires one scan of the data.
The decomposition by Algorithm 1 is fast and unique with sufficient data.

Theorem 2. Assume M 2 and M 3 are defined as in Equa-tion (7) ,  X  z &gt; 0 , and v z  X  X  are linearly independent with unit-norm, then Algorithm 1 finds exactly the same set of
Algorithm 1: EXP ( t,k ) (  X  iteration step of Line 1.11 converges in a quadratic rate. It satisfies single-run consistency. Multi-run consistency is guaranteed if the empirical moments are close to theoretical moments. We empirically evaluate it in Section 5.
 The overall time complexity for EXP is O ( LK 2 + Km + NnK 4 ), which can be regarded linear to the data size since N and n can be as small constants as 10 to 30, and K is a small number like 10 to 50 due to our assumption of human-manageable tree width. It requires only three scans of data.
Algorithm 2: MER ( t 1 ,t 2 )
To merge two topics t 1 and t 2 , we need to find their least common ancestor t (Line 2.1), subtract the topical counts c ( t 0 ) and the Dirichlet prior  X  t 0 for any other topic t 0 path between t 1 and t 2 (Lines 2.2 X 2.11), and then create a new node t 3 to sum up the topical counts and Dirichlet prior of t 1 and t 2 (Lines 2.14 X 2.16) with one exception: when t is t 2  X  X  direct ancestor or direct descendant, we can just use t as the merged topic node (Line 2.12). We then move the children of t 1 and t 2 to be under the merged topic node (Lines 2.17 X 2.18). Last, we remove t 1 and t 2 and add t the topical hierarchy (Line 2.19). The complexity for MER is O ( LH ).
Algorithm 3: MOV ( t 1 ,t 2 )
To move the subtree rooted at t 1 to be under t 2 , we first subtract topical counts and Dirichlet prior from every an-cestor of t 1 (Lines 3.1 X 3.5), and then add them to every ancestor of t 2 , including t 2 itself (Lines 3.6 X 3.10). Finally, we set the parent of t 1 to be t 2 . The complexity for MOV is O ( LH ).

The implementation of MER and MOV using Algorithms 2 and 3 satisfy both multi-run and single-run consistency re-quirement.
After the term distribution in each topic is inferred, we can then mine and rank topical phrases within each topic. The phrase mining and ranking in STROD adapt CATHY [27] to generic text. Here we briefly present the process.
In this work, a phrase is defined as a frequent consecu-tive sequence of terms of arbitrary lengths. To filter out in-complete phrases ( e.g. ,  X  X ector machine X  instead of  X  X upport vector machine X ) and frequently co-occurred terms that do not make up a meaningful phrase ( e.g. ,  X  X ften use X ), we use a statistical test to select quality phrases [7], and record the count c d,P of each phrase P in each document d .

After the phrases of mixed lengths are mined, they are ranked with regard to the representativeness of each topic in the hierarchy, based on two factors: popularity and dis-criminativeness . A phrase is popular for a topic if it appears frequently in documents containing that topic ( e.g. ,  X  X nfor-mation retrieval X  has better popularity than  X  X ross-language information retrieval X  in the Information Retrieval topic). A phrase is discriminative of a topic if it is frequent only in the documents about that topic but not in those about other topics ( e.g. ,  X  X uery processing X  is more discriminative than  X  X uery X  in the database topic).

We use the topical term distributions inferred from our model to estimate the  X  X opical count X  c d,P ( t ) of each phrase P in each document d , in a similar way as we estimate the topical count of terms in Equation (5):
Let the conditional probability p ( P | t ) be the probability of  X  X andomly choose a document and a phrase that is about topic t , the phrase is P . X  It can be estimated as p ( P | t ) = topic t can be quantified by p ( P | t ). The discriminativeness can be measured by the log ratio between the probability p ( P | t ) conditioned on topic t and the probability p ( P |  X 
A good ranking function to combine these two factors is their product: which has an information-theoretic sense: the pointwise KL-divergence between the two probabilities [27]. Finally, we use r t ( P ) to rank phrases in topic t in the descending order.
In this section we first introduce the datasets and the methods used for comparison, and then describe our evalu-ation on efficiency, consistency, and quality.
 Datasets . Our performance study is on four datasets:  X  DBLP title: A set of titles of recently published papers in DBLP ( www.dblp.org ). The set has 1.9M titles, 152K unique terms, and 11M tokens.  X  CS abstract: A dataset of computer science paper ab-stracts from Arnetminer ( www.arnetminer.org ). The set has 529K papers, 186K unique terms, and 39M tokens.  X  TREC AP news: A TREC news dataset (1998). It has 106K full articles, 170K unique terms, and 19M tokens.  X  Pubmed abstract: A dataset of life sciences and biomed-ical topic. We crawled 1.5M abstracts from Jan. 2012 to Sep. 2013 on Pubmed ( www.ncbi.nlm.nih.gov/pubmed ). The dataset has 98K unique terms and 169M tokens.
 We remove English stopwords from all the documents. Methods for comparison. We mainly evaluate EXP be-cause it dominates the runtime, and its consistency in real-world data is subject to empirical evaluation. We compare the following topical hierarchy construction methods.  X  hPAM  X  parametric hierarchical topic model. The hierar-chical Pachinko Allocation Model [20] is a state-of-the-art parametric hierarchical topic modeling approach. hPAM outputs a specified number of supertopics and subtopics, as well as the associations between them.  X  nCRP  X  nonparametric hierarchical topic model. We choose nCRP to represent this category for its relative efficiency.
It outputs a tree with a specified height. The number of topics cannot be set exactly. We tune its hyperparameter to generate an approximately identical number of topics as other methods.  X  splitLDA  X  recursively applying LDA, as discussed in Sec-tion 2. This heuristic method is more efficienct than the above two methods. We implement splitLDA on top of an efficient single-machine LDA inference algorithm [30].  X  CATHY  X  recursively clustering term co-occurrence net-works. CATHY [27] uses a term co-occurrence network to compress short documents and performs topic discovery through an EM algorithm.  X  STROD and its variations RTOD, RTOD 2 , RTOD 3  X  re-cursively applying our EXP operator to expand the tree.
We implement several variations to analyze our scalability improvement techniques: (i) RTOD: recursive tensor or-thogonal decomposition without scalability improvement [3]; (ii) RTOD 2 : RTOD plus the efficient computation of whiten-ing matrix by avoiding creation of M 2 ; (iii) RTOD 3 : RTOD plus the efficient computation of tensor product by avoid-ing creation of M 3 ; and (iv) STROD: Algorithm 1 with the full scale-up technique.
The first evaluation assesses the efficiency of different algo-rithms when constructing a topical hierarchy with the same depth and width.

Figure 3 shows the overall runtime in these datasets. STROD is several orders of magnitude faster than the existing meth-ods. On the largest dataset it reduces the runtime from one or more days to 18 minutes in total. CATHY is the second best method in short documents such as titles and abstracts because it compresses the documents into term co-occurrence networks. But it is still more than 100 times slower than STROD due to many rounds of EM iterations. splitLDA and hPAM rely on Gibbs sampling, and the former is faster because it recursively performs LDA, and consid-ers fewer dependencies in sampling. nCRP is two orders of magnitude slower.

We then conduct analytical study of the runtime growth with respect to different factors. Figures 4a X 4c show the runtime varying with the number of tokens, the tree height and the tree width. We can see that the runtime of STROD grows slowly, and it has the best performance in all occa-sions. The margin of our method over others grows quickly when the scale increases. In Figure 4b, we exclude hPAM because it is designed for H = 2. We exclude nCRP from all these experiments because it takes too long time to finish ( &gt; 90 hours with 600K tokens).

Figure 4d shows the performance in comparison with the variations of STROD. Both RTOD and RTOD 2 fail to fin-ish when the vocabulary size grows beyond 1K, because the third-order moment tensor M 3 ( t ) requires O ( V 3 ) space to create. RTOD 3 also has limited scalability because the sec-ond order moment tensor M 2 ( t )  X  R V  X  V is dense. STROD scales up easily by avoiding explicit creation of these tensors.
The second evaluation assesses the multi-run consistency of different algorithms. For each dataset, we sample 10,000 documents and run each algorithm 10 times and measure the variance among the 10 runs for the same method as fol-lows. Each pair of algorithm runs generate the same num-ber of topics, but their correspondence is generally unknown (STROD makes an exception with its ability to obtain a unique order of subtopics according to learned  X  ). For ex-ample, the topic o  X  1 in the first run may be close to o  X  3 in the second run. We measure the KL divergence between all pairs of topical term distributions between the two runs, build a bipartite graph using the negative KL divergence as the edge weight, and then use a maximum matching algorithm to determine the best correspondence (top-down recursively). Then we average the KL divergence between matched pairs as the difference between the two algorithm runs. Finally, we average the difference between all 10  X  9 = 90 ordered pairs of algorithm runs as the final variance. We exclude nCRP in this section, since even the number of topics is not a constant after each run.

Table 2 summarizes the results: STROD has lowest vari-ance in all the three datasets. The other three methods based on Gibbs sampling have variance larger than 1 in all datasets, which implies that the topics generated across mul-tiple algorithm runs are considerably different.

We also evaluate the variance of STROD when we vary the number of outer and inner iterations N and n . As shown in Figure 5, the variance of STROD quickly diminishes when the number of outer and inner iterations grow to 10. This validates the theoretical analysis of their fast convergence.
In conclusion, STROD achieves consistent performance with small runtime. It is stable and robust to be used as a hierarchy construction method for large text collections.
The final evaluation assesses the interpretability of the constructed topical hierarchy, via human judgment. We evaluate hierarchies constructed from DBLP titles and TREC AP news. For simplicity, we set the number of subtopics to Table 2: The variance of multiple algorithm runs in each dataset Figure 5: The variance and runtime of STROD when varying # outer and inner iterations N and n (CS abstract) be 5 for all topics. For hPAM, we post-process them to ob-tain the 5 strongest subtopics for each topic. For all the methods we use the same phrase mining and ranking pro-cedure to enhance the interpretability. We do not include nCRP in this study because hPAM has been shown to have superior performance of it [20].

In order to evaluate the coherence of the hierarchy, we use an Topic Intrusion (TI) task which were proposed in [27]: Evaluators are shown a parent topic t and X candidate child topics. X  X  1 of the child topics are actual children of t in the generated hierarchy, and the remaining child topic is not. Each topic is represented by its top-5 ranked phrases. Evaluators are asked to select the intruder child topic, or to indicate that they are unable to make a choice.

For this study we set X = 4. 160 Topic Intrusion questions are randomly generated. We then calculate the agreement of the human choices with the actual hierarchical structure constructed by the various methods. We consider a higher match between a given hierarchy and human judgment to imply a higher quality hierarchy. For each method, we re-port the F-1 measure of the answers matched consistently by three human judgers with CS background.

Figure 6 summarizes the results. STROD is the best per-forming method in both datasets. This suggests that the quality of the hierarchy is not compromised by the strong ef-ficiency and consistency of STROD. As the tree goes deeper, splitLDA degrades in quality due to inclusion of irrelevant portion of each document. Compared to splitLDA, STROD does not assign a document entirely to a topic. In addition, STROD has a theoretically guaranteed inference method for expansion, which may also account for the superior quality.
A subset of the hierarchy constructed from CS abstract by  X  X xpand X  is presented in Figure 7. For each non-root node, we show the top ranked phrases. Node o  X  1 is about  X  X ata X , while its children involve database, data mining and bioinformatics. The lower the level is, the more specific the topic is, and the more multigrams emerge ahead of unigrams in general. This initial hierarchy helps users quickly see the main topics without going through all the documents. They can then use other operators to make small changes to the hierarchy to confidently and continuously refine the quality.
In this work, we tackle the efficiency and consistency chal-lenge of interactive topical hierarchy construction from large-scale text data. We design a novel moment-based frame-work to build the hierarchy recursively. Our framework di-vides the construction task into simpler operations in which users can be interactively involved. To support these opera-tions, we design a new model for topical hierarchy which can be learned recursively. For consistent inference, we extend a theoretically guaranteed tensor orthogonal decomposition technique to this model. Utilizing the special structure of the tensor in our task, we scale up the algorithm signifi-cantly. By evaluating our approach on a variety of datasets, we demonstrate a prominent computational advantage. Our algorithm generates consistent and quality topic hierarchy 100-1000 times faster than the state of the art, and the mar-gin grows when the corpus size increases.

This invention opens up numerous possibilities for future work. On the application side, it is foundation for building new systems to support explorative generation of textual data catalogs. Existing choice is either fully manual or fully automatic. The former is high quality but labor-expensive, and the latter is the opposite. By adding interaction capa-bility to automated methods, there is hope to reduce human effort and meanwhile allow users to have quality control. On the methodology side, the advantage of STROD can be fur-ther fulfilled by parallelization and adaptation to dynamic text collections.
