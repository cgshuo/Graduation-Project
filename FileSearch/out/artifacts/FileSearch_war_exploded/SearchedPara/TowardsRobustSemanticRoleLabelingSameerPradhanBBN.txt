 Automatic, accurate and wide-co verage techniques that can annotate naturally occurring text with se-mantic argument structure play a key role in NLP applications such as Information Extraction (Sur -deanu et al., 2003; Harabagiu et al., 2005), Question Answering (Narayanan and Harabagiu, 2004) and Machine Translation (Boas, 2002; Chen and Fung, 2004). Semantic Role Labeling (SRL) is the pro-cess of producing such a markup. When presented with a sentence, a parser should, for each predicate in the sentence, identify and label the predicate X  s se-mantic arguments. In recent work, a number of re-searchers have cast this problem as a tagging prob-lem and have applied various supervised machine learning techniques to it. On the Wall Street Jour -nal (WSJ) data, using correct syntactic parses, it is possible to achie ve accuracies rivaling human inter -annotator agreement. Ho we ver, the performance gap widens when information deri ved from automatic syntactic parses is used.

So far, most of the work on SRL systems has been focused on impro ving the labeling performance on a test set belonging to the same genre of text as the training set. Both the Treebank on which the syntac-tic parser is trained and the PropBank on which the SRL systems are trained represent articles from the year 1989 of the WSJ. While all these systems per -form quite well on the WSJ test data, the y sho w sig-nificant performance degradation (approximately 10 point drop in F-score) when applied to label test data that is dif ferent than the genre that WSJ represents (Pradhan et al., 2004; Carreras and M ` arquez, 2005). Surprisingly , it does not matter much whether the data is from another newswire, or a completely dif-ferent type of text  X  as in the Bro wn corpus. These results indicate that the systems are being over-fit to the specific genre of text. Man y performance im-pro vements on the WSJ PropBank corpus may re-flect tuning to the corpus. For the technology to be widely accepted and useful, it must be rob ust to change in genre of the data. Until recently , data tagged with similar semantic argument structure was not available for multiple genres of text. Recently , Palmer et al., (2005), have PropBank ed a significant portion of the Treebank ed Bro wn corpus which en-ables us to perform experiments to analyze the rea-sons behind the performance degradation, and sug-gest potential solutions. In the PropBank 1 corpus (Palmer et al., 2005), pred-icate argument relations are mark ed for the verbs in the text. PropBank was constructed by assign-ing semantic arguments to constituents of the hand-corrected Treebank parses. The arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P
ROTO -A GENT (usually the subject of a transiti ve verb) A RG 1 is the P ROTO -P ATIENT (usually its di-rect object), etc. In addition to these C ORE A RGU -referred to as A RG Ms are also mark ed.

More recently the PropBanking effort has been extended to encompass multiple corpora. In this study we use PropBank ed versions of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1994) and part of the Bro wn portion of the Penn Treebank.

The WSJ PropBank data comprise 24 sections of the WSJ, each section representing about 100 documents. PropBank release 1.0 contains about 114,000 predicates instantiating about 250,000 argu-ments and covering about 3,200 verb lemmas. Sec-tion 23, which is a standard test set and a test set in some of our experiments, comprises 5,400 predi-cates instantiating about 12,000 arguments.
The Bro wn corpus is a Standard Corpus of Ameri-can English that consists of about one million words of English text printed in the calendar year 1961 (K u  X  cera and Francis, 1967). The corpus contains about 500 samples of 2000+ words each. The idea behind creating this corpus was to create a hetero-geneous sample of English text so that it would be useful for comparati ve language studies.

The Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Bro wn Corpus  X  sections F, G, K, L, M, N, P and R. Palmer et al., (2005) have recently PropBank ed a signifi-cant portion of this Treebank ed Bro wn corpus. In all, about 17,500 predicates are tagged with their se-mantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates  X  about 8,000 have also been tagged with frame sense information. We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsk y (2002) and use Support Vector Machine (SVM) classi-fiers (2005). We use TinySVM 2 along with Yam-Cha 3 (K udo and Matsumoto, 2000) (K udo and Mat-sumoto, 2001) as the SVM training and classifica-tion softw are. The system uses a polynomial kernel with degree 2; the cost per unit violation of the mar -gin, C =1; and, tolerance of the termination criterion, e =0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is sho wn in Table 1 Table 1: Performance of the SRL system on WSJ
The performance of the SRL system is reported on three dif ferent tasks, all of which are with respect to a particular predicate: i) argument identification (ID), is the task of identifying the set of words (here, parse constituents) that represent a semantic role; ii) argument classification (Class.), is the task of clas-sifying parse constituents kno wn to represent some semantic role into one of the man y semantic role types; and iii) argument identification and classifi-cation (ID + Class.), which involv es both the iden-tification of the parse constituents that represent se-mantic roles of the predicate and their classification into the respecti ve semantic roles. As usual, argu-ment classification is measured as percent accurac y (A), whereas ID and ID + Class. are measured in terms of precision (P), recall (R) and F-score (F)  X  the harmonic mean of P and R. The first three rows of Table 1 report performance for the system that uses hand-corrected Treebank parses, and the next three report performance for the SRL system that uses automatically generated  X  Charniak parser  X  parses, both during training and testing. This section describes experiments that we per -formed using the PropBank ed Bro wn corpus in an attempt to analyze the factors affecting the portabil-ity of SRL systems. 4.1 Ho w does the SRL system trained on WSJ In order to test the rob ustness of the SRL system, we used a system trained on the PropBank ed WSJ corpus to label data from the Bro wn corpus. We use the entire PropBank ed Bro wn corpus (about 17,500 predicates) as a test set for this experiment and use the SRL system trained on WSJ sections 02-21 to tag its arguments.

Table 2 sho ws the performance for training and testing on WSJ, and for training on WSJ and testing on Bro wn. There is a significant reduction in per -formance when the system trained on WSJ is used to label data from the Bro wn corpus. The degrada-tion in the Identification task is small compared to that of the combined Identification and Classifica-tion task. A number of factors could be responsible for the loss of performance. It is possible that the SRL models are tuned to the particular vocab ulary and sense structure associated with the training data. Also, since the syntactic parser that is used for gen-erating the syntax parse trees (Charniak) is hea vily lexicalized and is trained on WSJ, it could have de-creased accurac y on the Bro wn data resulting in re-duced accurac y for Semantic Role Labeling. Since the SRL algorithm walks the syntax tree classifying each node, if no constituent node is present that cor -responds to the correct argument, the system cannot produce a correct labeling for the argument. Table 2: Performance of the SRL system on Bro wn.
In order to check the extent to which constituent nodes representing semantic arguments were deleted from the syntax tree due to parser error , we gener -ated the performance numbers which are sho wn in Table 3. These numbers are for top one parse for the Charniak parser , and represent not all parser errors, but deletion of argument bearing constituent nodes. Table 3: Constituent deletions in WSJ and Bro wn.
The parser misses 6.7% of the argument-bearing nodes in the PropBank test set and about 8.1% in the Bro wn corpus. This indicates that the errors in syntactic parsing account for a fairly small amount of the argument deletions and probably do not con-trib uting significantly to the increased SRL error rate. Ob viously , just the presence of a argument-bearing constituent does not necessarily guarantee the correctness of the structural connections be-tween itself and the predicate. 4.2 Identification vs Classification Perf ormance Dif ferent features tend to dominate in the identifi-cation task vs the classification task. For example, the path feature (representing the path in the syntax tree from the argument to the predicate) is the sin-gle most salient feature for the ID task and is not very important in the classification task. In the next experiment we look at cross genre performance of the ID and Classification tasks. We used gold stan-dard syntactic trees from the Treebank so there are no errors in generating the syntactic structure. In addition to training on the WSJ and testing on WSJ and Bro wn, we trained the SRL system on a Bro wn training set and tested it on a test set also from the Bro wn corpus. In generating the Bro wn training and test sets, we used stratified sampling, which is often used by the syntactic parsing community (Gildea, 2001). The test set was generated by selecting ev-ery 10 th sentence in the Bro wn Corpus. We also held out the development set used by Bacchiani et al., (2006) to tune system parameters in the future. This procedure resulted in a training set of approxi-mately 14,000 predicates and a test set of about 1600 predicates. We did not perform any parameter tun-ing for any of the follo wing experiments, and used the parameter settings from the best performing ver-sion of the SRL system as reported in Table1. We compare the performance on this test set with that obtained when the SRL system is trained using WSJ sections 02-21 and use section 23 for testing. For a more balanced comparison, we retrained the SRL system on the same amount of data as used for train-ing on Bro wn, and tested it on section 23. As usual, trace information, and function tag information from the Treebank is stripped out.

Table 4 sho ws the results. There is a fairly small dif ference in argument Identification performance when the SRL system is trained on 14,000 predi-cates vs 104,000 predicates from the WSJ (F-score 95.3 vs 96.8). Ho we ver, there is a considerable drop in Classification accurac y (86.1% vs 93.0%). When the SRL system is trained and tested on Bro wn data, the argument Identification performance is not sig-nificantly dif ferent than that for the system trained and tested on WSJ data (F-score 95.2 vs 95.3). The drop in argument Classification accurac y is much more severe (86.1% vs 80.1%).

This same trend between ID and Classification is even more pronounced when training on WSJ and testing on Bro wn. For a system trained on WSJ, there is a fairly small drop in performance of the ID task when tested on Bro wn vs tested on WSJ (F-score 92.7 vs 95.3). Ho we ver, in this same condi-tion, the Classification task has a very lar ge drop in performance (72.0% vs 86.1%).

So argument ID is not very sensiti ve to amount of training data in a corpus, or to the genre of the corpus, and ports well from WSJ to Bro wn. This ex-periment supports the belief that there is no signifi-cant drop in the task of identifying the right syntactic constituents that are arguments  X  and this is intuiti ve since pre vious experiments have sho wn that the task of argument identification is more dependent on the structural features  X  one such feature being the path in the syntax tree.
 Ar gument Classification seems to be the problem. It requires more training data within the WSJ corpus, does not perform as well when trained and tested on Bro wn as it does for WSJ and does not port well from WSJ to Bro wn. This suggests that the features it uses are being over-fit to the training data and are more idiosyncratic to a given dataset. In particular , the predicate whose arguments are being identified, and the head word of the syntactic constituent being classified are both important features in the task of argument classification.

As a generalization, the features used by the Iden-tification task reflect structure and port well. The features used by the Classification task reflect spe-cific lexical usage and semantics, and tend to require more training data and are more subject to over-fitting. Ev en when training and testing on Bro wn, Classification accurac y is considerably worse than training and testing on WSJ (with comparable train-ing set size). It is probably the case that the predi-cates and head words in a homogeneous corpus such as the WSJ are used more consistently , and tend to have single dominant word senses. The Bro wn cor -pus probably has much more variety in its lexical usage and word senses. 4.3 Ho w sensiti ve is semantic argument This experiment examines the same cross-genre ef-fects as the last experiment, but uses automatically generated syntactic parses rather than gold standard ones.

For this experiment, we used the same amount of training data from WSJ as available in the Bro wn training set  X  that is about 14,000 predicates. The examples from WSJ were selected randomly . The Bro wn test set is the same as used in the pre vious experiment, and the WSJ test set is the entire section 23.

Recently there have been some impro vements to the Charniak parser , use n -best re-ranking as re-ported in (Charniak and Johnson, 2005) and self-training and re-ranking using data from the North American Ne ws corpus (N ANC) and adapts much better to the Bro wn corpus (McClosk y et al., 2006a; McClosk y et al., 2006b). The performance of these parsers as reported in the respecti ve literature are sho wn in Table 6 sho ws the performance (as re-ported in the literature) of the Charniak parser: when trained and tested on WSJ, when trained on WSJ and tested on Bro wn, When trained and tested on Bro wn, and when trained on WSJ and adapted with NANC.
We describe the results of Semantic Role Label-ing under the follo wing five conditions: 1. The SRL system is trained on features ex-2. The SRL system is trained on features ex-3. The SRL system is trained on features ex-4. The SRL system is trained on features ex-5. The SRL system is trained on features ex-
Table 5 sho ws the results. For simplicity of dis-cussion we have tagged the five conditions as 1., 2., 3., 4., and 5. Comparing conditions 2. and 3. sho ws that when the features used to train the SRL system are extracted using a syntactic parser that is trained on WSJ it performs at almost the same level on the task of Identification, regardless of whether it is trained on the PropBank ed Bro wn corpus or the PropBank ed WSJ corpus. This, howe ver, is sig-nificantly lower than when all the three  X  the syn-tactic parser training set, the SRL system training set, and the SRL system test set, are from the same genre (6 F-score points lower than condition 1, and 5 points lower than conditions 4 and 5). In case of the combined task, the gap between the performance for conditions 2 and 3 is about 10 points in F-score (59.1 vs 69.8). Looking at the argument classifica-tion accuracies, we see that using the SRL system trained on WSJ to test Bro wn sentences give a 12 point drop in F-score (84.1 vs 72.1). Using the SRL system trained on Bro wn using WSJ trained syntac-tic parser sho ws a drop in accurac y by about 5 F-score points (84.1 to 79.2). When the SRL system is trained on Bro wn using syntactic parser also trained on Bro wn, we get a quite similar classification per -formance, which is again about 5 points lower than what we get using all WSJ data. This sho ws lexical semantic features might be very important to get a better argument classification on Bro wn corpus. 4.4 Ho w much data is requir ed to adapt to a We would lik e to kno w how much data from a new genre we need to annotate and add to the training data of an existing corpus to adapt the system such that it gives the same level of performance as when it is trained on the new genre.

One section of the Bro wn corpus  X  section CK has about 8,200 predicates annotated. We use six dif ferent conditions  X  two in which we use correct Treebank parses, and the four others in which we use automatically generated parses using the varia-tions described before. All training sets start with the same number of examples as in the Bro wn train-ing set. The part of this section used as a test set for the CoNLL 2005 shared task is used as the test set here. It contains a total of about 800 predicates. Table 7 sho ws a comparison of these conditions. In all the six conditions, the performance on the task of Identification and Classification impro ves gradu-ally until about 5625 examples of section CK which is about 75% of the total added, abo ve which the y impro ve very little. In fact, even 50% of the new data accounts for 90% of the performance dif fer -ence. Ev en when the syntactic parser is trained on WSJ and the SRL is trained on WSJ, adding 7,500 instances of the new genres allo ws it to achie ve al-most the same performance as when all three are from the same genre (67.2 vs 69.9). Numbers for ar-gument identification aren X  t sho wn because adding more data does not have any statistically signifi-cant impact on its performance. The system that uses self-trained syntactic parser seems to perform slightly better than the rest of the versions that use automatically generated syntactic parses. The preci-sion numbers are almost unaf fected  X  except when the labeler is trained on WSJ PropBank data. 4.5 Ho w much does verb sense inf ormation In order to find out how important the verb sense information is in the process of genre transfer , we used the subset of PropBank ed Bro wn corpus that was tagged with verb sense information, ran an ex-periment similar to that of Experiment 1. We used the oracle sense information and correct syntactic in-formation for this experiment.
 Table 8 sho ws the results of this experiment. There is about 1 point F-score increase on using oracle sense information on the overall data. We look ed at predicates that had high perple xity in both the training and test sets, and whose sense distrib u-tion was dif ferent. One such predicate is  X  X o X . The impro vement on classifying the arguments of this predicate was about 2 points (46.9 to 48.9), which suggests that verb sense is more important when the sense structure of the test corpus is more ambiguous and is dif ferent from the training. Here we used ora-cle verb sense information, but one can train a clas-sifier as done by Girju et al., (2005) which achie ves a disambiguation accurac y in the 80s for within the WSJ corpus. Our experimental results on rob ustness to change in genre can be summarized as follo ws:  X  There is a significant drop in performance when  X  In this process the classification task is more  X  There is a performance drop in classification  X  The syntactic parser error is not a lar ge part of
An error analysis leads us to belie ve that some reasons for this beha vior could be: i) lexical us-ages that are specific to WSJ, ii) variation in sub-cate gorization across corpora, iii) variation in word sense distrib ution and iv) changes in topics and enti-ties. Training and testing on the same corpora tends to give a high weight to very specific semantic fea-tures. Two possibilities remedies could be: i) using less homogeneous corpora and ii) less specific fea-tures, for eg., proper names are replaced with the name entities that the y represent. This way the sys-tem could be forced to use the more general features. Both of these manipulations would most lik ely re-duce performance on the training set, and on test sets of the same genre as the training data. But the y would be lik ely to generalize better . We are extremely grateful to Martha Palmer for pro-viding us with the PropBank ed Bro wn corpus, and to Da vid McClosk y for pro viding us with hypothe-ses on the Bro wn test set as well as a cross-v alidated version of the Bro wn training data for the various models reported in his work reported at HL T 2006.
This research was partially supported by the ARD A AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132.

