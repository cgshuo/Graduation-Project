 Metric learning aims at automatically learning a metric from pair or triplet based constraints in data, and it can be potentially beneficial whenever the notion of metric between instances plays a nontrivial role. In Mahalanobis distance metric learning, distance matrix M is in symmetric positive semi-definite cone, and in order to avoid overfitting and to learn a better Mahalanobis distance from weakly supervised constraints, the low-rank regularization has been often imposed on matrix M to learn the correlations between features and samples. As the approximations of the rank minimization function, the trace norm and Fantope have been utilized to regularize the met-ric learning objectives and achieve good performance. However, these low-rank regularization models are either not tight enough to approximate rank minimization or time-consuming to tune an opti-mal rank. In this paper, we introduce a novel metric learning model using the capped trace norm based regularization, which uses a sin-gular value threshold to constraint the metric matrix M as low-rank explicitly such that the rank of matrix M is stable when the large singular values vary. The capped trace norm regularization can also be viewed as the adaptive Fantope regularization. We minimize singular values which are less than threshold value and the rank of M is not necessary to be k , thus our method is more stable and applicable in practice when we do not know the optimal rank of matrix M . We derive an efficient optimization algorithm to solve the proposed new model and the algorithm convergence proof is also provided in this paper. We evaluate our method on a variety of challenging benchmarks, such as LFW and Pubfig datasets. Face verification experiments are performed and results show that our method consistently outperforms the state-of-the-art metric learn-ing algorithms.  X 
To whom all correspondence should be addressed. This work was partially supported by US NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NIH R01 AG049371.
 Figure 1: An illustration of metric learning on Outdoor Scene Recognition (OSR) dataset.
  X  Information systems  X  Data mining;  X  Computing method-ologies  X  Machine learning; Metric Learning; Capped Trace Norm; Low-Rank Approximation; Face Verification
Metric learning has shown promising results with learning the proper Mahalanobis distance for many data mining tasks.The goal of metric learning is to learn an optimal linear or nonlinear projec-tion for original high-dimension features from supervised or weakly supervised constraints, and there have been a lot of works in this field [28, 1, 26, 9, 27, 24]. Metric learning has been widely used in applications where metric between samples plays an important role, such as image classification, face verification and recognition in computer vision [6, 17, 13, 14], learning to rank in information retrieval [16, 15], bioinformatics [25], etc .

In image mining and retrieval, there are many metric learning algorithms learning an optimal Mahalanobis distance from weakly supervised constraints between images. The main constraint paradigms include: pair constraint [1], triplet constraint [27], and quadruplet constraint [13]. To avoid overfitting and learn correlation among samples, many regularizations were proposed to impose on the pro-jection matrix. Among these regularizations, the low-rank regu-larization is proved to be effective and efficient to learn potential correlations from training data, e.g. trace norm and Fantope regu-larization[14].

In this paper, we propose a novel metric learning model using the capped trace norm as the low-rank regularization for Mahalanobis distance metric learning. Different from trace norm which mini-mizes sum of all singular values, or Fantope regularization which minimizes sum of k smallest singular values, the capped trace norm penalizes the singular values that are less than a threshold adap-tively learned in the optimization. As a result, the non-relevant information (associated to smallest singular values) can be filtered out, such that the metric learning model is more robust. Meanwhile, we only need input an approximate rank value, thus our regulariza-tion term is tighter than trace norm and more stable and applica-ble in practical problems. We also derive an efficient optimiza-tion algorithm with rigorous convergence analysis. In the exper-iments, we impose our novel low-rank regularization on different metric learning formulations and compare with other the state-of-the-art metric learning methods. Experimental results show that our method outperforms other related methods on benchmark datasets.
The goal of metric learning is to learn an adaptive distance, such as Mahalanobis distance d M ( x i , x j ) = p ( x i  X  x j for the problem of interest using the information brought by train-ing examples. Most of metric learning methods use weakly-supervised constraints. There are three mainly paradigms of constraints, such as pairwise, triplet, or quadruplet constraint.

The pairwise constraint contains information whether two ob-jects in a pair are similar or dissimilar, sometimes positive pairs or negative pairs. Pairwise constraint is represented by D and S as:
The information-Theoretic Metric Learning (ITML) is one of many methods using pairwise constraint training examples in met-ric learning field [28, 1, 11, 5], and it is formulated as follows: where u and l are upper bound and lower bound for similar sam-ples and dissimilar samples respectively.  X  ij is a safety margin distance for each pair and D ld ( M,M 0 ) is LogDet divergence and D ld ( M,M 0 ) = Tr ( MM  X  1 0 )  X  logdet ( MM  X  1 0 )  X  d where d is the dimension of input space and M 0 is a positive definite matrix.
Triplet constraint is also widely used in metric learning, and it is denoted by R as:
R = { ( x i , x j , x k ) : x i is more similar to x j than to x
Large Margin Nearest Neighbor (LMNN) [27] is one of the most widely used metric learning methods which uses triplet constraint on training examples. The LMNN model is to solve: min s.t. d 2 M ( x i , x k )  X  d 2 M ( x i , x j )  X  1  X   X  ijk where  X   X  [0 , 1] controls relative weight between two terms, and
S = { ( x i , x j ) : y i = y j and x j belongs to the k -neighborhood of x
R = { ( x i , x j , x k ) : ( x i , x j )  X  X  ,y i 6 = y k It is proved to be very effective to learn Mahalanobis distance in practice, and is extended to many methods for different applica-tions [20, 10, 9]. However, LMNN is prone to be overfitting some-times, and it is also sensitive to Euclidean distance when it com-putes neighbors of each sample at the beginning.

In [13], a novel quadruplet constraint was proposed to model similarity from complex semantic label relations, for example, the degree of presence of smile, from least smiling to most smiling. The scheme of quadruplet constraint is as follows:
A = { ( x i , x j , x k , x l ) : d 2 M ( x k , x l )  X  d where  X  is a soft margin. Quadruplet is able to encompass pair and triplet constraint. Pairwise constraint can be represented as ( x , x i , x i , x j ) , and set  X  = l , so d 2 M ( x i , x j dissimilar set; or ( x i , x j , x i , x i ) , set  X  =  X  u , then d ( x , x j ) are from similar set. Similarly, triplet constraint can also be represented as ( x i , x j , x i , x k ) .

To solve the problem of overfitting, many regularizations over matrix M were proposed in the past. In [22], they impose squared Frobenius norm on M , and form an SVM like structure to do metric learning: min s.t. d 2 M ( x i , x k )  X  d 2 M ( x i , x j )  X  1  X   X  ijk where M = A T WA , matrix A is fixed and known, and the diago-nal matrix W is learned.

There are some works imposing low-rank structure on M . The most direct way is let M = L T L , where M  X  R d  X  d , L  X  and k is smaller than d . So, M is a matrix of rank k .

In [16], they proposed a robust structure metric learning method, and used nuclear norm as convex approximation of low-rank regu-larization, and it can be expressed as a convex optimization prob-lem: where X  X  R d is the training set of n points, Y is the set of all permutations over X , C &gt; 0 is a slack trade-off parameter,  X  is a feature encoding of an input-output pair, and  X ( y q ,y ) is the de-signed margin.

In [14], a tighter rank minimization approximation, Fantope reg-ularization, was proposed and imposed on M , and holds an explicit control over rank of M . The formulation is Reg ( M ) = where  X  i ( M ) are k smallest singular values.
In this paper, we are going to introduce a novel low-rank regu-larization based metric learning method, so that we can avoid the problem of overfitting and learn an effective structure from limited training data. As we mentioned in last section, there have been al-ready many different types of regularization over M  X  X  d + ric learning literature. The Frobenius norm regularization proposed by [22] can avoid overfitting, however, the definition of M in this paper restricts the generation of M and it cannot learn correlation between features.

In weakly supervised metric learning, the algorithm does not have access to the labels of training data, and it is only provided with side information which is in the form of pair or triplet con-straints. In this case, the low-rank regularization seems to be an effective way to learn correlations between data. Trace norm (also called as nuclear norm) has been used as the convex relaxation of the rank minimization, however, there still is a gap between trace norm and rank minimization. Because trace norm is the sum of all singular values, if one of the large singular values changes, the trace norm will also change correspondingly, but the rank of the original matrix keeps constant.

Setting M = L T L or imposing Fantope regularization are both explicit way to control the rank of matrix M . The performance could be good if we can find a good fitted rank. However, in prac-tice, we do not know the rank of a matrix accurately, and we have to tune this parameter very carefully, because a small deviation of parameter k from optimal value may have large influence on the final performance. It is a tedious process to select the best k from a large range.

In this paper, we will use the capped trace norm as low-rank reg-ularization [29, 30]. It can be represented as Reg ( M ) = P where  X  is a threshold value. In this regularization, we only min-imize the singular values that are smaller than  X  , and we ignore other large singular values. Thus, when large singular values vary, our regularization behaves the same as low-rank regularization, and keeps constant too. In practical problems, it is difficult to estimate the rank of matrix M , but the  X  value in capped trace norm can be easily decided [4, 8].

Because quadruplet constraint can encompass pair and triplet constraints, in this paper, we use quadruplet constraint to form a new robust metric learning model as:
We will also use this model in the optimization and convergence analysis sections, but the conclusions are the same when we use pair or triplet constraint.
Objective function (9) is non-smooth and non-convex, and it is hard to optimize. In this section, at first, we will use re-weighted method to transform the original objective function to a convex sub-problem, then proximal gradient method is applied to solve this new subproblem. In next section, we will prove that our objective function will converge, and the values of original objective function (9) are non-increasing after each step, and a local optimum value is to be obtained.
 According to the re-weighted algorithm described in [19, 18], let M = U  X  V T and singular value  X  i are in ascending order. We define:
Therefore, original problem (9) can be transformed to: When we fix D , this problem is convex, and we use proximal gradi-ent method to optimize it iteratively. In each iteration, M is updated by performing a subgradient descent, and the subgradient descent of problem (11) with respect to M is: where A + denotes the subset of constraints in A that is larger than 0 in function (11). After each step, M is projected onto the positive semidefinite cone.
 Optimization algorithm of our method is summarized in Algorithm (1) below.
 Algorithm 1 Algorithm to solve problem (9).
 Input: A , X  X  X  d  X  n
Output: M  X  X  d + while not converge do end while
Using the algorithm above, we can solve our original non-smooth and non-convex objective function (9). In this section, we prove the convergence of our optimization algorithm, and a local solution can be obtained in the end. In capped trace norm optimization algo-rithm, the number of thresholded singular values varies for different iterations, thus the convergence proof is difficult.

T HEOREM 1. Through Algorithm 1, the objective function (9) will converge, or the values of objective function (9) are non-increasing monotonically.

In order to prove Theorem 1, at first, we need the following Lem-mas.
 L EMMA 1. According to [23], any two hermitian matrices A,B  X  R d  X  d satisfy the inequality (  X  i ( A ) ,  X  i ( B ) are singular values sorted in the same order)
X
L EMMA 2. Let M = U  X  V T ,  X  i are singular values of M in ascending order, and there are k singular values less than  X  .  X  M =  X  U  X   X   X  V T ,  X   X  i are singular values of  X  M in ascending order, and there are  X  k singular values less than  X  .  X  M denotes the updated parameter after M .  X  is a constant value. So it is true that, where D is defined as (10).

Proof : It X  X  obvious that hence the following inequality holds:
We know there are  X  k singular values of  X  M less than  X  and they are in ascending order, the first  X  k smallest singular values  X   X  less than  X  , thus no matter whether  X  k  X  k or  X  k &lt; k , it holds that:
Combining two inequalities (17) and (18), we get:
Then, summing d X  on both sides of inequality (19), where d is the dimension of matrix M , we are able to get the following in-equality:
As per the definition of matrix D = 1 2 that:
Via Lemma 1, we know that: 1 2
Combining Eq. (21), inequalities (20) and (22), we have:  X  Finally, inequality holds that:
L EMMA 3. Function (11), is convex with domain S d + , and gra-dient of this function is clearly Lipschitz continuous with a large enough constant L . Secondly, positive semidefinite cone is a closed convex cone. As per [2], when we use proximal gradient method and select a proper learning rate, the value of this function con-verges in each iteration.

Right now, we are able to prove Theorem 1 by using the Lemmas above.

Proof : Via Lemma 3, after we use proximal gradient descent method to minimize function (11) in Algorithm 1, it is guaranteed that:  X 
X
Via Lemma 2, we can easily know that: Finally, we combine inequalities (25) and (26) to achieve:
So far, it is clear that the value of our proposed objective func-tion will not increase by using our optimization algorithm, thus we prove the Theorem 1 that our optimization algorithm is non-increasing monotonically. Because M is a positive semidefinite matrix, we know that the objective function (9) is at least larger than zero. So our objective function is also lower bounded. There-fore we can conclude that our optimization algorithm converges, and a local optimum value is to be obtained in the end.
We evaluate our proposed model on different datasets, including synthetic dataset, widely used face recognition datasets, and some other datasets in image data mining. There are two main goals in our experiment: first, we will show that our model is able to outperform the state-of-the-art metric learning methods; second, our proposed capped trace norm is more stable to be applied to solve practical problems than Fantope regularization.
In this experiment, we evaluate our proposed metric learning on a synthetic dataset, and each constraint is quadruplet.

Dataset: We follow the setting of experiment in [14] to generate a synthetic dataset. We define a target symmetric positive semi-definite matrix T  X  S d + , and T = A 0 0 0 , where A  X  random symmetric positive definite matrix with rank ( A ) = e and e &lt; d . Matrix A is a multiplication of one random symmetric matrix and its transpose. So, rank ( T ) = rank ( A ) = e . X  X  R d  X  n is a feature matrix, each element is generated from gaussian distribution in [0 , 1] , and each sample is a feature vector x The Mahalanobis distance between two feature vectors x i and x is given by: d 2 T ( x i , x j ) = ( x i  X  x j ) T T ( x i
To build a training constraint set A , we randomly sample pairs of distance using quadruplets and get the ground truth using d so that:  X  ( x i , x j , x k , x l )  X  A ,d 2 T ( x i , x it denotes that distance between sample pair ( i,j ) is smaller than sample pair ( k,l ) . Training set A is used as training data to learn Mahalanobis metric matrix M . Validation set V and test set T are generated in the same way as A , and they are used to tune parameters and test evaluation respectively.

Setting: In the experiment, we set e = 10 , d = 100 , n = 10 |A| = |V| = |T| = 10 4 . After we learn a metric M , we evaluate these metrics on test set T , and measure accuracy of satisfying the constraints. In this experiment, we compare with four other meth-ods: metric learning with no regularization, metric learning with trace norm regularization and metric learning with fantope regular-ization. Parameter  X  are tuned in the range of { 10  X  2 , 10 and rank of Mahalanobis metric M are tuned from [5 , 20] .
Compared Methods: Because we use quadruplet constraints in this experiment, the general model we use to solve this problem is:
Evaluation Metrics: After learning a metric matrix M from training constraints A , we evaluate it by computing the number of dominant singular values, namely rank ( M ) . Then we test it on test-ing constraints T and compute the accuracy of satisfied constraints.
Results: Table 1 shows the results of our experiment. As we can see, metric learning with Fantope regularization and capped norm regularization performs much better than other two methods. Our method can get a comparable results to metric learning with Fan-tope regularization, and this is the result when we tune parameter k for Fantope regularization very carefully.

Figure 2 represents the accuracies of metric learning with Fan-tope regularization and our method when selection of rank changes. It is obvious that our method always outperforms Fantope regular-ization when we select parameter rank randomly except 10 . Our method performs more stable when we do not have enough time to tune the rank of Mahalanobis metric M . In practice, it common that we do not know the exact rank of a matrix, our method is more applicable to solve practical problems.

In this section, we evaluate our method, pairwise constraints with capped norm regularization, on two challenging face recognition datasets: Labeled Faces in the Wild (LFW) [7] and Public Figures Face Database (PubFig) [12]. In our experiments, we focus on face verification task, namely deciding if two face images are from the same person, and results show that our method can outperform the state-of-the-art metric learning algorithm.
Dataset: The Labeled Faces in the Wild dataset is considered as the current state-of-the-art face recognition benchmark. It contains 13,233 unconstrained face images of 5749 individuals, and 1680 of these pictured people appear in two or more distinct photos in this data set.

There are two different feature representations in this experi-ment, LFW SIFT feature dataset and LFW Attribute feature dataset. We use the face representation proposed by [5], it extracts SIFT de-scriptors [6] at 9 automatically detected facial landmarks over three scales. Each image is a feature vector of size 3,456. To make this dataset tractable for distance metric learning algorithm and save time, we perform principle component analysis to reduce the di-mension, and we select 100 largest principle components in this ex-periment [11]. We also use  X  X igh-level describable visual attributes (gender, race, age, hair color) in [12]. These features of face image are insensitive to pose, illumination, expression and other imaging conditions, and can avoid some obvious mistakes, for example men are confused for women or child for middle-aged. Each image is represented by a vector x  X  R d where d is the number of attributes to describe the image. Each entry in vector x means the score of presence of each specific attribute.

Setting: To compare with other state-of-the-art methods on face verification, we follow the experiment setting in [11]. Data are organized in 10 folds, and each fold consists of 300 similar con-straints (two faces in a pair are from the same person) and 300 dis-similar constraints (two faces in a pair are from different person). The average over results of 10 folds is used as final evaluation met-ric. In the experiment, we an only access pairwise constraints given by similar or dissimilar pairs, and labels or more training data are not allowed. In the experiment, we tune parameter  X  from range [10  X  2 , 10  X  1 , 1 , 10 , 10 2 ] , and parameter rank k of matrix M from { 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 } in LFW SIFT dataset and from { 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 } in LFW Attribute dataset. For other methods, we follow their already tuned parameter setting in [11].

To prove the stableness of our method, we split LFW SIFT fea-ture dataset and LFW attribute feature dataset into 5 folds, and eval-uate our metric learning with capped trace norm model and metric learning with Fantope norm on these datasets. We run experiments 5 times using different training/testing splits and compute mean value and standard variance for validation.

Compared Methods:
Evaluation Metrics: To measure the face verification accuracy for all these compared methods, we report a Receiver Operator Characteristic (ROC) curve. To compare the performance of each method, we compute Equal Error Rate (EER) of the respective method, and use 1  X  EER as evaluation criterion, and the method with the lowest EER, or the highest 1  X  EER is the most accurate one.

Results: We plot ROC curve for each method in Figure 4, and 1  X  EER values are also computed to evaluate their performance. Figure 4a shows the results on LFW SIFT feature dataset. Ma-halanobis distance between two similar pairs performs quite well comparing with Euclidean distance, it increases the performance from 67 . 5% to 74 . 8% . KISSME method is the state-of-the-art method on this feature type and reaches an 1  X  EER at 80 . 06% , it outperforms widely used metric learning methods ITML and LDML. It is clear that pairwise contraint with low-rank approximation meth-ods, Fantope and CAP, perform better than KISSME, and reach 81 . 4% , and 81 . 7% respectively. Our method increases the per-formance of Euclidean distance by 14 . 2% and KISSME method by 0 . 9% . Figure 4b presents the performance of each method on LFW Attribute feature dataset. Our method outperforms KISSME method, and also works better than metric learning than Fantope regularization by 0 . 4% . 3 shows the result samples on this dataset.
We run 5-fold cross-validation experiments on Fantope method and our method. In Figure 5, we plot accuracy result with respect to rank selection. When we select rank parameter k roughly, it is clearly that our method can get better results than metric learning with Fantope regularization. In large scale dataset, it not piratical to tune parameters as carefully as in small dataset, and sometimes, there is not exact rank at all. So, our method is much more appli-cable to this kind of situation, and performs better when we inputs an approximation of matrix rank. ROC curve for different methods on LFW Attribute dataset. Figure 5: Verification accuracy with respect to rank selection on LFW SIFT feature dataset 5a and LFW attribute feature dataset 5b
Dataset: The PubFig database is a large, real-world face dataset consisting of 58,797 images of 200 people collected from the in-ternet. Images in the data set are downloaded from the internet us-ing search query on a variety of image search engines. Compared to LFW, there are larger number of images per person, and more variation on different poses, lighting conditions, and expressions. Served as a complementary to LFW dataset, PubFig dataset con-sists  X  X igh-level X  features of visual face traits that are not sensitive to pose, illumination or other imaging conditions.

Setting: Face verification benchmark dataset consists of 20,000 pairs of images of 140 people. The data is divided into 10 folds with mutually disjoint sets of 14 people each, and each fold contains 1,000 intra and 1,000 extra-personal pairs.

Similar to experiment setting in LFW dataset, to prove stableness of our method, we split Pubfig feature dataset into 5 folds, and eval-uate our metric learning with capped trace norm model and metric learning with Fantope norm on these datasets. We run experiments 5 times using different training/testing splits and compute average and standard variance.
 Compared Methods: We use the same compared methods in LFW experiment.

Evaluation Metrics: ROC figure is plotted for each method and we also compare verification accuracy when we tune parameter k for methods Fantope and Cap.

Results: Figure 6 represents the performance of each compared method on Pubfig dataset. We calculate Equal Error Rate for them, and it is clear that our method outperforms other correlated metric learning methods. By imposing capped trace norm regularization, our method increases the performance of traditional Mahalanobis distance by about 6% , and the state-of-the-art KISSME method by over 1% .

We also perform another experiments to show the process of pa-rameter tuning procedure. In 7, we tune parameter rank k from 5 to 40 , and we can see that our method works more stable than metric learning with Fantope regularization. The performance of Fantope regularization is greatly subject to the selection of rank k . In this figure, we use trace norm regularization as baseline.
In this section, we evaluate our methods on image classification X  and the task is assigning an image to a predefined class. We can also look as this task as object recognition. In the experiments, we use image dataset with attribute features. Attribute is an important type of semantic properties shared among different objects or activities. Figure 6: ROC curves of different methods on the Pubfig datasets. Figure 8: Classification accuracy of each method on Pubfig dataset. It is a representation in a higher level than the raw feature repre-sentation directly extracted from images or videos. In recent years, several attribute datasets are used by various researchers for the study of utilizing attributes for different vision applications. There are two image classification datasets, PubFig dataset and Outdoor Scene Recognition (OSR) dataset.
Dataset: We use a subset face images of Pubfig dataase, and there are 771 images from 8 face categories.

Figure 9: Classification accuracy vs rank k on Pubfig dataset. Figure 10: Results of 5 neareset neighbors when we query an im-age. Green line means this neighbor is in the same class with query image, and red line denotes they are different.

Setting: We use the same experiment setup as [14]. Each person contributes 30 images as training data to learn Mahalanobis metric matrix M and build classifier, other images are used as testing data to evaluate classification performance. We run this experiment 5 times, and 30 images per person in training data are selected ran-domly each time, and average performance is used as evaluation criterion.

Compared Methods:
Evaluation Metrics: In this experiment, we compute classifi-cation accuracy for each method as evaluation criterion.
Results: Figure 8 represents the performance of compared meth-ods. 1NN method using Euclidean distance works really bad on this task, and its accuracy is just 55% . SVM method increases the performance of 1NN method greatly by about 20% . When we use LMNN method to learn Mahalanobis distance for this task, and use 1NN classifier, the accuracy reaches 77% and is better than SVM. We impose low-rank regularization, trace norm, Fantope regularization, and capped trace norm respectively, it is clear that our method works best and increases the performance of LMNN method by about 1 . 5% . Sample query results are presented in Fig-ure 10a, we plot 5 nearest neighbors for each query, and we can find out that our method does a better job than metric learning with Fantope regularization in this task.
 In Figure 9, we shows the performance of metric learning with Fantope regularization and capped trace norm method when we tune parameter rank k . The performance of Fantope regulariza-tion is very sensitive to the choice of parameter rank k , it is ob-vious that sometimes, the performance of Fantope regularization is worse than trace norm regularization. It is clear that when we select k from 40 to 100 , our method performs much stable than Fantope regularization, because it explicitly controls the rank of matrix M to be k . Our method can learn an adaptive threshold in the opti-mization and the rank of matrix M is not necessary to be k . Figure 11: Classification accuracy of each method on OSR dataset.
Figure 12: Classification accuracy vs rank k on OSR dataset.
Dataset: We use Outdoor Scene Recognition (OSR) dataset from [21], and there are 2688 images from 8 scene categories, and it is described by high level attribute features.

Setting: In the experiment, we also use 30 images for each cat-egory as training data, and other images are used as testing data. Each time, we select training data randomly and we repeat this pro-cedure 5 times, and use average accuracy as performance of each method.

Compared Methods: There are six compared methods as in last section, namely KNN, SVM, LMNN, LMNN+Trace norm, LMNN+Fantope and LMNN+capped trace norm.

Evaluation Metrics: Classification accuracy is compute as the performance of each method.

Results: In Figure 11, 1NN method reaches accuracy at about 65% , and SVM method perform a large increase by about 10% . Although the result of 1NN method using Mahalanobis distance learned by LMNN is a little worse than SVM method, LMNN method with low-rank regularization always outperform the result of SVM. We can also find out that our LMNN with capped trace regularization method works better than trace norm and Fantope regularization, and reaches an accuracy at about 76 . 5% . It im-proves LMNN method by about 1% . Figure 10b shows two sample query results of two compared methods. It is a little hard to distin-guish coast and mountain when both of them has sky in the back, it is clear that our method learns a better Mahalanobis distance to do classification and image search.

We also plot figure to compare the performance of LMNN method with low-rank regularization, and we use LMNN with trace norm regularization as a baseline. In Figure 12, it is clear that our method always works better than metric learning with Fantope regulariza-tion. When we make k = 40 , we can find that the performance of Fantope regularization is even worse than the baseline.
In this paper, we propose to use a novel low-rank regulariza-tion, capped trace norm regularization, to impose on metric learn-ing method. Capped trace norm regularization is a better rank mini-mization approximation than trace norm. It works more stable than Fantope regularization and can be seen as an adaptive Fantope reg-ularization as well. We also introduce an efficient optimization al-gorithm, and prove the convergence of our objective function. Ex-perimental results show that our method outperforms the state-of-the-art metric learning methods. [1] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. [2] L. V. EE236C. 6. proximal gradient method. [3] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. [4] H. Gao, F. Nie, W. Cai, and H. Huang. Robust capped norm [5] M. Guillaumin, J. Verbeek, and C. Schmid. Is that you? [6] M. Guillaumin, J. Verbeek, and C. Schmid. Multiple instance [7] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. [8] W. Jiang, F. Nie, and H. Huang. Robust dictionary learning [9] D. Kedem, S. Tyree, F. Sha, G. R. Lanckriet, and K. Q. [10] D. Kedem, Z. E. Xu, and K. Q. Weinberger. Gradient boosted [11] M. Koestinger, M. Hirzer, P. Wohlhart, P. M. Roth, and [12] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. [13] M. Law, N. Thome, and M. Cord. Quadruplet-wise image [14] M. T. Law, N. Thome, and M. Cord. Fantope regularization [15] D. Lim, G. Lanckriet, and B. McFee. Robust structural [16] B. McFee and G. R. Lanckriet. Metric learning to rank. In [17] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric [18] F. Nie, H. Huang, X. Cai, and C. Ding. Efficient and robust [19] F. Nie, J. Yuan, and H. Huang. Optimal mean robust [20] S. Parameswaran and K. Q. Weinberger. Large margin [21] D. Parikh and K. Grauman. Relative attributes. In Computer [22] M. Schultz and T. Joachims. Learning a distance metric from [23] C. Theobald. An inequality for the trace of the product of [24] H. Wang, F. Nie, and H. Huang. Robust distance metric [25] J. Wang, X. Gao, Q. Wang, and Y. Li. Prodis-contshc: [26] J. Wang, A. Kalousis, and A. Woznica. Parametric local [27] K. Q. Weinberger and L. K. Saul. Distance metric learning [28] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance [29] T. Zhang. Multi-stage convex relaxation for learning with [30] T. Zhang. Analysis of multi-stage convex relaxation for
