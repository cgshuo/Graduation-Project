 The task of prediction, estimating an output or response given an input, often involves a statistical model that de-scribes the probabilistic relationship between the input and the response. An elementary way to represent such a rela-tionship is a random field over the input and the response. When the stochastic system under consideration has many variables of interest, rather than just a single input and re-sponse, the random field in turn will have to cover all the variables characterizing the system. Undirected graphical models, or Markov random fields (MRFs), provide an im-portant framework for representing such joint probability distributions, and are thus used in a variety of domains, in-cluding statistical physics, natural language processing, im-age analysis, and spatial statistics, among others. Let X = ( X 1 ,...,X p ) denote a vector of random variables, with each variable X s taking values in a corresponding set X . A Markov random field over X is then a family of prob-ability distributions over X , and is specified by two objects. The first is a graph G = ( V,E ), where V = { 1 ,...,p } is a set of p vertices so that each variable X s is associated with a vertex s  X  V , and E  X  V  X  V is the set of edges of the graph. The main advantage of graphical models is that they represent probability distributions compactly. To see the importance of compactness of representation, sup-pose the variables take values in sets of size k , |X Then the total number of assignments of values to the ran-dom vector X is k p , which is exponential in the graph size p . Thus, for graphical models to be useful, they must re-duce this cost of representing a joint distribution. They do this by encoding in the edges E of the graph a set of con-ditional independence assumptions that any distribution in the family is supposed to satisfy. Termed Markov properties , they require that each variable be conditionally independent of its non-neighbors given its neighbors. For instance, if the graph G has no edges, E =  X  , then each vertex has no neigh-bors so that the corresponding graphical model is a family of distributions where the variables { X s } s  X  V are all inde-pendent. An important consequence of these conditional independence assumptions is provided by the Hammersley-Clifford theorem [1], which states that any positive distri-bution
P over X that satisfies all the Markov properties of a graph G , also factorizes according to the graph. By this is meant the following. Let C be the set of cliques, or fully con-nected components, of the graph G , and let {  X  C ( X C ) } be a set of clique-functions, so that each function  X  C ( X only depends on a subset of the variables { X s , s  X  C } cor-responding to a clique C  X  C . Then, a distribution P that factorizes according to the graph is given as, Note that since these clique functions depend only on a sub-set of the variables, they provide huge savings in the cost of representing probability distributions. We had noted that a graphical model distribution is completely specified by two objects, with the first being the graph G . This set of clique functions is the other characterizing object.
 There are two main classes of tasks in the Markov random field framework. The first, naturally, is to estimate the MRF distribution from data. This in turn has two subclasses of tasks, one for each of the two objects that specify an MRF distribution. The first is to estimate the clique functions, also called features , from data. This subtask is thus called feature estimation . The second is to estimate the underlying graph G = ( V,E ) of the MRF from data. This subtask is often called structure learning . Finally, given the features and the graph structure, which then completely specify an MRF distribution, we arrive at the task of inference , which is to answer queries about the probability distribution rep-resented by the MRF. Some basic inference tasks are as fol-lows.
 Computing the log partition function: The partition func-Event probability estimation: This is the most natural Computing upper and lower bounds: Some applications of the event probabilities. However, inference under general MRF settings is so hard that even computing constant factor approximations is intractable. One so-lution for this is to compute rigorous upper and lower bounds for the event probabilities, so that we obtain an interval in which the true event probability lies. Such an interval serves as an additive guarantee.
 Inference given moments: This is a setting where we are given only partial information about the MRF distri-bution. The task is to estimate event probabilities given just the expected values (moments) of the set of feature functions.
 Estimating the MAP configuration: Given an assign-ment of values to a subset of the random variables, the maximum a posteriori or MAP configuration is the most probable assignment of values to the rest of the variables. Again, estimating the MAP configuration is intractable for discrete MRFs under general settings. In this thesis [2], we address all the tasks listed above; the five inference tasks, the structure learning task, and finally the feature estimation task. Together these greatly lighten the load on any domain expert, who is required to merely list the random variables of the system. Given data, the procedures detailed in this thesis, as well as allied procedures in the literature, can then be used to construct a graphical model, and perform efficient approximate inference on those estimated models.
 Inference: (a) To approximate the log partition function, we propose preconditioner approximations . The general approach of approximate inference techniques is to  X  X roject X  the intractable graphical model to a space of tractable models, and perform inference with the projected X  and tractable X  X odel. The divergence characterizing this projection is typically a Kullback-Leibler diver-gence or its approximations. Our preconditioner ap-proximations on the other hand use a new divergence that we call the graphical model condition number . (b) To estimate general event probabilities, we propose variational Chernoff bounds and variational Chebyshev-Chernoff bounds. These involve extending the classi-cal Chernoff and Chebyshev bound framework, which apply to independent and identically distributed ran-dom variables, to the graphical model setting. These provide not just approximate estimates, but rigorous upper and lower bounds for general event probabili-ties. The Chernoff bounds require the complete spec-ification of the distribution, whereas the Chebyshev bounds require just the expected values or moments of the set of feature functions of the MRF. (c) To compute the MAP configuration, we propose a quadratic programming (QP) relaxation. The state of the art tractable algorithms for the MAP problem were previ-ously based on a linear programming (LP) relaxation, or its dual, instead. While counterintuitive, the QP provides huge savings in computational cost over the LP, due to an order of magnitude reduction in the number of variables. If each of the p variables have arity k , the LP then has O ( | E | k 2 ) variables while our QP has only O ( nk ) variables.
 Structure Learning: For structure learning, we inves-tigate procedures based on edge-appearance parameteriza-tions and ` 1 -regularized regression. Typical approaches for structure learning involve searching through the combinato-rial space of graphs and selecting the graph with the highest score according to some metric. For undirected graphical models, both of the steps involved, computing the score, and searching through the space of graphs is intractable. Using ` regularization on the other hand, we transform the search over the discrete space of graphs to a real-valued convex optimization problem, which is thus tractable. The guar-antee on our estimate is probabilistic; we show the proce-dure consistently estimates the true graph even under  X  X igh-dimensional X  scaling where the number of samples could be merely logarithmic in the number of variables.
 Feature Estimation: For feature estimation, we propose additive conditional random fields (aCRFs), a subclass of graphical models which allow efficient nonparametric esti-mation of feature functions from data given the structure, and sparse additive models (SpAM), a class of models which allow simultaneous variable selection and feature estimation from data.
 The dissertation owes all to my thesis advisor John Lafferty. Important parts of the thesis were also based on collabora-tions with Han Liu, Martin Wainwright and Larry Wasser-man. I X  X  also indebted to my thesis committee: John Laf-ferty, Carlos Guestrin and Eric Xing of Carnegie Mellon Uni-versity, and Martin Wainwright of University of California, Berkeley. [1] S. L. Lauritzen. Graphical Models . Oxford University [2] P. Ravikumar. Approximate inference, structure learn-
