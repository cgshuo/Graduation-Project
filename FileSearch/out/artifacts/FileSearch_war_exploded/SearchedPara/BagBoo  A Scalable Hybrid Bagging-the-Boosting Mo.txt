 In this paper, we introduce a novel machine learning ap-proach for regression based on the idea of combining bag-ging and boosting that we call BagBoo .Our BagBoo model borrows its high accuracy potent ial from Fried man X  X  gradi-ent boosting [2], and high efficiency and scalability through parallelism from Breiman X  X  bagging [1]. We run empirical evaluations on large scale Web ranking data, and demon-strate that BagBoo is not only showing superior relevance than standalone bagging or boosting, but also outperforms most previously published results on these data sets. We also emphasize that BagBoo is intrinsically scalable and paral-lelizable, allowing us to train order of half a million trees on 200 nodes in 2 hours CPU time and beat all of the com-petitors in the Internet Mathematics relevance competition sponsored by Yandex and be one of the top algorithms in both tracks of Yahoo ICML-2010 challenge. We conclude the paper by stating that while impressive experimental evalu-ation results are presented here in the context of regression trees, the hybrid BagBoo model is applicable to other do-mains, such as classification, and base training models. I.5.1 [ Models ]: Statistical Algorithms, Experimentation
The goal of our paper is two-fold: introduce a novel highly scalable machine learning algorithm inspired by bagging and gradient boosting ideas and demonstrate its utility for the learning-to-rank task. In particular, we show how BagBoo model blends bagging and boosting in a bagged ensemble of boosted trees, and how from that it gains an advantage over both of them: In particular, we conduct experiments on public large-scale data sets from LETOR [4] as well as data from Yandex and Yahoo-sponsored ranking challenges and demonstrate that the hybrid BagBoo model is superior to both pure bagging and boosting in ranking quality accord-ing to DCG and MAP metrics, as well as outperforms in many cases the best published results on these data sets. We further show that BagBoo is much faster than boosting, pri-marily because BagBoo typically gets its best performance on short boosted sequencies that are wrapped into bagging. Bagging is intrinsically parallelizable and while boosting is not, having a short boosted sequence to learn on a single processor is still a doable task. This finding underscores the offline time efficiency and scalability.

There has been previous work on combining bagging and boosting, mostly dealing either with different settings, or applications, or not making the simultaneous scalability and model accuracy argument for the learning-to-rank problem. Here we emphasize three most notable contributions in the past literature on the topic we are aware of. Firstly, BagBoo can be viewed as an extension of bagging [1], in which a short sequence of boosted trees is used as the basic element of the bagged ensemble. Next, our BagBoo model is similar in flavor, to the additive groves of regression trees [5], where the authors grow and back fit sequences of boosted trees, before voting them as a bag. Finally, in the recent KDD Cup X 09 competition [7] that dealt with the prediction of customer churn, the combination of bagging and boosting similar to ours has been shown to compete well against other models.
The rest of our paper is organized as follows: in section 2 we give a succinct introduction to the bagging and boost-ing ideas, and introduce the combined BagBoo model and the algorithm for its learning. In section 3, we describe the ranking application and the data used in our experiments, and in section 4 we discuss experimental results that provide empirical verification of BagBoo  X  X  advantage over standard bagging, boosting as well as state-of-the-art relevance algo-rithms. We conclude the paper in section 5.
The idea of bagging is to create an ensemble of models by sampling the training data without replacement, and voting the resulting models. Breiman [1] calls all methods in which every model in the ensemble is a function of the independent identically distributed random vectors, a random forest. He further outlines that the random forest family has a number of attractive properties: variance reduction, resistance to overfitting and effective parallelizable computation.
Gradient boosting invented by Friedman [2] has a flavor in many respects similar to that of bagging: it creates an en-semble of base learner models and sees significant training speed increase, approximation accuracy improvement and stability to overfitting resulting from the introduction of bagging-like randomness. The main difference, however, is that each subsequent model is trained in a fashion depen-dent on all previously obtained models. To make things more precise, in boosting, the least squares error between the additive ensemble of N models (of which the first N  X  are fixed and the N -th is optimized) and the target value is iteratively optimized. This procedure, while optimizing the concrete target of interest, suffers from being hard to parallelize.

By combining the good properties of both models we move away from simple tree models that are typically dealt with in random forests and inhale the power of gradient boosted trees in them by making every random forest model be a gradient boosted tree. At the same time, we alleviate the parallelization issues attributable to boosted trees by keep-ing the boosting sequence short, e.g. 10-20 base trees each, which is much lower than thousands of trees typically used in standard gradient boosting. This way, every boosted tree is quite powerful and can be learned fast on a single proces-sor node, and many such boosted trees are later averaged in a bagged, random forest like, paradigm.

To make things more concrete, lets review the algorithm: 1. Input: training data D ; NBag and NBoo iterations 2. Output: Random Forest of NBag x NBoo trees 3. for i =1 to NBag do 4. D [ i ]:= SampleData ( D ); # samples both data 5. BT [ i ]:= BoostedT ree ( D [ i ] ,NBoo ); # NBoo 6. endfor 7. Output additive model
In line 1, we list the inputs to the algorithm which include the training data D to run regression on, and the NBag and NBoo parameters that define the number of bagging and boosting iterations respectively. In line 2, we define the output of BagBoo to be a random forest of NBag by NBoo trees. The trees are learned in the for loop that goes in line 3from1to NBag , every time samplin g the data records and features (line 4) and learning a boosted sequence BT [ i ] of trees of length NBoo on the resulting data sample in line 5. After the loop is done, we return the sum of all BT [ i ] trees.

Thus, the only real and substantial change from bagging is that we bag boosted models, it allows the resulting model to be quite a bit more powerful as we will demonstrate in the experimental section 4. The only big change from boosting is that we reduce the number of boosted iterations while wrap-ping bagging around it which allows for substantial training time savings. We also employ the idea of sampling features in addition to sampling the data records that works quite well for decision trees allowing them to learn various  X  X s-pects X  of the data.

Interestingly, wrapping the other way around, i.e. creat-ing the ensemble of boosted bagged models, while possibly resulting in some time saving due to still existing possibility of parallelizing inner bagging iterations, will not be as effi-cient as BagBoo as the outer boosting process still needs to wait for the previous iteration to complete before starting the new one. Further, bagging of the inner classifiers make them stronger which is not something that is desirable for the external boosting process that typically requires weak classifiers.

In the following sections, we provide empirical comparison between the models.
The experimental data was chosen with one main goal in mind: to provide for the most comprehensive comparison of BagBoo model to the main available state-of-the-art meth-ods whose performance may be obtained from the widely ac-cepted published results. In pursuing this goal, we chose to use public standardized data sets: TD2004 from LETOR 3.0 and MQ2007 from LETOR 4.0 [4]. A variety of experimental results with different algorithms were reported on these data sets before, which makes them a widely used and acknowl-edged test-bed for experimental evaluation of the ranking methods. We also tested BagBoo in recent Yandex and Yahoo-sponsored ranking challenges.

The main parameters of the data sets, including the num-ber of queries, rows, features, labels, and the label distri-bution are summarized in table 1. The data sets provide nice coverage in terms of main characteristics of interest: the number of queries varies from below 100 in TD2004 to almost 10 , 000 in IMAT09, the number of rows in all cases is close to 100 , 000, the number of features varies from below 50 in MQ2007 to almost 250 in IMAT09, the label cardi-nality is binary for TD2004, tertiary in MQ2007 and cov-ering pretty much the continuous range from 0 to 4 for IMAT09, while, finally, the label distribution ranges from being severely skewed towards irrelevant in TD2004 to hav-ing over a quarter of relevant judgement in the training data of IMAT09. The biggest advantage of using MQ2007 and TD2004 is represented by having pre-folded data for cross-validation and tools for accuracy assessment, which makes the comparison to other algorithms more reliable [4]. IMAT09 and ICML10 datasets were released by Yandex and Yahoo respectively and are available for free download at the time of print.

For model comparison we used DCG , NDCG and MAP metrics that are described in detail in [4] and are omitted here for brevity.
Before embarking on the large scale experiments, we ran an experiment on a small data set called Concrete from the UCI repository in order to support our hypothesis that for fixed total number of trees T = NBag  X  NBoo ,thebest tradeoff between the offline model training time and the ac-curacy may be achieved when neither of NBag or NBoo equals 1, in other words, doing BagBoo with a non-trivial boosting sequence makes sense.

The data set at hand has 1030 records over 7 numeric features, and the regression task for it is to predict the com-pression strength of the concrete in megapascals. The target values fall in the range [2 . 33 , 82 . 6] with a mean 35 . 82 and standard deviation 16 . 71. For a naive baseline, using the mean as a predictive model results in mean squared error ( MSE ) of 278 . 89, while using a single regression tree in 10 fold cross-validation yields an MSE of 39 . 79. Figure 1: Model accuracy (mean squared error) and offline/modeling time as a function of NBag and NBoo for fixed T = NBag  X  NBoo
We fixed T = 1000, and varied NBoo on a logarithmic scale from 1 to 1000, derived NBag = T/NBoo and mea-sured both mean-squared error obtained by each of these models in 10 fold cross-validation as well as the time it took us to obtain the model. All experiments were simulated on a single machine, and appropriately normalized, i.e. we as-sumed that all bagging runs could be parallelized and would take roughly same time each, and we did not account for network latency and other factors that may have affected the performance should the data be distributed.

The results of this study are presented in Figure 1. The x -axisonthisfigureis NBag , implying the value NBoo = T/NBag . Thus,theoriginonthe x -axis corresponds to pure boosting, while NBag = 1000 corresponds to pure bagging, with all x values in between corresponding to some version of BagBoo .The y -axis on the figure measures both offline modeling time in seconds and the difference in mean-squared error ( MSE ).

Note, that boosting takes the longest time, close to 8 sec-onds on average to obtain the model, while bagging is the fastest (around 1 second), but, to trade this off, there is a significant difference in MSE  X  X oostingperformsmuch better than bagging. The best MSE we see is around 16 which is much better than naive predictions quoted above. BagBoo comes in as winner with the modeling time negli-gibly higher than that of bagging for pretty much any bag size greater than 20, and the best overall MSE performance achieved for NBag =50and NBoo = 20, beating the plain boosting. The results of this experiment lead us to conclude that the model performance close to that of boosting or bet-ter can be achieved by the BagBoo approach times faster than by plain boosting, and almost as fast as bagging. This effect is largely due to the parallelization abilities inherited from bagging.

Now we turn to the large scale experiments on learning-to-rank data sets and show that BagBoo is one of the top performing models among the modern ranking algorithms and across a variety of data sets.

On TD2004 and MQ2007 data sets, we ran the model training for each of the 5 folds on the training partition, with a quality control performed on the validation parti-tion and results assessed on the test partition of a given fold. During the model training and quality control steps, we adjusted various parameters of BagBoo .Inparticu-lar, the maximum number of leaves in each individual base tree model was 8, the feature sampling rate was 75% uni-form without replacement, the shrinkage factor [2] was equal to 0 . 1, NBoo = 250 and NBag = 4500 for the total of T = NBoo  X  NBag =1 , 125 , 000 tree models. The large number of models possible to train in a couple of hours time on 200 nodes is a direct result of BagBoo  X  X  high scalabil-ity. The quality assessment on the test partition consisted of computing metrics NDCG @1  X  5aswellas MAP .The final numbers presented in tables 2 and 3 represent the av-erages of these metrics across the folds. Numbers in bold font represent the winning method for the metric in a given column.

Tables 2 and 3 show that despite the fact that BagBoo is a pointwise method, i.e. it doesn X  X  explicitly optimize the ranking over documents, it outperforms the formidable list-wise approaches, such as ListNet [3] and BoltzRank [6]. It is also the best overall model in quality based on the experi-ments we conducted, where it looses only once to RankBoost and RankSVM on the MQ2007 data set in metric NDCG @1. Boosting with complexity equal to that of BagBoo , i.e. fit-ting the same number of trees, could not compete with BagBoo in any of the runs as it would take estimated 48 days to get the results for it, which is far out of practicality realm. The ranking performance of pure bagging which is given in the tables was noticeably lower than for any of the winning algorithms.

The BagBoo method also won (unofficially as an insider) the Internet Mathematics competition under id Joker held by Yandex in 2009. The impressive parallelization capability of BagBoo lead us to building 20 , 000 bags of 20 boosted trees for a whopping total of 400 , 000 trees in a winning combination. On a 200 node cluster, the whole process took about 2 hours, including the network latency and the map-reduce process to train the models and collect the results. The BagBoo model won the 3 places in both tracks of Yahoo-sponsored ICML-2010 contest, where the reciprocal rank was a final scoring metric, and scored the first and the second with respect to the nDCG metric.
In this paper, we presented the method for combining bag-ging and boosting into a hybrid approach we called BagBoo . While similar ideas were discussed in the recent literature, our particular setup and extensive experimental results on the learning-to-rank data sets are novel. The main motiva-tion for this research came from the observation that typ-ically boosting outperforms bagging but suffers from being hard to parallelize. The BagBoo model fills this gap nicely: not only that it is comparable to boosting in terms of qual-ity or even sometimes better but it is also straightforward to parallelize given that the main wrapper algorithm is bag-ging. We use this ability to scale the training to learn nearly half a million trees to dominate in the Internet Mathematics ranking competition. We also use over 1 . 1 million trees to win in all but one metric on TD2004 and MQ2007 data sets. In both cases, the training took only couple hours on the cluster of 200 machines, while boosting would take orders of magnitude longer provided it were able to fit this many trees in memory at once at all. The experimental results we pre-sented in the paper also confirm that BagBoo is one of the top to-date known ranking methods that, being pointwise itself, still outperforms the best known pointwise, pairwise and listwise methods.

It is worth a final note that, while we focus in our experi-ments on the tree based models, the whole setup is general-izable to non-decision tree base learners.

Overall, BagBoo appears to be in an attractive position of being among the top most accurate ranking models, and, at the same time, enjoying the benefits of high scalability which we hope will make it a valuable tool in the IR community. [1] L. Breiman. Random Forests. Machine Learning , [2] J. H. Friedman. Greedy function approximation: A [3] T.-Y. Liu. Learning to rank for information retrieval. [4] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: [5] D. Sorokina, R. Caruana, and M. Riedewald. Additive [6] M. Volkovs and R. Zemel. BoltzRank: Learning to [7] J. Xie, V. Rojkova, S. Pal, and S. Coggeshall. A
