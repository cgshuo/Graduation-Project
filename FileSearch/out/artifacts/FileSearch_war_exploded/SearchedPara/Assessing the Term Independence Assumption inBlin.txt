 } When applying blind relevance feedback for ad hoc docu-ment retrieval, is it possible to identify, a priori , the set of query terms that will most improve retrieval performance? Can this complex problem be reduced into the simpler one of making independent decisions about the performance ef-fects of each query term? Our experiments suggest that, for the selection of terms for blind relevance feedback, the term independence assumption may be empirically justified. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval X  Relevance feedback General Terms: Experimentation Keywords: pseudorelevance feedback, automatic query ex-pansion
Blind relevance feedback (e.g., [3, 4, 2]) is a common in-formation retrieval technique that has been shown, on aver-age, to increase retrieval performance. Because this strat-egy hurts performance in some cases, the ability to predict whether or not a set of feedback terms is beneficial remains an open research question that has important implications (cf. recent NRRC RIA workshop). If we could differentiate  X  X ood X  from  X  X ad X  terms, the overall gains from blind rele-vance feedback could be further boosted. Our study explores the term independence assumption in blind relevance feed-back. Can we individually predict the performance effect of each feedback term, or are we hampered by interactions be-tween multiple terms? The answer appears to be yes and no , respectively. The quality of each feedback term can be de-cided independently, suggesting that the selection of  X  X ood X  terms for blind relevance feedback can be recast as a binary classification problem on each individual term. Our experiments involved 150 topics from TREC-6 through TREC-8, and were performed with Indri 1 [1]. Candidate feedback terms were gathered by using the description field http://www.lemurproject.org/
Table 1: Performance of blind relevance feedback of the topics as queries and collecting the twenty highest scoring terms from the top twenty Indri hits using a simple tf.idf measure; no stemming was employed in these experi-ments. The baseline results from adding all the candidate feedback terms are shown in Table 1. 2 For each topic, we then ran twenty separate queries, one with each of the candi-date feedback terms and the original query. The difference between the mean average precision of original query and the one-off query was calculated; we called this difference the marginal MAP gain (MMG), since it quantified the ef-fect of adding that additional term. If a term has positive MMG, we considered it  X  X ood X ; otherwise, it was a  X  X ad X  term.

Since the goal of this study was to explore the term inde-pendence assumption, we first established an upper bound on performance. If an oracle told the system the MMG of each candidate feedback term, and it only added terms that were  X  X ood X , how well would our system perform? The re-sults of this experiment are shown in Table 1. As we can see, if a system were able to correctly choose  X  X ood X  terms, the improvement in MAP would be more than three times that of simply adding all feedback terms.

Although these numbers are encouraging, they tell us noth-ing about the potential interactions between multiple query terms. For the simplest case of two term interactions, this can be quantified by comparing the MMGs of all possible two-off queries against the sum of individual term MMGs. This scatter plot is shown in Figure 1. The distribution can be well-fit by a straight line with a slope of 0.78, which means that for pairs of terms, marginal MAP gains are addi-tive with a constant discount factor. Of the 28,500 possible queries, there were only 315 cases where the MMG sums in-correctly predicted increased performance (those points in the fourth quadrant).

To truly assess the validity of the term independence as-sumption, we needed to compare the performance achieved
By simple parameter tuning, we established 0.4 as the op-timal weight for feedback terms. Figure 1: Scatter plot shown two-term interactions. Figure 2: The validity of the term independence assumption. by the optimal set of query terms and the performance ob-tained by independently making decisions about each query term. Naturally, the optimal set of feedback terms can only be determined by considering the power set of all candi-dates, which, for twenty terms, is unrealistic. However, since the previous experiment suggested that adding two  X  X ood X  terms is unlikely to decrease performance, we might approx-imate the optimal set by exhaustively searching the power set of only  X  X ood X  terms. Due to the exponential nature of power sets, even this was impractical. Thus, we considered topics that had 16 or fewer  X  X ood X  terms, which yielded results for 138 topics. In total, 826,759 queries were exe-cuted by Indri in this experiment. Overall, we discovered that adding all the  X  X ood X  terms yielded a performance im-provement in MAP that was, on average, 87% that of the op-timal power set approximation. For 43 of the topics, adding all the  X  X ood X  terms was the best combination. Figure 2 shows these results graphically. Each bar represents a topic, with the shorter bar indicating MAP gain under the term independence assumption, and the longer bar the power set results. These numbers appear to suggest that term X  X erm interactions are not a significant cause for concern in select-ing terms for blind relevance feedback.

To further study the term independence assumption in the context of blind relevance feedback, we explored the effect of Figure 3: Cumulative MAP gain by number of  X  X ood X  terms added. adding different numbers of  X  X ood X  terms. Do most of the improvements in performance come from only a few terms? Is there a point of diminishing returns in adding more terms? To answer these questions, we first sorted the  X  X ood X  feed-back terms for each topic by their MMGs, in descending order. We iteratively added each individual term and plot-ted the change in mean average precision X  X ssentially, a cu-mulative distribution. These results are shown in Figure 3. Obviously, as the number of feedback terms increases, the number of data points decreases; for example, only 62 out of 150 topics have 10 or more  X  X ood X  terms. The error bars denote the 95% confidence intervals 3 and reflect the decreasing sample size as the number of terms increases. It is interesting to note that the MMG for adding all 20 feed-back terms (0 . 0154) is worse than adding only the best term (MMG=0 . 0236, 95% confidence interval=  X  0 . 0057).
Our study suggests that the problem of choosing the opti-mal set of feedback terms can be approximated by individual binary decisions about each term independently X  X hat the term independence assumption in choosing feedback terms may be empirically justified. This reformulation of the prob-lem immediately suggests a classification approach based on machine learning techniques.
Omitted for 19 and 20 terms because the data points are too few to give meaningful intervals.
