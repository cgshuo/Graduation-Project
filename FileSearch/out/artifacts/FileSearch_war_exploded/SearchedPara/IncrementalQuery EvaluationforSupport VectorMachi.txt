 Supp ort vector machines (SVMs) have been widely used in multimedia retriev al to learn a concept in order to  X nd the best matc hes. In such a SVM activ e learning environmen t, the system  X rst processes k sampling queries and top-k un-certain queries to select the candidate data items for train-ing. The user's top-k relev ant queries are then evaluated to compute the answ er. This approac h has shown to be e X ec-tive. However, it su X ers from the scalabilit y problem associ-ated with larger database sizes. To address this limitation, we prop ose an incremen tal query evaluation technique for these three types of queries. Based on the observ ation that most queries are not revised dramatically during the iter-ative evaluation, the prop osed technique reuses the results of previous queries to reduce the computation cost. Fur-thermore, this technique takes advantage of a tuned index structure to e X cien tly prune irrelev ant data. As a result, only a small portion of the data set needs to be accessed for query processing. This index structure also provides an in-expensiv e means to process the set of candidates to evaluate the  X nal query result. This technique can work with di X eren t kernel functions and kernel parameters. Our experimen tal results indicate that the prop osed technique signi X can tly re-duces the overall computation cost, and o X ers a promising solution to the scalabilit y issue.
 H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al| Query Formulation, Retrieval Mod-els, Search Process, Sele ction Process.
 Algorithms, Performance, Design, Experimen tation. Supp ort Vector Machines, Activ e Learning, Multimedia Re-trieval, Relev ance Feedbac k.

In this section, we discuss the prop osed query evaluation technique. Speci X cally , we  X rst presen t index construction and tuning to facilit y e X cien t query evaluation in Section 2.1. Then, we discuss in details our incremen tal query eval-uation technique in Section 2.2. Our index structure is constructed and tuned as follows:
Hierarc hical Clustering : A hierarc hical clustering tech-nique, similar to the R*-tree [1], is used to organize the entire image database into a hierarc hical tree structure.
Information Augmen ting : We traverse the tree in a postorder fashion. In an original R*-tree, an internal node contains an array of node entries. Each node entry is a pair ( mbb , node-id ), where mbb is the minim um bounding box (MBB) that spatially contains the MBBs in the child node, with node-id as the child node address. In our index structure, each node entry is extended to be a tuple ( mbb , node-id , imageID-r ange ), where imageID-r ange refers to the range of image identi X cations contained in the pointed child node and imageID-r ange  X   X  1 ; j S j  X  where j S j is the cardinal-ity of the whole image database.

Index Tuning : The design of most existing hierarc hical index structures (e.g., R*-tree) usually overlooks the di X er-ences between sequen tial and random accesses. Since the disk pages allocated to sibling nodes are often not physi-cally consecutiv e (typically a disk page contains only one node), a query may incur a large number of random accesses even for each feedbac k iteration. To reduce the number of disk random accesses, we use the Hilbert curve [6] for disk page allocation. Speci X cally , we create a tuned index struc-ture as follows: we traverse the non-tuned index structure in a breadth- X rst fashion, and then create a tuned index structure with the disk page allocation almost following the traversal order except for the children nodes in the same node. For the children nodes in the same node, we allocate them to the disk in the order of the Hilbert curve values of their centers.
We discuss our query processing technique E VALUA TE Q UER Y ( Q ) on top of the above index structure for the three types of queries (i.e., k sampling queries, top-k uncertain query , and top-k relev ant queries) used in SVM activ e learning. In this paper, we focus on the two-class SVM activ e learn-ing. Given a data set X that consists of vectors in a metric space M . Among X , the training data set is denoted as X t = f x t; 1 ;  X  X  X  ; x t;n g with the corresp onding labels Y t = f y 1 ;  X  X  X  ; y n g , where y i 2 f X  1 ; 1 g . The testing data set is denoted as X u = f x u; 1 ;  X  X  X  ; x u;m g , and X u = X n X t . During the query-concept learning phase, SVM typically transfers X t from M into a feature space F , and deriv es a hyperplane separating the relev ant training instances (i.e., those with the label 1) from irrelev ant ones (i.e., those with the label -1), and achieving the largest margin [2]. The weights W = f  X  1 ;  X  X  X  ;  X  n g associated with X t are deter-mined accordingly . Those testing instances with  X  i &gt; 0 are called supp ort vectors , and they are in fact the closest points to the hyperplane. The class mem bership of a test-ing instance x u;j can be predicted by the following function: function. If S ( x u;j )  X  0, x u;j is classi X ed as +1; otherwise, -1. In fact, a top-k relev ant query is to retriev e k instances with the largest values of S in X , and a top-k uncertain query is to retriev e the k instances with the smallest abso-lute values of S in X u . Note that the query cost is the sum of disk seek (including cylinder seek and rotation), data trans-fer and CPU time, in which seek time dominates the total query cost. Figure 2 lays out our query processing technique, designed to minimize the disk I/O cost.

For k sampling queries, we just need to retriev e the root node (in line 1) that contains all possible image IDs. If k is relativ ely small, we sample instances from k di X eren t leaf nodes in order to make the sampled instances as represen-tativ e as possible (in line 6). Otherwise, random sampling can be performed to retriev e the query result (in line 8).
For top k uncertain queries, we  X rst determine the positiv e and negativ e supp ort vectors in X u (in line 12). Speci X cally , the positiv e supp ort vectors are those testing instances with  X  i &gt; 0 and y i = 1 while the positiv e supp ort vectors are those with  X  i &gt; 0 and y i =  X  1. Top k uncertain query aims to retriev e the k instances in X u closest to the hyperplane. These instances typically lie between positiv e supp ort vec-tors and negativ e supp ort vectors. Therefore, a bounding box B that covers both positiv e and negativ e supp ort vec-tors has a high probabilit y to cover the desired query result. After deriving B , we can partition B into multiple ranges (in line 14) to eliminate the empt y space by some partitioning strategies, such as Equi-Coun t, Equi-Area, Min-Sk ew and Min-Ov erlap. The Equi-Coun t partitioning strategy creates ranges containing roughly the same number of instances. The Equi-Area partitioning strategy creates ranges having the same area. The Min-Sk ew partitioning strategy divides B into ranges such that each range contains uniformly dis-tributed instances. The Min-Ov erlap partitioning strategy creates ranges that have minimal overlaps among them. We can adopt any of the above partitioning strategies. In our ex-perimen ts, we have implemen ted the Equi-Area partitioning strategy . After a set of ranges is determined, we can avoid some unnecessary range queries by eliminating those ranges within the ranges of previous queries(in line 15). Such incre-mental strategy works because of the observ ation that most queries are not revised dramatically during the iterativ e eval-uation. Of course, this strategy incurs memory overhead be-cause we need to bu X er the previous results in memory . Con-Figure 4: Disk accesses of k sam-pling queries
In this section, we evaluate the e X ectiv eness of the pro-posed query processing technique describ ed in Section 3. Our dataset consists of more than 68,040 images from the COREL library . There are a total of 37 visual image fea-tures. The COREL images have been classi X ed into distinct categories by domain professionals, and each category con-tains about 100 images. For each chosen category , 50% of images were used as the training data. We chose LIBSVM [3] with the Gaussian kernel for SVM learning. The node size of the original R*-tree and our index structure were both set to 4KB, and both had three levels in our experimen-tal settings. We compare the performance of the prop osed query processing technique on top of our index structure (denoted as TNEW ) against the existing technique with R*-tree (denoted as TOLD ). Speci X cally , our metho ds to evaluate k sampling queries, top-k uncertain queries, top-k relev ant queries are from line 4 to 10, 11 to 17, and 18 to 25 in Figure 2, respectiv ely. The existing coun terparts are prop osed in [4], [5], and [5], respectiv ely. The experimen ts were performed on a 3.4-GHz Pentium IV-based computer with 1.5GBytes of RAM, and the results are averaged over 100 runs.
 Figures 4 to 6 show that TNEW signi X can tly outp erforms TOLD for answ ering three types of queries in terms of disk accesses with di X eren t k 2 f 5 ; 15 ; 20 ; 25 ; 35 ; 50 g , and the performance gap widens as k increases. Clearly , the total number of disk access increases as k increases. As shown in Figure 4, TOLD performs about  X ve times more disk ac-cesses than TNEW when k = 5, 58 times when k = 25, and 120 times when k = 50 for k sampling queries. This  X gure shows that TNEW is indep enden t of the number of sample points (i.e., k ) because TNEW just needs to access the root node of our index structure, resulting in only one disk access for answ ering a sampling query . On the other hand, TOLD is prop ortional to k , which is because TOLD has to traverse the R*-tree to obtain sample points one by one, incurring almost 3 disk accesses per sample point. If a sampling point is not met the criteria, another traversal is needed. For top-k uncertain queries, TOLD performs about twice disk accesses compared to TNEW when k = 5, and about three times when k = 50 (see Figure 5). For top k relev ant queries, TNEW performs up to about three times better (see Fig-ure 6). More importan tly, the curves of TNEW are quite low and  X  X t, indicating that it can supp ort a large k with much less disk I/O overhead. The performance di X erence between
