 The problem of assessing the significance of data mining re-sults on high-dimensional 0 X 1 data sets has been studied extensively in the literature. For problems such as mining frequent sets and finding correlations, significance testing can be done by, e.g., chi-square tests, or many other meth-ods. However, the results of such tests depend only on the specific attributes and not on the dataset as a whole. More-over, the tests are more difficult to apply to sets of patterns or other complex results of data mining. In this paper, we consider a simple randomization technique that deals with this shortcoming. The approach consists of producing ran-dom datasets that have the same row and column margins with the given dataset, computing the results of interest on the randomized instances, and comparing them against the results on the actual data. This randomization technique can be used to assess the results of many different types of data mining algorithms, such as frequent sets, clustering, and rankings. To generate random datasets with given mar-gins, we use variations of a Markov chain approach, which is based on a simple swap operation. We give theoretical results on the efficiency of different randomization methods, and apply the swap randomization method to several well-known datasets. Our results indicate that for some datasets the structure discovered by the data mining algorithms is a random artifact, while for other datasets the discovered structure conveys meaningful information.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms, Management, Experimentation Significance testing, randomization tests, 0 X 1 data, swaps Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.
One of the most important considerations in data min-ing is deciding whether the discovered patterns or models are significant. While traditional statistics has long been considering the issue of significance testing, in data mining people have been less interested in the theme.

The framework of hypothesis testing in statistical data analysis is very well developed for assessing the significance of individual patterns or models. The methods are typi-cally based either on analytical expressions or randomization tests. However, often they are not well-suited for assessing complex results of data mining, such as clusterings or pat-tern sets.

In this paper we consider the use of swap randomiza-tion [5] for assessing data mining results on 0 X 1 datasets. The basic idea of swap randomization is as follows. Given the dataset D , create random datasets with the same row and column margins D , run the data mining algorithm on those, and see if the results are significantly different on the real data than on the randomized datasets. If not, then we presume that the results are really due to the row and column margins, and not due to interesting relations in the data. The datasets with the same margins as the original one are generated by swaps , as shown in Figure 1: take two rows u and v and two columns A and B of the data table with u ( A )= v ( B )=1and u ( B )= v ( A ) = 0, and change the rows so that u ( B )= v ( A )=1and u ( A )= v ( B )=0. This operation maintains the row and column sums of the dataset, and all datasets with the same row and column sums can be reached through a series of swaps [5].
Thus swap randomization is an extension of traditional randomization methods. For instance, a chi-square test for assessing the significance of frequent itemsets is an analyti-cal technique based on studying the distribution of datasets with given column margins, but the row margins are allowed to vary. Similarly, methods that randomize the target value in prediction tasks keep the column margins fixed, but they do not impose any constraint on the row margins. A moti-vating example of why it is important to maintain also the row margins is given in the next section.

Swap randomization has been considered in various appli-cations; see, e.g., the survey paper by Cobb and Chen [5]. The problem of creating 0 X 1 datasets with given row and col-umn margins has theoretical interest of its own; see, e.g., [1, 7]. Generating contingency tables with fixed margins is a problem that has been studied in statistics (see, e.g., [4]). Randomization methods in general form a large research area [8].

Our contributions in this paper are twofold: (i) we de-scribe the algorithmic aspects of swap randomization when applied to large data sets, and (ii) we show how this method can be applied in the data mining setting. In more detail, we give a description of several different ways of generating random matrices with given margins and discuss their per-formance. Swap randomizations can be performed efficiently and can be applied to reasonably large datasets, as our ex-periments show. We give extensive empirical results showing that some well-known datasets appear to have very little in-teresting patterns or cluster centers, while other datasets have lots of structure.

The rest of this paper is organized as follows. In Section 2 we present an overview of the swap randomization method, and in Section 3 we discuss the applications of the approach to specific data mining tasks. Section 4 describes how the random matrices with given marg ins are generated and gives results on the performance of the algorithms. In Section 5 we describe the experimental results. Section 6 discusses related work, and Section 7 gives some concluding remarks.
In this section we give an overview of the method, explain the intuition behind it, describe the algorithmic challenges it poses, and show how it can be applied to testing signifi-cance of results obtained by different kinds of data mining algorithms.
Let D denote a 0 X 1 matrix with m rows and n columns that represents our dataset. Assume that we are interested in assessing the result obtained by a particular data mining algorithm A on input D .Let A ( D ) denote the result of the algorithm. For simplicity, assume that it can be described by a single number. For instance, for frequent set mining algorithms, it can be the number of sets whose frequency ex-ceeds a certain support threshold. Similarly, for a clustering algorithm, it can be the error of the clustering solution. In our randomization approach we generate k datasets D ,...,D k , such that each D t , t =1 ,...,k ,isan m  X  n 0 X 1 matrix that has the same row and column sums as the original matrix D ; each dataset D t is assumed to be a uni-form and independent sample from the space of all m  X  n 0 X 1 matrices with the given margins. Then the algorithm A is executed on each sampled dataset D t , yielding results X t = A ( D t )for t =1 ,...,k .
 Now, the significance of the result A ( D ) of the algorithm A on the data D is tested by comparing it against the set X = { X 1 ,...,X k } of the results of A on the sampled datasets. If the result of the algorithm on the original data does not deviate significantly from the values in X , then the result A ( D ) is not surprising and its significance is small.
Assuming that the sampled datasets are independent and that k is large enough so that X gives an approximation of the real distribution, then the empirical p -value of X A ( D )is i.e., the fraction of the random datasets in which we see a value more extreme than the value in the real data.
Another measure for quantifying the significance of the value X 0 is captured by the Z score where b X = E [ X 1 ,...,X k ] is the empirical mean of the set X and b  X  2 = Var [ X 1 ,...,X k ] is the empirical variance. Large values of Z indicate that X 0 deviates a lot from the mean of the results obtained on the random datasets.
As mentioned in the introduction, randomization is widely used as a significance testing method. For example, in con-trol studies in medical genetics it is customary to estimate the interestingness of discovered patterns by a permutation test. In such a test the variable describing whether a patient belongs to the case or the control group is permuted ran-domly, and the original data analysis is repeated. The find-ings on the real data are accepted only if they are stronger than on, say, 99% of the randomized datasets.

However, in many data mining tasks the goal is not to predict a single variable. For example, pattern discovery and clustering look at the structure of the whole data set. One could of course think of randomizing each column of the dataset independently, but this method ignores some of the structure of the dataset.

As an example, consider the datasets D 1 and D 2 in Fig-ure 2. In both datasets variables X and Y are positively correlated, and the itemset { X, Y } occurs more often than the independence assumption would imply. As the columns of X and Y are the same for both datasets, any measure of the importance of the association between X and Y that takes only the columns of X and Y into account will give the same results for D 1 and D 2 . However, in dataset D 1 X and Y co-occur in all types of rows, whereas in dataset D 2 the co-occurrence of X and Y happens exclusively in very dense rows. Thus, in D 2 the high frequency of the pair { X, Y not due to some specific property that binds X with Y , but rather to the fact that X and Y tend to occur on rows that have lots of 1 X  X .
 Indeed, consider the dataset E 1 containing 10 copies of D ,and E 2 containing 10 copies of D 2 .Thecolumnsfor X and Y are still same in both datasets, and in both cases the frequency of the pair is 60. When we generate 1000 random datasets with the margins of E 1 the maximum and average frequencies of { X, Y } were 59 and 52.4, and the standard deviation 2.5; thus, all values were smaller than 60, yield-ing an empirical p -value of 0.001. For E 2 the corresponding numbers are 69, 63.2, and 2.0; and in only 70 cases the fre-quency was 60 or less, giving an empirical p -value of 0.07. Thus, we can conclude that in E 1 the pair { X, Y } is strongly overrepresented, while in E 2 it occurs slightly less often than one would expect. This indicates that the context of the pair of variables can have a strong effect on the significance of the frequency of a pair.
 Figure 2: Examples of two 0 X 1 datasets, D 1 and D 2 . In both cases we are interested in the correlation between columns (attributes) X and Y .Thesignifi-cance of the correlation result might depend on the overall context of the dataset
The above example demonstrates the basic concept under-lying swap randomization: it takes the bias of row and col-umn counts into account by randomizing over datasets with the same row and column margins as the original dataset. As a result the notion of interestingness we consider is con-ditional to the knowledge of the marginal sums. We are interested in assessing information in the dataset that is not conveyed by the marginal sums of the data table.

As an example, consider a dataset whose row sums satisfy a power law. This finding can be interesting, but it does not offer any additional information about the structure of the dataset. Using swap randomization one can assess whether a quantity of interest is not immediately implied by the power-law marginals, and thus common to all datasets with the same margin distributions.
The technical challenge in the above approach is to gener-ate random 0 X 1 datasets with given row and column sums. This problem has been studied extensively in statistics [4, 5], theoretical computer science [1, 7] and in application ar-eas [11, 14].

In this paper we use a Markov chain approach to the prob-lem of sampling. Starting from the original dataset, we make a small random local move, which interchanges a pair of 1 X  X  with a pair of 0 X  X  and does not change the row and column sums. Such a local move is called a swap , and a sequence of swaps is performed until the data mixes sufficiently enough and a random sample is obtained.

This Markov chain thus consists of datasets with the given margins; two datasets are adjacent, if there is a swap that changes one to the other. The Markov chain is reversible, i.e., a swap can be undone by a single (reverse) swap. How-ever, the chain is not regular, i.e., some datasets (states) have more neighbors than others. This implies that the sta-tionary distribution of the chain is not the uniform distribu-tion. Therefore, a straightforward application of swapping does not guarantee uniform sampling.

The problem of non-uniformity can be fixed in at least two ways: ( i )byusingthe Metropolis-Hastings algorithm [9, 13], which is a well-studied method for converting a Markov chain with stationary distribution  X  to another one with stationary distribution  X  ,and( ii ) by adding self-loops in order to guarantee that all states have the same degree.
For applying the Metropolis-Hastings algorithm, one needs to compute the degree of any given state of the chain, that is, the number of all valid swaps for a given 0 X 1 matrix. We give a simple formula for computing the degree at each state, and we show how to maintain this quantity incrementally. The complexity of incremental maintenance of the state degree is O (min { m, n } ) for an m  X  n matrix, making the algorithm somehow inefficient. On the other hand, adding self-loops does not require computing any additional expensive infor-mation; so while more steps are needed for convergence, the time complexity of each step is, in expectation, constant, making it a very efficient algorithm in practice.
In this section we describe how the swap randomization framework can be applied to different data mining tasks, such as finding frequent sets and correlations, clustering, and spectral analysis of datasets. Our methodology allows us to investigate the significance of the patterns that exist in a given dataset, at different levels of granularity.
First, we are able to characterize the significance of global aspects of the dataset. If the number of frequent sets, or the number of highly correlated pairs contained in the dataset is not significant with respect to that found in a randomly rearranged dataset, then we can conclude that the dataset does not contain any interesting global structure of frequent sets, or of highly significant correlations.

Additionally, we can also look at individual itemsets. In this case we are interested in identifying itemsets whose fre-quency is smaller or larger in the sampled datasets when compared with the original dataset. If the frequency of an itemset drops in the sampled dataset, it is implied that the frequency can not be explained by the margins of the dataset. If the frequency increases, a possible explanation is that the items in the itemset are anti-correlated in the original dataset.

The above observations apply also when mining simple association rules. Recall that the accuracy (confidence) of arule( X  X  B ) is defined to be f ( XB ) f ( X ) ,where f ( XB )and f ( X ) are the frequencies of X  X  X  B } and { X } , respectively. Assume now that X is a singleton set. Since f ( X )re-mains fixed, the confidence of the rule is proportional to the frequency f ( XB ). Therefore, the significance of the rule ( X  X  B ) is determined by the significance of the pair {
X, B } . Due to this observation and space constraints we omit further discussion on association rules.

Swap randomization can be applied to testing the signifi-cance of clustering results. Given a clustering algorithm like k -means, and a target number of clusters k , simply compare the clustering error in the original dataset with the cluster-ing error in the sampled datasets. If the difference is large, then one can deduce that the dataset has meaningful cluster structure. This simple approach turns out to yield very clear results on synthetic datasets with known cluster structure.
A different notion of global structure is captured in the singular values and vectors of the data matrix. The singular vectors capture the linear trends in the dataset. The corre-sponding singular values capture the strength of the linear
Figure 3: A swap in the graph representation G D . trend, that is, the tendency of the rows or columns to align with the corresponding singular vectors.

In randomly generated data, the strongest linear trends should be determined by the degree structure of the dataset. This is usually the first singular value. The remaining dataset has no structure; thus we expect the remaining singular val-ues to be small. If the original data contains some linear structure, then the top singular values (especially the non-principal ones) should be higher than those of random sets with the same margins.
We now describe the process of sampling a matrix from the space of all m  X  n 0 X 1 matrices with given margins.
Let D be a 0 X 1 dataset with m rows and n columns. We denote by r i the sum of the i -th row of D , i =1 ,...,m ,and by c j the sum of the j -th column, j =1 ,...,n .Anequiv-alent way to represent the input matrix D is as a bipartite graph G D =( R, C, E )with | R | = m and | C | = n .Vertex i  X  R corresponds to the i -th row of D ,vertex j  X  C corre-sponds to the j -th column of D ,and( i, j )  X  E if and only if D ( i, j ) = 1 for all i and j . The degrees of the vertices of the graph are r i for i  X  R ,and c j for j  X  C .

The main idea is to start from the graph G D correspond-ing to the original data set and perform a local swap that leaves the margins unchanged. When many such swaps have been performed the resulting graph can be considered as a random dataset drawn randomly from the stationary distri-bution.

In more detail, a local swap can be defined by four vertices, i , j , k ,and l of G D , such that i, k  X  R and j, l  X  C ,and is then formed by updating the edges as follows.
 that is, we remove the current edges { ( i, j ) , ( k, l ) add new edges { ( i, l ) , ( k, j ) } . Visually, a local swap is de-picted in Figure 3 for the graph representation and in Fig-ure 1 for the matrix representation.
 Formally, a local swap is a step on the a Markov chain M = {S , T} , where the state space S is the set of all graphs with the given degree sequences, and T is the set of transi-tions defined by swaps. In other words, the set T contains all pairs of graphs ( G, G ) such that it is possible to obtain G from G (or vice versa) by performing a local swap.
Algorithm 1 shows a straightforward implementation of this Markov approach.

Finding the next transition ( G, G )  X  X  from graph G , that is, performing line 3 of Algorithm Na  X  X ve , is not a com-pletely straightforward task. The simplest way is to pick a pair of edges in G , reject if the edges are not swappable, Algorithm 1 Na  X  X ve Input: Graph G D , number of random walk steps k n Ouput: Graph G withthesamedegreesequencesas G D 1: G  X  G D 2: while k n &gt; 0 do 3: G  X  Find adjacent ( G ) 4: G  X  G 5: k n  X  k n  X  1 6: end while 7: Return G and repeat until finding a pair of swappable edges. This is shown in Algorithm 2. Alternatively, one could store all swappable pairs in a structure, and select one uniformly at random. The selection process becomes faster, but there is additional cost of updating the data structure at each step. Algorithm 2 Find adjacent Input: Graph G Ouput: Graph G that differs from G in exactly one swap (i.e., ( G, G )  X  X  ) repeat until ( i, l )  X  E ( G )and( k, j )  X  E ( G )
E ( G )  X  E ( G ) \{ ( i, j ) , ( k, l ) } X  X  ( i, l ) , ( k, j )
Given graph G , the Algorithm Find adjacent generates a graph G uniformly at random among all graphs G such that ( G, G )  X  X  . The reason is that each such graph G can be set at an one-to-one correspondence with a pair of swappable edges X  X he non-swappable edges can be ignored. Algorithm Find adjacent clearly samples uniformly at random from the set of swappable pairs: each swappable pair is sampled with probability proportional to 2 / | E | 2 .

Now, in order for the Markov chain to sample graphs uni-formly at random from the set S , the following conditions have to hold: 1. The state space S is connected under the transitions 2. M has uniform stationary distribution. 3. Starting from G D a sufficiently large number of local
Connectedness: The Markov chain is connected. One can go by any state of the chain to any other state via local swaps [5].

Uniformity: First notice that the Markov chain M is re-versible. Now, for each graph (state) G  X  X  we define d ( G ), the degree of the Markov chain M at G ,tobethenum-ber of different graphs (states) G such that ( G, G )  X  X  . From the theory of the Markov chains, it is well known that the stationary distribution of a reversible chain is propor-tional to the degree at each state in the underlying transition graph. Therefore, in order to obtain a uniform distribution, all states of the Markov chain must have the same degree. A simple construction shows that this is not true in general for the Markov chain M . Therefore, the Na  X  X ve Algorithm 1 does not converge to the uniform distribution.

Mixing time: The mixing time of the Markov chain we defined above has been the object of theoretical study [5], but without any conclusive results. It is estimated, that running the chain for a number of steps in the order of the number of 1 X  X  in the matrix is sufficient for convergence. We do not deal with the theoretical aspects of converge, but we study it empirically in the experimental section.
The straightforward application of the Markov Chain ap-proach does not produce uniform sampling. There are two ways to fix this bias and obtain uniform distribution. The first way is by adding self-loops, as it is shown in Algo-rithm 3. Algorithm Self loop works as Na  X  X ve does. It sam-ples pairs of edges until it finds a swappable pair. The differ-ence with Na  X  X ve , however, is that in Self loop all steps are counted and they decrease the counter, thus, non-swappable pairs of edges are counted as self-loops. The reason that Self loop leads to uniform distribution is that when self-loops are counted the degree of each G  X  X  becomes fixed and equal to | E | 2 . Each pair of edges, swappable or non-swappable, contributes one to the degree of all states. Algorithm 3 Self loop Input: Graph G D , number of random walk steps k s Ouput: Graph G with the same degree sequences as G D 1: G  X  G D 2: while k s &gt; 0 do 3: Select edges ( i, j ) , ( k, l )  X  E ( G ) 4: if (( i, l )  X  E ( G )and( k, j )  X  E ( G )) then 5: E ( G )  X  E ( G ) \{ ( i, j ) , ( k, l ) } X  X  ( i, l ) , ( k, j ) 6: end if 7: k s  X  k s  X  1 8: end while 9: Return G
The second way of sampling from the uniform distribu-tion is by using the Metropolis-Hastings algorithm [9, 13], which is a standard method of converting a Markov chain with stationary distribution  X  to another Markov chain with stationary distribution  X  .Inourcase  X  ( G )  X  d ( G )andwe want  X  ( G )  X  1, so the Metropolis algorithm becomes as shown in Algorithm 4. For some swap that takes the algo-rithm from state (graph) G to state G if the state G has higher degree then the algorithm performs the swap with probability d ( G ) d ( G ) . The algorithm assumes knowledge of the degree d ( G ) for each graph G  X  X  . We will discuss soon how d ( G ) can be computed.
 Algorithm 4 Metropolis-Hastings Input: Graph G D , number of random walk steps k m Ouput: Graph G with the same degree sequences as G D 1: G  X  G D 2: while k m &gt; 0 do 3: G  X  Find adjacent ( G ) 4: G  X  G , with probability min { 1 , d ( G ) d ( G ) } 5: k m  X  k m  X  1 6: end while 7: Return G
We now analyze the running time of the algorithms. We will prove some results on the complexity of the approaches, including a result characterizing the degree of a state in the Markov chain of the datasets. The conclusion in this section is that the Self loop algorithm is always more efficient than the Metropolis-Hastings algorithm.

First, we assume that we can sample edges in constant time and we can test if a pair of edges is swappable or not in constant time. The former task can be performed by keeping all edges in an array, while the latter task can be performed by keeping in memory the data D in the matrix form, or by storing all edges in a hash table.

The running time of Find adjacent is a random variable and it depends on the number of swappable edges for each graph (state) G . Recall that the number of swappable pairs of graph G is d ( G ). Therefore, the probability of finding a the stationary distribution of Algorithm Na  X  X ve at G .Thus, the expected running time of Algorithm Find adjacent is Notice that |T| / |S| = O ( | E | 2 ), since the degree of each graph G in S is at most | E | 2 . One the other hand, the following Lemma is immediate.

Lemma 1. For bipartite graphs G =( U, V, E ) in which the maximum degree is o ( | E | ) , we have |T| / |S| = X ( Proof. Notice that the random walk leaves the degrees at each vertex unaffected in all states. Given any state (graph G )in S , consider an edge ( i, j )  X  E ( G ). Any other edge ( k, l )  X  E ( G ) can be swapped with ( i, j ) unless either l  X   X ( i )or k  X   X ( j ) (or both), where  X ( i ) are the neighbors of i in the bipartite graph. Thus, the number of edges that should be excluded from swapping with ( i, j )is o ( | E | ing a total number of at least ( | E | 2  X  o ( | E | )  X | pairs. Since each state in S has degree  X ( | E | 2 ), the lemma follows.

Corollary 1. For bipartite graphs G =( U, V, E ) whose degree distribution follows power law with  X &gt; 2 we have |T| / |S| = X ( | E | 2 ) .
 Proof. For simplicity assume that | U | = | V | = n .For power laws with exponent  X &gt; 2wehave | E | = O ( n )in expectation and the maximum degree is n 1  X   X  1 = o ( n )(e.g., see [15]). Thus, the conditions of Lemma 1 are satisfied.
The above results imply that for some important classes of datasets  X  such as graphs with bounded degrees or degrees that follow a power law distribution  X  X he expected time T of the Find adjacent algorithm is constant. Thus, for those classes of data, the running time of Algorithm Na  X  X ve is T T
F  X  k n = O ( k n ). Similarly, for the Self loop algorithm the overall running time is T S = O ( k s ). Furthermore, the expected time spent in each state for performing self-loops (before moving out to a new state) is constant.
 We now turn to the running time of Metropolis-Hastings . This running time can be written as T M = T 0 D + k m ( T T
D ), where T F is the running time of Find adjacent , T D the time need to compute d ( G )giventhat d ( G )isalready computed, and T 0 D is the time needed to compute d ( G )for the first time. Next we explain how to compute d ( G )and how to update the computation for d ( G ). The time needed for the update is linear time with respect to min { m, n }
Theorem 1. Let G =( U, V, E ) be a bipartite graph repre-sented as a binary matrix D with m = | U | rows and n = | columns. Let r i be the  X  X eft X  degree of node i  X  U , c j  X  X ight X  degree of node j  X  V , and define M = DD T .Then, the number of graphs G that are yielded with one local swap from G is equal to where is the number of disjoint pairs of edges, is the number of  X  X  X  structures and K 22 ( G )= is the number of K 2 , 2 cliques of G .
 Proof. Each disjoint pair of edges is swappable unless it is part of a  X  X  X  structure. In each K 2 , 2 there are 2 disjoint pairs of edges and 4  X  X  X  X , but there are no swappable pairs, so we should add 2 to bring the count to 0.
 Corollary 2. Given graphs G and G such that ( G, G )  X  T , d ( G ) can be calculated from d ( G ) in time O (min { Proof. Without loss of generality assume that min { m, n } m , and we are using the m  X  m matrix M = DD T .Oth-erwise we can use the n  X  n matrix M = D T D .Using Equation (2) we have Graphs G and G differ only by one swap. so, matrices D ( G )and D ( G ) differ only in four positions, and matrices M ( G )and M ( G ) differ only in two rows and two columns. Therefore  X  Z can be computed in constant time and  X  K 22 can be computed in O ( m )time.

We note that the Metropolis-Hastings still needs to run the Find adjacent algorithm for finding a candidate swap pair. Although the way that the algorithm moves between states is different (neighboring states are not chosen uni-formly at random), and thus it may guarantee faster con-vergence, we believe that this probably does not offset the additional cost incurred by the computation of the degrees, and thus we prefer to experiment with the Self loop al-gorithm. We note that it may be possible to maintain the number of swappable pairs in linear time, thus eliminating Abstracts 128820 25335 10449902 0.32 Abstracts 128803 5918 7150992 0.94 the cost of the Find adjacent algorithm. However, this is non-trivial task, and it still does not guarantee that the Metropolis-Hastings algorithm would be faster. Recall also that in many cases, the cost of Find adjacent is con-stant in expectation.
We perform experiments with many of the well known datasets used in the data mining community. A description of the datasets we are using is as follows: Abstracts con-tains document X  X ord information on a collection of project abstracts submitted for funding by NSF. Abstracts is a pruned version of Abstracts , where we keep only words of medium frequency (only words with frequency counts be-tween 200 and 8854 are kept). Courses is a student X  X ourse dataset of courses completed by the Computer Science stu-dents of the University of Helsinki. Retail is a market-basket dataset collected in a Belgian supermarket [2]. Kosa-rak is a click-stream dataset from a Hungarian news web-site. Finally, Pa l e o , the smallest dataset, contains informa-tion of species fossils found in specific paleontological sites in Europe. Exact information of the datasets, including the sizes and the density of 1 X  X  are shown in Table 1.
We have tested extensively the convergence properties of the swap-based Markov chain for various datasets. Design-ing diagnostics for the convergence of a Markov chain is far from easy and it is an open research question, so our tests can provide only evidence that the chain has mixed and by no means do they constitute a proof.

An example among the many tests we have performed is showed in Figure 4. For each of our datasets, we measure the number of frequent itemsets for a given threshold. The y -axis in Figure 4 shows the number of frequent itemsets in the sampled datasets, divided by the number of frequent itemsets in the original dataset. The x -axis shows the num-ber of steps in the Markov chain scaled by the number of 1 X  X  in the corresponding dataset, i.e., position x = i shows asampleafter iL steps, where L is the number of 1 X  X  in the corresponding dataset. We see that in almost all cases the chain mixes quite rapidly: already after L steps (4 L in the case of Kosarak ) the number of frequent sets has stabilized.
Similar kind of convergence evidence was obtained for all our measures: frequencies of specific itemsets, number of correlations above a certain thre shold, clusteri ng errors, etc. In all of our experiments shown below we have run the chain with very large steps in order to ensure convergence. Additionally, the swaps can be performed quite efficiently. Table 2 shows the running time for the different datasets, using a modest Perl implementation on a 3GHz Pentium machine with 2GB of memory. The reported times is for Figure 4: Convergence: x -axis: the number of steps (  X  the number of 1 X  X  in the data); y -axis: the num-ber of frequent itemsets in the sampled datasets, divided by the number of frequent itemsets in the original dataset.
 Table 2: Running times needed to perform swap randomization on the different datasets. We report the clock time (in secs) needed to perform a number of swaps equal to 5 times the number of the 1 X  X  in the dataset. obtaining one sample after performing 5 L swaps. In most cases much smaller steps can be used.
In this section we describe the results of frequent itemset mining using swap randomization. Table 3 shows the num-ber of frequent sets for the datasets described in Section 5. We compute the collections of frequent sets in the original data, in random data under swap randomization and in ran-dom data under independent permutations of columns (i.e., only column margins are maintained). The collections are denoted by F , F s and F p , respectively. The minimum sup-port thresholds were chosen so that the number of frequent sets is not exceedingly large. Frequent items, i.e., frequent sets of size one, are omitted from the table since they do not change by swapping or permuting the columns. In the case of swapped and permuted datasets, we show the mean values (and standard deviations) of 500 randomized versions of the datasets.

Table 3 clearly demonstrates the differences between the randomization methods. All datasets seem to contain many interesting frequent sets when compared to the frequent sets in the corresponding datasets with permuted columns. The sizes of the frequent set collections were always consider-ably smaller in the permuted data than in the original data. On the other hand, different datasets show very different behavior with respect to swapping. In Abstracts and Re-tail the number of frequent sets remain about the same un-der swap randomization, whereas in Abstracts , Courses , Table 3: The number of frequent itemsets. | X | :the minimum cardinality of the itemset we include to the count. |F| : the number of frequent sets of car-dinality at least | X | in the original data; |F s | :the expected number of frequent sets in swapped data; |F p | : the expected number of frequent sets in ran-dom data with the same column margins as the orig-inal data. The values in parentheses are the stan-dard deviations. The expectations and standard de-viations were computed on 500 experiments. and Pa l e o the numbers decrease significantly. Finally, there is a considerable increase in the number of frequent sets in Kosarak .

Interpreting the results, we can conclude that the struc-ture captured by frequent itemsets in Abstracts and Re-tail can be attributed mainly to the row and column mar-gins, and thus it is preserved in random datasets where the margins are preserved. On the other hand, in the datasets Abstracts , Courses and Pa l e o the structure captured by frequent sets is more interesting, since it disappears un-der swap randomization.
 The increase in the number of frequent sets in the case of Kosarak implies that many sets of items are anti-correlated with each other. A possible explanation for this phenomenon lies in the structure and origin of the data. The row and column margins are highly skewed in Kosarak .Theanti-correlations can be interpreted by the nature of the dataset. Kosarak consists of anonymized click-stream data from a Table 4: Changes in the collections of frequent sets. D : the dataset; F : the frequent itemset collection in the dataset; F s : the frequent itemset collection in the swapped dataset; |F X  X  s | |F| : the fraction of itemsets that are preserved in the collection; |F\F s | |F| :thefrac-tion of frequent itemsets that disappear from the collection. The values involving swapped data are expectations on 500 experiments. The values in the parentheses are the standard deviations. news portal: the link structure of the websites can cause negative correlations between groups of pages.

Although the number of frequent itemsets is indicative of the structure that is contained in the data, it is not informa-tive with respect to what are the actual itemsets contained in the collections, and how the collections relate to each other. It may well be the case that collections have about the same size yet they are completely disjoint.

We now describe how the collections change under swap randomization. Table 4 shows the average fraction of item-sets that are preserved or disappear, compared to the origi-nal collection. For the datasets Abstracts , Kosarak , Re-tail where the size of the collection remains relatively stable (or in the case of Kosarak increases), the mean fraction of preserved itemsets is around 70%, confirming our intuition that the original collection did not contain much interesting structure. This is especially pronounced in the case of the Retail data, where on average 88% of the frequent item-sets are preserved. For the remaining datasets, the mean fraction of preserved itemsets drops below 9%.

The swap randomization can be used also to suggest un-expected sets in the data, i.e., sets that are very frequent in the original data but very rare in the swapped data. For ex-ample, the { dissertation, doctoral, promising } is com-moninthe Abstracts data (support 682) but rare in the corresponding swapped data (mean support 2.4). Similarly, the set { differential, equations, partial, solutions } has support 679 whereas its mean support in swapped data is less than 0 . 4. The most  X  X ull X  itemset is the set { address, result } with supports 691 and 691 . 6, respectively.
We now study how correlations between items change un-der swap randomization. Computing all pairwise correla-tions between the columns in the data tables is computation-ally expensive for most of our datasets. Instead we focus on the k columns with the highest column degree, for k = 100. The rationale is that items that appear frequently are usu-ally more interesting and we want to study their correlations. Furthermore, this allows for an interesting comparison with the randomization technique that permutes columns inde-pendently. Since the column counts are large, our experi-ments give an indication as to how the row counts affect the significance of the pair.
 Table 5: Statistics for correlation values. A row of type max contains the value of the largest correla-tion, while a row of type, say  X  0 . 01 ,containsthe number of correlation pairs with value greater than 0.01. The empirical p statistic in all the above re-sultsis1%.

Table 5 shows our results for different datasets. We com-pute the maximum and minimum correlation values, as well as the number of pairs whose correlation exceeds a certain threshold, for different thresholds. We present the values for the original data, as well as the mean value, and the z -value for both the swapped and the independently permuted data. The statistics are taken over 100 different samples.
From the results we make the following observations. As expected, when randomizing the dataset, strong correla-tions, either positive or negative, tend to disappear for both methods of randomization. However, the way that this is done differs between the two methods. For the inde-pendent permutation model co rrelations concentrate very sharply and almost symmetrically around zero. For swap randomization negative correlations disappear in a much faster rate, e.g., for Retail and Abstracts they disap-pear almost completely. On the other hand, the number of Dataset kE mean std Zp
S1 10 1777.3 3669.9 11.1 170.43 0.01
S2 10 4075.4 4084.4 11.6 0.77 0.22
Courses 10 17541.6 24405.1 30.2 227.09 0.01
Pa l e o 10 1040.7 1401.7 4.8 74.74 0.01
Retail 10 23920.9 24086.0 135.2 1.22 0.10 Table 6: Results on Clustering. k :numberofclus-ters used in k -means, E : clustering error in the origi-nal dataset, mean: mean clustering error in the sam-pled datasets, std: standard deviation of the cluster-ing error in the sampled datasets, Z :distanceof E from mean measured in standard deviations, p :em-pirical p -value. positive correlations remains relatively high, indicating that to some extend the correlations in the dataset (especially low correlations) can be explained by the row and column degrees. This become especially clear when one compares the mean values for the swap and the independent model in the retail and the abstracts datasets. On the other hand, for the Courses and Abstracts datasets we observe that positive correlations drop significantly faster.
Our results on assessing the clustering structure of datasets are showed in Table 6. We perform our clustering experi-ments using Matlab X  X  k -means default function. We obtain results only for the small-and medium-size datasets X  X ur largest datasets cannot be clustered by Matlab X  X  k -mean function. For each dataset, we measure the clustering er-ror for the original dataset for clustering with k =10and k = 20, which is showed in the third column of Table 6. Then we sample 100 sampled datasets, which we cluster with the same parameters. We compute the mean and standard deviation of the clustering error in the sampled datasets, which are shown in the fourth and fifth columns of Table 6. The sixth column ( Z ) reports the distance in standard de-viations between the error in the original dataset and the mean error in sampled datasets. Finally, the last column records the empirical p -value as described in Section 2.1.
The datasets S1 and S2 are synthetically generated datasets with 1000 points, 20 dimensions, and 10 embedded clusters having 100 points each. The difference is that S1 has 10% noise (probability of fliping a bit in the matrix), while S2 has 45% noise, that is, its clustered structure has been hugely corrupted by noise. We see that our results indicate that S1 has indeed clustered structure, while this is not the case for S2. For the real datasets, we see again that all the datasets have clustered structure except from the Retail dataset.
We compute the top-20 singular values for the random-ized sets, and we compare the average value of each singular value with the corresponding one of the original dataset. We observed that in most cases, the first singular value of Figure 5: Singular values of the original dataset and of sampled datasets (mean out 100 samples) for syn-thetic datasets. The crossing point of the two lines at position 10 corresponds to the number of clusters planted in the data. the random datasets is relatively large, comparable to the first singular value of the original data. The explanation is that the first singular value captures the linear trend that is defined by the degree sequences. In contrast, the non-principal singular values are significantly smaller in the ran-dom datasets. Thus, we conclude that swap randomization destroys the linear trend in the data.

Since the sum of the singular values is equal in the origi-nal and the swapped data, and since the first singular value in the original data is larger than the first singular value in the swapped data, there should be a  X  X rossing point X  (see Figure 5). This actually suggests an interesting heuristic for estimating the correct number of dimensions when project-ing to low-dimensional spaces. The index of the first singu-lar value that is significantly lower than the corresponding random one is probably a good indicator that the structure contained in the remaining singular vectors is no more in-teresting than that contained in a random matrix.
We observed that in many cases this crossing point has a meaningful interpretation in the data. For example, the Pa l e o data is conjectured to contain three clusters and the crossing point for this data is indeed at position 3. We experiment further with the above idea on synthetic data, in which we can plant a known number of clusters. Figure 5 shows that for a dataset with 10 clusters the crossing point is at position 10, for noise levels of 10% and 30%, that is, with in a large range of noise.
Defining the significance of discovered patterns has at-tracted a lot of attention in data mining. In one of the first papers, Brin et al. [3] considered measuring the significance of associations via chi-square test. A lot of other measures have been proposed to capture the interestingness of pat-terns, e.g., see [10, 12, 20]; a comprehensive presentation and comparison of such methods can be found in [18].
The problem is also very well studied in statistics, and there is a significant amount of work for sampling from the space of contingency tables [4, 5, 6, 17] as well as several studies that give asymptotics on the exact number of such tables, e.g., [19]. A good survey on the topic is provided by Chen et al. [4].

In theoretical computer science the subject has drawn at-tention in the context of providing bounds for the mixing of the Markov chain. Very recently Bez  X  akov  X  a et al. [1] gave a polynomial time algorithm for sampling binary 0 X 1 matrices with given margins. The algorithm is based on a different Markov chain than the one based on swaps.

The problem of generating random matrices with fixed margins has also been studied in many application areas, such as ecology [16] and biology [11], and analysis of complex networks [14].

Finally we remark that it is possible to generate directly random datasets that do not preserve exact row and col-umn sums but on expectation. This involves setting each entry ( i, j ) equal to 1 with probability r i c j /L , and equal to 0 otherwise. This expectation model has the drawback that the fraction r i c j /L has probability interpretation only if max { r i ,c j } X  L . Experimenting with this model we found that it gives similar results as the swap method, but some-times it is slightly inaccurate. For instance, such inaccura-cies were observed in the Kosarak dataset in which both the row and column sums follow power-law distribution. Ad-ditionally, the savings in running time are not significant.
We have studied the algorithmic properties of swap ran-domization, and described how it can be used in assessing results of data mining algorithms. We gave an algorithmic treatment of the swap randomization method, showing some results on the computation of the number of immediately reachable states in the Markov chain, and we showed that the Self loop algorithm is always more efficient than the Metropolis-Hastings method for this problem. Our work shows that swap randomization is efficient in practice, and that it can be used for large datasets.

We have conducted extensive experiments on the use of swap randomization. The result s are very interesting in that they show big differences in the amount of structure that are present in the datasets. For example, the Retail dataset ap-parently has very little structure apart from its very skewed degree distribution for columns and (slightly less) for rows. The number of frequent sets in the real dataset is about the same as in the randomized versions, and clustering the orig-inal or randomized version yields about the same error. On the other hand, several of the other datasets clearly have lots of second-order structure, as evidenced by the dramatic drop in the number of frequent sets and strong correlations when moving to the randomized version of the data.
Swap randomization is a technique that maintains the first-order statistics of the data. Thus it should not be used to study the significance of discoveries that depend only on the first-order statistics of the data, i.e., the row and col-umn margins; power laws are an example of these types of statistics. An interesting question is whether it is possible to generate, from a dataset D , random datasets having the same margins as D while keeping also some second-order statistics (e.g., the frequency of certain variable pairs) fixed. Software for swap randomization can be found at http://www.cs.helsinki.fi/hiit bru/software/swaps/ We thank Jean-Fran  X  cois Boulicaut, C  X  eline Robardet, and J  X er  X  emy Besson for interesting discussions that got us started on swaps.
