 Nowadays, hard disk recorders that can record more than one thousand hours of TV shows are on the market, so that people can watch TV shows at the time of their convenience. But there is a problem: Nobody can spend one thousand hours just watching TV! Thus, unless there are ways to let the user handle recorded TV contents efficiently, the recorded contents will eventually be deleted or forgotten before put to use in any way.

Many researchers have tackled the problem of efficient information access for broadcast news [3,4,5,6,8,13,14,20,22], by means of news story segmentation , segment/shot retrieval , topic labelling (i.e., assigning a closed-class category) and so on. Broadcast news is clearly an impor tant type of video contents, especially for professionals and for organisations such as companies and governments. Both timely access to incoming news and retrospective access to news archives are required for such applications.

However, at the personal level, there are many other TV genres that need to be considered: soap, drama, comedy, quiz, talkshow, sport, music, wildlife, cook-ery, education, and so on. In fact, one could argue that these types of contents are more important than news for general consumers, as these are the kind of contents that tend to accumulate in hard disks, waiting to be accessed by the user some day, often in vain.

Among the aforementioned  X  X ntertaining X  kinds of TV genres, we are cur-rently interested in separable contents. By  X  X eparable X , we casually mean that a TV show can be broken down into several segments, where each segment is in-dependent enough to provide the user with a useful piece of information. Thus, according to our definition, most factual TV shows are separable, while most soaps and dramas are not. That is to say, a short segment drawn from a drama is less informative on its own than one drawn from a factual TV show.
For separable TV contents, we believe that topic segmentation is useful for solving the aforementioned  X  X ard disk information overload X  problem. For ex-ample, suppose that there is a recorded TV show that is two-hour long, which contains several distinct topics. If it is possible to segment the TV show accord-ing to topics and provide the user with a clickable  X  X able-of-contents X  interface from which he can select an interesting topic or two, then the user may be able to obtain useful information by viewing the selected segments only, which may only last for several minutes. In short, we believe that efficient, selective viewing of separable TV contents is important.

As a first step to tackling separable contents, we decided to handle TV shows on travel , since they are very popular in Japan and are representative (i.e., general approaches that work for travel TV shows should also work for those on cookery , wildlife , and so on). A typical Japanese travel TV show involves a group of non-professional reporters (typically an actor and his family), and may run as follows:  X  They visit a place in the country by train or by car;  X  They visit a couple of sightseeing spots or a cafe;  X  They check in at a hotel in a hot springs resort area;  X  They have a good open-air bath;  X  They enjoy their dinner;  X  They go to sleep;  X  Next morning, they have breakfast and check out;  X  They go to a souvenir shop and head for home.
 In fact, one TV show may contain two or three such sequences. Each of the above list items could viewed as a topic , that may correspond to a video segment ,that typically runs for several minutes. In contrast to news story segmentation [4,20], our definition of a  X  X opic X  is loose: if a video segment is useful on its own for the user for obtaining some information, we regard it as an acceptable topical seg-ment. Some users might choose to view segments that are to do with food; others might choose to view segments that provides information on the hotel facilities.
We currently provide the user with a clickable table-of-contents interface like the one shown in Figure 1, although this is only a prototype mainly for debugging and testing purposes and not intended for the end user. The interface relies on two functionalities which we have developed: Topic Segmentation. Our current algorithm analyses closed captions (or tran-Topic Sentence Selection. We currently select, from the closed-caption text,
In Figure 1, two thumbnails per segment are shown to the user. These key frames are selected based on the timestamps of the aforementioned topic sen-tences. The thumbnail on the left corresponds to the first topic sentence, and the one on the right corresponds to the second topic sentence. (Our current user interface is admittedly not user-friendly, but we stress that it is not intended for the end user.) By clicking a segment, the user can start playing it.
We call our prototype system Pic-A-Topic , because it enables the user to  X  X ick a topic X  from a TV show for selective viewing, and also because our click-able table-of-contents contains not only topic sentences but also key frames (i.e., Pic tures). The objective of this paper is to describe our current topic segmenta-tion and topic sentence selection algorithms and to report on some experimental results, primarily on topic segmentation, for the travel domain.
 The remainder of this paper is organised as follows. Section 2 describes the Pic-A-Topic system. Section 3 reports on two sets of experiments for evaluating our topic segmentation algorithm. Section 4 discusses previous work, with an emphasis on those that deal with TV genres other than broadcast news. Finally, Section 5 concludes this paper. 2.1 Overview Figure 2 provides an overview of the Pic-A-Topic system. As can be seen, it consists of three main components: topic segmentation , topic sentence selection and table-of-contents creation .

The input to the topic segmentation component are Japanese closed captions and Electronic Program Guide (EPG) data. In our initial experiments, we also used shot boundaries as input, for treating them as candidate topical boundaries. However, this approach was not successful because there simply were too many shot boundaries, and most of them were not topical boundaries.

Currently, the EPG data (if available) is used only for the purpose of obtaining names of celebrities, in order to automatically augment our named entity recog-nition dictionaries used in vocabulary shift detection . This is because celebrities such as comedians tend to have  X  X nomalous X  names, which named entity recog-nition tend to overlook (e.g.,  X  X eat Takeshi X  and  X  X apaya Suzuki X ).

The topic segmentation component performs both cue phrase detection and vocabulary shift detection , and then finally  X  X uses X  the two results. The contri-bution of each approach can be controlled by a parameter. Details will follow in Section 2.2.

The input to the topic sentence selection component are the result of topic seg-mentation as well as the raw closed-caption text. For each topical segment, this component first selects topic words based on a relevance feedback algorithm used widely in information retrieval. Subsequently, it selects topic sentences based on the weights of the aforementioned topic words. Note that this functionality is different from  X  X opic labelling X  [5] which assigns a closed-class category to (say) a news story. Topic Sentence Selection can assign any text extracted from closed captions, and are more flexible. Details will follow in Section 2.3.
The input to the table-of-contents creation component are the results of topic segmentation and topic sentence selection. This component performs several postprocessing functions, including: Keyframe selection/thumbnail creation. One keyframe is selected for each Start/End time adjustment. Because there may be a time lag between the 2.2 Topic Segmentation The task of topic segmentation is to take the timestamps in the closed captions as candidates and output a list of selected timestamps that are likely to represent topical boundaries, together with confidence scores. We assume that the number of required topical segments will be given from outside, based on constraints such as the size of the TV screen.
 Cue Phrase Detection. Our first approach to topic segmentation is cue phrase detection using the Semantic Role Analysis (SRA) techniques. SRA first per-forms morphological analysis, breaks the text into fragments (which in our case are sentences), and assigns one or more fragment labels to each fragment based on hand-written pattern-matching rules. A heuristically-determined weight is assigned to each rule. For details on SRA, we refer the reader to [18,19]. For the present study, we currently have four fragment labels: CONNECTIVE. This covers expressions such as  X  X s a starter X ,  X  X t last X  and MOVEMENT. This covers verbs such as  X  X ead for X  and  X  X isit X .
 TIME ELAPSED. This covers expressions that refer to the passage of time, OTHER. Anything else that are useful as cues.
 Note that the above labels are not necessarily domain-specific. We currently have 45 regular expression pattern-matching rules in total.

For each candidate boundary (i.e., timestamp), cue phrase detection calculates the raw confidence score by summing up the weights of all rules that matched the corresponding sentence. Finally, it obtains the normalised confidence scores c by dividing the raw confidence scores with the maximum one among all candidates.
The result of cue phrase detection may be used on its own for defining top-ical segments. In this case, we sieve the topical boundary timestamps before handing them to topic sentence selection: For each timestamp s (in millisec-onds) obtained, we examine all its  X  X eighbours X  (i.e., timestamps that lie within [ s  X  30000 ,s + 30000]), and overwrite its confidence score c with zero if any of the neighbours has a higher confidence score than s .Thisisforobtain-ing  X  X ocal optimum X  X imestamps which are at least 30 seconds apart from one another.
 Vocabulary Shift Detection. Our second approach to topic segmentation is vocabulary shift detection, which is similar in spirit to standard topic seg-mentation algorithms such as TextTiling [7]. Although these algorithms origi-nally designed for  X  X ritten X  text are often directly applied to closed captions (e.g., [11]), our preliminary experiments suggested that they are not satisfactory for analysing closed-captions which mainly consist of dialogues. Since closed captions contain timestamps, our algorithm uses timestamps explicitly and ex-tensively. Moreover, as our preliminary experiments showed that domain spe-cific knowledge is effective for our topic segmentation task, we use named entity recognition tuned specifically for the travel domain. Our algorithm is described below.

We first analyse the closed -caption text and extract morphemes and named entities , which we collectively refer to as terms . We have over one hundred generic named entity classes covering person names, place names, organization names, numbers and so on, originally developed for open-domain question an-swering [18]. In addition, we devised four domain-specific classes for the travel domain: TRAVEL ACTIVITY. This class covers typical activities of a tourist, such TRAVEL ATTRACTION CLASS. This class covers concepts that repre-TRAVEL HOTEL CLASS. This class covers words such as  X  X otel X  and  X  X nn X  . TRAVEL BATH CLASS. This class covers words such as  X  X ot spring X  and It is clear that, in order to deal with other TV genres such as cookery ,adifferent set of domain-specific classes will be requ ired. We feel optimistic about this issue since our named entity recogniser is fairly easy to customise.

Let t denote a term in a given closed-caption text, and s denote a candidate topical boundary (represented by a timestamp in milliseconds). For a fixed widow size S ,let WL denote the set of terms whose timestamps (by which we actually mean start times) lie within [ s  X  S, s ), and WR denote the set of terms whose timestamps lie within [ s, s + S ). For each t  X  WL  X  WR  X  WL  X  WR (i.e., term included in either WL or WR but not both), we define a downweighting factor dw ( t ) as follows: where dw domain ( t )= DW domain if t is a domain-specific named entity; Otherwise 0; dw generic ( t )= DW generic if t is a generic named entity; Otherwise 0; dw morph ( t )= DW morph if t is a single morpheme; Otherwise 0.
 Here, DW domain , DW generic and DW morph are tuning constants between 0 and 1.
Next, for each t  X  WL  X  WR  X  WL  X  WR whose timestamp is s ( t ), we compute: f ( t ) takes the maximum value of 1 when s ( t )= s and the minumum value of 0 when | s ( t )  X  s | = S .Thatis, f ( t )getssmallerasthetermmovesaway(along the timestamp) from the candidate boundary.

Meanwhile, for each t  X  WL  X  WR ,let s WL ( t )and s WR ( t )denotethetimes-tamps of t that correspond to WL and WR . (If there are multiple occurrences within the interval covered by WL or WR , then we take the timestamp that is closest to s .) Then we compute: If s WL ( t )and s WR ( t ) are close (i.e., the term occurs just before the candidate boundary and just after it), then g ( t )iscloseto1.If s WR ( t )  X  s WL ( t )iscloseto 2 S (i.e., the two occurrences of the same term are far apart), then g ( t )isclose to 0.

The term weighting functions f ( t )and g ( t ) have been designed in order to make the algorithm robust to the choice of window size S , which is currently fixed at 30 seconds. We currently do not to use global statistics such as idf (e.g., [21]).

Based on dw ( t ), f ( t )and g ( t )aswellastwopositiveparameters  X  and  X  ,we compute the novelty of each candidate boundary s as follows: novelty = Thus, a candidate boundary receives a high novelty score if WR has many terms that are not in WL (and vice versa) and if WL and WR have few terms in common. Using  X &lt; 1 implies that WR and WL are not treated symmetrically, unlike the cosine-based segmentation met hods (e.g., [7,8]). This corresponds to the intuition that terms that occur after the candidate boundary may be more important than those that occur before it. However, preliminary experiments suggested that this asymmetrical treatment may not be beneficial for our data set: We let  X  =1and  X  =0 . 5 hereafter.

The final confidence score v based on vocabulary shift is given by: where maxnovelty and minnovelty are the maximum and minimum values among all the novelty values computed for the closed-caption text.

The result of vocabulary shift detection may be used on its own for defining topical segments. Again, sieving isperformedinsuchacase.
 Fusion. The confidence scores based on cue phrase detection and vocabulary shift detection can be fused as follows: In fact, we fix  X  to 0.5. Sieving is performed after the above fusion. 2.3 Topic Sentence Selection Our topic sentence selection relies on a standard relevance feedback algorithm [15]. For each topical segment, we select 10 topic words from the closed-caption text by regarding the segment as a set of relevant documents and the other seg-ments as nonrelevant ones. (A  X  X ocument X  is usually a sentence or two, defined by a start time and an end time.)
Let N be the total number of  X  X ocuments X  in the closed-caption text, and R be the number of  X  X ocuments X  within the segment in question. Let w denote a candidate keyword (morpheme, actually). Moreover, let n ( w ) denote the number of  X  X ocuments X  containing w ,andlet r ( w ) denote the number of  X  X ocuments X  within the segment containing t . Then the term selection value for w is the offer weight ow ( w ): where
Next, we select a given number of sentences as follows: 1. Let L be the list of 10 topic words obtained above; 2. For each sentence in the segment in question, compute the sentence score by 3. Take the sentence with the highest score as the topic sentence, and remove 4. Repeat from 2, until a desired number of topic sentences (which is our case The above algorithm tries to obtain unique sentences that contain different topic words. This is because our preliminary experiments showed that selecting sen-tences independently based on the keyword weights (as was done for question-naire analysis in [17]) generally yield two topic sentences that are too similar to each other. This meant that the corresponding two key frames were also similar, and the entire table-of-contents did not look so informative. This section reports on our experiments primarily designed for validating our topic segmentation algorithms. Section 3.1 describes our video test collection. Section 3.2 discusses the segmentation accuracy of Pic-A-Topic for the travel domain, by comparing its performance to those of humans. Section 3.3 discusses what the accuracy values would actually mean to real users by conducting some subjective evaluations. 3.1 Topic Segmentation Test Collection for Travel TV Shows Unfortunately, there is no standard test collection available for evaluating topic segmentation for TV genres such as travel TV shows. We therefore had no choice but to create our own collection by recording real Japanese broadcast TV shows on travel, extracting the closed captions and preparing the  X  X ight an-swers X  for ourselves. Our test collection consists of ten clips from four different travel TV show series, totalling approximately 14.5 hours. Note that, with only ten clips, it is difficult for us to discuss statistical significance of experimental results.

Recall that, unlike the news story segmentation task, our definition of  X  X opic X  segmentation is rather ill-defined: We are happy as long as Pic-A-Topic is useful to the user for quickly obtaining desir ed information from a long TV show. Because of this subjective nature of the task, we let four assessors manually segment each clip, after giving them a common set of instructions. (We used a different set of assessors for each clip.) Each assessor was asked to view a clip and provide the timestamps of topical boundaries, using a simple graphical user interface with buttons such as  X  X lay X ,  X  X ause X ,  X  X ast forward X  and  X  X ecord this timestamp as a topical boundary X . Based on some pilot studies, we encouraged the assessors to find about 20 boundaries per hour. (An extremely short segment would not be informative; whereas, an extremely long segment would prevent the user from viewing the TV show efficiently.)
For each clip, the timestamp files produced by the first three assessors were merged, to create a single  X  X round truth X  file. Each ground truth file is a set of timestamp intervals , reflecting the individual assessment. The philosophy is that, if the system agrees with any of the three assessors about a topical boundary, then that boundary is acceptable.

The fourth assessor X  X  timestamps were used for p roviding the  X  X est-possible X  performance by comparing it with the ground truth. (Note that the Fourth As-sessor is not a single person.) The output of Pic-A-Topic was compared with the ground truth in exactly the same way, and we examined the relative perfor-mance of Pic-A-Topic, by dividing the system performance by the best-possible one. The method of comparison with the ground truth will be described below. 3.2 Segmentation Accuracy For both Pic-A-Topic and the Fourth Assessor, the topic segmentation accu-racy was computed in terms of Precision, Recall and F1-measure (i.e., harmonic mean of Precision and Recall). Suppose that the ground truth file contains N timestamp intervals, and that the Fourth Assessor file contains M timestamps. In general, N&gt;M holds, because the ground truth file covers the timestamps of three assessors. Since Pic-A-Topic requires the target number of topical bound-ariesasaparameter, M was given to Pic-A-Topic to compare its performance with the Fourth Assessor. (Note that giving N to Pic-A-Topic would not be a fair comparison: the recall of the Fourth Assessor would suffer very much since N&gt;M , so the relative performance of Pic-A-Topic would be overestimated.)
A topical boundary detected by Pic-A-Topic (or the Fourth Assessor) is counted as correct if it lies within the window [ start  X  10000 ,end + 10000], where start and end are the start/end times in millisecs of one of the ground-truth interval. (Of course, at most one boundary can be counted as correct for each ground-truth interval: If there are plural boundaries that lie within a sin-gle ground-truth interval, then only the one that is closest to the center of the interval is counted as correct.) The ten-second margins are for handling the time lags between video and closed captions, but this is not a critical choice since Pic-A-Topic and the Fourth Assessor are evaluated in exactly the same way.
Based on a couple of preliminary runs, we set the downweighting parameters (See Section 2.2) as follows: DW domain =1, DW generic =0and DW morph =0 . 1. That is, the domain-specific named entities play the central role in vocabulary shift detection, but the generic named entities are  X  X witched off X . Although our experiments do not separate training data from test data, we argue that they are useful for exploring the advantages and limitations of our current approaches.  X  X pen data X  evaluations may have to wait until good standard test collections become available.

Table 2 summarises the results of our topic segmentation experiments, in which the four TV series on travel are represented by A, B, C and D. For example, for Clip A1, the ground-truth file contained 61 intervals but the Fourth Assessor (and therefore Pic-A-Topic) produced only 37 topical boundaries. As a result, relative Precision, relative Recall and relative F1-measure are 76%, 77% and 77%, respectively. The results were obtained by fusing the cue phrase detection ouput and the vocabulary shift detection output: Figure 3 shows the absolute F1-measure values of the individual approaches as well, in which  X  X  X  and  X  X  X  represent cue phrase detection and vocabulary shift detection, respectively. It can be observed that fusion ( X  X +v X ) is generally beneficial, with a few exceptions.
The best relative F1-measure perfor mance is for Clip B1 (95%), while the worst one is for Clip C2 (50%). We found that the TV series C is generally challenging, because unlike the other series, it has a complex program structure. For example, in the middle of a footage showing reporters having a walk in the countryside, a studio scene is inserted several times, in which the presenters of the show and the same reporters make some comments over a coffee. Thus this probably shows a limitation of our purely linguistic approach: Some travel TV shows may require video feature analy sis (e.g., indoor/outdoor detection and face recognition) for accu rate topic segmentation.

More generally, our failure analysis found that Pic-A-Topic tends to break up a single dinner sequence into several  X  X ubtopics X . This is because, in a typical Japanese travel TV show, a dinner sequence is rather long, and it does contain some vocabulary shifts, involving words that are to do with the starter, the main dish, the dessert and so on. That is, it appears that Pic-A-Topic is currently too sensitive to change in  X  X ood X  vocabularies! There are ways to remedy this prob-lem: Since we already use named entity r ecognition, we could ignore food-related named entities and morphemes in vocabulary shift detection. A possibly more robust method would be to incorporate (lack of) scene changes, by analysing the video. We have also encountered some cases in which we felt that music/sound effect detection would be useful for detecting the beginning of a topical segment.
Another limitation of our approach is that closed captions do not contain all textual information. Some important textual information, such as the title of a travel episode and the names of hotels and restaurants, are shown as overlay text, and are not included in closed captions. That is, closed captions and overlays are complementary. This suggests that overlay text recognition/detection may be useful for topic segmentation of travel TV shows. Moreover, when a travel TV show consists of several episodes, tiny banners that represent each episode are often overlaid in the corner of each frame. This would be useful for obtaining the overall structure of the show. To sum up, image analysis techniques known to be useful for broadcast news may transfer well to our domain in some cases.
In spite of all these limitations, however, our overall relative performances (each obtained by averaging the ten corresponding relative values in the table) are all 82%, which is quite impressive even if they do not necessarily represent performances for  X  X pen X  data. (Dividing the average absolute performance of Pic-A-Topic by that of the Fourth Person yields similar results.) The question is,  X  X hat does a relative F1-measure of 82% mean in terms of practical usefulness? X  3.3 Blind Tests: Preliminary Subjective Evaluation We conducted a preliminary experiment to answer the above question. From Table 2, we first selected Clips A2, B1 and C2, representing the relative F1-measure of 81%, 95% and 50%, respectively. Thus, the three clips represent our  X  X verage X ,  X  X est X  and  X  X orst X  perform ances. Then, for each clip, we generated two table-of-contents interfaces similar to the one shown in Figure 1, one based on the Fourth Assessor X  X  segmentation and the other based on Pic-A-Topic X  X  segmentation. The idea was to conduct  X  X lind X  tests, and see which output is actually preferred by the user for an information seeking task.

For each of the three clips, five subjects were employed. Each subject was given a clip with a table-of-contents interface based on either the Fourth Assessor X  X  output or Pic-A-Topic X  X , and was given two questions per interface in random order. The subjects were blind as to how each interface was created, and were asked to locate the answers to the above questions within the given clip using the given interface. Each subject was finally asked which of the two interfaces he preferred for efficient information access. An example from our question set is:  X  X hat was the name of the restaurant that Actress X and Comedian Y visited? X . Thus this experiment is similar in spirit to the evaluation of summarisation using reading comprehension questions [10].

Initially, we tried to measure the time the subjects took to find the answer through the interface. However, we quickly gave this up because the variance across subjects was far greater than the efficiency differences between the two interfaces. Hence we decided to compare the two interfaces based on the subjects X  preferences only. Note that, since Pic-A-Topic and the Fourth Assessor interfaces have different topical boundaries, the topic sentences presented to the subjects are also different.

Table 3 summarises the results of our subjective evaluation experiments. It can be observed that, for Clip B1 for which the relative F1-measure is 95%, Pic-A-Topic was actually a little more popular than the Fourth Assessor. Moreover, even for Clip A2, which represents a typical performance of Pic-A-Topic, only one out of the five subjects preferred the Fourth Assessor interface, and the other four could not tell the difference. Thus, although this set of experiments may be preliminary, it is possible that 80% relative F1-measure is a practically acceptable level of performance. Note also that, even at 50% relative F1-measure, one subject said that Pic-A-Topic was better than the Fourth Assessor. This reflects the subjective nature of our topic segmentation task.

In summary, our results are encouraging: On average, Pic-A-Topic achieves a relative F1-measure of 82% at least for a known data set, and it is possible that this level of performance is practically acceptable to the end user. We have already mentioned in Section 1 that many researchers focus on the problem of efficient information access from broadcast news video. Below, we briefly mention some researches that handle TV genres other than news, and point out how their approaches differ from ours.

Extracting highlights from sports TV programs is a popular research topic, for which audio features [16] or manually transcribed utterances [23] are often utilised. Aoki, Shimotsuji and Hori [1] used colour and layout analysis for select-ing unique keyframes from movies . More recently, Aoki [2] reported on a system that can structuralise variety shows based on shot interactivity . While these ap-proaches are very interesting, we believe that audio and image features alone are not sufficient for identifying topics within a separable and informative TV con-tent. Lack of language analysis also implies that providing topic words or topic sentences to the user is difficult with these approaches. Zhang et al. [24] handled non-news video contents such as travelogue material to perform video parsing , but their method is based on shot boundary detection , not topic segmentation. As we mentioned in Section 2.1, we feel that shot boundaries are not suitable for the purpose of viewing a particular topical segment.

There exist, of course, approaches that effectively combine audio, image and textual evidence. Jasinschi et al. [9] report on a combination-of-evidence system that can deal with talk shows . However, what they refer to as  X  X opic segmenta-tion X  appears to be to segment closed ca ptions based on speaker change markers for the purpose of labelling each  X  X losed caption unit X  with either financial news or talk show . Nitta and Babaguchi [12] structuralise sports programs by analysing both closed captions and video. Smith and Kanade [21] also combine image and textual evidence to handle news and non-news contents: They first select keyphrases from closed captions based on tf -idf values, and use them as the basis of a video skim . While their keyphrase extraction involves detection of breaks between utterances, it is clear that this does not necessarily corresond to topical boundaries. Thus, even though their Video Skimming interface may be very useful for viewing the entire  X  X ummary X  of a TV program, whether it is also useful for selecting and viewing a particular topical segment or two is arguably an open question. We introduced a system called Pic-A-Topic, which analyses closed captions of Japanese TV shows on travel to perform topic segmentation and topic sentence selection. According to our experiments using 14.5 hours of recorded travel TV shows, Pic-A-Topic X  X  F1-measure for the topic segmentation task is 82% of man-ual performance on average. Moreover, a preliminary user evaluation experiment suggested that this level of performance may be indistinguishable from manual performance.

Some of our approaches are domain-specific, but we believe that domain-specific knowledge is a necessity for practical, high-quality topical segmentation of TV shows. Since genre information can be obtained from EPGs, we would like to let Pic-A-Topic select the optimal segmentation strategy for each genre. Our future work includes the following:  X  Incorporating video and audio information in our topic segmentation algo- X  Expanding our TV genres;  X  Developing a user-friendly table-of-contents interface;  X  Building other applications, such as selective downloading of TV content
