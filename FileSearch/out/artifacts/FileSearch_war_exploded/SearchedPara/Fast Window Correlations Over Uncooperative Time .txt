
Data arriving in time order (a data stream) arises in fields including physics, finance, medicine, and music, to name a few. Often the data comes from sensors (in physics and medicine for example) whose data rates continue to improve dramatically as sensor technology improves. Further, the number of sensors is increasing, so correlating data betwee n sensors becomes ever more critical in order to distill knowl -ege from the data. In many applications such as finance, recent correlations are of far more interest than long-term correlation, so correlation over sliding windows ( windowed correlation ) is the desired operation. Fast response is de-sirable in many applications (e.g., to aim a telescope at an activity of interest or to perform a stock trade). These thre e factors  X  data size, windowed correlation, and fast respons e  X  motivate this work.

Previous work [10, 14] showed how to compute Pearson correlation using Fast Fourier Transforms and Wavelet tran s-forms, but such techniques don X  X  work for time series in which the energy is spread over many frequency compo-nents, thus resembling white noise. For such  X  X ncoopera-tive X  time series, this paper shows how to combine several simple techniques  X  sketches (random projections), convol u-tion, structured random vectors, grid structures, and com-binatorial design  X  to achieve high performance windowed Pearson correlation over a variety of data sets.
 E.1 [ Data ]: Data Structures Algorithms Science Foundation under grants NSF IIS-9988345, N2010-0115586, MCB-0209754 and CCR-0105678.

Given N s streams and a window of size winsize , comput-ing all pairwise correlations naively requires O ( winsize  X  ( N s ) 2 ) time. Fortunately, extremely effective optimizations are possible, though the optimizations vary according to th e type of time series.
Previous work [14, 10] showed how to solve the windowed correlation problem in the cooperative setting using high quality digests obtained via Fourier transforms. Unfortu-nately, many applications generate uncooperative time se-ries. Stock market returns (change in price from one time period (e.g., day, hour, or second) to the next divided by initial price, symbolically ( p t +1  X  p t ) /p t ) for example are  X  X hite noise-like. X  That is, there is almost no relation fro m one time point to the next.

For collections of time series that don X  X  concentrate power in the first few Fourier/Wavelet coefficients, which we have termed uncooperative , we adopt a sketch-based approach. There are several difficulties to overcome: 1. Unfortunately, computing sketches directly for each 2. (Free choice of window size) Maintaining stream di-
The sketch approach, as developed by Kushilevitz et al. [13], Indyk et al. [12], and Achlioptas [11], provides a very nice guarantee: with high probability a random mapping taking points in R m to points in ( R d ) 2 b +1 (the (2b+1)-fold cross-product of R d with itself) approximately preserves dis-tances (with higher fidelity the larger b is).

Quantitatively, given a point x  X  R m , we compute its dot product with d random vectors r i  X  X  1 ,  X  1 } m . The first random projection of x is given by y 1 = ( x  X  r 1 , x  X  r , ..., x  X  r d ). We compute 2 b more such random projections y , ..., y 2b + 1 . If w is another point in R m and z 1 , ..., z 2b + 1 are its projections using dot products with the same random z 2b + 1 k is a good estimate of k x  X  w k . It lies within a  X  (1 /d ) factor of k x  X  w k with probability 1  X  (1 / 2) b .
Sketches work much better than Fourier methods for un-cooperative data. Figure 2 compares the distances of the Fourier and sketch approximations for 1,000 pairs of 256 timepoint windows having a basic window size of length 32. As you can see, the sketch distances are close to the real distances. On the other hand, the Fourier, Wavelet, and even SVD approximations work very poorly for uncoopera-tive data, because the information needed to capture unco-operative time series is spread out over all their coefficient s.
Our approach is to use a  X  X tructured X  random vector. The apparently oxymoronic idea is to form each structured ran-dom vector r from the concatenation of nb = sw/nb random vectors: r = s 1 , ..., s nb , where each s i has length bw . Fur-ther each s i is either u or  X  u , and u is a random vector in { 1 ,  X  1 } bw . This choice is determined by a random bi-nary nb -vector b : if b i =1, s i = u and if b i =0, s i =  X  u . The structured approach leads to an asymptotic performance of O ( nb ) integer additions and O (log bw ) floating point opera-tions per datum and per random vector. In our applications, we see 30 to 40 factor improvements over the naive method.
One may wonder whether the basic window and therefore the delay can be reduced. The tradeoff is with computation time. Reducing the size of the basic window reduces the compression achieved and increases the frequency and hence expense of correlation calculations.
 4 dimensions in practice [10]. Comparing each sketch vector with every other one destroys scalability though because it introduces a term proportional to the square of the number of windows under consideration.

For this reason, we adopt an algorithmic framework that partitions each sketch vector into subvectors and builds da ta structures for the subvectors. For example, if each sketch vector is of length 40, we might partition each one into ten groups of size four. This would yield ten data structures. We then combine the closeness results of pairs from each data structure to determine an overall set of candidate correlat ed windows 2 .
Given the idea of partitioning sketch vectors, we have to say how to combine the results of the different partitions. This introduces four parameters, as we will see. Suppose we are seeking points within some distance d in the original timeseries space.
The above framework eliminates the curse of dimensional-ity by making the groups small enough that multi-dimensiona l search structures (even grid structures) can be used. The framework also introduces the challenge of optimizing the settings of four parameters: the length N of the sketch vec-tor, the size g of each group, the distance multiplier c , and the fraction f .

Our optimization goal is to achieve extremely high recall (above 0.95) and reasonable precision (above 0.02). We are satisfied with a fairly low precision because examining even 50 times the number of the winning pairs on the raw data is much much better than examining all pairs, as we show later in our experiments.

Increasing the size of the sketch vector improves the accu-racy of the distance estimate but increases the search time. In our experiments, accuracy improved noticeably as the sizes increased to about 60; beyond that, accuracy did not improve much. Larger group sizes also improve accuracy, but increase the search time. A typical set of possible pa-rameter values therefore would be:
Note that we use correlation and distance more or less in-terchangeably because one can be computed from the other once the data is normalized. Specifically, Pearson correla-tion is related to Euclidean distance as follows: Here  X  x and  X  y are obtained from the raw time series by com-spot exrates 0.18 0.02 0.2 0.03 foetal ecg 0.22 0.01 0.25 0.008 evaporator 0.007 0.0001 0.007 0.0001 steamgen 0.32 0.02 0.34 0.01 Table 2: Combinatorial design vs. exhaustive search over a parameter search space of size 2,080. finement step from the initial parameter search works well, we tested whether an exhaustive search on a dense parame-ter space ( c values having two digits of precision in our case) would have yielded a substantially different result from a combinatorial design followed by refinement approach. Us-ing combinatorial design gave the same recall as exhaustive search and a precision at least 86% as good as exhaustive search across the variety of data sets from the UC Riverside collection[4, 7].
Optimizing parameter settings for one data sample may not yield good parameter settings for others. For exam-ple, suppose that we find the optimal parameter settings for stock return data over the first month. Will those set-tings still work well for a later month? Without further assumptions we cannot answer this, but we can estimate out-of-sample variability by using bootstrapping [8].
The goal of bootstrapping is to test the robustness of a conclusion on a sample data set by creating new samples from the initial sample with replacement. In our case, the conclusion to test is whether a given parameter setting with respect to recall and precision shows robust good behavior. To be concrete, suppose we take a sample S of one million pairs of windows. A bootstrapped sample would consist of one million pairs drawn from S with replacement. Thus the newness of a bootstrapped sample comes from the dupli-cates.

We use bootstrapping to test the stability of a choice of parameters. After constructing each bootstrapped sample, we check the recall and precision of that sample given our chosen parameter settings. Provided the mean recall over all bootstrapped samples less the standard deviation of the recall is greater than our threshold (say 0.95) and the stan-dard deviation for precision is low, then the parameter set-ting is considered to be good. This admittedly heuristic criterion for goodness reflects the idea that the parameter setting is  X  X sually good X  (under certain normality assump-tions roughly 3/4 of the time). 3
An alternative, which avoids a parametric normality as-sumption, is to sort the calculated recalls over all the boot -straps and take the value that is x% from the lowest where x could be typically 1, 5 or 25. We have done this for our data (results not shown) and recalls of the 1% from lowest bootstrap value are very close to the mean whenever the normality assumptions lead us to a  X  X sually good X  conclu-
In this experiment, we took a window size of 256 ( sw = 256 and bw = 32) across 10 data sets and tested the accuracy of the Fourier coefficients as an approximation to distance compared with structured random vector-based sketches. Figure 4 shows that the Discrete Fourier Transform-based distance perform badly in some data types while our sketch based distance works stably across all the data sets. On the other hand, when the time series closely resemble a random walk, as for stock price data, the Fourier series approach gives significantly better precision levels at the same reca ll as compared with the sketch method. Database people will appreciate the following analogy to data structures: sketc hes are like B-trees (the default choice) and Fourier Transform approaches are like bit vectors (better in some cases).
The operational claim of bootstrapping is to simulate sam-ples across a whole data set by repeated samples from a single initial sample. In our case, we want the optimal pa-rameters found on one sample (with bootstrapping) to meet the recall and precision thresholds on completely disjoint samples. As shown in our technical report [4], using the pa-rameters derived from a training sample (and confirmed by bootstrapping) of a data set works well across that entire data set. We also show there that different data sets should have different sets of parameters.
The previous subsection shows that the sketch framework gives a sufficiently high recall and precision. The next ques-tion is what is the performance gain of using (i) our sketch framework as a filter followed by verification on the raw data from individual windows compared with (ii) simply compar-ing all window pairs. Because the different applications hav e different numbers of windows, we take a sample from each application, yielding the same number of windows.
To make the comparison concrete, we should specify our software architecture a bit more. The multi-dimensional search structure we use is in fact a grid structure. The rea-son we have rejected more sophisticated structures is that we are asking a radius query: which windows (represented as points) are within a certain distance of a given point? A multi-scale structure such as a quadtree or R-tree would not help in this case. Moreover, the grid structure can be stored densely in a hash table so empty cells take up no space.
Figure 5 compares the results from our system, a Fourier-based approach, and a linear scan over several data sets. To perform the comparison we normalize the results of the linear scan to 1. The figure shows that both the sketch-based approach described here and the Fourier-based ap-proach are much faster than the linear scan. Neither is con-sistently faster than the other. However as already noted, the sketch-based approach produces consistently accurate results though the Fourier-based one wins when the data resembles a random walk.
Correlation can indicate that two time series exhibit sim-ilar trends. Windowed correlation is useful because it indi -cates that the two time series trend together over a certain [8] B. Efron and R. J. Tibshirani, An Introduction to the [9] D. M. Cohen, S. R. Dalal, J. Parelius and G. C. [10] D. Shasha and Y. Zhu, High Performance Discovery in [11] D. Achlioptas, Database-friendly Random Projections , [12] P. Indyk, Stable distributions, pseudorandom [13] E. Kushilevitz, R. Ostrovsky and Y. Ranbani, [14] Y. Zhu and D. Shasha, StatStream: Statistical [15] A. C. Gilbert, Y. Kotidis, S. Muthukrishnan and M. [16] N. Thaper, S. Guha, P. Indyk and N. Koudas, [17] P. Indyk, N. Koudas and S. Muthukrishnan, [18] G. Cormode, P. Indyk, N. Koudas and S.
 [19] W. Johnson and J. Lindenstrauss, Extensions of [20] M. Vlachos, M. Hadjieleftheriou, D. Gunopulos and E. [21] E. Keogh, Exact indexing of dynamic time warping, [22] B. K. Yi and C. Faloutsos, Fast time sequence [23] T. Palpanas, M. Vlachos, E. Keogh, D. Gunopulos
