 Automatic detection of non-literal expressions (in-cluding metaphors and idioms) is critical for many natural language processing (NLP) tasks such as information extraction, machine translation, and sentiment analysis. For this reason, the last decade has seen an increase in research on identifying literal vs. non-literal meaning (Birke and Sarkar, 2006; Birke and Sarkar, 2007; Li and Sporleder, 2009; Sporleder and Li, 2009; Turney et al., 2011; Shutova et al., 2013; Tsvetkov et al., 2014), as well as the establishment of workshops on metaphori-cal language in NLP. 1
In this paper, we explore the prediction of lit-eral vs. non-literal language usage of a compu-tationally challenging class of multiword expres-sions: German particle verbs (PVs) such as an-lachen (laugh at) are compositions of a base verb (BV ) such as lachen (smile/laugh) and a verb par-ticle such as an . German PVs are highly produc-tive (Springorum et al., 2013b; Springorum et al., 2013a), and the particles are notoriously ambigu-ous (Lechler and Ro X deutscher, 2009; Haselbach, 2011; Springorum, 2011). Furthermore, the par-ticles often trigger (regular) meaning shifts when they combine with base verbs (Springorum et al., 2013b), so the resulting PVs represent frequent cases of non-literal meaning. The contributions of this paper are as follows: 1. We present a random forest classifier that cor-2. We successfully incorporate salient PV-3. We demonstrate that PVs with semantically 4. We illustrate the potential and the limits of
In the remainder of this paper we describe pre-vious work on non-literal language identification and computational models of German particle verbs (Section 2), before we introduce our dataset on German particle verbs (Section 3), the particle verb features (Section 4), and the experiments, re-sults and analyses (Section 5). Previous work relevant to this paper includes re-search on identifying non-literal language usage, and computational work on (German) particle verb meaning.
 Identification of non-literal language usage: Birke and Sarkar (2006), Birke and Sarkar (2007), Li and Sporleder (2009) and Sporleder and Li (2009) performed binary token-based classifications for English datasets, relying on various contextual in-dicators. Birke &amp; Sarkar exploited seed sets of literal vs. non-literal sentences, and used distri-butional similarity to classify English verbs. Li &amp; Sporleder defined two models of text cohesion (a cohesion chain and a cohesion graph) to clas-sify V+NP and V+PP combinations. Shutova et al. (2013) performed both metaphor identification and interpretion (by paraphrasing), focusing on English verbs. She relied on a seed set of annotated metaphors and standard verb and noun cluster-ing, to classify literal vs. metaphorical verb senses. Gedigian et al. (2006) also predicted metaphorical meanings of English verb tokens, however heav-ily relying on manual rather than unsupervised data (i.e. labeled sentences and PropBank anno-tation) and a maximum entropy classifier. Turney et al. (2011) assumed that metaphorical word us-age is correlated with the degree of abstractness of the word X  X  context, and classified word senses in a given context as either literal or metaphori-cal. Their targets were adjective X  X oun combina-tions and verbs. Tsvetkov et al. (2014) presented a language-independent approach to metaphor identification. They used affective ratings, Word-Net categories and vector-space word representa-tions to train a metaphor-detecting classifier on English samples, and then applied it to a different target language using bilingual dictionaries. Computational research on particle verbs was initially concerned with the automatic acquisition of particle verbs from corpora (Baldwin and Villav-icencio, 2002; Baldwin, 2005; Villavicencio, 2005). Afterwards, the main focus has been on modelling the degree of compositionality of particle verbs as based on distributional features (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). All these approaches were type-based, and predicting the compositionality was mainly concerned with PV X  X V similarity, not taking the contribution of the particle into account. In cases where the particle semantics was respected (such as Bannard (2005)), the results were disappointing because modelling particle senses is still an unsolved problem.
Regarding German particle verbs, there has also been a focus on modelling PV compositionality (K X hner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015). As in English, the approaches were all type-based and mainly concerned with PV X  X V similarity. Another line of research categorized particle meanings by relating formal semantic def-initions to automatic classifications (R X d, 2012; Springorum et al., 2012). Furthermore, Springo-rum et al. (2013b) recently provided a corpus-based study on regular meaning shift conditions for German particle verbs. We selected 165 particle verbs across 10 parti-cles, based on previous experiments and datasets that incorporated German particle verbs with reg-ular meaning shifts, various degrees of ambigu-ity, and across frequency ranges (Springorum et al., 2013b; Springorum et al., 2013a; Bott and Schulte im Walde, 2015). For the 165 PVs, we ran-domly extracted 50 sentences from DECOW14AX , a German web corpus containing 12 billion to-kens (Sch X fer and Bildhauer, 2012; Sch X fer, 2015). The sentences were morphologically annotated and parsed using SMOR (Faa X  et al., 2010), Mar-MoT (M X ller et al., 2013) and the MATE depen-dency parser (Bohnet, 2010). Combining part-of-speech and dependency information, we were able to reliably sample both separated and non-separated PV occurrences ( X  X er Ast bricht ab  X  vs.  X  X er Ast ist abgebrochen  X ).

Three German native speakers with a linguis-tic background annotated each of the 8 128 sen-tences 2 on a 6-point scale [0, 5], ranging from clearly literal (0) to clearly non-literal (5) usage. The total agreement of the annotators on all six categories was 43%, Fleiss X   X  = 0.35. Dividing the scale into two disjunctive ranges with three categories each ([0, 2] and [3, 5]), the total agree-ment of the annotators on the two categories was 79%, Fleiss X   X  = 0.70. In the experiments we used the binary-class distinction, and disregarded all cases of disagreement. This final dataset com-prises 6 436 sentences: 4 174 literal and 2 262 non-literal uses across 159 particle verbs and 10 parti-cles. 3 Figure 1 shows the distribution of literal and non-literal sentences across the particles.
Figure 1: Lit/Non-lit distribution across particles. Our feature space includes standard features to detect non-literal language uses (bags-of-words and affective ratings) as well as PV-specific fea-tures and abstraction over common nouns. 4.1 Unigrams As a standard feature in vector space models, we used all words in the particle verb sentences, i.e., a bag-of-words model relying on unigrams. We ex-pected this standard information to be useful, be-cause some words such as the abstract noun Hoff-nung (hope) and the concrete noun Geld (money) frequently occur with non-literal rather than literal language usage: 1. (non-lit.)  X  X ie Hoffnung keimte fr X h auf . X  2. (non-lit.)  X  X r versucht das Geld abzugraben . X 
To overcome data sparseness, we did not use the unigrams as individual features ( | V | = feature space), but implemented this feature as the out-put of a text-classifier. We relied on the Multino-mial Naive Bayes (MNB) classifier by McCallum and Nigam (1998). While the classifier was de-signed for document classification, we considered a sentence as a document and the possible class outcomes were literal and non-literal.
 Noun Clusters Because of the severe data sparse-ness in our PV feature sets, we performed noun generalization and applied the generalized infor-mation to all nouns in our PV contexts. Using all approx. 430 000 nouns that appeared &gt; 100 times in the DECOW14AX corpus, we applied k-Means clustering with k  X  [2, 10 000]. As an alternative to the standard unigrams, we then replaced every noun in the PV sentences with its corresponding cluster tag. 4.2 Affective Ratings Previous work on detecting non-literal language often makes use of psycholinguistic attributes, namely abstractness and concreteness ratings (Turney et al., 2011), and imageability ratings (Tsvetkov et al., 2014). Words with high abstract-ness ratings refer to entities that cannot be per-ceived with our senses; a large subset of which are non-visual (i.e., receive low imageability). It has been shown that non-literal expressions tend to occur with abstract words ( dark humor versus dark hair ). We thus expected affective ratings to be useful for particle verbs as well: 1. (lit.)  X  X en Lippenstift kannst du dir ab-2. (non-lit.)  X  X  X en Job kannst du dir ab-We reimplemented the algorithm from (Turney and Littman, 2003) to create large-scale abstract-ness and imageability ratings for German (K X per and Schulte im Walde, 2016). Based on these rat-ings, we defined the following (partially redun-dant) features for the PV sentential contexts: 1. Rating of the PV subject 2. Rating of the PV object 3. Average rating of all nouns (excluding proper 4. Average rating of all proper names 5. Average rating of all verbs, excluding the PV 6. Average rating of all adjectives 7. Average rating of all adverbs While features 3 X 7 have been adopted from (Tur-ney et al., 2011), features 1 X 2 represent additional, PV-specific features. 4.3 Distributional Fit of PV, BV and Context Particle verbs with a meaning shift are non-compositional regarding their base verbs. We thus implemented a PV-specific feature that measures the distributional fit of PVs and their BVs in the PV contexts. For example, looking at the follow-ing two PV sentences containing the BV klingen (to sound), the context of the first, literal sentence fits well to the BV meaning, but the context of the second, non-literal sentence does not. The distri-butional fit of the BV in the literal context should therefore be high, but the distributional fit of the BV in the non-literal context should be low. 1. (lit.)  X  X er Ton der Gitarre klingt aus . X  2. (non-lit.)  X  X en Abend lassen wir mit Wein
To measure the distributional fit of PVs and BVs to PV contexts, we created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015) and the DECOW14AX corpus. We re-lied on a symmetrical window of size 3 and applied positive pointwise mutual (PPMI) feature weight-ing together with singular value decomposition (SVD). Based on the word representations, we cal-culated cosine similarities between the PVs and their contexts, and likewise between the respec-tive BVs and the PV contexts. The contexts we used were the same seven dimensions we used for the affective ratings (cf. Section 4.2). For exam-ple, regarding the sentence  X  X ie Katze springt auf den Tisch X  ( The cat jumps on the table ), we calcu-lated the distributional similarity between the PV  X  X ufspringen X  and the subject  X  X atze X , and the dis-tributional similarity between the BV  X  X pringen X  and the subject  X  X atze X , etc. Each PV X  X ontext and each BV X  X ontext dimension represents an individ-ual feature. In this section, we present a series of binary clas-sification experiments to distinguish literal and non-literal PV usage. Section 5.1 presents the main experiments comparing our features in a global classification setup, and Section 5.2 presents PV-specific additional experiments that zoom into the role of particle types and into the role of semanti-cally related PVs and BVs. Section 5.3 provides a qualitative analysis of the features. 5.1 Main Experiments We used a random forest with multiple (in our case 100) random decision trees, 4 with each tree voting for an overall classification result. The unigram in-formation was represented by stacking the output of a multinomial naive bayes text classifier as a sin-gle feature into the random forest. For all machine learning algorithms we relied on the W E KA toolkit (Witten et al., 2011).

The experiments were performed in two modes, (a) without knowledge of the particle (i.e., the indi-vidual particle was not provided as a feature), and (b) with explicit knowledge of the particle. In this way, we could identify the contribution of the par-ticle.
 The classification results are shown in Table 1. We report on the feature type, and on the size 5 of the feature set f . We further present literal and non-literal f-scores F 1 , and accuracy with and without particle knowledge. We compare against the majority baseline (literal). The right-most columns indicate whether the differences in per-formance are statistically significant, using the  X  2 test and  X  for p &lt; 0.001 and  X  for p &lt; 0.05 to mark significance.

The results demonstrate that the classification results across all feature types are significantly bet-ter than the majority baseline. The single best performing feature type (cf. lines 1 X 6) is the un-igram information; in combination with the par-ticle information ( + P ), the distributional PV/BV X  context fit is best. Combining the best feature types (2+4+6) once more improves the results, and ditto when adding noun cluster information. 6 We can also see that abstractness (AC) ratings outper-form imageability (IMG) ratings.

So overall, the best performing feature set successfully combines unigrams that incorporate clusters for noun generalization; abstractness rat-ings; and PV-specific information regarding the distributional PV/BV X  X ontext fit and knowledge about the particle. This setup correctly classifies literal sentences with an f-score of 88.8 and non-literal sentences with an f-score of 77.3; overall ac-curacy is 86.8 over a baseline of 64.9.

It is difficult to compare our results against previous approaches on different datasets and in different languages. Regarding the closest approaches to our work, Tsvetkov et al. (2014) report an accuracy score of 82.0 using 10-fold cross-validation on a training dataset with a ma-jority baseline of 59.2, combining multiple lexi-cal semantic features on a dataset of 1 609 En-glish subject X  X erb X  X bject triples. Birke and Sarkar (2007) trained a single classifier for each of twenty-five verbs in the English T RO F I verb dataset and reported only an average f-score: 64.9 against a majority baseline of 62.9. Turney et al. (2011) ob-tained an average f-score of 63.9 and addition-ally report an accuracy score of 73.4 on the same dataset, using abstractness ratings.
 In contrast to our work, the two approaches by Birke and Sarkar (2007) and Turney et al. (2011) treated each group of sentences for a given tar-get verb as a separate learning problem, while we learn one classifier across different verbs. Our method 4 (AC Ratings) can be considered a Ger-man re-implementation of the approach by Tur-ney et al. (2011). In comparison to the results of previous work, our approach can safely be consid-ered state-of-the-art. 5.2 PV-Specific Experiments 5.2.1 Incorporating Standard Measures of
One traditional line of research to identify type-based multiword collocations or idiomatic expres-sions relies on the association strength between the multiword parts (Evert and Krenn, 2001; Krenn and Evert, 2001; Stevenson et al., 2004): The stronger the association between the parts of a multiword expression (as determined by raw fre-quency, some variant of mutual information, etc.), the stronger the collocation/idiomaticity of the combination of the parts. Based on this assump-tion, we calculated the association strength be-tween PVs and their contextual subjects/objects, using local mutual information (LMI) , cf. Evert (2005). The LMI scores were based on type-based frequency counts in the DECOW14AX corpus and added as features to the respective contexts, as-suming that large LMI scores indicate non-literal PV usage.
Adding the LMI values to the overall best fea-ture set from the main experiments decreased ac-curacy from 86.8  X  86.0. Using the LMI associ-ation strength values of the PV X  X ubject and PV X  object pairs by themselves provided slightly but non-significantly better results in comparison to the majority baseline: 65.9 &gt; 64.9. 7 Manual inves-tigations revealed that verb X  X oun pairs with high LMI scores represent collocations in many cases, but the collocations are not only used in non-literal language but also in literal language, e.g.,  X  X endung ausstrahlen X  ( X  X roadcast a program X ). 5.2.2 Non-Literality across Particles
In order to explore the predicability of literal vs. non-literal uses with respect to specific particles, we trained the best classifier from the main exper-iments on all particle verbs with particle X and ap-plied the classifier to all particle verbs with particle Y . Our hypothesis was that pairs of particles with similar ambiguities might predict each other bet-ter than pairs with different particle meanings.
This PV-specific setup could also be applied within a PV group with the same particle: We trained the classifier on all PVs with particle X ex-cept for one, and then applied the trained classi-fier to the missing PV with particle X . The setup was repeated for all PVs with particle X , and the average accuracy was calculated.

Figure 2 provides the results as a heat map, with red indicating high and blue indicating low accu-racy scores. The vertical particles on the left corre-spond to the training particles, and the horizontal particles at the bottom correspond to the test par-ticles. The bottom line shows the majority base-line. For example, training a classifier on  X  X in X  PVs and evaluating it on  X  X us X  PVs results in an accu-racy of 76.56, which is significantly better (  X  X  X  X  for p &lt; 0.001) than the baseline for  X  X us X  (65.55).
The diagonal in the heat map (showing the within-particle setup) provides particularly high accuracy scores, so the PVs with the same parti-cle predict (non-)literality within the group very well. This demonstrates that the meanings and the meaning shifts across PVs with the same par-ticle (e.g., aufdecken and auftischen ) are quite reg-ular. A comparably strong prediction is found be-tween  X  X or X  (before/in front of ) and  X  X ach X  (af-ter/behind), with both particles carrying highly similar temporal and local senses. Other exam-ples of strongly related antonymous particle pairs are  X  X uf X / X  X u X ,  X  X in X / X  X us X , and  X  X us X / X  X n X . Exam-ples of strongly related synonymous particle pairs are  X  X n X / X  X in X , and  X  X us X / X  X u X .  X  X urch X  correlates poorly with all other particles, which is probably due to the few sentences we collected from the corpus.  X  X it X  also correlates poorly with all other particles, because it is the only particle with little ambiguity. So overall, the heat map corresponds to intuitions about semantic relatedness across par-ticle pairs. Figure 2: Train a classifier on PVs with particle X and test it on PVs with particle Y . 5.2.3 Non-Literality across Particle Verbs
An even more fine-grained experiment setting explored the predictability of a specific particle verb based on the classifier trained on a differ-ent particle verb. Our hypothesis was that pairs of PVs that predict each other particularly well share some meaning aspects, either (i) because the training and the test verb share the same BV ( SameBV : ab graben :auf graben ), or (ii) the PVs are synonymous according to the German Duden 8 dictionary ( PVSyn : auftragen : auftischen ), or (iii) because the BVs of two PVs with identical particles are synonymous according to the Duden ( BVSyn : auf reissen :auf platzen ).

Figure 3 shows the f-scores for predicting liter-ality and non-literality across the three settings, in comparison to the main experiments ( X  X ll X ). The number of PV pairs in the settings and the majority accuracy for these PV pairs are also provided, be-cause the experiment sets differ in size. We can see that PVs with the same BV ( SameBV ) predict each other X  X  classifications well regarding literal but not regarding non-literal sentences. This behaviour il-lustrates the contribution of the particle to the PV meaning: The same BVs with different particles potentially differ strongly, if the particles do not agree on one or more senses. Synonymous PVs ( PVSyn ) predict each other as well in literal as in non-literal cases. Since the PVs in all cases are sup-posed to have the same meaning, this behaviour is also reasonable. An increase in both literal and non-literal F1 is reached for PV pairs with the same particle and synonymous BVs ( BVSyn ), because the BVs are supposed to carry the same meaning, and the identical particles trigger similar mean-ing shifts. Overall, the experiment demonstrates that synonymous verbs undergo similar meaning shifts, and that a particle initiates similar meaning shifts when applied to synonymous BVs.
Figure 3: Prediction for semantically related PVs. 5.3 Indicators of Non-Literality In the final part of the paper, we perform a quali-tative analysis of the most salient features. 5.3.1 Information Gain
First of all, we looked into the feature space by computing the information gain within the best random forest classifier. The information gain ( I-Gain ) provides the improvement in information entropy regarding our feature space and the class labels, as defined by equation (1).
 I-Gain ( Class,Feat ) = H ( Class )  X  H ( Class | Feat ) (1)
The information gain does not take feature in-teraction into account, but determines the im-portance of the individual features. Applying this method reveals the three most salient features: unigrams (0.31), abstractness ratings of the con-text nouns (0.17), and distributional fit of the base verbs (0.11). The information gain therefore confirms our results from the main experiments, where these three features worked best.

In addition, we noticed that for all features higher weights were given to dimensions that de-pend on nouns (such as the common nouns in the PV contexts, and the subject and object nouns), in comparison to proper names, verbs, adjectives and adverbs. For example, the abstractness ratings of the adverbs were ranked second lowest with a score of 0.005, and the distributional fit between BVs and adjectives was ranked last with a zero score, indicating that this feature provides no ad-ditional information for our dataset. 5.3.2 Distributional Fit
We now take a look at the distributional fit fea-ture, which was the best performing feature in the main experiments, when combined with par-ticle knowledge. Figure 4 focusing on the distribu-tional fit between BVs and common nouns (as de-termined third best by the information gain) con-firms that the feature is helpful in distinguishing literal vs. non-literal PV sentences across particles: The medians in the boxplots for literal sentences are clearly above those for non-literal sentences. The plots confirm that BVs can be exploited to identify compositional uses of PVs (which in turn refer to literal usage).

Looking into individual PVs confirms that this feature distinguishes well between the literal and non-literal sentences. On the other hand, we also find PVs where this feature is not able to iden-tify non-literal language use. Figure 5 presents the boxplots with cosine values for aufbl X hen (blos-som out) and auflodern (burn up), where the feature works well, in comparison to absaufen (drown), where the feature cannot distinguish (non-)literal language usage. Figure 4: Distributional fit of BVs and context nouns in (non-)literal sentences across particles. Figure 5: Example PVs and their distributional fit of BVs and context nouns in (non-)literal use. 5.3.3 Abstractness of Contexts
Finally, we take a look at the abstractness fea-ture, which was also among the best performing features in the main experiments, and which is generally assumed to represent a salient indicator of non-literal language usage. Figure 6 focusing on the abstractness of common nouns in the PV sentences 9 (as determined second best by the in-formation gain) confirms that the feature is also helpful in distinguishing literal vs. non-literal PV sentences across particles: Again, the medians in the boxplots for literal sentences are clearly above those for non-literal sentences. The plots confirm that contextual abstractness is a salient indicator of non-literal language usage. Figure 6: Average abstractness ratings of context nouns in (non-)literal sentences across particles.
Looking into individual PVs again confirms that this feature distinguishes well between the literal and non-literal sentences but also that there are PVs where this feature is not able to identify non-literal language use. Figure 7 presents the box-plots with abstractness ratings for anstauen (ac-cumulate) and durchsickern (leak through), where the feature works well, in comparison to antanzen (waltz in) and especially ausklingen (fade/finish), where the feature cannot distinguish (non-)literal language usage. Figure 7: Example PVs and their average abstract-ness ratings of context nouns in (non-)literal use.
Two example sentences where the abstractness feature goes wrong for a good reason are as fol-lows. In (1)  X  X ber wir sollten doch um f X nf zum Es-sen antanzen . X  ( But we should show up (lit: waltz in) for dinner at five ), the context nouns are con-crete ( we; dinner ) but the language usage is non-literal. In contrast, in (2)  X  X ch liebe Emotionen, de-shalb summen alle mit . X  ( I love emotions, there-fore everyone hums along ), the object noun in the sentence is highly abstract ( emotion ), but the lan-guage usage is literal. These examples illustrate that contextual abstractness is not a perfect indi-cator of non-literal language usage. We presented a classifier that predicts literal vs. non-literal language usage for German particle verbs, a semantically challenging type of multi-word expressions. The classifier significantly out-performed the baseline by improving standard features with noun clusters and a PV-specific dis-tributional fit feature. PV-specific experiments in-dicated that PVs whose particles share aspects of ambiguity and which incorporate semantically re-lated BVs seem to undergo similar meaning shifts. The research was supported by the DFG Col-laborative Research Centre SFB 732 (Maximil-ian K X per) and the DFG Heisenberg Fellowship SCHU-2580/1 (Sabine Schulte im Walde).
 Timothy Baldwin and Aline Villavicencio. 2002. Ex-tracting the Unextractable: A Case Study on Verb Particles. In Proceedings of the 6th Conference on
Computational Natural Language Learning , pages 98 X 104, Taipei, Taiwan.
 Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An Empirical Model of
Multiword Expression Decomposability. In Proceed-ings of the ACL Workshop on Multiword Expressions: Analysis, Acquisition and Treatment , pages 89 X 96, Sapporo, Japan.
 Timothy Baldwin. 2005. Deep Lexical Acquisition of Verb X  X article Constructions. Computer Speech and Language , 19:398 X 414.
 Collin Bannard. 2005. Learning about the Meaning of Verb X  X article Constructions from Corpora. Com-puter Speech and Language , 19:467 X 478.
 Julia Birke and Anoop Sarkar. 2006. A Clustering Ap-proach for the Nearly Unsupervised Recognition of
Nonliteral Language. In Proceedings of the 11th Con-ference of the European Chapter of the ACL , pages 329 X 336, Trento, Italy.
 Julia Birke and Anoop Sarkar. 2007. Active Learn-ing for the Identification of Nonliteral Language. In Proceedings of the Workshop on Computational Approaches to Figurative Language , pages 21 X 28, Rochester, NY.
 Bernd Bohnet. 2010. Top Accuracy and Fast Depen-dency Parsing is not a Contradiction. In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics , pages 89 X 97, Beijing, China. Stefan Bott and Sabine Schulte im Walde. 2014. Opti-mizing a Distributional Semantic Model for the Pre-diction of German Particle Verb Compositionality. In Proceedings of the 9th International Conference on Language Resources and Evaluation , pages 509 X 516, Reykjavik, Iceland.
 Stefan Bott and Sabine Schulte im Walde. 2015.
Exploiting Fine-grained Syntactic Transfer Features to Predict the Compositionality of German Particle
Verbs. In Proceedings of the 11th Conference on Com-putational Semantics , pages 34 X 39, London, UK. Stefan Evert and Brigitte Krenn. 2001. Methods for the
Qualitative Evaluation of Lexical Association Mea-sures. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics , pages 188 X 195, Toulouse, France.
 Stefan Evert. 2005. The Statistics of Word Co-
Occurrences: Word Pairs and Collocations . Ph.D. thesis, Institut f X r Maschinelle Sprachverarbeitung, Universit X t Stuttgart.
 Gertrud Faa X , Ulrich Heid, and Helmut Schmid. 2010.
Design and Application of a Gold Standard for Mor-phological Analysis: SMOR in Validation. In Proceed-ings of the 7th International Conference on Language Resources and Evaluation , pages 803 X 810, Valletta, Malta.
 Matt Gedigian, John Bryant, Srini Narayanan, and Bra-nimir Ciric. 2006. Catching Metaphors. In Proceed-ings of the 3rd Workshop on Scalable Natural Lan-guage Understanding , pages 41 X 48, New York City, NY.
 Boris Haselbach. 2011. Deconstructing the Meaning of the German Temporal Verb Particle  X  X ach X  at the
Syntax-Semantics Interface. In Proceedings of Gen-erative Grammar in Geneva , pages 71 X 92, Geneva, Switzerland.
 Maximilian K X per and Sabine Schulte im Walde. 2016.
Automatically generated affective norms of abstract-ness, arousal, imageability and valence for 350000 german lemmas. In Proceedings of the 10th Interna-tional Conference on Language Resources and Evalu-ation , Portoro X , Slovenia.
 Brigitte Krenn and Stefan Evert. 2001. Can we do better than Frequency? A Case Study on Extracting PP-Verb Collocations. In Proceedings of the ACL Workshop on
Collocations , pages 39 X 46, Toulouse, France. Natalie K X hner and Sabine Schulte im Walde. 2010.
Determining the Degree of Compositionality of Ger-man Particle Verbs by Clustering Approaches. In
Proceedings of the 10th Conference on Natural Lan-guage Processing , pages 47 X 56, Saarbr X cken, Ger-many.
 Andrea Lechler and Antje Ro X deutscher. 2009. German
Particle Verbs with auf . Reconstructing their Com-position in a DRT-based Framework. Linguistische Berichte , 220:439 X 478.
 Omer Levy, Yoav Goldberg, and Ido Dagan. 2015.
Improving Distributional Similarity with Lessons learned from Word Embeddings. Transactions of the Association for Computational Linguistics , 3:211 X  225.
 Linlin Li and Caroline Sporleder. 2009. Classifier Com-bination for Contextual Idiom Detection Without La-belled Data. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing , pages 315 X 323, Singapore.
 Andrew McCallum and Kamal Nigam. 1998. A Com-parison of Event Models for Naive Bayes Text Clas-sification. In Proceedings of the AAAI Workshop on Learning for Text Categorization .
 Diana McCarthy, Bill Keller, and John Carroll. 2003. De-tecting a Continuum of Compositionality in Phrasal
Verbs. In Proceedings of the ACL Workshop on Mul-tiword Expressions: Analysis, Acquisition and Treat-ment , pages 73 X 80, Sapporo, Japan.
 Thomas M X ller, Helmut Schmid, and Hinrich Sch X tze. 2013. Efficient Higher-Order CRFs for Morpholog-ical Tagging. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing , pages 322 X 332, Seattle, WA, USA.
 Stefan R X d. 2012. Untersuchung der distributionellen
Eigenschaften der Lesarten der Partikel  X  X uf  X  mit-tels Clustering-Methoden. Master X  X  thesis, Insti-tut f X r Maschinelle Sprachverarbeitung, Universit X t Stuttgart.
 Roland Sch X fer and Felix Bildhauer. 2012. Building Large Corpora from the Web Using a New Efficient Tool Chain. In Proceedings of the 8th International
Conference on Language Resources and Evaluation , pages 486 X 493, Istanbul, Turkey.
 Roland Sch X fer. 2015. Processing and Querying Large
Web Corpora with the COW14 Architecture. In Pi-otr Ba  X nski, Hanno Biber, Evelyn Breiteneder, Marc Kupietz, Harald L X ngen, and Andreas Witt, editors, Proceedings of the 3rd Workshop on Challenges in the Management of Large Corpora , pages 28  X  34. Ekaterina Shutova, Simone Teufel, and Anna Korhonen. 2013. Statistical Metaphor Processing. Computa-tional Linguistics , 39(2):301 X 353.
 Caroline Sporleder and Linlin Li. 2009. Unsupervised
Recognition of Literal and Non-Literal Use of Id-iomatic Expressions. In Proceedings of the 12th Con-ference of the European Chapter of the ACL , pages 754 X 762, Athens, Greece.
 Sylvia Springorum, Sabine Schulte im Walde, and An-tje Ro X deutscher. 2012. Automatic Classification of German an Particle Verbs. In Proceedings of the 8th International Conference on Language Resources and Evaluation , pages 73 X 80, Istanbul, Turkey.
 Sylvia Springorum, Sabine Schulte im Walde, and An-tje Ro X deutscher. 2013a. Sentence Generation and
Compositionality of Systematic Neologisms of Ger-man Particle Verbs. Talk at the Conference on Quan-titative Investigations in Theoretical Linguistics. Sylvia Springorum, Jason Utt, and Sabine Schulte im Walde. 2013b. Regular Meaning Shifts in German
Particle Verbs: A Case Study. In Proceedings of the 10th International Conference on Computational Se-mantics , pages 228 X 239, Potsdam, Germany.
 Sylvia Springorum. 2011. DRT-based Analysis of the
German Verb Particle "an" . Leuvense Bijdragen , 97:80 X 105.
 Suzanne Stevenson, Afsaneh Fazly, and Ryan North. 2004. Statistical Measures of the Semi-Productivity of Light Verb Constructions. In Proceedings of the 2nd Workshop on Multiword Expressions: Integrating Processing , pages 1 X 8, Barcelona, Spain.
 Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric
Nyberg, and Chris Dyer. 2014. Metaphor Detection with Cross-Lingual Model Transfer. In Proceedings of the 52nd Annual Meeting of the Association for Com-putational Linguistics , pages 248 X 258.
 Peter D. Turney and Michael L. Littman. 2003. Mea-suring Praise and Criticism: Inference of Semantic Orientation from Association. ACM Transactions on Information Systems , 21(4):315 X 346.
 Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen. 2011. Literal and Metaphorical Sense Identi-fication through Concrete and Abstract Context. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 680 X 690, Ed-inburgh, UK.
 Aline Villavicencio. 2005. The Availability of Verb-
Particle Constructions in Lexical Resources: How much is enough? Computer Speech and Language , 19:415 X 432.
 Ian H. Witten, Eibe Frank, and Mark A. Hall. 2011. Data
Mining: Practical Machine Learning Tools and Tech-niques . Morgan Kaufmann Publishers.
