 Big data has introduced many challenges to traditional database systems, e.g., reducing with the storage and performance needs in big data management, database clusters involving several small servers can offer better time performance for storing and querying big data [1, 2]. Moreover, the size of such a database cluster can be adjusted to workloads so that we can dynamically turn on/off the nodes to obtain better energy proportionality [3]. 
Energy proportionality means that the energy consumption of a system is proportional to the workloads running on it [3]. This issue was first studied by Google researchers in 2007 [3], where they tested the energy consumption and server utilizations for 5,000 servers running Google services. Consequently, they found that generally all the nodes only used about 20% of their CPU capabilities. In other words, can be turned off to save energy. However, it is not a trivial task to realize an energy-proportional database cluster. The most critical challenge is that we have to keep high time performance when making some nodes power-off. Therefore, we have to find a best trade-off between time performance and energy savings. Obviously, the best solution has to adapt to workload changes. For this purpose, effect ive algorithms regarding load allocation and node activation/deactivation should be proposed. 
Aiming for providing an energy-proportional database cluster to meet the urgent energy-proportional database clusters, and then present new algorithms involving load allocation and node activation/deactivation to deal with the key issues in such a cluster. In summary, we make the following contributions in this paper: (1) We introduce a new architecture for energy-proportional database clusters. It is a hybrid architecture combining the share-nothing and share-disk architectures. Furthermore, we introduce a query stream buffer on top of the hybrid architecture to cache new incoming queries, so that we are able to keep high time performance even when the cluster is over-loaded. ( Section 3 ) (2) We propose an unbalanced load allocation algorithm to distribute workloads traditional load balance algorithms for clusters. ( Section 4 ) (3) We present new algorithms for node activation/deactivation that can adapt to work-load changes. ( Section 5 ) (4) We build a prototype database cluster consisting of five nodes and conduct comparative experiments using the TPC-H benchmark on PostgreSQL, with various metrics, varying numbers of query streams, and different query patterns. ( Section 6 ) The researches on energy-aware data management are mainly based on two viewpoints, namely single-server-based and cluster-based. So far, most previous work focuses on the single-server-based energy-eff icient approaches [4-6], which aims to such as buffer management, query processing, and indexing. For example, in the literature [4], the authors propose the QED method to reduce the energy costs in query clustered into some small groups, such that the queries in each group can be evaluated together. They demonstrate that this approach is more energy-efficient than traditional query processing methods. 
Although single-server-based energy-efficient approaches are helpful to reducing the total energy costs of a database system, many studies have shown that even a stand-by more and more attention from both academia and industries [7, 8]. proportionality on commodity hardware. WattDB uses the share-storage architecture and all the data are placed in a single node, which may lead to single-point failure. In the experimental results, they present some guiding principles for building energy-efficient clusters. The most critical issue in the cluster-based energy proportionality is turn on or off some nodes if necessary. They use various predictive methods including Last Arrival, MWA, Exponentially Weighted Average, and LR, to predict the future request rate for a website cluster, and then accordingly add or remove servers from a heterogeneous pool. Our node activation/deactivation depends on more than the prediction result. Specifically, in addition to the prediction result, we also consider the change patterns of workloads so as to determine the right time to turn on /off nodes. 3.1 General Idea We propose a new architecture for energy-proportional database cluster, as shown in Fig. 1. It consists of a control node and several backend nodes, and the backend nodes are divided into share-disk nodes and share-nothing nodes. 
The motivation of our design is two-folds. First, the architecture should support removing nodes from the cluster. This is intrinsic for energy proportionality, because Therefore, we first use the share-disk architecture in Fig. 1. However, the share-disk architecture is not suitable for storage-volume extension. Hence, we further introduce the share-nothing architecture for organizing the storage nodes, as shown in the bottom part of Fig. 1. Thus, the share-nothing nodes can be regarded as storage nodes, forming a distributed storage center, where each node can have their own storage media such as HDD and SSD (Solid State Drives). On the other hand, the share-disk on/off while the share-nothing nodes should be kept active. 
The control node has two main modules: Load Allocation queries to share-disk nodes. The Node Activation / Deactivation and turn off nodes when necessary. Both the two modules run on top of a new incoming queries, which offers necessary information for load allocation and node activation/deactivation. The details of the QSB 3.2. We will discuss the algorithms of Load Allocation Deactivation in Section 4 and 5, respectively. 
For the purpose of energy proportionality, the control node will change the power states of share-disk nodes according to wo rkload changes. Basically, a share-disk node has the states shown in Fig. 2. A share-disk node is initially at the adapter alive so that the control node can tu rn on the node when needed by using the Wake-on-LAN (WOL) technology [12]. When the node is turned on, it goes into the start up the database system. Then the node changes into the accept query requests. A node with workload running is at the the energy consumption is approximately linear proportional to the usage of CPU. nodes at these two state active nodes. When a share-disk node receives shutdown request from control node, it moves to the power off state. A node at this state firstly shut down the database system and then executes the shutdown procedure. The node changes into standby state again when it finishes the procedures. 3.2 Query Stream Buffer The Query Stream Buffer (QSB) is an enhanced module in the control node to cache the new incoming query streams when the cluster is overloaded. It is motivated by the following observations: (1) We found that the execution time of a query stream is linearly growing with the increase of concurrent number of queries when the nodes are overloaded (details can be queries, which will in turn worsen the response time for the queries running in the node. (2) We found that the powerful nodes with high processing capability usually finish the tasks quickly and are very likely to turn to the the weak nodes with low computing resources tend to be overloaded. Without overloaded. Consequently, each node will have the similar number of queries. However, since the processing capability varies from node to node, the powerful nodes can quickly finish the queries while the weak nodes may be overloaded. To this turns to the idle state. Hence, we can better balance the workloads among the share-disk nodes. (3) When the current workload is becomi ng heavy, we need to wake up some ready for receiving queries. Thus, it is better to cache the queries (using when they are ready. Otherwise, the query streams during the wake-up time period worsen the time performance of the cluster. Figure 3 shows the basic structure of QSB . Note that workload exceeds the current capacity of the active nodes. We use the mechanism to maintain the buffered query streams. In particular, new query streams will be placed in the queue tail of QSB . When a node is ready to receive new queries, move the query in the head of the queue to the available node. 
The benefits of the Query Stream Buffer are three-folds. First, the control node can mode and will not be overloaded. Second, as QSB caches the extra query steams temporarily, the powerful nodes will not be idle when the buffer is not empty. Thus, we can let the powerful nodes in the load state and improve the overall time performance of the cluster. Third, by caching the new queries during the wake-up new queries can be distributed to the newly wake-up nodes. 
We conduct an experiment to verify the efficiency of QSB allocates all query streams to available share-disk nodes and we compare two different methods when allocating the workloads: with QSB method with QSB , the control node caches the extra streams when the cluster is overloaded. For the method without QSB , the control node directly allocates the query streams to the share-disk nodes when it receives new query streams. Figure 4 shows the total execution time of forty query streams for a single node and the cluster. For the method with QSB , the execution time of a query stream includes the time that consumes and the execution time. We can notice that QSB can effectively reduce the execution time for different kinds of nodes. As a result, performance up to 49.7% compared with the method without In this section, we propose our load allocation algorithm that utilizes the Differing from traditional load balance de sign for clusters, we adopt the load make full use of the current active nodes and therefore let more nodes to be These idle nodes can be set to the standby state with some appropriate node activation/deactivation algorithm, and thereby we can reduce the energy consumption of the cluster. 
Algorithm 1 shows the load allocation pro cedure. When the control node receives a new query stream, it first checks all active share-disk nodes to see if any of them can accommodate this new query. If one of these nodes has less query streams running than its capacity, the control node will send the new query stream to it. If there are no query stream into QSB . Next, we invoke the function cached query streams exceed the predefined size of QSB . 
The proposed load allocation algorithm differs from the previous work. First, it makes good use of the QSB buffer that we prove to be effective for time performance. according to our algorithm, a new query stream will be cached instead of being streams can be issued to any of the share-disk nodes later when they have finished at nodes according to their execution speeds. In other words, our load allocation algorithm combines the advantages of load balancing and unbalancing policies. 5.1 Node Activation The node activation algorithm needs to ma ke two decisions: (a) when to activate nodes and, (b) how to predict the future workload with the help of the number of nodes to be turned on. 
Regarding the first issue, we first classify workloads into two different types while the second type is gradually changing. In turn, we propose two methods to decide when to turn on nodes w.r.t. the different changing patterns of the workload: (1) For the rapidly changing workloads, we propose the judge has been used in the lo ad allocation algorithm shown in Fig. 5). The algorithm detects whether it is necessary to turn on some standby nodes and activate nodes after that. This algorithm runs on top of query streams in QSB reaches the size of QSB , which means that the workload exceeds future workload, the control node determines whether to turn on additional nodes and how many nodes should be turned on. If the prediction result still outstrips the size of QSB , control node determines to turn on a few nodes, the number of which is more energy. To ensure high time performance, the powerful nodes in the cluster are first selected as new active nodes. (2) For gradually changing workloads, we have to consider a new method rather of the cached query streams exceeds the size of situation that the workload is gradually changing. In this situation, when the workload streams in QSB will be still less than the size of QSB whenever the first query stream is put into QSB. When the last query stream is moved out from QSB , the timer stops and resets to zero. If the aggregated time duration of the timer exceeds a threshold called Timer _ powerup , the control node activates a standby node to enhance the capability of the cluster. 
Regarding the second issue, namely future workload prediction, we adopt a simple way to predict the future workload to reduce the time overhead of the prediction procedure. Suppose that at time T we start to predict the future workload and it costs boot-up period is similar to that in the last period of Tpowerup future workload by the following Formula (1). Here, Load represents the size of QSB at the time represents the current number of the query streams in the number of the new query streams and  X  completedqueries the completed query streams during Tpowerup . We also use a sample thread to record the useful information about the cluster during every Tpowerup 5.2 Node Deactivation allocation algorithm, the workload is mo re likely to concentrate on a few active nodes. Consequently, the remaining active nodes will stay in the turned off to save energy consumption. Unlike most previous algorithms, we allow each active node in the cluster to decide independently whether to turn itself off or not threshold Timer _ shutdown . Similarly, the timer starts when the node X  X  state changes into idle and resets to zero when the node receives a new query stream. 6.1 Experiment Setup Our prototype database cluster consists of a control node and four backend nodes, running PostgreSQL 9.2.0. All components are interconnected using the TP-LINK Gigabit switcher. The control node can turn on the standby on -Lan technology [12]. Each backend node is with an Intel CPU (i3, i5, i7, and G2030), a hard disk of Seagate ST1000DM003 1TB 7200RPM, and a memory of Kingston 8GB DDR3. The energy related information of the backend nodes are shown in Table 1, where the nodes are represented by their CPU type. 
We take queries from the well-known TPC-H benchmark as the workload for our different backend nodes consistent, we eliminate the update statements and use the 22 query statements to generate query streams. A query stream consists of 22 query statements and each query statement is random ly generated by official tools provided by TPC-H. Besides, the sequence of 22 queries in a query stream is randomly arranged. 
To observe the effect of energy proportionality, we need to run the workload for a period of time and dynamically turn on/off nodes. To the best of our knowledge, there is no such long-term workload benchmark for database clusters. Thus, we manually create three types of workload patterns in our experiment. The three types of workload patterns are Fixed Type , Poisson Flow , and Step Type the number of queries changes regularly. For the Poisson Flow streams generated each time satisfies the Poisson distribution. Both the and Poisson Flow are changing dramatically. In addition, we also create the Type , in which the number of queries is gradually changing. The total number of query streams of the above three patterns is 268, 302, and 680 respectively. 
In addition, we compare our proposal with three baseline methods: (1) AlwaysOn [2], which always leaves the backend nodes active regardless of the workload change. (2) Reactive [2, 15], which reacts to the current number of query streams, attempting to attempts to predict the future workload by using the linear regression method. It adjusts the number of active nodes according to the prediction result and the current capacity. For this policy, we consider a time window of 100 seconds (ten sample times). 6.2 Proportionality in Node Activation In this section, we show the advantages of our proposal in node activation. For each workload running, we sample every 10 s econds to record the number of active backend nodes and concurrent query streams. 
We show the performance of our proposal in Fig. 5. In our algorithm, there are two tunable parameters named Timer _ powerup and Timer _ powerup to 10s and Timer _ shutdown to 40s. We will discuss the effect of these two parameters later. 
Our proposal adopts the load unbalancing policy so that it can shut down unnecessary nodes quickly when needed. For instance, at the beginning of the Timer _ shutdown ), while LR -Predictive needs 200s to complete this procedure. Further, our algorithm can effectively reduce the misjudgment phenomenon, which noticeable in the Step Type workload. We can find from Fig. 5 that our design introduces rare misjudgments and the number of active nodes is pretty close to the current workload. The major reason is that we adopt the time-out principle to conduct the decision of node performance with the help of QSB . sample time. Then, we compare the actual conditions with the ideal ones. Figure 6 shows the average distance between the actual number and the ideal number of the active nodes, finishing the workload. We can see that our proposal obtains a value more close to the idle Reactive policy has the least difference for the drawbacks, as we have discussed before. 6.3 Time Performance shown in Fig. 7, where our algorithm is denoted as TAS . As Fig. 7 shows, our algorithm ( TAS ) is superior to Reactive and LR -Predictive shows the execution time of our proposal on the Remember that there are totally 680 query streams in the pages in the node are cached in the buffer. Thus, we have to read pages from disks when starting to process queries in those newly active nodes. 
The performance degradation percentage compared to AlwaysOn streams to some extent. Considering that we have determined the capacity of a node by P _ degradation , i.e., 30%, only our proposal meets the performance requirement. 6.4 Energy Savings Table 3 shows the energy saving percentage compared to Alwayson total energy consumption. We collect the en ergy consumption via power meters attached read the values of energy consumption from the screens of the power meters. 
As Fig. 9 shows, our method ( TAS ) consumes the least energy for all kinds of workload patterns. Regarding the energy savings, our method saves up to 21.38% energy compared to Alwayson under the Poisson Flow workload pattern. The ability to save energy not only depends on the energy proportional algorithms, but also the low-utilization period in workload and energy parameters of each node. So far, we can find efficiently under the premise of ensuring the query execution performance. In this paper, we construct an energy-proportional database cluster enhanced with a Query Stream Buffer on top of a hybrid architecture involving the share-disk and share-nothing architecture. We also presen t new algorithms involving load allocation finally build a prototype database cluster consisting of five nodes and conduct comparative experiments using the TPC-H benchmark on PostgreSQL, with various demonstrate that our proposal is superior to its competitors and provides new insights for storing and querying big data. 
