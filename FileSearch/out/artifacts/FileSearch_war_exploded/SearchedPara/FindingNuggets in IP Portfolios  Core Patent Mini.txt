 Patents are critical for a company to protect its core tech-nologies. Effective patent mining in massive patent databases can provide companies with valuable insights to develop strategies for IP management and marketing. In this paper, we study a novel patent mining problem of automatically discovering core patents (i.e., patents with high novelty and influence in a domain). We address the unique patent vocab-ulary usage problem, which is not considered in traditional word-based statistical methods, and propose a topic-based temporal mining approach to quantify a patent X  X  novelty and influence. Comprehensive experimental results on real-world patent portfolios show the effectiveness of our method. H.2.8 [ Database Management ]: Database Applications X  Data Mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.7 [ Arti cial Intelli-gence ]: Natural Language Processing X  Text Analysis Core patent mining, patent novelty, patent influence, textual temporal analysis
Effective patent portfolio management requires the assess-ment of patent quality, identification of technology gaps, and analysis of competitors X  patent activities. This current work focuses on identification of core patents (patents with high novelty and influence). Automated Core Patent Min-ing (CPM) can play an important role in patent portfolio management and R&amp;D strategy development.

CPM improves the efficiency of patent portfolio manage-ment. For a company with large number of patents, it can be time consuming and costly to manually screen patents to identify ones with licensing potential and/or patents with low values that could be dropped. By automatically evalu-ating a patent X  X  novelty and influence, CPM methods help IP analysts to focus on a smaller set of patents that require manual analysis. In this way, the overall time on patent analysis is reduced, resulting in more efficient portfolio man-agement.
 CPM can also be used to track competitors X  activities. Generally speaking, patent work reveals research details be-fore a product hits the marketplace. By continuously mon-itoring the change of core patents and their owners, a com-pany can spot competitive threats, and adjusts its R&amp;D strategy accordingly.

In this paper, we propose a topic-based temporal min-ing approach that quantifies the novelty and influence of a patent, and ranks all patents by combining the novelty score and influence score. The top-ranked patents are selected as core patents in the domain. Unlike traditional word-based statistical methods, our approach considers the unique vo-cabulary usage in patent literature, and can effectively dis-cover core patents from massive patent databases.
The rest of the paper is organized as follows. We survey related work in Section 2. Section 3 formulates the task of core patent mining and illustrates the problem challenges. Ou r methodology is described in Section 4. Section 5 ana-lyzes the experimental results, and Section 6 concludes our work.
Our work is mainly related to patent quality assessment, and document novelty and influence analysis.
Liu et al. [4] study the problem of predicting patent qual-ity. They propose a measurement model in which patent quality is a latent variable. Patent quality can be esti-mated from correlated measurements, including court rul-ing decisions and some lexical features. Jin et al. [3] work on automatic patent maintenance recommendation. Their method extracts a set of patent lexical features. Based on the features, a classifier is trained on historical patent main-tenance decisions, and predicts whether a patent should be maintained or not. Finally, they propose a network-based optimization process to refine the prediction results.
Our work is different from the above. Our method only utilizes patent text as input. Court ruling decisions used in [4] and patent maintenance records used in [3] are only available for a small number of patents. In addition, [4] and [3] are more focused on the writing quality of patents, while we assess patent quality from the technical perspective. Hasan et al. [2] analyze patent novelty from patent claims. Their method obtains a set of keywords from the claim sec-tion. The score of a keyword in a patent equals to the ratio of the keyword X  X  support to its age (i.e., the time difference between the word X  X  first appearance in patents and the issue year of the subject patent). A patent X  X  novelty score is the sum of all its keywords X  scores.

Shaparenko et al. [6] discover important documents in a document collection. The documents are clustered by their word bags. A document is important if it has fewer similar documents published before it, and has more similar docu-ments published after it.

Unlike the above work, our approach is topic-based, and addresses the unique vocabulary usage in patent literature. The experimental results show that our approach is more effective than the methods in [2] and [6].
Suppose the patent set in a domain is D = { D t | t = 1 ; :::; T } , where D t  X  X  contains the patents issued at year t . We define patent novelty and patent influence as follows.
Definition 1 (Patent Novelty). A patent d  X  D t is novel if its ideas are not presented or little mentioned in its prior art, i.e., D P A ( d ) = { D i | 1  X  i &lt; t } .
Definition 2 (Patent Influence). A patent d  X  D t is in uential if its ideas are adopted or expanded by its follow-up work, i.e., D F W ( d ) = { D i | t &lt; i  X  T } .
In this paper, we use latent topics to represent the ideas in a patent, and quantify patent novelty and influence by topic activeness variations. Then we combine the novelty score and influence score to rank the patents in D , and select the top-ranked patents as core patents in the domain.
As a scientific literature with legal significance and po-tential profits, patents have complex structures and special nomenclature. The sophisticated patent language can pose significant challenges to patent mining. We have noted that the semantic meanings of technical terms in patents are of-ten inconsistent due to the following two reasons. 1. Lack of standard terminology for emerging tech-2. Heterogeneity in nomenclature. Some technical
A combination of these factors greatly raises the diffi-culty of capturing the real contributions of a patent. Classi-cal word-based statistical methods, such as TF-IDF [2] and word similarity [6], are based on a homogeneous assumption. Those methods can cause serious word mismatch problem in patent literature, since inventors may use different terms in different fields to describe the same technology. In ad-dition, traditional TF-IDF methods can only discover the significant-frequent keywords, yet miss many signi cant-rare keywords which are low-frequency but highly informative.
To address the unique vocabulary usage in patent litera-ture, we propose a novel topic-based temporal mining ap-proach. We use topics to depict the ideas in patents, which can cluster synonyms and relevant keywords to avoid the word mismatch problem. In addition, the topic activeness trend is used to characterize the technology developments in a field, which outperforms the traditional word statistics.
Our method consists of three steps. Firstly, identify la-tent topics in the patents. Secondly, model topic activeness trend, and remove noisy topics. Thirdly, quantify patent novelty and influence, and rank patents by their scores.
Patents are semi-structured documents containing several sections. Each section has a specific purpose. For exam-ple, abstract gives an overview of the invention, background summarizes related works, and detailed description explains the invention in details. The claim section is the heart of a patent as claims define the protection scope of the in-vention. In this paper, we use title , abstract , claims and detailed description to analyze patent novelty and influence. All patents are transformed into lowercase with stopwords removed.

We utilize the distributed version of Latent Dirichlet Allo-cation model (AD-LDA) [5] to efficiently discover the latent topics inside the patents in a domain. Suppose there are K topics { z 1 ; :::; z K } in the patent set D = { D 1 ; :::; D topic z k is a probabilistic distribution of words in the word set V of D , i.e., { p ( w | z k ) } w  X  V . A patent d  X  X  abilistic distribution of topics, i.e., { p ( z k | d ) } better represent the semantic meaning of a topic, we also estimate the topic-bigram distributions. The probability of a bigram phrase w i w j ( w i  X  = w j ) generated by a topic z T able 1: 10 Basic Jittering Patterns for 3 Topic Ac-tiveness Levels (1-inactive , 2-active , 3-bursty ) est imated as follows. where p ( w i w j ) is the occurrence probability of the phrase w w j , and is estimated as p ( w i w j ) = wh ere c ( w i w j ; d ) is the count of the phrase w i w
Finally, for each topic z k , we extract a set of unigrams and bigrams with the highest probabilities under z k (i.e., p ( w | z k ) and p ( w i w j | z k )) to be the topic signatures of z For each discovered topic, we use a Markov-Modulated Poisson Process (MMPP) [1] to model its activeness trend. The time span of D is divided into T time intervals, and each interval lasts for a year. Suppose there are M levels of topic activeness, and each level is represented by a hidden state in ascending order S i  X  X  1 ; :::; M } ; i = 1 ; :::; T . MMPP X  X  observation is the occurrence count of the topic X  X  signatures in each interval, and the emission probabilities are set as Poisson distribution: where B [ C i ][ S i ] is the emission probability for state S generate C i signatures of the topic in the ith interval; is the expectation of the topic X  X  signature count in the ith interval. We equally divide the range of all observations {
C i | 1  X  i  X  T } into M bins, and the initial value of i is set as the mean value of the observations in the bin that C i belongs to. The initial state probabilities and transition probabilities of the Markov chain, and also the rate parame-ters of the Poisson process are estimated via EM algorithm. Finally, we sort the value of { i | 1  X  i  X  T } in ascending or-der, and the topic activeness level in the ith interval equals to the rank of i , i.e., S i = Rank ( i ).

Topic models have a common problem: the number of topics must be determined in advance. It is inevitable that some topics have little semantic meanings (i.e., noisy topics). In this paper, we propose an automatic noisy topic filtering method. We observe that noisy topics can be characterized by their activeness variations, and we have summarized two classes of noisy topics. The first class is trendless topics , whose activeness trends have few variations along the time-line. The second class is jittering topics . A topic jitters when its activeness fluctuates capriciously in adjacent time intervals. Suppose there are 3 topic activeness levels, we can define 10 basic jittering patterns as shown in Table 1. More complex jittering patterns can be decomposed into several basic jittering patterns. If a topic jitters in too many time intervals, we consider it as a jittering topic. All the trendless topics and jittering topics are removed as noisy topics.
After removing the noisy topics in a domain, we quan-tify a patent X  X  novelty and influence by analyzing the topic activeness variations along the timeline.

For each patent d  X  D t , our method uses its dominant top-evaluate the novelty of d , we focus on the activeness trends of dominant topics in d  X  X  prior art { [ S i;z ] | t  X  1 i =1 where S i;z is the activeness level of topic z in the ith interval. Low topic activeness in the prior art indicates that d is very novel. To evaluate the influence of d , we focus on the topic trends in d  X  X  follow-up work { [ S i;z ] | T i = t +1 ; z High topic activeness in the follow-up work indicates that d is very influential.

However, temporal bias exists in patent novelty and influ-ence analysis. On one hand, an old patent has fewer patents in its prior art, and more patents published after it. On the other hand, a new patent has fewer patents published after it, yet has more patents in its prior art. Thus, old patents tend to be over-estimated in their novelty and influ-ence, while new patents are under-estimated. We have no-ticed that core patents are time-sensitive, and their values depend on certain aspects of the technology developments. Thus, it is more appropriate to measure a patent X  X  novelty and influence within a certain period of the topic activeness trends. In this paper, we use time decay factor to restrict the scope of the topic trends, and eliminate the temporal bias in patent novelty and influence analysis.

We consider two typical window functions to determine the time decay factor, namely Rectangular window and Gaus-sian window . Suppose  X  t is the time difference between two time points, and 2 is the window size. Rectangular window and Gaussian window are defined in (3) and (4) respectively. For each topic z , the normalized topic activeness level S Nor ( i; z ) in the ith interval is defined as: where  X  t is the time difference between the pending interval t and neighboring interval t  X  , i.e.,  X  t = t  X  t  X  .
To quantify the novelty of patent d , we determine the novelty score of topic z  X  Z Dom ( d ) as follows:
The novelty score of d is the sum of its dominant topics X  novelty scores, N ovelty ( d ) =
To quantify the influence of d , we determine the influence score of topic z  X  Z Dom ( d ) as follows:
The influence score of d is the sum of its dominant topics X  influence scores, Inf luence ( d ) =
The score of d is the product of its novelty score and in-fluence score, Score ( d ) = N ovelty ( d )  X  Inf luence ( d ). We rank all patents by their scores, and the top-ranked patents are selected as core patents in the domain.
We construct our dataset with patents from the petroleum industry. Firstly, we select 108 large petroleum companies listed by Wikipedia. Then we download the patents assigned to the 108 companies from USPTO patent database. Our dataset contains 82,648 U.S. patents from 1976 to 2010. Secondly, we define a set of domains based on the United States Patent Classification (USPC) system, which is an authoritative classification standard adapted by USPTO. Each U.S. patent has a mandatory USPC class according to its technical subject, and we use this class to classify the patents. A USPC class is defined as a domain, if it has at least 800 patents in our dataset. The remaining classes with their patents are not used in our experiments, since a domain with too few patents is more suitable for manual analysis. As a result, we have defined 21 domains in the petroleum industry with 65,846 U.S. patents, accounting for 79.7% patents in our dataset.
The parameters in our method are set as follows. In the topic identification step, we empirically set the topic-patent density to be 200 patents per topic in each domain. We se-lect 50 unigrams and 50 bigrams with the highest probabil-ities under a topic to be the topic X  X  signatures. In the topic activeness modeling step, we set 3 topic activeness levels in the MMPP model to trade-off between performance and training complexity. To show the effectiveness of MMPP, we also use equal-size binning to model the topic activeness trend, in which the range of a topic X  X  signature counts is equally divided into 3 bins, and the topic activeness level in an interval depends on the bin that its signature count belongs to. A topic whose activeness trend reaches inactive or bursty only once, or jitters in over 50% of all intervals, is removed as a noisy topic. We also test the effects of Rect-angular window and Gaussian window for topic activeness normalization.

We have also implemented three word-based statistical methods to compare to our approach. Baseline 1 (COA1) is the algorithm used in [2]. In this method, stopwords are removed from the patent text. Those words whose document frequency exceeds 90% in the patent set are also removed as domain stopwords. For each word w in a patent d , the contribution of w is determined as follows: where age ( w ) is the time difference between the earliest year w occurs in the patent set and the issue year of d ; support ( w ) is the number of follow-up patents that contain w . The score of d is the sum of the contributions of the words in d .
Baseline 2 (COA2) takes the same procedures as in base-line 1 (COA1). The difference is that the score of a patent equals to its word count after removing domain stopwords.
Baseline 3 (KeyPlayer) is adapted from [6]. Each patent is represented as a TF-IDF vector of its words after remov-ing stopwords. Then for each patent d , the method finds the 50 most similar patents of d using the cosine similarity between the patents X  TF-IDF vectors. Finally, the score of Table 2: Four Con gurations of Our Approach Na me T opic Trend T o pic F iltering Ti me Window MR TF M M PP Y es R ect angular
MG TF M M PP Y es G au ssian d is calculated as follows: where Leader ( d ) and F ollower ( d ) is the number of the sim-ilar patents published before and after d respectively.
In this section, we utilize two indicators to help assess the discovered core patents in a domain.

The first indicator is patent forward citation , which is the citation count a patent receives from its follow-up work. In this paper, we assume that novel and influential patents (i.e., core patents) are more likely to receive more citations than those non-core patents in a domain.

Since new patents are less cited than old ones, patent for-ward citation tends to under-estimate the importance of new patents. To eliminate this bias, we evaluate the discovered core patents within each year, instead of on the whole time-line. All patents published in the same year are ranked by their scores, and also by forward citations as the gold stan-dard. Then the Spearman correlation coefficient of the two patent rankings is calculated to evaluate the algorithm. In addition, we also assign the 25% most cited patents as the real core patents in the domain, while the 25% highest scored patents as the discovered core patents. Precision and mean average precision (MAP) of the two patent sets are calcu-lated to complement the ranking correlation coefficient.
The second indicator is patent maintenance status . In the U.S., a patent can be kept valid for up to 20 years, and maintenance fees must be paid by the 4th (E1 stage), 8th (E2 stage) and 12th (E3 stage) years after the issue date of the patent. Due to the significant increase in the maintenance fees from E1 stage to E3 stage, assignees tend to abandon their worthless patents as early as possible (e.g., at E1 stage), and only those truly important patents are maintained at E3 stage to complete their full terms.
We have obtained the patent abandonment information between 1995 and 2011 from USPTO. To construct the gold standard, we regard those patents expired at E1 stage as non-core patents , and those patents maintained throughout the 20 years as core patents . In our dataset, there are 9527 core patents and 6078 non-core patents in the 21 domains. For each domain, the patents are ranked by their scores, and the top 20%, 40%, 60% and 80% patents are set as core patents respectively to construct 4 cut-off levels. False positive rate (FPR) and true positive rate (TPR) are cal-culated at each cut-off level to draw the algorithm X  X  ROC (Receiver Operating Characteristic) curve, and AUC (Area Under the ROC Curve) is used to evaluate the algorithm X  X  performance.
We first compare the effects of different configurations of our approach, as discussed in Section 5.2. Table 2 shows the Table 3: Experimental Results of Baselines ( =10) d etails of the configurations, including topic activeness mod-eling (BR vs. MR), noisy topics filtering (MR vs. MRTF), and time decay factor (MRTF vs. MGTF). We also test the algorithm X  X  performance by varying the window size param-eter to be 1, 5, 10 and 15 years respectively. Figure 1 shows the average results on the 21 domains in our dataset. From Figure 1, we can draw the following conclusions. Firstly, for all four methods, large window size can bring bet-ter performance than small window size. Small window only considers the most recent part of the topic trends, and loses a lot of information of technology developments in a domain. Thus, large window size is desirable to acquire sufficient in-formation to estimate a patent X  X  novelty and influence. Be-sides, we also notice that too large window size ( as 15) may deteriorate the performance by some metrics (e.g., precision and AUC in Figure 1). Since some patents published long before or after the subject patent may not influence or be influenced by that patent, too large window size may intro-duce unnecessary topic activeness variations that harm the algorithm X  X  performance. Secondly, MR outperforms BR on all metrics for large window size. This is because MMPP can estimate the topic activeness trend more accurately through global model fitting, and has better smoothing effect than equal-size binning. Thirdly, MRTF outperforms MR on all metrics, showing that topic filtering can effectively remove noisy topics and improves the algorithm X  X  performance. Fi-nally, Gaussian window (MGTF) outperforms Rectangular window (MRTF) on all metrics. To eliminate the tempo-ral bias in core patent mining, Rectangular window simply drops the non-recent topic information away, while Gaus-sian window maintains more useful information through the smoothing factor. Thus, Gaussian window can better reflect the time-sensitive characteristic of technology developments.
Next, we compare the performance of the baselines against our approach (MGTF) with large window size ( as 10). Ta-ble 3 shows the experimental results, and we can draw the following conclusions. Firstly, although baseline 1 (COA1) uses more complex function, it performs worse than baseline 2 (COA2) on all metrics, which agrees with the experimental results in [2]. The unique patent vocabulary usage discussed in Section 3 can bring significant noises in word statistics, thus quantifying the patent value by term weights (COA1) would be more error-prone than word counts (COA2). Sec-ondly, baseline 2 (COA2) outperforms baseline 3 (KeyPlayer) on citation-based metrics (precision, MAP and Spearman coefficient), while baseline 3 (KeyPlayer) performs better on maintenance-based metric (AUC). This reflects the fact that those patents similar to their prior art in content have less values in the business market, thus are more likely to be abandoned. Finally, MGTF outperforms all baselines on all metrics. The core patents discovered by our method are more likely to be cited by their follow-up work, and have longer maintenance lifespan than those non-core patents.
In this paper, we study automatic core patent mining, which is an important problem in patent portfolio manage-ment. Compared to traditional word-based statistical meth-ods, our topic-based temporal mining approach can effec-tively discover novel and influential patents, thus can help companies develop better IP strategy and improve compet-itiveness.
This work is supported by ExxonMobil Research and En-gineering Company. We would like to thank the three anony-mous reviewers for their valuable comments. [1] W. Fischer and K. Meier-Hellstern. The [2] M. A. Hasan, W. S. Spangler, T. Griffin, and A. Alba. [3] X. Jin, S. Spangler, Y. Chen, K. Cai, R. Ma, L. Zhang, [4] Y. Liu, P. yun Hseuh, R. Lawrence, S. Meliksetian, [5] D. Newman, A. Asuncion, P. Smyth, and M. Welling. [6] B. Shaparenko, R. Caruana, J. Gehrke, and
