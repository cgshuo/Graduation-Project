 In this tutorial we discuss the evaluation of recommender systems. We discuss the main reason for evaluating recommender systems, i.e., making the most appropriate choices for a given task. We overview some general guidelines for conducting evaluation tests. We then discuss the evaluation of the system accuracy given specific system tasks. We also overview many properties of recommender systems, and explai n how these properties can be evaluated. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Information filtering. Algorithms, Performance, Reliability, Experimentation. Keywords are your own designated keywords. Evaluating the effectiveness of a recommendation system is a difficult task. In the past, many methods were suggested for recommendations. Little work was devoted to other properties of the recommendation problem, such as the novelty, diversity, and serendipity of the system. In this tutorial we begin with discussing system performance, focusing on identifying appropriate methods for different system tasks, such as predicting ratings, recommending items, or maximizing profit. We show how different choices for an evaluation criterion may result in selecting different methods. Thus, it is most important to use the most appropriate method for the task at hand. We discuss several methods for computing the system accuracy in novelty are also important for the success of recommender systems. We discuss a set of su ch properties, suggesting for each property appropriate evaluation criteria. We also discuss briefly testing, user studies, and online experiments, a nd discuss their merits. The tutorial will be st ructured as follows: 1. Why should we evaluate recommender systems -the selection problem: We begin with discussing the r easons for evaluating recommender systems, focusing on the problem of finding the best system for an application or task, known as the selection problem. We also discuss the difference between the evaluation of recommender systems and the evaluation of classic prediction and classification algorithms from the IR and machine learning literature 2. Types of evaluation method s -offline, user studies, online flights -and what can be learned from each method. We discuss the general setting of evaluation experiments. We overview the strengths and w eaknesses of each method, and suggest the phase at which it should be used. For offline tests, we focus on simulating user interactions with the system. We also discuss briefly how one can a rrive at reliable conclusions. 3. Testing for accuracy -matchi ng the evaluation measure to the system tasks and goals. We suggest that different recommendation tasks should be evaluated using different evalua tion metrics. We match popular tasks with appropriate evaluati on metrics, and show what may happen when a wrong choice is made. 4. Beyond accuracy -testing other properties of the system. We overview a large set of r ecommender system properties, suggesting how these properties can be evaluated. We also discuss the case of multi-criteria evalua tion -measuring the tradeoff between properties. [1] Gunawardana, A. and Shani, G. 2009. A Survey of Accuracy [2] Gunawardana, A. and Shani, G. 2010. Evaluating [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., and Riedl, J. [4] McNee, S. M., Riedl, J., a nd Konstan, J. A. 2006. Being 
