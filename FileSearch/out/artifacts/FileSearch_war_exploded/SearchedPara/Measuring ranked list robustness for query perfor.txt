 Yu n Z h o u  X  W. B r u c e C ro f t Abstract We introduce the notion of ranking robustness, which refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of uncer-tainty in the ranked documents. We propose a statistical measure called the robustness score to quantify this notion. Our initial motivation for measuring ranking robustness is to predict topic difficulty for content-based queries in the ad-hoc retrieval task. Our results demonstrate that the robustness score is positively and consistently correlation with average precision of content-based queries across a variety of TREC test collections. Though our focus is on prediction under the ad-hoc retrieval task, we observe an interesting negative correlation with query performance when our technique is applied to named-page finding queries, which are a fundamentally different kind of queries. A side effect of this different behavior of the robustness score between the two types of queries is that the robustness score is also found to be a good feature for query classification.
 General terms Algorithms  X  Experimentation  X  Theory Keywords Ranking robustness  X  Query performance prediction  X  Query classification  X  Named-page finding  X  Ad-hoc retrieval 1 Introduction In a typical retrieval system, a user forms a query according to his information need and a number of documents are presented to the user by the retrieval system in response to the query. Query performance prediction refers to the process of estimating the quality of the output of a retrieval system in response to a user X  X  query without any relevance information. Compared to the long history of developing sophisticated retrieval models for improving performance in IR, research on predicting query performance is still in its early stage. However, researchers have started to realize the importance of this problem and a number of new methods have been proposed for prediction recently [ 27 ]. The ability to predict query performance has the potential of a fundamental impact both on the user and the retrieval system.

From the perspective of a user, performance prediction provides valuable feedback that can be used to direct a search. For example, when the retrieved documents are estimated to be of low quality, the user may rephrase his query or be more willing to cooperate with the system to improve retrieval effectiveness, such as providing relevance feedback. With the help of prediction, the user can quickly form a good query to acquire satisfying results for his information need. Otherwise, the user must spend time reading the returned documents to rewrite the query when the results for the initial query are not satisfactory.
On the other hand, from the perspective of a retrieval system, performance prediction is the first step at solving the crucial problem of retrieval consistency. Current retrieval systems are evaluated by the average effectiveness on a fixed set of queries. Although failures on a small number of queries may not have a significant effect on average performance, users who are interested in these queries are unlikely to be tolerant of this kind of deficiency. A reliable system that always produces acceptable retrieval performance is more preferred by users than another system that works extremely well on a number of queries but occasionally makes terrible mistakes. To improve the consistency of retrieval systems, we first need to distinguish poorly-performing queries by performance prediction techniques. The important role of performance prediction in improving retrieval consistency has been recognized by the IR community. For example, in 2003, the Robust Track [ 2 , 22 ] was proposed by TREC, which addresses the problem of enhancing the retrieval of poorly performing queries. As the first footprint in finding a solution to this problem, the Robust Track requires systems to rank the queries by predicted effectiveness to investigate the capabilities of systems to detect hard queries [ 27 ].

However, accurate performance prediction with zero-judgment is not an easy task. The major difficulty of performance prediction comes from the fact that many factors, such as the query, the ranking function and the collection, have an impact on retrieval performance. Each factor affects performance to a different degree and the overall effect is hard to predict accurately. Therefore, it is not surprising to notice that simple features, such as the frequency of query terms in the collection [ 10 ] and the average IDF of query terms [ 25 ], do not predict well. In fact, most of the successful techniques are based on measuring some characteristics of the retrieved document set (usually in the form of a ranked list) to estimate performance. For example, the clarity score [ 4 ] measures the coherence of a list of documents by the KL-divergence between the query model and the collection model.

In this paper, we investigate another property of a ranked list of documents called ranking robustness, which refers to how stable the ranking is in the presence of uncertainty in the ranked documents. This method was first introduced in [ 30 ]. The idea of predicting retrieval performance by measuring ranking robustness is inspired by a general observation in noisy data retrieval that the degree of ranking robustness against noise is correlated with retrieval performance. Regular documents also contain  X  X oise X  if we interpret noise as uncertainty. We propose a statistical measure called the robustness score to quantify the notion of ranking robustness. For content-based queries in the traditional ad-hoc retrieval task, we demonstrate that the robustness score significantly and positively correlates with query performance in a variety of TREC test collections. In comparison to the clarity score method, our exper-imental results show that the robustness score performs better than or at least as good as the clarity score. Although the robustness score is initially designed for estimating topic dif-ficulty, we also explore the relation between the robustness score and retrieval performance of named-page finding (NP) queries. An interesting negative correlation with NP-query performance is observed, suggesting the fundamental difference in the retrieval processes for the two types of queries. Considering the opposite behavior of the robustness score between these two types of queries, our further investigation reveals that the robustness score is a good feature to distinguish between the two types.

The rest of this paper is organized as follows. Section 2 describes related work. In Sect. 3 , we propose a statistical measure called the robustness score to quantify the notion of ranking robustness. In Sect. 4 , we present our evaluations that show the effectiveness of our approach. In Sect. 5 , we summarize the main conclusions of this paper. 2 Related work 2.1 Query performance prediction Prediction of query performance has long been of interest in information retrieval and has been investigated under different names such as query-difficulty or query-ambiguity. Query prediction is a challenging task as shown in [ 21 , 27 ]. Some of the first success at addressing this task was demonstrated by the clarity score method proposed in [ 4 ].
 Recently, a number of prediction methods have been tried since the introduction of the TREC Robust Track in 2003. In the Robust Track systems are required to rank the queries by predicted performance, with the goal of utilizing the prediction capability to do query-specific processing. One thing we want to point out is that most study on performance prediction focuses on content-based queries in the ad-hoc task. At the time of writing this paper, we know of no published work that explicitly addresses other types of queries such as named-page finding queries.

Generally speaking, current prediction methods extract features of retrieval and compute the performance score for each query by using the features to estimate the query performance. One way to measure the quality of the performance prediction methods is to compare the rankings of queries based on their actual precision (such as MAP) with the rankings of the same queries ranked by their performance scores (that is, predicted precision).

Some researchers have used IDF-related (inverse document frequency) features as predic-tors. For example, Tomlinson et al. [ 25 ] adopted the weighted average IDF of the query terms for predicting. He and Ounis [ 10 ] proposed a predictor based on the standard deviation of the IDF of the query terms. Plachouras [ 20 ] represented the quality of a query term by Kwok X  X  inverse collection term frequency. The above IDF-based predictors showed some moderate correlation with query performance. These predictors are easy to compute but they do not take the retrieval algorithms into account and thus are unlikely to predict query performance well.

Inspired by the success of the clarity score, some researcher have proposed methods that are related to the ideas in the clarity score technique. Amati [ 1 ] proposed to use the KL-divergence between a query term X  X  frequency in the top retrieved documents and the frequency in the whole collection, which is very similar to the definition of the clarity score. He and Ounis [ 10 ] proposed a simplified version of the clarity score where the query model is estimated by the term frequency in the query. Motivated by the observation that the clarity score indicates the specificity of a query, they [ 10 ] also proposed the notion of the query scope, which is quantified as the percentage of documents that contain at least one query term in the collection. Diaz and Jones [ 6 ] extended clarity scores to include time features. They showed that using these time features together with clarity scores improves prediction.
Realizing that the retrieved document set provides valuable information for estimating retrieval performance, a few researchers have focused on investigating properties of the search results that may relate to search performance. Our approach and the clarity method measured by the Jensen-Shannon divergence between the retrieved document set and the measures to capture the geometry of the top retrieved documents for prediction. The most effective measure they found is the sensitivity to document perturbation, an idea somewhat similar to our idea. Generally speaking, techniques that make use of the search results for prediction are more accurate that those that do not.
 Other researchers have applied machine-learning techniques for prediction. For example, Elad Yom-Tov et al. [ 28 ] proposed a histogram-based predictor and a decision tree based predictor. The features used in their models were the document frequency of query terms and the overlap of top retrieval results between using the full query and the individual query terms. Their idea was that well-performing queries tend to agree on most of the retrieved documents. Kwok et al. [ 15 , 16 ] built a query predictor using support vector regression. For features, they chose the best three terms in each query and used their log document fre-quency and their corresponding frequencies in the query. They also included the number of top retrieved documents that contain some or all query terms as a feature. They observed a small correlation between predicted and actual query performance. Using visual features, such as titles and snippets, from a surrogate document representation of retrieved documents, Jensen et al. [ 7 ] trained a regression model with manually labeled queries to predict precision at the top 10 documents (P@10) in the Web search. They reported moderate correlation with P@10. 2.2 Information retrieval on noisy data With regard to text document collections in information retrieval, it is often convenient to assume that the contents of the collections are clean and free of errors. With the advent of large collections of multimedia documents (such as audio or image document), techniques such as OCR (optical character recognition) or ASR (automatic speech recognition) have been widely used to extract text from multimedia archives. In the following description, the text output of a recognition process applied to multimedia documents is noisy data or cor-rupted data since the recognition process is error prone and brings significant levels of noise to the data. The recognition process that produces corrupted data is data corruption .
One of the core problems in the field of information retrieval on corrupted data is to explore the impact of data corruption on retrieval effectiveness in order to design a ranking function that is robust to unexpected errors in corrupted data. Here a robust retrieval model means that some changes in document or collection statistics caused by data corruption do not alter the retrieval results much compared to retrieval on perfect documents (that is, the results of a recognition process with 100% accuracy).

A general observation about experiments on investigating the effects of data corruption is that as retrieval effectiveness improves, the ranking function becomes more robust against data corruption. For example, Lopresti and Zhou [ 11 ] explored the effectiveness of three retrieval functions on simulated OCR noisy data. They found that the ranking of the three functions with respect to retrieval effectiveness is the same as their ranking with respect to their ability to deal with simulated noise. Another example is that Singhal, Salton and Buckley [ 23 ] proposed a new robust length normalization method to alleviate the problem that the regular cosine normalization is sensitive to OCR errors. Although the original motiva-tion for this technique was to deal with OCR data corruption, surprisingly they found that the new normalization scheme also brought significant improvements on correct text collections in comparison to the original cosine normalization. Moreover, Mittendorf [ 17 ] studied data corruption effects on retrieval and presented a theorem on ranking robustness that partially explained the phenomenon that retrieval performance on corrupted data is often correlated with the degree of resilience against noise.

The above work reveals the interesting relationship between ranking robustness and retrieval performance. Although this work was done in the context of retrieval on noisy data, clean documents in regular retrieval also contain  X  X oise X  if we interpret noise as uncertainty. In the remaining of this paper, we will propose a framework to quantify ranking robustness and show its correlation with query performance. 3 Measure ranking robustness The notion of ranking robustness originates in the field of noisy data retrieval, where retrieval is performed on the output of a recognition process that exacts text from multimedia archives. Ranking robustness in noisy data retrieval refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of noise brought by the recognition process. Note that clean documents also contain  X  X oise X  if we generalize the notion of noise from recognition errors to uncertainty in text documents. For example, the meaning of a document may remain the same even after adding or deleting some words. Synonymy and homonymy are another two popular examples that can bring uncertainty to clean text docu-ments. Therefore, we can extend the notion of ranking robustness to regular ad-hoc document retrieval. In essence, ranking robustness reflects the ability of a retrieval system to handle uncertainty.

The idea of predicting retrieval performance by measuring ranking robustness is inspired by a general observation in noisy data retrieval that the degree of ranking robustness against noise is positively correlated with retrieval performance. We hypothesize that when it comes to regular ad-hoc retrieval, the positive correlation between robustness and performance still holds. Our hypothesis will be thoroughly examined in the next section.

Next we describe our way of measuring ranking robustness in regular retrieval. We begin by considering how to calculate ranking robustness in noisy data retrieval. If we can acquire a clean version of the corrupted data, one straightforward way is to compare a ranked doc-ument list from the corrupted collection to the corresponding ranked list from the perfect collection using the same query and ranking function. With regard to regular document retrieval, usually documents are assumed to be free of corruption. To simulate data cor-ruption, we assume that there exists a noisy channel which is analogous to the recognition process in noisy data retrieval. Documents are corrupted after going thought the channel. One way to implement the noisy channel is to design a document model for each document (Document models are distributions over words or other linguistic units). One corrupted version of the original document is one random sample from the corresponding document model.

Specifically, suppose we have query Q , ranking function G and collection C .Wegen-erate corrupted collection C by sampling from the document models of the documents in C . Then we perform retrieval on both C and C and two ranked list L and L are returned respectively. Finally we compute the similarity between the two rankings. Note that L is a fixed ranked list while L is a random variable. We call the expected similarity between L and L the robustness score and use it to measure ranking robustness. This process is illustrated in Fig. 1 .

Let us formally define the robustness score. Consider query Q and a document collection of M documents C = ( D 1 , D 2 ,... D M ) .Let V denote the size of vocabulary, both query Q and the documents are represented as vectors of indexed term counts, that is, where D k , i is the number of times that term i appears in document D k and q j is the number of times that term j appears in query Q . N denotes nonnegative integer and N V denotes a V -dimension vector space of nonnegative integer. Under our representation, collection C is a M  X  V matrix with nonnegative integer entries, that is, C  X  S ( M  X  V ) ,where S ( M  X  V ) denotes the set of a M  X  V matrix with nonnegative integer entries . The rows of matrix C can be viewed as a set of documents represented by V -dimension vectors.
 We introduce a few definitions before we show the computation of the robustness score. Definition 1 Retrieval function G ( D , Q ) Retrieval function G ( D , Q ) maps query Q and document D into a real number, that is, G ( D , Q )  X  R , D  X  N V , Q  X  N V Definition 2 Ranked list L ( Q , G , C ) permutation of the documents in collection C that describes the ordering of documents by decreasing G ( D , Q ) where D  X  C Definition 3 Document model X k and probability mass function (pmf) f X k ( x ) We assume that document D k , k  X  X  1 , M ] , corresponds to document model X k which is a V -dimension multivariate distribution and can be represented by a random vector X k = (
X term i occurs. The joint pmf of X k is the function defined by f X k ( x ) = f X k ( x 1 ,..., x V ) = Pr ( X k , 1 = x 1 ,..., X k , V = x V ) where x = ( x 1 ,..., x V )  X  N V .
 Definition 4 Ranking similarity Sim Rank ( L 1 , L 2 ) a real number that measures the similarity between the two ranked lists.(we assume that the documents in C 1 have one-to-one correspondence to the documents in C 1 ). Moreover, Sim Rank ( L 1 , L 2 ) should be bounded.
 Definition 5 Random collection X Given document model X 1 ,... X M ,where X k (k  X  X  1 , M ] ) is a V -dimension random vector, we define random collection X = ( X 1 , X 2 ,... X M ), that is, X is a M  X  V matrix whose entries consist of random nonnegative integers from some distributions. The pmf of X is the function defined by f X ( T ) = f X ( t 1 ,..., t M ) = Pr ( X 1 = t 1 ,..., X M = t M ) ,where X k denotes the k-th row of X and t k  X  N V , k  X  X  1 , M ] .
 With the above definitions, we give the definition of the robustness score.
 Given query Q  X  N V , retrieval function G , collection C = ( D 1 , D 2 ,... D M )  X  S ( M  X  V ) and random collection X = ( X 1 , X 2 ,... X M ) , the robustness score is defined as the expected value of random variable SimRank(L(Q,G,C),L(Q,G,X)) :
Robustness Score ( Q , G , C , X ) = E { Sim Rank ( L ( Q , G , C ), L ( Q , G , X )) } To m a ke E q . 1 feasible to calculate, we further make the following five assumptions: (1) We assume independence between any two document models X i and X j ,thatis, (2) Instead of the whole collection, only the top J retrieved documents in L ( Q , G , C ) (3) The Spearman rank correlation coefficient [ 12 ] is adopted to compute the value of (4) For each document model, we assume independence between any terms. We also (5) The expectation in Eq. 1 is very hard to evaluate directly. Instead, we independently The error of this estimation is proportional to the reciprocal of the square root of K [ 13 ]. According to our experiments, we find that a relatively small value of K is good and stable enough for query performance prediction.

In summary, evaluating robustness takes the following steps. First, we perform retrieval with query Q and retrieval function G . Then we generate J simulated documents using the document models of the top J documents retrieved and rank the simulated documents with the same query and retrieval function. The similarity between the two ranked lists is computed using the Spearman rank correlation coefficient. We repeat this K times and the average of the Spearman correlation coefficient is the robustness score.

We briefly explain why the robustness score defined above gives us useful information on retrieval performance. A low robustness score means that after document perturbation the ranking function provides a very different ranking compared to the original ranking. The low robustness score suggests that the degree of correlation between documents in the ranked list is low and the original ranking is more like a random ranking. In other words, the low robustness score is likely to correspond to a poorly-performing retrieval that returns a ranked list of loosely related topic covering many topics.

In the above discussion, we assume that the retrieval task is the traditional ad-hoc retrieval based on topic relevance. We will show later on that the use of the robustness score to predict retrieval performance is particularly appropriate for content-based queries. However, with regard to named-page finding queries that often have only a single relevant document, the expected positive correlation with query performance may not exist any more. This is largely due to the fact that top ranked documents in the ranked list in response to a named-page finding (NP) query are not necessarily related while those documents are connected more or less by topic in the case of ad-hoc retrieval. 4 Experiemntal results Our evaluation focuses on performance prediction within the context of ad-hoc retrieval at which the robustness method primarily aims. In addition, we investigate the effect of this technique on named-page finding queries. Namely, we consider the issue of predicting query performance for the two types of queries: content-based and NP finding queries, correspond-ing to the ad-hoc retrieval task and the NP finding task respectively. Other than performance prediction, we also investigate the possibility of utilizing the robustness score for query clas-sification, motivated by the different behavior of the robustness score in the two types of queries observed in our prediction experiments. 4.1 Prediction for content-based queries In this section, we present the results of predicting query performance by the robustness score within the context of the ad-hoc retrieval task. We adopt the clarity method as our baseline. Query performance is measured by average precision.

First, we study the correlation with average precision. Our results show that robustness scores have statistically significant correlation with average precision across a variety of TREC collections. We note that the clarity score is barely correlated with query performance on the GOV2 collection while the correlation between the robustness score and query perfor-mance remains significant. We also observe that a combination of the two usually performs better than either one when used in isolation.

Second, we perform a linear regression analysis to evaluate the ability to directly predict the value of average precision. This analysis reveals that the robustness score predicts the value of average precision better than the clarity score. Again, we observe further improve-ments with a combination of the two.

Our experiments use a variety of TREC collections and the web collection GOV2. All queries used in our experiments are titles of TREC topics. Table 1 gives the summary of these test collections.

With regard to the calculation of the robustness score, we use the query likelihood model [ 24 ] with Dirichlet smoothing as the ranking function (Dirichlet prior  X  is set to 1,000). We set parameter K in Eq. 4 X 100 and choose top 50 documents to compute the rank similarity in Eq. 4 . We tried different values of K ranging from 10 to 500,000 and found that the results change very little starting from 100. This means we do not have to require a large number of samples to compute robustness scores.

For computing the clarity score, we use the equations defined in [ 4 ] .The document model is estimated by using Dirichlet smoothing with Dirichlet prior  X  = 1 , 000. Relevance models are mixed from Jelinek-Mercer smoothed document models with  X  = 0 . 6.

To obtain average precision, all document retrieval is done by using the query-likelihood model and the results are evaluated by the trec_eval program. Again, Dirichlet smoothing with Dirichlet prior  X  = 1 , 000 is used for smoothing. 4.1.1 Correlation with average precision We measure the correlation with average precision by both the Kendall X  X  rank correlation test [ 12 ] and the Pearson X  X  correlation test [ 14 ]. Kendall X  X  rank correlation is a non-parametric test since it does not assume any distributions of both variables. In our experiments, Kendall X  X  rank correlation is used to compare the ranking of queries by average precision to the ranking by the clarity scores or the robustness scores of these queries. Pearson X  X  correlation reflects the degree of linear relationship between the two variables. 1 The values of both kinds of cor-relation range between  X  1.0 and 1.0 where  X  1.0 means perfect negative correlation and 1.0 means perfect positive correlation.

The results for correlation with average precision are presented in Tables 2 and 3 . When we combine the clarity score and the robustness score, we adopt a simple linear combination, that is, (1  X   X  )  X  clarity score +  X   X  robustness score. For the collections other than TREC 123, we use the  X  that yields the highest value of Pearson X  X  coefficient on TREC123. For TREC123, we use the best  X  on Robust 2004. In fact, we find that the optimal linear combination weight changes little across our test collections. Note that when using linear regression to combine the two, we essentially apply learning to our method. But we have only one parameter and we find the regression generalizes well.

From these results, we first observe statistically significant correlation between the robust-ness scores and the average precision over all test collections no matter, which metric is adopted. The extent of the correlation in the Robust 2004 Track is visible in Fig. 2 as a linear trend for average precision of queries to increase as their robustness score increases.
Second, we see that the linear combination of the two features usually performs better than either one when used in isolation. This is within our expectation since clarity scores and robustness scores measure two different properties of a ranked document list. 2 Note that the only exception occurs in TREC 4 because the robustness scores correlate with the average precision much better than the clarity scores.

Third, the robustness score shows a stronger linear relationship with average precision compared to the clarity score. The linear regression analysis performed in the next section will further confirm this observation.
 We observe that the performance of the clarity score drops greatly on the GOV2 collection. We speculate that this is due to the fact that there are a relatively large number of low quality documents in this collection. Moreover, it seems that this characteristic has a more negative impact on clarity scores than on robustness scores. To understand this, let us recall that the clarity score measure the degree of dissimilarity between the language usage associated with the query and the generic language of the collection. The ability of clarity scores to pre-dict query performance is based on the following assumption: a query whose highly ranked documents contain many relevant documents (high query performance) is likely to receive a high clarity score because these highly ranked documents tend to be about a single topic and therefore have unusual word usage. However, when it comes to large web collections, the low quality documents retrieved in respond to a query are likely to have unusual word distributions [ 29 ], resulting in high clarity scores. In other words, the clarity score method cannot distinguish whether a high clarity score is caused by a small number of topic terms in the query language model or by the noise from the low quality documents retrieved. 4.1.2 Linear regression analysis Both Kendall X  X  rank correlation and Pearson X  X  correlation are not capable of directly pre-dicting average precision scores. To address this problem, we adopt the linear regression technique which yields an equation that predicts the values of average precision from predic-tors. Although there are fancier non-linear models, linear regression models often perform better in situations with sparse data or highly noisy data. Moreover, the linear regression analysis provides an adequate and interpretable description of how the predictors affect the dependent variable. In this section, we first evaluate the linear prediction quality of the clarity score and the robustness score. Then we investigate the relative importance of each predictor in terms of prediction power.

One common way to measure how well a linear regression model fits data is the so-called coefficient of determination or R -square. The range of R -square is between 0 and 1 and a high value means fitting well. Here we perform simple linear regression and the predictor is either the robustness score or the clarity score or the linear combination of the two. Table 4 shows the results which are consistent to what we have observed in Tables 2 and 3 . For example, we see that the robustness scores fit the average precision much better than the clarity scores on all collections. The goodness-of-fit is low on the GOV2 collection. Again, we observe that the linear combination of the two predictors often boost the quality of linear regression. The effect of linear regression between average precision and robustness score for the 50 title queries from the TREC4 collection is shown in Fig. 3 .

To identify the predictor that bestows the greatest impact on the dependent variable, we compare the regression coefficients of the two predictors. However, the values of the original regression coefficients depend on both the importance of each predictor and the variance of that predictor. To make a fair comparison, we adopt the standardized regression coefficient called Beta that eliminates the influence of variance. The standardized coefficient is what the regression coefficient would be if the model were fitted to standardized data, that is, if from each observation we subtracted the sample mean and then divided by the sample deviation. Hence, the magnitudes of these Beta values represent the importance of each pre-dictor. Table 5 shows the results for standardized regression coefficients. We used the SPSS software to compute the standardized regression coefficients. We observe the similar trends as in Table 4 . Based on the results from Tables 4 and 5 , our results suggest that when using linear regression robustness scores predict average precision better than clarity scores. 4.2 Prediction for named-page finding queries In the previous section, we have demonstrated that the robustness score consistently correlate with topic difficulty. In this section, our goal is to examine whether there is any relationship between the robustness score and the performance of name-page finding (NP) queries. The data sets used for evaluation come from the Named-Page finding topics of the Terabyte Tracks of 2005 and 2006 and we name them TB05-NP and TB06-NP respectively. Table 6 gives more details on the two data sets. We adopt the mixed language model [ 18 , 19 ] for our named-page finding retrieval. Retrieval parameters are the same as in [ 18 ]. Retrieval perfor-mance of individual NP queries is measured by the reciprocal rank of the first correct answer. We use the correlation with the reciprocal ranks measured by the Pearson X  X  correlation test to evaluate prediction quality. The results are presented in Table 7 . Again, our baseline is the clarity score.

For the clarity score, we tried different parameters and found that using the first ranked document to build the query model yields the best prediction accuracy. This makes sense because NP-query performance heavily depends on the relevance of the first ranked docu-ment. From Table 7 , we can see that the correlation with query performance on both test sets is low, suggesting that measuring ranked list coherence is not effective for NP queries.
Regarding the robustness score, we observe an interesting and surprising negative corre-lation with reciprocal ranks. We explain this finding briefly. A high robustness score means that a number of top ranked documents in the original ranked list are still highly ranked after perturbing the documents. The existence of such documents is a good sign of high performance for content-based queries as these queries usually contain a number of rele-vant documents. However, with regard to NP queries, one fundamental difference is that there is only one relevant document for each query. The existence of such documents can confuse the ranking function and lead to low retrieval performance. Although the nega-the strength of this correlation is stronger compared to the clarity method as shown in Ta b l e 7 .

On the other hand, in comparison to the results shown in the previous section for content-based queries, we notice that prediction by the robustness method for named-page finding queries is less accuracy and consistent on average, suggesting that the robustness score is more appropriate for predicting topic difficulty. 4.3 Query classification In this section, we show that the robustness score, though originally proposed for performance prediction, is also a good indicator of query types. The use of the robustness score for query classification is motivated by the observation obtained from our prediction experiments that the robustness score behaves very differently between these two types of queries: named-page finding and content-based. In the following experiments, we create a set of content-based queries consisting of all of the 150 ad-hoc title queries from Terabyte Track 2004 X 2006 and a set of NP queries consisting of 252 NP queries from Terabyte Track 2005.

We first investigate the distributions of robustness scores for NP and content-based queries respectively. Since we do not know what distribution the scores actually follow, we adopt Kernel density estimation [ 9 ], which does not assume any specific form of distribution on the features we want to estimate. Kernel density estimator belongs to a class of estimators called non-parametric density estimators that have no fixed structure and depend upon all data points to reach an estimate. Specifically, for a query set (in our case, the set is either the NP query set or the content-based query set) of size N, we calculate robustness scores x , x 2 ,..., x N for each query and the probability density function f(x) of robustness score on the set is estimated by : where  X  is the bandwidth and K  X  is a Kernel function.

In this paper we use the Gaussian Kernel. There is a standard way to select the bandwidth (  X  ) based on minimizing the expected square error between the estimated density and the original density [ 9 ]. In this paper, we adopt this method to calculate  X  .

The results are shown in Fig. 4 . As we can see, on average content-based queries have a much higher robustness score than NP queries.
Next we test the accuracy of query classification by the robustness score . To this end, we combine the two query sets mentioned above into one query pool. That is, the query pool consists of 150 content-based (CB) queries and 252 NP queries. Each time we pick one query that has not been selected before from the query set and all other queries are used as training data. Our strategy for predicting the type of the selected query is simple: the robustness score classifier will attach a NP (CB) label to the query if the robustness score for the query is below (above) a threshold trained from training data. Table 8 shows the results. For example, the value 25 at the intersection between the second row and the third column means 25 (out of 150) content-based queries are incorrectly labeled as the NP type. That is, the chance of correctly classifying a content-based query is about 83%. From the results in Table 8 we can see that our classifier reaches fairly good classification accuracy. 5 Conclusions In this paper, we introduce the notion of ranking robustness and propose a statistical measure called the robustness score to quantify ranking robustness. The robustness score is an effective tool for predicting retrieval performance of content-based queries. We demonstrate across a variety of test collections that there is a strong positive correlation between the robustness score of a content-based query and the performance of that query. We also apply this tech-nique to predict performance of another fundamentally different kind of queries: named-page finding. An interesting negative correlation between ranking robustness and retrieval perfor-mance is observed. However, our experiments show that predicting using robustness scores for named-page queries is less effective compared to content-based queries, suggesting that the robustness technique is more appropriate for predicting topic difficulty. In addition, the oppo-site behavior of the robustness score between the two types of queries motivates us to investi-gate the possibility of the use of the robustness score for query classification. Our results show that the robustness score is a good feature for distinguishing between the two query types. The results reported in this paper give fresh insight into our understanding of principles underlying different retrieval tasks and open up possibilities for exploring other applications of ranking robustness. References Authors Biography
