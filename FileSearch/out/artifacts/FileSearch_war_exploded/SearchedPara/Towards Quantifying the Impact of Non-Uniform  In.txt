 The majority of research into Collaborative Information Retrieval (CIR) has assumed a uniformity of information access and visibil-ity between collaborators. However in a number of real world scenarios, information access is not uniform between all collabo-rators in a team e.g. security, health etc. This can be referred to as Multi-Level Collaborative Information Retrieval (MLCIR). To the best of our knowledge, there has not yet been any systematic investigation of the effect of MLCIR on search outcomes. To address this shortcoming, in this paper, we present the results of a simulated evaluation conducted over 4 different non-uniform information access scenarios and 3 different collaborative search strategies. Results indicate that there is some tolerance to remov-ing access to the collection and that there may not always be a negative impact on performance. We also highlight how different access scenarios and search strategies impact on search outcomes. H.3.3 Information Search and Retrieval Measurement, Performance, Experimentation. Collaborative search; non-uniform access; effectiveness measures Collaborative Information Retrieva l (CIR) involves people with common information needs worki ng together, exploring and col-lecting useful information, and collectively making decisions that help them move toward their common goal. A simple example might be of a group of colleagues collaborating for a project where they may, individually or together, go through a number of information resources and then di scuss their results, exchanging information and knowledge in order to contribute to the project. A common assumption in much of the research in CIR is that all members of a team have equal access to the information sources, tools etc., and that they may share any relevant information they find with each other without any restriction [4, 5, 11]. However, in reality it may not always be the case that all searchers have equal information access. There are numerous situations where societal, legal or security reasons may prevent a searcher from sharing information within or out with a group. Handel and Wang [6] presented an example of such a scenario involving two intelli-gence analysts engaged in collaborative search, where one analyst is a signal intelligence specialist and the other a human intelli-gence specialist. Despite their unequal access to intelligence data-bases and underlying intelligence, as well as differing information needs and shareability, the two analysts must collaborate to achieve an outcome. This type of scenario was referred to as Mul-ti-Level Collaborative Information Retrieval (MLCIR) [6]. Simi-lar scenarios have been examined by other researchers who have looked at the effect of organisationa l structure in legal search [2], crisis management [3] and healthcare [10] to gain a better under-standing of how these can impede collaboration. Others have considered how different roles within a search team might be leveraged to assist with CIR. Fo r example, Pickens et al. [12] studied the impact of having two different roles in a collaborative exploratory search team, and looked into developing algorithms to support this. However, the main focu s of these studies has been on the division of labour in CIR and although, to date, having differ-ent roles has been viewed as positive in collaborative search tasks, it might not always be. In fact, MLCIR is different from division of labour in that any system that supports MLCIR has to be aware of information flow, accessibility and shareability between col-laborators [6]. Thus many of the concepts previously used to support CIR such as awareness, sense-making and persistence [4, 5, 11] may need to be revised. Previous research [2, 3, 9, 10] has focused primarily on qualitative observations which may not be co mpletely applicable in all non-uniform information access scenarios. To the best of our knowledge, there has yet to be a systematic evaluation on the impact of non-uniform informa tion access within a team of searchers. We attempt to overcom e this shortcoming by conduct-ing a simulated user evaluation where we investigate the impact of two different kinds of non-uniformit y in access, namely removing document access and search-term blacklisting for team members (Details are presented in Section 2.2). There are three main re-search questions that we attempt to answer in this paper: 1. What is the impact of non-uniform information access on the 2. Do different types of non-uniformity have different impacts 3. Are there scenarios where non-uniform access may be benefi-As there are a number of potential parameters for collaboration and non-uniformity in information access, we decided to use a simulated study. This approach means that we can more easily compare different variables and combinations than in a user eval-uation. In future work, we anticipate exploring the findings from this study in more depth with a user evaluation. Our evaluation followed the same procedure as Joho et al. X  X  simu-lation of collaborative search [7], with some small changes as outlined below. We utilised the TREC HARD 2005 [1] collection (AQUAINT corpus) and topics. For their study, Joho et al. [8] generated a query pool through a user evaluation for 13 of the topics. We were provided with th is query pool and thus use the same 13 topics (303, 344, 363, 3 67, 383, 393, 397, 439, 448, 625, 651, 658 689). The query pool has a total of 1157 queries across the 13 topics and each quer y contains up to 9 terms. Joho et al. [7] simulated teams of searchers (of variable size from 2 to 5) to carry out collaborative search tasks. Each team had 20 search iterations per topic. During each iteration, a team member selected a random query from the query pool and was assumed to judge 20 documents per iteration. For simplicity in our evaluation we simulate a pair of users rather than vary team size, as this would introduce extra complexity, whereby combining a multi-tude of possible access combinations could become intractable. In other words, we assume that there are always 2 people in a search team for any given search se ssion and the team performs 20 search iterations. Thus each i ndividual in a team would judge a maximum of 400 documents per topic, with a team judging a maximum of 800 documents. One of the goals of Joho et al. was to compare a number of collaborative search strategies [7]; we utilise 3 of these search strategies for our study. These 3 strategies are: 1) Independent Search (IS) : team members judge documents independently without any interaction between each other, and have their results merged at the end of each search iteration. 2) Independent Relevance Feedback (IRF) : same as (1) but query expansion is performed based on their independent relevance feedback and then the expanded queries are resubmitted inde-pendently to the system. Team members do not share any knowledge on relevancy of documents. 3) Shared Relevance Feedback (SRF) : same as (2) but the query expansion is performed based on the relevance feedback of both members. Thus, team members share knowledge on relevancy of the documents. For Joho et al. [7], IS was the most basic and simplest search strategy whereas the other two were the most effective. Due to its simplicity, IS is also the easiest to compare directly with any other search strategies in terms of performance, collection coverage, etc. The other two strategies chos en were the best performing in their experiments. We devised 4 scenarios to simulate non-uniform information access amongst team members comp leting a collaborative search task; these are summarised in Table 1 and outlined in detail be-low. For each scenario, we assumed that each of the two searchers have access to more or less of the collection relative to their search partner. For example, in one case, one searcher might be able to access only 10% of the collection while their partner can access 20% of the collection. Also, there is a possibility that one searcher cannot retrieve any doc uments that contain certain phrases or terms. Therefore, starting with S1 (d ocument removal), we began by indexing a random selection of 10% of the documents from the document collection. Then an iterative process was adopted whereby we increased the percentage of documents indexed by 10% until 100% of the collection had been indexed. This resulted in 10 different indexes for each person and 55 possible access combinations of indexes for tw o people (i.e. combinations of 10%-10%, 10%-20%, 10%-30%, 10%-40%; up to 100%-100%). This simulates a scenario laid out by Handel and Wang [6] where a person with higher security cl earance may have access to more documents than a subordinate. Code Scenario S1 Remove access to documents from collection S2 Term blacklisting  X  remove access to random terms from S3 Term blacklisting  X  remove access to te rms based on their S4 Term blacklisting -remove access to terms based on their Scenarios S2, S3 and S4 simulate term blacklisting, this is a major problem highlighted by Handel a nd Wang [6]. For S2, we began by analysing the collection for a list of terms. After that, we in-dexed the entire corpus meaning there is complete access. We then created other indexes by iteratively removing 10% of the terms randomly, until only 10% remained. This also resulted in 55 possible combinations of indexes for 2 individuals. Scenarios S3 and S4 took a more systematic a pproach. We analysed term fre-quencies in both collection and query pool, which contain 841498 and 591 unique terms respectively. We then followed the same procedure as S2 but instead of removing random terms we re-moved terms based on their frequenc ies in the collection and in the query pool respectively for S3 and S4. Therefore, for S3 the first 10% removed were the most frequent terms in the collection whereas for S4 those were the most frequent terms in the query pool. In each scenario we had 10 indexes for each team member and 55 different access combinations, although the indexes in S4 are of different size to S1, S2 and S3 because in S1, S2 and S3 we can theoretically exclude everything from the collection whereas for S4 this is dependent on the query pool. Thus for each scenario, there are 55 possible combinations; for each of these combinations, we conducted each search simulation 10 times in order to reduce ran domness and inconsistencies. In total, there were 1,716,000 search sessions performed by teams in our simulation (i.e. 3 search stra tegies x 4 access scenarios x 10 runs x 55 combinations x 13 topics x 20 iterations). For all of the indexing and retrieval, we used th e Inverted File indexing method and BM25 retrieval algorithm, these were developed using the Terrier 1 library with out of the box settings. For the evaluation we utilised traditional IR evaluation metrics: recall, precision and f-measure in conjunction with specific met-rics for CIR proposed by Shah and Gonz X lez-Ib X  X ez [13]: cover-age, relevant coverage, unique c overage and unique relevant cov-erage. Coverage is the average number of distinct documents discovered by the team throughout the entire search session. Rele-vant coverage is the average number of documents in coverage that are actually relevant. Unique coverage is the average number of distinct documents that are on ly discovered in a given access combination, and not in any other. Unique relevant coverage is the average number of documents in uni que coverage that are actually relevant. http://terrier.org Table 2 shows the access combinations which yield the highest values for recall, precision and f-measure across all access scenar-ios and search strategies and Ta ble 3 shows those for coverage, relevant coverage, unique coverage and unique relevant coverage. As our data was not normally dist ributed, for each measure across 4 access scenarios and 3 search strategies, we conducted a Fried-man analysis to compare the 55 access combinations (i.e. 10-10, 20-10, 20-20, 30-10, etc.) and found that there was a statistically significant difference in every ca se. Post hoc analysis with Wil-coxon signed-rank tests was conducte d with a Bonferroni correc-tion applied, resulting in a si gnificance level set at p&lt;0.00003367. We present more detailed results of the pairwise comparisons in the following sub-sections. For reasons of space as there were many comparisons we do not present all of these comparisons. Our first research question exam ined the impact of non-uniform information access on the outcomes of CIR. First of all, statistical analysis of recall, precision a nd f-measure values showed a num-ber of access combinations that were not significantly different from the best performing access combinations. However, what was interesting among these is that for S1, S2 and S4, relevance feedback search strategies had a very high number of combina-tions that are not significantly di fferent from their best performing access combinations (ranging from 50-20 to 90-60 for S1, 70-70 to 90-80 for S2, and 70-70 to 100-80 for S4) whereas the IS strat-egy had only a few (90-80, 90-90, 100-80, 100-100 for S1; 90-90, 100-10, 100-60 100-90 for S2; 90-90, 100-90 for S4). It suggests that in terms of recall, precision and f-measure non-uniform ac-cess for S1, S2 and S4 had very little effect when relevance feed-back strategies were employed. 
Table 2. Highest recall, prec ision and f-measure values with their respective access combinations. * indicates those values Looking at Table 2, we found that when the IS strategy was em-ployed for S1, the values of the 3 measures (recall , precision and f-measure) were highest at non-full access (i.e. 100-90) whereas for the rest of the scenarios (S2, S3 and S4) the values reached the highest at full access. When relevance feedback strategies were employed, however, it was found that the values reached the high-est at non-full access (mostly at 90-90) for all 4 scenarios (S1, S2, S3 and S4). This suggests that ther e is some tolerance to removing access from the collection, and while it was expected that there would be a decrease in performance when access had been re-duced, there were some cases which indicate that there may not always be a negative impact on performance. In addition, as men-tioned earlier, our statistical test results revealed a number of combinations that are not significantly different from the best performing access combinations, which suggests that there are certain combinations that allow search performance to be compa-rable to the best performing access combination regardless of the users X  unequal, or equal but not full (e.g. 90-90) access to the collection. This finding addresse s our third research question. Moreover, the statistical test results also showed us that depending on the type of access scenario and search strategies being utilised, the resulting combinations were different, and thus resulted in different outcomes, addressing ou r second research question. In terms of coverage for the do cument removing scenario (S1), statistical test results showed that in all 3 search strategies, there were many access combinations which were not significantly different from the best performing access combination and also represent the case where team members had access to a very di-verse amount of the collection from each other (these are 50-10, 60-10, 70-10, 80-10, 80-20, 90-10, 90-20, 100-10, 100-20, 100-30). It appears that regardless of the search strategy, reducing access to documents for one member of the team means that a different member can make judgem ents about different parts of the collection thereby covering similar amount of documents as they would in the best performing access combinations. This finding is in contrast to term bl acklisting scenarios (S2, S3 and S4) in which most combinations that are not significantly different from the best performing access combination represent the case where both team members had a higher access to the collection (e.g. 60-60, 100-80, etc.). Next, looking at coverage in Table 3, the fact that the highest values were obtained at non-full access again indicates that there may not always be a negative impact on performance when access has been reduced, addressing our third research question. In addition, sta tistical test results of coverage also showed that the resulting access combinations are different depending on the type of access scenario and search strategy being utilised which addresses our second research question. In terms of relevant coverage, Table 3 indicates that when the IS strategy was utilised, the highest values were obtained at full access (100-100) for all of the term blacklisting scenarios (S2, S3 and S4). However, statistical test results also indicated that there were non-full-access combinations where relevant coverage was as high as the full access. Besides, it also showed that the resulting access combinations and their ou tcomes are different depending on the type of access scenario and search strategy being utilised, again addressing our second resear ch question. With respect to unique coverage for S1, it can be seen in Table 3 that across all search strategies the access combination that has highest value is the lowest access (10-10), and this is opposite to S3 where the full access has the highest unique coverage . In addition, it is interest-ing to note that for all 4 scenarios (S1, S2, S3 and S4) the SRF strategy was able to obtain very high unique coverage in all access combinations compared to the other two strategies. Statistical test results showed that for S2, when the IS and IRF strategies were utilised, many of the access combinations ranging from 20-10 to 100-100 showed no significant difference from the best perform-ing access combinations (i.e. 50-40 and 10-10 respectively). A similar outcome was also found for S4, but across all 3 search strategies. Unique relevant covera ge in Table 3 shows that for all scenarios (other than for S3 of the IS strategy), the highest values were not obtained at full access. Ho wever, it appears that reducing access to the collection has little or no effect in terms of unique relevant coverage as statistical test results indicated that for al-most every access scenario and search strategy, none of the access combinations showed any significant difference from the best performing access combinations.

Table 3. Highest values of different CIR measures with their respective access combinations. * indicates values of those While a great deal of research has focused on CIR, only a few papers have considered the im pact of non-uniform information access on CIR outcomes. This paper is one of the first attempts to quantify the impact of non-uniform information access on CIR outcomes. To that end, we cond ucted a simulated user evaluation using established scenarios [6] and search strategies [7]. In relation to our first research question it was found that in terms of recall, precision and f-measure that non-uniform access for S1, S2 and S4 had very little impact when relevance feedback strate-gies were employed. In addition, it was also found that in some cases, one member of the team having a high level of access can compensate for the other team member. Besides, our results have also highlighted that there is some tolerance to removing access from the collection and that there may not always be a negative impact on performance. This leads us into our second and third research questions. We have found that depending on the type of access scenario and search strategy, access combinations yield different outcomes. Removing ac cess to documents and term blacklisting had different impacts in terms of coverage: for re-moving document access, coverage remained stable where at least one team member had high access, whereas for blacklisting both members needed high access to re tain high coverage. We have also found that in some scenarios, performance is even increased due to non-uniformity. This may in part be because this ensures that parts of the collection which might otherwise be ignored due to overlap in retrieved documents are now examined. Thus, there can be some benefits to non-uniform access depending on the search task. To address our research questions in this paper we used 3 search strategies, 4 access scenarios, 7 different measures and teams of 2 simulated users. We anticipate extending this study in various ways to be able to produce findi ngs that greatly generalise to a number of real situations. Thus, we intend to look at more com-plex strategies and access scenarios, and incorporate more users within each team. Furthermore, the findings from this study will be examined further via a user evaluation. To conclude, our find-ings provide a better understandi ng on the impact of non-uniform information access amongst searchers in collaborative information retrieval, as well as a roadma p for further user studies. [1] Allan, J: HARD Track Overview in TREC 2005: High [2] Attfield, S., Blandford, A., Makri, S.: Social and interac-[3] Bjurling, B., Hansen, P.: Cont racts for Information Sharing [4] Gonz X lez  X  Ib X  X ez, R., Shah, C.: Coagmento: A system for [5] Halvey, M., Vallet, D., Hannah, D., Feng, Y., Jose, J. M.: [6] Handel, M. J., Wang, E. Y.: I can't tell you what i found: [7] Joho, H., Hannah, D., Jose, J. M.: Revisiting IR techniques [8] Joho, H., Hannah, D., Jose, J. M.: Comparing collaborative [9] Karunakaran, A., Reddy, M.: Barriers to collaborative in-[10] Karunakaran, A., Reddy, M.: The Role of Narratives in [11] Morris, M. R., Horvitz, E.: SearchTogether: an interface for [12] Pickens, J., Golovchinsky, G., Shah, C., Qvarfordt, P., Back, [13] Shah, C., Gonz X lez-Ib X  X ez, R.: Evaluating the synergic 
