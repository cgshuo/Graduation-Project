 It has been an extensively sought-after goal to learn an appr opriate distance metric in image clas-sification and retrieval problems using simple and efficient algorithms [1 X 5]. Such distance metrics are essential to the effectiveness of many critical algorit hms such as k -nearest neighbor ( k NN), k -means clustering, and kernel regression, for example. We sh ow in this work how a Mahalanobis metric is learned from proximity comparisons among triples of training data. Mahalanobis dis-tance, a.k.a. Gaussian quadratic distance, is parameterized by a positiv e semidefinite (p.s.d.) matrix. Therefore, typically methods for learning a Mahalanobis di stance result in constrained semidefinite programs. We discuss the problem setting as well as the diffic ulties for learning such a p.s.d. ma-where dist ij measures the distance between a i and a j . We are interested in the case that dist computes the Mahalanobis distance. The Mahalanobis distan ce between two vectors takes the form: k a jection matrix L and X = LL  X  . Constraints such as those above often arise when it is known that a i these comparison constraints are much easier to obtain than either the class labels or distances be-tween data elements. For example, in video content retrieva l, faces extracted from successive frames at close locations can be safely assumed to belong to the same person, without requiring the indi-vidual to be identified. In web search, the results returned b y a search engine are ranked according to the relevance, an ordering which allows a natural convers ion into a set of constraints. The requirement of X being p.s.d. has led to the development of a number of methods for learning a Mahalanobis distance which rely upon constrained semidefi nite programing. This approach has a number of limitations, however, which we now discuss with re ference to the problem of learning a p.s.d. matrix from a set of constraints upon pairwise-dista nce comparisons. Relevant work on this topic includes [3 X 8] amongst others.
 Xing et al [4] firstly proposed to learn a Mahalanobis metric for cluste ring using convex optimiza-tion. The inputs are two sets: a similarity set and a dis-simi larity set. The algorithm maximizes the distance between points in the dis-similarity set under the constraint that the distance between points in the similarity set is upper-bounded. Neighborhood compo nent analysis (NCA) [6] and large mar-gin nearest neighbor (LMNN) [7] learn a metric by maintainin g consistency in data X  X  neighborhood and keeping a large margin at the boundaries of different cla sses. It has been shown in [7] that LMNN delivers the state-of-the-art performance among most distance metric learning algorithms. The work of LMNN [7] and PSDBoost [9] has directly inspired ou r work. Instead of using hinge loss in LMNN and PSDBoost, we use the exponential loss functi on in order to derive an AdaBoost-like optimization procedure. Hence, despite similar purpo ses, our algorithm differs essentially in the optimization. While the formulation of LMNN looks more si milar to support vector machines (SVM X  X ) and PSDBoost to LPBoost, our algorithm, termed B OOST M ETRIC , largely draws upon AdaBoost [10].
 In many cases, it is difficult to find a global optimum in the pro jection matrix L [6]. Reformulation-linearization is a typical technique in convex optimizatio n to relax and convexify the problem [11]. In metric learning, much existing work instead learns X = LL  X  for seeking a global optimum, e.g. , [4, 7, 12, 8]. The price is heavy computation and poor scalabi lity: it is not trivial to preserve the semidefiniteness of X during the course of learning. Standard approaches like int erior point Newton methods require the Hessian, which usually requires O ( D 4 ) resources (where D is the input dimen-sion). It could be prohibitive for many real-world problems . Alternative projected (sub-)gradient is adopted in [7, 4, 8]. The disadvantages of this algorithm are : (1) not easy to implement; (2) many parameters involved; (3) slow convergence. PSDBoost [9] co nverts the particular semidefinite pro-gram in metric learning into a sequence of linear programs (L P X  X ). At each iteration of PSDBoost, an LP needs to be solved as in LPBoost, which scales around O ( J 3 . 5 ) with J the number of iterations (and therefore variables). As J increases, the scale of the LP becomes larger. Another probl em is that PSDBoost needs to store all the weak learners (the rank-one matrices) during the optimization. When the input dimension D is large, the memory required is proportional to JD 2 , which can be prohibitively huge at a late iteration J . Our proposed algorithm solves both of these problems. Based on the observation from [9] that any positive semidefin ite matrix can be decomposed into a linear positive combination of trace-one rank-one matrice s, we propose B OOST M ETRIC for learning a p.s.d. matrix. The weak learner of B OOST M ETRIC is a rank-one p.s.d. matrix as in PSDBoost. The proposed B OOST M ETRIC algorithm has the following desirable properties: (1) B OOST M ETRIC is efficient and scalable. Unlike most existing methods, no s emidefinite programming is required. At each iteration, only the largest eigenvalue and its corre sponding eigenvector are needed. (2) B
OOST M ETRIC can accommodate various types of constraints. We demonstra te learning a Maha-lanobis metric by proximity comparison constraints. (3) Li ke AdaBoost, B OOST M ETRIC does not have any parameter to tune. The user only needs to know when to stop. In contrast, both LMNN and PSDBoost have parameters to cross validate. Also like Ad aBoost it is easy to implement. No sophisticated optimization techniques such as LP solvers a re involved. Unlike PSDBoost, we do not need to store all the weak learners. The efficacy and efficienc y of the proposed B OOST M ETRIC is demonstrated on various datasets.
 Throughout this paper, a matrix is denoted by a bold upper-ca se letter ( X ); a column vector is denoted by a bold lower-case letter ( x ). The i th row of X is denoted by X i : and the i th column X : i . Tr ( ) is the trace of a symmetric matrix and h X , Z i = Tr ( XZ  X  ) = inner product of two matrices. An element-wise inequality b etween two vectors like u  X  v means u  X  v i for all i . We use X &lt; 0 to indicate that matrix X is positive semidefinite. 2.1 Distance Metric Learning As discussed, the Mahalanobis metric is equivalent to linea rly transform the data by a projection matrix L  X  R D  X  d (usually D  X  d ) before calculating the standard Euclidean distance: Although one can learn L directly as many conventional approaches do, in this settin g, non-convex constraints are involved, which make the problem difficult t o solve. As we will show, in order to convexify these conditions, a new variable X = LL  X  is introduced instead. This technique has been used widely in convex optimization and machine learning suc h as [12]. If X = I , it reduces to the Euclidean distance. If X is diagonal, the problem corresponds to learning a metric in which the different features are given different weights, a.k.a. feature weighting.
 In the framework of large-margin learning, we want to maximi ze the distance between dist ij and large as possible under some regularization. To simplify no tation, we rewrite the distance between dist 2 ij and dist 2 ik as dist 2 ij  X  dist 2 ik = h A r , X i , r = 1 , , | SS | . | SS | is the size of the set SS . 2.2 Learning with Exponential Loss We derive a general algorithm for p.s.d. matrix learning wit h exponential loss. Assume that we want to find a p.s.d. matrix X &lt; 0 such that a bunch of constraints are satisfied as well as possible. These constraints need not be all strictly sati sfied. We can define the margin  X  r = h A r , X i ,  X  r . By employing exponential loss, we want to optimize Note that: (1) We have worked on the logarithmic version of th e sum of exponential loss. This transform does not change the original optimization proble m of sum of exponential loss because the logarithmic function is strictly monotonically decrea sing. (2) A regularization term Tr ( X ) has been applied. Without this regularization, one can always m ultiply an arbitrarily large factor to X to make the exponential loss approach zero in the case of all c onstraints being satisfied. This trace-norm regularization may also lead to low-rank solutions. (3 ) An auxiliary variable  X  r , r = 1 , . . . must be introduced for deriving a meaningful dual problem, a s we show later.
 So Here H rj is a shorthand for H rj = h A r , Z j i . Clearly Tr ( X ) = P J j =1 w j Tr ( Z j ) = 1  X  w . 2.3 The Lagrange Dual Problem We now derive the Lagrange dual of the problem we are interest ed in. The original problem (P0) now becomes In order to derive its dual, we write its Lagrangian with p  X  0 . Here u and p are Lagrange multipliers. The dual problem is obtained by fin ding the saddle point of L ; i.e. , sup u , p inf w ,  X  L . inf The infimum of L 1 is found by setting its first derivative to zero and we have: The infimum is Shannon entropy. L 2 is linear in w , hence L 2 must be 0 . It leads to The Lagrange dual problem of (P1) is an entropy maximization problem, which writes Weak and strong duality hold under mild conditions [11]. Tha t means, one can usually solve one problem from the other. The KKT conditions link the optimal b etween these two problems. In our case, it is While it is possible to devise a totally-corrective column ge neration based optimization procedure for solving our problem as the case of LPBoost [13], we are mor e interested in considering one-at-a-time coordinate-wise descent algorithms, as the case of AdaBoos t [10], which has the advantages: (1) computationally efficient and (2) parameter free. Let us start from some basic knowledge of column generation because our coordinate descent strategy is inspired by column generation. If we knew all the bases Z j ( j = 1 . . . J ) and hence the entire matrix H is known, then either the primal (P1) or the dual (D1) could be trivially solved (at lea st in theory) because both are convex optimization problems. We can solve them in polynomial time . Especially the primal problem is convex minimization with simple nonnegativeness constrai nts. Off-the-shelf software like LBFGS-B [14] can be used for this purpose. Unfortunately, in practi ce, we do not access all the bases: the number of possible Z  X  X  is infinite. In convex optimization, column generation is a technique that is designed for solving this difficulty.
 Instead of directly solving the primal problem (P1), we find t he most violated constraint in the dual (D1) iteratively for the current solution and add this const raint to the optimization problem. For this purpose, we need to solve Here  X  1 is the set of trace-one rank-one matrices. We discuss how to e fficiently solve (7) later. Now we move on to derive a coordinate descent optimization proce dure. 2.4 Coordinate Descent Optimization We show how an AdaBoost-like optimization procedure can be d erived for our metric learning prob-lem. As in AdaBoost, we need to solve for the primal variables w j given all the weak learners up to iteration j .
 Optimizing for w j Since we are interested in the one-at-a-time coordinate-wise optimization, we the following derivation, we drop those terms irrelevant to the variable w j ) Clearly, C p is convex in w j and hence there is only one minimum that is also globally opti mal. The first derivative of C p w.r.t. w j vanishes at optimality, which results in Algorithm 1 Bisection search for w j .
 If H rj is discrete, such as { +1 ,  X  1 } in standard AdaBoost, we can obtain a close-form solution similar to AdaBoost. Unfortunately in our case, H rj can be any real value. We instead use bisection to search for the optimal w j . The bisection method is one of the root-finding algorithms. It repeat-simple and robust, although it is not the fastest algorithm f or root-finding. Newton-type algorithms are also applicable here. Algorithm 1 gives the bisection pr ocedure. We have utilized the fact that the l.h.s. of (8) must be positive at w l . Otherwise no solution can be found. When w j = 0 , clearly the l.h.s. of (8) is positive.
 Updating u The rule for updating the dual variable u can be easily obtained from (6). At iteration j , we have derived from (6). So once w j is calculated, we can update u as 2.5 Base Learning Algorithm In this section, we show that the optimization problem (7) ca n be exactly and efficiently solved using eigenvalue-decomposition (EVD). From Z &lt; 0 and rank ( Z ) = 1 , we know that Z has the format: Z =  X  X   X  ,  X   X  R D ; and Tr ( Z ) = 1 means k  X  k 2 = 1 . We have By denoting problem. Note that  X  A is symmetric. Also see [9] for details.  X   X  converges. We summarize our main algorithmic results in Alg orithm 2. 3.1 Classification on Benchmark Datasets We evaluate B OOST M ETRIC on 15 datasets of different sizes. Some of the datasets have very h igh dimensional inputs. We use PCA to decrease the dimensionali ty before training on these datasets (datasets 2-6). PCA pre-processing helps to eliminate nois es and speed up computation. We have Algorithm 2 Positive semidefinite matrix learning with boosting.
 used USPS and MNIST handwritten digits, ORL face recognitio n datasets, Columbia University Image Library (COIL20) 1 , and UCI machine learning datasets 2 (datasets 7-13), Twin Peaks and Helix. The last two are artificial datasets 3 .
 Experimental results are obtained by averaging over 10 runs (except USPS-1). We randomly split the datasets for each run. We have used the same mechanism to gene rate training triplets as described as well as k nearest neighbors that have different labels from y i (imposers) are found. We then construct triplets from a i and its corresponding targets and imposers. For all the data sets, we have set k = 3 except that k = 1 for datasets USPS-1, ORLFace-1 and ORLFace-2 due to their la rge size. We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7]. LMNN is one of the state-of-the-art according t o recent studies such as [15]. Also in Table 1,  X  X uclidean X  is the baseline algorithm that uses t he standard Euclidean distance. The codes for these compared algorithms are downloaded from the corresponding authors X  websites. We have released our codes for B OOST M ETRIC at [16]. Experiment setting for LMNN follows [7]. For B
OOST M ETRIC , we have set v = 10  X  7 , the maximum number of iterations J = 500 . As we can see from Table 1, we can conclude: (1) B OOST M ETRIC consistently improves k NN classification using Euclidean distance on most datasets. So learning a Mah alanobis metric based upon the large margin concept does lead to improvements in k NN classification. (2) B OOST M ETRIC outperforms other algorithms in most cases (on 11 out of 15 datasets). LMNN is the second best algorithm on these 15 datasets statistically. LMNN X  X  results are consistent wit h those given in [7]. (3) Xing et al [4] and NCA can only handle a few small datasets. In general th ey do not perform very well. A good initialization is important for NCA because NCA X  X  cost function is non-convex and can only find a local optimum.
 Influence of v Previously, we claim that our algorithm is parameter-free l ike AdaBoost. However, we do have a parameter v in B OOST M ETRIC . Actually, AdaBoost simply set v = 0 . The coordinate-wise gradient descent optimization strategy of AdaBoost le ads to an  X  1 -norm regularized maximum the coefficient vector. Given the similarity of the optimiza tion of B OOST M ETRIC with AdaBoost, we conjecture that B OOST M ETRIC has the same property. Here we empirically prove that as long 10  X  8 to 10  X  4 and run B OOST M ETRIC on 3 UCI datasets. Table 2 reports the final 3 NN classification error with different v . The results are nearly identical.
 Computational time As we discussed, one major issue in learning a Mahalanobis di stance is heavy computational cost because of the semidefiniteness co nstraint. Our algorithm is generally fast. It involves matrix operati ons and an EVD for finding its largest eigenvalue and its corresponding eigenvector. The time com plexity of this EVD is O ( D 2 ) with D the input dimensions. We compare our algorithm X  X  running ti me with LMNN in Fig. 1 on the artificial dataset (concentric circles). We vary the input d imensions from 50 to 1000 and keep the number of triplets fixed to 250 . Instead of using standard interior-point SDP solvers that do not scale well, LMNN heuristically combines sub-gradient descent in both the matrices L and X . At each iteration, X is projected back onto the p.s.d. cone using EVD. So a full EVD with time complexity O ( D 3 ) is needed. Note that LMNN is much faster than SDP solvers like CSDP [18]. As seen from Fig. 1, when the input dimensions are low, B OOST M ETRIC is comparable to LMNN. As expected, when the input dimensions become high, B OOST M ETRIC is significantly faster than LMNN. Note that our implementation is in Matlab. Improvements are expe cted if implemented in C/C++. 3.2 Visual Object Categorization and Detection The proposed B OOST M ETRIC and the LMNN are further compared on four classes of the Calte ch-101 object recognition database [19], including Motorbike s ( 798 images), Airplanes ( 800 ), Faces ( 435 ), and Background-Google ( 520 ). For each image, a number of interest regions are identified by the Harris-affine detector [20] and the visual content in e ach region is characterized by the SIFT descriptor [21]. The total number of local descriptors extr acted from the images of the four classes ject categorization (Motorbikes vs. Airplanes) and object detection (Faces vs. Background-Google) problems. To accumulate statistics, the images of two invol ved object classes are randomly split as 10 pairs of training/test subsets. Restricted to the images in a training subset (those in a test subset are only used for test), their local descriptors are cluster ed to form visual words by using k -means clustering. Each image is then represented by a histogram co ntaining the number of occurrences of each visual word.
 Motorbikes vs. Airplanes This experiment discriminates the images of a motorbike fro m those of an airplane. In each of the 10 pairs of training/test subsets, there are 959 training images and 639 test images. Two visual codebooks of size 100 and 200 are used, respectively. With the result-ing histograms, the proposed B OOST M ETRIC and the LMNN are learned on a training subset and evaluated on the corresponding test subset. Their averaged classification error rates are compared in Fig. 2 (left). For both visual codebooks, the proposed B OOST M ETRIC achieves lower error rates than the LMNN and the Euclidean distance, demonstrating its superior performance. We also apply a linear SVM classifier with its regularization parameter ca refully tuned by 5 -fold cross-validation. contrast, a 3 NN with B OOST M ETRIC has error rates 3 . 63%  X  0 . 68% and 2 . 96%  X  0 . 59% . Hence, the performance of the proposed B OOST M ETRIC is comparable to or even slightly better than the SVM classifier. Also, Fig. 2 (right) plots the test error of th e B OOST M ETRIC against the number of triplets for training. The general trend is that more triple ts lead to smaller errors. Faces vs. Background-Google This experiment uses the two object classes as a retrieval pr ob-lem. The target of retrieval is the face images. The images in the class of Background-Google are randomly collected from the Internet and they are used to rep resent the non-target class. B OOST -M
ETRIC is first learned from a training subset and retrieval is condu cted on the corresponding test subset. In each of the 10 training/test subsets, there are 573 training images and 382 test images. Again, two visual codebooks of size 100 and 200 are used. Each face image in a test subset is used as a query, and its distances from other test images are calcu lated by B OOST M ETRIC , LMNN and the Euclidean distance. For each metric, the precision of the retrieved top 5 , 10 , 15 and 20 images are computed. The retrieval precision for each query are ave raged on this test subset and then aver-aged over the whole 10 test subsets. B OOST M ETRIC consistently attains the highest values, which again verifies its advantages over LMNN and the Euclidean dis tance. With a codebook size 200 , very similar results are obtained. See [16] for the experime nt results. We have presented a new algorithm, B OOST M ETRIC , to learn a positive semidefinite metric using boosting techniques. We have generalized AdaBoost in the se nse that the weak learner of B OOST -M
ETRIC is a matrix, rather than a classifier. Our algorithm is simple and efficient. Experiments show its better performance over a few state-of-the-art exi sting metric learning methods. We are currently combining the idea of on-line learning into B OOST M ETRIC to make it handle even larger datasets.
 [1] T. Hastie and R. Tibshirani. Discriminant adaptive near est neighbor classification. IEEE Trans. [2] J. Yu, J. Amores, N. Sebe, P. Radeva, and Q. Tian. Distance learning for similarity estimation. [3] B. Jian and B. C. Vemuri. Metric learning using Iwasawa de composition. In Proc. IEEE Int. [4] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance metric learning, with application to [5] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Le arning a Mahalanobis metric from [6] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdino v. Neighbourhood component anal-[7] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance met ric learning for large margin nearest [8] A. Globerson and S. Roweis. Metric learning by collapsin g classes. In Proc. Adv. Neural Inf. [9] C. Shen, A. Welsh, and L. Wang. PSDBoost: Matrix-generat ion linear programming for pos-[10] R. E. Schapire. Theoretical views of boosting and appli cations. In Proc. Int. Conf. Algorithmic [11] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, 2004. [12] K. Q. Weinberger and L. K. Saul. Unsupervised learning o f image manifolds by semidefinite [13] A. Demiriz, K.P. Bennett, and J. Shawe-Taylor. Linear p rogramming boosting via column [14] C. Zhu, R. H. Byrd, and J. Nocedal. L-BFGS-B: Algorithm 7 78: L-BFGS-B, FORTRAN [15] L. Yang, R. Jin, L. Mummert, R. Sukthankar, A. Goode, B. Z heng, S. Hoi, and M. Satya-[16] http://code.google.com/p/boosting/ . [17] S. Rosset, J. Zhu, and T. Hastie. Boosting as a regulariz ed path to a maximum margin classifier. [18] B. Borchers. CSDP, a C library for semidefinite programm ing. Optim. Methods and Softw. , [19] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Trans. [20] K. Mikolajczyk and C. Schmid. Scale &amp; affine invariant in terest point detectors. Int. J. Comp. [21] D. G. Lowe. Distinctive image features from scale-inva riant keypoints. Int. J. Comp. Vis. ,
