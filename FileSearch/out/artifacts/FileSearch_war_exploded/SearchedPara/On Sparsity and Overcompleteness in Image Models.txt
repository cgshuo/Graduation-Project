 Computational models of visual cortex, and in particular th ose based on sparse coding, have re-cently enjoyed much attention. The basic assumption behind sparse coding is that natural scenes are composed of structural primitives (edges or lines, for exam ple) and, although there are a potentially large number of these primitives, typically only a few are ac tive in a single natural scene (hence the term sparse, [1, 2]). The claim is that cortical processing u ses these statistical regularities to shape a representation of natural scenes, and in particular conve rts the pixel-based representation at the retina to a higher-level representation in terms of these st ructural primitives.
 Traditionally, research has focused on determining the cha racteristics of the structural primitives and comparing their representational properties with those of V1. This has been a successful enterprise, but as a consequence other important questions have been neg lected. The two we focus on here over-complete), and how many primitives are active in a sing le scene (how sparse)? We will also be interested in the coupling between sparseness and over-com pleteness. The intuition is that, if there are a great number of structural primitives, they can be very specific and only a small number will be active in a visual scene. Conversely if there are a small nu mber they have to be more general and a larger number will be active on average. We attempt to map th is coupling by evaluating models with different over-completenesses and sparsenesses and d iscover where natural scenes live along this trade-off (see Fig. 1).
 In order to test the sparse coding hypothesis it is necessary to build algorithms that both learn the primitives and decompose natural scenes in terms of them. Th ere have been many ways to derive such algorithms, but one of the more successful is to regard t he task of building a representation of natural scenes as one of probabilistic inference. More sp ecifically, the unknown activities of the structural primitives are viewed as latent variables that m ust be inferred from the natural scene data. Commonly the inference is carried out by writing down a gener ative model (although see [3] for an alternative), which formalises the assumptions made about the data and latent variables. The rules of probability are then used to derive inference and learnin g algorithms.
 Unfortunately the assumption that natural scenes are compo sed of a small number of structural primitives is not sufficient to build a meaningful generativ e model. Other assumptions must therefore be made and typically these are that the primitives occur ind ependently, and combine linearly. These are drastic approximations and it is an open question to what extent this affects the results of sparse coding. The distribution over the latent variables x are Student-t, a Mixture of Gaussians (with zero means), and the Generalised Gaussian (which includes the Laplace distribution). The output y D -dimensional structural primitives g noise (the model reduces to independent components analysi s in the absence of this noise [4]), The goal of this paper will be to learn the optimal dimensiona lity of the latent variables ( K ) and the optimal sparseness of the prior (  X  ). In order to do this a notion of optimality has to be defined. One option is to train many different sparse-coding models a nd find the one which is most  X  X imilar X  to visual processing. (Indeed this might be a fair character isation of much of the current activity in field.) However, this is fraught with difficulty not least as i t is unclear how recognition models map to neural processes. We believe the more consistent approac h is, once again, to use the Bayesian framework and view this as a problem of probabilistic infere nce. In fact, if the hypothesis is that the visual system is implementing an optimal generative model, then questions of over-completeness and sparsity should be addressed in this context.
 Unfortunately, this is not a simple task and quite sophistic ated machine-learning algorithms have to be harnessed in order to answer these seemingly simple que stions. In the first part of this paper we describe these algorithms and then validate them using ar tificial data. Finally, we present results concerning the optimal sparseness and over-completeness f or natural image patches in the case that the prior is a Student-t distribution. As discussed earlier, there are many variants of sparse-cod ing. Here, we focus on the Student-t prior for the latent variables x There are two main reasons for this choice: The first is that th is is a widely used model [1]. The second is that by implementing the Student-t prior using an a uxiliary variable, all the distributions in the generative model become members of the exponential fami ly [5]. This means it is easy to derive efficient approximate inference schemes like variational B ayes and Gibbs sampling.
 The auxiliary variable method is based on the observation th at a Student-t distribution is a continuous mixture of zero-mean Gaussians, whose mixing proportions a re given by a Gamma distribution over the precisions. This indicates that we can exchange the Stud ent-t prior for a two-step prior in which we first draw a precision from a Gamma distribution and then dr aw an activation from a Gaussian with that precision, This model produces data which are often near zero, but occas ionally highly non-zero. These non-zero elements form star-like patterns, where the points of t he star are determined by the direction of the weights (e.g., Fig. 2).
 One of the major technical difficulties posed by sparse-codi ng is that, in the over-complete regime, the posterior distribution of the latent variables p ( X | Y,  X  ) is often complex and multi-modal. Ap-proximation schemes are therefore required, but we must be c areful to ensure that the scheme we choose does not bias the conclusions we are trying to draw. Th is is true for any application of sparse coding, but is particularly pertinent for our problem as we w ill be quantitatively comparing different sparse-coding models. the space with models and learn the parameters at each point ( as schematised in Fig. 1). A model comparison criterion could then be used to rank the models, a nd to find the optimal sparseness/over-completeness. One such criterion would be to use cross valid ation and evaluate the likelihoods on some held-out test data. Another is to use (approximate) Bay esian Model Comparison, and it is on this method that we focus.
 To evaluate the plausibility of two alternative versions of a model M , each with a different setting of the hyperparameters  X  Since we do not have any reason a priori to prefer one particular configuration of hyperparameters to another, we take the prior terms P ( M ,  X  marginal-likelihoods (or Bayes Factor), The marginal-likelihoods themselves are hard to compute, b eing formed from high dimensional integrals over the latent variables V and parameters  X  , One concern in model comparison might be that the more comple x models (those which are more over-complete) have a larger number parameters and therefo re  X  X it X  any data set better. However, the Bayes factor (Eq. 9) implicitly implements a probabilistic version of Occam X  X  razor that penalises more complex models and mitigates this effect [6]. This make s the Bayesian method appealing for determining the over-completeness of a sparse-coding mode l.
 Unfortunately computing the marginal-likelihood is compu tationally intensive, and this precludes tiling the sparseness/over-completeness space. However, an alternative is to learn the optimal over-completeness at a given sparseness using automatic relevan ce determination (ARD) [7, 8]. The advantage of ARD is that it changes a hard and lengthy model co mparison problem (i.e., computing the marginal-likelihood for many models of differing dimen sionalities) into a much simpler infer-ence problem. In a nutshell, the idea is to equip the model wit h many more components than are believed to be present in the data, and to let it prune out the w eights which are unnecessary. Prac-tically this involves placing a (Gaussian) prior over the co mponents which favours small weights, and then inferring the scale of this prior. In this way the sca le of the superfluous weights is driven to zero, removing them from the model. The necessary ARD hyper-priors are In the previous two sections we described a generative model for sparse coding that is theoretically able to learn the optimal over-completeness of natural scen es. We have two distinct uses for this model: The first, and computationally more demanding task, i s to learn the over-completeness at a variety of different, fixed, sparsenesses (that is, to find th e optimal over-completeness in a vertical slice through Fig. 1); The second is to determine the optimal point on this trade-off by evaluating the (approximate) marginal-likelihood (that is, evaluati ng points along the trade-off line in Fig. 1 to find the optimal model -the star). It turns out that no single m ethod is able to solve both these tasks, but that it is possible to develop a pair of approximate algor ithms to solve them separately. The first approximation scheme is Variational Bayes (VB), and it excels at the first task, but is severely biased in the case of the second. The second scheme is Anneale d Importance Sampling (AIS) which is prohibitively slow for the first task, but much more accura te on the second. We describe them in turn, starting with VB.
 The quantity required for learning is the marginal-likelih ood, Computing this integral is intractable (for reasons simila r to those given in Sec. 2), but a lower-bound can be constructed by introducing any distribution over the latent variables and parameters, q ( V,  X ) , and using Jensen X  X  inequality, This lower-bound is called the free-energy, and the idea is t o repeatedly optimise it with respect to the distribution q ( V,  X ) so that it becomes as close to the true marginal likelihood as possible. Clearly the optimal choice for q ( V,  X ) is the (intractable) true posterior. However, by constrain ing this distribution headway can be made. In particular if we as sume that the set of parameters and set of latent variables are independent in the posterior, so that q ( V,  X ) = q ( V ) q ( X ) then we can sequentially optimise the free-energy with respect to each of these distributions. For large hierar-chical models, including the one described in this paper, it is often necessary to introduce further factorisations within these two distributions in order to d erive the updates. Their general form is, As the Bayesian Sparse Coding model is composed of distribut ions from the exponential family, the functional form of these updates is the same as the correspon ding priors. So, for example the latent variables have the following form: q ( x Although this approximation is good at discovering the over -completeness of data at fixed sparsities, it provides an estimate of the marginal-likelihood (the fre e-energy) which is biased toward regions of low sparsity. The reason is simple to understand. The differ ence between the free energy and the true likelihood is given by the KL divergence between the approxi mate and true posterior. Thus, the free-energy bound is tightest in regions where q ( V,  X ) is a good match to the true posterior, and loosest in regions where it is a poor match. At high sparsities, the true posterior is multimodal and highly non-Gaussian. In this regime q ( V,  X )  X  which is always uni-modal  X  is a poor approximation. At low-sparsities the prior becomes Gaussian-like and the posteri or also becomes a uni-modal Gaussian. In this regime q ( V,  X ) is an excellent approximation. This leads to a consistent bi as in the peak of the free-energy toward regions of low sparsity. One might al so be concerned with another potential source of bias: The number of modes in the posterior increase s with the number of components in the model, which gives a worse match to the variational app roximation for more over-complete models. However, because of the sparseness of the prior dist ribution, most of the modes are going to be very shallow for typical inputs, so that this effect sho uld be small. We verify this claim on artificial data in Section 6.2. An approximation scheme is required to estimate the margina l-likelihood, but without a sparsity-dependent bias. Any scheme which uses a uni-modal approxima tion to the posterior will inevitably fall victim to such biases. This rules out many alternate var iational schemes, as well as methods variational method which has a multi-modal approximating d istribution (e.g. a mixture model). The approach taken here is to use Annealed Importance Sampling ( AIS) [9] which is one of the few methods for evaluating normalising constants of intractab le distributions. The basic idea behind AIS is to estimate the marginal-likelihood using importanc e sampling. The twist is that the proposal distribution for the importance sampler is itself generate d using an MCMC method. Briefly, this inner loop starts by drawing samples from the model X  X  prior d istribution and continues to sample as the prior is deformed into the posterior, according to an a nnealing schedule. Both the details of this schedule, and having a quick-mixing MCMC method, are cr itical for good results. In fact it is simple to derive a quick-mixing Gibbs sampler for our applic ation and this makes AIS particularly appealing. Before tackling natural images, it is necessary to verify th at the approximations can discover the correct degree of over-completeness and sparsity in the cas e where the data are drawn from the forward model. This is done in two stages: Firstly we focus on a very simple, low-dimensional example that is easy to visualise and which helps explicate t he learning algorithms, allowing them to be tuned; Secondly, we turn to a larger scale example desig ned to be as similar to the tests on natural data as possible. 6.1 Verification using simple artifical data In the first experiment the training data are produced as foll ows: Two-dimensional observations are generated by three Student-t sources with degree of free dom chosen to be 2 . 5 . The generative weights are fixed to be 60 degrees apart from one another, as sh own in Figure 2.
 A series of VB simulations were then run, differing only in th e sparseness level (as measured by the degrees of freedom of the Student-t distribution over x iterations performed on a set of 3000 data points randomly ge nerated from the model. We initialised the simulations with K = 7 components. To improve convergence, we started the simulat ions with weights near the origin (drawn from a normal distribution wi th mean 0 and standard deviation 10  X  8 ) and a relatively large input noise variance, and annealed th e noise variance between the iterations of VBEM. The annealing schedule was as following: we started with  X  2 reduced this linearly down to  X  2 iterations. During the annealing process, the weights typi cally grew from the origin and spread in all directions to cover the input space. After an initial growth period, where the representation usually became as over-complete as allowed by the model, some of the w eights rapidly shrank again and collapsed to the origin. At the same time, the corresponding precision hyperparameters grew and effectively pruned the unnecessary components. We perform ed 7 blocks of simulations at different sparseness levels. In every block we performed 3 runs of the a lgorithm and retained the result with the highest free energy. importance weights using a fixed data set with 2500 data point s, 250 samples, and 300 intermediate distributions. Following the recommendations in [9], the a nnealing schedule was chosen to be linear initially (with 50 inverse temperatures spaced uniformly f rom 0 to 0.01), followed by a geometric section (250 inverse temperatures spaced geometrically fr om 0.01 to 1). This mean that there were a total of 300 distributions between the prior and posterior .
 The results indicate that the combination of the two methods is successful at learning both the over-completeness and sparseness. In particular the VBEM algori thm was able to recover the correct dimensionality for all sparseness levels, except for the sp arsest case  X  = 2 . 1 , where it preferred a model with 5 significant components. As expected, however, fi gure 2 shows that the maximum free energy is biased toward the more Gaussian models. In contras t to this, the marginal likelihood esti-mated by AIS (Fig. 2), which is strictly greater than the free -energy as expected, favours sparseness levels close to the true value. 6.2 Verification using complex artificial data Although it is necessary that the inference scheme should pa ss simple tests like that in the previous section, they are not sufficient to give us confidence that it w ill perform successfully on natural data. One pertinent criticism is that the regime in which we t ested the algorithms in the previous section (two dimensional observations, and three hidden la tents) is quite different from that required to model natural data. To that end, in this section we first lea rn a sparse model for natural images with fixed over-completeness levels using a Maximum A Poster iori (MAP) algorithm [2] (degree of freedom 2 . 5 ). These solutions are then used to generate artificial data a s in the previous section. The goal is to validate the model on data which has a content an d scale similar to the natural images case, but with a controlled number of generative components .
 The image data comprised patches of size 9  X  9 pixels, taken at random positions from 36 natural images randomly selected from the van Hateren database (pre processed as described in [10]). The patches were whitened and their dimensionality reduced fro m 81 to 36 by principal component analysis. The MAP solution was trained for 500 iterations, w ith every iteration performed on a new batch of 1440 patches (100 patches per image).
 The model was initialised with a 3-times over-complete numb er of components ( K = 108 ). As above, the weights were initialised near the origin, and the input noise was annealed linearly from  X  d = 0 . 5 of 500 VBEM iterations, with every iteration performed on 36 00 patches generated from the MAP solution. We performed several simulations for over-compl eteness levels between 0.5 and 4.5, and retained the solutions with the highest free energy.
 The results are summarised in Figure 3: The model is able to re cover the underlying dimensionality for data between 0.5 and 2 times over-complete, and correctl y saturates to 3 times over-complete (the maximum attainable level here) when the data over-comp leteness exceeds 3. In the regime between 2.5 and 3 times over-complete data, the model return s solutions with a smaller number of components, which is possibly due to the bias described at th e end of Section 5. However, these values are still far above the highest over-completeness le arned from natural images (see section 6.3), so that we believe that the bias does not invalidate our conclusions. 6.3 Natural images Having established that the model performs as expected, at l east when the data is drawn from the forward model, we now turn to natural image data and examine t he optimal over-completeness ratio and sparseness degree for natural scene statistics.
 The image data for this simulation and the model initialisat ion and annealing procedure are identical to the ones in the experiments in the preceeding section. We p erformed 20 simulations with different sparseness levels, especially concentrated on the more spa rse values. Every run comprised 500 VBEM iterations, with every iteration performed on a new bat ch of 3600 patches.
 As shown in Figure 4, the free energy increased almost monoto nically until  X  = 5 and then stabilised and started to decrease for more Gaussian models. The algori thm learnt models that were only slightly over-complete: the over-completeness ratio was d istributed between 1 and 1.3, with a trend for being more over-complete at high sparseness levels (Fig . 4). Although this general trend accords with the intuition that sparseness and over-completeness a re coupled, both the magnitude of the effect and the degree of over-completeness is smaller than m ight have been anticipated. Indeed, this result suggests that highly over-complete models with a Stu dent-t prior may very well be overfitting the data.
 Finally we performed AIS using the same annealing schedule a s in Section 6.1, using 250 samples for the first 6 sparseness levels and 50 for the successive 14. The estimates obtained for the log marginal likelihood, shown in Figure 4, were monotonically increasing with increasing sparseness (decreasing  X  ). This indicates that sparse models are indeed optimal for n atural scenes. Note that maximal marginal likelihood. The weights resemble the Gabo r wavelets, typical of sparse codes for natural images [1]. Our results suggest that the optimal sparse-coding model fo r natural scenes is indeed one which is very sparse, but only modestly over-complete. The antici pated coupling between the degree of sparsity and the over-completeness in the model is visible, but is weak.
 One crucial question is how far these results will generalis e to other prior distributions; and indeed, which of the various possible sparse-coding priors is best a ble to capture the structure of natural scenes. One indication that the Student-t might not be optim al, is its behaviour as the degree-of-freedom parameter moves towards sparser values. The distri bution puts a very small amount of mass at a very great distance from the mean (for example, the k urtosis is undefined for  X  &lt; 4 ). It is not clear that data with such extreme values will be encoun tered in typical data sets, and so the model may become distorted at high sparseness values.
 Student-t in terms of a random precision Gaussian is computa tionally helpful. While no longer within the exponential family, other distributions on the p recision (such as a uniform one) may be approximated using a similar approach.
 Acknowledgements This work has been supported by the Gatsby Charitable Founda tion. We thank Yee Whye Teh, Iain Murray, and David McKay for fruitful discussions.
 References
