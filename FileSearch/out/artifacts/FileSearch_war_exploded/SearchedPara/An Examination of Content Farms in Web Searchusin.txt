 On the Web, content farms produce articles engineered such that search engines rank them highly, in order to turn a profit from online advertising. Recently, content farms have increasingly been the target of demotion strategies by Web search engines, since content farm articles are often con-sidered to be of suspect quality. In this paper, we study the prevalence of content farms in the results returned by three major Web search engines over time. In particular, we develop a crowdsourced approach to identify content farm articles from the results returned by these search engines. Our results show that between the period of March and Au-gust 2011, the number of content farm articles observed on a number of indicative queries was reduced by up to 55% in the top ranks.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Experimentation, Performance Keywords: Web Search, Content Farms, Crowdsourcing
On the Web, a content farm is a large collection of articles, each engineered to maximise its chances to be ranked highly by search engines for particular user queries. However, the primary purpose of a content farm is not to inform. In-deed, they function as space-fillers for websites that seek to attract as many visitors as possible in order to profit from online advertising [2]. The material produced for content farms is often written by authors with limited knowledge of the subjects they cover, resulting in signature low quality articles. The rise of content farm websites has introduced a new challenge for Web search engines, namely how to avoid over-favouring content farms when better quality (but less optimised) content is available.

During early 2011, pages from content farms were often returned by the major search engines for popular queries [8]. Subsequently, search engines have indicated that they are at-tempting to promote original and high quality content at the expense of content farms [8]. Analysis of content farms and how to tackle them present new potential lines of research within the field of Web search. However, it is important to first examine how well search engines currently tackle them.
In this paper, we present a first study into the success (or otherwise) that search engines have had in tackling the prevalence of content farms within their rankings using a new dataset developed using crowdsourcing . In particular, we examine how the top documents retrieved by three major search engines have changed over time, specifically with re-gard to the number of returned documents that come from content farms. To achieve this, we first select queries for which the search engines are known to retrieve content farm articles. Then we employ crowdsourced workers to assess the top retrieved results from the search engines for these selected queries, creating a new content farm evaluation dataset. We use this dataset to examine how the preva-lence of content farm articles differs over time for the search engines tested.

The contributions of this work are three-fold. Firstly, we identify and motivate the study of content farms within a Web search scenario, which has seen little investigation within academia. Secondly, we develop a reusable crowd-sourced methodology and an associated dataset for identi-fying content farms within Web search results that can be used to drive future evaluations. Thirdly, we empirically evaluate how effective Web search engines have been at de-moting content farms within their search rankings. Indeed, we show that search engines have reduced the number of content farm pages in their top 5 results by between 29% and 55% on the selected queries.
A content farm is a large collection of human authored articles about a wide range of topics. Content farm articles may be informative, however this is not their primary aim. Indeed, they are designed to generate revenue from on-page advertising. For this reason, content farms can be consid-ered to be a form of Web spam. Spam pages are those that have been modified or created with the specific intent to make search engines unjustifiably favour them within their search rankings [3]. Indeed, there are companies known as search engine optimisers (SEOs) that provide consultancy to Figure 1: Example content farm page from e how.co.uk. websites on how to promote themselves within Web search rankings. By using SEO techniques, a content farm can leverage search engines to drive large volumes of users to its pages, generating revenue from on-page adverts. Fig-ure 1 illustrates an example of a typical page from a content farm. As can be seen, advertising features prominently on the page in the form of three banner adverts (these have been highlighted). Content farms are now also leveraging social tools to collect traffic from social networks and fur-ther promote their pages. For instance, from Figure 1, we can see prominently placed Twitter retweets, Google +1 and Facebook like buttons on each page, encouraging viewers to share the page with their social network. These can both directly increase page-views and may lead to further pro-motion by search engines if they consider social page fea-tures. Content farms may also make use of social spamming techniques such as tag spamming to make their pages seem popular and highly relevant to search engines [7]. In gen-eral, content farms make use of a variety of spamming and page optimisation techniques to promote themselves within the rankings of Web search engines. It is desirable for Web search engines to avoid over-favouring content farms, since pages that may seem highly relevant may in-fact provide little real-value [8].
In this work, we build a dataset to facilitate the evaluation of content farms over time. To do this, we require a fast and effective methodology for identifying content farms within search rankings. However, to date, there are no automatic approaches within the literature to do so. One possible op-tion is to manually label pages as belonging to content farms. However, such an approach is both time-consuming and does not scale to large numbers of pages. On the other hand, the crowdsourcing approaches to such large-scale labelling tasks have become popular [6]. Crowdsourcing in general is the act of outsourcing tasks, traditionally performed by a spe-cialist person or group, to a large undefined group of people or community (referred to as the  X  X rowd X ), through an open call [4]. Indeed, labelling tasks can be completed at a rel-atively small cost, and often very quickly [1], through the leverage of online crowdsourced marketplaces such as Ama-zon X  X  Mechanical Turk (MTurk). However, crowdsourced workers are not typically experts at the task they are as-signed. Hence, it is important to develop an effective crowd-sourced methodology that both aids the worker in complet-i ng the task effectively and that ensures the quality of the work produced. To evaluate how effective search engines are tackling content farms, we later use MTurk workers to label search results from major Web search engines.
If we are to investigate how effectively current search en-gines demote content farms, we first require a dataset upon which we can evaluate. However, no such prior dataset ex-ists. Instead, we develop a new dataset to facilitate our investigation into content farms. In particular, our dataset is comprised of three components listed below:
Firstly, we manually developed a query set comprised of 50 Web search queries, each shown to contain at least 3 content farm articles in the top 10 results on Google.com in late February 2011. Notably, the resultant query set is largely comprised of question answering queries, e.g.  X  X ow to get a US visa X  or  X  X ow to unlock an iphone X , as we identified these as being particularly prone to attracting content farm articles. Indeed, this is intuitive as such popular queries could potentially drive many users to content farm articles.
Next, we collected ranked results from three Web search engines for the afore-mentioned 50 queries. In particular, on the 3rd of March 2011, and later on the 23rd of August 2011, we collected the top 64 pages ranked by three major search engines, namely Google.com , Google.co.uk and Bing.com , via their public APIs. It is these rankings that we later evaluate to determine how prevalent content farm articles are within the respective search engines. Of note is that we consider the U.S. and U.K. regions of Google to be separate search engines, as they produce different rankings for the same query. Furthermore, on the 3rd of March when we first tested the search engines, Google had deployed its  X  X anda X  update [8]  X  that was designed to combat content farms  X  in the U.S. region, but not yet the U.K. region. By comparing across regions we can see the effect that the update had on the ranked content farm articles. For each search engine and each query, on the 3rd of March and later on the 23rd of August, we downloaded the top 64 web pages, resulting in approximately 20,000 ranked results. The statistics of the pages downloaded from each of the three search engines from the two time points are provided in Table 1.
For evaluation, we generate a content farm ground truth against which we can compare each ranked result. We create this ground truth using human assessors. However, due to time and budget constraints, it is infeasible to have 20,000 pages individually assessed. Instead, since we aim to identify articles (pages) from content farms (hosts), we reduce the
Figure 2: MTurk assessment interface template. v olume of assessments required by evaluating unique hosts rather than individual pages. Furthermore, by definition, content farms should appear often in the ranked results for the selected queries. Hence, hosts that appear infrequently need not be assessed as they are unlikely to be content farms. The host frequency follows a power distribution. For this reason, we select only the 600 most frequently appearing hosts to be assessed, covering the  X  X ead X  of this distribution.
We use crowdsourcing to assess each of these 600 selected hosts. To this end, we developed an assessment interface template comprised of four components, illustrated in Fig-ure 2. At the top, detailed instructions to the worker are pro-vided, including a description of content farms, since crowd-sourced workers may not be familiar with the concept. This component is collapsible to save screen real-estate. The sec-ond component facilitates host assessment. A link to the host in question is provided that replaces the $ { url } field in the template. The workers are asked to follow the link to the host and spend approximately one minute assessing it. Workers record their assessment on a six point scale, as shown in Figure 2. By providing a wide range of choices describing the different types of site the workers might en-counter, we aim to reduce the risk that a worker might er-roneously mark a page as a content farm. Despite this, the aim of our study is to examine the prevalence of content farms, not other types of host. As such, we distinguish only those hosts labelled as  X  X ontent farms X  from hosts labelled otherwise, leading to binary assessments.

In the third component, inspired by Captcha validation and prior work in crowdsourced worker evaluation [5], we asked workers to provide the name of the host that they assessed. The worker X  X  input here is used as an indicator of whether that worker is attempting the task in good faith. In particular, post assessment, we listed side-by-side the host name provided by each worker and the title header from that host, enabling human validation at  X  X  glance X . Work was rejected if no text or random/unrelated text was entered into the field.

Our final component allows workers to provide free-text feedback. Such feedback is important to allow workers to express problems, uncertainty or suggest improvements to the labelling task [6]. Indeed, such feedback allows us to iteratively improve the task and provide workers with clar-ifications [1]. For instance, two of our workers questioned whether youtube.com could be considered a content farm, to which we answered no since youtube.com does not collect or create its own content.

Following best practises in crowdsourcing [6], we had three different workers located in the U.S. assess each host, re-sulting in 1800 assessments. U.S. workers were chosen as we assume that they may be more familiar with the concept of a content farm than Asian workers, as many content farms are run from the U.S. The final assessment is produced via a majority vote across the three worker assessments. The crowdsourcing marketplace used was Amazon X  X  Mechanical host assessed. The total cost was $99 including MTurk X  X  10% fee. In addition to the host-name validation described above, following [6], we performed a post assessment gold-judgment validation using 7% of the hosts. Both these ap-proaches were used to reject workers and their assessments without payment. To further estimate the quality of the assessments produced, we calculated the Kappa Fleiss ag-greement between assessors across hosts. We observed a substantial Kappa Fleiss agreement of 0.7145, attesting to the quality of the assessments produced.

Using these assessments, we also analysed the proportion of the top ranked results that can be associated with a la-bel, i.e. those for which its host was assessed. Indeed, recall that only a subset of the pages will have assessments, since we assessed only frequently appearing hosts. Figure 3 re-ports the proportion of the top 50 documents from each search engine for the two points in time that have an asso-ciated label. From Figure 3, we see that in March, all three search engines have assessments for approximately 45% of their ranked results on average. However, interestingly, we see that for the second time point in August, a lower propor-tion of the rankings were assessed. The fact that a markedly smaller proportion of the ranked results had assessments for the second time point in comparison to the first, indicates that the search engines tested were returning pages from a much wider range of hosts. This is an important prelimi-narily result, because it shows each search engine tackling their over-reliance on particular hosts. In the next section, we report how effectively the search engines tested were at demoting content farms over time.
Using the dataset described in the previous section, we evaluate the rankings produced by the three search engines to determine whether the prevalence of content farms has been reduced between March and the August 2011. We measure the prevalence of content farms using typical in-formation retrieval measures, where a content farm page is considered relevant, and a non-content farm page is consid-ered not-relevant. Note that in this case, a lower score is desirable, i.e. less content farm hosts are observed within the ranking. In particular, we report the prevalence of con-tent farms in terms of precision at rank N (P@N) and the top-heavy measure mean average precision (MAP).

Table 2 reports the prevalence of content farms in the top results of the Google.com , Google.co.uk and Bing.com h ttp://mturk.com Statistically significant decrease (t-test p &lt; 0.01) denoted *. Figure 3: Percentage of the top 50 results returned b y each search engine that were assessed. search engines for the 3rd of March and the 23rd of Au-gust 2011. Statistically significant decreases in content farm prevalence (t-test p &lt; 0.01) under each measure between the March and August rankings are denoted *.

From Table 2, we observe the following three points of interest. Firstly, examining our first time point in May, we see that the Google.com rankings contain approximately 26% less content farm articles than the Google.co.uk rank-ing. Indeed, this improvement may be partially attributed to  X  X anda X  Google ranking update that had been just rolled out to the U.S. region of the search engine at the time, as noted in Section 3.

Secondly, on the 3rd of May, we see that in the top five ranked results, all three search engines were returning around one content farm article on average for the queries within our dataset. This highlights both the prevalence of content farms within Web search results at the time, as well as the challenge that they pose.
 Thirdly, when comparing the rankings produced during March and August, for all search engines and measures, the prevalence of content farms was reduced by a statistically significant margin, except for two settings where the preva-lence of content farms was only markedly reduced. Indeed, this reduction is particularly acute in the top ranks, with both Google search engines reducing the number of content farm articles by over 50% for P@5, while Bing.com improved over their March ranking for the same measure by approxi-mately 29% for the selected queries. This result shows that all three of the search engines tested have been successful in demoting content farms within their search results. How-ever, we also see that on average over our test queries during our second time point in August, one in every two rankings contained a content farm article in the top five results. For instance, for the Google.com search engine, the well-known content farm E-How was still responsible for a total of 23 top five results over our 50 queries. Indeed, for the ambigious query  X  X ow to wax X , E-How comprised three of the top five pages. An interesting line of future research might be to in-vestigate whether these remaining content farm articles were in-fact good results to return, or whether these too should have been demoted, as well as how this could be achieved.
In this paper, we performed a study on how the prevalence of content farms returned in Web search results for selected queries has changed for three major Web search engines between March and August 2011. We developed a crowd-sourced approach to assess web page hosts and subsequently had 600 websites assessed by three different assessors using the Amazon X  X  Mechanical Turk marketplace. Using the re-sultant crowdsourced assessments, we have shown that the prevalence of content farm pages returned was reduced by between 55% and 29% across different search engines. We conclude that those engines tested are increasingly effective at combating content farm articles in their search results. However, we also showed that for approximately one in ev-ery two of the queries tested, a content farm article made the top five ranked results at the end of our study, highlighting a new direction for future research.
