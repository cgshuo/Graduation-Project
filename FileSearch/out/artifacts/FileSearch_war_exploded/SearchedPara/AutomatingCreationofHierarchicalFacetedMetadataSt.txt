 It is becoming widely accepted that the standard search interf ace, consisting of a query box and a list of retrie ved items, is inadequate for navig ation and exploration in lar ge information collections such as online catalogs, dig-ital libraries, and museum image collections. Instead, user interf aces which organize and group retrie val results have been sho wn to be helpful for and preferred by users over the straight results-list model when eng aged in ex-ploratory tasks (Yee et al., 2003; Pratt et al., 1999; Kaki, 2005). In particular , a representation kno wn as hierarchi-cal faceted metadata is gaining great traction within the information architecture and enterprise search communi-ties (Yee et al., 2003; Weinber ger , 2005).

A considerable impediment to the wider adoption of collection navig ation via metadata in general, and hierar -chical faceted metadata in particular , is the need to cre-ate the metadata hierarchies and assign the appropriate cate gory labels to the information items. Usually , meta-data cate gory structures are manually created by infor -mation architects (Rosenfeld and Morville, 2002). While manually created metadata is considered of high qual-ity, it is costly in terms of time and effort to produce, which mak es it dif ficult to scale and keep up with the vast amounts of new content being produced.

In this paper , we describe Castanet, an algorithm that mak es considerable progress in automating faceted meta-data creation. Castanet creates domain-specific overlays on top of a lar ge general-purpose lexical database, pro-ducing surprisingly good results in a matter of minutes for a wide range of subject matter .

In the next section we elaborate on the notion of hier -archical faceted metadata and sho w how it can be used in interf aces for navig ation of information collections. Sec-tion 3 describes other algorithms for inducing cate gory structure from textual descriptions. Section 4 describes the Castanet algorithm, Section 5 describes the results of an evaluation with information architects, and Section 6 dra ws conclusions and discusses future work. A hierarchical faceted metadata system (HFC) creates a set of cate gory hierarchies, each of which corresponds to a dif ferent facet (dimension or type). The main applica-tion of hierarchical faceted metadata is in user interf aces for bro wsing and navig ating collections of lik e items.
In the case of a recipe collection, for example, facets may consist of dish type (salad, appetizer), ingredients such as fruits (apricot, apple), vegetables (broccoli, cab-bage), meat (beef, fish), preparation method (fry , bak e, etc.), calorie count, and so on. Decomposing the descrip-tion into independent cate gories allo ws users to mo ve through lar ge information spaces in a flexible manner . The cate gory metadata guides the user toward possible choices, and organizes the results of keyw ord searches, allo wing users to both refine and expand the current query , while maintaining a consistent representation of the collection X  s structure. This use of metadata should be inte grated with free-te xt search, allo wing the user to fol-low links, then add search terms, then follo w more links, without interrupting the interaction flo w.
Usability studies have sho wn that, when incorpo-rated into a properly-designed user interf ace, hierarchical faceted metadata pro vides a flexible, intuiti ve way to ex-plore a lar ge collection of items that enhances feelings of disco very without inducing a feeling of being lost (Yee et al., 2003).

Note that the HFC representation is intermediate in comple xity between that of a monolithic hierarch y and a full-blo wn ontology . HFC does not capture relations and inferences that are essential for some applications. For example, faceted metadata can express that an image contains a hat and a man and a tree, and perhaps a wear -ing acti vity , but does not indicate who is wearing what. This relati ve simplicity of representation suggests that au-tomatically inferring facet hierarchies may be easier than the full ontology inference problem. There is a lar ge literature on document classification and automated text cate gorization (Sebastiani, 2002). Ho w-ever, that work assumes that the cate gories of interest are already kno wn, and tries to assign documents to cate-gories. In contrast, in this paper we focus on the problem of determining the cate gories of interest.

Another thread of work is on finding synon ymous terms and word associations, as well as automatic acqui-sition of IS-A (or genus-head) relations from dictionary definitions and free text (Hearst, 1992; Caraballo, 1999). That work focuses on finding the right position for a word within a lexicon, rather than building up comprehensible and coherent faceted hierarchies.

A major class of solutions for creating subject hier -archies uses data clustering. The Scatter/Gather sys-tem (Cutting et al., 1992) uses a greedy global agglomer -ative clustering algorithm where an initial set of clusters is recursi vely re-clustered until only documents remain. Hofmann (1999) proposes the probabilistic latent seman-tic analysis algorithm (pLSA), a probabilistic version of clustering that uses latent semantic analysis for grouping words and annealed EM for model fitting.

The greatest adv antage of clustering is that it is fully automatable and can be easily applied to any text col-lection. Clustering can also reveal interesting and po-tentially une xpected or new trends in a group of docu-ments. The disadv antages of clustering include their lack of predictability , their conflation of man y dimensions si-multaneously , the dif ficulty of labeling the groups, and the counter -intuiti veness of cluster sub-hierarchies (Pratt et al., 1999).
 Blei et al. (2003) developed the LD A (Latent Dirichlet Allocation) method, a generati ve probabilistic model of discrete data, which creates a hierarchical probabilistic model of documents. It attempts to analyze a text cor -pus and extract the topics that combined to form its doc-uments. The output of the algorithm was evaluated in terms of perple xity reduction but not in terms of under -standability of the topics produced.

Sanderson and Croft (1999) propose a method called subsumption for building a hierarch y for a set of doc-uments retrie ved for a query . For two terms x and y , x is said to subsume y if the follo wing conditions hold: y and is a parent of y , if the documents which contain y , are a subset of the documents which contain x . To evalu-ate the algorithm the authors ask ed 8 participants to look at parent-child pairs and state whether or not the y were  X  X nteresting X . Participants found 67% to be interesting as compared to 51% for randomly chosen pairs of words. Of those interesting pairs, 72% were found to display a  X  X ype-of  X  relationship.
 Ne vill-Manning et.al (1999), Anick et.al (1999) and Vossen (2001) build hierarchies based on substring inclu-sion. For example, the cate gory full text inde xing and retrie val is the child of inde xing and retrie val which in turn is the child of inde x . While these string inclusion ap-proaches expose some structure of the dataset, the y can only create subcate gories which are substrings of the par -ent cate gory , which is very restricti ve.

Another class of solutions mak e use of existing lex-ical hierarchies to build cate gory hierarchies, as we do in this paper . For example, Na vigli and Velardi (2003) use WordNet (Fellbaum, 1998) to build a comple x ontol-ogy consisting of a wide range of relation types (demon-strated on a tra vel agent domain), as opposed to a set of human-readable hierarchical facets. The y develop a com-ple x algorithm for choosing among WordNet senses; it requires building a rich semantic netw ork using Word-Net glosses, meron yms, holon yms, and other lexical rela-tions, and using the semantically annotated SemCor col-lection. The semantic nets are intersected and the correct sense is chosen based on a score assigned to each inter -section. Mihalcea and Moldo van (2001) describe a so-phisticated method for simplifying WordNet in general, rather than tailoring it to a specific collection. The main idea behind the Castanet algorithm 1 is to carv e out a structure from the hypern ym (IS-A) relations within the WordNet (Fellbaum, 1998) lexical database. The pri-mary unit of representation in WordNet is the synset, which is a set of words that are considered synon yms for a particular concept. Each synset is link ed to other synsets via several types of lexical and semantic relations; we only use hypern ymy (IS-A relations) in this algorithm. 4.1 Algorithm Ov erview The Castanet algorithm assumes that there is text associ-ated with each item in the collection, or at least with a representati ve subset of the items. The textual descrip-tions are used both to build the facet hierarchies and to assign items (documents, images, citations, etc.) to the facets. The text does not need to be particularly coher -ent for the algorithm to work; we have applied it to frag-mented image annotations and short journal titles, but if the text is impo verished, the information items will not be labeled as thoroughly as desirable and additional manual annotation may be needed.

The algorithm has five major steps: 1. Select tar get terms from textual descriptions of in-2. Build the Core Tree: 3. Augment the Core Tree with the remaining terms X  4. Compress the augmented tree. 5. Remo ve top-le vel cate gories, yielding a set of facet
We describe each step in more detail belo w. 4.2 Select Target Terms Castanet selects only a subset of terms, called tar get terms , that are intended to best reflect the topics in the documents. Similarly to Sanderson and Croft (1999), we use the term distrib ution  X  defined as the number of item descriptions containing the term  X  as the selection crite-rion. The algorithm retains those terms that have a distri-bution lar ger than a threshold and eliminates terms on a stop list. One and two-w ord consecuti ve noun phrases are eligible to be considered as terms. Terms that can be ad-jecti ves or verbs as well as nouns are optionally deleted. 4.3 Build the Cor e Tree The Core Tree acts as the  X  X ackbone X  for the final cate-gory structure. It is built by using paths deri ved from un-ambiguous terms, with the goal of biasing the final struc-ture towards the appropriate senses of words. 4.3.1 Disambiguate using Wordnet Domains
A term is considered unambiguous if it meets at least one of two conditions: (1) The term has only one sense within WordNet, or (2) (Optional) The term matches one of the pre-selected
From our experiments, about half of the eligible terms have only one sense within WordNet. For the rest of terms, we disambiguate between multiple senses as fol-lows.

WordNet pro vides a cross-cate gorization mechanism kno wn as domains , whereby some synsets are assigned general cate gory labels. Ho we ver, only a small subset of the nouns in WordNet have domains assigned to them. For example, for a medicine collection, we found that only 4% of the terms have domains medicine or biolo gy associated with them. For this reason, we use an addi-tional resource called Wordnet Domains (Magnini, 2000), which assigns domains to WordNet synsets. In this re-source, every noun synset in WordNet has been semi-automatically annotated with one of about 200 De we y Decimal Classification labels. Examples include history , liter atur e, plastic arts, zoolo gy, etc.
 In Castanet, Wordnet Domains are used as follo ws. First, the system counts how man y times each domain is represented by tar get terms, building a list of the most well-represented domains for the collection. Then, in a manual interv ention step, the information architect se-lects the subset of the well-represented domains which are meaningful for the collection in question.

For example, for a collection of biomedical journal ti-tles, Sur gery should be selected as a domain, whereas for an art history image collection, Architectur e might be chosen. When processing the word lancet , the choice of domain distinguishes between the hypon ym path entity strument medical instrument sur gical instrument construction arch pointed arch Gothic arch lancet arch, lancet lancet .

In some cases, more than one domain may be rele-vant for a given term and for a given collection. For example, the term brain is annotated with two domains, Anatomy and Psyc holo gy , which are both rele vant do-mains for a biomedical journal collection. Currently for these cases the algorithm breaks the tie by choosing the sense with the lowest WordNet sense number (corre-sponding to the most common sense), which in this case selects the Anatomy sense. Ho we ver, we see this forced choice as a limitation, and in future work we plan to ex-plore how to allo w a term to have more than one occur -rence in the metadata hierarchies. 4.3.2 Add Paths to Cor e Tree
To build the Core Tree, the algorithm marches down the list of unambiguous terms and for each term looks up its synset and its hypern ym path in WordNet. (If a term does not have representation in WordNet, then it is not included in the cate gory structure.) To add a path to the Core Tree, its path is mer ged with those paths that have already been placed in the tree. Figure 1(a-b) sho ws the hypern ym paths for the synsets corresponding to the terms sundae and ambr osia . Note that the y have several hypern ym path nodes in common: (entity), (substance , matter), (food, nutrient), (nutriment), (cour se), (dessert, sweet, after s) . Those shared paths are mer ged by the al-gorithm; the results, along with the paths for parfait and sherbert are sho wn in Figure 1(c).

In addition to augmenting the nodes in the tree, adding in a new term increases a count associated with each node on its path; this count corresponds to how man y docu-ments the term occurs in. Thus the more common a term, the more weight it places on the path it falls within. 4.4 Augment the Cor e Tree / Disambiguate Terms The Core Tree contains only a subset of terms in the col-lection (those that have only one path or whose sense can be selected with WordNet Domains). The next step is to add in the paths for the remaining tar get terms which are ambiguous according to WordNet.

The Core Tree is built with a bias towards paths that are most lik ely to be appropriate for the collection as a whole. When confronted with a term that has multiple possible Figure 3: Two path choices for an ambiguous term. IS-A paths corresponding to multiple senses, the system favors the more common path over other alternati ves. Assume that we want to add the term date to the Core Tree for a collection of recipes, and that currently there are two paths corresponding to two of its senses in the Core Tree (see Figure 3). To decide which of the two paths to mer ge date into, the algorithm looks at the num-ber of items assigned to the deepest node that is held in common between the existing Core Tree and each candi-date path for the ambiguous term. The path for the calen-dar day sense has fewer than 20 documents assigned to it (corresponding to terms lik e Valentine X  s Day ), whereas the path for the edible fruit sense has more than 700 doc-uments assigned. Thus date is added to the fruit sense path. (The counts for the ambiguous terms X  document hits are not incorporated into the new tree.)
Also, to eliminate unlik ely senses, each candidate sense X  s hypern ym path is required to share at least of its nodes with nodes already in the Core Tree, where the user sets (usually between 40 and 60%). Thus the romantic appointment sense of date would not be consid-ered as most of its hypern ym path is not in the Core Tree. If no path passes the threshold, then the first sense X  s hy-pern ym path (according to WordNet X  s sense ordering) is placed in the tree. 4.5 Compr ess the Tree The tree that is obtained in the pre vious step usually is very deep, which is undesirable from a user interf ace per -specti ve. Castanet uses two rules for compressing the tree: 1. Starting from the lea ves, recursi vely eliminate a par -2. Eliminate a child whose name appears within the
For example, consider the tree in Figure 1(c) and as-sume that fewer than two children.

Starting from the lea ves, by applying Rule 2, nodes ( ice cream sundae ), ( sherbet, sorbet ), ( cour se ), ( nutriment ), ( food, nutrient ), ( substance , matter ) and ( entity ) are elim-inated since the y have only one child. Figure 2(a) sho ws the resulting tree. Ne xt, by applying Rule 3, the node frozen dessert is eliminated, since it contains the word dessert which also appears in the name of its parent. The final tree is presented in Figure 2(b). Note that this is a rather aggressi ve compression strate gy, and the algorithm can be adjusted to allo w more hierarch y to be retained. 4.6 Prune Top Le vel Categories / Cr eate Facets The final step is to create a set of facet sub-hierarchies. The goal is to create a moderate set of facets, each of which has moderate depth and breadth at each level, in order to enhance the navig ability of the cate gories. Prun-ing the top levels can be automated, but a manual editing pass over the outcome will produce the best results.
To eliminate the top levels in an automated fashion, for each of the nine tree roots in the WordNet noun database, manually cut the top levels (where for the recipes collection). Then, for each of the resulting trees, recur -sively test if its root has more than does, then the tree is considered a facet; otherwise, the current root is deleted and the algorithm tests to see if each new root has children. Those subtrees that do not meet the criterion are omitted from the final set of facets.
Consider the tree in Figure 4(a). In this case, the cate-gories of interest are ( flavor er ) and ( kitc hen utensil ) along with their children. Ho we ver, to reach any of these cate-gories, the user has to descend six levels, each of which has very little information. Figure 4(b) sho ws the re-sulting facets, which (subjecti vely) are at an informati ve level of description for an information architecture. (In this illustration, .)
Often the internal nodes of WordNet paths do not have the most felicitous names, e.g., edible fruit instead of fruit . Although we did not edit these names for the us-ability study , it is advisable to do so. The intended users of the Castanet algorithm are infor -mation architects and others who need to build structures for information collections. A successful algorithm must be percei ved by information architects as making their job easier . If the proposed cate gory system appears to re-quire a lot of work to modify , then IAs are lik ely to reject it. Thus, to evaluate Castanet X  s output, we recruited in-formation architects and ask ed them to compare it to one other state-of-the-art approach as well as a baseline. The participants were ask ed to assess the qualities of each cat-egory system and to express how lik ely the y would be to use each in their work. 5.1 Study Design The study compared the output of four algorithms: (a) Baseline (frequent words and two-w ord phrases), (b) Castanet, (c) LD A (Blei et al., 2003) 2 and (d) Subsump-tion (Sanderson and Croft, 1999). The algorithms were applied to a dataset of recipes from Southwest-cooking.com. Participants were recruited via email and were required to have experience building information ar-chitectures and to be at least familiar with recipe websites (to sho w their interest in the domain).

Currently there are no standard tools used by informa-tion architects for building cate gory systems from free text. Based on our own experience, we assumed a strong baseline would be a list of the most frequent words and two-w ord phrases (stopw ords remo ved); the study results confirmed this assumption. The challenge for an auto-mated system is to be preferred to the baseline.
The study design was within-participants, where each participant evaluated Castanet, a Baseline approach, and either Subsumption (N=16) or LD A (N=18). 3 Order of sho wing Castanet and the alternati ve algorithm was coun-terbalanced across participants in each condition.
Because the algorithms produce a lar ge number of hierarchical cate gories, the output was sho wn to the Def. Yes 4 2 0 2 2 0 Yes 10 10 0 13 11 6 No 2 2 2 1 3 2 Def. No 2 4 16 0 0 8 Table 1: Responses to the question  X  X  ould you be lik ely to use this algorithm in your work? X  comparing Castanet to the Baseline and LD A (N=18), and comparing Cas-tanet to the Baseline and Subsumption (N=16).
 Meaningful 2.9 1.2 1.8 Systematic 2.8 1.4 1.8 Import. Concepts 2.8 1.3 1.9 Table 2: Average responses to questions about the quality of the cate gory systems. N sho wn in parentheses. As-sessed on a four point scale where higher is better . participants using the open source Flamenco collection bro wser 4 (see Figure 5). Clicking on a link sho ws sub-cate gories as well as items that have been assigned that cate gory . For example, clicking on the Penne subcate gory beneath Pasta in the Castanet condition sho ws 5 recipes that contain the word penne as well as the other cate gories that have been assigned to these recipes. Since LD A does not create names for its output groups, the y were assigned the generic names Cate gory 1, 2, etc. Assignment of cat-egories to items was done on a strict word-match basis; participants were not ask ed to assess the item assignment aspect of the interf ace.

At the start of the study , participants answered ques-tions about their experience designing information archi-tectures. The y were then ask ed to look at a partial list of recipes and think briefly about what their goals would be in building a website for navig ating the collection.
Ne xt the y vie wed an ordered list of frequent terms dra wn automatically from the collection (Baseline condi-tion). After this, the y vie wed the output of one of the two tar get cate gory systems. For each algorithm, participants were ask ed questions about the top-le vel cate gories, such as Would you add any cate gories? (possible responses: (a) No, None, (b) Yes, one or two, (c) Yes, a few, and (d) Yes, man y). The y were then ask ed to examine two specific top level cate gories in depth (e.g., For cate gory Bread, would you remo ve any subcate gories? ). At the end of each assessment, the y were ask ed to comment on general aspects of the cate gory system as a whole (dis-cussed belo w). After having seen both cate gory systems, participants were ask ed to state how lik ely the y would be to use the algorithm (e.g., Would you use Oak? Would you use Bir ch? Would you use the frequent wor ds list? ) An-swer types were (a) No, definitely not, (b) Probably not, (c) Yes, I might want to use this system in some cases, and (d) Yes, I would definitely use this system. 5.2 Results Table 1 sho ws the responses to the final question about how lik ely the participants are to use the results of each algorithm for their work. Both Castanet and the Baseline fare well, with Castanet doing some what better . 85% of the Castanet evaluators said yes or definitely yes to us-ing it, compared to 74% for the Baseline. Only one par -ticipant said  X  X o X  to Castanet but  X  X es X  to the Baseline, suggesting that both kinds of information are useful for information architects.

The comparison algorithms did poorly . Subsumption recei ved 38% answering  X  X es X  or  X  X efinitely yes X  to the question about lik elihood of use. LD A was rejected by all participants. A t-test (after con verting responses to a 1-4 scale) sho ws that Castanet obtains significantly better scores than LD A ( = 7.88 2.75) and Subsumption ( = 4.50 2.75), for = 0.005. The dif ferences between Castanet and the Baseline are not significant.

Table 2 sho ws the average responses to the questions (i) Over all, these are cate gories meaningful; (ii) Over all, these cate gories describe the collection in a systematic way; (iii) These cate gories captur e the important con-cepts. ) The y were scored as 1= Strongly disagree, 2 = Disagree Some what, 3 = Agree Some what, and 4 = Strongly agree. Castanet X  s score was about 35% higher than Subsumption X  s, and about 50% higher than LD A X  X .
Participants were ask ed to scrutinize the top-le vel cate-gories and assess whether the y would add cate gories, re-mo ve some, mer ge or rename some. The ratings were again con verted to a four point scale (no changes = 4, change one or two = 3, change a few = 2, change man y = 1). Table 3 sho ws the results. Castanet scores as well as or better than the others on all measures except Rename; Subsumption scores slightly higher on this measure, and does well on Split as well, but very poorly on Remo ve, reflecting the fact that it produces well-named cate gories at the top level, but too man y at too fine a granularity .
Participants were also ask ed to examine two subcate-gories in detail. Table 4 sho ws results averaged across the two subcate gories for number of cate gories to add, remo ve, promote, mo ve, and how well the subcate gories matched their expectations. Castanet performs especially well on this last measure (2.5 versus 1.5 and 1.7). Partic-ipants generally did not suggest mo ves or promotions.
Thus on all measures, we see Castanet outperforming the other state-of-the-art algorithms. Note that we did not explicitly evaluate the  X  X acetedness X  of the cate gory sys-tems, as we thought this would be too dif ficult for the participants to do. We feel the questions about the coher -Add 2.8 2.8 2.4 Remo ve 3.4 2.2 2.5 Promote 3.7 3.4 3.8 Mo ve 3.8 3.3 3.6 Matched Exp. 2.5 1.5 1.7 ence, systematicity , and coverage of the cate gory systems captured this to some degree. We have presented an algorithm called Castanet that cre-ates hierarchical faceted metadata using WordNet and Wordnet Domains. A questionnaire revealed that 85% information architects thought it was lik ely to be use-ful, compared to 0% for LD A and 38% for Subsumption. Although not discussed here, we have successfully ap-plied the algorithm to other domains including biomedi-cal journal titles and art history image descriptions, and to another lexical hierarch y, MeSH. 5
Although quite useful  X  X ut of the box,  X  the algorithm could benefit by several impro vements and additions. The processing of the terms should recognize spelling variations (such as aging vs. ageing) and morphological variations. Verbs and adjecti ves are often quite impor -tant for a collection (e.g., stir -fry for cooking) and should be included, but with caution. Some terms should be al-lowed to occur with more than one sense if this is re-quired by the dataset (and some in more than one facet even with the same sense, as seen in the brain example). Currently if a term is in a document it is assumed to use the sense assigned in the facet hierarchies; this is often in-correct, and so terms should be disambiguated within the text before automatic cate gory assignment is done. And finally , WordNet is not exhausti ve and some mechanism is needed to impro ve coverage for unkno wn terms. displayed in the Flamenco interf ace.
