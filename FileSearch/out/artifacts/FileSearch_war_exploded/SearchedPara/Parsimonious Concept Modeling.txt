
Maarten de Rijke 1 H.3 [ Information Storage and Retrieval ]: H.3.3 Information Search and Retrieval Algorithms, Measurement, Theory, Experimentation Parsimonious Models, Language Models, Relevance Feedback
In many collections, documents are annotated using concepts from a structured knowledge source such as an ontology or the-saurus. Examples include the news domain [ 7 ], where each news item is categorized according to the nature of the event that took place, and Wikipedia, with its per-article categories [ 1 ]. These categorizing systems originally stem from the cataloging systems used in libraries and conceptual search is commonly used in digital library environments at the front-end to support search and navi-gation. In this paper we want to employ the explicit knowledge used for annotation at the back-end, not just to improve retrieval performance, but also to generate high-quality term and concept suggestions. To do so, we use the dual document representation X  concepts and terms X  X o create a generative language model for each concept, which bridges the gap between vocabulary terms and concepts. Related work has also used textual representations to rep-resent concepts, see e.g., [ 1 , 11 ], however, there are two important differences. First, we use statistical language modeling techniques to parametrize the concept models, by leveraging the dual represen-tation of the documents. Second, we found that simple maximum likelihood estimation assigns too much probability mass to terms and concepts which may not be relevant to each document. Thus we apply an EM algorithm to  X  X arsimonize X  the document models.
The research questions we address are twofold: (i) what are the results of applying our model as compared to a query-likelihood baseline as well as compared to a run based on relevance mod-els [ 9 ] and (ii) what is the influence of parsimonizing? To answer these questions, we use the TREC Genomics track test collections in conjunction with MedLine. MedLine contains over 16 million bibliographic records of publications from the life sciences domain and each abstract therein has been manually indexed by trained cu-rators, who use concepts from the MeSH (Medical Subject Head-ings) thesaurus [ 10 ]. We show that our approach is able to achieve similar or better performance than relevance models, whilst at the same time providing high quality concepts to facilitate navigation. Examples will show that our parsimonious concept models gener-ate terms that are more specific than those acquired through maxi-mum likelihood estimates.
To integrate concepts in the retrieval process, we propose a con-ceptual query model which is an interpolation of the initial query with terms obtained from a double concept translation. In this translation, concepts are used as a pivot language [ 8 ]; the initial query is translated to concepts and back to expanded query terms: where #( t,Q ) is the number of times term t occurs in query Q and | Q | is the query length. Two components need to be estimated here: the probability of a concept given a query, P ( c | Q ) , and of a term given a concept, P ( t | c ) .

To acquire P ( t | c ) , we will use the assignments of MeSH con-cepts to documents in MedLine and aggregate over the documents D c which are labeled with a particular concept c : We drop the conditional dependence of t on c given a document D , assume P ( D ) to be uniform, and apply Bayes X  rule to obtain: where P ( c ) is a maximum likelihood (ML) estimation on a back-ground collection M : However, if P ( t | D ) and P ( c | D ) are estimated based on ML, more general terms and concepts acquire too much probability mass, simply because they occur more frequently. To make the distri-butions more document specific, we consider both models to be a mixture of a document model P ( x | D ) and a background model P ( x | M ) , where x  X  X  t,c } , and we  X  X arsimonize X  the ML esti-mate using the following EM algorithm [ 6 ]: M-step: P ( x | D ) = e x P For our experiments we fix  X  = 0 . 15 [ 6 ]. Table 1 a shows the ef-fect of applying the parsimonious model to the estimation of con-cept D000544 ( X  X lzheimer Disease X ). The parsimonious approach emphasizes more specific and thus more useful terms, including acronyms or abbreviations X  X  X d X  in this particular example.
Next, we also need need a way of estimating concepts for each query, which means that we are looking for a set of concepts C such that c  X  X  Q have the highest posterior probability P ( c | Q ) . We approach this again by looking at the assignment of concepts to documents, but this time we consider documents which are related to the original query, by using the top ranked documents D an initial retrieval run: where P ( D | Q ) is determined using the retrieval scores. Note that we assume that P ( c | D,Q ) = P ( c | D ) , such that we can directly use Eq. 3 . A clear example of the effects of applying our model to the estimation of P ( c | Q ) is given in Table 1 b. The parsimonious approach is not only able to retrieve more specific concepts, such as  X  X resenilin-1 X , but it is also able to retrieve multiple aspects of the topic, such as related genes, proteins, and diseases. Table 2: Results for baseline query-likelihood run (QL), rele-vance models (RM), and conceptual query models (CM) (best results in boldface).
To determine the retrieval performance of our conceptual query model, we compare it with a baseline query-likelihood run (QL) and a relevance feedback run based on Lavrenko and Croft [ 9 ] X  X  relevance models (RM) on the full range of available TREC Ge-nomics test collections [ 2 , 3 , 4 , 5 ]. We use the same document set D Q ( |D Q | = 50 ) and parameter settings for the RM runs and for our runs based on Eq. 1 (CM). The results of our experiments are listed in Table 2 . (We did not perform extensive sweeps over possible values for |D Q | or  X  ; we did explore  X  and found that the optimal setting lies within the range 0 . 15  X  0 . 35 .)
Although the differences in results are not statistically significant (between QL and RM, QL and CM, and RM and CM X  X ested using a two-tailed paired t-test at p&lt; 0 . 01 ), we note that the conceptual query model and the relevance model consistently outperform the query-likelihood baseline. The only test collection where RM does not perform well is the 2007 collection, which may be the effect of the new task introduced that year [ 5 ]. CM thus rivals the per-formance of relevance models on most of the evaluated test collec-tions, whilst it is able to generate sensible navigation suggestions in the form of relevant concepts.
We have introduced a parsimonious conceptual query model whose retrieval performance matches that of relevance models, while it is also able to generate high quality navigation suggestions in the form of concepts. Future work concerns further experimental vali-dation of our results on additional test collections, as well as revis-iting the modeling assumptions we made.
This work was carried out in the context of the BioRange pro-gramme of the Netherlands Bioinformatics Centre, supported by a BSIK grant through the Netherlands Genomics Initiative, by the Virtual Laboratory for e-Science project, by the Netherlands Or-ganisation for Scientific Research (NWO) under project numbers 220-80-001, 017.001.190, 640.001.501, 640.002.501, STE-07-012 and by the E.U. IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.
