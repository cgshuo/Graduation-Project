 Presenting search results over a speech-only communication chan-nel involves a number of challenges for users due to cognitive lim-itations and the serial nature of speech. We investigated the impact of search result summary length in speech-based web search, and compared our results to a text baseline. Based on crowdsourced workers, we found that users preferred longer, more informative summaries for text presentation. For audio, user preferences de-pended on the style of query. For single-facet queries, shortened audio summaries were preferred, additionally users were found to judge relevance with a similar accuracy compared to text-based summaries. For multi-facet queries, user preferences were not as clear, suggesting that more sophisticated techniques are required to handle such queries.
 H.5.1 [ Multimedia Information Systems ]; H.3.3 [ Information Search and Retrieval ] Spoken Retrieval; Search Result Summarisation; Crowdsourcing
Speech-based web search (i.e., posing search queries using voice rather than a keyboard/touchscreen interface) is increasingly ubiq-uitous, particularly through the use of mobile devices. Several systems (e.g., Siri, Google Now, Cortana) can speak a reply to  X  X actoid X -style search queries (e.g.,  X  X ow high is Everest? X ). If no factoid answer exists, the systems revert to displaying a ranked list of results on screen. However, there are situations where a full speech-only interface is preferable, such as while driving a car [6, 7], when there is no screen or keyboard available [20], when users are mobile [12, 16, 18], or when using wearable devices [4]. c  X  Moreover, visually presented results may be inaccessible for cer-tain populations, such as visually impaired users [14, 17] or people with limited literacy skills.

Presenting lists of search results over a speech-only communi-cation channel presents a number of challenges; in particular, sim-ply speaking the textual component of a standard search results list has been shown to be ineffectual [17]. The serial nature of the speech/audio channel makes it difficult for users to  X  X kim X  back and forth over a list of results (a standard process in browsing a visual list).

Few studies have investigated techniques for effective presenta-tion of web search results via a speech-only communication chan-nel (hereafter referred to as audio) [7]. The present study seeks to address this. In particular, we seek a better understanding of how to present search results over audio while not overwhelming the users with information [18], nor leaving users uncertain as to whether what they heard covered the information space [19].

The length of a spoken search result summary plays a crucial role in the success or failure of presenting search results over au-dio. A short summary might not yield enough information to judge whether the retrieved document is relevant or not; in contrast, a more descriptive summary might take too long to be played and thereby diminish user experience. Thus a trade-off is necessary be-tween a short summary and a longer, more descriptive summary.
The present study investigated these trade-offs via a crowdsource-based interactive experimental design. The aim of the study was to develop a baseline of understanding about the audio result sum-mary length users prefer. This study aimed to answer the following research questions:
Our experiments used a crowdsourcing platform to present queries and search results to users. Result summaries of various length were presented in text or audio form. Summary length was ei-ther a full Google-length summary or a truncated version extracted from the original summary. Users were asked to select a result that best addressed the query, and were also presented with post-task and exit questionnaires. The present study used the CrowdFlower crowdsourcing tool [9, 10] to provide user data for the tasks and questionnaires.
We first describe the task users undertook, followed by the queries, search engine results summaries, post-task and exit questionnaires, and use of text as a baseline for audio. The crowdsourcing setup for presenting results lists to users and collecting judgements is also described. 1
Users were presented with a task which consisted of three queries, corresponding lists of result summaries, a post-task questionnaire, and an exit questionnaire. Users were asked to read three query descriptions and read/listen to the summaries, before stating their preferred result description in the questionnaires. One set of sum-maries was the full length, one set was truncated. The task was designed to reflect common search tasks on the Web: query topics from the Text REtrieval Conference (TREC) 2013 Web Track were used [5]. The queries in the track were man-ually selected from logs of a commercial search engine [5]. Since this was a preliminary study, a subset of twenty queries from the TREC 2013 Web Track dataset were used. An assessment of the queries indicated two categories, single-facet queries (queries with a clear intent) and faceted queries (typically broader in intent and represented with subtopics). It was decided to investigate whether these categories impacted on result summary preference.

The present study included seven single-facet and thirteen faceted queries. Table 1 shows an example for each type of query. Only informational subtopics were selected for the present study, since they have a primary interpretation which is reflected in the descrip-tion field.
Each query was sent to the Google search engine and Google-generated text summaries were extracted for the top five search re-sults. 2 The summaries were converted into a spoken synthetic voice (audio). Instructions were added to each summary set: These are summaries of the top results. Select the summary that leads to the information you are looking for . A list-rank number was added to the front of each summary to allow easy identification of the users X  selection. Table 1 shows a sample summary before and after the conversion for the task.

Truncated versions of the original Google-generated summaries were created manually. Here, a contiguous subset of nine words was selected from each full summary. Nine was found to be a little less than half the length of a standard Google-generated summary.
All experiments were performed under Ethics Application BSEH 10-14 at RMIT University.
Only the top five were presented to keep the audio task manage-able.
 For this initial work, manual summaries were created to avoid bias introduced by poor automatic truncation, which may negatively im-pact user perception. Human judgement was assumed to be the best way to preserve the meaning of the summary.

The presentation of the twenty search queries was randomized with the use of a Latin square design. Each user saw three queries per task. The order in which the users were presented with the result description (original summary vs. truncated summary) was rotated. These steps were implemented to avoid learning effects triggered by usage order and by users becoming accustomed to a synthetic voice [1, 11].

A problem reported with crowdsourcing is that users try to re-ceive payment without completing the task properly [3]. Therefore, every task was populated with a Gold Question to help with data integrity and to detect if the participant was paying attention to the task [2]. The Gold Questions in this study used queries with clear pre-determined answers. Users were presented with three query descriptions and corresponding summaries. However, one of these summaries was populated with unrelated summary results. A user that was not able to identify that the summaries were not related to the query had their judgements discarded.

Though crowdsourcing provides a significant and easily accessi-ble sample, we discovered that despite the precaution of releasing the tasks at different times and on different days, the results showed that many users completed every task. As a result, a modification was made and instead of running a new task for every set of queries, the queries were grouped into batches of ten and restrictions placed on how many batches users could judge.
Post-task questionnaires are frequently implemented to assess the system X  X ask interaction and gather user feedback on their ex-periences with using a particular system to complete a particular task [11]. Since no validated questionnaire has been published for studying user reaction to audio summaries, we used questionnaires adapted from previous studies.

Users completed the post-task questionnaire three times for each of the queries given. The post-task questionnaire (see Table 2) consisted of five questions on a five-point Likert scale (1 X 5); one question on query judgement with multiple choice answers (6); one question on how the participant listened to the audio with tick boxes (7); and a text box for further comments (8).

In addition, [11] suggests conducting a questionnaire at the end of the completed task to capture comparisons for within-subjects studies. Thus users were also presented with an exit questionnaire. Using a dynamic panel, the exit questionnaire was available only to users who were successful in answering a Gold Question. The exit questionnaire was used to measure users X  preferences for informa-tion exploration using different result description configurations. The exit questionnaire was analysed with the help of responses from the post-task questionnaire.
Tasks were paired, whereby one task X  X  summaries were audio and the other X  X  were text. The text output was used to create a base-line measure of the system, facilitating analysis of the difference in preference between audio and text [11], enabling us to compare audio against the text baseline. CrowdFlower allows contributors (users who submit tasks to CrowdFlower) to place constraints on the users assigned to a task. The following constraints were put in place for the present study:
Although users were not permitted to participate more than once in a task which had the same set of queries, they were allowed to participate in tasks with different queries. A minimum of 36 users were recruited for each given task [13].

When a participant did not answer the Gold Question success-fully, that participant X  X  submission was discarded. These users were also not allowed to participate in later tasks. It was found that 11.8% of all users did not answer the Gold Question successfully, their submissions were discarded.
The exit questionnaire was analyzed using the  X  2 goodness-of-fit test to compare the distribution of scores across two levels. Results are shown in Table 3.

The  X  2 goodness-of-fit test [11] was used to assess whether chang-ing the result summary had an effect on user preference.

Users were asked (in the post-task questionnaire) which sum-mary made the users want to know more about the underlying doc-ument. 3 These judgements were analysed with the two-sample Kolmogorov-Smirnov test ( KS test ) to determine whether two given samples follow the same distribution [15]. The tasks compared the result  X  X lick X  distributions where the length of the summary
This is equivalent to asking which result they would click in a traditional search engine result page (SERP).
 Table 3: Exit questionnaire results for preferences in the search engine result summaries. N p &lt; . 01 .
 was manipulated. The tasks were conducted in pairs: audio and text [11].

The KS test showed that for truncated summaries, only two out of twenty queries were there different distributions for audio-based versus text-based summaries. For full-length summaries and three out of twenty summaries resulted in different distributions for au-dio versus text. In general, the same distribution was found for query judgements between the text baseline and audio indicating that users made similar query judgements regardless of the presen-tation style being audio or text.
Table 3 shows that users tend to prefer full text summaries rather than their truncated counterpart. For instance, 57% of users would recommend full text summaries to a friend and 57% indicated that full summaries gave better results. The  X  2 goodness-of-fit tests were statistically significant ( p &lt; . 01 ) for three exit questions in re-lation to the use of the original summary for presenting text results, indicating that this information exploration style was preferred.
Results in audio summaries do not indicate a clear preference between full and truncated (preferences differ at most by only 2%). The  X  2 goodness-of-fit tests were not statistically significant ( p &gt; . 05 ) for any of the exit questions about presenting audio results. No statistically significant difference were found for faceted queries. However, for single-facet queries using audio, all exit questions where statistically significant ( p &lt; . 05 ) with a user preference for truncated summaries.

The KS test revealed that only one out of seven single-facet queries judgements was statistically significant ( p &lt; . 05 ) when comparing truncated audio to the truncated text baseline. The KS test was not statistically significant for any faceted queries in audio, with one exception.

Users reported that overall it was easier to recall truncated audio summaries (54.4%) than full audio summaries (49.9%). Moreover, fewer users reported that they had to listen to the audio more than once (16.8%) for truncated summaries than for full-length audio summaries (23.7%). Only three users reported that they stopped the audio for truncated summaries, possibly indicating that both the information presented and the length of the information were short enough to avoid cognitive overload.
The present study investigated whether summaries of shorter length would be preferred for audio presentation as they could avoid over-loading users X  memory [20]. The exit questionnaire responses demon-strated that, for text summaries, full-length were preferred. How-ever, for audio, no significant length preference was found.
Users reported that truncated summaries for single-facet queries were preferred. Thus for simpler, less ambiguous queries, shorter audio summaries were both effective and preferred. However, for faceted queries, users may have benefited from a more informative audio response even at the cost of listening time.

The single-facet query judgement distribution for both audio and text followed the distribution reported in past work [8] where query results ranked first and second receive most user attention. How-ever, this expected distribution was not reflected in the faceted query judgements; rather, summaries ranked first and last obtained the most attention. This is also of interest: the serial nature of audio seems to lead to a bias towards most-recently-heard results, a be-havior not found in visual presentation.

Users left comments in questionnaires. For summaries of faceted queries, they indicated that the summaries were missing key infor-mation. This suggests that the way of presenting summaries may differ depending on query intent: short audio summaries may be appropriate for clear intent queries (single-facet), whereas broader intent queries (faceted) may need more complex techniques (e.g., interactive/conversational approaches).
The present paper describes an initial investigation into result summaries for audio-based search. This study aimed to answer the following research questions:
Differences were observed when result summary lengths were presented in the spoken retrieval scenario. In general, there was no preference for fuller descriptive summaries or for truncated sum-maries. However, results revealed that different kinds of queries (single-facet vs. faceted) benefited from an optimised summary depending on the type of query.

Extensions of the research include testing based on a larger num-ber of queries and using automated techniques for truncating sum-maries for audio presentation. (The current method used manual truncation to avoid poor truncation from confounding the results.) More significantly, the results suggest a need for developing more sophisticated approaches to handling result-presentation over audio for faceted queries.
 This research was partially supported by Australian Research Coun-cil Project LP130100563 and Real Thing Entertainment Pty Ltd. The authors wish to thank Bruce Croft who provided valuable feed-back. [1] W. Albert, T. Tullis, and D. Tedesco. Beyond the Usability [2] S. Buchholz, J. Latorre, and K. Yanagisawa. Crowdsourced [3] C. Callison-Burch and M. Dredze. Creating speech and [4] E. Chang, F. Seide, H. M. Meng, C. Zhuoran, S. Yu, and [5] K. Collins-Thompson, P. Bennett, F. Diaz, C. L. Clarke, and [6] V. Demberg and A. Sayeed. Linguistic cognitive load: impli-[7] V. Demberg, A. Winterboer, and J. D. Moore. A strategy for [8] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. [9] G. Jones. An introduction to crowdsourcing for language and [10] F. Jur  X  c X   X  cek, S. Keizer, M. Ga X ic, F. Mairesse, B. Thomson, [11] D. Kelly. Methods for evaluating interactive information re-[12] S. R. Klemmer, A. K. Sinha, J. Chen, J. A. Landay, [13] B. P. Knijnenburg, M. C. Willemsen, and A. Kobsa. A prag-[14] J. Lai and N. Yankelovich. Speech Interface Design . Elsevier, [15] F. J. Massey. The Kolmogorov-Smirnov test for goodness of [16] L. J. Najjar, J. J. Ockerman, and J. C. Thompson. User inter-[17] N. G. Sahib, D. Al Thani, A. Tombros, and T. Stockman. Ac-[18] M. Turunen, J. Hakulinen, N. Rajput, and A. A. Nanavati. [19] S. Varges, F. Weng, and H. Pon-Barry. Interactive question [20] N. Yankelovich, G.-A. Levow, and M. Marx. Designing
