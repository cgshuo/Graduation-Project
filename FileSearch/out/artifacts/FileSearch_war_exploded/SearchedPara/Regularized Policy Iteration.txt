
Amir-massoud Farahmand 1 , Mohammad Ghavamzadeh 2 , Csaba Szepesv  X  ari 1 , Shie Mannor 3  X  A key idea in reinforcement learning (RL) is to learn an action-value function which can then be used to derive a good control policy [15]. When the state space is large or infinite, value-function approximation techniques are necessary, and their quality has a major impact on the quality of the learned policy. Existing techniques include linear function approximation (see, e.g., Chapter 8 of [15]), kernel regression [12], regression tree methods [5], and neural networks (e.g., [13]). The user of these techniques often has to make non-trivial design decisions such as what features to use in the linear function approximator, when to stop growing trees, how many trees to grow, what kernel bandwidth to use, or what neural network architecture to employ. Of course, the best answers to these questions depend on the characteristics of the problem in hand. Hence, ideally, these questions should be answered in an automated way, based on the training data.
 A highly desirable requirement for any learning system is to adapt to the actual difficulty of the learning problem. If the problem is easier (than some other problem), the method should deliver better solution(s) with the same amount of data. In the supervised learning literature, such proce-dures are called adaptive [7]. There are many factors that can make a problem easier, such as when only a few of the inputs are relevant, when the input data lies on a low-dimensional submanifold of the input space, when special noise conditions are met, when the expansion of the target function is sparse in a basis, or when the target function is highly smooth. These are called the regularities of the problem. An adaptive procedure is built in two steps: 1) designing flexible methods with a few tunable parameters that are able to deliver  X  X ptimal X  performance for any targeted regular-ity, provided that their parameters are chosen properly, and 2) tuning the parameters automatically (automatic model-selection).
 Smoothness is one of the most important regularities: In regression when the target function has smoothness of order p the optimal rate of convergence of the squared L 2 -error is n  X  2 p/ (2 p + d ) , where n is the number of data points and d is the dimension of the input space [7]. Hence, the rate of convergence is higher for larger p  X  X . Methods that achieve the optimal rate are more desirable, at least in the limit for large n , and seem to perform well in practice. However, only a few methods in the regression literature are known to achieve the optimal rates. In fact, it is known that tree methods with averaging in the leaves, linear methods with piecewise constant basis functions, and kernel estimates do not achieve the optimal rate, while neural networks and regularized least-squares estimators do [7]. An advantage of using a regularized least-squares estimator compared to neural networks is that these estimators do not get stuck in local minima and therefore their training is more reliable.
 In this paper we study how to add L 2 -regularization to value function approximation in RL. The problem setting is to find a good policy in a batch or active learning scenario for infinite-horizon expected total discounted reward Markovian decision problems with continuous state and finite ac-tion spaces. We propose two novel policy evaluation algorithms by adding L 2 -regularization to two widely-used policy evaluation methods in RL: Bellman residual minimization (BRM) [16; 3] and least-squares temporal difference learning (LSTD) [4]. We show how our algorithms can be imple-mented efficiently when the value-function approximator belongs to a reproducing kernel Hilbert space. We also prove finite-sample performance bounds for our algorithms. In particular, we show that they are able to achieve a rate that is as good as the corresponding regression rate when the value functions belong to a known smoothness class. We further show that this rate of convergence carries through to the performance of a policy found by running policy iteration with our regularized policy evaluation methods. The results indicate that from the point of view of convergence rates RL is not harder than regression estimation, answering an open question of Antos et al. [2]. Due to space limitations, we do not present the proofs of our theorems in the paper; they can be found, along with some empirical results using our algorithms, in [6].
 To our best knowledge this is the first work that addresses finite-sample performance of a regularized RL algorithm. While regularization in RL has not been thoroughly explored, there has been a few works that used regularization. Xu et al. [17] used sparsification in LSTD. Although sparsification does achieve some form of regularization, to the best of our knowledge the effect of sparsification on generalization error is not well-understood. Note that sparsification is fundamentally different from our approach. In our method the empirical error and the penalties jointly determine the solu-tion, while in sparsification first a subset of points is selected independently of the empirical error, which are then used to obtain a solution. Comparing the efficiency of these methods requires further research, but the two methods can be combined, as was done in our experiments. Jung and Polani [9] explored adding regularization to BRM, but this solution is restricted to deterministic problems. The main contribution of that work was the development of fast incremental algorithms using sparsi-fication techniques. L 1 penalties have been considered by [11], who were similarly concerned with incremental implementations and computational efficiency. As we shall work with continuous spaces, we first introduce a few concepts from analysis. This is followed by an introduction to Markovian Decision Processes (MDPs) and the associated concepts and notation.
 For a measurable space with domain S , we let M ( S ) and B ( S ; L ) denote the set of probability measures over S and the space of bounded measurable functions with domain S and bound 0 &lt; L &lt;  X  , respectively. For a measure  X   X  M ( S ) , and a measurable function f : S  X  R , we define the L 2 (  X  ) -norm of f , k f k  X  , and its empirical counterpart k f k  X ,n as follows: If { s t } is ergodic, k f k 2  X ,n converges to k f k 2  X  as n  X  X  X  .
 A finite-action discounted MDP is a tuple ( X , A , P, S,  X  ) , where X is the state space, A = { a 1 , a 2 , . . . , a M } is the finite set of M actions, P : X  X A  X  M ( X ) is the transition probability gives the corresponding distribution of immediate rewards, and  X   X  (0 , 1) is a discount factor. We make the following assumptions on MDP: Assumption A1 (MDP Regularity) The set of states X is a compact subspace of the d -dimensional Euclidean space and the expected immediate rewards r ( x, a ) = R rS ( dr | x, a ) are bounded by R We denote by  X  : X  X  M ( A ) a stationary Markov policy . A policy is deterministic if it is a mapping from states to actions  X  : X  X  X  . The value and the action-value functions of a policy  X  , denoted respectively by V  X  and Q  X  , are defined as the expected sum of discounted rewards that are encountered when the policy  X  is executed:
V  X  ( x ) = E  X  Here R t denotes the reward received at time step t ; R t  X  S (  X | X t , A t ) , X t evolves according to X policy  X  , the functions V  X  and Q  X  are bounded by V max = Q max = R max / (1  X   X  ) . The action-value function of a policy is the unique fixed-point of the Bellman operator T  X  : B ( X X A )  X  B ( X X A ) defined by Given an MDP, the goal is to find a policy that attains the best possible values, V  X  ( x ) = sup  X  V  X  ( x ) ,  X  x  X  X . Function V  X  is called the optimal value function . Similarly the optimal action-value function is defined as Q  X  ( x, a ) = sup  X  Q  X  ( x, a ) ,  X  x  X  X ,  X  a  X  A . We say that a deterministic policy  X  is greedy w.r.t. an action-value function Q and write  X  =  X   X  (  X  ; Q ) , if,  X  ( x )  X  argmax a  X  X  Q ( x, a ) ,  X  x  X  X ,  X  a  X  A . Greedy policies are important because any greedy policy w.r.t. Q  X  is optimal. Hence, knowing Q  X  is sufficient for behaving optimally. In this paper we shall deal with a variant of the policy iteration algorithm [8]. In the basic version of policy iteration an optimal policy is found by computing a series of policies, each being greedy w.r.t. the action-value function of the previous one.
 Throughout the paper we denote by F M  X  { f : X  X A X  R } some subset of real-valued func-tions over the state-action space X  X  A , and use it as the set of admissible functions in the op-timization problems of our algorithms. We will treat f  X  F M as f  X  ( f 1 , . . . , f M ) , f j ( x ) = otherwise. The ability to evaluate a given policy is the core requirement to run policy iteration. Loosely speak-ing, in policy evaluation the goal is to find a  X  X lose enough X  approximation V (or Q ) of the value the term  X  X lose enough X  in this context and it does not necessarily refer to a minimization of some norm. If Q  X  (or noisy estimates of it) is available at a number of points ( X t , A t ) , one can form a and then use a supervised learning algorithm to infer a function Q that is meant to approximate Q  X  . Unfortunately, in the context of control, the target function, Q  X  , is not known in advance and its high quality samples are often very expensive to obtain if this option is available at all. Most often these values have to be inferred from the observed system dynamics, where the observations do not necessarily come from following the target policy  X  . This is referred to as the off-policy learning problem in the RL literature. The difficulty arising is similar to the problem when training and test distributions differ in supervised learning. Many policy evaluation techniques have been developed in RL. Here we concentrate on the ones that are directly related to our proposed algorithms. 3.1 Bellman Residual Minimization The idea of Bellman residual minimization (BRM) goes back at least to the work of Schweitzer and Seidmann [14]. It was used later in the RL community by Williams and Baird [16] and Baird [3]. The basic idea of BRM comes from writing the fixed-point equation for the Bellman operator in the form Q  X   X  T  X  Q  X  = 0 . When Q  X  is replaced by some other function Q , the left-hand side becomes non-zero. The resulting quantity, Q  X  T  X  Q , is called the Bellman residual of Q . If the magnitude of the Bellman residual, k Q  X  T  X  Q k , is small, then Q can be expected to be a good approximation of Q  X  . For an analysis using supremum norms see, e.g., [16]. It seems, however, more natural to use a weighted L 2 -norm to measure the magnitude of the Bellman residual as it leads to an optimization problem with favorable characteristics and enables an easy connection to regression function estimation [7]. Hence, we define the loss function L BRM ( Q ;  X  ) = k Q  X  T  X  Q k 2  X  , where  X  is the stationary distribution of states in the input data. Using Eq. (2) with samples ( X t , A t )  X Q ( X t +1 ,  X  ( X t +1 )) , the empirical counterpart of L BRM ( Q ;  X  ) can be written as However, as it is well-known (e.g., see [15],[10]), in general,  X  L BRM is not an unbiased estimate of non-vanishing variance term in Eq. (3). A common suggestion to deal with this problem is to use uncorrelated or  X  X ouble X  samples in  X  L BRM . According to this proposal, for each state-action pair in the sample, at least two next states should be generated (e.g., see [15]). This is neither realistic nor sample-efficient unless a generative model of the environment is available or the state transitions are deterministic. Antos et al. [2] recently proposed a de-biasing procedure for this problem. We will refer to it as modified BRM in this paper. The idea is to cancel the unwanted variance by introducing approximating the action-value function Q  X  by solving where the supremum comes from the negative sign of k h  X  T  X  Q k 2  X  . They showed that optimizing the new loss function still makes sense and the empirical version of this loss is unbiased. Solving Eq. (4) requires solving the following nested optimization problems: Of course in practice, T  X  Q is replaced by its sample-based approximation  X  T  X  Q . 3.2 Least-Squares Temporal Difference Learning Least-squares temporal difference learning (LSTD) was first proposed by Bradtke and Barto [4], and later was extended to control by Lagoudakis and Parr [10]. They called the resulting algorithm least-squares policy iteration (LSPI), which is an approximate policy iteration algorithm based on LSTD. Unlike BRM that minimizes the distance of Q and T  X  Q , LSTD minimizes the distance of Q and  X  T  X  Q , the back-projection of the image of Q under the Bellman operator, T  X  Q , onto the space of admissible functions F M (see Figure 1). Formally, this means that LSTD minimizes the loss the fixed-point of operator  X  T  X  . The projection operator  X  : B ( X  X A )  X  B ( X  X A ) is defined by  X  f = argmin h  X  X  M k h  X  f k 2  X  . In order to make this minimization problem computationally feasible, it is usually assumed that F M is a linear subspace of B ( X  X A ) . The LSTD solution can therefore be written as the solution of the following nested optimization problems: where the first equation finds the projection of T  X  Q onto F M , and the second one minimizes the distance of Q and the projection.
 Figure 1: This figure shows the loss functions minimized by BRM, modified BRM, and LSTD methods. The function space an action-value function Q  X  F M to a function T  X  Q . The vec-tor connecting T  X  Q and its back-projection to F M ,  X  T  X  Q , is orthogonal to the function space F M . The BRM loss function is the squared Bellman error, the distance of Q and T  X  Q . In order to obtain the modified BRM loss, the squared distance of T  X  Q and  X  T  X  Q is subtracted from the squared Bellman error. LSTD aims at a function Q that has minimum distance to  X  T  X  Q .
 Antos et al. [2] showed that when F M is linear, the solution of modified BRM (Eq. 4 or 5) coincides with the LSTD solution (Eq. 6). A quick explanation for this is: the first equations in (5) and (6) are (6) have the same solution. In this section, we introduce two regularized policy iteration algorithms. These algorithms are in-stances of the generic policy-iteration method, whose pseudo-code is shown in Table 1. By assump-by a policy  X  , thus, A t =  X  ( X t ) and R t  X  S (  X | X t , A t ) . Examples of such policy  X  are  X  i plus some exploration or some stochastic stationary policy  X  b . The action-value function Q (  X  1) is used to initialize the first policy. Alter-natively, one may start with an arbitrary initial policy. The proce-dure PEval takes a policy  X  i (here the greedy policy w.r.t. the current action-value function Q ( i  X  1) ) along with training sample D i , and returns an approximation to the action-value function of policy  X  i . There are many possibilities to design PEval. In this paper, we propose two approaches, one based on regularized ( modified ) BRM (REG-BRM), and one based on regularized LSTD (REG-LSTD). In REG-BRM, the next iteration is computed by solving the following nested optimization problems: Z norms), and  X  h,n ,  X  Q,n &gt; 0 are regularization coefficients.
 In REG-LSTD, the next iteration is computed by solving the following nested optimization prob-lems: It is important to note that unlike the non-regularized case described in Sections 3.1 and 3.2, REG-BRM and REG-LSTD do not have the same solution. This is because, although the first equations therefore the objective functions of the second equations in (7) and (8) are not equal and they do not have the same solution.
 We now present algorithmic solutions for REG-BRM and REG-LSTD problems described above. We can obtain Q ( i ) by solving the regularization problems of Eqs. (7) and (8) in a reproducing kernel Hilbert space (RKHS) defined by a Mercer kernel K . In this case, we let the regularization terms J ( h ) and J ( Q ) be the RKHS norms of h and Q , k h k 2 H and k Q k 2 H , respectively. Using the Representer theorem, we can then obtain the following closed-form solutions for REG-BRM and REG-LSTD. This is not immediate, because the solutions of these procedures are defined with nested optimization problems.
 be obtained by C k ( Z 0 i ,  X  Z j ) , and [ K Q ] ij = k (  X  Z i ,  X  Z j ) . In this section, we analyze the statistical properties of the policy iteration algorithms based on REG-BRM and REG-LSTD. We provide finite-sample convergence results for the error between Q  X  N , the action-value function of policy  X  N , the policy resulted after N iterations of the algorithms, and Q  X  , the optimal action-value function. Due to space limitations, we only report assumptions and main results here (Refer to [6] for more details). We make the following assumptions in our analysis, some of which are only technical: Assumption A2 (1) At every iteration, samples are generated i.i.d. using a fixed distribution over policy being evaluated. We further assume that  X  b selects all actions with non-zero probability. (2) The function space F used in the optimization problems (7) and (8) is a Sobolev space W k ( R d ) with 2 k &gt; d . We denote by J k ( Q ) the norm of Q in this Sobolev space. (3) The selected function space F M contains the true action-value function, i.e., Q  X   X  X  M . (4) For every function Q  X  F M with bounded norm J ( Q ) , its image under the Bellman operator, T  X  Q , is in the same space, and we have J ( T  X  Q )  X  BJ ( Q ) , for some positive and finite B , which is independent of Q . (5) We assume F M  X  B ( X  X A ; Q max ) , for Q max &gt; 0 . (1) indicates that the training sample should be generated by an i.i.d. process. This assumption is used mainly for simplifying the proofs and can be extended to the case where the training sample is a single trajectory generated by a fixed policy with appropriate mixing conditions as was done in [2]. (2) Using Sobolev space allows us to explicitly show the effect of smoothness k on the convergence rate of our algorithms and to make comparison with the regression learning settings. Note that Sobolev spaces are large: In fact, Sobolev spaces are more flexible than H  X  older spaces (a generalization of Lipschitz spaces to higher order smoothness) in that in these spaces the norm mea-sures the average smoothness of the functions as opposed to measuring their worst-case smoothness. Thus, functions that are smooth most over the place except for some parts that have a small measure will have small Sobolev-space norms, i.e., they will be looked as  X  X imple X , while they would be viewed as  X  X omplex X  functions in H  X  older spaces. Actually, our results extend to other RKHS spaces that have well-behaved metric entropy capacity, i.e., log N (  X , F )  X  A X   X   X  for some 0 &lt;  X  &lt; 2 and some finite positive A . In (3), we assume that the considered function space is large enough to include the true action-value function. This is a standard assumption when studying the rate of convergence in supervised learning [7]. (4) constrains the growth rate of the complexity of the norm of Q under Bellman updates. We believe that this is a reasonable assumption that will hold in most practical situations. Finally, (5) is about the uniform boundedness of the functions in the selected function space. If the solutions of our optimization problems are not bounded, they must be trun-cated, and thus, truncation arguments must be used in the analysis. Truncation does not change the final result, so we do not address it to avoid unnecessary clutter.
 We now first derive an upper bound on the policy evaluation error in Theorem 2. We then show how the policy evaluation error propagates through the iterations of policy iteration in Lemma 3. Finally, we state our main result in Theorem 4, which follows directly from the first two results. Theorem 2 (Policy Evaluation Error) . Let Assumptions A1 and A2 hold. Choosing  X  Q,n = Theorem 2 shows how the number of samples and the difficulty of the problem as characterized T action-value function using REG-BRM or REG-LSTD.
 the Bellman residual at the i th iteration of our algorithms. Theorem 2 indicates that at each iteration Lemma 3, which was stated as Lemma 12 in [2], bounds the final error after N iterations as a function of the intermediate errors. Note that no assumption is made on how the sequence  X  Q ( i ) is generated in this lemma. In Lemma 3 and Theorem 4,  X   X  X  ( X ) is a measure used to evaluate the performance of the algorithms, and C  X , X  and C  X  are the concentrability coefficients defined in [2]. Lemma 3 (Error Propagation) . Let p  X  1 be a real and N be a positive integer. Then, for any sequence of functions { Q ( i ) }  X  B ( X  X  A ; Q max ) , 0  X  i &lt; N , and  X  i as defined above, the following inequalities hold: Theorem 4 (Convergence Result) . Let Assumptions A1 and A2 hold,  X  h,n and  X  Q,n use the same schedules as in Theorem 2, and the number of samples n be large enough. The error between the optimal action-value function, Q  X  , and the action-value function of the policy resulted after N iterations of the policy iteration algorithm based on REG-BRM or REG-LSTD,  X  Q  X  N , is with probability at least 1  X   X  for some c &gt; 0 .
 Theorem 4 shows the effect of number of samples n , degree of smoothness k , number of iterations N , and concentrability coefficients on the quality of the policy induced by the estimated action-value function. Three important observations are: 1) the main term in the rate of convergence it is an optimal rate value-function estimation, 2) the effect of smoothness k is evident: for two problems with different degrees of smoothness, learning the smoother one is easier  X  an intuitive, but previously not rigorously proven result in the RL literature, and 3) increasing the number of iterations N increases the error of the second term, but its effect is only logarithmic. In this paper we showed how L 2 -regularization can be added to two widely-used policy evalua-tion methods in RL: Bellman residual minimization (BRM) and least-squares temporal difference learning (LSTD), and developed two regularized policy evaluation algorithms REG-BRM and REG-LSTD. We then showed how these algorithms can be implemented efficiently when the value-function approximation belongs to a reproducing kernel Hilbert space (RKHS). We also proved finite-sample performance bounds for REG-BRM and REG-LSTD, and the regularized policy iter-ation algorithms built on top of them. Our theoretical results indicate that our methods are able to achieve the optimal rate of convergence under the studied conditions.
 One of the remaining problems is how to find the regularization parameters:  X  h,n and  X  Q,n . Us-ing cross-validation may lead to a completely self-tuning process. Another issue is the type of regularization. Here we used L 2 -regularization, however, the idea can be extended naturally to L 1 -regularization in the style of Lasso, opening up the possibility of procedures that can handle a high number of irrelevant features. Although the i.i.d. sampling assumption is technical, extending our analysis to the case when samples are correlated requires generalizing quite a few results in super-vised learning. However, we believe that this can be done without problem following the work of [2]. Extending the results to continuous-action MDPs is another major challenge. Here the interest-which scales quite unfavorably with the dimension of the action space [1].

