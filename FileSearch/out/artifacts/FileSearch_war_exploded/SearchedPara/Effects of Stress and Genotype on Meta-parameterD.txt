 Animals choose their actions based on reward expectation an d motivational drives. Different aspects of learning are known to be influenced by acute stress [1, 2, 3] and genetic background [4, 5]. Stress effects on learning depend on the stress type ( eg task-specific or unspecific) and intensity, as well stress can affect short-and long-term memory by modulating plasticity through stress hormones and neuromodulators [1, 2, 3, 6]. However, there is no integr ative model that would accurately predict and explain differential effects of acute stress. A lthough stress factors can be described in quantitative measures, their effects on learning, memory, and performance are strongly influenced by how an animal perceives it. The subjective experience can be influenced by emotional memories as well as by behavioral genetic traits such as anxiety, impu lsivity, and novelty reactivity [4, 5, 7]. and under different stress conditions were combined with a m odeling approach. In our models, behavioral performance as a function of time was described i n the framework of temporal difference reinforcement learning (TDRL).
 In TDRL models [8] a modeled animal, termed agent , can occupy various states and undertake actions in order to acquire rewards. The expected values of c umulative future reward (Q-values) are learned by observing immediate rewards delivered under dif ferent state-action combinations. Their update is controlled by certain meta-parameters such as lea rning rate, future reward discount factor, and memory decay/interference factor. The Q-values (toget her with the exploitation/exploration factor) determine what actions are more likely to be chosen w hen the animal is at a certain state, ie they represent the goal-oriented behavioral strategy lear ned by the agent. The activity of certain neuromodulators in the brain are thought to be associated wi th the role the meta-parameters play in the TDRL models. Besides dopamine (DA), whose levels are k nown to be related to the TD reward prediction error [9], serotonin (5-HT), noradrenal ine (NA), and acetylcholine (ACh) were meta-parameter dynamics can give an insight into the putati ve neuromodulatory activities in the brain. Dynamic parameter estimation approaches, recently applied to behavioral data in the context of TDRL [11], could be used for this purpose.
 In our study, we carried out 5-hole-box light conditioning a nd Morris water maze experiments with C57BL/6 and DBA/2 inbred mouse strains (referred to as C57 an d DBA from now on), renown for their differences in anxiety, impulsivity, and spatial lea rning [4, 5, 12]. We exposed subgroups of animals to different kinds of stress (such as motivational s tress or task-specific uncertainty) in order to evaluate its effects on immediate performance, and also t ested their long-term memory after a break of 4-7 weeks. Then, we used TDRL models to describe the m ouse behavior and established a number of performance measures that are relevant to task le arning and memory (such as mean response times and latencies to platform) in order to compar e the outcome of the model with the an-imal performance. Finally, for each experimental session w e ran an optimization procedure to find a set of the meta-parameters, best fitting to the experimenta l data as quantified by the performance measures. This approach made it possible to relate the effec ts of stress and genotype to differences in the meta-parameter values, allowing us to make specific in ferences about learning dynamics (gen-eralized over two different experimental paradigms) and th eir neurobiological correlates. In the TDRL framework [8] animal behavior is modelled as a seq uence of actions. After an action is performed, the animal is in a new state where it can again choo se from a set of possible actions. In certain states the animal is rewarded, and the goal of learni ng is to choose actions so as to maximize the expected future reward, or Q-value , formally defined as where ( s the future reward discount factor which controls to what extent the future rewards are taken in to account. As soon as state s state X  X  value Q ( s where  X  is the learning rate . The action selection at each state is controlled by the exploitation factor  X  such that actions with high Q-values are chosen more often if the  X  is high, whereas random actions are chosen most of the time if the  X  is close to zero. Meta-parameters  X  ,  X  and  X  are the free parameters of the model. Experimental subjects were male mice (24 of the C57 strain, a nd 24 of the DBA strain), 2.5-month old at the beginning of the experiment, and food deprived to 8 5-90% of the initial weight. During an experimental session, each animal was placed into the 5-hol e-box (5HB) (Figure 1a). The animals had to learn to make a nose poke into any of the holes upon the on set of lights and not to make it in the absence of light. After the response to light, the anim als received a reward in form of a food pellet. Once a poke was initiated (see starting a poke in Figure 1b), the mouse had to stay in the hole at least for a short time (0.3-0.5 sec) in order to find the delivered reward ( continuing a poke ). Trial ended (lights turned off) as soon as the nose poke was fin ished. If the mouse did not find the reward, the reward remained in the box and the animal could fin d it during the next poke in the same box. The inter-trial interval (ITI) between subsequent tri als was 15 sec. However, a new trial could only start when during the last 3 sec before it there were no wr ong (ITI) pokes, so as to penalize spontaneous poking. The total session time was 10 min. Hence , the number of trials depended on how fast animals responded to light and how often they made IT I pokes.
 Figure 1: a. Scheme of the 5HB experiment. Open circles are the holes wher e the food is delivered, filled circles are the lights. All 5 holes were treated as equi valent during the experiment. b. 5HB state-action chart. Rectangles are states, arrows are acti ons.
 holes, they underwent 8 consecutive days of training. Durin g days 5-7 subsets of the animals were exposed to different stress conditions: motivational stre ss (MS, food deprivation to 85-87% of the initial weight vs. 88-90% in controls) and uncertainty in th e reward delivery (US, in 50% of correct responses they received either none or 2 food pellets). Mice of each strain were divided into 4 stress groups: controls, MS, US, and MS+US. After a break of 26 days t he long-term memory of the mice was tested by retraining them for another 8 days. During days 5-8 of the retraining, we again evaluated the impact of stress factors by exposing half of th e mice to extrinsic stress (ES, 30 min on an elevated platform right before the 5HB experiment).
 To model the mouse behavior we used a discrete state TDRL mode l with 6 states: [ ITI , trial ]  X  [ staying outside , starting a poke , continuing a poke ], and 2 actions: move (in or out), and stay (see Figure 1b). Actions were chosen according to the soft-max me thod [8]: where k runs over all actions and  X  is the exploitation factor. Initial Q-values were equal to z ero. Since the time spent outside the holes was comparatively lon g and included multiple (task irrelevant) actions, state/action pair staying outside / stay was given much more weight in the above formula. The time step (0.43 sec) was constant throughout the experim ent and was chosen to fit the animal performance in the beginning of the experiment. Finally, to account for the memory decay a fter each day all Q ( s, a ) values were updated as follows: where  X  is a memory decay/interference factor, and  X  Q ( s, a )  X  states and all actions at the end of the day.
 All performance measures (PMs) used in the 5HB paradigm (num ber of trials, number of ITI pokes, mean response time, mean poke length, TimePref 1 and LengthPref 2 ) were evaluated over the entire session (10 min, 1400 time steps), during which diffe rent states 3 could be visited multiple times. As opposed to an online  X  X ARSA X -type update of Q-valu es, we work with state occupancy probabilities p ( s The same mice as in the 5HB (4.5-month old at the beginning of t he experiment) were tested in a variant of the Morris water maze (WM) task [13]. Starting from one of 4 starting positions in the circular pool filled with an opaque liquid they had to learn th e location of a hidden escape platform using stable extra-maze cues (Fig. 2a). Animals were initia lly trained for 4 days with 4 sessions a day (to avoid confusion with 5HB, we consider each WM session c onsisting of only one trial). Trial length was limited to 60s, and the inter-session interval wa s 25 min.). Half of the mice had to swim in cold water of 19  X  C (motivational stress, MS), while the rest were learning at 26  X  C (control). After a 7-week break, 3-day long memory testing was done at 22 -23  X  C for all animals. Finally, after another 2 weeks, the mice performed the task for 5 more d ays: half of them did a version with uncertainty stress (US), where the platform location was ra ndomly varying between the old position and its rotationally opposite; the other half did the same ta sk as before.
 Behavior was quantified using the following 4 PMs: time to rea ch the goal (escape latency), time spent in the target platform quadrant, the opposite platfor m quadrant, and in the wall region (Fig. 2a). Figure 2: WM experiment and model. a. Experimental setup. 1  X  target platform quadrant, 2  X  opposite platform quadrant, 3  X  wall region. Small filled circles mark 4 starting positions , large filled circle marks the target platform, open circle marks th e opposite platform (used only in the US condition), pool  X  = 1 . 4 m . b. Activities of place cells (PC) encode position of the animal in the WM, activities of action cells encode direction of the next mo vement.
 A TDRL paradigm (1)-(3) in continuous state and action space s has been used to model the mouse behavior in the WM [14, 15]. The position of the animal is repre sented as a population activity of N pc = 211 modelled circular arena (Fig. 2b). Activity of place cell j is modelled by a Gaussian centered at the preferred location ~p where ~p is the current position of the modelled animal and  X  spatial receptive field relative to the pool radius. Place ce lls project to the population of N  X  X ction cells X  (AC) via feed-forward all-to-all connectio ns with modifiable weights. Each action cell is associated with angle  X  the level of place cells (i.e. state s depending on the value of the weight vector. The activity of a ction cell i is considered as the value of the action (defined as a movement in direction  X  The action selection follows -greedy policy, where the optimal action a  X  is chosen with probability  X  = 1  X  and a random action with probability 1  X   X  . Action a  X  is defined as movement in the direction of the center of mass  X   X  of the AC population 5 . Q-value corresponding to an action with continuous angle  X  is calculated as linear interpolation between activities o f the two closest action cells. During learning the PC  X  AC connection weights are updated on each time step in such a w ay as to decrease the reward prediction error  X  The Hebbian-like form of the update rule (9) is due to the fact that we use distributed representations for states and actions, i.e. there is no single state/action pair responsible for the last movement. To simulate one experimental session it is necessary to ( i ) initialize the weight matrix { w choose meta-parameter values and starting position ~p sponding movements until k ~p  X  ~p platform radius). Wall hits result in a small negative rewar d ( r For each session and each set of the meta-parameters, 48 diff erent sets of random initial weights w (corresponding to individual mice) were used to run the mode l, with 50 simulations started out of each set. Final values of the PMs were averaged over all repet itions for each subgroup of mice. To account for the loss of memory, after each day all weights w ere updated as follows: where  X  is the memory decay factor, w old the initial weight value before any learning took place. To compare the model with the experiment we used the followin g goodness-of-fit function [16]: where PM exp N with fixed values of the meta-parameters. PM exp or for each subgroup (WM). Using stochastic gradient ascent, we minimized (11) with respect to  X ,  X ,  X  for each session separately by systematically varying the m eta-parameters in the following otherwise constant values of  X  = 0 . 03 (5HB) and  X  = 0 . 2 (WM) were used.
 Several control procedures were performed to ensure that th e meta-parameter optimization was sta-tistically efficient and self-consistent. To evaluate how w ell the model fits the experimental data we used  X  2 -test with  X  = N distributed random variable would exceed  X  2 by chance, was calculated for each session separately. of the estimated meta-parameters we used the same optimizat ion procedure with PM exp generated by the model itself. In a self-consistent model su ch a procedure is expected to find meta-parameter values similar to those with which the PMs were gen erated. Finally, to see how well the model generalizes to previously unseen data, we used hal f of the available experimental data for optimization and tested the estimated parameters on the other half. Then we evaluated  X  2 and P (  X  2 ,  X  ) values for the testing as well as the training data. The meta-parameter estimation procedure was performed for the models of both experiments using stochastic gradient ascent in  X  2 goodness-of-fit. For the 5HB, meta-parameters were estimat ed for Figure 3: a. Example of PM evolution with learning in the WM (platform quad rant time, top) and in the 5HB (mean response time, bottom). b. Self-consistency check: true (open circles) and estimated (filled circles) meta-parameter values for the 24 random set s in the 5HB each animal and each experimental day. Further (sub)group v alues were calculated by averaging the individual estimations. For the WM, meta-parameters wer e estimated for each subgroup and each experimental session. Learning dynamics in both exper iments are illustrated in Figure 3a for 2 representative PMs, where average performances for all mi ce and the corresponding models (with estimated meta-parameters) are shown.
 The results of both meta-parameter estimation procedures i ndicated a reasonably good fit between satisfied for 92.5% of 5HB estimated parameter sets, and for 9 8.4% in the WM. The mean  X  2 values ( P (  X  2 , 3) = 0 . 15 ). There was a slight over-fitting only in the WM estimation.
 To evaluate the quality of the estimated optima and sensitiv ities to different meta-parameters, we calculated eigenvalues of the Hessian of 1 / X  2 around each of the estimated points. 98.4% of all eigenvalues were negative, and most of the corresponding ei genvectors were aligned with the direc-tions of  X  ,  X  , and  X  , indicating that there were no significant correlations in p arameter estimation. Furthermore, the absolute eigenvalues were highest in the d irections of  X  and  X  , thus the error sur-face is steep along these meta-parameters. To test the relia bility of estimated meta-parameters, the self-consistency check was performed using a number of rand om meta-parameter sets. The mean absolute errors (distances between real and estimated para meter values) were quite small for ex-estimated  X  values should be considered more reliable than those of  X  and  X  . 6.1 Meta-parameter dynamics During the course of learning, exploitation factors (  X  ) (Figure 4a,b) showed progressive increase (regression p 0 . 001 for both the 5HB and the WM), reaching the peak at the end of each learning block. They were consistently higher for the C57 mice than fo r the DBA mice (2-way ANOVA with replications, p 0 . 001 for both experiments), indicating that the DBA mice were exp loring the environment more actively, and/or were not able to focus the ir attention well on the specific task. Finally, C57 mouse groups, exposed to motivational stress i n the WM and to extrinsic stress in the 5HB, had elevated exploitation factors (ANOVA p &lt; 0 . 01 for both experiments), however there was no effect for the DBA mice.
 either 5HB or WM. There were no differences between the 2 genet ic strains (nor among the stress conditions) with one exception: for the first several days of the training, C57 learning rates were memory decay / interference factors for the first day after th e break in the 5HB. significantly higher (ANOVA p &lt; 0 . 01 in both experiments), indicating that C57 mice could learn a novel task more quickly.
 Under uncertainty (in reward delivery for the 5HB, and in the target platform location for the WM) future reward discount factors (  X  ) were significantly elevated (ANOVA p &lt; 0 . 02 , Figure 4c,d). In the 5HB, memory decay factors (  X  ), estimated for the first day after the break, were significan tly suggests that uncertainty makes animals consider rewards f urther into the future, and it seems to impair memory consolidation. In this paper we showed that various behavioral outcomes (ca used by genetic traits and/or stress factors) could be predicted by our TDRL models for 2 differen t tasks. This provides hypotheses concerning the neuromodulatory mechanisms, which we plan t o test using pharmacological manip-ulations (typically, injections of agonists or antagonist s of relevant neurotransmitter systems). errors) the acquired knowledge is used more for choosing act ions. This might also be related to decreased subjective stress and higher stressor controlla bility. The difference between C57 and DBA strains shows two things. Firstly, the anxious DBA mice cann ot exploit their knowledge as well as C57 can. Secondly, in response to motivational or extrinsic stress C57 mice are the only ones that increase their exploitation. This may be related to an inver se-U-shaped effect of the noradrenergic influences on focused attention and performance accuracy [1 7]. Animals with low anxiety (C57) might be on the left side of the curve, and additional stress m ight lead them to optimal performance, while those with high anxiety  X  already on the right side, lea ding to possibly impaired performance. Our results may also suggest that the widely proclaimed defic iency of DBA mice in spatial learning (as compared to C57) [4, 12] might be primarily due to differe ntial attentional capabilities. The increased future reward discount factors under uncerta inty indicate a reasonable adaptive re-sponse  X  animals should not concentrate their learning on im mediate events when task-reward rela-tions become ambiguous. Uncertainty in behaviorally relev ant outcomes under stress causes a de-crease in subjective stressor controllability, which is kn own to be related to elevated serotonin levels [18]. Higher memory decay / interference factors for the ani mals previously exposed to uncertainty could be due to partially impaired memory consolidation and /or due to stronger competition between different strategies and perceptions of the uncertain task .
 Although estimated meta-parameter values can be easily com pared between certain experimental conditions, it is difficult to study in this way the interacti ons between different genetic and environ-mental factors or extrapolate beyond the limits of availabl e conditions. One could overcome this disadvantage by developing a black-box parameter model tha t would help us to evaluate in a flexible way the contributions of specific factors (motivation, unce rtainty, genotype) to meta-parameter dy-namics, as well as their relationship with dynamics of TD err ors (  X  Acknowledgments This work was partially supported by a grant from the Swiss Na tional Science Foundation to C.S. (3100A0-108102).

