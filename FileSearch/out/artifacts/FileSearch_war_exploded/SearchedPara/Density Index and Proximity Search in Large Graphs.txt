 Given a large real-world graph where vertices are associated with labels, how do we quickly find interesting vertex sets according to a given query? In this paper, we study label-based proximity search in large graphs, which finds the top-k query-covering vertex sets with the smallest diameters. Each set has to cover all the labels in a query. Existing greedy al-gorithms only return approximate answers, and do not scale well to large graphs. We propose a novel framework, called gDensity , which uses density index and likelihood ranking to find vertex sets in an e ffi cient and accurate manner. Promis-ing vertices are ordered and examined according to their likelihood to produce answers, and the likelihood calcula-tion is greatly facilitated by density indexing. Techniques such as progressive search and partial indexing are further proposed. Experiments on real-world graphs show the e ffi -ciency and scalability of gDensity .
 G.2.2 [ Discrete Mathematics ]: Graph Theory X  graph al-gorithms Graph mining, Proximity search, Indexing
Graphs and networks can model various types of interac-tions in a myriad of applications [12, 15, 18]. They are used to encode complex relationships such as chemical bonds, en-tity relations, social interactions, and so on. In contempo -rary graphs, vertices and edges are often associated with attributes. While searching the graphs, what is interesting is not only the connectivity, but also the attributes, such as labels and weights. Figure 1 shows a graph where vertices contain numerical labels. Consider a succinct yet fundamen -tal graph query formula: given a set of labels, find vertex sets covering these labels and rank the sets by their connec-tivity. Viable connectivity measures include diameter, edge density, and minimum spanning tree. In Figure 1, if we want to find vertex sets that cover labels { 1 , 2 , 3 } ,andthediam-eter of a vertex set is its longest pairwise shortest-path, w e can return S 3 , S 1 and S 2 in ascending order of diameters.
Applications of such a setting abound. The vertex la-bels can represent movies recommended by a user, functions carried by a gene, skills owned by a professional, and so on. Such queries help solve various interesting problems in real-world graphs: (1) in a protein network where vertices are proteins and labels are their annotations, find a set of closely-connected proteins with certain annotations ;(2)in a collaboration network where vertices are experts and la-bels are their skills, find a well-connected expert team with required skills [18]; (3) in an intrusion network where ver-tices are computers and labels are intrusions they initiate , find a set of intrusions that happen closely together .The list of applications continues: finding a group of close frie nds with certain hobbies, finding a set of related movies covering certain genres, finding a group of well-connected customers interested in certain products, and many others.

We study label-based graph proximity search ,tofindthe top-k vertex sets with the smallest diameters, for a query containing distinct labels. Each set covers all the labels i n the query. The advantages of using diameter as a measure are shown in [16]. Graph proximity query is a general and simple way to query graphs. Lappas et al. [18] studied a similar problem called Diameter-Tf for expert team for-mation and adopted a greedy algorithm, RarestFirst ,to return a 2-approximate answer (the returned set has a di-ameter no greater than two times of the optimal diameter). Diameter-Tf is NP-hard [18]. In this paper, we propose ascalablesolutiontofindtop-k answers e ffi ciently in large graphs, for queries with moderate sizes. Our goals are: (1) finding the exact top-k answers, not approximate answers; (2) designing a novel graph index for fast query processing. Other similar studies include [16] and [8]. Kargar and An [16] studied finding the top-kr -cliques with smallest weights, where an r -clique is a set of vertices covering all the input keywords and the distance between each two is con-strained. Two algorithms are proposed: branch and bound and polynomial delay. The former is an exact algorithm, but it is slow and does not rank the answers; the latter ranks the answers, but is a 2-approximation. Gajewar and Sarma [8] studied the team formation problem with subgraph density as the objective to maximize and focused on approximation algorithms. The problem definition is di ff erent in our paper and we aim for exact and fast solutions.

A naive approach is to enumerate all query-covering ver-tex sets, linearly scan them and return the top-k with the smallest diameters. This is costly for large graphs. It is de -sirable to have a mechanism to identify the most promising graph regions, or local neighborhoods, and examine them first. If a neighborhood covers the query labels, and mean-while has high edge density, it tends to contain vertex sets with small diameters that cover the query. We propose a framework, called gDensity , to address the proximity search problem using this principle. Empirical studies on real-world graphs show that gDensity improves the query performance over competing methods.

Our contributions. (1) We introduce density index in graphs and the workflow to answer graph proximity queries using such index. (2) We demonstrate that if the neighbor-hoods are sorted and examined according to the likelihood, the search time can be reduced. (3) We propose partial indexing techniques to significantly reduce index size and index construction time, with negligible loss in query per-formance. (4) Empirical studies on real-world graphs show that gDensity is e ff ective and scalable.
For a vertex-attributed graph G =( V, E, L ), each vertex is attached with a set of labels from L = {  X  1 ,...,  X  l denotes the label set of vertex u .Fortheeaseofpresenta-tion, we focus on un-directed and un-weighted graphs. How-ever, the proposed framework can be extended to directed and weighted graphs as well.
 Definition 1 (Cover). Given a vertex-attributed graph G =( V, E, L ) ,avertexset S  X  V ,andaquery Q  X  L , S  X  X overs X  Q if Q  X  ! u  X  S L ( u ) . S is also called a query cov-ering vertex set. S is a minimal cover if S covers Q and no subset of S covers Q .

Definition 2 (Diameter). Given a graph G =( V, E ) and a vertex set S  X  V ,thediameterof S is the maximum of the pairwise shortest distances of all vertex pairs in S ,i.e., max u,v  X  S { dist ( u, v ) } ,where dist ( u, v ) is the shortest-path distance between u and v in G .
 The diameter of a vertex set S ,denotedby diameter ( S ), is di ff erent from the diameter of a subgraph induced by S , since the shortest path between two vertices in S might not completely lie in the subgraph induced by S .

Problem 1 (Graph Proximity Search). Given a ver-tex attributed graph G =( V, E, L ) and a query Q containing q labels ( | Q | = q ), label-based graph proximity search finds the top-k vertex sets { S 1 ,S 2 ,...,S k } with the smallest di-ameters. Each set S i is a minimal cover of Q .
 In many applications, it might not be useful to generate sets with large diameters, especially for graphs exhibiting the small-world property. One might apply a constraint such that diameter ( S i ) does not exceed a threshold.
RarestFirst is a greedy algorithm proposed by [18] that approximates the top-1 answer. First, RarestFirst finds the rarest label in query Q that is contained by the smallest number of vertices in G .Secondly,foreachvertex v with the rarest label, it finds its nearest neighbors that contain the remaining labels in Q .Let R v denote the maximum distance between v and these neighbors. Finally, it returns the vertex with the smallest R v ,anditsnearestneighbors containing the other labels in Q , as an approximate top set. RarestFirst yields a 2-approximation in terms of diameter, i.e., the diameter of the top set found by RarestFirst is no greater than two times that of the real top set.

RarestFirst can be very fast if all pairwise shortest dis-tances are pre-indexed. This is costly for large graphs. gDen-sity does not have such prerequisite. Besides, the goal of gDensity is finding the real top-k answers (not approximate answers). gDensity works well for queries with small-diameter answers, which are common in practice. For small graphs where all pairwise shortest distances can be pre-indexed, or for some di ffi cult graphs where optimal solutions are hard to derive, RarestFirst could be a better option. In Section 8, we implement a modified top-k version of RarestFirst us-ing the proposed progressive search technique, whose query performance is compared with gDensity .
The naive solution in Section 1 scales poorly to large graphs because: (1) It entails calculating all-pairs shortest distances. It takes O ( | V | 3 ) time using the Floyd-Warshall algorithm and O ( | V || E | ) using the Johnson X  X  algorithm. (2) It examines all query-covering sets without knowing their likelihood to be a top-k set. The time complexity is O ( | V | with q being the size of the query.

An alternative approach is to examine the local neighbor-hoods of promising vertices, and find high-quality top-k can-didates quickly. The search cost is the number of vertices examined times the average time to examine each vertex. It is important to prune unpromising vertices. A possible pruning strategy is: let d  X  be the maximum diameter of the current top-k candidates. d  X  decreases when new query cov-ers are found to update the top-k list. d  X  can be used to prune vertices which do not locally contain covers with di-ameter &lt;d  X  . gDensity instantiates such idea using nearest label pruning and progressive search, to quickly prune ver-tices which are unable to produce qualified covers. The key is to find vertices whose neighborhoods are likely to pro-duce covers with small diameters, so that the diameter of the discovered top-k candidates can be quickly reduced.
Definition 3 ( d -Neighborhood). Given a graph G = ( V, E, L ) and a vertex u in G ,the d -neighborhood of u , N ( u ) ,denotesthesetofverticesin G whose shortest dis-tance to u is no more than d ,i.e., { v | dist ( u, v )  X  d } .
Intuitively, the d -neighborhood of u , N d ( u ), can be re-garded as a sphere of radius d centered at u .Figure2shows an example of the 1-neighborhood, 2-neighborhood and 3-neighborhood of vertex u .Foreachvertex u in G ,wehave to determine if its d -neighborhood is likely to generate ver-tex sets with small diameters to cover the query. The key question is: how do we estimate such likelihood? Figure 3: Pairwise Distance Distribution Example
We propose density index to solve the likelihood estima-tion problem. Figure 3 shows the intuition behind density index. Assume there are two regions, i.e., the 3-neighborhoo ds of vertices u and v .Thedistributionsofpairwiseshortest distances in both regions are plotted in Figure 3. The hor-izontal axis is the pairwise distance, which are 1, 2, 3 and greater than 3. The vertical axis shows the percentage of vertex pairs with those distances. Given a query Q ,ifboth regions exhibit similar label distribution, which one has a higher chance to contain a query cover with smaller diame-ter? Very likely u  X  X  ! This is because there is a much higher percentage of vertex pairs in u  X  X  neighborhood that have smaller pairwise distances. Density index is built on this i n-tuition. For each vertex, the pairwise distance distribution in its local neighborhood is indexed o ffl ine, which will later be used to estimate its likelihood online. Section 4 describ es our indexing techniques in depth. gDensity consists of the following components.

Density Index Construction :Wecreateahistogram-like profile for each vertex depicting the distribution of th e pairwise shortest distances in its d -neighborhood, for 1  X  d  X  d I . d I is a user specified threshold.

Seed Vertex Selection : Instead of examining the en-tire vertex set V , we only examine the neighborhoods of the vertices containing the least frequent label in the query Q . These vertices are called seed vertices . Since a qualified ver-tex set must contain at least one seed vertex, we can solely focus on searching the neighborhoods of seed vertices.
Likelihood Ranking : Seed vertices are examined ac-cording to their likelihood to produce qualified vertex sets in their local neighborhoods. Vertices with the highest lik e-lihoods are examined first.

Progressive Search :Wemaintainabu ff er, B k ,ofthe top minimal query covers discovered so far. A sequential ex-amination finds qualified vertex sets with diameters 1 , 2 ,... , until the top-k bu ff er is full (contains k answers). This mechanism enables early termination of the search. Once the top-k bu ff er is full, the algorithm stops, because all of undiscovered vertex sets will have diameter at least as larg e as the maximum diameter in the top-k bu ff er.

Nearest Label Pruning :Let d be the current diameter used in progressive search. Once d is determined, gDensity traverses seed vertices to find query covers with diameter exactly as d .Thevalueof d increases from 1 and is used to prune seed vertices that are unable to generate qualified covers. Such seeds have their nearest neighbor with any query label further than d . gDensity Algorithm. Algorithm 1 shows the overall work flow of gDensity .Inthefollowingsections,weelaboratethe details regarding the above components.
 Algorithm 1: gDensity Framework
In order to estimate the likelihood online fast, we propose density indexing to pre-compute indices that reflect local edge connectivity. How to utilize the density index to facil-itate likelihood estimation is discussed in Section 4.2.
Density indexing records the pairwise shortest distance distribution of a local neighborhood. In order to main-tain a succinct index structure, the density index is solely based on topology. For each vertex u ,wefirstgrowits d -neighborhood, N d ( u ), using breadth-first search. The pair-wise shortest distances for all vertex pairs in N d ( u )arethen calculated. Some pairwise distances might be greater than d (at most 2 d ). Density index records the histogram of the discrete distance distribution, i.e., the percentage of pa irs whose distance is h ,for1  X  h  X  2 d ,asshowninFigure3. Density index only needs to record the distribution, not all -pairs shortest distances. Section 6 will discuss how to deri ve density index approximately.
Let I be an indicator function and P ( h | N d ( u )) be the percentage of vertex pairs with distance h .Wehave
Users can reduce the histogram size by combining the per-centage of pairs whose distance is greater than a certain threshold  X  h , as in Eq. (2). Usually  X  h = d .

Since the distribution can change with respect to the ra-dius of the neighborhood, we build the histograms for vary-ing d -neighborhoods of each vertex, with 1  X  d  X  d I ,where d
I is a user-specified indexing locality threshold. Figure 2 shows the neighborhoods of vertex u with di ff erent radii. For each radius d ,webuildahistogramsimilartoFigure3. Intuitively, if N d ( u )containsahigherpercentageofvertex pairs with small pairwise distances and it also covers Q , N ( u )shouldbegivenahigherpriorityduringsearch.This intuition leads to the development of likelihood ranking.
Supplementary indices are also used to facilitate likeliho od ranking and nearest label pruning (Section 5.2). (1) For each label  X  i in G ,globallabeldistributionindexrecordsthe number of vertices in G that contain label  X  i .(2)Inspired by the indexing scheme proposed by He et al. [12], gDensity further indexes, for each vertex in G ,itsclosestdistanceto each label within its d -neighborhood.

Since density index has histogram structure as in Figure 3, For index time, suppose the average vertex degree in G is b ,thenforeachvertex u , the expected size of its d -neighborhood is O ( b d ). If we use all pairwise distances within d  X  [1 ,d I ] to build the density index, the total time huge even for small d I .Thismotivatesustodesignpartial indexing (Section 6), which greatly reduces index time and size, while maintaining satisfying index quality.
Given a query Q = {  X  1 ,  X  2 ,...,  X  q } ,let  X  1  X  Q be the label contained by the smallest number of vertices in G .  X  1 is called the rarest or least frequent label in Q .Let V 1 = { v 1 ,v 2 ,...,v m } be the vertex set in G containing la-bel  X  1 .Theseverticesarereferredtoasthe seed vertices .Al-gorithm 1 shows that the d -neighborhoods of all seed vertices will be examined according to their likelihood to produce minimal query covers with diameter exactly as d ,while d is gradually relaxed. For each seed vertex v i ( i = { 1 ,...,m } ), its likelihood depends on the pairwise distance distributi on of its d -neighborhood, N d ( v i ). The likelihood reflects how densely the neighborhood is connected and can be computed from the density index.
Definition 4 (Distance Probability). Randomly se-lecting a pair of vertices in N d ( v i ) , let p ( v i ,d ) denote the probability for this pair X  X  distance to be no greater than d . p ( v i ,d ) can be obtained from the density index, P ( h | N
Definition 5 (Likelihood). Randomly selecting a ver-ability for this set X  X  diameter to be no greater than d .With density index (Eq. (1) ), " ( v i ,d ) can be estimated as
If the diameter of a vertex set is no greater than d ,allthe vertex pairs within this set must be at most d distance away from each other. If we assume independency of pairwise distances among vertex pairs, Eq. (4) can be obtained, given that the vertex set has size q .Certainly,itisanestimation, since pairwise distances should follow some constraints, s uch as triangle inequality in metric graphs. For a given query Q of size q , gDensity uses " ( v i ,d )asthelikelihoodtorank all the seed vertices. Apparently, seed vertices whose loca l neighborhoods exhibit dense edge connectivity tend to be ranked with higher priority. With the presence of density index, likelihood can be easily computed as in Eq. (4).
For all the seed vertices in V  X  1 ,wesortthemindescending order of " ( v i ,d ) and find minimal query covers with diame-ter d individually. For each seed vertex under examination, we first perform (unordered) cartesian product across query label support lists to get candidate query covers, and then select minimal covers from those covers. Such approach as-sures that all possible minimal query covers will be found from each seed vertex X  X  d -neighborhood.
For each seed vertex v i with label  X  1 ,wegenerateasup-port vertex list for each label in the query Q = {  X  1 ,  X  in v i  X  X  d -neighborhood. Let n j be the size of the support list for  X  j .Let  X  d ( v i ) denote the total number of possible query covers generated by performing a cartesian product across all label support lists, where each cover is an unordered ver -tex set consisting of one vertex from each support list.
Not all such covers are minimal. In Figure 4, if Q = { 1 , 2 , 3 } ,threesupportlistsaregeneratedin a  X  X  1-neighorhood. For example, label 1 has two vertices in its list, a and b . One of the covers across the lists is { a, b, c } ,whichisnot minimal. From { a, b, c } , we shall generate 3 minimal cov-ers, { a, b } , { b, c } and { a, c } . For each seed vertex, gDensity scans all candidate covers and generates those minimal ones to update the top-k list. Note that generating minimal cov-ers from the supporting lists is an NP-hard problem itself. Here we find the minimal covers in a brute-force manner. It Figure 5: Pruning and Progressive Search Example is a relatively a time-consuming process. However, with pro -gressive search, which will be described later, we only need to do this locally in a confined neighborhood. Experiment results will show that gDensity still achieves good empirical performance on large graphs.
Progressive search enables search to terminate once there are k answers found. Nearest label pruning is used together with progressive search to prune unpromising seed vertices. The search cost increases exponentially when d increases. Instead of testing a large value of d first, we propose to check neighborhoods with gradually relaxed radii. A top-k bu ff er, B k ,ismaintainedtostorethetopvertexsetswiththe smallest diameters found so far. We progressively examine the neighborhoods with d =1, d =2,andsoon,until B k is full. Such mechanism allows the search to terminate early. For example, if k answers are found while checking the 1-hop neighborhoods of all seed vertices, the process ca n be terminated without checking neighborhoods with d  X  2. In Figure 5, suppose the query is Q = { 1 , 2 , 3 } ,andwe have three seed vertices { u, v, w } .Startingwith d =1,we explore the 1-hop neighborhoods of all three, looking for covers with diameter 1, which gives us  X  { w, i } , 1  X  .Here,  X  { w, i } , 1  X  means the diameter of { w, i } is 1. Moving onto d = 2, we explore the 2-hop neighborhoods of all the three vertices (in dashed lines), seeking covers with diameter 2, k =4,searchprocesscanterminatehere.
We further propose a pruning strategy called nearest label pruning. Used together with progressive search, it is able t o prune unfavorable seeds from checking. Suppose the current diameter used in progressive search is d .Foreachseedvertex v ,wecalculateitsshortestdistancetoeachlabelin Q within its d -neighborhood, N d ( v i ). If there is a label  X   X  Q such that the shortest distance between a vertex with  X  and v i is greater than d , we skip checking v i and its neighborhood, since N d ( v i ) is not able to generate a query cover with di-ameter  X  d .Furthermore, v i and the edges emanating from it can be removed. For example in Figure 5, if Q = { 1 , 2 , 3 } and at certain point d = 2. Four query covers have been inserted into B k together with their diameters, which are longer need to check the neighborhood of vertex v .Thisis because the shortest distance between v and label 2 is 3, which is greater than the current diameter constraint d =2.
Building the complete density index for large graphs can be expensive. We propose partial indexing to build an ap-proximate index using partial neighborhood information.
Using random sampling, partial materialization allows den-sity index to be built approximately by accessing only a portion of the local neighborhoods. For each vertex u to in-dex: (1) only a subset of vertices in u  X  X  d -neighborhood are used to form an approximate neighborhood; (2) only a per-centage of vertex pairs are sampled from such approximate neighborhood to construct the partial density index. More specifically, the following steps are performed. (a) Given a vertex u and an indexing distance d ,asub-set of vertices are randomly sampled from N d ( u ). An ap-proximate d -neighborhood,  X  N d ( u ), consists of those sampled vertices and their distances to u . (b) Randomly pick a vertex v from  X  N d ( u ). (c) Get the intersection of  X  N d ( u )and  X  N d ( v ),  X  For a random vertex x in  X  d ( u, v ), sample the pair ( x, v ) and record their distance as in  X  N d ( v ). (d) For a random vertex x in  X  N d ( u )butnotin  X  d ( u, v ), sample the pair ( x, v )andrecordtheirdistanceas &gt;d . (e) Repeat Steps (b) to (d) until a certain percentage, p , of vertex pairs are sampled from N d ( u ). (f) Draw the pairwise distance distribution using sampled pairs to approximate the real density distribution in N d
Figure 6 (better viewed in color) shows an example. The solid circles centered at vertices u and v are their actual 2-neighborhoods. The white free-shaped region surrounding u is its approximate 2-neighborhood,  X  N 2 ( u ); similarly, the gray free-shaped region surrounding v is  X  N 2 ( v ). The region with grid pattern circumscribed by a solid red line is the intersection of both approximate neighborhoods,  X  2 ( u, v ). Each sampled vertex x from u  X  X  approximate 2-neighborhood forms a pair with v ,( x, v ). If x is in the intersection,  X  ( u, v ), the pair ( x, v )issampledwithapairwisedistance recorded as in  X  N d ( v ); otherwise it is sampled with a pairwise distance recorded as &gt;d .AlocalizedversionofMetropolis-Hastings random walk (MHRW) sampling [6, 9] is used to sample vertices from N d ( u )(Step(a)).
Partial materialization reduces the indexing cost for an individual vertex. To further reduce the indexing cost, we can reduce the number of vertices to be indexed. The intu-ition is: if two vertices u and v have similar local topological structure, there is no need to build the density index for u and v separately, given that the distance distributions in the neighborhoods of u and v are similar. For example, in Fig-ure 7, the 1-hop neighborhoods of vertices u and v overlap each other to a great extent. The common adjacent neigh-bors of u and v in Figure 7 are { a, b, c, d } ,whichis66 . 7% of u and v  X  X  1-neighborhoods. Can we build the density index of v with the aid of the density index of u ?
Asimplestrategyemployedin gDensity is to use the den-sity of u to represent that of v (or vice versa), if the per-centage of common 1-hop neighbors of u and v exceeds a certain threshold in both u and v  X  X  neighborhoods. Let  X  denote such threshold. In this case, vertex u is considered as the representative vertex of v .Weonlyindexthosevertices which are representatives of some others, and use their den-sity index to represent others X . Such strategy quickly cuts down the number of vertices to index, thus reduces the index time and index size. As experimented in Section 8,  X   X  30% would su ffi ce to produce e ff ective partial index, which still yields good online query processing performance.
Theorem 1 (Optimality of gDensity ). For a query, gDensity finds the optimal top-k answers. Partial indexing and likelihood ranking a ff ect the speed of query processing, but not the optimality of the results.

Proof Sketch. Since seed vertices contain the least fre-quent label in the query, all query covers contain at least one seed vertex. Confining the search to the neighborhoods of seed vertices does not leave out any answers. Progres-sive search assures that the diameters of unexamined vertex sets will be no less than the maximum diameter in the top-k bu ff er. Therefore the final top-k answers returned will have the smallest diameters. Indexing and likelihood rank-ing identify  X  X romising X  seed vertices and guide the algo-rithm to discover the top-k answers faster .Ifmorepromis-ing seeds are ranked higher, the top-k bu ff er will be filled up faster. It is possible for a seed vertex, whose neighborhood contains good answers, to be ranked lower than other less promising seeds. However, this would only a ff ect the speed of filling up the top-k bu ff er. It would not change the fact that the top-k bu ff er contains the top-k smallest diameters. Partial indexing further reduces the indexing cost by index-ing only partial information. It approximates the indexing phase, and will not a ff ect the optimality of the query phase. Therefore gDensity always returns the optimal answers.
The likelihood computed in Eq. (4) for ranking seed ver-tices assumes independence among distances between ver-tices, which might not be valid for some seeds due to pos-sible skewed distribution. However, as long as it is valid for some seed vertices, the top-k bu ff er can be quickly up-dated with answers discovered surrounding those seeds, thu s speeding up the search. The goal of likelihood ranking is to locate promising regions containing many potential answer s and fill up the top-k bu ff er quickly. Section 8.2.3 empirically confirms the e ff ectiveness of likelihood ranking.
We reiterate that partial indexing only a ff ects the esti-mated density and likelihood ranking. Only the speed of the top-k search will be a ff ected by partial indexing. Partial indexing will not impair the optimality of gDensity in terms of returning the top-k answers with the smallest diameters.
The empirical evaluation contains: (1) comparison be-tween gDensity and the modified RarestFirst ;(2)evalu-ation of partial indexing; (3) scalability test of gDensity .All experiments are run on a machine that has a 2.5GHz Intel Xeon processor (only one core is used), 32G RAM, and runs 64-bit Fedora 8 with LEDA 6.0 [22].
DBLP Data. DBLP is a collaboration network in com-puter science. Each vertex is an author and each edge is a collaborative relation. We consider the keywords in the paper titles of an author as vertex labels. The DBLP graph used contains 387,547 vertices and 1,443,873 edges.
Intrusion Data. In this graph, each vertex is a com-puter and each edge is an attack. A vertex has a set of la-bels, which are intrusion alerts initiated by this computer . There are 1035 distinct alerts. Intrusion alerts are logged periodically. We use one daily data set ( IntruDaily )with 5,689 vertices and 6,505 edges, and one annual data set ( In-truAnn )with486,415verticesand1,666,184edges.

WebGraph Data. Web graph ( http://webgraph.dsi. unimi.it/ )isacollectionofUKwebsites.Eachvertexisa web page and each edge is a link. A routine is provided to attach the graph with random integer labels following Zipf distribution [20]. Five subgraphs are used, whose vertex numbers are 2M, 4M, 6M, 8M and 10M, and whose edge numbers are 9M, 16M, 23M, 29M and 34M. The 2M graph is a subgraph of the 4M graph, and so on. 50 queries are generated for each graph used. Query time is averaged over all the queries. Table 1 shows some query examples. Indexing is conducted up to 3 hops for all the graphs. If not otherwise specified, partial indexing is the de-fault indexing. The vertex pair sampling percentage is 40% and the 1-hop neighborhood similarity threshold in repre-sentative vertex selection is  X  =30%. We discovered in our experiments that the original Rarest-First method does not scale well to large graphs. Thus we add a constraint D on the diameters of the top-k vertex sets in RarestFirst ,limitingthesearchtoeachseed X  X  D -neighborhood. We further use progressive search to speed up RarestFirst .Algorithm2outlinesthecustomizedtop-k RarestFirst .Anotherbaselinemethodisavariantof gDensity ,called X  gDensity w/o LR X , which removes likelihood ranking from gDensity .Alloftheothercomponentsarestill kept in gDensity w/o LR. gDensity w/o LR examines the seed vertices in a random order. The goal is to inspect the Algorithm 2: RarestFirst With Progressive Search actual e ff ect of likelihood ranking. Both methods are used for comparative study against gDensity .
The comparison is done on two measures, query time (in seconds) and answer miss ratio (in percentage). Rarest-First could miss some real top-k answers since it is an ap-proximate solution. Miss ratio is the percentage of real top-k answers RarestFirst fails to discover. For example, if the real top-5 all have diameter 2 and if 2 of the top-5 answers returned by RarestFirst have diameter greater than 2, the miss ratio is 2 / 5=40%. gDensity and gDensity w/o LR are able to find all real top-k answers.

We also examine the impact of label distribution. There is an interesting tradeo ff here. If labels are densely dis-tributed (the average number of vertices containing each la -bel is high), each neighborhood might potentially contain many answers and the algorithm stops early; if the labels are sparsely distributed, the seed vertex list is shorter an d the candidate set for each seed is smaller. We thus design a group of experiments where we synthetically regenerate la-bels for graphs DBLP , IntruAnn and WebGraph 10M, under certain label ratios. The ratio is measured as | L | / | V | ,where | L | is the total number of distinct labels in G .Eachvertex is randomly assigned one of those synthetic labels.
Figure 8 shows the query time comparison of gDensity , gDensity w/o LR and the modified RarestFirst .Theleft-most column shows how the average query time changes with k .Theadvantageof gDensity over the modified Rarest-First is apparent. The e ff ectiveness of likelihood ranking is evident on DBLP and IntruAnn ,where gDensity greatly outperforms gDensity w/o LR. Likelihood ranking does not work as well on WebGraph 10M. It is possible that Web-Graph 10M does not contain many patterns or dense regions, rendering it di ffi cult to rank seed vertices e ff ectively.
The remaining columns depict how the average query time changes with the synthetic label ratio, | L | / | V | .Thetradeo ff between dense (small label ratio) and sparse (large label ratio) label distribution clearly shows on DBLP ,wherethe gDensity query time first goes up and then goes down. It goes up because as label distribution becomes sparse, more seeds and larger values of d need to be examined to find the top-k ,sinceeachregioncontainslessanswers. Itthen goes down because the seed vertex list gets shorter and the set of candidate covers to check for each seed gets smaller. RarestFirst sometimes outperforms gDensity because the diameter constraint lets RarestFirst finish without finding all the optimal top-k sets. In the next section, we will show the percentage of answers missed by RarestFirst .
Query optimality is measured by the answer miss ratio. gDensity discovers the real top-k answers, thus the miss ra-tio of gDensity and gDensity w/o LR will be 0. Figure 10 shows how the miss ratio of RarestFirst changes with k . Miss ratio gradually increases with k .Intheworstcase, RarestFirst misses 52 . 8% of the real top-k answers. On average, the miss ratio is around 30%. Clearly, compared to gDensity , RarestFirst su ff ers from the drawback of possibly leaving out the optimal answers.

We also observe how miss ratio changes with the synthetic label ratio, | L | / | V | ,inFigure9. RarestFirst again is disad-vantageous in terms of top-k optimality. The average miss ratios are 46 . 6%, 23 . 6% and 35 . 1% for synthetically labeled DBLP , IntruAnn and WebGraph 10M, respectively.
Partial indexing reduces cost by indexing a subset of ver-tices using their approximate neighborhoods. We refer to the alternative, indexing all vertices using their exact neig h-borhoods, as  X  X ll indexing X . The query performances of both indexing techniques are compared. Three thresholds of representative vertex selection are used for partial ind ex-ing:  X  = { 10% , 30% , 50% } . Since all indexing is very time-consuming for large graphs, we can only a ff ord to conduct the comparison on a small graph, IntruDaily .
As shown in Figure 11, the additional online query time partial indexing induces over all indexing is almost negligi-ble, especially when  X   X  30%. The moderate performance margin between  X  =10%and  X   X  30% might be attributed to the fact that there are not that many vertex pairs whose neighborhood similarity falls between 10% and 30%.
Table 2 shows the indexing time (seconds) and index size (MB) comparison. Partial indexing e ff ectively reduces in-dexing cost. The indexing cost increases with the represen-tative vertex threshold, because a higher threshold means a larger number of representative vertices to index.
Figure 11: gDensity Partial vs. All Index: Query Time
We conduct experiments on web graphs of increasing size to show the scalability of gDensity .Table3showshowthe index time and size change when the graph increases from 2M to 10M vertices. Overall, the index time is satisfying and reasonable. The largest graph only takes 3.9 hours to index. The index size is no more than 30% of the graph size. Most importantly, the index time and size are approximately linear to the size of the graph. Therefore, partial indexing exhibits satisfying scalability over large graphs. Table 3: gDensity Scalability Test: Index Time &amp; Size
Figure 12 shows how query time changes when the graph size increases from 2M to 10M vertices, for various k values. We can see that the query time gradually increases and is still satisfying even for very large graphs. The query time is contingent on a number of factors such as the graph struc-ture, the graph size, the label distribution, the query, k ,and so on. Since the queries are randomly generated on each of the web graphs, it is possible for a smaller graph to en-counter a more di ffi cult query that entails longer processing time. Even for the same query, it is possible for it to be processed faster in a larger graph, since more answers might be found at an early stage. If we compare the runtime of top-5 for WebGraph 2M, top-10 for WebGraph 4M, top-15 for WebGraph 6M, and top-20 for WebGraph 8M (i.e., the value of k increases with the graph size), it is observed that the runtime increases linearly. Overall, gDensity is scalable with respect to the graph size.
Proximity Search. Typical proximity search in social networks includes link prediction [24], expert team forma-tion [2, 8, 18, 26, 27]. The latter finds a team of experts with required skills. Existing methods include generic algo-rithms [26], simulated annealing [2], and so on. [18] adopts a 2-approximation algorithm to find a team of experts with the smallest diameter, where all-pairs shortest distances nee d to be pre-computed and no index structure is used to ex-pedite the search. [8] presents approximation algorithms to find teams with the highest edge density. Proximity search is also studied in Euclidean space [1, 11], such as finding the smallest circle enclosing k points. Since the diameter of such a circle is not equal to the maximum pairwise distance between the k points, even with mapping methods such as Figure 12: gDensity Scalability Test: Query Time ISOMAP [25], the techniques for the k -enclosing circle prob-lem can not be directly applied here. The points here also do not contain label information.

Motif Finding in Graphs. Label-based graph proxim-ity search is also related to the graph motif problem, intro-duced by [17] in the bioinformatics field. [7] further invest i-gates three variants of the initial problem. A key di ff erence is that diameter is not used to rank the discovered motifs.
Ranked Keyword Search in Graphs. Ranked key-word search in graphs [3, 12, 15] has focused on answers structured as rooted-trees. An answer is ranked mainly by the aggregate distances from the leaves to the root [12]. The distance among leaves is not considered. gDensity ranks an answer by its entire diameter. Finding subgraphs instead of trees is also studied in [16, 19]. Finding r -cliques that cover all the keywords is proposed in [16], which only finds answers with 2-approximation. [19] finds r -radius Steiner graphs that cover all the keywords.. [10] uses personalized PageRank vectors to find answers in the vicinity of vertices matching the query keywords in entity-relation graphs. [13] proposes XKeyword for e ffi cient keyword proximity search in large XML graph databases. gDensity di ff ers since it does not require a schema on the graphs. gDensity focuses on the diameter of a vertex set. and ensures optimality.
Top-k Query Processing. Top-k query processing is also studied for RDBMS [4, 10, 23] and middleware [5, 14, 21]. Supporting top-k queries in SQL is proposed in [4]. In middleware, top-k query is abstracted as getting objects with the top-k aggregate ranks from multiple data sources. Our work is di ff erent since gDensity answers top-k queries on a single source with graph data. Existing techniques for RDBMS and middleware are no longer applicable.
In this paper, we study the label-based graph proximity search problem, which finds the top-k query-covering ver-tex sets with the smallest diameters. The proposed gDen-sity framework introduces likelihood ranking of seed vertices to speed up the search. Fast pruning and early stopping are enabled by nearest label pruning and progressive search . Density indexing is proposed for fast likelihood estimation. Partial indexing is further proposed to reduce indexing cost. Empirical studies show the e ffi ciency and scalability of gDen-sity . Our future works include: (1) extending gDensity to weighted and directed graphs; and (3) extending gDensity to handle other objective functions besides diameter. This research was sponsored in part by the U.S. National Science Foundation under grant IIS-0905084 and the Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053 (NS-CTA). The views and conclusions in this document are those of the authors and should not be interpreted as representing the o ffi cial policies, either ex-pressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. [1] A. Aggarwal, H. Imai, N. Katoh, and S. Suri. Finding [2] A. Baykasoglu, T. Dereli, and S. Das. Project team [3] G. Bhalotia, A. Hulgeri, C. Nakhe, S. Chakrabarti, [4] M. Carey and D. Kossmann. On saying  X  X nough [5] K. Chang and S. Hwang. Minimal probing: supporting [6] S. Chib and E. Greenberg. Understanding the [7] R. Dondi, G. Fertin, and S. Vialette. Finding [8] A. Gajewar and A. D. Sarma. Multi-skill collaborative [9] M. Gjoka, M. Kurant, C. T. Butts, and [10] M. Gupta, A. Pathak, and S. Chakrabarti. Fast [11] S. Har-Peled and S. Mazumdar. Fast algorithms for [12] H. He, H. Wang, J. Yang, and P. Yu. BLINKS : [13] V. Hristidis, Y. Papakonstantinou, and A. Balmin. [14] I. Ilyas, G. Beskales, and M. Soliman. A survey of [15] V. Kacholia, S. Pandit, S. Chakrabarti, S. Sudarshan, [16] M. Kargar and A. An. Keyword search in graphs: [17] V. Lacroix, C. G. Fernandes, and M.-F. Sagot. Motif [18] T. Lappas, K. Liu, and E. Terzi. Finding a team of [19] G. Li, B. C. Ooi, J. Feng, J. Wang, and L. Zhou. Ease: [20] W. Li. Random texts exhibit Z ipf X  X -law-like word [21] A. Marian, N. Bruno, and L. Gravano. Evaluating [22] K. Mehlhorn and S. N  X  aher. LEDA: a platform for [23] L. Qin, J. X. Yu, L. Chang, and Y. Tao. Querying [24] P. Sarkar, A. W. Moore, and A. Prakash. Fast [25] J. Tenenbaum, V. Silva, and J. Langford. A global [26] H. Wi, S. Oh, J. Mun, and M. Jung. A team formation [27] A. Zzkarian and A. Kusiak. Forming teams: an
