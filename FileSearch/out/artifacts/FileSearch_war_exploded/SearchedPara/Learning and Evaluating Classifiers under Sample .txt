 Bianca Zadrozny zadrozny@us.ibm.com IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 One of the most common assumptions in the design of learning algorithms is that the training data con-sist of examples drawn independently from the same underlying distribution as the examples about which the model is expected to make predictions. In many real-world applications, however, this assumption is vi-olated because we do not have complete control over the data gathering process.
 For example, suppose we are using a learning method to induce a model that predicts the side-effects of a treatment for a given patien t. Because the treatment is not given randomly to individuals in the general population, the available examples are not a random sample from the population. Similarly, suppose we are learning a model to predict the presence/absence of an animal species given the characteristics of a geograph-ical location. Since data gathering is easier in certain regions than others, we would expect to have more data about certain regions than others.
 In both cases, even though the available examples are not a random sample from the true underlying distri-bution of examples, we would like to learn a predictor from the examples that is as accurate as possible for this distribution. Furthermore, we would like to be able to estimate its accuracy for the whole population using the available data.
 This problem has received a great deal of attention in econometrics, where it is ca lled sample selection bias. There it appears mostly b ecause data are collected through surveys. Very often people that respond to a survey are self-selected, so they do not constitute a random sample of the general population. In Nobel-prize winning work, Heckman (1979) has developed a procedure for correcting s ample selection bias. The key insight in Heckman X  X  work is that if we can es-timate the probability that an observation is selected into the sample, we can use this probability estimate to correct the model. The drawback of his procedure is that it is only applicable to linear regression models, commonly used in econometrics.
 Also, in statistics, the related problem of missing data has been considered (Little &amp; Rubin, 2002). However, they are generally concerned with cases in which some of the features of an example are missing, and not with cases in which whole examples are missing.
 In this paper, we address the sample selection bias problem in the context of learning and evaluating clas-sifiers. In Section 2 we formally define the sample se-lection bias problem in machine learning terms. In Section 3 we present a new cat egorization of learning methods that is useful for characterizing their behavior under sample selection bias and study how a number of well-known classifier learning methods are affected by sample selection bias. In Section 4, we present a bias correction method bas ed on estimating the prob-ability that an example is selected into the sample and using rejection sampling to obtain unbiased samples of the correct distribution. It can be used both for learn-ing classifiers and, more importantly, for evaluating a classifier using a biased sample. Standard classifier learning algorithms (implicitly or explicitly) assume that we have examples ( x, y ), each drawn independently from a distribution D with do-main X X Y where X is the feature space and Y is a (discrete) label space.
 Here, we assume that examples ( x, y, s )aredrawn independently from a distribution D with domain X X Y X S where X is the feature space, Y is the label space and S is a binary space. The variable s controls the selection of e xamples (1 means the exam-ple is selected, 0 means the example is not selected). We only have access to the examples that have s =1, which we call the selected sample. If the selected sam-ple (ignoring s ) is not a random sample of D we say that the selected sample is biased.
 There are four cases worth co nsidering regarding the dependence of s on the example ( x, y ) 1 : 1. If s is independent of x and independent of y ,the 2. If s is independent of y given x (that is P ( s | x, y 3. If s is independent of x given y (that is P ( s | x, y 4. If no independence assumption holds between x , y In econometrics, the usual assumption is (4) because the goal is to estimate the parameters of a model for y that reflects the true dependence of y on x .Any feature variable that only affects the selection should not be included in x (and it is included in x s , instead). In classifier learning, this is not a concern, because we are mostly interested in the predictive performance of the model and not in making conclusions about the underlying mechanisms that generate the data. For this reason, we argue that the most important sample selection bias case in the practice of classi-fier learning is case (2). In order to make the con-dition P ( s | x, y )= P ( s | x ) true in practice, the input to the classifier x has to include all the variables that affect the sample selection. For example, in the med-ical treatment case, we need to include in x the vari-ables about the patients that the doctors use to decide who gets the treatment (even if they do not affect the side-effects of the tr eatment directly).
 Even if this assumption is not true in practice (either because we do not have access to all the variables that control the selection or because it truly depends di-rectly on y ), assuming case (2) is more realistic than the usual assumption of case (1). In the rest of this paper, sample selection bias will refer to case (2). We can separate classifier lea rners into two categories:  X  local: the output of the learner depends asymp- X  global: the output of the learner depends asymp-The term  X  X symptotically X  refers to the behavior of the learner as the number of training examples grows. The names  X  X ocal X  and  X  X lobal X  were chosen because P ( x ) is a global distribution over the entire input space, while P ( y | x ) refers to many local distributions, one for each value of x . Local learners are not af-fected by sample selectio n bias because, by definition P ( y | x, s =1)= P ( y | x ) while global learners are af-fected because the bias changes P ( x ).
 Although this categorization is very simple, it is not straightforward to classify existing learners into it. Be-low, we study analytically and experimentally how sample selection bias affect s different types of classi-fiers learning methods, including Bayesian classifiers, logistic regression, SVM and decision trees. 3.1. Bayesian classifiers Bayesian classifiers compute posterior probabilities P ( y | x ) using Bayes X  rule: where P ( x | y ), P ( y )and P ( x ) are estimated from the training data. An example x is classified by choosing the label y with the highest posterior P ( y | x ). We can easily show that bayesian classifiers are not affected by sample selection bias. By using the biased sample as training data, we are effectively estimating P ( x | y, s =1), P ( x | s =1)and P ( y | s = 1) instead of estimating P ( x | y ), P ( y )and P ( x ). However, when we substitute these estimates into the equation above and apply Bayes X  rule again, we see that we still obtain the desired posterior probability P ( y | x ): P ( x | y, s =1) P ( y | s =1) since we are assuming that y and s are independent given x . Note that even though the estimates of P ( x | y, s =1), P ( x | s =1)and P ( y | s = 1) are different from the estimates of P ( x | y ), P ( x )and P ( y ), the dif-ferences cancel out. Therefore, bayesian learners are local learners.
 In practice, we have a limited amount of examples to estimate P ( y | x ). Compared to a random sample of the same size, the biased sample contains more examples in parts of the feature space where P ( s =1 | x )ishigh and less examples where P ( s =1 | x )islow. Thiswill lead to estimates of P ( y | x ) with lower variance where P ( s =1 | x ) is high and with higher variance where P ( s =1 | x ) is low. However, as long as P ( s =1 | x )is greater than zero for all x , as we increase the sample size, the results on a select ed sample will asymptoti-cally approach the results on a random sample. 3.1.1. Naive Bayes In practical Bayesian learning, we often make the as-sumption that the features are independent given the label y , that is, we assume that This is the so-called naive Bayes assumption. With naive Bayes, unfortunately, the estimates of P ( y | x ) obtained from the biased sample are incorrect. The posterior probability P ( y | x ) is estimated as which is different (even asymptotically) from the es-timate of P ( y | x ) obtained with naive Bayes without sample selection bias. We cannot simplify this further because there are no independence relationships be-tween each x i , y and s . Therefore, naive Bayes learners are global learners. 3.2. Logistic regression In logistic regression, we use maximum likelihood to find the parameter vector  X  of the following model: With sample selection bias we will instead fit: However, because we are assuming that y is indepen-dent of s given x we have that P ( y =1 | x, s =1)= P ( y =1 | x ). Thus, logistic regression is not affected by sample selection bias, excep t for the fact that the num-ber of examples is reduced. Asymptotically, as long as P ( s =1 | x ) is greater than zero for all x ,theresults on a selected sample approach the results on a random sample. In fact, this is true for any learner that models P ( y | x ) directly. These are a ll local learners. Figure 1 illustrates the effect of sample selection bias on logistic regression for synthetically generated data, where x is one-dimensional. The graph on the left shows 1000 points where the x value is chosen uni-formly between -10 and 10 and the y value is drawn with probabilities calculated using a logistic function (  X  0 =3 and  X  1 =2). The curve is the logistic function obtained using the plotted points. The dashed line is the separator between the two classes. The graph on the right shows a selected sample of the points, where the probability of each point being selected is proportional to its x value. We also show the logistic function obtained using the selected points. Although the selected sample contains many less points on the negative side than the original sample, the estimated curve and the resulting separator are the same. 3.3. Decision tree learners Decision tree learners such as C4.5 (Quinlan, 1993) and CART (Breiman et al., 1984) split the i nput space x in a recursive, top-down manner. Each branch of the tree is a test on the value of one the features. For discrete features, the tree branches into nodes corre-sponding to each of the possible values. For real-valued features, the tree branches into two nodes correspond-ing to some threshold. To predict the class of a new example, we work down the tree, at each node choos-ing the appropriate branch by comparing the example with the values of the variable being tested for that node (Hand et al., 2001). The splitting criteria used by different decision tree learners vary, but they are all based on the nodes X  impurity after the split. For example, CART uses the GINI index where p ( y | t ) is the relative frequency of class y at node t . GINI is maximal when the examples are equally dis-tributed among the classes and minimal when all the examples belong to one class. For each possible split, ber of records at the node, n i is the number of records at child i and k is the number of children induced. C4.5 uses an information gain criterion given by where P ( y | t ) is the relative frequency of class y at node t . Like GINI, INFO is maximal when the examples are equally distributed among the classes and minimal when all the examples belong to one class.
 Because the splitting cr iteria are dependent on P ( y | t where t is a test on only one of the feature values, and, in general, P ( y | t, s =1) = P ( y | t ), the splits chosen by the learners are sensitiv e to sample selection bias. Thus, decision tree learne rs are global learners. 3.4. Support vector machines In its basic form, the support vector machine (SVM) algorithm (Joachims, 2000a) learns the parameters a and b describing a linear decision rule whose sign determines the label of an example, so that the smallest distance between each training example and the decision boundary, i.e. the margin, is max-imized. Given a sample of examples ( x i ,y i ), where y  X  X  X  1 , 1 } , it accomplishes margin maximization by solving the following optimization problem: The constraint requires that all examples in the train-ing set are classified correctly. Thus, sample selection bias will not systematically affect the output of this optimization, assuming that the selection probability P ( s =1 | x ) is greater than zero for all x .
 Figure 2 illustrates the effect of sample selection bias on SVM for synthetically generated data, where x is two-dimensional. The graph on the left-hand side shows 500 points for each of two classes, generated from two different gaussians. The line is the maxi-mal margin separator. The graph on the right-hand side shows a selected sample from these points where the probability of each point being selected is propor-tional to its horizontal coordinate. We also show max-imal marginal separator using the selected points. Al-though the selected sample contains many less points on the negative side than the original sample, the re-sulting separator is not significantly altered. In practice, a decision rule that classifies all the ex-amples correctly may not ex ist because of class over-lap. To allow for misclassified examples, one intro-duces slack variables  X  i &gt; 0 for each example ( x i ,y This is called a soft margin SVM classifier (Sch  X  olkopf &amp; Smola, 2002). The optimization is changed to If a training example lies on the wrong side of the de-cision boundary, the corresponding  X  i is greater than 1. Therefore, n i =1  X  i is an upper bound on the num-ber of training errors. The factor C is a parameter that allows one to trade off training error and model complexity. We note that the algorithm can be gen-eralized to non-lin ear decision rules by replacing inner products with a kernel function (Joachims, 2000a). While sample selection bias does not affect the hard margin SVM, it does affect the soft margin version because it considers the sum of  X  i values. By making regions of the feature space denser than others, sample selection bias changes this sum and, with it, the deci-sion boundary. Soft margin SVM is a global algorithm because changes in P ( x ) will change the output. 3.5. Experimental results To verify the effects of sample selection bias exper-imentally, we apply Naive Bayes, logistic regression, C4.5 and SVMLight (soft margin) (Joachims, 2000b) to the Adult dataset, available from the UCI Machine Learning repository (Blake &amp; Merz, 1998). We assume that the original dataset is not biased and artificially simulate biasedness by generating a value for s for each example, such that s is correlated with one of the input features. When training, we only use the examples in the training set for which s = 1. When testing, we use all the examples in the test set, because we are inter-ested in measuring the per formance of the classifiers on the original distribution of examples.
 Figure 3 shows the results of applying the learners to the Adult dataset using unbiased and biased training sets of increasing size. For each size shown on the x -axis, we generated 50 unbiased samples from the orig-inal Adult training set. W e also generated 50 biased samples by assigning s such that examples with feature age less than 30 are 9 times more likely to have s =1 than examples with age more than 30. We trained the learners using each of the 50 samples (in both the biased and unbiased cases) and tested the models on the Adult test set, to obtain the mean and standard error of the error rate, as shown in the graphs. In accordance with our analysis, for logistic regression, the difference in error rate between using a biased or an unbiased sample goes down as we increase the size of the training set. Also, as expected, we see that naive Bayes is very sensitive to sample selection bias. The error rate using the biased sample goes up as we increase the number of training examples.
 Surprisingly, C4.5 performs very well under sample se-lection bias. This might be explained by the fact that even though the choice of splits is biased, the class es-timates at the leaves are not. More research specific to decision tree learners is necessary to understand the effect of sample selection bias on them.
 With SVM, we see that the error rate using the bi-ased training set decreases as the training set sizes increases. However, the difference between the er-ror rates using biased and unbiased samples does not decrease. This indicates that, asymptotically, SVM (with soft margin) is affected by sample selection bias. In the last section, we saw that some classifier learning methods are affected by sample selection bias, while others are not. In this section, we present a bias cor-rection method that can be applied to any classifier learner, provided that we have a model for the se-lection probabilities P ( s =1 | x ). The method works by correcting the distribution of examples through re-sampling and then applying the classifier learner to the corrected sample. It bears resemblance to weighting methods proposed in the statistics literature for miss-ing data (Little &amp; Rubin, 2002) and also to costing, a cost-sensitive learning method by example weighting presented in Zadrozny et al. (2003).
 Classifier learners try to find h to minimize the ex-pected value of loss function over the distribution of examples given by The loss function is, in many cases, given by an indi-cator of error I ( h ( x ) = y ), but we make the analysis more general by considering an arbitrary loss function. Under sample selection bias, a classifier learner will minimize instead because only the examples with s = 1 are available. Assume that we know the selection probabilities P ( s = 1 | x ) and that they are greater than zero for all x .Let  X  D be a new distribution such that where P ( s =1)= ( x,y,s )  X  D P ( s =1 ,x )istheoverall selection probability.
 The following theorem shows that if we change the distribution of examples from D to  X  D , we will obtain the desired expected value under sample selection bias. Theorem 1 (Bias Correction Theorem) For all dis-tributions, D , for all classifiers, h , for any loss func-tion l = l ( h ( x ) ,y ) ,ifweassumethat P ( s =1 | x, y P ( s =1 | x ) (that is, s and y are independent given x ) then Proof: value that we would like to minimize but cannot di-rectly under sample selection bias. The right-hand side (
E as we can draw examples from  X  D .
 As discussed in Zadrozny et al. (2003), obtaining a sample from a weighted distribution given a finite set of training examples is not completely straightfor-ward. They have demonstrated that costing, a method based on rejection sampling, achieves the best results in practice. For this reason, we recommend using cost-ing for sample selection bias correction, where instead of using costs as weights we use the selection ratio P ( s =1) /P ( s =1 | x ) as a weight for each example. Up to now, we have assumed that we know the selec-tion probabilities P ( s =1 | x ). In practice, we would have to estimate these from data. If we have a sample ( x, s )  X  D (note that y is not necessary), we can use it to estimate these probabilities by feeding the sample to a classifier learner that outputs class membership probability estimates (using s as the label). Obtain-ing this sample is not difficult in many practical sit-uations. For example, in medical treatment, we only know the outcome of the treatment ( y ) for patients x that were given the treatment ( s = 1). On the other hand, we can obtain examples of the form ( x, s )that are drawn from the population as a whole. 4.1. Evaluation under sample selection bias In evaluation, for a given a classifier h ,wewantto obtain an estimate of the classifier loss, given by Usually this is done by applying the classifier to a set of test examples drawn from D and obtaining the em-where m is the number of available examples.
 Under sample selection bias, we only see the examples for which s = 1, and instead obtain an estimate of which generally is not an unbiased estimate of the loss. As seen in Section 3, local learning methods are in-sensitive to sample selection bias. However, the eval-uation step is always affected by sample selection bias because we are calculating an expected value over the whole input space (which is always  X  X lobal X ). There-fore, we argue that accounting for sample selection bias on the evaluation step is more important than account-ing for sample selection bias during learning. We can use the bias correction theorem for evaluating a classifier if we have estim ates of the selection prob-abilities P ( s =1 | x ). We simply have to weigh each example by P ( s =1) /P ( s =1 | x ) when calculating the expected loss on the biased test sample. The unbiased empirical estimate of the loss should be calculated as 4.2. Example To illustrate how the bias correction method works, we constructed an example using the KDD-98 compe-tition dataset, available from the UCI KDD Archive (Bay, 2000). We assume we know the selection prob-abilities P ( s =1 | x ) and we enforce the selection of examples using these probabilities. By doing this, we can compare the estimates of the expectation obtained using the whole sample and using the selected sample. The KDD-98 dataset contains information about peo-ple who have made donations to a charity. For the purpose of this example, we only need to look at two variables: income and amount. Income indi-cates the person X  X  income level and takes values in { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 } . Amount is the donation amount in the last campaign. We only use examples of people who have donated in the last campaign. In the nota-tion of the theorem, income is x and amount is l .(We chose to side-step h ( x )and y and assume we have l directly for each example). Let s be such that where the overall selection probability P ( s =1)is0.6. The empirical estimate of the expected amount ob-tained by averaging the amounts of all the examples is 15.62. Because there is a positive correlation be-tween income and donation amount, if we select the examples according to the probabilities above, we will overestimate the expected amount.
 To demonstrate this experimentally we can assign s values for each example according to the probabilities above and calculate the empirical mean of l using only the examples that have s = 1. By repeating this for 1000 different random draws of s , i.e., 1000 different selected samples, we obtain the distribution of esti-mated expected values of Y seen in Figure 4 (left). The vertical dashed line (on the left side) shows the esti-mated expected amount using the whole sample. The graph shows that, by using only the selected examples to estimate the expected value of l , we consistently overestimate it, as expected.
 In contrast, Figure 4 (right) shows the distribution of estimated expected values for l ,whenweuseonly the selected examples but a pply the bias correction method. The distribution is centered near the value estimated from the whole sample (and the mean is 15.62). Therefore, we can conclude that the proposed method succeeds at correcting the bias. We note, how-ever, that there is a slig ht increase in variance. We presented a formal definition of sample selection bias in classifier learning. By studying the behavior of different classifier learners under sample selection bias, we separated them into two categories:  X  local: the asymptotical beh avior only depends  X  global: the asymptotical behavior depends on While global learners are affected by sample selection bias, local learners are not. This is a new categoriza-tion, different from the more usual categorization of learning methods into discriminative and generative (Ng &amp; Jordan, 2002). As seen in Section 3.1, although generative (or Bayesian) methods model P ( x | y ), P ( y and P ( x ), their behavior is generally independent of P ( x ) (although this is not true for naive Bayes). This categorization is also useful for defining situations in which we can learn from both labeled and unlabeled data, an area of research t hat has received some atten-tion in recent years (see, for example, Szummer and Jaakkola (2003)). Clearly, global learners can take ad-vantage of unlabeled data, wh ile local learn ers cannot. For global learners, we show ed that we can still learn correctly under sample selection bias if we have data to estimate the selection probabilities P ( s =1 | x ). Also, we showed how to evaluate a classifier using a biased sample and estimates of the selection probabilities. I thank my Ph.D. advisor, Charles Elkan, for intro-ducing me to sample selection bias and for reviewing earlier versions of this paper. I also thank the anony-mous reviewers for their valuable suggestions.
