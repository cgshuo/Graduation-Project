 Kernel-based regression represents an important family of learning techniques for solving challenging regression tasks with non-linear patterns. Despite being studied extensively, most of the existing work suffers from two major drawbacks: (i) they are often designed for solving regression tasks in a batch learning setting, making them not only computa-tionally inefficient and but also poorly scalable in real-world applications where data arrives sequentially; and (ii) they usually assume a fixed kernel function is given prior to the learning task, which could result in poor performance if the chosen kernel is inappropriate. To overcome these draw-backs, this paper presents a novel scheme of Online Multi-ple Kernel Regression (OMKR), which sequentially learns the kernel-based regressor in an online and scalable fash-ion, and dynamically explore a pool of multiple diverse ker-nels to avoid suffering from a single fixed poor kernel so as to remedy the drawback of manual/heuristic kernel selec-tion. The OMKR problem is more challenging than regular kernel-based regression tasks since we have to on-the-fly de-termine both the optimal kernel-based regressor for each in-dividual kernel and the best combination of the multiple ker-nel regressors. In this paper, we propose a family of OMKR algorithms for regression and discuss their application to time series prediction tasks. We also analyze the theoretical bounds of the proposed OMKR method and conduct exten-sive experiments to evaluate its empirical performance on both real-world regression and times series prediction tasks. I.2.6 [ Artificial Intelligence ]: Learning Theory, Algorithms, Experimentation Online Learning; Multiple Kernel Learning; Kernel Regres-sion; Time Series Prediction
Kernel methods have been extensively studied for regres-sion tasks and found successes in many real-world appli-cations [29, 28]. In contrast to linear regression methods, kernel-based regression methods are able to tackle challeng-ing non-linear regression tasks using the kernel trick that im-plicitly maps data from the original space to a high or even infinite dimensional space by means of a kernel function. Although a variety of kernel methods have been proposed for regression tasks [28], most conventional kernel methods suffer from two major drawbacks. First of all, they are of-ten designed for solving regression tasks in a batch learning setting. This often results in a high re-training cost when there is any new training data, making them poorly scal-able in many real-world applications where data arrives se-quentially. Second, they usually assume that prior to the learning task, a fixed kernel function is given either by man-ual selection or via cross validation. This could result in poor performance if the chosen kernel is inappropriate in a new environment, which happens commonly for some real-world applications, such as time series prediction where data observations can be non-stationary and the optimal kernel function may change over time.

To overcome the above drawbacks, this paper investigates a novel scheme of Online Multiple Kernel Regression (OMKR), which sequentially learns a kernel-based regressor with mul-tiple kernels in an online fashion for regression tasks. On one hand, the proposed OMKR technique, as an online learning method that often makes simple incremental update for a new training data example, avoids the expensive re-training cost of conventional batch kernel methods, and thus sig-nificantly improves the efficiency and scalability, especially when handling data stream applications. On the other hand, OMKR explores a pool of multiple diverse kernels to rem-edy the drawback of using a single fixed kernel by existing kernel-based regression methods that often suffer consider-ably when the single kernel is inappropriate.

The proposed OMKR problem is however very challeng-ing since we not only need to sequentially learn the opti-mal kernel-based regressor for each individual kernel in the pool, but also need to simultaneously decide the best way of combining the multiple kernel regressors on the fly at every learning round. We tackle the challenges by (i) exploring two online kernel regression algorithms, Widrow-Hoff learning [33] and NORMA learning [17], for online regression tasks with each individual kernel; and (ii) determining the best combination of the multiple kernel regressors by applying two online learning techniques: Hedge algorithm [9] that can track the best kernel regressor, and Online Gradient Descent (OGD)[38] that can find the optimal linear combi-nation. We analyze the theoretical bound of OMKR, and also discuss a natural extension of OMKR for the prediction of Autoregressive (AR) time series. To validate the efficacy of the proposed method, we conduct extensive experiments by evaluating the proposed algorithms on both real-world regression and time series datasets, in which our empirical results show that OMKR significantly outperforms conven-tional single kernel online regression approaches for most cases, especially for time series prediction tasks.
The rest of the paper is organized as follows. Section 2 reviews the related work of both online learning and kernel methods. Section 3 gives the problem formulation of on-line learning with multiple kernels, and then presents the OMKR algorithms followed by theoretical analysis. Sec-tion 4 presents our experimental results and discussions, and finally Section 5 concludes this paper.
In this section, we review some of major related work in online learning and kernel learning in the context of OMKR.
Online learning algorithms have been extensively explored in different contexts and applications [6, 35, 22]. For more references please refer to [27, 5, 14]. Many online algorithms have been proposed for extending kernel methods in an on-line setting, in which several techniques have have been pro-posed for online kernel regression, such as Naive Online R Minimization Algorithm (NORMA) [17], Online Passive Ag-gressive Regression [6], Sparse Implicit Online Learning with Kernels (ILK and SILK) [26] and Primal Online Algorithm (PRIONA) [3].

In addition, some kernel-based online learning studies fo-cus on the budget issue [7, 4]. These help to speed up com-putation cost by bounding the number of support vectors. Some well-known example algorithms include Forgetron [8], Projectron [23], and the Bounded Online Gradient Descent (BOGD) [36]. Further, our work is also related to online prediction with expert advice [9, 20, 32]. One of the most well-known algorithms is the Hedge Algorithm [9], which was a direct generalization of Weighted Majority Algorithm [20].
Most kernel methods often assume that a predefined para-metric kernel is given a priori, where the parameters are cho-sen either manually or via cross validation. Kernel learn-ing aims to learn an effective kernel from data automati-cally. Some studies have attempted to learn kernel func-tions or matrices from labeled and unlabeled data. Exam-ples include marginalized kernels [16], idealized kernel learn-ing [18], graph-based spectral kernel learning [2, 13], and non-parametric kernel learning [11, 37]. These methods of-ten follow a batch (and transductive) learning setting and thus are difficult to be applied in an online learning scenario. Another prevalent kernel learning technique is Multiple Kernel Learning (MKL) [19], which aims to find the optimal combination of multiple kernels. Unlike most existing MKL techniques that are batch learning [19, 30, 10], our work focuses on online regression tasks, and is related to existing online MKL studies that focus on classification tasks [15, 12] and that addresses structured prediction [21]. In this section, we present the proposed Online Multiple Kernel Regression (OMKR) scheme. We will first motivate the problem by introducing the formulation of batch Mul-tiple Kernel Learning (MKL). We then present our OMKR framework, the detailed algorithms for addressing different challenges, and finally theoretical analysis of OMKR.
Consider a set of training examples D = ( x i ,y i ) ,i = 1 ,...,T where x i  X  R d , y i  X  R and a collection of m kernel func-tions K = {  X  i :  X   X   X   X  R ,i = 1 ,...,m } . Multiple Kernel Learning aims to learn a kernel-based prediction model by identifying the best linear combination of the m kernels, that is, a weighted combination  X  = (  X  1 ,..., X  m ). The learning task can be cast into the following optimization [19]: where  X  = {  X   X  R m + |  X  T 1 m = 1 } , K (  X  )(  X  ,  X  ) = and ` ( f ( x i ) ,y i ) is a convex loss function. The above convex optimization problem of regular batch MKL can be solved by different schemes [30, 34, 10]. De-spite being studied extensively, it remains very challenging when solving the batch MKL for large-scale applications. Besides, similar to most batch kernel methods, regular MKL has some drawbacks: (i) the trained model, if it is not re-trained with new data, may work poorly for non-stationary data in a new environment; but (ii) the re-training cost is ex-tremely expensive for data streams, making it non-scalable.
To overcome the limitations of MKL for a regression task, we propose a new scheme of Online Multiple Kernel Re-gression (OMKR) by applying the emerging online multiple kernel learning principle [12] for tackling regression tasks, which attempts to sequentially learn the online multiple-kernel regressor given a new data example using a two-step updating scheme: (i) update the set of kernel-based regres-sors for each individual kernel; and (ii) update the weights for combining the multiple kernel regressors. In the follow-ing, we discuss the details of the proposed algorithms for tackling online regression tasks at each of the two steps.
The goal of this task is to learn a regression function f  X  H  X  in an online setting, where H  X  a reproducing ker-nel Hilbert space (RKHS) induced by a given specific kernel  X   X  K . We solve this task by exploring two online regres-sion solutions: Kernel Widrow-Hoff [33] and NORMA [17], which follows the same principle of Online Gradient Descent (OGD) [38] for online convex optimization and but optimizes two slightly different objective functions.

Kernel Widrow-Hoff Learning. Given a sequence of data instances D = ( x i ,y i ) ,i = 1 ,...,T , the goal of kernel-ized Widrow-Hoff learning is to minimize the total cumula-tive loss over the whole regression task L defined as follows: where f t ( x t ) is the prediction made by a kernel regressor on the t -th instance, ` ( f t ( x t ) ,y t ) denoted by L t is a convex loss function. Following OGD [38], we have the following online update rule given a data instance ( x t ,y where  X  t &gt; 0 is a learning rate parameter that can be either a small constant  X  t =  X  used in Widrow-Hoff [33] or a factor depending on t . When choosing the squared loss for ` : we have the online updating rule expressed explicitly as
NORMA. The above method has two potential draw-backs. First, it may lead to overfitting when dealing with noisy data. Second, due to the use of squared loss, almost every training instance will be added as support vectors (un-less f t ( x t ) is identical to y t ), making the prediction function computationally intensive when handling large-scale datasets. To overcome these drawbacks, we explore another online re-gression scheme by following the idea of NORMA [17], which replaces L t ( f t ) by the following regularized loss: By the OGD principle, we have the online updating rule as: where  X  t &gt; 0 is the learning rate parameter. Instead of using the square loss, we exploit the -insensitive loss function which is defined as where represents the width of the insensitivity zone. We can further modify the loss function by making as a vari-able of the optimization: where  X  &gt; 0 is a parameter, and t is a variable to be up-dated in online learning process. Using the above loss func-tion, we can derive the online updating rule for NORMA: f where we denote d = y t  X  f ( x t ).

Remark. For both of the above methods, at the end of each online learning round, we can express the prediction function of the regressor as a kernel expansion [25]: where the  X  i coefficients are computed based on the updat-ing rules in (4) or (8). When  X  i 6 = 0, the i -th instance is often called as a Support Vector (SV). Thus, the time com-plexity for prediction is linear with respect to the number of SV X  X . When using the squared loss, we will have  X  i 6 = 0 for almost every instance, leading to a large number of sup-port vectors. By contrast, when using the -insensitive loss, whenever the difference between the prediction on the i -th instance f i ( x i ) and y i is small enough, i.e., within the tube, we have  X  i = 0, which thus generates a much smaller SV size and significantly improves the prediction efficiency.
The previous online kernel regression method allows us to learn a set of kernel regressors f i t  X  H  X  i ,i = 1 ,...,m with respect to the pool of multiple diverse kernels K . The idea of OMKR is to learn an effective regressor F t ( x ) by combining the set of multiple kernel regressors: where w i t  X  R denotes the combination weight for the i -th kernel regressor. The remaining problem then is to deter-mine the appropriate combination weights w t for the set of kernels. We note that this is a very challenging task since we may not have prior knowledge for empirical performance of each kernel, and the optimal combination weights may even change over time in the online learning process especially when dealing with non-stationary data.

One naive solution is to simply adopt a uniform combi-nation for all the kernels, i.e., w i t = 1 /m , which does ex-plore all the kernels, but often results in sub-optimal perfor-mance, as observed in our empirical studies. In this section, we attempt to learn the best kernel combination weights by exploring two different online learning algorithms: the Hedge algorithm [9] and the OGD algorithm [38]. We will first present each algorithm in detail and finally discuss their strengths and weaknesses for different scenarios.

Hedge Algorithm : The Hedge algorithm is the most popular online algorithm for solving the problem of decision-theoretic online learning or known as prediction with expert advice [32, 5]. Specifically, by treating each online kernel re-gressor as an expert, the Hedge algorithm aims to minimize the regret of the learner for the regression task, which is the difference between the learner X  X  cumulative loss and the cu-mulative loss of the best kernel regressor. In theory, Hedge can achieve an optimal upper bound of regret O ( T ln m ) with T learning rounds and m kernel regressor experts. It is thus an ideal online learning algorithm for tracking the best online kernel regressor especially when there is some kernel regressor significantly dominates the rest.

Specifically, the Hedge algorithm runs in a fairly simple way. Consider the OMKR problem, at the beginning, the combination weights w t are initialized as a uniform distri-bution, i.e., w i 1 = 1 /m,i = 1 ,...,m . At the end of each learning round, according to the performance of the multi-ple kernel regressors, the weights are updated by: where  X   X  (0 , 1) is a discounting (learning rate) parameter, and ` i t denotes the loss suffered by the i -th kernel regressor w t +1  X  X  to ensure the combination weights as a distribution.
We refer to the proposed OMKR algorithm that adopts the Hedge algorithm as the Deterministic OMKR (Hedge) algorithm, as shown in Algorithm 1. In the algorithm, we can update each kernel regressor f i t +1 by adopting either the Widrow-Hoff learning in (4) or NORMA in (8).

Although Hedge is ideal for tracking the best kernel regres-sor, it is not always perfect for solving a practical OMKR problem since our goal is to learn the best combination of multiple kernels. In the following, we present an online gra-dient descent (OGD) based algorithm that attempts to learn the optimal linear combination of multiple kernel regressors. Algorithm 1 Deterministic OMKR (Hedge) INPUT: -Kernels:  X  (  X  ,  X  ) :  X   X   X   X  i = 1 ,...,m -Discounting Parameter:  X   X  (0 , 1) -Step size parameter for each kernel:  X  -Regression parameters:  X  and  X  for OMKR(NORMA) Initialization : f 1 = 0 , w 1 = 1 m 1 for t = 1,. . . ,T do end for
OGD Algorithm : Our goal is to learn the optimal com-bination weight vector w t  X  R m for combining the multiple kernel regressors. It can be cast into the following online optimization where f t ( x t ) is a vector representing the predictions made by all the kernel regressors on instance x t , and ` is a loss function denoting the loss suffered by the OMKR. We simply adopt the squared loss in our solution (though it may also include a regularizer). Following the OGD, we can derive the updating rule as follows: where  X  w is a learning rate parameter, and  X  y t = w &gt;
Using the above OGD algorithm for learning the optimal combination weights, we propose another OMKR scheme, called Deterministic OMKR (OGD), as shown in Algorithm 2. Like in OMKR(Hedge), we can also update each kernel regressor by either Widrow-Hoff in (4) or NORMA in (8). Algorithm 2 Deterministic OMKR (OGD) INPUT: -Kernels:  X  (  X  ,  X  ) :  X   X   X   X  i = 1 ,...,m -Learning rate parameter:  X  w -Step size parameter for each kernel:  X  -Regression parameters:  X  and  X  for OMKR(NORMA) Initialization : f 1 = 0 , w 1 = 0 for t = 1,. . . ,T do end for
Remark. In online MKL work related to classification [15, 12], Hedge algorithm was used to combine multiple pre-dictions. In contrast, our proposed OGD approach inter-prets the kernel predictions as new rich features which can be combined linearly. In terms of update rules, Hedge makes multiplicative updates while OGD makes additive updates. Further, for the combination weight vector w t , Hedge al-ways keep w t a distribution ( w i t  X  0 and P i w i t = 1) while OGD is able to learn any real-valued vector for w general, both Hedge and OGD have their different merits. Hedge is good at tracking the best kernel regressor, while OGD is good at learning the optimal combination of multi-ple kernel regressors. However, OGD often suffers from slow convergence rate. In practice, the empirical performance of OMKR(Hedge) and OMKR(OGD) may vary a lot in differ-ent scenarios. Due to the nature of multiplicative update of Hedge, it converges quickly, and in an online setting, may tend to achieve better performance than OGD, if the dataset is small, or if the pattern changes due to non-stationarity. We conduct more in-depth analysis through our extensive experimental studies in Section 4.
OMKR can be applied a variety of online regression tasks, especially for mining data streams. A natural application of OMKR is Time Series Prediction, which is the task of pre-dicting the future value based on given past values. Kernel methods have been commonly used for solving such prob-lems [31, 24]. We first introduce the popular time series prediction model known as Autoregressive (AR) model, and then present a kernelized AR model, followed by showing the application of OMKR for time series prediction.
Autoregressive (AR) model is used for a univariate time series where the value of the series at a particular time is linearly dependent on its own previous values. An AR ( p ) model denotes an autoregressive process of order p , i.e., y described by a noisy linear combination of [ y t  X  1 y t  X  2 where c is a constant, t is white noise, and  X  i are the pa-rameters describing the dependency. A simple AR ( p ) model assumes linear dependency on the previous p values. This may not be true. We use kernels to explore nonlinear de-pendencies. The kernelized AR ( p ) model is given by: where f ( Y p t  X  1 )  X  X   X  is the prediction of the regression func-tion using a kernel  X  . Here, a new challenge arises, i.e., to choose the appropriate kernel function. In addition, another issue is how to choose the appropriate parameter p .
To solve these two issues, we propose to construct a pool of multiple kernels for varying values of parameter p . For example, for p  X  [ p 1 ,p 2 ,...,p k ], and m kinds of diverse ker-nels, we can create the following pool of mk kernel functions: The above can now be directly plugged into the OMKR framework for solving time series prediction tasks. In com-parison to existing kernel methods for times series predic-tion, the proposed OMKR solution enjoys the important advantages of avoiding tedious kernel selection and parame-ter selection (e.g., p ) and exploiting the power of combining multiple kernels for more accurate prediction.
Without loss of generality we assume that  X  i,  X  t,  X  i ( x x )  X  1, and ` t ( f i t ( x t ) ,y t )  X  1 We define the optimal kernel regressor with respect to the squared loss (Widrow-Hoff) as: where D is a sequence of instances.

Theorem 1. After receiving a sequence of T instances, the cumulative loss suffered by OMKR (Hedge) using the Widrow-Hoff Algorithm is bounded as Here L OMKR is the total loss suffered at each prediction, and due to the convexity of the loss function, we have L and by choosing  X  = L Proof. The proof follows from combining the proof of Hedge Algorithm and the Widrow-Hoff Regression. Let  X  i t || f t  X  f || 2 2 for any f  X  X   X  i . Also, let  X  t denote the change in f during each update, such that  X  t =  X  ( f t ( x t )  X  y We also define ` t = f t ( x t )  X  y t as the signed error suffered by f t , and `  X  t = f ( x t )  X  y t be the signed error suffered by f .  X  The inequality follows from the assumption  X  ( x t  X  x t )  X  1. In the above equation we use the algebraic inequality ab  X  ( a 2 + b 2 ) / 2. From this, by assuming f 1 = 0 , and using a telescoping sum, it is very simple to prove that where L i WH is the cumulative loss suffered by the regres-sion function learnt by the the Widrow-Hoff Algorithm in the RKHS by the i th kernel. Plugging this into the Hedge Algorithm gives us the bound. The choice of  X  maybe over-estimated because of the assumption that the loss suffered by the algorithm is T .

Similarly, bounds can be derived for any generic convex loss function by using online gradient descent for learning each regression function. For a decaying  X  t = 1  X  t can achieve a sublinear regret bound of O ( to the best linear combination of multiple kernel predic-tions. For a fixed  X  , the bound is weaker and may indi-cate poor learning. However, a fixed  X  is more suitable in a non-stationary environment, as it can adapt to a changing pattern faster.
In the worst case, all the instances become support vectors for each kernel in the OMKR framework(which is invariably the case when using squared loss). For the t th instance, the time taken for a prediction to be made by a single kernel is in O ( t ), and for m predictions to be made by m kernels is in O ( mt ). However, not all kernels are good candidates for prediction, especially when their weights are low. In addition, not all the historical instances are good candidates for making the prediction, particularly in a non-stationary setting. With this motivation, we propose stochastic update and budget online kernel learning strategies.
An update to a kernel regressor involves adding a new sup-port vector. If SVs are not added to less important kernels, the time taken for prediction by these kernels is significantly reduced. The intuition is if there is only one good kernel or a small subset of good performing kernels, it is only these should be given more data to learn the function, and the poor kernels are still allowed to make predictions (but with limited data), which takes much lesser computational time. We define a probability sampling denoted by q i t , which deter-mines the probability of a kernel being selected for updates. This indicates that higher the absolute weight, the higher is the probability, and the best kernel has a probability of 1. When OMKR(Hedge) is used the weights can never be negative. In case of OGD updates in weights, there is a theoretical possibility for the weights to become negative, and hence we take absolute values to compute q i t , so as to account for weights having the maximum impact on the pre-diction. To prevent kernels with low weights, that do not have a significant impact to the prediction, from completely losing out, we introduce a smoothing parameter  X   X  (0 , 1). The idea is to add a small component of uniform weights. The new probability of a kernel being selected for update is denoted by: Here  X  is a small value. A similar idea was used in [1], to tradeoff between exploration and exploitation. Using p we sample a subset of kernels based on Bernoulli Sampling, i.e., m t = Bernoulli ( p i t ). Only those kernels that are selected will be chosen for an update. The steps are described in Algorithm 3.

Theorem 2. After receiving a sequence of T instances the expected loss of the Stochastic Update OMKR (Hedge) denoted by E [ L ] = E [ X  T t =1 ` ( X  m i =1  X  i t f i t ( x Algorithm 3 Stochastic OMKR scheme INPUT: -Kernels:  X  (  X  ,  X  ) :  X   X   X   X  i = 1 ,...,m -Update Parameter:  X   X  (0 , 1) if Hedge or  X  w for OGD -Smoothing Parameter:  X   X  (0 , 1) -Step Size Parameter for each kernel:  X  -Regression parameters:  X , X  for OMKR(NORMA) NORMA) Initialization : f 1 = 0 , w 1 = 1 m 1 for t = 1,. . . ,T do end for and by choosing  X  =
E [ L ]  X  1 The proof is similar to the proof of OMKR(Hedge). We omit the details.
Often, a subset of instances can explain the data as well as the entire data. Further, in a non-stationary time series setting, it is common to introduce a sliding window so as to give importance to only the most recent instances. In our case, we have a sliding window of the most recent support vectors that explain the data. This is also particularly help-ful in the case of NORMA, where in each iteration, the old support vectors get reduced by a factor of (1  X   X  X  ). As t grows, the  X  values of the old support vectors get reduced to almost zero. Such support vectors can be ignored with-out any significant impact to the prediction. Therefore, we propose a parameter  X  which restricts the total number of support vectors that are allowed to be stored by each regres-sor. The older support vectors are deleted.
Stochastic OMKR improves efficiency by reducing the num-ber of support vectors of poor performing kernels. However, if all kernels are equally good, there is no reduction in com-putational time. The budget strategy restricts the number of support vectors of each kernel, and has a worst case com-plexity of O ( m X  ) in each iteration to make the prediction. Both address non-stationarity in different ways. Stochastic approach trades off between exploration and exploitation to determine if another kernel has become more suitable for the changing pattern, and the budget approach gives more importance to the most recent data.
In this section, we empirically evaluate the performance of all OMKR variants on regression data and time series data.
We use five regular regression datasets and seven time se-ries datasets. The data is from different applications, with a wide range of data size and dimensionality. All data at-tributes including the target were scaled to [0 , 1]. The algo-rithms were run on ten random permutations of the regular regression datasets to establish robustness. Such permuta-tions are not applicable in the case of time series. The details of the datasets used can be seen in Table 1.
 ID Name # Instances # Attributes D1 Abalone 4177 8 D2 Parkinsons 5875 20 D3 Spacega 3107 6 D4 Cadata 20640 8 D5 Add10 9792 11 D6 Laser 10073 20 | 10 D7 Physiological 17000 2 D8 Currency Exch. 1 3000 20 | 10 D9 Currency Exch. 2 3000 20 | 10 D10 Astrophysical 598 20 | 10 D11 Santafe Computer 100000 20 | 10 D12 Twitter 583250 77
Datasets D1, D2 and D12 were taken from the UCI repos-itory 1 , D3-D4 from StatLib 2 , D5 is a synthetic dataset ob-tained from Delve 3 . D6-D11 are datasets from the Santa Fe Time Series Competition Data 4 . D6 is stationary, D7 is non-stationary, and unlike other time series data, is not univariate, but is dependent on 2 attributes, D8 and D9 X  X  stationarity property is unknown, and D10 is characterized by noise. D11 is non-stationary with a slow drift. D12 is about predicting buzz in social media. For univariate time series data the attribute column having 20 | 10 indicates the choice of 2 kernelized AR ( p ) process with p = 10 , 20 each having its own m kernel functions.
We evaluate the performance of OMKR by using a pool of 24 predefined kernels. These include 4 polynomial ker-nels  X  ( x,y ) = ( x T y ) p of degree parameter p = 1 , 2 , 3 , 4, rameter  X  in [2  X  6 , 2  X  5 ,..., 2 6 ], 5 Cauchy kernels (  X  ( x,y ) = moid kernel(  X  ( x,y ) = tanh( xy )) and a Chi-Square Kernel http://archive.ics.uci.edu/ml/ http://lib.stat.cmu.edu/ http://www.cs.toronto.edu/~delve/data/datasets. html http://www-psych.stanford.edu/~andreas/ Time-Series/SantaFe.html to [0 , 1], we clip the kernel prediction to this range, i.e.,  X  y = max(0 , min(1 ,  X  y t )).
We compare the algorithms based on Mean Squared Er-ror (MSE), time taken, and the weight distribution. The algorithms compared are -(i) Regression(V) : Best Kernel by validation; (ii) Regression(H) : Best Kernel in hindsight; (iii) Uniform OMKR : Uniform weight distribution over ker-nels (to see if this can eliminate the impact of a poor kernel choice); (iv) Deterministic OMKR (Hedge) ; and (v) De-terministic OMKR (OGD) . We then analyze the perfor-mance of efficiency enhancing variants of OMKR and study the tradeoff between accuracy and efficiency. For the large datasets (D11 and D12), we compare only the budget ver-sions of all algorithms.
All parameters for the regression tasks (if any), and the best kernel for Regression(V) were chosen by online valida-tion technique. We performed a grid search and evaluated the performance of the parameters on the of first 100 in-stances or first 10% of the instances, whichever was lesser. The value of Hedge parameter  X  was fixed to 0 . 5 in all cases, and the learning rate  X  w was fixed to 0 . 025 for OGD update of weights). We also conducted sensitivity analysis for the weight update parameters. The learning rate  X  for each kernel regression was fixed at 0 . 1. Since  X  is the same for both single kernel and multi kernel versions, its choice does not affect the comparison between Single Kernel Regression and OMKR. For budget strategies, we fixed the budget size  X  = 500 support vectors. In stochastic OMKR, the smooth-ing parameter  X  was set to 0.05 in all cases. The detailed results of single kernel regression against OMKR can be seen in Table 2. Columns Reg(V) and Reg(H) represent single kernel regression by validation and in hind-sight. Columns Uniform, Hedge, OGD represent OMKR with uniform weights, weight updated by Hedge, and weight updated by OGD respectively.
 With almost no exception both our proposed methods OMKR (Hedge and OGD) outperform Reg(V) very signifi-cantly, at times achieving as low as 1% of error of Reg(V). We should note that in a real world setting, it is hard to choose a better kernel for unseen data than by a valida-tion method. Reg(H) is the best kernel in hindsight, and is not known prior to running the experiments. Despite this, OMKR algorithms significantly outperform Reg(H) in most cases. In cases, where it OMKR does not beat Reg(H), their performance is very closely matched. Thus, without any a priori knowledge, OMKR is able to outperform even the best kernel in hindsight. This is because OMKR is able to identify a linear combination of kernels, which provide complementary information to each other in order to give a weighted prediction which beats any single best kernel. Uniform OMKR is affected by the usage of certain poor ker-nels and its performance is very inconsistent across datasets. It never beats OMKR(OGD), and beats OMKR(Hedge) in only one case (D1-Norma). This however is probably an exception, in which the optimal linear combination is close to a uniform distribution, because of which uniform weights are probably just a lucky guess. The difference in perfor-mance by Reg(V) and Reg(H), and the poor performance by Uniform(OMKR) highlight the difficulty of choosing the best kernel function for a given task. In terms of efficiency, Deterministic OMKR takes roughly m times the amount of time take by single kernel regression.

Hedge and OGD are suitable in different scenarios. Due to a multiplicative update, Hedge converges very quickly, by identifying the single kernel that best represents the data, which is often the case. However, since Hedge only offers a linear combination of the best kernel(s), we expect the optimal linear combination determined by OGD to outper-form Hedge. This does not happen if the the data is not large enough for OGD to converge to optimal linear com-bination, or the data is non-stationary such that the ap-propriate kernel function changes too frequently for OGD to be able to learn the optimal combination. We plot the cumulative mean squared error against time for some rep-resentative datasets in Figures 1 and 2. It can be seen, that in most cases, OMKR(Hedge) attains a very low MSE from the beginning and does not improve much further, whereas, OMKR(OGD) starts with a relatively higher MSE, but it is continuously improving its performance. Refer-ring back to Table 2, it can be seen that in general that OMKR(OGD) has relative advantage in larger datasets, and OMKR(Hedge) in smaller ones. Additionally, we also look at the weight distribution attained by the algorithms, which is shown in Figure 3. The weight distribution by OMKR(Hedge) concentrates largely on the best kernel in hindsight, and oth-erwise has weights over certain reasonably good perform-ing kernels. Unlike OMKR(Hedge), OMKR(OGD) does not have a concentrated distribution of weights over few kernels.
The MSE and the time taken by Deterministic, Stochastic and Budget OMKR are detailed in Tables 3 and 4. Clearly the time taken by both stochastic and budget techniques is significantly lower than Deterministic OMKR. Despite this, in most cases, the efficiency enhancers give compara-ble MSEs with respect to Deterministic OMKR. In many cases, particularly time series, the variants are able to out-perform the deterministic version. This shows their ability to retain important information from the data, and adapt to changes in the pattern. Stochastic is faster than budget in smaller datasets, but in larger datasets, the number of SVs in stochastic start dominating even if only for a few kernels, and hence Budget is faster.

Using Budget OMKR, we run the algorithm on two large datasets (D11 and D12). In this case, we use a budget for all 5 algorithms. The details are shown in Table 5. The results are consistent with our previous findings of OMKR outperforming single kernel regression.
OMKR(Hedge) is not very sensitive to the value of the dis-count rate parameter  X  . There is a reasonably large range of values of  X  in which OMKR(Hedge) X  X  relative performance to other algorithms remains the same. OMKR(OGD) X  X  sen-sitivity to the learning rate  X  w shows a tradeoff between large and small learning rates. This behavior is typical of all gradient descent algorithms.
This paper proposes a family of OMKR algorithms for ker-nel based regression using a pool of predefined kernels. They overcome the challenges of existing work which are largely designed for a batch setting and assume that the appropri-ate kernel function is known. OMKR sequentially learns the kernel based regressor in an online and scalable fashion, and dynamically explores a pool of multiple diverse kernels to avoid problems of poor kernel choice by manual or heuristic selection. Further, OMKR is particularly useful in a time series setting, where the appropriate window size p of an AR ( p ) process is not known. Varying values of p , each with its own set of predefined kernels, are plugged into the OMKR framework, thus exploring and identifying the best kernel based regressor dynamically. We evaluate the performance of OMKR based on two types of loss functions. Any kernel based regression task can be plugged into the generic OMKR framework, and is likely to achieve a better result than any single kernel. We also propose stochastic and budget tech-niques to enhance efficiency. Our empirical evaluations show the excellent performance of OMKR, often beating the best prediction function determined in hindsight. [1] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. [2] O. Bousquet and D. J. Herrmann. On the complexity [3] D. Brugger, W. Rosenstiel, and M. Bogdan. Online svr [4] G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. [5] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, [6] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, [7] K. Crammer, J. Kandola, and Y. Singer. Online [8] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The [9] Y. Freund and R. E. Schapire. A desicion-theoretic [10] M. G  X  onen and E. Alpayd X n. Multiple kernel learning [11] S. C. Hoi, R. Jin, and M. R. Lyu. Learning [12] S. C. Hoi, R. Jin, P. Zhao, and T. Yang. Online [13] S. C. Hoi, M. R. Lyu, and E. Y. Chang. Learning the [14] S. C. Hoi, J. Wang, and P. Zhao. LIBOL: A Library [15] R. Jin, S. Hoi, and T. Yang. Online multiple kernel [16] H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized [17] J. Kivinen, A. Smola, and R. Williamson. Online [18] J. T. Kwok and I. W. Tsang. Learning with idealized [19] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. E. [20] N. Littlestone and M. K. Warmuth. The weighted [21] A. F. Martins, M. A. Figueiredo, P. M. Aguiar, N. A. [22] E. Moroshko and K. Crammer. A last-step regression [23] F. Orabona, J. Keshet, and B. Caputo. The [24] N. Sapankevych and R. Sankar. Time series prediction [25] B. Sch  X  olkopf and A. J. Smola. Learning with kernels . [26] L. C. S. V. D. Schuurmans and S. W. Caelli. Implicit [27] S. Shalev-Shwartz. Online learning: Theory, [28] J. Shawe-Taylor and N. Cristianini. Kernel methods [29] A. J. Smola and B. Sch  X  olkopf. A tutorial on support [30] S. Sonnenburg, G. R  X  atsch, C. Sch  X  afer, and [31] U. Thissen, R. Van Brakel, A. De Weijer, W. Melssen, [32] V. Vovk. A game of prediction with expert advice. In [33] B. WIDROW, M. E. HOFF, et al. Adaptive switching [34] Z. Xu, R. Jin, I. King, and M. R. Lyu. An extended [35] H. Yang, Z. Xu, I. King, and M. R. Lyu. Online [36] P. Zhao, J. Wang, P. Wu, R. Jin, and S. C. Hoi. Fast [37] J. Zhuang, I. W. Tsang, and S. C. Hoi. A family of [38] M. Zinkevich. Online convex programming and
