 Unsupervised domain adaptation is a scenario that practitioners often face when having to build ro-bust NLP systems. They have labeled data in the source domain, but wish to improve performance in the target domain by making use of unlabeled data alone. Most work on unsupervised domain adaptation in NLP uses batch learning : It assumes that a large corpus of unlabeled data of the tar-get domain is available before testing. However, batch learning is not possible in many real-world scenarios where incoming data from a new target domain must be processed immediately. More im-portantly, in many real-world scenarios the data does not come with neat domain labels and it may not be immediately obvious that an input stream is suddenly delivering data from a new domain.

Consider an NLP system that analyzes emails at an enterprise. There is a constant stream of in-coming emails and it changes over time  X  without any clear indication that the models in use should be adapted to the new data distribution. Because the system needs to work in real-time, it is also de-sirable to do any adaptation of the system online , without the need of stopping the system, changing it and restarting it as is done in batch mode.
In this paper, we propose online unsupervised domain adaptation as an extension to traditional unsupervised DA. In online unsupervised DA, do-main adaptation is performed incrementally as data comes in. Specifically, we adopt a form of representation learning . In our experiments, the incremental updating will be performed for repre-sentations of words. Each time a word is encoun-tered in the stream of data at test time, its repre-sentation is updated.

To the best of our knowledge, the work re-ported here is the first study of online unsuper-vised DA. More specifically, we evaluate online unsupervised DA for the task of POS tagging. We compare POS tagging results for three distinct ap-proaches: static (the baseline), batch learning and online unsupervised DA. Our results show that online unsupervised DA is comparable in perfor-mance to batch learning while requiring no retrain-ing or prior data in the target domain. Tagger. We reimplemented the FLORS tagger (Schnabel and Sch  X  utze, 2014), a fast and simple tagger that performs well in DA. It treats POS tag-ging as a window-based (as opposed to sequence classification), multilabel classification problem. FLORS is ideally suited for online unsupervised DA because its representation of words includes distributional vectors  X  these vectors can be easily updated in both batch learning and online unsu-pervised DA. More specifically, a word X  X  represen-tation in FLORS consists of four feature vectors: one each for its suffix, its shape and its left and right distributional neighbors. Suffix and shape features are standard features used in the litera-ture; our use of them is exactly as described by Schnabel and Sch  X  utze (2014).

Distributional features. The i th entry x i of the left distributional vector of w is the weighted num-ber of times the indicator word c i occurs immedi-ately to the left of w : where c i is the word with frequency rank i in the corpus, freq (bigram( c i , w )) is the number of oc-currences of the bigram  X  c i w  X  and we weight non-zero frequencies logarithmically: tf( x ) = 1 + log( x ) . The right distributional vector is defined analogously. We restrict the set of indicator words to the n = 500 most frequent words. To avoid zero vectors, we add an entry x n +1 to each vector that counts omitted contexts:
Let f ( w ) be the concatentation of the two dis-tributional and suffix and shape vectors of word w . Then FLORS represents token v i as follows: where  X  is vector concatenation. FLORS then tags token v i based on this representation.
FLORS assumes that the association between distributional features and labels does not change fundamentally when going from source to target. This is in contrast to other work, notably Blitzer et al. (2006), that carefully selects  X  X table X  dis-tributional features and discards  X  X nstable X  dis-tributional features. The hypothesis underlying FLORS is that basic distributional POS properties are relatively stable across domains  X  in contrast to semantic and other more complex tasks. The high performance of FLORS (Schnabel and Sch  X  utze, 2014) suggests this hypothesis is true.

Data. Test set. We evaluate on the development sets of six different TDs: five SANCL (Petrov and McDonald, 2012) domains  X  newsgroups, we-blogs, reviews, answers, emails  X  and sections 22-23 of WSJ for in-domain testing.

We use two training sets of different sizes. In condition l:big (big labeled data set), we train FLORS on sections 2-21 of Wall Street Journal (WSJ). Condition l:small uses 10% of l:big.

Data for word representations. We also vary the size of the datasets that are used to compute the word representations before the FLORS model is trained on the training set. In condition u:big , we compute distributional vectors on the joint corpus of all labeled and unlabeled text of source and tar-get domains (except for the test sets). We also include 100,000 WSJ sentences from 1988 and 500,000 sentences from Gigaword (Parker, 2009). In condition u:0 , only labeled training data is used.
Methods. We implemented the following mod-ification compared to the setup in (Schnabel and Sch  X  utze, 2014): distributional vectors are kept in memory as count vectors. This allows us to in-crease the counts during online tagging.
 We run experiments with three versions of FLORS: S TATIC , B ATCH and O NLINE . All three methods compute word representations on  X  X ata for word representations X  (described above) be-fore the model is trained on one of the two  X  X rain-ing sets X  (described above).

S TATIC . Word representations are not changed during testing.

B ATCH . Before testing, we update count vectors by freq (bigram( c i , w )) += freq  X  (bigram( c i , w )) , where freq  X  (  X  ) denotes the number of occurrences of the bigram  X  c i w  X  in the entire test set.

O NLINE . Before tagging a test sentence, both left and right distributional vectors are updated via freq (bigram( c i , w )) += 1 for each appearance of bigram  X  c i w  X  in the sentence. Then the sentence is tagged using the updated word representations. As tagging progresses, the distributional represen-tations become increasingly specific to the target domain (TD), converging to the representations that B ATCH uses at the end of the tagging process.
In all three modes, suffix and shape features are always fully specified, for both known and un-known words. Table 1 compares performance on SANCL for a number of baselines and four versions of FLORS: S&amp;S, Schnabel and Sch  X  utze (2014) X  X  version of FLORS,  X  X &amp;S (reimpl.) X , our reimplementation of that version, and B ATCH and O NLINE , the two versions of FLORS we use in this paper. Compar-ing lines  X  X &amp;S X  and  X  X &amp;S (reimpl.) X  in the ta-ble, we see that our reimplementation of FLORS is comparable to S&amp;S X  X . For the rest of this pa-per, our setup for B ATCH and O NLINE differs from S&amp;S X  X  in three respects. (i) We use Gigaword as additional unlabeled data. (ii) When we train a FLORS model, then the corpora that the word rep-resentations are derived from do not include the test set. The set of corpora used by S&amp;S for this purpose includes the test set. We make this change because application data may not be available at training time in DA. (iii) The word representations used when the FLORS model is trained are derived from all six SANCL domains. This simplifies the experimental setup as we only need to train a sin-gle model, not one per domain. Table 1 shows that our setup with these three changes (lines B ATCH and O NLINE ) has state-of-the-art performance on SANCL for domain adaptation (bold numbers).

Table 2 investigates the effect of sizes of labeled and unlabeled data on performance of O NLINE and B ATCH . We report accuracy for all (ALL) to-kens, for tokens occurring in both l:big and l:small (KN), tokens occurring in neither l:big nor l:small (OOV) and tokens ocurring in l:big, but not in in a few cases, both using more labeled data and using more unlabeled data improves tagging accu-racy for both O NLINE and B ATCH . O NLINE and B
ATCH are generally better or as good as S TATIC (in bold), always on ALL and OOV, and with a few exceptions also on KN and SHFT.

O NLINE performance is comparable to B ATCH performance: it is slightly worse than B ATCH on u:0 (largest ALL difference is .29) and at most .02 different from B ATCH for ALL on u:big. We ex-plain below why O NLINE is sometimes (slightly) better than B ATCH , e.g., for ALL and condition l:small/u:big. 3.1 Time course of tagging accuracy The O NLINE model introduced in this paper has a property that is unique compared to most other work in statistical NLP: its predictions change as it tags text because its representations change.
To study this time course of changes, we need a large application domain because subtle changes will be too variable in the small test sets of the SANCL TDs. The only labeled domain that is big enough is the WSJ corpus. We therefore reverse the standard setup and train the model on the dev sets of the five SANCL domains ( l:big ) or on the first 5000 labeled words of reviews ( l:small ). In this reversed setup, u:big uses the five unlabeled SANCL data sets and Gigaword as before. Since variance of performance is important, we run on 100 randomly selected 50% samples of WSJ and report average and standard deviation of tagging error over these 100 trials.
 only slightly worse for O NLINE than for B ATCH or the same. In fact, l:small/u:0 known error rate (.1186) is lower for O NLINE than for B ATCH (sim-ilar to what we observed in Table 2). This will be discussed at the end of this section.

Table 3 includes results for  X  X nseens X  as well as unknowns because Schnabel and Sch  X  utze (2014) show that unseens cause at least as many errors as unknowns. We define unseens as words with a tag that did not occur in training; we compute unseen error rates on all occurrences of unseens, i.e., occurrences with both seen and unseen tags are included. As Table 3 shows, the error rate for unknowns is greater than for unseens which is in turn greater than the error rate on known words.
Examining the single conditions, we can see that O NLINE fares better than S TATIC in 10 out of 12 cases and only slightly worse for l:small/u:big (unseens, known words: .1086 vs .1084, .0802 vs .0801). In four conditions it is significantly better with improvements ranging from .005 (.1404 vs .1451: l:big/u:0, unknown words) to &gt; .06 (.3094 vs .3670: l:small/u:0, unknown words).

The differences between O NLINE and S TATIC in the other eight conditions are negligible. For the six u:big conditions, this is not surprising: the Gigaword corpus consists of news, so the large un-labeled data set is in reality the same domain as WSJ. Thus, if large unlabeled data sets are avail-able that are similar to the TD, then one might as well use S TATIC tagging since the extra work re-quired for O NLINE /B ATCH is unlikely to pay off.
Using more labeled data (comparing l:small and l:big) always considerably decreases error rates. We did not test for significance here because the differences are so large. By the same token, us-ing more unlabeled data (comparing u:0 and u:big) also consistently decreases error rates. The differ-ences are large and significant for O NLINE tagging in all six cases (indicated by  X  in the table). There is no large difference in variability O N -LINE vs. B ATCH (see columns  X  X td X ). Thus, given that it has equal variability and higher perfor-mance, O NLINE is preferable to B ATCH since it assumes no dataset available prior to the start of tagging.

Figure 1 shows the time course of tagging ac-curacy. 3 B ATCH and S TATIC have constant error rates since they do not change representations dur-ing tagging. O NLINE error decreases for unknown words  X  approaching the error rate of B ATCH  X  as
