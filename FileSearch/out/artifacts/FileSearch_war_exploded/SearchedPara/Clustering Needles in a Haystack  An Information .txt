
Identifying atypical objects is one of the traditional top-ics in machine learning. Recently, novel approaches, e.g., Minority Detection and One-class clustering, have explored further to identify clusters of atypical objects which strongly contrast from the rest of the data in terms of their distri-bution or density. This paper analyzes such tasks from an information theoretic perspective. Based on Information Bottleneck formalization, these tasks interpret to increas-ing the averaged atypicalness of the clusters while reducing the complexity of the clustering. This formalization yields a unifying view of the new approaches as well as the clas-sic outlier detection. We also present a scalable minimiza-tion algorithm which exploits the localized form of the cost function over individual clusters. The proposed algorithm is evaluated using simulated datasets and a text classification benchmark, in comparison with an existing method.
Detecting atypical objects and rare events is one of the important topics in machine learning. The application of Outlier Detection [7] has been studied extensively in the field of data mining and has shown that such events are source of useful knowledge in many practical domains. Recent works, e.g. One-class clustering [3], Bregman Bubble Clustering [6], and Minority Detection [1], have ex-tended the task to identifying clusters of objects with atyp-ical properties. These approaches aim to identify relevant events which exhibit a strong clustering tendency or a con-trasting distribution from the rest of the dataset. These ap-proaches have been effective in several practical problems, e.g., document retrieval and image classification.
In this paper, we analyze the task of identifying atypical objects from an information theoretic perspective. Infor-mation Bottleneck (IB) method [2, 4, 11], has become an important theoretical basis for unsupervised learning. Its framework extends from the Rate Distortion theory and ad-dresses unsupervised learning problems, namely probabilis-tic clustering, with regards to the complexity of clustering.
We first define the atypicalness of a subset based on its distribution using KL divergence. Using IB principle, we formalize the task of identifying such subsets as a cluster-ing of data sources. This formalization derives a cost func-tion which expresses the tradeoff between the atypicalness of the clusters against the simplicity of the clustering. Fur-thermore, it provides a unifying view of the existing ap-proaches including Minority Detection, Outlier Detection, and One-class clustering, as its special cases.

Implicitly, the distribution of the atypical objects is as-sumed to be narrow and bounded. This assumption amounts to the localized form of the cost function, which can be min-imized independently over respective clusters. We exploit such a form in a scalable algorithm which sequentially iden-tifies multiple clusters of atypical objects.

Rest of this paper is organized as follows. Section 2 gives a brief description of the Information Bottleneck method. In Section 3-5, we define the task of identifying atypical ob-jects using Information Bottleneck framework and derive a formal cost function. Section 6 unifies existing approaches with the proposed formalization. Section 7 describes a scal-able algorithm for minimizing the cost function. Section 8 compares the performance of the proposed algorithm to an existing method using simulated datasets and a text classi-fication benchmark. Section 9 discusses the contribution of this paper and gives concluding remarks. Information Bottleneck method [11] extends from Rate Distortion theory and addresses unsupervised learning problems, typically probabilistic clustering, with regards to two criteria in lossy data compression: rate and distortion .
Let X be a random variable taking the observed values and Y be another variable whose values correspond to re-spective clusters. The clustering is expressed in the proba-bilistic assignment of X to Y , denoted by p ( Y | X ) .
Looking at Y as a compressed expression of X , rate refers to the level of compression measured by the mu-tual information I ( Y ; X ) . Meanwhile, distortion refers to the disruption of the data caused by the compression. It corresponds to clustering objectives, such as within-cluster distances. In Information Bottleneck method, distortion is measured using another variable Z , which holds relevant information about X with regards to the task at hand. Three variables are expressed in a Markov process as Y is a function of X and independent of Z given X .
Information Bottleneck method operates to minimize the rate I ( Y ; X ) while maximizing I ( Z ; Y ) , which quantifies the information about Z captured by Y . The task is formal-ized as a minimization with regards to p ( Y | X ) , where  X  denotes the tradeoff between the two criteria.
In (1), I ( Y ; X ) can also be viewed as a penalty for com-plex clustering, similar to the cost of internal representation in Minimum Description Length principle.
Let X be an i.i.d. variable taking observed values X = { x nite set Y = { y i } k i =1 (1  X  k&lt;n ) , where each y i tively represents a cluster.

Denoting the conditional probability of a cluster y i by p ( x | y i ) , the probability density of x is given by
Kullback-Leibler(KL) divergence for two probability distributions q 1 ( x ) and q 2 ( x ) , is defined as
Intuitively, (3) expresses the distance between the two probabilistic distributions. In this study, we measure the atypicalness of a subset based on its distribution, i.e., by KL divergence from the overall distribution of the dataset. Formally, the atypicalness of a cluster y i is expressed as The mutual information between Y and X is defined as Alternatively, (4) can be written as which formally expresses the expected KL divergence be-tween p ( x ) and p ( x | y ) over Y . Intuitively, (5) quantifies the averaged atypicalness among the clusters, thus can be viewed as a natural clustering objective for the task at hand.
Note that adding another value to Y always increases the amount of information captured by Y , therefore the mutual information should be compared for the same number of clusters k = |Y| .
In addition to the observed and hidden variables X and Y , we introduce another variable T to represent the sources of the observed data. Let T take values from T = { 1 , 2 ,...,n } , thus |T| = |X| = n . Since each  X   X  X  corresponds to one observed data, T expresses a modeling assumption that each data comes from an inde-pendent source.

Based on Information Bottleneck principle, we define the task of clustering the sources T into Y = { y 1 ,...,y to form a simplified explanation of the observed data. From above, T is a function of X and Y afunctionof T . Thus, three variables are expressed in a Markov process
Given (6), IB clustering is formalized as
The probabilistic assignment p ( Y | T ) clusters the sources T into Y , which intuitively, expresses a generalized model that assumes fewer sources.

The formalization of (7) interprets as exploring a simpler explanation of the observation while preserving the amount of information obtained from the data. The first term, I ( Y ; T ) , expresses the preference for simplicity, which is pertinent for the addressed setting where the majority of the objects is presumed to be typical , i.e., to have come from a single source. On the other hand, I ( Y ; X ) emphasizes the preservation of information. Unlike the original IB, this formalization uses only one observed variable, X , thus is applicable in standard unsupervised learning settings.
It is naturally assumed that the number of atypical ob-jects is relatively small, and in turn, typical objects are dom-inant in ratio. Moreover, the notions of typicalness and atypicalness imply the different extent to which their prop-erties apply. When two atypical objects are distant from one another, they are considered to have come from different sources, as the distribution of atypical objects are assumed to be local , i.e., constrained to the locality of its objects. On the other hand, we consider the distribution of typical ob-jects to be global , thus consistent over most of the dataset.
The cost function of IB clustering can be simplified un-der above assumptions. Let  X   X  X  denote the source that corresponds to an observation x  X  . We denote by y 1 the clus-ter of typical objects and by y 2 ,...,y k the clusters of atyp-ical objects. We assume Y to be a hard clustering of T , i.e., each  X  is assigned deterministically to a cluster y assignment is denoted by p ( y i |  X  )  X  X  0 , 1 } .
Y is completely determined by T , therefore H ( Y | T )= 0 .From I ( Y ; T )= H ( Y )  X  H ( Y | T ) , the cost function of (7) is rewritten as
The distribution of a cluster of atypical objects that is constrained to the cluster X  X  locality is formally expressed as
Note that (9) does not apply to the distribution of typical objects p ( x | y 1 ) , which commonly is a broad distribution overlapping that of the atypical clusters.

Given the above constraint, the cost function (8) can be divided into contributions from respective clusters.
We denote by  X  i the individual contribution of the clus-ter y i to I ( Y ; X ) , i.e., k i =1  X  i = I ( Y ; X ) and
The contribution of atypical clusters,  X  i =1 , is written as a summation over as p ( x  X  | y i )=0 over X i =  X  from (9).
 We further divide  X  1 into summations over respective X , which we denote by  X  1 ( X i ) , i.e.,
From (9), the summation in is irrelevant over X 1 . Then,  X  1 ( X 1 ) is written as, From (11), (12), and (13), we obtain k i =1  X  i = I ( Y ; X ) in a localized form, In (8), the complexity of the clustering is quantified as
Since the marginal distribution p ( y i ) satisfies entropy H ( Y ) is rewritten as
Changing the order of the summations, (15) can be di-vided into summations over respective X i as well, which we denote by  X ( X i ) .
 From (9), only p ( x, y 1 ) is relevant over X 1 , thus Similarly, only p ( x, y 1 ) and p ( x, y i ) is relevant over X =1 , therefore,  X ( X i )=
From
Substituting (14) and (16) into (8) with  X  =1 , we elim-inate the summation over X 1 and obtain a cost function in a localized form,
In (17), two terms within the summation express the distances between p ( x ) and the relevant joint distributions p ( x, y 1 ) and p ( x, y i ) , respectively.

In practice, a good estimate of p ( x | y 1 ) is usually avail-able, as samples of typical objects are easier to obtain. As such, it is efficient to focus on the local property of the dataset, i.e., the second term, when minimizing (17).
The tradeoff  X  =1 is a typical value. Substituting the value to (8) rewrites the cost as F = H ( Y | X ) . Introduc-ing the constraint (9) directly into F also derives (17).
We now provide a unifying view of the existing methods, including Minority Detection, Outlier Detection, and One-class clustering based on the proposed formalization.
Minority Detection (MD) [1] addresses the problem of finding minority objects whose distribution contrasts signif-icantly with that of the majority of the dataset. The problem is formalized as a clustering problem, where each observa-tion X = { x j } n j =1 is assigned deterministically to either the majority or the minority class, respectively denoted by y and y 2 . The deterministic assignment is expressed by where  X  denotes the Kronecker X  X  delta.

The scarcity of the minority objects is formally expressed as p ( y 2 ) 1 , which leads to the approximation
Subject to (19), Minority Detection employ p ( x |  X   X  ) as an estimate of p ( x ) , where  X  is the natural parameter of the majority distribution and  X   X  denotes the maximum likeli-hood estimate of  X  over X .

Similar to the Information Bottleneck method, Minor-ity Detection formalizes the task with regards to two objec-tives. The first objective is to maximize the atypicalness of the minority cluster measured by the information geometric distance from the majority, written as log p ( x | y 2 ) p ond objective is to minimize the amount of information that minority objects hold, quantified by log p ( y 2 ) .
The cost function is defined as a combination of above measures averaged over minority objects, where X 2 = { x  X  X  : p ( y 2 | x )=1 } denotes the set of mi-nority objects.

Comparing F MD to the proposed formalization (17) for k =2 , which writes as we obtain the following proposition.
 Proposition 1. Minority Detection is a special case of the proposed formalization (21) under the assumptions (18) and (19) .
 Due to the limited space, we outline the proof as follows. Proof. From their respective definitions, X 2 in (20) and (21) denote the same subset.

Given (19), the first term in the summation of F 2 L is ir-relevant to the minimization thus negligible.

Considering the hard assignment (18) and that p ( x |  X   X  is an estimate of p ( x ) , it is apparent that the second term of the summation is in equivalent form as (20).

Above proposition is easily generalized for k&gt; 2 ,in which case the cost function becomes
An outlier is an object for which one can reject the hy-pothesis that it comes from the same distribution as the ma-jority of the dataset [8]. The decision function for classi-fying an outlier is defined by the likelihood of an object, given a generative model of the data and a preferred confi-dence level. For a normally distributed variable, outliers are objects with Mahalanobis distance above a given threshold.
One-class classification [9] transfers the problem of de-tecting outliers to a quadratic program solved by Support Vector Machine. [9] also describes a classification of out-liers using a ball , as a special case of One-class classifica-tion. The ball refers to a hypersphere which separates  X n objects from the rest of the data with the largest margin.  X  is the maximum ratio of outliers in the dataset of n objects.
Given a kernel function  X  and  X  c , the mean vector of objects within the ball, defines a unique probability density function. Z is a normal-izer. Using (23), the problem of classifying outliers with a ball is formalized as a minimization of the cost function Minimizing (24) is a dual problem of Outlier Detection. For a given confidence level, there exists a maximum num-ber of objects  X  max which can be labeled as outliers. Mini-mizing F OB for  X  max yields the same set of outliers as Out-lier Detection with the corresponding confidence level. Proposition 2. Minimization of F OB is a special case of Minority Detection for k =  X n , where the cluster size is constrained to 1, and the conditional distribution of a clus-ter y i is described by narrow, identical kernel density func-tions p  X  and an object x ( i ) , s.t. The proof is outlined as follows.
 Proof. Given the constraint on p  X  and (18), { x ( i ) } is the unique cluster member of y i , i.e., X i = { x ( i ) } . Therefore, marginal distributions { p ( y i ) } and conditional distribution { p ( x | y i ) } are constant and identical for all clusters. In such case, only p ( x |  X   X  ) in (22) is relevant to the minimization. Considering p ( x |  X   X  ) is an estimate of the majority objects thus can be expressed as p  X  ( x,  X  1 ) with a corresponding kernel function  X  and a mean vector  X  1 , it is apparent that F
MD takes the same form as (24).
One-class clustering (OCC) [3] is an interesting ap-proach to identifying a small, coherent set of objects which forms a dense region in the data. In practical problems, such a coherent set of objects often holds relevant informa-tion about the dataset. This approach has been effective in gene expression clustering and document retrieval [3, 5].
OCC operates to minimize the sum of Bregman diver-gence [3] between the cluster center and its members. Let D  X  denote a class of Bregman divergence, ter, and  X  C the cluster center. The task is defined as a mini-mization of the cost function
Bregman Bubble Clustering (BBC) [6] is an extension of OCC for multiple clusters. In BBC, objects without clus-tering tendencies are described by a sparse background dis-tribution. Let Y be a hidden variable taking values from 0 to k . 0 corresponds to the background distribution and the rest of the values correspond to k mixture components, re-spectively. { p ( x | y ): y =1 ,...,k } denotes the conditional probabilities of respective components. p 0 denotes a sparse, uniform background distribution, whose parameter is fixed at an initial estimate and not subject to optimization.
A Bregman divergence D  X  uniquely defines an exponen-tial distribution p  X  , where  X  denotes the natural parameter of the exponential distribution and Z the normalization function.

Representing p ( x | y ) by p ( x,  X  ) , [6] showed that the minimization of F OC is equivalent to maximizing the ex-pected log likelihood, where p ( y | x ) denotes the deterministic assignment of Y to X as described in (18).

From (27), considering p 0 is constant over X thus irrel-evant to the optimization, the cost function is rewritten as where C y = { x  X  X  : p ( y | x )=1 } denotes each set of clustered objects.
 Proposition 3. Minimizing F BBC is a special case of Mi-nority Detection where the majority is described by a uni-form distribution. Proof. The proof follows directly from p ( x |  X   X  ) being a uniform distribution, thus constant over X .Removing p ( x |  X   X  ) from (22), it is apparent that F k MD takes an equiv-alent form as (28).
As discussed in [3], One-class classification and One-class clustering define atypicalness based on two contrast-ing views. As respective cost functions, (24) and (28), show, the definition of the former is subject to the global property of the dataset while the latter reflects the distribution of a local, relatively small subset.

The cost function of MD (22) is rewritten as
It is apparent that (29) combines the costs of above two approaches (24) and (28). Thus, the definition of minority objects can be viewed as a generalization of those of out-liers and Bregman bubble clusters. That is, minority objects seen as isolated events are outliers and a set of minority objects among a sparse, uniform majority distribution is a Bregman bubble cluster. Unlike BBC however, MD is ap-plicable when the background is dense or a Gaussian.
Information Bottleneck method provides a formal algo-rithm which alternatingly updates marginal and conditional distributions and converges at a local minimum of the cost function [11]. Each distribution is updated so as to mini-mize the cost function while the others are fixed. Since (7) follows from the same formalization, the principle of the algorithm applies to the minimization of its special cases.
In Both MD and BBC, the majority/background distri-bution is fixed to an initial estimate. As a result, the cost function is subject to the local properties that can be com-puted independently over individual clusters.
We first focus on minimizing the individual contributions of the clusters to the cost function, which we denote by L where  X  i is the natural parameter of p ( x | y i ) .
Similar to the formal IB algorithm [11], three distribu-using following equations at each iterative step t . where  X   X  = arg max mined from p t +1 ( y i |  X  ) by (10). We provide that Proposition 4. Alternating iterations of (31) -(33) con-verges at a local minimum of (22) .
 The proof is outlined as follows.
 Proof. It follows from p ( x | y i ) &lt; 1 and p ( x |  X  the first logarithm in (30) is lower-bounded. From p ( y i 1 , the second logarithm is non-negative and therefore L i lower-bounded as well. Then, it suffices to verify that (31), (32), and (33) respectively decrease L i .

Assume that at step t , X i is larger than the actual set of atypical objects. Given the definition of X i , (31) effectively removes objects with positive likelihood ratio log p ( x | from X i , therefore decreases L i .

With a smaller cluster size, (32) decreases log 1 p ( y ) therefore decreases L i as well. Note that p (  X  )= 1 |T| observations are of equivalent significance. (33) computes the maximum likelihood estimate of  X  over X i . Looking at (30), p t +1 ( x | y ) which minimizes L with fixed p t +1 ( y ) and p ( x |  X   X  ) is clearly the maximum likelihood estimate p ( x |  X   X  ) .

Since each update decreases L i and the number of hard clustering p ( y i |T ) is finite, the algorithm will converge at a local minimum of L i after finite iterations.

Algorithm 1 shows the pseudo code of the iterative pro-cedure, given an initial assignment p 0 ( y i |T ) . From the out-put, atypical objects are identified as { x  X  : p ( y 2 | In practice, iterations for trivial clusters smaller than the threshold s min is omitted.
 We further provide that Proposition 5. If the cluster X i consists entirely of typical objects, Algorithm 1 converges to a solution with smallest possible cluster size.
 The proof is outlined as follows.
 Proof. Consider X i , which consists only of typical objects, and its subset X i = X i  X  x . Since x is a typical object and adding another typical object to the cluster always reduces the distance between the distributions p ( x |  X   X  ) and p ( x L ( X i , X  i ) &lt;L i ( X i , X  i ) .

From above, it is induced that the local minimum of L i is achieved at the smallest cluster size. Algorithm 1 Minimization of Individual Clusters
INPUT: Observed variable X = { x j } n j =1 , majority parameter  X   X  , initial assignment p 0 ( y i |T ) OUTPUT:  X  , p ( y |T )
METHOD: initialize  X  = arg max repeat until F MD converges return  X  , { p ( y |  X  ) } n  X  =1
Above proof conveys that the algorithm is robust against bad local optima, which is a substantial benefit when detect-ing a relatively small number of objects.

As seen in Algorithm 1, the computational complexity of ing p ( x | y i ) requires a maximum likelihood estimation over X . For many models where the sufficient statistics of the data can be updated incrementally, e.g. exponential fami-lies, the computational order of such procedure is constant. Therefore, the computational complexity of each iterative step in Algorithm 1 is O ( |X i | ) . 7.2 k -Minority Detection Algorithm
In (9), atypical clusters are assumed to be mutually in-dependent and non-overlapping. As such, Algorithm 1 can be reapplied to find other clusters after excluding previously clustered objects from the dataset.

Denoting Algorithm 1 as a function MD (  X  ) , the steps for identifying multiple clusters is described as follows. 1. Initialize  X =  X  , i =1 , define T = {  X  : x  X   X  X } 2. Prepare an initial assignment p 0 ( y i |T ) and compute 3. if T p ( y i |  X  ) &gt;s min , remove { x  X  : p ( y i | 4. if i&lt;i max , increment i and repeat from step 2 Algorithm 2 shows the pseudo code of the procedure. i max denotes the maximum number of initialization in a run.
Notably, Algorithm 2 does not require the mixture size k as an input. But unlike the traditional information criteria Algorithm 2 k -Minority Detection ( k MD) Algorithm
INPUT: observation X = { x i } n i =1 , majority parameter  X   X  , max. initialization count t max , min. cluster size s OUTPUT: assignment p ( Y|T ) , parameter set  X = {  X  i } METHOD: Initialize i =1 , X = X ,  X =  X 
Define T = {  X  : x  X   X  X  } repeat until t&gt;t max return p ( Y|T ) ,  X  principle, it does not compare between different values of k . Rather, k = |  X  | +1 is assumed at each step. Then, after i max trials, it is estimated that the k th cluster does not exist from Proposition 5. Intuitively, this allows for a parallel im-plementation of the algorithm, with non-overlapping initial clusters and independent update of each cluster.
We prepare datasets consisting of typical and atypical objects. Atypical objects are 100, 150, 150, and 200 points from different Gaussians of full-ranked covariance matri-ces. Typical objects are prepared in two types. The first con-sists of uniformly distributed points over [  X  1:1] d  X   X  d . d denotes the dimension of the domain which are 2 , 3 , 5 , 8 . There are 1000  X  2 d points, thus is not sparse in larger dimensions. The second consists of 10,000 points from a Gaussian with 0 mean, variance/covariances of 1 and 0. 8.1.1 Graphical Verification For a 2-D dataset, graphical verification is an intuitive and reliable validation of clustering. Figure 1 plots the dataset with uniformly distributed typical objects, denoted by UG2. Gray  X  s denote the typical objects and + s denote the atypi-cal objects. Figure 2 illustrates a typical clustering result of the k MD algorithm for UG2.  X  s denote the objects identi-fied as atypical and  X  s denotes the rest of the dataset.
Figure 3 shows a dataset with Gaussian typical objects, denoted by GG2.  X  s denote the typical objects and + s de-note the atypical objects. Figure 4 illustrates the typical clustering result for GG2. Dark + s denote the objects iden-tified as atypical and gray  X  s denote the rest of the dataset.
As Figures 1-4 show, the atypical clusters correspond very well with the original Gaussians. 8.1.2 Classification Performance Discriminating atypical objects from typical objects can be seen as a classification problem. In this section, we com-pare classification performances of the k MD and a BBC al-gorithm using simulated datasets from Section 8.1. Precision and recall is defined as follows.
 TP , FP , and TN denote the number of true positives, true negatives, and false positives.

The high density of typical objects accounts for very low precision. To offset the difference in density among the datasets, we also compute a relative precision Pr rel as fol-lows. First, we define a smallest ellipsoid which encloses respective sets of atypical objects using the eigenvectors of the original Gaussians. Using such an ellipsoid as the class boundary, we compute the base precision Pr base . Since the typical objects are densely distributed, the Pr base is much smaller than 1. More specifically, the ratio of the atypical objects within two standard deviations of the Gaussians are less than 0.1 for all datasets. For each classification result, relative precision is defined as Pr rel = Pr/Pr base . These measures are computed collectively for all atypical objects, i.e., as in two-class classification problems.

Among several BBC algorithms [6], BBC-Press is em-ployed in this experiment. It employs a scheduling scheme which gradually reduce the cluster size to avoid local op-tima. The convergence of the algorithm is checked after the cluster reaches a specified size s = 200 . The initial size of the clusters is set to s 0 = 100  X  2 d and the cluster size is decremented by 1 at each step. We set k =4 and repeat five iterations with different initial parameters. Four clusters with lowest costs are labeled as atypical objects.
The parameters of the k MD algorithm are: i max =10  X  2 d  X  1) and s min =50 . The clusters are initialized with s = 100  X  2 d points. Four clusters with lowest costs are labeled as atypical objects. k MD uses Gaussians as generative models and BBC em-ploys Mahalanobis distance as the divergence measure. The performance is averaged over 20 datasets with uniformly distributed typical objects. For each dataset, atypical ob-jects are generated by a different set of Gaussians. Tables 1 and 2 show the performance of the k MD and BBC algorithms, respectively. The performance of k MD is consistent in all tested dimensions. The relative precision is consistently higher than 1, indicating that k MD slightly fa-vors precision over recall. As atypical objects accounts for less than 0.2 % of the 8-D dataset, its result also indicates k MD X  X  feasibility for very large datasets.

Table 2 shows that the precision and recall of BBC is high in variance and significantly lower than that of k MD. This results from frequent convergence at bad local optima, in which case precision and recall is 0. As reported in [6], bad local optima are problematic for OCC and BBC even with a sparse background distribution. BBC X  X  performance worsens at larger d , which may be attributed to the increased number of local optima with a larger dataset.
Reuters 21578 corpus 1 is a a popular benchmark for text classification. It consists of 10,789 documents, each of which is assigned one or more categories. We prepared a dataset for the experiment as follows. Five topics are cho-sen: one typical topic, acq , and four atypical topics, oilseed , money-supply , sugar , and gnp . The number of documents with respective topics are 2448, 192, 190, 184, and 163. The documents are merged into a single test dataset D 1 .
Given a vocabulary W = { w i } k i =1 , a document x is rep-resented by the frequency of their occurrence v i ,
The selection scheme for vocabulary W follows from [10]. k words with largest contribution to I ( W ; X ) , i.e., are chosen. For the following experiment, we used the vo-cabulary size of k = 200 .

Using multinomial distribution as the generative model, the conditional probability of x given a topic y is where V = k i =1 v i is the total occurrence of the words in the document. We denote the labels of typical and atypical topics by q and r , i.e., y  X  X  q, r } . The conditional proba-bilities p ( w j | q ) and p ( w j | r ) are given by p ( w j | q )= where Z i is the subset of X that is assigned the label r . k MD was applied to D 1 with i max =20 . An initial clus-ter consists of 800 documents with largest likelihoods based on the word frequency of a randomly selected document. We apply BBC-Press using the same initial cluster size. It checks for convergence after the cluster size reaches s = 200 . The distance between texts is measured by KL di-vergence between their multinomial distributions. We per-form five iterations with k =4 , therefore execute as many iterations as k MD with different initial clusters.
The precision and recall of four clusters with lowest costs are computed. Each cluster is labeled with the most dominant topic other than the majority topic, acq ,inthe cluster. A similar scheme has been suggested in [10].
Table 3 shows the two algorithms X  precision and recall for respective topics. The classification performance of k MD is excellent in all topics, and comparable to that of supervised classification methods.
Meanwhile, a substantial number of BBC X  X  iterations converges to a set of majority topic documents, which yields very poor precision and recall. We also note that its preci-sion depends heavily on the choice of s , as BBC-Press con-verges as soon as the cluster size reaches s . This suggests that s needs to be very close to the actual number of minor-ity topic documents for BBC to achieve a good precision.
In this paper, we presented a general formalization for identifying atypical objects based on Information Bottle-neck principle. Additionally, a sequential algorithm to iden-tify multiple clusters of atypical objects was shown.
Empirical results show that the proposed algorithm achieves a good performance over difficult classification tasks. In contrast to the compared method, it displays ro-bustness against bad local optima, yielding very few false positives. This can be attributed to the emphasis on smaller clusters by the IB principle, which helps to eliminate trivial clusters that do not merit the gain in informational cost. As such, fixing the cluster size, therefore p ( y i ) , as in OCC and BBC may void the emphasis on simplicity and increase the chance of being trapped in bad local optima.

Unlike traditional information criteria, penalty on com-plex clustering in IB is effective even for a specified mix-ture size k . This contributed to a novel selection scheme of k ; since trivial solutions are suppressed by the penalty, the presence of k th cluster is empirically denied after a speci-fied number of runs. Removing the mixture size from the the algorithm input is a practical advantage, as properties of atypical objects are generally unavailable.

Moreover, IB formalization (7) presented an interesting interpretation of an unsupervised learning task. Taking val-ues {  X  } n j =1 that corresponds bijectively with the observa-tions, T presents a very detailed modeling assumption in which each data comes from an independent source. This assumption is similar in concept with the non-parametric density estimation. It is interesting however, that p ( x which would correspond to the kernel density function in non-parametric density estimation, does not appear in the algorithm therefore need not be parameterized.

In an extreme case where the cluster size is one, the func-tion of p ( x | y ) is analogous to a needle , since its distribution is limited to the locality of an object. As such, the proposed method is a clustering of a handful of such needles among a broad distribution of typical objects, for creating a simple explanation of the deviation in the dataset.

In future works, we plan to explore new applications based on the IB interpretation of the unsupervised learn-ing task. For knowledge discovery purposes, the clusters of atypical objects with assessable statistical properties can be much more useful than as isolated events. In addition, the value of tradeoff  X  should be further discussed. The prefer-ence for precision/recall can be adjusted by modifying  X  .
