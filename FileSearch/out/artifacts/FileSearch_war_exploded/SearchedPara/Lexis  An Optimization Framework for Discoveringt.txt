 Data represented as strings abounds in biology, linguistics, document mining, web search and many other fields. Such data often have a hierarchical structure, either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary pro-cess that creates repeatedly more complex strings from sim-pler substrings. We propose a framework, referred to as Lexis , that produces an optimized hierarchical representa-tion of a given set of  X  X arget X  strings. The resulting hi-erarchy,  X  X exis-DAG X , shows how to construct each target through the concatenation of intermediate substrings, min-imizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-hardness for two cost formulations, we propose an efficient greedy algorithm for the construction of Lexis-DAGs. We also con-sider the problem of identifying the set of intermediate nodes (substrings) that collectively form the  X  X ore X  of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse ap-plications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents.
In both nature and technology, information is often rep-resented in sequential form, as strings of characters from a given alphabet [11]. Such data often exhibit a hierarchical structure in which previously constructed strings are re-used in composing longer strings [21]. In some cases this hierar-chy is formed  X  X y design X  in synthetic processes where there are some cost savings associated with the re-use of exist-ing modules [15, 19]. In other cases, the hierarchy emerges naturally when there is an underlying evolutionary process that repeatedly creates more complex strings from simpler ones, conserving only those that are being re-used [19, 21]. For instance, language is hierarchically organized starting from phonemes to stems, words, compound words, phrases, and so on [20]. In the biological world, genetic information is also represented sequentially and there is ample evidence that evolution has led to a hierarchical structure in which se-quences of DNA bases are first translated into amino acids, then form motifs, regions, domains, and this process contin-ues to create many thousands of distinct proteins [8].
In the context of synthetic design, an important problem is to construct a minimum-cost Directed Acyclic Graph (DAG) that shows how to produce a given set of  X  X arget strings X  from a given  X  X lphabet X  in a hierarchical manner, through the construction of intermediate substrings that are re-used in at least two higher-level strings. The cost of a DAG should be related somehow to the amount of  X  X oncatenation work X  (to be defined more precisely in the next setion) that the corresponding hierarchy would require. For instance, in de novo DNA synthesis [4, 7], biologists aim to construct tar-get DNA sequences by concatenating previously synthesized DNAs in the most cost-efficient manner.

In other contexts, it may be that the target strings were previously constructed through an evolutionary process (not necessarily biological), or that the synthetic process that was followed to create the targets is unknown. Our main premise is that even in those cases it is still useful to construct a cost-minimizing DAG that composes the given set of targets hier-archically, through the creation of intermediate substrings. The resulting DAG shows the most parsimonious way to rep-resent the given targets hierarchically, revealing substrings of different lengths that are highly re-used in the targets and identifying the dependencies between the re-used substrings. Even though it would not be possible to prove that the given targets were actually constructed through the inferred DAG, this optimized DAG can be thought of as a plausible hypoth-esis for the unknown process that created the given targets as long as we have reasons to believe that that process cares to minimize, even heuristically, the same cost function that the DAG optimization considers. Additionally, even if our goal is not to reverse engineer the process that generated the given targets, the derived DAG can have practical value in the applications such as compression or feature extraction.
In this paper, we propose an optimization framework, re-ferred to as Lexis , 1 that designs a minimum-cost hierarchi-cal representation of a given set of target strings. The re-sulting hierarchy, referred to as  X  X exis-DAG X , shows how to construct each target through the concatenation of inter-mediate substrings, which themselves might be the result
Lexis means  X  X ord X  in Greek. of concatenation of other shorter substrings, all the way to a given alphabet of elementary symbols. We consider two cost functions: minimizing the total number of concatena-tions and minimizing the number of DAG edges. The choice of cost function is application-specific. The Lexis optimiza-tion problem is related to the smallest grammar problem [6, 14]. We show that Lexis is NP-hard for both cost functions, and propose an efficient greedy algorithm for the construc-tion of Lexis-DAGs. Interestingly, the same algorithm can be used for both cost functions. We also consider the prob-lem of identifying the set of intermediate nodes (substrings) that collectively form the  X  X ore X  of a Lexis-DAG. This is the minimal set of DAG nodes that can cover a given fraction of source-to-target paths, from alphabet symbols to target strings. The core of a Lexis-DAG represents the most cen-tral substrings in the corresponding hierarchy. We show that the Lexis framework can be applied in diverse appli-cations such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents.
Given an alphabet S and a set of  X  X arget X  strings T over the alphabet S , we need to construct a Lexis-DAG. A Lexis-DAG D is a directed acyclic graph D ( V,E ), where V is the set of nodes and E the set of edges, that satisfies the following three constraints. 2 First, each node v  X  V in a Lexis-DAG represents a string S ( v ) of characters from the alphabet S . The nodes V S represent characters of S are referred to as sources , and they have zero in-degree. The nodes V T that represent target strings T = { t 1 ,t 2 ,...,t m } are referred to as targets , and they have zero out-degree. V also includes a set of interme-diate nodes V M , which represent substrings that appear in the targets T . So, V = V S  X  V M  X  V T .

Second, each node in V M  X  V T of a Lexis-DAG represents a string that is the concatenation of two or more substrings, specified by the incoming edges from other nodes to that node. Specifically, an edge e  X  E from node u to node v is a triplet ( u,v,i ) such that the string S ( u ) appears as substring of S ( v ) at index i (the first character of a string has index 1). Note that there may be more than one edges from node u to node v . The number of incoming and outgoing edges for v is denoted by d in ( v ) and d out ( v ), respectively. I ( v ) is the sequence of nodes u that appear in the incoming edges ( u,v,i ) of v , ordered by edge index i . We require that for each node v in V M  X  V T replacing the sequence of nodes in I ( v ) with their corresponding strings results in exactly S ( v ).
Third, a Lexis-DAG should only include intermediate nodes that have an out-degree of at least two, In other words, every intermediate node v  X  V M in a Lexis-DAG should be such that the string S ( v ) is re-used in at least two concatenation operations. Otherwise, S ( v ) is either not used in any concatenation operation, or it is used only once and so the outgoing edge from v can be replaced by re-wiring
To simplify the notation, even though D is a function of S and T , we do not denote it as such. Figure 1: Illustration of the Lexis-DAG for targets T = the incoming edges of v straight to the single occurrence of S ( v ). In both cases node v can be removed from the Lexis-DAG, resulting in a more parsimonious hierarchical representation of the targets. Fig. 1 illustrates the concepts introduced in this section.
The Lexis optimization problem is to construct a minimum-cost Lexis-DAG for the given alphabet S and target strings T . In other words, the problem is to determine the set of intermediate nodes V M and all required edges E so that the corresponding Lexis-DAG D is optimal in terms of a given cost function C ( D ).
The selection of an appropriate cost function is somewhat application-specific. A natural cost function to consider is the number of edges in the Lexis-DAG. In certain applica-tions, such as DNA synthesis, the cost is usually measured in terms of the number of required concatenation operations. In the following, we consider both cost functions. Note that we choose to not explicitly minimize the number of inter-mediate nodes in V M ; minimizing the number of edges or concatenations, however, tends to also reduce the number of required intermediate nodes. Additionally, the constraint (1) means that the optimal Lexis-DAG will not have redun-dant intermediate nodes that can be easily removed without increasing the concatenation or edge cost. More general cost formulations, such as a variable edge cost or a weighted av-erage of a node cost and an edge cost, are interesting but they are not pursued in this paper.
Suppose that the cost of each edge is one. The edge cost to construct a node v  X  V is defined as the number of incoming edges required to construct S ( v ) from its in-neighbors, which is equal to d in ( v ). The edge cost of source nodes is obviously zero. The edge cost E ( D ) of Lexis-DAG D is defined as the edge cost of all nodes, equal to the number of edges in D : Figure 2: Illustration of G-Lexis given target T =
Suppose that the cost of each concatenation operation is one. The concatenation cost to construct a node v  X  V M  X  V is defined as the number of concatenations required to con-struct S ( v ) from its in-neighbors, which is equal to d The concatenation cost C ( D ) of Lexis-DAG D is defined as the concatenation cost of all non-source nodes; it is easy to see that this is equal to the number of edges in D minus the number of non-source nodes,
C ( D ) = X
Theorem 1. The optimization problem in Eq. (2) is NP-hard for both the edge cost of Eq. (3) and the concatenation cost of Eq. (4) .
 The proof is given in the Appendix. Note that the objective in Eq. (4) is an explicit function of the number of interme-diate nodes in the Lexis-DAG. Hence the optimal solutions for the concatenation cost can be different than those for the edge cost. An example is shown in the Appendix.
In this section, we describe a greedy algorithm, referred to as
G-Lexis , for both previous optimization problems. The basic idea in G-Lexis is that it searches for the substring  X  that will lead, under certain assumptions, to the maximum cost reduction when added as a new intermediate node in the Lexis-DAG. The algorithm starts from the trivial Lexis-DAG with no intermediate nodes and edges from the source nodes representing alphabet symbols to each of their occur-rences in the target strings.

Recall that for every node v  X  V T  X  V M , I ( v ) is the se-quence of nodes appearing in the incoming edges of v , i.e., the sequence of nodes whose string concatenation results in the string S ( v ) represented by v . The sequences I ( v ) can be interpreted as strings over the  X  X lphabet X  of Lexis-DAG nodes. Note that every symbol in a string I ( v ) has a cor-responding edge in the Lexis-DAG. We look for a repeated substring  X  in the strings I T  X  M = { I ( v ) | v  X  V T  X  V can be used to construct a new intermediate node. We can construct a new intermediate node for  X  , create incoming edges based on the symbols in  X  (remember  X  is a substring over the alphabet of nodes), and replace the incoming edges to each of the non-overlapping repeated occurrences of  X  with a single outgoing edge from the new node.
 Consider the edge cost first. Suppose that  X  is repeated R
T  X  M, X  times in the strings I T  X  M . If these occurrences of  X  are non-overlapping, the number of required edges would be |  X  | R T  X  M, X  . After we construct a new intermediate node for  X  as outlined above, the edge cost will be |  X  | + R T  X  M, X  reduction in edge cost from re-using  X  would be ( R T  X  M, X  1)( |  X  | X  1)  X  1. Under the stated assumptions about  X  , this reduction is non-negative if  X  is repeated at least twice and its length is at least two.

Consider the concatenation cost now. If these occurrences of  X  are non-overlapping, the number of required concate-nations for all the repeated occurrences would be ( |  X  |  X  1) R T  X  M, X  . After we construct a new intermediate node for  X  as outlined above, the concatenation cost will be |  X  | X  1. We expect a reduction in the number of required concatenations by ( R T  X  M, X   X  1)( |  X  | X  1).

So, the greedy choice for both cost functions is the same: select the substring  X  that maximizes the term SavedCost = ( R
T  X  M, X   X  1)( |  X  | X  1). For this reason, our G-Lexis algo-rithm can be used for both cost functions we consider. It starts with the trivial Lexis-DAG, and at each iteration it chooses a substring of I T  X  M in the Lexis-DAG that maxi-mizes SavedCost , creates a new intermediate node for that substring and updates the edges of the Lexis-DAG accord-ingly. The algorithm terminates when there are no more substrings of I T  X  M with length at least two and repeated at least twice. The pseudocode for G-Lexis is shown in Algorithm 1. An example of application of the G-Lexis algorithm is shown in Fig. 2.
 Algorithm 1. G-Lexis
At each iteration of G-Lexis , we need to find efficiently the substring of I T  X  M with maximum SavedCost . We ob-serve that the substring that maximizes SavedCost is a  X  X ax-imal repeat X . Maximal repeats are substrings of length at least two, whose extension to the right or left would reduce its occurrences in the given set of strings. Suppose that it is not. Then, there is a substring  X   X  , which is not a maximal repeat, that maximizes SavedCost . If we can extend  X   X  to the left or right we can increase its length without reducing its number of occurrences. By doing so, we construct a new substring with higher SavedCost than  X   X  , violating our initial assumption. So, the substring that maximizes SavedCost is a maximal repeat. A suffix tree over a set of input strings captures all right-maximal repeats, and right-maximal re-peats are a superset of all maximal repeats [11]. To pick the one with maximum SavedCost , we need the count of non-overlapping occurrences of these substrings. A Minimal Augmented Suffix Tree [5] over I T  X  M can be constructed and used to count the number of non-overlapping occurrences of all right-maximal repeats in overall O ( L log L ) time, where L is the total length of target strings. Using a regular suf-fix tree instead, this can be achieved in only O ( L ) time; but suffix tree may count overlapping occurrences. In our implementation we prefer to use regular suffix tree, follow-ing related work [10] that has shown that this performance optimization has negligible impact on the solution X  X  qual-ity. So, the substring that is chosen for the new Lexis-DAG node is based on length and overlapping occurrence count. We then use the suffix tree to iterate over all occurrences of the selected substring, skipping overlapping occurrences. If a selected substring has less than two non-overlapping oc-currences, we skip to the next best substring. Using the suffix tree, we can update the Lexis-DAG with the new in-termediate node, and with the corresponding edges for all occurrences of that substring, in O ( L ) time. The maximum number of iterations of G-Lexis is O ( L ) because each itera-tion reduces the number of edges (or concatenations), which at the start is O ( L ). So, the overall run-time complexity using suffix tree is O ( L 2 ).

We have also experimented with other algorithms, such as a greedy heuristic that selects the longest repeat in each iteration of building the DAG, i.e., it chooses based on length among all substrings that appear at least twice in the targets or intermediate node strings. This heuristic can be efficiently implemented to run in only O ( L ) time [13]. Our evaluation shows that G-Lexis performs significantly better than the longest repeat heuristic in terms of solution quality, despite some running time overhead. Running both algorithms on a machine with an Intel Core-i7 2.9 GHz CPU and 16GB of RAM on the NSF abstracts dataset (introduced in Section 5) of 2 , 309 target strings with total length 245 , 968 symbols takes 562 sec for G-Lexis and 408 sec for the longest repeat algorithm. The edge cost with G-Lexis is 169,060 compared to 183,961 with the longest repeat algorithm. More detailed results can be found in the Appendix.
After constructing a Lexis-DAG, an important question is to rank the constructed intermediate nodes in terms of sig-nificance or centrality. Even though there are many related metrics in the network analysis literature, such as closeness, betweenness or eigenvector centrality [22], none of them cap-tures well the semantics of a Lexis-DAG. In a Lexis-DAG, a path that starts from a source and terminates at a target represents a dependency chain in which each node depends on all previous nodes in that path. So, the higher the num-ber of such source-to-target paths traversing an intermediate node v is, the more important v is in terms of the number of dependency chains it participates in. More formally, let P
D ( v ) be the number of source-to-target paths that traverse node v  X  V M ; we refer to P D ( v ) as the path centrality of intermediate node v . The path centrality of sources and targets is zero by definition. First, note that: where P S ( v ) is the number of paths from any source to v , and P T ( v ) is the number of paths from v to any target. This suggests an efficient way to calculate the path centrality of all nodes in a Lexis-DAG in O ( | E | ) time: perform two DFS traversals, one starting from sources and following the direction of edges, and another starting from targets and following the opposite direction. The first DFS traversal will recursively produce P S ( v ) while the second will produce P ( v ), for all intermediate nodes.

Second, it is easy to see that P T ( v ) is equal to the number of times string S ( v ) is used for replacement in the target strings T . Similarly, P S ( v ) is equal to the number of times any source node is repeated in S ( v ), which is simply the length of S ( v ). So, the path centrality P ( v ) of a node in a Lexis-DAG can be also interpreted as its  X  X e-use count X  (or number of replaced occurrences in the targets) times its length. Thus, an intermediate node will rank highly in terms of path centrality if it is both long and frequently re-used. An important follow-up question is to identify the core of a Lexis-DAG, i.e., a set of intermediate nodes that represent, as a whole, the most important substrings in that Lexis-DAG. Intuitively, we expect that the core should include nodes of high path centrality, and that almost all source-to-target dependency chains of the Lexis-DAG should traverse at least one of these core nodes.

More formally, suppose K is a set of intermediate nodes and P  X  ( K ) is the set of source-to-target paths after we re-move the nodes in K from D . The core of D is defined as the minimum-cardinality set of intermediate nodes  X  K such that the fraction of remaining source-to-target paths after the removal of  X  K is at most  X  : where | P  X  (  X  ) | is the number of source-to-target paths in the original Lexis-DAG, without removing any nodes. 3
Note that if  X  = 0 the core identification problem becomes equivalent to finding the min-vertex-cut of the given Lexis-DAG. In practice a Lexis-DAG often includes some tendril-like source-to-target paths traversing a small number of in-termediate nodes that very few other paths traverse. These paths can cause a large increase in the size of the core. For this reason, we prefer to consider the case of a positive, but potentially small, value of the threshold  X  .

We solve the core identification problem with a greedy al-gorithm referred to as G-Core . This algorithm adds in each iteration the node with the highest path-centrality value to the core set, updates the Lexis-DAG by removing that node and its edges, and recomputes the path centralities before the next iteration. The algorithm terminates when the de-sired fraction of source-to-target paths has been achieved. G-Core requires at most O ( | V | ) iterations, and in each it-
It is easy to see that | P  X  (  X  ) | is equal to the cumulative length of all target strings L . eration we update the path centralities in O ( | E | ) time. So the run-time complexity of G-Core is O ( | V || E | ).
We now discuss a variety of applications of the proposed framework. Note that in all experiments, we use the library from [10] for extracting the maximal repeats and NetworkX [12] as the graph library in our implementation.
Lexis can be used as an optimization tool for the hierarchi-cal synthesis of sequences. One such application comes from synthetic biology, where novel DNA sequences are created by concatenating existing DNA sequences in a hierarchical process [7]. The cost of DNA synthesis is considerable today due to the biochemical operations that are required to per-form this  X  X enetic merging X  [7, 4]. Hence, it is desirable to re-use existing DNA sequences, and more generally, to per-form this design process in an efficient hierarchical manner.
Biologists have created a library of synthetic DNA se-quences, referred to as iGEM [2]. Currently, there are 787 elementary  X  X ioBrick parts X  from which longer composite sequences can be created. Longer sequences are submitted to the Registry of Standard Biological Parts in the annual iGEM competition, then functionally evaluated and labeled. In the following, we analyze a subset of the iGEM dataset. In particular, this dataset contains 1 , 375 composite DNA sequences that are labeled as iGEM devices because they have distinct biological functions; we treat these sequences as Lexis targets. The cumulative length of the target se-quences is 6 , 957 symbols. The 787 elementary BioBrick parts are treated as the Lexis sources. The iGEM dataset also includes other BioBrick parts that are neither devices nor elementary, and that have been used to construct more complex parts in iGEM; we ignore those because they do not have a distinct biological function (i.e., they should not be viewed as targets but as intermediate sequences that dif-ferent teams of biologists have previously constructed).
We constructed an optimized Lexis-DAG for the given sets of iGEM sources and targets. To quantify the gain that re-sults from using a hierarchical synthesis process, we compare the number of edges and concatenations in the Lexis-DAG versus a flat synthesis process in which each target is inde-pendently constructed from the required sources. The Lexis solution requires only 52% of the edges (or 56% of the con-catenations) that the flat process would require. The se-quence with the highest path centrality in the Lexis-DAG is B0010-B0012 . 4 This part is registered as B0015 in the iGEM library and it is the most common  X  X erminator X  in iGEM devices. Lexis identified several more high central-ity parts that are already in iGEM, such as B0032-E0040-B0010-B0012 , registered as E0240 . Interestingly, however, the Lexis-DAG also includes some high centrality parts that have not been registered in iGEM yet, such as B0034-C0062-B0010-B0012-R0062 . A list of the top-15 nodes in terms of path centrality is given in the Appendix.

To explore the hierarchical nature of the iGEM sequences, we compared the  X  X riginal X  Lexis-DAG, the one we con-structed with the actual iGEM devices as targets, with  X  X an-domized X  Lexis-DAGs.  X  X andomized X  Lexis-DAG is the re-sult of applying G-Lexis to a target set where each iGEM
BioBricks start with BBa prefix that are omitted here. Figure 3: Comparison of the Lexis-DAGs that result from the device sequence is randomly reshuffled. We compare the Original Lexis-DAG characteristics to the average Lexis-DAG characteristics over ten randomized experiments. The Original Lexis-DAG has fewer intermediate nodes than the Randomized ones (169 in Original vs 359 in Randomized), and its depth is twice as large (8 vs 4.4). Importantly, the Randomized DAGs are significantly more costly: 44% higher cost in terms of edges and 52% in terms of concatenations.
To further understand these differences from the topo-logical perspective, Fig. 3 shows scatter plots for the length, path centrality, and re-use (number of replacements) of each intermediate node in the Original Lexis-DAG vs one of the Randomized Lexis-DAGs. With randomized targets, the in-termediate nodes are short (mostly 2-3 symbols), their re-use is roughly equal to their out-degree, and their path central-ity is determined by their out-degree; in other words, most intermediate nodes are directly connected to the targets that include them, and the most central nodes are those that have the highest number of such edges. On the contrary, with the original targets we find longer intermediate nodes (up to 11-12 symbols) and their number of replacements in the targets can be up to an order of magnitude higher than their out-degree. This happens when intermediate nodes with a large number of replacements are not only used directly to con-struct targets but they are repeatedly combined to construct longer intermediate nodes, creating a deeper hierarchy of re-use. In this case, the high path centrality nodes tend to be those that are both relatively long and common, achieving a good trade-off between specificity and generality.
As mentioned in the introduction, it is often the case that the hierarchical process that creates the observed sequences is unknown. Lexis can be used to discover underlying hier-archical structure as long as we have reasons to believe that that hierarchical process cares to minimize, even heuristi-cally, the same cost function that Lexis considers (i.e., num-ber of edges or concatenations). A related reason to apply Lexis in the analysis of sequential data is to identify the most parsimonious way, in terms of number of edges or con-catenations, to represent the given sequences hierarchically. Even though this representation may not be related to the process that generated the given targets, it can expose if the given data have an inherent hierarchical structure.
As an illustration of this process, we apply Lexis on a set of protein sequences. Even though it is well-known that such sequences include conserved and repeated subsequences (such as motifs) of various lengths, it is not currently known whether these repeats form a hierarchical structure. That would be the case if one or more short conserved sequences are often combined to form longer conserved sequences, which can themselves be combined with others to form even longer sequences, etc. If we accept the premise that a conserved sequence serves a distinct biological function, the discovery of hierarchical structure in protein sequences would suggest that elementary biological functions are combined in a Lego-like manner to construct the complexity and diversity of the proteome. In other words, the presence of hierarchical struc-ture would suggest that proteins satisfy, at least to a certain extent, the composability principle, meaning that the func-tion of each protein is composed of, and it can be understood through, the simpler functions of hierarchical components.
Our dataset is the proteome of baker X  X  Yeast 5 , which con-sists of 6,721 proteins. However, this includes many protein homologues. It is important that we replace each cluster of homologues with a single protein; otherwise Lexis can detect repeated sequences within two or more homologues. To remedy this issue, we use the UCLUST sequence clus-tering tool [1], which is based on the USEARCH similarity measure (or identity search) [9]. The Percentage of Identity (PID) parameter controls how similar two sequences should be so that they are assigned to the same cluster. We set PID to 50%, which reduces the number of proteins to 6,033. Much higher PID values do not cluster together some obvi-ous homologues, while lower PID values are too restrictive. To reduce the running time associated with the randomiza-tion experiments described next, we randomly sample 1,500 proteins from the output of UCLUST .

The total length of the protein targets is about 344K amino acids. The resulting Lexis-DAG has about 151K edges and 5,171 intermediate nodes, and its maximum depth is 7. Fig. 4(a) shows a scatter plot of the length and number of replacements of these intermediate Lexis nodes (repeated sequences discovered by Lexis).

Of course some of these sequences may not have any bi-ological significance because their length and number of re-placements may be so low that they are likely to occur just based on chance. For instance, a sequence of two amino acids that is repeated just twice in a sequence of thousands of amino acids is almost certain (the distribution of amino acids is not very skewed). To filter out the sequences that are not statistically significant, we rely on the following hypoth-esis test. Consider a node that corresponds to a sequence with length l and number of replacements r in the given targets. The null-hypothesis is that sequences with these values of l and r will occur in a Lexis-DAG that is con-structed for a random permutation of the given targets. To evaluate this hypothesis, we randomize the given target se-quences multiple times, and construct a Lexis-DAG for each randomized sequence. We then estimate the probability that sequences of length l and number of replacements r occur in the randomized target Lexis-DAG, as the fraction of 500 experiments in which this is true. For a given significance level  X  = 0 . 1, we can then identify the pairs ( l,r ) for which we can reject the null-hypothesis; these pairs correspond to the nodes that we view as statistically significant. 7 http://www.uniprot.org/proteomes/UP000002311 http://drive5.com/usearch/manual/uclust algo.html
Another way to conduct this hypothesis test would be to estimate the probability that a specific sequence of length l will be repeated r times in a permutation of the targets. The number of randomization experiments would need to
On average, the randomized target Lexis-DAGs have a smaller depth (5 . 0) and more edges (155 K ) than the original Lexis-DAG. Fig. 4(b) shows the intermediate nodes of the original Lexis-DAG that pass the previous hypothesis test. Figure 4: Length and number of replacements for the intermedi-
Fig. 5 shows a small subgraph of the Lexis-DAG, show-ing only about 30 intermediate nodes; all these nodes have passed the previous significance test. The grey lines rep-resent indirect connections, going through nodes that have not passed the significance test (not shown), while the bold lines represent direct connections. Interestingly, there seems to be a non-trivial hierarchical structure with several long paths, and with sequences of several amino acids that re-peat several times even in this relatively small sample of proteins. Despite these preliminary results, it is clearly still early to draw any hard conclusions about the presence of hi-erarchical structure in protein sequences. We are planning to further pursue this question using Lexis in collaboration with domain experts.
Recent work has highlighted the connection between pat-tern mining and sequence compressibility [16]. Data com-pression looks for regularities that can be used to compress the data, while patterns are often useful as such regulari-ties. In dictionary-based lossless compression, the original sequence is encoded with the help of a dictionary, which is a subset of the sequence X  X  substrings as well as a series of pointers indicating the location(s) at which each dictionary element should be placed to fully reconstruct the original se-quence. Following the Minimum Description Principle, one strives for a compression scheme that results in the smallest size for the joint representation of both the dictionary and the encoding of the data using that dictionary. The size of this joint representation is the total space needed to store the dictionary entries plus the total space needed for the re-quired pointers. We assume for simplicity that the space to store an individual character and a pointer are the same.
We now evaluate the use of a Lexis-DAG for compression (or compact representation) of strings. To do so, we need to decide 1) how to choose the patterns that will be used for compression, and 2) if a pattern appears more than once, which occurrences of that pattern to replace. A naive ap-proach is to simply use the set of substrings that appear in the Lexis-DAG as dictionary entries, and compare them to sets of patterns found by other substring mining algorithms. be much higher in that case, however, to cover all sequences that we see in the actual Lexis-DAG, each with a given value of r . Given a set of patterns as potential dictionary entries, select-ing the best dictionary and pointer placement is NP-hard. A simple greedy compression scheme, that we refer to as Com-pressLR , is to iteratively add to the dictionary the substring that gives the highest compression gain when replacing all Left-to-Right non-overlapping occurrences of that substring with pointers. We re-evaluate the compression gain of can-didate dictionary entries in each iteration. For a substring v with length | v | and number of left-to-right non-overlapping occurrences R v , the compression gain is:
We compare the substrings identified by Lexis with the substrings generated by a recent contiguous pattern mining algorithm called ConSgen [27] (we could only run it on the smallest available dataset). Additionally, we compare the Lexis substrings with the set of patterns containing all 2-and 3-grams of the data. The comparisons are performed on six sequence datasets: the  X  X east X  and iGEM datasets of the previous sections, as well as four  X  X SF CS awards X  datasets that will be described in more detail in the next section.
 Table 1 shows the comparison results under the headings: Lexis-CompressLR, 2+3grams-CompressLR and ConSgen-CompressLR. These naive approaches are all on par with each other. This comparison, however, treats G-Lexis a mere pattern mining algorithm. Instead, the G-Lexis al-gorithm constructs a Lexis-DAG that puts the generated patterns in a hierarchical context. One can think of the Lexis-DAG as the instructions in constructing a hierarchical  X  X ego-like X  sequence. The edges into the targets tell us how to place the final pointers, i.e., which occurrences of a dictio-nary entry to replace in the targets. Further, the rest of the DAG shows how to compress the patterns that appear in the targets using smaller patterns. It is easy to see that using this strategy the compressed size becomes equal to the num-ber of edges in the DAG. Using this strategy that is encoded in the Lexis-DAG results in an additional 2%-20% reduction in the compressed size over the CompressLR approaches.
The Lexis-DAG can also be used to extract machine learn-ing features for sequential data. The intermediate nodes that form the core of a Lexis-DAG, in particular, correspond to sequences that are both relatively long and frequently re-Table 1: Compression ratio (i.e., percentage of compressed data used in the targets. We hypothesize that such sequences will be good features for machine learning tasks such as classifi-cation or clustering because they can discriminate different classes of objects (due to their longer length) and at the same time they are general within the same class of objects (due to their frequent occurrence in the targets of that class).
To test this hypothesis, we used Lexis to extract text fea-tures for four classes of NSF research award abstracts during the 1990 X 2003 time period. 8 We pre-processed each award X  X  abstract through stopword removal and Porter stemming. The alphabet S is the set of word stems that appear at least once in any of these abstracts. Table 2 describes this dataset in terms of number of abstracts, cumulative abstract length, and average length per abstract for each class.

We constructed the Lexis-DAG for each class of abstracts, and then used the G-Core algorithm to identify the core for each DAG. We stopped G-Core at the point where 95% of indirect paths in the Lexis-DAG are covered. The strings in each core are the extracted features for the corresponding class of abstracts. Table 3 shows the 5 strings extracted by G-Core for each class. We create a common set of G-Core features by taking the union of the sets of core substrings derived for each class. The next step is to construct the archive.ics.uci.edu/ml/machine-learning-databases/ nsfabs-mld/nsfawards.data.html feature vector for each abstract. We do so by representing each abstract as a vector of counts, with a count for each substring feature.
 Table 3: 5 word stems in the Lexis-DAG core of each class of Table 4: SVM classification accuracy. # nonzeros is the number of nonzero elements in the term-document matrix with each feature set. The accuracies and the parameters are fit based on 10-fold cross-validation for each feature set.
To assess how good these features are, we compare the classification accuracy obtained using the Lexis features with more mainstream representations in text mining on NSF data:  X  X ag-of-words X , 2-gram, 3-gram, and two combina-tions of these representations. We use a basic SVM classifier with an RBF kernel. We used the SVM implementation in MATLAB. The accuracy results are similar to those with a KNN classifier that we tried with a Cosine distance, and the accuracy is evaluated with 10-fold cross-validation.
Table 4 shows that the Lexis features result in a much sparser term-document matrix, and so in smaller data over-head in learning tasks, without sacrificing classification ac-curacy. Lexis also results in a lower feature dimensionality (with the exception of the 1-gram method but the accuracy of that method is much lower). Note that most Lexis features are 2-grams but the Lexis core (for 95% of path coverage) may also include longer n -grams. Lexis becomes better, rel-ative to the other feature sets, as we decrease the number of considered features. For instance, with 3 , 000 features the accuracy with Lexis is 74%, while the accuracy with the 1-gram and 2-gram features is 69% and 64%, respectively.
Lexis is closely related to the Smallest Grammar Problem (SGP), which focuses on the following question: What is the smallest context-free grammar that only generates a given string? The constraint that the grammar should generate only one string is important because otherwise we could sim-ply consider a  X   X  as the generator of any string over  X . The SGP is NP-hard, and inapproximable beyond a ratio of 8569 [6]. Algorithms for SGP have been used for string compres-sion [18] and structure discovery [21] (for a survey see [10]). There are major differences between Lexis and SGP. First, in Lexis we are given a set of several target strings, not only one string. Second, Lexis infers a network representation (a DAG) instead of a grammar, and so it is a natural tool for questions relating to the hierarchical structure of the targets. For instance, the centrality analysis of intermediate nodes or the core identification question are well understood problems in network analysis, while it is not obvious how to approach them with a grammar-based approach.

One can also relate Lexis to the body of work on sequence pattern mining , where one is interested in discovering fre-quent or interesting patterns in a given sequence. Most work in this area has focused on mining subsequences, i.e., a set of ordered but not necessarily adjacent characters from the given sequence. In Lexis, we focus on identifying substrings, also known as contiguous sequence patterns . A couple of recent papers develop algorithms for mining substring pat-terns [27, 26], since sequence mining algorithms do not read-ily apply to the contiguous case. However, they rely on the methodology of candidate generation (commonly used in se-quence pattern mining), where all patterns meeting a certain criterion are found, such as having frequency of at least two or being maximal. In the sequence mining literature, it has been recently observed that the size of the discovered set of patterns as well as their redundancy can be better controlled by mining for a set of patterns that meet a criterion collec-tively, as opposed to individually. This is useful when these patterns are used as features in other tasks such as sum-marization or classification. Algorithms for such set-based pattern discovery have been recently developed for sequence pattern mining [16, 25]. In the context of substring pat-tern mining, Paskov et al [24] show how to identify a set of patterns with optimal lossless compression cost in an unsu-pervised setting, to be then used as features in supervised learning for classification. In a follow-up paper [23], DRAC-ULA provides a  X  X eep variant X  of [24] that is similar to Lexis, in terms of the general problem setup. DRACULA X  X  focus is mostly on complexity and learning aspects of the problem, while Lexis focuses on network analysis of the resulting opti-mized hierarchy. For instance, DRACULA considers how to take into account how dictionary strings are constructed to regularize learning problems, and how the optimal Dracula solution behaves as the cost varies. We have shown that, although not specifically designed for feature extraction or compression, the Lexis framework also results in a small and non-redundant set of substring patterns that can be used in classification and compression tasks.

Optimal DNA synthesis is a new application domain, and we are only aware of the work by Blakes et. al. [4]; they de-scribe DNALD, an algorithm that greedily attempts to max-imize DNA re-use for multistage assembly of DNA libraries with shared intermediates. Even though the Lexis frame-work was not specifically designed for DNA synthesis, the Lexis-DAGs can be seemlessly used as solutions for this task. In our illustrative example with the iGEM dataset, G-Lexis returns solutions with 11% lower synthesis cost (equivalent to concatenation cost) than DNALD.

Structure discovery in strings has been explored from sev-eral different perspectives. For example, the grammar-based algorithm SEQUITIR [21] presents interesting possible ap-plications in natural language and musical structure iden-tification. In an information-theoretic context, Lancot et. al. [17] shows how to distinguish between coding and non-coding regions by analyzing the hierarchical structure of ge-nomic sequences.
Lexis is a novel optimization-based framework for explor-ing the hierarchical nature of sequence data. In this pa-per, we stated the corresponding optimization problems in the admittedly limited context of two simple cost functions (number of edges and concatenations), proved their NP-hardness, and proposed a greedy algorithm for the construc-tion of Lexis-DAGs. This research can be extended in the direction of more general cost formulations and more effi-cient algorithms. Additionally, we are working on an incre-mental version of Lexis in which new targets are added to an existing Lexis-DAG, without re-designing the hierarchy.
We also applied network analysis in Lexis-DAGs, propos-ing a new centrality metric that can be used to identify the most important intermediate nodes, corresponding to sub-strings that are both relatively long and frequently occurring in the target sequences. This network analysis connection raises an interesting question: are there certain topological properties that are common across Lexis-DAGs that have re-sulted from long-running evolutionary processes? We have some evidence that one such topological property is that these DAGs exhibit the hourglass effect [3].

Finally, we gave four examples of how Lexis can be used in practice, applied in optimized hierarchical synthesis, struc-ture discovery, compression and feature extraction. In future work, we plan to apply Lexis in various domain-specific prob-lems. In biology, in particular, we can use Lexis in compar-ing the hierarchical structure of protein sequences between healthy and cancerous tissues. Another related application is generalized phylogeny inference , considering that horizon-tal gene transfers (which are common in viruses and bacte-ria) result in DAG-like phylogenies (as opposed to trees).
This research was supported by the National Science Foun-dation under Grant No. 1319549. [1] drive 5 .com/usearch/manual/uclust algo.html . [2] igem.org/main page . [3] S. Akhshabi and C. Dovrolis. The evolution of layered [4] J. Blakes, O. Raz, U. Feige, J. Bacardit, P. Widera, [5] G.S. Brodal, R.B. Lyngso, A. Ostlin, and C.N.S. [6] M. Charikar, E. Lehman, D. Liu, R. Panigrahy, [7] D. Densmore, T.H. Hsiau, J.T. Kittleson, [8] W. Dubitzky, M. Granzow, and D. P. Berrar.
 [9] R. C. Edgar. Search and clustering orders of [10] M. Gall  X e. Searching for compact hierarchical structures [11] D. Gusfield. Algorithms on strings, trees, and [12] A. A. Hagberg, D. A. Schult, and P. J. Swart. [13] S. Inenaga, T. Funamoto, M. Takeda, and [14] J.C. Kieffer and E. Yang. Grammar based codes: A [15] J. Kleinberg. Bursty and hierarchical structure in [16] H. T. Lam, F. M  X  orchen, D. Fradkin, and T. Calders. [17] J.K. Lanctot, M. Li, and E. Yang. Estimating DNA [18] N.J. Larsson and A. Moffat. Offline dictionary-based [19] H. Mengistu, J. Huizinga, J.B. Mouret, and J. Clune. [20] S. Miyagawa, R.C. Berwick, and K. Okanoya. The [21] C.G. Nevill-Manning and I.H. Witten. Identifying [22] M. Newman. Networks: An introduction . Oxford [23] H. S. Paskov, J. C. Mitchell, and T. J. Hastie. Data [24] H. S. Paskov, R. West, J. C. Mitchell, and T. Hastie. [25] N. Tatti and J. Vreeken. The long and the short of it: [26] J. Zhang, Y. Wang, and D. Yang. CCSpan: Mining [27] J. Zhang, Y. Wang, C. Zhang, and Y. Shi. Mining We prove that the Lexis problem is NP-hard through a re-duction from the Smallest Grammar Problem (SGP) [6].
Formally, The Smallest Grammar Problem for a string s is to identify a Straight-Line Grammar (SLG) G  X  such that L ( G  X  ) = { s } and | G  X  |  X  | G | for any other G with L ( G ) = { s } , where | G | denotes the size of grammar G . Charikar et al. [6] define the size of a grammar as the cumulative length of the right-hand side of all rules, i.e., | G | = P T  X   X   X   X  where |  X  | is the number of symbols appearing in the term  X  of a grammar rule. Under this grammar size, Charikar et al. show that the Smallest Grammar Problem is NP-hard [6]. Theorem 1. The Lexis problem in Eq. (2) is NP-hard for C ( D ) being edge cost (Eq. (3) ) or concatenation cost (Eq. (4) ).

Proof: Let us first start with edge cost. Consider an in-stance of SGP in which we are given string s and we are asked to compute an SLG G such that L ( G ) = { s } and | G |  X  m , where | G | = P T  X   X   X   X  |  X  | . We reduce it to an instance of the Lexis problem with a single target string T = { s } , in which we are asked to compute a Lexis-DAG D with E ( D )  X  m .
 Given a grammar G = ( X  ,  X  ,S,  X ) as a solution to the SGP problem, we construct a solution D for the reduced Lexis problem. For each symbol in  X   X   X , construct a node. For a non-terminal T  X   X , we refer to the corresponding node also as T , and associate that node with the string S ( T ) that is produced by expanding rule T according to grammar G . Also, for each rule T  X   X  in G , we scan  X  and add an edge in D from every node that corresponds to a terminal or nonterminal in  X  to the node that corresponds to T (along with the corresponding index). It is easy to see that D is acyclic since G is a straight-line grammar and that the number of edges in D is: E ( D ) = P T  X   X   X   X  |  X  | X  m .
Conversely, consider a Lexis-DAG D = ( V S  X  V M  X  V T ,E ) which is a solution to the Lexis problem from our reduction above , i.e., it has a single target string s , and E ( D )  X  m . We can construct a corresponding grammar G for the SGP as follows. For each source node in V S , construct a terminal in  X . For each node v in V T  X  V M , construct a nonterminal NT ( v ) in  X . For the single target node v in V T , designate NT ( v ) as the start symbol S . For each node v in V T  X  V add a rule in  X  with the right-hand side listing the corre-sponding  X   X   X  symbols for all nodes in the sequence I ( v ) (i.e., ordered as their respective strings appear concatenated in S ( v )). The constructed grammar is straight-line, since every nonterminal has one rule associated with it, and the grammar is also acyclic because the Lexis-DAG D is acyclic. It is easy to see that | G | X  m .

The NP-hardness proof of the Smallest Grammar Problem with grammar size defined as | G | = P T  X   X   X   X  |  X  | [6] can be adapted for a modified grammar size definition, i.e., | G | = P
T  X   X   X   X  ( |  X  | X  1) = P T  X   X   X   X  |  X  |  X  |  X  | . We can then use the same reduction from SGP to Lexis as in the case of edge cost to show that Lexis with concatenation cost is also NP-hard.
 Fig. 6 illustrates an example that the Lexis-DAG when op-timizing for the edge cost function may be different than the Lexis-DAG when optimizing for concatenation cost. Figure 6: Illustration of Lexis-DAGs for target T = { abcdabcefcdgce } and sources S = { a,b,c,d,e,f,g } -(a) Lexis-DAG D 1 with E ( D 1 ) = 13 (optimal) and C ( D 1 ) = 11 (suboptimal). (b) Lexis-DAG D 2 with E ( D 2 ) = 14 (subop-timal) and C ( D 2 ) = 10 (optimal).
 We compare G-Lexis with an algorithm that greedily re-places the longest repeated substring, in terms of both run-time and cost. We implemented the latter, originally pro-posed in [13] using suffix trees, using our own efficient linked-suffix array. We used the NSF data described in the main text and ran the two algorithms on different fractions of the total dataset, repeating the experiments 10 times and recording the average runtime and edge cost. As seen in Fig. 7, the Longest Substring Replacement heuristic offers a better runtime but its cost becomes increasingly worse than G-Lexis as the size of the dataset grows. Also, G-Lexis is still reasonably fast on all datasets we have analyzed so far. Figure 7: Comparison of G-Lexis with Longest Substring Re-Table 5: Top-15 nodes with highest path-centrality iGEM dataset X  X  Lexis-DAG. Nodes shown in bold are already reg-istered in the iGEM library.

