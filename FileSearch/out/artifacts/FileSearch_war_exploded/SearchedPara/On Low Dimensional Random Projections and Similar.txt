 Random projection (RP) is a common technique for di-mensionality reduction under L 2 norm for which many sig-nificant space embedding results have been demonstrated. However, many similarity search applications often require very low dimension embeddings in order to reduce overhead and boost performance. Inspired by the use of symmet-ric probability distributions in previous work, we propose a novel RP algorithm, Beta Random Projection, and give its probabilistic analyses based on Beta and Gaussian approx-imations. We evaluate the algorithm in terms of standard similarity metrics with other RP algorithms as well as the singular value decomposition (SVD). Our experimental re-sults show that BRP preserves both similarity metrics well and, under various dataset types including random point sets, text (TREC5) and images, provides sharper and con-sistent performance.
 G.3 [ Mathematics of Computing ]: PROBABILITY AND STATISTICS; F.2 [ Theory of Computation ]: ANALY-SIS OF ALGORITHMS AND PROBLEM COMPLEXITY Algorithms, Design, Theory Random Projection, Probabilistic Methods, Dimensionality Reduction  X  Copyright 2008 ACM 978-1-59593-991-3/08/10 ... $ 5.00.
Dimensionality reduction is a common technique to sim-plify and accelerate large scale data processing, especially for applications such as information retrieval, text and multime-dia databases. In these applications, documents/objects are modelled as points in a high dimensional space in which each dimension captures a certain feature. For modern applica-tions such as text and multimedia indexing, the number of dimensions could easily go beyond thousands 1 .Thiscauses significant overhead for high dimensional query processing techniques which suffer from the so called  X  X urse of dimen-sionality X .

Clearly, reduced dimensionality means simultaneous im-provements in terms of computational complexity through-out storage, indexing and query processing. From the sys-tems point of view, the additional advantages comes from fitting more indices or points into local or nearby network storage and ease of design due to low dimensionality. For ex-ample, [5] uses a 1-D synopsis of high dimensional datasets and [20], suffered from high data dimensionality, actively uses lower dimensional projections to provide load-balancing and efficient retrieval.

In this paper, we present a family of randomised projec-tion (RP) algorithms and analyse its properties for dimen-sionality reduction problems under the L 2 metric. One of the main advantages of randomised algorithms is simplic-ity. Standard statistical methods such as singular value decomposition (SVD) or principle component analysis re-quire repetitive iteration throughout the data and thus suf-fers time complexity polynomial to the data dimensional-ity 2 . RP methods, on the other hand, require only seed values independent of the dataset and guarantee successful reduction in constant number of trials with high probabil-ity. This is especially beneficial for WWW and multimedia databases where the number of documents is so large that the cost to directly apply some super-linear algorithms is prohibitive. We also present the performance difference be-tween the distortion due to the projection with those due to SVD. Interestingly, our results indicates that for text and image datasets, RP can be competitive to much more ex-pensive methods such as SVD as far as similarity search is concerned.
For example, as we shall see later, LA Times documents from TREC 5 consist of 187K keywords as document fea-tures after stemming. For example, the standard SVD algorithm would require O ( d 3 ) time steps to decompose a d by d matrix due to its nature to find the optimal rank approximation.
A common problem for all similarity search schemes is that the measure of the document space varies from appli-cation to application. While the choice of such measure is beyond the scope of this paper, we provide two arguments below to motivate our use of L 2 norm. Firstly, the L 2 norm has been shown to be very expressive in practice and is piv-otal for many applications such as web page indexing [3]. Secondly, and most importantly, algorithms constructed un-der L 2 are more generalised than one might expect initially. It can be shown that all norms are equivalent in the sense that they could simulate each other with up to some con-stant factor distortion, due to a special case of Weierstrass X  theorem [8]. For a comprehensive review of this field, see [9].
For the rest of this paper, we first present the high level ideas of RP in Section 1.1 followed by a summary of contri-butions. In Section 2, we formally present the Beta random projection and give derivations on its exact distributions and tail bounds. We show the key observation that each projec-tion in BRP obeys Beta distribution in Section 3. Finally, we quantitatively evaluate our algorithms against represen-tative RP schemes as well as against SVD in Section 4. In essence, a random projection is a function H A : R d  X  R k where A d,k is k choices of d -dimensional estimation vec-tors in R d . Its objective is, given a set of points, to produce a new image in R k with estimation vectors such that the pair-wise distance simulates those in R d . Typically, the new image is a projection onto the subspace spanned by the esti-mation vectors. One then apply probabilistic arguments to estimate how much distortion this transform can introduce.
This possibility is first shown in the 80s when Johnson and Lindenstrauss showed that n points in R d could be embedded into R k with 1  X  factor distortion where k = O (log n/ 2 ) [12]. To see that this bound is tight, consider the case for dimensionality reduction of 3 points each of which is of distance 1 to each other on a 2D plane. It is not possible to find an embedding of these three points on a line in which their pair-wise distance could all be preserved. An optimal embedding could be to have three points with coordinate 0 , 1 , 2 on a line, thus matching the bound with error equal to 1.

While the JL-embedding demonstrates the possibility of reduction, its proof is based on geometric approximations and thus makes it difficult to comprehend and design algo-rithms to discover such mappings. Indyk et al. [10] proposed that drawing the estimators from d -dimensional Gaussian distribution could lead to simpler proofs and constant fac-tor improvements over the original JL-embedding bounds.
Later on, with an eye on the benefits of random projec-tion, various work was proposed to give additional proper-ties such as volume preserving [15] and to accelerate random projection. Most notably, Achlioptas [1] demonstrated that one could yield as good an embedding as by Indyk X  X  using sparse estimation vectors. Specifically, his estimators are drawn from a sparse distribution in which each coordinate of the estimation vector has equal probability of 1 / 6tobe either down the computational complexity, as 2 / 3 of the coordi-nates are expected to be discarded. Surprisingly, this does not hurt the accuracy of the projection, at least in theory. In fact, the bound of k is pushed down to 6 2 / 2  X  3 / 3 using moment methods in [1].

This sparse estimator approach is followed up by vari-ous further attempts to accelerate projection [2, 13, 14] by trading off distortion with performance. Li et al. presented projections based on even more sparse estimators [14] and another based on marginal information [13]. Their results showed that further performance gain is possible by incur-ring some more distortion. Ailon [2] presented techniques based on Fast Fourier transforms for both L 2 and L 1 with running time down to O ( d log d +min { d  X  2 log n, p  X  where p is the choice of L 2 or L 1 .

While sparse random projections are significantly faster in terms of computation, however, there are some draw-backs when faced with certain datasets. Consider two points u, v  X  X  100 with u =(1 , 0 , 0 ,  X  X  X  , 0) and v =(0 , 0 , Dropping attributes with probability 0 . 9 as in [14] would yield at least a 0 . 82 k probability that the two points have zero distance on a R k projection. In fact, this problem would become more severe as the dimensionality increases due to the fact that the probability that each  X  X alid X  co-ordinate is dropped increases inversely proportioned to the square root of dimensionality.

One might argue that bad datasets could be just rare, real world datasets are, in fact, much more sparse than one might expect. In TREC5, for example, only 0 . 084% of the La Times term document matrix is non-zero and that of FBIS is 0 . 077%. In FreeDB, an album database containing 21 million song names, 90% of the songs use less than 10 keywords out of a 13 thousand vocabulary and 98% use less than 20.
In this paper, we present the Beta random projection for vector spaces under the L 2 norm. Our estimation vectors in A are drawn from unit d -dimensional vectors in R d uni-formly at random. Our key insight is to discover that, for an arbitrary vector, each projection as such is in fact a random variable based on the beta distribution and its original norm, hence the name Beta random projection. Its formal proof is discussed separately in Section 3. We show that k could in We later analyse the exact distributions of the distortion due to k projections. We first give tight bounds for the case in which k is large enough 4 to be treated by the central limit theorem (see Section 2.1). For small k , we present results basedonbetadistributionapproximationinSection2.2.
We evaluate BRP via three different types of datasets  X  sparse random pointset, text (TREC5), and image ( flickr.com crawl and private image collection). We measure the dis-tortionintermsofboth L 2 and cosine similarity metric. Secondly, we give our experimental results on text corpus in Section 4.1 along with comparison with SVD and Latent Semantic Indexing (LSI). Our results suggest that for the TREC-5 corpus, RP in general achieves as good precision in terms of cosine similarity as those from SVD, while SVD yields better results when the dimensionality is low (less than 5). As LSI could discover effective concept space rep-resentations by combining dimensions of similar meaning, we compare if the similarities due to BRP would agree with LSI on TREC-5 dataset (Section 4.1). Interestingly, our re-taken from [1], Theorem 2, with  X  =1
In practice, it suffices to have k larger than 30. We shall use this value for distinguishing between the proofs for the two approximations for the rest of the paper. sults show that their agreement increases with the number of dimensionality with only minor variation. This suggests that minor dissimilarities diminish over each projection in RP and thus the resulting projections would largely reflect the key dissimilarities among documents X  thus simulating how LSI works.

For our image collection and the 250k images crawl of flickr.com , our benchmark on L 2 metric with other RP schemes show that projections due to BRP are consistently sharper. With SVD, BRP behaves at least as good when only 1D is used, and outperforms when the dimensionality is larger than 1. This contrast of performance is mainly due to the fact that low rank approximations are more accurate when the intrinsic dimensionality is low, as is the case in WWW. For datasets with large intrinsic dimensionality such as images, SVD could not derive feasible approximations as far as cosine similarity is concerned.
Let A be a k  X  d matrix in which A i , a uniformly ran-dom point on the unit d -dimensional sphere, is the i -th row of A . Notice that compared to previous work, this in ef-fect removes the randomness due to varying length of the estimation vectors. As we shall see later, this gives more consistent results for BRP. In practice, we generate each A by normalising each vector drawn from N d (0 , 1) where N is a d-dimensional Gaussian distribution [16]. We define the Beta random projection H ( v ; d, j ) for any point v  X  X  d follows: where each row A i is sampled uniformly at random from unit d -dimensional sphere.

We endeavour to show that the lower bound of k could be improved up to 40% compared to [1] as indicated by the theorem below.

Theorem 2.1. For any u, v  X  S  X  X  d and | S | = n ,an random instance of H A in equation 1 where each row A i is a random vector from the unit d -sphere yield distortion with probability at least 1  X  n  X  1 ,when k  X 
Proof. It suffices to show that for any vector v ,Pr[  X  1 to occur C ( n, 2) times which is the number of all distance pairs.

For the first case, given Theorem 2.2 and Lemma 2.5, this amounts to solving for k such that 2  X  For the second case when 0 &lt;k&lt; 30, observe that by Lemma 2.9 and 2.8, we solve (1  X  2 / 3 ) By induction on n and observe that 2 (1  X  2 / 3 ) peaks when = 1, we arrive at the claim.

In the following, we first show the distortion distribution in the case when k is large enough for central limit theorem in Theorem 2.2. For the case in which k is small, we present the approximation by the Beta distribution in Theorem 2.6. We would like to note that these two approximations is accu-rate in most cases 5 . Following each approximation results, we derive the probability bo unds for the event that the dis-tortion is more than 1  X  .
In this Section, we present the results for BRP with k large enough for treatise using the Central Limit Theorem. The proof proceeds by first exercising the Lemma 3.1 that each A i  X  v in (1) would yield a distortion obeying Beta distribu-tion 6 . Then, we apply the theorem to obtain the aggregate distortion as the number of projection increases. Theorem 2.2. There exists a random projection H : R d  X  R k under L 2 norm such that for any &gt; 0 ,k  X  30 and v where erf( z )  X  2  X   X 
Proof. (2.2) First, we rewrite (1) into H ( v ; d, k )= ( b ,b 2 ,b 3 ,...,b k ). Without loss of generality, we assume an unit-vector v as this is a linear transform of v .From lemma 3.1, each b i is a random variable such that b 2 i  X  ( Pr[1  X   X  Since k is large enough, the central limit theorem indicates that Thus (3) can be rewritten as standard normal distribution (denoted as variable Z ) as in (5). Integrating through the standard normal distribution and taking into account the fact that the error function is monotonically decreasing, we arrive at the claim.

Due to space limitations, we defer the presentation until the full version of this paper.
We defer the proof of Lemma 3.1 until Section 3 for smoother presentation. Here, we would like to bound the probability of the event E that a vector X  X  projection has more than factor distortion to its original. For ease of comparison, we first present the sharp bounds given by Achlioptas in [1].

Lemma 2.3 (Achlioptas [1]). Let R be a k  X  d matrix where { r ij } are i.i.d. random variables following the discrete density Pr[ r ij =  X   X  vector in R d .

Below, we provide tail bounds for the case where the cen-tral limit theorem applies. Our approach is based on Taylor expansion on the error function which exhibits different con-vergence behaviour according to its argument separate the two cases in Lemma 2.4 and 2.5.

Lemma 2.4. Let d&gt;k&gt; 30 ,
Proof. Since Theorem 2.2 indicates Pr[ E ]  X  2 erf ( and let z = (9) is the standard Taylor expansion. Exercising the fact that ease of comparison, observe that 1  X  x  X  e  X  x ,ourclaimis less than exp (  X  2  X   X 
Lemma 2.5. Let d&gt;k&gt; 30 ,
Proof. Let z = bound in Theorem 2.2 with asymptotic series Thus,
As discussed in the introduction, most applications does not always allow for k  X  30, we give results based on Beta approximation in this Section. Again, it can be shown that this approximation is precise as the error bound is given in [11]. Below, we establish the aggregate distortion distri-bution followed by bounds for the event that the distortion is larger than 1  X  .
 Theorem 2.6. There exists a random projection H : R d  X  R k under L 2 norm such that for any &gt; 0 , 1 &lt;k&lt; 30 ,d k and v  X  X  d , where  X  (  X ,  X  ) is the beta distribution with parameter  X ,  X  .
Proof. (2.6) From Lemma 3.1, we know each ( A i  X  v ) 2  X   X  (
To derive the distribution of Y , we invoke the beta-sum approximation below:.
 Fact 2.7 (Johannesson and Giri [11]). Let S = where X i are i.i.d. random variables of  X  (  X ,  X  ) . The distri-bution of S can be approximated by: where E =
Since d k ,then f  X  d +2 2 Thus, we have Y  X   X  ( k 2 , d 2 1).
Below, we show that the approximated probabilities for the case of small k where the resulting distribution is ap-proximated by the Beta distribution. Due to the fact that beta distributions under our parameters exhibit significant skew, we bound the right and left tails separately in lem-mas 2.8 and 2.9. In lemma 2.9, it is unexpected to see that it indicates that it is relatively unlikely to have a distortion less than the original.

In the following analyses, we shall separate E into two for easy discussion. Let E R be the right tail, the event that |
H ( v ) | 2 / | v | 2  X  1+ ,and E L be the left, the event that | H ( v ) | 2 / | v | 2  X  1  X  .
 Lemma 2.8. Let d&gt; 30 &gt;k&gt; 0 and &gt; 0 .

Proof. We denote the pdf of  X  (  X ,  X  )by f ( z ;  X ,  X  )= We can thus bound the right tail as an integral of f over A =[(1+ ) k d , 1]: Differentiating f (  X  )showthatitreachesmaximaatmax { 0 , } and then drops exponentially, thus (15). Via a little calcu-Notice that t  X  ln t&gt;t/ 2, therefore the rate at which this summation increases must be faster than e  X   X / 2 .Takingthe first term of the original series, we have the the inequality (19). Observing that the term in parenthesis is strictly less than 2, we arrive at the claim.

For the special case in which 0  X   X  1, one could ob-tain an improved bound over the classic result as in [1], via Taylor X  X  expansion for ln(1 + ), ln(1 + )  X   X  2 / 2+ 3 / 3. That is:
Lemma 2.9. Let d 30 &gt;k&gt; 0 , 0  X  &lt; 1 ,and E L be the event that | H ( v ) | 2 / | v | 2  X  1  X  where 0  X   X  k (1  X  )  X  1 .

Proof. Let  X  =1  X  .Noticethatas b  X  X  X  and cb  X   X  , then (1  X  b ) a  X  e  X   X  . Substituting x with t =(  X a/b ) basic beta-distribution pdf results in (21). Invoke Lemma 2.10, we have (22). Cleaning the equation yields our propo-sition.
 Pr[ E L ]= 1
Lemma 2.10. Let a&gt; 0 ,  X a  X  1 ,and 0  X   X   X  1 . Z
Proof. Since this integral does not have a closed form for arbitrary value of a , We prove this by mathematical in-duction. Firstly, for a = 1, LHS is induced to 1  X  e  X   X  standard calculus  X  the desired inequality holds. For a =2,
Assuming the claim holds for a = k , the inequality can be rewritten as below by standard calculus.
 Z
Similarly, the case for a = k + 1 can be written as (24) which, with some calculus, becomes (25). Observe that t e  X   X  ( k +1) t is monotonically increasing. The additional area due to changing the starting point  X  ( k +1) to  X k is at least  X  (  X k ) k e  X   X k . Expanding the integral again, we can establish the inequality of (26). The terms in parenthesis of (28) is Figure 1: An illustration of the inner product on a 2D sphere, i.e., a circle. strictly less than 1  X  e  X   X  ( k +1) after  X   X  1 /k , as suggested by partial differentiation along  X  and checking the base case  X k = 1. Finally, we arrive at the claim after cleaning up (28).
 LHS =
In this Section, we demonstrate our key observation that the ratio of each inner product and its original vector norm is, in effect, beta distributions with parameters (1 / 2 , ( d 1) / 2). This yields sharper concentration compared to the previous bounds.

Lemma 3.1. Let X be an uniformly random point on the surface of the unit d-dimension sphere and v  X  X  d .Then, we have where  X  (  X ,  X  ) is the beta distribution.

Proof. (3.1) Without loss of generality, we consider the case in which v is an unit vector. Since the unit d -dimension sphere is symmetric and X spreads uniformly on its surface, it suffices to consider the case in which X =( x 1 ,x 2 ,...,x and v =(0 , 0 ,..., 0 , 1). Notice that x 2 1 + x 2 2  X  X  X  which is a ( d  X  1)-dimension hypersphere by definition.
We first give some intuitions on the proof. Consider each v is a hyperplane. Since we sample uniformly random from the unit hypersphere, the probability distribution of the in-ner product is essentially proportionate to the region of the surface that intersects with the hyperplane. We illustrate the2DcaseinFigure1.
 That is: We can denote x i via hyper-spherical coordinates and choose t =cos  X  1 ,thus Thus the cdf reduces to the following region of area: where S d (1) denotes the surface area of the unit d -sphere. The above equations thus reduce to
Pr[ | X  X  v | X  t ]= Pr[ | X  X  v | X  cos  X  1 ] where 2 F 1 (  X  ) is the generalised hyper-geometric function,  X  (  X  ) is the beta function,  X  t 2 (  X  ) is the incomplete beta func-tion, and I t 2 (  X  ) is the regularised beta function. By the definition of beta distribution, we obtain the claim.
In this Section, we compare several random projection algorithms along with SVD in terms of the L 2 norm and the cosine similarity, which are the dominant distance measures in both multimedia and text similarity search. We pick the RP algorithms proposed by Indyk [10] (marked as Indyk98) and Achlioptas [1] (marked as Ach01) as our benchmarks since the former is representative of non-sparse RPs and the latter is the most accurate RP algorithms based on sparse distributions. Our choice of SVD is due to the its ability to derive optimal rank approximations to the original matrix and its application to Latent Semantic Indexing (LSI). In the following, we present the evaluations based on the TREC 5 corpus (Section 4.1), and an image collection con-sisting of 142 images of landscape, portrait, and close-up photos (Section 4.2) as well as a large scale 250k image crawl of flickr. Due to space limitations, we defer the evaluation results of the random point sets and the 142 photo image vector dataset to appendix should the readers like to see more results.

In particular, we focus our evaluation under low k most of the time. This is due to that practical applications such as distributed hash tables [19] or multi-dimensional overlays [20] do not scale well for data beyond a few dimensions. For TREC 5 datasets, we consider both LA Times and Foreign Broadcast Informatio n Service (FBIS). The LA Times and FBIS dataset consists of 131K documents with 187K keywords and 130K documents with 277K keywords respec-tively. We map these documents into high dimensional space using the standard term-frequ ency inverse document fre-quency (TFIDF) representation, generated via the Lemur toolkit [18] with standard stemming. The baseline bench-mark here is with the seminal latent semantic indexing (LSI) [6], which is essentially singular value decomposition (SVD).
Below, we first present results of the three RP schemes in terms of L 2 norm so as to demonstrate the consistent performance of BRP. Then we give a brief review of SVD and LSI, and present the cosine similarity results of BRP compared to SVD. Last, we present how closely BRP could approximate LSI on the TREC5 dataset.
 In Figure 2, we present the results of the three RP algo-rithms under small k in terms of mean over 50 instances with 3000 random samples of pair-wise distance each. We plot the accumulate percentage of projections that yield dis-tortions below . The X-axis are the values of measured in these projections and Y-axis being their accumulated per-centage.

Similar to the phenomena observed for the random point sets and their sparse version, we see that sparse random pro-jections generally are more likely to pick estimation vectors which under-estimate each vector in TREC.

We can see that pick estimation vectors from the unit d -sphere, BRP yields better projections since the given the same percentage of instances, BRP consistently produce im-ages with smaller distortion (lower ). The only exception being Figure 2(d), sparse random projection somehow gives 20% instances with lower distortion and then lose out again when the accumulated percentage reaches 90%. The varia-tion of distortion in these instances also indicates that BRP gives consistent performance over most cases by yielding smaller variations. Due to space limitation, we defer the variation figures in the appendix.
 A singular value decomposition of matrix A is a triplet of matrices ( U, S, V ) such that A m,n = U m,k S k,k V T n,k is a diagonal matrix consists of k singular values. Using all k singular values would yield the original matrix while the first, say 10, singular values would give optimal approx-imation to A with rank 10. As the typical computational complexity of SVD is O ( d 2 n + n 2 d + d 3 )[7]foranymatrix of size ( d, n ) and that of RP is O ( dkn ), which is of RP X  X  clear advantage, we concentrate our discussion on the dis-tortions of both approaches under low-dimension. Notice that the dimensionality of SVD and RP are not on the same terms. While all n points in R d are reduced to use only O ( kn ) storage, points using the first s singular values would imply using O (( d + n + s ) s ) space due to the fact that SVD requires storing both the left and right singular matrix.
LSI works by decomposing the raw TFIDF matrix into k dimensional concept space which is essentially the left and right singular matrix of SVD. In fact, the left and right singular matrix, respectively, encodes the term and document in the same k -dimensional concept space. The resulting spaces could represent the underlying concept is due to the fact that similar concepts usually appears in documents sharing the same keywords. For example, for two documents containing { Ferrari, Car, Modena } and {
Bugatti, Car, Modena } , the optimal rank 1 approxima-tion should combine  X  X  X ugatti X  X  and  X  X  X errari X  X  into the same singular vector since this would minimise approxima-tion error. For a detailed description of LSI, see [6]. For comparisons between SVD and BRP, we derive approx-imations of SVD using 1  X  40 singular values and random projections of dimensionality 1  X  40. We simulate 300 in-stances of BRP for each of which we measure 3000 pairs of points. The variance shown in the figures are the mean of variances occurred across these 300 instances.

As RP algorithms are shown to preserve L 2 distances well previously, we endeavour to compare the document cosine similarity distortion due to RP and SVD in Figure 3. No-tice that we measure the cosine distortion in terms of their difference in the resulting cosine similarity.

Observe that although the SVD line has a rather large mean, its small variation implies easy calibration. We could see that the SVD approximation gives satisfactory results even when only one singular value is used. This is due to the fact that SVD practically exams all possible angles with which the data could be combined. However, as we shall see later, such a good low-dimension approximation might not always exist for real world datasets. The BRP performances in the first few dimensions are due to the fact that few di-mensions were offered for embedding which means variations could easily dominate the entire projection.

On the other hand, once the dimensionality increases a little, say 15, we could see that the two schemes offer simi-lar variations with BRP X  X  distortion centred around 0. This result is mostly due to the sharp L 2 norm preservation char-acteristic of RP algorithms. Although the pair-wise distance which RP aims to preserve does not hold a direct relation to cosine similarity; the latter, after all, depends on the length of three vectors, being the two vectors in question and their difference. However since the L 2 distortion obeys  X  ( k/ 2 ,d/ 2+1), the distortions on these three vectors quickly converges to 1 once shape of Beta distribution becomes bell-like after k/ 2  X  3. This explains why the curve picks up with SVD after dimensionality reaches 5.
 In Figure 4, we would like to present our results on how close the cosine similarity due to BRP would agree with those from LSI. Similar to the TREC experiment, we instantiate 300 instances of BRP for each of which we sample 3000 pairs of points.

Given the previous results in low dimension spaces, it is not surprising to see that the the BRP results do not agree well with LSI in low dimension spaces. However, BRP starts to agree well with LSI after more dimensions (20 or so) be-come available for embedding. This suggests the possibility of using RP methods to directly approximate the effect of LSI, at least for correlated corpus such as TREC-5. (a) Distortion results on FBIS. SVD consistently produce sharper results than BRP in the low dimensional cases. How-ever, their difference diminishes as the available projection dimensionality reaches 15. The error bars measures the stan-dard variance of the experiment. (b) Distortion results on LA Times. Echoing the same trend as on FBIS, BRP converges quickly after 10 dimensions be-come available to it.
 Figure 3: A comparison of BRP with SVD schemes on LA Times and FBIS.

Intuitively, this should be due to dissimilarities sharing a large number of the same coordinates only having a minor effect on the projections. Thus, large dissimilarities have a higher chance to dominate the resulting distance, and ulti-mately, the cosine value. While 20 might seem like a large number of dimensions, one of the original LSI paper by Fur-nas et al. [6] actually suggested using k =50  X  200 in LSI for good query results.
In this Section, we present our results on images using cosine similarity. Our image collection consists of the raw block vectors of 142 photos of landscapes, portraits, and close-ups of resolution 1600  X  1200 as well as a 250k im-ages crawl from flickr.com 7 .The flickr.com collection represents images using 166 dimensions based on HSV as described in [4]. We compare RP schemes in terms of L 2 distortion under this dataset.
Please see acknowledgement Section for the source of this collection. Figure 4: Measuring coherence of RP and LSI based on LA Times and FBIS. The distortion is measured as the difference of the cosine similarity of vectors by RP and the corresponding vectors in right singu-lar vector space (documents in concept space) from SVD.

Due to the large number of data points involved, we present reduction results of dimension 1  X  8, each based on 50 trials consisting of 1000 samplings of pair-wise cosine similarity. Cosine values are measured from the approximate matrices derived from BRP and SVD. We measure L 2 distortion in the parameter and the cosine distortion by the difference between the cosine similarity values of the original and the reduced image.
Finally, we present the performance of BRP compared to other RP techniques under flickr dataset of HSV represen-tation. In Figure 5(a), we first show a 2D projection of the dataset just to give an idea on how the data points are located in the 166 dimension space. We can observe the ex-istence of several manifolds by noticing the outward peaks. This is later confirmed by principle component analysis in Figure 5(b). Notice that the first 10 principle components can explain almost 60% of the variance to the principle vec-tors. The remaining 40% are then shared by the rest 156 principle vectors. This implies that though the intrinsic di-mensionality is lower, the data points are not sparse.
As shown in Figure 5, BRP consistently yields sharper and more consistent results under L 2 norm. It gives significant more instances that achieves the same distortion and invari-ably more consistent than either scheme. In this dataset, BRP projections are usually sharper as the percentage of instances with low indicated. Sparse random projections somehow can lose out up to 50% in mean and have larger variance in those projections. This is due to the fact that the estimation vectors are more rigid and creates larger distor-tion and hence variance. Compare to Indyk98, BRP is still more consistent and sharper, this is due to the use of nor-malisation and sharper concentration inherent to the Beta distribution. projections usually have much lower variation.
In this paper, we present a novel random projection algo-rithm for dimensionality reduction under L 2 . We show ana-lytically that our algorithm further improves previous work by at least a constant factor up to 40%. Also, our analy-ses demonstrates that the preferred behaviour still holds for very small distortions and very low dimensions. We demon-strate that BRP, for the case of indexing and retrieval under L 2 norm and cosine similarity, could serve as a good alterna-tive to SVD. Moreover, we push the argument and analysis given in [17] further by showing that, surprisingly, direct ap-plication of RP could yield similar results to that of LSI in terms of cosine similarity. Although we do not claim that RP could replace LSI entirely, we believe that this could have further implications on distributed indexing and query processing.

In the future, we would like to further characterise the effect of random projection for highly correlated datasets and its applications in distributed indexing.
 We would like to thank the chair of media informatics at University of Bamberg for providing us with this valuable image dataset. All requests regarding this image collection shall be directed to Prof. Andreas Henrich, Lehrstuhl f  X  ur Medieninformatik, Feldkirchenstr. 21, 96052 Bamberg, Ger-many. Email: andreas.henrich@uni-bamberg.de Also, we thank the anonymous referees for the useful discussion and feedback.
