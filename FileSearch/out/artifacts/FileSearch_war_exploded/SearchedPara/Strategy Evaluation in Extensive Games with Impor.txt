 Evaluating an agent X  X  performance is a component of nearly all research on sequential decision making. Typ-ically, the agent X  X  expected payoff is estimated through Monte Carlo samples of the (often stochastic) agent act-ing in an (often stochastic) environment. The degree of stochasticity in the environment or agent behavior deter-mines how many samples are needed for an accurate esti-mate of performance. For results in synthetic domains with artificial agents, one can simply continue drawing samples until the estimate is accurate enough. For non-synthetic environments, domains that involve human participants, or when evaluation is part of an on-line algorithm, accurate estimates with a small number of samples are critical. This paper describes a new technique for tackling this problem in the context of extensive games.
 An extensive game is a formal model of a sequential in-teraction between multiple, independent agents with im-perfect information. It is a powerful yet compact frame-work for describing many strategic interactions between decision-makers, artificial and human 1 . Poker, for ex-ample, is a domain modeled very naturally as an exten-sive game. It involves independent and self-interested agents making sequential decisions based on both public and private information in a stochastic environment. Poker also demonstrates the challenge of evaluating agent per-formance. In one typical variant of poker, approximately 30,000 hands (or samples of playing the game) are some-times needed to distinguish between professional and ama-teur levels of play. Matches between computer and human opponents typically involve far fewer hands, yet still need to draw similar statistical conclusions.
 In this work, we present a new technique for deriving low variance estimators of agent performance in extensive games. We employ importance sampling while exploit-ing the fact that the strategy of the agent being evaluated is typically known. However, we reduce the variance that importance sampling normally incurs by selectively adding synthetic data that is derived from but consistent with the sample data. As a result we derive low-variance unbiased estimators for agent performance given samples of the out-come of the game. We further show that we can efficiently evaluate one strategy while only observing samples from another. Finally, we examine the important case where we only get partial information of the game outcome (e.g., if a player folds in poker, their private cards are not revealed during the match and so the sequence of game states is not fully known). All of our estimators are then evaluated em-pirically in the domain of poker in both full and partial in-formation scenarios.
 This paper is organized as follows. In Section 2 we in-troduce the extensive game model, formalize our problem, and describe previous work on variance reduction in agent evaluation. In Section 3 we present a general procedure for deriving unbiased estimators and give four examples of these estimators. We then briefly introduce the domain of poker in Section 4 and describe how these estimators can be applied to this domain. In Section 5 we show empirical results of our approach in poker. Finally, we conclude in Section 6 with some directions for future work. We begin by describing extensive games and then we for-malize the agent evaluation problem. 2.1. Extensive Games Definition 1 (Osborne &amp; Rubenstein, 1994, p. 200) a fi-nite extensive game with imperfect information has the fol-lowing components:  X  A finite set N of players .  X  A finite set H of sequences, the possible histories of  X  A player function P that assigns to each non-terminal  X  A function f c that associates with every history h for  X  For each player i  X  N a partition I i of { h  X  H :  X  For each player i  X  N a utility function u i from the A strategy of player i  X  i in an extensive game is a func-tion that assigns a distribution over A ( I i ) to each I A strategy profile  X  consists of a strategy for each player,  X  ,  X  2 , . . . , with  X   X  i referring to all the strategies in  X  ex-cept  X  i .
 Let  X   X  ( h ) be the probability of history h occurring if play-ers choose actions according to  X  . We can decompose  X  this probability. Hence,  X   X  i ( h ) is the probability that if player i plays according to  X  then for all histories h 0 that are a proper prefix of h with P ( h 0 ) = i , player i takes the subsequent action in h . Let  X   X   X  i ( h ) be the product of all players X  contribution (including chance) except player i . The overall value to player i of a strategy profile is then the expected payoff of the resulting terminal node, i.e., u (  X  ) = P z  X  Z u i ( z )  X   X  ( z ) . For Y  X  Z , a subset of possi-ble terminal histories, define  X   X  ( Y ) = P z  X  Y  X   X  ( z ) , to be the probability of reaching any outcome in the set Y given  X  , with  X   X  i ( Y ) and  X   X   X  i ( Y ) defined similarly. 2.2. The Problem Given some function on terminal histories V : Z  X &lt; we want to estimate E z |  X  [ V ( z )] . In most cases V is simply u , and the goal is to evaluate a particular player X  X  expected payoff. We explore three different settings for this problem. In all three settings, we assume that  X  i (our player X  X  strat-egy) is known, while  X  j 6 = i (the other players X  strategies) are not known.  X  On-policy full-information . In the simplest case, we  X  Off-policy full-information . In this case, we get sam- X  Off-policy partial-information . In the hardest case, 2.3. Monte Carlo Estimation The typical approach to estimating E z |  X  [ V ( z )] is through simple Monte Carlo estimation. Given independent sam-ples z 1 , . . . , z t from the distribution  X   X  , simply estimate the expectation as the sample mean of outcome values. As the estimator has zero bias, the mean squared error of the estimator is determined by its variance. If the variance of V ( z ) given  X  is large, the error in the estimate can be large and many samples are needed for accurate estimation. Recently, we proposed a new technique for agent eval-uation in extensive games (Zinkevich et al., 2006). We showed that value functions over non-terminal histories could be used to derive alternative unbiased estimators. If the chosen value function was close to the true expected value given the partial history and players X  strategies, then the estimator would result in a reduction in variance. The approach essentially derives a real-valued function  X  V ( z ) that is used in place of V in the Monte Carlo estimator from Equation 1. The expectation of  X  V ( z ) matches the ex-pectation of V ( z ) for any choice of  X  , and so the result is an unbiased estimator, but potentially with lower vari-ance and thus lower mean-squared error. The specific ap-plication of this approach to poker, using an expert-defined value function, was named the DIVAT estimator and was shown to result in a dramatic reduction in variance. A sim-pler choice of value function, the expected value assuming the betting is  X  X et-call X  for all remaining betting rounds, can even make a notable reduction. We refer to this concep-tually and computationally simpler estimator as (Bet-Call) BC-DIVAT.
 Both traditional Monte Carlo estimation and DIVAT are fo-cused on the on-policy case, requiring outcomes sampled from the joint strategy that is being evaluated. Further-more, DIVAT is restricted to full-information , where the exact outcome is known. Although limited in these re-gards, they also don X  X  require any knowledge about any of the players X  strategies. We now describe our new approach for deriving low-variance, unbiased estimators for agent evaluation. In this section we almost exclusively focus on the off-policy full-information case. Within this setting we observe a sampled outcome z from the distribution  X   X   X  , and the goal is to esti-mate E z |  X  [ V ( z )] . The outcomes are observed based on the strategy  X   X  while we want to evaluate the expectation over  X  , where they differ only in player i  X  X  strategy. This case subsumes the on-policy case, and we touch on the more dif-ficult partial-information case at the end of this section. In order to handle this more challenging case, we require full knowledge of player i  X  X  strategies, both the strategy being observed  X   X  i and the one being evaluated  X  i . At the core of our technique is the idea that synthetic his-tories derived from the sampled history can also be used in the estimator. For example, consider the unlikely case when  X  is known entirely. Given an observed outcome z  X  Z (or even without an observed outcome) we can ex-actly compute the desired expectation by examining every outcome.
 Although impractical since we don X  X  know  X  , V Z ( z ) is an unbiased and zero variance estimator.
 Instead of using every terminal history, we could restrict ourselves to a smaller set of terminal histories. Let U ( z Z )  X  Z be a mapping of terminal histories to a set of ter-minal histories, where at least z 0  X  U ( z 0 ) . We can con-struct an unbiased estimator that considers the history z in the estimation whenever we observe a history from the set U ( z 0 ) . Another way to consider things is to say that U  X  1 ( z ) is the set of synthetic histories considered when we observe z . Specifically, we define the estimator V U ( z ) for the observed outcome z as, The estimator considers the value of every outcome z 0 where the observed history z is in the set U ( z 0 ) . Each outcome though is weighted in a fashion akin to impor-tance sampling. The weight term for z 0 is proportional to the probability of that history given  X  , and inversely pro-portional to the probability that z 0 is one of the considered synthetic histories when observing sampled outcomes from  X   X  . Note that V U ( z ) is not an estimate of V ( z ) , but rather has the same expectation.
 At first glance, V U may seem just as impractical as V Z since  X  is not known. However, with a careful choice of U we can insure that the weight term depends only on the known strategies  X  i and  X   X  i . Before presenting example choices of U , we first prove that V U is unbiased.
 Theorem 1 If  X   X   X  i ( z ) is non-zero for all outcomes z  X  Z , then, i.e., V U is an unbiased estimator.
 Proof: First, let us consider the denominator in the weight term of V U . Since z 0  X  U ( z 0 ) and  X   X   X  i is always positive, the denominator can only be zero if  X   X   X   X  i ( z 0 ) is zero. If this were true,  X   X   X  i ( z 0 ) must also be zero, and as a consequence so must the numerator. As a result the terminal history z is never reached and so it is correct to simply exclude such histories from the estimator X  X  summation.
 Define 1 ( x ) to be the indicator function that takes on the value 1 if x is true and 0 if false.
 The derivation follows from the linearity of expectation, the definition of  X   X   X  , and the definition of expectation. We now look at four specific choices of U for which the weight term can be computed while only knowing player i  X  X  portion of the joint strategy  X  .
 Example 1: Basic Importance Sampling. The simplest choice of U for which V U can be computed is U ( z ) = { z } . In other words, the estimator considers just the sampled history. In this case the weight term is: The weight term only depends on  X  i and  X   X  i and so is a known quantity. When  X   X  i =  X  i the weight term is 1 and the result is simple Monte Carlo estimation. When  X   X  i is different, the estimator is a straightforward application of importance sampling.
 Example 2: Game Ending Actions. A more interest-ing example is to consider all histories that differ from the sample history by only a single action by player i and that action must be the last action in the history. For exam-ple, in poker, the history where the player being evalu-ated chooses to fold at an earlier point in the betting se-quence is considered in this estimator. Formally, define S  X  i ( z )  X  H to be the shortest prefix of z where the re-maining actions in z are all made by player i or chance. Let U ( z ) = { z 0  X  Z : S  X  i ( z ) is a prefix of z 0 } . The weight term becomes, As this only depends on the strategies of player i , we can compute this quantity and therefore the estimator. Example 3: Private Information. We can also use all histories in the update that differ only in player i  X  X  pri-vate information. In other words, any history that the other players wouldn X  X  be able to distinguish from the sampled history is considered. For example, in poker, any history where player i receiving different private cards is consid-ered in the estimator since the opponents X  strategy cannot depend directly on this strictly private information. For-mally, let U ( z ) = z 0  X  Z :  X   X   X   X   X  i ( z 0 ) =  X   X   X  i weight term then becomes, As this only depends on the strategies of player i , we can again compute this quantity and therefore the estimator as well.
 Example 4: Combined. The past two examples show that we can consider histories that differ in the player X  X  private information or by the player mak-ing an alternative game ending action. We can also combine these two ideas and consider any history that differs by both an alternative game ending action and the player X  X  private information. Define Q ( z ) = h  X  H : | h | = | S  X  i ( z ) | and  X   X  X   X   X  i ( h ) =  X   X  Let U ( z ) = { z 0  X  Z : a prefix of z 0 is in Q ( z ) } . Once again this quantity only depends on the strategies of player i and so we can compute this estimator as well. We have presented four different estimators that try to ex-tract additional information from a single observed game outcome. We can actually combine any of these estima-tors with other unbiased approaches for reducing variance. This can be done by replacing the V function in the above estimators with any unbiased estimate of V . In particular, these estimators can be combined with our previous DIVAT approach by choosing V to be the DIVAT (or BC-DIVAT) estimator instead of u i . 3.1. Partial Information The estimators above are provably unbiased for both the-policy and off-policy full-information case. We now briefly discuss the off-policy partial-information case. In this case we don X  X  directly observe the actual terminal history z t only a many-to-one mapping K ( z t ) of the history. One simple adaptation of our estimators to this case is to use the history z 0 in the estimator whenever it is possible that the unknown terminal history could be in U ( z 0 ) , while keep-ing the weight term unchanged. Although we lose the un-biased guarantee with these estimators, it is possible that the reduction in variance is more substantial than the error caused by the bias. We investigate empirically the mag-nitude of the bias and the resulting mean-squared error of such estimators in the domain of poker in Section 5. To analyze the effectiveness of these estimators, we will use the popular game of Texas Hold X  X m poker, as played in the AAAI Computer Poker Competition (Zinkevich &amp; Littman, 2006). The game is two-player and zero-sum. Pri-vate cards are dealt to the players, and over four rounds, public cards are revealed. During each round, the players place bets that the combination of their public and private cards will be the strongest at the end of the game. The game has just under 10 18 game states, and has the properties of imperfect information, stochastic outcomes, and observa-tions of the game outcome during a match exhibit partial information.
 Each of the situations described in Section 2, on-policy and off-policy as well as full-information and partial informa-tion, have relevance in the domain of poker. In particular, the on-policy full-information case is the situation where one is trying to evaluate a strategy from full-information descriptions of the hands, as might be available after a match is complete. For example, this could be used to more accurately determine the winner of a competition involving a small number of hands (which is always the case when humans are involved). In this situation it is critical, that the estimator is unbiased, i.e., it is an accurate reflection of the expected winnings and therefore does not incorrectly favor any playing style.
 The off-policy full-information case is useful for examin-ing past games against an opponent to determine which of many alternative strategies one might want to use against them in the future. The introduction of bias (depending on the strategy used when playing the past hands) is not prob-lematic, as the goal in this case is an estimate with as little error as possible. Hence the introduction of bias is accept-able in exchange for significant decreases in variance. Finally, the off-policy partial-information case corresponds to evaluating alternative strategies during an actual match. In this case, we want to evaluate a set of strategies, which aren X  X  being played, to try and identify an effective choice for the current opponent. The player could then choose a strategy whose performance is estimated to be strong even for hands it wasn X  X  playing.
 The estimators from the previous section all have natural applications to the game of poker:  X  Basic Importance Sampling . This is a straightfor- X  Game ending actions . By selecting the fold betting  X  Private information . In Texas Hold X  X m, a player X  X  Over the past few years we have created a number of strong Texas Hold X  X m poker agents that have competed in the past two AAAI Computer Poker Competitions. To evalu-ate our new estimators, we consider games played between three of these poker agents: S2298 (Zinkevich et al., 2007), PsOpti4 (Billings et al., 2003), and CFR8 (Zinkevich et al., 2008). In addition, we also consider Orange, a competitor in the First Man-Machine Poker Championship.
 To evaluate these estimators, we examined records of games played between each of three candidate strategies (S2298, CFR8, Orange) against the opponent PsOpti4. Each of these three records contains one million hands of poker, and can be viewed as full information (both players X  private cards are always shown) or as partial information (when the opponent folds, their private cards are not re-vealed). We begin with the full-information experiments. 5.1. Full Information We used the estimators described previously to find the value of each of the three candidate strategies, using full-information records of games played from just one of the candidate strategies. The strategy that actually played the hands in the record of games is called the on-policy strat-egy and the others are the off-policy strategies. The results of one these experiments is presented in Table 1. In this ex-periment, we examined one million full-information hands of S2298 playing against PsOpti4. S2298 (the on-policy strategy) and CFR8 and Orange (the off-policy strategies) are evaluated by our importance sampling estimators, as AC+EF+BC-DIVAT 0* 1778 56 AC+EF+BC-DIVAT 2  X  12 2514 80
AC+EF+BC-DIVAT 6  X  12 2421 77 well as DIVAT, BC-DIVAT, and a few combination estima-tors. We present the empirical bias and standard deviation of the estimators in the first two columns. The third col-umn,  X  X MSE X , is the root-mean-squared error of the esti-mator if it were used as the method of evaluation for a 1000 hand match (a typical match length). All of the numbers are reported in millibets per hand played. A millibet is one thousandth of a small-bet, the fixed magnitude of bets used in the first two rounds of betting. To provide some intu-ition for these numbers, a player that always folds will lose 750 millibets per hand, and strong players aim to achieve an expected win rate over 50 millibets per hand. In the on-policy case, where we are evaluating S2298, all of the estimators are provably unbiased, and so they only dif-fer in variance. Note that the Basic estimator, in this case, is just the Monte-Carlo estimator over the actual money lost or won. The Early Folds estimator provides no vari-ance reduction over the Monte-Carlo estimate, while the All Cards estimator provides only a slight reduction. How-ever, this is not nearly as dramatic as the reduction pro-vided by the DIVAT estimator. The importance sampling estimators, however, can be combined with the DIVAT es-timator as described in Section . The combination of BC-DIVAT with All Cards ( X  X C+BC-DIVAT X ) results in lower variance than either of the estimators separately. 3 The addition of Early Folds ( X  X C+EF+BC-DIVAT X ) produces an even further reduction in variance, showing the best-performance of all the estimators, even though Early Folds on its own had little effect.
 In the off-policy case, where we are evaluating CFR8 or Or-ange, we report the empirical bias (along with a 95% con-fidence bound) in addition to the variance. As DIVAT and BC-DIVAT were not designed for off-policy evaluation, we report numbers by combining them with the Basic estima-tor (i.e., using traditional importance sampling). Note that bias is possible in this case because our on-policy strategy (S2298) does not satisfy the assumption in Theorem 1, as there are some outcomes the strategy never plays. Basic importance sampling in this setting not only shows statis-tically significant bias, but also exhibits impractically large variance. DIVAT and BC-DIVAT, which caused consid-erable variance reduction on-policy, also should consider-able variance reduction off-policy, but not enough to offset the extra variance from basic importance sampling. The All Cards estimator, on the other hand, shows dramatically lower variance with very little bias (in fact, the empirical bias is statistically insignificant). Combining the All Cards estimator with BC-DIVAT and Early Folds further reduces the variance, giving off-policy estimators that are almost as accurate as our best on-policy estimators.
 The trends noted above continue in the other experiments, when CFR8 and Orange are being observed. For space con-siderations, we don X  X  present the individual tables, but in-stead summarize these experiments in Table 2. The table shows the minimum and maximum empirically observed bias, standard deviation, and the root-mean-squared error of the estimator for a 1000 hand match. The strategies be-ing evaluated are separated into the on-policy case, when the record involves data from that strategy, and the off-policy case, when it doesn X  X . 5.2. Partial Information The same experiments were repeated for the case of partial information. The results of the experiment involving S2298 playing against PsOpti4 and evaluating our three candidate strategies under partial information is shown in Table 3. For DIVAT and BC-DIVAT, which require full information of the game outcome, we used a partial information vari-ant where the full-information estimator was used when the game outcome was known (i.e., no player folded) and win-nings was used when it was not. This variant can result in a biased estimator, as can be seen in the table of results. The All Cards estimator, although also without any guarantee of being unbiased, actually fares much better in practice, not displaying a statistically significant bias in either the off-policy or on-policy experiments. However, even though the DIVAT estimators are biased their low variance makes them preferred in terms of RMSE in the on-policy setting. In the off-policy setting, the variance caused by Basic im-portance sampling (as used with DIVAT and BC-DIVAT) makes the All Cards estimator the only practical choice. As in the full-information case we can combine the All Cards and BC-DIVAT for further variance reduction. The resulting estimator has lower RMSE than either All Cards or BC-DIVAT alone both in the on-policy and off-policy cases. The summary of the results of the other experiments, showing similar trends, are shown in Table 4. We introduced a new method for estimating agent perfor-mance in extensive games based on importance sampling. The technique exploits the fact that the agent X  X  strategy is typically known to derive several low variance estima-tors that can simultaneously evaluate many strategies while playing a single strategy. We prove that these estimators are unbiased in both the on-policy and off-policy case. We empirically evaluate the techniques in the domain of poker, showing significant improvements in terms of lower vari-ance and lower bias. We show that the estimators can also be used even in the challenging problem of estimation with partial information observations.
 We would like to thank Martin Zinkevich and Morgan Kan along with the entire University of Alberta Computer Poker Research Group for their valuable insights. This research was supported by NSERC and iCore.

