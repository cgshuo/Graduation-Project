
Nowadays, many real-world networks possess intrinsic community structure, such as large social networks, Web graphs, and biological networks. A community (also referred to as module or cluster) is typically thought of as a group of nodes with dense connections within groups and sparse connections between groups as well. Community discovery within networks is an important problem with many applica-tions in a number of disciplines ranging from social network analysis to image segmentation and from analyzing protein interaction networks to the circuit layout problem.
Finding communities in complex networks is a nontrivial task, since the number of communities in the network is typically unknown and the communities often have arbitrary size and shape. Moreover, besides the cluster nodes densely connected with communities, there are some nodes in special roles like hubs and outliers. As we know, hubs play impor-tant roles in many real-world networks. For example, hubs in the WWW could be utilized to improve the search engine rankings for relevant authoritative Web pages [1], and hubs in viral marketing and epidemiology could be central nodes for spreading ideas or diseases. On the contrary, outliers are marginally connected with the community members. Since the characteristics of outliers deviate significantly from the communities, they should be isolated as noise. Therefore, how to detect communities as well as hubs and outliers in a network becomes an interesting and challenging problem.
Most existing approaches only study the problem of community detection without considering hubs and outliers. A density-based network clustering algorithm SCAN [2] can overcome this difficulty. However, it needs user to specify a minimum similarity  X  and a minimum cluster size  X  to define clusters, and is sensitive to the parameter  X  which is difficult to determine. Actually, how to locate the optimal parameter  X  automatically for the density-based clustering methods (e.g., DBSCAN [3] and SCAN) is a long-standing and challenging task.

The connectivity structure of a large-scale network is highly complex, which makes the selection of optimal pa-rameter  X  difficult. However, we have found that a cluster defined by density-based clustering algorithms is composed of two types of objects: cores and borders. The cluster can be determined by the density-connected cores embedded in it uniquely. Hence, once all the components of connected cores have been detected, all clusters can be revealed. Accordingly, we convert the problem of detecting  X  -clusters in a network to finding core-connected components in the network weighted by a new measurement: core-connectivity-similarity. It is equal to partitioning the core-connected network with a minimal threshold  X  (i.e., remove all the edges whose weights are below  X  from the network). Actually, the problem above can be easily solved by the Maximal Spanning Tree (MST), a connectivity skeleton of the network [4]. To motivate this, we illustrate a schematic network in Figure 1(a). Given  X  =3 , if all the edges with weights no more than current  X  are removed from the network, two unconnected sub-graphs will emerge, as shown in Figure 1(c). In contrast, if we first construct the MST of the network, as shown in Figure 1(b), and then partition the MST with the same threshold, the partitioning result on the MST is equal to that on the original network. In the same way, the core-connected components can be detected on the Core-Connected Maximal Spanning Tree (CCMST) of the network. Since each edge of the CCMST is a cut edge, different edge weights will lead to different partition results and form different clusters. It also means that all possible  X  , which we can adopt to cluster the network, can be found in the edge weights of the CCMST. Furthermore, the best clustering result can be selected by a quality function.
In this paper, a novel density-based network clustering algorithm, called gSkeletonClu, is proposed to perform the clustering on the CCMST. To our best knowledge, our work is the first that builds the connection between the density-based clustering principle and the MST-based clustering method. The main contributions of this paper are summa-rized in the following: 1) By projecting the network to its CCMST, we convert 2) We discover that all possible values of the parameter  X  3) Our algorithm not only uncovers the communities 4) Our algorithm can overcome the chaining effect that
The rest of the paper is organized as follows. First we briefly review the related work in Section 2. In section 3, we introduce the concepts of structural-connected clusters. In section 4, we formulize the notion of clusters derived from the CCMST and present the theoretical analysis. In section 5, we describe the algorithms in detail. In section 6, we report the experimental results. Finally, we summarize our conclusions and suggest future work in section 7.
The problem of detecting communities within networks has been extensively studied for years. Graph partitioning is a natural choice for this problem, such as Kernighan-Lin algorithm [5], Metis [6], and normalized cut [7]. To make the calculation of cut functions more efficient, spectral clustering method has been proposed where the eigenvectors of certain normalized similarity matrices are used for the clustering purpose [8]. Since the community structure in networks is highly complex, new clustering methods have recently been introduced to solve this challenging problem.

Density-based Clustering Methods: Density-based clus-tering approaches (e.g., DBSCAN [3] and OPTICS [9]) have been widely used in data mining owing to their ability of finding clusters of arbitrary shape even in the presence of noise. Recently, Xu et al. proposed a structural network clus-tering algorithm SCAN [2] extended from DBSCAN. This algorithm can find communities as well as hubs and outliers. However, the main difficulty for the SCAN algorithm is that it is sensitive to the minimal threshold  X  of the structure-similarity. A simple  X  X nee hypothesis X  has been presented to locate the parameter  X  manually for the SCAN algorithm. However, the knee dose not always correspond to the optimal  X  value. More importantly, there are no obvious knees on the k -nearest ranked similarity plot for most of the real-world networks. To deal with this problem, Bortner et al. proposed a new algorithm, called SCOT+HintClus [10], to detect the hierarchical cluster boundaries of network through extension of the algorithm OPTICS [9]. However, it does not find the global optimal  X  . Our work tries to solve the sensitive parameter problem of density-based network clustering from another angle.

Graph-theoretical Clustering Methods: Clustering al-gorithms based on graph theory can be used to detect clusters of different shapes and sizes. One of the best-known graph-based hierarchical clustering algorithms is based on the construction of the minimal (or maximal) spanning tree (MST) of the objects which was initially proposed by Zahn [11]. The standard MST divisive algorithm removes edges from the MST in order of decreasing length until the specified number of clusters results. A clustering algorithm using an MST takes the advantage that it is able to only consider the necessary connections between the data patterns and the cost of clustering can be decreased. However, the number of the desired clusters should be given in advance. Moreover, simple linkage-based methods often suffer from the problem of chaining effect. Some clustering algorithms have been shown closely related to MST, such as Single-link [12]. Recently, some researchers utilized the MST-based clustering method to analyze complex networks [13]. Our work introduces tree clustering into the framework of density-based clustering which is rather different from the traditional MST-based clustering method.

Quality Functions for Network Clustering: Clustering validity checking is an important issue in cluster analysis [14]. For community detection, the most popular quality function is the modularity measure  X  proposed by Newman and Girvan [15]. Recently, a similarity-based modularity function  X   X  was presented by Feng et al. [16] through extension from the connection-based modularity  X  , which has a better ability to deal with hubs and outliers. The  X  function is defined as follows: where  X  is the number of clusters,  X  X  X   X  = is the total similarity of nodes within cluster  X   X  ,  X  X  X   X   X   X   X   X   X  , X   X   X   X  (  X ,  X  ) is the total similarity between nodes in cluster  X   X  and any node in the network, and  X  X  X  =  X   X , X   X   X   X  (  X ,  X  ) is the total similarity between any two nodes in the network.

Modularity optimization itself is a popular method for community detection. Most modularity-based algorithms find the optimal clustering result via greedily maximizing the value of  X  , such as FastModularity [17] and Simulated Annealing (SA for short) [18]. However, recent research shows that modularity is not a scale-invariant measure, and hence, by relying on its maximization, detection of communities smaller than a certain size is impossible. This serious problem is well known as the resolution limit [19]. Compared with the traditional modularity-based methods, our work uses modularity as a quality function to guide the selection of optimal clustering results.

Given a minimum similarity  X  and a minimum cluster size  X  , the main idea of structure-connected clustering is that for each node in a cluster, it must have at least  X  neighbors whose structural similarities are at least  X  . In this section, we formulize the notion of a structure-connected cluster, which extends that of a density-based cluster [3], [9]. Definition 1. (Structural Similarity) Let  X  =(  X , X , X  ) a weighted undirected network and  X  (  X  ) be the weight of the edge  X  . For a node  X   X   X  , we define  X  ( {  X ,  X  } )=1 The structure neighborhood of a node  X  is the set  X (  X  ) containing  X  and its adjacent nodes which are incident a common edge with  X  : X (  X  )= {  X   X   X   X {  X ,  X  } X   X  } X  X   X  } . The structural similarity between two adjacent nodes  X  and  X  is then
The above structural similarity is extended from a cosine similarity used in [2] which denotes the local connectivity density of any two adjacent nodes in an undirected network. It can be replaced by other similarity definitions such as Jaccard similarity, and our experimental results show that the cosine similarity is better.
 Definition 2. (Core) For a node  X  , the set of neighbors having structural similarity greater than  X  forms the neighborhood of  X  : If  X   X   X  (  X  )  X  X  X   X  , then  X  is a core node , denoted by K  X , X 
A core node is the one whose  X  -neighborhood is at least  X  . As shown in Figure 2, when we set  X  =0 . 75 and  X  =4 , node  X  is a core and node  X  is in the  X  -neighborhood of  X  . According to the principle of density-based clustering, the clusters grow from core nodes. If a node is in the  X  -neighborhood of a core, it should be in the same cluster with the core. This idea is formulized in the following definition of direct structure reachability.
 Definition 3. (Structure-Reachable) Given  X   X   X  ,  X   X   X  a node  X   X   X  is directly structure-reachable from a node  X   X   X  iff K  X , X  (  X  )  X   X   X   X   X  (  X  ) , denoted by  X   X   X , X   X   X   X  is structure-reachable from  X   X   X  iff  X  X   X  1 , ...,  X   X  s . t .  X  =  X  1 , X  =  X   X  , and  X   X   X  X  1 , 2 , ...,  X   X  1 }  X 
A node  X   X   X  is directly structure-reachable from a core node  X   X   X  if  X   X   X   X  (  X  ) . Obviously, directly structure-reachable is symmetric for pairs of core nodes. If a non-core node  X  is directly structure-reachable from a core node  X  then  X  is a border node attached to  X  . In general, directly structure-reachable is not symmetric if a core node and a border node are involved.
 We depict the notion of structure-reachable in Figure 2. When we set  X  =0 . 75 and  X  =4 , there are four nodes in the neighborhood of node  X ,  X  and  X  respectively, whose structure similarities with them are greater than current thus  X ,  X  and  X  are all core nodes. Since the structure similarity between  X  and  X  is greater than 0.75,  X  and  X  are directly structure-reachable from each other. And for the same reason,  X  and  X  are also directly structure-reachable from each other. According to Definition 3, core nodes  X ,  X  and  X  are structure-reachable from each other. However, the border node  X  is only structure-reachable from the three core nodes on one side.

The transitive closure of directly structure-reachable rela-tion forms clusters, and any pair of nodes in the same cluster is structure connected . Definition 4. (Structure-Connected Cluster) The set  X  [  X  ]  X   X  is a cluster represented by K  X , X  (  X  )  X   X  iff (1)  X   X   X  [  X  ] (2)  X   X   X   X  ,  X   X   X , X   X   X   X   X   X  [  X  ] ; and (3)  X   X  [  X  ]  X  X  X   X .
Acluster  X  contains exactly the nodes which are structure-reachable from an arbitrary core in  X  . If there are two core nodes  X ,  X   X   X  s . t .  X   X   X , X   X  , then  X  [  X  ]=  X  [  X  ] Consequently, a cluster is uniquely determined by the con-nected core nodes in it. As shown in Figure 3, cluster 1 contains all the nodes which are structure-reachable from core nodes  X ,  X  and  X  . Thus,  X  [  X  ]=  X  [  X  ]=  X  [  X  ] . There also may exist some border nodes (e.g., node  X  in Figure 3).
Given parameters  X  and  X  ,a clustering of a network  X  is the set  X  X  X   X , X  of distinct structure-connected clusters found in the network. After the network is clustered, there are still some nodes that are not suitable to be assigned to any cluster and they may be hubs or outliers.
 Definition 5. (Hub and Outlier) Given parameters  X ,  X  , and a clustering  X  X  X   X , X  of the network  X  , a node  X   X   X  is a hub iff (1)  X  does not belong to any cluster:  X   X  [  X  ]  X   X  X  X   X /  X   X  [  X  ] ;(2)  X  bridges multiple clusters:  X   X ,  X   X   X  X  X   X   X  =  X  ,  X   X   X   X   X   X   X  ,  X  .  X  .  X   X   X (  X  )  X   X   X   X (  X  ) that is not in a cluster and not a hub is called an outlier .
As shown in Figure 3, node  X  that connects two nodes of different clusters is regarded as a hub, and nodes  X  1  X  , which are connected with only one cluster, should be regarded as outliers.

To introduce the notion of core-derived clusters, we make the following observation: each cluster is determined by the structure-connected core nodes in it. Given {  X ,  X  } X   X  ,we must decide whether  X  and  X  are core nodes w.r.t. current  X  and whether  X  and  X  are structure-reachable from each other. If so,  X  and  X  will lie in the same cluster. Our new algorithm gSkeletonClu bases on the principle of finding the core-connected components at different  X  levels. The involved concepts are introduced in the following. A. Core Connectivity Similarity Definition 6. (Core-Similarity) Given a node  X   X   X  ,the core-similarity of  X  is
The core-similarity of a node  X  is the maximum of structural-similarity  X   X  such that  X  would be a core node w.r.t.  X   X   X   X  (  X  )  X  X  X   X  . Otherwise, the core-similarity is zero. As shown in Figure 4(a), the core-similarity of node  X  is 0.75 when  X  =4 , because the size of the  X  -Neighborhood of node  X  is just four when  X  =0 . 75 and node  X  will no longer be a core when  X &gt; 0 . 75 .
 Definition 7. (Reachability-Similarity) Given  X  ,  X   X   X  ,the reachability-similarity of  X  w.r.t.  X  is
Intuitively, the reachability-similarity of a node  X  w.r.t. a core node  X  is the maximum of structural similarities such that  X  is directly structure-reachable from  X  .Asshownin Figure 4(a), the reachability-similarity of node  X  w.r.t. core node  X  is 0.75 which is equal to the core-similarity of node because the similarity between the two nodes is not less than the core-similarity of  X  . While the reachability-similarity of node  X  w.r.t. core node  X  is 0.7 which is equal to the similarity between  X  and  X  .
 Definition 8. (Core-Connected) Given  X   X   X  ,  X   X   X  ,  X ,  X   X   X  ,  X  and  X  are directly core-connected with each other iff K  X , X  (  X  )  X  K  X , X  (  X  )  X   X   X   X , X   X  . This is denoted by  X   X 
Since directly structure-reachable is symmetric for any pair of core nodes, if two core nodes are directly structure-reachable from each other, then they are directly core-connected . The transitive closure of the directly core-connected relation forms core-connected components, and any pair of nodes in the same component are core-connected . Definition 9. (Core-Connectivity-Similarity) Given {  X ,  X  } X   X  ,the core-connectivity-similarity of  X  and  X  is
The core-connectivity-similarity of two nodes is the min-imum of their core-similarities and the structure-similarity between them. As shown in Figure 4(a), nodes  X  and  X  are core-connected when  X  =0 . 75 and  X  =4 , because  X  and  X  are both core nodes and their structure-similarity is not less than the current  X  . But if we set current  X &gt; 0 . 75 , nodes and  X  are not core-connected any more, because  X  will not be a core under this configuration. Thus, the core-connectivity-similarity of  X  and  X  is 0.75 when  X  =4 . The following Theorem 1 shows that the core-connectivity-similarity of two nodes is the maximal structural similarity such that they are both core nodes and directly core-connected from each other. Theorem 1. Given a network  X  =(  X , X , X  ) , {  X ,  X  } X   X  ,  X   X   X  ,if  X  X  X  X  (  X ,  X  )= X   X  , then  X   X  is the maximum of
For each edge  X  = {  X ,  X  } X   X  in a weighted net-work  X  =(  X , X , X  ) , we calculate the core-connectivity-similarity for each pair of adjacent nodes  X  and  X  . Set  X  (  X  )=  X  X  X  X  (  X ,  X  ) . Then we get a core-connected network  X  =(  X , X , X  ) . Note that the core-connectivity-similarity of any two adjacent nodes is equal to their structure similarity when  X  =2 , because here the core-similarity of two adjacent nodes is equal to the highest similarity of their incident edges which is not less than their structure-similarity. Hence, the CCMST of a core-connected network is equal to the MST of the original network when  X  =2 .
 B. Partition Networks on Its CCMST
According to the Cut Property of MST, we can prove that partitioning core-connected network  X  with  X  (remove edge  X  s.t.  X  (  X  )  X   X  from  X  ) is equal to the partition of its CCMST with  X  .
 Definition 10. (Connectivity Level) Let  X  =(  X , X , X  ) be a core-connected undirected network and each edge  X   X   X  has a value of core-connectivity-similarity  X  (  X  )  X  [0 , 1] nodes  X ,  X   X   X  ,  X   X  =  X  and  X   X   X  ,  X  and  X  are connected if each edge  X   X   X  X  X . X . X  (  X  ) &lt; X  is removed from  X  they are not connected if each edge  X   X   X  X  X . X . X  (  X  )  X   X  is removed from  X  . Then the connectivity level of  X  and  X  in  X  is  X  . This is denoted by  X   X  (  X ,  X  )=  X  .
 Theorem 2. Let  X  =(  X , X , X  ) be a core-connected undirected network and  X  be an MST of  X  .  X   X ,  X   X   X , X   X  =  X , X   X  (  X ,  X  )=  X   X  (  X ,  X  ) .
 the path between  X  and  X  in  X  . Obviously,  X   X  = {  X ,  X  } X   X  X  X . X . X  (  X  )=  X  and  X  is the minimal edge weight in  X  . When each edge  X  s . t .  X  (  X  ) &lt; X  is removed from  X  , path still remains in  X  . Thus, nodes  X  and  X  will stay connected in  X  .

Assume that each edge  X   X   X  s . t .  X  (  X  )  X   X  is removed from  X  ,  X  and  X  are still connected. There must be a path  X   X  between  X  and  X  in  X  s.t.  X   X   X   X   X  ,  X  (  X  ) &gt; X  .So  X   X   X  = {  X   X  , X   X  } X   X   X   X   X   X  /  X   X  , otherwise  X  and  X   X  a cycle in  X  .  X   X  =(  X   X  X   X  } )  X  X   X   X  } is also a Spanning Tree of  X  s.t.  X  (  X   X  ) &gt; X  (  X  ) . It is inconsistent with that  X  is a maximal spanning tree of  X  .So  X  and  X  will not be connected when each edge  X   X   X  s.t.  X  (  X  )  X   X  is removed from  X  . Thus  X   X  (  X ,  X  )=  X   X  (  X ,  X  )=  X .

Since each edge of  X  is a cut edge, different edge weights of  X  will produce different partition results and form different clusters. Without considering the slight effect of border nodes, all possible  X  values lie in the edge weights of the CCMST. C. Bulid the Attractor Indices
The partition on the CCMST of a network can detect the core nodes for each cluster. In addition, there may exist some border nodes in a cluster. In order to attach the border nodes of each cluster efficiently, we build the indices in advance. The involved concepts are given as follows.
 Definition 11. (Attachability-Similarity) Given  X   X   X  ,the attachability-similarity of  X  w.r.t. its neighbor nodes is
The attachability-similarity of a node  X  is the maximum of reachability-similaries w.r.t. its neighbor nodes. The neigh-bor node that possesses the maximal reachability-similarity to  X  is regarded as the attractor of  X  .

If  X  X  X  (  X  ) &lt; X  X  X  (  X  ) (i.e., the core-similarity of  X  is less than its attachability-similarity), then  X  is not a core node, but its attractor  X  is a core node when  X  =  X  X  X  (  X  ) . That is to say,  X  should be attached as a border node to the cluster containing core node  X  . In this case, an index of the attachability-similarity for node  X  and its attractor  X  will be built in advance. As shown in Figure 4(b), the reachability-similarities of node  X  w.r.t. its neighbor nodes are labeled on the arrowhead lines. Since the value of reachability-similarity from  X  to  X  is the highest, node  X  should be the attractor of  X  and the attachability-similarity of node  X  0.82 which is equal to the reachability-similarity of  X  w.r.t  X  . Note that the attractor can be selected arbitrarily with ties and the indices can be built during the calculation of core-connectivity-similarity.

Our network clustering algorithm on the CCMST includes two phases, and the procedure is illustrated in Figure 5. In the first phase, we calculate the core-connectivity-similarity for each edge in the network, build the attractor indices and construct the CCMST accordingly. After that, we record all the different edge weights of the CCMST as the  X  candidates and sort them ascending or descending. In the second phase, we perform the tree clustering on the CCMST. Actually, the clustering results are not sensitive to the parameter  X  which can be set as a constant.

There are two different ways to implement the tree cluster-ing. The first is tree agglomerative clustering and the pseudo-code is given in Algorithm 1. We consider an initial forest consisting of  X  isolated nodes. Then we add the edges having the same weights in the CCMST to the forest, from the highest value to the lowest. For each  X  , it will agglomerate the nodes into multiple core-connected components after all the edges whose weights are equal to  X  have been added to the forest. After attaching the border nodes, we can get the clusters. Then we calculate the  X   X  value of current clustering result. The output of the algorithm is the clusters with the highest  X   X  value and the corresponding  X  .

The second method is tree divisive clustering. We remove the edges in the CCMST w.r.t. different weights or  X  , from the lowest value to the highest. For each  X  , it will partition the tree into multiple core-connected components. The remaining process is the same as the agglomerative one.
The two tree clustering algorithms above are equivalent in clustering results, but the agglomerative one is convenient for detecting the connected components in the forest. Thus, it is faster than the divisive one. Because the core-connectivity-similarity of any two nodes is equal to their structure-similarity when  X  =2 , the procedure of attaching borders can be passed over in this case. Hence, our algorithm contains the traditional MST-based clustering as a special case. We also compared our algorithm with the SCAN al-gorithm. Though the clustering procedures of gSkeletonClu and SCAN are quite different, the results of these two algorithms are almost the same w.r.t. the same values of parameters  X  and  X  . In addition, our algorithm can detect overlapping communities by permitting hubs and border nodes to be shared by multiple clusters.

The running time of gSkeletonClu is mainly consumed by the construction of CCMST and tree clustering. The core-connectivity-similarity and attractor indices can be cal-culated within a complexity of  X  (  X  ) . We construct the CCMST of the core-connected network using the Prim X  X  al-gorithm with a Fibonacci Heap. Its running time complexity is  X  (  X  log  X  ) . In the procedure of agglomerative clustering, we only need to add the edges to the forest in order of weights with a complexity of  X  (  X  log  X  ) . However, the calculation of  X   X  is a little time consuming. We implement Algorithm 1 gSkeletonClu Agglomerative it in an incremental way and the complexity is  X  (  X  ) when we try all possible  X  . In total, the time complexity of our algorithm is  X  ((  X  +  X  )log  X  ) . For the scale-free networks, it is  X  (  X  log  X  ) . The algorithm can also be implemented in a more efficient way, which starts the clustering process from the 1/2 of ordered edges and stops when the current  X   X  value decreases by 0.05 from the previous one. The optimized algorithm reduces almost 1/2 running time with the same clustering results.

In this section, we evaluate the proposed algorithm using some real-world networks and synthetic datasets. The perfor-mance of gSkeletonClu is compared with two state-of-the-art methods: SCAN and FastModularity. SCAN is an efficient density-based network clustering algorithm, and FastMod-ularity is a representative modularity-based algorithm for community detection. Our algorithm is implemented using ANSI C++. All the experiments were conducted on a PC with a 2.4 GHz Pentium IV processor and 4GB of RAM. A. Datasets
We evaluate the performance of our algorithm on two types of datasets. One is the real-world networks and the other is the computer-generated benchmark networks with known community structure. 1) Real-world Networks: To assess the performance of the proposed method in terms of accuracy, we conduct experiments on two popular real-world networks: NCAA College-football network and US Political Books network [20]. The NCAA College-football is a social network with communities (or conferences) of American college football teams. The network, representing the schedule of Division I-A games for the 2000 season, contains 115 nodes and 613 edges. All college football teams are divided into eleven conferences and five independent teams (Utah State, Navy, Notre Dame, Connecticut and Central Florida) that do not belong to any conference. There is a link between two teams if they played a game together. Now the question is to find out the communities from the graph that represents the schedule of games played by all teams. The network of US Political Books contains 105 nodes and 441 edges. The nodes of the network represent the US political books sold by the online bookseller Amazon.com and have been given labels  X  X iberal X ,  X  X eutral X , or  X  X onservative X , respectively, by Newman. There is a link between two books if they are co-purchased frequently enough by the same buyers. 2) Synthetic Benchmark Networks: We also use the Lancichinetti-Fortunato-Radicchi (LFR) benchmark graphs [21] to evaluate the performance of our algorithm. By varying the parameters of the networks, we analyze the behavior of the algorithms in detail. Some important parameters of the benchmark networks are:  X   X  : number of nodes  X   X  : average number of edges  X   X  : average degree of the nodes  X   X  X  X  X  X  X  : maximum degree  X   X  X  X  : mixing parameter, i.e., each node shares a fraction  X  X  X  of its edges with nodes in other communities  X   X  X  X  X  X  X  : minimum for the community sizes  X   X  X  X  X  X  X  : maximum for the community sizes
We generate several weighted undirected benchmark net-works with the number of nodes  X  = 10,000 and 100,000. In Table I, we list the values of the parameters for the generated datasets. For each  X  , two types of networks are generated with different ranges of the community sizes, where S means that the sizes of the communities in the dataset are relatively small and B means that the sizes of the communities are relatively big. For each type of datasets, we generate fifteen networks with different mixing parameter  X  X  X  ranging from 0.1 to 0.8 with a span of 0.05. Generally, the higher the mixture parameter of a network is, the more difficult it is to reveal the community structure.
 B. Selection of the Parameter  X 
In [2], the authors presented a  X  X nee hypothesis X  to find the proper  X  value. We sort 3-nearest similarity of all nodes in the networks to locate the knee. Two plots for the US Political Books and the benchmark 10000B are given respectively in Figure 6, and it can be observed that there are no obvious knee in the curves. Furthermore, there is no rigorous way to ensure that the identified  X  X nees X  are the appropriate values of the parameter  X  .

In the experiment, we try our best to select a  X  X nee X  as the parameter  X  for the SCAN algorithm from the 3-nearest similarity plot of each network. In contrast, our algorithm locate the optimal  X  automatically which achieves the highest  X   X  value. In Table II, we show the manually selected values of  X  and the corresponding  X   X  for some adopted datasets, as well as the optimal  X  and the corresponding  X   X  values found by gSkeletonClu, where  X  X  X  =0 . 4 for all of the benchmark networks. As shown in Table II, the manually selected  X  is always not the optimal one because it depends greatly on the intuition of user.
 We also present the relationship between  X  and  X   X  in Figure 7. It can be observed that the maximum of  X   X  in the curve is always near the middle of all possible  X  values. Thus we can locate the optimal  X  by testing only a portion of all possible values. We introduce two parameters for our algorithm:  X  and  X  . The parameter  X  indicates the start position of all the sorted edges in the CCMST and the parameter  X  indicates the stop criteria. For example, if we set  X  =0 . 5 and  X  =0 . 05 , it means that the clustering process will start from a half of all edges with higher weights which have been appended in the forest and stop when the current  X   X  value decreases by 5% from the previous one.
 C. Criteria for Accuracy Evaluation
In our experiments, we adopt Normalized Mutual Infor-mation (NMI) [22], an information-theoretic based measure-ment, to evaluate the quality of clusters generated by differ-ent methods. This is currently widely used in measuring the performance of network clustering algorithms. Formally, the measurement metric NMI can be defined as where  X  is the confusion matrix,  X   X  X  X  is the number of nodes in both  X   X  and  X   X  ,  X   X . is the sum over row  X  of  X  and the sum over column  X  of  X  . Note that the NMI value ranges between 0.0 (total disagreement) and 1.0 (total agreement). D. Accuracy Comparison on Real-world Networks
NCAA College-football network: Figure 8 illustrates the original NCAA College-football network and the clustering result of our algorithm with each node representing a school team. For the original network, the conferences and the group of five independent teams are indicated by cliques. Our algorithm obtains eleven clusters in this network which demonstrates a perfect match with the original conference system. The teams belonging to a conference and the independent teams are denoted by circles and diamonds respectively, and the teams in the same conferences are represented by the same color. Four independent teams are correctly identified as hubs. Although another four teams (i.e., Louisiana Monroe, Louisiana Lafayette, Louisiana Tech, and Middle Tennessee State) are identified as hubs, and three other teams are misclassified, our algorithm still performs much better than other methods including SCAN and FastModularity, which will be described as follows.
The SCAN algorithm detects thirteen communities as its best result in this dataset with parameters (  X  =0 . 5466  X  =3 ). The teams of two conferences are divided into two clusters respectively. Meanwhile, it finds ten hub nodes with only four are correctly identified, and one independent team UtahState is misclassified into a conference. It follows that the parameter value selected manually lowers the accuracy of SCAN. The modularity-based algorithm FastModularity discovers seven communities, but only four communities matching with the conferences. For the five independent teams, they are assigned to three different communities.
US Political Books network: The original network and the clustering result of our algorithm are presented in Fig-ure 9. For the original network, the  X  X iberal X ,  X  X eutral X  and  X  X onservative X  books are represented by circle, triangle and box, respectively. Our algorithm successfully identifies three clusters and four hubs with  X  =4 . The clusters detected by our algorithm are illustrated by three different colors: blue for  X  X iberal X  books, thistle for  X  X eutral X  books and red for  X  X onservative X  books. In addition, the hubs which do not belong to any cluster are denoted by pink diamonds.
The SCAN algorithm detects three clusters and nine hubs in the network with parameters (  X  =0 . 4376 , X  =4 ). The FastModularity algorithm finds four clusters in this network, and even worse it mixes some nodes from all the three communities together as a new cluster.

In summary, gSkeletonClu generates promising clustering results along with hubs and outliers in community detec-tion, consistently outperforming baseline methods including SCAN and FastModularity.
 E. Accuracy Comparison on Synthetic Networks
For a more standardized comparison, we turn to the re-cently proposed LFR benchmark graphs, which are claimed to possess properties found in real-world networks and incorporate more realistic scale-free distributions of node-degree and cluster-size [21]. We compare the three clustering algorithms on the networks with size of 10,000 and 100,000. The NMI scores of the three methods are plotted in Figure 10. On most of the datasets, our algorithm gets  X  X  X  X  =1 as  X  X  X  &lt; 0 . 4 , which means a perfect match with the original network structure. However, the performance of our algorithm decreases when  X  X  X  &gt; 0 . 4 , especially in the small-scale network with big communities (e.g., 10000B). This is because that more and more hubs and outliers are isolated with the increasing of parameter  X  X  X  .Asshownin Figure 10, the performance of gSkeletonClu is better than that of FastModularity on all generated networks, because the FastModularity algorithm tends to produce a small number of big communities on the large-scale networks, due to the well known resolution limit of modularity [19].
For SCAN, its NMI values are lower than that of our algorithm but higher than FastModularity in most cases. This verifies the advantage of the density-based methods in community detection within complex network. However, we observe that the clustering results of SCAN are sometimes unreasonable. For example, the NMI value of SCAN on the network with  X  X  X  =0 . 6 is higher than that with  X  X  X  =0 . 5 in Figure 10(b), and the NMI value of SCAN on the network with  X  X  X  =0 . 1 is much lower than that with  X  X  X  =0 . 2 in Figure 10(d). This is in conflict with the characteristic of the LFR benchmark networks, which demonstrates that SCAN is sensitive to the parameter  X  and the manually selected parameter can not provide the optimal clustering results.
The NMI values clearly demonstrate that gSkeletonClu can always locate the optimal  X  and produce clusters that resemble the true classes of the datasets in our study. When we use the optimal  X  found by our algorithm as the input parameter for SCAN, it can get the same clustering results in most cases. But there will still be some minor differences because our algorithm assigns the border node to its neighbor core node with the maximum of reachability-similarity, while SCAN assigns the border node to the core node which associates with it first.
 F. Tackling the Problem of Chaining Effect
The traditional MST clustering method may suffer from the problem of chaining effect (also called as single-link effect), which is caused by some linearly connected nodes that run through a sparse area. In Figure 11, we present a network, called Dumbbell, which consists of two cliques with five nodes connected by another five nodes in single links. If we use the MST clustering approach to cluster the original weighted Dumbbell network, we can get only one cluster containing all the nodes in the network. The reason is that the weights of the edges in the MST are all equal to five. But if we use the MST clustering algorithm smoothed by local similarity (i.e., gSkeletonClu with  X  =2 ), we get three clusters: { 1,2,3,4,5 } , { 6,7,8,9,10 } , and { 11,12,13,14,15 Interestingly, our algorithm gSkeletonClu obtains the most reasonable clustering results with  X  =4 or 5 , in which the two density cliques are discovered as two clusters and the middle five nodes connected by single links are identified as outliers.

Obviously, our algorithm can overcome the chaining effect because it uses structure similarity as edge weight, which is smoothed by local link-density. Moreover, the parameter  X  can be used to determine the minimal size of clusters freely. G. Analyzing the Problem of Resolution Limit
Despite the good performance of the modularity measure on many practical networks, it may lead to apparently unreasonable partitions in some cases. It has been shown that modularity contains an intrinsic scale that depends on the total number of links in the network. Communities that are smaller than this intrinsic scale may not be resolved, even in the extreme case that they are complete graphs connected by some single bridges. The resolution limit of modularity actually depends on the degree of interconnectedness be-tween pairs of communities and can reach values of the order of the size for the whole network [19].

In Figure 12(a), we show a network consisting of a ring of several cliques, connected through single links. Each clique is a complete graph with  X  nodes and  X  (  X   X  1) / 2 links. Suppose there are  X  cliques (with  X  even), the network has a total of  X  =  X  X  X  nodes and  X  =  X  X  X  (  X   X  1) / 2+  X  edges. According to [19], modularity optimization would lead to a partition where the cliques are combined into groups of two or more (represented by dotted lines). Here, we use a synthetic dataset with  X  =5 and  X  =30 , called Ring. Another synthetic network is shown in Figure 12(b). In this network, the larger circles represent cliques with  X  nodes, denoted as  X   X  , and there are two small cliques with  X  nodes. According to [19], we set  X  =20 ,  X  =5 and obtain the network called Pairwise. Modularity optimization merges the two small communities into one (shown with a dotted line).
We present the clustering results on the above two datasets in Table III, where  X  is the number of nodes,  X  is the num-ber of edges, and  X  is the correct number of communities. Our algorithm gSkeletonClu discovers the exact communi-ties. For the Ring and Pairwise networks, the modularity-based algorithms SA and FastModularity both possess the resolution limit problem which results in merging two small cliques into one cluster.

The reason that our algorithm can overcome the resolu-tion limit is that it combines the density-based clustering principle and the modularity measure. The connected nodes with higher similarity will be considered preferentially as in the same community than the lower ones. Moreover, all of the adjacent nodes with equal similarities will be merged in one community together or be staying alone.

In this paper, a novel network clustering algorithm, called gSkeletonClu, is presented to overcome the sensitive pa-rameter problem of density-based network clustering and detect clusters, hubs and outliers in large-scale undirected networks. By projecting the original weighted network to its CCMST, we convert the problem of community discovery to finding core-connected components in the CCMST. A theoretical analysis shows that the structural clustering result on the CCMST is equal to that on the original network. Our algorithm also overcomes the problem of chaining effect possessed by the traditional MST-based clustering methods and the resolution limit possessed by other modularity-based algorithms. In the future, it is interesting to use our method to analyze the database data and more complex graphs from various applications.
 The work was supported in part by the Natural Science Basic Research Plan in Shaanxi Province of China (No. SJ08-ZT14), the National High-Tech Research and Devel-opment Plan of China under Grant No.2008AA01Z131, the U.S. National Science Foundation grants IIS-08-42769, CCF-0905014, and BDI-07-Movebank, and the Air Force Office of Scientific Research MURI award FA9550-08-1-0265. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of the funding agencies.

