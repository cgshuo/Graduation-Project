 There is a deep tension in statistical modeling of grammatical structure between providing good ex-pressivity  X  to allow accurate modeling of the data with sparse grammars  X  and low complexity  X  making induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) non-parametric models into TSGs has provided an effi-cient solution to the problem of segmenting train-ing data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009; Cohn and Blunsom, 2010; Post and Gildea, 2009). DP infer-ence tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data.
The elementary trees combined in a TSG are, in-tuitively, primitives of the language, yet certain lin-guistic phenomena (notably various forms of modifi-cation)  X  X plit them up X , preventing their reuse, lead-ing to less sparse grammars than might be ideal. For instance, imagine modeling the following set of structures: A natural recurring structure here would be the structure  X  X  NP the [ NN president]] X , yet it occurs not at all in the data.

TSGs are a special case of the more flexible gram-mar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for  X  X plicing in X  of syntactic frag-ments within trees. In the example, by augmenting a TSG with an operation of adjunction, a grammar that hypothesizes auxiliary trees corresponding to ad-joining  X  X  NN former NN ] X ,  X  X  NN NN of the uni-versity] X , and  X  X  NN NN who resigned yesterday] X  would be able to reuse the basic structure  X  X  NP the [ NN president]] X .

Unfortunately, TAG X  X  expressivity comes at the cost of greatly increased complexity. Parsing com-plexity for unconstrained TAG scales as O ( n 6 ) , im-practical as compared to CFG and TSG X  X  O ( n 3 ) . In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial op-tions with two types of derivation operators. 1 This has led researchers to resort to heuristic grammar ex-traction techniques (Chiang, 2000; Carreras et al., 2008) or using a very small number of grammar cat-egories (Hwa, 1998).

Hwa (1998) first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the ad-junction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of context-free formalisms (with cubic complexity) while still modeling rich linguistic phenomena (Schabes and Waters, 1995). Figure 1 depicts some examples of TIG derivations.

Sharing the same intuitions, Shindo et al. (2011) have provided a previous attempt at combining TIG and Bayesian nonparametric principles, albeit with severe limitations. Their TIG variant (which we will refer to as TIG 0 ) is highly constrained in the follow-ing ways.
In this paper we explore a Bayesian nonparamet-ric model for estimating a far more expressive ver-sion of TIG, and compare its performance against TSG and the restricted TIG 0 variant. Our more gen-eral formulation avoids these limitations by support-ing the following features and thus relaxing four of the five restrictions of TIG 0 .
The increased expressivity of our TIG variant is motivated both linguistically and practically. From a linguistic point of view: Deeper auxiliary trees can help model large patterns of insertion and potential correlations between lexical items that extend over multiple levels of tree. Combining left and right auxiliary trees can help model modifiers of the same node from left and right (combination of adjectives and relative clauses for instance). Simultaneous in-sertion allows us to deal with multiple independent modifiers for the same constituent (for example, a series of adjectives). From a practical point of view, we show that an induced TIG provides modeling performance superior to TSG and comparable with TIG 0 . However we show that the grammars we in-duce are compact yet rich , in that they succinctly represent complex linguistic structures. In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = NP ), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions.
 The canonical P 0 uses a probabilistic CFG  X  P that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitu-tions should occur (Cohn et al., 2009; Cohn and Blunsom, 2010).

We extend this model by adding specialized DPs for left and right auxiliary trees. 3 Therefore, we have an exchangeable process for generating right auxiliary trees as for initial trees in TSG.

We must define three distinct base distributions for initial trees, left auxiliary trees, and right aux-iliary trees. P init 0 generates an initial tree with root label c by sampling CFG rules from  X  P and making a binary decision at every node generated whether to leave it as a frontier node or further expand (with probability  X  c ) (Cohn et al., 2009). Similarly, our P 0 generates a right auxiliary tree with root la-bel c by first making a binary decision whether to generate an immediate foot or not (with probability  X  c ), and then sampling an appropriate CFG rule from  X  P . For the right child, we sample an initial tree from P init 0 . For the left child, if decision to gener-ate an immediate foot was made, we generate a foot node, and stop. Otherwise we recur into P right 0 which generates a right auxiliary tree that becomes the left child.

We bring together these three sets of processes via a set of insertion parameters  X  left c , X  right c . In any derivation, for every initial tree node labelled c (ex-cept for frontier nodes) we determine whether or not there are insertions at this node by sampling a Bernoulli(  X  left c ) distributed left insertion variable and a Bernoulli(  X  right c ) distributed right insertion vari-able. For left auxiliary trees, we treat the nodes that are not along the spine of the auxiliary tree the same way we treat initial tree nodes, however for nodes that are along the spine (including root nodes, ex-cluding foot nodes) we consider only left insertions by sampling the left insertion variable (symmetri-cally for right insertions). Given this model, our inference task is to explore optimal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previ-ous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blun-som, 2010; Shindo et al., 2011). This is achieved by proposing derivations from an approximating distri-bution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007).

Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a proposal distribution. Fortunately, Schabes and Wa-ters (1995) provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages. It is then straightforward to repre-sent this TSG as a CFG using the Goodman trans-form (Goodman, 2002; Cohn and Blunsom, 2010). Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them. We use the standard Penn treebank methodology of training on sections 2 X 21 and testing on section 23. All our data is head-binarized and words occurring only once are mapped into unknown categories of the Berkeley parser. As has become standard, we carried out a small treebank experiment where we train on Section 2, and a large one where we train on the full training set. All hyperparameters are re-sampled under appropriate vague gamma and beta priors. All reported numbers are averages over three runs. Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG.

We compare our system (referred to as TIG) to our implementation of the TSG system of (Cohn and Blunsom, 2010) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011) (re-ferred to as TIG 0 ). The upshot of our experiments is that, while on the large training set all models have similar performance ( 85 . 6 , 85 . 3 , 85 . 4 for TSG, TIG and TIG respectively), on the small dataset inser-tion helps nonparametric model to find more com-pact and generalizable representations for the data, which affects parsing performance (Figure 4). Al-though TIG 0 has performance close to TIG, note that TIG achieves this performance using a more suc-cinct representation and extracting a rich set of aux-iliary trees. As a result, TIG finds many chances to apply insertions to test sentences, whereas TIG 0 de-pends mostly on TSG rules. If we look at the most likely derivations for the test data, TIG 0 assigns 663 insertions (351 left insertions) in the parsing of en-tire Section 23, meanwhile TIG assigns 3924 (2100 left insertions). Some of these linguistically sophis-ticated auxiliary trees that apply to test data are listed in Figure 3. We described a nonparametric Bayesian inference scheme for estimating TIG grammars and showed the power of TIG formalism over TSG for returning rich, generalizable, yet compact representations of data. The nonparametric inference scheme presents a principled way of addressing the difficult model selection problem with TIG which has been pro-hibitive in this area of research. TIG still remains within context free and both our sampling and pars-ing techniques are highly scalable.
 The first author was supported in part by a Google PhD Fellowship in Natural Language Processing.
