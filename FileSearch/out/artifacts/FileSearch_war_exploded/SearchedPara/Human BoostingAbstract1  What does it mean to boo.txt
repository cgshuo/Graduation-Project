 Harsh Pareek harshp@cs.utexas.edu Pradeep Ravikumar pradeepr@cs.utexas.edu Human learning ability is indeed a piece of work, but man is certainly not infinite in faculties. There has been a long line of work on human learning in the psy-chology and cognitive science literature. The study of human category learning in particular  X  study-ing how humans classify objects into different cat-egories  X  has seen considerable advances in recent years (Ashby &amp; Maddox, 2005). Humans have been shown to have differing learning abilities depending on the nature of the task at hand. An important distinction for instance can be made between  X  X ule-based X  and  X  X nformation-integration X  category learn-ing tasks, depending on the type of decision boundary. Rule-based tasks have easily verbalizable classification rules, and typically depend on a single semantic dimen-sion (e.g.  X  X he width of the lines is small X ), or simple logical operations on a few such dimensions (e.g.  X  X he width of the lines is small, and the color is red X ). In information-integration tasks on the other hand, dif-ferent semantic dimensions are combined at a predeci-sional stage and the learner is unable to precisely de-scribe the classification rule they are using. An exam-ple of an information-integration task is object recog-nition, which humans can perform quickly using little to no working memory and attention, but the strat-egy for which is hard to describe in words. Research over the last two decades has led to the understand-ing(Ashby &amp; Maddox, 2011) that learning in humans is mediated by multiple systems, each governed by sepa-rate specialized biological processes. Deficits in any of these processes could lead to deficits in the correspond-ing aspects of human learning. Another interesting limit to human category learning arises from the mis-match between the discriminative dimensions in the data and those dimensions that humans find natural. It has been observed that some pairs of perceptual di-mensions are  X  X eparable X  for humans, so that they are able to reason about one dimension without involving the other; for instance, humans can separately reason about the shape and the color of an object. On the other hand, there are dimension pairs that are  X  X n-tegral X , which humans interpret holistically and they find it difficult to reason about any of the individ-ual dimensions separately. For example, for describ-ing a color, red, green and blue are separable dimen-sions while hue and saturation are integral dimensions. These fundamental limitations, and practical consid-erations we discuss in Section 2 limit human learning ability.
 In the face of seemingly intrinsic limits of human cat-egory learning, here we ask whether it is possible to  X  X oost X  the learning ability of humans? This question might seem ill-posed, but precisely such a question was posed in the context of computational learning theory by Kearns &amp; Valiant (1989), where they asked whether a  X  X eak learner X , specifically a learning algorithm which can learn a classifier that is only slightly better than random guessing, could be  X  X oosted X  into an ar-bitrarily accurate strong learning algorithm?. Interest-ingly, Schapire (1990) was able to answer this question in the affirmative. This technique, called boosting, has been the subject of considerable research over the last two decades, and its performance is now reason-ably well-understood (Friedman et al., 2000; Collins et al., 2002; Meir &amp; R  X atsch, 2003). Boosting constructs an ensemble of learners by providing a weak learning algorithm with iteratively modified training datasets, putting increasingly greater weights on the examples deemed more difficult as iterations proceed. We review boosting in Section 2.
 In this paper, we investigate the possibility of using hu-mans as weak learners and boosting them to strongly learn complex concepts better than any single human. Our principal contributions are as follows. (a) The standard boosting algorithms are not amenable to hu-man learners (weighted examples may not be appropri-ately interpreted by humans for instance). We describe the modifications that need to be made, along with theoretical justifications, that extend the state of the art in the analysis of boosting. (b) We design suitable experiments for boosting on humans: we consider two synthetic tasks, which we call the crosshair and gabor tasks, and one real world task, detecting fake reviews in the Opinion Spam dataset of (Ott et al., 2011). (c) We run category learning experiments on the crowd-sourcing platform, Amazon X  X  Mechanical Turk. Our experimental results show that boosting can overcome some intrinsic limitations of human learners and pro-duce gains in accuracy.
 Related work: Our work is in the vein of interesting recent work by (Zhu et al., 2011) which has focused on combining human classifiers using co-training. It will be useful to position our work against the frame-works of Human Computation and Crowdsourcing. Human Computation (Quinn &amp; Bederson, 2011) has been termed  X  X  paradigm for utilizing human process-ing power to solve problems that computers cannot yet solve X . Thus, it uses humans as  X  X ubroutines X  for solv-ing sub-tasks, particularly those that computers find difficult. Crowdsourcing on the other hand, has been termed  X  X he outsourcing of a job typically performed by an expert to a crowd that typically have lower-expertise but are large in number X . The key idea in crowdsourcing is to suitably aggregate the outputs of the members of the crowd and this can at times rival the performance of an expert. A key contrast of our work with human computation and crowdsourcing, is that the goal in human boosting is to use humans not as fixed classifiers and subroutines, but as changing and learning algorithms . Moreover, as we detail in the main section of the paper, the protocol of using hu-mans in our human boosting method, is complicated and even asynchronous, unlike typical tasks in crowd-sourcing. Nonetheless, the work here is very related to these frameworks. As such, our research raises the question of whether a richer use of humans (e.g. as learning algorithms) would allow them to be embed-ded in non-trivial ways within other larger machine learning frameworks? Our work can also be positioned as the converse of active learning. In active learning, eg.(Settles, 2010), the human acts as an oracle and the algorithm asks the human to label what it believes are the  X  X ost informative X  examples: a machine per-forms classification and is guided by a human. In our work, the human performs the classification task and the machine guides humans.
 Applications: The question of whether human learn-ers can be boosted is certainly interesting from a cog-nitive science point of view. But as we detail, it has a number of practical applications as well.
 The first is the use of humans as  X  X upervised X  feature extractors. For many natural tasks such as those in vision and text, humans have access to evolutionar-ily evolved implicit feature spaces, which we find diffi-cult to even capture with a computer. If each human learner is asked to learn a  X  X ecision stump X  involv-ing a single or a few features of their choosing within our boosting framework, then we can use these to ex-tract task-driven implicit human features. It is useful to compare this to manifold learning. For instance, in the high dimensional space of images represented as a vector of their pixels, images of a given face lie on a manifold, and face recognition essentially requires a learner to identify this manifold. However, for any manifold, there is an appropriate mapping into lower-dimensional Euclidean spaces where classification be-comes easy. Manifold learning techniques aim to ex-tract these embedding using large amounts of data. Extracting implicit features from humans could po-tentially allow us to achieve similar performance using much smaller amounts of training data.
 A human boosting framework can also be used to boot-strap labeled data for difficult learning problems. Us-ing a small number of labeled examples, we could have humans  X  X trongly learn X  complex concepts and cre-ate larger datasets for consumption by machine algo-rithms. Indeed, ground truth labels for typical ma-chine learning datasets are either provided by or val-idated by humans. Thus, the problems selected are typically those in which humans excel. Boosting hu-man learners allows us to solve problems which even humans find difficult, and expands the frontier of ma-chine learning research. These methods could be used to replace a domain expert or a complexly engineered solution with a number of non-expert humans. While it is true that it is very expensive to use humans, the success of crowdsourcing has shown that for several problems, it is cheaper on the whole to use humans than to employ engineering solutions. We start the section by discussing the characteristics of humans as  X  X earners, X  in the context of machine learn-ing. We then discuss boosting, and Adaboost, and why modifications to the Adaboost are essential to be amenable to the characteristics of human learners. We then detail our  X  X umanBoost X  algorithm. Finally, we provide a convergence analysis for an idealized version of our HumanBoost algorithm.
 Human Learners: Humans as learners differ from machine learning algorithms in several ways. First, hu-man learners are a black box, in the sense that we do not have access to their inner workings. Second, ma-chine learners use high dimensional feature vectors as input, which humans cannot interpret or visualize. On the other hand, humans do understand images, video, audio and text, and to an extent that even state of the art machine learning algorithms cannot capture. In this paper in particular, we will represent input vectors via intuitive visual stimuli for use with hu-man learners. Third, machine learning algorithms take large numbers of training examples as input. Human learners can only be given a small number of train-ing examples. Finally, it is not clear whether humans can handle classification noise and how they would re-spond to differing distributions in the data. There is some evidence (Fried &amp; Holyoak, 1984) that humans can learn distributions and can learn both discrimina-tive and generative models (Hsu et al., 2010). Boosting and Adaboost: The framework of boost-ing is now understood(Friedman et al., 2000) to greedily fit an additive model of the form H ( x ) = P t  X  t h t ( x ) to a classification-surrogate loss such as the exponential function; and where the functions { h t (  X  ) } are the classifiers learnt by a  X  X eak X  learn-ing algorithm, and {  X  t } are the weights assigned to these classifiers. With a careful selection of weights and modified training sets for the intermediate steps, it can be shown that the output classifier H ( x ) is a  X  X trong X  classifier that might well have been learnt by a  X  X trong X  learning algorithm.
 We first describe the popular boosting algorithm of Adaboost (Schapire, 2003) as it is closely related to our proposed algorithm. In each round, Adaboost maintains a weighted distribution over the training ex-amples, with weights corresponding to the difficulty of that example. Initially, all examples are weighted equally. Each iteration of Adaboost has three steps: (a) train the weak-learning algorithm on this weighted data distribution, (b) compute its classification error given the new data distribution (c) multiplicatively in-crease the weights on the  X  X ifficult X  examples. This ensures that the weak learning algorithm tries to fit those points more accurately in the next iteration. In-deed, over the past decade, Adaboost has been ana-lyzed both experimentally (Dietterich, 2000) and the-oretically (Friedman et al., 2000; Collins et al., 2002). Certain properties of Adaboost make it amenable for use with human learners. First, it is a wrapper proce-dure, i.e. it does not need to know the inner workings of the weak learner; this makes it conducive for use with  X  X lack box X  human learners. Second, it does not directly operate on the training data ( x i ,y i ) (where x is the input stimulus and y i is the label), so x i could be complex stimuli such as text or video. Third, there is variability among humans learners, they are not re-liable and their output is noisy. Adaboost addresses this by adaptively reweighting the bad human learners. Certain other parts of Adaboost need to be modified. Adaboost reweights training examples. Humans may not interpret these weights correctly or consistently, and simply presenting the numerical weights would be unreliable. The theoretical guarantees of Adaboost fail in such a case. One potential solution is to resample training examples, drawn from a multinomial distribu-tion corresponding to Adaboost weights. The caveat with this is that it will lead to examples with large weights being repeated, and humans may not inter-pret repetitions correctly. For instance, if we consider stimuli such as text reviews (as in section 3.2.3), it will be obvious to the learner if a review was repeated, and the learned results would be unreliable.
 Proposed algorithm: Our proposed boosting algo-rithm, called HumanBoost, is detailed in Algorithm 1. Recall that any boosting algorithm has the following three steps in each iteration: (a) train a weak-learner on the new data distribution, (b) compute the classi-fication error given the new data distribution, and (c) create a new data distribution in light of the difficult examples in the previous round. We now discuss how Algorithm 1 HumanBoost
Input: MTurk oracle O , D = { ( x i ,y i ) } N i =1 ,  X  t
Set weights w 1 i = 1 /N , i = 1 ,...N for t = 1 ,...,T do end for
Return strong learner H T ( x ) = sign( P t  X  t h t ( x )). function Filter ( m t ,w t , X  t ) end function the HumanBoost algorithm performs these three key steps. HumanBoost maintains the new distribution via weights over the data points, just as in Adaboost. But for training a human weak-learner, we do not want to provide either weights over the samples, or a resampled data set with potentially repeated samples. Instead, we perform sampling without replacement, with prob-ability proportional to the normalized weights. Such subsampling without replacement in the context of boosting was first experimentally analyzed for boosted trees in (Friedman, 2002) in the context of Stochastic Gradient Boosting. Here, we use a rejection sampling framework similar to FilterBoost(Bradley &amp; Schapire, 2008) and that of (Zadrozny et al., 2003), and as de-scribed in the function filter in algorithm 1. We go through the samples in a random order, and reject easy examples one by one till we collect a sufficient number of difficult examples S . For step (b), and computing the weights  X  t , we need to measure the weighted ac-curacy of h t . As we detail in the experimental section, we train the human learner h t over S in an online fash-ion, and then ask them to label examples from D  X  S . Learners may memorize examples from S so we cannot use S in the test set. This introduces an unavoidable error in the computation of  X  t , as S contains the most difficult examples and so D  X  S contains easy exam-ples. However, for human learners, S is small, and if the learner has high accuracy on D  X  S , we can assume that it is sufficiently accurate. A key algorithmic de-sign aspect of HumanBoost as pertaining to step (b) is the classification-surrogate loss used to compute the classification error, and the weights. While Adaboost uses the exponential loss, its weights can grow arbi-trarily large which reduces its tolerance to noise. One robust approach as in Madaboost(Domingo &amp; Watan-abe, 2000), is to truncate the weights explicitly, to a maximum of 1. We use the logistic function as in Log-itBoost(Friedman et al., 2000). Step (c) of creating the new data distribution is similar to that of Adaboost, given the classification error in Step (b).
 Remark: Algorithm 1 extends Discrete Adaboost us-ing a logistic weight function and rejection sampling without replacement. Note that we can also extend  X  X eal Adaboost X  in a similar fashion. However, Real Adaboost requires classifiers to output probability es-timates. Experiments have shown that humans are not good at estimating probabilities(Cosmides &amp; Tooby, 1996). Teaching humans to output confidence values or calibrating their output for Real Adaboost is an interesting direction for future research. Another ex-tension of Adaboost potentially amenable to humans is the boosting by relabeling, also known as Agnostic Boosting(Ben-David et al., 2001). In agnostic boost-ing, easy examples are randomly relabeled so that the weak learner can focus on more difficult examples. This effectively adds label noise to the dataset pro-portional to the weight in the distribution. Whether humans will interpret this noise correctly is an open question, which we will investigate in future work. Sampling: We can show that our sampling procedure is efficient, following the lines of (Bradley &amp; Schapire, 2008). Specifically, if the rejective filtering procedure terminates without giving enough examples, then the error err t of our hypothesis H is small.
 Proposition 2.1 If in any call of Filter , n  X  ln(1 / X  ) consecutive examples are rejected, then err t  X  with probability 1  X   X  .
 Analysis: A popular class of analytical results in boosting (Collins et al., 2002; Bradley &amp; Schapire, 2008) use the recurrences arising from the new data distribution reweighting difficult examples in the pre-vious iteration, and the weak-learners having an  X  X dge X  over random guessing even on the new data distributions. Another recent line of analytical results are based on the view of boosting as greedy stagewise functional gradient descent (Mason et al., 1999; Fried-man et al., 2000; Grubb &amp; Bagnell, 2011). Recently, Grubb &amp; Bagnell (2011) provided an elegant analysis of boosting under the restriction of using weak learn-ers, by interpreting it as restricted gradient descent in function space. Our analysis is an extension of this recent result, where we interpret our algorithm as a stochastic version of their restricted gradient descent. Our presentation here is terse for lack of space, and we refer to (Grubb &amp; Bagnell, 2011) for extended de-tails of their framework. Consider the function space L ( X , R ,P ) of square-integrable functions, equipped with the inner product  X  f,g  X  P = E P [ f ( x ) g ( x )] = R
X f ( x ) g ( x ) dP ( x ). Denote by sponding inner product,  X  f,g  X   X  P = P N i =1 f ( x i ) g ( x Consider the objective R : L 2 (  X  P )  X  R derived from the empirical risk R emp [ f ] = 1 N P N i =1 l ( y i f ( x sume the loss function l is convex, and differentiable (note that the function l in our case is the logistic func-tion l ( t ) = log(1 + exp(  X  t ))). Denote the  X  X estriction set X  of weak learners as H . Boosting can then be de-rived as a method to minimize R emp [ f ] subject to the constraint that f = P t  X  t h t with h t  X  H and  X  t  X  R . The subgradient of R emp [ f ] at any f can be written denote this by  X  . The key insight in Grubb &amp; Bagnell (2011) was to leverage the fact that each boosting step can be written as f t +1 = f t +  X  h t , where h t  X  arg max h  X  H  X  X  X  ,h  X  / k h k , and  X   X  1 / X  t  X  X  X  ,h t  X  / k h t k 2 , for some step-size  X  t key idea there was that the weak-learning assumption can be asserted as an approximate gradient condition: arg max h  X  H  X  X  X  ,h  X  / k h k X   X  k X  X  , for some  X  &gt; 0. They then relate this  X  to the edge parameter used in typ-ical boosting analyses. Note that the boosting step is in effect a restricted gradient descent, which they then leverage to show that (i) the algorithm asymp-totically converges to the optimum of the desired loss function, and that (ii) it does so efficiently, i.e. at a rate polynomial in 1 / , where is the desired error. The key difference in our boosting algorithm when compared to the setting of (Grubb &amp; Bagnell, 2011) is that we sample training data without replacement, and provide them to the learner without weights, so that instead of the  X  derived above, we use a stochas-tic gradient  X  0 : where  X  0 ( x i ) = S i y i , with S i is an indicator for data instance i , i.e. 0 if x and 1 if x i  X  S . Our algorithm is thus a stochas-tic restricted gradient descent, and we can extend the analysis of (Grubb &amp; Bagnell, 2011) to obtain con-vergence rates even in our setting. We first note that the function Filter in 1 ensures that p ( x i  X  S ) = current hypothesis. It thus follows that the inclusion probability of x i satisfies: E [ S i ] = l 0 ( y i f ( x E [  X  0 ] =  X  on x i . Our boosting algorithm can then be abstracted as performing the step: f t +1 = f t +  X  t h t where h t  X  arg max h  X  H  X  X  X  0 ,h  X  / k h k , where  X  0 is the stochastic gradient detailed above. For the purpose of analysis, set the weight as  X  t =  X  1 / X  t  X  X  X  0 ,h t  X  / k h Further, we require that the restriction set satisfy the weak-learning assumption with respect to the stochas-tic gradient, so that arg max h  X  H  X  X  X  0 ,h  X  / k h k X   X  k X  for some  X  &gt; 0. We can then extend the analysis of (Grubb &amp; Bagnell, 2011) to obtain the following: Theorem 2.2 Let R emp [ f ] be a  X  -strongly con-vex, and  X  -strongly smooth functional over F := L ( X , R ,  X  P ) . Let H X  X  be a restriction set with edge  X  . Suppose further that the variance of the stochastic gradients is bounded so that E [ k X  0 t  X  X  X  t k ]  X   X  k X  Let f  X   X  arg min f  X  X  R emp [ f ] . Given a starting point f , and step-size  X  t = 1 / X  , after T iterations of our algorithm, we have: E where  X  := (1  X  (2  X / X  )(  X / 4  X   X  2  X   X  (1 +  X / 2))) , and where the expectation is over the randomization in our sampling procedure. We first describe our experimental setup and then present our experimental results demonstrating Hu-man Boosting on category learning tasks, where learn-ers are expected to learn to distinguish between two classes A and B of stimuli, based on examples. 3.1. Experimental Setup We conduct experiments using Amazon X  X  Mechani-cal Turk(MTurk). MTurk is a crowdsourcing market-place which connects requesters , who assign HITs (Hu-man Intelligence Tasks), to workers who perform the HITs. MTurk is ideally suited to microtasks which can be performed quickly. Our program posted HITs on MTurk to recruit workers and led them to our server for experiments. On completing the experiment, they were given a unique random string to enter on MTurk as verification for payment. Since HITs are paid tasks, workers have the incentive to spam tasks, i.e. click through them quickly giving wrong results. Workers on MTurk are also typically less attentive than vol-unteers in laboratory experiments. To ensure worker attentiveness and to discourage spammers, a one sec-ond delay was given after every training example and a half second delay was given after every test example. Our experiments used workers with &gt; 1000 approved HITs, and a &gt; 95% acceptance rate. All HITs were de-signed to be completed in less than 20 minutes. Each HIT corresponds to one round of boosting and can only be picked up by a single worker, and we ensured that the workers in a run of the boosting algorithm were all distinct.
 Interface: Our interface (Figure 1) is a point and click web-page built using python CGI and javascript. Each HIT is a session consisting of two phases, the training phase and the testing phase. In the training phase, the learner is shown a stimulus and presented two buttons, one for each class ( X  X lass A X  and  X  X lass B X ). After he chooses an option, he is told whether his response was correct, and the next stimulus is shown. Previous stimuli are not removed from the screen. This kind of training is called  X  X raining with feedback X , and is known to be superior to observational training where the learner is only shown examples from each class(Ashby et al., 2002). After the training phase, we ask the participant to decide on a rule, write it down and verify that this rule works on the training exam-ples they have been provided. In the testing phase, workers are asked to use the learned rule to label un-seen examples. Note that since all test examples are shown to each learner, the test examples will not have the same distribution as the reweighted training data (in fact, their distribution is the same as the origi-nal data). Human learners respond to the test exam-ples as if they were unlabeled examples and this may cause drifting of the classification boundary (Zhu et al. (2010) call this the test-item effect ). We attempt to mitigate this by asking them to explicitly write down the rule before they begin labeling examples. 3.2. Results We present experiments on three stimuli: two syn-thetic, Crosshairs and Gabor patches, and one real world, the Opinion Spam detection dataset (Ott et al., 2011). For each stimulus, we contrast the performance of two groups of learners: the control group and the boosted group. The control group is trained on a small sample of the dataset and their accuracy on a test set is measured. Each learner in the boosted group is used only in one round of boosting. Humans in both groups are given the same number of training examples. The exact instructions given to the work-ers is included in supplementary materials. Synthetic stimuli prevent learners from using preconceived no-tions of classes, so that we can observe the effects of learning more clearly. Preconceived notions are un-avoidable when using real world stimuli, however we mitigate this by hiding the true class labels and telling the learner only that there are two classes which need to be distinguished.
 We compare the accuracy of the best learner in the control group to the boosted learner. In the case of noisy learners such as humans on Turk, the best learner performs better than the majority vote learner, and we just compare the performance of the best hu-man learner with that of the boosted algorithm. Note that our algorithm is in fact complementary to crowd-sourcing algorithms used to aggregate human learners such as those of (Dawid &amp; Skene, 1979; Sheng et al., 2008), and we are solving the essentially different prob-lem of boosting human learners. 3.2.1. Crosshair Task Two intersecting perpendicular lines, see Figure 4(a), are presented to the subject as a 100  X  100 pixel im-age. The class of the cross-hair image depends on the control dimensions, viz the x and y coordinate of the point of intersection. We draw classes from the overlapping gaussians in Figure 4(b). Preliminary ex-periments indicated that if the classes were separated, humans would easily learn a linear boundary involv-ing both dimensions, but they report confusion if the classes overlap. Further, the optimal decision bound-ary in Figure 4(b) is quadratic and thus nonlinear. Since knowing the control parameters for a data point only probabilistically determine a class, this is an ex-ample of a probabilistic decision boundary as described in (Ashby &amp; Maddox, 2005). The human learners were informed that some class labels may be incorrect. Due to the class overlap, the maximum accuracy achievable in this task is 85%. The aim of this experiment is to demonstrate the boosting can help recover from noisy input which confuses human learners.
 Figure 2(a) shows a histogram of the human learners in the control group who were trained with 5 exam-ples each. They achieved an average test accuracy of 55.74%  X  11.35% with a maximum of 70%. The re-ported rules were typically noisy decision stumps in either dimension. With 5 examples, humans are weak learners on this task. With more examples, humans do perform better, but report confusion about the class boundary. We perform experiments with 5 examples to demonstrate the effect of boosting.
 Figure 2(b) shows the performance of the boosted learners. In 10 rounds, human boosting achieves 85% training accuracy and 75% test accuracy, which is bet-ter than the best control group learner. The gap be-tween the training and test accuracy is called the  X  X en-eralization error X  and is a common occurrence in ma-chine learning algorithms. This error typically goes to zero as the size of the training set is increased. 3.2.2. Gabor Patch Task A Gabor patch(Figure 6(a)) is a sinusoidal grating with control parameters: spatial frequency d and ori-entation  X  . The patches are generated using a sine wave z = sin ((0 . 25 + d/ 50) x ) rotated  X  anticlockwise from the positive x -axis and presented as a 200  X  200 pixel image. These dimensions are separable so hu-mans can reason independently about each of these dimensions but they find it difficult to learn decision rules jointly involving both dimensions. This task is an example of learning information-integration bound-aries, albeit around 800 examples are required to learn the information-integration boundary. The classes are drawn as in Figure 6(b) where the optimal decision boundary is linear at a 45  X  angle to the axes. We use parameter values as in (Ell et al., 2009), which were chosen such that the resulting patches are not too dense and it is possible to count the number of lines in the grating and the effects of the dimensions are balanced. Since humans naturally find this task diffi-cult, we do not add noise. Note that an ideal boosting procedure using decision stumps on the true param-eters can achieve 100% accuracy on this task. Thus, this task is one of learning an information-integration boundary using an aggregation of rule-based learners. We demonstrate that boosting excels at this task and achieves very high accuracy.
 In preliminary experiments, we found that if the deci-sion boundary is axis-parallel, then it is learned per-fectly with &lt; 10 examples. On the distribution shown in Figure 6(b), the average accuracy of human learn-ers from the control group trained with 20 examples each was 60.11%  X  12.22%(see Figure 3(a)). Most learners learned decision rules involving the spatial fre-quency while none used the orientation. Some partic-ipants learned rules based on irrelevant artifacts such as  X  X opiness X  of lines and their responses could not discriminate between classes. The optimal spatial fre-quency decision boundary achieves accuracy of 72%, achieved by 3 learners, though 100% performance is possible in principle.
 Boosting with 20 training examples(Figure 3(b)) quickly achieves 72% training accuracy and stabilizes at this value (first 11 rounds shown). This is a failure of the boosting algorithm as it only performs as well as the best human learner. Examining the user responses revealed that all learned decision rules used only the spatial frequency. We speculate that this is because, with 20 examples, learners focus on the most obvi-ous discriminative perceptual dimension(spatial fre-quency) and ignore the others. Boosting with 5 train-ing examples(Figure 3(b)) on the other hand allows weak learners to learn decision rules based on the ori-entation and consequently achieves &gt; 90% training ac-curacy. This finding might be of interest even from a cognitive science point of view. It is interesting to note that (Friedman, 2002) observed a similar effect with decision trees, viz subsampling can improve per-formance, though their differences were not as stark. 3.2.3. Opinion Spam Task While the earlier experiments were based on synthetic data, we now consider the performance of our method on a real world dataset. The Opinion Spam task aims to separate truthful and deceptive reviews. Each stim-ulus is a hotel review, usually a small paragraph with 4-5 sentences. We used the opinion spam detection dataset introduced in (Ott et al., 2011) which con-tains truthful hotel reviews extracted from TripAd-visor and deceptive reviews for the same hotels gen-erated using Mechanical Turk. By design, deceptive reviews are meant to deceive humans and Turkers do find this task difficult, often performing near random. This task is challenging because of the number of pos-sible perceptual dimensions to the task, because the nature of the decision boundary is unknown and be-cause the classes may overlap. This task is particularly interesting for a black-box procedure such as boosting, since the relevant features are unknown. The decision boundaries learned by human learners are based on semantic features which are difficult to capture with computers (is the review humorous?, sarcastic?, well-written?). Preliminary experiments showed that if the learners are informed that they are trying to separate fake and truthful reviews, they completely ignore the given training set and use their own preconceived no-tions of what constitutes a truthful review. Hence, in our experiments they are only informed that the task is to separate two classes of reviews. This may, however, lead to many features not being discriminative. The average accuracy over the control group human learn-ers (Figure 5(a)) trained with 10 examples was 51%  X  8.76%, i.e. only slightly better than random. In this task, we only expect human learners to identify dis-criminating features in the training set and to classify the remaining instances according to that feature. With human boosting(Figure 5(b)), we gradually achieve 70% training accuracy and 65% test accuracy on this task after 30 boosting rounds. We expect the increasing trend in the training accuracy to continue if the algorithm is run for more iterations.
 We acknowledge the support of NSF via IIS-1149803, and DoD via W911NF-12-1-0390.
 Amazon mechanical turk. http://www.mturk.com . Ashby, F.G. and Maddox, W.T. Human category learning. Annu. Rev. Psychol. , 56:149 X 178, 2005. Ashby, F.G. and Maddox, W.T. Human category learning 2.0. Annals of the New York Academy of Sciences , 1224(1):147 X 161, 2011.
 Ashby, F.G., Maddox, W.T., and Bohil, C.J. Obser-vational versus feedback training in rule-based and information-integration category learning. Memory &amp; cognition , 30(5):666 X 677, 2002.
 Ben-David, S., Long, P., and Mansour, Y. Agnostic boosting. In CoLT , pp. 507 X 516. Springer, 2001. Bradley, J. K. and Schapire, R. Filterboost: Regres-sion and classification on large datasets. NIPS , 20: 185 X 192, 2008.
 Collins, M., Schapire, R.E., and Singer, Y. Logistic re-gression, adaboost and bregman distances. Machine Learning , 48(1):253 X 285, 2002.
 Cosmides, Leda and Tooby, John. Are humans good intuitive statisticians after all? rethinking some con-clusions from the literature on judgment under un-certainty, 1996.
 Dawid, A.P. and Skene, A.M. Maximum likelihood estimation of observer error-rates using the em al-gorithm. Applied Statistics , pp. 20 X 28, 1979. Dietterich, T. G. An experimental comparison of three methods for constructing ensembles of deci-sion trees: Bagging, boosting, and randomization. Machine learning , 40(2):139 X 157, 2000.
 Domingo, C. and Watanabe, O. Madaboost: A modi-fication of adaboost. Citeseer, 2000.
 Ell, S., Ing, A., and Maddox, W. Critrial noise ef-fects on rule-based category learning: The impact of delayed feedback. Attention, Perception, &amp; Psy-chophysics , 71:1263 X 1275, 2009. ISSN 1943-3921. Fried, L.S. and Holyoak, K.J. Induction of category distributions: A framework for classification learn-ing. Journal of Experimental Psychology: Learning, Memory, and Cognition , 10(2):234, 1984.
 Friedman, J. H. Stochastic gradient boosting. Compu-tational Statistics &amp; Data Analysis , 38(4):367 X 378, 2002.
 Friedman, J. H., Hastie, T., and Tibshirani, R. Addi-tive logistic regression: a statistical view of boost-ing (with discussion and a rejoinder by the authors). Ann. Stats. , 28(2):337 X 407, 2000.
 Grubb, A. and Bagnell, J.A. Generalized boosting algorithms for convex optimization. arXiv preprint arXiv:1105.2054 , 2011.
 Hsu, A.S., Griffiths, T.L., et al. Effects of generative and discriminative learning on use of category vari-ability. 2010.
 Kearns, M. and Valiant, L. Cryptographic limitations on learning boolean formulae and finite automata. JACM , 41(1):67 X 95, 1989.
 Mason, L., Baxter, J., Bartlett, P.L., and Frean, M.
Functional gradient techniques for combining hy-potheses. NIPS , pp. 221 X 246, 1999.
 Meir, R. and R  X atsch, G. An introduction to boost-ing and leveraging. Advanced lectures on machine learning , pp. 118 X 183, 2003.
 Ott, M., Choi, Y., Cardie, C., and Hancock, J.T. Find-ing deceptive opinion spam by any stretch of the imagination. Arxiv preprint arXiv:1107.4557 , 2011. Quinn, A.J. and Bederson, B.B. Human computation: a survey and taxonomy of a growing field. In CHI , pp. 1403 X 1412. ACM, 2011.
 Schapire, R.E. The strength of weak learnability. Ma-chine learning , 5(2):197 X 227, 1990.
 Schapire, R.E. The boosting approach to machine learning: An overview. Lecture Notes in Statistics , pp. 149 X 172, 2003.
 Settles, B. Active learning literature survey. University of Wisconsin, Madison , 2010.
 Sheng, V.S., Provost, F., and Ipeirotis, P.G. Get an-other label? improving data quality and data min-ing using multiple, noisy labelers. In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , pp. 614 X 622. ACM, 2008.
 Zadrozny, B., Langford, J., and Abe, N. Cost-sensitive learning by cost-proportionate example weighting.
In Data Mining, 2003. ICDM 2003. Third IEEE In-ternational Conference on , pp. 435 X 442. IEEE, 2003. Zhu, X., Gibson, B.R., Jun, K.S., Rogers, T.T., Harri-son, J., and Kalish, C. Cognitive models of test-item effects in human category learning. In ICML , 2010. Zhu, X., Gibson, B.R., and Rogers, T.T. Co-training
