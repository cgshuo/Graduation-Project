 Donghong Ji  X  Shiju Zhao  X  Guozheng Xiao Abstract In this paper, we address the problem of document re-ranking in information retrieval, which is usually conducted after initial retrieval to improve rankings of relevant documents. To deal with this problem, we propose a method which automatically constructs a term resource specific to the document collection and then applies the resource to document re-ranking. The term resource includes a list of terms extracted from the documents as well as their weighting and correla-tions computed after initial retrieval. The term weighting based on local and global distribution ensures the re-ranking not sensitive to different choices of pseudo relevance, while the term correlation helps avoid any bias to certain specific concept embedded in queries. Experiments with NTCIR3 data show that the approach can not only improve performance of initial retrieval, but also make significant con-tribution to standard query expansion.
 Keywords Term extraction  X  Term weighting  X  Maximal marginal relevance  X  Document re-ranking  X  Information retrieval 1 Introduction Document re-ranking is to re-order initially retrieved documents in information retrieval. It can be regarded as an add-on module to traditional information retrieval systems (Balinski and Danilowicz 2005 ; Crouch et al. 2002 ). Due to its potential to improve accuracy of top retrieved documents, document re-ranking has received great attention recently (Lee et al. 2001 ; Luk and Wong 2002 ; Qu and Xu 2000 ; Yang 2004 ; Yang et al. 2005 ).

In general, most current document re-ranking methods use the following three kinds of information for re-ranking. (1) Inter-document relationship. For example, Lee et al. ( 2001 ) proposed a (2) Specific information extracted from queries or top retrieved documents. For (3) External lexical resources. For example, Qu and Xu ( 2000 ) used manually
Generally, methods in the first category are more computationally expensive, and those in the third category require manual resources which are often unavailable for specific domain applications. In this paper, we adopt the second strategy and make use of a specific kind of information, i.e., terms (including individual words and multi-word units) existing in top retrieved documents. The rationale behind the idea comes from feedback relevance (also held behind traditional query expansion) that statistically key terms present in top retrieved documents tend to be representative of the relevant documents, and thus can act as their indicators.

The motivation of using terms instead of individual words is that terms generally contain more conceptually complete or specific information than individual words, thus having more potential for improving performance of information retrieval. For example,  X   X   X   X   X   X  (Han Dynasty relic exhibition) is a term consisting of three words:  X   X  (the Han Dynasty),  X   X  (relic) and  X   X  (exhibition), and intuitively, is more specific than the three individual words. Such specific terms would be more useful in retrieving relevant documents.

An immediate problem is where to acquire such terms. Since such terms are generally specific to some domains, we cannot expect them to be included in any existing manual resources. Even if an existing resource contain some such terms, they need to be updated if the application domain changes. So, a better option would be automatic construction of such resources, i.e., extracting such domain-specific terms automatically, based on analysis of domain-specific texts.

Another problem is how many top documents would be regarded as relevance feedback in initial retrieval, which is a long term obstacle also faced by most approaches mentioned above (Crouch et al. 2002 ; Kamps 2004 ; Lee et al. 2001 ; Luk and Wong 2002 ; Yang 2004 ; Yang et al. 2005 ). Usually, a smaller and pre-defined number ( k ) of the documents (say top 20 X 25) are considered in practice. However, in the cases that very few relevant documents fall within the range, the method will not work. If a larger scope (say 500 or 1,000) is considered, many irrelevant documents will fall inside, and noisy terms will dominate. To address this problem, we propose a term weighting scheme based on their distribution to capture the characteristics of relevant documents and restrict the noises. With the weighting, the performance would be insensitive to different choices of k , if only a larger k is selected. Consequently, we can randomly but safely choose a larger k as relevance feedback, and would not worry about the cases that top retrieved documents contain very fewer relevant ones.
The third problem is that most current methods do not consider correlations between query terms. Usually, a document covering more aspects of a query should get higher scores, which can be captured somehow by term correlations. Mitra and Buckley ( 1998 ) used Maximal Marginal Relevance ( MMR ) to capture the correlations. However, their idf (inverse document frequency) based weighting was too simple to reflect the contribution of terms, and the weight-preferred scheme also suffers from a bias towards terms with higher weights. To address the problem, we replace the idf -based weighting with our distribution-based weighting in order to restrict the noise, and the weight-preferred scheme with a relevance-preferred scheme in order to avoid the bias.

In sum, the automatically built term resource plays an important role in the task of document re-ranking. Term weighting based on local and global distribution ensures the re-ranking not sensitive to different choices of pseudo relevance, term correlation helps avoid any bias to specific aspects in queries, and the automatic resource construction makes assure that the method can be ported to any domain easily.
The rest of this paper is organized as the following. In Sect. 2 , we describe key term extraction from documents. In Sect. 3 , we talk about the term weighting scheme. In Sect. 4 , we specify how query weighting is based on term weighting. In Sect. 5 , we talk about how to re-rank the documents based on the extracted key terms and their weighting. In Sect. 6 , we evaluate the approach on NTCIR3 CLIR dataset and give some analysis. In Sect. 7 , we talk about some related work on document re-ranking. Finally in Sect. 8 , we present the conclusion and future work. 2 Term extraction Term extraction concerns the problem of what is a term. Intuitively, key terms in a document are some word strings which are conceptually prominent in the document and play main roles in discriminating the document from other documents.
We adopt a seeding-and-expansion mechanism to extract key terms from documents. The procedure of term extraction consists of two phases, seed positioning and term determination. Intuitively, a seed for a candidate term is an individual word (or a Chinese character in the case of Chinese language, henceafter, we focus on Chinese language), and seed positioning is to locate the rough position of a term in the text, while term determination is to figure out which string covering the seed in the position forms a key term.

To determine a seed, Chinese characters need to be weighed someway to reflect their significance in the text. We make use of a very large corpus r (LDC X  X  Mandarin Chinese News Text) as a reference corpus. Suppose d is a document, c is of c occurring in r and d , respectively, we adopt relative probability or salience of c in d with respect to r (Schutze 1998 ) as the criteria for evaluation of seeds.
We call c a seed if S r , d (c)  X   X  (  X   X  1). That is, its probability occurring in the document must be equal with or higher than its probability in the reference corpus.
Although it is difficult to give out the definition of terms, we have the following heuristics about a key term in a document. (1) A key term contains at least one seed; (2) A key term occurs at least  X  (  X  [ 1) times in the document; (3) A key term is a maximal word string meeting (1) and (2).

Suppose s is a word string in a document and c is any character, according to the three conditions, s is a key term, if it meets (1), (2) and (3) that there is no word string cs or sc existing in the document, which also meets (1) and (2). Notice that one key term can be embedded in another one. For example, in  X   X   X   X   X  (Imperial Palace Museum), both  X   X  (Imperial Palace) and  X   X   X  (Museum) are embedded key terms. To capture such terms, suppose that t is a key term in respectively, we have the fourth heuristics in (4).
Intuitively, the former three constraints make sure that a key term should be important, frequent and complete in a document, respectively, while the fourth one implies that a key term can be embedded in another one.

As an example, given a document d , suppose a Chinese character  X  is a seed in d ,  X   X   X   X   X  (National Palace Museum) occurs three times in d ,  X   X   X  (Museum) occurs 5 times in d . If we set  X  as 2, then both  X   X   X   X   X  (National Palace Museum) and  X   X   X  (Museum) are key terms in d , since that they, respectively, occur 3 and 2 times independently in d . However, if we set  X  as 3, then  X   X   X   X   X  (National Palace Museum) is key term in d , while  X   X   X  (Museum) is not since its independent occurrence is two (excluding the three occurrences as a sub-string in  X   X   X   X   X  (National Palace Museum)). 3 Query term weighting To re-rank the retrieved documents, we use the key terms in top retrieved documents. Here, we only focus on those occurring in both the documents and the queries. There are two reasons for this choice. One is to reduce the computational cost, since we need to compute correlations between terms during query weighting (Sect. 4 ), the other is that document re-ranking is only an intermediate phase in information retrieval, and it is usually followed by query expansion, which will normally consider other terms. So, the key terms here can also be referred to as query terms. To weight a query term, we make use of the information regarding its local and global distribution.
The global information of a term refers to its document frequency in the whole document collection, while the local information of a term refers to its distribution in top K retrieved documents. We use the following three factors. (1) Document frequency. If a key term occurs in a document, its document (2) Document positions. The position refers to the location of the document in the (3) Term length. The length refers to the number of Chinese characters a term
Intuitively, the more frequently a term occurs in the k documents, and the longer a term is, the more contribution to the precision the term may have. On the other hand, the higher ranking a document is, the more important the terms it contains tend to be.
With both the local and global information taken into consideration, the weight assigned to a key term t is given by the following formula. where d i is the i -th ( i = 1,..., k ) document, R is the number of total documents in the whole collection C , df ( t , d ) and df ( t , C ) are the document frequency in d and C , to d i , which implies a downgraded document frequency.
Intuitively, a term gets a lower document frequency if occurring in a lower-ranking document, and a higher one if occurring in a higher-ranking document. This is in contrast of the usual way for document frequency that a document gets 1 count no matter where the document is located in the list.

The weighting scheme in ( 2 ) is reminiscent of the definition given in (Robertson and Jones 1977 ). However, our method further considers document positions and term length, while theirs does not.
 4 Query weighting In general, a query may consist of multiple aspects or concepts and a concept may be denoted by multiple query terms. To prevent the bias toward specific query concepts, the documents matching with more query concepts should be preferred. To identify such documents, we need to measure the relatedness or independence between the terms, and use MMR to adjust their contribution to the query-document matching based on their relatedness or independence.

To estimate the relatedness or independence between query terms, we investigate their co-occurrence patterns in top K initially retrieved documents. If two query terms are correlated, they are expected to co-occur in these documents frequently, i. e., given the presence of one query term in a document, the chance of the other occurring within the same document is likely to be relatively high. On the other hand, if two query terms denote independent concepts, the occurrences would not be strongly correlated.

Given a query term t j , the conditional probability of another term t i given t j  X  X  containing both t i and t j , and d ( t j ) is the set of the documents containing t j . With this probability, Mitra and Buckley ( 1998 ) proposed a weight-based MMR decreasing of their weights, are query terms in a query and w ( t i ) is the weight of term t i , then the weight for the query is defined in ( 6 ). In this scheme, the lower ranking terms are subsequently penalized based on their relevance with the preceding ones. Notice that this strategy has a bias toward those terms with higher weights, since according to the decreasing order, the higher weight a term has, the fewer terms ranking before it and the less likely its weight will be reduced by a higher portion.

To avoid this bias, we propose a relevance-preferred strategy, in which the terms are ordered directly by their relevance with the preceding terms. Suppose T is the set Furthermore, t i,j (2  X  j  X  m ) meets ( 7 ). This can be seen as the weight of the query in the view of the term t i . To avoid the bias toward any specific term, we define (9) as the weight of the query. Compared with ( 6 ) and ( 9 ) is an average weight generated by the m term orderings, furthermore, the penalty of a term is solely based on its relevance with previous terms, irrespective of its own weight and without bias to terms with larger weights. The reason why we add a subscript, d , is that T is the set of terms shared by the query and the document, so the weight is relevant with the document. 5 Document re-ranking During the initial retrieval, we adopt two models, vector-space model (Salton 1968 ) and BM25 model (Robertson and Walker 1995 ). For the vector-space based model, each document or query is represented as a vector in vector space of which each dimension is a character bigram. The weight of a bigram b in a document d , w d ( b ), is given by the following tf / idf scheme: where tf d ( b ) is term frequency of b in d , N is the total number of the documents in the collection, and df ( b ) is document frequency of b in the collection. The weight of
For the BM25 model, the relevance between the document and the query is defined in ( 12 ). parameters. k 1 and b are set as 1.2 and 0.75, respectively, by default, and k 3 is set as 7. dl and avdl are, respectively, the document length and average document length measured by the number of the words.

During the re-ranking phase, a new relevance score between a document and a query is adjusted based on the weight of the query, which is in fact determined by the terms shared by the query and the document. We define the new score as in ( 15 ). According to the new score, the initially retrieved documents can be re-ordered. Here, we add 1 to w d ( q ) in order to maintain the original relevance score when there are no terms shared by a query and a document. 6 Experiments and evaluation In this section, we first specify experiment design including test data, evaluation measure and initial retrieval, and then focus on term resource result as well as re-ranking evaluation, including a sample query analysis, overall performance, selection of k , evaluation of MMR, and some comparisons. 6.1 Experiment design 6.1.1 Data We used NTCIR3 1 CLIR Chinese dataset as test data. The dataset consists of two corpora, CIRB011 and CIRB20. The former contains 132, 173 documents from China Times, China Times Express, Commercial Times, Central Daily News and China Daily News, and the latter contains 249, 508 documents from United Daily News. We used all the 42 queries released officially by NTCIR (Chen et al. 2003 ).
In the dataset, each query is a short description of a topic in Chinese language. (16) is an example. (16) Notice that queries (as well as sentences in documents) are continuous strings of Chinese characters, in which most characters and many bigrams are meaningful. This makes it a non-trivial issue to select appropriate indexing units among characters, words, bigrams and terms in Chinese information retrieval. 6.1.2 Evaluation measure For each query, NTCIR released relevant documents according to two judgments, Relax relevance and Rigid relevance. The former considers highly relevant, relevant and partially relevant documents, while the latter only considers highly relevant and relevant ones. For both judgements, we used MAP (mean average precision) of top K retrieved documents to evaluate retrieval performance.

Formally, for a query q , suppose d 1 , d 2 ,..., d K are top K retrieved documents. We set rel ( i )as1if d i is a relevant document and 0 otherwise. Let R be the number of all the relevant documents for a query, the average precision (AP) for the r th query is given in ( 17 ), and the MAP for all N queries in ( 18 ).
 In the following experiments, we set K as 1,000 unless specified, which means that we normally take the MAP of top 1,000 documents as evaluation measure. In addition, we set two parameters for term extraction (  X  and  X  ) as 10 and 4, respectively, through all the experiments. 6.1.3 Initial retrieval Commonly used indexing units for initial retrieval include characters, character bigrams and words. Table 1 lists their respective MAPs in initial retrieval under the two models, VSM and BM25.

From Table 1 , we can see that the performance of character bigrams as indexing units is much better than that of character unigrams, and comparable to that of words. Furthermore, with character bigrams as indexing units, it does not need word segmentation, which usually requires domain specific dictionaries and still suffers from the difficulty of domain portability. So we chose bigrams as indexing units for initial retrieval. 6.2 Query term results After extracting terms from documents, we had a term list for the entire document collection. This term list can be seen as a term resource specific to the collection. Table 2 gives the statistics of the terms.
 From Table 2 , although most automatically extracted terms consist of 2 or 3 Chinese characters, as can also be seen in other existing manual resources, there are terms with length up to 5 X 7 characters, which are rarely seen in manual resources.
Based on the term list, for each query, we determined query terms by simply checking whether they occur in the query or not. After initial retrieval, weights of these terms were computed based on the weighting scheme (2). Table 3 gives the statistics of query terms occurring in the 42 queries.

From Table 3 , we can see that bi-character terms have the lowest average weights. The reason is twofold. One is that most bi-character terms may be commonly used, leading to higher occurrence probabilities in the document collection. The other is that length of terms directly contributes to the weights.
Equation 19 lists the query terms with length as 5 X 7 characters. Compared with shorter terms, they tend to be more specific to some topics, and it is difficult to expect them to be included in any existing external resources. For example,  X   X   X   X   X  (Nissan and Renault) is normally analyzed as a phrase with a coordinate structure, which is rarely seen in any lexical resources. So, automatic identification of these terms or construction of such resources would be necessary. As an example for queries, Table 4 lists the query terms generated from the query Eq. 16 and their respective weights.

From Table 4 , we can see that the terms closely related with the topic, e.g.,  X   X  (the Imperial Palace),  X   X  (the Han Dynasty) and  X   X   X   X   X   X  (Han Dynasty relic exhibition), etc., were assigned with higher weights, while those not closely related or general words, e.g.,  X   X  (find out),  X   X  (relevant) and  X   X  (content), etc., got lower weights.
 Table 5 lists the correlation between some query terms extracted from the query Eq. 16.

From the probabilities in Table 5 , we can see that  X   X  (Imperial Palace) and  X   X   X  (museum) is mutually and closely related,  X   X   X   X  (the Han Dynasty relics) and  X   X   X   X  (relic exhibition) is somewhat related, and  X   X  (the Han Dynasty) is partially related with  X   X   X   X  (the Han Dynasty relics). Intuitively, these corre-lations reflect the concepts or aspects embedded in the query, and would be useful in ensuring the re-ranking to avoid any bias to a specific aspect.

To specify the weighting of queries, we consider a simplified version of query 16), which consists of only three terms:  X   X  (the Han Dynasty),  X   X   X   X  (relic exhibition) and  X   X   X   X   X  (Imperial Palace museum). Under the weight-based MMR, their ordering should be  X   X  (the Han Dynasty),  X   X   X   X  (relic exhibition) and  X   X   X   X   X  (Imperial Palace Museum). Based on their correlation, the penalty of  X   X   X   X  (relic exhibition) and  X   X   X   X   X  (Imperial Palace Museum) would be 0.034 and 0.306, respectively. Intuitively the latter two terms each denotes a relatively independent concept compared with the first tone, however, the third term gets a much higher penalty than the second one. Under the relevance-based MMR, their ordering would be  X   X  (the Han Dynasty),  X   X   X   X   X  (Imperial Palace museum) and  X   X   X   X  (relic exhibition), and the penalty of the latter two terms would be 0.042 and 0.089, respectively, which is purely based on their relevance with previous ones. To compare the overall impact of the two MMR schemes, we took the relevant documents for 16) as those for this simplified query, and checked the query weights with respect to these documents. The query weight under the relevance-based MMR is 11 X 26% higher than that under the weight-based MMR, which suggests that the relevance-based MMR better capture the balance of the concepts in the query. 6.3 Re-ranking results 6.3.1 An example As an example for the document re-ranking, consider query (16) again. For this particular query, it had 24 and 30 relevant documents under the two judgments, respectively. Take VSM as an example, the relevant documents were distributed in top 436 and 625 documents in initial retrieval, with only 4 and 8 falling beyond top 100, respectively. Figures 1 and 2 show the position change for the documents within top 100 before and after the re-ranking.

Figures 1 and 2 demonstrate that most relevant documents moved forward after re-ranking, while only 3 rigid and 6 relax documents moved backward.
Since top documents are generally more important, for example, in query expansion, we checked the increase of relevant documents in top 50. Figures 3 and 4 show the comparison of relevant documents in top 50 under VSM.

Figures 3 and 4 demonstrate that the number of relevant documents increased among all the intervals in top 50 documents. On the whole, MAPs of this query increased by 58.4% (from 0.2615 to 0.4200) and 34.7% (from 0.4351 to 0.5859), respectively, under the two judgments. 6.3.2 Overall performance The overall performance of document re-ranking can be directly reflected from the improvement of achieved MAPs over those of initial retrieval. On the other hand, since the immediate application of document re-ranking is to improve query expansion, the overall performance can also be reflected by its contribution to query expansion. Tables 6 and 7 show comparison of MAPs of initial retrieval, re-ranking, query expansion and re-ranking plus query expansion under the two models (VSM and BM25). Here, we use standard Rocchio X  X  method for query expansion (Rocchio 1971 ).

Tables 6 and 7 indicate that re-ranking helped to improve MAPs of initial retrieval by from 14.2 to 34.1%, which is statistically significant according to significance tests. On the other hand, although document re-ranking achieved comparable (VSM) or even worse (BM25) performance than query expansion individually, it helped to improve that of query expansion significantly by 11.1 X 12.6%. This demonstrates that re-ranking can not only improve the performance of initial retrieval, but also have a significant contribution to query expansion.

In addition, the overall performance of document re-ranking can also be demonstrated by the increase of relevant documents in top 50 under the two models. Figures 5 , 6 , 7 , and 8 show the change of relevant documents under the two models, which were averaged over the 42 queries.

In details, relevant documents in top 50 increased by 13.8% (from 8.81 to 10.024, rigid, VSM), 17.6% (from 12.071 to 14.191, relax, VSM), 21.1% (from 8.238 to 9.976, rigid, BM25) and 15.8% (from 12.476 to 14.452, relax, BM25), respectively. 6.3.3 Selection of k For pseudo-relevance feedback, selection of appropriate top k documents is critical, which normally varies with different topics and document collections. To see that, we randomly chose 4 queries, and used different top k documents as relevance feedback to do query expansion based on Rochio X  X  method. Figure 9 shows MAP curves with k .

Figure 9 demonstrates that the values of k maximizing MAPs ( A , B , C , D ) are queries.
However, with our weighting scheme, we can randomly, but safely, select a bigger k . Figure 10 shows MAP curves with different number of top documents used for document re-ranking under the two models.

The curves in Fig. 10 indicate that in all the four cases, MAPs kept increasing as more documents were used until some points (roughly A , B , C and D , respectively), and then remained stable afterwards. The reason may be that in the beginning, terms may acquire better weighting with more documents, which leads to better performance. Later after some points, those lower ranking documents may have less impact in term weighting and inclusion of them will not help improve the performance, so the performance remains stable from the points afterwards. This suggests that we could randomly select a large k , i.e., as many documents as possible to weigh the query terms, and would not worry about noisy documents among them.

Another finding from Fig. 10 is that the stable points were different for the four cases, which again means that the appropriate value of k varies with different models and judgments. 6.3.4 Evaluation of MMR To explore the impact of MMR in the re-ranking, Table 8 shows comparison of MAPs for non-MMR, Mitra X  X  MMR and our MMR.

In Fig. 10 ,  X  X NI X  stands for initial retrieval result, and  X  X o X  stands for the re-ranking without MMR,  X  X itra X  X  X  stands for the re-ranking with Mitra X  X  MMR, while  X  X urs X  stands for the re-ranking with our MMR. From the comparison, we can see that the overall performance of the re-ranking with our MMR was significantly better than that with Mitra X  X . Furthermore, our MMR module helped to improve the performance by 2.1 X 7.0%, respectively, in the two models, which indicates that the correlation between query terms is useful for improvement of the re-ranking.
To further compare our MMR with Mitra X  X , Fig. 11 shows MAP curves with different numbers of top documents used for term weighting in BM25 model.
From the comparison, we can see that for all choices of k , the performance with our MMR was significantly better than that of Mitra X  X , especially for larger k  X  X . Furthermore, the performance with our MMR kept increasing or remained stable while the performance with Mitra X  X  deteriorated as k increases. This suggests that the idf based weighting in Mitra X  X  scheme could not properly reflect the contribution of terms in document re-ranking, which may render the scheme suffering from the noises in top documents. 6.3.5 Evaluation of weighting factors To explore the impact of initial document positions, local and global distribution, as well as length of terms in the re-ranking performance, Table 9 shows comparison of MAPs in the two models. To justify the factors in (2), MMR was not considered here.
The comparison demonstrates that the three factors are all useful for improve-ment of the performance, which motivates the weighting scheme in (2). Notice that if term length and document positions are not considered, the weighting scheme is similar with that proposed in (Robertson and Jones 1977 ), which only considers local and global distributions. The comparison indicates that both document positions and term length are helpful for improvement of performance. 6.3.6 Terms, words and bigrams In the re-ranking process, we used query terms to compute new relevance scores between queries and documents. Other two options were words and character bigrams. Table 10 shows comparison between their performances.
 From the comparison, we can see that terms well outperform words and bigrams. The reason may be twofold. One is that term extraction can help exclude some common words or bigrams if they do not meet the criteria for seed words. The other is that the length of the terms is helpful for improving the performance.
To further confirm the usefulness of the automatically acquired terms, we removed the terms in the decreasing order of their length, i.e., removed those with length as 7 in the first step, and removed those with length as 6 in the second step and so on. Then we checked the impact of the removal on the performance. Figure 12 shows the MAP change with the removal of the terms.
 From Fig. 12 , we have several findings. First, as more terms were removed, both MAP scores decreased, which demonstrates that the terms with any length all have more or less contribution to the performance. Second, when removing terms with 4 characters, there is a bigger loss in MAP scores, which suggests that the terms with 4 characters be very important to ensure the performance. Third, when only considering terms with 2 or 3 characters, the MAP scores were similar with that acquired using words to re-order the documents, which is reasonable since most words contain 2 or 3 characters. In general, these findings show that the automatically acquired terms are all useful for the re-ordering performance. 7 Related work One related work is local context analysis for query expansion (Xu and Croft 1996 , 2000 ), where concepts (often represented by noun phrases) were chosen from top ranked documents based on their co-occurrence with query terms. One distinguished characteristics of the method is that passages rather than documents were used for concept ranking. Compared with our work here, there are some differences. First, while our focus is on weighting query terms for document re-ordering, theirs is on weighting other (not query) terms relevant with query terms for query expansion. Second, their weighting scheme does not consider document rankings and term length. Third, in their method, the number of passages used for concept ranking needs to be pre-specified, as in cases of top k document assumption in query expansion, and the performance is sensitive to the number.

Another related work is based on language modeling to estimate amount of relevant information in feedback documents (Zhai and Lafferty 2002 ; Tao 2004 ). The intuition behind this method is similar to ours that the feedback documents may have different amounts of relevant information, and higher ranked documents should have more than lower ranked ones. In their method, a parameter is associated with each feedback document to encode its ratio of relevance, which can be directly estimated along with other parameters by EM algorithm. In comparison, our method tries to weigh terms first, and then the weight of a document, intuitively encoding relevance of the document, is derived from accumulation of the weights of all the terms it contains. On the other hand, document rankings in language modeling method do not contribute to the ration of relevance directly, which makes it difficult to penalize the lower ranking documents sufficiently to ensure the estimated model to be mainly based on top ranked documents. So, their method still suffers from top k document assumption, as their experiments on TREC data showed. In contrast, weights of terms in our method are directly related with document rankings, which can ensure sufficient penalty to lower ranking documents. In fact, the evaluation shows that it is possible to set a much larger k randomly but safely without sacrificing the performance in our method.

Recently, there is a trend to explore intrinsic structure of documents to re-rank document. For example, Zhang et al. ( 2005 ) proposed an affinity graph to re-rank documents by optimizing their diversity and information richness, Kurland ( 2005 ) proposed a structural re-ranking approach using asymmetric relationships among documents induced by language models, Diaz ( 2005 ) used score-regularization to adjust ad-hoc retrieval scores from an initial retrieval so that topically related documents received similar scores, and Yang et al. ( 2006 ) gave a label propagation algorithm to capture document relatedness for document re-ranking. In general, such methods normally make use of information regarding manifold structure behind the documents, while our method uses term-level information. In addition, the methods normally require some positive examples to retrieve relevant documents, and if the examples are more than the query, they still face the problem of top k document assumption. 8 Conclusions and future work In this paper, we propose a document re-ranking method based on an automatically built term resource specific to the text collection. The resource includes a list of terms as well as weighting and correlation of query terms acquired after initial retrieval. The weighting based on local and global distribution of terms ensures the document re-ranking not sensitive to different choices of pseudo relevance, while the correlation between terms helps avoid any bias to specific concepts in queries.
Compared with other approaches, this method does not rely on any external manual resources, but builds the resources automatically from the document collection. So, the method can be easily ported to information retrieval for other languages. Particularly, for Chinese information retrieval, we do not use any word segmentation, part-of-speech tagging and syntactic analysis, etc. So, the approach can also be quickly adapted to any specific domains in Chinese information retrieval.

The experiments based on NTCIR3 CLIR tasks demonstrate that the re-ranking method can achieve significant improvement of the retrieval performance against the baseline. On the other hand, the experiments also show that the re-ranking method, as an add-on retrieval module, can make significant contribution to the performance of standard query expansion strategies. The experimental results support our assumptions: automatically acquired key terms in retrieved documents can be used to improve the performance; longer key terms contain more precise information, and document ranking positions and relative document frequency of query terms imply importance of terms.

Currently, the method only considers those terms which appear in queries. In fact, terms with higher weights but not occurring in queries may also contribute to the re-ranking. In future, we will also use such terms, while taking the computational cost into consideration. In addition, when computing the re-ranking score of documents, we do not consider term frequency in documents. So, another future work is to incorporate term frequency into the re-ranking scores. Moreover, the term extraction module, as the basis of this document re-ranking approach, adopts a very simple and purely statistical method. There may come out some error terms due to the fact that the compactness within terms is not considered. Although these may not necessarily hurt the performance, we will still consider more effective approaches in future for better term extraction to provide more support for document re-ranking. Finally, although the experiments are based on Chinese data, the method is in fact language independent. In future, we X  X l do further tests on datasets in other languages.
 References
