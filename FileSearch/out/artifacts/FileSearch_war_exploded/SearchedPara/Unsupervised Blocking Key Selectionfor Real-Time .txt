 Massive amounts of data are being collected by most business and government organisations. Given that many of these organisations rely on information in their day-to-day operations, the quality of the collected data has a direct impact on the quality of the produced outcomes [ 4 ]. Data validation and cleaning are often employed to improve data quality [ 4 ]. One important practice in data cleaning is entity resolution (ER), which is the task of identifying records that refer to the same real-world entity.
 cleans and standardizes the data to be used; indexing or ( blocking ), which reduces the search space; record comparison , which compares candidate records in detail using a set of similarity matching functions [ 8 ]; classification , where pairs or groups of candidate records are classified into matches (records that are assumed to refer to the same entity) and non-matches (records that are assumed to refer to different entities); and finally, evaluation , where the ER process is evaluated with regard to matching accuracy and completeness [ 4 ]. Since many services in both the private and public sectors are moving online, organisations increasingly require real-time ER (with sub-second response times) on query records that need to be matched with existing entity databases [ 7 , 15 ]. it reduces the number of candidate records to be compared in detail to find matching records. This can be achieved by two main approaches. The first is to partition a database to be matched into several blocks according to a blocking key criterion, where only records that are inserted into the same block are compared with each other [ 9 ]. The second approach is to sort the records in a database according to a sorting key criterion that brings similar records close to each other, so that only records that are close to each other will be compared [ 11 ]. close to each other in the index [ 5 ]. This depends mainly on the blocking/sorting key used to partition/sort the records in a database. An optimal key needs to find all the true matching records, while keeping to a minimum the number of true non-matching records. However, an optimal key for one domain will likely not work for another domain [ 5 ].
 ER, because for real-time ER we need to have small block sizes to achieve fast query matching. Selecting an optimal key needs expert knowledge of the nature of the data and the requirements of the domain. To the best of our knowledge, no existing learning technique for indexing considers real-time ER. Therefore, there is a need for novel techniques that learn optimal keys for different real-time ER domains without the need for manual intervention.
 Contribution: In this paper, we propose a general learning technique that auto-matically selects optimal keys for building indexes to be used in real-time ER in order to find matches in a database effectively and efficiently. Our approach can be used with different indexing techniques. We demonstrate how this automatic key selection can be used with an existing sorted neighbourhood-based real-time indexing technique [ 19 ]. We learn more than one key to be used with multi-pass sorting or blocking techniques. We evaluate the proposed technique on three real-world databases and compare it with an existing technique [ 12 ]. The earliest proposed indexing approach is standard blocking [ 9 ], which inserts records into blocks according to a blocking key criterion. This criterion is usually based on one or more attribute values. Only records within the same block are compared with each other. This approach has the disadvantage of assigning records into the wrong block in case of errors in the attributes used as blocking keys (i.e. dirty data). To prevent this from occurring, iterative blocking [ 24 ]can be applied where multiple blocking keys are used and each record can be inserted into more than one block.
 The sorted neighbourhood method (SNM) [ 11 ] arranges all records in the database(s) to be matched into a sorted array using a sorting key . Then a fixed-size widow is used to scan over the sorted records comparing only records within the window at any step. The main drawback of this method is its sensitivity to errors and variations at the beginning of the attribute values that are used as sorting keys, which can significantly affect the quality of the matching results [ 5 ]. This drawback is handled by performing a multi-pass approach [ 11 ] where dif-ferent sorting keys are used in each pass to improve the matching quality of the approach. Various other indexing techniques that are based on either one of the of these techniques blocking or sorting keys need to be defined manually by an expert who has domain and application knowledge.
 Various automatic techniques were proposed that allow learning optimal blocking/sorting keys based on supervised learning which requires the use of gold standard data for training. Bilenko et al. [ 2 ] proposed an approach that deals with the learning process as an approximation problem that is based on the red-blue set cover problem. Michelson et al. [ 18 ] proposed a related approach for learning which attributes are more suitable as blocking keys, and which sim-ilarity measures should be used for comparing these attributes.
 Another supervised approach was recently proposed by Vogel and Naumann [ 23 ]. The authors use unigrams of attribute values (i.e. a combination of single characters from different attributes) as blocking keys. Both accuracy and effi-ciency of the generated blocks are used to learn the set of optimal blocking keys. They also improved their approach by taking the length of attribute values into consideration when generating the unigrams to be used as keys. All of the above automatic approaches require labeled training data. However, such labeled data is not always available and is usually expensive to generate.
 To overcome this problem, several un-supervised automatic blocking key selection techniques have been developed [ 3 , 10 , 12 , 16 ]. Ma et al. [ 16 ] has pro-posed an approach that is based on type semantics, where the authors con-sider the type of entities when learning the blocking keys for data from the web. Another unsupervised learning approach was proposed by Kejriwal and Miranker [ 12 ] where the authors automatically generate a weakly labeled data set. This labeled data is then used as a training set to learn the optimal blocking keys using the Fisher discrimination criterion [ 12 ]. Giang [ 10 ] on the other hand proposed a technique that learns the blocking keys in context of the classifier function that is used in the classification step of the ER process. The classifier is used to generate labeled data. The authors then use the Probably Approximately Correct (PAC) approach to learn the blocking keys.
 generated blocks and do not consider the block sizes when selecting blocking keys. However, a blocking key that can be used with real-time ER must also ensure that the sizes of the generated blocks are small enough to be able to resolve queries in real-time. In this paper, we propose an automatic blocking key selection technique that considers the coverage of a key, the maximum size of the generated blocks, as well as the distribution of the size of the generated blocks. Our aim is to learn blocking keys that are suitable for real-time ER. We use the following notation to present our approach. We assume a database { r We denote the attribute value a j in r i with r i .a j , where 1 | A | .
 attribute a j  X  A and a blocking function f l  X  F , with F date blocking functions. Examples of such functions include exact value ( same first character ( sameFirst1 ), or same last three characters ( blocking function f l is applied on attribute a j , and the resulting value for a record r is called a blocking key value (BKV) and denoted as k j,l denote the set of all candidate BKs with K . We assume the functions in are manually selected by domain and ER experts, but for future work we aim to investigate techniques to automatically identify suitable blocking functions based on the content of a database. Our optimal key selection approach will identify the best BKs for real-time ER based on three criteria, as described in Sect. 4 .
 R BK k j,l on all records in R .
 learn keys that are suitable for real-time ER. In real-time ER, the selected BK(s) should generate block sizes within a controllable range to make sure that the number of detailed comparisons needed to match a query record (denoted as is within an allocated time. Also, keys that generate blocks of similar sizes are more suitable for real-time ER than keys that generate blocks of different sizes, as the time required to resolve different query records will be the same [ 6 ]. to be used with multi-pass indexing techniques [ 19 ] to perform ER and deliver high quality matching results in real-time. Following [ 12 ], our approach does not require existing training data sets to learn these optimal keys.
 as illustrated in Fig. 1 . In step (1) we generate positive and negative training data sets ( R P and R N ) to be used in the learning process [ 12 ], as detailed below. In step (2) the set of candidate blocking keys K is generated. The proposed learning algorithm, described in Sect. 4 , is employed in step (3) using the generated training data sets R P and R N to select a set of optimal blocking keys The selected optimal keys O are used in step (4) to index (block) all records from the database R . Any real-time indexing technique can be used for this step [ 19 ]. Finally, in step (5), the built index is used for matching query records records within the index in real-time. 3.1 Generating Training Data Sets As in most practical applications of ER no training data sets (gold standard data) are available, such data can be generated using classification functions as training data sets using a TF-IDF weighting scheme to calculate the similarity between record pairs ( r x ,r y )  X  R as follows.
 A lower and upper thresholds 0 &lt;l&lt;u&lt; 1 are used to generate the training data sets. Record pairs ( r x ,r y ) that have a TF-IDF similarity value sim ( r x ,r y )below l are labeled as negative matches, and all pairs that have a TF-IDF value above u are labeled as positive matches. We generate a positive training set R P  X  R where the similarity between record pairs is greater than or equal to the upper threshold u : R P = { r x ,r y  X  R : negative training set R N  X  R where the similarity between record pairs is less than or equal to the lower threshold l : R N = { r x ,r y R  X  R N =  X  .
 Both R P and R N are then used to generate a set of blocking key vectors , and V N respectively, by applying all keys k j,l  X  K on the record pairs in R [ 12 ]. Each record pair is converted into a vector of Boolean values (i.e 0 or 1 bits), with one value for each candidate key k j,l  X  K . If a record pair ( R or R N for a certain candidate key k j,l results in having the same key value, i.e. k j,l ( r x )= k j,l ( r y ), then the corresponding element in the pair X  X  vector is set to 1 and the pair is said to be covered by this key. Otherwise, the corresponding vector element is set to 0, and the pair is said to be uncovered by that key. tion algorithm to learn the optimal keys, as described in the following section. Alternatively, if a truth training set is available, step (1) of our framework is not required. The rest of the steps of our framework are described in more detail in the following sections. The indexing step of real-time ER should bring similar records close to each other while maintaining small block sizes to be able to match query records in real-time. The BKs used in the indexing step have an impact on the quality and efficiency of query matching. To make sure that the keys we select are suitable for real-time ER we use three criteria:  X  Key coverage: The coverage C of a key k j,l that is applied on record pairs  X  Block Size: The size of a block b is the number of records that are inserted  X  Distribution of blocks: Forthesetofblocks B generated from applying where S b is the size of a block b  X  B and  X  S is the mean of all block sizes in B . A variance value that is equal to 0 means that all generated blocks have exactly the same size. For real-time ER it is better to generate blocks of similar sizes where the time required to match a query record is similar for different query records. Therefore, a BK is more suitable for real-time ER if its variance of the sizes of the generated blocks is close to 0. Generating Candidate Keys: The candidate key list is the list of all keys which we select our optimal keys from. The candidate BKs can differ based on the domain, the used indexing technique, and the databases to be matched. Because we are evaluating our key selection algorithm using a sorted neighbour-hood indexing technique (as will be discussed in Sect. 5 ), we generate a list of candidate BKs K that capture the beginning of attribute values (i.e. sameFirst1 , sameFirst2 , sameFirst3 , sameFirst4 , sameFirst5 Exact ). To generate a set of K blocking keys we apply all blocking functions in F on all r i .a j  X  R . The generated set of blocking keys K learning algorithm along with V P and V N to select optimal keys as follows. Learning Optimal Keys: Our learning algorithm (see Algorithm 1) automat-ically selects the list of optimal keys O based on the three criteria discussed earlier (key coverage, generated block sizes, and distribution of block sizes) to ensure that the selected keys can be used with real-time ER to provide matching results efficiently. We start the algorithm by initialising the valid key list ( to be empty. This list is then filled with keys that cover less than V , where n m is the maximum allowed number of covered vectors from negative key vectors (lines 1-4). Then, for each key in the valid key list a maximum block size S b size s m ,itisremovedfrom K v (lines 5-8). In lines 9-13, for all keys we calculate an overall score SC k to determine which keys should be added to the optimal key list O based on the following equation: where C k is the coverage of k (as calculated in Equation 1 ), the average block size and the variance between the block sizes, respectively. We assume that the blocks are generated by applying the blocking key records in R . The aim is to select a set of blocking keys that have high coverage, low average block size, and low variance between block sizes (note that keys with large maximum block size S b ( max ) were removed earlier from parameters  X  and  X  are used to control the weights of the three criteria based on the domain and application area. Each weight parameter is a value between 0 and 1 where the total of all weights is equal to 1. Regardless of the weight parameters used, the lower the overall score for a key is, the more this key is suited for real-time ER.
 sorted in an ascending order, since a lower overall score is better (line 14). The first key in the overall score list is then added to the optimal key list 15-16). In lines 17  X  18, all positive vectors that are covered by the selected optimal key are removed from V P , and the optimal key is also removed from in line 19. This process continues until the required number of optimal keys is reached or until there are no positive vectors left in keys O are evaluated by performing the ER process on database keys selected in the indexing step as described next. In our experiments, we use three data sets (see Table 1 ). The OZ data set contains personal information that is generated by randomly selecting records from an Aus-tralian telephone directory (a clean data set). Duplicates are added to this data set by randomly modifying attribute values based on typing, scanning and OCR errors, or phonetic variations [ 22 ]. Both the Cora 1 and DBLP/ACM [ 14 ] data sets contain bibliographic information and are commonly used in ER research. We use the blocking key selection approach proposed in [ 12 ] as a baseline. The authors in [ 12 ] propose a Fisher Disjunctive algorithm (FDJ) that uses the Fisher discrimination criterion to select optimal blocking keys. Unlike our approach (that considers key coverage, block sizes, and blocks distribution), this approach only considers key coverage when selecting optimal blocking keys. We compare our approach with the baseline approach using recall (the fraction of relevant instances that are retrieved) to measure the quality of the compared approaches, and query time (the time required to resolve a single query record) to measure efficiency. In addition, we generate various statistics about the number of candidate records required by both approaches to resolve a query record. For generating the training data sets (described in Sect. 3.1 ) we used a lower threshold l =0 . 1 and an upper threshold u =0 . 7 to weakly label record pairs into positive and negative pairs. The generated training data sets are then used in our learning algorithm as described in Sect. 4 . To learn the optimal blocking keys we used n max = 100 for the maximum allowed number of covered vectors from V , we used a maximum block size of sm = 100, and for the weight parameters we used  X  =0 . 2and  X  =0 . 4. Weights and thresholds used are selected based on an experimental investigation of using different values. We aim to investigate learning these values to produce blocks with high quality and small size in our future work.
 To conduct the evaluation we use the keys selected by our approach and the keys selected by the FDJ approach to build indexes that can be used to resolve query records in real-time. A real-time forest-based dynamic sorted neighbour-hood index (F-DySNI) is used for this purpose [ 19 ]. The index consists of multiple tree data structures where each tree is built using a different sorting key. The F-DySNI has two phases: a build phase where index trees are built using records from an existing entity database, and a query phase where the built index can be queried by retrieving candidate records for a query record from all index trees, and the index is updated by inserting the query record.
 When a query record arrives it is inserted into all trees in the index. Then, in each tree, a window of size w is used to generate a set of candidate records from the tree node that contains the query record and the neighbouring tree nodes that fall within the window w . The query is then compared (using an approximating string similarity function [ 4 ]) with the generated candidate records. Candidate records with similarities above a specific threshold are considered to be matches. We use 50% of the records in each data set to build the indexes, and the remain-ing records are used as query records. For generating the candidate records we use a window of size w = 2 (the same window size used in [ 19 ]).
 The aim of the experiments is to investigate if the optimal keys selected by our learning approach are suitable for real-time ER. Results in Fig. 2 illustrate the query time required to resolve a single query in milliseconds (ms) and recall values for the three data sets. The results show that the blocking keys selected by our approach improve the efficiency of query matching significantly. The selected keys using the proposed approach achieved an average query time of 2, 8, and 9 ms for the OZ, Cora, and DBLP-ACM respectively, while the selected keys using the baseline achieved an average query time of 206, 175, and 938 ms for OZ, Cora and DBKP-ACM respectively. This significant improvement in query time is achieved while maintaining recall for the OZ and DBLP-ACM but with a 5% decrease in recall value for Cora. Note that in our experiments the weight (  X  =0 . 2) that we use for the quality (i.e. key coverage) is half of the weight (  X  =0 . 4 , X  =0 . 4) that we use for the block size and the distribution. candidate records generated using the proposed and the FDJ approaches using the F-DySNI with a window of size w = 2. It is clear from the table that the proposed approach has decreased the number of candidate records greatly which is the reason behind the significant decrease in query times.
 selected by the proposed and the FDJ approaches on the OZ data set (the other two data sets were too small to clearly show how the blocks are distributed). The results show that the keys selected by our approach lead to having block sizes that do not exceed the maximum allowed block size. It also shows that the generated blocks using our approach have similar sizes. In contrast, the keys selected by the FDJ approach lead to blocks of various sizes. These results are also supported by Table 2 where the standard deviation values (which measure the amount of variation from the average) of the number of candidate records required using our selected keys are small (compared to the values of the baseline) for the three data sets. This means that the block sizes tend to be close to the average block size. We can conclude that the blocking keys selected by our proposed approach are suitable for use with real-time ER. We proposed an unsupervised blocking key selection algorithm that automati-cally selects optimal blocking keys for building indexes that can be used with real-time ER. We specifically learnt multiple keys to be used with multi-pass sorted neighbourhood indexing. We evaluated our approach using three real-world data sets and compared it with an existing automatic blocking key selec-tion technique. The results show that our approach can learn keys that are suitable for real-time ER. The keys selected by our approach reduced query times significantly while maintaining matching quality. For future work we aim to investigate how we can automatically identify candidate blocking functions based on the content of the database. We also aim to investigate learning the weights that are used in our key selection algorithm to produce blocks with high quality and small size. Additionally, we plan to compare our proposed approach with other existing blocking key selection approaches.

