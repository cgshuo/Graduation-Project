 In many applications, classification labels may not be associ-ated with a single instance of records, but may be associated with a data set of records. The class behavior may not be possible to infer effectively from a single record, but may be only be inferred by an aggregate set of records. Therefore, in this problem, the class label is associated with a set of instances both in the training and test data . Therefore, the problem may be understood to be that of classifying a set of data sets . Typically, the classification behavior may only be inferred from the overall patterns of data distribution, and very little information is embedded in any given record for classification purposes . We refer to this problem as the setwise classification problem .

The problem can be extremely challenging in scenarios where the data is received in the form of a stream, and the records within any particular data set may not necessarily be received contiguously. In this paper, we present a first approach for real time and streaming classification of such data. We present experimental results illustrating the effec-tiveness of the approach.
 H.2.8 [ Database Management ]: Database Applications Data Streams; Data Classification
In many applications, classification labels are not associ-ated with individual records, but with groups of records in the underlying data. Thus, each group of records is treated as an indivisible entity, along with an associated class la-bel. In most cases, the classification behavior can only be inferred from the overall distribution pattern of the records in this entity, and a given record typically provides very lit-tle information about classification behavior. This kind of problem can arise in the context of a wide variety of appli-cations:
This problem is referred to as the setwise classification problem . We note that each set may contain tremendous variations in the feature values of the given records, and in many applications, it is by analyzing the specific patterns of these variations , that the setwise entities can be assigned a particular class. In many cases, individual records cannot be meaningfully assigned to classes, because many different classes may contain very similar data points. In order to illustrate this point, we have shown the distribution of the data records in a two such entities belonging to different classes in Figure 1, which are marked by  X  X  X  and  X  X  X  respec-tively. At first sight, it would seem that there is no difference in the distributions of the two classes, and the two entities have identical distributions. The subtle differences in distri-bution can be understood only upon closer examination. In particular, the setwise entity belonging to the class marked  X  X  X  has marginally greater concentrations at the peripheries of the data ranges, whereas the setwise entity belonging to the class marked  X  X  X  has marginally greater concentration at the central regions. The average behaviors of the records in the two entities are also not very different. In such cases, it may not be meaningful to classify any single data point, because of the fact the differences in distributions are very subtle over different regions of the data. The example in Figure 1 is a very simple case, in which we have shown the relative behavior of only two such entities. In practice, there may be thousands of such entities with subtle variations in their data distributions and shapes. For example, another entity belonging to class  X  X  X  may not necessarily be concen-trated at the peripheries, but may have a slightly different shape which is characteristic of that class. Typically, there may be a small number of characteristic properties in the data distributions for a particular class , but different enti-ties may use a different subset of these characteristic prop-erties. When the entire data is viewed as a distribution of individual records across different classes, it may be vir-tually impossible to distinguish the instances at the lowest level. The challenge is to learn these subtle differences and similarities at the entity-distribution level efficiently in the stream scenario. Since the data points for a particular set entity are not received sequentially, this further increases the challenge from the perspective of stream processing [1, 6]. In this paper, we will design an efficient classification method which captures the subtle entity-specific character-istics in the form of classification signatures. These clas-sification signatures essentially create a summarized model of the typical patterns in the underlying class behavior of different entities. We will show how to use this model for effective classification.

This paper is organized as follows. We will present related work in the remainder of this section. In section 2, we de-sign efficient methods for classification of multi-set streams. The experimental results are presented in section 3. The conclusions and summary are present in section 4.
The problem of classification has been widely studied in the machine learning and data mining literature [9, 12, 21, 23]. Surveys on data classification may be found in [7]. Re-cently, the problem has also been extended to the stream scenario. The earliest work in the area focussed on the extension of decision trees for stream classification [6, 11, 14, 15, 16, 22]. A popular ensemble classifier was proposed in [24], and the use of data selection for stream classification for explored in [13]. A method for on-demand classification of data streams was proposed in [2]. Recently, stream clas-sification has also been adapted to the rare-class detection problem [8, 17, 18, 19]. A survey of stream classification may be found in [3]. All these methods are proposed for the model construction and classification of test instances.
A setwise method for the clustering problem has recently been proposed in [4]. Other methods have also been pro-posed in which test instances are classified on the basis of the a-priori knowledge that all test instances belong to the same class [20], or for labels to be attached to bags of train-ing instances [10]. However, we note that these problems still attach labels to individual instances at a fundamental level both during training and testing. The specification of labels with bags of instances is simply a result of either unavailabil-ity of labels with individual training instances, or additional meta-information provided with test instances. Individual instances can be meaningfully classified to labels in these cases. In the case of [10], the attaching of labels to bags of training instances is simply an approximation because of unavailability of labels about individual training instances instances. In the case of [20], the additional knowledge that asetof test instances belong to the same class simply en-hances the accuracy of classification of individual instances, which could otherwise be performed without this informa-tion. Our model is fundamentally different, because labels are attached to setwise entities both during training and testing, and it is not even meaningful to attach labels to individual instances of data records. Rather, the classifica-tion behavior of an entity can only be defined on the basis of the distribution of the records inside it. This is a much more challenging scenario for the classification process, and especially so in the stream scenario.
We will first introduce the notations and definitions which are relevant to our work. We assume that the different set-wise entities in the training stream are denoted by D 1 ... Each entity D i is associated with the class label l i .Weas-sume that there are a total of k classes, and the class label is drawn from { 1 ...k } . The dimensionality of each data set is d , and each data set has the same set of features. It is assumed that the i th data set D i has n i records. The j th record of the i th training entity is denoted by X j ( i ). Since the dimensionality of the data is d , it is evident that the record X j ( i ) is a vector containing d components. In addi-tion, we have a set of n test entities, which are denoted by T ... T n .The j th record of the i th test entity is denoted by Z ( i ).

Definition 1 (Setwise Classification). Given a set of entities D 1 ... D N , with associated labels l 1 ...l struct a training model M , which allows us to classify the different set entities T 1 ... T n .
 The aforementioned problem definition is actually a simplifi-cation from the stream scenario. In the stream scenario, the individual records of an entity may be received in any par-ticular order. Therefore, the individual records are tagged with the identifier of their entity, and also the corresponding entity label.

We assume that the data is received in the form of a stream &lt; Y 1 ,entityid 1 ,label 1 &gt;...&lt; Y r ,entityid ... . The tuple Y r represents a d -dimensional data point which could be either from the training or the test data, the notation entityid r represents the id of the entity, and label represents the label of the underlying data. The value of label r is drawn from { 1 ...k } , if the record Y r is drawn from an entity belonging to the training data. Otherwise, the record Y r is drawn from an entity belonging to the test data, and the value of label r is  X  1. Thus, the records are not only out of order, but the records from the training and test stream mayalsobemixedwithoneanother. Furthermore, we note that the class label for a test entity can be predicted as soon as a sufficient number of records are received in order to be able to predict its underlying distribution. As more records are received, the predicted label for a test entity will change (and typically become more accurate), because its underly-ing data distribution can also be estimated more accurately. The entity identifier is an integer drawn from [1 ,N ], if the entity is drawn from the training data. Otherwise, the entity identifier is an integer drawn from [1 ,n ]. Therefore, assum-ing that Y r is the j th data point of its entity, the value of Y r is the same as either X j ( entityid r )or Z j ( entityid pending upon whether the r th record is a training record or test record respectively.
In this section, we will describe the process of setwise model maintenance of the different entities. The core idea is to create a set of supervised entity profiles which concisely characterize the distribution of the different entities. One possibility is to use a density estimation approach [25], which can model the relationships between the different classes and the corresponding data distributions. However, such an ap-proach is unlikely to computationally efficient in the stream scenario, because of the need to track the densities over many different portions of the space; the number of such portions typically grows exponentially with the dimension-ality of the space. Therefore, we will design a method which computes supervised class fingerprints in order to construct an effective classification model.

In order to concisely represent the distribution behavior of the different entities, we will use the concept of fingerprints in order to concisely summarize them. The idea of a finger-print is to maintain the distribution patterns of the different entities in the data stream. These distribution patterns can be further leveraged to create concise representations of the profiles of different classes in the underlying data in online fashion . Clearly, such a model needs to be updated contin-uously. As more data points arrive in the stream, existing entity distribution patterns may evolve because of concept drift, and other entities with entirely new distribution pat-terns are received.

We note that the data points in the underlying data stream may belong to either labeled or unlabeled entities, depend-ing upon whether they need to be used for model build-ing (training data) or model-based prediction (test data). Clearly, this can be the case in many real applications, in which the model construction and prediction needs to be performed simultaneously. A given data point is used for model building, only if a label is associated with it, which is drawn from { 1 ...k } .Otherwise,ifthelabelvalueis  X  1, the entity is a test instance, and we update its current clas-sification status with the new data record, which is added to the distribution of the test instance. This goal is achieved by modeling the entity distributions in relation to landmarks or anchor points that are generated from the underlying data. The entity-specific statistical information, which are also re-ferred to as fingerprints, are used to dynamically maintain the sets of class profiles. These represent the common entity distributions which are relevant to the different classes. A given class may contain multiple class profiles ,sincethere may be distributional differences in the profiles belonging to that class. For a given test entity, we determine its class label by using the relationship of the existing class labels to the current distribution of that test entity. If a given test entity is classified multiple times during stream progression, its reported class label may change, as more and more data points are received for that entity, and its underlying distri-bution changes. The changed distribution may sometimes more closely match the profile for another class. Such situa-tions are quite natural, because the test entity can be known more accurately over time, as new data points arrive.
The supervised model maintenance process continuously maintains the entity-specific profiles, and the different class profiles which represent the key entity distribution charac-teristics. At the same time, we maintain the class-specific profiles of the different fingerprints. Both these statistics need to be maintained simultaneously in the stream sce-nario. Correspondingly, we also need two input parameters that correspond to the granularity of the fingerprint mainte-nance and that of class profile maintenance. The granularity of the fingerprints is regulated by the parameter q ,which represents the number of anchor points used for fingerprint construction. An additional input to the algorithm is the pa-rameter p , which represents the total number of class profiles tracked in the data. The value of p is typically much larger than k , which is the total number of classes in the data. This ensures that the variations in the distributions of the entities belonging to a particular class can be captured by one of these class profiles. We will denote the q different anchor points by W 1 ... W q . The simultaneous maintenance of the class profiles together with the fingerprints can be sig-nificantly challenging, because the former depends upon the latter, and the latter can change significantly, as new data points arrive and the distributions of the different entities evolve over time. The concept of fingerprints [4] is formally defined as follows;
Definition 2 (Fingerprints). Let the entity contain-ing the data points Y 1 ... Y r be partitioned into the q clusters C ... C q , with anchors W 1 ... W q respectively, where each data point is assigned to its closest anchor. The correspond-ing (relative) cluster frequencies are denoted by f 1 ...f where q i =1 f i =1 . Then, the fingerprint of the set of data points Y 1 ... Y r , defined with respect to these anchors is de-noted by the q -dimensional tuple [ f 1 ...f q ] . We denote the fingerprint with respect to anchors W 1 ... W and set of data points D by F ( W 1 ... W q , D ). It is important to notice that the fingerprints provide a way to represent the distribution of the data points in a given entity. As we will see later, this turns out to be very useful from the perspective of classification.

In order to dynamically maintain the classification pro-files, a total of q anchors W 1 ... W q are used during classi-fication. These anchors are fixed throughout the algorithm execution, and are generated in an initial step with the use of a k -means approach. Let E i ( t ) be the subset of the train-ing entity D i , at the time of the arrival of the t th point in the data stream. The fingerprints for all the entities en-countered so far, are actively maintained, and are denoted by F ( W 1 ... W q , E 1 ( t )) ... F ( W 1 ... W q , E N upon the number of such entities which have been encoun-tered so far, these fingerprints can either be maintained in main memory, or they can be maintained on disk. If the training and test entities are arriving simultaneously, then the fingerprints of the test entities are maintained dynami-cally. As the test entity becomes successively more refined with the arrival of new data points, it is successively re-classified with its new representation. Presumably, the new fingerprint is a more accurate representation of the class structure and behavior. Let H i ( t ) be the subset of the test entity T i ,atthetimeofarrivalofthe t th point in the data stream. We corresponding fingerprint is denoted by F ( W 1 ... W q , H i ( t )).

These finger prints are used in order to maintain the dif-ferent class distribution profiles . Multiple class distribution profiles may be associated with a single class. This reflects the fact that many entity distributions may be relevant to a particular class. A class distribution profile is constructed from a set of fingerprints, all of which belong to the same class . Therefore, we define a class distribution profile as follows:
Definition 3 (Class Distribution Profiles). The class profiles for a set S = {L 1 ,... L s } of q -dimensional fingerprints, which belong to the same class label l is defined as the following ( q +2) -tuple containing the following components: We will next describe the overall process of fingerprint and class profile maintenance. As mentioned earlier, the finger-prints are actively maintained by the algorithm. Since the fingerprints are generated with the use of anchor points, it is important to have an efficient method for generation of the these anchors in the initial phase of the algorithm. Ide-ally, the anchor points should be dense regions of the data, around which the pattern of data points can be analyzed. Therefore, we use a sample of the data points, denoted by InitSample , and we apply a k -means clustering algorithm on these data points in order to generate these initial an-chors. The initial anchors are the centroids of the clusters determined on the initial sample of the data points. We denote these anchors by W 1 ... W q .

In the event that the t th incoming point Y t is from a train-ing entity set (and belongs to entity set D j ), we denote the subset of entity set D j to which the point belongs by U j In the event that the t th incoming point is from a test entity (and belongs to entity set T j ), we denote the subset of entity set T j to which the point belongs by V j ( t ). This data point is then assigned to its closest anchor point in the data. In order to compute the closest anchor point, we compute the euclidian distance between the data point Y t and the an-chors W 1 ... W q . Once the assignment has been performed, the fingerprints are correspondingly updated. The update process of a fingerprint needs to account for the addition of the new data point. The index of the selected anchor is de-noted by m . The fingerprints for the data subsets before and after modification are denoted by [ f 1 ...f q ]and[ f 1 ...f spectively. Then, if the t th incoming data point is a training point, the new fingerprint after the addition of a data point is denoted as follows: Algorithm Set-Classify ( Stream: Y , Profiles: p , begin
Generate q anchors W 1 ... W q on
Divide p class profiles into k different classes
Cluster class-specific segment i with parameter p i ; t =1; for each &lt; Y s ,j &gt;  X  X  do begin t = t +1; end end In the event that the t th incoming data point is a test point, the new fingerprint after addition of the test point is as follows: The denominator always increases by one point since the denominator represents the number of data points in the entity. However, the numerator increases by one for the case of the m th centroid, since only the fingerprint bucket for that bin has been updated.

At the same time, we maintain the class profiles for the different data points. Each class profile consists of a set of closely related fingerprints, all of which belong to the same class. A single class may of course contain multiple profiles, since there may be different entity distributions belonging to the same class. Only the entities which belong to the training data are used to update the class profiles. We as-sume that the p different class profiles which are consistently maintained are denoted by P 1 ... P p . Each class profile contains a number of different features which are tracked with it, according to Definition 3. The entities that be-long to the test data are not assigned to any of the profiles. Rather, the fingerprints of these entities are continuously constructed over the course of the data stream and are used in order to perform the successively more accurate classifi-cations as it gradually becomes possible to characterize the entity more and more accurately over time with the use of its fingerprint.

The average fingerprint characterization of the entities in a class profile P i can be obtained by dividing each of the bins in AG ( P i )by n ( S i ). This averaged profile can be very useful for computing the similarity between a fingerprint and an averaged class profile. For each incoming data point, we update its underlying entity fingerprint, and assign it to its closest class profile. The closeness of an entity fingerprint to the class profile is computed with the use of the cosine distance between the fingerprint and the averaged class pro-file. Because an entity may contain many points, the finger-print will be repeatedly updated, after it has already been assigned to a particular class profile. Correspondingly, the class-profile assignment may also vary with the arrival of additional data points.

In order to meaningfully assign a fingerprint to a class profile, the fingerprint needs to have a sufficient number of data points, so that it can be characterized in a statistically robust way. Therefore, we maintain a tentative set of fin-gerprints R , which correspond to those entities for which a sufficient number of data points have not been received so far. We impose the condition that any fingerprint needs to have a minimum of min stat data points in order to mean-ingfully assigned to a class profile. Otherwise, it is assigned to the tentative set R , where it  X  X aits X  for more individual data points to be included in it. Therefore, a fingerprint will always be assigned to a tentative set at the initial stage of the algorithm. The first time that a fingerprint contains more than min stat data points, it will be assigned to one of the p class profiles denoted by P 1 ... P p .

The other main issue is that each of the p different class profiles belongs to a specific label from the k different classes. In order to define the number of representatives of each class among the class profiles, we use the initial sample of Init-Sample points. Let r 1 ...r k be the fraction of entities which belong to the k different classes in this initial sample. Then, the p different class profiles are divided up among the k dif-ferent classes in proportion to r 1 ...r k . Specifically, the j -th class is assigned p  X  r j +0 . 5 different profiles. Furthermore, each class is assigned at least one profile, in the event that p  X  r j +0 . 5 evaluates to 0. Note that the profile distribu-tion among the different classes may not necessarily sum to p . In the event that the number of profiles (assigned to the different classes) is more than p ,weremoveoneprofileal-location from each of the classes with the largest number of assigned profiles in decreasing order, until the total number is p . In the event that the number of profiles is less than p , we add one profile allocation to each of the classes with the least number of assigned profiles in increasing order, until the total number is p . We assume that the number of pro-files allocated to the class i is denoted by p i . Therefore, we
As an initial step in the algorithm, we use the InitSample (training) points in the data stream in order to create an initial profile set of classes. This is achieved by applying a k -means clustering algorithm to the entities (with at least min stat points) from the initial sample of points. For each class i ,adifferent k -means clustering is applied with the use of p i centroids. The set of entities in each cluster are used to create a different class profile. Thus, a total of p profiles are available, which are distributed among the different classes. The entities which do not contain at least min stat points are assigned to the tentative set R .

Subsequently, the online profile maintenance and classi-fication processes are executed simultaneously. The train-ing and test data points may be mixed with one another. The training data points are used for profile maintenance, whereas the test data points are used for online classification. For each incoming (training) data point, we first update its entity fingerprint by adding the data point to the appro-priate anchor bin within the fingerprint. At this point, if the fingerprint does not contain at least min stat points, we allow it to stay in the tentative set R . Otherwise, we deter-mine the closest class profile with the same class among the current set of profiles, and update that class profile with the modified fingerprint. This process of modification re-quires us to remove the fingerprint from the profile to which it was assigned earlier, and assign it to a new profile. This requires us to subtract out the fingerprint from the old pro-file and add it to the new profile. Because the information stored in a profile is additive in nature, it is relatively easy to subtract out the information in one profile and add it to another. In some cases, the addition of a point to an en-tity may cause it to exceed the threshold of min stat .In such cases, the setwise entity is moved out of the tentative set and added to the closest entity. In this case, the profile only needs to be updated in terms of adding an entity to the profile, but it does not need to be subtracted from a profile.
The fingerprints for the test entities are also maintained simultaneously with the training entities. For each incoming (test) data point, the fingerprint of its test entity is updated. These test entities are maintained in a separate test profile set Q . Each time a fingerprint in Q is updated, it is assumed that the fingerprint is now presumably more  X  X ccurate X  be-cause of the addition of more data points to it. Therefore, this refined fingerprint is classified with the use of the train-ing profiles already stored. The classification process deter-mines the closest training profile to the test fingerprint with the use of the cosine metric. The label of this training profile is reported as the relevant label at that instant. This ap-proach essentially re-classifies a test entity every time new information arrives in the stream in order to characterize it more accurately. This seems like a logical way to perform the classification in a scenario in which the entire entity is never available at any given time. The basic algorithm for classification is illustrated in Figure 2.

One observation about the process is that the change to each training entity fingerprint of U ( t ) is relatively small when the absolute number of elements in these sets (de-noted by |U ( t ) | ) is large. Therefore, it is not necessary to update the class profile for a fingerprint, each time it is up-dated. Rather, we only update the fingerprint of the entity, but do not use it to update the class profile at all in most it-erations. In fact, other than updating the fingerprint of the entity, we do not make any operation (such as cosine dis-tance computations) on the class profiles at all. In order to decide the periods at which fingerprints have changed  X  X uffi-ciently X  X n order to allow updates, we maintain an additional quantity for each training entity U ( t )denotedby# U ( t any given instant t . This quantity denotes the number of updates to the entity U ( t ) since the last time it was used to update a class profile. The quantity is updated each time the fingerprint is updated, and is reset to 0 every time the fingerprint is used to update a class profile. As long as the entity U ( t ) has a significantly larger number of points than the number of updates to it, we know that the fingerprint has not changed significantly enough to worry about issues with training. When the value of # U ( t ) / |U ( t ) | is less than a pre-defined fraction (say 1%), we do not perform the up-dates. The updates are performed, only when this threshold is reached. An immediate observation is that the frequency of class profile updates will reduce with progression of the data stream, as the number of points in the fingerprint be-come larger and larger. Since a specific percentage of points in the fingerprint need to be added in order for the class profiles to be updated, it is evident that the frequency of profile updates reduces, as the absolute number of points in each fingerprint increases over the progression of the stream. In reality, the updates to the class profiles tend to be rela-tively infrequent in steady state, and continuously reduces over time. This results in an increasing efficiency of the algorithm with stream progression.
In this section, we present results on two real and one synthetic data sets. For the real data sets, the class labels were either available with the data set, or could be derived from one of the features of the data sets. For the synthetic data sets, each class was generated from a mixture model gaussian distribution.
We used two real data sets and one synthetic data set in order to test our approach. The classes in these data sets were highly mixed with one another, and could not be distinguished from one another on the basis of individual data points. These data sets are referred to as Hub64 Data Stream , E-Cover Data Streams ,and Gauss50Mix10D1000 Data Stream . We describe these data sets in detail below:
We created multi-variate density distributions in order to characterize the different entities, and used these density distributions in order to construct the training models. The density distributions are constructed as sampled grid points in the data space. The number of such grid points are ex-ponentially related to the number of dimensions. For each dimension, we had 3 possible grid points at  X  i  X   X  i ,  X   X  +  X  i ,where  X  i and  X  i are the mean and standard devia-tion of dimension i . Therefore, for a d -dimensional data set, we have 3 d possible grid points. Since this number can be very large, it is necessary to sample grid-points in order to create the density profiles in a reasonably efficient way. We used these density profiles were used as a surrogate for the fingerprints in conjunction with the approach in Figure 2 as the baseline technique. In order to create a comparable com-putational efficiency with the multi-set clustering technique, we constructed density estimates on as many grid points as the number of anchors which were used by our multi-set clustering technique. We used kernel density estimation on the incoming stream in order to create the density profiles at the grid points.
All results were tested on an Lenovo X201 Thinkpad run-ning Microsoft Windows XP, with a 2.4GHz CPU and 2.9 GB of main memory. The code was implemented with Mi-crosoft Visual C++. In all cases, unless otherwise men-tioned, the default number of anchors (or grid-points) used was 50, the default number of classification profiles was 80, and default initial number of sample points was 10,000. We first tested the classification accuracy with increasing num-ber of anchors. The results for the Hub64 data stream , E-cover data stream and the Gauss50Mix10D1000 data stream are illustrated in Figures 3(a), (b), and (c) respectively. The number of anchors (or grid-points for the baseline scheme) is illustrated on the X -axis, whereas the classification accu-racy is illustrated on the Y -axis. The number of classifica-tion profiles was set to its default value of 80. In most cases, an increase in the number of anchors increased the classifi-cation accuracy, though this was not always the case. This is because a certain amount of noise is associated with the anchor construction process, and a large number of anchors may not always imply a higher classification accuracy. In all cases, the accuracy of the Set-Classify scheme was signifi-cantly higher than the density-based baseline. For example, in the case of the Hub64 data stream, the classification accu-racy of the Set-Classify scheme was greater than 40% with the use of 75 anchors and 80 classification profiles, whereas the classification accuracy of the density-based baseline was about 25%. This difference was even larger for the other two data sets. For example, for the case of the E-Cover data set, the density-based scheme performed disastrously with a classification accuracy of only about 20% at these same parameter values. On the other hand, the Set-Classify scheme had an accuracy which was greater than 80%. The different between the two schemes was also very significant in the case of the synthetic data set.

We also tested the classification accuracy of the schemes with increasing number of classification profiles. The re-sults for the Hub64 data stream , E-cover data stream and the Gauss50Mix10D1000 data stream are illustrated in Figures 3(d), (e), and (f) respectively. The number of classification profiles are illustrated on the X -axis, whereas the classifica-tion accuracy is illustrated on the Y -axis. The number of anchors was set to the default value of 50. As in the previous case, the Set-Classify scheme was significantly more accurate than the baseline methods. The other observation was that the trend with increasing classification profiles was much less clear as compared to the trend with increasing number of anchors. This is because an increase in the number of profiles increased the representation granularity (which im-proves accuracy), but also reduced the number of points in each profile (which reduces its robustness and therefore the overall accuracy).

Finally, we also tested the classification accuracy with pro-gression of the data stream. The results for the Hub64 data stream , E-cover data stream and the Gauss50Mix10D1000 data stream are illustrated in Figures 3(g), (h), and (i) re-spectively. The progression of the data stream (in terms of the number of points) is illustrated on the X -axis, whereas the classification accuracy (on the last block of points) is illustrated on the Y -axis. The number of anchors and clas-sification profiles was set to 50 and 80 respectively. We note that the progression of the stream on the X -axis is illus-trated in terms of the number of test data points ,wh i ch are mixed randomly with the training data points. It is clear that in each case, the classification accuracy increases rapidly with stream progression, but stabilizes over time, as the distribution of classification profiles reaches steady state. As in the previous cases, the classification accuracy of the Set-Classify method is significantly greater than the density-based baseline for all the three data sets.
We also tested the efficiency of the scheme in terms of the number of data points processed per second. The pro-cessing rates for training and testing wee tabulated sepa-rately by separating out the running times for the algorithm loops which correspond to the training and testing data. The initialization times were charged to the training rates. We tested the processing rates with increasing number of anchors. The results for the Hub64 data stream , E-cover data stream and the Gauss50Mix10D1000 data stream are illustrated in Figures 4(a), (b), and (c) respectively. The number of anchors are illustrated on the X -axis, whereas the processing rate (in terms of the number of data points processed per second) are illustrated on the Y -axis. The number of classification profiles was set to its default value of 80. The processing rates for all schemes reduces with an increasing number of anchors. This is because an increase in the number of anchors increases the amount of time it re-quires to update each fingerprint. It is evident that both the training and testing rates for the Set-Classify scheme are sig-nificantly greater than that for the baseline scheme. This is because the fingerprint generation process is much more ef-ficient than the process of density estimation. Furthermore, the training rates tend to be significantly higher than the testing rates. The reason for this was that the fingerprint of a training entity only needed to be compared with the class profiles which belonged to the same class ,whereasthe fingerprint of a testing entity need to be compared with the class profiles of every class . This ensured that the training rates were always much higher than the testing rates. Fur-thermore, the absolute training and testing rates were of the order of several thousand data points per second. This en-sures that very large volumes of streams could be processed with the use of such an approach.

We also tested the processing rates with increasing num-ber of classification profiles. The results for the Hub64 data stream , E-cover data stream and the Gauss50Mix10D1000 data stream are illustrated in Figures 3(d), (e), and (f) re-spectively. The number of classification profiles are illus-trated on the X -axis, whereas the processing rates is illus-trated on the Y -axis. The number of anchors was set to the default value of 50. One observation was that while the pro-cessing rates reduces with increasing number of classification profiles, they were not quite as sensitive to this parameter, as they were to the number of anchors. The reason for this is that most of the time is spent in constructing the finger-prints, and this phase is essentially independent of the num-ber of class profiles. Therefore, the processing rates are only mildly sensitive to the number of classification profiles. As in the previous case, the processing rates of the Set-Classify scheme are significantly greater than that of the density-based classification scheme. Furthermore, these processing rates are typically of the order of several thousands data points per second. This ensures that our approach is an ef-fective and efficient method for setwise classification of data streams.
Setwise stream classification scenarios are increasingly com-mon, where one needs to classify an entire set of data records, rather than an individual data point. An additional compli-cating factor is that the stream points from different multi-set entities may be mixed with one another. This can make the classification process much more challenging. This pa-per designs a technique for online generation of classification profiles, which are used for the purposes of classification. We presented experimental results illustrating its effectiveness and efficiency. [1] C. Aggarwal, J. Han, J. Wang, and P. Yu. A [2] C. Aggarwal, J. Han, J. Wang, P. Yu. On Demand [3] C. Aggarwal. A Survey of Stream Classification [4] C. Aggarwal. The Multi-Set Stream Clustering [5] C. Aggarwal. On Segment-based Stream Modeling and [6] C. Aggarwal. Data Streams: Models and Algorithms , [7] C. Aggarwal. Data Classification: Algorithms and [8] T. Al-Khateeb, M. Masud, L. Khan, C. Aggarwal, J. [9] T.M.Cover,andP.E.Hart.NearestNeighbor [10] T. Dietterich, R. Lathrop, and T. Lozano-Perez. [11] P. Domingos, and G. Hulten. Mining High-Speed Data [12] R. Duda, P. Hart, and D. Stork. Pattern [13] W. Fan. Systematic Data Selection to Mine Concept [14] J. Gama, R. Rocha, and P. Medas. Accurate Decision [15] G. Hulten, L. Spencer, and P. Domingos. Mining Time [16] R. Jin, and G. Agrawal. Efficient Decision Tree [17] M. Masud, Q. Chen, L. Khan, C. Aggarwal, J. Gao, J. [18] M. Masud, T. Al-Khateeb, L. Khan, C. Aggarwal, J. [19] M. Masud, Q. Chen, L. Khan, C. Aggarwal, J. Gao, J. [20] X. Ning, and G. Karypis. The Set Classification [21] J. R. Quinlan. C4.5: Programs for Machine Learning, [22] S. Ruping. Incremental Learning with Support Vector [23] V. Vapnik. The Nature of Statistical Learning Theory , [24] H. Wang, W. Fan, P. Yu, J. Han. Mining [25] T. Zhang, R. Ramakrishnan, M. Livny. Fast Density
