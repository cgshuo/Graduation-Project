 Abstract The last decade has seen an explosion in the number of people learning English as a second language (ESL). In China alone, it is estimated to be over 300 million (Yang in Engl Today 22, 2006 ). Even in predominantly English-speaking countries, the proportion of non-native speakers can be very substantial. For example, the US National Center for Educational Statistics reported that nearly 10 % of the students in the US public school population speak a language other than English and have limited English proficiency (National Center for Educational Statistics (NCES) in Public school student counts, staff, and graduate counts by state: school year 2000 X 2001, 2002 ). As a result, the last few years have seen a rapid increase in the development of NLP tools to detect and correct grammatical errors so that appropriate feedback can be given to ESL writers, a large and growing segment of the world X  X  population. As a byproduct of this surge in interest, there have been many NLP research papers on the topic, a Synthesis Series book (Leacock et al. in Automated grammatical error detection for language learners. Synthesis lectures on human language technologies. Morgan Claypool, Waterloo 2010 ), a recurring workshop (Tetreault et al. in Proceedings of the NAACL workshop on innovative use of NLP for building educational applications (BEA), 2012 ), and a shared task competition (Dale et al. in Proceedings of the seventh workshop on building educational applications using NLP (BEA), pp 54 X 62, 2012 ; Dale and Kilgarriff in Proceedings of the European workshop on natural language generation (ENLG), pp 242 X 249, 2011 ). Despite this growing body of work, several issues affecting the annotation for and evaluation of ESL error detection systems have received little attention. In this paper, we describe these issues in detail and present our research on alleviating their effects.
 Keywords NLP Grammatical error detection systems Evaluation Annotation Crowdsourcing 1 Introduction While there has been considerable emphasis placed on the system development aspect of the field of grammatical error correction, there has been a woeful lack of attention paid to developing best practices for annotation and evaluation. Annotation for ESL error detection research has typically relied on just one trained annotator, and thus on using the same annotator X  X  judgments as the gold standard for evaluating systems. Consequently, it is very rare for inter-annotator reliability to be reported even though, in other areas of NLP, reporting reliability is the norm. Considerable increases in time and cost are the two most important reasons why multiple annotators are not used on the same ESL texts.

In addition to annotation, the evaluation of error detection systems is another major issue. First, most results are reported against the judgments of a single annotator even though many grammatical errors are a matter of degree rather than clear cut violations of prescriptive rules. Secondly, due to the proprietary nature of many learner corpora, most corpora cannot be shared and thus systems are almost never compared directly. 1
In this paper, we describe our research on alleviating the annotation and evaluation problems affecting ESL error detection systems. Our research is multi-pronged and makes several contributions:  X  We first confirm empirically that using only one annotator can skew system  X  We show that crowdsourced annotations (via Amazon Mechanical Turk 2 or,  X  We further show that crowdsourcing can, in fact, be leveraged to address the two  X  Finally, as a byproduct of our extensive use of the two major crowdsourcing
The paper is organized as follows. In Sect. 2 we provide background on usage errors that are problematic for non-native learners of English. In Sect. 3 we provide a review of ESL error detection systems. Section 4 shows, through a battery of human judgment experiments and system evaluations, how system performance can be skewed if only one annotator is employed. In Sects. 5 and 6 , we describe how crowdsourcing services such as Amazon Mechanical Turk and CrowdFlower can be used to address the problem of collecting multiple judgments in an efficient and inexpensive manner. In Sect. 7 we outline best practices for using crowdsourcing for error detection tasks.

Note that we restrict our evaluations to measuring the impact of our research ideas on intrinsic measures of system performance such as accuracy and inter-annotator reliability. A larger-scale evaluation that measures the impact of our ideas on the learning performance of ESL students is outside the scope of this paper and left for future work. 2 ESL usage errors Some of the most common types of ESL usage errors involve prepositions, determiners and collocations. In the work discussed here, we target preposition usage errors, specifically those of incorrect selection ( X  X  we arrived to the station  X  X  ) and extraneous use ( X  X  he went to outside  X  X ). 4 Preposition errors account for a substantial proportion of all ESL usage errors. For example, Bitchener et al. ( 2005 ) found that preposition errors accounted for 29 % of all the errors made by intermediate to advanced ESL students. In addition, such errors are relatively common. In our learner corpora, we have found that 6 % of all prepositions were incorrectly used. Some other estimates are even higher: for example, Izumi et al. ( 2003 ) reported error rates that were as high as 10 % in a Japanese learner corpus. Furthermore, extraneous prepositions account for a significant proportion of all preposition usage errors X  X s much as 18 % in essays by advanced English learners (Rozovskaya and Roth 2010a ).

At least part of the difficulty in mastering prepositions seems to be due to the great variety of linguistic functions that they serve. When a preposition marks the argument of a predicate, such as a verb, an adjective, or a noun, the selection of the preposition is constrained by the argument role that it marks, the noun which fills that role, and the particular predicate. Many English verbs also display alternations (Levin 1993 ) in which an argument is sometimes marked by a preposition and sometimes not (e.g.,  X  X  They loaded the wagon with hay  X  X  versus  X  X  They loaded hay on the wagon  X  X ). When prepositions introduce adjuncts, such as those of time or manner, selection is constrained by the object of the preposition ( X  X  at length  X  X  ,  X  X  in time  X  X  ,  X  X  with haste  X  X ). Finally, the selection of a preposition for a given context also beach  X  X  ,  X  X  we sat near the beach  X  X  ,  X  X  we sat by the beach  X  X ).

Another common error that English language learners make in their writing appear without an article or other determiner. Others can appear without an article not  X  X  a pollution  X  X ). Articles pose a particularly difficult challenge for learners whose native languages do not have them, such as Chinese, Japanese, and Russian.

Collocations are generally defined as words that co-occur within a short distance of each other or a sequence of words or terms which co-occur more often than would be expected by chance. Collocations represent preferred or conventional  X  X  hold an election  X  X  instead of  X  X  make an election  X  X ). Mastery of a language involves learning thousands of these conventional expressions.

In this paper, we use all of the above tasks to motivate the changes in annotation and evaluation practices that we propose in the subsequent sections. 3 Grammatical error detection systems While prepositions, articles, and collocations (among others) are problematic for ESL learners to master, developing systems to check for usage errors made by ESL writers using these constructions has proved to be equally difficult. Although there has been much work over the last few years in developing ESL error detection systems, there is still considerable room for improvement as many of the leading methods perform with high precision (as much as 90 %) but have correspondingly low recall (sometimes as low as 10 %).

A comprehensive overview of the field of grammatical error detection systems is outside the scope of this paper, which is focused on the annotation and evaluation needed to properly improve these systems (see Leacock et al. ( 2010 ) for a broader and more detailed description of the field). In this section, we will review some of the leading methods for error detection and detail the system we employ for preposition error detection, since it is used in the annotation and evaluation experiments we present later. 3.1 Background  X  Rule-Based Approaches : Many of the earliest grammatical error detection systems  X  Machine Learning Classifier Approaches : The next generation of error detection  X  Language Modeling Approaches : Another method based on large scale data  X  Web as Corpus : Similar to LM approaches is the use of large scale web data.  X  Machine Translation Approaches : Other research casts the error detection or 3.2 ETS preposition error detection system In Tetreault and Chodorow ( 2008 ) we developed a preposition error detection system which performs at 80 % precision and 19 % recall on an annotated corpus of essays written by non-native speakers of English for the Test of English as a Foreign Language (TOEFL). Since this system is used in several of the experiments discussed below, we briefly review its details here.

The error detection system is based on a maximum entropy model trained on 7 million examples of correct preposition usage extracted from well-formed texts, specifically San Jose Mercury News and Lexile, a collection of reading materials ranging from the primary school level to high school. For each training instance, a set of 25 features is used to describe the context around the preposition. Examples of features include the head noun following and preceding the preposition, the commanding verb, and the lexical and part-of-speech (POS) sequences to the left and right of the preposition. Because the model is trained solely on well-formed or  X  X  X ositive X  X  data, it has no knowledge of what constitutes an error ( X  X  X egative X  X  data). As a result, a series of thresholds is used to determine the model X  X  decision: the model produces a probability for the preposition that the writer used, and probabilities for all other prepositions. If the most probable preposition is not the preposition by a certain threshold amount, then the writer X  X  preposition is flagged as an error. Essentially the model measures how close the writer is to being correct.
This system has been integrated into the e-rater automatic essay scoring system as well as the Criterion SM Online Writing Evaluation Service, both developed by Educational Testing Service. 4 Need for multiple judgments In this section, we present a series of experiments that explore the reliability of human judgments in rating preposition usage. While one tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition or other usage errors in the writing of non-native speakers of English.

To date, single human annotation has typically been the gold standard for grammatical error detection (Eeg-Olofsson and Knutsson 2003 ; Gamon et al. 2008 ; Han et al. 2006 ; Izumi et al. 2004 ; Nagata et al. 2006 ). 5 Although there are several learner corpora annotated for preposition and determiner errors (such as the Cambridge Learners Corpus 6 and the Chinese Learner English Corpus 7 ), it is unclear which portions of these, if any, were doubly annotated. This previous work has side-stepped the issue of annotator reliability, which we feel is important to address since it has implications for how systems are evaluated, and thus how effective they can be as learning tools. This section makes the following contributions:  X  Judgments of Native Usage To motivate our work in non-native usage, we first  X  Judgments of Non-Native Usage As stated earlier, most computational work in
In short, through a battery of experiments described below we show how rating preposition usage, in either native or non-native texts, is a task that has surprisingly low inter-annotator reliability and thus greatly impacts system evaluation. For all of the experiments in this section, we used two research assistants employed by Educational Testing Service, both of whom are native English speakers with a college degree. Prior to participating in these experiments, they had accumulated several hundred hours of experience in providing annotations for various NLP tasks. Given their experience and training, we refer to them as  X  X  X xperts X  X  for the purpose of this paper, especially in contrast to the untrained annotators on Amazon Mechanical Turk. 4.1 Human judgments of native usage 4.1.1 Cloze task With so many sources of variation in English preposition usage, we wondered if the task of selecting a preposition for a given context might prove challenging even for native speakers. To investigate this possibility, we randomly selected 200 sentences from Microsoft X  X  Encarta Encyclopedia, and, in each sentence, we replaced a randomly selected preposition with a blank. We then asked the experts to perform a cloze task by filling in the blank with the best preposition, given the context provided by the rest of the sentence.
 In addition, we used our preposition error detection system (Tetreault and Chodorow 2008 ) to fill in each blank as well. Our results (Table 1 ) showed only about 76 % agreement between the two experts (bottom row), and between 74 and 78 % when each expert was compared individually with the original preposition used in Encarta. Surprisingly, the system performed just as well as the two experts, when compared with Encarta (third row). Although these results seem very promising, it should be noted that in many cases where the system disagreed with Encarta, its prediction was not a good fit for the context. But in the cases where the experts disagreed with Encarta, their prepositions were also licensed by the context, and thus were acceptable alternatives to the preposition that was used in the text.
Our cloze study shows that even with well-formed text, experts can disagree with each other by 25 % in the task of preposition selection. We can expect even more disagreement when the task is preposition error detection in  X  X  X oisy X  X  learner texts. DeFelice and Pulman ( 2009 ) carried out a similar experiment on 841 contexts from the British News Corpus and found that accuracy between the two experts averaged 88 %. 4.1.2 Choice test The cloze test presented above was scored by automatically comparing the system X  X  choice (or the expert X  X  choice) with the preposition that was actually written. But there are many contexts that license multiple prepositions, and in these cases, requiring an exact match is too stringent a scoring criterion.

To investigate how the exact match metric might underestimate system performance, and to further test the reliability of human judgments in native text, we conducted a choice test in which the experts were presented with 200 sentences from Encarta where the system had disagreed with the writer, and they were asked to select which of two prepositions better fit the context. One was the originally written preposition and the other was the system X  X  suggestion, displayed in random order. The experts were also given the option of marking both prepositions as equally good or equally bad. The results indicated that both Expert 1 and Expert 2 considered the system X  X  preposition equal to or better than the writer X  X  preposition in 28 % of the cases. This suggests that 28 % of the mismatched cases in the automatic evaluation are not system errors but rather are instances where the context licenses multiple prepositions. If these mismatches in the automatic evaluation are actually cases of correct system performance, then the Encarta task which performs at 75 % accuracy (third row of Table 1 ), is more realistically around 82 % accuracy (28 % of the 25 % mismatch rate is 7 %). 4.2 Human judgments of non-native usage In this section, we address the central problem of evaluating NLP error detection tools on learner data. As stated earlier, most previous work has relied on only one annotator to either create an annotated corpus of learner errors, or to check the system X  X  output. While some grammatical errors, such as number disagreement between subject and verb, no doubt show very high reliability, others, such as usage errors involving prepositions or determiners are likely to be much less reliable. In Sect. 4.2.1 , we describe our efforts in annotating a large corpus of student learner essays for preposition usage errors. Unlike previous work which required the annotator to consider nearly 40 different error types (Izumi et al. 2004 ), we focus on annotating only preposition errors in hopes that having a single type of target will insure higher reliability by reducing the cognitive demands on an annotator. Section 4.2.2 asks whether, under these conditions, one annotator is acceptable for this task. 4.2.1 Annotation scheme To create a gold-standard corpus of error annotations for system evaluation, and also to determine whether multiple annotators are better than one, we trained the experts to annotate preposition errors in ESL text. The training was very extensive: both experts were trained on 2,000 preposition contexts and the annotation manual was iteratively refined as necessary. To our knowledge, this is the first scheme that specifically targets annotating preposition errors. 8
The experts were shown sentences randomly selected from TOEFL essays, with each preposition highlighted in the sentence. They were also shown the sentence which preceded the one containing the preposition that they rated. Each expert was preposition (  X  2-word window and the commanding verb). Next they noted determiner or plural errors in the context, and then checked if there were any other grammatical errors (for example, wrong verb form). The reason for having the experts check spelling and grammar is that other modules in a grammatical error detection system would be responsible for these error types. For an example of a sentence with multiple spelling, grammatical and collocational errors, consider the following sentence:  X  X  X n consion, for some reasons, museums, particularly known travel place, get on many people. X  X  A spelling error follows the preposition In , and a collocational error surrounds on . If the contexts are not corrected, it is impossible to discern if the prepositions are correct. Of course, there is the chance that by removing these we will screen out cases where there are multiple interacting errors in the context that involve prepositions. When comparing human judgments to the performance of the preposition module, the latter should not be penalized for other kinds of errors in the context.

Finally, the expert judged the writer X  X  preposition with a rating of  X  X 0-extraneous preposition X  X ,  X  X 1-incorrect preposition X  X ,  X  X 2-correct preposition X  X , or  X  X  X -equally good prepositions X  X . If the writer used an incorrect preposition, the expert supplied the best preposition(s) given the context. Very often, when the writer X  X  preposition was correct, several other prepositions could also have occurred in the same context. In these cases, the expert was instructed to use the  X  X  X  X  X  category and list the other equally plausible alternatives. After judging the use of the preposition and, if applicable, supplying alternatives, the expert indicated her confidence in her judgment on a 2-point scale of  X  X 1-low X  X  and  X  X 2-high X  X . 4.2.2 Two annotators versus one? Following training, each expert judged approximately 18,000 occurrences of preposition use. Annotation of 500 occurrences took an average of 3 X 4 h. In order to calculate agreement and kappa values, we periodically provided identical sets of 100 preposition occurrences for both experts to judge (totaling 1,800 in all). After removing instances where there were spelling or grammar errors, and after combining categories  X  X 2 X  X  and  X  X  X  X  X , both of which were judgments of correct usage, we computed the kappa values for the remaining doubly judged sets. These ranged from 0.411 to 0.786, with an overall combined value of 0.630. 9 The confusion matrix for the combined set (totaling 1336 contexts) is shown in Table 2 . The rows represent Expert 1 X  X  (E1) judgments while the columns represent Expert 2 X  X  (E2) judgments. As one would expect given the prior reports of preposition error rates in non-native writing, the experts X  agreement for this task was quite high overall (0.952) due primarily to the large agreement count where both experts rated the usage  X  X  X K X  X  (1,213 total contexts). However there were 42 prepositions that both experts marked as a  X  X  X rong Choice X  X  and 17 as  X  X  X xtraneous. X  X  It is important to note the disagreements in judging these errors: for example, Expert 1 judged 26 prepositions to be errors that Expert 2 judged to be OK, for a disagreement rate of .302 (26/86). Similarly, Expert 2 judged 37 prepositions to be errors that Expert 1 judged to be OK, for a disagreement rate of .381 (37/97).

The kappa of 0.630 and the off-diagonal cells in the confusion matrix both show the difficulty of this task and also show how two highly trained experts can produce very different judgments. This suggests that for certain error annotation tasks, such as preposition usage, it may not be appropriate to use only one annotator and that using two or more annotators to produce an adjudicated gold-standard set is the more acceptable path.

As a second test, we used a set of 2,000 preposition contexts from ESL essays (Chodorow et al. 2007 ) that were annotated by the two experts with a scheme similar to that described above. We then compared an earlier version of our system to both experts X  judgments, and found that there was a 10 % difference in precision and a 5 % difference in recall between the two system/expert comparisons. That means that if one is using only a single expert as a gold standard, there is the potential to over-or under-estimate precision by as much as 10 %. Clearly this is problematic when evaluating a system X  X  performance. The results are shown in Table 3 . 5 Crowdsourcing for annotation Using multiple experts for error annotation makes it possible to create an adjudicated set, or at least calculate the variability of system evaluation. However, annotation with multiple experts has its own disadvantages in that it is much more expensive and time-consuming. Even using one expert to produce a sizable evaluation corpus of preposition errors is extremely costly. For example, if we assume that 500 prepositions can be annotated in 4 h using our annotation scheme, and that the error rate for prepositions in ESL writing is 10 %, then it would take at least 80 h for an expert to find and mark 1,000 errors. In this section, we propose a more efficient annotation approach, crowdsourcing , to circumvent this problem. 5.1 What is crowdsourcing? With the expansion of Internet connectivity, the popularity of crowdsourcing has increased dramatically over the last few years. Although a good number of the tasks being crowdsourced are of the mundane  X  X  X lick here, fill that X  X  variety, the use of crowdsourcing in scientific and social science disciplines has also increased significantly. Tasks with scientific underpinnings range from filling out surveys to labeling images (for improving computer vision systems) to psycholinguistic or sociological experiments (Paolacci and Warglien 2010 ). In particular, the compu-tational sciences have also seen incredible growth in the use of crowdsourcing (Bennett et al. 2009 ; Chandrasekar et al. 2010 ).

In a similar vein, the use of crowdsourcing has become extremely popular for empirical research in NLP. The most common method of leveraging crowds for NLP uses a large number of (generally untrained) annotators (or judges) to provide annotations (or judgments) regarding linguistic or speech data. These annotations can then be used either to inform analyses of the data or, more ambitiously, train machine learners to perform a classification or disambiguation task. Given that the annotators are generally untrained, the cost per annotator can be extremely low; sometimes as low as $0.01 per unit of annotation. Since annotations are being collected from a large number of annotators, the  X  X  X isdom of the crowds X  X  can, in effect, prove to be as informative as a small number of experts or trained annotators.
One of the most widely used avenues for crowdsourcing, and not just for NLP tasks, is Amazon X  X  Mechanical Turk (AMT) service. AMT provides an easy way to pay people small amounts of money to perform tasks known as Human Intelligence Tasks or HITs. Anyone with an Amazon account can either submit HITs or work on HITs submitted by others. Workers are generally referred to as Turkers and people designing the HITs are called Requesters . Requesters set the amount that they will pay for each item that is completed. Turkers are free to select whichever HITs interest them and to disregard HITs that they find uninteresting or which they think pay too little.

The first published work in the NLP community that utilized AMT on a large scale collected labeled data for several NLP tasks including word sense disambig-uation, word similarity, textual entailment, and temporal ordering of events (Snow et al. 2008 ). This study had two interesting results: first, they showed that non-expert judgments obtained from a large number of untrained annotators had a strong correlation with judgments provided by a few expert annotators. Second, and more importantly, they were able to collect 21,000 judgments for as little as $25. These two findings indicated AMT X  X  suitability for NLP research (it might take several Turkers to achieve the same reliability as a single expert annotator but it will cost significantly less).

Since the publication of that paper, the use of AMT for NLP research, either for creating labeled data for consumption by NLP systems or for evaluating the output of NLP systems, has increased significantly. It has been used in machine translation (Callison-Burch 2009 ; Zaidan and Callison-Burch 2010 ), speech recognition (Evanini et al. 2010 ; Novotney and Callison-Burch 2010 ) as well as automated paraphrase generation (Madnani 2010 ). The first ever workshop on Creating Speech and Language Data with Amazon Mechanical Turk was held in 2010 in Los Angeles as part of the Annual Conference for the North American chapter of the Association for Computational Linguistics (Callison-Burch and Dredze 2010 ). Participants presented their research on using crowdsourcing for a large variety of NLP tasks such as word sense disambiguation (Akkaya et al. 2010 ), lexicon construction for less commonly taught languages (Irvine and Klementiev 2010 ), fact mining (Wang and Callison-Burch 2010 ) and named entity recognition (Finin et al. 2010 ) among several others.
In this section, we show how crowdsourcing can be as effective as trained experts for judgments in preposition selection (Sect. 5.2 ), preposition error detection (Sect. 5.3 ) and other usage errors (Sect. 5.4 ). We also show in Sect. 5.4 that using control questions can have varying effects on the quality of the crowdsourcing. 5.2 Crowdsourcing a selection task Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer X  X  preposition given the context around the preposition. We showed in Sect. 4.1.1 that trained experts can achieve agreement of about 76 % on this task. In that experiment, an expert was shown a sentence with a target preposition replaced with a blank, and was asked to select the preposition that the writer may have used. We replicated this experiment with new experts and with untrained Turkers from AMT to answer two research questions: 1. Can untrained annotators be as effective as trained experts? 2. If so, how many untrained annotators does it take to match experts? In our replicated experiment, the expert raters we used were different from those used in our previous studies but had similar backgrounds.
 In the experiment, a Turker was presented with a sentence from Microsoft X  X  Encarta encyclopedia, with one preposition in that sentence replaced with a blank. There were 194 HITs, each containing a single sentence, and we requested 10 Turker judgments per HIT. Some Turkers did only one HIT, while others completed more than 100, though none did all 194. The Turkers X  performance was analyzed by comparing their responses to those of the two experts and to the Encarta writer X  X  preposition, which was considered the gold standard in this task. Comparing each expert to the writer yielded kappas of 0.822 and 0.778, and the experts had a kappa of 0.742. To determine how many Turker responses would be required to match or exceed these levels of reliability, we randomly selected samples of various sizes from the sets of Turker responses for each sentence. For example, when samples were of size N = 4, four responses were randomly drawn from the set of ten responses that had been collected. The preposition that occurred most frequently in the sample was used as the Turker response for that sentence. In the case of a tie, a preposition was randomly drawn from those tied for most frequent. For each sample size, 100 samples were drawn and the mean values of agreement and kappa were calculated. The reliability results presented in Fig. 1 show that, with just three Turker responses, kappa with the writer (top line) is comparable to the values obtained from the experts (around 0.8). Most notable is that with ten judgments, the reliability measures are much higher than those of the expert raters. 10 5.3 Crowdsourcing an error detection task While the previous results look quite encouraging, the task they are based on X  preposition selection in well-formed text X  X s quite different from, and less challenging than, the task that a system must perform in detecting errors in learner writing. To examine the reliability of Turker preposition error judgments, we ran another experiment in which Turkers were presented with a preposition highlighted in a sentence taken from an ESL corpus, and were instructed to judge its usage as either correct , incorrect ,or the context is too ungrammatical to make a judgment . The set consisted of 152 prepositions in total, and we requested 20 judgments per preposition. We showed in Sect. 4.2.2 that this task is a difficult one for experts to attain high reliability, e.g., the kappa between two experts averaged 0.630.
Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three experts. For pairs of the experts, kappa ranged from 0.574 to 0.650, for a mean kappa of 0.606. In Fig. 2 , kappa is shown for the comparisons of Turker responses to each expert rater for samples of various sizes ranging from N = 1to N = 18. At sample size N = 13, the average kappa is 0.608, virtually identical to the mean found among the experts. 5.4 Incorporating quality control In the previous two sections, we showed that AMT is an effective tool for annotating grammatical errors. At a fraction of the time and cost, it is possible to acquire high quality judgments from multiple untrained Turkers without sacrificing reliability. In the task of preposition selection, only three Turkers are needed to match the reliability of two experts; in the more complicated task of error detection, up to 13 Turkers are needed. However, it should be noted that these numbers can be viewed as upper bounds. The error annotation scheme that was used is a very simple one.

One of the most important caveats with crowdsourced annotations is that it is extremely important to have some form of quality control in place. Given that most HITs generally pay very little, a majority of the Turkers are motivated to earn as much money as possible and, therefore, spend as little time on each task unit as possible. To avoid the unreliable annotations that can result most NLP requesters embed control questions with pre-determined answers. The Turker must then match the annotation on these questions to get credit. On AMT, Turkers that do not pass quality control can only be rejected post-hoc which requires resubmitting the task for the remaining annotations. Another way to add quality control to tasks is to use CrowdFlower service, in contrast to AMT, automatically monitors Turkers X  responses to these control questions and rejects them while the task is running. In this section, we extend our previous crowdsourcing experiments by incorporating quality control in order to eliminate noisy judgments. The method we chose is to use the CrowdFlower service, an alternative crowdsourcing service that leverages AMT as one of its  X  X  X hannels X  X  and provides additional functionality not available directly in AMT. Note that using CrowdFlower without any quality control is analogous to using AMT. In order to determine the effect of such quality control on a wide range of tasks, we repeated the preposition error detection experiment from Sect. 5.3 on CrowdFlower (referred to below as the Confused Prepositions task) and obtain judgments for three additional tasks: Extraneous Prepositions, Determiner Errors and Collocation Errors .

For all but the extraneous preposition task, we used 150 sentences containing the target construction. We used a larger set of sentences (approx. 1,000) for the extraneous preposition task since we wanted to use the results to demonstrate that crowdsourcing can also be used to improve the evaluation of error detection systems (see Sect. 6 ). For all 4 tasks, we had three types of annotators: trained experts, untrained Turkers from Crowdflower without quality control (or  X  X  X old X  X ) questions and untrained Turkers from CrowdFlower with gold. 11 The results of these experiments, shown in Table 4 , mirror those from Sects. 5.2 and 5.3 ; majority judgments of errors from untrained annotators can equal or exceed the reliability of judgments by expert annotators. However, the table also shows that the number of Turkers needed to match the reliability of experts drops considerately in the presence of embedded quality control (gold) to eliminate untrustworthy judgments. In the table, the numbers in brackets indicate the kappas. Interestingly, the ( j = 0.48). Table 5 shows the various details for these four tasks as conducted on CrowdFlower, particularly the fast turn-around times and relatively low annotation costs.
 5.5 Qualitative analysis Next, we examined the judgments of our experts and the Turkers in order to see which items were difficult or easy for both groups, as reflected in either a lack of consensus in their judgments or near unanimity. Perhaps not surprisingly, high frequency n-grams (e.g.,  X  X  a new car  X  X ) were easier to judge than low frequency ones ( X  X  the business building  X  X ). Target constructions which violated rules of English, such as number agreement, were also easy, but almost any target that appeared in a garbled or confusing sentence was hard to evaluate ( X  X  The man ability is diver from man to man , there are tow type of succed to humen  X  X ). For cases such as these, many annotators used the option of  X  X  X oo hard to judge X  X . Even in an otherwise well formed sentence, a grammatical error in a location adjacent to or near the target string led to less agreement, especially among Turkers. For example, when asked if the highlighted word  X  X  they  X  X  contained an article error in the sentence  X  X  However, what they makes of human life is most important  X  X , the Turkers were almost evenly split in their judgments, perhaps because some of them were influenced by the number agreement error of  X  X  X hey makes X  X . The experts, on the other hand, were unanimous in saying that there was no need for an article to modify the pronoun  X  X  they  X  X . Experts were also better able to ignore irrelevant errors contained within a target string when making their decisions. Our instructions for judging collocations stated that  X  X  X t is very likely that there will be other errors in the sentence or even in the collocation itself. However, you should try to ignore those errors (or fix them in your head) and determine whether the collocation would sound natural if these errors were not present X  X . Despite such explicit directions, almost half the Turkers judged  X  X  finish an job  X  X  as unacceptable in the sentence  X  X  After you finish an job , the next one comes without loosing time  X  X ; apparently they failed to follow the instructions to ignore or fix the  X  X  an  X  X . These differences in the performance of the experts and the Turkers seem less likely to reflect differences in their knowledge than in the time and effort they spent in completing the tasks.

There are other factors specific to the different error types that also seemed to affect the level of agreement. In judging preposition selection, certain classes of preposition usage were easier than others, in particular, time/duration ( X  X  only a week before the audition  X  X ) and locatives ( X  X  Many people visit museums when they travel to new places  X  X ). In the case of articles, it was hard for Turkers and experts to evaluate whether a definite article was warranted in a sentence ( X  X  For the stressed people ; entertainment is probably the only source to ease the stress  X  X ). This was, in part, due to the design of the HIT in which the sentences were presented in isolation without information about the prior context. Without knowledge of the discourse, either form of the noun phrase was acceptable.

In summary, there were high levels of agreement for frequently occurring n-grams in  X  X  X lean X  X  sentences. Lower frequency strings and the presence of other types of errors resulted in less consensus. We also saw some evidence that experts may be more conscientious than Turkers in following instructions to ignore sources of noise in the input. Finally, individual types of usage are subject to additional factors which influence agreement, like design of the HIT and the existence of constrained usage subtypes, such as locative prepositions. 6 Crowdsourcing for evaluation Despite the rising interest in developing grammatical error detection systems for non-native speakers of English, progress in the field has been hampered by two problems. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human annotator. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than simple rule violations such as number agreement. As a consequence, it is common for two native speakers to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. For example, DeFelice and Pulman ( 2008 ), Gamon et al. ( 2008 ), Tetreault and Chodorow ( 2008 ) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. In fact, it is only very recently that systems developed by different groups in the field have been compared on a shared error detection task (Dale et al. 2012 ).

In this section, we show how to leverage crowdsourcing to both address the lack of appropriate evaluation metrics and to make system comparison easier. Our solution is general enough for, in the simplest case, intrinsically evaluating a single system on a single dataset and, more realistically, comparing two different systems (from the same or different groups).

In our experiments from the previous sections, expert and Turker judgments for each item were reduced to the single most frequent judgment category:  X  X  X orrect X  X ,  X  X  X ncorrect X  X , or  X  X  X oo difficult to judge X  X . This reduction is not an accurate reflection of a complex phenomenon. It discards valuable information about the acceptability of usage because it treats all  X  X  X ad X  X  uses as equal (and all good ones as equal), when they are not. Arguably, a better measure of this kind of linguistic judgment is one based on a continuous scale, such as the proportion of annotators who judge a usage as correct or incorrect. With multiple judgments from crowdsourcing, it should be possible to construct distributions of correctness or acceptability and use them to conduct more realistic annotation and evaluation of system performance. For example, if 90 % of crowdsourcing workers agree on a rating of  X  X  X ncorrect X  X  for a preposition, then that is stronger evidence that the usage is an error than if 51 % rated it as incorrect and 49 % considered it correct. In the future, measures of precision and recall should reflect such weighted judgments so that a system is not penalized as harshly for missing a usage error when annotator agreement is low as it is for missing an error when annotator agreement is high. We used the extraneous preposition error detection task, as outlined in Sect. 5.4 , as our case study for this set of experiments. We also used two different error detection systems to illustrate our evaluation methodology 12 :  X  LM : A 4-gram language model trained on the Google Web1T 5-gram Corpus  X  PERC : An averaged Perceptron (Freund and Schapire 1999 ) classifier X  X s
Note that expert judgments are not used at all in the following evaluation methodology; it is based entirely on crowdsourced judgments. 6.1 Crowd-informed evaluation measures When evaluating the performance of grammatical error detection systems against human judgments, the judgments for each instance are generally reduced to the single most frequent category: Error or OK . As we mentioned earlier, this type of reduction does not accurately represent a complex phenomenon and discards valuable information. A better solution again is to use a continuous scale, i.e., the proportion of annotators who judge an instance as correct or incorrect. For example, if 90 % of annotators agree on a rating of Error for an instance of preposition usage, then that is stronger evidence that the usage is an error than if 56 % of Turkers classified it as Error and 44 % classified it as OK (the sentence  X  X  In addition classmates play with some game and enjoy  X  X ). We have argued that measures of precision and recall would be fairer if they reflected the continuous nature of acceptability judgments for word usage. Besides fairness, another reason to use a continuous scale is that of stability, particularly with a small number of instances in the evaluation set (quite common in the field). By relying on majority judgments, precision and recall measures tend to be unstable (see below).

We modify the measures of precision and recall to incorporate distributions of correctness, obtained via crowdsourcing, in order to make them fairer and more stable indicators of system performance. Given an error detection system that classifies a sentence containing a specific preposition as Error (class 1) if the preposition is extraneous and OK (class 0) otherwise, we propose the following weighted versions of hits (H w ), misses (M w ) and false positives (FP w ):
In the above equations, N is the total number of instances, c sys i is the class (1 or 0), and p crowd i indicates the proportion of the crowd that classified instance i as Error . Note that if we were to revert to the majority crowd judgment as the sole judgment for each instance, instead of proportions, p crowd i would always be either 1 or 0 and the above formulae would simply compute the normal hits, misses and false positives. Given these definitions, weighted precision can be defined as Pre-cision w = H w /(H w ? FP w ) and weighted recall as Recall w = H w /(H w ? M w ). To illustrate the utility of these weighted measures, we evaluated the LM and PERC systems on a dataset containing 923 preposition instances, against 20 Turker judgments. Figure 3 shows a histogram of the Turker agreement for the majority rating over the set. Table 6 shows both the unweighted (discrete majority judgment) and weighted (continuous Turker proportion) versions of precision and recall for this system.

The numbers clearly show that in the unweighted case, the performance of the system is overestimated simply because the system is getting as much credit for each contentious case (low agreement) as for each clear one (high agreement). In the weighted measure we propose, the contentious cases are weighted lower and therefore their contribution to the overall performance is reduced. This is a fairer representation since the system should not be expected to perform as well on the less reliable instances as it does on the clear-cut instances. Essentially, if humans cannot consistently decide whether a case is an error then a system X  X  output cannot be considered entirely right or entirely wrong. 13
As an added advantage, the weighted measures are more stable. Consider a contentious instance in a small dataset where 7 out of 15 Turkers (a minority) classified it as Error . However, it might easily have happened that 8 Turkers (a majority) classified it as Error instead of 7. In that case, the change in unweighted precision would have been much larger than is warranted by such a small change in the data. However, weighted precision is guaranteed to be more stable. Note that the instability decreases as the size of the dataset increases but still remains a problem. 6.2 Enabling system comparison In this section, we show how to easily compare different systems both on the same data (in the ideal case of a shared dataset being available) and, more realistically, on different datasets. Figure 4 shows (unweighted) precision and recall of LM and PERC (computed against the majority Turker judgment) for three agreement bins , where each bin is defined as containing only the instances with Turker agreement in a specific range. We chose the bins shown since they are sufficiently large and represent a reasonable stratification of the agreement space. Note that we are not weighting the precision and recall in this case since we have already used the agreement proportions to create the bins.

Using agreement bins enables us to compare the two systems easily on different levels of item contentiousness and, therefore, conveys much more information than what is usually reported (a single number for unweighted precision/recall over the whole corpus). For example, from this graph, PERC is seen to have similar performance as LM for the 75 X 90 % agreement bin. In addition, even though LM precision is perfect (1.0) for the most contentious instances (the 50 X 75 % bin), this turns out to be an artifact of the LM classifier X  X  decision process. When it must decide between what it views as two equally likely possibilities, it defaults to OK . Therefore, even though LM has higher unweighted precision (0.957) than PERC (0.813), it is only really better on the most clear-cut cases (the 90 X 100 % bin). If one were to report unweighted precision and recall without using any bins X  X s is the norm X  X his important qualification would have been harder to discover.

While this example uses the same dataset for evaluating two systems, the procedure is general enough to allow two systems to be compared on two different datasets by simply examining the two plots. However, two potential issues arise in that case. The first is that the bin sizes will likely vary across the two plots. However, this should not be a significant problem as long as the bins are sufficiently large. A second, more serious, issue is that the error rates (the proportion of instances that are actually erroneous) in each bin may be different across the two plots. To handle this, we recommend that a kappa-agreement plot be used instead of the precision-agreement plot shown here. 7 Our experiences with crowdsourcing In this section, we provide a set of observations that were made while using the two crowdsourcing services we employed for the experiments in this paper (AMT and CrowdFlower). In addition to these observations, we also outline specific suggestions for the crowdsourcing service providers. 7.1 Observations Although we found both services to be extremely useful in providing cost-effective annotations, there are several factors that must be considered for such annotation to be of any significant use in NLP research: 1. Judgments obtained for very complex tasks are not likely to be as reliable as 2. It is extremely important to have some form of quality control in place. One 3. Although it is tempting to think of these services as potentially unlimited 4. On a related note, the instructions for any task must be clear and easy to 5. It is important to look at the reviews that Turkers provide outside of the HIT in 6. A crowdsourcing protocol should be created and adopted by the NLP 7.2 Suggestions In addition to the above desiderata for the NLP community, we have suggestions for improving crowdsourcing services in the following ways: 1. Allow participation in HITs to be contingent on other HITs. For example, when 2. Achieve feature parity between AMT and CrowdFlower. For example, it would 3. Create a channel for educating Turkers about the inner workings of 4. Add an easy method for tracking top performing and reliable Turkers such that
Finally, we think it is important to point out that the utility of crowdsourcing for grammatical error detection systems and, indeed, other NLP tasks stems not parallelize the annotations across a very large global pool of workers, which may cut down significantly on annotation time. In fact, we envision, and strongly support, the creation of a crowdsourcing service targeted specifically to NLP annotation tasks. Obviously, the remuneration to the workers employed by such a service would need to be several times what a Turker earns on AMT. However, the benefits of such a service would greatly outweigh this added cost. The annotators from such a service represent a healthy compromise between the completely untrained Turkers requiring considerable investment in quality control and the fully trained, expensive experts. 8 Conclusions In summary, we show empirically that some of the current annotation and evaluation practices employed when building ESL error detection systems are less than ideal. We also propose some new practices, based primarily on crowdsourcing, that are more robust and representative of the idiosyncrasies present in learner data.
More specifically, we showed that:  X  The standard approach to evaluating error detection systems (comparing the  X  However, one reason why a single annotator is commonly used is that building a  X  We also show how Crowdsourced judgments can be used to improve system  X  For system comparison, we argue that the best solution is to use a shared dataset
To facilitate the adoption of these improved annotation and evaluation practices, we have made all our evaluation code and data available to the community. 14 References
 Abstract The last decade has seen an explosion in the number of people learning English as a second language (ESL). In China alone, it is estimated to be over 300 million (Yang in Engl Today 22, 2006 ). Even in predominantly English-speaking countries, the proportion of non-native speakers can be very substantial. For example, the US National Center for Educational Statistics reported that nearly 10 % of the students in the US public school population speak a language other than English and have limited English proficiency (National Center for Educational Statistics (NCES) in Public school student counts, staff, and graduate counts by state: school year 2000 X 2001, 2002 ). As a result, the last few years have seen a rapid increase in the development of NLP tools to detect and correct grammatical errors so that appropriate feedback can be given to ESL writers, a large and growing segment of the world X  X  population. As a byproduct of this surge in interest, there have been many NLP research papers on the topic, a Synthesis Series book (Leacock et al. in Automated grammatical error detection for language learners. Synthesis lectures on human language technologies. Morgan Claypool, Waterloo 2010 ), a recurring workshop (Tetreault et al. in Proceedings of the NAACL workshop on innovative use of NLP for building educational applications (BEA), 2012 ), and a shared task competition (Dale et al. in Proceedings of the seventh workshop on building educational applications using NLP (BEA), pp 54 X 62, 2012 ; Dale and Kilgarriff in Proceedings of the European workshop on natural language generation (ENLG), pp 242 X 249, 2011 ). Despite this growing body of work, several issues affecting the annotation for and evaluation of ESL error detection systems have received little attention. In this paper, we describe these issues in detail and present our research on alleviating their effects.
 Keywords NLP Grammatical error detection systems Evaluation Annotation Crowdsourcing 1 Introduction While there has been considerable emphasis placed on the system development aspect of the field of grammatical error correction, there has been a woeful lack of attention paid to developing best practices for annotation and evaluation. Annotation for ESL error detection research has typically relied on just one trained annotator, and thus on using the same annotator X  X  judgments as the gold standard for evaluating systems. Consequently, it is very rare for inter-annotator reliability to be reported even though, in other areas of NLP, reporting reliability is the norm. Considerable increases in time and cost are the two most important reasons why multiple annotators are not used on the same ESL texts.

In addition to annotation, the evaluation of error detection systems is another major issue. First, most results are reported against the judgments of a single annotator even though many grammatical errors are a matter of degree rather than clear cut violations of prescriptive rules. Secondly, due to the proprietary nature of many learner corpora, most corpora cannot be shared and thus systems are almost never compared directly. 1
In this paper, we describe our research on alleviating the annotation and evaluation problems affecting ESL error detection systems. Our research is multi-pronged and makes several contributions:  X  We first confirm empirically that using only one annotator can skew system  X  We show that crowdsourced annotations (via Amazon Mechanical Turk 2 or,  X  We further show that crowdsourcing can, in fact, be leveraged to address the two  X  Finally, as a byproduct of our extensive use of the two major crowdsourcing
The paper is organized as follows. In Sect. 2 we provide background on usage errors that are problematic for non-native learners of English. In Sect. 3 we provide a review of ESL error detection systems. Section 4 shows, through a battery of human judgment experiments and system evaluations, how system performance can be skewed if only one annotator is employed. In Sects. 5 and 6 , we describe how crowdsourcing services such as Amazon Mechanical Turk and CrowdFlower can be used to address the problem of collecting multiple judgments in an efficient and inexpensive manner. In Sect. 7 we outline best practices for using crowdsourcing for error detection tasks.

Note that we restrict our evaluations to measuring the impact of our research ideas on intrinsic measures of system performance such as accuracy and inter-annotator reliability. A larger-scale evaluation that measures the impact of our ideas on the learning performance of ESL students is outside the scope of this paper and left for future work. 2 ESL usage errors Some of the most common types of ESL usage errors involve prepositions, determiners and collocations. In the work discussed here, we target preposition usage errors, specifically those of incorrect selection ( X  X  we arrived to the station  X  X  ) and extraneous use ( X  X  he went to outside  X  X ). 4 Preposition errors account for a substantial proportion of all ESL usage errors. For example, Bitchener et al. ( 2005 ) found that preposition errors accounted for 29 % of all the errors made by intermediate to advanced ESL students. In addition, such errors are relatively common. In our learner corpora, we have found that 6 % of all prepositions were incorrectly used. Some other estimates are even higher: for example, Izumi et al. ( 2003 ) reported error rates that were as high as 10 % in a Japanese learner corpus. Furthermore, extraneous prepositions account for a significant proportion of all preposition usage errors X  X s much as 18 % in essays by advanced English learners (Rozovskaya and Roth 2010a ).

At least part of the difficulty in mastering prepositions seems to be due to the great variety of linguistic functions that they serve. When a preposition marks the argument of a predicate, such as a verb, an adjective, or a noun, the selection of the preposition is constrained by the argument role that it marks, the noun which fills that role, and the particular predicate. Many English verbs also display alternations (Levin 1993 ) in which an argument is sometimes marked by a preposition and sometimes not (e.g.,  X  X  They loaded the wagon with hay  X  X  versus  X  X  They loaded hay on the wagon  X  X ). When prepositions introduce adjuncts, such as those of time or manner, selection is constrained by the object of the preposition ( X  X  at length  X  X  ,  X  X  in time  X  X  ,  X  X  with haste  X  X ). Finally, the selection of a preposition for a given context also beach  X  X  ,  X  X  we sat near the beach  X  X  ,  X  X  we sat by the beach  X  X ).

Another common error that English language learners make in their writing appear without an article or other determiner. Others can appear without an article not  X  X  a pollution  X  X ). Articles pose a particularly difficult challenge for learners whose native languages do not have them, such as Chinese, Japanese, and Russian.

Collocations are generally defined as words that co-occur within a short distance of each other or a sequence of words or terms which co-occur more often than would be expected by chance. Collocations represent preferred or conventional  X  X  hold an election  X  X  instead of  X  X  make an election  X  X ). Mastery of a language involves learning thousands of these conventional expressions.

In this paper, we use all of the above tasks to motivate the changes in annotation and evaluation practices that we propose in the subsequent sections. 3 Grammatical error detection systems While prepositions, articles, and collocations (among others) are problematic for ESL learners to master, developing systems to check for usage errors made by ESL writers using these constructions has proved to be equally difficult. Although there has been much work over the last few years in developing ESL error detection systems, there is still considerable room for improvement as many of the leading methods perform with high precision (as much as 90 %) but have correspondingly low recall (sometimes as low as 10 %).

A comprehensive overview of the field of grammatical error detection systems is outside the scope of this paper, which is focused on the annotation and evaluation needed to properly improve these systems (see Leacock et al. ( 2010 ) for a broader and more detailed description of the field). In this section, we will review some of the leading methods for error detection and detail the system we employ for preposition error detection, since it is used in the annotation and evaluation experiments we present later. 3.1 Background  X  Rule-Based Approaches : Many of the earliest grammatical error detection systems  X  Machine Learning Classifier Approaches : The next generation of error detection  X  Language Modeling Approaches : Another method based on large scale data  X  Web as Corpus : Similar to LM approaches is the use of large scale web data.  X  Machine Translation Approaches : Other research casts the error detection or 3.2 ETS preposition error detection system In Tetreault and Chodorow ( 2008 ) we developed a preposition error detection system which performs at 80 % precision and 19 % recall on an annotated corpus of essays written by non-native speakers of English for the Test of English as a Foreign Language (TOEFL). Since this system is used in several of the experiments discussed below, we briefly review its details here.

The error detection system is based on a maximum entropy model trained on 7 million examples of correct preposition usage extracted from well-formed texts, specifically San Jose Mercury News and Lexile, a collection of reading materials ranging from the primary school level to high school. For each training instance, a set of 25 features is used to describe the context around the preposition. Examples of features include the head noun following and preceding the preposition, the commanding verb, and the lexical and part-of-speech (POS) sequences to the left and right of the preposition. Because the model is trained solely on well-formed or  X  X  X ositive X  X  data, it has no knowledge of what constitutes an error ( X  X  X egative X  X  data). As a result, a series of thresholds is used to determine the model X  X  decision: the model produces a probability for the preposition that the writer used, and probabilities for all other prepositions. If the most probable preposition is not the preposition by a certain threshold amount, then the writer X  X  preposition is flagged as an error. Essentially the model measures how close the writer is to being correct.
This system has been integrated into the e-rater automatic essay scoring system as well as the Criterion SM Online Writing Evaluation Service, both developed by Educational Testing Service. 4 Need for multiple judgments In this section, we present a series of experiments that explore the reliability of human judgments in rating preposition usage. While one tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition or other usage errors in the writing of non-native speakers of English.

To date, single human annotation has typically been the gold standard for grammatical error detection (Eeg-Olofsson and Knutsson 2003 ; Gamon et al. 2008 ; Han et al. 2006 ; Izumi et al. 2004 ; Nagata et al. 2006 ). 5 Although there are several learner corpora annotated for preposition and determiner errors (such as the Cambridge Learners Corpus 6 and the Chinese Learner English Corpus 7 ), it is unclear which portions of these, if any, were doubly annotated. This previous work has side-stepped the issue of annotator reliability, which we feel is important to address since it has implications for how systems are evaluated, and thus how effective they can be as learning tools. This section makes the following contributions:  X  Judgments of Native Usage To motivate our work in non-native usage, we first  X  Judgments of Non-Native Usage As stated earlier, most computational work in
In short, through a battery of experiments described below we show how rating preposition usage, in either native or non-native texts, is a task that has surprisingly low inter-annotator reliability and thus greatly impacts system evaluation. For all of the experiments in this section, we used two research assistants employed by Educational Testing Service, both of whom are native English speakers with a college degree. Prior to participating in these experiments, they had accumulated several hundred hours of experience in providing annotations for various NLP tasks. Given their experience and training, we refer to them as  X  X  X xperts X  X  for the purpose of this paper, especially in contrast to the untrained annotators on Amazon Mechanical Turk. 4.1 Human judgments of native usage 4.1.1 Cloze task With so many sources of variation in English preposition usage, we wondered if the task of selecting a preposition for a given context might prove challenging even for native speakers. To investigate this possibility, we randomly selected 200 sentences from Microsoft X  X  Encarta Encyclopedia, and, in each sentence, we replaced a randomly selected preposition with a blank. We then asked the experts to perform a cloze task by filling in the blank with the best preposition, given the context provided by the rest of the sentence.
 In addition, we used our preposition error detection system (Tetreault and Chodorow 2008 ) to fill in each blank as well. Our results (Table 1 ) showed only about 76 % agreement between the two experts (bottom row), and between 74 and 78 % when each expert was compared individually with the original preposition used in Encarta. Surprisingly, the system performed just as well as the two experts, when compared with Encarta (third row). Although these results seem very promising, it should be noted that in many cases where the system disagreed with Encarta, its prediction was not a good fit for the context. But in the cases where the experts disagreed with Encarta, their prepositions were also licensed by the context, and thus were acceptable alternatives to the preposition that was used in the text.
Our cloze study shows that even with well-formed text, experts can disagree with each other by 25 % in the task of preposition selection. We can expect even more disagreement when the task is preposition error detection in  X  X  X oisy X  X  learner texts. DeFelice and Pulman ( 2009 ) carried out a similar experiment on 841 contexts from the British News Corpus and found that accuracy between the two experts averaged 88 %. 4.1.2 Choice test The cloze test presented above was scored by automatically comparing the system X  X  choice (or the expert X  X  choice) with the preposition that was actually written. But there are many contexts that license multiple prepositions, and in these cases, requiring an exact match is too stringent a scoring criterion.

To investigate how the exact match metric might underestimate system performance, and to further test the reliability of human judgments in native text, we conducted a choice test in which the experts were presented with 200 sentences from Encarta where the system had disagreed with the writer, and they were asked to select which of two prepositions better fit the context. One was the originally written preposition and the other was the system X  X  suggestion, displayed in random order. The experts were also given the option of marking both prepositions as equally good or equally bad. The results indicated that both Expert 1 and Expert 2 considered the system X  X  preposition equal to or better than the writer X  X  preposition in 28 % of the cases. This suggests that 28 % of the mismatched cases in the automatic evaluation are not system errors but rather are instances where the context licenses multiple prepositions. If these mismatches in the automatic evaluation are actually cases of correct system performance, then the Encarta task which performs at 75 % accuracy (third row of Table 1 ), is more realistically around 82 % accuracy (28 % of the 25 % mismatch rate is 7 %). 4.2 Human judgments of non-native usage In this section, we address the central problem of evaluating NLP error detection tools on learner data. As stated earlier, most previous work has relied on only one annotator to either create an annotated corpus of learner errors, or to check the system X  X  output. While some grammatical errors, such as number disagreement between subject and verb, no doubt show very high reliability, others, such as usage errors involving prepositions or determiners are likely to be much less reliable. In Sect. 4.2.1 , we describe our efforts in annotating a large corpus of student learner essays for preposition usage errors. Unlike previous work which required the annotator to consider nearly 40 different error types (Izumi et al. 2004 ), we focus on annotating only preposition errors in hopes that having a single type of target will insure higher reliability by reducing the cognitive demands on an annotator. Section 4.2.2 asks whether, under these conditions, one annotator is acceptable for this task. 4.2.1 Annotation scheme To create a gold-standard corpus of error annotations for system evaluation, and also to determine whether multiple annotators are better than one, we trained the experts to annotate preposition errors in ESL text. The training was very extensive: both experts were trained on 2,000 preposition contexts and the annotation manual was iteratively refined as necessary. To our knowledge, this is the first scheme that specifically targets annotating preposition errors. 8
The experts were shown sentences randomly selected from TOEFL essays, with each preposition highlighted in the sentence. They were also shown the sentence which preceded the one containing the preposition that they rated. Each expert was preposition (  X  2-word window and the commanding verb). Next they noted determiner or plural errors in the context, and then checked if there were any other grammatical errors (for example, wrong verb form). The reason for having the experts check spelling and grammar is that other modules in a grammatical error detection system would be responsible for these error types. For an example of a sentence with multiple spelling, grammatical and collocational errors, consider the following sentence:  X  X  X n consion, for some reasons, museums, particularly known travel place, get on many people. X  X  A spelling error follows the preposition In , and a collocational error surrounds on . If the contexts are not corrected, it is impossible to discern if the prepositions are correct. Of course, there is the chance that by removing these we will screen out cases where there are multiple interacting errors in the context that involve prepositions. When comparing human judgments to the performance of the preposition module, the latter should not be penalized for other kinds of errors in the context.

Finally, the expert judged the writer X  X  preposition with a rating of  X  X 0-extraneous preposition X  X ,  X  X 1-incorrect preposition X  X ,  X  X 2-correct preposition X  X , or  X  X  X -equally good prepositions X  X . If the writer used an incorrect preposition, the expert supplied the best preposition(s) given the context. Very often, when the writer X  X  preposition was correct, several other prepositions could also have occurred in the same context. In these cases, the expert was instructed to use the  X  X  X  X  X  category and list the other equally plausible alternatives. After judging the use of the preposition and, if applicable, supplying alternatives, the expert indicated her confidence in her judgment on a 2-point scale of  X  X 1-low X  X  and  X  X 2-high X  X . 4.2.2 Two annotators versus one? Following training, each expert judged approximately 18,000 occurrences of preposition use. Annotation of 500 occurrences took an average of 3 X 4 h. In order to calculate agreement and kappa values, we periodically provided identical sets of 100 preposition occurrences for both experts to judge (totaling 1,800 in all). After removing instances where there were spelling or grammar errors, and after combining categories  X  X 2 X  X  and  X  X  X  X  X , both of which were judgments of correct usage, we computed the kappa values for the remaining doubly judged sets. These ranged from 0.411 to 0.786, with an overall combined value of 0.630. 9 The confusion matrix for the combined set (totaling 1336 contexts) is shown in Table 2 . The rows represent Expert 1 X  X  (E1) judgments while the columns represent Expert 2 X  X  (E2) judgments. As one would expect given the prior reports of preposition error rates in non-native writing, the experts X  agreement for this task was quite high overall (0.952) due primarily to the large agreement count where both experts rated the usage  X  X  X K X  X  (1,213 total contexts). However there were 42 prepositions that both experts marked as a  X  X  X rong Choice X  X  and 17 as  X  X  X xtraneous. X  X  It is important to note the disagreements in judging these errors: for example, Expert 1 judged 26 prepositions to be errors that Expert 2 judged to be OK, for a disagreement rate of .302 (26/86). Similarly, Expert 2 judged 37 prepositions to be errors that Expert 1 judged to be OK, for a disagreement rate of .381 (37/97).

The kappa of 0.630 and the off-diagonal cells in the confusion matrix both show the difficulty of this task and also show how two highly trained experts can produce very different judgments. This suggests that for certain error annotation tasks, such as preposition usage, it may not be appropriate to use only one annotator and that using two or more annotators to produce an adjudicated gold-standard set is the more acceptable path.

As a second test, we used a set of 2,000 preposition contexts from ESL essays (Chodorow et al. 2007 ) that were annotated by the two experts with a scheme similar to that described above. We then compared an earlier version of our system to both experts X  judgments, and found that there was a 10 % difference in precision and a 5 % difference in recall between the two system/expert comparisons. That means that if one is using only a single expert as a gold standard, there is the potential to over-or under-estimate precision by as much as 10 %. Clearly this is problematic when evaluating a system X  X  performance. The results are shown in Table 3 . 5 Crowdsourcing for annotation Using multiple experts for error annotation makes it possible to create an adjudicated set, or at least calculate the variability of system evaluation. However, annotation with multiple experts has its own disadvantages in that it is much more expensive and time-consuming. Even using one expert to produce a sizable evaluation corpus of preposition errors is extremely costly. For example, if we assume that 500 prepositions can be annotated in 4 h using our annotation scheme, and that the error rate for prepositions in ESL writing is 10 %, then it would take at least 80 h for an expert to find and mark 1,000 errors. In this section, we propose a more efficient annotation approach, crowdsourcing , to circumvent this problem. 5.1 What is crowdsourcing? With the expansion of Internet connectivity, the popularity of crowdsourcing has increased dramatically over the last few years. Although a good number of the tasks being crowdsourced are of the mundane  X  X  X lick here, fill that X  X  variety, the use of crowdsourcing in scientific and social science disciplines has also increased significantly. Tasks with scientific underpinnings range from filling out surveys to labeling images (for improving computer vision systems) to psycholinguistic or sociological experiments (Paolacci and Warglien 2010 ). In particular, the compu-tational sciences have also seen incredible growth in the use of crowdsourcing (Bennett et al. 2009 ; Chandrasekar et al. 2010 ).

In a similar vein, the use of crowdsourcing has become extremely popular for empirical research in NLP. The most common method of leveraging crowds for NLP uses a large number of (generally untrained) annotators (or judges) to provide annotations (or judgments) regarding linguistic or speech data. These annotations can then be used either to inform analyses of the data or, more ambitiously, train machine learners to perform a classification or disambiguation task. Given that the annotators are generally untrained, the cost per annotator can be extremely low; sometimes as low as $0.01 per unit of annotation. Since annotations are being collected from a large number of annotators, the  X  X  X isdom of the crowds X  X  can, in effect, prove to be as informative as a small number of experts or trained annotators.
One of the most widely used avenues for crowdsourcing, and not just for NLP tasks, is Amazon X  X  Mechanical Turk (AMT) service. AMT provides an easy way to pay people small amounts of money to perform tasks known as Human Intelligence Tasks or HITs. Anyone with an Amazon account can either submit HITs or work on HITs submitted by others. Workers are generally referred to as Turkers and people designing the HITs are called Requesters . Requesters set the amount that they will pay for each item that is completed. Turkers are free to select whichever HITs interest them and to disregard HITs that they find uninteresting or which they think pay too little.

The first published work in the NLP community that utilized AMT on a large scale collected labeled data for several NLP tasks including word sense disambig-uation, word similarity, textual entailment, and temporal ordering of events (Snow et al. 2008 ). This study had two interesting results: first, they showed that non-expert judgments obtained from a large number of untrained annotators had a strong correlation with judgments provided by a few expert annotators. Second, and more importantly, they were able to collect 21,000 judgments for as little as $25. These two findings indicated AMT X  X  suitability for NLP research (it might take several Turkers to achieve the same reliability as a single expert annotator but it will cost significantly less).

Since the publication of that paper, the use of AMT for NLP research, either for creating labeled data for consumption by NLP systems or for evaluating the output of NLP systems, has increased significantly. It has been used in machine translation (Callison-Burch 2009 ; Zaidan and Callison-Burch 2010 ), speech recognition (Evanini et al. 2010 ; Novotney and Callison-Burch 2010 ) as well as automated paraphrase generation (Madnani 2010 ). The first ever workshop on Creating Speech and Language Data with Amazon Mechanical Turk was held in 2010 in Los Angeles as part of the Annual Conference for the North American chapter of the Association for Computational Linguistics (Callison-Burch and Dredze 2010 ). Participants presented their research on using crowdsourcing for a large variety of NLP tasks such as word sense disambiguation (Akkaya et al. 2010 ), lexicon construction for less commonly taught languages (Irvine and Klementiev 2010 ), fact mining (Wang and Callison-Burch 2010 ) and named entity recognition (Finin et al. 2010 ) among several others.
In this section, we show how crowdsourcing can be as effective as trained experts for judgments in preposition selection (Sect. 5.2 ), preposition error detection (Sect. 5.3 ) and other usage errors (Sect. 5.4 ). We also show in Sect. 5.4 that using control questions can have varying effects on the quality of the crowdsourcing. 5.2 Crowdsourcing a selection task Typically, an early step in developing a preposition or article error detection system is to test the system on well-formed text written by native speakers to see how well the system can predict, or select, the writer X  X  preposition given the context around the preposition. We showed in Sect. 4.1.1 that trained experts can achieve agreement of about 76 % on this task. In that experiment, an expert was shown a sentence with a target preposition replaced with a blank, and was asked to select the preposition that the writer may have used. We replicated this experiment with new experts and with untrained Turkers from AMT to answer two research questions: 1. Can untrained annotators be as effective as trained experts? 2. If so, how many untrained annotators does it take to match experts? In our replicated experiment, the expert raters we used were different from those used in our previous studies but had similar backgrounds.
 In the experiment, a Turker was presented with a sentence from Microsoft X  X  Encarta encyclopedia, with one preposition in that sentence replaced with a blank. There were 194 HITs, each containing a single sentence, and we requested 10 Turker judgments per HIT. Some Turkers did only one HIT, while others completed more than 100, though none did all 194. The Turkers X  performance was analyzed by comparing their responses to those of the two experts and to the Encarta writer X  X  preposition, which was considered the gold standard in this task. Comparing each expert to the writer yielded kappas of 0.822 and 0.778, and the experts had a kappa of 0.742. To determine how many Turker responses would be required to match or exceed these levels of reliability, we randomly selected samples of various sizes from the sets of Turker responses for each sentence. For example, when samples were of size N = 4, four responses were randomly drawn from the set of ten responses that had been collected. The preposition that occurred most frequently in the sample was used as the Turker response for that sentence. In the case of a tie, a preposition was randomly drawn from those tied for most frequent. For each sample size, 100 samples were drawn and the mean values of agreement and kappa were calculated. The reliability results presented in Fig. 1 show that, with just three Turker responses, kappa with the writer (top line) is comparable to the values obtained from the experts (around 0.8). Most notable is that with ten judgments, the reliability measures are much higher than those of the expert raters. 10 5.3 Crowdsourcing an error detection task While the previous results look quite encouraging, the task they are based on X  preposition selection in well-formed text X  X s quite different from, and less challenging than, the task that a system must perform in detecting errors in learner writing. To examine the reliability of Turker preposition error judgments, we ran another experiment in which Turkers were presented with a preposition highlighted in a sentence taken from an ESL corpus, and were instructed to judge its usage as either correct , incorrect ,or the context is too ungrammatical to make a judgment . The set consisted of 152 prepositions in total, and we requested 20 judgments per preposition. We showed in Sect. 4.2.2 that this task is a difficult one for experts to attain high reliability, e.g., the kappa between two experts averaged 0.630.
Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three experts. For pairs of the experts, kappa ranged from 0.574 to 0.650, for a mean kappa of 0.606. In Fig. 2 , kappa is shown for the comparisons of Turker responses to each expert rater for samples of various sizes ranging from N = 1to N = 18. At sample size N = 13, the average kappa is 0.608, virtually identical to the mean found among the experts. 5.4 Incorporating quality control In the previous two sections, we showed that AMT is an effective tool for annotating grammatical errors. At a fraction of the time and cost, it is possible to acquire high quality judgments from multiple untrained Turkers without sacrificing reliability. In the task of preposition selection, only three Turkers are needed to match the reliability of two experts; in the more complicated task of error detection, up to 13 Turkers are needed. However, it should be noted that these numbers can be viewed as upper bounds. The error annotation scheme that was used is a very simple one.

One of the most important caveats with crowdsourced annotations is that it is extremely important to have some form of quality control in place. Given that most HITs generally pay very little, a majority of the Turkers are motivated to earn as much money as possible and, therefore, spend as little time on each task unit as possible. To avoid the unreliable annotations that can result most NLP requesters embed control questions with pre-determined answers. The Turker must then match the annotation on these questions to get credit. On AMT, Turkers that do not pass quality control can only be rejected post-hoc which requires resubmitting the task for the remaining annotations. Another way to add quality control to tasks is to use CrowdFlower service, in contrast to AMT, automatically monitors Turkers X  responses to these control questions and rejects them while the task is running. In this section, we extend our previous crowdsourcing experiments by incorporating quality control in order to eliminate noisy judgments. The method we chose is to use the CrowdFlower service, an alternative crowdsourcing service that leverages AMT as one of its  X  X  X hannels X  X  and provides additional functionality not available directly in AMT. Note that using CrowdFlower without any quality control is analogous to using AMT. In order to determine the effect of such quality control on a wide range of tasks, we repeated the preposition error detection experiment from Sect. 5.3 on CrowdFlower (referred to below as the Confused Prepositions task) and obtain judgments for three additional tasks: Extraneous Prepositions, Determiner Errors and Collocation Errors .

For all but the extraneous preposition task, we used 150 sentences containing the target construction. We used a larger set of sentences (approx. 1,000) for the extraneous preposition task since we wanted to use the results to demonstrate that crowdsourcing can also be used to improve the evaluation of error detection systems (see Sect. 6 ). For all 4 tasks, we had three types of annotators: trained experts, untrained Turkers from Crowdflower without quality control (or  X  X  X old X  X ) questions and untrained Turkers from CrowdFlower with gold. 11 The results of these experiments, shown in Table 4 , mirror those from Sects. 5.2 and 5.3 ; majority judgments of errors from untrained annotators can equal or exceed the reliability of judgments by expert annotators. However, the table also shows that the number of Turkers needed to match the reliability of experts drops considerately in the presence of embedded quality control (gold) to eliminate untrustworthy judgments. In the table, the numbers in brackets indicate the kappas. Interestingly, the ( j = 0.48). Table 5 shows the various details for these four tasks as conducted on CrowdFlower, particularly the fast turn-around times and relatively low annotation costs.
 5.5 Qualitative analysis Next, we examined the judgments of our experts and the Turkers in order to see which items were difficult or easy for both groups, as reflected in either a lack of consensus in their judgments or near unanimity. Perhaps not surprisingly, high frequency n-grams (e.g.,  X  X  a new car  X  X ) were easier to judge than low frequency ones ( X  X  the business building  X  X ). Target constructions which violated rules of English, such as number agreement, were also easy, but almost any target that appeared in a garbled or confusing sentence was hard to evaluate ( X  X  The man ability is diver from man to man , there are tow type of succed to humen  X  X ). For cases such as these, many annotators used the option of  X  X  X oo hard to judge X  X . Even in an otherwise well formed sentence, a grammatical error in a location adjacent to or near the target string led to less agreement, especially among Turkers. For example, when asked if the highlighted word  X  X  they  X  X  contained an article error in the sentence  X  X  However, what they makes of human life is most important  X  X , the Turkers were almost evenly split in their judgments, perhaps because some of them were influenced by the number agreement error of  X  X  X hey makes X  X . The experts, on the other hand, were unanimous in saying that there was no need for an article to modify the pronoun  X  X  they  X  X . Experts were also better able to ignore irrelevant errors contained within a target string when making their decisions. Our instructions for judging collocations stated that  X  X  X t is very likely that there will be other errors in the sentence or even in the collocation itself. However, you should try to ignore those errors (or fix them in your head) and determine whether the collocation would sound natural if these errors were not present X  X . Despite such explicit directions, almost half the Turkers judged  X  X  finish an job  X  X  as unacceptable in the sentence  X  X  After you finish an job , the next one comes without loosing time  X  X ; apparently they failed to follow the instructions to ignore or fix the  X  X  an  X  X . These differences in the performance of the experts and the Turkers seem less likely to reflect differences in their knowledge than in the time and effort they spent in completing the tasks.

There are other factors specific to the different error types that also seemed to affect the level of agreement. In judging preposition selection, certain classes of preposition usage were easier than others, in particular, time/duration ( X  X  only a week before the audition  X  X ) and locatives ( X  X  Many people visit museums when they travel to new places  X  X ). In the case of articles, it was hard for Turkers and experts to evaluate whether a definite article was warranted in a sentence ( X  X  For the stressed people ; entertainment is probably the only source to ease the stress  X  X ). This was, in part, due to the design of the HIT in which the sentences were presented in isolation without information about the prior context. Without knowledge of the discourse, either form of the noun phrase was acceptable.

In summary, there were high levels of agreement for frequently occurring n-grams in  X  X  X lean X  X  sentences. Lower frequency strings and the presence of other types of errors resulted in less consensus. We also saw some evidence that experts may be more conscientious than Turkers in following instructions to ignore sources of noise in the input. Finally, individual types of usage are subject to additional factors which influence agreement, like design of the HIT and the existence of constrained usage subtypes, such as locative prepositions. 6 Crowdsourcing for evaluation Despite the rising interest in developing grammatical error detection systems for non-native speakers of English, progress in the field has been hampered by two problems. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human annotator. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than simple rule violations such as number agreement. As a consequence, it is common for two native speakers to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. For example, DeFelice and Pulman ( 2008 ), Gamon et al. ( 2008 ), Tetreault and Chodorow ( 2008 ) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. In fact, it is only very recently that systems developed by different groups in the field have been compared on a shared error detection task (Dale et al. 2012 ).

In this section, we show how to leverage crowdsourcing to both address the lack of appropriate evaluation metrics and to make system comparison easier. Our solution is general enough for, in the simplest case, intrinsically evaluating a single system on a single dataset and, more realistically, comparing two different systems (from the same or different groups).

In our experiments from the previous sections, expert and Turker judgments for each item were reduced to the single most frequent judgment category:  X  X  X orrect X  X ,  X  X  X ncorrect X  X , or  X  X  X oo difficult to judge X  X . This reduction is not an accurate reflection of a complex phenomenon. It discards valuable information about the acceptability of usage because it treats all  X  X  X ad X  X  uses as equal (and all good ones as equal), when they are not. Arguably, a better measure of this kind of linguistic judgment is one based on a continuous scale, such as the proportion of annotators who judge a usage as correct or incorrect. With multiple judgments from crowdsourcing, it should be possible to construct distributions of correctness or acceptability and use them to conduct more realistic annotation and evaluation of system performance. For example, if 90 % of crowdsourcing workers agree on a rating of  X  X  X ncorrect X  X  for a preposition, then that is stronger evidence that the usage is an error than if 51 % rated it as incorrect and 49 % considered it correct. In the future, measures of precision and recall should reflect such weighted judgments so that a system is not penalized as harshly for missing a usage error when annotator agreement is low as it is for missing an error when annotator agreement is high. We used the extraneous preposition error detection task, as outlined in Sect. 5.4 , as our case study for this set of experiments. We also used two different error detection systems to illustrate our evaluation methodology 12 :  X  LM : A 4-gram language model trained on the Google Web1T 5-gram Corpus  X  PERC : An averaged Perceptron (Freund and Schapire 1999 ) classifier X  X s
Note that expert judgments are not used at all in the following evaluation methodology; it is based entirely on crowdsourced judgments. 6.1 Crowd-informed evaluation measures When evaluating the performance of grammatical error detection systems against human judgments, the judgments for each instance are generally reduced to the single most frequent category: Error or OK . As we mentioned earlier, this type of reduction does not accurately represent a complex phenomenon and discards valuable information. A better solution again is to use a continuous scale, i.e., the proportion of annotators who judge an instance as correct or incorrect. For example, if 90 % of annotators agree on a rating of Error for an instance of preposition usage, then that is stronger evidence that the usage is an error than if 56 % of Turkers classified it as Error and 44 % classified it as OK (the sentence  X  X  In addition classmates play with some game and enjoy  X  X ). We have argued that measures of precision and recall would be fairer if they reflected the continuous nature of acceptability judgments for word usage. Besides fairness, another reason to use a continuous scale is that of stability, particularly with a small number of instances in the evaluation set (quite common in the field). By relying on majority judgments, precision and recall measures tend to be unstable (see below).

We modify the measures of precision and recall to incorporate distributions of correctness, obtained via crowdsourcing, in order to make them fairer and more stable indicators of system performance. Given an error detection system that classifies a sentence containing a specific preposition as Error (class 1) if the preposition is extraneous and OK (class 0) otherwise, we propose the following weighted versions of hits (H w ), misses (M w ) and false positives (FP w ):
In the above equations, N is the total number of instances, c sys i is the class (1 or 0), and p crowd i indicates the proportion of the crowd that classified instance i as Error . Note that if we were to revert to the majority crowd judgment as the sole judgment for each instance, instead of proportions, p crowd i would always be either 1 or 0 and the above formulae would simply compute the normal hits, misses and false positives. Given these definitions, weighted precision can be defined as Pre-cision w = H w /(H w ? FP w ) and weighted recall as Recall w = H w /(H w ? M w ). To illustrate the utility of these weighted measures, we evaluated the LM and PERC systems on a dataset containing 923 preposition instances, against 20 Turker judgments. Figure 3 shows a histogram of the Turker agreement for the majority rating over the set. Table 6 shows both the unweighted (discrete majority judgment) and weighted (continuous Turker proportion) versions of precision and recall for this system.

The numbers clearly show that in the unweighted case, the performance of the system is overestimated simply because the system is getting as much credit for each contentious case (low agreement) as for each clear one (high agreement). In the weighted measure we propose, the contentious cases are weighted lower and therefore their contribution to the overall performance is reduced. This is a fairer representation since the system should not be expected to perform as well on the less reliable instances as it does on the clear-cut instances. Essentially, if humans cannot consistently decide whether a case is an error then a system X  X  output cannot be considered entirely right or entirely wrong. 13
As an added advantage, the weighted measures are more stable. Consider a contentious instance in a small dataset where 7 out of 15 Turkers (a minority) classified it as Error . However, it might easily have happened that 8 Turkers (a majority) classified it as Error instead of 7. In that case, the change in unweighted precision would have been much larger than is warranted by such a small change in the data. However, weighted precision is guaranteed to be more stable. Note that the instability decreases as the size of the dataset increases but still remains a problem. 6.2 Enabling system comparison In this section, we show how to easily compare different systems both on the same data (in the ideal case of a shared dataset being available) and, more realistically, on different datasets. Figure 4 shows (unweighted) precision and recall of LM and PERC (computed against the majority Turker judgment) for three agreement bins , where each bin is defined as containing only the instances with Turker agreement in a specific range. We chose the bins shown since they are sufficiently large and represent a reasonable stratification of the agreement space. Note that we are not weighting the precision and recall in this case since we have already used the agreement proportions to create the bins.

Using agreement bins enables us to compare the two systems easily on different levels of item contentiousness and, therefore, conveys much more information than what is usually reported (a single number for unweighted precision/recall over the whole corpus). For example, from this graph, PERC is seen to have similar performance as LM for the 75 X 90 % agreement bin. In addition, even though LM precision is perfect (1.0) for the most contentious instances (the 50 X 75 % bin), this turns out to be an artifact of the LM classifier X  X  decision process. When it must decide between what it views as two equally likely possibilities, it defaults to OK . Therefore, even though LM has higher unweighted precision (0.957) than PERC (0.813), it is only really better on the most clear-cut cases (the 90 X 100 % bin). If one were to report unweighted precision and recall without using any bins X  X s is the norm X  X his important qualification would have been harder to discover.

While this example uses the same dataset for evaluating two systems, the procedure is general enough to allow two systems to be compared on two different datasets by simply examining the two plots. However, two potential issues arise in that case. The first is that the bin sizes will likely vary across the two plots. However, this should not be a significant problem as long as the bins are sufficiently large. A second, more serious, issue is that the error rates (the proportion of instances that are actually erroneous) in each bin may be different across the two plots. To handle this, we recommend that a kappa-agreement plot be used instead of the precision-agreement plot shown here. 7 Our experiences with crowdsourcing In this section, we provide a set of observations that were made while using the two crowdsourcing services we employed for the experiments in this paper (AMT and CrowdFlower). In addition to these observations, we also outline specific suggestions for the crowdsourcing service providers. 7.1 Observations Although we found both services to be extremely useful in providing cost-effective annotations, there are several factors that must be considered for such annotation to be of any significant use in NLP research: 1. Judgments obtained for very complex tasks are not likely to be as reliable as 2. It is extremely important to have some form of quality control in place. One 3. Although it is tempting to think of these services as potentially unlimited 4. On a related note, the instructions for any task must be clear and easy to 5. It is important to look at the reviews that Turkers provide outside of the HIT in 6. A crowdsourcing protocol should be created and adopted by the NLP 7.2 Suggestions In addition to the above desiderata for the NLP community, we have suggestions for improving crowdsourcing services in the following ways: 1. Allow participation in HITs to be contingent on other HITs. For example, when 2. Achieve feature parity between AMT and CrowdFlower. For example, it would 3. Create a channel for educating Turkers about the inner workings of 4. Add an easy method for tracking top performing and reliable Turkers such that
Finally, we think it is important to point out that the utility of crowdsourcing for grammatical error detection systems and, indeed, other NLP tasks stems not parallelize the annotations across a very large global pool of workers, which may cut down significantly on annotation time. In fact, we envision, and strongly support, the creation of a crowdsourcing service targeted specifically to NLP annotation tasks. Obviously, the remuneration to the workers employed by such a service would need to be several times what a Turker earns on AMT. However, the benefits of such a service would greatly outweigh this added cost. The annotators from such a service represent a healthy compromise between the completely untrained Turkers requiring considerable investment in quality control and the fully trained, expensive experts. 8 Conclusions In summary, we show empirically that some of the current annotation and evaluation practices employed when building ESL error detection systems are less than ideal. We also propose some new practices, based primarily on crowdsourcing, that are more robust and representative of the idiosyncrasies present in learner data.
More specifically, we showed that:  X  The standard approach to evaluating error detection systems (comparing the  X  However, one reason why a single annotator is commonly used is that building a  X  We also show how Crowdsourced judgments can be used to improve system  X  For system comparison, we argue that the best solution is to use a shared dataset
To facilitate the adoption of these improved annotation and evaluation practices, we have made all our evaluation code and data available to the community. 14 References
