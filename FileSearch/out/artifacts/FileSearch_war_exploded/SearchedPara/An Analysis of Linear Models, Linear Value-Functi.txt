 Broadly speaking, there are two types of reinforcement-learning (RL) algorithms: model-free and model-based al-gorithms. Model-free approaches typically use samples to learn a value function, from which a policy is implicitly de-rived. In contrast, model-based approaches build a model of system behavior from samples, and the model is used to compute a value function or policy. Both approaches have advantages and disadvantages, and function approximation can be applied to either, to represent a value function or a model. Examples of function approximators include deci-sion trees, neural networks, and linear functions. The first contribution of this paper shows that, when linear value-function approximation is used for policy evaluation as in nominally model-free approaches such as linear TD learning (Sutton, 1988) or LSTD (Bradtke &amp; Barto, 1996), the value function is precisely the same as the value func-tion that results from an exact solution to a corresponding approximate, linear model, where the value function and linear model are defined over the same set of features. This insight results in a novel view of the Bellman error and a deeper understanding of the problem of feature selec-tion when linear function approximation is used. Specifi-cally, we show that the Bellman error can be decomposed into two types of errors in the learned linear model: the re-ward error and the feature error. This decomposition gives insight into the behavior of existing approximation tech-niques, suggests new views of feature selection, and ex-plains the behavior of existing feature-selection algorithms. We are interested in both controlled and uncontrolled Markov processes with a set S of states s and, when ap-plicable, a set A of actions a . Our main results and experi-ments consider the uncontrolled or policy-evaluation case, but many of the ideas can be applied to the controlled case, as is discussed in more detail in Section 3.2.
 We refer to the uncontrolled case as a Markov reward pro-cess (MRP): M = ( S, P, R,  X  ) , and the controlled case as a Markov decision process (MDP): M = ( S, A, P, R,  X  ) . Given a state s i , the probability of a transition to a state s given action a is given by P a ij and results in an expected reward of R a i . In the uncontrolled case, we use P ij and R to stand for the transitions and rewards.
 We are concerned with finding value functions V that map each state s i to the expected total  X  -discounted reward for the process. In particular, we would like to find the solution to the Bellman equation in the controlled case (the  X  X ax X  and a  X  X  are eliminated from the equation in the uncontrolled case).
 For any matrix, A , we use A T to indicate the transpose of A and s pan ( A ) to indicate the column space of A . 2.1. Linear Value Functions In cases where the value function cannot be represented ex-actly, it is common to use some form of parametric value-function approximation, such as a linear combination of features or basis functions: where  X  = {  X  1 , . . . ,  X  k } is a set of linearly independent 1 basis functions of the state, with  X  i ( s ) defined as the value of feature i in state s . The vector w = { w 1 , . . . , w set of scalar weights. We can think of  X  as a design matrix with  X [ i, j ] =  X  j ( s i ) , that is, the basis functions span the columns of  X  and the states span the rows. Expressing the weights w as a column vector, we have  X  V =  X  w .
 Methods for finding a reasonable w given  X  and a set of samples include linear TD (Sutton, 1988), LSTD (Bradtke &amp; Barto, 1996) and LSPE (Yu &amp; Bertsekas, 2006). If the model can be expressed as a factored MDP, then the weights can be found directly (Koller &amp; Parr, 1999). We refer to this family of methods as linear fixed-point meth-ods because they all solve for the fixed point where  X   X  is an operator that is the  X  -weighted L 2 jection into s pan ( X ) , where  X  is a state weighting dis-tribution, typically the stationary distribution of P . If an unweighted projection (uniform  X  ) or some other  X  is used. Most of our results do not depend upon the projec-tion weights, so we shall assume uniform  X  unless other-wise stated. Solving for w  X  yields: In this paper, we assume that P is known and that  X  can be constructed exactly, while in all but the factored model case, these would be sampled. This assumption allows us to characterize the representational power of the features as a separate issue from the variance introduced by sampling. 2.2. Linear Models As in the case of linear value functions, we assume the existence of a set of linearly independent features  X  1 . . .  X  k for representing transition and reward models, with  X  i ( s ) defined as the value of feature i in state s . While value-function approximation typically uses features to predict values, we will consider the use of these fea-tures to predict next features. For feature vector  X ( s ) = [  X  of next features: our objective will be to produce a k  X  k matrix P  X  that predicts expected next feature vectors, and minimizes the expected feature-prediction error: (For brevity, we shall henceforth leave s 0  X  P ( s 0 | s ) im-plicit). One way to solve the minimization problem in Eq. (4) is to compute the expected next feature explicitly as the n  X  k matrix P  X  and then find the least-squares so-lution to the over-constrained system  X  P  X   X  P  X  , since the i th row of  X  P  X  is P  X   X  X  prediction of the next feature values for state i and the i th row of P  X  is the expected value of these features. The least-squares solution is with approximate next feature values d P  X  =  X  P  X  . To pre-dict the reward model using the same features, we could perform a standard least-squares projection into s pan ( X ) to compute an approximate reward predictor: with corresponding approximate reward:  X  R =  X  r  X  . As in the value-function approximation case, it is possible to do a weighted L 2 projection, a straightforward generalization that we omit for conciseness of presentation.
 Classically, an advantage of learning a model and deriv-ing values from the model (indirect learning) over using samples to estimate the values (direct learning) is that such a method can be very data efficient. On the other hand, learning an accurate model can require a great deal of ex-perience. Surprisingly, we find that the two approaches are the same, at least in the linear approximation setting. The notion that linear fixed-point methods are implicitly computing some sort of model has been recognized in vary-ing degrees for several years. For example, Boyan (1999) considered the intermediate calculations performed by LSTD in some special cases, and interpreted parts of the LSTD algorithm as computing a compressed model. In this section, we show that the linear fixed-point solution for features  X  is exactly the solution to the linear model de-scribed by P  X  and r  X  . We first prove it for the uncontrolled case, and then generalize our result to the controlled case. Our results concern unweighted projections, but generalize readily to weighted projections. 3.1. The Uncontrolled Case Recall that the approximate model transforms feature vec-tors to feature vectors, so any k -vector is a state in the ap-proximate model. If x is such a state, then, in the approxi-mate model, r  X  T x is the reward for this state and P  X  T the next state vector. The Bellman equation for state x is: Expressed with respect to the original state space, the value function becomes which is a linear combination of the columns of  X  . Since V =  X  w for some w , the fixed-point equation becomes: We call the solution to the system above the linear model solution . A solution will exist when P  X  has a spectral ra-dius less than 1 / X  . This condition is not guaranteed be-cause P  X  is not necessarily a stochastic matrix; it is simply a matrix that predicts expected next feature values. The cases where the spectral radius of P  X  exceeds 1 / X  corre-spond to the cases where the value function defined by P  X  and r  X  assigns unbounded value to some states.
 Theorem 3.1 For any MRP M and set of features  X  , the linear-model solution and the linear fixed-point solution are identical.
 Proof We begin with the expression for the linear-model solution from Eq. (9) and then proceed by substituting the definitions of P  X  and r  X  from Eq. (5) and Eq. (6), yielding: This result demonstrates that for a given set of features  X  , there is no difference between using the exact model to find an approximate linear fixed-point value function in terms of  X  and first constructing an approximate linear model in terms of  X  and then solving for the exact value function of the approximate model using Eq. (9). Although the model-based view produces exactly the same value function as the value-function-based view, the model-based view can give a new perspective on error analysis and feature selection, as shown in later sections. 3.2. The Controlled Case: LSPI For the controlled case, we denote a policy as  X  : S 7 X  A . Since rewards and transitions are action dependent, the value function is defined over state X  X ction pairs and is called a Q-function: For a fixed policy  X  , Q  X  is the unique fixed-point solution to the Bellman equation
As in Section 2.1, Q  X  can be approximated by functions in s pan ( X ) :  X  Q =  X  are defined over state X  X ction pairs rather than states. In the controlled case, the policy  X  can be refined itera-tively, as in the Least-Squares Policy Iteration (LSPI) al-gorithm (Lagoudakis &amp; Parr, 2003). Starting with an arbi-trary policy  X  1 , LSPI performs two steps iteratively until certain termination conditions are satisfied. In iteration i , it first computes an approximate linear value function  X  Q i the current policy  X  i (the policy-evaluation step), and then computes a new policy  X  i +1 that is greedy with respect to  X  Q i (the policy-improvement step).
 In the policy-evaluation step, an algorithm LSTDQ, which is the Q-version of the LSTD algorithm, is used to com-pute  X  Q i . Since a Markov decision process controlled by a fixed policy is equivalent to an induced Markov reward pro-cess whose state space is S  X  A , LSTDQ can be viewed as LSTD running over this induced MRP. Due to Theorem 3.1, LSTDQ effectively builds a least-squares linear model ap-proximation and then finds the exact solution to this model. Therefore, the intermediate value functions  X  Q i found by LSPI are the exact value functions of the respective approx-imate linear models with the smallest (weighted) L 2 error. Value-function-based methods often analyze the error of a value function  X  V in terms of the one-step lookahead error, or Bellman error : In the context of linear value functions and linear models, we shall define the Bellman error for a set  X  of features as the error in the linear fixed-point value function for  X  : To understand the relationship between the error in the lin-ear model and the Bellman error, we define two compo-nents of the model error, the reward error : and the per-feature error : The per-feature error is the error in the prediction of the ex-pected next feature values, so both terms can be thought of as the residual error of the linear model. The next theorem relates the Bellman error to these model errors.
 Theorem 4.1 For any MRP M and features  X  , Proof Using the definitions of BE ( X ) ,  X  R , and  X   X  :
The final step follows from the definition of  X  R , and the penultimate step follows from Eq. (9) and Theorem 3.1. This decomposition of the Bellman error lets us think of the Bellman error as composed of two separate sources of er-ror: reward error, and per-feature error. In the next section, we show that this view can give insight into the problem of feature selection, but we also caution that there can be interactions between  X  R and  X   X  . For example, consider the basis composed of the single basis function  X   X  = [ V  X  ] . Clearly, BE ( X   X  ) = 0 , but for any non-trivial problem and approximate model,  X  R and  X   X  will be nonzero and will cancel each other out in Eq. (10).
 A similar result may be possible for the controlled case, but there are some subtleties. For example, there is not a clean notion of a fixed point for the outer loop of the LSPI algorithm since the algorithm is not guaranteed to converge to a single policy or w . We present several insights on the problem of feature se-lection that follow from the results presented above. 5.1. General Observations about  X  R and  X   X  The condition  X  R =  X   X  = 0 is sufficient (but not nec-essary) to achieve zero Bellman error and a perfect value function. Specifically, it requires that the features of the ap-proximate model capture the structure of the reward func-tion, and that the features of the approximate model are suf-ficient to predict expected next features. In the case where  X  is a set of indicator functions over disjoint partitions of S , these conditions are similar to those specified for model minimization (Dean &amp; Givan, 1997) in MDPs.
 Features that are insufficient to represent the immediate re-ward are likely to be problematic since any error in the pre-diction of the immediate reward based upon the features (  X 
R ) can appear directly in the Bellman error through the first summand of Eq. (10). This finding is consistent with the observation of Petrik (2007) of the problems that arise when the reward is orthogonal to the features.
 For  X   X  = 0 , the Bellman error is determined entirely by  X 
R , with no dependence on  X  . This observation has some interesting implications for feature selection and the anal-ysis of the resulting approximate value function, topics we address further in Section 5.3. 5.2. Incremental Feature Generation This section presents two existing methods for incremen-tally building a basis, the Krylov basis , and Bellman Error Basis Functions (BEBFs). We also propose a new method based upon the model error, Model Error Basis Functions (MEBFs), then show that all three methods are equivalent given the same initial conditions. The Krylov basis is defined in terms of powers of the tran-sition matrix multiplied by R. We refer to the Krylov basis with k basis functions, starting from X , as Krylov k ( X ) , with Krylov k ( X ) = { P i  X  1 X : 1  X  i  X  k } . For an MRP, typically X = R . The Krylov basis, and Krylov methods in general, are standard techniques for the iterative solu-tion to systems of linear equations. Its relevance to feature selection for RL was demonstrated by Petrik (2007). 5.2.2. BEBF S Many researchers have proposed using features based upon the residual error in the current feature set (Wu &amp; Givan, 2004; Sanner &amp; Boutilier, 2005; Keller et al., 2006). Parr et al. (2007) describe this family of techniques as Bellman Error Basis Functions (BEBFs), and analyze some of the properties of this approach. More formally, if  X  k w  X  the current value function, BEBF adds  X  k +1 = BE ( X  k ) as the next basis function. We refer to the basis resulting from k  X  1 iterations of BEBF, starting from X , as BEBF k ( X ) . Theorem 5.1 (Petrik 2 ) For any k  X  1 , Proof The proof is by induction on k . For the basis: For the inductive step, we assume equality up to k , so for both methods the value function can be expressed as: Now, observe that:
BE ( X  k ) = R +  X P ( The only part of the above that is not already in the basis added in Krylov k +1 ( R ) . 5.2.3. MEBF S A natural generalization of BEBFs to the model-based view would be to add features that capture the residual error in the model. Starting from  X  k , this technique (MEBF) adds  X  R and  X   X  (or the linearly independent components thereof) to the basis at each iteration to create  X  k +1 . In contrast to BEBFs, this method can add a large number of basis functions at each iteration since  X   X  has as many columns as  X  . One might imagine that this process could result in an exponential growth in the number of basis func-tions. In fact, however, the number of new basis functions added at each iteration will not grow since each new set of basis functions that is added will drive the error in the previous basis functions to 0 .
 We refer to the basis resulting from k  X  1 iterations of MEBF, starting from X , as MEBF k ( X ) . For an initial ba-sis of  X  , the MEBF basis expansion is guaranteed to con-tain the BEBF basis expansion.
 Theorem 5.2 s pan ( BEBF 2 ( X ))  X  s pan ( MEBF 2 ( X )) . Proof Follows immediately from Eq. (10).
 Theorem 5.3 For k  X  1 : Proof The proof is by induction on k . For the basis: For the inductive step, we assume equality up to k and con-sider the behavior of MEBF. For k  X  1 ,  X  R = 0 , since R is the first basis function added. The basis  X  k is equivalent to a collection of basis functions of the form  X  i = P i  X  1 R for 1  X  i  X  k . As a result, P X  i is already in the basis for all 1  X  i &lt; k . Thus, the only nonzero column of  X   X  will correspond to feature  X  k and will be P k R  X  P  X  P k  X  1 R . Since P  X  P k  X  1 R is necessarily in s pan ( X  k ) , the only new contribution to the basis made by MEBF will be from P k R , which is precisely what is added by Krylov k +1 ( R ) . These results show that, starting from R , all three methods will produce the same basis. An advantage of BEBF is that it will produce orthogonal basis vectors. An advantage of MEBF is that it can add multiple new basis vectors at each iteration if it is initialized with a set of basis functions. 5.3. Invariant Subspaces of P The form of the Bellman error in Eq. (10) suggests that features for which  X   X  = 0 are particularly interesting. If a dictionary of such features were readily available, then the feature-selection problem would reduce to the problem of predicting the immediate reward using this dictionary. The condition  X   X  = 0 means that, collectively, the features are a basis for a perfect linear predictor of their own next, expected values. More formally, features  X  are subspace invariant with respect to P if P  X  is in s pan ( X ) , which means that there exists a  X  such that P  X  =  X  X  .
 At first, it may seem like subspace invariance is an extraor-dinary requirement that could hold only for a complete ba-sis for P . It turns out, however, that there are many ways to describe invariant subspaces of P . Any set of eigenvec-tors of P forms an invariant subspace. For eigenvectors X 1 . . . X k with eigenvalues  X  1 . . .  X  k ,  X  = diag (  X  The set of generalized eigenvectors corresponding to a par-ticular eigenvalue  X  of P is subspace invariant with re-spect to P . For an eigenvalue  X  with multiplicity i , there will be i generalized eigenvectors, X 1 . . . X i satisfying ( P  X   X I ) X j = X j  X  1 for 1  X  j  X  i and ( P  X   X I ) j X for 0  X  j  X  i . More generally, if  X  1 and  X  2 are subspace invariant with respect to P , then so is their union. Finally, the Schur decomposition of a matrix P provides a set of nested invariant subspaces of P .
 In fairness, we point out that these methods all require knowledge of P and superlinear computation time in the dimension of P . We defer discussion of the practicality of implementing these methods to Section 6 and Section 7. Theorem 5.4 For any MRP M and subspace invariant feature set  X  ,  X   X  = 0 .
 Proof First, we observe that P  X  has a particularly simple form as a consequence of subspace invariance: Substituting into the definition of  X   X  :  X 
 X  = P  X   X  Subspace invariant features have additional intriguing properties. The resulting value function always exists and can be interpreted as the result of using the true transition model with the approximate reward function  X  R .
 Theorem 5.5 For any MRP M and subspace invariant feature set  X  , w  X  always exists and Proof Starting with the form of w  X  from Eq. (7) and the fact that  X   X  = 0 : To confirm that such a w  X  actually exists, we note that  X  s pan ( X ) by construction, and that ( I  X   X P )  X  1 must exist for the actual P and 0  X   X  &lt; 1 , allowing us to rewrite: which remains in s pan ( X ) because of  X   X  X  subspace invari-ance with respect to P .
 Our analysis has some similarities with that of Petrik (2007). Petrik considered the eigenvalue de-composition of P as a basis and considered the error in the projection of V  X  into this basis. Petrik also suggested the use of the Jordan form, which would provide generalized eigenvectors for matrices that are not diagonalizable. Our analysis focuses on the Bellman error of the linear fixed-point solution. Insights from the model-based view of linear approximation architectures allow us to decompose the error into distinct components corresponding to the reward and transition models, making the role of invariant subspaces particularly salient. We present policy-evaluation results on three different problems. Our objective is to demonstrate how our theoret-ical results can inform the feature-selection process and ex-plain the behavior of known feature-selection algorithms. We consider 4 algorithms: PVF: This is the proto-value function (PVF) framework described by Mahadevan and Maggioni (2007). PVFs use eigenvalues of the Laplacian derived from an empirically constructed adjacency matrix (from random walk trajecto-ries), enumerated in increasing order of eigenvalue. We reproduced their method as closely as possible, includ-ing adding links to the adjacency matrix for all policies, not just the policy under evaluation. Curiously, remov-ing the off-policy links seemed to produce worse perfor-mance. We avoided using samples to eliminate the con-founding (for our purposes) issue of variance between ex-periments. We used the combinatorial Laplacian for the 50-state and blackjack problems, but used the normalized Laplacian in the two-room problem to match Mahadevan and Maggioni (2007).
 PVF-MP: This algorithm selects basis functions from the set of PVFs, but selects them incrementally based upon the Bellman error. Specifically, basis function k +1 is the PVF that has highest dot product with the Bellman error result-ing from the previous k basis functions. It can be inter-preted as a form of matching pursuits (Mallat &amp; Zhang, 1993) on the Bellman error with a dictionary of PVFs. Eig-MP: This algorithm is similar to PVF-MP, but selects from a dictionary of the eigenvectors of P . Both Eig-MP and PVF-MP are similar in spirit to Petrik X  X  WL algorithm. BEBF: This is the BEBF algorithm starting with  X  0 = R , as described in Section 5.2.
 Our experiments performed unweighted L 2 projection and report unweighted L 2 norm error. We also considered L  X  error and L 2 projections weighted by stationary distribu-tions, but the results were not qualitatively different. We report the Bellman error, the reward error, and the feature error , which is the contribution of the per-feature errors to the Bellman error:  X   X   X  w  X  . These metrics are presented as a function of the number of basis functions. 6.1. 50-state Chain We applied all 4 algorithms to the 50-state chain problem from Lagoudakis and Parr (2003), with the results shown in Figure 1(a X  X ). As demanded by theory, Eig-MP has 0 fea-ture error, which means that the entirety of the Bellman er-ror is expressed in  X  R . BEBFs represent the other extreme since  X  R = 0 after the first basis function is added and the entirety of the Bellman error is expressed through  X   X  . For this problem, PVFs appear to be approximately subspace invariant, resulting in low  X   X  . However, both Eig-MP and the PVF methods do poorly because the reward is not eas-ily expressed as linear combination of a small number of PVFs. PVF-MP does better than plain PVFs because it is actively trying to reduce the error, while plain PVFs choose basis functions in an order that ignores the reward. 6.2. Two-room Problem We tried all four algorithms on an optimal policy for the two-room navigation problem from Mahadevan and Mag-gioni (2007). The transition matrix for this problem is not diagonalizable and typical methods for extracting gener-alized eigenvectors proved unreliable, so we do not show results for the Eig-MP method. Figure 1(d X  X ) shows the breakdown of error for the remaining algorithms. In this case, the Laplacian approach produces features that behave less like an invariant subspace, resulting in high  X  R and  X 
 X  . However, there is some cancellation between them. 6.3. Blackjack We tested a version of the bottomless-deck blackjack prob-lem from Sutton and Barto (1998), evaluating the policy they propose. For the model described in the book, all methods except BEBF performed extremely poorly. To make the problem more amenable to eigenvector-based methods, we implemented an ergodic version that resets to an initial distribution over hands with a value of 12 or larger and used a discount of 0 . 999 . The breakdown of error for the different algorithms is shown in Figure 1(g X  i), where we again omit Eig-MP. As expected, BEBFs ex-hibit  X  R = 0 , and drive the Bellman error down fairly rapidly. PVFs exhibit some interesting behaviors: When the PVFs are enumerated in order of increasing eigenvalue, they form an invariant subspace. As a result, the feature error for PVFs hugs the abscissa in Figure 1(i). However, this ordering completely fails to match R until the very last eigenvectors are added, resulting in very poor performance overall. In contrast, PVF-MP adds basis eigenvectors in an order that does not result in subspace invariant features sets, but that does match R earlier, resulting in a more consistent reduction of error. A significant finding in our work is the close relationship between value-function approximation and model-based learning. Our experimental results illustrate the relation-ship between the power of the features to represent an ap-proximate model and the Bellman error.
 While features that represent feature transitions accurately have highly desirable properties, both components of the model, the reward function and the transition function, should be respected by the features. Both a strength and weakness of the BEBF/MEBF/Krylov methods is their connection to specific policies and reward structures. Our results are consonant with those of Petrik (2007), which showed good performance for the Krylov basis and some surprisingly weak performance for eigenvector-based methods despite their appealing properties.
 To focus on the expressive power of the features, our re-sults in this paper do not directly consider sampled data, the regime in which linear fixed-point methods are most often employed. Some initial results on the effects of noise in feature generation for BEBF/MEBF/Krylov methods can be found in Parr et al. (2007), however further analysis would still be helpful. For eigenvector-based methods, there are some questions about the cost of estimating eigen-vectors of P , or an approximation to P via the Laplacian. Computing eigenvectors can be computationally intensive and, for general P , prone to numerical instabilities. An important direction for future work is seeking a deeper understanding of the interaction between feature-selection and policy-improvement algorithms such as LSPI. This paper demonstrated a fundamental equivalence be-tween linear value-function approximation and linear model approximation for RL. This equivalence led to a novel view of the Bellman error, which then gave insight into the problem of feature selection. These insights were used to explain the behavior of existing feature-selection algorithms on some sample problems. While this research has not, yet, led to a novel algorithmic approach, we believe that it helps address fundamental questions of representa-tion and feature selection encountered by anyone wishing to solve real RL problems.

