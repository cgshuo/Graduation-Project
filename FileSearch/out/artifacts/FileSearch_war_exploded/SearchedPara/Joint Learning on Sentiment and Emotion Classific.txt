 Sentiment and emotion classifi cation have been popularly but separately studied in natural langua ge processing. In this paper, we address joint learning on sentiment and emotion classification where both the labeled data for sentiment and emotion classification are available. The objective of this joint-learning is to benefit the two tasks from each other for improving their performances. Specifically, an extra data set that is annotated with both sentiment and emotion labels are employed to estimate the transformation probability between the two kinds of labels. Furthermore, the transformation probability is leveraged to transfer the classification labels to benefit the two tasks from each other. Empirical studies demons trate the effectiveness of our approach for the novel joint learning task. I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  Text analysis Algorithms, Experimentation Joint Learning, Sentiment Classifi cation, Emotion Classification. Sentiment and emotion classifi cation are two popular tasks in Natural Language Processing (Pang et al., 2002; Das and Bandyopadhyay, 2009). Specifically, sentiment classification aims to predict the sentimental orientation (e.g., positive or negative ) of a text while emotion classifica tion aims to recognize basic emotions in the text, such as expect , joy , love , surprise , and hate . Both of them have attracted considerable interests due to their wide applications in natural language processing (Liu, 2012; Wilson, et al., 2009; Chen et al., 2010). Although sentiment cla ssification and emotio n classification are two different tasks and separately studied in the literature, these two tasks are highly related to each other due to the strong relationship between the sentime nt and emotion labels. For example, a text with positive sentiment is more likely to be accompanied by a positive emotion, such as joy, love, and expect. Consider the following sentence, denoted as E1, from the Ren-CECps corpus (Quan and Ren, 2009). The emotion of this sentence is joy and the sentiment label is positive. E1:  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  ( It's fine now. I have finally realized my goal .) Therefore, it is both interesting and practical to jointly learn the two tasks, i.e., sentiment and emo tion classification, given the fact that each task often has a limited amount of training data. Here, joint learning means to make full use of all training data from both tasks to help each other to impr ove the classification performance. Intuitively, it would appear that we could simply map the emotion labels to sentiment ones. For example, if one sample is predicted as love, it would be a positive sample. However, the emotion and sentiment labels are not always mapping. Considering another sentence from the Ren-CECps corpus: E2:  X  X  X  X  X  X  X  X  X  X  ,  X  X  X  X  X  X  X  X  X  X  ,  X  X  X  X  X  X  X  X  X  X  X  ( I could not help but worry in a silly way, whether you would be happy to be with her? ) The emotion of this sentence is love while its sentimental label is negative. Moreover, if one sample is predicted as positive, it is hard to directly map it to either joy or love. Therefore, directly mapping the emotion and sentimen t labels is incapable of handling this problem. In this paper, we propose a join t learning approach for sentiment and emotion classification. The basic idea of the approach is to annotate a small size of data wher e the samples are annotated with both sentiment and emotion labels. This data set, which we refer to as sharing data, is then employed to estimate the strength of statistical relationship, i.e., transformation probabilities, between the sentiment and emotion labels. Specifically, in the training phase, the two individual labeled da ta sets are used to train two separate classifiers. In the testin g phase, one sample is classified by both classifiers and obtains the probabilities belonging to each sentiment or emotion label. Then the statistical relationship is leveraged to transfer the sentiment (or emotion) labels to the emotion (or sentiment) labels. The final label of the sample is determined by combining the original sentiment (or emotion) label and the transferred one. The remainder of this paper is organized as follows. Section 2 overviews the related work on both sentiment and emotion classification. Section 3 proposes our approach to jointly learning sentiment and emotion classification. Section 4 presents the experimental results. Finally, Se ction 5 gives the conclusion and future work. Sentiment classification has become a hot research topic in NLP community and various kinds of cl assification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pa ng et al., 2002), semi-supervised learning methods (Li et al., 2010 ), and cross-domain classification methods (He et al., 2011). Generall y, sentiment classification can be performed on four different levels: word level (Wiebe, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and docum ent level (Turney, 2002; Pang et al., 2002; Riloff et al., 2006). This paper focuses on sentence-level sentiment classification with supervised learning approaches. Recently, emotion classification has become more and more popular in NLP (Xu et al., 2010; Purver and Battersby, 2012). One main group of related studies on this task is about emotion resource construction, such as emo tion lexicon buildi ng (Xu et al., 2010) and sentence-level or docume nt-level corpus construction (Quan and Ren, 2009). Another main group of related studies is about the supervised learning approaches on emotion classification (Alm et al., 2005; Chen et al., 2010; Purver and Battersby, 2012). Although both sentiment and emo tion classification have been widely studied, almost all previ ous studies consider these two tasks separately. Unlike all of them, our work focuses on a new task of how to benefit sentiment and emotion classifications from each other. To the best of our knowledge, this is the first attempt to jointly learn the two tasks. In a standard supervised classification problem, we seek a predictor f that maps an input vector x to the corresponding class label y . The predictor is trained on a finite set of labeled examples 
XY } ( i =1,..., n ) and aims to minimize expected error, i.e., Wherein L is a prescribed loss function and H is a set of functions called the hypothesis space, which consists of functions from x to y . In sentiment classification, the input vector of one document is constructed from weights of terms. The terms 1 ( ,..., ) possibly words, word n-grams, or even phrases extracted from the training data, with N being the number of terms. The output label y has a value of 1, -1 or 0 representing positive, negative or neutral respectively. In emotion classification, the same input vectors are used as the ones in sentiment classification. But the output labels are different: in emotion classification, the output label y has more values, representing different kinds of emotions, such as joy, love, expect, hate, sorrow, anger, surprise, anxiety, and neutral (without any emotion). In previous studies, these two tasks are performed individually, as shown in Figure 1. However, given the limited training data in each task, it is better to jointly learn the two tasks to benefit them from each other. Our approach to perform joint learning is to leverage both the sentiment and emotion classifier to classify the testing samples for sentiment or emotion classification, given the fact that they are sharing the same input vectors. In this way, we obtain both the emotion and sentiment labels of the samples. Then, the sentiment (or emotion) labels are transferred to the emotion (or sentiment) labels with the transformation model. Finally, the original emotion (or sentiment) labels and the transferred emotion (or sentiment) labels are combined to make the final decision. The framework of this joint learning for emotion cl assification is shown in Figure 2. classification with resources from both sentiment and emotion Sentiment and emotion classifica tion have a close relationship which makes it possible to transfer the output probability distribution from one task to another. Formally, let X denote a feature vector of a sample. The output of the sentiment classification S f on this sample is the probability distribution (| ) i p sX over the set of sentiment labels 12 {, ... } output of the emotion classifier E f is the probability distribution (| ) i p eX over the set of emotion labels 12 {, ,..., } study, the sentiment labels are positive, negative and neutral and the emotion labels are joy, hate, love, sorrow, anxiety, surprise, anger, expect, and neut ral. According to Bayes theory, the output probability distribution of the sen timent or emotion classifier can be transferred as follows: Wherein (| ) ij ps e and (| ) ij pe s are the statistic probabilities estimated with formula (4) and (5) from an extra data set, called sharing data Sharing L . Laplace smoothing has been used to avoid zero probability. Where () S YX is the sentiment label of the sample and () the emotion label of the sample. The function () Ix is defined as: For a testing sample, both the sentiment and emotion classifiers provide classification results. The two kinds of results could be transferred to each other according to (2) or (3). Given the original result and the transferred one, the combination result is defined as: Where (0 1)  X   X   X  X  X  is a parameter used to balance the weights of which has the maximum value among the joint posterior probabilities. Data Set: We carry out our experiments on the Ren-CECps corpus (Quan and Ren, 2009). It contains 34,603 sentences and each of them is annotated with a sentiment label and an emotion vector (sometimes contains multiple emotion labels). In our experiment, we only consider the majority emotion, that is, the emotion label with the highest value in the emotion vector. Features: Word unigram features are used. Each sentence is treated as a bag-of-words and transformed into binary vectors encoding the presence or absence of one word feature. Classification Algorithm: The maximum entropy (ME) classifier implemented with the public tool, Mallet Toolkits 1 . The posterior probabilities belonging to different categories are also provided in this tool. Implementation: From the corpus, we randomly select 6000 sentences as the testing data for both sentiment and emotion classification. Among the remaining, for sentiment classification, we randomly select 3000, 5000, and 8000 sentences as training data which take the sentiment labels only. Similarly, for emotion classification, we randomly se lect another 3000, 5000, 8000 sentences as training data whic h take the emotion labels only. Besides, we randomly select 400 sentences as sharing data. The parameter  X  in formula (7) and (8) is set to 0.5. To perform sentiment and emotion classification, we implement the following approaches for comparison: 
Single_Sentiment: construct a sentiment classifier with sentiment labeled data and sharing data.
Single_Emotion: construct an emotion classifier with emotion labeled data and sharing data.
Joint_Sentiment: construct a sentiment classifier with all labeled data using our joint learning approach.
Joint_Emotion: construct an emotion classifier with all labeled data using our joint learning approach.
Figure 3: Accuracy performances of different classification approaches on the testing data over different sizes of labeled Figure 3 shows the results of these approaches when different quantities of training data are empl oyed. From this figure, we can see that our joint-learning appr oach robustly outperforms single task classifiers across different sizes of labeled data on either sentiment classification or emotion classification. Averagely, they increase the accuracy from 0.638 to 0.662 in sentiment classification and the accuracy from 0.356 to 0.385 in emotion classification. Sharing data is the key component in our approach. It is used to estimate the transformation probabilities from a sentiment (or emotion) label to an emotion (or sentiment) label. However, sharing data is a costly work th at requires extra human annotation. Fortunately, our approach only need s a very small size of sharing data to estimate the transforma tion probabilities. Figure 4 shows classification performances of diffe rent approaches with different sizes of sharing data. From Figure 4, we can see that 300 to 400 sentences in sharing data are enough to train a good label-transformation model. Compared to the size of training data, the size of sharing data is much smaller.
 The parameter  X  in formula (7) and (8) is an important parameter in our approach. To evaluate the sensitiveness of this parameter, we change it from 0 to 1 and we find that the performance is robustly good when  X  is between 0.4 and 0.8. The detailed discussion of the results is omitted due to space limit. 
Figure 4: Classification perfor mances of different approaches In this paper, we propose an approach to jointly learning sentiment and emotion classification. Our approach mainly leverages an extra labeled data set to estimate the transformation probabilities between emotion and sentiment labels. Experimental results demonstrate that, compared to individual classifications, our joint learning classification can consistently achieve much better results. In our future work, we will try to find a solution on joint learning when non sharing data is given, which is the more popular case in real applications. Moreover, we would like to adapt our joint learning approach to other NLP tasks. This research work has been partially supported by two NSFC grants, No.61003155, and No.6 1273320, one National High-tech Research and Development Program of China No.2012AA011102, one General Research Fund (GRF) project No.543810 and one Early Career Scheme (ECS) pr oject No.559313 sponsored by the Research Grants Council of Hong Kong, the NSF grant of Zhejiang Province No.Z1110551. [1] Alm C., D. Roth and R. Sproat. 2005. Emotions from Text: [2] Chen Y., S. Lee, S. Li and C. Huang. 2010. Emotion Cause [3] Das D. and S. Bandyopadhyay. 2009. Word to Sentence [4] He Y., C. Lin and H. Alani. 2011. Automatically Extracting [5] Kim S. and E. Hovy. 2006. Extracting Opinions, Opinion [6] Li S., C. Huang, G. Zhou and S. Lee. 2010. Employing [7] Liu B. 2012. Sentiment Analysis and Opinion Mining [8] Liu B., M. Hu, and J. Che ng.2005. Opinion Observer: [9] Pang B., L. Lee and S. Vaithyanathan. 2002. Thumbs up? [10] Purver M. and S. Battersby. 2012. Experimenting with [11] Quan C. and F. Ren. 2009. Construction of a Blog Emotion [12] Riloff E., S. Patwardhan an d J. Wiebe. 2006. Feature [13] Turney P. 2002. Thumbs up or Thumbs down? Semantic [14] Wiebe J., T. Wilson, and C.2005. Cardie. Annotating [15] Wilson T., J. Wiebe, and P. Hoffmann. 2009. Recognizing [16] Xu G., X. Meng and H. Wang. 2010. Build Chinese Emotion 
