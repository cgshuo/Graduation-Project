 Recently, models of end-to-end machine translation based on neural network classification have been shown to produce excellent translations, rivalling or in some cases surpassing traditional statistical ma-chine translation systems (Kalchbrenner and Blun-som, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). This is despite the neural approaches using an overall simpler model, with fewer assumptions about the learning and prediction problem.

Broadly, neural approaches are based around the notion of an encoder-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation. We focus on the attentional model of translation (Bahdanau et al., 2015) which uses a dynamic representation of the source sentence while allowing the decoder to attend to different parts of the source as it gener-ates the target sentence. The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993).

In this paper we map modelling biases from word based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute po-sitional bias whereby word order tends to be simi-lar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fer-tility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for mono-tonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are en-couraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)).

We provide an empirical analysis of incorporat-ing the above structural biases into the attentional model, considering low resource translation sce-nario over four language-pairs. Our results demon-strate consistent improvements over vanilla encoder-decoder and attentional model in terms of the per-plexity and BLEU score, e.g. up to 3.5 BLEU points when re-ranking the candidate translations gener-ated by a state-of-the-art phrase based model. We start by reviewing the attentional model of trans-lation (Bahdanau et al., 2015), as illustrated in Fig. 1, before presenting our extensions in  X 3. Encoder The encoding of the source sentence is formulated using a pair of RNNs (denoted bi-RNN ) one operating left-to-right over the input sequence and another operating right-to-left, where h  X  i and h  X  i are the RNN hidden states. The left-to-right RNN function is defined as where h  X  0  X  R H is a learned parameter vector, as and b  X  s  X  R H , with H the number of hidden units, V
S the size of the source vocabulary and E the word then represented as a pair of hidden states, one from each RNN, e i = the word but also its left and right context, which can provide important evidence for its translation.
A crucial question is how this dynamic sized ma-trix E = [ e 1 , e 2 ,..., e I ]  X  R I  X  H can be used in the decoder to generate the target sentence. As with Sutskever X  X  encoder-decoder, the target sentence is created left-to-right using an RNN, while the en-coded source is used to bias the process as an auxil-iary input. The mechanism for this bias is by atten-tional vectors, i.e. vectors of scores over each source sentence location, which are used to aggregate the dynamic source encoding into a fixed length vector. Decoder The decoder operates as a standard RNN over the translation t , formulated as follows where the decoder RNN is defined analogously to Eq 1 but with an additional input, the source atten-tion component c j  X  R 2 H and weighting matrix in combination with the source attention and target formed to be target vocabulary sized, using weight ter which a softmax is taken, and the resulting nor-malised vector used as the parameters of a Categor-ical distribution in generating the next target word.
The presentation above assumes a simple RNN is used to define the recurrence over hidden states, however we can easily use alternative formula-tions of recurrent networks including multiple-layer RNNs, gated recurrent units (GRU; Cho et al. (2014)), or long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) units. These more advanced methods allow for more efficient learning of more complex concepts, particularly long distance effects. Empirically we found LSTMs to be the best performing, and therefore use these units herein.

The last key detail is the attentional component c j in Eqs 2 and 3, which is defined as follows with the scalars f ji denoting the compatibility be-tween the target hidden state g j  X  1 and the source en-coding e i . This is defined as a neural network with one hidden layer of size A and a single output, pa-and v  X  R A . The softmax then normalises the scalar compatibility values such that for a given tar-get word j , the values of  X  j can be interpreted as alignment probabilities to each source location. Fi-nally, these alignments are used to to reweight the source components E to produce a fixed length con-text representation.

Training of this model is done by minimising the cross-entropy of the target sentence, measured word-by-word as for a language model. We use standard stochastic gradient optimisation using the back-propagation technique for computation of par-tial derivatives according to the chain rule. The attentional model, as described above, provides a powerful and elegant model of translation in which alignments between source and target words are learned through the implicit conditioning context af-forded by the attention mechanism. Despite its ele-gance, the attentional model omits several key com-ponents of a traditional alignment models such as the IBM models (Brown et al., 1993) and Vogel X  X  hidden Markov Model (Vogel et al., 1996) as imple-mented in the GIZA++ toolkit (Och and Ney, 2003). Combining the strengths of this highly successful body of research into a neural model of machine translation holds potential to further improve mod-elling accuracy of neural techniques. Below we out-line methods for incorporating these factors as struc-tural biases into the attentional model. 3.1 Position bias First we consider position bias, based on the obser-vation that a word at a given relative position in the source tends to align to a word at a similar relative position in the target, i Related, the IBM model 2 learns discrete mappings between positions i and j conditioned on sentence lengths I and J .

We include a position bias through redefining the pre-normalised attention scalars f ji in Eq 5 as: where the new component in the input is a simple feature function of the positions in the source and target sentences and the source length,  X  ( j,i,I ) = log(1 + j ) , log(1 + i ) , log(1 + I ) J as this is unknown during decoding, as a par-tial translation can have several (infinite) different lengths. The use of the log(1+  X  ) function is to avoid numerical instabilities from widely varying sentence lengths. The non-linearity in Eq 6 allows for com-plex functions of these inputs to be learned, such as relative positions and approximate distance from the diagonal, as well as their interactions with the other inputs (e.g., to learn that some words are exceptional cases where a diagonal bias should not apply). 3.2 Markov condition The HMM model of translation (Vogel et al., 1996) is based on a Markov condition over alignment ran-dom variables, to allow the model to learn local ef-fects such as when i  X  j is aligned then it is likely that i + 1  X  j + 1 or i  X  j + 1 . These corre-spond to local diagonal alignments or one-to-many alignments, respectively. In general, there are many correlations between the alignments of a word and the alignments of the preceding word.

Markov conditioning can also be incorporated in a similar manner to positional bias, by augmenting the attentional input from Eqs 5 and 6 to include: data, we also removed sentences containing head-Models and Baselines. We have implemented our neural translation model with linguistic features our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the stan-dard features: relative-frequency and lexical trans-lation model probabilities in both directions; distor-tion model; language model and word count. We used KenLM (Heafield, 2011) to create 3-gram lan-guage models with Kneser-Ney smoothing on the target side of the bilingual training corpora. Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set per-plexities and translation results, as well as in an ad-ditional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap re-sampling (Koehn, 2004) to measure statistical sig-nificance, p &lt; 0 . 05 , of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the under-lying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. 4.1 Analysis of Alignment Biases We start by investigating the effect of various lin-guistic constraints, described in Section 3, on the attentional model. Table 2 presents the perplexity of trained models for Chinese  X  English translation. For comparison, we report the results of an encoder-decoder-based neural translation model (Sutskever et al., 2014) as the baseline. All other results are for the attentional model with a single-layer LSTM as encoder and two-layer LSTM as decoder, using 512 configuration test #param (M) Sutskever encdec 5 . 35 8 . 7
Attentional 4 . 77 15 . 0 +align 4 . 56 15 . 0 +align+glofer 5 . 20 15 . 5 +align+glofer-pre 4 . 31 15 . 5 +align+sym 4 . 44 30 . 1 +align+sym+glofer-pre 4 . 43 31 . 2 embedding, 512 hidden, and 256 alignment dimen-sions. For each model, we also report the number of its parameters. Models are trained end-to-end using stochastic gradient descent (SGD), allowing up to 20 epochs. We use a held-out development set for reg-ularisation by early stopping, which terminated the training after 5-10 epochs for most cases.

As expected, the vanilla attentional model greatly improves over encoder-decoder (perplexity of 4.77 vs. 5.35), clearly making good use of the additional context. Adding the combined positional bias, local fertility, and Markov structure (denoted by +align) further decreases the perplexity to 4.56. Adding the global fertility (+glofer) is detrimental, however, in-creases perplexity to 5.20. Interestingly, global fer-tility helps to reduce the perplexity (to 4.31) when used with the pre-training setting (+align+glofer-pre). In this case, it is refining an already excel-lent model from which reliable global fertility es-timates can be obtained. This finding is consistent with the other languages, see Figure 3 which shows typical learning curves of different variants of the attentional model. Note that when global fertility is added to the vanilla attentional model with align-ment features, it significantly slows down training as it limits exploration in early training iterations, however it does bring a sizeable win when used to fine-tune a pre-trained model. Finally, the bilin-gual symmetry also helps to reduce the perplexity scores when used with the alignment features, how-ever, does not combine well with global fertility (+align+sym+glofer-pre). This is perhaps an unsur-prising result as both methods impose a often-times similar regularising effect over the attention matrix.
Figure 4 illustrates the different attention matri-ces inferred by the various model variants. Note the difference between the base attentional model and its variant with alignment features ( X +align X ), where more weight is assigned to diagonal and 1-to-many alignments. Global fertility pushes more attention to the sentinel symbols  X  s  X  and  X  /s  X  . Determiners and prepositions in English show much lower fertility than nouns, while Estonian nouns have even higher fertility. This accords with Estonian morphology, wherein nouns are inflected with rich case mark-ing, e.g., n  X  oukoguga has the cogitative -ga suffix, meaning  X  X ith X , and thus translates as several En-glish words ( with the council ). The right-most col-umn corresponds to joint symmetric training, with many more confident attention values especially for consistent 1-to-many alignments ( difficult in English and raskeid in Estonian, an adjective in partitive case meaning some difficult ). 4.2 Experimental Results The perplexity results of the neural models for the two translation directions across the four language pairs are presented in Table 3. In all cases, our work achieves lower perplexities compared to the vanilla attentional model and the encoder-decoder architec-ture, owing to the linguistic constraints. We also obtained similar patterns of improvements when de-coding, using a greedy decoding strategy, as shown in Table 4. The exception was for en  X  ru, where the addition of the global fertility (in addition to the other aligment features) was detrimental, resulting in a decrease in BLEU score (5.94  X  5.26). This may be due to highly noisy nature of the web text corpus of Russian-English language pair, compared to the much cleaner sources for the other language pairs.
Greedy decoding does not appear to be competi-tive for neural models trained on small parallel cor-pora, not reaching the level of a phrase-based base-line (see Table 5). Despite this, however, these mod-els still provide substantial gains when used for re-ranking (as shown in Table 5) for translating into En-glish from the other four languages. We compare re-ranking settings using the log probabilities produced probabilities from the vanilla attentional model and the encoder-decoder. The re-rankers based on our model are significantly better than the rest for Chi-nese and Estonian, and on par with the other for Rus-sian and Romanian  X  English. In all cases our model has performance at least 1 BLEU point better than the baseline phrase-based system. It is worth not-Lang. Pair Zh-En Ru-En Et-En Ro-En Phrase-based 40 . 63 18 . 70 31 . 99 45 . 21 Enc-Dec 40 . 41 18 . 83 32 . 20 45 . 36 Attentional 41 . 16 19 . 79 32 . 78 46 . 83 Our Work 43 . 50  X  19 . 73 33 . 26  X  46 . 88 ing that for Chinese-English, our re-ranker leads to a substantial increase of almost 3 BLEU points. Kalchbrenner and Blunsom (2013) were the first to propose a full neural model of translation, using a convolutional network as the source encoder, fol-lowed by an RNN decoder to generate the target translation. This was extended in Sutskever et al. (2014), who replaced the source encoder with an RNN using a Long Short-Term Memory (LSTM) and leveraged the last hidden RNN states as source context for generating the output. Inspired by this, Bahdanau et al. (2015) introduced the notion of  X  X t-tention X  to the model, whereby the source context can dynamically change during the decoding pro-cess to attend to the most relevant parts of the source sentence. Further, Luong et al. (2015) refined the at-tention mechanism to be more local, by constraining attention to a text span, whose words X  representa-tions are averaged.

Similar in spirit to our work, recent research has proposed different ways of leveraging the attention history to incorporate alignment structural biases. (Luong et al., 2015) made use of the attention vector of the previous position when generating the atten-tion vector for the next position. Feng et al. (2016) added another recurrent structure for the attention mechanism to enhance its memorization capabilities and capture long-range dependencies between the attention vectors. Tu et al. (2016) proposed a cov-erage vector to keep track of the attention history, hence refining future attentions. Finally, Cheng et al. (2015) proposed a similar agreement-based joint training for bidirectional attention-based neural ma-chine translation, and showed significant improve-ments in BLEU for the large data French  X  English translation. We have shown that the attentional model of transla-tion does not capture many well known properties of traditional word-based translation models, and pro-posed several ways of imposing these as structural biases on the model. We show improvements across several challenging language pairs in a low-resource setting, as well as in perplexity, translation and re-ranking evaluations. In future work we intend to investigate the model performance on larger-scale datasets, and incorporate further linguistic informa-tion, such as morphological representations. Acknowledgements The work reported here was started at JSALT 2015 in UW, Seattle, and was supported by JHU via grants from NSF (IIS), DARPA (LORELEI), Google, Mi-crosoft, Amazon, Mitsubishi Electric, and MERL. Dr Cohn was supported by the ARC (Future Fellow-ship).

