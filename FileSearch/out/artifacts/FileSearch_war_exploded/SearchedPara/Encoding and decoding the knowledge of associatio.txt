 Shaoning Pang  X  Nikola Kasabov Abstract This paper presents a constructive method for association rule extraction, where the knowledge of data is encoded into an SVM classification tree (SVMT), and linguistic association rule is extracted by decoding of the trained SVMT. The method of rule extrac-tion over the SVMT (SVMT-rule), in the spirit of decision-tree rule extraction, achieves rule extraction not only from SVM, but also over the decision-tree structure of SVMT. Thus, the obtained rules from SVMT-rule have the better comprehensibility of decision-tree rule, mean-while retains the good classification accuracy of SVM. Moreover, profiting from the super generalization ability of SVMT owing to the aggregation of a group of SVMs, the SVMT-rule overwhelmingly, class-imbalanced data distribution. Experiments with a Gaussian synthetic data, seven benchmark cancers diagnosis, and one application of cell-phone fraud detec-tion have highlighted the utility of SVMT and SVMT-rule on comprehensible and effective knowledge discovery, as well as the superior properties of SVMT-rule as compared to a purely support-vector based rule extraction. (A version of SVMT Matlab software is available online at http://kcir.kedri.info ) Keywords Association rule extraction  X  Support vector machine  X  SVM aggregating intelligence  X  SVM ensemble  X  SVM classification tree  X  Class imbalance  X  Class overlap 1 Introduction Since support vector machines (SVM) [ 7  X  9 ] demonstrate a good ability in classification and regression, rule extraction from a trained SVM (SVM-rule) accordingly performs well on the decision making for data mining and knowledge discovery, sometime even better than comprehensible than our expectation because there is a large number of incomprehensi-ble numerical parameters (i.e., support vectors) that appear in these rules. Compared to the SVM-rule, the decision-tree is a simple, but very efficient rule extraction method in terms of comprehensibility [ 35 ]. The extracted rules from decision tree may not be so accurate as SVM rules, but they are easy to comprehend because every rule represents one decision path that is traceable in the decision tree. 1.1 Background In the literature, rule extraction based on single SVM focuses on interpolating the support vector and hyperplane boundary from a trained SVM to a set of linguistic rule expression. Nunez et al. [ 1 ] extract a rule by first clustering support vectors by k-mean, then for each furthest to the prototype vector to build an ellipsoid using a geometric method. Finally, the used a reduced number of support vectors, but made the generated ellipsoid rule overlaps each other. In addition, this method require having parameters such as the number of cluster and initial cluster centers as prior knowledge. As an improvement over Nunez X  X  method, Zhang et al. [ 2 ] proposed a hyper-rectangle rule extraction (HRE) method. Instead of clus-tering support vectors, the HRE clusters the original data using a support vector clustering (SVC) method to find the prototypes of each class samples, then constructs hyper-rectangle by the obtained prototypes and support vectors from the trained SVM. Because of the merits of the SVC algorithm, HRE can generate high quality rules even when training data contains outliers. Later, Fu et al developed rule extraction from support vector machine (RulExSVM) [ 3 , 6 ]. RulExSVM generates rules straightforwardly based on each of the support vectors. is built upon a hyper-rectangle associated with a certain support vector. RulExSVM is easy, but needs a tuning and pruning phase to reduce rules that have overlap and outliers.
The above SVM rule extraction methods are regarded as incomprehensible techniques because they are completely support vector based rule extraction methods, and the knowl-edge of obtained rules is all concealed in a number of numerical support vectors that are normally not transparent to the user. To mitigate this problem, the decision tree is a good rule extraction example that is recommended here for SVM rule extraction because every rule generated by a decision tree represents a certain decision path that has a comprehensible rule antecedent and rule consequence. For instance, C4.5 Rule [ 18 , 32 ] interpolates every as the rule consequence. The comprehensibility of the C4.5 Rule is better than that of the C4.5 decision tree, because the C4.5 Rule has the knowledge over a decision tree fused and grouped, and comes with a concise knowledge structure. 1.2 Relevant works Unlike decision trees, SVM classification tree (SVMT) is a type of SVM based aggregation method towards combining a family of concurrent SVMs for an optimized problem-solv-ing in artificial intelligence [ 12 ]. SVMT is different from the well known SVM ensemble [ 11 , 19 ] aggregation in that (1) SVM ensemble takes the number of SVMs for aggregation and the structure of aggregation as prior knowledge that is assumed to be known in advance, whereas SVMT has this prior knowledge learned automatically from data. (2) SVMT has an even more outstanding generalization ability than the SVM ensemble in particular when it comprehensible and a more robust rule extraction to class-imbalance, this paper presents a new type of SVMT in the form of depth-first spanning (DFS) SVMT and breath-first-span-ning (BFS) SVMT, and delivers rule extraction from SVMT through a rule induction over the constructed SVM decision-tree and support vectors (SVs) from local SVMs. 1.3 Paper organization This remainder of this paper is structured as follows: Sect. 2 introduces the difficulties and motivations for SVMT-rule. Section 3 derives the decision function and loss func-tion of SVMT. Section 4 presents the spanning algorithms of DFS-SVMT and BFS-SVMT. experiments with a synthetic data and two applications of cancer diagnosis and fraud detec-tion are discussed in detail. Finally, the conclusions and outlines of future work are pointed out in Sect. 7 . 2 Difficulties and motivations for SVMT-rule 2.1 Comprehensibility A difficulty with SVM rule extraction is that the extracted rules have the problem with comprehensibility, because support vectors (SVs) are represented in the rule as a set of incomprehensible numerical parameters. However, in many applications, we have to deal with a large number of SVs.

To solve the comprehensibility problem of SVM rule extraction, achieving a smaller num-ber of SVs meanwhile a good accuracy of extracted rules, several types of method have been researched so far: (1) the decision tree interpolation method [ 38 , 39 ]. This method uses no SVs, but an decision tree to interpolate the result of trained SVM for rule extraction. Owing to the decision tree interpolation, the extracted rules are as comprehensible as decision-tree rule, but has lost the accuracy of the SVM, because no SVs are encoded in the extracted rules; (2) SVs clustering method [ 2 ]. It is to reduce the number of SVs by clustering SVs and use only the SV cluster centers for rule extraction. The retaining of SVM classification accuracy is not ensured for the extracted rules, because the altered SVs actually has made the hyperplane different from the SVM origin. (3) SVs grouping and selecting method [ 40 ]. This approach is more effective than the the above two, because it introduces cost functions for grouping and selecting representative SVs for rule extraction. The disadvantage with the ation criteria including true positives (TPs), false positives (FPs), and AUC (the area under the receiver operating characteristic (ROC)). 2.2 Class imbalance problem On the other hand, support-vector based rule extraction also has the difficulty of class-imbalance. In other words, when class-imbalance occurs in a dataset, the number of samples from one class is much larger than that from the other class (in the case of binary class dataset, the class with the smaller number of samples is called  X  X kewed X  class, and the other class is called  X  X verweighted X  class), SVM often fails to classify the skewed class correctly [ 42  X  45 ], as most classification algorithm including SVM assume an balanced distribution of fication of the weighted-class, but lose almost completely the accuracy of the skewed-class classification.

For the class-imbalance problem, data sampling method is a straightforward solution. The approaches include undersampling, which reduces the number of samples in the bigger class closer to the smaller class; oversampling, which takes random equal number of subsamples repeatedly from the two classes [ 48 ], and keeps a desired ratio of the two classes in each subset of samples; and resampling, which is a mixture of undersampling and oversampling [ 46 , 47 ]. Algorithmic method is a more effective solution. The approaches include adjusting ifying learning function to adjust the class boundary [ 42 , 43 ]. As class imbalance occurs often together with class-overlap in many applications, the above approaches still confront the following problem: while improving the recognition of the skewed-class, but degrading too much of the performance on weighted-class classification, in particular when a serious class-overlap is involved. 2.3 Motivations Towards dealing with both the above two difficulties of SVM rule extraction, our idea is to put those numerical support-vector rules into a comprehensible decision-tree structure, so that the comprehensibility of the rule set can be improved because that different sets of support-vector rules are traceable within the decision tree, and the number of support vectors can be reduced as part of support-vector rules transformed into simple and comprehensible decision-tree rules. On the other hand, support-vector rules are encoded from a group of individual simple SVMs such as linear SVM, instead of one SVM with a complicated classi-fication hyperplane, so that the class-imbalance problem can be treated easily by allocating more computing power (i.e., individual SVMs) for the classification of the skewed-class. Motivated by this, we proposed in this paper a new method for rule extraction from SVM. We encoded the knowledge of rules by aggregating a set of SVMs into a decision tree and extracted rules by decoding the knowledge out of the SVMT. The following is a summary of the steps of the proposed SVMT-rule algorithm, which consists of SVM classification tree construction, and followed by three steps of rule extraction over the SVMT, including rule extraction over the tree structure, the SVM node, and the one-class SVM node, respec-tively. Individual steps of the SVMT-rule are detailed in the subsections below and shown schematically in Fig. 1 . 3 SVM classification tree modeling The SVMT was first proposed in [ 13 ], in the context of face membership authentication appli-cation [ 11 ]. The SVMT was shown to be very capable of reducing the classification difficulty due to class-overlap in a recursive procedure, thus is useful in pattern recognition problems with large training samples but with noise information suppressed via feature extraction. Unlike SVM ensemble aggregation [ 19 ] assumes that the number of SVMs in aggregation should be known in advance as the prior knowledge despite the number of SVMs is often difficult to determine in real applications, SVMT aggregation has solved the difficulty of the SVM ensemble with the determination of the number of SVMs in a SVMT learned auto-matically from data. However, the spanning of the previous SVMT in [ 13 ] is completely data-driven, which grows easily an overfitting of learning and comes up with a large size decision tree. Obviously, this type of SVMT is not optimal for rule extraction and decision making [ 37 ]. To deal with this difficulty, a new type of SVMT with spanning order preference are modeled as follows. 3.1 SVMT decision function Mathematically, a 2-SVMT can be formulated as a composite structural model as follows: Given a 2-class dataset D for classification, and a predefined data partitioning function P on D , the whole dataset D can be divided through an optimized P  X  into N partitions { g 1 , g 2 ,..., g N } .
 Then, a 2-SVMT can be modeled as,  X  X ne-class classifier X  of class 1 and class 2, respectively. I,J,K such that I + J + K = N , correspond to the number of three data-partition types: partition with data from class 1 and class 2, partition with only data from class 1, and partition with only data from class 2, respectively. I,J,K and N are determined after the SVM tree model generation. Figure 2 gives an example of 2-class SVM and one-class SVM over data partition with data in 2-class and one-class, respectively.

In the case that g i contains two classes data, a typical 2-class SVM f Svm ,asFig. 2 a, is applied to model a local f i on g i as, where  X  is the kernel function, l is the number of training samples, and w, b is optimized through where C and k are used to weight the penalizing variable  X  ,  X (.) acts the role of kernel function.

In another case, when g i contains only data from one-class, g i can be modeled strictly data of the partition S and negative on the complement  X  S : where i represents class label  X 1 X  or  X 2 X  in binary classification.

Note that one-class SVM node is modeled under the assumption that g i contains not only the one-class sample, but also some outlier samples (i.e., samples that are not belonging to that one class). However, in the case that g i contains no outlier sample, then the one-class SVM (node) is equivalent to a clustering decision function, which is if x by P belongs to class i , then the output of the one-classifier is assigned as i .

Thus, we can have the decision function of 2-SVMT  X  f as, 2-SVMT. 3.2 SVMT loss function used to capture the extent of this error. Loss L is therefore data dependent: from the whole data D under a distribution of g . The expected loss can be quantified as,
In experiments, g can be realized by applying a tenfold cross validation policy on D .Since we have no information of the real classification function f , for simplicity we can fix y as the class label of the training dataset.

Thus, given data drawn under a distribution g , constructing an SVM tree requires choosing a function f  X  such that the expected loss is minimized: where, F is a suitably defined SVM tree functions.
 Substituting  X  f to Eq. ( 6 ), the loss function of 2-SVMT is,
In Eq. ( 9 ), L is determined by the performance of every regional classification from a regional SVM classifier or a one-class classifier. Given every SVM in 2-SVMT with the same kernel and penalty parameters, loss function L now depends only on the data distribu-tion of local regions. In other words, the data partitioning method P eventually determines the loss function of the resulting SVM tree.
 In this sense, the construction of SVMT f  X  is equivalent to seeking a partition function P  X  that is able to minimize the following loss function, above function is the risk from SVM classification, and the remaining two terms are the risks from one-class classifier f Sgn .

The risk from f Sgn can be minimized through the training of an SVM one-classifier. In the case that Sgn is simplified by making every decision of classification through P ,the of
P has been minimized when the data partitioning is performed. For a binary partitioning [
X 1 , X 2 ]= P ( X ) , the risk can be the same minimized by a SVM training using X 1and X 2 as class  X +1 X  and  X   X  1 X , respectively. So, Eq. ( 10 ) can be updated as,
Thus, we can solve the above function by discovering every individual partition with the minimized SVM classification risk,
Equation ( 12 ) is proportional to the size of the region (i.e., number of samples in the region), thus in practice Eq. ( 12 ) can be implemented by seeking data partitions with dif-ferent sizes and a minimized SVM classification risk. To this end, P  X  is modelled as the following recursive supervised and scalable data partitioning procedure, local 2-class SVM loss function.  X  is the SVM class separability threshold. P n represents a of partitioning.

As a result of Eqs. ( 12 )and( 13 ), partitions with L Svm minimized is produced by P at partitions that contain only one class data are also being produced. This builds the one-class 4 The encoding of SVM tree To construct a SVM tree, a data-driven spanning (DDS) approach is, to split data whenever data partition contains just one class data. As mentioned above, SVMT is grown up automat-ically without knowing any prior knowledge, but such a DDS type SVMT grows easily an overfitting of learning, thus gives a huge size SVM tree, which obviously is not optimal for the purpose of efficient rule extraction. To approach this problem, we construct here a new type SVMT by applying the following two spanning order preferences. 4.1 Depth-first spanning tree The idea of depth-first spanning is to expand the tree to a node of the next layer as quickly as possible before fanning out to other nodes of the current layer.
 tree expansion goes to the next layer, and one terminal node is produced at the current layer of the tree.

Thus Eq. ( 13 ) can be realized by repeating the following binary data partitioning, where X  X  is the one optimal partition at scale  X  generated by P ,and X is the remaining dataset such that X = X  X   X  X . 4.2 Breadth-first spanning tree Breadth-first is another tree spanning approach. The basic idea is to fan out to as many nodes as possible of current layer before expanding the tree to a node of the next layer.
This can be explained as, at a data partitioning scale  X  ,ifthereare  X  data partitions judged  X  nodes are produced at current layer of the SVMT.

In this case, Eq. ( 13 ) uses a multiple data partitioning,  X  , and it is determined by partitioning function  X  and the used partitioning scale  X  . 4.3 The SVMT algorithms As constructing a SVMT, Eq. ( 10 ) is implemented by a 2-step recursive procedures. First, a classifier or a SVM classifier is built to serve as a node of the 2-SVMT.
Algorithm 1 and 2 below present the detailed steps of constructing a Depth-first spanning (DFS) 2-SVMT, and Breadth-first spanning (BFS) 2-SVMT, respectively. [Algorithm 1. Depth-first spanning 2-SVMT] Outputs: T (a trained DFS 2-SVMT) Step 1: Initialize T as the root node, and current partitioning scale  X  as  X  0 . Step 2: if X train is empty then iteration stops, return constructed SVMT T . Step 4: if X  X  belongs to one-class, then train a one-class SVM as Eq. ( 4 ), and append a Step 5: otherwise, X  X  belongs to two-class, and the classification loss function of the par-Step 6: if no new node is added to T , then adjust current partitioning scale by  X  =  X   X   X  . Step 7: set X train as X train  X  X  X  , and go to Step 2. [Algorithm 2. Breadth-first spanning 2-SVMT] Outputs: T (a trained BFS 2-SVMT) Step 1: Initialize T as the root node, and current partitioning scale  X  as  X  0 . Step 2: if X train is empty then iteration stops, return constructed SVMT T . Step 4: For each data partition X i , do the following operations Step 5: if no new node is added to T , then adjust current partitioning scale by  X  =  X   X   X  . Step 6: set X train as X train  X  X  X 1 , X 2 ,..., X } , and go to Step 2.
 To test the above constructed 2-SVMTs, a input sample x is first judged by the test function T ( x ) at the root node in the SVM tree. Depending on the decision made by the root node, x will be branched to one of the children of the root node. This procedure is repeated until a leaf node or a SVM node is reached, then the final classification decision is made for the 3 illustrates the testing procedure of a 2-SVMT (DFS 2-SVMT or BFS 2-SVMT). [Algorithm 3. 2-SVMT testing algorithm] Inputs: T (a trained 2-SVMT), x (a testing sample) Output: C (class label of the testing sample) Step 1: Set the root node as the curr ent node.
 Step 2: If the curr ent node is an internal node, then start the following loop operations: Step 3: The above loop is stopped when a terminal node is reached, and the class label C is
In the above SVMT algorithms, K specifies the type of SVM used in 2-SVMT construc-tion.  X  0 determines an initial resolution for 2-SVMT to start zooming in the data and to construct an SVMT over the data.  X  is the permitted minimum loss function value, which gives a criterion of partition selection for optimal SVM classification.

The default  X  can be set as the biggest the partitioning scale that the used partitioning method allows. For example, for the K-Mean partitioning method,  X  0 canbesetas2;forECM partitioning method,  X  0 can be set as 0.9. Given a serious class-imbalance and class-overlap of data, a finer scale can be taken to enable 2-SVMT to separate classes in a more accurate the greater the time cost to obtain the 2-SVMT trained. 4.4 Coping with class imbalance and class overlap In the above SVMT algorithms, the class-imbalance is addressed in two parallel ways: 1. The recursive data zooming-in schema, where larger size data partitions (particularly for 2. The skewed-class is protected by SVMT by turning SVM loss function L Svm to the Note that the regional skewed class represents the skewed-class within a data partition. In other words, a regional skewed class also could be a global overweighted class, when the ment as most data sampling methods do to favor the skewed class by using the skewed-class data repeatedly, which does obtain some improvement on the classification of the skewed class, but meanwhile has an even larger performance deduction on the classification of the both the skewed class and the overweighted class from being lower valued in a regional deci-sion making (acting as a node in SVMT) due to the class-imbalance. Equation ( 16 ) eventually works effectively for the classification of the skewed class, because the probability for the skewed class data to distribute skewed regionally in practice is often much higher than that of the overweighted class to show skewed. 5 The decoding of SVM tree 5.1 Logic association rules a standard logic rule is formed as where the if part is the rule antecedent, and the then part is the rule consequence. The rule means that If x belongs to the subspace g i , the it is assigned a class label C k .
Typically, there is a conjunction of logical predicate function T ( x ) ={ True , False } to belongs to an interval x i  X  X  a i  X  , a i +] ,then x  X  g i .

However, when interpolating the knowledge from the decision tree structure of an SVMT, the predicate function T is extended to be on multiple attributes of D because every subspace g in an SVMT is obtained through a supervised data partitioning Eq. ( 13 ), and both the loss function and the partitioning function of Eq. ( 13 ) are a multivariate computation. Thus, the predicate function T will be a multivariate function determined by the choice of partitioning function P T depending on the choice of partitioning function.

On the other hand, when interpolating a support vector from a local SVM into a lignitic 5.2 SVM nodes knowledge interpolation As discussed above, a trained SVMT has two types of SVM nodes: (1) Two-class SVM node V ={ g , X , f Svm } ,where g contains 2 classes data with L Svm &lt; X  5.2.1 2-class SVM node For a 2-class SVM node (called SVM node hereafter), rule extraction according to [ 6 ]is based on hyper-rectangular boundary built upon support vector of a trained SVM. Given support vector s m of class l , the hyper-rectangular boundary for s m is, where 1  X   X  pi  X  0 , p ={ 1 , 2 } ,and i = 1 ,..., n the sequence number of dimension. the i th dimension, respectively, where, and
Given a trained SVM, the rule based on support vector s m can be generated by Fu X  X  algorithm [ 6 ]as, [Algorithm 4. two-class support vector rule extraction] Input: s m (support vector) Step 1: set l = 1, refers to dimension l Step 2: calculate x l subject to f ( x ) = 0and x j = s mj ( j = 1 ,..., n ,and j = l )bythe Step 3: determine L o and H o according to the solutions of the problem in step 2. The number Step 4: l = l + 1, if l &lt; n ,gotostep2,elseend.
 Figure 3 gives an example of hyper-rectangular rule extraction from a trained 2-class SVM in SVMT, where the black circles and points represent the support vectors from class L dashed rectangle in Fig. 3 . 5.2.2 One-class SVM node of one-class SVM node, the hyper-rectangular rules from one-class SVM node are required to cover as much of the class data as possible.

To expand the coverage of the hyper-rectangular rule, given two support vectors s m and s from a trained one-class SVM, where S mi = s ni along the i th dimension, the above L o and H o of Eqs. ( 21 )and( 22 ) are updated as, and
Accordingly, Algorithm 4 can be modified to perform a one-class support vector rule extraction as, [Algorithm 5. One-class support vector rule extraction] Input: s m and s n (two support vectors) Step 1: set l = 1, refers to dimension l Step 2: calculate x l subject to f ( x ) = 0and x j = s mj ( j = 1 ,..., n ,and j = l )bythe Step 3: determine L o and H o according to the solutions of the problem in Step 2. The number Step 4: l = l + 1, if l &lt; n ,gotoStep2,elseend.

It is noticeable that Algorithm 5 has the problem of rule disaster. Given m one-class n a small number of support vectors gives possible a huge number of rules for the same partition and the same class of data. Clearly, the redundancy occurs in the extracted rule set.
Alternatively, we use all one-class support vectors together for the rule extraction of one-class SVM, because the data from one-class SVM, both the outliers and the origin of the class eventually belong to the same class, and there is no need to separate one from another from the viewpoint of classification.
 In this way, the above rule extraction of Eqs. ( 23 )and( 24 ) can be re-written as,
Algorithm 5. is simplified into Algorithm 6 to extract rectangular rules only from the pair of vectors (maximum and minimum). Figure 4 b gives an example of such rule extraction, where a set of smaller rectangles of Fig. 4 a are replaced with one rectangle, which is built on a pair of support vectors, and which covers most of the one-class data. [Algorithm 6. Simplified one-class support vector rule extraction] Input: s 1 , s 2 ,..., s m (set of support vector from a trained one-class SVM) Step 1: set l = 1, refers to dimension l Step 2: calculate L o ( l ) by Eq. ( 25 ) Step 3: calculate H o ( l ) by Eq. ( 26 ) Step 4: l = l + 1, if l &lt; n ,gotoStep2,elseend. 5.3 The SVMT-rule algorithm For extracting rule from a SVMT, the rule can be decoded through decision-tree rule extrac-tion and SVM nodes knowledge interpolation. The description of the proposed SVMT-rule is presented as Algorithm 7, where we construct SVMT by Algorithm 1 or Algorithm 2, and extract rules by tree interpolation and SVM node decoding through Algorithm 4 and Algorithm 6. [Algorithm 7. Rule extraction over SVM trees (SVMT-rule)] Inputs: X train (Training dataset) Output: R (extracted ruleset) Step 1: Apply the SVM classification tree algorithm (Algorithm 1. for DFS SVMT, and Step 2: For every path in T from the root node to a terminal node (i.e., one-class node or Step 3: For every decision-tree rule in C , do the following operations: Step 6: repeat Step 3 until no rule is removed from R , output R .

Figure 5 gives an illustration of SVMT rule extraction, where a,b,c,and d are four decision paths of an example SVMT. For decision path a X  X , decision rules are extracted by taking all test functions along the path as the conjunctive rule antecedent, and the class label held by the leaf as the rule consequence. If outlier samples are considered for one-class node, additional SV rules are extracted from the one-class SVM on the node by Algorithm 6. For extraction. Therefore, the extracted rule set by SVMT-rule consists of decision-tree rules and local SV rules structured by the decision tree. 6 Experiments and applications In our experiments, algorithms were implemented in Matlab Version 6.5, run on Pentium 4 PC, 3.0 GHZ 512 Mb RAM, and comparison tests were performed in the environment of Neucom 2.0 [ 30 ].
For extracting rules over SVMT, all single 2-class SVMs are set with a standard linear kernel. A one-class SVM of RBF  X  = 3 . 5 kernel is used only for our fraud detection appli-cation, where a large database with noise is involved. All SVMs have the same penalty coefficient of 0 . 1.  X  is assigned as a K -means clustering partitioning function, and all 2-SVMTs use the same default parameters of  X  0 = 2and  X  = 0 . 02. Because polynomial SVMs perform better than linear SVM on most classification tasks [ 36 ], for a simple validity verification we merely compare the SVMT-rule with the SVM-rule, and the rule extraction from a trained 2nd order polynomial single SVM [ 6 ]. Note that all single SVMs are also set with the same penalty coefficient of 0 . 1.

To evaluate the quality of extracted rules, we measure accuracy as the percentage of clas-sification, and comprehensibility as the number of of rules plus the number of antecedents by computing the class bias ratio (CBR) as, A larger CBR value means a more imbalanced class data distribution, or class classification accuracy. 6.1 Synthetic dataset We first experimented the SVMT-rule with an synthetic dataset for classification. Figure 6 ian distribution, and the number of class 2 samples is approximately 1/10 of class 1, which follows that the CBR of this dataset in terms of class data distribution is 0.9.
Constructing SVMT over the dataset of Fig. 6 ,Fig. 7 gives an example of DFS-SVMT and BFS-SVMT generated by Algorithm 1 and Algorithm 2, respectively. The root node P 1 and SVM node denoted as a ellipse circled  X  X VM X  label. Each SVM node has two sons, which means that each SVM node provides, class 1 and class 2, two decision choices. They are symbolized the same as the one-class node, but they are not counted here as one-class node. The DFS X  X VMT is seen to have 8 SVM nodes, 14 one-class nodes, and the BFS X  X VMT has 8 SVM nodes, 24 one-class nodes. The number of one-class nodes is several times of SVM nodes, suggesting that the decisions of BFS X  X VMT are more dependent on the tree than decisions made by DFS X  X VMT.

In the construction of SVMT, the data is zoomed in recursively by a supervised data parti-tioning. Figure 8 b shows the resulting hyper-plane from Algorithm 2 as the data partitioning step goes to the second iteration. Compared to the single SVM hyperplane in Fig. 8 a, the SVMT hyper-plane in Fig. 8 b is a combination of the part of data partitioning boundary esti-mated in Fig. 8 c and two pieces of short SVM hyper-planes. It turns out that SVMT conducts classification using less support vectors from SVM, but more decision boundaries from data partitioning. In other words, SVM is not used unless it is absolutely necessary.
Table 1 compares the SVMT-rule with SVM-rule and the original SVMT on the classifi-cation of the above synthetic dataset, based on the following six measurements: (1) the number of obtained rules (N. of Rules). (2) the number of support vectors (N. of SVs). (3) the average general classification accuracy (Ave. Acc.). (4) the classification accuracy of class 1 (Cls1 Acc.) (5) the classification accuracy of class 2 (Cls2 Acc.) (6) the class bias ratio in terms of class classification accuracy (CBR).

For the CBR 0.9 dataset in Fig. 6 , SVMT-rules come up with a CBR  X  0 . 3 class classifica-tion accuracy while the SVM-rule gives a completely imbalanced output with CBR = 1 . 0. It is distinct that SVMT-rules have profited from the outstanding generalization ability of SVMTs as both statistics are shown to be quite similar in Table 1 . The two BFS type SVMT methods (i.e., BFS SVMT-rule and BFS X  X VMT) perform even better than the DFS type SVMT methods (i.e., DFS SVMT-rule and DFS X  X VMT), this suggests that BFS is able to fit the data more appropriate than DFS for constructing SVMT. 6.2 Cancer diagnosis We have implemented the BFS SVMT-rule technique in cancer diagnosis and decision sup-port. Table 2 gives the seven well-known cancer microarray datasets with a two-class clas-sification problem. As seen in the table, most datasets were biased in the sense of having an unbalanced number of patients in the two classes, e.g., normal versus tumor.

For the 3 datasets that had an independent validation dataset available, we selected 100 genes from the training dataset by a typical t -test gene selection [ 27 ], and used them on the validation dataset. For the remaining four datasets, we verify the method using tenfold cross-validation, removing randomly one tenth of the data and then using the remainder of the data as a training set. For each fold, we similarly selected 100 genes over the training set, then applied the obtained 100 genes to the corresponding testing set. The comparison of SVMT-rule to single SVM-rule and the original SVMT on seven cancer diagnosis is shown in Table 3 , where rule extraction is measured in terms of diagnosis accuracy, the number of rules, and the number of involved support vectors, respectively.

Although the SVMT-rule does not perform better than the SVMT, it is still impressive that Table 4 indicates the prior generalization ability of SVMT-rule to SVM-rule is about 0 . 946 ))/ 7 = 0 . 1096 ) . More importantly, the comprehensibility of the BFS SVMT-rule is clearly improved as compared to SVM-rule because the average number of support vec-tors for SVMT-rule is only about 60% that of SVM-rule. For CNS and Lung Cancer, BFS SVMT-rule is found with less support vectors, but even more rules than SVM-rule. This inconsistency stays for the reason that these two cancers are more difficult than the other type of cancers for a correct knowledge representation, where the BFS-SVMT has allocated more efforts on decision tree spanning. 6.3 Fraud detection The cell phone fraud often occurs in the telecommunication industry. We have investigated a fraud detection using SVMT-rule rule extraction. The database was obtained from a mobile telecom company. It recorded 1 year X  X  action data of 53,696 customers. The historical data on fraud includes 15 tables. Among them, three core tables for the topic are IBS_USERINFO (customerprofile),IBS_USRBILL(customerpaymentinformation),andIBS_USERPHONE (customer action) and all others are the additional tables.

Fraud detection basically is a binary classification problem, where customers are divided into two classes: fraud or non-fraud. The aim of fraud detection is to distinguish the fraud customers from the remaining honest customers. The method of fraud detection is to analyze the historical data, which is a typical data mining procedure as follows: Step 1: Data cleaning, to purge redundant data for a certain fraud action analysis.
Step 2: Feature selection and extraction, to discover indicators corresponding to changes Step 3: Modeling, to determine fraud pattern by classi6er and predictor.
 Step 4: Fraud action monitoring and prediction, to issue the alarm.

Step 2 extracts the eight salient features to measure the customer payment behaviors using PROC DATA of SAS/BASE and combine the customer profile(IBS USERINFO) and customer action (IBS BILL) to one table (IBS_USERPERFORMANCE). Table IBS_ USERPERFORMANCE has 53,696 records and nine attribute items.

To discover the fraud patterns, BS_USERPERFORMANCE data is divided into a train-ing set and a test set, in which the training set contains 70% of 53,696 records, and the test data set is the remaining 30% of 53,696 records. Table 4 lists the statistical results of rule extraction using SVMT-rule, SVM-rule, and C4.5-rule for the fraud detection. As expected, the SVMT-rule shows a more accurate fraud detection than SVM-rule, as well as C4.5-rule, where BFS SVMT-rule performs slightly better than DFS SVMT-rule at the price of that more support vectors and more rules are used. It is surprising that the number of rules from SVMT-rule is only about half of the number from SVM-rule. One possible explanation is that the SVMT-rule suits better a large database, where the decision tree part of SVMT plays an absolutely more important role in encapsulating data for rule extraction. 7 Discussions and conclusions The SVMT aggregates the decision tree and the support vector machine in an SVM clas-sification tree, manipulates the two sides rule extraction by selecting the simple part of the problem for a comprehensible decision tree representation, and leaving the remaining diffi-cult part with an support vector machine approximation. For extracting rules, SVMT-rule is introduced in the form of DFS SVMT-rule and BFS SVMT-rule, which encodes the knowledge of rules by an induction over the decision tree of SVM, and the interpolation of the support vectors from local SVMs.

The experimental results and applications have shown that the SVMT-rule, compared to single SVM rule extraction, has the following desirable properties: (1) SVMT-rule is more accurate, and especially more robust for the classification of data with a class-imbalanced data distribution. (2) SVMT-rule has a better comprehensibility for rule extraction because only about half the number of single SVM support vectors are kept in SVMT rules. (3) The BFS SVMT-rule commonly outperforms DFS SVMT-rule.

SVMT-rule is also able to apply to a multi-class case by extending the 2-class SVMT to a m -class ( m  X  2) SVMT. One straightforward way is to do  X  X ne-to-one X  or  X  X ne-to-all X  SVMT integration. However, this approach creates a new problems of decision forests [ 29 ].
In the principle of SVM aggregation, the construction of a m -SVMT is to decompose an m -class task into a certain number of 1  X  m classes regional tasks as, where  X  &lt; X &gt; , X   X  2 represents here a multi-split data partitioning function. Similarly, f represents a j th-class one-class SVM.

Accordingly, the classification function of a m -SVMT can be written as, where 1  X  i  X  m and 2  X  i  X  m .

SVMT-rule has improved the comprehensibility of SVM rule extraction by reducing the number of support vectors effectively. However, there are still a few support vectors left in the rule of the SVMT-rule. To further improve the comprehensibility of the SVMT-rule, it is suggested that a symbolic rule interpolation [ 34 ] could be developed for SVMT-rule in the future.
 References Author biographies
