 well-defined meaning, better enabling computers and people to work in cooperation [1]. In recent years, semantic web has made significant progress, in particular through the development of infrastructure such as: ontology language like RDF, DAML+OIL and OWL, ontology editor like Prot X g X  [2]. 
The ontology based semantic web has many applications, for example, semantic re-trieval, intelligent reasoning, and social network analysis. In these applications, there are a large number of requests to the ontology base. The request includes accesses of concept or instance. Such frequently accesse s to the ontology base lead the ontology server to be very low efficient. In order to achieve high efficient response of the on-tology server, it is necessary to conduct caching for the ontology data. This is exactly the problem addressed in this paper. 
In database community, the problem of data caching has been intensively investi-gated. The methodologies proposed can be used in the ontology caching. However, they are not sufficient for dealing with the problem. In the task of caching in database, usually the most frequently accessed data are cached and the recently less frequently accessed data in the cache are removed from it. Different from that, in ontology base, data are organized as objects and relations among objects. User may request one ob-also possibly request a similar object that has not any relations to the object. Ontology caching should consider more factors and is more difficult. Unfortunately, despite the search community. No previous study has so far sufficiently investigated the problem, to the best of our knowledge. 
Three questions arise for ontology caching: (1) how to formalize the problem make an implementation. (1) We formalize ontology caching as that of classification. Specifically, we a clas-sification model to determine if the ca ndidates should be cached or not. (2) We propose to conduct ontology caching using machine learning methods. In the approach, when a user requests an object, we take its similar objects as candidates, and use a classification model to predict whether they should be cached or not. (3) It turns out that some of the tasks in the approach can be accomplished with ex-isting methodologies, but some cannot. Caching the current accessed entity is similar to the principle of the "the same data is likely to accessed again in near future" in the the current accessed entity into the cache can't be solved using existing technologies. Because an entity may not be accessed in the previous history can have relations with the current accessed entity. Whether caching it or not is according to the importance implement a classification model which considers not only access history but also relations to the current accessed entity to dete rmine if the object that has relations to the current accessed one should be cached. We tried to collect data from as many sources as possible for experimentation. Since in our project SWARMS, we use the software domain for demo, we collect the data in sourceforge and formalized them to instances in our software ontology (http://www.schemaweb.info/schema/SchemaDetails.aspx?id=235). Our experimental than the baseline methods for caching. plementation. Section 6 gives our experimental results. We make concluding remarks in Section 7. There have been some industry tools that investigate ontology caching methods. For example, Jena [3] is an ontology access tool which supports parsing of RDF, DAML+OIL and OWL files. However, the cache method in Jena is simple. It just creates an ontology model in memory and lo ads all the concepts and instances to the model. 
Keller and Basu propose a predicate-based client-side caching scheme that aims to load the client cache and predicate descriptio ns derived from these queries are stored and Chu propose a semantic caching method for web databases via query matching techniques [5]. The scheme proposed utilizes the query naturalization to cope with schematic, semantic and querying capability differences between the wrapper and matched query from the cache. Awaring context in database caching has been also noticed by Davidson [6] and Hanson [7]. 
There has been some other related works in data cache. For example, mediator sys-tem, which is usually as the role of cont ent translation systems, needs the data cache method to get efficiency. A semantic cache of the ontology-based mediator system named YACOB is pro-posed by Marcel et al [8]. YACOB's objective is to overcome performance issues on ontology-based mediator system which provides domain knowledge for heterogeneous web sources' integration. The semantic region [9] was cache manager to aggregate information about multiple tuples. Unlike pages, how-ever, the size and shape (in the semantic space) of regions can change dynamically. Ontology caching is an important subject in semantic web. A large number of applica-tions need ontology caching as a fundamenta l component to build up. Unfortunately, despite the importance of the problem, ontology caching has received little attentions. The problem of ontology caching is quite different from that of database caching. We here give a case study to show the problem of ontology caching and the main difference with database caching. Figure 1 shows the metadata definition in an ontol-ogy of a department of university. In the ontology, concept  X  X ssociate Professor X  and  X  X ull Professor X  are sub classes of concept  X  X rofessor X . Concept  X  X aster Student X  and  X  X hd Student X  are sub classes of concept Student. Concept  X  X rofessor X  has both properties and relations. The properties include  X  X ame X ,  X  X ge X  and the relations are  X  X asStudent X  and  X  X orksInProject X . The difference between properties and relations is that the values of properties are data types, for example, string, integer, etc and the  X  X rofessor X  shows the values of the properties and relations. Then, let us consider two users visiting the ontology base as shown in figure 2. 
User 1 visits the instance  X  X ro1 X  and gets the value of the property  X  X ame X . User 2 the students' name and age. If the data cache method is used to perform the ontology dents to the cache. Thus, the data cache can't be simply applied to the ontology cache without change. Ontology cache must care about semantic relations between concepts and instances. This makes it different from data cache. This paper proposes a method based on semantic similarity among concepts and instances to determine whether a concept or instance can be cached. In this paper, we formalize the ontology caching as that of the following definition: will give whether an entity in Candidate(e c ) should be cached or not. We propose the SSOC (Semantic Similarity based Ontology Caching) for the task of ontology caching. Figure 3 shows the flow. training process, the cache model is learned with machine learning methods. The detail process is detailed in the following sections. 4.1 Semantic Similarity ontology. We define the similarity measure as a real-valued function: ity and symmetry: The similarity between two concepts or tw o instances can be calculated by hierarchy similarities, property similarities, la bel similarities and access similarities. 
Concepts and instances' hierarchy similarities have different meanings. Denote the hier-archy similarity of two entities as Sim h . For every two concepts, the similarity can be cal-parent concepts as ) ( B P , the hierarchy similarity of these two concepts are as follows: 
For every two instances, the hierarchy si milarity can be calculated as follows: 1. Denote the concept of instance A as ) ( A C and concept of instance B as ) ( B C . 2. Use the concept hierarchy similarity to compute this similarity between instance Concepts have properties which may have data value or point to other concepts. Denote the property similarity of two entities as Sim p . Two kinds of Properties should The property similarity of two concepts is as follows: stances have values of their properties. The same properties with the same value should be considered. Here we define the  X  X ame value X  of two kinds of properties: 1) The same value of the properties which have data values is that the two values are 2) The same value of the properties which points to another instance is that the two of the two instances can be defined as follows: 
Both concept and instance have labels that represent the textual names of the enti-ties. Denote the label similarity of two entities as Sim l . Although the textual name can be any symbols, the sharable and reusable features of ontologies make the textual entity are usually made up by one or more words. To calculate the label similarity of two entities, WordNet, a popular electronic dictionary is used to calculate the words X  name . For two words 1 w and 2 w , the formula of the similarity is as follows: Where defined as the follows: Where n is the word count in name 1 and m is the word count in name 2 . 
The access log is very important for cache. Denote the access similarity of two enti-ties as Sim a . The access similarity can be obtained by the statistics on access log. The access similarity between two concepts or instances is as follows: 
In a period of time, a client may access many concepts and instances. Define the set clients with different access patterns about entity A and entity B . They are as follows: increases and will be reduced if the number of C 2 increases. We can get little informa-tion from the number of C 3 . Therefore, the access similarity of A and B is as follows: Where ( ) x Count C is the number of x C . 
The four semantic similarities make up the whole similarity. Here the linear 
Sim Sim Sim Sim Sim Sim Sim Sim ==== , And the formula of the similarity function is as follows: that when one of them is accessed, the other will be also accessed in near future. So when one of them is accessed, the other will be loaded into the cache. Therefore, the cache model will be defined as the following formula: 4.2 Semantic Similarity Based Cached Model The next task is to determine the parameters of , 1, 2, 3, 4 k xk = and  X  . A search algo-algorithm should have the following features: 1) The algorithm can get the approximate optimal solutions of the parameters. 2) The algorithm must be convergent in definite iteration steps. 
After the , 1, 2, 3, 4 k xk = and  X  are determined, the cache model is generated. The cache model is the final output of the approach flow. 4.3 The Approach Flow ogy base. After similarity calculation, every sub similarity of two entities is calculated and ready for training. In the initial step, the  X  X ache X  and  X  X ot cache X  label is assigned specific period. We consider one implementation of the proposed approach. We employ a unified machine learning approach in caching pred iction. The whole algorithm of the model the model and the model stores the cache information in the following format: accessed. The feedback process alters the cache result and refines the training corpus. 
Genetic Algorithms (GAs) are adaptive heur istic search algorithm premised on the evolutionary ideas of natural selection and genetic [10]. The improved genetic algo-selected to train the parameters of the cache model. The training process includes two cess log of the ontology server. The features for concepts and instances are computed and discretized for the improved genetic algorithm process. Secondly, each feature random method to determine whether the feature means cached or not. For every feature, we generated fifteen random numbers range in [0, 1]. If the numbers ranged lowing format: 
The feedback results are used to adjust the parameter values. The feedback results can be the approval or decline of the existing training data. An example is as follows: crossover, mutation and stop conditions are important parts of the algorithm. The individuals are represented in the following format: There are many crossover methods in genetic algorithm. Here the arithmetical cross-over is used. For two individuals as follows: Generate a random number  X  whose range is from 0 to 1. The crossover operation will generate two new individuals as follows: example we select 3 x . Here denote 3 x  X  X  possible minimal value as value as x  X  . The formula is as follows: will be as follows: 1234 {, , , ,} m sxxxx  X   X  X  = . 
There are two conditions to end the search process. One is that the fitness of an individ-ual is higher than a given value. The definition of an individual X  X  fitness is as follows: rect if one of the following conditions is matched. Denote the correct row number of the training data as correct Count . Denote the whole fitness the algorithm can stop. Currently, exp ect fitness is given manually. The other is that the generation has reached the given generation. It can be defined as follows: 
So the stop condition can be summarized as follows: 
The model has decided what entities should be loaded to the cache when a certain entity is accessed. And when the server is running, the model makes the cache work to process the clients X  requests. 
The feedback process is us ed to adjust the model. The cache results and the access information are shown to administrators fo r evaluation. The administrator can decide whether the result of each cache element correct or incorrect. The result will be added to the corpus. And the model can be refo rmed by beginning the training process again. The new model can be used to refresh the cache. The cache approach has been used in project SWARMS (http://keg.cs.tsinghua.edu.cn/ tology server in the experiment contains 140 concepts, 116 relations and 6700 instances. 
We have two kinds of measures. One measure is the consume time of a sequence of queries. We define the time consume for processing n queries as T n . For a specific n , the lower T n , the better performance of the algorithm. The other measure is the hit rate of a sequence of queries. The hit rate is defined as H n . For a number of queries n , if the number of hits is t . H n can be computed by the following formula: 
We compare our cache approach to the baseline experimental results. The training corpus is constructed using 60 concepts, 40 relations and 1000 instances from the elements in the ontology server. The stop conditions of the improved genetic algo-rithm are given as follows: The training result of the parameters is as follows: 
The cache replacement strategy is Least Recently Used. If the cache has been be loaded. The ontology access is simulated by eight distributed clients. The simu-late algorithm notices the different importance of concepts and instances. The con-cepts and instances have different access probabilities. Some of them contain im-portant knowledge of the domain while some of them contain trivial knowledge. others. Figure 4 shows the different importance of an ontology. 
Assume that the probability of accessing entities in round shape is P . So the prob-ability of accessing entities other than the ones in round shape is 1 P  X  . An entity in the server is given a unique number. Client randomly produces a number from 0 to 1, denote it as D . If DP  X  , it will randomly select an entity in round shape to access. If DP &gt; , it will randomly select an entity in diamond shape to access. When = , it means the entities are accessed with equal probability. We take the cache size as 800 entities. So the ratio of the cache size to the whole entities is as follows: Ratio cache enabled and not enabled is as follows: 
When 0.8 P = , let the ratio of the entities in round shape to the whole entities is 0.2. The comparison of time consume and hit rate for cache enabled and not enabled is as follows: 
When a portion of entities has more probability of accessibility which is reasonable in the real world, the cache algorithm will get better performance. If the entities have same probability of accessibility, the algorith m is not so effective because the under-The region is mostly a sub graph of the whole ontology X  X  graph. When the probability long in the cache. For this reason, the cache algorithm is not so effective when P=0.5 . In this paper, we have investigated the problem of ontology caching. We have defined the problem as that of classification. We have proposed a machine learning based approach to the task. Using Improved Generi c Algorithm, we have been able to make an implementation of the approach. Experimental results show that our approach can significantly outperform baseline methods for ontology caching. The approach has been applied to a research pr oject that is called SWARMS. As future work, we plan to make further improvement on the accuracy of caching. We also want to apply the caching method to other semantic web applications. 
