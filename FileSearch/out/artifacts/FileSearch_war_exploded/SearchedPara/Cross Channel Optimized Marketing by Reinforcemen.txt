 The issues of cross channel integration and customer life time value modeling are two of the most important top-ics surrounding customer relationship management (CRM) today. In the present paper, we describe and evaluate a novel solution that treats these two important issues in a unified framework of Markov De cision Processes (MDP). In particular, we report on the results of a joint project be-tween IBM Research and Saks Fifth Avenue to investigate the applicability of this technology to real world problems. The business problem we use as a testbed for our evalua-tion is that of optimizing direct mail campaign mailings for maximization of profits in the store channel. We identify a problem common to cross-channel CRM, which we call the Cross-Channel Challenge , due to the lack of explicit link-ing between the marketing actions taken in one channel and the customer responses obtained in another. We provide a solution for this problem based on old and new techniques in reinforcement learning. Ou r in-laboratory experimental evaluation using actual customer interaction data show that as much as 7 to 8 per cent increase in the store profits can be expected, by employing a mailing policy automatically generated by our methodology. These results confirm that our approach is valid in dealing with the cross channel CRM scenarios in the real world.
 I.2.6 [ Artificial Intelligence ]: Learning Experimentation customer life time value, CRM, cost sensitive learning, rein-forcement learning, targeted marketing Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00.
The issues of cross channel integration and customer life time value modeling are undoubtedly two of the most impor-tant topics surrounding customer relationship management (CRM) today. Despite the wide-spread acknowledgement of their importance, there has not been a satisfactory solu-tion to these issues in the market. In many cases, vendors provide infrastructure that enables cross channel integration of customer data, but provide no special-purpose analytics. Applying existing data analytics tools will not fully leverage the integrated data. This is because existing tools do not help optimize decision making in the respective channels for maximization of future profits across different channels. In the present paper, we propose a novel solution that treats these two important issues in a unified framework.
The proposed solution is based on our earlier work on the application of reinforcement learning to sequential cost-sensitive decision making [8]. In that paper, it was demon-strated that by combining reinforcement learning and scal-able data mining technologies, decision rules that are opti-mized with respect to long term benefits can be automati-cally generated solely from data analytics. We use variants of this basic technology to perform customer life time value modeling in a cross-channel setting, and optimize marketing actions with respect to long term, multi-channel profits.
We have conducted a joint study between IBM Research and Saks Fifth Avenue to investigate the applicability of this technology on a practical problem, which we will report on in the current paper. The business problem we elected as a testbed for our investigation is that of optimizing interac-tions between the direct mail channel and the store channel. Of the various channels of customer interactions that Saks Fifth Avenue owns and operates, such as the web, telemar-keting, direct mailing and the store, we chose to focus on the interactions between the latter two channels, based on the relative ease of evaluation and readiness of the relevant data.

The largest obstacle that we faced in our effort is one that is characteristic of a cross-channel scenario, and is of general interest to most applications involving cross-channel interactions. The problem, which we call the Cross Channel Challenge , is the lack of explicit linking between the market-ing actions taken in one channel and the responses (profits or rewards) obtained in another. This translates, in prac-tice, to very low correlations observed between marketing actions and their effects across channels. Therefore, apply-ing off-the-shelf regression methods to model the rewards as a function of various variables, is likely to produce a model that is independent of the marketing actions, thus leading to useless marketing rules.

We resolve this challenge by invoking the reinforcement learning technology, and devising a number of modifications to it. One of these modifications, based on an existing method of reinforcement learning called advantage updat-ing [3], is particularly notable. It manages to solve the cross channel challenge by focusing on learning the difference in the effects on the rewards of competing actions, thereby by-passing the accurate estimation of the noisy reward function.
We conducted experimental evaluation of the proposed methods using actual customer interaction data from Saks Fifth Avenue. The results of our in-laboratory evaluation experiments suggest that we can expect as much as 7 to 8 per cent increase in the store profits by employing targeting rules automatically produced by our methodology, as compared to the current mailing policy used at Saks. These results seem to confirm that our approach is valid in dealing with the cross channel CRM scenarios in the real world.
The rest of the paper is organized as follows. We be-gin by describing the business problem that we address, in Section 2. Section 3 presents the methodology including de-tailed descriptions of newly devised methods. Section 4 will describe the experiments we conducted and the results we obtained. Section 5 concludes with a summary.
The business problem we address is that of optimizing direct mail catalogue mailings over multiple campaigns, to maximize the effect on the profits/revenue obtained in the store channel. At Saks Fifth Avenue over 60 major di-rect mail campaigns are conducted each year. These cam-paigns vary from mailings of general store catalogues to those specific to particular product groups, such as women X  X  apparel and cosmetics. Some are seasonal campaigns, such as Christmas season campaigns. Some may involve store coupons, while others may provide information on upcom-ing sales and their contents and durations. Many of these campaign features are available for use in data analysis.
Currently, the generation of mailing list for each of these campaigns is based on a number of criteria and constraints, and is not fully automated. There are intricate issues sur-rounding the process of genera ting these mailing lists. Our goal is not necessarily a full and immediate automation of this process, but rather in demonstrating the potential use of sophisticated data analytics in assisting and improving it. As a step in this endeavor, our technical task is to an-alyze the past data and automatically generate targeting rules that can be used to construct mailing lists for future campaigns.
The problem just described is a challenging one, and turns out not to be solvable by straightforward applications of ex-isting modeling techniques. For example, the simplest solu-tion would be to model the short term profits (or revenues) in the store generated by a particular customer, say in a window of one month, as a function of various features of that customer, including the control variable of whether a given catalogue is mailed to that customer. As we elaborate in the section on experiments, this will result in a model that is independent of the control variable, thus giving no interesting information on the effect of the mailing action.
At the heart of the problem is the credit assignment prob-lem. That is, there is no explicit information in the data, linking the actions taken in the direct mail channel to the responses observed in the store channel. This would be pos-sible, if a good part of the transactions were associated with coupons issued in the outbound channel of interest, and this fact were recorded. This is rarely the case in practice, and in particular is not the situation we face in our current prob-lem.

To place further burden on the modeling task, our prob-lem setting involves events with variable length time inter-vals, i.e. the intervals between the decision points (campaign mailings) are variable in length. This adds considerable amount of noise in the data, and makes the task of mod-eling the responses of marketing actions even more difficult.
Common practice in database marketing and CRM today is to organize customer data into a table consisting of fields representing various attributes of customers and response fields, model a response field of interest as a function of those attributes, and then optimize marketing actions against the obtained model. Here we go a step further: We use time stamped sequences of such data to represent time-varying sequences of customer X  X  attributes (state) and marketing ac-tions. We then model the process of customer interactions as a dynamic process, and optimize marketing actions with respect to such a model. The technical framework we em-ploy is the so-called Markov Decision Process (MDP) model, popular in dynamic programming and reinforcement learn-ing. Here we refer the reader to the literature for detailed descriptions of theory and methods in MDP, e.g. [5, 9].
In the current application, we can use the attribute vec-tor corresponding to a customer at a given point in time to represent the state for that customer at that point in time. Cross-channel integration of data allows us to repre-sent the entire history of interactions between a given cus-tomer and the enterprise, across all channels, thereby pro-viding a unified view of the customer. The maximization of life time value of a given customer, across all channels, can then be naturally formulated as the maximization of the discounted cumulative rewards in the standard MDP termi-nology. It follows that life time value maximization in the cross-channel setting is reduced to solving for the optimum policy of the MDP with the cross-channel state representa-tion. Since the obtained policy is a mapping from customer attribute vectors to actions, it can be readily translated to generic if-then style rules for use in any rules engine.
As we mentioned in Introduction, for our problem it is necessary to extend the MDP framework to a formulation involving variable time intervals. The variable time interval MDP we consider here is identical to the standard discrete time MDP, except that every event is timed. Here we as-sume that the time at the initial state is 0, and then all subsequent events will have positive time associated with them. The process starts in some initial state s 1 at t 1 =0, and then the learner repeatedly takes actions, resulting in a sequence of action, state, reward and time quadruples, { ( as the maximization of the total discounted rewards, with the discounted factors determined as a function of the time durations. That is, it is to maximize the cumulative reward R , We note that the model we introduce here is different from and simpler than the extension of MDP to the continuous time setting e.g. SMDP proposed by [4], in that we still assume discrete time steps, though having variable interval length.

The challenge in devising a learning method in the vari-able time interval MDP is in determining how the rewards in various time intervals should be normalized, in order to lessen the effect of noise introduced by the varying interval length. For estimating the immediate rewards, it is clear that the reward received in a time interval can be normal-ized by dividing it by the time interval. For estimating the Q-value function (the expected cumulative rewards), which is the objective function in an MDP, the situation is signif-icantly more complicated. In particular, we must account for the fact that, at any learning iteration, the interval over which the discounted rewards is summed (for approximating the value function) is incremented. Furthermore, the effec-tive interval is affected by the learning rate, since a larger learning rate corresponds to assigning greater weights to the rewards received far into the future. Here we propose a nor-malization scheme in which both of these factors are taken into account in determining the effective time interval to be used for normalization. More specifically, we use an anal-ogous update rule for the normalization factor, as that for the Q-value estimate. The resulting variant of the batch Q-learning method for the variable time interval set-up, Vari-RL(Q), is presented in Figure 1. (The update rules for both Q-values, v ( k ) i,j , and normalization factors, Z ( k ) 4.2 of the pseudo-code.)
A number of past papers have addressed the problem of extending Q-learning and other related learning methods to variable time intervals and more generally to the continuous time setting, e.g. [3, 4]. Of these, the work by Baird [3] is of particular interest to us, since his solution appears to be addressing closely related problems to the one we cur-rently face: function approximation in Q-learning involving variable time intervals is often difficult due to the noise in-troduced by the varying intervals. Baird proposes a novel method, called advantage updating , which tries to learn the relative advantage of competing actions in any given state. This procedure avoids having to explicitly estimate the Q-value function, thereby bypassing the noisy estimation prob-lem. We briefly describe this method and some modifica-tions we made to make it work for our problem.

Advantage updating is based on the notion of advantage of an action a relative to the optimal action at a given state s , written A ( s, a ). The following is one of alternative defi-Procedure Vari-RL(Q) Premise: A base learning module, Base, for regression is given.
Input data: D = { e i | i =1 , ..., N } where 1. For all e i  X  D 2. For all e i  X  D 4. For k =1to K 5. Output the final unnormalized model, i.e. Q ( K )  X  Z ( K )
Figure 1: Variable time interval batch Q-learning. nitions of this quantity.
 In the above, note that V  X  is defined as V  X  ( s ) = max a where Q  X  ( s, a ) is the Q-value of an optimal policy. We used  X  t s ,and s to denote the state reached after s . Alternatively, the advantage can be written in terms of the Q -value by: This quantity is extremely interesting for us for two reasons: It factors out the dependence of the value function on the time interval, and on the state. Given this notion of advan-tage, advantage updating is an on-line learning method that learns this function iteratively, by a coupled set of update rules for the estimates for A and V , and a normalization step for A  X  ( s, a ) which drives max a A  X  ( s, a )towardszero. We exhibit a batch version of this method in Figure 2.
A couple of other modifications were necessary, before the two methods just presented, Vari-RL(Q) and batch-AU, could be made to work satisfactorily. One modification has to do with the initialization of the quantities being estimated in the two methods, the Q-value and A-value, using the em-pirical cumulative rewards observed in the data, rather than the immediate rewards as in the original on-line methods. The other modification is allowing optional applications of function approximation, e.g. applying function approxima-tion in every n -th iteration. Due to space limitations, we will omit the details of these modifications, and refer the reader to our technical report [1]. Procedure Batch-AU Premise: A base learning module, Base, for regression is given.
Input data: D = { e i | i =1 , ..., N } where 1. For all e i  X  D 2. For all e i  X  D 4. For all e i  X  D and for j =1to l i  X  1, initialize 5. For k =1to K 6. Output the final advantage model, A ( K ) .
 Figure 2: Batch reinforcement learning based on ad-vantage updating.
We evaluated the proposed methods using actual cus-tomer interaction data from Saks Fifth Avenue from the past years. Below we will describe the data we used for this experimentation in some detail. Then we will discuss the challenge we face in evaluating the methods using past data. We then describe the experimental results.
As briefly explained in Introduction, the data we used for our data analysis can be categorized into the following four types. 1. Customer data for 1.6 million customers, whose recent 2. Transaction data for the said 1.6 million customers for 3. Campaign data for the major campaigns for the year 4. Product data for the purchased items in the trans-
These data were then used to generate time stamped se-quences of feature vectors containing summarized informa-tion on the history of cross-channel interactions. The fea-tures we generated and elected to use, representing the state of a customer at any given point in time, are summarized in Table 1. (Note that the features also fall into four types according to the data sources.)
Note that as the third column in Table 1 the correlation coefficients between each of the features and the response variable (reward) are listed. Here, the response variable was calculated simply by summing the observed profits in the data, over a fixed period of time since the time of the mail-ing in question, and it does not exactly correspond to the cumulative discounted profits that we wish to maximize. It is worth noting, nonetheless, that a very low correlation is observed between the control variable (mailing action) and the response variable, as compared to some of the other fea-tures. The control variable has the third lowest correlation coefficient (at 0.008), and is magnitudes lower than those of typical (transaction and campaign) features. This explains thenatureof the cross-channel challenge , and in particular why we tend to get a model that is independent of the con-trol variable, when we run a standard regression engine to model the response variable as a function of these features.
A common problem in performance evaluation of rein-forcement learning methods is that often it is difficult or im-possible to conduct real life experiment in which the learning methods have access to on-line interactions with the MDP. Our current application domain of CRM and database mar-keting is no exception. To conduct such performance eval-uation reliably using only static historical data is itself a big challenge. The problem is that we need to evaluate the policy generated by the learning procedure using only past data, which presumably were collected using some policy that is different from it.

Here we propose a solution to this problem, and use it to conduct performance evaluation for our methodology. Our solution is based on a notion recently proposed by Kakade and Langford [6] called policy advantage , and a bias correc-tion technique based on importance sampling, c.f. [10]. We elaborate on this below.

First, we define a discrete time version of the notion of advantage introduced earlier (in Eq. 3), with respect to any policy  X  .
 Then the policy advantage of a new policy  X  with respect to an old (or sampling) policy  X  and initial state distribution several features of this type.)  X  , written A  X , X  (  X  ), is defined as follows.
 Intuitively, the policy advantage measures how much advan-tage can result by replacing the action of the old policy by that of the new policy at a random state selected by the sampling policy, while all other actions remain unchanged (specified by the sampling policy). In some sense, this mea-sure quantifies how much local improvement is attained by changing the old policy by the new policy, assuming that the overall state distribution is not significantly affected by that change. 1
The policy advantage can be estimated using only data collected by the sampling policy  X  , using a bias correction technique based on importance sampling as follows. Note that  X  ( a | s ) is known since it is the (possibly stochas-tic) policy generated by reinforcement learning, but  X  ( a | s need be estimated from the data, since we do not know the sampling policy explicitly. It should be pointed out that this quantity becomes impossible to estimate for a deterministic sampling policy, since the bias correction factor  X  ( a | s ) verges for actions a never chosen by the sampling policy  X  . In real world settings, however, the state information is often not sufficient to determine the chosen action determin-istically, as is the case in our setting.
Kakade and Langford [6] have established a theoretical re-sult which implies that a new policy with a positive policy advantage can be used to define a new policy, which prov-ably has better performance than the old policy.
We used the evaluation method just introduced, namely that of bias corrected estimation of policy advantage in the data, to validate the performance of the proposed methods. We used both of the proposed methods, Vari-RL(Q) and Batch-AU. In both cases, we used IBM X  X  scalable regression engine, ProbE TM [7, 2], as the basic regression module. For both methods, we used the feature to initialize the Q-value and A-value estimates using the empirical life time val-ues. Also, for both methods, we used the option of applying function approximation at every fourth learning iteration. In our evaluation, we randomly sampled approximately 1.0 percent of the individual customers from the entire data (ap-proximately 16 thousand customers from 1.6 million). The episodic data were then generated, for use in training, by randomly selecting a sub-episode of length 10 (consisting of 10 events) corresponding to each of the sampled individu-als. A separate test data set consisting of 5,000 randomly selected individuals was also sampled for calculating the pol-icy advantage. The policy advantage was calculated using these individual data over the entire 68 campaigns, and for each learning iteration, the results were averaged over 10 random runs.

The results of this evaluation are exhibited in Figures 3 and 4. Figure 3 plots how the policy advantage of the policy output by the Vari-RL(Q) method changes as the learning iteration progresses in a typical run. Figure 4 shows the analogous graph for the batch-AU method. In each of the graphs, the y-axis is the policy advantage shown as the per-centage over the value of the old policy. Strictly speaking, what is shown on the x-axis is not the number of iterations, but rather is the number of times function approximation is performed. In both cases, we chose to run function approx-imation at every fourth learning iteration. Figure 3: The policy advantage for the variable time interval Q-learning method (in a typical run.) Figure 4: The policy advantage for the Batch Ad-vantage Updating method (in a typical run.)
A definite trend can be read off from these graphs. For both of the methods, a typical run starts with a policy that is relatively uninformative which does not show any advantage over the sampling policy. It is worth noting that, since both methods were initialized with the empirical life time value observed in the data, this shows that direct modeling with empirical LTV does not lead to any advantage over the ex-isting mailing policy. At the third function approximation, or after 9 learning iterations, the policy advantage peaks and then it starts declining again. This behavior is thought to be attributable, in part, to the nature and limitation of the evaluation method. Policy advantage measures the ad-vantage of a new policy with respect to an old policy, using the old policy as the sampling policy. It is therefore more effective when the two policies are relatively similar. As the learning progresses and the two policies start diverging, the measure becomes less and less reliable.

Even with the limitation mentioned above, the obtained results are quite encouraging. With the assumption that the new policy does not significantly change the state dis-tribution, the results imply that as much as 7 to 8 percent increase in the store profits can be expected, by employ-ing the policy output by our methodology, over the current mailing policy used at Saks.
We validated our reinforcement learning based approach to life time value modeling and cross-channel optimized mar-keting on a real world problem. In the course of our investi-gation, we identified a general problem common to modeling cross-channel interactions, and proposed a solution based on old and new techniques of reinforcement learning. We also provided a solution to the sampling bias problem in evalua-tion of learned policies, and used it to evaluate the proposed approach. Some issues for future investigation include the following: (1) Easing deployment by reducing the need to customize; (2) Handling various channel constraints includ-ing budget constraints; We wish to thank Bill Franks and Sheri Wilson-Gray of Saks Fifth Avenue for their executive leadership in making the joint project possible. We also wish to thank Edwin Pednault and Bianca Zadrozny of IBM Research and John Langford of TTI at Chicago for helpful discussions and as-sistance. [1] N.Abe,N.Verma,C.Apte,andR.Schroko.Cross [2] C. Apte, E. Bibelnieks, R. Natarajan, E. Pednault, [3] L. C. Baird. Reinforcement learning in continuous [4] S. Bradtke and M. Duff. Reinforcement learing [5] L. P. Kaelbling, M. L. Littman, and A. W. Moore. [6] S. Kakade and J. Langford. Approximately optimal [7] R. Natarajan and E. Pednault. Segmented regression [8] E. Pednault, N. Abe, B. Zadrozny, H. Wang, W. Fan, [9] R. S. Sutton and A. G. Barto. Reinforcement [10] B. Zadrozny. Policy mining: Learning decision policies
