 Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms: Algorithms, Experimentation Keywords: test collections, learning to rank, web search
Document corpora are key components of information re-trieval test collections. However, for certain tasks, such as evaluating the effectiveness of a new retrieval technique or estimating the parameters of a learning to rank model, a corpus alone is not enough. For these tasks, queries and rel-evance judgments associated with the corpus are also neces-sary. However, researchers often find themselves in scenarios where they only have access to a corpus, in which case eval-uation and learning to rank become challenging.
 Document corpora are relatively straightforward to gather. On the other hand, obtaining queries and relevance judg-ments for a given corpus is costly. In production environ-ments, it may be possible to obtain low-cost relevance infor-mation using query and click logs. However, in more con-strained research environments these options are not avail-able, and relevance judgments are usually provided by hu-mans. To reduce the cost of this potentially expensive pro-cess, researchers have developed low-cost evaluation strate-gies, including minimal test collections [2] and crowdsourc-ing [1]. Despite the usefulness of these strategies, the result-ing relevance judgments cannot easily be  X  X orted X  to a new or different corpus.

To overcome these issues, we propose a new method to reduce manual annotation costs by transferring relevance judgments across corpora. Assuming that a set of queries and relevance judgments have been manually constructed for a source document corpus D s , our goal is to automatically construct a test collection for a target document corpus D by projecting the existing test collection from D s onto D
The goal of projecting test collections is not to produce manual quality test collections. In fact, it is assumed that projected test collections will contain noisy relevance judg-ments (i.e., ones which humans are unlikely to agree with). The important question, however, is whether these noisy projected judgments are useful for training ranking models in the target corpus.

As mentioned earlier, the input to our proposed model consists of a source document corpus D s along with a set of existing queries and relevance judgments for that corpus. It is assumed that these judgments are provided in the form of  X  X elevance X  ( Q, D, R ) tuples, i.e., (query, document, rele-vance). Our goal is to leverage the existing judgments for corpus D s and create tuples of the same form for the target corpus D t for the same set of queries. In this way, we use the same set of queries across test collections, but find new documents and estimate their relevance in the target corpus.
We group existing tuples based on their Q field and per-form the procedure described below for each of the groups separately. In other words, we project the relevance judg-ments for each query separately.

Using a set of features extracted from existing relevance tuples for a query, we train what we refer to as a supervised  X  X elevance classifier X . This classifier is capable of predicting relevance scores for a given document with respect to the query for which the classifier is designed. It is important to note that the type of features used to train the classifiers as well as the learning model have a significant impact on the performance of the relevance classifiers. Therefore, it is important to choose an expressive set of features, such as those commonly used in learning to rank models.

Once a classifier is trained over the source corpus for a query, the next step is to find a set of documents from the target corpus that can be labeled by the relevance classifier. Unlike the pooling method, this set of documents does not necessarily have to be the output of a given retrieval system. However, for convenience, we use BM25 to retrieve 1000 candidate documents per query for automatic labeling.
The final task is to assign relevance labels to the set of doc-uments found in the target corpus D t . Even though research has shown the usefulness of graded relevance judgments, it makes the most sense for our relevance classifiers to assign binary judgments (i.e.,  X  X elevant X  and  X  X on-relevant X ). How-ever, it might be desirable for each label to have an associ-ated confidence score. These scores may be informative for evaluation or learning to rank.

After repeating the above steps for all queries from a source corpus, a complete set of relevance tuples is con-structed for the target corpus. It is important to note that since projections are carried out per query, there is no re-striction on the number of source corpora that can be used to construct test collections for a target corpus. That is to say, a set of relevance judgments can be constructed for a query, given the relevance classifier designed for that query,
S ource Table 1: Effectiveness (ERR@20) of training on the projected ( proj ) test collections versus the baseline models ( base ). regardless of the source corpus. This provides a mechanism for accumulating projected judgments from different sources for a fixed target test collection. We performed experiments using three TREC datasets: Wt10g (1.6m pages, topics 451 X 550), Gov2 (25m pages, top-ics 701 X 850), and the first English segment of ClueWeb09 (50m pages, topics 1 X 100). Title queries are used in all cases.
Our relevance classifier uses a maximum entropy model and is trained on a feature set consisting of basic information retrieval scores (language modeling and BM25 scores), and term proximity features (exact phrases, ordered windows, unordered windows, etc.).

The criterion we use to determine if our proposed ap-proach is successful or not is whether a learning to rank model trained using a projected test collection over the tar-get corpus can achieve better performance than a learn-ing to rank model trained on the source corpus and tested on the target corpus. If this is the case, then we have shown that our projection method can be used to effectively transfer judgments across corpora. The effectiveness of the learned models is measured using manual judgments from each TREC test collection described above, using Expected Reciprocal Rank (ERR). To determine if differences between the various models are statistically significant, we utilize a one-side paired t -test.

We use a relatively straightforward learning to rank model in our experiments: an iterative greedy feature selection strat-egy that directly optimizes ERR to produce a linear ranking function, similar to the one described by Metzler [3]. Note that our ranking model shares the same features as the rel-evance classifier. This is a deliberate choice, but the alter-native approach (disjoint features) may also have merits.
Table 1 shows the effectiveness (ERR@20) of the models trained using the projected test collections. It also illustrates the effectiveness of the baseline systems across different cor-pora. As described previously, the baseline is a ranking model trained using manual judgments from the source cor-pus and evaluated using the target corpus X  X n other words, no domain adaptation (but otherwise using exactly the same features and same learning method). The differences are not statistically significant in most cases, which means that rel-evance projection provides little benefit. However, the pro-jected conditions fared no worse either, which suggests that despite the noisy projection process, relevance signals from the source corpus are largely preserved in the target corpus.
However, an opportunity that our proposed method cre-ates is the ability to project judgments from multiple sources onto a single target corpus. Table 2 shows the effectiveness of projecting from two different sources onto each target cor-pus. Here, the baseline is simply joining the feature vectors
Sources  X  Target Baseline Projection (1) Gov2,Wt10g  X  ClueWeb09 0.091 0.101  X  (2) ClueWeb09,Wt10g  X  Gov2 0.174 0.182  X  (3) ClueWeb09,Gov2  X  Wt10g 0.122 0.127 Table 2: Effectiveness (ERR@20) of training on pro-jections from multiple sources. from the two source collections to train the model, and then testing on the target corpus.

Results show that the somewhat na  X   X ve baseline of taking the union of training data from the two sources works rather poorly X  X orse than simply training on the  X  X etter X  of the two sources alone. However, it is important to note that in a real-world scenario, we wouldn X  X  have access to test data in the target corpus, so there would be no way of knowing which source was  X  X etter X .
 We see that models learned by projecting from Gov2 and Wt10g onto ClueWeb09 (1) and from ClueWeb09 and Wt10g onto Gov2 (2) are significantly better than the baseline (as denoted by asterisks in Table 2). In the case of (1), the projection condition is significantly better than training on Gov2 alone and testing on ClueWeb09, but not true for Wt10g. In the case of (3), the projection condition is signifi-cantly better than training on ClueWeb09 alone and testing on Wt10g, but not true for Gov2.

Although the results are somewhat inconclusive, we find that our proposed approach for projecting relevance judg-ments yields reasonable quality training data in the target corpus. Our method also allows more than one source to be projected onto the same target, and hints that more, but possibly lower-quality, training data in some cases leads to significant improvements.
We proposed a classification-based approach to transfer existing relevance judgments from one or more source cor-pora to a target corpus. The resulting projected test collec-tion, which contains the same queries as the source, can be used for evaluation and to train learning to rank models. Ex-perimental evaluations show that when projecting multiple source corpora onto a target corpus, it is sometimes possible to obtain significant improvements. Although the results are somewhat inconclusive, this finding is nevertheless in-teresting, and encourages further study in a few directions: different classifiers, different feature sets, and a different set of candidate documents in the target corpus on which the classifier is applied. [1] O. Alonso, D. Rose, and B. Stewart. Crowdsourcing for [2] B. Carterette. Low-Cost and Robust Evaluation of [3] D. Metzler. Automatic feature selection in the Markov
