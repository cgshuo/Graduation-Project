 In this paper, we argue that the performance of content-based news recommender systems has been hampered by using relatively old and simple matching algorithms. Using more current probabilis-tic retrieval algorithms results in significant performance boosts. We test our ideas on a test collection that we have made publicly available. We perform both binary and graded evaluation of our algorithms and argue for the need for more graded evaluation of content-based recommender systems.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Soft-ware X  performance evaluation Experimentation, measurement, performance Recommender systems, information retrieval, evaluation, language modeling, probabilistic IR, news recommendation
During the first quarter of 2007 more than 59 million people (37.6% of all active Internet users) visited over 2000 available news-paper Web sites in the US alone [ 6, 17]. This number has been steadily increasing over the past decade and shows the growing ap-peal of reading news online. Many newspapers post at least a sub-set of their hard-copy articles online and, depending on online sub-scription schemes, sometimes all of them. One of the advantages of adding articles to a newspaper X  X  website is that the online versions can be augmented with extra information, such as links to recom-mended related articles. Finding these related articles is currently usually a time-consuming job with editors manually searching for related articles.

Recommending related online content from the same website or domain is not only a useful functionality for newspaper websites. Co pyrigh t200 7ACM 978-1-59593-730-8/07/001 0... $ 5.00. from a text categorization perspective in their book recommender system [ 13 ], so they gathered a labeled data set. Document cluster-ing has also been used for recommendation, as related documents are likely to be found in the same cluster [ 3 ]. Clustering can be re-garded as a particular instance of similarity-based IR methods, and requires an unlabeled document set as training material and query-document set pairs for evaluation [ 16 ]. Collaborative filtering has also been used often in news recommendation [ 12 ] and has proven successful on a very large scale [ 8 ]. In our scenario we do not have user-preference data, so our situation is likely to benefit most from a topic-centric IR approach.

Even though we approach recommendation as an IR problem and as such use document similarity to find related documents, we can-not use regular test collections. IR test collections based on news articles have been used in the past, e.g. in the Ad Hoc tracks of TREC 1-5 [ 20 ], containing short queries, such as  X  X hat progress has been made in fuel cell technology? X , coupled with sets of re-lated articles. In contrast, our recommendation task requires the full documents to be labeled as related or unrelated to other full docu-ments. We therefore chose to create our own collection using the Reuters RCV1 collection. This collection contains 806,791 news articles published between August 20, 1996 and August 19, 1997.
Our approach to creating the test collection was different from [ 16 ] in that we did not use a combination of metadata and relevance feedback to create the queries and their corresponding relevance judgments. Instead, we used complete documents as our queries and aimed to find the related documents for those focus articles . We specifically focused on relatedness instead of relevance: the two concepts are likely to be correlated, but we do not assume them to be identical.

Based on the TREC pooling approach [ 20 ], we used three differ-ent IR algorithms to create a pool of potentially relevant documents for 50 query articles that we randomly selected from the  X  807K ar-ticles. For each query article, the document rankings from the three IR algorithms were merged and the top 100 results were selected to be judged. In accordance with the TREC pooling procedure, the non-judged documents were considered irrelevant. We invited colleagues to participate in judging. The only difference with the TREC pooling approach is that here each document was judged by only one person. After a short briefing on our news recommen-dation scenario, participants were asked to judge the relatedness between each of the 100 recommended articles and the focus artice on a 5 point scale. A score of 0 meant the articles were not related; 1  X  slightly related; 2  X  fairly related; 3  X  very related; and 4  X  highly related. We did not ask participants to take temporal aspects into account when judging the relatedness of two articles.
An analysis of earlier work on content-based recommendation from an IR perspective reveals that relatively old or simple re-trieval models have been used, such as simple keyword match-ing [ 6 ]. More than half of the news recommenders Montaner et al. compare in their 2003 paper use some form of the vector space model with tf  X  idf weighting [ 12 ]. More recently, Ha also reports using tf  X  idf [ 10 ]. More advanced algorithms such as probabilistic IR models or language modeling have hardly been used, with [ 11 ] as a notable exception. Lavrenko et al. used language modeling to predict which stories are likely to influence the financial markets.
In our experiments we have compared two retrieval models that have been underutilized in content-based recommendation to tf  X  idf . The first algorithm is the Okapi retrieval function ( okapi ), which has been proposed as an effective retrieval formula that represents it harder to compare systems objectively. Binary evaluation is of-ten mentioned in the literature [ 2 , 13 ]. We suspect this is because (especially implicit) user feedback is harder to translate to graded relevance judgments than to binary labels. In the binary evaluation of the experiments described in this paper, we converted the docu-ment relatedness ratings to a binary relatedness scale. Preliminary experiments showed that a good threshold was to regard articles rated 3 or 4 as related, and lower scores as unrelated. This resulted in an average of 31.7 related documents per focus article.
We used the mean uninterpolated average precision (MAP) mea-sure to perform the binary relevance evaluation. MAP is the mean of the precision scores obtained after each relevant document is re-trieved, using zero as the precision for relevant documents that are not retrieved.

Graded evaluation is used by [ 6 ] and [ 13 ] among others. The attractiveness of performing graded evaluation lies in the fact that relevance (or relatedness) is not simply a binary concept: recom-mendation relevance occurs in different gradations. Because we collected our judgments on a 5-point graded scale, we also per-formed graded evaluation by correlating the gold standard ranking with the system X  X  output. Popular measures of rank correlation are Spearman X  X  rank correlation and Kendall X  X  tau. We used Kendall X  X  tau because the distribution of this statistic has slightly better sta-tistical properties [ 7 ]. However, in almost all situations the values of Spearman X  X  rank correlation and Kendall X  X  tau are very close and will lead to the same conclusions. We compare MAP and Kendall X  X  tau in section 6.3 .
We compared the three recommender algorithms by perform-ing basic document retrieval using each of the 50 focus articles as queries. The only metadata we included in the article representa-tions were the title and the body of the article. We experimented with adding other metadata (e.g. location, author, and topic codes) to the article representations, but both weighted and unweighted these additions did not produce significant performance gains.
Our baseline tf  X  idf system achieved a MAP of 0.6136, but okapi had a significantly higher score of 0.7016 ( p&lt; 0 . 001 ), an im-provement of 14.33%. LM also improved significantly over tf  X  idf ( p&lt; 0 . 013 ) with a score of 0.6973, a performance gain of 13.63%. This is consistent with the reported gains in the literature [ 14 ]. The difference between okapi and LM , however, was not significant ( p = 0 . 81 ). This suggests that significant performance gains can be made in content-based recommendation simply by switching re-trieval models.

Another factor to perhaps take into consideration here is the ex-ecution time, as recommendations should be generated as quickly as possible: LM was on average 5 . 5 times faster than okapi . Table 1 shows the results of the experiments with article length. Using the title and the first couple of sentences provides the biggest jump in performance compared to using only the title, but perfor-mance keeps increasing with the amount of information used. The best performances can be observed when using all of the focus and collection article text. This suggests that even though news articles tend to be written in inverted pyramid style, this does not neces-sarily mean that information can be thrown away safely for recom-mendation purposes.

In addition, the increase in performance does seem to level off
In this paper we have presented work on comparing and evalu-ating news recommendation systems. We identified and examined three elements notably absent from the news recommendation lit-erature. First, we showed that significant performance gains can be made by choosing more advanced, probabilistic retrieval algo-rithms such as language modeling and Okapi over the popular, but relatively old tf  X  idf weighting model.

We also examined the role that representation length can play in news recommendation, finding that for the probabilistic meth-ods, using more of the article offers the best performance. This seems to suggest that relying on an linear style of writing alone is not good enough: using less text is detrimental to performance. We also showed that, in contrast to the above findings, the pop-ular tf  X  idf model actually suffers from using all available text. It does, however, work better when there is little information to work with. We also showed that, in contrast, the popular tf  X  idf model ac-tually suffers from using all available text. It does, however, work better when there is little information (i.e. only the title) to work with. We expect these findings to translate well into personalized content-based recommendation.

We evaluated our experiments using both binary and graded eval-uation and both evaluation approaches turned out to be just moder-ately correlated. We argued that graded evaluation is intrinsically better, so we suggest that future evaluation of content-based rec-ommender systems should at least be evaluated using graded mea-sures. Finally, we also made available the test collection used in our experiments to facilitate experimentation on at least one fixed collection.
In extending the current study we plan to examine the relation-ship between binary and graded evaluation further by looking at other measures such as Discounted Cumulated Gain. We would also like to experiment with generating personalized article rec-ommendations. One possible way of doing this is simulating the browsing behaviour of an online reader and constructing personal recommendation profiles based on this behaviour. Other possible directions for future work are finding other collections with more metadata to exploit and combining our content-based approach with a collaborative approach to recommendation. Finally, investigat-ing the influence of other performance indicators (such as article length) on the recommendation process might also be a worthwhile direction for future research.
We would like to thank our colleagues from the ILK workgroup for helping us judge the Reuters documents on their relatedness. We would also like to thank Kirstine Wilfred Christensen for com-ments on the draft of this article. The work of Toine Bogers was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the ` A Propos project. An-tal van den Bosch is funded by NWO, the Netherlands Organisation for Scientific Research. [1] P. N. Bennett, S. T. Dumais, and E. Horvitz. The [2] D. Billsus and M. J. Pazzani. A personal news agent that
