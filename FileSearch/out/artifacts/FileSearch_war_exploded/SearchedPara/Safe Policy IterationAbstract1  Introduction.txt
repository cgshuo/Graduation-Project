 Matteo Pirotta matteo.pirotta@polimi.it Marcello Restelli marcello.restelli@polimi.it Alessio Pecorino alessio.pecorino@mail.polimi.it Daniele Calandriello daniele.calandriello@mail.polimi.it In this paper, we focus on approaches derived from policy X  X teration (Howard, 1960), one of the two main classes of dynamic programming algorithms to solve Markov Decision Processes (MDPs). Policy iteration is an iterative algorithm that alternates between two main steps: policy evaluation and policy improvement . At each iteration, the current policy  X  k is evaluated estimating the action-value function Q  X  k and the new policy  X  k +1 is generated by taking the greedy policy the best action according to Q  X  k . If, for each k , Q  X  is computed exactly and  X  k +1 is the related greedy policy, policy iteration generates a sequence of mono-tonically improving policies that reaches the optimal policy in a finite number of iterations (Ye, 2011).  X  k +1 cannot be computed exactly, approximate pol-icy iteration (API) algorithms (refer to (Bertsekas, 2011) for a recent survey on API) need to be consid-ered. Most API studies and algorithms focus on reduc-ing the approximation error in the policy evaluation step (Lagoudakis &amp; Parr, 2003; Munos, 2005; Lazaric et al., 2010; Gabillon et al., 2011), and then perform policy improvement by taking the related greedy pol-icy. However, when only an approximated value  X  Q  X  k policy space  X  is considered in the policy improve-ment step, the greedy policy may perform worse than  X  , thus leading to policy oscillation phenomena (Bert-sekas, 2011; Wagner, 2011).
 A few approaches (Perkins &amp; Precup, 2002; Kakade &amp; Langford, 2002; Wagner, 2011; Azar et al., 2012) to this problem, instead of iterating on a sequence of greedy policies computed on approximated action X  value functions, propose converging algorithms that exploit smaller updates in the space of stochastic poli-cies. The idea is that the action-value function of a policy  X  can produce a good estimate of the perfor-mance of another policy  X  0 when the two policies give rise to similar state distributions, which can be guar-anteed when the policies themselves are similar. In-cremental policy updates are also considered in the re-lated class of policy gradient algorithms (Sutton et al., 2000; Kakade, 2001; Peters et al., 2005).
 Following the approach of Conservative Policy Itera-tion (CPI) (Kakade &amp; Langford, 2002), we propose new approximate policy X  X teration algorithms (useful both in model X  X ree contexts and when a restricted sub-set of policies is considered) that produce a sequence of monotonically improving policies and are character-ized by a faster improving rate. The main contribu-tions of this paper are: 1) The introduction of new, more general lower bounds on the policy improvement. 2) The proposal of two approximate policy X  X teration algorithms whose policy improvement moves toward the estimated greedy policy by maximizing the policy improvement bounds. 3) An empirical evaluation and comparison of the pro-posed algorithms with related approaches (as far as we know, this is the first paper to present experimen-tal results with CPI). A discrete X  X ime finite Markov decision process (MDP) is defined as a 6-tuple  X  X  , A , P , R , X ,D  X  , where S is a finite set of states, A is a finite set of actions, P is a Markovian transition model where P ( s 0 | s,a ) is the probability of making a transition to state s 0 when tak-ing action a from state s , R : S X A X  [0 , 1] is the re-ward function, such that R ( s,a ) is the expected imme-diate reward for the state-action pair ( s,a ),  X   X  [0 , 1) is the discount factor for future rewards, and D is the ini-tial state distribution. The policy of an agent is char-acterized by a density distribution  X  ( a | s ) that specifies the probability of taking action a in state s . When the policy is deterministic, with abuse of notation, we use  X  to denote the mapping between states and actions:  X  : S  X  A . We consider infinite horizon problems where the future rewards are exponentially discounted with  X  (where possible, we will generalize our results to the undiscounted case  X  = 1). For each state s , we define the utility of following a stationary policy  X  as:
It is known that V  X  solves the following recursive (Bellman) equation: Policies can be ranked by their expected discounted reward starting from the state distribution D : where d  X  D ( s ) = P  X  t =0  X  t Pr ( s t = s |  X ,D ) is the unnor-malized  X   X  X iscounted future state distribution (with normalizing factor 1  X   X  ) for a starting state distribu-tion D (Sutton et al., 2000). In the undiscounted case, such term is replaced by the stationary state distribu-tion d  X  ( s ) = lim t  X  X  X  Pr ( s t = s |  X  ), that, for unichain MDPs, is unique and independent from the initial state distribution. Solving an MDP means to find a pol-icy  X   X  that maximizes the expected long-term reward:  X  least one deterministic optimal policy that simultane-ously maximizes V  X  ( s ),  X  s  X  X  . For control purposes, it is better to consider action values Q  X  ( s,a ), i.e., the value of taking action a in state s and following a pol-icy  X  thereafter: Given the action-value function Q  X  ( s,a ), we define the greedy policy  X  + as:  X  + ( s )  X  arg max a  X  X  Q  X  ( s,a ). Furthermore, we define the advantage function: that quantifies the advantage (or disadvantage) of taking action a in state s instead of following pol-icy  X  . In particular, for each state s , we de-fine the advantage of a policy  X  0 over policy  X  as A  X  ( s ) = P a  X  X   X  done in (Kakade &amp; Langford, 2002), we define its ex-pected value w.r.t. an initial state distribution  X  as A For sake of brevity, in the following we will use ma-trix notation, where I denotes the identity matrix and e is a column vector of all ones (with sizes appar-ent from context). Given a vector v and a matrix M , v T and M T denote their transpose, and, given a non-singular square matrix M , M  X  1 denotes its in-verse. The L 1  X  X orm k M k 1 of a matrix M is its maxi-mum absolute column sum, while its L  X   X  X orm k M k  X  is its maximum absolute row sum. It follows that k M k 1 = k M T k  X  . Using matrix notation, we can rewrite previous equations as follows: where J  X  D and A  X  0  X , X  are scalars, v  X  , r  X  , D , d  X  A  X  are vectors of size |S| , q tors of size |S||A| , P is a stochastic matrix of size ( |S||A| X |S| ) that contains the transition model of the process P (( s,a ) ,s 0 ) = P ( s 0 | s,a ),  X   X  is a stochastic matrix of size ( |S| X |S||A| ) that describes policy  X  :  X   X  ( s, ( s,a )) =  X  ( a | s ), and P  X  =  X   X  P is a stochastic matrix |S|  X  |S| that represents the state transition matrix under policy  X  .
 In this section we want to lower bound the perfor-mance improvement of a policy  X  0 over a policy  X  given the policy advantage function A  X  0  X  . As we will see, A can provide a good estimate of J  X  0  X  only when the two policies  X  and  X  0 visit all the states with similar prob-abilities, i.e., d  X  0  X   X  d  X   X  . The following lemma provides an upper bound to the difference between the two  X   X  discounted future state distributions.
 Lemma 3.1. Let  X  and  X  0 be two stationary policies for an infinite horizon MDP M with state transition matrix P . The L 1  X  X orm of the difference between their  X   X  X iscounted future state distributions under starting state distribution  X  can be upper bounded as follows: This bound needs knowledge of the transition model P , but often such model is not available. Furthermore, even when the state transition model is known, the bound requires the inverse of a |S| X |S| matrix, which in many applications is not practical. The following Corollary provides a (looser) model X  X ree version of the bound, where the difference between the two distribu-tions depends only on the discount factor  X  and the difference between the two respective policies. Corollary 3.2. Let  X  and  X  0 two stationary policies for an infinite horizon MDP M . The L 1  X  X orm of the difference between their  X   X  X iscounted future state dis-tributions under starting state distribution  X  can be upper bounded as follows: As a further step to prove the main theorem, it is useful to rewrite the difference between the performance of policy  X  0 and the one of policy  X  as a function of the policy advantage function A  X  0  X  .
 Lemma 3.3. (Kakade &amp; Langford, 2002) For any stationary policies  X  and  X  0 and any starting state distribution  X  : Unfortunately, computing the improvement of policy  X  w.r.t. to  X  using the previous lemma is really expen-sive, since it requires to estimate d  X  0  X  for each candidate  X  . In the following, we will provide a bound to the policy improvement and we will show how it is possible to find a policy  X  0 that optimizes its value, but first we need to introduce the following lemma: Lemma 3.4. (Haviv &amp; Heyden, 1984, Corollary 2.4) For any vector d and any vector c such that c T e = 0 , where  X  d = max i,j | d i  X  d j | .
 We can now state the theorem that bounds the policy improvement between policy  X  0 and policy  X  .
 Theorem 3.5. For any stationary policies  X  and  X  0 and any starting state distribution  X  , given any base-line policy  X  b , the difference between the performance of  X  0 and the one of  X  can be lower bounded as follows: The bound is the sum of two terms 1 : the advantage of policy  X  0 over policy  X  averaged according to the dis-tribution induced by policy  X  b and a penalization term that is a function of the discrepancy between policy  X  0 and policy  X  b and the range of variability of the ad-vantage function A  X  0  X  . In the following section we will show that this bound is tight. Finally we introduce a looser, but simplified version of bound in Theorem 3.5 that will be useful later: Corollary 3.6. For any stationary policies  X  and  X  0 and any starting state distribution  X  , the difference between the performance of  X  0 and the one of  X  can be lower bounded as follows: At each iteration i , the policy improvement step of policy iteration selects the greedy policy w.r.t. Q as the new policy for the next iteration:  X  i +1 =  X  i + . This choice is guaranteed to improve the performance at each iteration until convergence to the optimal pol-icy. Nonetheless, it may not correspond to the safest choice, since there may be other policies that are guaranteed to perform better. Things get more com-plex when approximations or restrictions to the policy space are involved, since the greedy policy may be even worse than the current policy. To avoid this problem, following the approach of CPI, we consider the class of safe policy X  X teration (SPI) algorithms. These algo-rithms produce a sequence of monotonically improving policies and stop when no improvement can be guar-anteed. The idea is to implement the policy improve-ment step as the maximization of a lower bound to the policy improvement (like the ones in Theorem 3.5 and Corollary 3.6). In the following, we propose two safe policy X  X teration algorithms for the exact case (value functions are known without approximation): unique  X  parameter safe policy improvement (USPI) and multi-ple  X  X arameter safe policy improvement (MSPI). The two algorithms differ in the set of policies that they consider in the policy improvement step. 4.1. Unique X  X arameter Safe Policy Following the approach proposed in CPI (Kakade &amp; Langford, 2002), given the current policy (  X  ) and a target policy  X  (in CPI  X  is the greedy policy), we define the policy improvement update rule step as: where  X   X  [0 , 1]. It can be easily shown that if A  X   X  ( s )  X  0 for all s , then  X  0 is not worse than  X  for any  X  (such condition always holds when  X  =  X  + ). By taking  X  b =  X  , the value of  X  that maximizes the lower bound in Theorem 3.5 is provided by the following Corollary. Corollary 4.1. If A  X   X , X   X  0 , then, using  X   X  =  X   X   X  1 the following policy improvement is guaranteed: and when  X   X  &gt; 1 , we perform a full update towards the target policy  X  with a policy improvement equal to the one specified in Theorem 3.5.
 Remark 1 (Comparison with the policy im-provement guaranteed in CPI.) Using the nota-tion introduced in this paper, a slightly improved ver-sion of the bound on the guaranteed policy improve-ment of CPI (refer to Corollary 4.2 in (Kakade &amp; Lang-ford, 2002) or Corollary 7.2.3 in (Kakade, 2003)) can be rewritten as: The only difference between such bound and the one of USPI (see Corollary 4.1) is in the denominator. Since
 X   X   X   X   X   X   X  A  X   X   X  4 1  X   X  , the improvement guaran-teed by USPI is no worse than the one of CPI. From the tightness of CPI bound, it follows that also USPI bound is tight. In general, the difference between the two approaches can be much larger whenever  X  is not completely different from  X  (i.e.,  X   X   X   X   X   X  &lt; 2) and/or the values of the advantage function are not spread from the theoretical minimum to theoretical maximum (i.e.,  X  A  X   X  &lt; 2 1  X   X  ). In particular, using pol-icy iteration algorithms without approximation, where  X  is the greedy policy  X  + , as the sequence of poli-cies approaches the optimal policy, the discrepancy between the current policy  X  and the greedy policy  X  + decreases and so happens for the advantage val-ues A  X  +  X  , thus allowing USPI to guarantee much larger improvements than CPI (whose convergence is asymp-4.2. Multiple X  X arameter Safe Policy The USPI approach aims at finding the convex combi-nation between a starting policy  X  and a target policy  X  that maximizes the bound on the policy improve-ment. In this section, we consider a more general kind of update, where the new policy is generated using dif-ferent convex combination coefficients for each state:  X   X  ( s )  X  [0 , 1] ,  X  s . When per-state parameters are ex-ploited, the bound in Theorem 3.5 requires to solve two dependent maximization problems over the state space that do not admit simple solution. Therefore, to compute the values  X  ( s ) that maximize the policy im-provement in the worst case, we consider the simplified bound from Corollary 3.6.
 Corollary 4.2. Let S  X   X  be the subset of states where the advantage of policy  X  over policy  X  is positive:  X  = { s  X  X | A The bound in Corollary 3.6 is optimized by taking  X  s  X  S  X   X  , where k  X  (  X | s )  X   X  (  X | s ) k 1 = P a  X  X   X  ( a | s ) | and  X   X  is the value that maximizes the follow-ing function: Remark 2 (Computing  X   X  ) Differently from USPI, the coefficients of MSPI cannot be computed in closed form due to their dependency from  X   X  , whose value requires the maximization of a function with discon-tinuous derivative. However, the maximization of B can be computed using an iterative algorithm like the one proposed in Algorithm 1. To illustrate how the algorithm works, we consider the graph in Figure 1, where we can see that the function B is a continu-ous quadratic piecewise function, whose derivative is a discontinuous linear piecewise function (notice that all the pieces have the same slope). Since the deriva-tive is non negative at  X  = 0, and it is monotonically decreasing, B is guaranteed to have a unique maxi-mum. The discontinuity points corresponds to values of  X  for which some state s saturates its coefficient to 1, so that, for larger values  X  , the coefficient  X  ( s ) does not depend on  X  anymore, thus disappearing from the derivative whose value changes discontinuously with a is to start from  X  = 0 and to search for the zero X  Algorithm 1 Computing  X   X  for MSPI crossing value of the derivative of B by running over the values that lead to coefficient saturation. The al-gorithm stops when either the derivative of B becomes negative or all the coefficients are saturated to 1 (the last return in Algorithm 1). When the derivative be-comes negative, two different cases may happen: (1) the derivative equals zero at some value of  X  (as it hap-pens in Figure 1), which is the case of the first return in Algorithm 1; (2) the derivative becomes negative in correspondence of a discontinuity without taking the value of zero (the second return in Algorithm 1), i.e., the maximum falls on an angular point of B .
 The computational complexity of Algorithm 1 is dom-inated by the cost of sorting the states according to the discrepancy between the current policy  X  and the target policy  X  , that is O ( |S| ( |A| + log |S| )). Remark 3 (Comparing USPI and MSPI). Al-though MSPI maximizes over a set of policies that is a very large superset of the policies considered by USPI, it may happen that the policy improvement bound found by MSPI is smaller than the one of USPI. The reason is that the former optimizes the bound in Corollary 3.6 that is looser than the bound in Theo-rem 3.5 optimized by the latter. Finally, notice that, following the same procedure described in Remark 1 and constraining MSPI to use a single  X  for all the states (so that the MSPI improvement is bounded by that the improvement of MSPI is never worse than the one of CPI. The exact algorithms proposed in the previous section cannot be exploited when the state-transition model is unknown. In these cases, sample X  X ased versions of the previous algorithms need to be considered in order to estimate the terms that appear in the bounds. Since accurate estimates of L  X   X  X orms ( X  A for USPI and k q  X  k  X  for MSPI ) need many samples, for approxi-mate settings we consider the following simplification of the bound in Corollary 3.6: that is obtained by maximizing k q  X  k  X  with 1 1  X   X  . In this way, the only value that needs to be estimated is
A  X  0  X , X  . Following the sampling procedure described in (Kakade, 2003), it is possible to compute  X  A  X  that is an  X  X ccurate estimate of A  X  0  X , X  . The gen-eral algorithm for the approximated versions of USPI (aUSPI) and MSPI (aMSPI) is similar to the one for CPI (see (Kakade, 2003, Algorithm 13): (1) Choose an initial policy at random. (2) Select the target policy  X   X   X   X   X   X  through the maximization of a sample-based version of the Q -function. (3) Produce an 3(1  X   X  )  X  X ccurate estimate of the average advantage:  X  A  X  . (4) If cording to the USPI or the MSPI approach) the new policy for the next iteration using the bound in Eq. 1. For instance, in the case of aUSPI, the value of the parameter  X  , to take into account the approximation the algorithm stops returning the current policy. Given that aMSPI and aUSPI optimize the same per-formance improvement bound, since aMSPI optimizes over a set of policies larger than the one considered by aUSPI, the improvement rate of the former is al-ways faster than the one of the latter. Furthermore, since the bound in Eq. 1 is never worse than the one optimized by CPI, the number of iterations of aMSPI and aUSPI are no more than the one of CPI, that is O 1 2 (1  X   X  ) 2 (refer to Theorem 7.3.2 in (Kakade, 2003)). Currently, we are not able to theoretically prove that our approximated SPI algorithms terminate in a number of iterations that is significantly less than the one of CPI, but we can state the following theorem that provides interesting insights into the converging properties of the proposed algorithms.
 Theorem 5.1. If the same target policy  X  is used at each iteration, aUSPI and aMSPI terminate after If the set of target policies used by SPI algorithms were  X  X mall X  w.r.t. the number of iterations of CPI (this may always happen by choosing small enough values), we could prove that the number of itera-tions grows linearly with the accuracy ( 1 ) instead of quadratically as in the case of CPI. Next section pro-vides empirical evidence to support such conjecture. In this section, we empirically test the algorithms pro-posed in this paper into two different domains: some chain-walk problems and the Blackjack card game. 6.1. Chain-walk domains We have chosen the simple chain walk prob-lem (Lagoudakis &amp; Parr, 2003) for its simplicity that makes the comparison with other approaches straight-forward and particular instructional. Chain walk do-main is modeled as an N -state chain (numbered from 1 to N ). Chain is traversed performing two actions,  X  X eft X  (L) and  X  X ight X  (R). Each action induces a tran-sition into the associated direction and to the opposite one with probability p and 1  X  p (in the following ex-periments p is set to 0.9), respectively. Reward +1 is assigned only when the agent enters one of the two states located at a distance of N/ 4 from the bound-aries, otherwise the reward is 0. The starting state distribution D is assumed uniform over state space. We start the analysis by considering the case in which no approximation is involved (so that  X  =  X  + ). To give an idea of how the two SPI algorithms work, in Figure 2 we compare their performance with the ones of policy iteration (PI), conservative policy iteration (CPI) and natural policy gradient (NPG) on a single run using a chain with 50 states and  X  = 0 . 9. All the algorithms have been initialized with the same starting policy. The graph shows, for each algorithm, the value of J  X  D as a function of the number of iterations. As ex-pected (since there is no approximation), PI converges to the optimal policy in only 5 iterations. At the oppo-site end, CPI (whose convergence to the optimal policy is asymptotic) has a very slow performance improving rate when compared to the other algorithms. Both SPI algorithms converge to the optimal policy in a finite number of iterations: USPI reaches the optimal policy in 274 iterations, while MSPI takes more than 1,000 iterations. The improving rate of NPG with a hand X  tuned (with a line-search strategy) learning rate equal to 0.1 is similar to the one of USPI. Figure 3 displays how the values of the convex combination coefficients change over the iterations for CPI, USPI, and MSPI (since MSPI has different  X  for each state, we plot the average of  X  ( s )). As expected, the value of  X  for CPI is always very low and decreases with iterations. On the other hand, the coefficients for the SPI algorithms start to increase when the current policy approaches the greedy one. Considering Figures 2 and 3, we can notice that the value of  X  for USPI suddenly drops twice. Such phenomena are due to a change in the greedy policy and can also being observed as a change in the performance improving rate. The faster conver-gence of USPI w.r.t. MSPI, although not theoretically proved, has been empirically verified in many different versions of the chain-walk domain obtained by vary-ing the discount factor and the number of states. We can explain this behavior by considering that USPI ex-ploits a better bound w.r.t. the one of MSPI, and, in the exact context, the advantage of choosing different convex combination coefficients for each state is not enough for MSPI (at least in this domain) to attain the same improving rate of USPI.
 Things change when approximate versions of the algo-rithms are considered. Figure 4 shows a comparison between aCPI, aUSPI, and aMSPI in the same 4 X  X tate chain X  X alk domain presented in (Koller &amp; Parr, 2000), where, assuming a uniform starting distribution, the optimal policy is RRLL. Koller and Parr (Koller &amp; Parr, 2000) showed that policy iteration, when the state-value function is approximated with a second or-der polynomial and starts from policy RRRR, oscil-lates between non X  X ptimal policies: RRRR and LLLL. Figure 4 confirms that policy iteration oscillates be-tween RRRR and LLLL which both have the same suboptimal performance. Conservative policy itera-tion (Kakade &amp; Langford, 2002) does not suffer the approximation and slowly converges (at infinity) to the optimal policy. On the other side, the proposed algorithms aUSPI and aMSPI are able to reach the optimal policy in a finite number of iterations, 49 and 61 respectively.
 In Tables 1(a) and 1(b) we compare (in 4-state and 10-state chain walks respectively) the performance of the tabular versions of aCPI, aUSPI, and aMSPI us-ing two different values for the approximation error : 0.1 and 0.2, and for the discount factor  X  : 0.5 and 0.65. As expected, the higher is the accuracy required (small values of ), the larger is the number of itera-tions needed by the algorithms to converge. Nonethe-less, it can be shown that the rate of improvement is higher for smaller values of . The reason is that low values of imply a more accurate estimate of the advantage function, thus allowing the algorithms to take larger update steps. This advantage comes at the price of significantly increasing the number of samples that at each iteration are used to obtain more accu-rate estimates of the Q  X  X unction (see (Kakade, 2003, Lemma 7.3.4)). aCPI takes much longer to converge w.r.t. both the approximated SPI algorithms and this difference gets more sensible as decreases (as conjec-tured in the previous section). aMSPI is faster than aUSPI and such advantage increases with the number of states, since aMSPI may better exploit its possibility of choosing convex coefficients independently for each state. For what concerns computational times, the dif-ference between aMSPI and aUSPI grows as the size of the problem increases. Nonetheless the complexity is dominated by the sampling procedure; in fact, in all our experiments the improvement step requires less than 1% of the per X  X teration time.
 6.2. BlackJack card game Blackjack is a card game where the player attempts to beat the dealer by obtaining a total score greater than the dealer X  X  one without exceeding 21. In this work, we consider the simplified version of the blackjack game usually used as RL benchmark (refer to (Dutech et al., 2005) for more details). The state of the game is de-fined by the sum of the cards of the player (2 to 20), the dealer X  X  faced-up card (1 to 10) and the soft hand flag (that is irrelevant when player X  X  value is greater than 11), for a total of 260 states. The player can choose between two actions: to receiver a new card ( hit ) or to stop ( stand ). The rewards are +1 for winning (+1 . 5 for blackjack),  X  1 for loosing and 0 for every hit. Re-wards have been scaled to fit the interval [0 , 1] and the discount factor has been set to 0 . 8.
 In this experiment we want to analyze the effect of searching the greedy policy within a subset  X   X  of the set of determinstic policies. In particular, we consider only two policies:  X   X  = {  X  S , X  H } . Both policies select the best action (H) when player X  X  value is greater than 19 and opposite actions for the other states (  X  S selects S and  X  H selects H). States with dealer X  X  values equal to 9 and 10 are treated in a complementary way: policy  X 
S selects H and policy  X  H chooses S. Policy  X  H has been chosen as initial policy. Figure 5 reports the per-formance of the policies obtained by aPI, aCPI, aUSPI and aMSPI algorithms using an approximation error of 0 . 01 and an estimation probability  X  of 0 . 1. While aPI oscillates between  X  H and  X  S , other algorithms do not get stuck and converge to policies that perform better than both  X  H and  X  S . Notice that aMSPI, ex-ploiting the flexibility given by the multiple convex co-efficients, converges faster and to a significantly better policy than both aUSPI and aCPI. In this section we will discuss the contributions of this paper and we will propose directions for future studies to overcome some limitations of the current approach. This paper provides three types of contributions: the-oretical, algorithmic, and empirical. The main contri-bution is the theoretical one, that consists in the intro-duction of new lower bounds to the performance differ-ence between two policies. Such results are of general interest since they can be exploited in many different contexts. Starting from these bounds we have derived some policy iteration algorithms that are of particular interest in approximate settings. Finally, through em-pirical validation we have shown how such approaches lead to significantly better performance than CPI. The proposed SPI algorithms have also some limita-tions that make their use in complex domains (very large or continuous state spaces) quite impractical. In fact, if, at each iteration, we choose as target policy the greedy policy, the algorithms need to enumerate all the states. When state enumeration is prohibitive, it is possible to restrict the search for the target policy to a subset of the policy space (as suggested in (Kakade, 2003)). Another interesting direction to address this problem consists in considering a parameterized sub-space of policies and use the bounds provided in this paper to compute a safe value for the step size to be used in a policy gradient algorithm. We are currently developing such approach for multivariate Gaussian policies in the natural policy gradient algorithm. Domains with large number of states can rise problems especially in the case of aMSPI, since, as described in this paper, it requires to compute a convex combina-tion coefficient for each state. To alleviate this issue, it is possible to consider a slightly modified version of aMSPI, where the state space is split into subre-gions (using state aggregation) and all the states in a region share the same coefficient. By changing the size of these subregions, we can generate several dif-ferent situations that range from the original aMSPI approach (no aggregation) to the aUSPI one (where all the states are associated to the same coefficient). A further research direction is to exploit the proposed bounds to perform approximate policy iteration in the off-policy case, that is when the samples have been initially collected (once for all) following some explo-ration strategy. In this case, we can use the bound in Theorem 3.5 where  X  b is the exploration strategy. Finally, it will be interesting to theoretical prove that SPI algorithms halt after a number of iterations that is significantly less than the one needed by CPI. Azar, M. Gheshlaghi, G  X omez, V., and Kappen, H. J. Dynamic policy programming. Journal of Machine Learning Research , 13(Nov):3207 X 3245, 2012.
 Bertsekas, D.P. Approximate policy iteration: A sur-vey and some new methods. Journal of Control The-ory and Applications , 9(3):310 X 335, 2011.
 Dutech, A., Edmunds, T., Kok, J., Lagoudakis, M., Littman, M., Riedmiller, M., Russell, B., Scherrer,
B., Sutton, R., Timmer, S., et al. Reinforcement learning benchmarks and bake-offs ii. In Workshop at advances in neural information processing sys-tems conference . Citeseer, 2005.
 Gabillon, V., Lazaric, A., Ghavamzadeh, M., and
Scherrer, B. Classication-based policy iteration with a critic. In Proceedings of ICML , pp. 1049 X 1056, 2011.
 Haviv, Moshe and Heyden, Ludo Van Der. Pertur-bation bounds for the stationary probabilities of a finite markov chain. Advances in Applied Probabil-ity , 16(4):pp. 804 X 818, 1984. ISSN 00018678. URL http://www.jstor.org/stable/1427341 .
 Howard, R.A. Dynamic programming and Markov processes. 1960.
 Kakade, S.M. A natural policy gradient. NIPS , 14: 1531 X 1538, 2001.
 Kakade, S.M. On the sample complexity of reinforce-ment learning . PhD thesis, PhD thesis, University College London, 2003.
 Kakade, S.M. and Langford, J. Approximately optimal approximate reinforcement learning. In Proceedings of ICML , pp. 267 X 274, 2002.
 Koller, Daphne and Parr, Ronald. Policy Iteration for
Factored MDPs. In Proceedings of the 16th Con-ference on Uncertainty in Artificial Intelligence , pp. 326 X 334, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860-709-9.
 Lagoudakis, M.G. and Parr, R. Least-squares policy iteration. Journal of Machine Learning Research , 4: 1107 X 1149, 2003.
 Lazaric, A., Ghavamzadeh, M., and Munos, R. Analy-sis of a classication-based policy iteration algorithm. In Proceedings of ICML , pp. 607 X 614, 2010.
 Munos, R. Error bounds for approximate value itera-tion. In Proceedings of AAAI , volume 20, pp. 1006, 2005.
 Perkins, T.J. and Precup, D. A convergent form of approximate policy iteration. NIPS , 15:1595 X 1602, 2002.
 Peters, J., Vijayakumar, S., and Schaal, S. Natural actor-critic. In Proceedings of ECML , volume 3720, pp. 280 X 291. Springer, 2005.
 Sutton, R.S., McAllester, D., Singh, S., and Man-sour, Y. Policy gradient methods for reinforcement learning with function approximation. In NIPS , vol-ume 12, pp. 1057 X 1063. MIT Press, 2000.
 Wagner, P. A reinterpretation of the policy oscilla-tion phenomenon in approximate policy iteration. In NIPS , 2011.
 Ye, Y. The simplex and policy-iteration methods are strongly polynomial for the markov decision prob-lem with a fixed discount rate. Mathematics of Op-
