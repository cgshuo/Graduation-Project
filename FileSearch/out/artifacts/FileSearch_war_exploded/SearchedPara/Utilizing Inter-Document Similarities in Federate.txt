 We demonstrate the merits of using inter-document simi-larities for federated search. Specifically, we study a results-merging method that utilizes information induced from clus-ters of similar documents created across the lists retrieved from the collections. The method significantly outperforms state-of-the-art results merging approaches.

Federated search is the task of retrieving documents from multiple (possibly non-overlapping) collections in response to a query [1]. The task is typically composed of three steps: attaining resource (collection) description, selecting resources (collections), and merging the results retrieved from the selected collections [1]. We focus on the results-merging step; specifically, we study the merits of using in-formation induced from inter-document similarities.
While there is much work on utilizing inter-document sim-ilarities for the single-corpus retrieval setting, there is little work along that venue for federated retrieval. For example, clustering was used to transform a single-collection retrieval setting into that of multiple collections [8]. Clusters of sam-pled documents were used for performing query expansion in federated search [5]; yet, inter-document similarities were not used for results merging. Furthermore, it was shown that among the clusters created across the lists retrieved from different collections there are some that contain a high percentage of relevant documents [3]; still, a results merging method exploiting these clusters was not proposed
The only work, to the best of our knowledge, that uses inter-document-similarities for (direct) results merging is based on scoring a document by its similarity with other docu-ments in the retrieved lists [6]. We show that the method we study substantially outperforms this approach.

The method we present for merging results in federated search is adapted from recent work on fusing lists that were retrieved from the same col lection [4]. In contrast to the non-overlapping collections setting we explore here, the re-trieved lists in this work [4] were (partially) overlapping and the merging (fusion) methods used to assign initial scores to documents exploited this overlap. The adapted method that we study integrates retrieval scores assigned by a state-of-the-art results-merging approach (e.g., CORI [1] and SSL [7]) with information induced from clusters created from similar documents across the retrieved lists. Specifically, a document can provide relevance-status support to docu-ments in the same list or in other lists that it is similar to. The resultant retrieval performance is substantially better than that of using only the initial retrieval scores assigned by the state-of-the-art merging approach.
Suppose that some resource (collection) selection method was applied in response to query q [1]. We assume that docu-ment lists were retrieved from the selected (non-overlapping) collections and merged by some previously proposed results-merging algorithm [1, 7]. Let D [ n ] init denote the list of the n documents most highly ranked by the merging algorithm that assigns the (initial) score F init ( d ; q )todocument d .
Our goal is re-ranking D [ n ] init to improve ranking effec-tiveness. (Documents not in D [ n ] init remain at their origi-nal ranks.) To that end, we study the utilization of inter-document similarities by adapting a method proposed in work on fusing lists retrieved from the same corpus [4]. Let Cl be the set of document clusters created from D [ n ] init some clustering algorithm; c will denote a cluster. Then, the score assigned to document d (  X  D [ n ] init )bythe Clust method is: F F ( c ; q ) def = the same number of documents (see details below), there is no cluster-size bias incurred; Sim (  X  ,  X  ) is the inter-document similarity measure used to create Cl ;  X  is a free parameter.
Thus, d is highly ranked if (i) its (normalized) initial score is high; and, (ii) it is similar to documents in clusters c that contain documents that were initially highly ranked. In other words, similar documents provide relevance-status support to each other via the clusters to which they belong. Note that if  X  = 0, then cluster-based information is not used and F Clust ( d ; q )is d  X  X  normalized initial score. Table 1: Results. Boldface: the best result per testbed, evaluation measure, and initial merging method;  X  X  X  and  X  X  X : statistically significant differ-ences with initial and CRSC, respectively.
We conducted experiments with testbeds that are com-monlyusedinworkonfederatedsearch[1,2,5,8,7]: (i) Uni : Trec123-100col-bysource (Uniform), (ii) KM :Trec4-kmeans (K-means), (iii) Rep : Trec123-2ldb-60col (Repre-sentative), (iv) Rel : Trec123-AP-WSJ-60col (Relevant), and (v) NRel : Trec123-FR-DOE-81col (non-relevant). Titles of the TREC topics 51-150 served for queries for all testbeds except for KM where the description fields of TREC top-ics 201-250 were used. Tokenization, Porter stemming, and stopword removal were applied using the Lemur toolkit (www.lemurproject.org), which was used for experiments. To acquire resource (collection) description, we adopt the query-based sampling method from [2] which was also used in [1, 7, 5]. Following common practice [1, 7], the 10 high-est ranked collections are selected in the resource-selection phase using CORI X  X  resource selection method. As in previ-ous report [7], 1000 documents are retrieved from each se-lected collection using the INQUERY search engine. Then, the retrieved lists are merged using either CORI X  X  merging method [1] or the single-model SSL merging approach [7]. The initial score, F init ( d ; q ), assigned to d by these meth-ods is used in our Clust method.

To cluster D [ n ] init , we use a simple nearest-neighbors-based clustering approach [4]. For each d  X  D [ n ] init we create a clus-ter that is composed of d and the  X   X  1(  X  =5)documents d in D [ n ] init ( d = d ) that yield the highest Sim ( d, d ) exp igram language model induced from x with the smooth-ing parameter  X  (= 1000); D is the KL divergence; the term-counts statistics used for smoothing language models is based on the query-based sampling mentioned above.
The initial ranking induced by the CORI and SSL results-merging methods serves for a baseline. Additional reference comparison that we use is the Cross Rank Similarity Com-parison ( CRSC ) approach [6] that re-ranks D [ n ] init here by scoring d with Clust method incorporates two free parameters: n (  X  X  10 , 30 , 50 , 100 } ), the number of documents in D [ n ] init ,and  X  ( ..., 1 } ), the interpolation parameter; CRSC only depends on n . We set the free-parameter values for each method us-ing leave-one-out cross validation performed over the queries per testbad; MAP@1000 serves as the optimization criterion in the learning phase. In addition to MAP, we also present p@5 and p@10 performance numbers. Statistically signif-icant differences in performance are determined using the two-tailed paired t-test at a 95% confidence level. always outperforms the initial ranking that was induced by a state-of-the-art results-merging method; often, the im-provements are statistically significantly. This finding at-tests to the merits of integrating the initial results-merging score with information induced from clusters of similar doc-uments. Further exploration reveals that  X   X  X  0 , 1 } often yields optimal performance. This shows that the integration just mentioned yields performance that is better than that of using each of the two integrated components alone. We also see that Clust consistently outperforms CRSC, which does not utilize the initial results-merging scores, nor uses a cluster-based approach.
 Acknowledgments We thank the reviewers for their com-ments. This paper is based upon work supported in part by the Israel Science Foundation under grant no. 557/09. Any opinions, findings and conclusions or recommendations ex-pressed here are the authors X  and do not necessarily reflect those of the sponsors.
