 Nouns are more important than other parts of speech in informa-tion retrieval and are more often found near the beginning or the end of sentences. In this paper, we investigate the effects of re-warding terms based on their location in sentences on information retrieval. Particularly, we propose a novel Term Location (TEL) retrieval model based on BM25 to enhance probabilistic informa-tion retrieval, where a kernel-based method is used to capture term placement patterns. Experiments on f ve TREC datasets of var-ied size and content indicate the proposed model signif cantly out-performs the optimized BM25 and DirichletLM in MAP over all datasets with all kernel functions, and excels the optimized BM25 and DirichletLM over most of the datasets in P@5 and P@20 with different kernel functions.
 H.3.3 [ Information Systems ]: Information Search and Retrieval Term location, probabilistic information retrieval, noun
English has 5 basic sentence patterns [1, 10]: (1) Subject + Verb (e.g., Joe swims); (2) Subject + Verb + Object (e.g., Joe plays the guitar); (3) Subject + Verb + Complement (e.g., Joe becomes a doc-tor); (4) Subject + Verb + Indirect Object + Direct Object (e.g., I give her a gift); and (5) Subject + Verb + Object + Complement (e.g., We elect him president). Most English simple sentences fol-low these 5 patterns and exceptions are rare (compound and com-plex sentences can be split into simple sentences) [10], where nouns and noun-phrases are mostly located in the beginning or the end of sentences. On the other hand, past research has indicated that nouns and noun-phrases are more information-bearing than the other parts of speech in information retrieval (IR) [6, 3, 8, 12]. Therefore, in-tegrating term location information into IR may help improve re-trieval performances.

Two illustrative experiments are conducted to verify the hypoth-esis. The f rst illustrative experiment on WT2g dataset (247,491 web documents, TREC X 99 Web track) indicates nouns do concen-c trate on both ends of sentences as shown in Table 1, where is the average of the normalized distances of a set T of nouns from the middle of their sentences as def ned by Eq. 1. In Eq. 1, means the cardinality of set T (In this paper, except as explicitly noted, |x| means absolute value of x).
 w here Mid ( t ) is the middle position of the sentence that noun in, P os ( t ) is the position of t , and T is the set of nouns. Since a term may appear in a document more than once, average function avg ( . ) is used. AvgDis has a range of [ 0 , 1 ], and is closer to all nouns are gathered in the middle of sentences.

Table 1 shows that AvgDis &gt; 0 . 5 on both halves of the sen-tences, which means that the nouns are nearer to the beginning or the end of sentences than to the middle of sentences.
 End AvgDis # Nouns Avg. sent. len. # Sentences Left 0.5901 24,918,926 10.5619 14,360,676 Right 0.613 26,286,542 The second illustrative experiment on Har d Times by Charles Dickens [2] illustrates that relevant terms are more likely located in the beginning or the end of than in the middle of sentences as shown in Table 2, where Score &gt; 0 if a term is more often found near the beginning or the end of sentences, and Score &lt; 0 otherwise. To obtain Score of a term t in a document D , sentences in D partitioned into three parts, { p 1 p 2 p 3 }, where | p 1 | p | = | p 1 | + | p 3 | , and a score to t for its each occurrence in assigned by Eq. 2:
Then Score of t for all of its occurrences in D is given by Eq. 3: where t i is the ith occurrence of t in D .

Table 2 shows that the highest scoring terms "louisa X , "grad-grind X , "bounderbi X , "tom X , and "slackbridg X  turn out to be the main or minor characters in the book, whereas the lowest scoring t erms are not particularly related with the novel.

The results from the two illustrative experiments above indicate the hypothesis deserves a deeper investigation. The main contribu-tions of this paper are as follows:
Jing and Croft [6] proposed PhraseFinder to automatically con-struct collection-dependent association thesauri, and then used the association thesauri to assist query expansion and found that nouns and noun-phrases were most effective in improving IR. Liu et al. [8] classif ed noun-phrases into four types  X  proper names, dictio-nary phrases, simple phrases, and complex phrases  X  and ranked documents based on phrase similarity. Zheng et al [17] used noun-phrases and semantic relationships to represent documents in order to assist document clustering, where noun-phrases were extracted with the assistance of WordNet. Yang et al [14] used a parse tree to transform the sentences in legal agreements into subject-verb-object (SVO) representations, which are then used in conjunction with cue terms to identify the provisions provided by the sentences. However, they found that provision extraction using the SVO rep-resentation resulted in high precision but low recall, which could be due to the specif city of SVO sentence patterns and the diff culty in parsing complex sentences. Hung et al. [4] used syntactic pat-tern matching to extract syntactically complete sentences that ex-press event-based commonsense knowledge from web documents, and then semantic role labeling was used to tag the semantic roles of the terms in the sentences, such as the subject and the object. Ibekwe-SanJuan et al. [5] built f nite state automaton with syntac-tic patterns and synonyms from WordNet, which was used to tag the sentences in scientif c documents according to its category. It is diff cult to f nd syntactic patterns that are effective in all the docu-ments of a single corpus, and rule-based part-of-speech taggers are less effective in unseen text [7]. Terms were rewarded based on their locations in a document in [15, 16].
We assume that the most important terms in the documents are near the beginning or the end of the sentences. We determine the importance (relevancy) of a term t by examining its distance from the middle of its sentence in document D as def ned by Eq. 4: SL ( t, D ) is the length of the sentence in D that contains P os ( t, D ) is the location of t in the sentence. We use the average distance of t from the middle of its sentences in D if t appears more than once in D , which is def ned as r ( t, D ) by Eq. 5: where t i is the i th occurrence of t in D and tf ( t, D ) frequency of t in D . We def ne m ( t, D ) to be the average length of the sentence(s) that contain t in D as Eq. 6: where parameter  X  has a larger effect for longer sentences since it is proportional to the lengths of the sentences, and parameter a proportionally smaller effect for longer sentences since it is the same for all sentences. These two parameters are used to balance term weights in particularly short or long sentences.
In order to measure the distances of the terms from the middle of their sentences, we f t a kernel function over each sentence. We adjust the weight of each term based on its average distance to the middle of its sentences. In this paper, we explore the following kernel functions for our location based reward function RN ( t, D ) used in Eq. 9: Amo ng them, Gaussian kernel is widely used in statistics and ma-chine learning such as Support Vector Machines, Triangle kernel, Circle Kernel, and Cosine Kernel are applied to estimate the proximity-based density distribution for the positional language model [9]. Since the kernel functions are not maturely applied in IR, we also explore Quartic kernel, Epanechnikov kernel and Triweight kernel in this work. In these kernel functions, r and m are def ned by Eqs. 5 and 6, respectively. With these kernel functions, the num-ber of terms that are given maximum reward decreases as m ( t, D ) increases.
In this experiment, we use the BM25 weighting model as our base weighting model. BM25 is def ned as follows: k , k 3 , and b are tuning parameters for BM25, qtf ( t ) is the fre-quency of t in the query, | D | is the number of terms in is the average length of the documents in the collection, number of documents in the collection, and n ( t ) is the number of documents in the collection that contain t . We modify T F ( t, D ) account for the reward given to the terms based on their locations in the sentences. We def ne the Term Location score (TL) as follows: where K T L = k 1  X  1  X  b + b  X  | D | We integrate our model into BM25 to form the Term Location Score (TEL) as follows: T EL ( t, D ) = ((1  X   X  )  X  T F ( t, D ) +  X   X  T L ( t, D ))  X  IDF ( t ) where  X  controls the contribution of our model. We conduct experiments on f ve standard TREC collections: WT2G, DISK4&amp;5, WT10G, BLOGS06, and GOV2. These datasets vary in both size and content, where WT2g contains 247,491 general Web documents (TREC X 99 Web track), DISK4&amp;5 is comprised of 528,155 newswire documents from sources such as the Finan-cial Times and the Federal Register (TREC X 97-99 Ad hoc track), WT10G has 1,692,096 general web documents (TREC X 00-01 Web track), BLOGS06 consists of 3,215,171 feeds from late 2005 to early 2006 with associated permalink and homepage documents (TREC X 06-08 Blog track), and GOV2 holds 25,178,548 documents crawled from .gov sites (TREC X 04-06 Terabyte track).

We compare our model against the following weighting models when they perform best on each dataset with parameters obtained as follows: 1. BM25, with k 1 = 1 . 2 and k 3 = 8 . We adjust b in the range 2. DirichletLM. We adjust  X  in the range of [100, 3000] in steps The proposed model T EL uses the same values as BM25 for k k , and b , and sets  X  = 0 . 2 ,  X  = 3 , and  X  = 3 for all datasets. In the future, we would study the optimal values of the model pa-rameters and their relations with the characteristics of the datasets. We use the TREC off cial evaluation measures in our experiments, namely the topical MAP on BLOGS06 [11], and Mean Average Precision (MAP) on all the other datasets [13]. To stress the top re-trieved documents, we also include P@5 and P@20 as the evalua-tion measures. All statistical tests are based on Wilcoxon Matched-pairs Signed-rank test.

The experimental results are presented in Table 3. To illustrate the performance differences graphically, we plot the results in Fig-ures 1 and 2. As shown by the two f gures, our model TEL outper-forms optimized BM25 and DirichletLM in MAP over all datasets with all kernel functions, and outperforms the two optimized base-line models over most of the datasets in P @5 and P @20 with different kernel functions. The performance improvements of our model TEL against DirichletLM are greater than those against BM25. According to the two f gures, each kernel function has its advantage on some datasets. There is no single kernel function that outper-forms others on all the datasets.
In this paper, we extend BM25 and reward the terms based on their locations in the sentences with kernel functions. Experimental study shows that the proposed model performs signif cantly better than BM25 and DirichletLM on MAP over all datasets, and signif-icantly better on P@5, p@10, and p@20 over most datasets.
In the future, more experiments will be conducted to further in-vestigate the proposed model. We would investigate non-symmetric kernel functions and kernel functions with negative values since the placement of the terms at the beginning of the sentences is different from that at the end of the sentences as indicated in the f rst illustra-over BM25 and DirichletLM, respectively. tive experiment. It is also worthwhile to analyze the optimal values of the model parameters and their relations with the characteristics of the datasets. Different term proximity measures would also be explored to improve the performance of our model.
This research is supported by the research grant from the Natu-ral Sciences &amp; Engineering Research Council (NSERC) of Canada and NSERC CREATE Award. We thank anonymous reviewers for their thorough review comments on this paper.
