 We present a novel approach to mining dependency rules that explain the scenes present during a video sequence. The approach first characterizes activities based on their most important events. Next, an HMM-based approach finds the mixture components that best describe the clustering de-pendencies between events and activities in video data. The dependencies among activities are taken as association pat-terns with temporal precedence and analyzed using their co-occurrence relationships in time windows. This technique is meant to understand the multiple actions taken in a video or to predict future occurrences of certain activities. H. [ Information Systems, Information Storage and Retrieval, Content Analysis and Indexing ]: Abstract-ing methods Algorithms, Measurement, Performance, Theory
In this work, we address the problem of explaining the activities in a sequence of frames in a video by providing meaningful rules that unearth the complex spatio-temporal dependences among activities. Consider the frames shown in Figure 1, which contains multiple activities regulated by a red traffic light in the center of every frame. An object mov-ing over consecutive frames yields a set of important points that represent intense activity within a spatio-temporal re-gion. Similar groups of points are grouped forming topics (types of activities) G 1 , G 2 , and G 3 . Mining the sequence of topics over different frames to find dependencies between them may answer complex questions about the video such as:  X  X hould a car turning left receive a ticket? X ,  X  X hat is likelihood that a person moving from left to right in the Figure 1: A sequence that contains three types of activities G ,G 2 , and G 3 in different frames. Only moving objects yields activities based on events, represented as points in each figure. We want to find groups of objects performing the same activity, represented as points with different colors. frames represents a dangerous activity? X , and  X  X oes the cur-rent scene corresponds to a red light scenario? X . As an ex-ample, the following rule can be extracted from the video to answer the last question. { #cars moving from up to down  X  [4, 6), #cars turning right  X  [1,5) }  X  { #people walking from right to left  X  [1, 4) }
The above rule suggests that a strong relationship exists between cars going straight, some people crossing the street from left to right, and relatively few cars starting to turn right because a red light is active. Though humans can discern the logical relationships among those activities, be-casue humans are slow and expensive, it would be beneficial to automate the machine understanding of which rules are relevant for a video. We could use these types of rules to un-derstand the dependencies between activities and to identify unusual situations. Previous research has considered events to characterize activities. We also consider video activities to be sequences of events that have some physical interpreta-tion, but with a different representation based on gradients in spatio-temporal neighborhoods. In this paper, we define an activity as an action performed by a moving object such as walking, running, or turning left; we use the term event to denote an important moment during an activity.
In this section, we introduce the concept of motion grids as a succinct representation of an activity in a video. In a motion grid each region in the grid has a probability of containing an event that is part of the activity. We ob-serve that, in general, similar types of activities will have events similarly distributed on a grid, so we use grids to un-derstand which regions are more active during an activity. Since the information contained in a grid mainly depends on how events are distributed within regions, we first explain the notion of an event as used in this research and then study how events are used to define a motion grid .
An event is an important moment in an activity. For different moving objects performing the same activity, the events will be similar because intense motions are similarly distributed [5]. This property may be difficult to obtain in a video sequence due to changes of illumination, scale, and a dynamic background. Hence, the detection of events needs to be robust against noise. In previous research, events have been defined as key frames, changes in velocity, or changes in curvature of motion trajectories [2]. However, these de-scriptors are sometimes not stable enough to represent an activity. We use the notion of spatio-temporal gradients introduced by Laptev and Lindeberg in [4] because they demonstrated that it provides more stable results than other features such as textures and lines [5].

Given a point p ( x 0 ,y 0 ,t 0 ) placed in the position ( x of the image j contained at the frame t 0 , we are interested in detecting any point p whose local variations in all directions ( x , y , and t ) is intense. The variations of p ( x 0 ,y directions x , y and t are represented by the first order spa-tial and temporal derivatives (  X  x ,  X  y , and  X  t ) of the image intensity i p at point p . The values of  X  s and  X  t circumscribe the extension of the spatio-temporal neighborhood around the point p , and g is the spatio-temporal Gaussian kernel centered at p . The second moment matrix is used to com-pute the variations of the points p in spatial and temporal directions.
Thus, events correspond to local maxima in  X  above some threshold value. A good approximation to finding these points is the Harris corner function [4] which combines the determinant and the trace of  X  by: .
 Positive values of H ( u ) correspond to gradient points with high variation of intensity. As we are interested in signifi-cant gradients, we consider the top-k points to represent the activity while suppressing irrelevant variations. An event is then formally defined as follows.

Definition 1 (event). Let H (  X  ) be a matrix that con-tains the universe of all the possible gradients in a frame sequence v . Let  X  I ( g ) be a scalar value of a gradient g that represents its change of intensity. Formally, given a sub-set E of H (  X  ) that contains the k most important gradients, | E | = k , Events ( v,k ) = { g  X  H (  X  ) ,  X  e  X  E,g  X  H (  X  )  X  such that  X  I ( g )  X   X  I ( e ) } . 2
Intuitively, a motion grid is a 2D array that character-izes the continuous density estimation formed by the events present during an activity. Grids with higher resolution than 4  X  4 will yield more spatial information about which regions have more active behavior during the activity. Thus, the res-olution of a grid is an external parameter that controls the amount of variability allowed to describe activities. While a coarse grid can detect large variations when comparing activities, a fine grid will catch slight differences.
For an input video with multiple moving objects, we track the positions of every object over a set of consecutive frames and extract a collection of events that characterizes the un-derlying activity performed by an object (as in Section 2.1). Those events are then normalized with respect to the center of the object and discretized to be placed in one of the 16 re-gions. The locations of these points have a particular spatial distribution with respect to the type of activity performed In other words, for objects performing similar activities, the values in the motion grid will be similar. Thus, the pro-posed motion grid is a two dimensional structure defined as follows.

Definition 2 (MOTION GRID). Let X  X  consider a set of events normalized with respect to the center of a moving object found in a set of consecutive frames. A is a squared two dimensional matrix where each entry contains the num-ber of events found in region A [ i,j ] after discretization. A motion grid is defined by the matrix M with entries deter-Note that the sequence of entries found in a motion grid M n  X  n = ( m k ) n  X  n k =1 constructed by Definition 2 forces that P k =1 m k = 1. This is important to interpret every motion grid as a random variable and every entry as a measure of how likely an event is to occur in that region.
In this section, we propose a model to generate rules to interpret motion activities in video data. This model is a technique that discovers complementary information in two steps. The first step learns topics by discovering the depen-dencies among events that describe an activity. The second step interprets a sequence of frames as a whole by analyzing the co-occurring topics found in the first step. While the first step learns significant activities in a video, the second step discovers rules that explain which activities frequently appear together.

For the topic detection stage, a motion grid represents the activity of a moving object based on events. A row-wise traversal of the grid generates an observed sequence, which is assumed to be generated by a HMM whose transitions be-tween hidden states model that sequence. Among the state sequences, the optimal state sequence is the one that max-imizes the probability that a given motion grid is observed by the HMM. Thus, classifying the motion grids found in a video into a group of topics can be considered as a clus-tering process which demands the choosing of an adequate number of clusters. In other words, every HMM groups sim-ilar motion grids by learning their observed sequences and modeling them into a single topic. Hence, we consider the number of topics as a variable experimentally set for each video. If we assume that topics are distributed according to a known distribution in the video, we can use this prior knowledge to approximate this number with an allocation model, as in [3].

For the rules discovery stage, we automatically discover association rules that correlate topics that appear frequently in a video. This method takes the output of the previ-ous stage and enumerates activities in a list of N frames T = { t 1 ,t 1 ,...,t N } . Every frame contains a group of topics related to moving objects doing some activity at time t However, activities do not have to co-occur in one frame. We are instead interested in finding dependencies between activities considering a sequence of frames. To do this, a sliding window size /w/ is defined to specify the maximum allowed time difference between the earliest and latest occur-rence of activities forming a scene over consecutive frames. We thus assure to find rules that describe groups of small scenes. This constraint is specially important in video data since the relationship between activities is commonly bet-ter explained with more than one frame. As an example, in the rule  X  if more than five people cross the street and any car is moving, then a red light may be active  X  the relation-ship between cars and people, two different activities, can be better understood by taking consecutive frames rather than only one time instant. Anyhow, a window size /w/ = 1 will mean that activities occurs in individual frames and a window size of /w/ = N will consider the entire video as a single transaction.

The number of objects performing certain topic G i is an attribute that need to be binarized before applying existing association analysis algorithms. For example, the topic G can be divided into the following intervals to generate the binary attributes: We can use the resulting dataset to produce more mean-ingful rules that considers partial amounts of objects do-ing some activity. Every row is a transaction that contains zero or more items (intervals associated to the topic G We denominate a collection of zero or more items as an itemset. The number of transactions that contains an item-set I is known as support count  X  ( I ). Given the set of N transactions in a video, we choose two itemsets I 1 ( I 1  X  I 2 =  X  ) to form an association rule of the form I 1 The importance of this rule is measured by its support and confidence. Both support and confidence allow us to discover subsets of activities with strong co-occurrence relationship between the antecedent and consequent part of the rule. We discard rules that may occur simply by chance by arbitrarily considering two thresholds minsup and minconf such that rules with values support &lt; minsup and confidence &lt; minconf will be filtered out.

Association rules exhibit some desirable properties. First, given a frequent itemset of k +1 elements, we assure that sub-sets of k items are also frequent. This property is known as monotonicity and the Apriori [1] algorithm takes advantage of it to extract rules in datasets with millions of transactions in a very efficient time. The confidence measure of a rule can be considered as an estimation of the maximum likeli-hood on the conditional probability P ( I 2 | I 1 ). Second, the extracted rules are easy to interpret and we employ them to understand scenes in video data. We use the Apriori al-gorithm in our experiments to discover rules in the binary datasets related to activities.
In this section, we conduct experiments to show the sig-nificance of the rules generated to describe scenes in video. We employ outdoor video sequences where different activi-ties cooperate to avoid potential conflicts (e.g., people cross-ing a street when cars are not moving or people assisting an aircraft when it is stationary). We experiment on the fol-lowing datasets: Karl-Wilhelm &amp; Strabe 1 (normal defini-tion, 25fps, 1 hour), Street Intersection 2 (normal quality, 25fps, 45 minutes), and Aircraft Assistance 3 (high defini-tion, 30fps, 1 hour). The experiments are run on a 3.6 GHz Pentium 4 with 2 GB RAM and all the datasets are publicly available to facilitate experimental comparisons.
In this experiment, we study the significance of the rules generated to understand the dependencies between topics found in Experiment 1. Time windows of size /w/ = 25 are employed in all the datasets to group activities in consecu-tive frames into transactions. Each transaction contains in-formation about the topics present within a time window. If a transaction does not contain any topic, then it is discarded. To use the Apriori algorithm, we discretize the attributes in every transactions to represent the number of objects doing some activity with a sliding window of size /w/ = 5.
The number of transactions, topics, and the processing time to discover association rules for every dataset are sum-marized in Table 1. The Street intersection dataset ex-hibits more topics than the Karl-Wilhelm &amp; Strabe dataset since five traffic lights regulates complex activities into a large number of well defined scenes. On the other hand, the Aircraft Assistance dataset contains few topics due to the limited types of activities performed and the considerable number of frames with no activities. The processing time to generates rules seems to be proportional to the number of topics discovered in each dataset.

In our model every time window is a transaction that con-tain subsets of zero or more topics. We take subsets with support values above a threshold to identify rules that de-scribe frequent co-occurrence dependencies between topics. Above the support value of 10%, we find sequences that con-tain the most frequent topics correlated in similar fashion, but with different objects performing the activities. The http://i21www.ira.uka.de/image sequences/ http://www.eecs.qmul.ac.uk/  X  jianli/Junction.html http://www.bbcmotiongallery.com/ Figure 2: The subset of topics with highest support values in the Karl-Wilhelm &amp; Strabe dataset. Note that mul-tiple frames share the same relationship between topics for different moving objects.
 Street Intersection  X  15000 23 8.74 sec.
 Karl-Wilhelm&amp;Strabe  X  11000 17 5.41 sec.
 Aircraft Assistance  X  7000 10 3.28 sec.
 Table 1: Information on the datasets preprocessed to dis-cover association rules. common scene shared by three frames in Figure 2 corre-sponds to the most frequent activity found in the Karl-Wilhelm &amp; Strabe dataset, cars moving from up to down because of a green traffic light in their lane (topics G 1 G ). Another group of cars simultaneously moves from the center to the upper right corner of the frame (topic G Both topics do not collide since the next rule allow cars to go in opposite directions by parallel lanes with 14% of sup-port and 94% of confidence.
We consider a minimum support value of 2%, a minimum confidence value of 90%, and topics G i with more than 5 in-stances during the video in order to generate representative rules. We thus extract rules from the Street Intersection (12 topics and 15 rules), Karl-Wilhelm &amp; Strabe (6 top-ics and 10 rules), and Aircraft Assistance (4 topics and 6 rules) datasets. We see that while more constraints gov-ern the activities (e.g., traffic lights, one-way roads, inter-sections, etc), more topics are generated and more frequent rules are discovered. This seems to indicate that every con-straint imposes an underlying logic that fragments complex activities into a large number of small scenes, which are easy to represent with events, form well defined topics, and there-fore are likely to be frequent during the video.

Some of the high-confidence rules uncovered with the al-gorithm proposed in this paper are shown in Table 2. The two first rules suggest a strong correlation between vehicles moving from left to right and a person going in opposite di-rection in Figure 3 (a)-(c). Both activities are not mutually exclusive since there are two traffic lights that regulate other vehicles move from up to down or vice-versa. The third rule indicates the frequent presence of two actors doing different activities during the fueling of an aircraft in Figure 3 (d)-(f). This rule suggests that one person is serving the plane while the other is supervising his activity, as actually happens in the video.
 { G 1  X  [1 , 3) } X  X  G 2  X  [1 , 3) } 98.5% 11.5% { G 1  X  [1 , 3) } X  X  G 2  X  [3 , 6) } 96.2% 15.1% { G 4  X  [1 , 3) } X  X  G 3  X  [1 , 3) } 93.8% 12.3%
Table 2: A selection of high confidence association rules. Figure 3: Scenes represented by same rules. Each row groups scenes of the same video.
In this paper, we propose a method to generate rules that explain video scenes with an unknown number of topics. We also present a two dimensional structure that describes the spatial arrangement of events on a motion grid to describe activities. By considering every grid as a random variable, we obtain a sequence of event probabilities that provides ac-tivity information which is then grouped into activities and activities into topics. We report the results of experiments to study the significance of our rules on real datasets. Our results show that we can find meaningful topics in our video datasets and that we can effectively describe dependency re-lationships between those topics. In future, we plan to study temporal association analysis of activities in video data. [1] R. Agrawal, T. Imieli  X nski, and A. Swami. Mining [2] N. P. Cuntoor, B. Yegnanarayana, and R. Chellappa. [3] T. Hospedales, S. Gong, and T. Xiang. A markov [4] I. Laptev, B. Caputo, C. Sch  X  uldt, and T. Lindeberg. [5] H. Wang, M. Ullah, A. Kl  X  aser, I. Laptev, and
