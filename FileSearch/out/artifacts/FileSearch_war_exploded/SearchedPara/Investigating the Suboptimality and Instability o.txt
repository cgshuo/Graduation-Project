 Although Pseudo-Relevance Feedback (PRF) techniques im-prove average retrieval performance at the price of high vari-sons for their instability. In this work, we study more than 800 topics from several test collections including the TREC Robust Track and show that PRF techniques are highly suboptimal, i.e. they do not make the fullest utilization of pseudo-relevant documents and under-perform. A care-ful selection of expansion terms from the pseudo-relevant document with the help of an oracle can actually improve retrieval performance dramatically (by &gt; 60%). Further, we show that instability in PRF techniques is mainly due to wrong selection of expansion terms from the pseudo-relevant documents. Our findings emphasize the need to revisit the problem of term selection to make a break through in PRF. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Algorithms, Experimentation
Pseudo-relevance feedback (PRF) uses terms from the top ranking documents of the initial unexpanded retrieval for expanding the query [5]. Although PRF improves average retrieval performance, improvements have been incremental despite years of research [1, 3]. It is not known whether PRF techniques are under-performing or have already given their best by making the fullest use of pseudo-relevant documents in Query Expansion. Further, there is no satisfactory expla-nation of instability of PRF techniques. It is commonly be-lieved that PRF is unstable because the initial unexpanded retrieval brings many non-relevant documents in the top for some topics and therefore, query expansion produces topic drift. However, for a good number of queries, retrieval per-formance does not change significantly. It is not known why PRF techniques fail to make a difference to such queries.
It is important that the twin issues of optimality and in-stability be addressed to decide whether to continue invest-ment on new research in PRF and to devise effective ways of combating instability. In this work, we take the first steps to-wards understanding the optimality and instability of PRF pseudo-relevant documents in such a way as to maximize retrieval performance. techniques by studying more than 800 topics from several test collections including the TREC Robust Track.
We develop DEX, an oracle method for extracting a set of expansion terms from the pseudo-relevant documents using discriminative learning. Being an oracle method, DEX can be viewed as a good approximation to the ideal PRF tech-nique that we can hope to design. As state-of-the-art PRF techniques and DEX extract expansion terms from the same set of pseudo-relevant documents, the gap in their retrieval performance indicates the future potential for improvement in retrieval performance of PRF techniques.
DEX is an oracle for extracting a set of useful expan-sion terms from the pseudo-relevant documents by using the knowledge of relevant documents. DEX first extracts a set of candidate expansion terms 2 t 1 , ..., t N from the pseudo-relevant documents and then partitions this set into a set of useful terms and a set of non-useful terms using statistical learning [6]. It treats relevant documents for the topic as +ve instances and top scoring non-relevant documents as -ve instances 3 . It learns a linear discriminant function w to discriminate the + ve instances from the  X  ve instances. The linear discriminant function classifies a vector x as + ve if w
T x &gt; 0 and as  X  ve if w T x  X  0. Therefore, DEX treats terms t i : w i &gt; 0 as useful terms and the rest as non-useful. Finally, DEX picks the largest weighted k &gt; 0 terms for expansion.
We employed a KL-divergence based retrieval system with two stage Dirichlet smoothing as our baseline [4]. We used model-based feedback technique (Mixture Model) as a repre-sentative PRF technique [3]. For expanded retrieval, we in-terpolated the feedback model with the original query model with  X  set to 0 . 5. For estimating the feedback model, we used the top 10 documents fetched by the initial retrieval. Topics as well as documents were stemmed using the well known Porter stemmer and stop-words were removed. We compared model-based feedback with DEX-based PRF. We used the DEX algorithm (Section 2) to extract k = 5 expan-sion terms from the top 10 documents of the unexpanded retrieval. We formed a feedback model from the expansion pseudo-relevant documents whose idf &gt; ln 10 and collection frequency  X  5 [2]. whose dimensions i = 1 , ..., N correspond to the candidate expansion terms t 1 , ..., t N respectively.
 CLEF 03,05,06 0.38 0.42 0.41 0.43 0.66* 0.72 terms by assigning equal probability mass to the DEX terms. As with model-based feedback, we interpolated our feedback model with the query model with  X  set to 0 . 5. We call un-expanded retrieval, model-based feedback, and DEX-based PRF as LM, MF and DEX respectively. We used the CLEF (LATimes 94, Glasgow Herald 95) and TREC (Associated Press 88-89, Wall Street Journal, San Jose Mercury, Disks 4&amp;5 minus the Congressional Record) document collections in our experiments. We studied re-trieval performance on the following sets of topics: CLEF Topics 1 -140 (CLEF 2000-2002), Topics 141-200 (CLEF 2003), Topics 251-350 (CLEF 2005-2006), TREC Topics 51 -200 (TREC Adhoc Tasks 1, 2, 3), Assorted Topics (TREC Robust 2003, Hard), Topics 301-450, 601-700 (TREC Ro-bust 2004). There were totally 821 unique topics. Some topics were used for retrieval on multiple document collec-tions.
We used MAP and P@5 as the average performance mea-sures to compare the three retrieval models. We say that a topic is hurt by expanded retrieval if the average preci-sion decreases by 0 . 01 or more relative to the unexpanded retrieval. Similarly, we say that a topic is improved (bene-fitted) by a retrieval model if the average precision increases by 0 . 01 or more. We compare the performance of LM, MF, and DEX on all topics in Table 1. We see that MF fares better than LM overall but the improvement in retrieval performance is modest. In contrast, DEX gives dramatic improvement in retrieval performance relative to both LM and MF on all test sets despite using the same set of doc-uments for estimating the feedback model. DEX improves MAP by &gt; 60% over LM and by &gt; 42% over MF in general and on Robust03, DEX improves MAP by 123% over MF. Not only the MAP has improved dramatically P@5 has also improved. The huge gap between the average retrieval per-formance of MF and DEX highlights a very important fact: PRF techniques are highly suboptimal . Their average retrieval performance is much lower than what can be po-tentially achieved using the same set of feedback documents.
Table 2 shows the percentage of topics which benefitted from MF and DEX, topics which got hurt by MF and DEX and topics which remained indifferent to MF and DEX. We observe that nearly 25% of topics in all collections are indif-ferent to MF whereas a smaller percentage of topics are hurt by MF. DEX reduces the percentage of topics in these two categories substantially. In the Robust03 track (hard), we see that the percentage of topics which got hurt reduced to 6% from 14% and the percentage of topics which remained indifferent from 28% to 4%. The relatively high robustness Table 2: Effect of MF and DEX on individual topics of DEX gives hope for PRF techniques to achieve a higher degree of robustness while not sacrificing the gain in aver-age retrieval performance. Wrong selection of terms is at the root of instability and PRF techniques will need to relook term selection strategies [1, 2].
To understand why MF is suboptimal and unstable, we computed the average rank of the DEX terms in the rank list of the MF expansion terms (ranked according to p ( . |  X  We suspected that DEX terms would not be at the top of the rank list. Because otherwise MF would not be comparatively so worse. Our suspicion turned out to be true. For a large majority of the topics, DEX terms were deep down in the MF rank list. For instance, the average rank of DEX terms for Topic 193 from TREC 3 was 114 and for TREC Topics 350 and 190 it was 229 and 735 respectively. It is clear that MF fails to recognize the importance of DEX terms and ranks them poorly. As a consequence of not choosing the right terms, expanded retrieval fails to improve the retrieval performance of these topics.
Our study shows that current PRF techniques are highly suboptimal and also that wrong selection of expansion terms is at the root of instability of current PRF techniques. A careful selection of expansion terms from the pseudo-relevant documents with the help of an oracle can actually improve retrieval performance dramatically (by &gt; 60%). We believe our findings will motivate PRF researchers to revisit the is-sue of term seleaction in PRF. It might be worthwhile to selectively extract expansion terms from the feedback doc-uments. Further, term interactions may prove crucial in addressing the problems of suboptimality and instability[2]. [1] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting [2] R. Udupa, A. Bhole, and P. Bhattacharyya. On [3] C. Zhai and J. Lafferty. Model-based feedback in the [4] C. Zhai and J. Lafferty. A study of smoothing methods [5] E. N. Efthimiadis. Query expansion. Annual Review of [6] T. Hastie, R. Tibshirani, and J. Friedman. The
