 including traditional data mi ning, content-based image retrieval, machine learning and computer vision need all to cooperate to achieve the retrieval goal. Therefore, mining images that are visually identical or similar to the query object still remains difficult. 
Recently, there is a trend of using quantized image keypoints in image object mining [6][21] and classification [8]. The idea was originally inspired by the bag-of-words (BOW) approach in text IR. In the BOW representation, a text document is encoded as a histogram of the number of occurrences of each word. Similarly, one can characterize an image by a histogram of counts of visual words. Then the image object mining can work as text information retrieval, except that there is no pre-given visual vocabulary for the image mining problem. As a result, the visual vocabulary for the image mining problem has thus needed to be learned from a training set in advance. 
To obtain the visual vocabulary, each image in the corpus is first processed to extract features in some high-dimensional descriptor space. These invariant descriptors are then offline clustered to form a much smaller discrete set of prototypes called visual vocabulary. The quantized feature vectors (i.e. cluster centers) in the visual vocabulary are referred to as the  X  X isual words X  by analogy to the  X  X eywords X  in text categorization. The visual vocabulary has thus provided a mid-level repr esentation which helps bridge the semantic gap between the low-level features and the high-level concepts. At the query time the descriptors extracted from a query image are mapped onto the visual vocabulary in a similar way, and the system then returns a set of images that contain a large number of visual words in common with the query one from the image database. 
Despite its simplicity and computational efficiency, result of the BOW method is often unsatisfactory. The reason of the limited performance can be summarized in two-folds. 
First, replacing invariant descri ptors with prototypes reduces some discriminating powers present in the unique features, and consequently tends to suffer more seriously from false negatives than most text retrieval systems. Second, image mining differs from common task s of classification such as face detection and character recognition. In a retrieval task, the number of potential image classes is extremely large and usually only a small number (or even only one) of the query examples are given. The insufficient information of the Section 2 reviews the related researches. In Section 3, we describe the designation of the mining framework, including the visual BOW representation, noise removal, and relative-entropy-based ranking function. Section 4 presents the proposed PRF approaches and Section 5 shows the experimental results of the retrieval accuracy. Finally, conclusions are given in Sections 6. research streams: visual bag-of-words and relevance feedback. 
The problem of image modeling using low-level features has been studied in multimedia retrieval for decades [22][24]. One leading method for imag e object mining from large image database is the visual bag-of-words or visual-vocabulary-based approach. The idea of visual dictionary was initially proposed in [21]. In this work, Sivic and Zisserman performed retrieval of shots from a movie using a text retrieval approach. Descriptors extracted from local affine invariant regions are quantized into visual words. Term Frequency Inverse Document Frequency (TF-IDF) is then used to compute the relevant scores between query and database images. In [8], the authors presented a method for visual categorization based on the vector quantization of affine invariant descriptors. Each image was mapped to a vector of visual keywords according to the visual dictionary, Na X ve Bayes or SVM was then used to train the classifiers for image categorization. 
Despite the popularity of visual bag-of-words retrieval/classification performance is not totally satisfied. Quantizing local descriptors inevitably decreases the discriminative power, and reduces the capacity to describe the detailed image structure. To further improve the retrieval performance, Zheng et al. [31] argued that phrases in text have a higher semantic granularity than words, and thus propose a visual phrase-based approach to retrieve images containing the desired objects. Visual word pairs that frequently appear closely at the same time will be merged to form the visual phrase by using Apriori rule [1]. Their experiments demonstrate that the visual phrase-based retrieval approach outperforms the visual word-based approach. Some semantic-relevance images that can not be found under the typical visual bag-of-words model were successfully retrieved. Nowak et al. [15] investigated the correlation between patch sampling strategies and performance. Two popular mul ti-scale keypoint detectors: Laplacian of Gaussian and Harris-Laplace, and their proposed random sampling strategy are tested for performance comparison. Surprisingly, they have experimentally shown that the random sampling strategy gives equal or better performance than the sophisticated multi-scale interest operators that are commonly used. In feedback architecture to enhance the system performance. Unlike [6], we concentrate on the study of pseudo-relevance feedback, instead of the geometric information originated from the computer vision for post-processing. In addition, the language model also provides us a more concrete theoretical basis to re-estimate the query model, and thus leads to a more feasible architecture in practice. image object retrieval and relevance-based language modeling. We use visual BOW to represent an image, and refine the query language model through the pseudo-relevance feedback procedure. In this section, we first give a brief outline of the proposed visual BOW mining architecture and then the relevance-based language model to introduce the terminology which is used in this paper. 
The basic idea of BOW is to treat images as a collection of the representative prototypes sampled from the training image corpus, and then use th e resulted distribution in the descriptor space as a characterization of the image. 
In this paper, the SIFT algorithm [12] is employed to extract the features due to its impressive performance in image recognition [13]. The SIFT algorithm was invented for object recognition and widely adopted for applications in image/video retrieval. It consis ts of four major stages: scale-space extrema detection, keypoint localization, orientation assignment, and keypoint descri ptor construction. First, the DoG (Difference of Gaussian) operator is convolved with the image in the scale space, and a pixel is selected as a keypoint if it is the scale-space extrema. Then, an edge orientation histogram (EOH), determined by the gradient orientations in each keypoint X  X  neighborhood, is constructed for the keypoint. The resulted 128-dimensional vector of EOH is then employed as a distinctive local descriptor. 
When building the visual vocabulary, the size is an important parameter affecting the system performance. A proper number of the visual word s are benefit to balance the retrieval accuracy and response time. However, conventional visual BOW approaches adopted the k-means algorithm that is easy to implement but hard to scale to a large vocabulary. For generating a finer and larger scale corpus, we thus employed a clustering approach based on approximate-nearest-neighbor (ANN) [2] to balance the computation time and quantization accuracy. 
As we know that the computati on effort in typical k-means mostly spent on calculating the nearest neighbors between the points and cluster centers, we have thus tried to lessen the computation by replacing the exact nearest-neighbor (NN) problem with an approximate k-d tree (Ak-d tree) [2]. The idea behind the Ak-d tree is to return the closest neighbor with a high probability. It only visits the nodes that lie within a threshold distance. Given a set of visual words in the visual vocabulary V and a slackness parameter  X  &gt; 0, the true NN divergence. Hence, the retrieva l problem turns out to be an estimation problem that estimat es the unigram models for a query and a set of documents. 
More formally, suppose that we are given a collection T of the target (database) images. Each image I  X  T is represented generated as described in S ection 3.1. Assume that a database image I is obtained as a sample from a unigram language model (i.e., a multinomial word distribution) P(w|  X  I ) with the parameters  X  I . The simplest way to estimate the document language model is to treat the image as a sample from the underlying multinomial word distribution and use the maximum likelihood estimator [11] and | I | is the total number of visual words in I . 
However, (3) could generate a zero probability if a visual word never occurs in the document image I , which causes the problem in scoring the likelihood of a document with the query. To avoid the incorrectness, the Dirichlet smoothing [29] technique is employed. Dirichlet smoothing uses a document-dependent coefficient (parameterized with  X  ) to control the interpolation, Here P(w|  X  B ) is the probability of visual word w given by the collection language model  X  B , which is usually estimated by using the whole collection of image documents T , e.g., 
The generative model for a query is simply set as a unigram language model, which is defined as: where Q denotes the query image, tf(w, Q) is the count of the visual word w in the query Q , and | Q | is the total number of visual words in Q . 
Given the estimated query and a document language model Q  X   X  and I  X   X  , the relevance value of I with respect to Q can then be measured by the KL-divergence: where V is the set of all the visual words. 
The KL-divergence-based retrieval method described in the previous section, however, is not very discriminative because the information contained in a single query image is limited. The paucity of query information causes the use an EM-based generative model [30] to remove the irrelevant noise. Given a background weighting parameter  X  , the feedback documents set R , and the background language model P(w|  X  B ) , the EM (Expectation Maximum) algorithm is employed to compute the maximum likelihood estimate of the feedback model as The EM updating formulas for P(w|  X  F ) are and For convenience, we denote this method as VIDE-EM. 
The VIDE method presented in the previous section provides a way to incorporate information in top-ranked images into feedback process. However, it uses an indirect way to reconstruct the query model. The refined query model is learned directly from the feedback image documents, but each of which may contain visual terms that are irrelevant to the query topics. Consequently, feedbacks based on the whole image content could introduce noises as expansion terms, causing drift of the original query X  X  focus. 
We tackle the problem by proposing an alternative PRF strategy called VIsual Term Expansion (VITE), which directly selects the candidate terms to avoid the problem of irrelevant terms. Only a subset of visual terms in the top-ranked images is chosen to join the feedback process. We detail the candidate visual te rm extraction and feedback in VITE as follows. 
The challenge of candidate terms extraction lies on how to find the informative visual terms for query expansion. We here present three term-scoring functions to associate each visual term a score. The terms considered for query refinement are then those having the best ranking scores. z Term Frequency (TF) 
The frequency of visual term appearing in the feedback images is one of the simplest possible measures, and has the merit of being very fast to compute. In text categorization, words with small term frequency are removed since rare words are relatively non-informative for category prediction. 
We thus independently associate a score with each term based on each of the frequency statistics. The average term frequency is computed across the total feedback image set R , and the formula is as follows: otherwise; are chosen as relevant 
We start this section by describing the experimental setup and implementation details of the proposed framework. We then conduct several experiments to show the effectiveness of our approach. We used two datasets to evaluate the effectiveness of the proposed method: a database of Oxford building images [16], and a RBI (Recognition Benchmark Images) database [18]. 
The RBI database contains 6376 real-world object images, which are categorized as groups of 4 images each. All images in the same group are the same objects taken from different camera poses. We thus use it as our first test data for multimedia object mining. The second database, Oxford Buildings Dataset [16], contai ns 5062 images collected from 
Flickr by searching for particular Oxford landmarks. The 
Figure 1. Examples of our ex perimental image data sets: (a) RBI database (b) Oxford Building Images. 
Figure 2. Retrieval performance of different approaches. where Q r is the number of ground truth images found in the top-k list, and Q A is the total number of ground truth images. In addition, to get the general performance evaluation, we also calculate the average probability of the top-10 retrieval: 
The results are summarized as the curves shown in Figure 2. The curves marked by  X  X OW-KLD X ,  X  X IDE X , and  X  X ITE X  are all obtained by using the KL-divergence retrieval model, but  X  X IDE X  and  X  X ITE X  have further performed a single-round pseudo-relevance feedback by using the visual document and the word expansion techniques respectively. In this case, we use all the top-k retrieved images as feedback information, and 500 most informative visual words are selected for term-level feedback. As can be seen, the PRF-based approaches (i.e., VIDE and VITE) outperform the baseline BOW-KLD for all the different top-k values. With Figure 3. Performance improvement by different rounds of feedback: (a) VIDE-EM approach. (b) VITE-CHI approach. Table 1. Performance comparison of VITE-CHI approaches with different number of visual terms selected into feedback. The basic idea of EScore is to evaluate the ability of a retrieval algorithm that can push relevant images to the front of the search result. It is quite reasonable for real world retrieval applications since users usually pay most of their attention to the very top search results, and tend to ignore the rest in the rear. 
In this experiment, we randomly pick 5 images for each 11 different landmark categories, and all the selected 5  X  11=55 images served as queries to search the Oxford database. After an initial run using this baseline visual BOW mining method, we take the top 20 images for pseudo-relevance feedback. Since VIDE-EM and VITE-CHI presented in the previous experiment get better performances in the corresponding document-level and term-level feedback, we thus compare these two methods with the baseline BOW method. The results are summarized and shown in Figure 4. 
As can be seen, VIDE-EM achieves better EScore than the baseline BOW-KLD approach in most cases. Since the Oxford database has a more complex scene, the baseline BOW-KLD approach often gets a poor retrieval result. In this case, if the quality of the initial search is extremely low, then most of the top-ranked images are then probably non-relevant. Feedback on the whole document level could thus fail. This is the reason why the EScore of VIDE-EM in several landmark categories ar e not improved significantly (e.g., Balliol and Pittrivers), and some of them are even worse (e.g., Keble). 
In contrast to document expansion, VITE-CHI outperforms the baseline BOW-KLD in all of the landmark categories. Also, feedback on term level has clearly gained better results than that on document level. The advantages of VITE compared to VIDE can be summarized as two-folds. First, VITE does not force the query model to accept the unrelated visual words in the irrelevant images. It thus avoids the risk of bringing unwanted terms into the query model. Second, since irrelevant image could still contain relevant visual words with respect to the query object, the term-level feedback strategy thus helps keep informative visual words and drop the rest. Hence, the unreliable initial search result affects less to the feedback process. Figure 5 gives some visual examples of the object retrieval results based on the baseline BOW-KLD and VITE-CHI. As can be seen, the proposed PRF-based approach provides a Figure 4. Performance improvement by different rounds of feedback: (a) VIDE-EM approach. (b) VITE-CHI approach. retrieval: A comparative evaluation. International Joint Conference on Artificial Intelligence , pp. 708-715, 1997. [5] C. Carpineto, R. Mori, G. Romano, and B. Bigi, An information-theoretic approach to automatic query expansion, ACM Transactions on Information Systems , vol. 19, no. 1, pp. 1-27, 2001. [6] Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman, Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval, Proceedings of the International Conference on Computer Vision, 2007. [7] T.M. Cover and J.A. Thomas, Elements of Information Theory, John Wiley &amp; sons, 1991. [8] G. Csurka, C. Dance, L. Fa n, J. Willamowski, and C. Bray. Visual Categorization with Bags of Keypoints. Proceedings of the European Conference on Computer Vision , 2004. [9] R. Fergus, P. Perona, and A. Zisserman, A Visual Category Filter for Google Images. Proceedings of the European Conference on Computer Vision , pp. 242-256, 2004. [10] W. H. Hsu and S.-F. Chang. Visual cue cluster construction via information bottleneck principle and kernel density estimation. Proceedings of the International Conference on Content-Based Image and Video Retrieval , pp. 82-91, 2005. [11] J. Lafferty and C. Zhai, Document language models, query models, and risk minimization for information retrieval, Proceedings of the international ACM SIGIR conference on Research and development in information retrieval , pp. 111-119, 2001. [12] D. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision , vol. 60, pp. 91-110, 2004. [13] K. Mikolajczyk and C. Schmid, A performance evaluation of local descriptors, Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition , vol. 2, pp. 18-20, 2003. [14] D. Nister and H. Stewenius, Scalable recognition with a vocabulary tree, Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition , pp. 2161-2168, 2006. [15] E. Nowak, F. Jurie, and B. Triggs, Sampling strategies for bag-of-features image classification, Proceedings of European Conference on Computer Vision , pp. 490-503, 2006. http://www.robots.ox.ac.uk/ ~vgg/data/oxbuildings/ . [17] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, Object retrieval with large vocabularies and fast spatial matching, Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition , pp. 1-8, 2007. [18] RBI (Recognition Benchmark Images) Dataset. http://vis.uky.edu/%7Estewe/ukbench/ . [19] J. J. Rocchio. Relevance feedback in information retrieval. Processing of the SMART Retrieval System Experiments in Automatic Document , pp. 313 X 323, 1971. [20] Y. Rui, T. S. Huang, and S. Mehrotra, Content-based image retrieval with relevance feedback in Mars, context, Proceedings of ACM international conference on Image and video retrieval . pp. 162-169, 2007. [27] R. Yan, A. Hauptmann, and R. Jin. Multimedia search with pseudo-relevance feedback. Proceedings of ACM International Conference on Image and Video Retrieval , pp. 238-247, 2003. [28] R. Yan, A. Hauptmann, and R. Jin, Negative pseudo-relevance feedback in content-based video retrieval, Proceedings of the ACM international conference on Multimedia , pp. 343-346, 2003. [29] C. Zhai and J. Lafferty, A study of smoothing methods for language models applied to information retrieval, ACM Transactions on Information Systems , vol. 22, no. 2, pp. 179-214, 2004. [30] C. Zhai and J. Lafferty, Model-based feedback in the language modeling approach to information retrieval, Proceedings of the international conference on Information and knowledge management , pp. 403-410, 2001. [31] Q.-F. Zheng, W.-Q. Wang, W. Gao, Effective and efficient object-based image retrieval using visual phrases, Proceedings of ACM international conference on Multimedia , pp. 77-80, 2006. 
