 In this paper, we propose a set of novel regression-based ap-proaches to effectively and efficiently summarize frequent item-set patterns. Specifically, we show that the problem of minimiz-ing the restoration error for a set of itemsets based on a proba-bilistic model corresponds to a non-linear regression problem . We show that under certain conditions, we can transform the non-linear regression problem to a linear regression problem . We propose two new methods, k -regression and tree-regression , to partition the entire collection of frequent itemsets in order to minimize the restoration error. The K-regression approach, employing a K-means type clustering method, guarantees that the total restoration error achieves a local minimum. The tree-regression approach employs a decision-tree type of top-down partition process. In addition, we discuss alternatives to estimate the frequency for the collection of itemsets being covered by the k representative itemsets. The experimental evaluation on both real and synthetic datasets demonstrates that our approaches sig-nificantly improve the summarization performance in terms of both accuracy (restoration error), and computational cost. Categories and Subject Descriptors: H.2.8 [Database Manage-ment]: Database Applications -Data Mining General Terms : Algorithms, Performance Keywords: frequency restoration, pattern summarization, regres-sion
Since its introduction in [3], frequent pattern mining has re-ceived a great deal of attention and quickly evolved into a ma-jor research subject in data mining. The tools offered by fre-quent pattern mining research span a variety of data types, in-cluding itemsets, sequences, trees, and graphs [25, 4, 31, 5]. Re-searchers from many scientific disciplines and business domains have demonstrated the benefits from frequent pattern analysis X  insight into their data and knowledge of hidden mechanisms [10]. At the same time, frequent pattern mining serves as a basic tool for many other data mining tasks, including association rule min-ing, classification, clustering, and change detection [14, 32, 13, 15]. Recently, standard frequent mining tools, like Apriori, have been incorporated into several commercial database systems [18, 30, 23].

The growing popularity of frequent pattern mining, however, does not exempt it from criticism. One of the major issues facing frequent pattern mining is that it can (and often does) produce an unwieldy number of patterns. So-called complete frequent pat-tern mining algorithms try to identify all the patterns which occur more frequently than a minimal support threshold (  X  ) in the de-sired datasets. A typical complete pattern mining tool can easily discover tens of thousands, if not millions, of frequent patterns. Clearly, it is impossible for scientists or any domain experts to manually go over such a large collection of patterns. In some sense, the frequent patterns themselves are becoming the  X  X ata X  which needs to be mined.

Indeed, reducing the number of frequent patterns has been a major theme in frequent pattern mining research. Much of the research has been on itemsets; itemset data can be generalized to many other pattern types. One general approach has been to mine only patterns that satisfy certain constraints; well-known examples include mining maximal frequent patterns [21], closed frequent patterns [19] and non-derivable itemsets [8]. The last two methods are generally referred to as lossless compression since we can fully recover the exact frequency of any frequent itemsets. The first one is lossy compression since we cannot re-cover the exact frequencies. Recently, Xin et al. [27] generalize closed frequent itemsets to discover a group of frequent itemsets which  X  -cover the entire collection of frequent itemsets. If one itemset is a subset of another itemset and its frequency is very close to the frequency of the latter superset, i.e., within a small fraction (  X  ), then the first one is referred to as being  X  -covered by the latter one. However, the patterns being produced by all these methods are still too numerous to be very useful. Even the  X  -cover method easily generates thousands of itemset patterns. At the same time, methods like top-k frequent patterns [11], top-k redundancy-aware patterns [26], and error-tolerant patterns [29 ] try to rank the importance of individual patterns, or revise the fre-quency concept to reduce the number of frequent patterns. How-ever, these methods generally do not provide a good representa-tion of the collection of frequent patterns.

This leads to the central topic of this paper: what are good criteria to concisely represent a large collection of frequent item-sets, and how can one find the optimal representations efficiently? Recently, several approaches have been proposed to tackle this issue [2, 28, 24]. Two key criteria being employed for evaluating the concise representation of itemsets are the coverage criterion and frequency criterion . Generally speaking, the coverage crite-rion assumes the concise representation is composed of a small number of itemsets with the entire collection of frequent item-sets represented or covered by those itemsets. Typically, if one itemset is a subset of another, then we refer to the first one as being covered (or represented) by the latter one. The frequency criterion refers to the capability of the frequency of any frequent itemset to be inferred (or estimated) from the concise representa-tion. Following these criteria, we summarize the previous efforts. The Spanning Set Approach [2]: In this work, the authors de-fine a formal coverage criterion and propose to use K itemsets as a concise representation of a collection of frequent itemsets. Ba-sically, the K itemsets are chosen to maximally cover the collec-tion of frequent itemsets. They further show that finding such K itemsets corresponds to the classical set-cover problem and thus is NP-hard. The well-known greedy algorithm is applied to find the concise representation.

The major problem with this approach is that the frequency (or the support measure) is not considered. Indeed, how to effec-tively integrate a frequency criterion with the coverage criterion is an open problem pointed out by the authors in [2].
 The Profile-Based Approach [28]: Yang et al.  X  X  work [28] is the first attempt to answer the call from [2]. Their concise rep-resentation is derived from the K clusters formed by all frequent itemsets. All frequent itemsets in a cluster are covered and esti-mated by a so-called pattern profile , which contains three com-ponents. The first component is an itemset which is the union of all the frequent itemsets in the cluster. For instance, if a cluster contains three frequent itemsets, { a, c } , { a, d, e } and { c, f, g } , then their first component is the itemset { a, c, d, e, f, g } . Let D be the set of transactions in which these itemsets occur. Then, the second component is the total number of transactions in D , and the third component (also referred to as the distribution vector ) records all the relative frequencies for each individual item in D . Given this, a frequent itemset in the cluster is estimated based on the independence assumption: for instance, the frequency of and p ( a ) , p ( c ) are the relative frequencies for items a and c in D . To form the K clusters, they propose applying the Kullback-Leibler divergence of the distribution vectors to pairs of pattern profiles. Initially, the profile for an individual frequent itemset is derived by treating it as a single-member cluster. Given this, they apply hierarchical agglomerative and k-means clustering to partition the collection of itemsets into K clusters.
 The Markov Random Field Approach [24]: Wang et al. tar-get better estimation of the frequent itemsets. They propose the construction of a global Markov random field (MRF) model to estimate the frequencies of frequent itemsets. This model utilizes the dependence between items, specifically, the conditional inde-pendence, to reduce their estimation error. Specifically, this ap-proach processes all the frequent itemsets in a level-wise fashion. In each level, it identifies those frequent itemsets which cannot be estimated well by the current model, i.e., the estimation er-ror is higher than a user-defined tolerance threshold (  X  ), and add those itemsets into the model. Then, it will re-train the model after each level if any new itemsets are added. In the empirical study, their model shows better estimation accuracy than earlier approaches.

Though the last two approaches make significant progress to-wards restoring frequencies for frequent itemsets, they fall short of being effective and efficient for pattern summarization from several perspectives. First, with regard to effectiveness, the key question is how we can minimize the estimation error (restora-tion error) given the collection of itemsets. Both approaches, however, are very heuristic in nature and so do not provide any theoretical justification of why their methods can achieve such goal. As for efficiency, both approaches are computationally ex-pensive. For instance, Yang et al.  X  X  method must repetitively ac-cess the original database, and the heuristics they introduce to avoid such access do not maintain good estimation results. Wang et al.  X  X  approach repetitively invokes the expensive probabilis-tic learning procedures, such as the Junction tree inference and MCMC procedure [24]. Finally, neither method considers cov-erage rate; it is not clear how their methods can integrate with the spanning set approach to provide frequency estimation in addi-tion to the coverage criterion. In other words, the open problem raised by [2] has yet to be answered.
Based on our discussion, we see that the effective and efficient restoration of the frequency of summarized frequent itemsets re-mains a significant open problem. Before we formally define this problem, we present some basic notation.

Let I be the set of all items I = { o 1 , o 1 , , o m } . An itemset or a pattern P is a subset of I ( P  X  I ) . A transaction database is a collection of itemsets, D = { d 1 , d 2 , , d n } , such that d I , for all i = 1 , 2 , , n . The collection of transactions that con-tains P is represented as D P = { d i | P  X  d i and d i  X  D } . The frequency of an itemset P is denoted as f ( P ) = | D P | / | D | . Let F  X  be the collection of frequent itemsets with minimum support  X  : F  X  = { P : | D P |  X   X  } .

D EFINITION 1. (Restoration Function and Restoration Er-ror) A restoration (estimation) method for a set of itemsets S is a function mapping S to a value between 0 to 1 :  X  f : S  X  [0 , 1] . The restoration quality can be measured by a p -norm for the rel-ative error (or alternatively the absolute error): For computational purpose, the 2 -norm is chosen in this study. Clearly, the best restoration quality a restoration method can achieve is E p = 0 . However, using this measure, this would mean either we record the frequency of each itemset or the restoration method scans the original database. Such methods provide neither a suc-cinct representation nor better interpretation of the collection of itemsets. We would like a restoration function to be more con-cise.

To build a concise restoration function for S , certain proba-bilistic models with a list of parameters  X  = (  X  1 , ,  X  commonly employed. Generally speaking, the fewer the number of parameters, the more concise the model is. For instance, in [28], a probabilistic model employs the independence assump-tion for all the items in a set of S . The number of parameters for this model is bounded by the number of items in S . For a more complex model, such as the MRF model [24], the number of parameters can change depending on the number of constraints being incorporated into the model. Given this, we can see that finding a concise restoration function needs to consider both the feature selection and model fitting issues [12].

In this study, we will investigate how to identify the best pa-rameters for the restoration function utilizing probabilistic mod-els based on the independence assumption and its extension (the factor-graph model). Note that our work is distinguished from the existing work [28, 24] since we formalize the restoration problem as an optimization problem and seek methods to di-rectly optimize the restoration error. By comparison, the existing methods do not recognize the optimization nature of this problem and simply develop some heuristics for the restoration purpose. Given this, we introduce our first research question as follows. Research Problem 1: Given a set of itemsets S and assuming a single probabilistic model either based on the independence assumption or conditional independence assumption, what is the best restoration method, i.e., the optimal parameters, which can minimize the restoration error?
Research problem 1 will answer the question of how well a single probabilistic model can maximally restore the frequency for a collection of itemsets. However, using a single probabilistic model to restore the frequency of the entire collection of frequent itemsets, F  X  , can lead to very high restoration error. The general strategy to handle this problem is to partition the entire collec-tion into several parts and to restore the frequencies for each part independently. Research problem 2 studies how to find a good partition of F  X  so that total restoration error can be minimized. Research problem 3 studies the situation where the individual parts are given. This essentially corresponds to the open problem raised in [2].
 Research Problem 2: In this problem, we study the best parti-tion of F  X  so that the total restoration error can be minimized. Assuming we can partition the entire collection of frequent item-sets into K disjoint subsets: where F i  X   X  F j  X  =  X  for i 6 = j , and we can build a restoration function for each partition individually, how can we minimize the total restoration error (the sum of the restoration errors from each restoration function)?
Here,  X  f i is the restoration function for partition F i the optimal parameter set for  X  f i to minimize the restoration error in partition F i  X  , i.e., to minimize P P Research Problem 3: In [2], to approximate F  X  , K representa-tive itemsets A 1 , A 2 , , A K are chosen: where, 2 A i is the power set of A i , i.e., containing all the subsets of A i . Given the K representative itemsets, how we can derive a good restoration function to estimate the frequencies of the item-sets being covered by the K itemsets?
In this paper, we propose a set of novel regression-based ap-proaches to effectively and efficiently summarize frequent item-set patterns. Specifically, our contributions are as follows. 1. We show that to minimize the restoration error for a set 2. We propose two new methods, the K-regression approach 3. We discuss how to apply the K-regression approach to es-4. We have evaluated our approaches on both real and syn-
The restoration function based on the independence proba-bilistic model for a given set of itemsets S is as follows. Under the independence assumption, all the items in S are totally inde-pendent. In addition, we also assume such independence is held only for a fraction of the entire database D , where the fraction is denoted as p ( S ) . The relative frequency for an item a is denoted as p ( a i ) . Then for an itemset I s  X  S , we can apply the independence assumption to estimate the frequency of I follows: As an example, to estimate the frequency of { a, c, d }  X  S , we have:
Given this, our optimal restoration function corresponding to the parameter set  X  =( p ( S ) , p ( a 1 ) , , p ( a n ) ), which mini-mizes the total restoration error, is given as follows: min This corresponds to a nonlinear least square optimization prob-lem. We can further formulate it as a regression problem. For each itemset I s  X  S , we define the dependent variable , y , to be the frequency of I s , f ( I s ) , and the independent variables , 1 1 this, the regression function can be written: y = f ( I s )  X   X  f [  X  ]( x 1 , , x n ) = p ( S )  X 
Several well-known methods, including Gauss-Newton method , the method of steepest descent , and the Marquardt algorithm , can be deployed to identify the optimal parameters  X  to minimize the total restoration error [22]. However, these algorithms are gener-ally very computationally expensive. Therefore, in this study, we consider another commonly used alternative approach, to trans-form the nonlinear regression into linear regression. Applying a logarithmic transformation on both side of (  X  ) , we have Assuming S has a total of l itemsets, I 1 , , I l , then we can rep-resent our linear regression problem as follows:
We denote the left most vector as Y , the matrix as X , and the right most vector as  X  . Thus, the system of linear equations is denoted as The parameters  X  with the least square error, can be derived by the standard linear regression technique [12]:
Clearly, linear regression provides an elegant solution for find-ing the optimal parameters for the restoration function. However, we need to consider several important issues regarding the linear and nonlinear regression.
 The Bounded Parameter Problem: An important problem fac-ing both nonlinear and linear regression is that each parameter in  X  is bounded: However, the general solutions for regression will not take this into consideration. A simple analysis can show that the identi-fied parameters will always be non-negative. Thus, we only have to deal with the problem that the parameters can be potentially larger than 1 . Here, we discuss two alternative strategies: The first strategy is based on the assumption that the main goal of a restoration function is to minimize the restoration error. Given this, the bounded parameter problem is not significant. But, we still would like the estimator  X  f to be bounded, i.e.,  X  simple solution is to introduce the wrapped restoration function ,  X  f : We note that the total restoration error introduced by the wrapped restoration function (  X  f  X  ) is always smaller than the original restora-tion function  X  f .

The other strategy is to find the optimal parameters which sat-isfy the bounded parameter constraint. This problem is referred to as the bounded-variable (or bound constraint) least square prob-lem [9]. Several methods have been developed to numerically search for the optimal solution.
 The Optimality of Linear Regression: Logarithmic transfor-mation and linear regression provide an elegant solution for the optimal restoration function. However, we note that the crite-rion optimized in the linear regression is different from the orig-inal total restoration error. Thus, a key question is how the op-timal parameters identified in linear regression can minimize the restoration error. To answer this question, we need to analyze the relationship between these two criteria, which we denote as C (for nonlinear regression) and C L (for linear regression): First, we can see that they both tend to minimize the difference between  X  f ( I i ) and f ( I i ) . Clearly, This only suggests their convergence tendency. Taylor expansion provides a more detailed analysis: Thus, we have C where O ( z ) represents the term having the same order of magni-tude as z . Note that here we assume |  X  f ( I i )  X  f ( I does not overestimate f ( I i ) by a factor of two:  X  f ( I Under this condition, C L can be reasonably close to C N . In Sec-tion 5, we empirically evaluate the restoration accuracy by the linear regression method and our results will show our method indeed achieves very small restoration errors.
 Validity of Independence Assumption: An interesting question is the validity of the restoration function model. Since we apply the independence assumption to model the set of itemsets S , is it a probabilistically valid model? Statistical tests [6], such as Person X  X  Chi-Square test and Wilks X  G 2 statistics, can be used to test the independence hypothesis. For the restoration purpose, we generally do not explicitly perform these tests. The better the independence model fits the data, the more accurate the estima-tion will be. Therefore, we will try to find an optimal partition for a set of itemsets, such that each partition can fit the model nicely, i.e. all its itemsets can be estimated accurately. This essentially corresponds to finding several independence models to represent S . Finding the optimal partition to minimize the total restoration error is the central topic of Section 3.
As we mentioned before, using a single probabilistic model to restore the frequency for all the frequent itemsets F  X  is likely to result in high restoration error. Instead, we can try to partition F  X  into several clusters, and then construct the optimal restora-tion function for each cluster. In this section, we introduce two heuristic algorithms which try to find the optimal partition of F  X  so that the total restoration error can be minimized. Sub-section 3.1 introduces the k -Regression approach which employs a k -means type clustering method. The k -partition achieved by k -regression is guaranteed to achieve a local minimum of the to-tal restoration error. Subsection 3.2 discusses the tree-regression partition approach which employs a decision-tree type of top-down partitioning process. Algorithm 1 employs a k -means type clustering procedure. Initially, the entire collection of frequent itemsets F  X  partitioned into k groups (Line 1). Then, we apply the regression approach to find the optimal parameters which can minimize the restoration error for each group (Line 2). The regrouping step (Lines 4-7) will apply all the k computed restoration functions to estimate the frequency for each itemset (Line 5) and assign it to the group whose restoration function results in the smallest error (Line 6). After the regrouping, we will again apply the regres-sion approach to compute the new restoration function for each newly-formed group (Line 8). We will compute the total restora-tion error by adding the restoration error of the k groups. This process is repeated (Lines 4-9) until the total restoration error converges, generally by testing if the improvement for the total restoration error is less than a certain threshold.
 Algorithm 1 K-Regression( F  X  , K ) 1: randomly cluster F  X  into K disjoint groups: F 1  X  , 2: apply regression on each group, F i  X  to find the restoration 3: repeat 4: for all itemset I s  X  F  X  do 5: compute the estimation error for each restoration func-6: reassign I s to the group which minimizes  X  i ; 7: end for 8: apply regression on each new group, F i  X  to find the 9: compute the new total restoration error; 10: until the total restoration error converges to a local minimum 11: output the K groups, F 1  X  , , F K  X  , and their restoration
Note that the k -regression is generic in the sense that it may be applied with any of the approaches discussed in Section 2 X  X he independence model or its generalization and their correspond-ing regression (nonlinear and linear). In the following, we in-vestigate several important issues for k -Regression . Convergence Property for K -Regression: The major differ-ence which distinguishes the k -regression algorithm from the k -means algorithm is that k -regression repetitively computes the optimal parameters for each newly formed group using regres-sion and use the estimation error to determine the reassignment for each itemset. Will this algorithm converge? We provide a positive answer for this question. Let E i be the restoration error the regression approach tries to optimize for each local cluster. For instance, if we apply the nonlinear regression, we directly optimize E i = P P regression, we optimize E i = P P this, we define the total restoration error E = P K i =1 E
T HEOREM 1. Each iteration (Line 4-9) in the k -regression algorithm will decrease the total restoration error monotonically, i.e., the new total restoration error is always less than or equal to the total restoration error. In other words, the k -Regression algorithm will tend to converge the total restoration error to a local minimum.
 Proof: This result can be established based on the following ob-servation. Let E be the total error based on the k groups and their corresponding restoration functions. Consider the start of a new iteration from Line 4 . In the regrouping step, the estima-tion error  X  i for each itemset is monotonically decreasing since  X  = arg min i |  X  f ( I s )  X  f ( I s ) | . If we use the current restoration function and the new grouping to compute the intermediate to-tal restoration error E  X  , we can easily have E  X  E  X  . Then, the regression will compute a new restoration function based on the new group, which will minimize the restoration error for each group. Thus, for the new total restoration error E  X  X  , we have 2 Shrinking Property for the Covering Itemsets: Another inter-esting property of k -regression relates to the covering itemsets . For a group of itemsets F i  X  , the covering itemset, denoted as A is the set of all the items appearing in F i  X  . To use the covering itemsets to represent the collection itemsets, we would like a low algorithm does not directly minimize this criteria, a related quan-tity, the size of the covering itemset, | A i | will indeed monotoni-cally decrease.

T HEOREM 2. At each iteration in k -regression, the size of the covering set of the newly-formed i group, F i  X  , is less than or equal to the size of the covering set for the previous i group. Proof: This is based on the observation that if we reassign an itemset I s to a new group i according to the minimal estimation error, then I s must be covered by A i , the covering set of the original i group ( I s  X  A i ). This is because if I s is not covered by I s , the restoration function will estimate the frequency I be 0 . Clearly, any other groups which can cover I s will have a better estimation accuracy. Given this, after regrouping, each new member in group i is already covered by the original A Thus, the new covering set, which is the union of all the new members in group i , will also be a subset of the original one. 2
This property shows that all the covering sets tend to shrink or at least maintain the same size after each iteration. In Section 4, we will utilize this property to derive the restoration function for the given k representative itemsets of F  X  .
 Computational Complexity: The computational complexity de-pends on the restoration function and its corresponding regres-sion method. Here, we consider the restoration function based on the independence probabilistic model and then apply linear regression. Given this, the computational complexity for the k -regression algorithm is O ( L | F  X  | N 2 ) , where L is the number of iterations, N is the total number of items in F  X  , and O ( | F is the complexity of a typical regression solver [9].
This approach tries to provide a high level description of the entire collection of frequent itemsets F  X  through a decision tree structure. For instance, Figure 1 shows a decision tree partition for a set of itemsets. The non-leaf node records a condition for an item x , such that all the itemsets of the left partition contain x , and all of the itemsets of the right partition do not. The leaf node is the final partition of the set of itemsets. Given this, the problem is how to partition F  X  into k disjoint parts, such the total restoration error can be minimized (under the condition that each part will have its own restoration function). Here, we introduce a greedy algorithm for this purpose.

Algorithm 2 builds the decision tree partition in a top-down and greedy fashion. At the root node, we will try to partition the entire set of frequent itemsets F  X  into two disjoint sets. The criteria for choosing the best split condition, i.e., including item x or not, is based on the total restoration error. Suppose the original set of itemsets is split into two sets, and then we construct the optimal restoration function for each of them independently. The sum of the two restoration errors from the two sets is denoted as the total restoration error. Once we identify the item x which results in the best partition with minimal restoration error, we will put the two new sets into a priority queue. The queue will determine which set of itemsets will be partitioned next. The reason for introducing such a queue is that we will only partition F  X  into k parts. We will always choose the set in the queue with the highest average restoration error (the restoration error for the set over the number of itemsets in the set) to be partitioned first. This procedure (repetitively choosing set of itemsets from the queue and split it into two parts) will continue until we have our k partition.
 Algorithm 2 TreeRegression( F  X  , K ) 1: Put F  X  as the only element into priority queue Q ; 2: repeat 3: let S the first partition in Q ; 4: for all item x  X  A do {A is the covering itemset of S } 5: split S into two parts using item x ; 6: apply regression on each part to find the restoration 7: compute the restoration errors for each part and their 8: end for 9: select the partitioning whose total error is the smallest; 10: score both S 1 and S 2 , the two parts of S from the best 11: put S 1 and S 2 into the priority Q (the higher the average 12: until Q has K parts 13: output the K parts in Q and the corresponding partitioning
As in the k -regression algorithm, the computational complex-ity of the tree-regression algorithm depends on the format of the restoration function and the corresponding regression method. Assuming the restoration function is based on the independence probabilistic model and then parameter-fitted using linear regres-sion, the computational complexity for the tree-regression algo-rithm is O ( K ( | F  X  | N 2 ) N ) = O ( K | F  X  | N 3 ) , where O ( | F is the complexity of the regression solver, and the additional N is the cost for testing each split condition. In general, the tree-regression has higher computational cost than k -regression. However, it provides a simpler description for the k -partition of the entire collection of frequent itemsets.
In [2], to approximate F  X  , k representative itemsets A 1 A 2 i is the power set of A i , i.e., containing all the subsets of A (How to extract these k representative itemsets is omitted. The detail is in [2]). In this section, we consider how to derive a good restoration function to estimate the frequencies of the item-sets being covered by the k itemsets.

Here, the main issue is that an itemset can be covered by more than one set of k representative itemsets. If we look at each pow-erset of the representative itemsets as the grouping of itemsets, these groups are not disjoint, but overlapped with each other. Further, we assume each powerset has its own restoration func-tion. The problem is that when we try to restore the frequency of the itemsets, assuming we have only the k representative item-sets, we will know which restoration function we should use for recovery.

Our address this problem with a two-step procedure. In the first step, we will try to construct a restoration function for each representative itemset. Then, in the second step, we propose a weighted average method to build a global restoration function. For the first step, we consider two alternatives: 1) we simply ap-ply all the itemsets which are subsets of A i , i.e., 2 A sion to find the optimal restoration function; 2) we modify the k -regression algorithm so that each itemset is grouped to only the one itemset which covers it. Basically, in Line 1 of Algo-rithm 1, for each itemset being covered by the k representative itemsets, we will randomly assign it to one of the representative itemsets which covers it. Then, based on Theorem 2 (the shrink-ing property), the k -regression will assign each itemset to one representation itemset and minimize the total restoration error.
For step two, the global restoration function combines the k local restoration functions produced in step 1 to estimate the fre-quency of an itemset. The frequency of a given itemset I s estimated to be the average of all the local functions, whose cor-responding representative itemsets cover I s :
In this section, we study the performance of our proposed ap-proaches, k -regression and tree-regression , on both real and syn-thetic datasets. We will directly compare our approaches against Yan et al.  X  X  pattern profile method (the k-means approach) [28], which has become the benchmark for other research [24]. We implement both our approaches using C++ and R [1], which is one of the most popular open-source programming environment for statistical computing. The implementation of pattern profile method is provided by the author of [24]. We use Chris Bolget X  X  Apriori implementation to collect the frequent itemsets [7].
All tests were run on an AMD Opteron 2.0GHz machine with 2GB of main memory, running Linux (Fedora Core 4), with a 2.6.17 x86 64 kernel. We use four real datasets and two synthetic Table 1: dataset characters; |I| is the number of distinct items; |T | is the number of transactions; | DB | is the size of the transaction database in terms of the total items datasets. The real datasets are chess, mushroom, BMS-POS and BMS-WebView-1 [16]. The synthetic datasets are T10I4D100K and T40I10D100K. All dataset information are publicly avail-able at the FIMI repository (http://fimi.cs.helsinki.fi/). The main characters of the datasets are summarized in table 1.

We evaluate the summarization performance in terms of the restoration error and the summarization time. Even though our approach seeks to optimize L 2 error (i.e. 2-norm. See Definition 1), we report the average L 1 (1-norm) error as below, to compare our results with previous efforts [28].
 In addition, we note that in our approaches, we restore the fre-quency for all frequent itemsets. Thus, we use S = F  X  for a given support. The pattern profile only restores the frequency for all closed frequent itemsets. Thus, the corresponding S would be the set of closed itemsets. Restoration Performance Evaluation on Real Datasets: Fig-ure 2 shows the average restoration error for the four real datasets, chess, mushroom, BMS-POS, BMS-WebView-1, with support level 75% , 25% , 0 . 8% , and 0 . 2% , respectively. Their corre-sponding number of frequent itemsets are 7635 , 3781 , 1695 , and 798 , respectively. For each dataset, we compare the aver-age restoration errors of the three methods: the pattern profile method [28], the k -regression method, and the tree-regression method. Here, we use simple linear regression for the indepen-dence probabilistic model discussed in Subsection 2.1. For each dataset, we vary the number of clusters from 10 to 100 .
From Figure 2, we can see that the two new methods, tree-regression and k -regression, achieve consistently and significantly smaller restoration errors than the pattern profile method. Ta-ble 2(e) shows the average restoration errors for figures 2(b) 2(a) 2(c) 2(d). On average, k -regression achieves lower error than the pattern profile method by a factor of approximately 8 , 17 , 7 , and 20 times for chess, mushroom, BMS-POS and BMS-WebView-1, respectively. On average, tree-regression achieves lower restoration error than the pattern profile method by a fac-tor of 4 3 , 5 , and 3 times for chess, mushroom, BMS-POS and BMS-WebView-1 datasets respectively. In addition, the run-ning time of k -regression is generally much faster than that of tree-regression, especially, when the number of distinct items is relatively large. This is understandable since in tree-regression, for each partition we will choose the best splitting item from all the distinct items. Such iteration can be rather costly. Figure 3(a) compares the running time of the three methods using BMS-POS dataset. Due to space limitations, we omit other figures on the running time using real dataset.

In summary, we can see that k -regression performs restoration more efficiently and accurately.
 Scalability Study using Synthetic Datasets: In this group of experiments, we compare the performance of the pattern profile methods and k -regression with respect to different support level on the two synthetic datasets T 10 I 4 D 100 K and T 40 I 10 D 100 K . Figure 3 shows the experimental results. Here, we choose the number of clusters to be 20 , 50 and 80 for both methods. For in-stance, k -regression_ 20 means running the k -regression method with 20 clusters. Figure 3(b) and 3(c) shows the restoration error against the support level. Figure 3(d) and 3(e) shows the running time against the support level.

In both datasets, k -regression performs much more accurate estimation than the pattern profile method. On average, k -regression has only 0 . 07% and 0 . 8% restoration error, while the pattern pro-file method has 10% and 6 . 8% restoration error, on the T 10 I 4 D 100 K and T 40 I 10 D 100 K dataset, respectively. In terms of the running time, the k -regression method runs an av-erage of 20 and 8 times faster than the pattern profile method on the T 10 I 4 D 100 K and T 40 I 10 D 100 K dataset, respectively. In addition, the restoration error of the pattern profile method increases rapidly as the support level decreases, while the k -regression method maintains almost the same level of accuracy. Restoration Function for the K representative itemsets: In this experiment, we compare the two alternatives in Section 4 to restore the frequency for the itemsets which are subsets of the given k -representative itemsets, produced by the greedy algo-rithm introduced in [2]. The two methods are the k -regression method and the method which distributes an itemset to all the representative itemsets which covers it (referred to as the dis-tribute method. We found that in most cases, the k -regression method works as well as or better than the distribute methods. In particular, in certain cases, such the mushroom datasets of Fig-ure 4, the k -regression method performs significantly better than the other one. Here, the mushroom is at 25% support level and chess is at 75% support level. The reason we believe that the distribute method works not as good as the k -regression is that it does not utilize re-partitioning like the k -regression does. Thus, the independence model is likely not to work well.
In this work, we have introduced a set of regression-based ap-proaches to summarize frequent itemsets. We have shown how the restoration problem can be formulated as a non-linear least square optimization problem and how linear regression can be applied to solve it efficiently. The two methods we proposed successfully marry the well-know k -means and decision tree al-gorithms with the linear regression problem. In addition, the k -regression methods can be naturally applied to the open problem of how to estimate the frequency for the collection of itemsets being covered by k representative itemsets. The experimental results on both real and synthetic datasets have shown that our method can achieve orders of magnitude improvement in accu-racy over the pattern profile approach, with much smaller running time. We believe our approaches offer an interesting way to han-dle estimation problems for other types of data as well. In the future, we plan to investigate how our methods can be applied to other patterns, include subtrees and subgraphs. datasets pattern profile regression tree K-regression mushroom 8.5% 2.7% 0.53% Chess 0.69% 0.16% 0.09% BMS-WebView-1 35.0% 7.5% 1.8% BMS-POS 12.9% 4.6% 1.8%
Figure 2: Average Restoration Errors for Real Datasets Figure 4: Restoration Function for K Representative Item-sets [1] The r project for statistical computing. [2] Foto Afrati, Aristides Gionis, and Heikki Mannila. [3] Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. [4] Rakesh Agrawal and Ramakrishnan Srikant. Fast [5] Rakesh Agrawal and Ramakrishnan Srikant. Mining [6] Alan Agresti. Categorical Data Analysis . Wiley, 2002. [7] Christan Borgelt. Apriori implementation. [8] Toon Calders and Bart Goethals. Non-derivable itemset [9] Gene H. Golub and Charles F. Van Loan. matrix [10] Jiawei Han and Micheline Kamber. Data Mining: [11] Jiawei Han, Jianyong Wang, Ying Lu, and Petre Tzvetkov. [12] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. [13] Jun Huan, Wei Wang, Deepak Bandyopadhyay, Jack [14] Akihiro Inokuchi, Takashi Washio, and Hiroshi Motoda. [15] Ruoming Jin and Gagan Agrawal. A systematic approach [16] Ron Kohavi, Carla Brodley, Brian Frasca, Llew Mason, [17] F. R. Kschischang, B. J. Frey, and H. A. Loeliger. Factor [18] Wei Li and Ari Mozes. Computing frequent itemsets inside [19] Nicolas Pasquier, Yves Bastide, Rafik Taouil, and Lotfi [20] Dmitry Pavlov, Heikki Mannila, and Padhraic Smyth. [21] Jr. Roberto J. Bayardo. Efficiently mining long patterns [22] G. A. F. Seber and C. J. Wild. Nonlinear Regression . John [23] Craig Utley. Microsoft sql server 9.0 technical articles: [24] Chao Wang and Srinivasan Parthasarathy. Summarizing [25] Takashi Washio and Hiroshi Motoda. State of the art of [26] Dong Xin, Hong Cheng, Xifeng Yan, and Jiawei Han. [27] Dong Xin, Jiawei Han, Xifeng Yan, and Hong Cheng. [28] Xifeng Yan, Hong Cheng, Jiawei Han, and Dong Xin. [29] M. T. Yang, R. Kasturi, and A. Sivasubramaniam. An [30] Takeshi Yoshizawa, Iko Pramudiono, and Masaru [31] Mohammed J. Zaki. Efficiently mining frequent trees in a [32] Mohammed J. Zaki and Charu C. Aggarwal. Xrules: an
