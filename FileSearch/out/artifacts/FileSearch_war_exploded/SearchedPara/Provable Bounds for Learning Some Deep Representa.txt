 Google Research, New York, NY 10011, USA Microsoft Research, Cambridge, MA 02142, USA Can we provide theoretical explanation for the practical success of deep nets? Like many other ML tasks, learn-ing deep neural nets is NP-hard, and in fact seems  X  X adly NP-hard X  because of many layers of hidden variables con-nected by nonlinear operations. Usually one imagines that NP-hardness is not a barrier to provable algorithms in ML because the inputs to the learner are drawn from some sim-ple distribution and are not worst-case. This hope was recently borne out in case of generative models such as HMMs, Gaussian Mixtures, LDA etc., for which learn-ing algorithms with provable guarantees were given (Hsu et al., 2012; Moitra &amp; Valiant, 2010; Hsu &amp; Kakade, 2013; Arora et al., 2012; Anandkumar et al., 2012). However, supervised learning of neural nets even on random inputs still seems as hard as cracking cryptographic schemes: this holds for depth-5 neural nets (Jackson et al., 2002) and even ANDs of thresholds (a simple depth two network) (Klivans &amp; Sherstov, 2009).
 However, modern deep nets are not  X  just X  neural nets (see the survey (Bengio, 2009)). Instead, the net (or a related net) run in reverse from top to bottom is also seen as a a generative model for the input distribution. Hinton pro-moted this viewpoint, and suggested modeling each level as a Restricted Boltzmann Machine (RBM), which is  X  re-versible X  in this sense. Vincent et al. (Vincent et al., 2008) suggested modeling each level as a denoising autoencoder , consisting of a pair of encoder-decoder functions (see Def-inition 1). These viewpoints allow unsupervised, and lay-erwise learning in contrast to the supervised learning view-point in classical backpropagation. The bottom layer (be-tween observed variables and first layer of hidden vari-ables) is learnt in unsupervised fashion using the provided data. This gives values for the first layer of hidden vari-ables, which are used as the data to learn the next layer, and so on. The final net thus learnt is also a good generative model for the distribution of the observed layer. In practice the unsupervised phase is usually used for pretraining , and is followed by supervised training 1 .
 This viewpoint of reversible deep nets is more promising for theoretical work and naturally suggests the following problem: Given samples from a sparsely connected neural network whose each layer is a denoising autoencoder, can the net (and hence its reverse) be learnt in polynomial time with low sample complexity? Many barriers present them-selves in solving such a question. There is no known math-ematical condition that describes when a layer in a neural net is a denoising autoencoder. Furthermore, learning even a a single layer sparse denoising autoencoder seems much harder than learning sparse-used overcomplete dictionar-ies  X  X hich correspond to a single hidden layer with lin-ear operations X  and even for this much simpler problem there were no provable bounds at all until the very recent manuscript (Arora et al., 2013) 2 .
 The current paper presents an interesting family of de-noising autoencoders  X  X amely, a sparse network whose weights are randomly chosen in [  X  1 , 1] ; see Section 2 for details X  X s well as new algorithms to provably learn almost all models in this family with low time and sample com-plexity; see Theorem 1. The algorithm outputs a network whose generative behavior is statistically (in ` 1 norm) in-distinguishable from the ground truth net. (If the weights are discrete, say in { X  1 , 1 } then it exactly learns the ground truth net.) Along the way we exhibit interesting properties of such randomly-generated neural nets. (a) Each pair of adja-cent layers constitutes a denoising autoencoder whp see Lemma 2. In other words, the decoder (i.e., sparse neural net with random weights) assumed in the generative model has an associated encoder which is actually the same net run in reverse but with different thresholds at the nodes. (b) The reverse computation is stable to dropouts and noise. (c) The distribution generated by a two-layer net cannot be represented by any single layer neural net (see Section 8), which in turn suggests that a random t-layer network can-not be represented by any t/ 2 -level neural net 3 . Note that properties (a) to (c) are assumed in modern deep net work (e.g., reversibility is called  X  X eight ty-ing X ) whereas they provably hold for our random genera-tive model, which is perhaps some theoretical validation of those assumptions.
 Context. Recent theoretical papers have analysed multi-level models with hidden features, including SVMs (Cho &amp; Saul, 2009; Livni et al., 2013). However, none solves the task of recovering a ground-truth neural network as we do.
 Though real-life neural nets are not random, our consid-eration of random deep networks makes sense for obtain-ing theoretical results. Sparse denoising autoencoders are reminiscent of other objects such as error-correcting codes, compressed sensing matrices, etc. which were all first analysed in the random case. As mentioned, provable re-construction of the hidden layer (i.e., input encoding) in a known single layer neural net already seems a nonlin-ear generalization of compressed sensing, whereas even the usual (linear) version of compressed sensing seems possible only if the adjacency matrix has  X  X andom-like X  properties (low coherence or restricted isometry or loss-less expansion). In fact our result that a single layer of our generative model is a sparse denoising autoencoder can be seen as an analog of the fact that random matrices are good for compressed sensing/sparse reconstruction (see Donoho (Donoho, 2006) for general matrices and Berinde et al. (Berinde et al., 2008) for sparse matrices). Of course, in compressed sensing the matrix of edge weights is known whereas here it has to be learnt, which is the main contribu-tion of our work. Furthermore, we show that our algorithm for learning a single layer of weights can be extended to do layerwise learning of the entire network.
 Does our algorithm yield new approaches in practice? We discuss this possibility after sketching our algorithm in the next section. Our generative model ( X  X round truth X ) has ` hidden layers of vectors of binary variables h ( ` ) , h ( `  X  1) , .., h denoted by h (0) ) at bottom (see Figure 1). Each layer has n nodes 4 . The weighted graph between layers i and i + 1 is denoted by G i = ( U i ,V i ,E i ,W i ) . Values at U i correspond between them and W i is the weighted adjacency matrix. The degree of this graph is d = n  X  , and all edge weights are in [  X  1 , 1] .
 The samples are generated by propagating down the net-work to the observations. First sample the top layer h ( ` ) where the set of nodes that are 1 is picked uniformly among all sets of size  X  ` n . We call  X  ` the sparsity of top level (sim-ilarly  X  i will approximately denote the sparsity of level i ). For i = ` down to 2 , each node in layer i  X  1 computes a weighted sum of its neighbors in layer i , and becomes 1 iff that sum strictly exceeds 0 . We will use sgn( x ) to denote the threshold function that is 1 if x &gt; 0 and 0 else. (Ap-plying sgn() to a vector involves applying it component-wise.) Thus the network computes as follows: h ( i  X  1) = no threshold at the observed layer). Random deep net model: We assume that in the ground truth network, the edges between layers are chosen ran-domly subject to expected degree d being 5 n  X  , where  X  &lt; 1 / ( ` +1) , and each edge e  X  E i carries a weight that is cho-sen randomly in [  X  1 , 1] . This is our model R ( `, X  ` , { G We also consider  X  X ecause it leads to a simpler and more efficient learner X  X  model where edge weights are random in { X  1 } instead of [  X  1 , 1] ; this is called D ( `, X  Recall that  X  ` &gt; 0 is such that the 0 / 1 vector input at the top layer has 1  X  X  in a random subset of  X  ` n nodes. It can be seen that since the network is random of degree d , applying a  X  ` n -sparse vector at the top layer is likely to produce the following density of 1  X  X  (approximately) at the successive layers:  X  ` d/ 2 , X  ` ( d/ 2) 2 , etc.. We assume the density of last layer  X  ` d ` / 2 ` = O (1) . This way the density at the last-but-one layer is o (1) , and the last layer is real-valued and dense.
 Now we state our main result. Note that 1 / X  ` is at most n . Theorem 1 When degree d = n  X  for 0 &lt;  X   X  0 . 2 , density  X  ` ( d/ 2) ` = C for some large constant C 6 , the network model D ( `, X  ` , { G i } ) can be learnt 7 using O (log n/ X  2 ` ) samples and O ( n 2 ` ) time. The network model R ( `, X  ` , { G i } ) can be learnt in polynomial time and using Algorithmic ideas. We are unable to analyse existing al-gorithms. Instead, we give new learning algorithms that exploit the very same structure that makes these random networks interesting in the first place i.e., each layer is a denoising autoencoder. The crux of the algorithm is a new twist on the old Hebbian rule (Hebb, 1949) that  X  X hings that fire together wire together. X  In the setting of layerwise learning, this is adapted as follows:  X  X odes in the same layer that fire together a lot are likely to be connected (with positive weight) to the same node at the higher layer. X  The algorithm consists of looking for such pairwise (or 3 -wise) correlations and putting together this information globally. The global procedure boils down to the graph-theoretic problem of reconstructing a bipartite graph given pairs of nodes that share a common neighbor (see Section 6). This is a variant of the GRAPH SQUARE ROOT problem which is NP-complete on worst-case instances but solvable for sparse random (or random-like) graphs.
 Note that existing neural net algorithms (to the extent that they are Hebbian) can also be seen as leveraging corre-lations. But putting together this information is done via the language of nonconvex optimization (i.e., an objective function with suitable penalty terms). Our ground truth network is indeed a particular local optimum in any rea-sonable formulation. It would be interesting to show that existing algorithms provably find the ground truth in poly-nomial time but currently this seems difficult.
 Can our new ideas be useful in practice? We think that using a global reconstruction procedure that leverages lo-cal correlations seems promising, especially if it avoids nonconvex optimization. Our proof currently needs that the hidden layers are sparse, and the edge structure of the ground truth network is  X  X andom like X  (in the sense that two distinct features at a level tend to affect fairly disjoint sets of features at the next level). We need this assumption both for inferring correlations and the reconstruction step. Finally, note that random neural nets do seem useful in so-called reservoir computing, and more structured neu-ral nets with random edge weights are also interesting in feature learning (Saxe et al., 2011). So perhaps they do provide useful representational power on real data. Such empirical study is left for future work.
 Throughout, we need well-known properties of random graphs with expected degree d , such as the fact that they are expanders (Hoory et al., 2006); these properties appear in the supplementary material. The most important one, unique neighbors property, appears in the next Section. As mentioned earlier, modern deep nets research often as-sumes that the net (or at least some layers in it) should approximately preserve information, and even allows easy going back/forth between representations in two adjacent layers (what we earlier called  X  X eversibility X ). Below, y de-notes the lower layer and h the higher (hidden) layer. Pop-ular choices of s include logistic function, soft max, etc.; we use a simple threshold function in our model.
 coder consists of a decoding function D ( h ) = s ( Wh + b ) and an encoding function E ( y ) = s ( W 0 y + b 0 ) where W,W 0 are linear transformations, b,b 0 are fixed vectors and s is a nonlinear function that acts identically on each coordinate. The autoencoder is denoising if E ( D ( h )+  X  ) = h with high probability where h is drawn from the distribu-tion of the hidden layer,  X  is a noise vector drawn from the noise distribution, and D ( h ) +  X  is a shorthand for  X  D ( h ) corrupted with noise  X  . X  The autoencoder is said to use weight tying if W 0 = W T .
 In empirical work the denoising autoencoder property is only implicitly imposed on the deep net by minimizing the reconstruction error || y  X  D ( E ( y +  X  )) || , where  X  is the noise vector. Our definition is slightly different but is ac-tually stronger since y is exactly D ( h ) according to the generative model. Our definition implies the existence of an encoder E that makes the penalty term exactly zero. We show that in our ground truth net (whether from model D ( `, X  ` , { G i } ) or R ( `, X  ` , { G i } ) ) every graph G isfies this definition, and with weight-tying.
 We show a single-layer random network is a denoising au-toencoder if the input layer is a random  X n sparse vector, and the output layer has density  X d/ 2 &lt; 1 / 20 . Lemma 2 If  X d &lt; 0 . 1 (i.e., the bottom layer is also fairly sparse) then the single-layer network G ( U,V,E,W ) in coder with high probability (over the choice of the random graph and weights), where the noise flips every output bit independently with probability 0 . 1 . It uses weight tying. The proof of this lemma highly relies on a property of ran-dom graph, called the strong unique-neighbor property. For any node u  X  U , let F ( u ) be its neighbors in V ; for any subset S  X  U , let UF ( u,S ) be the set of unique neighbors of u with respect to S , i.e., Property 1 In a bipartite graph G ( U,V,E,W ) , a node u  X  U has (1  X  ) -unique neighbor property (UNP) with respect to S if The set S has (1  X  ) -strong UNP if every node in U has (1  X  ) -UNP with respect to S .
 Note that we don X  X  need this property to hold for all sets of size  X n (which is possible if  X d is much smaller). However, for any fixed set S of size  X n , this property holds with high probability over the randomness of the graph.
 Now we sketch the proof for Lemma 2 (details are in full version) when the edge weights are in { X  1 , 1 } . First, the decoder definition is implicit in our generative model: y = sgn( Wh ) . (That is, b = ~ 0 in the autoencoder definition.) Let the encoder be E ( y ) = sgn( W T y + b 0 ) for b 0 = 0 . 2 d  X  ~ 1 .In other words, the same bipartite graph and different thresholds can transform an assignment on the lower level to the one at the higher level.
 To prove this consider the strong unique-neighbor property of the network. For the set of nodes that are 1 at the higher level, a majority of their neighbors at the lower level are unique neighbors.The unique neighbors with positive edges will always be 1 because there are no  X  1 edges that can cancel the +1 edge (similarly the unique neighbors with negative edges will always be 0). Thus by looking at the set of nodes that are 1 at the lower level, one can infer the correct 0 / 1 assignment to the higher level by doing a sim-ple threshold of say 0 . 2 d at each node in the higher layer. Our algorithm, outlined below (Algorithm 1), learns the network layer by layer starting from the bottom. We now focus on learning a single-layer network, which as we noted amounts to learning nonlinear dictionaries with ran-dom dictionary elements. The algorithm illustrates how we leverage the sparsity and the randomness of the edges, and use pairwise or 3-wise correlations combined with Recov-erGraph procedure of Section 6.
 Algorithm 1 High Level Algorithm Input: samples y  X  X  generated by a deep network de-Output: the network/encoder and decoder functions 1: for i = 1 TO l do 2: Construct correlation graph using samples of h ( i  X  1) 3: Call RecoverGraph to learn the positive edges E + i 5: Use LearnGraph/LearnDecoder to learn the 6: end for For simplicity we describe the algorithm when edge weights are { X  1 , 1 } , and sketch the differences for real-valued weights at the end of this section.
 As before we use  X  to denote the sparsity of the hidden layer. Say two nodes in the observed layer are related if they have a common neighbor in the hidden layer to which they are attached via a +1 edge. The algorithm first tries to identify all related nodes, then use this information to learn all the positive edges. With positive edges we show it is already possible to recover the hidden variables. Fi-nally given both hidden variables and observed variables the algorithm finds all negative edges.
 S
TEP 1: Construct correlation graph: This step uses a new twist on the Hebbian rule to identify correlated nodes. 9 Algorithm 2 PairwiseGraph Input: N = O (log n/ X  ) samples of y = sgn( Wh ) , Output:  X  G on vertices V , u,v connected if related for each u,v in the output layer do end for Claim In a random sample of the output layer, related pairs u,v are both 1 with probability at least 0 . 9  X  , while unre-lated pairs are both 1 with probability at most (  X d ) 2 . (Proof Sketch): First consider a related pair u,v , and let z be a vertex with +1 edges to u , v . Let S be the set of neighbors of u , v excluding z . The size of S cannot be much larger than 2 d . Under the choice of parameters, we know  X d 1 , so the event h S = ~ 0 conditioned on h z = 1 has probability at least 0.9. Hence the probability of u and v being both 1 is at least 0 . 9  X  . Conversely, if u,v are unrelated then for both u,v to be 1 there must be nodes y and z such that h y = h z = 1 , and are connected to u and v respectively via +1 edges. The probability of this event is at most (  X d ) 2 by union bound.
 Thus, if (  X d ) 2 &lt; 0 . 1  X  , using O (log n/ X  2 ) samples we can estimate the correlation of all pairs accurately enough, and hence recover all related pairs whp.
 S
TEP 2: Use RecoverGraph procedure to find all edges that have weight +1 . (See Section 6 for details.) S TEP 3: Using the +1 edges to encode all the samples y . Algorithm 3 PartialEncoder Input: positive edges E + , y = sgn( Wh ) , threshold  X  Output: the hidden variable h
Let M be the indicator matrix of E + ( M i,j = 1 iff ( i,j )  X  E + ) return h = sgn( M T y  X   X  ~ 1) Although we have only recovered the positive edges, we can use P ARTIAL E NCODER algorithm to get h given y ! Lemma 3 If support of h satisfies 11 / 12 -strong unique neighbor property, and y = sgn( Wh ) , then Algorithm 3 outputs h with  X  = 0 . 3 d .
 This uses the unique neighbor property: for every z with h z = 1 , it has at least 0 . 4 d unique neighbors that are con-nected with +1 edges. All these neighbors must be 1 so [( E + ) T y ] z  X  0 . 4 d . On the other hand, for any z with h z = 0 , the unique neighbor property implies that z can have at most 0 . 2 d positive edges to the +1  X  X  in h . Hence h = sgn(( E + ) T y  X  0 . 3 d ~ 1) .
 S TEP 4: Recover all weight  X  1 edges.
 Algorithm 4 Learning Graph Input: positive edges E + , samples of ( h,y ) Output: E  X  1: R  X  ( U  X  V ) \ E + . 2: for each of the samples ( h,y ) , and each v do 3: Let S be the support of h 4: if y v = 1 and S  X  B + ( v ) = { u } for some u then 5: For all s  X  S remove ( s,v ) from R 6: end if 7: end for 8: return R Now consider many pairs of ( h,y ) , where h is found using Step 3. Suppose in some sample, y u = 1 for some u , and exactly one neighbor of u in the +1 edge graph (which we know entirely) is in supp ( h ) . Then we can conclude that for any z with h z = 1 , there cannot be a  X  1 edge ( z,u ) , as this would cancel out the unique +1 contribution.
 Lemma 4 Given O (log n/ (  X  2 d )) samples of pairs ( h,y ) , with high probability (over the random graph and the sam-ples) Algorithm 4 outputs the correct set E  X  .
 To prove this lemma, we just need to bound the probability of the following events for any non-edge ( x,u ) : h x = 1 , | supp( h )  X  B + ( u ) | = 1 , supp( h )  X  B  X  ( u ) =  X  ( B + ,B  X  are positive and negative parents). These three events are almost independent, the first has probability  X  , second has probability  X   X d and the third has probability almost 1. Leveraging 3 -wise correlation: The above sketch used pairwise correlations to recover the +1 weights when  X  1 /d 2 . It turns out that using 3 -wise correlations allow us to find correlations under a weaker requirement  X  &lt; 1 /d 3 / 2 Now call three observed nodes u,v,s related if they are connected to a common node at the hidden layer via +1 edges. Then we can prove a claim analogous to the one above, which says that for a related triple, the probability that u,v,s are all 1 is at least 0 . 9  X  , while the probability for unrelated triples is roughly at most (  X d ) 3 . Thus as long as The RecoverGraph algorithm can be modified to run on 3 -uniform hypergraph consisting of these related triples to recover the +1 edges.
 The end result is the following theorem. This is the algo-rithm used to get the bounds stated in our main theorem. Theorem 5 Suppose our generative neural net model with weights { X  1 , 1 } has a single layer and the assignment of the hidden layer is a random  X n -sparse vector, with O ( n ( d 3 + n )) time and uses O (log n/ X  2 ) samples to re-cover the ground truth with high probability over the ran-domness of the graph and the samples.
 When weights are real numbers. We only sketch this and leave the details to the full version. Surprisingly, steps 1, 2 and 3 still work. In the proofs, we have only used the sign of the edge weights  X  the magnitude of the edge weights can be arbitrary. This is because the proofs in these steps relies on the unique neighbor property. If some node is on (has value 1 ), then its unique positive neighbors at the next level will always be on, no matter how small the positive weights might be. Also notice in PartialEncoder we are only using the support of E + , but not the weights. After Step 3 we have turned the problem of unsupervised learning to a supervised one in which the outputs are just linear classifiers over the inputs! Thus the weights on the edges can be learnt to any desired accuracy. We now consider multi-layer networks, and show how they can be learnt layerwise using a slight modification of our one-layer algorithm at each layer. At a technical level, the difficulty in the analysis is the following: in single-layer learning, we assumed that the higher layer X  X  assignment is a random  X n -sparse binary vector, however in the multilayer network, the assignments in intermediate layers do not sat-isfy this. We will show nonetheless that the the correlations at intermediate layers are low enough, so our algorithm can still work. Again for simplicity we describe the algorithm for the model D ( `, X  ` , { G i } ) , in which the edge weights are  X  1 . Recall that  X  i  X  X  are the  X  X xpected X  number of 1 s in the layer h ( i ) . Because of the unique neighbor property, we expect roughly  X  ` ( d/ 2) fraction of h ( `  X  1) to be 1 . Hence  X  Lemma 6 Consider a network from D ( `, X  ` , { G i } ) . With high probability (over the random graphs between layers) for any two nodes u,v in layer h (1) , (Proof Sketch): Call a node s at the topmost layer an an-cestor of u if there is a path of length `  X  1 from s to u . The number of ancestors of a node u is roughly 2 `  X  1  X  1 With good probability there is at most one ancestor of u that has value 1.Call s a positive ancestor of u , if when only s is 1 at the topmost layer, the node u is 1. The number of positive ancestors of u is roughly  X  1 / X  ` .
 The probability that a node u is on is roughly proportional to the number of positive ancestors. For a pair of nodes, if they are both 1, either one of their common positive ances-tor is 1, or both of them have a positive ancestor that is 1. The probability of the latter is O (  X  2 1 ) which by assumption is much smaller than  X  2 .
 When u,v share a common positive parent z , the positive ancestors of z are all common positive ancestors of u , v (so there are at least  X  2 / X  ` ). The probability that one of positive common ancestor is on and no other ancestors are on is at least  X  2 / 2 .
 If u,v do not share a common parent, then the number of their common positive ancestors depends on how many  X  X ommon grandparents X  (common neighbors in layer 3) they have. We show with high probability (over the graph structure) the number of common positive ancestors is small. Therefore the probability that both u , v are 1 is small. Once we can identify related pairs by Lemma 6, the Steps 2,3,4 in the single layer algorithm still work. We can learn the bottom layer graph and get h (2) . This argument can be repeated after  X  X eeling off X  the bottom layer, thus allowing us to learn layer by layer. Graph reconstruction consists of recovering a graph given information about its subgraphs (Bondy &amp; Hemminger, 1977). A prototypical problem is the Graph Square Root problem, which calls for recovering a graph given all pairs of nodes whose distance is at most 2 . This is NP-hard. Definition 2 (Graph Recovery) Let G 1 ( U,V,E 1 ) be an unknown random bipartite graph between | U | = n and | V | = n nodes where each edge is picked with probability d/n independently.
 Given: Graph G ( V,E ) where ( v 1 ,v 2 )  X  E iff v 1 and v share a common parent in G 1 (i.e.  X  u  X  U where ( u,v 1 )  X  E 1 and ( u,v 2 )  X  E 1 ).
 Goal: Find the bipartite graph G 1 .
 Some of our algorithms (using 3 -wise correlations) need to solve analogous problem where we are given triples of nodes which are mutually at distance 2 from each other, which we will not detail for lack of space.
 We let F ( S ) (resp. B ( S ) ) denote the set of neighbors of S  X  U (resp.  X  V ) in G 1 . Also  X (  X  ) gives the set of neigh-bors in G . Now for the recovery algorithm to work, we need the following properties (all satisfied whp by random graph when d 3 /n 1 ): 1. For any v 1 ,v 2  X  V , 2. For any u 1 ,u 2  X  U , | F ( u 1 )  X  F ( u 2 ) | &gt; 1 . 5 d . 3. For any u  X  U , v  X  V and v 6 X  F ( u ) , 4. For any u  X  U , at least 0 . 1 fraction of pairs v 1 ,v The first property says  X  X ost correlations are generated by common cause X : all but possibly d/ 20 of the common neighbors of v 1 and v 2 in G , are in fact neighbors of a com-mon neighbor of v 1 and v 2 in G 1 .
 The second property says the sets F ( u ) should not overlap much. This is clear because the sets are random.
 The third property says if a node v is not  X  X aused by X  u , then it is not correlated with too many neighbors of u . The fourth property says every cause introduces a signifi-cant number of correlations that are unique to that cause. In fact, Properties 2-4 are closely related from the unique neighbor property.
 Lemma 7 When graph G 1 satisfies Properties 1-4, Algo-rithm 5 successfully recovers G 1 in expected time O ( n 2 ) (Proof Sketch): We first show that when ( v 1 ,v 2 ) has more than one unique common cause, then the condition in the if statement must be false. This follows from Property 2. We know the set S contains F ( B ( v 1 )  X  B ( v 2 | B ( v 1 )  X  B ( v 2 ) |  X  2 then Property 2 says | S |  X  1 . 5 d , which implies the condition in the if statement is false. Then we show if ( v 1 ,v 2 ) has a unique common cause u , then S 0 will be equal to F ( u ) . By Property 1, we know S = F ( u )  X  T where | T |  X  d/ 20 . Now, every v in F ( u ) is connected to every other node in F ( u ) . Therefore |  X ( v )  X  S | X |  X ( v )  X  F ( u ) | X  0 . 8 d  X  1 , and v  X  S 0 . For any node v 0 outside F ( u ) , by Property 3 it can only be connected to d/ 20 nodes in F ( u ) . Therefore |  X ( v )  X  S | X  |  X ( v )  X  F ( u ) | + | T |  X  d/ 10 . Hence v 0 is not in S 0 . Fol-lowing these arguments, S 0 must be equal to F ( u ) , and the algorithm successfully learns the edges related to u . The algorithm will successfully find all nodes u  X  U be-cause of Property 4: for every u there are enough number of edges in G that is only caused by u . When one of them is sampled, the algorithm successfully learns the node u . Finally we bound the running time. By Property 4 we know that the algorithm identifies a new node u  X  U in at most 10 iterations in expectation. Each iteration takes at most O ( n ) time. Therefore the algorithm takes at most O ( n 2 ) time in expectation.
 Algorithm 5 RecoverGraph Input: G given as in Definition 2 Output: Find the graph G 1 as in Definition 2. repeat until all edges are marked Note that in our model, the lowest (observed) layer is real-valued and does not have threshold gates. Thus our earlier learning algorithm cannot be applied as is. However, we see that the same paradigm  X  identifying correlations and using RecoverGraph  X  can be used.
 The first step is to show that for a random weighted graph G , the linear decoder D ( h ) = Wh and the encoder E ( y ) = sgn( W T y + b ) form a denoising autoencoder with real-valued outputs, as in Bengio et al. (Bengio et al., 2013). Lemma 8 If G is a random weighted graph, the encoder E ( y ) = sgn( W T y  X  0 . 4 d ~ 1) and linear decoder D ( h ) = Wh form a denoising autoencoder, for noise vectors  X  which have independent components, each having variance at most O ( d/ log 2 n ) .
 The next step is to show a bound on correlations as before. For simplicity we state it assuming the layer h (1) has a ran-dom 0 / 1 assignment of sparsity  X  1 . In the full version we consider the correlations introduced by higher layers as we did in Section 5.
 Theorem 9 When  X  1 d = O (1) , d =  X (log 2 n ) , with high probability over the choice of the weights and the choice of the graph, for any three nodes u,v,s the assignment y to the bottom layer satisfies: 1. If u,v and s have no common neighbor, then 2. If u,v and s have a unique common neighbor, then In this section we show that a two-layer network with  X  1 weights is more expressive than one layer network with ar-bitrary weights. A two-layer network ( G 1 ,G 2 ) consists of random graphs G 1 and G 2 with random  X  1 weights on the edges. Viewed as a generative model, its input is h (3) and the output is h (1) = sgn( W 1 sgn( W 2 h (3) )) . We will show that a single-layer network even with arbitrary weights and arbitrary threshold functions must generate a fairly differ-ent distribution.
 Lemma 10 For almost all choices of ( G 1 ,G 2 ) , the follow-ing is true. For every one layer network with matrix A and vector b , if h (3) is chosen to be a random  X  3 n -sparse vector with  X  3 d 2 d 1 1 , the probability (over the choice of h (3) ) is at least  X (  X  2 3 ) that sgn( W 1 sgn( W sgn( Ah (3) + b ) .
 The idea is that the cancellations possible in the two-layer network simply cannot all be accomodated in a single-layer network even using arbitrary weights. More precisely, even the bit at a single output node v cannot be well-represented by a simple threshold function.
 First, observe that the output at v is determined by values of d 1 d 2 nodes at the top layer that are its ancestors. It is not hard to show in the one layer net ( A,b ) , there should be no edge between v and any node u that is not its an-cestor. Then consider structure in Figure 2. Assuming all other parents of v are 0 (which happens with probability at least 0 . 9 ), and focus on the values of ( u 1 When these values are (1 , 1 , 0 , 0) and (0 , 0 , 1 , 1) , v is off. When these values are (1 , 0 , 0 , 1) and (0 , 1 , 1 , 0) , v is on. This is impossible for a one layer network because the first two ask for P A P We did a basic implementation of our algorithms and ran experiments on synthetic data. The results show that our analysis has the right asymptotics, and that the constants involved are all small. As is to be expected, the most ex-pensive step is that of finding the negative edges (since the algorithm is to  X  X liminate non-edges X ). However, stopping the algorithm after a certain number of iterations gives a reasonable over-estimate for the negative edges.
 The following are the details of one experiment: a network with two hidden layers and one observed layer ( ` = 2 , both layers use thresholds) with n = 5000 , d = 16 and  X  ` n = 4 ; the weights are random { X  1 } but we made sure that each node has exactly 8 positive edges and 8 negative edges. Us-ing 500 , 000 samples, in less than one hour (on a single 2GHz processor) the algorithm correctly learned the first layer, and the positive edges of the second layer. Learn-ing the negative edges of second layer (as per our analysis) requires many more samples. However using 5  X  10 6 sam-ples, the algorithm makes only 10 errors in learning nega-tive edges. Many aspects of deep nets are mysterious to theory: re-versibility, why use denoising autoencoders, why this highly non-convex problem is solvable, etc. Our paper gives a first-cut explanation. Worst-case nets seem hard, and rigorous analysis of interesting subcases can stimu-late further development: see e.g., the role played in Bayes nets by rigorous analysis of message-passing on trees and graphs of low tree-width. We chose to study randomly gen-erated nets, which makes scientific sense (nonlinear gen-eralization of random error correcting codes, compressed sensing etc.), and also has some empirical support, e.g. in reservoir computing.
 The very concept of a denoising autoencoder (with weight tying) suggests to us a graph with random-like properties. We would be very interested in an empirical study of the randomness properties of actual deep nets learnt via super-vised backprop. (For example, in (Krizhevsky et al., 2012) the lower layers use convolution, which is decidedly non-random. But higher layers are learnt by backpropagation initialized with a complete graph and may end up more random-like.) Network randomness is not so crucial in our analysis of learning a single layer, but crucial for layerwise learning: the randomness of the graph structure is crucial for con-trolling (i.e., upper bounding) correlations among features appearing in the same hidden layer (see Lemma 6). Prov-able layerwise learning under weaker assumptions would be very interesting.
 We would like to thank Yann LeCun, Ankur Moitra, Sushant Sachdeva, Linpeng Tang for numerous helpful dis-cussions throughout various stages of this work and anony-mous reviewers for their helpful feedback and comments. Anandkumar, Anima, Foster, Dean P., Hsu, Daniel,
Kakade, Sham M., and Liu, Yi-Kai. A spectral algorithm for latent Dirichlet allocation. In Advances in Neural In-formation Processing Systems 25 , 2012.
 Arora, Sanjeev., Ge, Rong., and Moitra, Ankur. Learning topic models  X  going beyond svd. In IEEE 53rd Annual
Symposium on Foundations of Computer Science, FOCS 2012, New Brunswick NJ, USA, October 20-23 , pp. 1 X  10, 2012.
 Arora, Sanjeev, Ge, Rong, and Moitra, Ankur. New algo-rithms for learning incoherent and overcomplete dictio-naries. ArXiv , 1308.6273, 2013.
 Bengio, Yoshua. Learning deep architectures for AI. Foun-dations and Trends in Machine Learning , 2(1):1 X 127, 2009. Also published as a book. Now Publishers, 2009. Bengio, Yoshua, Courville, Aaron C., and Vincent, Pas-cal. Representation learning: A review and new per-spectives. IEEE Trans. Pattern Anal. Mach. Intell. , 35 (8):1798 X 1828, 2013.
 Berinde, R., Gilbert, A.C., Indyk, P., Karloff, H., and
Strauss, M.J. Combining geometry and combinatorics: a unified approach to sparse signal recovery. In 46th An-nual Allerton Conference on Communication, Control, and Computing , pp. 798 X 805, 2008.
 Bondy, J Adrian and Hemminger, Robert L. Graph recon-structiona survey. Journal of Graph Theory , 1(3):227 X  268, 1977.
 Cho, Youngmin and Saul, Lawrence. Kernel methods for deep learning. In Advances in Neural Information Pro-cessing Systems 22 , pp. 342 X 350. 2009.
 Donoho, David L. Compressed sensing. Information The-ory, IEEE Transactions on , 52(4):1289 X 1306, 2006. Hebb, Donald O. The Organization of Behavior: A Neu-ropsychological Theory . Wiley, new edition edition, June 1949.
 Hoory, Shlomo, Linial, Nathan, and Wigderson, Avi. Ex-pander graphs and their applications. Bulletin of the American Mathematical Society , 43(4):439 X 561, 2006. Hsu, Daniel and Kakade, Sham M. Learning mixtures of spherical gaussians: moment methods and spectral de-compositions. In Proceedings of the 4th conference on
Innovations in Theoretical Computer Science , pp. 11 X  20, 2013.
 Hsu, Daniel, Kakade, Sham M., and Zhang, Tong. A spec-tral algorithm for learning hidden Markov models. Jour-nal of Computer and System Sciences , 78(5):1460 X 1480, 2012.
 Jackson, Jeffrey C, Klivans, Adam R, and Servedio,
Rocco A. Learnability beyond ac 0 . In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing , pp. 776 X 784. ACM, 2002.
 Klivans, Adam R and Sherstov, Alexander A. Crypto-graphic hardness for learning intersections of halfspaces.
Journal of Computer and System Sciences , 75(1):2 X 12, 2009.
 Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Im-agenet classification with deep convolutional neural net-works. In Advances in Neural Information Processing Systems 25 , pp. 1106 X 1114. 2012.
 Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad. A provably efficient algorithm for training deep networks. ArXiv , 1304.7045, 2013.
 Moitra, Ankur and Valiant, Gregory. Settling the polyno-mial learnability of mixtures of gaussians. In the 51st
Annual Symposium on the Foundations of Computer Sci-ence (FOCS) , 2010.
 Saxe, Andrew, Koh, Pang W, Chen, Zhenghao, Bhand,
Maneesh, Suresh, Bipin, and Ng, Andrew. On random weights and unsupervised feature learning. In Proceed-ings of the 28th International Conference on Machine Learning (ICML-11) , pp. 1089 X 1096, 2011.
 Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and
Manzagol, Pierre-Antoine. Extracting and composing robust features with denoising autoencoders. In ICML ,
