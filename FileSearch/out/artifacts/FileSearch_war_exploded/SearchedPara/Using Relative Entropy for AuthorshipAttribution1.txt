 Authorship attribution (AA) is the problem of identifying who wrote a particular document. AA techniques, which are a form of document classification, rely on collections of documents of known authorship for training, and consist of three stages: preprocessing of documents, extracti on of style markers, and classification based on the style markers. Applications of AA include plagiarism detection, doc-ument tracking, and foren sic and literary investigations. Researchers have used attribution to analyse anonymous or disputed documents [6, 14]. The question of who wrote Shakespeare X  X  plays is an AA problem. It could also be applied to verify the authorship of e-mails and newsgroup messages, or to identify the source of a piece of intelligence.

Broadly, there are three kinds of AA problem: binary, multi-class, and one-class attribution. In binary classification, all the documents are written by one of two authors and the task is to identify who of the two authors wrote unattributed documents. Several approaches to this problem have been described [4, 6, 10]. Multi-class classification [1,5,12], in which documents by more than two authors are provided, is empirically less effective than binary classification. In one-class classification, also referred to as authorship verification, some documents training are written by a particular author while the authorship of the remainder is different but unknown [15]. The task is to determine whether a given document is produced by the target author. This is more difficult again, as it is easier to characterize documents as belonging to a certain class rather than to any class except the specified one.

Existing approaches use a range of methods for extracting features, most commonly style markers such as function words [1, 5, 10, 12] and grammatical elements such as part of speech [2, 21, 22]. Given these markers, AA requires use of a classification method such as support vector machines (SVMs) [5, 15] or principal component analysis [1,10,12]. However, much previous work in the area is marred by lack of use of shared benchmark data, verification on multiple data sets, or comparison between methods X  X ach paper differs in both style markers and classification method, making it difficult to determine which element led to success of the AA method, or indeed whether AA was successful at all. It is not clear whether these methods scale, and some are ad hoc. In previous work, we compared some of these methods using common data collections [25] drawn from the TREC data [8] and other readily ava ilable sources, and found Bayesian net-works and SVMs to be superior to the other approaches given function words as tokens. A secondary contribution of this new paper is extension of this previous work to grammatical style markers.

Our primary contribution is that we propose a new AA approach based on relative entropy, measured using the Kullback-Leibler divergence [17]. Language models have been successfully used in information retrieval [24], by, in effect, finding the documents whose models give the least relative entropy for the query. Here we explore whether relative entropy can provide a reliable method of cate-gorization, where the collection of documents known to be in a category are used to derive a language model. A strong motivation for exploring such an approach is efficiency: the training process is extremely simple, consisting of identifying the distinct terms in the documents and counting their occurrences. In contrast, existing categorization methods are quadratic or exponential.

To test the proposed method, we apply it to binary and multi-class AA, using several kinds of style marker. For consistency we use the same data collections as in our previous work [25]. We observe that our method is at least as effective for binary classification as the best previous approaches, Bayesian networks and SVMs, and is more effective for multi-class classification.

In addition, we apply our method to the standard problem of categorization of documents drawn from the Reuters newswire [16]. AA is a special case of text categorization, but it does not necessarily follow that a method that is effective for AA will be effective for categorization in general, and vice versa . However, these preliminary experiments have found that KLD is indeed an effective general categorization technique, with effectiveness comparable to that of SVMs. We infer that, given appropriate feature extraction methods, the same categorization techniques can be used for either problem. The basic processes of AA consists of three stages: text preprocessing, feature extraction, and categorization. In the first stage, the text is standardized and may be annoted with lexical information. In the second stage, features are iden-tified in the transformed text. In the third stage, feature sets are compared to determine likely authorship. A variety of AA approaches have been proposed, differing in all three stages.

In style-based classification, both lexical and grammatical markers have been used. Function words are a lexical style marker that has been widely used [1, 5, 10,12], on the basis that these words carry little content: a typical author writes on many topics, but may be consistent in the use of the function words used to structure sentences. Some researchers have included punctuation symbols, while others have experimented with n -grams [13, 18, 19]. Grammatical style markers have also been used for AA [2,21,22], with natural language processing techniques are used to extract features from the documents. However, the AA performance is subject to the performance of the corresponding natural-language tools that are used.

Once stylistic features have been extracted, they must be used in the way to classify documents. Several researchers have applied machine-learning tech-niques to AA. Diederich et al. [5] and Koppel et al. [15] have used SVMs in their experiments. Diederich et al. used a collection of newspaper articles in German, with seven authors and between 82 and 118 texts for each author. Documents with fewer than 200 words were not used, as they were considered to not have enough authorial information. Accuracies of 60% to 80% were reported. The data used by Koppel et al. consists of 21 English books by a total of 10 authors. An overall accuracy of 95.7% was reported; due to the small size of data col-lection, the high accuracy may not be statistically significant. In our previous work [25], we used a large data collection and tested five well-known machine learning methods. We concluded that machine learning methods are promising approaches to AA. Amongst the five methods, Bayesian networks were the most effective.

Principle component analysis (PCA) is a statistical technique that several re-searchers have employed for AA [1,10,12]. Baayen et al. [2] used PCA on a small data collection, consisting of material from two books. Holmes et al. [10] applied PCA to identify the authorship of unknown articles that have been tentatively at-tributed to Stephen Crane. The data consisted of only fourteen articles known to have been written by Crane and seventeen articles of unknown authorship. PCA has largely been used for binary classification. In our initial investigation [25], PCA appears ineffective for multi-class classification. Additionally, PCA is not easily scalable; it is based on linear alg ebra and uses eigenvectors to determine the principle components for measuring similarity between documents. In most cases, only the first two principle components are used for classification and other components are simply discarded. Although other components may con-tain less information compared to the first two components, discarding will cause information loss, which may reduce the classification effectiveness.
Compression techniques and language models are another approach to AA, including Markov chains [14, 21, 22] and n -gram models [13, 18, 19]. Khmelev and Tweedie [14] used Markov chains to identify authorship for documents in Russian. Character level n -grams are used as style markers. An accuracy of 73% was reported as the best result in multi-class classification, but in most cases there were generally only two instances of each authors X  work, raising doubts as to the reliability of the results. Peng et al. [19] applied character level n -gram language models to a data collection of newswire articles in Greek. The col-lection contains documents by 10 authors, with 20 documents for each author. Although an average of 82% accuracy was reported, the size of collection is prob-ably too small to draw any representative conclusions. The question of whether character-level n -grams are useful as style markers is, therefore, unclear. As the full text of the documents was retained in these experiments, it is possible that the effectiveness of topic markers rather than style markers was being measured.
Another compression-based approach is to measure the change in compressed file size when an unknown document is added to a set of documents from a single author. Benedetto et al. [3] used the s tandard LZ77 compression program and reported an overall accuracy of 93%. In their experiment, each unknown text is appended to every other known text and the compression program is applied to each composite file as well as to the original text. The increase in size due to the unknown text can be calculated for each case, and the author of the file with smallest increase is assumed to be the target. However, Goodman [7] failed to reproduce the original results, instead achieving accuracy of only 53%.
More fundamentally, the approach is based on two poor premises. One is that the full text of the data is used, so that topic as well as style information is con-tributing to the outcomes; document formatting is a further confounding factor. The other premiss is that compression is an unreliable substitute for modelling. Compression techniques build a model of the data, then a coding technique uses the model to produce a compact representation. Typical coding techniques used in practice have ad hoc compromises and heuristics to allow coding to proceed at a reasonable speed, and thus may not provide a good indication of properties of the underlying model. By using off-the-shelf compression rather than examining properties of the underlying model, much accuracy may be lost, and nothing is learnt about which aspects of the modelling are successful in AA. In the next sec-tion we explore how models can be directly applied to AA in a principled manner.
For classification tasks in general, two of the most effective methods are SVMs and Bayesian networks. SVMs [20] have been successfully used in applications such as categorization and handwriting recognition. The basic principle is to find values for parameters  X  i for data points that maximize These values define a hyperplane, where the dimensions correspond to features. Whether an item is in or out of a class depends on which side of the hyperplane it lies. However, the computational complexity of SVM is a drawback. Even the best algorithm gives O ( n 2 ) computational cost, for n training samples.
A Bayesian network structure [9] is an acyclic directed graph in which there is one node in the graph for each feature and each node has a table of transi-tion probabilities for estimating probabilistic relationships between nodes based on conditional probabilities. There are two learning steps in Bayesian networks, learning of the network structure and learning of the conditional probability tables. The structure is determined by identifying which attributes have the strongest dependencies between them. The nodes, links, and probability distri-butions are the structure of the network, which describe the conditional depen-dencies. However a major drawback of this approach is that asymptotic cost is exponential, prohibiting use of Bayesian networks in many applications. Entropy measures the average uncertainty of a random variable X .Inthecase of English texts, each x  X  X could be a token such as a character or word. The entropy is given by: where p ( x ) is the probability mass function of a random character or word. H ( X ) represents the average number of bits required to represent each symbol in X . The better the model, the smaller the number of bits.

For example, we could build a model for a collection of documents by iden-tifying the set W of distinct words w , the frequency f w with which each w occurs, and the total number n = w f w of word occurrences. This model is context free, as no use is made of word order. The probability p ( w )= f w /n is the maximum likelihood for w ,and is the minimum number of bits required to represent the collection under this model. The compression-based AA techni ques considered above can be regarded as attempting to identify the collections whose models yield the lowest entropy for a new document, where however the precise modelling technique is unknown and the model is arbitrarily altered to achieve faster processing.

A difficulty in using direct entropy measurements on new documents is that the document may contain a new word w that is absent from the original model, leading to p ( w ) = 0 and undefined log 2 p ( w ).Weexaminethisissuebelow.
Another way to use entropy is to compare two models, that is, to measure the difference between two random variables. A mechanism for this measurement of relative entropy is the Kullback-Leibler divergence (KLD) [17], given by: where p ( x )and q ( x ) are two probability mass functions.

In this paper, we propose the use of KLD as a categorization technique. If a document with probability mass function p is closer to q than to q  X  X hat is, has a smaller relative entropy X  X hen, we hypothesise, the document belongs in the category corresponding to q . The method is presented in detail later.
We use simple language modelling techniques to estimate the probability mass function for each document and category. Language models provide a principle for quantifying properties of natural language. In the context of using language models for AA, we assume that the act of writing is a process of generating natural language. The author can be considered as having a model generating documents of a certain style. Therefore, the problem is to quantify how different the authors X  models are.

Given a token sequence c 1 c 2 ...c n representing a document we need to es-timate a language model for the document. In an ideal model, we would have enough data to use context to estimate a high p ( c i | c 1 ...c i  X  1 ) should be obtained for each token occurrence. However, in common with most use of language mod-els in information retrieval, we use a unigram model; for example, if the tokens are words, there are simply not enough word sequences to estimate multigram probabilities, and thus we wish only to estimate each p ( c i ) independently.
Therefore, the task is to find out a probability function to measure the proba-bility of each component that occurs in the document. The most straightforward estimation in language modelling is the maximum likelihood estimate, in which the probability of each component is given by the frequency normalized by the total number of components in that document d (or, equivalently, category C ): where f c,d is the frequency of c in d and | d | = c  X  d f c ,d . We then can determine the KLD between a document d and category C as KLD as a Classifier for Authorship Attribution Given author candidates A = { a 1 ...a j } , it is straightforward to build a model for each author by aggregating the training documents. We can build a model for an unattributed document in the same way. We can then determine the author model that is most similar to the model of the unknown document, by calculating KLD values between author models and unknown documents to identify the target author for which the KLD value is the smallest.

However, it is usually the case that some components are missing in either the training documents or the documents to be attributed. This generates an undefined value in equation 1, and thus a KLD value cannot be computed. This is a standard problem with such models, and other researchers have explored a variety of smoothing techniques [24] to calculate the probability of missing components.

The Dirichlet prior is an effective smoothing technique for text-based applica-tions, in particular information retrieval. We use Dirichlet smoothing to remove these zero probabilities, under which the probability of component c in document d (or equivalently, category C )is: where  X  is a smoothing parameter and p B ( c ) is the probability of component c in a background model . For short documents, the background probabilities domi-nate, on the principle that the evidence for the in-document probabilities is weak. As document length grows, the influence of the background model diminishes. Choice of an appropriate value for  X  is a tuning stage in the use of language models.

In principle the background model could be any source of typical statistics for components. Intuitively it makes sense to derive the model from other documents of similar type; in attributing newswire articles, for example, a background model derived from poetry seems unlikely to be appropriate. As background model, we use the aggregate of all known documents, including training and test, as this gives the largest available sample of material. There is no reason why a background model could not be formed this way in practice.

In estimating KLD, the same background model is used for documents and categories, so KLD is computed as By construction of the background model, d  X  C , so there are no zeroes in the computation. Function words are an obvious choice of feature for authorship attribution, as they are independent of the content but do represent style. A related choice of feature is punctuation, though the limited number of punctuation symbols mean that their discrimination power must be low.

An alternative is to use lexical elements. We explored the use of parts of speech, that is, lexical categories. Linguists recognize four major categories of words in English: nouns, verbs, adjectives, and adverbs. Each of these types can be further classified according to morphology. Most part-of-speech tag sets make use of the same basic categories; however, tag sets differ in how finely words are divided into categories, and in how categories are defined.

In this paper, we propose the following approach to use of parts of speech in authorship attribution. We applied NLTK (a Natural Language ToolKit) 1 to extract the part-of-speech tags from each original document. The part-of-speech tag set we used to tag our data collection in text preprocessing is the  X  X rown X  tag set. For simplicity, and to ensure that our feature space was not too sparse, we condensed the number of distinct tags from 116 to 27, giving basic word classes whose statistical properties could be analysed.

A further refinement is to combine the classes. We explore combinations of function words, parts of speech, and punctuation as features in our experiments. We used experiments on a range of data sources to examine effectiveness and scalability of KLD for attribution. In preliminary experiments, we also exam-ined the effectiveness of KLD for other types of classification problems. Several data collections were used in our experiments, including newswire articles from the Associated Press (AP) collection [8], E nglish literature from the Gutenberg Project, and the Reuters-21578 test collection [16]. The first two data collec-tions are used for AA. The Reuters-21578 test collection was used to examine the applicability of KLD for general categorization.
 AP. From the AP newswire collection we have selected seven authors who each contributed over 800 documents. The average document length is 724 words. These documents are splitted into training and testing groups. The number of documents used for training was varied to examine the scalability of the methods. This collection was used in our previous work [25].
 Gutenberg project. We wanted to test our technique on literary works, and thus selected the works of five well known authors from the Gutenberg project 2 : Haggard , Hardy , Tolstoy , Trol lope ,and Twain . Each book is divided into chap-ters and splitted for training and testing. Our collection consists of 137 books containing 4335 chapters. The number of chapters from each author ranges from 492 to 1174, and the average chapter length is 3177 words. In our experiments, the number of chapters used for training is randomly selected and varied. Reuters-21578. These documents are from the Reuters newswire in 1987, and have been used as a benchmark for general text categorization tasks. There are 21578 documents. We use the Modapte split [16] to group documents for training and testing. The top eight categories a re selected as the target classes; these are acq , crude , earn , grain , interest , money-fx , ship ,and trade .
We used the KLD method in a variety of ways to examine robustness and scalability of classification. We first conducted experiments for two-class clas-sification , that is, to discriminate between two known authors. In this context, all the documents used for training and testing are written by either one of these two candidates. Multi-class classification , also called n -class classification for any n  X  2, is the extension of two-class classification to arbitrary numbers of authors.

We applied KLD classification to all three data collections for both binary classification and n -class classification. In all experiments, we compared our pro-posed KLD language model method to Bayesian networks, which was the most effective and scalable classification method in our previous work [25]. In addi-tion, we have made the first comparison between a KLD classifier with SVM, a successful machine learning method for classification. We used leave-one-out validation method to avoid the overfitting problem and estimate the true error rate for classification. The linear kernel was selected as most text categorization problems are linear separable [11]. More complex kernel functions have not been shown to significantly increase the classification rate [20, 23]. The package used in our experiments is SVM-light . 3
We also investigated the significance of different types of features that can be used to mark authorial structure of a particular document. As discussed above, we have used function words, parts of speech, and punctuation as features; these were used both separately and in combination.
 Two-class experiments. Our experiments were for the two-class classification task. The results were reported in Table 1, where outcomes are averaged across all 21 pairs of authors, because signific ant inconsistencies were observed from one pair of authors to another in our previous reported experiments [25]. We tested different values of  X  : 10, 10 2 ,10 3 ,and10 4 .

We observed that the best results were obtained for value of  X  =10 2 and  X  =10 3 . To examine the scalability of KLD attribution, we have increased the number of documents used for training and maintained the same set of test documents. As can be seen, the accuracy of classification increases as the number of documents for training is increased, but appears to plateau. The KLD method is markedly more effective than the Bayesian network classifier. With a small number of documents for modelling, the KLD method is more effective than SVM, while with a larger number of documents SVM is slightly superior.
As noted earlier, the computational cost of the SVM and Bayesian network methods is quadratic or exponential, whereas the KLD method is approximately linear in the number of distinct features. It is thus expected to be much more efficient; however, the diversity of the implementations we used made it difficult to meaningfully compare efficiency.

We next examined discrimination power of different feature types, using KLD classification on the two class classification task. As discussed above, we used function words, part-of-speech (POS) tags, POS with punctuation, and a com-bined feature set containing all previous three types of feature. Results are re-ported in Table 2, which shows the average effectiveness from the 21 pairs of authors. Function words were best in al l cases, and so we concentrated on these in subsequent experiments. With all feature types, effectiveness improved with volume of training data, but only up to a point.
 We then tested KLD attribution on the Gutenberg data we had gathered. Average effectiveness is reported in Table 3. The trends were similar to those observed on the AP collection. Again, our proposed KLD method is consistently more effective than Bayesian networks, and SVM is more effective than KLD only when a larger number of training documents is used; when SVM is superior, the difference is slight. In combination these results show that KLD attribution can be successfully used for binary attribution.

We applied the KLD approach to the Gutenberg data to examine the dis-crimination power of different feature types. Results are shown in Table 4. In one case, the combined feature set is superior; in the remainder, the best feature type is again the function words.
 Multi-class experiments. We next examined the performance of the KLD method when applied to multi-class classification. In the two-class experiments, the func-tion words were the best at discrimination amongst different author styles; in the following experiments, then, we compared Bayesian networks and the KLD classification method using only function words as the feature set. SVMs were not used, as they cannot be directly applied to multi-class classification. For each test, we used 50 and 300 documents from each author for training. The outcomes were again averaged from all possible author combinations, that is 21 combinations for 2 and 5 authors, and 35 combinations for 3 and 4 authors. As shown in Table 5, with appropriate  X  values, the KLD approach consistently and substantially outperforms Bayesian networks. Smaller values of  X  are the more effective, demonstrating that the influence of the background model should be kept low.

We then ran the corresponding experiments on the Gutenberg data, as shown in Table 6. The outcomes were the same as that on the AP data, illustrating that the method and parameter settings appear to be consistent between collections. General text categorization. In order to determine the suitability of KLD classifi-cation for other types of classification tasks, we used the Reuters-21578 collection to test topic-based classification using KLD. In the Reuters-21578 data collection, documents are often assigned to more than one category. (This is a contrast to AA, in which each document has only one class.) In our experi-ment, we chose the first category as the labelled class, as it is the main category for that document. In common with standard topic classification approaches we used all document terms as the classification features.

In these preliminary experiments X  X e do not claim to have thoroughly ex-plored the application of KLD to general categorization X  X e tested n -class clas-sification, where n = 8, both with and without stemming. We compared KLD classification and SVM in terms of precision, recall, and overall accuracy. Accu-racy measures the number of documents correctly classified. Thus for any given category, it is calculated as the total number of documents correctly classified as belonging to that category, plus the total number of documents correctly classified as not belonging to that category, divided by the total number of doc-uments classified. Results are shown in T able 7. KLD classification consistently achieves higher recall than SVMs, but with worse precision and slightly lower accuracy. We conclude that KLD classificat ion is a plausible method for general text categorization, but that further exploration is required to establish how best it should be used for this problem. We have proposed the use of relative entropy as a method for identifying author-ship of unattributed documents. Simple language models have formed the basis of a recent series of developments in information retrieval, and have the advantage of simplicity and efficiency. Following simple information theoretic principles, we have shown that a basic measure of relative entropy, the Kullback-Leibler divergence, is an effective attribution method.

Here and in other work we have explored alternative attribution methods based on machine learning methods. These methods are computationally ex-pensive and, despite their sophistication, at their best can only equal relative entropy. We have also explored other feature extraction methods, but the results show that function words provide a better style marker than do tokens based on parts of speech or patterns of punctuation. Compared to these previous meth-ods, we conclude that relative entropy, based on function word distributions, is efficient and effective for two-class and multi-class authorship attribution. Acknowledgements. This work was supported by the Australian Research Council.

