 Dimensionality reduction is introduced as a way to overcome the curse of dimen-sionality when dealing with high-dimensional data and as a modeling tool for such data. There are usually two kinds of methods for dimensionality reduction: linear and nonlinear methods. Linear subspace methods are the most popular linear methods, including PCA (Principle Component Analysis), FLA (Fisher Linear Analysis), and ICA (Independent Component Analysis). However, we will only concentrate on the nonlinear methods in this paper because these methods pay more attention to nonlinearity in the dataset, and nonlinearity is more universal than linearity in the real world. 
Recently, some manifold learning methods have been proposed to perform nonlinear dimensionality reduction, including LLE (Locally Linear Embedding) [1][8], ISOMAP [2], and Eigenmaps [3]. All the above nonlinear algorithms share the same framework, consisting of three steps: 1. Constructing a K nearest neighborhood graph over the dataset. 2. Constructing a  X  X ormalized X  matrix M. 3. Calculating spectral embedding based on the eigenvectors of M. 
The neighborhood size K which has to be specified manually in the first step plays mensionality reduction. A large value of K tends to introduce  X  X hort circuit X  edges [4] into the neighborhood graph, while a too small one may lead to an unconnected graph, tried to select the optimal value for K auto matically based on a predefined cost func-tion. Two main issues arise in this approach. First, it is an enumeration method in fact, and very time consuming. Second, the cost function is very difficult to define, and we do not think the cost functions used in [5] and [6] are reasonable and effective. We will give our explanation and proof in Section 3 and Section 5. 
In this paper, we focus on the selection of neighborhood size in manifold learning methods for nonlinear dimensionality reduction, and concentrate on LLE without loss of generality. We propose that K should be self-tuning according to local density not a uniform value for all the data, and present a new variant algorithm of LLE, which can effectively prune  X  X hort circuit X  edges by performing spatial search on the R*-Tree [7] LLE does. 2.1 LLE Algorithm Locally Linear Embedding [1][8] tries to find meaningful low-dimensional structure hidden in high-dimensional data. It maps a dataset 1 { ,..., } N XX X = steps: Step 1. For each data point i X Step 2. Compute the weights ij W that best reconstruct each data point i X nearest neighbors by solving a least squares problem as follows. Step 3. For each data point i X struction weights by solving the optimization problem as follows. 
This optimization problem can be solved by calculating the non-zero bottom d ei-genvectors of matrix ()() T M IW IW = X   X  , these eigenvectors form rows of matrix Y. We also list two properties the matrix M satisfies as follows, and these properties will help us to understand some results of LLE in Section 3. 1. M is symmetric and positive semi-definite. 2. M has N non-negative, real-valued eigenvalues 21 ... 0 N  X  X  X   X  X  X   X  = , and the cor-responding eigenvector to 0 is the constant one vector 1 2.2 R*-Tree most popular spatial index mechanisms which can help retrieve spatial data more ef-ficiently according to their sp atial locations. During these years, researchers and prac-titioners have applied R-Tree everywhere, from CAD and GIS to Multimedia Infor-mation Retrieval, and have made many variations including R+-Tree, R*-Tree, TV-Tree, X-Tree, Pyramid-Tree. Details about R-Tree and its variations can be found in [9]. 
R*-Tree [7] which is used in our topologically stable LLE algorithm, is also a variant  X  \ , and a m-dimensional bounding box represented by closed bounded interval [,] ii ab , using what spatial index mechanism can we retrieve the As we can see, LLE has one free parameter -K-the number of neighbors used in Step 1. K controls the range of neighbors based on which we reconstruct a data point, and we think the optimal K should satisfy two requirements at the same time: 1. The local linearity is preserved in K nearest neighbors. 2. K is as large as possible. 
The first requirement is easy to understand, as for requirement 2, we can see that the larger K is, the more information the neighbors can provide for reconstruction in Step 2. But it is very difficult to select the optimal K satisfying the two requirements manually. A small K can divide a continuous manifold into unconnected sub-manifolds. Fig. 1a-c illustrate the result of LLE performing on modified S-Curve dataset with a small K. 
Fig. 1a-c show that LLE performs very badly when K is not large enough to make the entire graph a connected one. We can inte rpret this situation when considering the properties of matrix M which are mentioned in Section 2.1. Considering M consists of C connected components (C=2 in Fig. 1), without loss of generality, we assume that the data points are ordered according to the connected components they belong to. In this case, M has a block diagonal form: same properties as M. Thus, we know that M has eigenvalue 0 with multiplicity C, and the corresponding eigenvectors are the indicat or vectors of the connected components multiplicity is lager than 1. 
In contrast, a large K tends to violate the requirement 1 through introducing  X  X hort circuit X  edges [4] into the neighborhood graph. Fig. 1 d-f show the failure caused by  X  X hort circuit X  edges. 
Some work [5][6] focused on choosing the optimal K automatically. They usually methods are computationally demanding as a result of enumerating every candidates of define. [2] proposed to use the residual variance to evaluate the mapping. The residual variance in [2] is defined as l 2 1
D which is a good estimate of real manifold distances M D , especially for data in real results in Section 5. 
In fact, we can calculate the exact M D on some artificial datasets, such as S-Curve and Swiss-Roll. We ignore the dimensionality of height, and only discuss the compu-tation of arc length on two-dimensional S-Curve and Swiss-Roll. The two dimensional radians) is simply: 12 * sr  X  X  = X  , where r is the radius. length is: As we can see in Section 3, it is not only difficult to specify the optimal K automati-cally, but also unreasonable to assume that each data point share the same number of cludes subsets with different statistics there may not be a single value of K that works well for all the data. 
Actually, every data point should have its own optimal K, not a uniform value. The means that for each data point, K should be as large as possible without introducing the  X  X hort circuit X  edges into the neighborhood graph. Investigating into the  X  X hort circuit X  edge, we can find that it usually passes by a low-density area in which very sparse data property. [10] proposed the pruned-ISOMAP algorithm, which first constructs an neighborhood graph with a relative large K, then prunes  X  X hort circuit X  edges existed kernel density estimation in [10] is computationally demanding, and needs to specify manually two elaborate parameters which depict the kernel function. In fact, if we just want to prune  X  X hort circuit X  edge, we need not to know the exact local density at the edge, the computation of which is very time consuming. Based on a spatial index built on the dataset, we can do the pruning more efficiently. In this paper we use R*-Tree to do the pruning. 4.1  X  X hort Circuit X  Edge Pruning Based on R*-Tree As we discussed in Section 2.2, given a data point and a range, R*-Tree helps us re-trieve its neighbors efficiently. So we can detect  X  X hort circuit X  edge by counting the number of its neighbors based on R*-Tree. As for an edge (, ) ij EX X connecting two 
Xxx = , we perform a R*-Tree search for the neighbors of ij X in a small ij E means there are sparse data points located in the neighborhood of the edge, and it is very likely to be a  X  X hort circuit X  edge, while a large ij E means the opposite. simply by setting a threshold on it. We set 0 as the threshold in our algorithm, it means graphs, and shows that it is suitable for us to set the threshold to be 0. 
As for ij  X  which confines the searching range, it should deal with the effect of local scaling [11], otherwise it tends to recognize the edges in large scale as  X  X hort circuit X  edges. It means that ij  X  should be self-tuning according to the local scales, dense dis-for edge (, ) ij EX X as follows. 
Where () ni NE X is the n nearest neighbors set of i X , and n=2 in our algorithm. 4.2 The More Topologically Stable LLE Algorithm In summary, we propose a more topologically stable LLE algorithm as follows. 1. Choose a relative large K with which the neighborhood graph is connected at least (1%-5% of total number of data points is recommended). Then construct a K nearest neighborhood graph on the dataset based on Euclidean metric. 2. Prune the  X  X hort circuit X  edges. 3. Run LLE on the pruned neighborhood graph. In this section, we present several examples to illustrate the performance of our algo-rithm that we name as R*-Tree LLE for brevity. We give both subjective and objective results: visualization of output data and residual variance metric which is discussed in Section 3. The test datasets include S-Curve and Swiss-Roll [8]. First, we compare R*-Tree LLE to LLE on uniformly sampled S-Curve and Swiss-Roll under different neighborhood sizes to illustrate the topologically stableness of our algorithm. Fig. 3 and Fig. 6a illustrate the comparison on 2000-point uniformly sampled S-Curve. Fig. 4 and Fig. 6b illustrate the comparison on 2000-point uniformly sampled Swiss-Roll. The residual variance in Fig. 6 is obtained by Eq. (5). 
Where M D is the matrix of manifold distances which is discussed in Section 3, 1 Y D beddings of LLE and R*-Tree LLE separately. X D is the matrix of Euclidean distances in the input high-dimensional space. From these figures, we can see R*-Tree LLE has more reasonable visual results and lower residual variances than LLE does, especially when the neighborhood size is large. But we also find that the performances of the two algorithms are similar when the neighborhood size is small. That is because the 2000-point S-Curve and Swiss-Roll are sampled uniformly from two smooth mani-troduce the  X  X hort circuit X  edges into the neighborhood graph when the neighborhood size is small. In Fig. 6a, we also show the residual variance RV_Y1X used in [5] and [6], it is obvious to see RV_Y1X is an improper evaluation metric for LLE as we know in Section 3. 
Then, we give the following example to illustrate a self-tuning neighborhood size neighborhood size has more superiorities over a uniform one. From Fig. 5 and Fig. 6c, we can see R*-Tree LLE performs much better than LLE at all the candidate neighborhood sizes, including at the optimal value for LLE. 
Finally, we give the execution time of the R*-Tree operations in our algorithm Implemented in C++ on a personal computer with Pentium-4 3.40 GHz CPU, 1 GB Memory, and MS Windows XP Professional SP2, it takes around 250 ms to create an R*-Tree over 2000 data points, and 0.4 ms to perform a search operation over 2000-point R*-Tree. In this paper, we explore the selection of neighborhood size in LLE, and propose that the neighborhood size should be self tuning according to the local density. Based on this idea, we propose a new variant of LLE which use self tuning K through pruning  X  X hort circuit X  edges based on R*-Tree. 
There are, however, some open problems. In our algorithm, we test every edge while pruning, the first step pruning is based on global information, and the second one de-TV-Tree and X-Tree, to substitute for R*-Tree, these indices outperforms R*-Tree in clustering which also needs to construct a neighborhood graph in its algorithm. 
