 Due to the popularity of the Internet, an ever-increasing number of documents in languages other than English are available in the Internet, thus creating the need of automatic organization of these multilingual documents. In addition, with the globalization of business environments, for many international companies and organizations, huge volume of documents in different languages need to be archived into common categories. On the other hand, in order to build a reliable model for automated text categorization, we typically need a large amount of manually labelled documents, which cost much human labor. Consequently, in multilingual scenario, how to employ the existing labelled documents written in a source language (e.g. English) to classify the unlabelled documents other than the language has become an important task, as it can be leveraged to alleviate cost of labelling. We refer to the mentioned-above task as cross language text categorization (CLTC).

Cross language information retrieval is highly related to CLTC. Also, the use of bilingual lexicon has been extensively studied in cross language information retrieval [1,2,3]. However, to our knowle dge, there is little research on the di-rection for CLTC. This paper will focus on investigating the use of a bilingual lexicon. Accordingly, we propose a novel refinement framework for CLTC.
The basic idea is that we assume that initial and inaccurate labels from the transferred model can be refined in the or iginal documents into better result-ing labels. Specifically, the framework consists of two stages. In the first stage, a cross language model transfer is proposed to generate preliminary labels of documents in target language. In the second stage, expectation maximization algorithm (EM) [4] based on naive Bayes model is introduced to generate result-ing labels of documents. Preliminary exp erimental results on collected corpora show that in the case of sufficient test data, with a small number of training documents, the proposed refinement framework can achieve better performance than monolingual text categorization and with a large number of training doc-uments, it can also obtain promising results close to that of monolingual text categorization.

The remainder of this paper is organized as follows. Section 2 introduces related work. Section 3 presents the refinement fr amework. Section 4 performs evaluation over our proposed framework. Section 5 is conclusions and future work. Cross language text categorization is divided into two cases, which are poly-lingual training and cross-lingual training [5]. The term poly-lingual training indicates the case that enough training documents available for every language. However, such scenarios are not particu larly interesting as they can be handled with separate monolingual solutions. The term cross-lingual training indicates that another case that enough training documents available for a language but no training documents for other languages. Currently, researchers focus their effort on the latter case. In this paper, we also focus on this case. Typically, some external lexical resources are used for CLTC. Li and Shawe-Taylor [6] applied kernel canonical correlation analysis (KCCA) and latent se-mantic analysis (LSA) to parallel corpora and induced the semantic space for CLTC. Olsson et al. [7] used the probabilistic bilingual lexicon induced by parallel corpora to ensure that test data is translated into the language of training data. However, a good semantic space or accurate translation probabilities depend on the amount of parallel corpora. Unfortunately, large-scale parallel corpora are not easily obtained. To alleviate the difficulty, Gliozzo and Strapparava [8] exploit comparable corpora to induce a semantic space by LSA. Nevertheless, this method is applicable only for language pairs, which have common words for the same concepts. Furthermore, Fortuna and Shawe-Taylor [9] applied machine translation system to generate pseudo domain-specific parallel corpus. Rigutini et al. [10] used a machine translation system to bridge the gap between dif-ferent languages. However, there are not machine translation systems for many language pairs and there is still wide gap of statistical chara cteristics between translated documents and original documents.

Compared with the above lexical resources, bilingual lexicon is a kind of cheap resource, which is readily available. Howe ver, there is little research on the use of a bilingual lexicon alone for CLTC. In this paper, we wish to concentrate on the direction. Figure 1 shows the overall architecture of our refinement framework. L 1 denotes the source language (i.e. the language in which documents are manually labelled); L 2 denotes the target language (i.e. the language in which documents are to be classified according to the categories from language L 1 ). The framework consists of two stages. The preliminary labels are generated in the first stage and a refinement with the preliminary labels i s performed in the second stage. In the following two sections, we shall explain the two stages in details. 3.1 The First Stage In this stage, preliminary labels about documents in L 2 are generated. Accord-ingly, a learning model about L 2 needs to be generated fo r label assignment of the documents in L 2 . However, a learning model about L 2 can not be derived directly. As a result, we propose an approach which transfers the trained model in
L as cross language model transfer (CLMT). In this paper, we choose to in-vestigate the transfer of the naive Bayes model from L 1 to L 2 , since naive Bayes model is efficient and effective for multi-class case. The details of naive Bayes model can be referred to [11]. For naive Bayes model in language L 2 , we need bility that word w f in L 2 occurs, given class c and P ( c ) denotes the probability that class c occurs in language L 2 .
In cross language information retrieval, a unigram language model in one language is combined with a probabilistic bilingual lexicon to yield a unigram language model in another language. Our approach is inspired by this well-known technique. We extend this by using a class-conditional bilingual lexicon. It can be formalized as follows: word w e is translated into word w f given class c .

P ( w are two solutions for P ( w f | w e ,c ). First, a naive and direct method is that we simply assume for each class c ,aword w e is translated into w f with the same probability, which is a uniform distribution on a word X  X  translations. If a word w e has otherwise P ( w f | w e ,c )=0.

Second, we propose to apply EM algorithm to deduce the conditional trans-lation probabilities given class c , via the bilingual lexicon L and the training document collection at hand. This idea is inspired by the work of word trans-lation disambiguation [12]. We can assume that given class c , each word w e in language L 1 is independently generated by a finite mixture model according to P ( w Therefore we can use EM algorithm to estimate the parameters of the model. Specifically, p ( w f | w e ,c ) is initialized through the first solution and then the following two steps are iterated until p ( w f | w e ,c ) remains unchanged.  X  E-step:  X  M-step: where N ( w e ,c ) denotes the times of co-occurrence of w e and c .
For P ( c ), there are two solutions, too. A simple solution is that we use es-timation from the labelled documents in langauge L 1 , since we assume that documents from different languages have the same class distribution. Another solution is that we can assume that the class distribution for documents in L 2 conforms to the uniform distribution, i.e. P ( c )= 1 |C| . The true class priors for documents in language L 2 may be different from tho se for documents in lan-guage L 1 . We do not simply estimate P ( c ) in language L 2 from the documents in language L 1 . That is, we have no idea of any information about the class distribution for documents in language L 2 . According to principle of maximum entropy, we can assume that the class distribution for documents in L 2 conforms to the uniform distribution. 3.2 The Second Stage In this stage, preliminary labels of documents in language L 2 from the first stage are used as input and an EM algorithm is introduced to obtain the final labels of document in language L 2 . The iterations of EM are a hill-climbing algorithm in parameter space that locally maximizes the entire log likelihood of documents in the collection. In this paper, we use naive Bayes model for the EM, similar to [11]. The algorithm we use is an unsupervised clustering whereas [11] is a semi-supervised learning. The basic idea is that EM is initialized to onto a right hill and then hill-climb the top. Specifically, P ( c | d ) is initialized based on the preliminary labels of documents in language L 2 and then the following two steps are iterated until P ( c | d ) stays unchanged, where d denotes a document.  X  E-step:  X  M-step: where the calculation of P ( d | c ) is referred to [11]. The resulting labels of docu-ments in language L 2 are assigned according to the following equation: Notice that in this stage only original documents in language L 2 are involved. 4.1 Setting We chose English and Chinese as our experimental languages, for we can easily setup our experiments and they are quite different languages. Standard evalu-ation benchmark is not available and thus we developed a test data from the Internet, containing Chinese Web pages and English Web pages. We applied RSS reader 1 to acquire the links to the needed content and then downloaded the Web pages. Although category information of the content can be obtained by RSS reader, we still used three Chinese-English bilingual speakers to organize these Web pages into the predefined categories. The data consists of news dur-ing December 2005. There are total 5462 English Web pages which are from 18 news Web sites and 6011 Chinese Web pages which are from 8 news Web sites. The details of the sources of Web pages are shown in Table 1 and Table 2. Data distribution over categories is shown in Table 3.

Some preprocessing steps are applied to those Web pages. First we extract the pure texts of all Web pages, excluding anchor texts which introduce much noise. Then for Chinese corpus, all Chinese cha racters with BIG5 encoding first were converted into ones with GB2312 encoding, applied a Chinese segmenter tool 2 by Zhibiao Wu from linguistic data consortium (LDC) to our Chinese corpus and removed words with one character and less than 4 occurrences; for English corpus, we used a stop list from SMART sy stem [13] to eliminate common words. Finally, We randomly split both the English and Chinese documents into 2 sets, 25% for training and 75% for test.
We compiled a general-purpose English-Chinese lexicon, which contains 276,889 translation pairs, including 53,111 English entries and 38,517 Chinese entries. Actually we used a subset of the lexicon including 20,754 English entries and 13,471 Chinese entries , which occur in our corpus. 4.2 Evaluation Measures The performance of the proposed metho ds was evaluated in terms of conven-tional precision, recall and F 1-measures. Furthermore , there are two conven-tional methods to evaluate overall performance averaged across categories, namely micro-averaging and macro-averaging [14]. Micro-averaging gives equal weight to each document while macro-averaging assigns equal weight to each category. In this paper, it is a multi-class case. Micro F1 and Macro F1 are short for micro-averaging F1 and macro-averaging F1. 4.3 Results In our experiments, all results are averaged over 10 arbitrary runs. For the proposed CLMT approach for initial labels of documents in language L 2 ,four variants are naturally yielded as differ ent parameter estimations may be used. As a result, we first investigate the impact on resulting performance, through varying different parameter estimation sofCLMT.Foreaseofdescription,we call them as R-D1, R-D2, R-EM1 and R-EM2, where R indicates refinement framework, D indicates the first solution to estimate P ( w f | w e ,c ), EM indicates estimate P ( c ) in language L 2 , and digit 2 denotes the second solution to esti-mate P ( c ) in language L 2 . Their results on collected corpora are shown in Fig. 2. We can notice that R-EM1 and R-EM2 consistently work better than R-D1 and R-D2 over experiments trained on English documents and tested on Chinese documents or trained on Chinese docum entsandtestedonEnglishdocuments. In addition, R-EM1 performs slightly better than R-EM2.

For further evaluation of our framework, we compare our approach with the following three baselines. In our experiments, we use Naive Bayes as our classifier forfaircomparison.
 Mono ( Monolingual text categorization ) . Training and testing are per-formed on documents in the same language. CLMT-EM1. It is used to generate preliminary labels for R-EM1. It sets a starting point of refinement for R-EM1, which is used as representative of our methods, since it perform better than other methods.
 MT ( machine translation ) . We used Systran premium 5.0 to translate train-ing data into the language of test data, since the machine translation system is one of the best commercial machine translation systems. Then use the translated data to learn a statistical model for classifying the test data.

The results are shown in Fig. 3. We notice that with fewer training docu-ments, R-EM1 works best among all methods and with more training docu-ments, R-EM1 achieves a performance close to monolingual text categorization. In addition, we observe that MT obtains poor performance. This may be be-cause statistical property of the translated documents is quite different from that of the original documents, although human can understand the translated documents produced by Systran premium 5.0.
 To examine how the size of test data affect s resulting performance, we compare CLMT-EM1 with R-EM1, varying the size of test data. The results are shown in Fig. 4. Experiments show that higher performance benefits from more test data. Meanwhile, we can also notice that when applied on a small portion of English test data set, EM based on naive Bayes model obtains results contrary to what we expect. The EM does not improve the p erformance of initial labels. On the contrary, it makes resulting performance worse than initial performance. It may be because there are too many parameters to be estimated but few data do not provide potential of accurate parameter estimation. This paper proposes a novel refinement framework for cross language text cate-gorization. Our preliminary experiments on the collected data show that our re-finement framework is effective for CLTC. This work has the following three main contributions. First, we are apparently the first to investigate the use of a bilingual lexicon alone for cross language text categorization. Second, a refinement frame-work is proposed for the use of a bilingual lexicon on cross language text cate-gorization. Third, a cross language model transfer approach is proposed for the transfer of naive Bayes models from different languages via a bilingual lexicon and an EM algorithm based on naive Bayes model is introduced for the refinement of initial labels yielded by the proposed cross language model transfer method. In the future, we shall improve our work from the following three directions. First, our data set is limited and the predefined categories are coarse. we plan to collect larger data co llection with finer categories and test our proposed refine-ment framework on it. Second, different monolingual text categorization algo-rithms will be explored with the framework and accordingly new cross language model transfer approaches need to be proposed. Third, the EM algorithm is eas-ily trapped into local optima. Therefore, we plan to propose a new refinement approach to avoid this case. Finally, peo ple have recently tried to automatically collect bilingual corpora from web [15,16], and therefore we may benefit by using the translation probabilities trained from the bilingual corpora.

