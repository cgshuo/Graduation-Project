 What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact , which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.
When computers are involved, determining disparate im-pact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses.
We present four contributions. First, we link disparate im-pact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effec-tiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny. H.2.8 [ Database Management ]: Database Applications X  Data Mining ; J.4 [ Computer Applications ]: Social and Behavioral Sciences; K.5.m [ Computing Milieux ]: Legal Aspects of Com-puting X  Miscellaneous Disparate impact, fairness, machine learning  X 
Author ordering on this paper is alphabetical.  X  Corresponding author.

In Griggs v. Duke Power Co. [19], the US Supreme Court ruled a business hiring decision illegal if it resulted in dis-parate impact by race even if the decision was not explicitly determined based on race. The Duke Power Co. was forced to stop using intelligence test scores and high school diplomas, qualifications largely correlated with race, to make hiring de-cisions. The Griggs decision gave birth to the legal doctrine of disparate impact , which today is the predominant legal the-ory used to determine unintended discrimination in the U.S. Note that disparate impact is different from disparate treat-ment , which refers to intended or direct discrimination. Ricci v. DeStefano [21] examined the relationship between the two notions, and disparate impact remains a topic of legal interest.
Today, algorithms are being used to make decisions both large and small in almost all aspects of our lives, whether they involve mundane tasks like recommendations for buy-ing goods, predictions of credit rating prior to approving a housing loan, or even life-altering decisions like sentencing guidelines after conviction [6]. How do we know if these algo-rithms are biased, involve illegal discrimination, or are unfair? These concerns have generated calls, by governments and NGOs alike, for research into these issues [17, 23]. In this paper, we introduce and address two such problems with the goals of quantifying and then removing disparate impact.
While the Supreme Court has resisted a  X  X igid mathemat-ical formula X  defining disparate impact [20], we will adopt a generalization of the 80 percent rule advocated by the US Equal Employment Opportunity Commission (EEOC) [24]. We note that disparate impact itself is not illegal; in hiring de-cisions, business necessity arguments can be made to excuse disparate impact.
 data set D = ( X , Y , C ) , with protected attribute X (e.g., race, sex, religion, etc.), remaining attributes Y , and binary class to be predicted C (e.g.,  X  X ill hire X ), we will say that D has disparate impact if for positive outcome class YES and majority protected attribute 1 where Pr ( C = c | X = x ) denotes the conditional probability (evaluated over D ) that the class outcome is c  X  C given protected attribute x  X  X. 1
Note that under this definition disparate impact is deter-mined based on the given data set and decision outcomes.
The two problems we consider address identifying and removing disparate impact. The disparate impact certification problem is to guarantee that, given D , any classification algo-rithm aiming to predict some C 0 (which is potentially different from the given C ) from Y would not have disparate impact. By certifying any outcomes C 0 , and not the process by which they were reached, we follow legal precedent in making no judgment on the algorithm itself, and additionally ensure that potentially sensitive algorithms remain proprietary. The dis-parate impact removal problem is to take some data set D and ing disparate impact. The goal is to change only the remaining attributes Y , leaving C as in the original data set so that the ability to classify can be preserved as much as possible. We have four main contributions.

We first introduce these problems to the computer science community and develop its theoretical underpinnings. The study of the EEOC X  X  80% rule as a specific class of loss func-tion does not appear to have received much attention in the literature. We link this measure of disparate impact to the balanced error rate ( BER ). We show that any decision exhibit-ing disparate impact can be converted into one where the protected attribute leaks, i.e. can be predicted with low
Second, this theoretical result gives us a procedure for certi-fying the impossibility of disparate impact on a data set. This procedure involves a particular regression algorithm which minimizes BER . We connect BER to disparate impact in a vari-ety of settings (point and interval estimates, and distributions). We discuss these two contributions in Sections 3 and 4.
In Section 5, we show how to transform the input dataset so that predictability of the protected attribute is impossible. We show that this transformation still preserves much of the signal in the unprotected attributes and has nice properties in terms of closeness to the original data distribution.
Finally, we present a detailed empirical study in Section 6. We show that our algorithm certifying lack of disparate impact on a data set is effective, such that with the three classifiers we used certified data sets don X  X  show disparate impact. We demonstrate the fairness / utility tradeoff for our partial repair procedures. Comparing to related work, we find that for any desired fairness value we can achieve a higher accuracy than other fairness procedures. This is likely due to our emphasis on changing the data to achieve fairness, thus allowing any strong classifier to be used for prediction.
Our procedure for detecting disparate impact goes through an actual classification algorithm. As we show in our experi-ments, a better classifier provides a more sensitive detector. We believe this is notable. As algorithms get better at learning patterns, they become more able to introduce subtle biases into the decision-making process by finding subtle dependen-cies among features. But this very sophistication helps detect such biases as well via our procedure! Thus, data mining can be used to verify the fairness of such algorithms as well.
There is, of course, a long history of legal work on disparate impact. There is also related work under the name statistical Notably, it does not use a broader sample universe, and does not take into account statistical significance as has been advo-cated by some legal scholars [16]. discrimination in Economics. We will not survey such work here. Instead, we direct the reader to the survey of Romei and Ruggieri [18] and to a discussion of the issues specific to data mining and disparate impact [1]. Here, we focus on data mining research relating to combating discrimination. This research can be broadly categorized in terms of methods that achieve fairness by modifying the classifiers and those that achieve fairness by modifying data.

Kamishima et al. [9, 10] develop a regularizer for classifiers to penalize prejudicial outcomes and show that this can re-duce indirect prejudice (their name for implicit discrimination like disparate impact) while still allowing for accurate classifi-cation. They note that as prejudicial outcomes are decreased, the classification accuracy is also decreased. Our work falls into the category of algorithms that change the input data. Previous work has focused on changing the class values of the original data in such a way so that the total number of class changes is small [2, 8], while we will keep the class val-ues the same for training purposes and change the data itself. Calders et al. [2] have also previously examined one method for changing the data in which different data items are given weights and the weights are adjusted to achieve fairness. In this category of work, as well, there is worry that the change to the data will decrease the classification accuracy, and Calders et al. have formalized this as a fairness/utility tradeoff [2]. We additionally note that lower classification accuracy may actually be the desired result, if that classification accuracy was due to discriminatory decision making in the past.
An important related work is the approach of  X  X airness through awareness X  of Dwork et al. [3] and Zemel et al. [25]. Dwork et al. [3] focus on the problem of individual fairness; their approach posits the existence of a similarity measure between individual entities and seeks to find classifiers that ensure similar outcomes on individuals that are similar, via a Lipschitz condition. In the work of Zemel et al. [25], this idea of protecting individual fairness is combined with a statistical group-based fairness criterion that is similar to the approach we take in this work. A key contribution of their work is that they learn a modified representation of the data in which fair-ness is ensured while attempting to preserve fidelity with the original classification task. While this group fairness measure is similar to ours in spirit, it does not match the legal defini-tion we base our work on. Another paper that also (implicitly) defines fairness on an individual basis is the work by Thanh et al. [11]. Their proposed repair mechanism changes class attributes of the data (rather than the data itself).
Pedreschi, Ruggieri and Turini [14, 15] have examined the  X 80% rule X  that we study in this paper as part of a larger class of measures based on a classifier X  X  confusion matrix.
We start by reinterpreting the  X 80% rule X  in terms of more standard statistical measures of quality of a classifier. This presents notational challenges. The terminology of  X  X ight X  and  X  X rong X ,  X  X ositive X  and  X  X egative X  that is used in clas-sification is an awkward fit when dealing with majority and minority classes, and selection decisions. For notational conve-nience only , we will use the convention that the protected class X takes on two values: X = 0 for the  X  X inority X  class and X = 1 for the  X  X efault X  class. For example, in most gender-discrimination scenarios the value 0 would be assigned to  X  X emale X  and 1 to  X  X ale X . We will denote a successful binary classification outcome C (say, a hiring decision) by C = YES and a failure by C = NO . Finally, we will map the majority class to  X  X ositive X  examples and the minority class to  X  X ega-tive X  examples with respect to the classification outcome, all the while reminding the reader that this is merely a conve-nience to do the mapping, and does not reflect any judgments about the classes. The advantage of this mapping is that it renders our results more intuitive: a classifier with high  X  X r-ror X  will also be one that is least biased, because it is unable to distinguish the two classes.

Table 1 describes the confusion matrix for a classification with respect to the above attributes where each entry is the prob-ability of that particular pair of outcomes for data sampled from the input distribution (we use the empirical distribution when referring to a specific data set).

The 80% rule can then be quantified as: Note that the traditional notion of  X  X ccuracy X  includes terms in the numerator from both columns, and so cannot be directly compared to the 80% rule. Still, other class-sensitive error metrics are known, and more directly relate to the 80% rule: sensitivity of a test (informally, its true positive rate) is defined as the conditional probability of returning YES on  X  X ositive X  examples (a.k.a. the majority class). In other words, The specificity of a test (its true negative rate) is defined as the conditional probability of returning NO on  X  X egative X  examples (a.k.a. the minority) class. I.e., hood ratio positive, denoted by LR + , is given by We can now restate the 80% rule in terms of a data set.
D EFINITION 3.3 (D ISPARATE I MPACT ). A data set has dis-parate impact if
It will be convenient to work with the reciprocal of LR + which we denote by This will allow us to discuss the value associated with dis-parate impact before the threshold is applied.
 Multiple classes. Disparate impact is defined only for two classes. In general, one might imagine a multivalued class attribute (for example, like ethnicity). In this paper, we will assume that a multivalued class attribute has one value desig-nated as the  X  X efault X  or majority class, and will compare each of the other values pairwise to this default class. While this ignores zero-sum effects between the different class values, it reflects the current binary nature of legal thought on dis-crimination. A more general treatment of joint discrimination among multiple classes is beyond the scope of this work.
Our notion of computational fairness starts with two play-ers, Alice and Bob. Alice runs an algorithm A that makes decisions based on some input. For example, Alice may be an employer using A to decide who to hire. Specifically, A takes a data set D with protected attribute X and unprotected attributes Y and makes a (binary) decision C . By law, Alice is not allowed to use X in making decisions, and claims to use only Y . It is Bob X  X  job to verify that on the data D , Alice X  X  algorithm A is not liable for a claim of disparate impact. Trust model. We assume that Bob does not have access to A . Further, we assume that Alice has good intentions: specifically, that Alice is not secretly using X in A while lying about it. While assuming Alice is lying about the use of X might be more plausible, it is much harder to detect. More importantly, from a functional perspective, it does not matter whether Alice uses X explicitly or uses proxy attributes Y that have the same effect: this is the core message from the Griggs case that introduced the doctrine of disparate impact. In other words, our certification process is indifferent to Alice X  X  intentions, but our repair process will assume good faith.
 We summarize our main idea with the following intuition:
We now present a formal definition of predictability and link it to the legal notion of disparate impact. Recall that D = ( X , Y , C ) where X is the protected attribute, Y is the remaining attributes, and C is the class outcome to be predicted. The basis for our formulation is a procedure that predicts X from Y . We would like a way to measure the quality of this predictor in a way that a) can be optimized using standard predictors in machine learning and b) can be related to LR The standard notions of accuracy of a classifier fail to do the second (as discussed earlier) and using LR + directly fails to satisfy the first constraint.

The error measure we seek turns out to be the balanced error rate BER .
 D EFINITION 4.1 (BER). Let f : Y  X  X be a predictor of X from Y . The balanced error rate BER of f on distribution D over the pair ( X , Y ) is defined as the (unweighted) average class-conditioned error of f . In other words, BER ( f ( Y ) , X ) =
D EFINITION 4.2 ( P REDICTABILITY ) . X is said to be e -predictable from Y if there exists a function f : Y  X  X such that
This motivates our definition of e -fairness, as a data set that is not predictable.
D EFINITION 4.3 ( e -FAIRNESS ). A data set D = ( X , Y , C ) is said to be e -fair if for any classification algorithm f : Y  X  X with (empirical) probabilities estimated from D.

Recall the definition of disparate impact from Section 3. We will be interested in examining the potential disparate impact of a classifier g : Y  X  C and will consider the value DI ( g ) = 1 /LR + ( g ( Y ) , X ) as it relates to the threshold  X  . Where g is clear from context, we will refer to this as DI .

The justification of our definition of fairness comes from the following theorem:
T HEOREM 4.1. A data set is ( 1 / 2  X   X  / 8 ) -predictable if and only if it admits disparate impact, where  X  is the fraction of elements in the minority class ( X = 0 ) that are selected ( C = 1 ) .
P ROOF . We will start with the direction showing that dis-parate impact implies predictability. Suppose that there exists some function g : Y  X  C such that LR + ( g ( y ) , c )  X  1 create a function  X  : C  X  X such that BER (  X  ( g ( y )) , x ) &lt; ( x , y )  X  D . Thus the combined predictor f =  X   X  g satisfies the definition of predictability.

Consider the confusion matrix associated with g , depicted in Table 2. Set  X  , b b + d and  X  , c a + c . Then we can write LR + ( g ( y ) , X ) = 1  X   X   X  and DI ( g ) =  X  1  X   X  .
We define the purely biased mapping  X  : C  X  X as  X  ( YES ) = 1 and  X  ( NO ) = 0. Finally, let  X  : Y  X  X =  X   X  g . The confusion matrix for  X  is depicted in Table 3. Note that the confusion matrix for  X  is identical to the matrix for g .
We can now express BER (  X  ) in terms of this matrix. Specifi-cally, BER (  X  ) =  X  +  X  2 .
 Representations. We can now express contours of the DI and BER functions as curves in the unit square [ 0, 1 ] 2 . Reparametriz-ing  X  1 = 1  X   X  and  X  0 =  X  , we can express the error measures as DI ( g ) =  X  0
As a consequence, any classifier g with DI ( g ) =  X  can be represented in the [ 0, 1 ] 2 unit square as the line  X  1 Any classifier  X  with BER (  X  ) = e can be written as the func-tion  X  1 =  X  0 + 1  X  2 e .

Let us now fix the desired DI threshold  X  , corresponding to the line  X  1 =  X  0 /  X  . Notice that the region { (  X  0  X  1  X   X  0 /  X  } is the region where one would make a finding of disparate impact (for  X  = 0.8).

Now given a classification that admits a finding of disparate impact, we can compute  X  . Consider the point (  X  ,  X  /  X  which the line  X  0 =  X  intersects the DI curve  X  1 =  X  0 / point lies on the BER contour ( 1 +  X   X   X  /  X  ) / 2 = e , yielding e = 1 / 2  X   X  ( 1  X  = 0.8, the desired BER threshold is and so disparate impact implies predictability.

With this infrastructure in place, the other direction of the proof is now easy. To show that predictability implies dis-parate impact, we will use the same idea of a purely biased classifier. Suppose there is a function f : Y  X  X such that BER ( f ( y ) , x )  X  e . Let  X   X  1 : X  X  C be the inverse purely biased mapping, i.e.  X   X  1 ( 1 ) = YES and  X   X  1 ( 0 ) = NO . Let g : Y  X  C =  X   X  1  X  f . Using the same representation as before, this gives us  X  1  X  1 +  X  0  X  2 e and therefore Recalling that DI ( g ) =  X  0 threshold of e = 1 2  X   X  8 .

Note that as e approaches 1 / 2 the bound tends towards the trivial (since any binary classifier has BER at most 1 / 2). In other words, as  X  tends to 0, the bound becomes vacuous.
This points to an interesting line of attack to evade a dis-parate impact finding. Note that  X  is the (class conditioned) rate at which members of the protected class are selected. Con-sider now a scenario where a company is being investigated for discriminatory hiring practices. One way in which the company might defeat such a finding is by interviewing (but not hiring) a large proportion of applicants from the protected class. This effectively drives  X  down, and the observation above says that in this setting their discriminatory practices will be harder to detect, because our result can not guarantee that a classifier will have error significantly less than 0.5.
Observe that in this analysis we use an extremely weak classifier to prove the existence of a relation between pre-dictability and disparate impact. It is likely that using a better classifier (for example the Bayes optimal classifier or even a classifier that optimizes BER ) might yield a stronger relation-ship between the two notions.
 Dealing with uncertainty. In general,  X  might be hard to estimate from a fixed data set, and in practice we might only know that the true value of  X  lies in a range [  X  ` ,  X  u the BER threshold varies monotonically with  X  , we can merely use  X  ` to obtain a conservative estimate.
 Another source of uncertainty is in the BER estimate itself. Suppose that our classifier yields an error that lies in a range [  X  ,  X  0 ] . Again, because of monotonicity, we will obtain an interval of values [  X  ,  X  0 ] for DI . Note that if (using a Bayesian approach) we are able to build a distribution over BER , this distribution will then transfer over to the DI estimate as well.
The above argument gives us a way to determine whether a data set is potentially amenable to disparate impact (in other words, whether there is insufficient information to detect a protected attribute from the provided data).
 Algorithm. We run a classifier that optimizes BER on the given data set, attempting to predict the protected attributes X from the remaining attributes Y . Suppose the error in this predic-tion is e . Then using the estimate of  X  from the data, we can substitute this into the equation above and obtain a thresh-old e 0 . If e 0 &lt; e , then we can declare the data set free from disparate impact.
 Assume that we have an optimal classifier with respect to BER . Then we know that all classifiers will incur a BER of at least e . By Theorem 4.1, this implies that no classifier on D will exhibit disparate impact, and so our certification is correct.
The only remaining question is what classifier is used by this algorithm. The usual way to incorporate class sensitivity into a classifier is to use different costs for misclassifying points in different classes. A number of class-sensitive cost measures fall into this framework, and there are algorithms for optimizing these measures (see [12] for a review), as well as a general (but expensive) method due to Joachims that does a clever grid search over a standard SVM to optimize a large family of class-sensitive measures [7]. Oddly, BER is not usually included among the measures studied.

Formally, as pointed out by Zhao et al [26], BER is not a cost-sensitive classification error measure because the weights assigned to class-specific misclassification depend on the rel-ative class sizes (so they can be normalized). However, for any given data set we know the class sizes and can reweight accordingly. We adapt a standard hinge-loss SVM to incorpo-rate class-sensitivity and optimize for (regularized) BER adaptation is standard, and yields a cost function that can be optimized using AdaBoost. For more details, see the extended version of this paper [5].
Once Bob X  X  certification procedure has made a determina-tion of (potential) disparate impact on D , Alice might request a repaired version  X  D of D , where any attributes in D that could be used to predict X have been changed so that  X  D would be certified as e -fair. We now describe how to construct such a set  X  D = ( X ,  X  Y , C ) such that  X  D does not have disparate impact in terms of protected attribute X . While for notational sim-plicity we will assume that X is used directly in what follows, in practice the attribute used to stratify the data for repair need not directly be the protected attribute or even a single protected attribute. In the case of the Texas Top 10% Rule that admits the top ten percent of every high school class in Texas to the University of Texas [22], the attribute used to stratify is the high school attended, which is an attribute that correlates with race. If repair of multiple protected attributes is desired, the joint distribution can be used to stratify the data. (We will look into the effects of this experimentally in Section 6.2.)
Of course, it is important to change the data in such a way that predicting the class is still possible. Specifically, our goal will be to preserve the relative per-attribute ordering as fol-lows. Given protected attribute X and a single numerical attribute Y , let Y x = Pr ( Y | X = x ) denote the marginal distri-bution on Y conditioned on X = x . Let F x : Y x  X  [ 0, 1 ] be the cumulative distribution function for values y  X  Y x and let F  X  1 x : [ 0, 1 ]  X  Y x be the associated quantile function (i.e F x ( 1 / 2 ) is the value of y such that Pr ( Y  X  y | X = x ) = 1 / 2). We will say that F x ranks the values of Y x .

Let  X  Y be the repaired version of Y in  X  D . We will say that  X  D strongly preserves rank if for any y  X  Y x and x  X  X , its preserving rank in this way, despite changing the true values Figure 1: Consider the fake probability density functions shown here where the blue curve shows the distribution of SAT scores ( Y ) for X = female , with  X  = 550,  X  = 100 , while the red curve shows the distribution of SAT scores for X = male , with  X  = 400,  X  = 50 . The resulting fully repaired data is the distribution in black, with  X  = 475, 75 . Male students who originally had scores in the 95th percentile, i.e., had scores of 500, are given scores of 625 in the 95th percentile of the new distribution in  X  Y , while women with scores of 625 in  X  Y originally had scores of 750. of Y , appears to allow Alice X  X  algorithm to continue choosing stronger (higher ranked) applicants over weaker ones. We present experimental evidence for this in Section 6.
With this motivation, we now give a repair algorithm that strongly preserves rank and ensures that  X  D = ( X ,  X  Y , C ) is fair (i.e., is e -fair for e = 1 / 2). In the discussion that follows, for the sake of clarity we will treat Y as a single attribute over a totally-ordered domain. To handle multiple totally-ordered attributes Y 1 , . . . , Y k we will repair each attribute individually.
We define a  X  X edian X  distribution A in terms of its quantile the term  X  X edian X  is not accidental.

L EMMA 5.1. Let A be a distribution such that F  X  1 A median x  X  X F  X  1 x ( u ) . Then A is also the distribution minimizing  X  mover distance on R .

P ROOF . For any two distributions P and Q on the line, the earthmover distance (using the underlying Euclidean distance d ( x , y ) = | x  X  y | as the metric) can be written as In other words, the map P 7 X  F  X  1 P is an isometric embedding of the earthmover distance into ` 1 .

Consider now a set of points p 1 , . . . , p n  X  ` 1 median  X  the point minimizing  X  i k p i  X  c k 1  X  is the point whose j th coordinate is the median of the j th coordinates of the p i . This is precisely the definition of the distribution A (in terms of F  X  1 A ).
 Algorithm. Our repair algorithm creates  X  Y , such that for all y  X  Y x , the corresponding  X  y = F  X  1 A ( F x ( y )) . The resulting ( X ,  X  Y , C ) changes only Y while the protected attribute and class remain the same as in the original data, thus preserving the ability to predict the class. See Figure 1 for an example. Notes. We note that this definition is reminiscent of the method by which partial rankings are combined to form a total ranking. The rankings are  X  X emeny X -ized by finding a ranking that minimizes the sum of distances to the origi-nal rankings. However, there is a crucial difference in our procedure. Rather than merely reorganizing the data into a total ranking, we are modifying the data to construct this consensus distribution.
 T HEOREM 5.1.  X  D is fair and strongly preserves rank.
P ROOF . In order to show that  X  D strongly preserves rank, re-call that we would like to show that F x ( y ) = F x (  X  y ) for all x  X  X ,  X  y  X   X  Y x , and y  X  Y x . Since, by definition of our algorithm,  X  y = F  X  1 A ( F x ( y )) , we know that F x (  X  y ) = F we would like to show that F x ( F  X  1 A ( z )) = z for all z  X  [ 0, 1 ] and for all x . Recall that F  X  1 A ( z ) = median x  X  X F
Suppose the above claim is not true. Then there are two values z 1 &lt; z 2 and some value x such that F x ( F  X  1 F ( F  X  1 A ( z 2 )) . That is, there is some x and two elements y F
A ( z 1 ) , y 2 = F that y 1 = median x  X  X F  X  1 x ( z 1 ) . Therefore, if y be that there are strictly less than | X | / 2 elements of the set { F x ( z 1 ) | x  X  X } below y 2 . But by the assumption that z z , we know that each element of { F  X  1 x ( z 1 ) | x  X  X } is above the corresponding element of { F  X  1 x ( z 2 ) | x  X  X } and there are | X | / 2 elements of this latter set below y 2 by definition. Hence we have a contradiction and so a flip cannot occur, which means that the claim is true.

Note that the resulting  X  Y x distributions are the same for all x  X  X , so there is no way for Bob to differentiate between the protected attributes. Hence the algorithm is 1-fair.
This repair has the effect that if you consider the  X  Y values at some rank z , the probability of the occurrence of a data item with attribute x  X  X is the same as the probability of the occurrence of x in the full population. This informal observa-tion gives the intuitive backing for the lack of predictability of X from  X  Y and, hence, the lack of disparate impact in the repaired version of the data. Since the repair process outlined above is likely to degrade Alice X  X  ability to classify accurately, she might want a partially repaired data set instead. This in effect creates a tradeoff be-tween the ability to classify accurately and the fairness of the resulting data. This tradeoff can be achieved by simply mov-ing each inverse quantile distribution only part way towards the median distribution. Let  X   X  [ 0, 1 ] be the amount of re-pair desired, where  X  = 0 yields the unmodified data set and  X  = 1 is the fully repaired version described above. Recall that F x : Y x  X  [ 0, 1 ] is the function giving the rank of y . The repair algorithm for  X  = 1 creates  X  Y such that  X  y = F where A is the median distribution.

In the partial repair setting we will be creating a differ-ent distribution A x for each protected value x  X  X and set-ting  X  y = F  X  1 A rank u in their respective conditional distributions i.e the set U ( u ) = { F  X  1 x ( u ) | x  X  X } . We can associate with U the cu-define the associated quantile function U F  X  1 ( u ,  X  ) = y where U F ( u , y ) =  X  . We can restate the full repair algorithm in this formulation as follows: for any ( x , y ) ,  X  y = U F  X  1
We now describe two different approaches to performing a partial repair, each with their own advantages and disad-vantages. Intuitively, these repair methods differ in which space they operate in: the combinatorial space of ranks or the geometric space of values.
The intuition behind this repair strategy is that each item, rather than being moved to the median of its associated distri-bution, is only moved part of the way there, with the amount moved being proportional (in rank) to its distance from the median.
 on X = x . Suppose that in the set U ( r ) (the collection of all y with rank r in their respective conditional distributions) the rank of y is  X  . Then we replace y by  X  y  X  U ( r ) whose rank in U ( r ) is  X  = b ( 1  X   X  )  X  +  X  / 2 c . Formally,  X  y = U F  X  1 ( r , resulting data set  X  D  X  .
 While this repair is intuitive and easy to implement, it does not satisfy the property of strong rank preservation. In other words, it is possible that two pairs ( x , y ) and ( x , y potentially affect the quality of the resulting data (we discuss this in Section 6.2), it does not affect the fairness properties of the repair. Indeed, we formulate the fairness properties of this repair as a formal conjecture.

C ONJECTURE 5.1.  X  D  X  is f (  X  ) -fair for a monotone function f .
The algorithm above has an easy-to-describe operational form. It does not however admit a functional interpretation as an optimization of a certain distance function, like the full repair. For example, it is not true that for  X  = 1 / 2 the mod-ified distributions  X  Y are equidistant (under the earthmover distance) between the original unrepaired distributions and the full repair. The algorithm we propose now does have this property, as well as possessing a simple operational form. The intuition is that rather than doing a linear interpolation in rank space between the original item and the fully repaired value, it does a linear interpolation in the original data space .
D EFINITION 5.2 (G EOMETRIC R EPAIR ). Let F A be the cu-mulative distribution associated with A , the result performing a full repair on the conditional cumulative distributions as described in Section 5. Given a conditional distribution F x ( y ) , its repair is given by
Linear interpolation allows us to connect this repair to the underlying earthmover distance between repaired and unre-paired distributions. In particular, T HEOREM 5.2. For any x , d ( Y x ,  X  Y x ) =  X  d ( Y x , Y is the distribution on Y in the full repair, and  X  Y x is the repair. Moreover, the repair strongly preserves rank.
P ROOF . The earthmover distance bound follows from the proof of Lemma 5.1 and the isometric mapping between the earthmover distance between Y x and  X  Y x and the ` 1 distance be-tween F x and  X  F x . Rank preservation follows by observing that the repair is a linear interpolation between the original data and the full repair (which preserves rank by Lemma 5.1).
The reason partial repair may be desired is that increasing fairness may result in a loss of utility. Here, we make this data set for some value of  X   X  [ 0, 1 ] as described above (where  X  D = 0 = D ). Let  X  g  X  :  X  Y  X  C be the classifier with the utility we are trying to measure.

D EFINITION 5.3 (U TILITY ). The utility of a classifier  X  g C with respect to some partially repaired data set  X  D  X  If the classifier  X  g  X  = 0 : Y  X  C has an error of zero on the unre-paired data, then the utility is 1. More commonly,  X  (  X  g 1. In our experiments, we will investigate how  X  decreases as  X  increases.
We will now consider the certification algorithm and repair algorithm X  X  fairness/utility tradeoff experimentally on three data sets. The first is the Ricci data set at the heart of the Ricci v. DeStefano case [21]. It consists of 118 test taker entries, each including information about the firefighter promotion exam taken (Lieutenant or Captain), the score on the oral section of the exam, the written score, the combined score, and the race of the test taker (black, white, or Hispanic). In our examination of the protected race attribute, we will group the black and Hispanic test takers into a single non-white category. The classifier originally used to determine which test takers to promote was the simple threshold classifier that allowed anyone with a combined score of at least 70% to be eligible for promotion [13]. Although the true number of people promoted was chosen from the eligible pool according to their ranked ordering and the number of slots available, for simplicity in these experiments we will describe all eligible candidates as having been promoted. We use a random two-thirds / one-third split for the training / test data.
The other two data sets we will use are from the UCI Ma-chine Learning Repository 2 . So that we can compare our results to those of Zemel et al. [25], we will use the same data sets and the same decisions about what constitutes a sensitive attribute as they do. First, we will look at the German credit data set, also considered by Kamiran and Calders [8]. It con-tains 1000 instances, each of which consists of 20 attributes and a categorization of that instance as GOOD or BAD. The protected attribute is Age. In the examination of this data set with respect to their discriminatory measure, Kamiran and Calders found that the most discrimination was possible when splitting the instances into YOUNG and OLD at age 25 [8]. We will discretize the data accordingly to examine this potential worst case. We use a random two-thirds / one-third split for the training / test data.

We also look at the Adult income data set, also considered by Kamishima et al. [10]. It contains 48,842 instances, each of which consists of 14 attributes and a categorization of that person as making more or less than $50,000 per year. The http://archive.ics.ucu.edu/ml protected attribute we will examine is Gender. Race is also an attribute in the data, and it will be excluded for classification purposes, except for when examining the effects of having multiple protected attributes -in this case, race will be catego-rized as white and non-white. The training / test split given in the original data is also used for our experiments.
For each of these data sets, we look at a total of 21 ver-sions of the data -the original data set plus 10 partially or fully repaired attribute sets for each of the combinatorial and geometric partial repairs. These are the repaired attributes for  X   X  [ 0, 1 ] at increments of 0.1. Data preprocessing was applied before the partial repair algorithm was run. Preprocessing. Datasets were preprocessed as follows: 1. Remove all protected attributes from Y . This ensures 2. Remove all unordered categorical features since our 3. Scale each feature so that the minimum is zero and the Classifiers. Three different classifiers were used as oracles for measuring discrimination (under the disparate impact measure and a measure by Zemel et al. [25]), and to test the accuracy of a classification after repair. The classifiers used for our experimental tasks were provided by the Scikit-learn python package.

LR: Logistic Regression: Liblinear X  X  [4] logistic regression
SVM: Support Vector Machine: Liblinear X  X  [4] linear SVM al-
GNB: Gaussian Na X ve Bayes: Scikit-Learn X  X  na X ve Bayes algo-Parameter selection and cross-validation. LR and SVM clas-sifiers were cross-validated using three-fold cross validation and the best parameter based on BER was chosen. We cross-validated the parameter controlling the tradeoff between reg-ularization and loss, and 13 parameters between 10  X  3 and 10 3 , with logarithms uniformly spaced, were searched. Repair details. The repair procedure requires a ranking of each attribute. The numeric values and ordered categorical attributes were ordered in the natural way and then quantiles were used as the ranks. Since the repair assumes that there is a point at each quantile value in each protected class, the
On the Adult Income data, it happens that all missing values were of these unordered categorical columns, so no data sets had missing values after this step. http://scikit-learn.org . Figure 2: Lack of predictability ( BER ) of the protected at-tributes on the German Credit Adult Income, and Ricci data sets as compared to the disparate impact found in the test set when the class is predicted from the non-protected attributes. The certification algorithm guarantees that points to the right of the BER threshold are also above  X  = 0.8 , the threshold for legal disparate impact. For clarity, we only show results using the combinatorial repair, but the geometric repair results follow the same pattern. quantiles were determined in the following way. For each attribute, the protected class with the smallest number of members was determined. This size determined how many quantile buckets to create. The other protected classes were then appropriately divided into the same number of quantile buckets, with the median value in each bucket chosen as a representative value for that quantile. Each quantile value in the fully repaired version is the median of the representative values for that quantile. The combinatorial partial repair determines all valid values for an attribute and moves the original data part way to the fully repaired data within this space. The geometric repair assumes all numeric values are allowed for the partial repair.
The goal in this section is to experimentally validate our certification algorithm, described in subsection 4.2. On each of the data sets described above, we attempt to predict the protected attribute from the remaining attributes. The re-sulting BER is compared to DI ( g ) where g : Y  X  C , i.e., the disparate impact value as measured when some classifier at-tempts to predict the class given the non-protected attributes. From the underlying data, we can calculate the BER threshold e = 1 / 2  X   X  / 8. Above this threshold, any classifier applied to the data will have disparate impact. The threshold is chosen conservatively so as to preclude false positives (times when we falsely declare the data to be safe from disparate impact).
In Figure 2 we can see that there are no data points greater than the BER threshold and also much below  X  = 0.8, the threshold for legal disparate impact. The only false positives are a few points very close to the line. This is likely because the  X  value, as measured from the data, has some error. We can also see, from the points close to the BER threshold line on its left but below  X  that while we chose the threshold con-servatively, we were not overly conservative. Still, using a classifier other than the purely biased one in the certification algorithm analysis might allow this threshold to be tightened.
The points in the upper left quadrant of these charts repre-sent false negatives of our certification algorithm on a specific data set and a specific classifier. However, our certification algorithm guarantees lack of disparate impact over any classi-fier, so these are not false negatives in the traditional sense. In fact, when a single data set is considered over all classifiers, we see that all such data sets below the BER threshold have some classifier that has DI close to or below  X  = 0.8.
One seemingly surprising artifact in the charts is the vertical line in the Adult Income data chart at BER = 0.5 for the GNB repair. Recall that the chart is based off of two different confusion matrices -the BER comes from predicting gender while the disparate impact is calculated when predicting the class. In a two class system, the BER cannot be any higher than 0.5, so while the ability to predict the gender cannot get any worse, the resulting fairness of the class predictions can still improve, thus causing the vertical line in the chart.
The goal in this section is to determine how much the partial repair procedure degrades utility. Using the same data sets as described above, we will examine how the utility (see Defi-nition 5.3) changes DI (measuring fairness) increases. Utility will be defined with respect to the data labels. Note that this may itself be faulty data, in that the labels may not themselves provide the best possible utility based on the underlying, but perhaps unobservable, desired outcomes. For example, the re-sults on the test from the Ricci data may not perfectly measure a firefighter  X  X  ability and so outcomes based on that test may not correctly predict who should be promoted. Still, in the absence of knowledge of more precise data, we will use these labels to measure utility. For the Ricci data, which is unla-beled, we will assume that the true labels are those provided by the simple threshold classifier used on the non-repaired version of the Ricci data, i.e. that anyone with a score of at least 70% should pass the exam. Disparate impact ( DI ) for all data sets is measured with respect to the predicted outcomes on the test set as differentiated by protected attribute. The SVM described above is used to classify on the Adult Income and German Credit data sets while the Ricci data uses the sim-ple threshold classifier. The utility (1  X  BER ) shown is based on the confusion matrix of the original labels versus the labels predicted by these classifiers.

The results, shown in Figure 3, demonstrate the expected decay over utility as fairness increases. Each unrepaired data set begins with DI &lt; 0.8, i.e., it would fail the 80% rule, and we are able to repair it to a legal value. For the Adult Income data set, repairing the data fully only results in a utility loss from about 74% to 72%, while for the German Credit data, repairing the data fully reduces the utility from about 72% to 50% -essentially random. We suspect that this difference in decay is inherent to the class decisions in the data set (and the next section will show that other existing fairness repairs face this same decay). We suspect that the lack of linearity in the utility decay in the German Credit data after it has fairness greater than DI = 0.8 is due to this low utility.

Looking more closely at the charts, we notice that some of the partially repaired data points have DI &gt; 1. Since DI is calculated with respect to fixed majority and minority classes, Figure 3: Disparate impact ( DI ) vs. utility (1-our combinatorial and geometric partial repair processes using the SVM to classify on the Adult Income and Ger-man Credit data sets and the simple threshold classifier on the Ricci data set. Recall that only points with DI  X   X  = 0.8 are legal. DI = 1.0 represents full fairness. this happens when the classifier has given a good outcome to proportionally more minority than majority class members. These points should be considered unfair to the majority class.
Figure 3 also shows that combinatorial and geometric re-pairs have similar DI and utility values for all partial repair data sets. This means that either repair can be used. Multiple Protected Attributes. Our repair procedure can operate over the joint distribution of multiple protected at-tributes. To examine how this affects utility, we considered the Adult Income data set repaired by gender only, race only, and over both gender and race. For the repairs with respect to race, a binary racial categorization of white and non-white is used. Repairs with respect to both race and gender are taken over the joint distribution. In the joint distribution case, the DI calculated is the average of the DI of each of the three protected sets (white women, non-white men, and non-white women) with respect to the advantaged group (white men). The classifier used to predict the class from the non-protected attributes is the SVM described earlier.

The results, shown in Figure 4, show that the utility loss over the joint distribution is close to the maximum of the util-ity loss over each protected attribute considered on its own. In other words, the loss does not compound. These good results are likely due in part to the size of the data set allowing each subgroup to still be large enough. On such data sets, allowing all protected attributes to be repaired appears reasonable.
Here, we compare our results to related work on the Ger-man credit data and Adult income data sets. Logistic regres-sion is used as a baseline comparison, fair naive Bayes is the solution from Kamiran and Calders [8], regularized logistic regression is the repair method from Kamishima et al. [10], and learned fair representations is Zemel et al. X  X  solution [25]. All comparison data is taken from Zemel et al. X  X  implementa-tions [25]. Zemel et al. define discrimination as ( 1  X   X  that increasing Zemel scores mean that fairness has increased, as is the case with DI , we will look at the Zemel fairness score Figure 4: Disparate impact ( DI ) vs. utility (1-BER ) from our combinatorial and geometric partial repair processes using the SVM as the classifier. For clarity in the figure, only the combinatorial repairs are shown, though the geometric re-pairs follow the same pattern. which we define as 1  X  (( 1  X   X  )  X   X  ) = 2  X  BER . Accuracy is the usual rate of successful classification. Unlike the com-pared works, we do not choose a single partial repair point. Figure 5 shows our fairness and accuracy results for both com-binatorial and geometric partial repairs for values of  X   X  [ 0, 1 ] at increments of 0.1 using all three classifiers described above.
Figure 5 shows that our method can be flexible with respect to the chosen classifier. Since the repair is done over the data, we can choose a classification algorithm appropriate to the data set. For example, on the Adult Income data set the repairs based on Na X ve Bayes have better accuracy at high values of fairness than the repairs based on Logistic Regression. On the German and Adult data sets our results show that for any fairness value a partially repaired data set at that value can be chosen and a classifier applied to achieve accuracy that is better than competing methods.

Since the charts in Figure 5 include unrepaired data, we can also separate the effects of our classifier choices from the effects of the repair. In each classifier repair series, the data point with the lowest Zemel fairness (furthest to the left) is the original data. Comparing the original data point when the LR classifier was used to the LR classifier used by Zemel et al. as a comparison baseline, we see a large jump in both fairness and accuracy. Configuring the classifier to weight classes equally may have accounted for this improvement.
Our experiments show a substantial difference in the per-formance of our repair algorithm depending on the specific algorithms we chose. Given the myriad classification algo-rithms used in practice, there is a clear need for a future systematic study of the relationship between dataset features, algorithms, and repair performance.

In addition, our discussion of disparate impact is necessar-ily tied to the legal framework as defined in United States law. It would be valuable in future work to collect the legal frameworks of different jurisdictions, and investigate whether a single unifying formulation is possible.

Finally, we note that the algorithm we present operates only on numerical attributes. Although we are satisfied with its performance, we chose this setting mostly for its relative theoretical simplicity. A natural avenue for future work is to investigate generalizations of our repair procedures for Figure 5: Zemel fairness vs. accuracy from our combinato-rial and geometric partial repairs as compared to previous work. Legend: RLR, Regularized Logistic Regression [10]; LFR, Learned Fair Representations [25]; FNB, Fair Na X ve Bayes [8]; GNB, Gaussian Na X ve Bayes with balanced prior; LR, Logistic Regression; SVM, Support Vector Machine. datasets with different attribute types, such as categorical data, vector-valued attributes, etc. This research was funded in part by the NSF under grant BIGDATA-1251049. Thanks to Deborah Karpatkin, David Robinson, and Natalie Shapero for helping us understand the legal interpretation of disparate impact. Any misunderstand-ings about these issues in this paper are our own. Thanks also to Mark Gould for pointing us to the Griggs v. Duke decision, which helped to set us down this path in the first place. [1] S. Barocas and A. D. Selbst. Big data X  X  disparate impact. [2] T. Calders, F. Kamiran, and M. Pechenizkiy. Building [3] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and [4] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. [5] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, [6] H. Hodson. No one in control: The algorithms that run [7] T. Joachims. A support vector method for multivariate [8] F. Kamiran and T. Calders. Classifying without [9] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma. [10] T. Kamishima, S. Akaho, and J. Sakuma. Fairness aware [11] B. T. Luong, S. Ruggieri, and F. Turini. k-nn as an [12] A. Menon, H. Narasimhan, S. Agarwal, and S. Chawla. [13] W. Miao. Did the results of promotion exams have a [14] D. Pedreschi, S. Ruggieri, and F. Turini. Integrating [15] D. Pedreschi, S. Ruggieri, and F. Turini. A study of top-k [16] J. L. Peresie. Toward a coherent test for disparate impact [17] J. Podesta, P. Pritzker, E. J. Moniz, J. Holdren, and [18] A. Romei and S. Ruggieri. A multidisciplinary survey [19] Supreme Court of the United States. Griggs v. Duke [20] Supreme Court of the United States. Watson v. Fort [21] Supreme Court of the United States. Ricci v. DeStefano. [22] Texas House of Representatives. House bill 588. 75th [23] The Leadership Conference. Civil rights principles for [24] The U.S. EEOC. Uniform guidelines on employee [25] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. [26] M.-J. Zhao, N. Edakunni, A. Pocock, and G. Brown.
