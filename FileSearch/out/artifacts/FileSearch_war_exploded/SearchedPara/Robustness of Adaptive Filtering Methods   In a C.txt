 This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering. Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings. We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function. Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11. Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with  X  =0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information. H.3.3 [ Information Search and Retrieval ]: Information filtering, Relevance feedback, Retrieval models , Selection process ; I.5.2 [ Design Methodology ]: Classifier design and evaluation Algorithms, Measurement, Performance, Experimentation Adaptive filtering, topic tracking, cross-benchmark evaluations, logistic regression, Rocchio Adaptive filtering (AF) has been a challenging research topic in information retrieval. The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest. Starting from 1997 in the Topic Detection and Tracking (TDT) area and 1998 in the Text Retrieval Conferences (TREC), benchmark evaluations have been conditions[6][7][8][3][4]:  X  A very small number (1 to 4) of positive training examples  X  Relevance feedback was available but only for the system- X  Relevance feedback (RF) was not allowed in the TDT  X  TDT2004 was the first time that TREC and TDT metrics The above conditions attempt to mimic realistic situations where an AF system would be used. That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback. Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing. These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based 2) it is not obvious how to correct the sampling bias (i.e., 3) it is not well understood how to effectively tune parameters None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once. The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13]. Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods b ecause the third problem has not been thoroughly investigated. Addressing the third issue is the main focus in this paper. We argue that robustness is an important measure for evaluating and comparing AF methods. By  X  X obust X  we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora. Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts. Available training examples, on the other hand, are often insufficient for tuning the parameters. In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective. This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set. Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other. Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters? Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported. In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR). Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13]. Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14]. It was recently evaluated in adaptive filtering and was f ound to have relatively strong performance (Section 5.1). Furthermore, a recent paper [13] reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus. Stimulated by those findings, we decided to include Rocchio and LR in our cross-benchmark evaluation for robustness testing. Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora. The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study. Section 3 analyzes the differences among the TREC and TDT metrics (u tilities and tracking cost ) and the potential implications of those differences. Section 4 outlines the Rocchio and LR approaches to AF, respectively. Section 5 reports the experiments and results. Section 6 concludes the main findings in this study. We used four benchmark corpora in our study. Table 1 shows the statistics about these data sets. TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7]. The first two weeks (August 20 th to 31 st documents is the training set, and the remaining 11 &amp;  X  months (from September 1 st , 1996 to August 19 th , 1997) is the test set. TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets. The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8]. TDT3 was the evaluation benchmark in the TDT2001 dry run The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998. Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well. The splitting point for training-test sets is different for each topic in TDT. TDT5 was the evaluation benchmark in TDT2004 [4]. The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories. We only used the English versions of those documents in our experiments for this paper. The TDT  X  X opics X  differ from TREC topics both conceptually categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are  X  X orn X  and  X  X ie X , typically associated with a bursty distribution over chronologically ordered news stories. The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics. Figure 1 compares the document densities of a TREC topic ( X  X ivil Wars X ) and two TDT topics ( X  X unshot X  and  X  X PEC Summit Meeting X , respectively) over a 3-month time period, where the area under each curve is normalized to one. The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting. For example, algorithms favoring large and stable topics may not work well for short-lasting and non-stationary topics, and vice versa. Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. http://www.ldc.upenn.edu/Projects/TDT 2001/topics.html To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and D C B A N + + + = be the total number of test documents. The TREC-conventional metrics are defined as: where parameters  X  and  X  were set to 0.5 and -0.5 respectively in TREC10 (2001) and TREC11 (2002). For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics ( macro-averaging) . The TDT-conventional metric for topic tracking is defined as: where P(T) is the percentage of documents on topic T, miss the miss rate by the system on that topic, fa P is the false alarm rate, and 1 w and 2 w are the costs (pre-specified constants) for a miss and a false alarm, respectively. The TDT benchmark of 1 1 = w , 1 . 0 2 = w and 02 . 0 ) ( = T P for all topics. For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk ). To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: Clearly, trk C is the average cost per error on topic T , with and 2 w controlling the penalty ratio for misses vs. false alarms. In addition to trk C , TDT2004 also employed 1 . 0 utility metric. To distinguish this from the TREC11, we call former TDT5SU in the rest of this paper. From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function. Our objective is to maximize the former or to minimize the latter on test documents. The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions. For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU. The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU. More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU. That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme. At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 1 1 = w and 1 . 0 2 = w . However, this is not true if on average for the test corpus. Using TDT3 as an example, the true percentage is: where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets. Using 02 . 0 ) (  X  = T P as an (inaccurate) estimate of 0. 002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking. To wit: trk C where 10 estimation of P(T) compared to the truth. Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk. Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1. The implications of the above analysis are rather significant:  X  Ctrk defined in the same formula does not necessarily mean  X  Systems optimized for Ctrk would not optimize TDT5SU  X  Parameters tuned on one corpus (e.g., TDT3) might not  X  Results in Ctrk in the past years of TDT evaluations may Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall. This was a challenging part of the TDT2004 evaluation for AF. Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods. This is the first time this issue is explicitly analyzed, to our knowledge. We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic ( T ) as follows: The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights. The second term is the weighted centroid of the set ) ( T D positive training examples, each of which is a vector of within-document term weights. The third term is the weighted centroid of the set ) ( T D  X  of negative training examples which are the nearest neighbors of the positive centroid. The three terms are given pre-specified weights of  X   X  ,and  X  , controlling the relative influence of these components in the prototype. The prototype of a topic is updated each time the system makes a  X  X es X  decision on a new document for that topic. If relevance feedback is available (as is the case in TREC adaptive filtering), either ) ( T D + or ) ( T D  X  , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the system X  X  prediction ( X  X es X ) is treated as the truth, and the new document is added to ) ( T D updating the prototype. Both cases are part of our experiments in this paper (and part of the TDT 2004 evaluations for AF). To distinguish the two, we call the first case simply  X  X occhio X  and the second case  X  X RF Rocchio X  where PRF stands for pseudo-relevance feedback. The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: Threshold calibration in incremental Rocchio is a challenging research topic. Multiple approaches have been developed. The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase. More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., ) | ( d T P local regression for risk reduction [11]. It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF. Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization. Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase. This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback (Section 5.1). Results of more complex variants of Rocchio are also discussed when relevant. Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function where x r is the document vector whose elements are term weights, w  X  X o X  with respect to a particular topic. Given a training set of labeled documents standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients ( X  X he model parameters X ): This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively [14]. Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: Note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold opt  X  is constant, depending only on the pre-defined utility (or cost) function for evaluation. If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33 . 0 ) 1 2 /( 1 = + for all topics. We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: where ) ( i y s is taken to be  X  ,  X  and  X  for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions ( X  X ueries X ), on-topic documents and off-topic documents. The second term in the objective function is for regularization , equivalent to adding a Gaussian prior to the regression coefficients with mean covariance variance matrix  X   X   X  2 / 1 , where  X  is the identity matrix. Tuning  X  (  X  0) is theoretically justified for reducing model complexity ( X  X he effective degree of freedom X ) and avoiding over-fitting on training data [5]. How to find an effective  X  r is an open issue for research, depending on the user X  X  belief about the parameter sp ace and the optimal range. The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0 =  X  . We report our empirical findings in four parts: the TDT2004 optimization results, and the results corresponding to the amounts of relevance feedback. The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004. Multiple research teams participated and multiple runs from each team were allowed. Ctrk and TDT5SU were used as the metrics. Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively. Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU. All the parameters of our runs were tuned on the TDT3 corpus. Results for other sites are also listed anonymously for comparison. 
Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback. ( X  X urs X  is the Rocchio method.) We also put the 1 st and 3 rd quartiles as sticks for each site. Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback. ( X  X urs X  is LR with 0 =  X  r and 005 . 0 = Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback. ( X  X urs X  is PRF Rocchio.) Adaptive filtering without using true relevance feedback was also a part of the evaluations. In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made. Such a setting has been conventional for the Topic Tracking task in TDT until 2004. Figure 4 shows the summarized official submissions from each team. Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. We use quartiles rather than standard deviations since the former is more resistant to outliers. How much the strong performance of our systems depends on parameter tuning is an important question. Both Rocchio and LR have parameters that must be pre-specified before the AF process. The shared parameters include the sample weights  X  ,  X  and  X  , the sample size of the negative training documents (i.e., ) ( T D  X  ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector. The method-specific parameters include the decision threshold in Rocchio, and  X  r ,  X  and MI (the maximum number of iterations in training) in LR. Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation. Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f. Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2). We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004. We also tested our methods on TREC10 and TREC11 for further analysis. Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters. We repeated this procedure for several passes as time allowed. Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied. These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal. If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004. The difficulty comes from the ad hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem. Logistic regression has less difficulty with respect to threshold tuning because it pr oduces probabilistic scores of ) | 1 Pr( x y = upon which the optimal threshold can be directly computed if probability estimation is accurate. Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively. suggests the range of near-optimal settings. With these threshold settings in our experiments for LR, we focused on the cross-corpus validation of the Bayesian prior parameters, that is,  X  r summarizes the results 3 . We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU. For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11. From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report. More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR. The robustness, we believe, comes from the probabilistic nature of the system-generated scores. That is, compared to the ad hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem. Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers X  parameters do not. performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases. This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR. We also believe that variance reduction (in the testing phase) should be controlled by the choice of  X  (but not  X  r ), for which we conducted the experiments as shown in Figure 6. LR(  X  =roc*,  X  =0.01) 0.813 Best Rocchio 0.662 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) b ecause parameter optimization has been improved afterwards. The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in 
T11SU, just indicative. The performance of LR is summarized with respect to  X  tuning on the corpora of TREC10, TREC11 and TDT3. The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,. In between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR. In the case of minimizing Ctrk, the safe range for  X  is between 0 and 0.1, and setting 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed. In either case, tuning  X  is relatively safe, and easy to do successfully by cross-corpus tuning. Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the  X  X tc X  version) schemes. We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications. To answer it, we evaluated Rocchio and LR on TDT with the following settings:  X  Basic Rocchio, no adaptation at all  X  PRF Rocchio, updating topic profiles without using true  X  Adaptive Rocchio, updating topic profiles using relevance  X  documents randomly sampled from the pool of system- X  LR with 0  X  All the parameters in Rocchio tuned on TDT3. Table 3 summarizes the results in Ctrk : Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information. Incremental LR, on the other hand, was weaker but still impressive. Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR. For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost. The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk. Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio. Table 4: AF methods on TDT5 (Performance in TDT5SU) Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization. Our main conclusions from this study are the following:  X  Parameter optimization in AF is an open challenge but has  X  Robustness in cross-corpus parameter tuning is important  X  We found LR more robust than Rocchio; it had the best  X  We found Rocchio performs strongly when a good For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting. This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-0434035, by the DoD under award 114008-N66001992891808 and by the Defense Advanced Research Project Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. [1] J. Allan. Incremental relevance feedback for information [2] J. Callan. Learning while filtering documents. In SIGIR-98 , [3] J. Fiscus and G. Duddington. Topic detection and tracking [4] J. Fiscus and B. Wheatley. Overview of the TDT 2004 [5] T. Hastie, R. Tibshirani and J. Friedman. Elements of [6] S. Robertson and D. Hull. The TREC-9 filtering track final [7] S. Robertson and I. Soboroff. The TREC-10 filtering track [8] S. Robertson and I. Soboroff. The TREC 2002 filtering [9] S. Robertson and S. Walker. Microsoft Cambridge at [10] R. Schapire, Y. Singer and A. Singhal. Boosting and [11] Y. Yang and B. Kisiel. Margin-based local regression for [12] Y. Zhang and J. Callan. Maximum likelihood estimation [13] Y. Zhang. Using Bayesian priors to combine classifiers for [14] J. Zhang and Y. Yang. Robustness of regularized linear [15] T. Zhang, F. J. Oles. Text Categorization Based on 
