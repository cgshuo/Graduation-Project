 Wei Song  X  Soon Cheol Park Abstract This paper proposes an improved latent semantic analysis (LSA) model to repre-sent textual document and takes advantage of a fuzzy logic based genetic algorithm (FLGA) for clustering. The standard genetic algorithm (GA) in conventional vector space model is rather difficult to deal with because the high dimensional encoding of GA makes it explore the optimal solution in a complicated space which is prone to cause an overflow problem. The LSA-based corpus model not only reduces the dimensions drastically, but also creates an underlying semantic structure which enhances its ability of distinguishing documents in terms of concepts and indirectly improves the ability of GA for clustering (genetic clustering). A novel FLGA is proposed in conjunction with this semantic model in this study. According to the nature of biological evolution, several fuzzy controllers are given to adaptively adjust and optimize the behaviors of the GA which can effectively prevent the premature conver-gence to a suboptimum solution. The experiment results show that the fuzzy logic controllers enhance the ability of the GA to explore the global optimum solution, and the utilization of the LSA-based text representation method to FLGA further improves its clustering performance. Keywords Clustering  X  Latent semantic analysis  X  Dimensionality reduction  X  Genetic algorithm  X  Fuzzy logic  X  Multidimensional scaling 1 Introduction Textual corpora, such as web pages available on the Internet and electronic press stored in the databases of libraries and companies, becomes huge in recent decades. The need to efficiently organize, search and manage textual corpora for knowledge has brought a new interest in the development of more intelligent and efficient information retrieval (IR) and data mining strategies. In IR system there are two important factors, i.e. the approach to represent text and the measure to evaluate the similarity between query and documents, directly affect the efficiency of the retrieval results. Vector space model (VSM), one of the most widely used, computes a measure of similarity by defining a vector that represents each document, and a vector that represents the query [ 7 ]. The model is based on the idea that, the meaning of a document is conveyed by its words comprised. If one can represent the words in the docu-ment and query by vectors, it is possible to compare documents with queries to determine how similar their content is. The traditional method of determining the closeness between two vectors is to use Euclidean distance which reflects the geometrical distance in R n space. However, this would tend to make any large document appear to be not relevant to most queries, which typically are short [ 18 ]. So the size of the angle between two vectors is used to determine the closeness between them. But it is not necessary to use the actual angle. Often the expression, cosine similarity coefficient, is used instead of the angle [ 21 ].
Vector space model itself has many drawbacks. Because each unique term in the vocab-ulary represents one dimension in feature space, VSM needs a large number of features to represent high dimensions and it is easy to cause the overflow problem. Meanwhile, if we represent all texts by this way the ambiguity meaning of terms may prohibit identifying the semantic closeness between vectors. This is due to the nature of text, where the same concept can be represented by many different words, and words have ambiguous meaning. Latent semantic analysis (LSA) [ 9 , 19 ] is an automatic method that reduces this large space via singular value decomposition (SVD) to filter out the noise found in a document. So in this semantic structure two documents which have the same semantics are located close to one another because the similar contexts in the documents will have similar vectors in the multi-dimensional space [ 34 ]. In this article a transformed LSA model is proposed and proved to provide a valid semantic structure in corpus. Under this structure the whole dimension of the corpus matrix precisely simulates the original document vectors in VSM for similarity cal-culation, e.g. cosine similarity and Euclidean distance. The reduced space hopefully captures the true relationships between documents. These derived indexing dimensions, rather than individual terms, not only greatly reduce the dimensions, but also create a semantic relation-ship between terms. To investigate how this semantic model could be used effectively in IR is a well-proven unsupervised classification technique which groups the input space into K regions based on some similarity metric. Some clustering techniques that are available in lit-erature are K-means algorithm [ 24 , 32 ], ant algorithm [ 30 , 33 ] and graph theoretic approach [ 14 ]. The K-means algorithm attempts to optimize the clustering problem into a fixed num-ber of K known a prior. However, it suffers from the limitation of the suboptimum which depends on the choice of its initial clustering distribution. The ant algorithm randomly pro-jects the original data into a space of the bidimensional output grids. It performs clustering by a cyclic process, e.g. picking up, moving and dropping, to group together items that are similar to each other. Xia et al. [ 33 ] propose a modified ant-based clustering algorithm which adjusts the movement of each ant. For example, it directs the laden ant toward the dense area containing the same type of items and the unladen ant is guided toward the area that contains dissimilar items. However, it still cannot overcome the problem of slow convergence by using the simple adaptive rules. Koontz et al. [ 14 ] propose a graph theoretic approach. A directed tree is formed first among the data set by estimating the density gradient at each point. The clustering is realized by finding the valley of the density function. The quality of the result relies on the quality of the estimation technique for the density gradient.
Genetic algorithm (GA) [ 17 ] is a robust probabilistic search and optimization technique guided by the ideas from natural genetics and evolution principles. It can provide near optimal solutions to the fitness function in multi-dimensional space [ 3 , 27 ]. In this paper we propose a novel fuzzy logic-based genetic algorithm (FLGA) which exerts several control parameters to manipulate the operators of GA. First of all, we use the evolution effect evaluated from the previous and the current generation to depict the trend of evolution. Different evolution effects lead to the different evolution strategies and then another two parameters from the current generation are defined to control the probability of evolution operators, e.g. cross-over and mutation. The method estimates the direction of the evolution which can effectively avoid convergence to a local optimal solution. Meanwhile it can appropriately adjust the influence between the selection pressure and the diversity of population via the properly defined evolution operators.

The remainder of this paper is organized as follows: The detail of LSA for vector space expansion is described in Sect. 2 . Section 3 explains how our FLGA is used, in conjunc-tion with the LSA-based model, for text clustering. Experiment results are given in Sect. 4 . Conclusions are given in Sect. 5 . 2 Latent semantic analysis for vector space expansion The LSA is a well developed method which projects the high dimensional document vectors into a space with latent semantic dimensions. SVD [ 12 , 22 , 31 ] is a mathematical concept, which is commonly used in most of the LSA methods. 2.1 The proof of singular value decomposition for vector space expansion The original corpus can be initially represented as a document-by-term matrix D ( n  X  m ) , where n is the number of documents and m is the number of terms. The transpose of matrix D is the term-by-document matrix A ( m  X  n ) and the SVD of A is defined as where U is an m  X  m orthogonal matrix whose columns define the left singular vectors of A , V T is an n  X  n orthogonal matrix whose rows define the right singular vectors of A ,and is an m  X  n diagonal matrix containing the singular values  X  1  X   X  2  X   X  X  X   X   X  min ( m , n ) . The relative sizes of the factors U ,, V T for the cases m  X  n and m &gt; n are illustrated in Figs. 1 and 3 . All off-diagonal elements of matrix are zeros.

It has been known in corpus literature that the number of terms is much more than the number of documents, that is, m is much greater than n . However, considering the integrated proof of our latent semantic space whose whole dimensions precisely simulate the original document-by-term matrix D for similarity calculation and the reduced space hopefully cap-tures the true relationships between documents, we will not ignore the m  X  n case. Thus, for the case m  X  n , in our method the new corpus matrix C is proposed as Figure 2 illustrates the construction of matrix C when m  X  n .Wecanseethenewlyformed C is an n  X  m matrix.
Suppose D 1 and D 2 are two random rows in the document-by-term matrix D ( n  X  m ). C 1 and C 2 are the two corresponding rows in matrix C. D 1 and D 2 are represented { v , 1 ,v 1 , 2 ,...,v 1 , m } and { v 2 , 1 ,v 2 , 2 ,...,v 2 , m } respectively. From ( 3 )wehave: The inner product between D 1 and D 2 is defined by D 1 and D 2 , respectively. Since matrix U is a unitary matrix, then we have: From ( 4a ), ( 4b )and( 5a ) we obtain that Similarly, from ( 4a )( 4b )( 5b )and( 5c ) we obtain that In VSM, cosine measure is commonly used to compute the similarity between two documents. The cosine similarity between D 1 and D 2 are defined by and the cosine similarity between C 1 and C 2 are defined by Hence, from ( 6a )( 6b )( 6c )( 7a )and( 7b ) we obtain that Moreover, if Euclidean distance is also used to evaluate the similarity between the two doc-uments, the Euclidean distance between D 1 and D 2 are defined by and the Euclidean distance between C 1 and C 2 are defined by Subsequently, from ( 6a )( 6b )( 6c )( 9a )and( 9b ) we obtain that
Here we have proved that the whole space of matrix C precisely simulates the original document-by-term matrix D for similarity calculation when m  X  n . For the case m &gt; n , the SVD of A is illustrated in Fig. 3 . We only use matrix U 1 for the new corpus matrix C construction. The size of the matrix U 1 is the economy size of the matrix U , depending on the number of the nonzero singular values in matrix . 1 ( n  X  n ) is the nonzero part in matrix . Thus, when m &gt; n the new corpus matrix C is defined by
Figure 4 illustrates the construction of matrix C when m &gt; n .FromFig. 4 we can see the newly constructed corpus C is an n  X  n matrix. That is, the number of dimensions in the original corpus matrix D is decreased from m to n in the new corpus matrix C .

Here we will prove the cosine similarity and Euclidean distance between two randomly selected rows in matrix C are also the same to those of the corresponding rows in the original corpus D when m &gt; n .
 that is, the original corpus matrix D can be represented by the multiplying of V , T 1 and U T 1 , rather than the whole dimension of U . Then, from ( 12 )wehave In terms of the property of unitary matrix, it X  X  easy to obtain Subsequently, from ( 13 )and( 14 ) we can obtain Namely, where D 1 and D 2 are two rows in matrix D ( n  X  m ) .Hence, Since U is also a unitary matrix, in terms of the property of unitary matrix and ( 11 )( 15a ) ( 15b )wehave where C 1 and C 2 are the two rows in matrix C corresponding to D 1 and D 2 in matrix D , respectively. Similarly, we can obtain Thus, according to the definition of cosine similarity ( 7a )( 7b ) and the definition of Euclid-ean distance ( 9a )( 9b ), ( 8 )and( 10 ) are also approved. That is, the cosine similarity and Euclidean distance between two randomly selected rows in matrix C aresametothoseof the corresponding rows in original corpus D when m &gt; n . 2.2 The Latent semantic analysis for vector space expansion In the remainder of this paper, we only consider the m &gt; n case, because in IR system the number of terms is much more than the number of documents. For reducing the dimension in SVD, we can simply choose the k largest singular values and the corresponding left and right singular vectors. The approximation matrix A k is given by where U k is comprised of the first k columns of the matrix U and V T k is comprised the first k rows of the matrix V T . k = diag ( X  1 , X  2 ,..., X  k ) is the diagonal matrix comprised of the first k factors. SVD is a mathematic concept commonly adopted in most LSA method. LSA is firstly proposed in query-based IR system [ 5 , 7 ]. Nowadays, it is also widely adopted in clustering [ 4 ] and classification [ 28 ] system. For example, when LSA is used in the query-based IR system, a query q is firstly represented as a vector in k -dimensional space. Hence, in traditional LSA method the query vector is given by
When LSA is used in document-based text clustering and classification system, Sun et al. [ 28 ] propose a supervised categorization approach which represents the document with the most discriminative basis vectors. Similarly, a document is regarded as a query in the query-based system, and each document vector is given by
We propose a transformed LSA technique in our systems. The main difference between our approach and the traditional LSA is that we only use U 1 k matrix instead of the multiply-ing of U k and  X  1 k in the later approach, because we have proved that the cosine similarity and the Euclidean distance between two randomly selected rows in matrix C ( 11 )issame to those of the corresponding rows in original corpus D. That is, the whole dimension of C can precisely simulate the matrix D. Meanwhile, the reduced ranks of matrix C can capture the true semantic relationships between documents with the help of LSA. Furthermore, our experimental results show that the traditional approach is not as good as our transformed method. Thus, our LSA-based corpus C k is given by where U 1 k is the first k columns of the economy part of the matrix U , and each document vector is newly formed by 3 Genetic clustering based on the latent semantic analysis In GA a random distributed population is created first. Each individual in the population is encoded in the form of strings, called chromosomes. A fitness function, corresponding to each chromosome, represents the degree of fitness. Biologically inspired operators, such as selection, crossover and mutation, are exerted to yield new offspring. These operators continue several generations till the termination criterion is satisfied [ 26 ]. 3.1 Chromosome encoding When we implement GA to document clustering, each chromosome ch i in the population is initialized by K centers.
In traditional VSM method, each center i is initialized by a randomly selected document from document-by-term matrix D ( n  X  m )
In our LSA-based approach each center i is initialized by a document vector C ( 11 ), that is, where n ( n &lt; m ) is the number of documents in the data set. From the definition of the number of bits in center i can be reduced from n to k ( k &lt; n )
In comparison with ( 23 ), the length of center i is much shorter than that of center i . 3.2 Evolution operators The selection process is directed under the expected value model. This mechanism can effectively overcome the restriction of the classical roulette wheel selection [ 7 ]. The later method may cause an error between the actual times and the expected times for selection. ( n  X  fit ( X i )/ n the total number of individuals. Then the decimal part of the expected value is ranked for the subsequent selection. For example, if the expected value for individual X i is 1.32, the individual X i will be selected one time first and then its decimal part (0.32) in the rank list will determine its subsequent selection.

In this paper we apply several control parameters to manipulate the crossover probability p and the mutation probability p m for each individual. We firstly define the evolution effect E i from the previous and the current generation to depict the evolution trend for individual X ,thatis, a progress in the current generation. Otherwise, the individual X i is retrogressive or remains the interval (0,1]. That is, we limit the effect of the later cases as a small positive number and stress the effect of the former case which can do better in denoting the development. Also, E i can be considered as an extension of Shepard X  X  law [ 25 ], which claims that exponential functions are a universal law of stimulus generalization for psychological science. The mean evolution effect for the current generation is given by where p is the number of the individuals in the population. The mean evolution effect is used to denote the direction of evolution. If it is a big progress, we know that the current generation contains more numbers of excellent individuals and we can moderately decrease a medium state, we can slightly adjust the diversity of population, and if it is retrogressive, we need to greatly expand the diversity of population by enhancing the probability of crossover based on the fitness function itself. The value of the evolution rate  X  is defined in Table 1 .
In order to vary p c and p m adaptively, for preventing premature convergence of GA to a local optimum solution, it is essential to be able to identify whether the GA is converging to an optimum. We use Var to depict the distribution of the population in the current generation, that is, where fit max , fit and fit min represent the maximum, the average and the minimum values of the fitness, respectively, in current generation. Var depicts the closeness of the distribution in the current generation. It is likely to be less for the population that has converged to an opti-mal solution than that for a population scattered in the solution space. We here empirically divide the value of Var into three intervals according to its mathematic definition.
The relative distance between the fitness of the individual X i and that of the best individual is defined by
The parameters Var and G are defined in the interval [0, 1]. Var is determined by the dis-tribution of all individuals in the current population and G is given by a specific individual.
In order to optimize the behaviors of GA, we adopt several principles to direct the crossover and mutation appropriately. The principles include: 3. If the evolution process tends to a local optimum solution, we will enhance p m ( X i ) to 4. If the current generation has an extensive diversity, we need to decrease p m ( X i ) . 5. At the beginning of the evolution an extensive crossover is used to make a strong explo-
From the rules above, we use a series of integers to depict the approximate trend of p c and p m in the next generation, where positive sign means enhancement, negative sign means reduction, and the absolute value represents the magnitude. In Table 2 , x and y represent the intensity of p c and p m respectively.

From Table 2 we can see that if G &lt; Va r ( G &lt; Va r a n d 0 &lt; Va r  X  1 4 ) ,thatis,the fitness of the individual X i is close to the best fitness and the excellent individual X i has high probability to be maintained in the next generation. So p c ( X i ) is decreased although Var is small. However, this case may cause a local optimum (0 &lt; Va r  X  1 4 ). Thus, the value current population, we need to decrease the p m ( X i ) although G  X  Var. However, the p c ( X i ) means the average distribution of all individuals is very distant from the best individual. So we need to enhance the proportion of selection p s although this case seldom occurs.
We use two monotonically increased functions to generate u and v which affects p c and p m in the next generation. Parameters u and v are defined as
The crossover probability p c determines the rate at which individuals are subjected to crossover. The higher the value of p c , the quicker are the new individuals introduced into the population. However, the individuals may be disrupted faster than the selection exploits them if a high p c is applied. Mutation is just a secondary operator to restore evolving material. So here we define a moderately small k 1 and a small k 2 to plot out the magnitude for p c and p m in the different situations. Moreover, we have defined the evolution rate  X  which can adjust p and p m in general. Hence, in the next generation the crossover probability p c ( X i ) and the where  X  is the evolution rate given in Table 1 and  X  is the rate of selection proportion. Because is selected from the parent solutions as the actual crossover probability. We use double-point crossover in the primary 200 generations to make a strong exploration and then the classical single-point crossover is used in the subsequent generations. In terms of the Gaussian prob-ability distribution [ 15 , 35 ], the mutation chromosome generates its offspring as survival to next generation. Table 3 illustrates the values of  X  X ,  X  X  and  X  in the different cases. From part I we can see that because the current generation makes a big progress, we need to decrease the diversity of population. However, if 0 &lt; Va r  X  1 4 and G  X  Var we still slightly enhance the p c .Thatis,the p c of the distant individual X i will increase when the distribution of the current generation is very tight which may lead to a local optical solution. Part II shows that the current generation makes a small progress or keeps a medium state. Thus, sometimes the diversity of population is enhanced and sometimes it is reduced depending on the synthetical influence by Var and G . Part III illustrates that the current generation is retrogressive and very distant from the best sample and close to the worst sample, the proportion of selection will enhance to achieve more numbers of excellent individuals, no matter which kind of development it is. Otherwise, the proportion of selection will be reset to the original value. Moreover, it is intuitive that when the current generation is progressive we will decrease the diversity of population and greatly enhance the rate of selection proportion  X  . 3.3 Fitness function and termination criterion The Davies X  X ouldin index [ 8 ] has been utilized for computing the fitness of chromosomes. separation [ 2 ]. The scatter within the i th cluster is computed as is, S i is the average cosine similarity between each document in cluster C i and centroid z i . The distance between cluster C i and C j is defined by cosine similarity Subsequently, we compute The Davis X  X ouldin (DB) is then defined as
Since the objective is to minimize the within-cluster spread and maximize the inter-cluster separation for obtaining appropriate clustering, the fitness function for each chromosome in this article is defined as DB. The population in our FLGA is 300. The algorithm is terminated when the number of generations reaches to maximum iterations (1,000) or when the iterations without improvement reach consecutive N max ( N max = 20 ) . Figure 5 displays the procedure the proposed algorithm based on the LSA for text clustering. 3.4 Comparison with the conventional GA In early versions of GAs, the parameters p c , p m and p s (selection proportion) are constant in all generations or they are simply adjusted by some analytical functions. So a large population is used to prevent a premature convergence. However, this would cause a high computation cost. Meanwhile, according to the analysis of evolving process, these three parameters affect largely the behaviors of GA, e.g. the capability to converge to an optimum and the capability to explore new regions of the solution space. Thus, they need to be properly defined manually or they are dynamically adjusted in accordance with the nature of evolution. Because GA(s) are extensive studied and have a large amount of implicit parallelism. We only use the stan-dard GA [ 17 ] combining with some simple evolving strategies and analytical functions as the conventional GA. So in the conventional GA, in order to make a high quality exploration, two individuals, with relatively high fitness, obtain more chance for crossover. When the iterations of the best fitness without improvement reach consecutive n max ( n max = 10 ) gen-erations, the diversity of the population is extended by increasing p c and p m . The dynamic p c and p m are given by where  X  1 and  X  2 are two small constants between 0 and 1, and n ( n &gt; n max ) is the number of the consecutive iterations without improvements. However, some important factors affecting the relationship between p c , p m and p s are not be taken into account. Generally speaking, if we consider the fuzzy logic-based parameters associated with the nature of the evolution, it not only can effectively avoid trap into a local optimum but also accelerate the evolving speed. The next section provides the results of FLGA based on our LSA model for text clustering, along with its comparison with that of the conventional GA. 4 Experiments results and analysis In this section we implement our method of FLGA to two widely adopted benchmark data sets, Reuters-21578 collection and 20 Newsgroups corpus (20-news-18828 version), for text clustering. In the current test data set 1 containing 200 documents from four 20 Newsgroups topics (comp.windows.x 50, rec.motorcycles 50, sci.space 50, talk.politics.mideast 50) and data set 2 containing 600 documents from six Reuters topics (coffee 100, crude 100, grain 100, ship 100, sugar 100, trade 100) are selected. After being processed by word extraction, stop word removal, and Porter X  X  stemming [ 20 ], there are 7,117 terms and 5,870 terms for data set 1 and data set 2, respectively, in the vocabulary. Moderately large values of p c and small values of p m are commonly employed in GA practice. In our experiment the parameters p c and p m for each individual are initially set as 0.3 and 0.05 respectively. The initial p s is 0.3 and we restrict the maximum value of p s to be 0.6. 4.1 Feature selection and the LSA-based corpus construction In order to create a set of initial feature vectors for the representations of the test documents, in the original case, we need to convey the documents to the feature vectors. So for data set 1 and data set 2, each vector contains 7,117 and 5,870 features respectively. The feature vector of each document can represented as The main difficulty in the application of GA to document clustering is the high dimension-ality of the input feature space, typical for textual data. This is because each unique term in the vocabulary represents one dimension. Savio and Lee [ 23 ] introduce four kinds of methods to reduce the dimensions of the feature space. In our experiment we decrease the number of terms from 7,117, 3,500 to 1,500 for data set 1 and from 5,870, 2,500 to 1,000 for data set 2, respectively, by choosing the highest term weights to construct the original document-by-term matrix D ( 1 ). The formula to calculate the term weight is given by where id f j is log ( N / n ), N is the total number of documents in the data sets, and n is the number of documents in which the i th term comprised; tf ij is the term frequency of i th indexing term in document j ; dl is the length of document. This formula normalizes the length of documents rather than the simple tf  X  id f . So the original documents and the reduced documents can be represented by
Then we use the LSA method proposed to further decrease the dimensions in the semantic space. We compare the performance by varying the number of the dimensions k in LSA-based corpus C k ( 20 ) from 200, 180, 160, 140, 120, 100, 90, 80, 70, 60, 50 to 40 for data set 1 and from 600, 550, 500, 450, 400, 350, 300, 250, 200, 180, 160, 140, 120, 100, 90, 80, 70, 60, 50 to 40 for data set 2 respectively.

As a library of programming functions mainly aimed at real time computer vision, OpenCV (Open Source Computer Vision) is performed for SVD in our approach. We implement the FLGA in C++ language and run on a Pentium 3.2 GHz PC. All the figures in the experiments are drawn by Matlab 7.0 using the data generated by the program.

We implement our experiments in two steps: (1) using SStress [ 29 ] to evaluate our LSA-clustering. 4.2 The evaluation of our LSA-based text model We apply multidimensional scaling (MDS) method to analyze the similarity (or dissimilarity) between the original document-by-term matrix D ( 1 ) and the proposed objective matrix C k ( 20 ) with different ranks. MDS represents objects as points in a Euclidean space so that the perceived distances between points can reflect similarity (or dissimilarity) between objects. The dissimilarity between the two matrixes is evaluated by SStress criterion [ 1 , 29 ], which is given by where n is the number of the documents in the test data set. s i , j represents the cosine sim-ilarity between documents d i and d j in the original document-by-term matrix D ( 1 )and s j represents the cosine similarity between our LSA-based documents represented by (w j , 1 ,w j , 2 ,...,w j , m ), s i , j is then given by where m is the total number of terms in the data set. Assuming given by where k ( k  X  n ) is number of the dimensions selected in the LSA-based corpus C k ( 20 )and n is the total number of documents. The SStress value against the number of dimensions k on data set 1 and data set 2 are shown in Figs. 6 and 7 .

From Fig. 6 we can see that, on each dimension k , the value of SStress between the original document-by-term matrix D 1 ( 200  X  7 , 117 ) and the corresponding LSA-based cor-D ( 200  X  1 , 500 ) &amp; C 3 k ,where C 2 k and C 3 k are the corresponding LSA-based corpus matri-than that of the later pair of matrices on each dimension. Generally, the value of SStress decreases with the raising of dimensionality for each pair of matrices. In other words, the objective matrices C k (s) are matching to their original matrices D (s) increasingly by cosine similarity. For all the three situations, the value of SStress is less than 100 ranging from 110 to 200 dimensions. Meanwhile, matrices C k (s) create a robust structure with the help of latent semantics which denotes a slight difference to their original matrices. When the dimension k reaches to 200, matrices C k (s) precisely imitate their original document-by-term matrices D (s) with SStress value zero.

Figure 7 shows that, on each dimension, the value of SStress between D 1 ( 600  X  5 , 870 ) and C 1 k is larger than that of the other pairs of matrices. Similarly, the value of SStress decreases with the raising of dimensionality for each pair of matrices. When the dimension k increases to 600, matrices C k (s) precisely simulate their original document-by-term matrices SStress D (s). From 310 to 600 dimensions, the value of SStress is less than 200. In the next part the proposed FLGA based on the LSA is implemented for text clustering. 4.3 The performance and analysis of FLGA for text clustering We use precision, recall and F-measure [ 10 ] to evaluate the performance of our clustering algorithm. Let D be the number of documents in cluster i and let A be the number of doc-uments from topic j , which is also predefined for purpose of test. Meanwhile, let M be the number of documents in the intersection of D and A .Thatis, M is the number of documents And the average clustering precision is defined by where K is the number of clusters. The recall R ij for the cluster i in topic j is defined by SStress And the average recall in K clusters is defined by Thus the F-measure is defined by In order to measure the performance, we consider several best F-measures of each method. The best F-measures are obtained by performing the experiments repeatedly (in our case, 30 times) and we choose the average value of the five best F-measures of each method as the final result in our experiments. Figures 8 and 9 show the performance of the FLGA proposed with the different ranks k . We also compare the FLGA with the conventional GA. Form Fig. 8 we can see that the F-measure of GA with 1,500 terms in VSM is 0.704. The F-measure of GA with 1,500 terms in the LSA model increases with the raising of dimensionality. From about 73 dimensions the performance of GA with 1,500 terms in LSA outperforms that of GA in VSM, from the dimension 80, it outperforms that of the FLGA in VSM and on dimension 120, it gets the best performance with value of 0.750. The F-measure of FLGA with 1,500 terms in VSM is 0.735 which is better than that of GA in VSM. The performance of FLGA is further enhanced in the LSA-based text representation model. From 70 dimensions the F-measures of FLGA(s), with the series numbers of terms in LSA model, outperform that of FLGA in VSM. For 7,117 and 3,500 terms in LSA, FLGA(s) obtain their F-measure FLGA+LSA+7117T best performance on 120 dimensions with values of 0.800 and 0.787 respectively. For 1,500 terms in the LSA, FLGA obtains its best performance on 140 dimensions with value of 0.783. Meanwhile, from Fig. 6 we can see that, from 110 to 200 dimensions, the value of SStress is less than 100. In such range, LSA captures the true relationships between documents which are slightly different from that provided by the original document-by-term matrix. Moreover, in Fig. 7 on 140 dimensions, the F-measure of FLGA with 1,500 terms is very close to that with 3,500 terms. When the dimension is close to the 200, the performance of FLGA(s), with the series numbers of terms in LSA model, are slightly decreased in comparison with their best performances.

From Fig. 9 we can see that the blue real line represents the F-measure of GA with 1,000 terms in VSM. The red real line represents the F-measure of FLGA with 1,000 terms in VSM. From 100 dimensions the performance of GA with 1,000 terms in LSA outperforms that of GA in VSM and from 160 dimensions it outperforms that of FLGA in VSM. The F-measures of FLGA(s), with the series numbers of terms in LSA, increase with the raising of dimen-sionality. For 5,870 and 2,500 terms in LSA model, FLGA(s) obtain their best performance on 300 dimensions. For 1,000 terms in the LSA, FLGA obtains its best performance on 350 dimensions. In comparison with Fig. 7 , from 310 to 600 dimensions, the value of SStress is less than 200 where the LSA constructs a robust semantic space and improves its ability of distinguishing documents. The best performances of FLGA(s) with 5,870, 2,500 and 1,000 terms in LSA model are 0.792, 0.780 and 0.765 respectively.

So far we have obtained that for data set 1 FLGA with 1,500 terms gets its best performance on 140 dimensions and the conventional GA with 1,500 terms gets its best performance on 120 dimensions. For data set 2, FLGA and the conventional GA, with 1,500 terms, obtain F-measure FLGA+LSA+5870T their best performances on 350 dimensions. We consider the value of the best fitness in each generation for these four situations. The number of generations is divided into ten steps: 100, 200, 300, 400, 500, 600, 700, 800, 900 and 1,000. In order to make a fair comparison, we create the same initial population for FLGA and the conventional GA first. Figure 10 illustrates the results of the best fitness as the increase of generations.

We can see from in Fig. 10 that the best fitness increases with the raising of number of generations. Although the conventional GA converges rapidly for data set 1, the best fitness increases slowly from 300 to 1,000 generations where the conventional GA is trapped into a local optimum. The best fitness of FLGA increases rapidly and has more good results than that of the conventional GA for data set 1. For data set 2, the conventional GA and FLGA converge almost from the same generations (about 400 generations), but the best fitness of FLGA is much better than that of the conventional GA. This implies that FLGA is more effective than the conventional GA in finding better results. That is, if we want to find a much better result as soon as possible, we had better apply our fuzzy logic controllers to GA. 5 Conclusions This paper proposes a FLGA for textual document clustering. The standard GA has the draw-back of slow evolving due to the high dimensional exploration space, especially in VSM. FLGA overcomes the problem of slow evolving and can also efficiently escape from the premature convergence to a suboptimum solution. An improved LSA model is proposed and proved in this study. The analysis reveals that the LSA model not only creates a robust Fitness semantic structure to represent document, but also reduces the dimensions drastically which indirectly improve the exploration ability of the clustering algorithm. The experiment results show that the fuzzy logic-based controllers enhance the performance of GA for text clustering and the application of the LSA model to FLGA further improves its accuracy and efficiency. References Author Biographies
