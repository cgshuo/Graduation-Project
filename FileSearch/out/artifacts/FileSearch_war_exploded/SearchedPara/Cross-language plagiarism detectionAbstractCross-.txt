 Abstract Cross-language plagiarism detection deals with the automatic identifi-cation and extraction of plagiarism in a multilingual setting. In this setting, a suspicious document is given, and the task is to retrieve all sections from the docu-ment that originate from a large, multilingual document collection. Our contributions in this field are as follows: (1) a comprehensive retrieval process for cross-language plagiarism detection is introduced, highlighting the differences to monolingual pla-giarism detection, (2) state-of-the-art solutions for two important subtasks are reviewed, (3) retrieval models for the assessment of cross-language similarity are surveyed, and, (4) the three models CL-CNG, CL-ESA and CL-ASA are compared. Our evaluation is of realistic scale: it relies on 120,000 test documents which are selected from the corpora JRC-Acquis and Wikipedia, so that for each test document highly similar documents are available in all of the six languages English, German, Spanish, French, Dutch, and Polish. The models are employed in a series of ranking tasks, and more than 100 million similarities are computed with each model. The results of our evaluation indicate that CL-CNG, despite its simple approach, is the best choice to rank and compare texts across languages if they are syntactically related. CL-ESA almost matches the performance of CL-CNG, but on arbitrary pairs of languages. CL-ASA works best on  X  X  X xact X  X  translations but does not generalize well.
 Keywords Cross-language Plagiarism detection Similarity Retrieval model Evaluation 1 Introduction Plagiarism, the unacknowledged use of another author X  X  original work, is considered as one of the biggest problems in publishing, science, and education. Texts and other works of art have been plagiarized all throughout history, but with the advent of the World Wide Web text plagiarism is observed at an unprecedented scale. This observation is not surprising since the Web makes billions of texts, code sources, images, sounds, and videos easily accessible, that is to say, copyable.

Plagiarism detection, the automatic identification of plagiarism and the retrieval of the original sources, is developed and investigated as a possible countermeasure. Although humans can identify cases of plagiarism in their areas of expertise quite easily, it requires much effort to be aware of all potential sources on a given topic and to provide strong evidence against an offender. The manual analysis of text with respect to plagiarism becomes infeasible on a large scale, so that automatic plagiarism detection attracts considerable attention.

The paper in hand investigates a particular kind of text plagiarism, namely the detection of plagiarism across languages, sometimes called translation plagiarism. The different kinds of text plagiarism are organized in Fig. 1 . Cross-language plagiarism, shown encircled, refers to cases where an author translates text from another language and then integrates the translated text into his/her own writing. It is reasonable to assume that plagiarism does not stop at language barriers since, for instance, scholars from non-English speaking countries often write assign-ments, seminars, theses, and papers in their native languages, whereas current scientific discourse to refer to is often published in English. There are no studies which directly assess the amount of cross-language plagiarism, but in 2005 a broader study among 18,000 students revealed that almost 40% of them admittedly plagiarized at least once, which also includes cross-lingual cases (McCabe 2005 ). Apart from being an important practical problem, the detection of cross-language plagiarism also poses a research challenge, since the syntactical similarity between source sections and plagiarized sections found in the monolingual setting is more or less lost across languages. Hence, research on this task may help to improve current methods of cross-language information retrieval as well. 1.1 Related work Clough ( 2003 ) and Maurer et al. ( 2006 ) survey plagiarism detection approaches; here, we merely extend these surveys by recent developments. All of the different kinds of plagiarism shown in Fig. 1 are addressed in the literature: the detection of exact copies (Brin et al. 1995 ; Hoad and Zobel 2003 ), the detection of modified copies (Stein 2005 , 2007 ), and, for both of the former, their detection without reference collections (Meyer zu Eissen and Stein 2006 ; Meyer zu et al. 2007 ; Stein and Meyer zu Eissen 2007 ). Cross-language plagiarism detection has also attracted attention (Barro  X  n-Ceden  X  o et al. 2008 ; Ceska et al. 2008 ; Pinto et al. 2009 ; Potthast et al. 2008 , Pouliquen et al. 2003b ). However, the mentioned research still focuses on a subtask of the retrieval task, namely text similarity computation across languages. i.e., the part is mistaken for the whole and it is overlooked that there are other subtasks that must also be tackled in order to build a practical solution. We also observe that the different approaches are not evaluated in a comparable manner. 1.2 Outline and contributions Section 2 introduces a comprehensive retrieval process for cross-language plagia-rism detection. The process is derived from monolingual plagiarism detection approaches, while two important subtasks that are different in a multilingual setting are discussed in detail: Sect. 3 is about the heuristic retrieval of candidate documents, and Sect. 4 surveys retrieval models for the detailed comparison of documents. With respect to the latter, Sect. 5 presents a large-scale evaluation of three retrieval models to measure the cross-language similarity of texts: the CL-CNG model (Mcnamee and Mayfield 2004 ), the CL-ESA model (Potthast et al. 2008 ), and the CL-ASA model (Barro  X  n-Ceden  X  o et al. 2008 ). All experiments were repeated on test collections sampled from the parallel JRC-Acquis corpus and the comparable Wikipedia corpus. Each test collection contains aligned documents written in English, Spanish, German, French, Dutch, and Polish. 2 Retrieval process for cross-language plagiarism detection Let d q denote a suspicious document written in language L , and let D 0 denote a document collection written in another language L 0 . The detection of a text section in d q that is plagiarized from D 0 can be organized within three steps (see Fig. 2 ): 1. Heuristic retrieval From D 0 a set of candidate documents D 0 q is retrieved where 2. Detailed analysis Each document in D 0 q is compared section-wise with d q , 3. Knowledge-based post-processing The candidates for cross-language plagia-
At first sight this process may appear rather generic, but the underlying con-siderations become obvious when taking the view of the practitioner: since plagiarists make use of the World Wide Web, a plagiarism detection solution has to use the entire indexed part of the Web as reference collection D 0 . This requires the retrieval of candidate documents D 0 q with | D 0 q | | D 0 |, since a comparison of d q against each Web document is infeasible. The following sections discuss particularities of step 1 and 2 with respect to a multilingual setting. Note that the third step requires no language-specific treatment. 3 Heuristic retrieval of candidate documents We identify three alternatives for the heuristic retrieval of candidate documents across languages. They all demonstrate solutions for this task, utilizing well-known methods from cross-language information retrieval (CLIR), monolingual informa-tion retrieval (IR), and hash-based search. Figure 3 shows the alternatives. The approaches divide into methods based on a focused search and methods based on hash-based search. The former reuse existing keyword indexes and well-known keyword retrieval methods to retrieve D 0 q , the latter rely on a fingerprint index of D 0 where text sections are mapped onto sets of hash codes.
 Approach 1 Research in cross-language information retrieval addresses keyword query tasks in first place, where for a user-specified query q in language L documents are to be retrieved from a collection D 0 in language L 0 . By contrast, our task is a so-called  X  X  X uery by example task X  X , where the query is the document d q , and documents similar to d q are to be retrieved from D 0 . Given a keyword extraction algorithm both tasks are solved in the same way using standard CLIR methods: translation of the keywords from L to L 0 and querying of a keyword index which stores D 0 .
 Approach 2 In this approach d q is translated from L to L 0 with machine translation technology, this way obtaining d 0 q . Afterwards keyword extraction is applied to d 0 q , which is similar to Approach 1, and the keyword index of D 0 is queried with the extracted words in order to retrieve D 0 q . This approach compares to the first one in terms of retrieval quality, however, Approach 3 provides a faster solution if d q is translated to d 0 q .
 Approach 3 A fingerprinted document d q is represented as small set of integers, called fingerprint. The integers are computed with a similarity hash function h u which operationalizes a similarity measure u and which maps similar documents with a high probability onto the same hash code. Given d q  X  X  translation d 0 q , the set of candidate documents is retrieved in virtually constant time by querying the fingerprint index of D 0 with h u ( d 0 q ). An alternative option, which has not been investigated yet, is the construction of a cross-language similarity hash function. With such a function at hand the task of translating d q to d 0 q can be omitted. Remarks Given the choice among the outlined alternatives the question is  X  X  X hich way to go? X  X . Today we argue as follows: there is no reason to disregard existing Web indexes, such as the keyword indexes maintained by the major search engine providers. This favors Approach 1 and 2, and it is up to the developer if he trusts the CLIR approach more than the combination of machine translation and IR, or vice versa. Both approaches require careful development and adjustment in order to work in practice. However, if one intends to index portions of the Web in order to build a dedicated index for plagiarism detection purposes, hash-based search (Approach 3) is the choice. It provides near-optimum retrieval speed at reasonable retrieval quality and a significantly smaller index compared to a keyword index (Potthast 2007 ; Stein 2007 ; Stein and Potthast 2007 ). 4 Detailed analysis: retrieval models to measure cross-language similarity This section surveys retrieval models which can be applied in the detailed analysis step of cross-language plagiarism detection; they measure the cross-language similarity between sections of the suspicious document d q and sections of the candidate documents in D 0 q . Three retrieval models are described in detail, the cross-language character 3-gram model, the cross-language explicit semantic analysis model, and the cross-language alignment-based similarity analysis model. 4.1 Terminology and existing retrieval models In information retrieval two real-world documents, d q and d 0 , are compared using a retrieval model R , which provides the means to compute document representations d q and d 0 as well as a similarity function u . u ( d q , d 0 ) maps onto a real value which indicates the topical similarity between d q and d 0 . A common retrieval model is the vector space model, VSM, where documents are represented as term vectors whose similarity is assessed with the cosine similarity.

We distinguish four kinds of cross-language retrieval models (see Fig. 4 ): (1) models based on language syntax, (2) models based on dictionaries, gazetteers, rules, and thesauri, (3) models based on comparable corpora, and (4) models based on parallel corpora. Models of the first kind rely on syntactical similarities between languages and on the appearance of foreign words. Models of the second kind can be called cross-language vector space models. They bridge the language barrier by translating single words or concepts such as locations, dates, and number expressions from L to L 0 . Models of the third and fourth kind have to be trained on an aligned corpus that contains documents from the languages to be compared. The two approaches differ with respect to the required degree of alignment: comparable alignment refers to documents in different languages, which describe roughly the same topic, while parallel alignment refers to documents that are translations of each other and whose words or sentences have been mapped manually or heuristically to their respective translations. Obviously the latter poses a much higher requirement than the former. The following models have been proposed:  X  CL-CNG represents documents by character n -grams (CNG) (Mcnamee and  X  CL-VSM and Eurovoc-based models build a vector space model (Levow et al.  X  CL-ESA exploits the vocabulary correlations of comparable documents  X  CL-ASA is based on statistical machine translation technology (Barro  X  n-Ceden  X  o  X  CL-LSI performs latent semantic indexing (Dumais et al. 1997 ; Littman et al.  X  CL-KCCA performs a kernel canonical correlation analysis (Vinokourov et al. The alternatives imply a trade-off between retrieval quality and retrieval speed. Also, the availability of necessary resources for all considered languages is a concern. CL-CNG can be straightforwardly operationalized and requires only little language-specific adjustments, e.g., alphabet normalization by removal of diacritics. The CL-VSM variants offer a retrieval speed comparable to that of the VSM in mono-lingual information retrieval, but the availability of handmade translation dictionaries depends on the frequency of translations between the respective languages. Moreover, this model requires significant efforts with respect to disambiguation and domain-specific term translations (Ballesteros 2001 ; Steinberger et al. 2004 ). CL-LSI and CL-KCCA are reported to achieve a high retrieval quality, but their runtime behavior disqualifies them for many practical applications: at the heart of both models is a singular value decomposition of a term-document matrix which has cubic runtime. This is why we chose to compare CL-CNG, CL-ESA, and CL-ASA. All of them are reported to provide a reasonable retrieval quality, they require no manual fine-tuning, pretty few cross-language resources, and they can be scaled to work in a real-world setting. A comparison of these models is also interesting since they operationalize different paradigms for cross-language similarity assessment.
 4.2 Cross-language character n -Gram model (CL-CNG) Character n -grams for cross-language information retrieval achieve a remarkable performance in keyword retrieval for languages with syntactical similarities (Mcnamee and Mayfield 2004 ). We expect that this approach extends to measuring the cross-language document similarity between such languages as well. Given a pre-defined alphabet R and an n [ [1, 5], a document d is represented as a vector d whose dimension is in O (| R | n ). Obviously d is sparse, since only a fraction of the possible n -grams occur in any d . In analogy to the VSM, the elements in d can be weighted according to a standard weighting scheme, and two documents d and d 0 can be compared with a standard measure u ( d, d 0 ). Here we choose R = { a , ... , z ,0, ... , 9}, n = 3, tf idf -weighting, and the cosine similarity as u . In the following we refer to this model variant as CL-C3G. 4.3 Cross-language explicit semantic analysis (CL-ESA) The CL-ESA model is an extension of the explicit semantic analysis model (Gabrilovich and Markovitch 2007 ; Potthast et al. 2008 ; Yang et al. 1998 ). ESA is a collection-relative retrieval model, which means that a document d is represented by its similarities to the documents of a so-called index collection D I . These similarities in turn are computed with a monolingual retrieval model such as the VSM (Stein and Anderka 2009 ): where A T D _ I denotes the matrix transpose of the term-document matrix of the documents in D I , and d VSM denotes the term vector representation of d . Again, various term weighting schemes are applicable in this connection.

If a second index collection D 0 I in another language is given such that the documents in D 0 I have a topical one-to-one correspondence to the documents in D I , the ESA representations in both languages become comparable. I.e., the cross-illustrates this principle for two languages. CL-ESA naturally extends to multiple languages; moreover, the approach gets by without translation technology, be it dictionary-based or other. The model requires merely a comparable corpus of documents written in different languages about similar topics. These documents may still be written independently of each other. An example for such a corpus is the Wikipedia encyclopedia where numerous concepts are covered in many languages. 4.4 Cross-language alignment-based similarity analysis (CL-ASA) The CL-ASA model is based on statistical machine translation technology; it combines a two-step probabilistic translation and similarity analysis (Barro  X  n-Ceden  X  o et al. 2008 ). Given d q , written in L , and a document d 0 from a collection D 0 written in L 0 , the model estimates the probability that d 0 is a translation of d q according to Bayes X  rule: p ( d q ) does not depend on d 0 and hence is neglected. From a machine translation viewpoint pd q d 0 j is known as translation model probability ; it is computed using a statistical bilingual dictionary. p ( d 0 ) is known as language model probability ;it describes the target language L 0 in order to obtain grammatically acceptable text in the translation (Brown et al. 1993 ).

Our concern is the retrieval of possible translations of d q written in L 0 (and not translating d q into L 0 ), and against this background we propose adaptations for the two sub-models: (1) the adapted translation model is a non-probabilistic measure depends on document lengths instead of language structures. Based on these adaptations we define the following similarity measure: Unlike other similarity measures this one is not normalized; note that the partial order induced among documents resembles the order of other similarity measures. The following subsections describe the adapted translation model w  X  d q j d 0  X  and the length model .  X  d 0  X  . 4.4.1 Translation model The translation model requires a statistical bilingual dictionary. Given the vocab-ularies of the corresponding languages X2 L and Y2 L 0 , the bilingual dictionary provides estimates of the translation probabilities p ( x , y ) for every x 2X and y 2Y . This distribution expresses the probability for a word x to be a valid translation of a word y . The bilingual dictionary is estimated by means of the well-known IBM M1 alignment model (Brown et al. 1993 ; Och and Ney 2003 ), which has been successfully applied in monolingual and cross-lingual information retrieval tasks (Berger and Lafferty 1999 ; Pinto et al. 2007 ). In order to generate a bilingual dictionary, M1 requires a sentence-aligned parallel corpus. 1 The translation probability of two texts d and d 0 is originally defined as: where p ( x , y ) is the probability that the word x is a translation of the word y . The model was demonstrated to generate good sentence translations, but since we are considering entire documents of variable lengths, the formula is adapted as follows: The weight wdd 0 j  X  X  increases if valid translations ( x , y ) appear in the implied e = 0.1. 4.4.2 Length model Though it is unlikely to find a pair of translated documents d and d 0 such that factor for each language pair. In accordance with Pouliquen et al. ( 2003b ) we define the length model probability as follows: where l and r are the average and the standard deviation of the character lengths between translations of documents from L to L 0 . Observe that in cases where a translation d 0 of a document d q has not the expected length, the similarity u ( d q , d 0 )is reduced.
Table 1 lists the values for l and r that are used in the evaluation for the considered language pairs; these values have been estimated using the JRC-Acquis training collection. The variation of the length between a document d q and its translation d 0 approximates a normal distribution (cf. Fig. 6 for an illustration). 5 Evaluation of retrieval models for the detailed analysis In our evaluation we compare CL-C3G, CL-ESA, and CL-ASA in a ranking task.
 Three experiments are conducted on two test collections with each model and over all language pairs whose first language is English and whose second language is one of Spanish, German, French, Dutch, and Polish. In total, more than 100 million similarities are computed with each model. 5.1 Corpora for model training and evaluation To train the retrieval models and to test their performance we extracted large collections from the parallel corpus JRC-Acquis and the comparable corpus Wikipedia. The JRC-Acquis Multilingual Parallel Corpus comprises legal docu-ments from the European Union which have been translated and aligned with respect to 22 languages (Steinberger et al. 2006 ). The Wikipedia encyclopedia is considered to be a comparable corpus since it comprises documents from more than 200 languages which are linked across languages in case they describe the same topic (Potthast et al. 2008 ). From these corpora only those documents are considered for which aligned versions exist in all of the aforementioned languages: JRC-Acquis contains 23,564 such documents, and Wikipedia contains 45,984 documents, excluding those articles that are lists of things or which describe a date. 2
The extracted documents from both corpora are divided into a training collection that is used to train the respective retrieval model, and a test collection that is used in the experiments (four collections in total). The JRC-Acquis test collection and the Wikipedia test collection contain 10,000 aligned documents each, and the corresponding training collections contain the remainder. In total, the test collections comprise 120,000 documents: 10,000 documents per corpus 9 2 corpora 9 6 languages. As described above, CL-ESA requires the comparable Wikipedia training collection as index documents, whereas CL-ASA requires the parallel JRC-Acquis training collection to train bilingual dictionaries for all of the considered language pairs. Note that CL-C3G requires no training. 5.2 Experiments and methodology The experiments are based on those of Potthast et al. ( 2008 ): let d q be a query document from a test collection D , let D 0 be the documents aligned with those in D , and let d 0 q denote the document that is aligned with d q . The following experiments have been repeated for 1,000 randomly selected query documents with all three retrieval models on both test collections, averaging the results.
 Experiment 1: Cross-language ranking Given d q , all documents in D 0 are ranked according to their cross-language similarity to d q ; the retrieval rank of d 0 q is recorded. Ideally, d 0 q should be on the first or, at least, on one of the top ranks. Experiment 2: Bilingual rank correlation Given a pair of aligned documents d q [ D and d 0 q [ D 0 , the documents from D 0 are ranked twice: (1) with respect to their cross-language similarity to d q using one of the cross-language retrieval models, and, (2) with respect to their monolingual similarity to d 0 q using the vector space model. The top 100 ranks of the two rankings are compared using Spearman X  X  q , a rank correlation coefficient which measures the disagreement and agreement of rankings as a value between -1 and 1. This experiment relates to  X  X  X iagonalization:  X  X  X  monolingual reference ranking is compared to a cross-lingual test ranking.
 Experiment 3: Cross-language similarity distribution This experiment contrasts the similarity distributions of comparable documents and parallel documents. 5.3 Results and discussion Experiment 1: Cross-language ranking This experiment resembles the situation of cross-language plagiarism in which a document (a section) is given and its translation has to be retrieved from a collection of documents (of sections). The results of the experiment are shown in Table 2 as recall-over-rank plots.

Observe that CL-ASA achieves near-perfect performance on the JRC-Acquis test collection, while its performance on the Wikipedia test collection is poor for all language pairs. CL-ESA achieves between a medium and a good performance on both collections, dependent on the language pair, and so does CL-C3G, which outperforms CL-ESA in most cases. With respect to the different language pairings all models vary in their performance, but, with the exception of both CL-ASA and CL-C3G on the English X  X olish portion of JRC-Acquis (bottom right plot), the performance characteristics are the same on all language pairs.
 It follows that CL-ASA has in general a large variance in its performance, while CL-ESA and CL-C3G show a stable performance across the corpora. Remember that JRC-Acquis is a parallel corpus while Wikipedia is a comparable corpus, so that CL-ASA seems to be working much better on  X  X  X xact X  X  translations than on comparable documents. Interestingly, CL-ESA and CL-C3G work better on comparable documents than on translations. An explanation for these findings is that the JRC-Acquis corpus is biased to some extent; it contains only legislative texts from the European Union and hence is pretty homogeneous. In this respect both CL-ESA and CL-C3G appear much less susceptible than CL-ASA, while the latter may perform better when trained on a more diverse parallel corpus. The Polish portion of JRC-Acquis seems to be a problem for both CL-ASA and CL-C3G, but less so for CL-ESA, which shows that the latter can cope with less related languages.
 Experiment 2: Bilingual rank correlation This experiment can be considered as a standard ranking task where documents have to be ranked according to their similarity to a document written in another language. The results of the experiment are reported as averaged rank correlations in Table 3 .

As in Experiment 1, CL-ASA performs good on JRC-Acquis and unsatisfactory on Wikipedia. In contrast to Experiment 1, CL-ESA performs similar to both CL-CNG and CL-ESA on JRC-Acquis with respect to different language pairs, and it outperforms CL-ASA on Wikipedia. Again, unlike in the first experiment, CL-C3G is outperformed by CL-ESA. With respect to the different language pairings all models show weaknesses, e.g., CL-ASA on English X  X olish and, CL-ESA as well as CL-C3G on English X  X panish and English X  X utch. It follows that CL-ESA is more applicable as a general purpose retrieval model than are CL-ASA or CL-C3G, while special care needs to be taken with respect to the involved languages. We argue that the reason for the varying performance is rooted in the varying quality of the employed language-specific indexing pipelines and not in the retrieval models themselves.
 Experiment 3: Cross-language similarity distribution This experiment shall give us an idea about what can be expected from each retrieval model; the experiment cannot directly be used to compare the models or to tell something about their quality. Rather, it tells us something about the range of cross-language similarity values one will measure when using the model, in particular, which values indicate a high similarity and which values indicate a low similarity. The results of the experiment are shown in Table 4 as plots of ratio of similarities-over-similarity intervals.

Observe that the similarity distributions of CL-ASA has been plotted on a different scale than those of CL-ESA and CL-C3G: the top x -axis of the plots shows the range of similarities measured with CL-ASA, the bottom x -axis shows the range of similarities measured with the other models. This is necessary since the similarities computed with CL-ASA are not normalized. It follows that the absolute values measured with the three retrieval models are not important, but the order they induce among the compared documents is. In fact, this holds for each of retrieval models, be it cross-lingual or not. This is also why the similarity values computed with two models cannot be compared to one another: e.g., the similarity distribution of CL-ESA looks  X  X  X etter X  X  than that of CL-C3G because it is more to the right, but in fact, CL-C3G outperforms CL-ESA in Experiment 1. 6 Summary Cross-language plagiarism is an important direction of plagiarism detection research but is still in its infancy. In this paper we pointed out a basic retrieval strategy for this task, including two important subtasks which require special attention: the heuristic multilingual retrieval of potential source candidates for plagiarism from the Web, and the detailed comparison of two documents across languages. With respect to the former, well-known and less well-known state-of-the-art research is reviewed. With respect to the latter, we survey existing retrieval models and describe three of them in detail, namely the cross-language character n -gram model (CL-CNG), the cross-language explicit semantic analysis (CL-ESA) and the cross-language alignment-based similarity analysis (CL-ASA). For these models we report on a large-scale comparative evaluation.

The evaluation covers three experiments with two aligned corpora, the comparable Wikipedia corpus and the parallel JRC-Acquis corpus. In the experiments the models are employed in different tasks related to cross-language ranking in order to determine whether or not they can be used to retrieve documents known to be highly similar across languages. Our findings include that the CL-C3G model and the CL-ESA model are in general better suited for this task, while CL-ASA achieves good results on professional and automatic translations. CL-CNG outperforms CL-ESA and CL-ASA. However, unlike the former, CL-ESA and CL-ASA can also be used on language pairs whose alphabet or syntax are unrelated. References
