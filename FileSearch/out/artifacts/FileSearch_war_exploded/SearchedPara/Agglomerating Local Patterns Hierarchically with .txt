 To increase the relevancy of local patterns discovered from noisy relations, it makes sense to formalize error-tolerance. Our starting point is to address the limitations of state-of-the-art methods for this purpose. Some extractors perform an exhaustive search w.r.t. a declarative specification of error-tolerance. Nevertheless, their computational complex-ity prevents the discovery of large relevant patterns. Alpha is a 3-step method that (1) computes complete collections of closed patterns, possibly error-tolerant ones, from arbitrary n -ary relations, (2) enlarges them by hierarchical agglomer-ation, and (3) selects the relevant agglomerated patterns. Categories and Subject Descriptors: H.2.8 [Database Management] : Database Applications X  Data mining General Terms: Algorithms Keywords: Noise, n -ary relation, clustering, relevancy
Many datasets of practical interest are n -ary relations and we are designing pattern discovery techniques from such 0/1 data where a 1 (resp. 0) value denotes that a given n -tuple does (resp. does not) belong to the relation. This paper deals with discovering relevant local patterns from noisy n -ary relations. The complete approaches extracting exact patterns (e.g., closed sets or formal concepts) are not suited. Indeed, even in the simpler case of binary relations, a faint noise makes the collections of computed patterns become far too large. Moreover thei r patterns are much smaller than what would be found in the same relations deprived of noise. To guarantee extraction completeness while im-proving the relevancy, several definitions of error-tolerant patterns have been proposed (see [5] for a survey). Never-theless, the space and/or time complexities of the related algorithms make them fail in many practical settings. This situation gets even worse when n increases. Therefore, te-dious or even impossible interpretation phases have to be performed by experts. However, they know that agglom-An error-tolerant closed n -set generalizes the closed n -set pattern type (when ( 1 ,..., n )=(0 ,..., 0)), which, itself, encompasses the popular closed itemset (associated with its support) that was often used to mine transactional datasets.
Further details on the considered pattern types are not necessary to understand Alpha . Indeed Alpha neither im-poses any restriction on the number of n -tuples absent from the relation a pattern contain nor on its maximality. Actu-ally, other pattern types capturi ng some relevant regularities may feed Alpha . This will be briefly discussed in Sec. 5.
Forcing the extracted local patterns to be closed discards many smaller patterns that a re directly derivable from a closed one ( C dense being anti-monotonic). Nevertheless, in practical settings, the complete collection of closed patterns remains huge and many of them are very similar to each other. In the particular case of binary relations, [7] theo-retically and empirically shows that the number of frequent itemsets exponentially grows with the level of noise while their sizes exponentially decrease with it. With a relation of a higher arity n , the situation turns out to be dramatic. Indeed, the number of n -tuples a hidden pattern covers ex-ponentially increases with n . As a consequence the noise has an exponentially greater probability to damage some of these n -tuples. Moreover, at a fixed number of false-negative n -tuples, which should be present in the relation but are missing, the number of closed n -sets linearly increases with n ( C closed is a maximality constraint w.r.t. every attribute).
Mining complete collections of error-tolerant patterns in n -ary relations is computatio nally hard. Erro r-tolerance widens the traversed search space because it delays pruning strategies. As a result, using large i values (see Def. 2) for complete error-tolerant closed n -set extractions can be infea-sible on large datasets. In practice, even a faint noise brings any complete extractor to its limits, i.e., the noise altering the  X  X eal X  patterns cannot be entirely compensated while keeping the extraction tractable. Instead of damning the an-alyst to the interpretation of long lists of insufficiently error-tolerant (hence too small and much-overlapping) closed n -sets, Alpha supports an automatic intermediary task be-tween the complete extraction of patterns and the actual in-terpretation. This approach looks trustier than fully heuris-tic ones because the lossy heuristics are delayed as far as possible in the knowledge discovery process.
The intermediary task, introduced in the previous sec-tion, can be compared to an n -dimensional jigsaw puzzle: every piece is a pattern returned during the complete extrac-tion phase and the image to construct is a perfect version of the one given on the box (the dataset), which is, con-trary to classical jigsaw puzzle, altered by some noise. The perfect image must be composed of large (possibly) over-lapping hyper-rectangles (modulo any permutation of the hyper-plans of any dimension) of 1 values  X  X mbedded X  in a 0 valued hyper-space. A pattern clustering approach will help us solve this tough game.

The quality of the constructed image should not be re-duced to how well every piece interlocks with its neighbors. This image should also match the noisy one on the box. To take into account this objective, the global (at every it-eration, a global clustering refines the previous one) and
According to [6],  X  X  local pattern is a data vector serv-ing to describe an anomalously high local density of data points X . In the context of an n -ary relation R ,arelevant cluster X describes an  X  X nomalously high local density X  of n -tuples present in R when, simultaneously , (a) it is apart from the rest of the data ( X  X nomalously X ), i.e., it maximizes its distance with the other clusters (but its ancestors and de-scendants), and (b) it minimizes the proportion of n -tuples absent from R on its worst hyper-plan ( X  X igh local density X ). Both are easily quantified: (a) The minimal distance between a parent cluster and its (b) The intrinsic distance measure of X , d ( X ), is the pro-Both quantities being proportions of n -tuples absent from R , the relevancy of X can now be computed by difference.
Definition 6. Given an n -set X and its parent X  X  Y after hierarchical agglomeration, r ( X ) denotes the relevancy of X :
Though sorted by relevancy, th e list of patterns to inter-pret remains very long and its tail contains poorly relevant clusters. In particular, it contains the initial collection of patterns (leaves of dendrogram). Alpha automatically de-cides where to cut off this tail. It simply reads, by decreasing relevancy order, the list of clusters and considers the leaves they cover. Once every leaf is covered by at least one pre-viously read cluster, Alpha removes the clusters with lower relevancy values. The completeness of the initial extraction is somehow preserved since every pattern of the related col-lection is part of at least one output cluster. Thus, Alpha assumes that every initially extracted closed error-tolerant n -set is a fragment of some relevant local pattern. As a con-sequence, a cluster with a lower relevancy than at least one of its ancestors is not to be kept. Indeed, it must be a fragment of such a larger ancestor. However both a large cluster and one of its sub-clusters are considered relevant if the latter has a greater relevancy: it describes an  X  X nomalously high local density X  of present n -tuples inside another anomalously high, but lower, local density of present n -tuples. Notice also that the actual satisfaction of the assumption made by Al-pha does not matter a lot. If a closed error-tolerant n -set partly covers regions of the dataset out of any relevant local pattern, it receives a high relevancy (being far away from the other clusters) but, being small, it can easily be filtered out by size constraints in a final post-processing step.
A real-life 4-ary relation was successfully mined with Al-pha . Because of space constraints, the results published here are only those obtained on synthetic relations that al-low to quantitatively benchmark the quality. Indeed, the added-value can be achieved (it deals with all the noise). De-spite this setting and even with the greatest tested tolerance to noise (implying very long extraction times) for Fenster alone, Fenster + Alpha significantly outperforms it. It returns far less patterns (see F ig. 2) and almost perfectly re-covers the hidden ones up to a noise level of 0.15 (see Fig. 3).
The size constraints imposed by Fenster to the closed 3-sets feeding Alpha are meant to provide enough fragments to construct relevant agglomerated patterns. Thus, they are set by merely looking at the size of the returned collection. Hence, in our experiments, when the noise level reaches 0.15, these constraints are loosened to  X  X t least three elements per attribute X . Indeed, with a noise level of 0.15, Fenster extracts, in average, only 16.1 closed 3-sets gathering four elements per attribute or more. Clearly, this is insufficient.
Alpha takes, as input, any collection of n -sets (see Def. 1) but a complete collection is trustier. Many such extractors work with binary relations. So me tolerate noise (see [5] for asurvey). Data-Peeler [3] and its generalization [2] re-spectively extract exact and error-tolerant closed n -sets (see Def. 2). CACTUS [4] and Clicks [10] extract subspace clus-ters from any n -ary relations. These local patterns (rather than clusters ) are of the form ( X 1 ,...,X m ) where every X i is a subset of one attribute domain and m  X  n .Thosewith m = n could feed Alpha . They tolerate noise in a far more constrained way than the closed error-tolerant n -sets.
Clustering complete collections of patterns to make them larger and tolerate some noise is not a new idea. Back in 1995, [8] applies it to association rules. However, to the best of our knowledge, the first metrics taking into account re-gions of the data outside the two patterns to agglomerate, were published last year. Indeed, [9] considers the (binary) relation they were extracted from too. Nevertheless, it fa-vors one attribute, whereas Alpha does not. The extension of pattern clustering methods beyond 2D-datasets is recent too. TriCluster [11] discovers complete collections of lo-cal patterns in real-valued tensors (in particular it can ex-tract exact closed 3-sets) and a post-process handles noise by deleting or merging some patterns. The involved distances only are counts of covered 3-tuples, i.e., their values (0 or 1 in the case of a relation) are not taken into consideration.
