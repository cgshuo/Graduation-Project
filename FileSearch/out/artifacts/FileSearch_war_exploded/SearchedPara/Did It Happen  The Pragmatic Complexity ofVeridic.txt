 StanfordUniversity StanfordUniversity StanfordUniversity
Natural language understanding depends heavily on assessing veridicality  X  X hether events pragmatically informed ones. Our annotations are more complex than the lexical assumption thereby providing a nuanced picture o fthe diverse factors that shape veridicality. 1. Introduction
Areader X  X orlistener X  X understandingofanutterancedependsheavilyonassessingthe extent to which the speaker (author) intends to convey that the events described did (or did not) happen. An unadorned declarative like The cancer has spread conveys firm speakercommitment,whereasqualifiedvariantssuchas There are strong indicators that the cancer has spread or The cancer might have spread imbuetheclaimwithuncertainty.We about the relationship between language and reader commitment (Montague 1969;
Barwise 1981; Giannakidou 1994, 1995, 1999, 2001; Zwarts 1995; Asher and Lascarides 2003; Karttunen and Zaenen 2005; Rubin, Liddy, and Kando 2005; Rubin 2007; Saur  X   X  2008).Thecentralgoalofthisarticleistobegintoidentifythelinguisticandcontextual factorsthatshapereaders X  X eridicalityjudgments. 1 (KiparskyandKiparsky1970;Karttunen1973).Onthisview,alexicalitem L is veridical if the meaning of L applied to argument p entails the truth of p . For example, because bothtrueandfalsethingscanbebelieved,oneshouldnotinferdirectlyfrom A believes that S that S istrue,making believe non-veridical.Conversely, realize appearstobeveridi-cal,becauserealizing S entailsthetruthof S .Theprototypicalanti-veridicaloperatoris veridicalityjudgmentscanbefurthersubdividedusingmodalorprobabilisticnotions.
Forexample,although may isnon-veridicalbythebasicclassifications,wemightclassify may S as possible withregardto S . 2 judgments, but they do not tell the whole story, because they neglect the pragmatic enrichmentthatispervasiveinhumancommunication.Inthelexicalview, say canonly beclassifiedas non-veridical (bothtrueandfalsethingscanbesaid),andyet,ifa New York
Times articlecontainedthesentence United Widget said that its chairman resigned ,readers would reliably infer that United Widget X  X  chairman resigned X  X he sentence is, in this context, veridical (at least to some degree) with respect to the event described by the embedded clause, with United Widget said functioning to mark the source of evidence (Simons2007). Cognitive authority ,astermedininformationscience(Rieh2010),playsa crucialroleinhowpeoplejudgetheveridicalityofevents.Here,theprovenanceofthe document(the New York Times )andthesource(UnitedWidget)combinetoreliablylead a reader to infer that the author intended to convey that the event really happened.
Conversely, allege is lexically non-veridical, and yet this only begins to address the complex interplay of world knowledge and lexical meaning that will shape people X  X  inferencesaboutthesentence FBI agents alleged in court documents today that Zazi had ad-pragmatic component, and, in turn, that veridicality should be assessed using infor-mation from the entire sentence as well as from the context. Lexical theories have a communicativepressures.Forexample,thelexicaltheorycantellusthat,asanarrowly semantic fact, X alleges S is non-veridical with regard to S . Where X is a trustworthy sourcefor S -typeinformation,however,wemightfairlyconfidentlyconcludethat S is true. Where X is known to spread disinformation, we might tentatively conclude that
S is false. These pragmatic enrichments move us from uncertainty to some degree of 302 certainty.Suchenrichmentscanbecentraltounderstandinghowalistener(orareader) understandsaspeaker X  X (orauthor X  X )message.
 canfeelreasonablyconfidentaboutthecorelexicalsemanticsofthewordsoftheirlan-guage,thereisnosuchfirmfoundationwhenitcomestothiskindofpragmaticenrich-butjusthowtrustworthyisUnitedWidgetonsuchmatters?Speakersarelikelytovary inwhattheyintendinsuchcases,andlistenersarethusforcedtooperateunderuncer-taintywhenmakingtherequisiteinferences.Lexicaltheoriesallowustoabstractaway fromthesechallenges,butapragmaticallyinformedapproachmustembracethem.
Pustejovsky 2009). Its annotations are  X  X extual-based X : They seek to capture the ways inwhichlexicalmeaningsandlocalsemanticinteractionsdetermineveridicalityjudg-ments.Inordertobetterunderstandtheroleofpragmaticenrichment,wehadalarge group of linguistically naive annotators annotate a portion of the FactBank corpus, given very loose guidelines. Whereas the FactBank annotators were explicitly told to avoidbringingworldknowledgetobearonthetask,ourannotatorswereencouraged tochooselabelsthatreflectedtheirownnaturalreadingofthetexts.Eachsentencewas annotatedby10annotators,whichincreasesourconfidenceinthemandalsohighlights thesortofvaguenessandambiguitythatcanaffectveridicality.Thesenewannotations helpconfirmourhypothesisthatveridicalityjudgmentsareshapedbyavarietyofother linguisticandcontextualfactorsbeyondlexicalmeanings.
 a range of natural language processing (NLP) tasks, including information extrac-tion, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al.2007; Morante and Daelemans 2009) andhedges or  X  X peculations X  (Szarvasetal.2008;Kimetal.2009)iscrucialtopropertextualunderstanding.Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010),wherethegoalwastodistinguishuncertaineventsfromtherest.Thecentrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the vast majority of information extraction systemsworkatroughlytheclauselevelandregardanyrelationtheyfindastrue.But relationsinactualtextmaynotbefactsforallsortsofreasons,suchasbeingembedded underanattitudeverblike doubt ,beingtheantecedentofaconditional,orbeingpartof thereportbyanuntrustworthysource. Toavoidwrongextractionsinthesecases,itis essentialforNLPsystemstoassesstheveridicalityofextractedfacts.
 foremost,weaimtoshowthatpragmaticallyinformedveridicalityjudgmentsaresys-tematic enough to be included in computational work on textual understanding. Sec-ond,weseektojustifyFactBank X  X seven-pointcategorizationoversimpleralternatives (e.g., certain vs. uncertain, as in the CoNLL task). Finally, the inherent uncertainty of pragmaticinferencesuggeststousthatveridicalityjudgmentsarenotalwayscategori-cal,andthusarebettermodeledasprobabilitydistributionsoververidicalitycategories.
To substantiate these claims, we analyze in detail the annotations we collected, and we report on experiments that treat veridicality as a distribution-prediction task. Our feature set includes not only lexical items like hedges, modals, and negations, but alsocomplexstructuralfeaturesandapproximationsofworldknowledge.Thoughthe resulting classifier has limited ability to assess veridicality in complex real world contexts, it still does a quite good job of capturing human pragmatic judgments of veridicality.Wearguethatthemodelyieldsinsightsintothecomplexpragmaticfactors thatshapereaders X  X eridicalityjudgments. 2. Corpus Annotation
FactBank X  X annotationsareintendedtoisolatesemanticeffectsfrompragmaticonesin theareaofveridicalityassessment.Ouroverarchinggoalistoexaminehowpragmatic enrichmentaffectsthispicture.Thus,weusetheFactBanksentencesinourowninves-tigation,tofacilitatecomparisonsbetweenthetwokindsofinformationandtocreatea supplementofFactBankitself.ThissectionintroducesFactBankinmoredetailandthen thoroughlyreviewsourownannotationprojectanditsresults. 2.1 FactBank Corpus
FactBank provides veridicality annotations on events relative to each participant in-volved in the discourse. It consists of 208 documents from newswire and broadcast news reports in which 9,472 event descriptions (verbs, nouns, and adjectives) were manually identified. There is no fundamental difference in the way verbs, nouns, and adjectives are annotated. Events are single words. The data come from TimeBank 1.2 andafragmentofAQUAINTTimeML(Pustejovskyetal.2006).Thedocumentsinthe AQUAINT TimeML subset come from two topics:  X  X ATO, Poland, Czech Republic, Hungary X  X nd X  X heSlepianabortionmurder. X  anyonementionedinthesentenceaswellasitsauthor.InExample(1),thetargetevent identified by the word means is assigned a value with respect to both the source some experts andtheauthorofthesentence.
 Example 1
Some experts now predict Anheuser X  X  entry into the fray means near-term earnings troubleforalltheindustryplayers.
 [CT], probable [PR], possible [PS], underspecified [U]) and a polarity value (positive [+],negative[  X  ],unknown[u]).CT+correspondstothestandardnotionofveridicality,
CT  X  to anti-veridicality, and Uu to non-veridicality. The PR and PS categories add a modalorprobabilisticelementtothescale,tocapturefiner-grainedintuitions. Example 2
But an all-out bidding war between the world X  X  top auto giants for Britain X  X  leading luxury-carmakerseemsunlikely.
 304 Example 3 Recently, analysts have said Sun also is vulnerable to competition from International
BusinessMachinesCorp.,whichplanstointroduceagroupofworkstationsearlynext year,andNextInc.
 heavilyskewed,with62%oftheeventsfallingtothepositivesideand57.6%intheCT+ categoryalone.Theinter-annotatoragreementforassigningveridicalitytagswashigh splitisdefinedinFactBank:ThedocumentsfromTimeBank1.2areusedasthetraining dataandtheonesfromthesubsetoftheAQUAINTTimeMLcorpusasthetestbed.
 aspossible;thegoaloftheprojectwasto X  X ocusonidentifyingwhatarethejudgments thattherelevantparticipantsmakeaboutthefactualitynatureofevents,independently from their intentions and beliefs, and exclusively based on the linguistic expressions employedinthetexttoexpresssuchjudgments, X  X isregarding X  X xternalfactorssuchas by lexical theories of veridicality. The resulting annotations are  X  X extual-based, that is, reflecting only what is expressed in the text and avoiding any judgment based on individual knowledge X  (Saur  X   X  and Pustejovsky 2009, page 253). In addition, discourse structureisnottakenintoaccount: X  X edecidedtoconstrainourannotationtoinforma-tiononlypresentatthesentencelevel X (Saur  X   X andPustejovsky2009,page253). 2.2 Annotations from the Reader X  X  Perspective
In the terminology of Levinson (1995, 2000), FactBank seeks to capture aspects of sen-tence meaning ,whereasweaimtocaptureaspectsof utterance meaning ,whichbringsus closer to characterizing the amount and kind of information that a reader can reliably extractfromanutterance.WethusextendtheFactBankannotationsbybringingworld knowledge into the picture. Whereas the FactBank annotators were explicitly told to avoidanyreaderbias,todisregardthecredibilityofthesource,andtofocusonlyonthe linguistic terms used in the text to express veridicality, we are interested in capturing howpeoplejudgetheveridicalityofeventswhenreaderbias,credibilityofthesource, andwhatweknowabouttheworldisallowedtoplayarole.
 levelandrecruitedannotatorsforthemusingMechanicalTurk.Werestrictedthetaskto annotatorslocatedintheUnitedStates.Oursubsetconsistsof642sentences(466verbs, 155nouns,21adjectives);weuseallthePR+,PS+,PR  X  ,PS  X  itemsfromtheFactBank training set plus some randomly chosen Uu, CT+, and CT  X  items. (We did not take any CTu sentences into account, as there are not enough of them to support experi-mentation.) The annotators were asked to decide whether they thought the boldfaced eventdescribedinthesentencedid(orwill)happen.Thusthejudgmentsarefromthe FactBankannotations.WeusedSaur  X   X  X  X seven-pointannotationscheme(removingCTu).
To ensure that the workers understood the task, we first gave them four mandatory trainingitems X  X implenon-corpusexamplesdesignedtohelpthemconceptualize the annotation categories properly. The sentences were presented in blocks of 26 items, three of which were  X  X ests X  very similar to the training items, included to ensure that the workers were careful. We discarded data from two Turkers because they did not correctlytagthethreetestsentences. 3 only disconnected sentences and judged the event sentence by sentence. Subsequent mentions of the same event in a discourse can, however, lead a listener to revise a veridicality judgment already posed for that event. For instance in Example (4) from probablychangewhenreadingthesecondsentence.
 Example 4 Yesterday, the police denied that drug dealers were tipped off before the operation.
However, it emerged last night that a reporter from London Weekend Television unwittingly tipped off residentsabouttheraidwhenhephonedcontactsontheestate toaskiftherehadbeenaraid X  X eforeithadactuallyhappened.
 structureforfuturework.Inotherwords,wecapturethereader X  X judgmentaboutthe veridicalityofaneventaftereachsentence,independentonwhetherthejudgmentwill berevisedwhenlaterinformationisread.Thisispartlytofacilitatecomparisonswith
FactBank and partly because we are presently unsure how to computationally model theeffectsofcontextinthisarea.
 event under consideration (the bold sentence in Figure 1), because it is not always rephrasing process as normalization . The normalization strips out any polarity and 306 modality markers to focus only on the core event talked about. For example, in Police gave no details , we needed to make sure that workers evaluated the positive form ( X  X olicegavedetails X ),ratherthanthenegativeone( X  X olicegavenodetails X ).Similarly, will remove the modality marker is expected ( X  X he proposed rights issue will raise about 396 million Canadian dollars net of expenses X ). We followed Saur  X   X  X  X  extensive guidelinesforthisrephrasingprocess(Saur  X   X 2008,pages218 X 222).
 participatedintheannotations.MostTurkersdidjustonebatchof23non-testexamples; themeannumberofannotationsperTurkerwas44,andtheyeachannotatedbetween23 and552sentences.Table2reportsFleisskappascores(Fleiss1971)usingthefullseven-category scheme. These scores are conservative because they do not take into account the fact that the scale is partially ordered, with CT+, PR+, and PS+ forming a  X  X osi-tive X  X ategory, CT  X  ,PR  X  ,andPS  X  forminga X  X egative X  category, andUuremaining alone. The overall Fleiss kappa for this three-category version is much higher (0.66), reflectingthefactthatmanyofthedisagreementswereaboutdegreeofconfidence(e.g.,
CT+ vs. PR+) rather than the basic veridicality judgment of  X  X ositive X ,  X  X egative X , or  X  X nknown X . At least 6 out of 10 Turkers agreed on the same tag for 500 of the 642sentences(78%).For53%oftheexamples,atleast8Turkersagreedwitheachother, andtotalagreementisobtainedfor26%ofthedata(165sentences). 2.3 An Alternative Scale
OneofourgoalsistoassesswhetherFactBank X  X seven-categoryschemeistherightone for the task. To this end, we also evaluated whether a five-tag version would increase agreementandperhapsprovideabettermatchwithreaders X  X ntuitions.Logically,PR is equivalent to PS+, and PS  X  to PR+, so it seemed natural to try to collapse them into a two-way division between  X  X robable X  and  X  X ossible X . We thus ran the MTurk experimentagainwiththefive-pointschemeinTable3.
 items were generally mapped to  X  X o X , and PS  X  to either  X  X o X  or  X  X nknown X . Some
Turkerschosetheexpectedmappings(PS  X  to X  X robable X  X ndPR  X  to X  X ossible X ),but onlyveryrarely.Thisisexplicableintermsofthepragmaticsofveridicalityjudgments.
ThoughPR  X  maybelogicallyequivalenttoPS+,andPS  X  toPR+,thereareimportant pragmatic differences between giving a positive judgment and giving a negative one.
Forexample,inExample(5),speakerBwillnotinferthathecanpossiblygetafurther discount, even if  X  X robably not X  is consistent with  X  X ossibly X . Conversely, had the answerbeen X  X ossibly X ,Awouldhaveremainedhopeful.
 Example 5 orderedscaleproposedbySaur  X   X .Intheirworkonassessingthedegreeofeventcertainty towhichanauthorcommits,Rubin,Liddy,andKando(2005)usedthefollowingfive-inter-annotator agreement (  X  = 0.41). Saur  X   X  hypothesized that their low agreement is due to a fuzzy approach and the lack of precise guidelines. Rubin, Liddy, and Kando had no clear identification of certainty markers, and no explicit test for distinguishing differentdegreesofcertainty(unlikeSaur  X   X ).Inourexperiment,however,theguidelines were similarly loose: Turkers were instructed only to  X  X ead 30 sentences and decide whether the events described in these sentences did (or will) happen. X  They were not askedtolimittheirattentiontojusttheinformationinthesentence,andtheywerenot 308 given any mappings between linguistic markers and veridicality values. Nonetheless,
Turkersreachedgoodagreementlevelsinassessingeventveridicalityonaseven-point scale. We conclude from this that Saur  X   X  X  X  scale comes closer than its competitors to capturingreaderintuitionsaboutveridicality.Thegoodagreementmirrorsthegeneral highinter-annotatoragreementlevelsthathavebeenfoundfortheRecognizingTextual
Entailmenttask(Manning2006),perhapsreflectingthefactthatjudginginferenceand veridicalityincontextisanatural,everydayhumantask.
 beliefs X : whether the author of the sentence indicates with linguistic means that he believesordisbelievesthattheeventdescribedbythesentenceisafact.Thus,inessence, theannotationsassessthedegreeofeventcertaintytowhichanauthorcommits,asin
Rubin X  X  work. They use a three-point scale: committed belief , non-committed belief ,and workers ). Sentences with modals and events embedded under reported verbs are an-expressingdesire( Some wish GM would lay o fworkers ),questions( Many wonder i fGM will (newswire, e-mail, blog, dialogue). The inter-annotator agreement was high (95%).
Prabhakaran,Rambow,andDiab(2010)usedthecorpustoautomaticallytagcommitted difficulttocompareitwithourowntask,fortworeasons.First,theannotatorssoughtto preventworldknowledgefrominfluencingtheirannotations,whichisconcernedonly withlinguisticmarkers.Second,thecategory non-committed belief conflatesthepossible, probable, and unknown categories of our corpus (Saur  X   X  X  X ). Though some work in the biomedicaldomain(i.e.,Hobbyetal.2000)suggeststhatthedistinctionbetweenpossi-bleandprobableishardtomake,wedidnotwanttoavoidit,becausepeopleroutinely make such fine-grained modal distinctions when assessing claims. What X  X  more, the approach we develop allows us to quantify the degree to which such judgments are infactvariableanduncertain. 3. Lessons from the New Annotations
Thissectionpresentstwokindsofhigh-levelanalysisofourannotations.Wefirstcom-parethemwithFactBankannotationsforveridicalityaccordingtotheauthor,identify-ingplaceswheretheannotationspointtosharpdivergencesbetweensentencemeaning andutterancemeaning.Wethenstudythefulldistributionofannotationswereceived (10 per sentence), using them to highlight the uncertainty of veridicality judgments.
BothofthesediscussionsdeeplyinformthemodelingworkofSection4. 3.1 The Impact of Pragmatic Enrichment
Although the MTurk annotations largely agree with those of FactBank, there are sys-tematicdifferencesbetweenthetwothatareindicativeofthewaysinwhichpragmatic enrichment plays a role in assessing veridicality. The goal of this section is to uncover thosedifferences.Tosharpenthepicture,welimitattentiontothesentencesforwhich there is a majority-vote category, that is, at least 6 out of 10 Turkers agreed on the annotation.Thisthresholdwasmetfor500ofthe642examples.  X  X  X  0 . 001 = 0 . 982 annotationsonthis500-sentencesubsetofthedata.WetreatFactBankasoneannotator andourcollectiveTurkersasasecondannotator,withthemajoritylabelthecorrectone forthatannotator.Whatweseeismodesttoveryhighagreementforallthecategories except Uu. The agreement level is also relatively low for CT+. The corresponding confusion matrix in Table 5 helps explicate these numbers. The Uu category is used muchmoreofteninFactBankthanbyTurkers,andthedominantalternativechoicefor the Turkers was CT+. Thus, the low score for Uu also effectively drops the score for CT+. The question is why this contrast exists. In other words, why do Turkers choose CT+whereFactBanksaysUu? embedded under attitude predicates like say , report ,and indicate : any such embedded eventistaggedUuinFactBank.Inourannotations,readersarenotviewingtheveridi-cality of reported events as unknown. Instead they are sensitive to a wide range of syntactic and contextual features, including markers in the embedded clause, expec-tations about the subject as a source for the information conveyed by the embedded clause, and lexical competition between the author X  X  choice of attitude predicate and itsalternatives.Forexample,eventhoughtheeventsinExample(6)areallembedded 310 under an attitude predicate ( say ), the events in Examples (6a) and (6b) are assessed as certain(CT+),whereasthewords highly confident inExample(6c)triggerPR+,and may inExample(6d)leadstoPS+.

Example 6 (a) MagnaInternationalInc. X  X chieffinancialofficer,JamesMcAlpine, (b) Intheair,U.S.AirForceflierssaytheyhave engaged in X  X littlecatand (c) Merieuxofficialssaidlastweekthattheyare X  X ighlyconfident X  X heoffer (d) U.S.commanderssaid5,500Iraqiprisonersweretakeninthefirsthoursof (Ross1973).Asher(2000),Rooryck(2001),andSimons(2007)arguethatsuchconstruc-tions often mark the evidential source for the main-clause information, rather than embeddingitsemantically.Intermsofourannotations,thispredictsthatCT+orCT judgmentswillbecommonforsuchconstructions,becausetheybecomemoreliketwo-partmeanings:themain-clauseandtheevidentialcommentary.
 tencestaggedUuinFactBankwheretheeventisdirectlyembeddedunderanattitude verborintroducedbyaparenthetical.Wealsoremovedexampleswhereamodalaux-iliarymodifiedtheevent,becausethoseareaprominentsourceofnon-CTannotations independently of attitude predication. This yielded a total of 78 sentences. Of these, 33 are parenthetical, and 31 (94%) of those are tagged CT+ or CT 45 non-parenthetical examples, 42 (93%) of those are tagged CT+ or CT parentheticalandnon-parentheticalverbsareaboutequallylikelytoleadtoaCTtag. but it suggests that standard embedding can function pragmatically as an evidential as well. This result is expected under the analysis of Simons (2007) (see also Frazier and Clifton 2005; Clifton and Frazier 2010). It is also anticipated by Karttunen (1973), who focuses on the question of whether attitude verbs are plugs for presuppositions, that is, whether presuppositions introduced in their complements are interpreted as semanticallyembedded.Hereviewsevidencesuggestingthattheseverbscanbeveridi-cal with respect to such content, but he tentatively concludes that these are purely pragmatic effects, writing,  X  X e do not seem to have any alternative except to classify all propositional attitude verbs as plugs, although I am still not convinced that this is therightapproach X (page190).Theevidenceofthepresentarticleleadsustoagreewith
Karttunenaboutthisbasiclexicalistclassification,withthemajorcaveatthattheutter-ancemeaningsinvolvedareconsiderablymorecomplex.(Foradditionaldiscussionof thispoint,seeZaenen,Karttunen,andCrouch[2005]andManning[2006].) case of Uu. As in FactBank, antecedents of conditionals (7), generic sentences (8), and clearcasesofuncertaintywithrespecttothefuture(9)weretaggedUubyamajorityof Turkers.

Example 7 (a) Iftheheavyoutflows continue ,fundmanagerswillfaceincreasing (b) AunitofDPCAcquisitionPartnerssaiditwouldseektoliquidatethe Example 8
Whenpricesaretumbling,theymustbe willing tobuysharesfromsellerswhennoone elsewill.
 312
Example 9 (a) Theprogramalsocallsfor coordination ofeconomicreformsandjoint (b) Butweakcarsalesraisequestionsaboutfuture demand fromthe gories for PS and PR events. In FactBank, markers of possibility or probability, such contrast,theTurkersallowthebiascreatedbytheselexicalitemstobeswayedbyother factors.Forexample,theauxiliary could cantriggerapossibleoranunknownevent(10). InFactBank,allsuchsentencesaremarkedPS+.

Example 10 (a) Theyaren X  X beingallowedtoleaveandcould become hostages.
 (b) Iraqcouldstart hostilities withIsraeleitherthroughadirectattack uniquelysoinFactBank,however,ourannotationsshowmuchshiftingtoPS.Examples (11) and (12) highlight the contrast: It seems likely that the annotators simply have differentoverallexpectationsabouttheforecastingdescribedineachexample,ahigh-levelpragmaticinfluencethatdoesnotattachtoanyparticularlexicalitem.

Example 11 (a) Bigpersonalcomputermakersaredeveloping486-basedmachines,which (b) Beneaththetepidnews-releasejargonliesapowerfulthreatfromthe
Example 12 (a) Despitethelackofanyobvioussuccessors,theIraqileader X  X internal (b) Saddamappearedto accept aborderdemarcationtreatyhehadrejectedin CT+bytheTurkersbutasCT  X  orPR  X  inFactBank.

Example 13 (a) However,itsequityinthenetincomeofNationalSteeldeclinedto the event involves a change of state (from orders existing to not existing). Saur  X   X and
Pustejovsky(2009,page260)notethatnouneventswereafrequentsourceofdisagree-mentbetweenthetwoannotatorsbecausetheannotationguidelinesdidnotaddressat allhowtodealwiththem. 3. 2The Uncertainty of Pragmatic Enrichment
ForthepurposeofcomparingourannotationswiththoseofFactBank,itisusefultosin-gleouttheTurkers X  X ajority-choicecategory,aswedidhere.Wehave10annotationsfor each event, however, which invites exploration of the full distribution of annotations, 314 to see if the areas of stability and variation can teach us something about the nature ofspeakers X  X eridicalityjudgments.Inthissection,weundertakesuchanexploration, arguingthatthepatternsrevealveridicalityjudgmentstobeimportantlyprobabilistic, asonewouldexpectfromatrulypragmaticphenomenon.
 tencesreceived.Thelabelsonthe y -axischaracterizetypesofdistribution.Forexample, 5/5 groups the sentences for which the annotators were evenly split between two categories (e.g., a sentence for which 5 Turkers assigned PR+ and 5 assigned PS+, or a sentence for which 5 Turkers chose PR+ and 5 chose Uu). The largest grouping, 10, poolstheexamplesonwhichalltheannotatorswereinagreement.
 noisiness of the crowd-sourced annotation process. Some annotators might have been inattentiveorconfused,orsimplylackedtheexpertisetomakethesejudgments(Snow et al. 2008). For example, the well-represented 1/9 and 1/1/8 groups probably rep-resent examples for which veridicality assessment is straightforward but one or two of the annotators did not do a good job. If all the distributions were this skewed, we might feel secure in treating veridicality as categorical. There are many examples for which it seems implausible to say that the variation is due to noise, however.
For example, 5/5 groups include sentences like Examples (14) and (15), for which the judgmentsdependheavilyonone X  X priorassumptionsabouttheentitiesandconcepts involved.
 Example 14
In a statement, the White House said it would do  X  X hatever is necessary X  to ensure compliance withthesanctions.
 Example 15
Diplomacy appears to be making headway in resolving the United Nations X  standoff withIraq.
 biningalloftherowswheretwocategoriesreceivedatleast3votes,weget162exam-ples, which is 25% of the total data set. Thus, a non-negligible subset of our sentences seem toinvolve examples where readers X  responses are divided, suggesting that there isnouniquecorrectlabelforthem.
 of the graph in Figure 2 is owed in large part to the fact that veridicality judgments are often not reachable with confidence, because the utterance is inherently under-specified or because additional contextual information is needed in order to be sure.
This, too, suggests to us that it would be foolhardy to assign a unique veridicality label to every example. Of course, situating the sentences in context would reduce some ofthisuncertainty, butnoamount ofbackground information couldeliminateit entirely.
 the idea that veridicality is graded and variable. One of the most striking patterns concerns the question of whether the annotators enriched an example at all, in the following sense. Consider an event that is semantically non-veridical. This could be simply because it is embedded under a non-factive attitude predicate ( say , allege ), or cases is to pick Uu. Depending on the amount and nature of the contextual infor-mation brought to bear on the assessment, however, one might enrich this into one of the positive or negative categories. A cautious positive enrichment would be PS+, forexample.
 inwhichoneofthechosencategories isUu,toseewhattheotherchoices arelike.On theenrichmenthypothesis,theotherchoicesshouldbeuniformlypositiveornegative (uptosomenoise).Figure3summarizesthesentencesinourcorpusthatresultinthis kind of split. The y -axis represents the percentage of non-Uu tags, with the positive values (CT+, PR+, PS+) extending upwards and the negative ones extending down-wards. For sentences 1 X 5 and 18 X 47 (74% of the total), all of the non-Uu tags were uniform in their basic polarity. What X  X  more, the distributions within the positive and negative portions are highly systematic. In the positive realm, the dominant choice is
PS+, the most tentative positive enrichment, followed by PR+, and then CT+. (In the negativerealm,CT  X  isthemostrepresented,butweareunsurewhetherthissupports any definitive conclusions, given the small number of examples.) Our generalization 316 aboutthesepatternsisthatenrichmentfromasemanticUubaselineissystematicand common, though with interesting variation both in whether it occurs and, if it does, howmuch.
 rangeofeffectsthatspecificlexicalitemscanhaveonveridicalityassessment.Toillus-trate,wefocusonthemodalauxiliaryverbs can , could , may , might , must , will , would . keepingwithlexicalisttheories,whentheyareclausematetoanevent,thateventisoften tagged with one of the PR and PS tags. The relationship is a loose one, however; the modalseemstosteerpeopleintotheseweakercategoriesbutdoesnotdeterminetheir finaljudgment.WeillustrateinExample(16)withexamplesinvolving may inpositive contexts.

Example 16 (a) LastFriday X  X announcementwasthefirstofficialwordthattheprojectwas (b) Inaletter,prosecutorstoldMr.Antar X  X lawyersthatbecauseoftherecent (c) Theprospectusdidn X  X includemanydetailsaboutthestudioandtheme we restrict attention to event descriptions that are clausemate to a modal, effectively taking each modal to be annotated with the distribution of annotations for its clause-mateevent.Wealsolookonlyatthepositivetags,becausethenegativeonesweretoo infrequenttoprovidereliableestimates.
 (Wierzbicka1987;S X bo2001;vonFintelandIatridou2008;Finlay2009).Eachtypehas differentdistributionprofiles.Asexpected,theweakpossibilitymodals can , could , may , and might correlate strongly with PS. The other categories are also well-represented forthesemodals,however,indicatingthatthecontributionofthesemarkersisheavily influenced by other factors. The strong (or necessity) modals must , will ,and would are muchmoreevenlydistributedacrossthecategories.
 generally. We do not have enough data to present a quantitative picture for items like potentially , apparently ,and partly ,butthefollowingsentencessuggestthattheyareevery bitasnuancedintheircontributionstoveridicality.
 318
Example 17 (a) Anheuser-BuschCos.saiditplanstoaggressivelydiscountitsmajor (b) TheportfoliounitoftheFrenchbankgroupCreditLyonnaistoldstock (c) Nonetheless,concernaboutthechipmayhavebeenresponsible embracevariationanduncertaintyaspartofthecharacterizationofveridicality,rather than trying to approximate the problem as one of basic categorization. We now turn toexperimentswithasystemforveridicalityassessmentthatacknowledgesthemulti-valuednatureofveridicality. 4. A System for Veridicality Assessment Inthissection,wedescribeamaximumentropyclassifier(Berger,DellaPietra,andDella
Pietra1996)thatwebuilttoautomaticallyassignveridicality.Forclassificationtasks,the dominant tradition within computational linguistics has been to adjudicate differing human judgments and to assign a single class for each item in the training data. In
Section 3.2, however, we reviewed the evidence in our annotations that veridicality is notnecessarilycategorical, byvirtueoftheuncertainty involvedinmakingpragmatic judgmentsofthissort.Inordertoalignwithourtheoreticalconceptionoftheproblemas probabilistic,wetreateachannotatorjudgmentasatrainingitem.Thus,eachsentence appears10timesinourtrainingdata. d asfollows: where, for us, the features f i are indicator functions of a property  X  of the data d and a particular class c : f i ( c , d )  X   X  ( d )  X  c = c parametersofthemodelchosentomaximizetheconditionallikelihoodofthetraining data according to the model. The maximum entropy model thus gives us a distribu-tion over the veridicality classes, which will be our output. To assess how good the output of the model is, we will give the log-likelihood of some data according to the model. For comparison, we will also give the log-likelihood for the exact distribution from the Turkers (which thus gives an upper-bound) as well as a log-likelihood for a baseline model which uses only the overall distribution of classes in the training data.
 logitlinkfunction.Itisalmostexactlyequivalenttothestandardmulti-class(alsocalled polytomousormultinomial)logisticregressionmodelfromstatistics,andreadersmore familiarwiththispresentationcanthinkofitassuch.Inallourexperiments,weusethe
StanfordClassifier(ManningandKlein2003)withaGaussianprior(alsoknownas L regularization)setto N (0,1). 6 4.1 Features Thefeatureswereselectedthrough10-foldcross-validationonthetrainingset.
Predicate classes. Saur  X   X (2008)definesclassesofpredicates(nounsandverbs)thatproject the same veridicality value onto the events they introduce. The classes also define the grammatical relations that need to hold between the predicate and the event it introduces, because grammatical contexts matter for veridicality. Different veridicality valueswillindeedbeassignedto X in He doesn X  X  know that X andin He doesn X  X  know i fX .
Theclasseshavenameslike ANNOUNCE , CONFIRM , CONJECTURE ,and SAY .LikeSaur  X   X , weuseddependencygraphsproducedbytheStanfordparser(KleinandManning2003; deMarneffe,MacCartney,andManning2006)tofollowthepathfromthetargetevent totherootofthesentence.Ifapredicateinthepathwascontainedinoneoftheclasses andthegrammaticalrelationmatched,weaddedboththelemmaofthepredicateasa featureandafeaturemarkingthepredicateclass. 320
World knowledge. Foreachverbfoundinthepathandcontainedinthepredicateclasses, we also added the lemma of its subject, and whether or not the verb was negated.
Our rationale for including the subject is that, as we saw in Section 3, readers X  inter-pretations differ for sentences such as The FBI said it received ... and Bush said he re-ceived ... ,presumablybecauseofworldknowledgetheybringtobearonthejudgment.
Toapproximatesuchworldknowledge,wealsoobtainedsubject X  X erbbigramandsub-jectcountsfromthe New York Times portionofGigaWordandthenincludedlog(subject X  verb-counts / subject-counts) as a feature. The intuition here is that some embedded clauses carry the main point of the sentence (Frazier and Clifton 2005; Simons 2007;
Clifton and Frazier 2010), with the overall frequency of the elements introducing the embeddedclausecontributingtoreaders X  X eridicalityassessments.

General features. Weusedthelemmaoftheevent,thelemmaoftherootofthesentence, theincominggrammaticalrelationtotheevent,andageneralclassfeature.

Modality features. We used Saur  X   X  X  X  list of modal words as features. We distinguished between modality markers found as direct governors or children of the event under consideration,andmodalwordsfoundelsewhereinthecontextofthesentence.Figure4 providessomeindicationofhowthesewillrelatetoourannotations.

Negation. A negation feature captures the presence of linguistic markers of negative contexts. Events are considered negated if they have a negation dependency in the graph or an explicit linguistic marker of negation as dependent (e.g., simple negation ( not ),downward-monotonequantifiers( no , any ),orrestrictingprepositions).Eventsare alsoconsiderednegatedifembeddedinanegativecontext(e.g., fail , cancel ).
Conditional. Antecedents of conditionals and words clearly marking uncertainty are reliable indicators of the Uu category. We therefore checked for events in an if -clause orembeddedundermarkerssuchas call for .

Quotation. Another reliable indicator of the Uu category is quotation. We generated a quotationfeatureifthesentenceopenedandendedwithquotationmarks,oriftheroot subjectwas we .
 to determine veridicality. To be sure, lexical features are important, but they must be allowed to interact with pragmatic ones. In addition, the model does not presume thatindividuallexicalitemswillcontributeinonlyonewaytoveridicalityjudgments.
Rather,theircontributionsareaffectedbytherestofthefeatureset. 4. 2Test Data
As test set, we used 130 sentences from the test items in FactBank. We took all the sentenceswitheventsannotatedPR+andPS+attheauthorlevel(thereareveryfew), andwerandomlychosesentencesfortheothervalues(CT+,CT  X  ,andUu,becausethe
FactBanktestsetdoesnotcontainanyPR  X  andPS  X  items).Threecolleaguesprovided thenormalizationsofthesentencesfollowingSaur  X   X  X  X guidelines,andthedatawerethen annotatedusingMechanicalTurk,asdescribedinSection2.For112ofthe130sentences, atleastsixTurkersagreedonthesamevalue. 5. Results withtheupperandlowerbounds.Theupperboundisthelog-likelihoodofthemodel thatusestheexactdistributionfromtheTurkers.Thelowerboundisthelog-likelihood ofamodelthatusesonlytheoverallrateofeachclassinourannotationsforthetraining data.
 oftheclassifier.TheKLdivergencebetweentwodistributionsisanasymmetricmeasure ofthedifferencebetweenthem.WeuseExample(6d)toillustrate.Forthatsentence,the classifierassignsaprobabilityof0.64toPS+and0.28toPR+,withverylowprobabilities for the remaining categories. It thus closely models the gold distribution (PS+: 7/10,
PR+: 3/10). The KL divergence is correspondingly low: 0.13. The KL divergence for a classifierthatassigned0.94probabilitytothemostfrequentcategory(i.e.,CT+)and0.01 totheremainingcategorieswouldbemuchhigher:5.76.
 0.81(SD0.91)forthetestdata.ThemeanKLdivergenceforthebaselinemodelis1.58 (SD0.57)forthetrainingdataand1.55(SD0.47)forthetestdata.Toassesswhetherour classifier is a statistically significant improvement over the baseline, we use a paired two-sided t-test over the KL divergence values for the two models. The t-test requires that both vectors of values in the comparison have normal distributions. This is not true of the raw KL values, which have approximately gamma distributions, but it is basicallytrueofthelogoftheKLvalues:Forthemodel X  X KLdivergences,thenormality assumption is very good, whereas for the baseline model there is some positive skew.
Nonetheless,thet-testprovidesafairwaytocontextualizeandcomparetheKLvalues ofthetwomodels.Bythistest,ourmodelimprovessignificantlyoverthelowerbound (two-sidedt=  X  11 . 1983,df=129,p-value &lt; 2 . 2 e  X  16).
 amajority vote,thatis,wheresix outoftenannotators agreed onthesamelabel.This allowsustogiveresultsperveridicalitytag.Wetakeasthetrueveridicalityvaluethe oneonwhichtheannotatorsagreed.Thevalueassignedbytheclassifieristheonewith the highest probability. Table 7 reports precision, recall, and F1 scores on the training andtestsets,alongwiththenumberofinstancesineachcategory.Noneoftheitemsin ourtestdataweretaggedwithPR  X  orPS  X  andthesecategorieswereveryinfrequent in the training data, so we leave them out. The table also gives baseline results: We 322 usedaweightedrandomguesser,asforthelower-boundgiveninTable6.Ourresults significantlyexceedthebaseline(McNemar X  X test,p &lt; 0 . 001). markers behave as linguistic theories predict. For example, believe is often a marker of probabilitywhereas could and may aremorelikelytoindicatepossibility.Butasseenin
Examples(10)and(16),worldknowledgeandotherlinguisticfactorsshapetheveridi-cality of these items. The greatest departure from theoretical predictions occurs with the SAY category, which is logically non-veridical but correlates highly with certainty (CT+) in our corpus. 8 Conversely, the class KNOW , which includes know , acknowledge , and learn , is traditionally analyzed as veridical (CT+), but in our data is sometimes a marker of possibility, as we discuss in the Conclusion. Our model thus shows that to accountforhowreadersinterpretsentences,thespaceofveridicalityshouldbecutup differentlythanthelexicalisttheoriespropose. 6. Error Analysis
We focus on two kinds of errors. First, where there is a majority label (a label six or more of the annotators agreed on) in the annotations, we can compare that label with theoneassignedthehighestprobabilityaccordingtoourmodel.Second,wecanstudy cases where the the annotation distribution diverges considerably from our model X  X  distribution(i.e.,caseswithaveryhighKLdivergence).
 wrongly assesses the polarity of only four events, shown in Example (18). Most of the errors are thus in the degree of confidence (e.g., CT+ vs. PR+). The graphs next to the examples compare the gold annotation from the Turkers (the black bars) with the distribution proposed by the classifier (the gray bars). The KL divergence value is includedtohelpconveyhowsuchvaluesrelatetothesedistributions.

Example 18 (a) AddressingaNATOflag-loweringceremonyattheDutchembassy,Orban (b) ButneverbeforehasNATO reached outtoitsformerEastern-bloc (c) Horsley was notadefendantinthesuit,inwhichthePortland,Ore.,jury (d) Atotalof$650,000,meanwhile,isbeingofferedforinformationleadingto asinExample(18b),where(duetoaparseerror) never istreatedasadependentofthe 324 verb have and not of the reaching out event. Similarly, the system could not capture in-stancesinwhichthenegationwasmerelyimplicit,asinExample(18d),wherethenon-veridicalityofthearrestingeventrequiresdeeperinterpretationthatourfeature-setcan manage.

PS+,orUubythesystembecauseofthepresenceofaweakmodalauxiliaryoraverb thatlowerscertainty,suchas believe .AswesawinSection3.2,thesemarkerscorrelate stronglywiththePScategories.

Example 19 (a) TheNATOsummit,shesaid,wouldproducean initiative that X  X esponds (b) Kopp,meanwhile,mayhaveapproachedtheborderwithMexico,butitis (c) TheybelieveKoppwasdriventoMexicobyafemalefriendafterthe are CT+. Some explicit modality markers were not seen in the training data, such as potential in Example (20a), and thus the classifier assigned them no weight. In other cases, such as Example (20b), the system did not capture the modality implicit in the conditional.
Example 20 (a) Albrightalsousedherspeechtoarticulateaforward-lookingvisionfor (b)  X  X ndwemustberesoluteinspellingouttheconsequencesof (c) ButthedecisionbyDistrictAttorneyFrankC.Clarktobeginpresenting (d) ThefirstroundofDNAtestsonthehairattheFBILaboratoryhere 326 ditional. For the other Uu events in Example (21), the system assigned CT+ or PR+.
The majority of Uu events proved to be very difficult to detect automatically since complexpragmaticfactorsareatwork,manyofthemonlyveryindirectlyreflectedin thetexts.

Example 21 (a) Kopp X  X stepmother,whomarriedKopp X  X fatherwhenKoppwasinhis (b) Indeed,oneparticularlyvirulentanti-abortionWebsiteliststhenames betweenourmodel X  X predicteddistributionandtheannotationdistribution.Veryoften, this is simply the result of a divergence between the predicted and actual majority label,asdiscussedearlier.InstanceslikeExample(22)aremoreinterestinginthisregard, however: These are cases where there was no majority label, as in Example (22a), or where the model guessed the correct majority label but failed to capture other aspects ofthedistribution,asinExamples(22b)and(22c).

Example 22 (a) OnTuesday,theNationalAbortionandReproductiveRightsAction (b) Vacco X  X campaignmanager,MattBehrmann,saidinastatementthat (c) Sincethereisnofederalhomicidestatuteassuch,thefederalofficialssaid kind of negation, which leads the system to assign a 0.78 probability to CT ample (22b), there are no features indicating possibility, but a number of SAY -related features are present, which leads to a very strong bias for CT+ (0.86) and a corre-sponding failure to model the rest of the distribution properly. In Example (23c), the classifiercorrectlyassignsmostprobabilitytoPS+,buttherestoftheprobabilitymass is distributed between CT+ and PR+. This is another manifestation of the problem, noted earlier, that we have very few strong indicators of Uu. The exception to that is conditional antecedents. As a result, we do well with cases like Example (23a), where theeventisinaconditional;theclassifierassigns70%oftheprobabilitytoUuand0.15 toPS+. 328
Example 23 (a) OnMonday,SpitzercalledforVaccotorevivethatunitimmediately, missesexplicitlinguisticmarkersofveridicality,butalsobecausecontextualandprag-matic factors cannot be captured. This is instructive, though, and serves to further supportourcentralthesisthatveridicalityjudgmentsarenotpurelylexical,butrather involvecomplexpragmaticreasoning. 7. Conclusion
Our central goal for this article was to explore veridicality judgments at the level of utterance meaning. To do this, we extended FactBank (Saur  X   X  and Pustejovsky 2009) withveridicalityannotationsthatareinformedbycontextandworldknowledge(Sec-tion2).Althoughthetwosetsofannotationsaresimilarinmanyways,theirdifferences highlight areas in which pragmatic factors play a leading role in shaping readers X  judgments(Section3.1).Inaddition,becauseeachoneofoursentenceswasjudgedby10 annotators,weactuallyhaveannotation distributions foroursentences,whichallowsus toidentifyareasofuncertaintyinveridicalityassessment(Section3.2).Thisuncertainty is so pervasive that the problem itself seems better modeled as one of predicting a distributionoververidicalitycategories,ratherthantryingtopredictasinglelabel.The predictivemodelwedeveloped(Section4)istruetothisintuition,becauseittrainson and predicts distributions. All the features of the model, even the basic lexical ones, showtheinfluenceofinteractingpragmaticfactors(Section5).Althoughautomatically assigningveridicalityjudgmentsthatcorrespondtoreaders X  X ntuitionswhenpragmatic factorsareallowedtoplayaroleischallenging,ourclassifiershowsthatitcanbedone effectivelyusingarelativelysimplefeatureset,andweexpectperformancetoimprove aswefindwaystomodelrichercontextualfeatures.

Textual Entailment challenges (Dagan, Glickman, and Magnini 2006), where the goal is to determine, for each pair of sentences T , H , whether T (the text )justifies H (the hypothesis ). The original task definition draws on  X  X ommon-sense X  understanding of language(Chapman2005),andfocusesonhowpeopleinterpretutterancesnaturalisti-cally.Thus,theseentailmentsarenotcalculatedoverjusttheinformationcontainedin thesentencepairs,asamoreclassicallogicalapproachwouldhaveit,butratheroverthe fullutterancemeaning.Asaresult,theyareimbuedwithalltheuncertaintyofutterance meanings(Zaenen,Karttunen,andCrouch2005;Crouch,Karttunen,andZaenen2006;
Manning 2006). This is strongly reminiscent of our distinction between semantic and pragmaticveridicality.Forexample,asapurelysemanticfact, might ( S )isnon-veridical withregardto S .Dependingonthenatureof S ,however,thenatureofthesource,the context, and countless other factors, one might nonetheless infer S .Thisisoneofthe centrallessonsofournewannotations.
 maticstogether,becausewedonotchallengethebasicveridicalitycategorizationsthat comefromlinguisticandlogicalworkonthistopic.Rather,wejustshowedthatthose semanticjudgmentsareoftenenrichedpragmatically X  X orexample,fromuncertaintyto oneofthepositiveornegativecategories,orfromPStoPRorevenCT.Theinteraction betweenlexicalmarkersandpragmaticcontextisalsocrucialinthecaseofabsolutism:
Too many lexical markers conveying certainty might in some cases undermine the speaker X  X  credibility (e.g., in a car salesman pitch) and actually incite mistrust in thehearer/reader.Insuchinstances,lexicalmarkersonlyarenotgoodindicatorsofthe veridicality of the event, but the pragmatic context of the utterance needs to be taken intoaccounttofullyappreciatetheinterpretationpeopleassigntoit.Thereis,however, evidence suggesting that we should be even more radically pragmatic (Searle 1978;
Travis 1996), by dropping the notion that lexical items can be reliably classified once and for all. For example, lexical theories generally agree that know is veridical with respect to its sentential complement, and the vast majority of itsuses seem to support that claim. There are exceptions, though, as in Example (24) (see also Beaver 2010; Hazlett2010):
Example 24 (a) Forthefirsttimeinhistory,theU.S.hasgonetowarwithanAraband (b) Letmetellyousomething,whenitcomestofinishingthefight,Rocky (c)  X  X hatwomanwhoknewI had dyslexia  X  X neverinterviewedher. X  weaker sense than a factive lexical semantics would predict. Example (24c) is the most striking of the group, because it seems to be pragmatically non-veridical: The continuation is Bush X  X  evidence that the referent of that woman could not possibly be in a position to determine whether he is dyslexic. Such examples further emphasize the importance of a pragmatically informed perspective on veridicality in natural language.
 text provenance. Our data did not allow us to examine its impact because we did not haveenoughvariationintheprovenance.AllFactBanksentencesarefromnewspaper and newswire text such as the Wall Street Journal , the Associated Press, and the New
Yo r k Ti m e s . The trustworthiness of the document provenance can affect veridicality judgments, however: People might have different reactions reading a sentence in the
New York Times versusinarandomblogontheWeb.Weplantoexamineandincorporate theroleoftextprovenanceinfuturework. 330 Acknowledgments References 332
