 Structural characterization of a protein through prediction is one of the most challenging problems in computational molecular biology. The prediction process itself involves many lesser but still challenging intermediate problems. Among others, one important problem worth of special attention is the prediction of sec-ondary structures from a g iven amino acid residue sequence. If one knows how a protein is composed of the secondary struct ural elements, their packing ways give an insight into the possible tertiary structures. The secondary structures refer to the regularly repeated folding patterns sustained by hydrogen bonds and conven-tionally grouped into three classes:  X  -helices,  X  -sheets and coils representing all the other structures withou t regularities. The arrangement of twenty amino acids in a primary sequence is not randomly composed, but rather shows some kinds of preferences and correlations between th e residues in the sequence[2]. The rule or principle governing these patterns behind the structure formation has been one of the most classical and frequent resear ch issues in computational molecular bi-ology. This is also the area where machine learning approaches have been suc-cessfully applied to [3]. The main reason why these machine learning approaches have been widely used is partly because the prediction process is independent of any direct information relating to protein conformation [21]. Following the early works based on the statistical analysis ([6]) and information theory([10]), which are categorized as the first generation methods, the advent of neural networks pro-vided a useful blackbox model to estimate a unknown function mapping the input sequences to secondary struct ures. [23] [18]. The neural network architecture pro-posed by Qian and Sejnowski was the fully connected feed-forward network with one hidden layer and a local window of length 13 amino acids. Prediction perfor-mance of the approaches based on single sequences and local windows are limited to about 65% accuracy. Qian and Sejnowsk i also remarked that prediction per-formance between a single layer network and a network with a hidden layer had no considerable difference. This indicates that the input data do not contain the second order correlations among amino acid residues in the window. The authors experimented further with artificial stru ctures of second order and drew a conclu-sion that no second order information was present. In addition to this discouraging observation, other main problems with the second generation approaches include that prediction results were not good enough for practical use [29]. In particular, the prediction accuracy for  X  -sheets was slightly better than a naive guess. When a protein folds into a natural conformation, amino acids that are far away on the primary chain may come into close contact in the tertiary space. This distal rela-tionship can not be captured by a short window, which seems to be a main reason for poor performance in predicting  X  -sheets. This problem can not be addressed by merely considering longer windows of residues because of the over-fitting prob-lem in training neural networks. Theref ore, one needs to devise a different way other than merely increasing the window size when incorporating long range in-teractions into prediction process. The first method that achieved a prediction ac-curacy of over 70% is the PHD prediction system by Rost and Sander [26], which later reached up to a level of 74% [27]. W hile they introduced a number of tech-niques including early stopping and ensemble of modular networks, the break-through in the performance improvement resulted mainly from the use of profiles of the input amino acid sequence derived from the multiple sequence alignment of homologous proteins [2]. As the amino acid sequence profiles contain information pertaining to inter-sequence relationships, one can naturally expect that profile-based approaches outperform the single sequence-based ones. More importantly, as evidenced by the observation that the s pace of possible amino acid sequences is highly constrained by protein structures, profiles contain a certain kind of long-range information in themselves. Howev er, it should be noted that profiles also discard the information pertaining to intra-sequence correlations among amino acid residues [2]. Once the use of profiles made a breakthrough of 70% level for the prediction accuracy, virtually all new prediction methods since then adopted the input profiles. On the other hand, there have been considerable efforts to fur-ther refine the predictor architectures. R iis and Krogh [24] employed the weight-sharing technique to encode the inputs and reduce the number of free parameters. Separate networks are developed for eac h secondary structure class to take into account a priori knowledge of amino acids within each class and the periodicity of  X  -helices. This scheme also employed output filtering and ensemble of single structure networks, the primary goal of which was to avoid the over-fitting prob-lem by a careful design of the network topology. Motivated by the observation that adaptive dynamics and non-causal relations are required to overcome the disadvantages of the local fixed-size window, Baldi et al . ([2]) proposed a bidirec-tional recurrent neural network to predict secondary structures. The PSIPRED method designed by Jones ([14]) used an ensemble of feedforward neural networks and position-specific scoring matrices (P SSM) derived from PSI-BLAST profiles. In spite of considerable volume of efforts in architectural design since the semi-nal PHD method, the prediction performance scored 74% to 77% accuracy in Q 3 measure, which is practically the same level as the PHD ([2]). Other methods that achieved similar performance include NNS P ([30]), PREDATOR ([9]), JPred ([7], [8]), PROF([28]), and SSPro ([22]). Recen tly, a number of novel architectures us-ing the support vector machine (SVM) have been proposed ([13] [20] [35] [34] [12]). Overall, secondary structure prediction systems have been developed along two kinds of elaboration: one is the refinement of architectural design and the other is the introduction of expressive coding schemes to amino acids.

To the best of our knowledge, all exiting methods implicitly assume the effect of neighboring residues to be uniform regardless of their distance from the center residue. Our proposal to improve the prediction performance is motivated by the hypothesis that performance gain can be achieved by imposing different weights on profiles according to the dist ance from the center residue. We have also observed that prediction performance is poor at the locations where a second structure class changes to another class. When a Gaussian weight function is applied to the input profiles, the possible negative effects from the boundary area could be alleviated while the loc al effects from the residues around the center residue is strengthened. We hav e tested the proposed method using the test data consisting of 513 protein seq uences and observed 2 to 5% increase in prediction accuracy. 2.1 Classes of Secondary Structure One of the most widely used methods for secondary structure assignment is the DSSP algorithm developed by Kabsch and Sander [16]. DSSP uses as input the experimentally determined 3D coordinates of the backbone atoms, and assigns secondary structure based on the hydrogen bonding patterns between carboxyl and amino group. In the DSSP classification, second structures are grouped into eight different classes as shown in Table 1.
Conventional way of performance measure is based on the three classes mapped from the eight classes as shown in the right-most column in Table 1. 2.2 Data Set Preparing a test data should be carefully done to include a representative set of data appropriate for the applications. A data set should reflect the underlying distribution of examples in the sample space and exclude any bias or artificial correlations. In the problem of secondary st ructure prediction, according to Rost and Sander [25], this means that a typical test suite is prepared so that no two amino acid sequences in the suite have pairwise identity over 25%. Many standardized test suite containing rep resentative samples have been proposed in the literature in order to facilitate a valid comparison of different methods [2]. The test data used in this work are the set of 126 proteins (23,348 amino acids) of Rost and Sander (referred to as RS126) [25] and the complementary set of 396 proteins (62,189 amino acids)of Cuff and Barton (CB396) [8]. The set RS126 was selected based on the per centage identity as above. However, percent identity has been considered as a poor measure in computing sequence similarity, in particular for the sequences with identity of below 30% [8]. To address this drawback, instead of per cent identity, the SD score can be used, which is given by ( V  X   X  ) / X  . Here, V is the alignment score of two sequences computed by a standard dynamic programming [19],  X  and  X  are the mean and standard deviation of alignment score computed for a set of typically 100 or more randomly re-ordered sequences from ea ch of the two sequences. Accordingly, unlike the percentage identity, SD score reduces the possible effects of bias due to sequence length and amino acid comp ositions. Cuff and Barton selected a non-redundant set (CB396) based on the SD score to remove redundancy of sequences, and constructed a new test set CB513 by adding two sets CB396 and RS126. After removing 16 correlated proteins (details of which are referred to [8]) from CB513, we have prepared a test suite of 497 protein chains. This data set is randomly partitioned into seven subsets in order to evaluate the prediction performance based on cross-validation.
 2.3 Proposed Method: Profiles and Adaptive Weights Using the set of 497 protein chains descr ibed above, we generated profiles follow-ing the steps specified by Cuff and Barton [8]. First, a search using the BLAST program ([1]) is performed over the NR (non-redundant) database ([4]) with standard default parameters (E = 10.0, BLOSUM62 matrix), generating a set of similar sequences. The Smith Waterman dynamic programming algorithm ([31]) is applied to the sequences and those whose score is lower than 10  X  4 are dis-carded. Short sequences are also discarded if their length is less than two-thirds of the target sequence. Sequences longer than 1.5 times the length of the target are truncated by removing the end residues. After the cut-off process, the se-quences are then aligned to the target sequence using the CLUSTALW program [32]. In the multiple sequence alignment process, the target sequence is not al-lowed to contain gaps to preserve its length. A profile is then generated from the aligned sequences by calculating the frequency of occurrences of each residue type at each location along the chain.

When inputs are presented to the predictor program, each amino acid residue i in the input window of length l is encoded as the corresponding profile entry, which is a 21-dimensional vector, Therefore, the input feature vector f has the dimension of 21  X  l , and represented as,
All existing prediction methods incorporating the profile as input do not differ-entiate between short-range interaction and long-distance interaction within the input window. This implies that each f i ( i =1 , 2 ,  X  X  X  ,l ) contributes equal portion to the decision process. Therefore, the vector f =( f 1 , f 2 ,  X  X  X  , f 21 )isidenticalto formance. Instead of this flat model, we propose an adaptive weighting scheme in which the features of the residues proportionally weighted according to their distance from the center resi due. The weighted feature vector g is obtained by imposing weights on feature vectors, where w i is a function of distance d from the center residue. The weight function could be any bell-shaped one such as the 1-st order spline, which was defined in this work as the Gaussian function, where m = 21  X  l 2 , namely the location of the center residue. 2.4 Prediction Scheme: Support Vector Machine Support vector machine (SVM) is a kernel-based machine learning technique that is capable of classification and functional mapping like traditional feedforward networks [33]. As the SVM is strongly founded on mathematical theory and has many interesting properties such as effective avoidance of overfitting and the ability to deal with high-dimensional features[13], it has been superior in a wide range of practical pattern recognition problems. SVM searches for a model which guarantees the lowest error through two stages as shown in Fig 1. At the first step, the input space is transformed by a non-linear mapping so that it maps the data into linear-separable regions. At the second step, the optimal hyperplanes separating classes are calculated, in such a way as to maximize the margin between two closest training samples.

The non-linear mapping is specified in terms of a kernel function K ( x , y ), which is designated in this work by a radial basis function, where  X  is a user-specified parameter . Regularization parameter C controls the trade-offs between margin (model comple xity) and classification error, which is specified by the following optimization problem, where x i is an input vector, y i =  X  1 depending on the binary classification of x , n is the number of training samples. Based on these values, the SVM method determines a separating hyperplane that optimally divides two classes. Because the SVM is inherently a binary classifier, multi-class classification is performed by combining SVMs in the cascading way. Here, choosing the optimal values of  X  and C is essential for each binary SVM classifier [17]. 2.5 Performance Measures After secondary structure prediction i s performed for the target sequence, we assess the performance using t he following statistics. Let a ij denote the number of structure elements of class i which was classified to be of class j .Let c i denote the number of structure elements in class i ,and p j the number of structure elements predicted to be class j , Then, the total number of residues, N , is defined as,
Based on these statistics, the performance measures are defined. The most intuitive measure is Q 3 , which is the percentage of co rrectly classified residues regardless of their types,
Although Q 3 is intuitively clear, merely relying on this measure can be easily misleading, in particular for the cases wh ere class distribution is heavily biased. Class accuracy, Q i is a measure to go around the unbalanced distribution of classes, where class i can have state H, E, or C. Another w idely used measure is Matthew X  X  correlation coefficient, where TP i , FP i , FN i ,and TN i are the number of true positives, false posi-tives, false negatives, and true negatives, respectively, in class i . The Matthew X  X  correlation coefficient is calculated sep arately for each class. The values range from -1 to 1, where 1 refers to comple te agreement of prediction with ground truth, -1 complete disagreement, and 0 un-correlation. This measure is useful for comparing with the random baseline, b ecause random predi ction according to class distribution would give its value close to zero. In secondary structure pre-diction, recognition of secondary structure segment is usually more useful than exact prediction of individual residues. For this purpose, Rost and Sander ([27]) proposed the segment overla p measure (SOV) as follows, where S ( i ) is the set of all overlapping pairs of segments ( s 1 ,s 2 )wherebothseg-is the length where s 1 and s 2 are both in state i ,and maxov ( s 1 ,s 2 ) is the total length spanned by s 1 and s 2 . 3.1 Optimal Parameter Selection As aforementioned, the performance of binary classifiers and eventually that of the overall classification system depends on the values of parameters  X  and C in Eq. (5) and (6). The optimal values for these parameters should be determined according to the performance measure of the final results. Most commonly used performance measures are Q 3 [17] and Matthew X  X  correlation coefficient [5]. The latter has the advantage to give a better indication of the prediction quality because Q 3 tends to favor the large class. The optimal values of  X  range from 0.185 to 0.410, and those of C from 0.86 to 1.62 depending on the window size when computed using the CB513 set for training the SVM networks [5]. However, as small variation of these values does not give considerable influence on the prediction performance, we used fixed values for training all SVM classifiers,  X  =0 . 18 and C =1 . 43. 3.2 Classifier Design and Testing Once the kernel function and optimal values of parameters are determined, six SVM binary classifiers are constructed based on the SVMlight [15]. When design-ing a classifier with SVM, the number of support vectors indicates how complex a problems is [5], i.e., more number of support vectors mea ns more complex the problem is. Table 2 shows the number of support vectors required for each binary classifier.

All experiments were carried out using th e three-fold cross validation method using the CB513 set. The main objective of this work is to investigate the pre-diction improvement when weighted profi les are used instead of normal profiles. Therefore, we have focused on comparing those two schemes under various con-ditions rather than on fine tuning the classifiers to obtain higher accuracies in prediction. Table 3 shows the prediction results for both weighted profiles (bold-faced numbers on upper rows) and normal profiles (on lower rows). One can see that, when the weighted profiles are u sed, consistent in crease in accuracy is achieved for all the SVM classifiers except the one for C/  X  C. This could be explained by the fact that  X  -helices and  X  -sheets are formed as regular structure whereas the coiled regions have no specific structure. Due to this lack of meaning-ful structure, the adaptively weighted profiles, namely emphasizing short-range interactions while slightly suppressing long-distance interactions have no effects on prediction for coiled regions. Consequently, performance gain of the weighted profiles scheme is not consistent, but as shown on the row of C/  X  C, the predic-tion accuracy degrades s lightly for the cases of l =7 , 9, and 15.
The final prediction is performed using the tertiary classifier which is con-structed based on the binary classifiers. Hua and Sun ([13] ) proposed a number of ways to construct tertiary classifiers as a cascade of two binary classifiers. For example, in the SVM TREE1 scheme, patterns first are classified into two classes of  X  -heix and others, then those in the non- X  -helix class are further clas-sified into  X  -sheet and coils. In the same way, SVM TREE2 combines E/  X  E and C/H classifier, and SVM TREE3 is made up of C/  X  C and H/E classifier. The SVM MAX D scheme operates in such a way that a pattern is assigned to the class corresponding to the largest distance from the optimal separating hy-perplane (OSH).Table 4 shows the final prediction results for overall prediction ( Q 3 ), state-wise prediction ( Q H , Q E ,and Q C ), and the results measured using SOV. Here again, the weighted profiles schemes perform better for all classifiers and measures except Q C . We have presented a new method for encoding profiles based on the adaptive weights. The input profiles for amino acid residues are differently weighted ac-cording to their distance from the center residue. Our contribution is that we formulated a non-uniform model in which short-range interactions and long-distance interactions are differentiat ed. Effectiveness of the proposed scheme has been proved by improvement of p rediction accuracies up to 5% in Q 3 on the widely used test data set. Moreover, we have observed that regular struc-ture elements such as  X  -helices and  X  -sheets are considerably sensitive with the weighted profiles, wherea s the coiled region do not show any difference between weighted and normal profiles. For future research, we are going to investigate how the proposed scheme is related to analyzing the boundary of two different structure elements. The shape of the optimal weight function can be another direction of enhancing the current work.
 This work was supported by the Korea Research Foundation Grant funded by the Korean Government(MOEHRD) under the Regional Research Universities Program/Chungbuk BIT Research-Oriented University Consortium.

