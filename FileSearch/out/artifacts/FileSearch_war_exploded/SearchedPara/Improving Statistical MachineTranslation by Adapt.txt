 University of Haifa, Israel University of Haifa, Israel University of Haifa, Israel
Translation models used for statistical machine translation are compiled from parallel corpora that are manually translated. The common assumption is that parallel texts are symmetrical:
The direction of translation is deemed irrelevant and is consequently ignored. Much research language ( translationese ) has many unique properties. It has already been shown that phrase outperform those constructed from corpora translated in the opposite direction. properties of translationese. We explore two adaptation techniques: First, we create a mixture model by interpolating phrase tables trained on texts translated in the  X  X ight X  and the  X  X rong X  directions. The weights for the interpolation are determined by minimizing perplexity. Second, we define entropy-based measures that estimate the correspondence of target-language phrases significant improvement in the quality of the translation. 1. Introduction
Much research in translation studies indicates that translated texts have unique genre, or a dialect, of the target language, which reflects both artifacts of the translation process and traces of the original language from which the texts were translated.
Among the better-known properties of translationese are simplification and explicitation (Blum-Kulka and Levenston 1983; Blum-Kulka 1986; Baker 1993): Translated texts tend more frequently than original texts. Interestingly, translated texts are so markedly different from original ones that automatic classification can identify them with very high accuracy (Baroni and Bernardini 2006; van Halteren 2008; Ilisei et al. 2010; Koppel and Ordan 2011).
 train translation models that reflect source-and target-language phrase correspondences.
Typically, SMT systems ignore the direction of translation of the parallel corpus. Given the unique properties of translationese, which operate asymmetrically from source to of the translation. Recently, Kurokawa, Goutte, and Isabelle (2009) showed that this is indeed the case. They trained a system to translate between French and English (and vice versa) using a French-translated-to-English parallel corpus, and then an English-corpus yields better results (in terms of higher BLEU scores), whereas for translating into English it is better to use the former.

Kurokawa, Goutte, and Isabelle (2009) trained an SVM-based classifier to predict which side of a bi-text is the origin and which one is the translation, and trained a translation model by utilizing only the subset of the corpus that corresponds to the direction of the task.

First, we demonstrate that the other subset of the corpus, reflecting translation in the  X  X rong X  direction, is also important for the translation task, and must not be ignored; allel corpus, whether manually annotated or machine-learned, is not mandatory. This is achieved by casting the problem in the framework of domain adaptation: We use domain-adaptation techniques to direct the SMT system toward producing output that better reflects the properties of translationese. We show that SMT systems adapted to translationese produce better translations than vanilla systems trained on exactly the same resources. We confirm these findings using automatic evaluation metrics, as well as through a qualitative analysis of the results.

Kurokawa, Goutte, and Isabelle (2009) in Section 3. We then (Section 4) explain why translation quality improves when the parallel corpus is translated in the  X  X ight X  di-rection. We do so by showing that the subset of the corpus that was translated in the direction of the translation task (the  X  X ight X  direction, henceforth source-to-target ,or
S  X  T ) yields phrase tables that are better suited for translation of the original language than the subset translated in the reverse direction (the  X  X rong X  direction, henceforth target-to-source ,or T  X  S ). We use several statistical measures that indicate the better quality of the phrase tables in the former case. 1000 are translated both in the  X  X ight X  and in the  X  X rong X  direction, improves the quality by adapting a translation model to the nature of translationese, thereby making the output of machine translation more similar to actual, human translation. Specifically, we create two phrase tables, one for the S  X  T portion of the corpus, and one for the
T  X  S portion, and combine them into a mixture model using perplexity minimization (Sennrich 2012) to set the model weights. We show that this combination significantly outperforms a simple union of the two portions of the parallel corpus.
 parallel corpus can be approximated by defining several entropy-based measures that correlate well with translationese, and, consequently, with translation quality. We use the entire corpus, create a single, unified phrase table, and then use these measures, and in particular cross-entropy , as a clue for selecting phrase pairs from this table. The also eliminates the need to directly predict the direction of translation of the parallel corpus.
 translationese. 1 To demonstrate the contribution of our methodology, we conduct in
Section 6 a thorough analysis of our results, both quantitatively and qualitatively. We show that translations produced by our best-performing system indeed reflect some well-known properties of translationese better than the output of baseline systems.
Furthermore, we provide several examples of SMT outputs that demonstrate in what ways our adapted system generates better results. 2. Related Work translation in the context of SMT. They found that a translation model based on the
S  X  T portion of the parallel corpus results in much better translation quality than a translation model based on the T  X  S portion. We replicate these results here (Section 3), and view them as a baseline. In taking direction into account, we are faced with two major challenges. First, using only the  X  X ight X  portion of the corpus results in discarding proportion between the two portions of the corpus can vary greatly. In the Hansard corpus, used by Kurokawa, Goutte, and Isabelle (2009), only 20% of the corpus is S
We show that the T  X  S portion is also important for machine translation and thus should not be discarded. Using information-theoretic measures, and in particular cross-entropy , we gain statistically significant improvements in translation quality beyond the results of Kurokawa, Goutte, and Isabelle (2009). The second challenge is to overcome the need to (manually or automatically) classify parallel corpora according to direction. We face this challenge by using an adaptation technique.
 translation, focusing on the language model (LM) (Lembersky, Ordan, and Wintner 2011, 2012b). We showed that LMs trained on translated texts yield better translation quality than LMs compiled from original texts. We also showed that perplexity is a good discriminator between original and translated texts. Importantly, we convincingly demonstrated that the differences between translated and original texts are indeed due to effects of translationese, and cannot be attributed to the domain or topic of the texts.
Whereas that work focused on the product of translation , namely, the language model, the current study focuses on the process of translation , to wit, the translation model. domain adaptation scenario, a system is trained on a large corpus of  X  X eneral X  (out-of-domain) training material, with a small portion of in-domain training texts. In our case, the translation model is trained on a large parallel corpus, of which some (generally unknown) subset is  X  X n-domain X  ( S  X  T ), and some other subset is  X  X ut-of-domain X  ( T  X  S ). Most existing adaptation methods focus on selecting in-domain data from a general domain corpus. In particular, perplexity is used to score the sentences in the general-domain corpus according to an in-domain language model. Gao et al. (2002) and
Moore and Lewis (2010) apply this method to language modeling, and Foster, Goutte, and Kuhn (2010) and Axelrod, He, and Gao (2011) apply this method to translation modeling. Moore and Lewis (2010) suggest a slightly different approach, using cross-entropy difference as a ranking function.
 focus on an adaptation of the phrase table used for SMT. In this sense, our work follows
Foster, Goutte, and Kuhn (2010), who weigh out-of-domain phrase pairs according to their relevance to the target domain. They use multiple features that help to distinguish between phrase pairs in the general domain and those in the specific domain. We rely on features that are motivated by the findings of translation studies, having established their relevance through a comparative analysis of the phrase tables. In particular, we use measures such as translation model entropy, inspired by Koehn, Birch, and Steinberger (2009). Additionally, we apply the method suggested by Moore and Lewis (2010) using perplexity ratio instead of cross-entropy difference.

They pass two phrase tables directly to the decoder using multiple decoding paths. As we show in Section 5, the application of this method to our scenario does not result method.
 weights for translation model mixture for domain adaptation. We successfully apply this method to the problem of adapting translation models to translationese, gaining statistically significant improvements in translation quality. 3. Baseline Experiments 3.1 Europarl Experiments
EN) and from English to French (EN-FR). To establish the robustness of our approach, we also conduct experiments with other translation tasks, including German X  X nglish (DE-EN), English X  X erman (EN-DE), Italian X  X nglish (IT-EN), and English X  X talian (EN-
IT). Our corpus is Europarl (Koehn 2005), specifically, portions collected over the years 1996 X 1999 and 2001 X 2009. This is a large multilingual corpus, containing sentences 1002 translated from several European languages. In most cases the corpus is annotated with the original language and the name of the speaker. For each language pair we extract from the multilingual corpus two subsets, corresponding to the original languages in which the sentences were produced. For example, in the case of FR-EN we extract from our corpus all sentences produced in French and translated into English, and all sentences produced in English and translated into French. All sentences are lowercased and tokenized using Moses (Koehn et al. 2007). Sentences longer than 80 words are discarded. Table 1 depicts the size of the subsets whose target language is English.
SMT) systems (Koehn et al. 2007), translating in both directions between the languages in each language pair. In other words, we train two PB-SMTs for each translation task, each based on a parallel corpus produced and translated in a different direction. We use GIZA++ (Och and Ney 2000) with grow-diag-final alignment, and extract phrases (2007), using at most 30 translations per source phrase and discarding singleton phrase pairs.
 struct English, German, French, and Italian 5-gram language models, using interpolated modified Kneser-Ney discounting (Chen 1998) and no cut-off on all n -grams. We use a specific symbol to mark out-of-vocabulary words (OOVs). The OOV rate is low, less collected over the year 2000 for tuning and evaluation. For each translation task we ran-domly extract 1,000 parallel sentences for the tuning set and another set of 5,000 parallel sentences for evaluation. These sentences are originally written in the translation task X  X  source language and are translated into the translation task X  X  target language (in real-world scenarios, the directionality of the test set is typically known). We use the MERT algorithm (Och 2003) for tuning and BLEU (Papineni et al. 2002) as our evaluation metric. We test the statistical significance of the differences between the results using the bootstrap resampling method (Koehn 2004).
 corpus corresponds to the translation task and T  X  S when a corpus is translated in the opposite direction to the translation task. For example, suppose the translation tasks are
English-to-French (E2F) and French-to-English (F2E). We use S original corpus is used for the F2E task or when the English-original corpus is used for the E2F task; and T  X  S when the French-original corpus is used for the E2F task or when the English-original corpus is used for the F2E task.
 the findings of Kurokawa, Goutte, and Isabelle (2009): Systems trained on S texts always outperform systems trained on T  X  S texts. The difference in BLEU score can be as high as 3 points. 3.2 Hansard Experiments The corpora used in the Europarl experiments are small (up to 200,000 sentences).
Also, the ratio between S  X  T and T  X  S materials varies greatly for different language pairs. To mitigate these issues we use the Hansard corpus, containing transcripts of the
Canadian parliament from 1996 X 2007, as another source of parallel data. The Hansard is a bilingual French X  X nglish corpus comprising approximately 80% English-original texts and 20% French-original texts. Crucially, each sentence pair in the corpus is annotated with the direction of translation.
 500K, 750K, 1M, 1.25M, and 1.5M parallel sentences) from each portion (English-original and French-original) of the corpus. Additionally, we use the devtest section of the
Hansard corpus to randomly select French-original and English-original sentences that are used for tuning (1,000 sentences each) and evaluation (5,000 sentences each).
PB-SMT systems using the Moses toolkit (Koehn et al. 2007). We use the same GIZA++ configuration and phrase table pruning as in the Europarl experiments. We also reuse the English and French language models. French-to-English MT systems are tuned and tested on French-original sentences and English-to-French systems on English-original ones.
 with our previous findings: Systems trained on S  X  T parallel texts always outperform 1004 systems trained on T  X  S texts, even when the latter are much larger. For example, a French-to-English SMT system trained on 250,000 S system trained on 1,500,000 T  X  S sentences. 4. Phrase Tables Reflect Facets of Translationese
The baseline results suggest that S  X  T and T  X  S phrase tables differ substantially, presumably due to the different characteristics of original and translated texts. In this respective phrase tables, as defined by a number of statistical measures. We first relate these measures to the unique properties of translationese.

Generally, translated texts are not as rich and variable as original ones, and, in particular, their type/token ratio is lower. Consequently, we expect S translationese) to have more unique source phrases and a lower number of translations per source phrase. A large number of unique source phrases suggests better coverage of the source text, whereas a small number of translations per source phrase means a lower phrase table entropy.
 size of the phrase table in tokens (Total), the number of unique source phrases (Source), and the average number of translations per source phrase (AvgTran), computed on the 24 phrase tables corresponding to our SMT systems. 2 Evidently, S have more unique source phrases, but fewer translation options per source phrase. This holds uniformly for all 24 tables.
 texts are not as rich as original ones; their type-to-token ratio is lower, and the variety of syntactic structures is more limited. S  X  T phrase tables capture correspondences between phrases written in the source language (original) and translated to the target language (translated). Consequently, more different types in the source language corre-spond to fewer types in the target language. For example, in the FR-EN S trained on 1.5M sentences, the French word r  X  eduite (reduced) has 77 translations, whereas in the T  X  S lexicon the same word has 143 translations. Moreover, in the
S  X  T lexicon the probability of the best translation, reduced , is 41.2%, whereas in the
T  X  S lexicon it is only 28.7%.
 based measures. Phrase table entropy captures the amount of uncertainty involved in choosing candidate translation phrases (Koehn, Birch, and Steinberger 2009). Given a source phrase s and a phrase table T with translations t of s whose probabilities are p ( t | s ), the entropy H of s is:
To compute the phrase table entropy, Koehn, Birch, and Steinberger (2009) search through all possible segmentations of the source sentence to find the optimal covering set of test sentences that minimizes the average entropy of the source phrases in the covering set. We refer to this measure as covering set entropy ,or CovEnt . table. This metric finds the minimal covering set of a given text in the source language using source phrases from a particular phrase table, and outputs the average length of a phrase in the covering set. This measure is referred to as covering set average length , or CovLen .
  X  X elatedness X  of a given phrase to original language or to translationese. Motivated by this observation, we design a cross-entropy-based measure that assesses how well each phrase table fits the properties of translationese. We then build language models from translated texts, and compute the cross-entropy of each target phrase in the phrase tables according to these language models.
 already used much of the Hansard data for training the translation model, we use instead an adaptation of an external corpus (Europarl) to the Hansard domain. We 1006 build language models of translated texts as follows. For English translationese, we extract 170,000 French-original sentences from the English portion of Europarl, and 3,000 English-translated-from-French sentences from the Hansard corpus (disjoint from trigram language model with interpolated modified Kneser-Ney discounting and no cut-off. All OOV words are mapped to a special token, unk . Then, we interpolate the Hansard and Europarl language models to minimize the perplexity of the target
For French translationese, we use 270,000 sentences from Europarl and 3,000 sentences from Hansard,  X  = 0 . 81.
 optimal covering set of test sentences that minimizes the weighted cross-entropy of the source phrase in the covering set. Given a phrase table T and a language model L , the weighted cross-entropy W for a source phrase s is: where H ( t , L ) is the cross-entropy of t according to a language model L . the data meet our expectations: S  X  T phrase tables uniformly and unexceptionally have lower entropy and cross-entropy, but higher covering set length.
 properties of translationese than T  X  S ones. But does this necessarily affect the quality of the generated translations? To verify that, we measure the correlation between the quality of the translation, as measured by BLEU (Table 3), with each of the entropy-based metrics. We compute the correlation coefficient R product-moment correlation coefficient) by fitting a simple linear regression model.
Table 6 lists the results; clearly, all three measures are strongly correlated with trans-lation quality. Consequently, we use these measures as indicators of better translations, more similar to translationese. Crucially, these measures are computed directly on the phrase table, and do not require reference translations or meta-information pertaining to the direction of translation of the parallel phrase. 5. Adaptation of the Translation Model to Translationese We have thus established the fact that S  X  T phrase tables have an advantage over
T  X  S ones that stems directly from the different characteristics of original and trans-lated texts. We have also identified three statistical measures that explain most of the variability in translation quality. We now explore ways for taking advantage of the entire parallel corpus, including translations in both directions, in light of these findings. Our goal is to establish the best method to address the issue of different translation direction components in the parallel corpus. 5.1 Baseline
As a simple baseline we take the union of the two subsets of the parallel corpus. This and MERT can be expected to optimize this selection process. For each translation task in Section 3.1, we concatenate the S  X  T and the T corpora and use the union to train an SMT system (henceforth UNION ). We use the same language and reordering models, Moses configuration, and the same tuning and evaluation sets as in Section 3.1. Table 7 reports the results. The UNION systems, which 1008 use twice as much training data as the S  X  T systems, outperform the S for all language pairs except English-to-Italian. Only in three cases out of six (German-to-English, English-to-German, and Italian-to-English), however, is the gain statistically significant. Nevertheless, this indicates that the T  X  S subset contains useful material that can (and does) contribute to translation quality.
 with two phrase tables using multiple decoding paths, and combine them in a log-linear model, following Koehn and Schroeder (2007). The performance of this approach
UNION systems (Table 7). 5.2 Perplexity Minimization
Next, we look at a linear interpolation of the translation models. We need a way to tune the weights of the translation model components, and we use perplexity minimization , following Sennrich (2012).
 i = 1  X  i = 1, where  X  i is the interpolation weight of phrase table i . Then, given a phrase pair ( s , t ), the linear interpolation of the n models is given by: be the observed, empirical probability of the pair ( s , t ) in the development corpus. This is obtained by training a phrase table on the development corpus using the standard methodology; the probability of the pair ( s , t ) is then extracted from the phrase table.
The cross entropy H of a translation model with probabilities p to a development corpus with probabilities  X  p is defined as:
To minimize the cross entropy, we look for a weight vector
Each feature of the standard SMT translation model (the phrase translation proba-bilities p ( t | s )and p ( s | t ), and the lexical weights lex ( t independently.

Ordan, and Wintner (2011) show that perplexity is a good differentiator between original and translated texts; second, the perplexity is minimized with respect to some development set. Consequently, if we use a S  X  T corpus for this purpose, we directly adapt the interpolated phrase table to the qualities of the S tables: we interpolate the S  X  T and the T  X  S models (we refer to this system as
PPLMIN-1 ) and we also interpolate the S  X  T with the UNION models ( PPLMIN-2 ), as a simple way of upweighting. Table 7 reports the results. In all cases, the interpolated systems yield higher BLEU scores than the simple UNION systems. Although the improvements are small (0.2 X 0.4 BLEU points), they are statistically significant in all cases, except for German-English. Clearly, the interpolated systems outperform the
S  X  T systems by 0.2-0.7 BLEU points (statistically significant in all cases). PPLMIN-2 seems to be better than PPLMIN-1 in four out of six systems.
 process rather than a quirk resulting from MERT instability, we use MultEval (Clark et al. 2011). This is a script that takes machine translation hypotheses from several (in our case, three) runs of an optimizer (MERT) and reports three popular metric scores: BLEU,
Meteor (Denkowski and Lavie 2011), and TER (Snover et al. 2006). Meteor and BLEU scores are higher for better translations (  X  ), whereas TER is a lower-is-better measure (
In addition, MultEval computes the ratio of output length to reference length (closer to 100% is better), as well as p-values (via approximate randomization). We use MultEval to compare translation hypotheses of the UNION and PPLMIN-2 systems. Table 8 presents the results for French-to-English and English-to-French (other translation tasks produce similar results). The improvement of the adapted systems is clear and robust. 5.3 Adaptation without Explicit Information on Directionality
A prerequisite for interpolating translation models, the method we advocate here, is that the direction of translation of every sentence pair in the parallel corpus be known in advance. When such information is not available, machine learning can automati-cally classify texts as original or translated (Baroni and Bernardini 2006; van Halteren 1010 2008; Ilisei et al. 2010; Koppel and Ordan 2011). Naturally, however, the quality of the interpolation of translation models trained on classified (rather than annotated) data is expected to decrease. In this section we establish an adaptation technique that does uses perplexity-based measures to evaluate the  X  X elatedness X  of a specific phrase to an original or a translated language  X  X ialect. X  FO (French original) refers to subsets of the parallel corpus that were translated from French to English, EO (English original) refers to texts translated from English to French.
We create three different mixtures of FO and EO: a balanced mix comprising 500K sentences each of FO and EO (MIX), an EO-biased mix with 500K sentences of FO and 1M sentences of EO (MIX-EO), and an FO-biased mix with 1M sentences of FO and 500K sentences of EO (MIX-FO). We use these corpora to train French-to-English and
English-to-French MT systems, evaluating their quality on the evaluation sets described in Section 3.2. We use the same Moses configuration as well as the same language and reordering models as in Section 3.2.
 tables an additional factor, as a measure of its fitness to the genre of translationese. The factors are used as additional features in the phrase table. We experiment with two such factors. First, we use the language models described in Section 4 to compute the cross-entropy of each translation option according to this model. We add cross-entropy as an additional score of a translation pair that can be tuned by MERT (we refer to this system as CrEnt ). Because cross-entropy is a  X  X he lower the better X  metric, we adjust the range of values used by MERT for this score to be negative.
 only measures how close phrases are to translated language, but also how far they are from original language, and use it as a factor in a phrase table (this system is referred to as PplRatio ). We build two additional language models of original texts as follows. For original English, we extract 135,000 English-original sentences from the English portion of Europarl, and 2,700 English-original sentences from the Hansard corpus. We train a trigram language model with interpolated modified Kneser-Ney discounting on each corpus and we interpolate both models to minimize the perplexity of the source side of the development set for the English-to-French translation task (  X  = 0 . 49). For original
French, we use 110,000 sentences from Europarl and 2,900 sentences from Hansard,  X  = 0 . 61. Finally, for each target phrase t in the phrase table we compute the ratio of the perplexity of t according to the original language model L with respect to the translated model L t (see Section 4). In other words, the factor F is computed as follows: tables built from the concatenated corpora, and use each phrase table to train an SMT system. We compare the performance of these systems to that of S both PPLMIN systems. Table 9 summarizes the results.
 significant improvements (p &lt; 0 . 05) on balanced scenarios (MIX) and on scenarios biased towards the S  X  T component (MIX-FO in the French-to-English task, MIX-
EO in English-to-French). PplRatio systems exhibit more consistent behavior, showing the new systems perform quite competitively compared to the interpolated systems, winning in four out of six cases. Note again that all systems in the same column (except
S  X  T ) are trained on exactly the same corpus and have exactly the same phrase tables. decoder to select translation options that are closer to translated texts than to original ones. 6. Analysis
We have demonstrated that SMT systems that are sensitive to the direction of translation perform better. The superior quality of SMT systems that are adapted to translationese is reflected in higher BLEU scores, but also in the scores of other automatic measures for evaluating the quality of machine translation output. In this section we analyze the better performance of translationese-adapted systems, both quantitatively and qualita-tively, relating it to established insights in translation studies.
 our previous work (Lembersky, Ordan, and Wintner 2011, 2012b), we convincingly demonstrated that this is not the case, by means of several experiments that abstracted the texts away from specific words. Although these results are concerned with the language model, we trust that they also hold for the translation model on which we focus here.
 tionese are consistent with human judgments. In other words, a machine translation system that produces translations with higher BLEU scores by taking into account 1012
Although we have not conducted such experiments here, we have shown this corre-lation in a previous work (Lembersky, Ordan, and Wintner 2012b) that focused on the language model rather than on the translation model. 6.1 Quantitative Analysis Is the output of translationese-adapted systems indeed more similar to translationese?
We begin with a set of properties of translationese that are easy to compute, and evaluate the output of our translationese-adapted SMT systems in terms of these properties. 6.1.1 Type X  X oken Ratio. Translated texts have been shown to have lower type-to-token ratio (TTR) than original ones (Al-Shabab 1996). Figure 1 compares the TTR of the trans-lation outputs of S  X  T , T  X  S , UNION, and PPLMIN-2 systems. For comparison, we also add the TTR of the reference translations for each task. To mitigate the effects of the different morphological systems of the various languages, we compute the TTR in terms of lemmas, rather than surface forms. Obviously, the TTR of S than T  X  S system. Recall that S  X  T systems produce markedly better translations than T  X  S ones, so indeed there is a clear correspondence between the TTR of the outputs and better translation quality. Figure 1 also compares the TTR of the outputs produced from two combination systems, UNION and PPLMIN-2. The UNION outputs are arbitrary: Their TTR is sometimes lower than the corresponding S sometimes higher than even the corresponding T  X  S system. In contrast, PPLMIN-2 systems (which are the best adapted systems) systematically produce outputs with the lowest TTR, that is, outputs closest to translationese. As expected, reference translations exhibit the lowest TTR in four out of the six tasks.
 6.1.2 Singletons. A related property of translated texts is that they tend to exhibit a much lower rate of words that occur only once in a text ( hapax legomena ) than original texts.
We thus count the number of singletons in the outputs of each of the SMT systems (and, for comparison, the reference translations). The results, which are depicted in Figure 2, are not totally conclusive, but are interesting nonetheless. Specifically, in all cases the
PPLMIN-2 system exhibits a lower number of singletons than the UNION system; and in all systems except the English-Italian one, the number of singletons produced by the
PPLMIN-2 system is lowest. Reference translations exhibit the lowest rate of singletons in five out of the six tasks. 6.1.3 Entropy. As another quantitative measure of the contribution of perplexity min-based measures discussed in Section 4 on three types of SMT systems: those compiled from S  X  T texts only, UNION, and PPLMIN-2 ones. Observe that the covering set cross-entropy measure, designed to reflect the fitting of a phrase table X  X  target side to translated texts, is significantly lower in PPLMIN-2 systems than in S systems. This indicates that perplexity minimization improves the system X  X  fitness to translationese. Interestingly, the PPLMIN-2 systems have better lexical coverage than the UNION systems. Table 10 lists data for French-English and English-French, but other language pairs exhibit similar behavior. 6.1.4 Mean Occurrence Rate. Original texts are known to be lexically richer than translated ones; in particular, translationese uses more frequent and common words (Laviosa 1998). To assess the lexical diversity of a given text we define Mean Occurrence Rate (MOR). MOR computes the average number of occurrences of tokens in the text with 1014 respect to a large reference corpus. Consequently, sentences containing more frequent words have higher MOR scores. More formally, given a reference corpus R with n word types r 1  X  X  X  r n ,let C ( r i ) be a number of occurrences of the word r the MOR of a sentence S = s 1  X  X  X  s k is:
C ( s i ) is calculated from the corpus R if s i  X  R . Otherwise, C ( s defined constant depending on the size of the reference corpus. In all our experiments we use  X  = 0 . 5.
 we compute MOR scores for each sentence of an SMT system output. Then, we sort the output sentences based on their MOR scores, split the output into two parts X  X elow and above the median of MOR X  X nd calculate BLEU score for each portion independently.
We perform these calculations on the outputs of UNION and PPLMIN-2 SMT systems for all our translation tasks. We use the Europarl corpus (Koehn 2005) as a reference (below the median) of SMT outputs has significantly lower BLEU scores (up to 5 BLEU points!) than the upper part, indicating that the MOR measure is a good (post factum) differentiator between poor and good translations.

Figure 3 shows the results. In all cases (except Italian to English), S
T  X  S ; and in all systems except EN-FR, PPLMIN-2 is best. 6.2 Qualitative Analysis interference , the so-called inevitable marks left by the source language on the target text, and standardization , the attempt of the translator to adapt the translation product to the target language and culture, to break away from the source text towards a more adequate text (Toury 1995). In order to study the effect of the adaptation qualitatively, rather than quantitatively, we focus on several concrete examples. We compare transla-tions produced by the UNION (henceforth baseline ) and by the PPLMIN-2 (henceforth adapted ) French-English Europarl systems. We selected 200 sentences from the French-
English evaluation set for manual inspection, focusing on sentences in which the trans-lations were significantly different from each other. Indeed, we find that the translations are better adapted along several dimensions.
 the adapted system creates a more adequate, standardized translation.

Baseline Mr president, ladies and gentlemen, storms that have ravaged france during 1016
Adapted Mr president, ladies and gentlemen, the storms which have devastated france
Source Tout d X  X bord, je tiens ` a saluer tous mes coll ` egues maires,  X  elus locaux, qui, au
Baseline First of all, I should like to pay tribute to all my colleagues, mayors,
Source Monsieur le pr  X  esident, je vous remercie de me laisser conclure , et je rappellerai
Baseline Mr president, thank you for allowing me to leave conclusion , and I would
Adapted Mr president, thank you for letting me finish , and I would like to remind you in the target language; in the first example, it is clear what is meant by storms that have ravaged France , and moreover, we find such expressions in a 1.5-G token-sized corpus (Ferraresi et al. 2008); it is just half as likely as what is offered by the adapted system.
The second example, on the other hand, misses the point altogether, and the third one is a clear case of interference, where the French laisser conclure is transferred verbatim as leave conclusion .
 following examples, the inability of the baseline system to reorder the words correctly stems from interference:
Baseline Madam president, ladies and gentlemen, we croyions, up to now, that the
Adapted Madam president, ladies and gentlemen, we croyions, up to now, that the
Baseline The lom  X  e convention has mainly to a few large industrial groups or financial
Adapted The rom  X  e convention has mainly to a few large financial and industrial  X  X atural X  expressions pay a high price and express the concern with the baseline system products:
Source Ces hommes et ces femmes qui bougent ` a travers l X  X urope paient leur voiture,
Baseline These men and women who are moving across europe are paying their car,
Adapted These men and women who are moving across europe pay their car, their
Source Je veux dire  X  egalement le souci que j X  X i d X  X ne bonne coop  X  eration entre interreg
Baseline I would like to say to the concern that I have good cooperation between
Adapted I also wish to express the concern that I have good cooperation between implicit utterances more explicit. Koppel and Ordan (2011), who used function words to discriminate between translated and non-translated texts, found that cohesive markers, words such as in fact, however, moreover , and so forth, were among the top markers of translationese, irrespective of source language and domain. And truly we find them also over-represented in the adapted system:
Baseline We s a y the opposite the political necessity to rebalance relations between
Adapted on the contrary , we maintain the political necessity of rebalancing relations
Source Cette mention semble alors contredire les explications linguistiques donn  X  ees
Baseline This note seems so contradict the explanations given by the language and
Adapted This note therefore seems to contradict the linguistic explanations given by unprofessional, even unethical (Beeby 2009). Many professional associations in Europe urge translators to work exclusively into their mother tongue (Pavlovi  X  c 2007). The two 1018 kinds of automatic systems built in this article reflect only partly the human situation, but they do so in a crucial way. The S  X  T systems learn examples from many human translators who follow the decree according to which translation should be made into one X  X  native tongue. The T  X  S systems are flipped directions of humans X  input and output. The S  X  T direction proved to be more fluent and accurate. This has to do with the fact that the translators  X  X over X  the source texts more fully, having a better  X  X ranslation model. X  7. Combining Translation and Language Models
When we experimented with translation models, we compiled language models from corpora comprising original and translated texts. Our previous work (Lembersky,
Ordan, and Wintner 2011, 2012b), however, shows that language models compiled from translated texts are better for machine translation than models trained on original texts. In this section we examine whether these findings have a cumulative effect. In other words, we test if an additional improvement in translation quality can be gained by combining our findings for both the language and the translation model.

FR). We re-use the Europarl-based translation models of Section 3.1. We compile lan-guage models from the French-English Hansard-based parallel corpora described in
Section 3.2. We use 1 million parallel sentence subsets. We train an original French LM on the source side of the S  X  T corpus and we train the translated English LM on the target side of the same corpus. In the same manner we compile the translated French LM and the original English LM from the T  X  S corpus. All language models are 5-grams with an interpolated modified Kneser-Ney discounting (Chen 1998). The vocabulary is limited to tokens that appear twice or more in the reference set. All unknown words are mapped to a special token. We tune and evaluate all SMT systems on two kinds of reference sets: Europarl (Section 3.1) and Hansard (Section 3.2).
 four SMT systems for each translation task: T  X  S TM with original (O) LM, T with translated (T) LM, S  X  T TM with O LM, and S  X  T TM with T LM. All systems are tuned and evaluated on both the Europarl and the Hansard reference sets. Table 12 shows the translation quality of the SMT systems in terms of BLEU. Both translation and language models contribute to the translation quality, but it seems that the contribution of the translation model is more significant. Even in the case of the Hansard reference set, in the English-to-French translation task, the S  X  T TM (compiled from Europarl texts) adds 1.2 BLEU points, and the T LM (compiled from Hansard texts) adds only 0.46 BLEU points.
 tation techniques described in Section 5 for the translation and language models can further improve the translation quality. First, we build a baseline SMT system with a translation model trained on a concatenation of S  X  and a language model compiled from a concatentation of translated and original texts.
Then, we build two other systems, one with an adapted translation model and one with an adapted language model. Finally, we use the adapted translation and language models to train yet another SMT system. We use the PPLMIN-2 method (Section 5) to adapt the translation model and linear interpolation to adapt the language model. The
SMT systems are then tuned and evaluated on the Europarl and the Hansard reference sets. The results, depicted in Table 13, show that SMT systems with an adapted TM usually outperform the baseline systems. LM adaptation alone does not improve the translation quality, but if combined with TM adaptation it produces the best results (but not significantly better than just TM adaptation). 8. Conclusion opposite direction. Nonetheless, even  X  X rong X  phrase tables contribute to the transla-tion quality. We analyze both  X  X orrect X  and  X  X rong X  phrase tables, uncovering a great deal of difference between them. We use insights from translation studies to explain these differences; we then adapt the translation model to the nature of translationese. interpolation to create a mixture model of S  X  T and T 1020 use perplexity minimization and an S  X  T reference set to determine the weights of each model, thus directly adapting the model to the properties of translationese. We show consistent and statistically significant improvements in translation quality on three different language pairs (six translation tasks) using several automatic evaluation metrics.
 with translationese into phrase tables as an additional score that can be tuned by MERT, and show a statistically significant improvement in the translation quality over all baseline systems. We also analyze the results qualitatively, showing that SMT systems adapted to translationese tend to produce more coherent and fluent outputs than the baseline systems. An additional advantage of our approach is that it does not require an annotation of the translation direction of the parallel corpus. It is completely generic and can be applied to any language pair, domain, or corpus.
 model), our previous work (Lembersky, Ordan, and Wintner 2012b) focuses on improv-ing the product of translation (i.e., the language model). We have shown preliminary results in which both models were adapted to translationese; an open challenge is finding the optimal combination of improving both process and product in a single unified system.
 Acknowledgments References 1022
