 Dirichlet process mixture (DPM) model is one of the most important Bayesian nonparametric models owing to its ef-ficiency of inference and flexibility for various applications. A fundamental assumption made by DPM model is that all data items are generated from a single, shared DP. This assumption, however, is restrictive in many practical set-tings where samples are generated from a collection of de-pendent DPs, each associated with a point in some covariate space. For example, documents in the proceedings of a con-ference are organized by year, or photos may be tagged and recorded with GPS locations. We present a general method for constructing dependent Dirichlet processes (DP) on ar-bitrary covariate space. The approach is based on restrict-ing and projecting a DP defined on a space of continuous functions with different domains, which results in a collec-tion of dependent random measures, each associated with a point in covariate space and is marginally DP distributed. The constructed collection of dependent DPs can be used as a nonparametric prior of infinite dynamic mixture models, which allow each mixture component to appear/disappear and vary in a subspace of covariate space. Furthermore, we discuss choices of base distributions of functions in a variety of settings as a flexible method to control dependencies. In addition, we develop an efficient Gibbs sampler for model in-ference where all underlying random measures are integrated out. Finally, experiment results on temporal modeling and spatial modeling datasets demonstrate the effectiveness of the method in modeling dynamic mixture models on differ-ent types of covariates.
 G.3 [ Probability and Statistics ]: Nonparametric Statis-tics; H.3.3 [ Information Storage and Retrieval ]: Infor-mation Search and Retrieval X  clustering ; I.2.6 [ Artificial Intelligence ]: Learning Hierarchical Topic Modeling, Bayesian Nonparametric mod-els
Bayesian nonparametric models are a family of techniques that help bypass the difficult model selection problems, i.e., they allow the models to grow in size to accommodate the complexity of the data. In machine learning and a wide range of data mining applications, such as text modeling[15, 18, 17], computer vision[25], recommender system [9, 31], Bayesian nonparametric models are playing an increasingly important role.

Dirichlet process mixture (DPM) model is one of the most important Bayesian nonparametric models owing to its ef-ficiency of inference and flexibility for various applications [27, 12, 23, 30]. DPM works by assigning Dirichlet process (DP) as prior distribution on an infinite number of mix-ture components, which to some extent is analog to the way Dirichlet distribution works in parametric topic model: La-tent Dirichlet Allocation(LDA)[6]. Another similar point between LDA and DPM is exchangeability assumption. For DPM model, exchangeability indicates all data items are generated from a single, shared DP, and the infinite compo-nents of DPM are exchangeable, i.e., under any permutation of their ordering, the corresponding probability remains the same. This implicit simplicity is consistent with mining la-tent semantic of each sample, however, is restrictive in many practical settings where samples are not independent, but correlated in some covariate space.

Consider a generative model for news documents, where a piece of news is assumed to be generated from one or a few topics, and each topic is a probability distribution of words. Over time, for a particular news topic, it may emerge at a specific time point and disappear later. Meanwhile its word distribution may continuously vary over time. Over different locations, a news topic may appear only in a certain local region, while the associated word distribution is slightly different within this region. Lots of efforts have been made to model such dynamics of topics over covariate space[26, 29, 32, 19]. Most, if not all, proposals focus on the evolution of topics of sequential organized corpus of documents. An intuitive method is to construct a Markov chain of DPs, which, however, can only deal with discrete time covariates due to constraints of Markov models.

We argue that temporal correlation and spatial correlation are not unrelated, instead they are consistent, i.e., without loss of generality, time can be formulated as one-dimensional Euclidean space, while location can be formulated as two-dimensional Euclidean space. It is natural to consider con-structing a collection of dependent Dirichlet processes on a given covariate space, such that each topic has a lifespan, which is a subset of the covariate space, while the topic pa-rameters vary smoothly within its lifespan. Note that the  X  X ifespan X  includes but not limits to timeline.

Dirichlet process (DP) is a stochastic process, meaning that it is a distribution over random distributions [11]. Specif-ically, it is a distribution over probability measures, which are functions that can be interpreted as distributions over some probability space. We denote a Dirichlet process as DP (  X G 0 ), where parameter G 0 is called base measure and  X  is the concentration parameter. The base measure G 0 , which is basically regarded as the mean of the DP, can be an ar-bitrary distribution. A draw from DP will return a random distribution over values drawn from G 0 . In this paper, mo-tivated by the correspondence between dynamics of topics and functions: the domain of functions corresponds to the life-span of topics; the change of topic distribution is char-acterized by function values , we present a general method to construct dependent DPs on arbitrary covariate spaces utilizing functional Dirichlet process (FDP) which is a DP over function space. That is to say, base measure G 0 is de-fined on a functional space, and a draw from this DP is a countably infinite collection of continuous functions. These functions thus naturally describe the correlation between co-variate space and atom space (the output space). Given a realization of FDP, at any covariate index, restriction and projection operations are applied on FDP to find an induced probability measure supported by valid functions and their corresponding values.

The main contributions of this paper can be listed as fol-lows:
In this section, we give some mathematical backgrounds on Dirichlet process mixture model.

Dirichlet process is a random process, whose domain is also a random process over some probability space  X  [11]. Typically, a DP can be denoted as DP (  X G 0 ), where G 0 is a distribution over  X ,  X  is a positive real number. Given a ran-dom distribution G over probability space  X , for any finite measurable partition of  X : A 1 ,A 2 ,...,A n , if random vector ( G ( A 1 ) ,...,G ( A n )) is Dirichlet distributed. Formally, We call the random distribution G is Dirichlet process dis-tributed, and it is usually denoted as G  X  DP (  X G 0 ).
By assigning DP as a prior of a hierarchical model, we get the Dirichlet process mixture model. DPM can be used for nonparametric clustering [22], i.e., the model need not know beforehand the number of clusters, but automatically grow in size to adapt the dataset. DPM can be formalized as a generative process: where { x 1 ,...,x n } denote a set of observations, and F ( . ) is a given likelihood function parameterized by {  X  1 ,..., X  Each parameter  X  i is drawn independently and identically from the Dirichlet process G , and  X  i is hence exchangeable. This implicit assumption, however, is inappropriate in many collections of interest as we have discussed in Section 1. In this paper, we are to design a dependent Dirichlet process to display G in Eq. 1. In this section, we present our construction of dependent Dirichlet processes using functional Dirichlet process (FDP), a DP defined on the space of continuous and bounded func-tions.

Model construction. Let ( X  ,  X ) be probability space of component parameters, termed atom space and  X  as the covariate space . Our proposal is to construct a probability measure D  X  over a countably infinite collection of continuous functions, regarded as functional atoms, where the domain of each function is a subspace of covariate space  X . Given D  X  and any covariate index  X   X   X , restrict and renormalize D  X  to include functional atoms whose domain contains  X  , and project each of these functional atoms f (in function space) to atom (in atom space) located at f (  X  ). This gives us an induced probability measure D  X  over  X .

Putting these steps together and posing a Dirichlet pro-cess prior on D  X  , we have: where G  X  is a probability distribution over F , which is the space of continuous functions whose domains are subsets of covariate space, { f i } is a collection of countably infinite functions independent sampled from G  X  , and the normaliza-measure D  X  is a properly normalized probability measure, for any element  X  in the covariate space. We defer the dis-cussion of construction G  X  to Section 4. Figure 1 depicts Basic concepts about a modicum of measure theory and Dirichlet distributions are necessary. intuitions of the construction on several exemplary covari-ate spaces.

Marginal distribution. D  X   X  and D  X  are automatically marginal DP distributed by the general property that DP is closed under restriction and projection operations which is established through the following lemma.
 Lemma 3.1. Let D = P p i  X   X  i  X  DP (  X  ) be a Dirichlet process on ( X  ,  X ) . Then: 1. The random probability measure obtained by restricting 2. The random probability measure obtained by projecting
We refer interested readers to [16] for a detailed treatment on the advanced properties of DP. Applying the lemma, we found the marginal distribution of D  X  is, where  X   X  is a measure defined on ( X  ,  X ) such that,  X  A  X   X ,
Infinite dynamic mixture model. The resulting col-lection of dependent DPs { D  X  }  X   X   X  is able to act as a non-parametric prior for a set of infinite mixture models. For-mally, given a set of covariate indices  X  1 ,..., X  n (may have repetition), the observed data ( x 1 , X  1 ) ,..., ( x n , X  erated by the following generative processes. where F  X  (  X  ) is a probability distribution parametrized by  X  . Observation x i at covariate  X  i is drawn from a mixture component with parameter  X  i whose value is determined by the underlying function f i which is marginalized out in this representation.
We have described a general solution to constructing a col-lection of dependent Dirichlet processes on arbitrary covari-ate space using functional Dirichlet process. In order to use the construction in machine learning applications, we need to consider a general question on choosing the base distribu-tion of function G  X  . This distribution essentially determines the dependency between constructed DPs and is application-dependent. In this section, we develop a principled method to construct such a distribution G  X  . Our construction of G  X  is applicable on arbitrary covariate space and does not preclude efficient inference. Furthermore, we give examples on several types of covariate space with different types of application data.

For simplicity, we assume that a random function g : domain( g )  X   X  is generated by restricting a stochastic pro-cess on the entire covariate space to an independent random subset of covariate space. In particular, 1) The function do-main is determined by drawing domain( g ) from a distribu-tion over subsets. 2) The function values are determined by drawing g 0 :  X   X   X  as a realization from a  X -valued stochas-tic process on entire covariate space: { g 0 (  X  ) }  X   X   X  process . The random function can be then obtained by re-stricting g = g 0 | domain( g ) . We note, in this procedure, the assumption we made is that function domain is independent with function values.
In our model, function domains characterize the life-spans of mixture components on covariate space. Determining the properties of life-span depends on domain knowledge of ap-plications. For example, when covariate is time, a proper choice of life-span is time-interval (segment on R ), in which the two endpoints represent  X  X orn X  and  X  X ie X  of certain com-ponent. On the other hand, in image segmentation applica-tions, the life-span of a component would be a contiguous region on R 2 . To model the life-span of mixture compo-nents, we need an effective way to control function domains via prior distribution.

By Kolmogrov extension theorem, the distribution of func-tion domains can be constructed indirectly via its finite-dimensional marginal distributions  X  , Pr ( {  X  1: n } X  domain( f )). Through different characterizations of  X  , we can construct arbitrary distributions of function domains. In particular, the diameter dependent  X  can used for covariate space with metric defined d :  X   X   X   X  R , We note the above characterization recovers the discrete case discussed by Lin et al [20]:  X  = { 1 , 2 ,..., } in which the covariate space is regarded as time slots. Their construc-tion corresponds to a special case of FDP where each do-main has the form { a i ,a i + 1 ,...,b i } , in which | b Geometric( q ). It is not hard to see this is consistent with (8) with d ( a,b ) = | a  X  b | and  X  = ln(1  X  q ). This can be easily extended to continuous covariate space R with little modification. In this scenario, each domain has form [ a and | b i  X  a i | X  Exponential( q ) ,  X  = q ln q . With general Eu-clidean covariate space R d , (8) also ensures the diameter of domain have statistical pressure to be kept small.
When using FDP as a nonparametric prior for infinite dynamic mixture models, the role of base processes closely resembles that of base distributions played in DPM model: both of them served as the prior of component parameters, while the base processes have an extra flexibility to allow the parameters changing over covariate space. Similar with the choice of base distribution in DPM, the choice of base processes is dependent on the application and the type of data. In particular, we are interested with the base pro-cesses that are conjugate with data likelihood. In the rest of this part, we investigate three different choices of conju-(  X  ) ,f 3 (  X  2 ) ,... spaces. a) Both covariate space and atom space are R . D  X  is a DP defined on function space. Each D  X  constructed by finding all functions f in D  X  whose domain contains  X  gate base process that cover the need of a range of common applications.

Constant functions. Constant functions are among the simplest choices of base process which precludes the flex-ibility of allowing variation of parameters. In particular,  X   X , g 0 (  X  ) = c, c  X  H 0 , where H 0 is a probability distribu-tion over  X . In practice, H 0 can be chosen as the conjugate prior respected to data likelihood which can enjoy a col-lapsed Gibbs sampler.

Gaussian process. When the data distribution F  X  ( x ) is such that the parameter  X  can be represented as a real-valued vector  X   X  R M , it is natural to use a Gaussian dis-tribution to describe the marginal distribution of  X  and a Gaussian process to encode the dependency between differ-ent covariates. That is, given a mean function m (  X  ) and a covariance function K (  X , X  0 ), the base process is a given by g  X  GP( m,K ).

Dependent Dirichlet distribution. When each da-tum is an element from W discrete ones, similar with many finite topic models, a categorical model may be suitable. In this case, g 0 (  X  ) takes value in the surface of the W -simplex. Motivated by the fact that a Dirichlet distribu-tion is a special case of DP with a finite space, we construct a collection of dependent Dirichlet distributions { g (  X  ) } using a special of case of FDP where  X  = [ W ]. In partic-ular, let D  X   X  DP (  X   X  ) where  X   X  is a finite measure sup-port over the space of constant functions, but with different domains. By construction, we obtain induced dependent probability measures { D  X  }  X   X   X  . D  X  (  X  1 ) ,..., D Dir (  X   X  (  X  1 ) ,..., X   X  (  X  W )) are Dirichlet distributed. This com-pletes the construction of base process as dependent Dirich-let distributions g 0 (  X  ) = D  X  . As we would show in Section 6, this construction does not preclude efficient inference as the predictive distribution  X   X  n +1 |  X   X  1 ,..., X   X  n is tractable to evaluate. We contrast this construction with previous work on dynamic topic models that are based on transforming from underlying Gaussian process [5, 2].
A few of DDP constructions are based on varying stick breaking weights over covariates of stick breaking represen-tation of DP. Dunson and Park [10] proposed kernel stick breaking process (KSBP), where the covariate dependency is involved via setting stick breaking weights as weighted sums of independent beta variables and kernel function on covariate space. Ren et al [25] proposed logistic stick break-ing process (LSBP) in which the stick breaking weights are determined by a hierarchy of spatial logistic regressions with sparseness promoting priors placed on regression coefficients. Both works are not marginally DP distributed, and their in-ference algorithms are based on truncated approximation. Alternatively, the  X  DDP, proposed by Griffin and Steel [14], allows the change of distribution over atoms but still pre-serves marginally DP distributed. In their work, the de-pendency is implemented by inducing a different ordering of stick breaking ratios at each time slot. Different from our model, these works assumed atom locations are fixed across different covariates. Moreover, these constructions missed interpretations of life-spans of atom, which are real-ized through function domains in our model.

Another line of works is based on Chinese restaurant pro-cess (CRP) representation of DP. The recurrent Chinese restaurant processes (RCRP) [1] and generalized Polya Urn scheme [8] both generalized the CRP to include time depen-dency and still maintain marginal DP property. However, these constructions are only defined for the real line as co-variate space. Blei and Fraizer developed distance depen-dent Chinese restaurant processes (DCRP) [4] generalized RCRP to arbitrary covariate spaces. Fundamentally dif-ferent from Bayesian nonparametric models, DCRP is not constructed based on random measures and does not exhibit marginal invariance.
 Lin et al [20] is one of the closest works to our approach. They proposed a Markov chain of DPs (DDP with discrete covariate space) constructed via superposition, subsampling and transition operations of DP. FDP recovers their model, by choosing the shape distribution (geometric distributed life-span) and covariate space (discrete) defined in Section 4.1. Moreover, our developed sampling algorithm is able to exploit global information of data in every observed covari-ate, in contrast with sequential sampler derived in [20]. An-other closely related work is the spatially normalized Gamma process (SN X P) developed by Rao and Teh [24] which en-ables to construct dependent DPs on arbitrary covariate space. Their work does not have the model flexibility for allowing atom transitions. In addition, their construction methods are different from our model(they use normalized restricted Gamma process, we use restricted Dirichlet pro-cess). Despite these differences, their resulting random mea-sures are equivalent to a special case of FDP which only al-lows constant functions in G  X  and the dependency is induced by overlapping function domains. Finally, we note they did not develop distribution of function domains for covariate space other than R .
We develop an efficient sampling algorithm for FDP where all underlying random measures are integrated out. Assum-ing we have observed n data, x 1: n with covariates  X  1: n spectively, we wish to infer the partition of data with an unknown number of partitions and parameters, where each partition corresponds to as a function in FDP formulation. In particular, we are interested with the functional assign-ments z 1: n of data, which resembles the table assignments in traditional Chinese restaurant process representations.
To formulate the problem, consider the following alterna-tive generative process of x 1: n , Denote f  X  1: K to be the unique functions among f 1: n in this representation (9). Each unique function f  X  l appears c l Then the assignments of functions are denoted as z i  X  [ K ], s.t. f  X  z i = f i .
Predictive distribution plays a central role in implement-ing a Gibbs sampler for model inference. The aim of this part is to derive the predictive distribution z, X  |{ z 1:( n  X  1) } , for datum x with covariate index  X  . The derivation relies on the fact that the posterior of D  X  given { z 1:( n  X  1) is a mixture of Dirichlet process (MDP) [3]. The predic-tive distribution follows by simplifying and marginalizing out DPs in the posterior. We leave the derivation of the Gibbs sampler in the appendix. The predictive distributions for i  X  [ K ], where  X  K +1 indicates the event of assigning to a new func-tion, q (  X  ) i and T (  X  ) i are induced from G  X  and are given by,
T i ( d X  ) = Pr ( f (  X  ) =  X  | f (  X  1 ) =  X  1 ,...,f (  X  n  X  1
Sampling z using the predictive distributions is made tractable itly sampled rather than to be marginalized out. We also note that the auxiliary variables have clear interpretations: t i  X  Bernoulli( q i ) is a random event that the domain of a function f includes  X  given that its domain include  X  1:( n  X  1) Meanwhile  X  (  X  ) i  X  T (  X  ) i is a random variable equals to f (  X  ) conditioned on f (  X  i ) =  X  i .
We have so far proposed the FDP model and analyze its corresponding posterior and predictive distributions. Hereby, we employ a Gibbs sampler to perform an approximate in-ference.

The Gibbs sampler maintains all assignments z 1: n , z [ K ], where K is the number of represented functions and will change over iterations. It also maintains c 1: K as the num-bers of occurrences of different functions: c j = |{ z i = j }| . During each iteration, the sampler iterates over all assign-ment variables z 1: n and resample each of them at a time. In addition, for some choices of G  X  and F , the Gibbs sampler can operate in a collapsed mode where parameters are inte-grated out. We discuss both Gibbs sampler for general case and that for collapsed model. In what follows, we describe the procedure of resampling z n , the assignment variable of datum x n with covariate  X  n , without loss of generality. 1. Sample auxiliary variables. For each k  X  [ K ], resample t (  X  n ) k  X  Bernoulli( q (  X  n ) k ) to determine whether the domain of f  X  k contains  X  n , where q (  X  n ) k is defined in (16). (general) 2. Sample assignments. First for each k  X  [ K ], draw parameter  X  (  X  n ) k via  X  (  X  n ) k  X  T probability of assigning to existing functions is P ( z n t new function is P ( z n = K + 1)  X   X   X  n ( X ) F ( x n ), where tribution of base process at  X  n . (collapsed) 2. Sample assignment. In certain cases, one can further integrate out parameters  X  (  X  n ) 1: K which leads to a collapsed sampling method for assignment variables. Denote the conditional density of x n be generated by func-tion f  X  k given all its data items as, The probability of joining a existing function is P ( z n t k c k F ( x i |{ x j } z j = k ), while the probability of assigning to a new function remains unchanged. Each of the three cases discussed in Section 4.2 can use this collapsed sampler.
In this section, we evaluate our methods on both synthetic and real-world datasets.
Temporally evolving Gaussian mixtures. In the first experiment, we simulate a collection of evolving Gaussian mixtures. Initially, 2 Gaussian components are generated. New components are introduced following a Poisson process (on average 0.4 new components per time phase). The life-span of each component is geometric distributed with mean 5. In addition, for each component, its mean evolves follows a Brownian motion with variance 1. We simulated for 30 phase with 200 data points being drawn independently from each component at each phase. We applied our model on generated data points with covariates as the time phases that data points be generated. The base process is chosen as a zero-mean Gaussian Process. The distribution of function domain is constructed as (8). We run the Gibbs sampler for 5000 iterations and collected the resulting assignment variables as partition results.

We evaluated the results by measuring the dissimilarity between inferred partition with ground truth using the vari-ation of information criteria[21]. Formally, the variation of information is defined as follows: where H and I denote the entropies of and the mutual infor-mation between the two clusters, respectively. d V I measures the distance between two clusterings in terms of the infor-mation difference between the two clusters. Hence, the lower the value of d V I , the better the performance is. We compared our model (F-DPM) with covariate-independent DP mixture model (DPM) and a state-of-the-art technique: Markov chain of DPs (Markov-DPM) described in [20]. We note that, in this scenario, Markov-DPM is equivalent to F-DPM, with discrete covariate and suitable base distribu-tion of functions, in the sense that the constructed random measures are identically distributed. The difference is that Markov-DPM uses a sequential sampling algorithm for in-ference. Based on a different analysis, F-DPM can use a batch inference algorithm as described in Section 6.
We can observe from Figure 2 that F-DPM consistently outperforms Markov-DP since the batch inference algorithm for F-DPM can exploit global information while sequential filtering algorithm for Markov-DP is not able to use fur-ther observations. The performance gain of F-DPM over Figure 2: Experiment results: Compare F-DPM with Markov-DPM and DPM for evolving Gaus-sian mixture model. The top graph shows the me-dian of distance between the resulting clusters and the ground truth at each phase. The bottom graph shows the actual numbers of clusters. Figure 3: Experiment results: Image segmentation results by F-DPM and DPM.
 Markov-DPM is especially significant for data with small timestamps, as a result of the sequential natural of Markov-DPM inference algorithm. In addition, both F-DPM and Markov-DPM outperforms DPM since they better captured the dynamic nature of data. Moreover, we can see that F-DPM almost exactly recovered the number of clusters. This is remarkable because the state of the art Bayesian nonpara-metric method DPM, which is often applied for its capability of estimating the number of clusters, produced much more inaccurate result on this quantity.

Gaussian mixture with other covariate spaces. We further tested our model with different covariate spaces: R and undirected network. On R 2 covariate space, we demon-strated a simple image segmentation application. We syn-thetic the original gray-scaled image with four continuous sub-regions on a background with different intensities. The background has an intensity 5 and the two pairs of sub-region have intensities of 10 and 25. Each pixel is added a independent white Gaussian noise with standard devia-tion of 1.5. We applied our model to infer partition of all pixels with covariate space R 2 . We take the base process as constant functions with H 0 and distributions of function domain as defined in (8). We compared the segmentation results with covariate-independent DP mixture model.
The results is shown in Figure 3. We found the segmenta-tion results generated from our model are clearer than that of DPM, since FDP accommodates spatial dependency such that closer pixels are more likely to be clustered together.
Evolving topic model. We analyze the proceedings of the NIPS conference from 1987 to 2001, which consist of 2484 documents. As in hierarchical Dirichlet process (HDP), we consider a topic as a distribution over words and each document as a distribution over topics. In order to model the evolution of topics, we place a FDP prior over topics. In this setting, each topic has a life-span over time and its word distribution evolves as well. In particular, we set the length of life-span to have an exponential distribution with param-eter 0.6. In addition, the base process is set to be depen-dent Dirichlet distributions in order to model the continuous transition. The recovered timeline of topics is demonstrated in Figure 4. From topic lifespan, we can see the model in-deed captures the death and birth of various topics. The top keywords showed in the same figure also demonstrate some topics (chains) evolve over time. In Table 2 (NIPS dataset), we have also compared the performance of DPM and F-DPM in terms of held-out log likelihood, which is a standard metric for measuring modeling performance (cf. [7, 27]). Held-out log likelihood is defined as log-likelihood of the held-out data, L = log p ( w heldout | m trained ticular, we randomly separate 90% words to train the topic model and use the remaining data for testing. We can see that the F-DPM obtained a higher held-out log likelihood than DPM, since the additional temporal information leads to a more preferable model.

Location based topic model. In the last experiment, we use Flickr dataset to evaluate our approach over two-dimensional location space. Specifically, we use F-DPM to mine latent topics from image tags with GPS informa-tion. We crawl images with GPS locations from Flickr web-site 2 . Three representative datasets, including Landscape, Activity, and National Park, whose time spans range from 2009/01/01 to 2010/01/01 and GPS locates in USA territory are kept. We crawl images using keyword landscape around USA for Landscape dataset, surfing and hiking around USA for Activities dataset, and nationalpark around USA for Na-tional Park dataset. Tags occurring less than 15 times are removed. Totally, we obtain 1505 + 11868 + 2109 images and 2313 + 2381 + 2374 tags corresponding to three datasets.
We compare our approach with DPM, which assumes all tags are sampled from a set of shared topics, i.e., without considering location information. Table 1 shows the num-ber of topics discovered by DPM and F-DPM from three datasets. The result shows that F-DPM recovers more top-ics than DPM, which is reasonable since F-DPM requires a slight constriction on the topic generation and separates more topics. Furthermore, three examples shown in Fig-ure 5 implicit topics found by F-DPM are less ambiguous, http://www.flickr.com/services/api Table 1: The number of topics discovered by DPM and F-DPM.

Table 2: Comparison of held-out log likelihood. and thus more satisfying. In the first example for land-scape dataset, Figure 5(a) and Figure 5(c) show that both DPM and F-DPM detect a clear topic about mexico . How-ever, in the second and third examples, we can see that top-ics found by F-DPM are significantly more interpretable, namely, Figure 5 (e) shows a topic on yellowstone , and Figure 5 (f) shows a topic about coast surfing . Moreover, the quantitative evaluation results using held-out log likeli-hood is shown in Table. 2 for all of the three datasets. The quantitative results demonstrated that, by incorporating the location information, the F-DPM yields a better modeling performance than DPM.
In this paper, we proposed a general method for con-structing dependent Dirichlet processes on arbitrary covari-ate space based on restricting and projecting functional Dirich-let process, where the atoms can appear/disappear and vary in subspaces of covariate space. We discussed the properties of constructed model, and proved that the output random measures are guaranteed to be marginally DP distributed. We developed a principled method to specify the base dis-tribution of functions, such that the model can be applied in more general setting. We presented an efficient Gibbs sam-pler inference algorithm by marginalizing out the underlying random measures. We conducted experiments on dynamic Gaussian mixture model, image segmentation, and dynamic topic model. The results demonstrated the effectiveness of our approach.

There are several interesting future directions. First, we are interested in applying the idea of constructing dependent random measures by restricting and projecting a shared un-derlying random measure on function space for constructing other types of dependent random measures, for example In-dian Buffet process [13], in which the underlying random measure is beta process [28]. Second, we plan to explore a variational method as an alternative deterministic form of approximate inference. Finally, as a byproduct of our con-struction, the dependent Dirichlet distributions can be used as a component in a variety of existing dynamic topic model applications for inducing dependency in arbitrary space and not precluding efficient sampling algorithm.
 This work is supported by Canada X  X  IDRC Research Chair in Information Technology program, Project No. 104519-006, China National Key Basic Research Program (also called 973 Program), Project No. 2013CB329403, and Tsinghua University Initiative Scientific Research Program, Project No. 2012108807. The authors would also like to express our gratitude to Shouyuan Chen for his assistance on exper-iments.
 [1] A. Ahmed and E. P. Xing. Dynamic non-parametric [2] A. Ahmed and E. P. Xing. Timeline: A Dynamic [3] C. E. Antoniak. Mixtures of Dirichlet processes with [4] D. M. Blei and P. Frazier. Distance dependent Chinese [5] D. M. Blei and J. D. Lafferty. Dynamic topic models. [6] D. M. Blei and J. D. Lafferty. A correlated topic model [7] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [8] F. Caron, M. Davy, and A. Doucet. Generalized Polya [9] N. Ding, Y. A. Qi, R. Xiang, I. Molloy, and N. Li. [10] D. B. Dunson and J. Park. Kernel stick-breaking pro-[11] T. S. Ferguson. A bayesian analysis of some nonpara-[12] E. Fox, E. Sudderth, M. I. Jordan, and A. Willsky. An [13] Z. Ghahramani and T. L. Griffiths. Infinite latent fea-[14] J. E. Griffin and M. F. J. Steel. Order-based dependent [15] T. Griffiths, M. Jordan, and J. Tenenbaum. Hierar-[16] N. L. Hjort, C. Holmes, P. M  X  uller, and S. G. Walker. [17] D. Kim and A. Oh. Accounting for data dependencies [18] J. H. Kim, D. Kim, S. Kim, and A. Oh. Modeling [19] P. Langley. Crafting papers on machine learning. In [20] D. Lin, E. Grimson, and J. Fisher. Construction of de-[21] M. Meila. Comparing clusterings -An axiomatic view. [22] R. M. Neal. Bayesian mixture modeling. Maximum [23] Y. Qi, D. Liu, D. B. Dunson, and L. Carin. Multi-task [24] V. Rao and Y. W. Teh. Spatial normalized Gamma [25] L. Ren, L. Du, L. Carin, and D. B. Dunson. Logistic [26] N. Srebro and S. Roweis. Time-varying topic models us-[27] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. [28] R. Thibaux and M. I. Jordan. Hierarchical Beta pro-[29] X. Wang and A. McCallum. Topics over time: a non-[30] S. Williamson, C. Wong, and K. A. Heller. The IBP [31] K. Yu, S. Zhu, J. Lafferty, and Y. Gong. Fast nonpara-[32] X. Zhu, Z. Ghahramani, and J. Lafferty. Time-sensitive
In this part, we show the derivation of predictive distri-oping the Gibbs sampler. Our approach is to first analyze the posterior distribution of FDP. After that we are able to integrate out all underlying random measures and compute predictive distribution, since the constructed random mea-sure D  X  are marginally DP distributed. Then, we will derive the predictive distribution based on the collapsed posterior distribution of FDP.
 functional assignments z 1: n and function values is possible if and only if that each unique function f  X  l satisfies, equivalent with given K different observations such that 1) the l th observation occurs c l times 2) the l th observation is in It is noted that this is different with the posterior analy-sis of traditional DP where locations of each observation are exactly known. In this model, however, the exact lo-cations of each functional atom are subjected to censoring and only possible regions of each observation are known. Such family of posterior, originally analyzed by Antoniak [3], is termed as mixture of Dirichlet processes (MDP). The posterior can be obtained by following steps. Given func-tions f  X  1: K , the posterior of D  X  is simply DP (  X   X  + P Integrating these with respect to the conditional probabil-ity of Pr( f  X  i | f  X  i  X  F i ), we obtain the posterior of D { f i  X  X  i } is M  X  , D  X  |{ z .

Similarly, for any  X   X   X , given functions f  X  1: K , the poste-rior of D  X  is a DP obtained from restricting and projecting the posterior of D  X  . Integrating all functionals f  X  1: K the posterior of D  X  which is MDP distributed, M =
Although the posterior of D  X  is a mixture of uncountably infinite number of DPs, it is that all functions f such that  X   X  domain( f ) and f (  X  ) =  X  for any fixed  X  have exactly same effect in (13). By rearranging and grouping the com-ponents in (13), we obtain a simpler form.
 Applying the independent assumption made in 4, we can further simplify 15 and express them by the distribution of function domains and the base process
T i ( d X  ) = Pr ( f (  X  ) =  X  | f (  X  1 ) =  X  1 ,...,f (  X  n  X  1 Predictive distribution.
 readily obtained by marginalizing DPs in 14 out, where  X  K +1 indicates the event of assigning to a new func-tion.
