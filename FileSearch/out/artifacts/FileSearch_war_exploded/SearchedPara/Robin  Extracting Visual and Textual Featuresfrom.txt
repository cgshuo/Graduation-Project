 Analysis of Web pages is a fundamental process for many applications including Web search engines, Web page clustering, and Web classification tasks. The key steps in this analysis are to select types of the information of interest and extract features that chara cterize the Web pages. Typical analysis of Web pages uses textual information such as words that indicate the subject of the page and extracts page features by performing syntactic and/or semantic analysis. However, Web pages also contain visual information , which is expressed through images, use of color, and layout. Making use of this visual information as well as the textual information displayed by Web pages may provide users with more rapid and accurate analysis of Web pages. In this paper, we investigate this idea and propose a framework for extracting the combined features of visual and textual information from Web pages.

To implement the idea of using both forms of information, we had to devise information-feature mapping functions to define the characteristic features of Web pages. The key point in our framework for visual-feature extraction is to view Web pages themselves as images (i.e., thumbnails). This enables intuitive incorporation of aspects of human visual perception in extracting the visual features that characterize the pages. The features of the visual information in thumbnails can be expressed through many different measurements. Our frame-work allows us to use measurements ranging from simple ones, such as color use on a page, to more complicated ones, such as the edge distribution pattern, fea-tures by two-dimensional FFT, two-dimensional Wavelet, or objects recognized by analyzing the thumbnail. As one way of representing the visual information in a thumbnail, we use a color histogram because color is known to be one of the visual properties processed preattentively (i.e., without the need for focused attention) by humans [1].
 There are a number of existing methods for extracting tex tual features from Web pages including the frequency with which a word occurs, n-grams, and tf-idf [2]. Our framework allows us to use any vector-space based feature extraction methods to map textual information. Here, we extracted textual features from simple but important textual information, namely, the titles of Web pages which are found in the HTML title tag.

Integration of visual and textual featu res is achieved using their vector-based representation and a mathematical tool called Principal Component Analysis (PCA). The use of PCA to enable processing of integrated features permits robust characterization of the two features.

To develop a working prototype of our framework, we designed a system called Robin , which is aimed at enabling users to browse search engine results intuitively. In the Robin system, features are extracted from a ranked list of search results using our proposed framework. To provide an overview of the search results, the extracted features are placed in a two-dimensional space that displays the relationships between Web p ages. Users can then browse the search results intuitively knowing that pages placed near (or further from) each other are heuristically similar (or dissimilar). In addition, several characteristic Web pages are displayed in the space as landmark pages to further assist browsing.
Below, we present our method for extra cting features from Web pages using visual and textual information and describe the Robin system. We also describe experiments to evaluate the quality of the results obtained and discuss related work. 2.1 Extracting Features from Visual Information In extracting features from the visual information displayed by Web pages, we use guidelines based on the perceptual ability of the human visual system. Previous work shows that color is one of the visual properties processed preattentively (i.e., without the need for focused attention) by humans [1]. We treat the Web pages themselves as images (obtained as thumbnails ) and attempt to characterize them by the use of color on the page. To do this, we use the HSV color model as it is more similar to the way humans tend to perceive color than alternative models such as RGB or CMYK. We first normalize  X  X  X ,  X  X  X , and  X  X  X  in the range between 0 to 1 for a Web-page thumbnail. We then generate a color histogram c by counting the number of pixels of each color in the thumbnail. 2.2 Extracting Features from Textual Information Web pages include several forms of textual information and there are thus sev-eral possible methods for extracting the features. As one form of textual in-formation, we use a simple but important piece of information, namely, the titles of the page, which is found in the HTML title tag. Specifically, we use the position of an input query as textual information. For an input query key , a title t i =( w 1 ,w 2 ,...,w M ) of a search result i is transformed into a vector b =(  X  (1) , X  (2) ,..., X  ( M )) where M is the maximum number of words w in the titles of all the search results plus an additional value reserved for distinguishing Web pages whose titles do not contain the input query term. The function  X  is defined by  X  ( i )= formation, the proposed framework can b e generalized to extr act features from other textual information such as the contents of Web pages by using other vector-space based methods such as n-grams and tf-idf [2]. 2.3 Extracting Characteristic Features For a Web page i , we integrate the vector of the color histogram c i with the vector of the title position b i and construct a single vector x i by concatenating them. This integrated vector may be highly dimensional depending on the interval of the color spaces and the maximum number of words in the title. To reduce this dimensionality and to extract the characteristic features from the vector, we use PCA [3].

Given a set of vectors x 1 ,x 2 ,...,x L ,where L is the number of Web pages and x is a vector of length n , we compute the average vector of the data set by x average by  X  x i = x i  X  x 0 . We use these differences to co mpute a covariance matrix R for the data set. The covariance between two sets of data shows how well the sets correlate. The covariance matrix R is obtained by R = L i =1  X  x i  X  x i T ,where T indicates the transposition of a matrix.

When using PCA to reduce highly dimensional data, eigenvalues and the corresponding eigenvectors of R are usually computed by the singular value de-eigenvector pairs of R ,wechoosethe N (  X  n ) eigenvectors with the largest eigenvalues. The larger N is, the higher the contribution rate of all the eigen-vectors becomes. The contribution rate is defined as N s =1  X  s n the i th largest eigenvalue. We form an n  X  N matrix U whose columns consist of N eigenvectors. To represent the data by the principal components, the data is projected on to the N-dimensional subspace by y i = U T ( x i  X  x 0 )= U T  X  x i ,where y is a vector of the N -dimensional space. The Robin system is based on our feature-extraction algorithm. The system accepts queries input by users and passes them to the Google [4] search engine. The default number of results is 50. To acquire Web page thumbnails, we use thumbshot.org [5] and filter out pages whose thumbnails are not retrieved. To provide an overview of the search results, the system places the first two values of the obtained feature vectors on a two-dimensional space map. Although only the first two values are used to represent each search result, the use of PCA means that their contribution rate is relatively high and thus the results accurately represent the extracted features.

Fig. 1 and 2 show the captured thumbnails of the search results for the input query term  X  X earch engine X  and a screenshot of the initial page of the Robin system, respectively. In Robin, each sear ch result is represented as a circle. The top-ranked page is placed in the cente r and the other pages are placed at a distance relative to the center. To give users an indication of the relationship between the position and content of the Web pages, several points are displayed as landmarks, e.g., the top-ranked pages of the search results. We conducted several experiments to evaluate the quality of the extracted fea-tures. First, we investigated whether o r not the extracted features did in fact capture the discriminating features of the Web pages in a way that was useful for browsing search results. The degree of reliability of the space in which the resulting feature vectors of the search results are mapped depends on whether or not the distances in the space actually reflect the visual as well as semantic similarity of the pages, i.e., the more similar (or dissimilar) the Web pages are, the closer (or further apart) they should be in the space. To evaluate this, we clustered the feature vectors and asked six people to validate the quality of each cluster from two different points of view, namely, visual and semantic similarity. We then examined the visual and semantic similarity by scoring the clustered pages to see how well they were correlated.
 4.1 Experimental Setup and Data Collection Ten queries, namely, gardens, movies, nature, galleries, castles, food, oil, search, business, and kids were selected from a query log collected at our laboratory over one day. We selected general query terms since they were more likely to contain subtopics and to benefit from clustering. For each query, we extracted all the thumbnails and titles from the search results as vectors. In extracting the feature vectors from the search results, we chose the number of quantifications of the thumbnail images K to be 10 and the number of eigenvectors N so that its contribution r ate exceeded 90%.

The feature vectors of the search resu lts thus obtained were clustered and shown to the evaluators. They were asked to first compare the individual pages in a cluster to a representative page of the cluster and to grade each page from 1 (not similar) to 4 (very similar) for visual and semantic similarity separately. The evaluators examined the quality of the clusters by grading the similarity of pages within the cluster and outside t he cluster (non-cluster pages). If the similarities were higher for the pages within the cluster than for those outside it, the cluster was regarded as high quality. Fo r each cluster, a feat ure vector closest to the delegate was chosen to be representative. For the clustering algorithm, we used an online clustering algorithm called the basic sequential algorithm scheme (BSAS) [6]. 4.2 Experimental Results We computed the average similarity scor es for the 10 queries for both the cluster and non -cluster pages with the representative page for each cluster. Fig. 3 shows the average scores for visual and semant ic similarity. The result shows that, globally, the pages in the clusters were visually as well as semantically more similar to their representative pages than were non -cluster pages. However, the overall similarity of the cluster pages was relatively low. Feedback from the evaluators suggested that this may hav e occurred because for some clusters the representative page was irrelevant, i.e., some clusters included pages that were similar to each other but were not similar to the representative page. It is possible that choosing the optimal representative page for a cluster would increase the similarity scores for cluster pages, but this remains an open question.
Next, we computed the correlation coeffi cient values for the visual and se-mantic similarity scores of the cluster pages. The results are shown in Fig. 4. As the figure shows, the values are relatively high, indicating that although the overall similarity scores were relatively low, there were strong relationships be-tween users X  visual impressions and the s emantic interpretat ion of Web pages in the clusters. Related work on tools for information visualization includes WebBrain [7] and Map.net [8] over visual interfaces of th e Open Directory Project (ODP) [9]. Web-Brain uses ODP categories to create a map showing the relationships between categories and sub-categories and displays the information in a graphic environ-ment as well as lists of hits. Map.net presents ODP categories as regions on a map showing their conceptual relationship to one another. Kartoo [10] visualizes search results in a network using key words extracted from Web pages to show the relationships between pages. All these studies share the idea of providing a more intuitive and convenient way of browsing Web pages. However, unlike our method, they do not make use of visual information in the process of extracting features from Web pages. We proposed a method that makes use of both the visual and textual informa-tion in Web pages to extract their charact eristic features. Our method enables seamless application of aspects of human visual perception of Web pages by treating the pages themselves as images. It also offers a way of integrating visual and textual features to extract robust features.

Our experimental results demonstrated that the features extracted by our method reflected the visual and semantic similarity of the search results relatively well. It also showed that there are strong relationships between users X  visual impressions and the semantic inter pretation of clust ers of Web pages.
