 While auto mated scoring of open-ende d written discourse has been approached by severa l groups recently (Rudner &amp; Gagne, 2001; Sher-mis &amp; Burstein, 2003), a utom ated scoring of spontaneous spoken langu age has proven to be more challenging an d com plex. Spoken lan-guage tests are still mostly scored by human rat-ers. However, several sy stems exist that scor e different aspects of s poken language; (Bernstein, 1999; C. Cu cchiarini, H. Strik, &amp; L. Boves, 1997a; Franco et al., 2000). Our work departs from previous rese arch in that our goal is to study the feasibility of automating scoring for spontaneous speech , that is, when the spoken text is not k nown in advan ce. zation of a s peaker X  s oral proficiency based on features that can be extrac ted from a sp oken re-sponse to a well defined test question by mean s of automatic speech reco gnition (ASR). W e further appro ach scoring as the construction of a mapping from a set of fe atures to a sc ore sc ale, in our case fi ve discrete sc ores fro m 1 (l east pro-ficient) to 5 (m ost proficient). The set of fea-tures and the specific mapping are m otivated by the concept of communicative com petence (Bach man, 1990; Canal e &amp; Swain , 198 0; Hy mes, 1972). This mean s that the features in the scoring s ystem we ar e developing a re meant to characterize specific com ponents of comm u-nicative co mpetence, such as mastery of pronun-ciation, fluen cy, prosodic, lexical, grammatical and pragm atical subskills. The selectio n of fea-tures is guided b y an u nderstanding of th e nature of speaking proficiency . We rely on t he scoring behavior of judges to evaluate the featu res (s ec-tion 8) as well as a convenient cri terion for evaluating the feasibility of autom ated scoring based on those fe atures ( section 7). That is, the role of hum an scorers in t his context is to pro-vide a standard for s yste m evaluations (see s ec-tion 7), as well as to validate specific features and feature c lasse s chosen by the authors (sec-tion 8) . We u se support ve ctor machines (SVMs) to determ ine how well the features recover hu-man scores. We collect perform ance data under three different conditions, where feat ures are either based on actual recognizer output or on forced alignment. (Force d alignm ent describes a procedure in speech r ecognition where the rec-ognizer is lookin g for t he most likely path throug h the Hidden Markov M odels given a transcription of the speech file by an experi-enced transc riber. This helps, e.g., in finding start and end tim es of words or phonemes.) We then use classific ation and regression trees (CART) a s a means to evaluate the rel ative i m-portance and salience of our features. When the classifi cation criterion is a hum an s core, as is the case in this study , an ins pection of the CAR T tree c an give us insights into the feature prefer-ences a hu man judge m ight have in deci ding on a score. first, we discuss relat ed work in spoken lan-guage scoring. Next, we introduce the data of our study and the speech recognizer used. In section 5 w e describe fe atures w e use d for this study . Section 6 describes the agree ment am ong raters for this data. Section 7 describes t he SVM analy sis, sect ion 8 the CA RT analy sis. This is followed b y a discussion and t hen f inally b y conclusions and an o utloo k on fut ure work. There has been previous work to charact eriz e aspects of c ommunicativ e co mpetenc e such as fluency , pron unciation, and prosod y. ( Franco et al., 2000) pre sent a system for auto matic evalua-tion of pr onunciation performance on a phone level and a sentence lev el of native and no n-native speakers of English and other languages (EduSpeak). Candidates read English te xt and a forced alignment betw een the spee ch signal and the ideal path thro ugh the Hidden Markov Model (HMM) was co mputed. Nex t, the log posterior probabilities for pronouncing a cert ain phone at a certain position in the signal were computed to achieve a local pronu nciation score. These s cores are then c ombined wit h other automatically derived m easures such as the rate of speech (n um ber of wo rds per second) or the duration of phonem es to yield global scores. 1997b)) and ( Cucchiarini et al., 1997a)) describe a sy stem for Dutch pronunciation scoring along similar lines. Their feature set, however, is more extensive and contains, in addition to l og likeli-hood Hidden Markov Mo del scores, various du-ration scores, and inform ation o n paus es, word stress, sy llable structure, and intonation. In an evaluation, they find good agree ment between hum an s cores and m achine scores. ken English (SET-10) that has the following types of items: reading, repetition, f ill-in-the-blank, o ppos ites and open-ended answers. All types except for the last are scored automatic ally and a score is reported that can be interpreted as an indicator of how na tive-like a speaker X  s speech is. In (Bernstein, DeJong, Pi soni, &amp; Townshend, 2000), an ex perim ent is perform ed to establish the generalizabilit y of the SET-10 test. It is shown that this test X  s output can suc-cessfully be mapped to the Council of Europe  X  X  Fram ework for describing second language pro-ficiency (North, 200 0). This paper further re-ports on studies done t o correlate the SET-10 with two other tests of English pr oficiency , which are sco red by hum ans and where comm u-nicative co mpetence is te sted for. Correlations were found to be between 0.73 and 0.8 8. in this paper comes fro m a 2002 trial administra-tion of TOEF LiBT X  (Test Of English as a For-eign Langua ge X  X nternet-Based Test) for non-native speakers (LanguEdg e  X ). Item responses were transcribed from the digital recor ding of each response. In all the re are 927 r esponses from 171 speakers. Of these, 798 re cordings were fro m one of five mai n test items, i dentified as P-A, P-C, P-T, P-E and P-W. The rem aining 129 response s were fro m other questions. As reported below, we use all 927 responses in the adaptation of the speech recognizer but the SVM and CART analy ses are based on the 798 re-sponses to the five te st it ems. Of the five test item s, three are independent tasks (P-A , P-C, P-T) where can didates have to talk freel y about a certain topic for 60 seconds. An example m ight be  X  X ell me about your favorite teacher.  X  Two of the test ite ms are integrated tasks (P-E, P-W) where candidates fir st read or listen to s ome ma-terial to which the y then have to relate in their responses (90 seconds speaking tim e). An ex-ample might be that the candidates li sten to a conversational argument about stud ying at home vs. study ing abroad and then are a sked to sum -marize the ad vantages and disadvantages of both points of view. 
The textual transcription of our data set con-tains about 1 23,0 00 words and the audi o files are in WAV format and reco rded with a sa mpling rate of 1102 5Hz and a reso lution of 8 bit . recognizer, we split the full data (927 re-cordings) int o a training (596) and a test set (331 recordings). For the CART and SVM analy ses we have 511 files in the train and 287 files in the eval set, summing up t o 798 . (Both data sets are subsets f rom the ASR adaptation training and test s ets, respectively .) The trans criptions of the audio fil es wer e done according to a tran-scription m anual derived from the German VerbMobil p roject (Burger, 19 95). A w ide vari-ety of disflu encies are ac counted for, such as, e.g., false st arts, repetitions, fillers, or incom -plete words. One single annotator transcribed the co mplete corpus; for the purpose of testing inter-coder agreement, a second annotator tran-scribed about 100 audio fi les, which were ran-dom ly select ed from the co mplete se t of 927 files. The disagreement between annotators, measured as word error ra te (WER = (substitu-tions + deleti ons + inserti ons) / (substitutions + deletions + correct)) was slightl y abo ve 20% (only lexical entries w ere mea sured her e). This is markedly more disagre ement than i n other corpora, e.g., in SwitchBoard (Meteer &amp; al. , 1995) where disagreemen ts in the ord er of 5% are reported, but we have non-nativ e speech from speakers at different levels of proficiency which is more challenging to transcribe. Our speech recognizer is a gender-independent Hidden Markov Model system that was trained on 200 ho urs of dictation data by native speakers of English. 32 cepstral co efficients ar e used; the dictionar y has about 30,0 00 entries. The sam-pling rate of the recognizer is 16000H z as op-posed to 11 025Hz for the LanguEdge  X  corpus. The recognizer can acco mm odat e this d ifference internally by up-sam pling t he input data stream. trained on da ta quite different from our applica-tion (dictation vs. spontan eous speech and native vs. non-nativ e speakers) we adapted the sy stem to the LanguEdge  X  corp us. We were able to increas e wor d accura cy on the unseen test s et from 15% before adaptati on to 33% in the fully adapted m odel (both acoustic and language model adaptation). Our feature set, partly ins pired by (Cucchiarini et al., 1997a), focuses on low-level fluency fea-tures, but also includes some f eatures r elated to lexical sophistication and to content. Th e feature set also st ems, in part, fr om the writt en guide-lines used by human raters for scoring this data. The features can be categ orized as follows: (1) Length m easures, (2) lexical sophistication measures, (3) fluenc y m easures, (4) rate meas-ures, and (5) content m easures. Table 1 renders a complete list of the features we co mputed, along with a brief explanation. We do not claim these features to provide a full characteri zation of comm unicati ve co mpetenc e; they should be seen as a first step in this direction. The goal of the rese arch is to gradually build such a set of fea-tures to eventually achieve as large a coverage of co mm unic ative co mpetence as possi ble. The features are com puted based on the outp ut of the recognition engine based on either forc ed align-ment or on actual recogni tion. The out put con-sists of (a) start and end time of every token and hence potential silence in between (used for most features); (b) identit y of filler words (for disfluency -related fe atures) ; and (c) wor d iden-tity (for conte nt features). Lexical counts and length measures Segdur Total duration in seconds of all the utterances Nu mutt Nu mber of utterances in the response Nu mwds Total num ber of word form s in the speech sam ple Nu mdff Nu mber of disfluencies (fil lers) Nu mtok Nu mber of tokens = Nu mwds+Nu mdff Lexical sophistication Types Nu mber of unique word forms in the speech sa mple Ttratio Ratio Types/Nu mtok (t ype-token ratio, TTR) 
Fluency mea sures (based on pause informat ion) Table 1: List of features wi th definitions. The training and scoring procedures followed standard prac tices in large scale te sting. Scorer s are trained to apply the scoring standards that have been previousl y agreed upon by the devel-opers of the test. The trai ning takes the for m of discussing multiple instances of responses at each score level. The scor ing of the responses used for training ot her raters is done by m ore experienced scorers work ing closely with the designers of the test. were rated once by one of several expert raters, which we call Rater1. A second rating was ob-tained for ap proxim ately one half (454) of the speaking sa mples, which we c all R ater2. We computed the exact agre ement for all Rater1-Rater2 pairs for all five test item s and report the results in the last colu mn of Table 2. Overall, the exact agreement was about 49% and t he kappa coefficient 0.34. These are rather low num ber s and certainly dem onstrate the difficult y of the rating task fo r humans. Int er-rater agreement for integrated tasks is lower than for independent tasks. W e conjecture that this is relat ed to the dual nature o f scoring i ntegrated tasks: for one , the communicative co mpetence per se needs to be asses sed, but on the other hand so does the correct interpretation of the written or auditor y stimulus mat erial. The lo w agre ement in general is also under standable since the num ber of fea-ture dim ensions that hav e to be m entally inte-grated pose a significant cognitive load for judges. 1 As we have mentioned ea rlier, the rati onale be-hind usi ng s upport vector machines for score prediction is to y ield a quantitative analy sis of how well our features wo uld work in an actual scoring sy stem , measur ed against hu man expert raters. The choice of the particular cl assifier be-ing SVMs was due to their superior performance in m any mac hine learning tasks. troduced b y (Vapnik, 199 5) as an instantiation of his appr oach to m odel regularization. T hey attem pt to so lve a multivariate discr ete classifi-cation problem wh ere an n-dim ensional hyper-plane separates the input vectors into, in the simplest c ase , two distinct classe s. The optim al hy perplane is selected to mini mize the classifi-cation error on the training data, while maintain-ing a maximally large margin (the distance of any point from the separ ating hyperplane). train data, one for each of the five test ite ms. Each model has two versions: (a) based on forced alignment with th e true r eferen ce, repr e-senting the case with 100% word accuracy (align), and ( b) based on t he actual recognition output hy potheses (hy po). The SVM models were test ed on the eval data set and th ere wer e three test co nditions: (1) both training and test conditions derived from fo rced align ment (align-align); (2) m odels trained on f orced alignm ent and evaluated based on actual recognition hy-potheses (align-hy po; this represented th e realis -tic situation t hat while human transcrip tions are made for the training set, they woul d turn out to be too costl y when the s ystem is running con-tinuousl y); a nd (3) b oth t raining and evaluation are based on ASR output in recogniti on m ode (hy po-hy po ). set of SVMs with var ying cost factors, ranging from 0.01 to 15, and three different kernels: ra-dial basis fu nction, and pol ynom ial, of second degree and of third degree. We sel ected the best perform ing m odels measured on the train set and report results with these models on the eval set. The cost factor for al l three confi gurations varied between 5 and 15 am ong the five test item s, and as best kernel we found t he radial basis function in al most all c ase s, ex cept for some poly nomial kernels in the h ypo-hypo con-figuration Table 2 shows the results for the SVM analy sis as w ell as a baseline mea sure of agree ment and the inter rater agreement. The baseline refers to the expect ed level of a greement with Rater1 by sim ply assigning t he mode of the dis tribution of scores for a given question, i.e., to alway s assign the most frequently occurring score on the train set. Table 2 also reports the agree ment between train ed raters. As can be s een the hu-man agreement is consistently higher than the mode agree ment but the difference i s less for the integrated questions suggesting that hum ans scorers found those questions more challenging to score consistently . results for the perfect agre ement betw een a s core assigned by the SVM de veloped for that test question and Rater1 on t he eval corpus, which was not used in the developm ent of th e SVM. We observe that for the align-align configura-tion, accurac ies are all clearly better than the mode ba selin e, except for P-C, which has an unusuall y sk ewed score distributio n a nd t here-fore a rather high m ode baseline. In the align-hy po case, where SVM models wer e built based on features derived fro m ASR forced alignment and where th ese models were t ested usi ng ASR output i n recognition m ode, we see a general drop i n perf ormance  X  again except f or P-C  X  which is to be expected as the training and test data were derived in differ ent way s. Fi nally, in the hypo-h ypo configurati on, using AS R recog-nition out put for bot h train ing and testing, SVM model s are, in com pari son to the align-align models, im proved for the two integrated tasks but not for the independe nt tasks, again except for P-C. The SVM cl assif ication ac curacies for the integrate d tasks ar e i n the range of hum an scorer agree ment, which indicates that a per-formance cei ling may ha ve been rea ched al-ready . These results suggest that the r ecover y of scores is more feasible for integrated rather than independent tasks. Howev er, it is also t he cas e that hum an scorers had more difficult y with the integrated ta sks, as discu ssed in the previous section. 
The fact that the classifi cat ion perform ance of the h ypo-hypo m odels is not greatl y lower than that of the align-align m odels, and in some ca ses even higher ---and that with the relatively low word accuracy of 33%---, leads to our c onjecture that this coul d be due to t he majority of features being based on measur es which do not require a correct word identity such as mea sures of rate or pauses. 
In a recent study (Xi, Zec hner, &amp; Bejar , 2006) with a sim ilar speech corpus we found that while the hypo-h ypo models are better than the align-align models when using features related to flu-ency , the converse is true when using word-based vocabulary features. 8.1 Classification and regression tre es Classifi cation and regression trees (CAR T trees) were introduced by (Brei man, Fried man, Ol-shen, &amp; Stone, 1984). The goal of a classifica-tion tree is to classify the data such tha t the dat a in the ter minal or cla ssific ation nodes is as pure as possible meaning all the cas es have the sa me true classifi cation, in the present cas e a score provided b y a hu man rater, the variable Rater1 above. At t he top of the tr ee all the data is avail-able and is sp lit into two groups based on a split of one of the features av ailable. Each split is treated in the same manner until no furt her splits are possible, in which case a term inal node has been reached. For each of t he five test it em s described above we esti mated a clas sificati on tree using as inde-pendent variables the f eatu res de scribed in Table 1 and as the dependent va riable a human score. The trees were built on the train set. Table 3 shows the distribution of features in th e CART tree nodes of the five test i tems (row s) based on feature cl ass es (colu mns). For P-A, for exa m-ple, it can be seen that three of th e fe ature classes h ave a count greater than 0. The last colum n shows the nu mber of cla sse s appearing in the tree and the num ber of total fe atures, in parentheses. The P-A tre e, for exa mple has six features fro m three class es. The last row su m-marizes the num ber of test item s that relied on a feature class and the num ber of fe atures fro m that class a cross all five test ite ms, in parenthe-sis. For exam ple, Rate and Length w ere present in every test item and lexical sophistication was present in all but one test ite m. The table sug-gests that across all t est item s there was good coverage of feature clas ses but length was espe-cially well re presented. T his is to be expected with a group heterogeneous in speaking profi-ciency . The length features often were used to classify students in the lower score s, tha t is, stu-dents who could n ot m anage to speak suffi-ciently to be r esponsive to t he test item . We successfu lly adapted an off-the-shelf speech recognition engine for the purpose of asse ssing spontaneous speaking pro ficiency . By acoustic and language model adaptati on, we were able to markedly inc rease our speech recognition en-gine X  s word accuracy , from initially 15% to eventually 33%. Although a 33% recognition rate is not hi gh b y current standards, th e hurdles to hi gher recognition are significant, i ncluding the fact that the recogni zer X  X  acoustic model was originall y trai ned on q uite different data, and the fact that our data is based on highl y accented speech fro m non-native speakers of English of a range of pr oficiencies, wh ich are harder to rec-ognize than n ative speakers. Our goal in this research has been to develop model s for au tomatically scoring communicative competenc e in non-native speakers of English. The approach we took is to com pute features from ASR output that may eventually serve a s indicators of communicat ive com petence. We evaluated those feature s (a) in quantitative re-spect by usin g SVM models for score p rediction and (b) in qualitative respect in term s of their roles in assigning scores based on a human crite-rion by m eans of CART analy ses. els that desp ite low word ac curacy , w ith ASR recognition a s a basis for training and testing, scores near i nter-rater agr eement level s can be reached for t hose ite ms th at include a l istening or reading passage. Wh en si mulating perfect word accurac y (in the alig n-align configuration), 4 of 5 test ite ms achiev e scoring ac curacie s above the m ode baseline. These results are very encouraging in the li ght t hat we are continui ng to add features to the m odels on various levels of speech proficiency . 
CART tr ees have the advantage of bei ng in-spectable an d interpretabl e (unlike, e.g., neural nets or suppo rt vector machines with no n-linear kernels). It is easy to trace a path fro m the root of the tree to any leaf node and record the final decisions ma de along the way . We looked at the distribution of features in these CA RT tree nodes (Table 3) and found that all the differen t categories of features most classes occurred in t he nodes of the respec-tive CART trees (with a mini mum of 3 out of 5 class es). This paper is concerned with explor ations into scoring spok en language test items of non-native speakers of English. We demonstrated that an ex-tended feature set co mprising features related to length, lexic al sophistication, fluenc y, rate and content could be used to predict hum an scores in SVM models and to ill uminate their distribution into five different clas ses by m eans o f a CART analy sis. train the acoustic and la nguage models of th e speech recog nizer directly from our corpus; we are additionally planning to use auto matic speaker ad-aptation and to evaluate its benefits. Furtherm ore we are a ware that, m aybe with the exception of the classes related to fluenc y, rate and lengt h, o ur fea-ture set is as of yet quite rudim entary and will need significant expansion in or der to obtain a broader coverage of communicativ e co mpetenc e. proving speech recognition, and on si gnificantl y extending the feature set s in different categories. The eventual goal is to have a well-balanced m ulti-component scoring sy ste m which can both rate non-native speech as closely as possible according to communic ative criteri a, as w ell as p rovide use-ful feedback for the langua ge learner. 
