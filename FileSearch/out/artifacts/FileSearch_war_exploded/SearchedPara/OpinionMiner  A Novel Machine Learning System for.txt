 Merchants selling products on the Web often ask their customers to share their opinions and hands-on experiences on products they have purchased. Unfortunately, reading through all customer reviews is difficu lt, especially for popular items, the number of reviews can be up to hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision. Th e OpinionMiner system designed in this work aims to mine customer reviews of a product and extract high detailed product entities on which reviewers express their opinions. Opinion expressions are identified and opinion orientations for each recognized product entity are classified as positive or negative. Different from previous approaches that employed rule-based or statistical techniques, we propose a novel machine learning approach built under the framework of lexicalized HMMs. The approach naturally integrates multiple important lingui stic features into automatic learning. In this paper, we de scribe the architecture and main components of the system. The evaluation of the proposed method is presented based on processing the online product reviews from Amazon and other publicly available datasets. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Information filtering. I.2.7 [ Natural Language Processing ]  X  Text analysis Algorithms, Design, Experime ntation, Human Factors Opinion Mining, Sentiment Analysis, Lexicalized HMMs As e-commerce is becoming more and more popular, it has become a common practice for online merchants to ask their customers to share their opinions and hands-on experiences on products they have purchased. Such information is highly valuable to manufacturers, online advertisers and potential customers. Unfortunately, reading through all customer reviews is difficult, especially for popul ar items, the number of reviews can be up to hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. This paper aims to design a system that is capable of extracting, learning and classifying product entities and opinion expressions automatically from product revi ews. A novel lexicalized HMM-based approach is proposed and an opinion mining and extraction system, OpinionMiner , has been developed. Our objective in this system is to answer the following questions: given a particular product, 1) how to automatically extract potential product entities and opinion entities from the reviews? 2) how to identify opinion sentences which describe each extracted product entity? and 3) how to determine opinion orientation (positive or negative) given each recognized product entity? Different from previous approaches that have mostly relied on rule-based techniques [3 , 4] or statistic information [10, 13], we propose a new framework that naturally integrates multiple linguistic features (e.g., part-of-speech, phrases X  internal formation patterns, a nd surrounding contextual clues of words/phrases) into automatic learning. The experimental results demonstrate the effectiv eness of the proposed approach in web opinion mining and extraction from online product reviews. Our contributions in this paper include: (1) a proposal of a new machine learning framework that naturally integrates multiple linguistic features into web opinion mining and extraction; (2) a proposal of a unified and self-adaptive tagging approach including use of dictionary, token transformations and bootstrapping; (3) a proposal of an effective approach of extracting complex product entities, opinion expressions, as well as infrequently mentioned entities from reviews; (4) a proposal of a practically effective system design. discusses related work. Section 3 describes in detail the system framework and each system component. We report in section 4 our experimental results and give our conclusions on this work in section 5. Opinion analysis has been studied by many researchers in recent years. Two main research directions are explored, i.e., document level opinion mining and featur e level opinion mining. In document level, Turney [3] presented an approach of determining document X  X  polarity by calculating the average semantic orientation (SO) of extracted phrases. SO was computed by using pointwise mutual information (PMI) to measure the dependence between extracted phrases and the reference words  X  X xcellent X  and  X  X oor X  by using web search hit counts. One year later Turney and Littman [4] further expanded their work by using cosine distan ce in latent semantic analysis (LSA) as the distance measur e. Dave, Lawrence and Pennock [5] classified reviews on Amaz on by calculating scores using normalized term frequency on uni-g ram, bi-gram and tri-gram with different smoothing technique s. Das and Chen [8] studied document level sentiment polarity classification on financial documents. Pang, Lee and Vaithyanathan [6] used several machine learning approaches to classify movie reviews and in [7], they further studied another machine learning approach based on subjectivity detection a nd minimum cuts in graphs for sentiment classification of movie reviews. Our work is different from these as their goal is to determine the sentiment of documents while ours is to perfo rm extraction and classification on entities. Another difference is they were not focused on features being commented on. In feature level opinion mining, Zhuang, Jing and Zhu [10] classified and summarized movi e reviews by extracting high frequency feature keywords and high frequency opinion keywords. Feature-opinion pairs were identified by using a dependency grammar gr aph. However, it used a fixed list of keywords to recognize high frequency feature words, and thus the system capability is limited. Popescu and Etzioni [11] proposed a relaxation labeling approach to find the semantic orientation of words. However, their approach only extracted feature words with frequency greater than an experimentally set threshold value and ignored low frequency feature words. Hu and Liu [9] proposed a statistical approach capturing high frequency feature words by using association rules. Infrequent feature words are captured by extracting known opinion words X  adjacent noun phrases. A summary is generated by using high frequency feature words (the t op ranked features) and ignoring infrequent features. Ding, Liu and Yu [12] further improved Hu X  X  system by adding some rule s to handle different kinds of sentence structures. However, the capability of recognizing phrase features is limited by the accuracy of recognizing noun-group boundaries. Their approach also lacks an effective way to address infrequent features. In this work, we propose a new machine learning framework that naturally integrates linguistic features into automatic learning. Complex product-specific features (which are possible low frequency phrases in the reviews) are effectively identified, and new potential product and opinion entities are discovere d based on the patterns the classifier has seen from the training data. Another related research area is the Part-of-Speech (POS) Tagging and Named Entity Recognition (NER) problems. The task of POS tagging is the process of marking up the words in a text (corpus) as corresponding to a particular part-of-speech, such as noun and verb . The task of NER is identifying and classifying person names, orga nization names, and etc. In opinion mining, similar tasks need to be performed, such as identifying different entity names, classifying entities into appropriate categories, and fu rther determining the opinion word/phrase X  X  polarity. To correlate the web opinion mining task with POS Tagging and NE R may well be a significant contribution in itself in this work. Motivated by [1, 2] which employed a lexicalized HMM approach in Korean part-of-speech (POS) tagging and Chinese named entity tagging respectively, we propose in this paper a hybrid approach integrating POS information with the lexicalization technique under the HMM framework. Figure 1 gives the architectural overvie w of our opinion mining system and each system component is detailed subsequently. In our work, we have defined four entity types as shown below (a digital camera is used as an example): Table 1. Definitions of entity types and examples 
Components: Physical objects of a camera, including the 
Functions: Capabilities provided by a camera, e.g., 
Features: Properties of components or functions, 
Opinions: Ideas and thoughts expressed by reviewers Correspondingly, we have further defined the basic tag set to identify each above entity type, which is given below. 
Tag set Corresponding Entities &lt;PROD_FUNCTION&gt; Function entity &lt;OPINION_POS_EXP&gt; Explicit Positive Opinion &lt;OPINION_NEG_EXP&gt; Explicit Negative Opinion &lt;OPINION_POS_IMP&gt; Implicit Positive Opinion &lt;OPINION_NEG_IMP&gt; Implicit Negative Opinion In general, an entity can be a single word or a phrase. In other words, a word may present itself as an independent entity or a component of entity. Therefore, a word w in an entity may take independent entity; (ii) w is the beginning component of an entity; (iii) w is at the middle of an entity; (iv) w is at the end of an entity. We adopt a pattern tag set proposed in [2] to denote the above four patterns, which is shown in table 3: Both the basic tag set and pattern tag set are used to represent each word X  X  entity type and pattern (referred to as a hybrid tag representation [2]) in the following format: t opinion sentence can be represented as &lt;t t p &gt;w 1 &lt;/t b t p &gt;......&lt;t b t p &gt;w n &lt;/t where w i stands for a single word. Patterns of background words are considered as independent entities . This hybrid-tag labeling method is applied to all the training data and system outputs. The following example illustrates the hybrid tag and basic tag representations of an opinion sentence:  X  X  love the ease of transferring the pictures to my computer. X  Hybrid tags: &lt;BG&gt;I&lt;/BG&gt; &lt;OPINION_POS_EXP&gt;love&lt;/OPINION_POS_E XP&gt; &lt;BG&gt;the&lt;/BG&gt; &lt;PROD_FEAT-BOE&gt;ease&lt;/PROD_FEAT-BOE&gt;&lt;PROD_FEAT-MOE&gt;of&lt;/PROD_FEAT-MOE&gt;&lt;PROD_FEAT-MOE&gt;transferring&lt;/PROD_ FEAT-MOE&gt;&lt;PROD_FEAT-MOE&gt;the&lt;/PROD_FEAT-MOE&gt;&lt;PROD_FEAT-EOE&gt;pictures&lt;/PROD_FEAT-Basic tags: &lt;BG&gt;I&lt;/BG&gt; &lt;OPINION_POS_EXP&gt;love&lt;/OPINION_POS_E omputer&lt;/BG&gt; Different from traditional Hidde n Markov Models (HMMs), in our work, we integrate linguistic features such as part-of-speech and lexical patterns into HMMs. An observable state is represented by a pair ( word i , POS(word i ) ) where POS(word represents the part-of-speech of word i . The task is then described as follows: Given a sequence of words W= w w 2 w 3 ...w n and corresponding parts of speech S = s the task is to find an appropriate sequence of hybrid tags  X  = that maximize the conditional probability P(T|W,S) such that  X  S W T P T = (1) By taking Bayes law, we can rewrite equation (1) as = (2) Since the probability P(W, S) remains unchanged for all candidate tag sequences, we can disregard it. Thus, we have a general statistical model as follows: Theoretically the above general model can provide the system with a powerful capacity of disambiguation. However, in practice this general model is not computable for it involves too many parameters. Two types of approximations are employed to simplify this general model. Th e first approximation is based on the independent hypothesis used in standard HMMs. First-order HMMs is used in view of data sparseness, i.e., P(t i | t P(t i | t i-1 ) . The second approximation employs the lexicalization technique together with POS where three main hypotheses are made: 1. The assignment of current tag t i is supposed to depend not 2. The appearance of current word w i is assumed to depend 3. The appearance of current POS s i is supposed to depend With a view to the issue of data sparseness, we set J=K=L=1. Based on these assumptions, the general model in equation (3) can be rewritten as: T  X  (4) Maximum Likelihood Estimation (MLE) is used to estimate the parameters in equation (4). For instance, P(s i |w estimated as: Note that the sum of counts of C(w i-1 , t i , s) for all s is equivalent equation (4) can be computed similarly. If a large training corpus is available, the parameters in equation (4) can be easily estimated using the MLE t echnique. To account for zero probabilities for any cases that are not observed in the training data, we employ the linear interpolation smoothing technique to smooth higher-order models with their relevant lower-order models, or to smooth the lexi calized parameters using the related non-lexicalized probabilities, namely Where  X  ,  X  and  X  denote the interpolation coefficients. To cover more language phenome non and large domains, in the training step, we have employed a new technique to automatically propagate information of each expert tagged entity to its synonyms, antonyms, similar words and related words. Figure 2 illustrates an example. As mentioned above, an entity can be a single word or a phrase. By expanding each combinations can be formed. In Figure 2, the sentence  X  X ood picture quality X  is an expert tagged opinion sentence. During the training course, the system looks up synonyms and antonyms for opinion entities. The tag of the original opinion entity  X  good  X , &lt;OPINION_POS_EXP&gt; ( positive opinion ), gets propagated to each synonym of  X  good  X  (red box on the left in Figure 2). The negative tag &lt;OPINION_NEG_EXP&gt; gets propagated to  X  good  X  X  X  antonyms (dark red box on the bottom left). Similarly, for each single word in other entity types, similar words and related words are looked up. The tag of the original word gets propagated to each newly discovered related word (blue boxes). Using this e xpansion, a number of bi-gram combinations (green arrows) can be obtained. In this example, there are several possible instances derived from  X  X ood picture quality X  , such as  X  X ecent picture quality X ,  X  X oor image quality X , and etc. Obviously, only  X  X ood picture quality X  is the expert tagged truth data. All other combina tions generated from expansion might contain noise. To reduce the noise impact, a confidence weight is given to each bi-gram combination when computing the MLE values in equation 4. We empirically set W W = 0.01 for expert tagged combinations and combinations obtained from expansion, respectively. 
Figure 2. Information propagatio n using entity X  X  synonyms, A dictionary program has been built to return an input word X  X  synonyms, antonyms, similar words and related words using Microsoft Word X  X  thesaurus. Th e reason we decided not to use WordNet 1 for this purpose is that after experimenting with WordNet, we found it returned too many less commonly used synonyms and antonyms. However, most reviewers are prone to use commonly used words to express their opinions. Expanding entities using less frequently used terms creates more noise and affects the classifier X  X  performance. Another problem with many entities is that they may be overly specific. For example,  X  X  love its 28mm lens X  and  X  X  love its 300mm lens X . Both sentences talk about lens. They could be ideally grouped together as  X  X  love its Xmm lens X  where X can be any numerical value. This transformation generalizes the information contained in sentences and is useful in solving the problem of sparseness in the training data. In our framework, we use this transformation to handle high detailed information in sentences such as Model Number , Focal Length , and ISO (the sensitivity of image sensor). The following three transformations are performed. Given the transformation table 4: Regular Expression Examples 1 ^'|[`";,!?:()\[\]]|\.+$|'$ Match ' ` " ; , ! ? : ( ) [ ] . 2 \d+|\d+\.\d+|\d+\-\d+ Match 3, 3.5, 3-4 3 [A-Z-]+\d+([A-Za-1. Remove any punctuation that ma tches regular expression 1. 2. Transform any token that matc hes regular expression 2 but does not match regular expre ssion 3 to symbol  X #NUM# X . 3. Transform any token that matc hes regular expression 3 to symbol  X #MODEL# X . http://wordnet.princeton.edu/ Step 1 removes any unnecessary punc tuations in a token. Step 2 generalizes all numerical expr essions except model numbers. For the previous example, both opinion sentences will be transformed into  X  X  love its #NUM#mm lens X  . Step 3 generalizes product model numbers . This transformation step was applied to both the tr aining and classification. Based on the above model, th e decoding algorithm aims at finding the most probable sequence of hybrid tags for a given sequence of known words and corresponding parts of speech. As discussed above, a hybrid tag of an observable word involves a category tag and a pattern tag. The candidate hybrid tags of a known word are a combination of its candidate category tags and its candidate pattern ta gs. The Viterbi algorithm is employed to score all candidate hybrid tags with the proposed language models, and then search the best path that has the maximal score. This step identifies opinion sentences in the reviews. Opinion sentences in our work are defined as sentences that express opinions on product related entities. In our system, the following two types of sentences are not considered as effective opinion sentences. 1. Sentences that describe pr oduct related entities without 2. Sentences that express opinions on another product The following step further classi fies opinion orientation given each identified product entity. Due to the complexity and flexibility of natural language, opinion orientation is not simply equal to opinion entity (word/phrase) X  X  orientation. For example,  X  X  can tell you right now that the auto mode and the program modes are not that good. X  The reviewer expressed his negative comment on both  X  X uto mode X  and  X  X rogram modes X  even in the presence of the opinion entity (word  X  good  X ) in the sentence. To determine opinion orientation, for each recognized product entity, we first search its matching opinion entity, which is defined as the nearest opinion entity identified by the tagger . The orientation of this matching opinion entity becomes the initial opinion orientation for the corresponding product entity. Next, natural language rules reflecting sentence context are employed to address specific la nguage constructs, such as the presence of negation words (e.g., not), which may change the opinion orientation. Specifically, we check the presence of any negation words (e.g., not, didn X  X , don X  X ) within five-word distance in front of an opinion entity and changes opinion orientation accordingly, except 1. A negation word appears in front of a coordinating 2. A negation word appears after the appearance of a product 3. A negation word appears befo re another negation word. The coordinating conjunctions such as  X  but  X  and prepositions such as  X  except  X  and  X  X part from X  are addressed as follows: if opinion entity is in front of the corresponding product entity and prepositions such as  X  X ut/excep t X  appear between opinion entity and product entity, then the opinion orientation for the corresponding product entity is upda ted with the opposite of its initial orientation. We used Amazon X  X  digital camera reviews as the evaluation dataset. The reviews for the first 16 unique cameras listed on Amazon during November 2007 were crawled. For each review page, each individual review content, model number as well as manufacturer name were extracted from the HTML documents. Sentence segmentation was applied to the data and the information was stored as plai n text documents, which we call review documents . POS parsing was applied to each review document. We used the Part-of-Speech tagger designed by Stanford NLP Group 2 and default settings were used. After downloading and pre-proce ssing, there were altogether 1728 review documents obtained. We separated the documents into 2 sets. One set (293 doc uments for 6 cameras) was manually tagged. Opinion senten ces were identified and product entities, opining entities and opinion orientations were manually labeled using the tag sets de scribed in section 3.1. The remaining documents (1435 docum ents for 10 cameras) were used by the bootstrapping process (described next) to self-learn new vocabularies. Labeling training documents manually is a labor intensive task. Thus, it would be nice if the system can identify new vocabularies automatically by using what it has learned. To achieve this, we have designe d a bootstrapping approach which can extract high confidence data through self-learning. The process is shown in Fig. 3 and composed of the following steps: 1. First, the bootstrapping program creates two child 2. We split the training documents into two halves, t 3. Each worker first trains its own HMM classifier based on 4. As two workers X  training documents are different from http://nlp.stanford.edu/software/lex-parser.shtml 5. A hash value is then calculated for each extracted opinion 6. Master then randomly splits the newly discovered data One characteristic of this bootstrap process is each HMM classifier always has its set of unique training data. Additionally, in each bootstrap cycle, both HMM classifiers X  training data are different from the previous cycle. As mentioned above, the review documents for 6 cameras were manually labeled by experts. We c hose the largest four data sets (containing 270 documents) and performed a 4-fold cross-validation. The remaining revi ew documents for 2 cameras (containing 23 documents) were used for training only. The bootstrap document set (containing 1435 documents for 10 cameras) was used by the bootstrapping process to extract high confidence data through self-learning (newly discovered high confidence data were then added into the original training set in each iteration). Finally, our best classifi er was trained based on the accumulated truth (and high confidence) data collected from the original training set and boot strap data set, and was then applied to our test data and evaluated against the baseline. The effectiveness of the proposed framework was evaluated by measuring the recall, precision and F-score of extracted entities, opinion sentences and opinion orientations, respectively. The system performance is evaluated by comparing the results tagged by the system with the manually tagged truth data. Only an exact match is considered as a correct recognition in our evaluation. For entity recognition, this means the exact same word/phrase is identified and classified correctly as one of four pre-defined entity types. Furthermore, each identified entity should occur in the same sentence, same position and same document as compared with the truth data. For opinion sentence extraction, exact match means the exact same sentence from the same document is identified compared with the truth data. For opinion orientation classification, exact match means the exact same entity and entity type are identified with correct orientation (positive or negative). We have designed and implem ented a rule-based baseline system motivated by [3] and [9] X  X  approaches. [3] describes a document level opinion mining system . It uses a number of rules to identify opinion-bearing words. In our baseline system, the rules shown in Table 5 were used to extract product entities and opinion-bearing words. This was accomplished by searching for any nouns and adjectives matc hing the rules. Matching nouns (considered as product entities) and matching adjectives (considered as opinion words) were extracted. The corresponding sentences were identified as opinion sentences. In the next step, identified adjectives X  semantic orientations were determined. We used twenty five commonly used positive adjectives and twenty five commonly used negative adjectives as seeds. By using the bootstrapping technique proposed in [9], we expanded these two seeds lis ts by searching synonyms and antonyms for each seed word. Newly discovered words were added into their corresponding seeds lists. This process was repeated until no new words were discovered. As semantic orientation of each list of adjective words is known, the orientations of extracted adjectives by the system can be determined by checking the existen ce of these words in the lists. Table 5. Baseline rules for extracting product entities and opinion-bearing words 1 JJ NN or NNS Anything 2 RB, RBR 3 JJ JJ NN or NNS 4 NN or In addition to using the data set downloaded from Amazon.com, the publicly available Hu and Liu X  X  corpus [9] was also used as evaluation data. Their corpus is for product review summarization, which is closely related to our work. However, there are two major differences between Hu X  X  task and ours. 1) Hu X  X  work is focused on summarization where extracting generic terms and frequent term s are their major concern, whereas our work is focused on extracting high detailed product entities and both frequent and infrequent entities are considered equally important; 2) Other than identifying desired entities, we further classify these entities in to different categories. This could lead to the automatic construction of a hierarchical relationship (such as the Entity-Relationship schema) from free texts between product entities and their associated attributes. Due to these differences, some extra work was done on Hu X  X  corpus. First, Hu X  X  corpus did not include any entity type information. We manually labeled the desired entity types so that the evaluation program can measure the system performance for each entity type. Second, if reviewers use specific terms (e.g., optical viewfinder) instead of generic terms (e.g., viewfinder), specific terms are considered as unique correct terms. In other words, identifying entities capturing finest details of the product is one of our aims. The following example illustrates the major difference between Hu X  X  labeled data and ours.  X  X he menus are easy to navigate and the buttons are easy to use. X  Hu X  X  labels: [menu|+] [button|+] Our labels: [menu|+ [navigate|+]] [buttons|+ [use|+]] Each bracket represents an entity (entity type is not shown). The  X + X  symbol represents positive polarity. In our corpus,  X  use  X (usability) as a feature of  X  buttons  X .  X  Menus  X  and component , respectively. The detailed evaluation results are presented in Table 6 and Table 7. As a post analysis, the proposed machine learning framework performs significantly better than the rule-based baseline system in terms of entity extraction, opinion sentence recognition and opinion polarity classification. Through manual inspection, we observed our approach effectively identified highly specific product entities and opinion expressions (usually complex phrases) and self-lear ned new vocabularies based on the patterns it has seen from the training data (the examples are shown in Table 8). Another observation is in addition to effectively extracting frequent entities, the system also excels in identifying important but infrequently mentioned en tities, which was under-analyzed or ignored by previously proposed methods. In this work, we  X  battery/memory compartment  X  and  X  focus assist light  X  (identified by the system but only occur once or twice in the dataset), could be useful produc t descriptors when answering user X  X  specific queries in many web applications (e.g., recommender systems). In such applications, frequent generic features might be satisfied by most candidate products. However, infrequent product-specific features might be better able to differentiate different products (e.g. recommending a list of cameras which have positive feedbacks on  X  X ids mode X ). Additionally, the user X  X  preferen ces could be highly specific. For example,  X  automatic white balance  X ,  X  custom white balance  X  and  X  preset white balance  X  represent different user preferences and a recommender system should be able to distinguish among these to answer the user X  X  specific queries. Such information can be effec tively extracted by the proposed learning system. In this paper, we also propose the potential non-noun product entities, such as  X  X ngineered X  and  X  X perated X  (an example is shown below). These non-noun entities were ignored by previously proposed approaches which were based on the assumption that product entities must be noun or noun phrases. Our system can well identify these overlooked product entity information. Operated = 2 ( X = X X  represents the number of occurrences of an entity) The &lt;PROD_FUNCTION&gt; zoom &lt;/PROD_FUNCTION&gt; is &lt;OPINION_POS_EXP&gt; easily &lt;/OPINION_POS_EXP&gt; &lt;PROD_FEAT&gt; operated &lt;/PR OD_FEAT&gt; without looking. The/DT zoom/NN is/VBZ easily /RB operated/VBN without/IN looking/VBG Figure 4 shows the OpinionMiner system interface and the format of answers we would like to provide for the user. In this interface, opinion sentences are identified; product related entities and opinion related entities appearing in opinion sentences are recognized and highlighted using different colors (corresponding to different entity types such as &lt;PROD_FEAT&gt;, &lt;PROD_FUNCTION&gt; and &lt;OPINION_POS_EXP&gt; ). In this paper, a novel and robus t machine learning system is designed for opinion mining and ex traction. The model provides solutions for several problems that have not been addressed by previous approaches. Specifically,  X  The model naturally integrates multiple linguistic features  X  The system can predict new potential product and opinion  X  Complex product entities and opinion expressions as well  X  A bootstrapping approach combining active learning The existing problems are: (1) People like to describe a long story about their experiences. For example, some people like to describe how bad/good their former cameras were. This influences the system performance on some camera reviews in the experiments. We are looking into this issue further. (2) Some researchers suggested pronoun resolution. We have applied pronoun resolution to each sentence in our experiments. However, the results are not satisfying. We found pronoun resolution caused too many false positives. After a closer look at the data, we consider sentence classification might be needed to determine which sentences s hould perform pronoun resolution. This is also left for future work. [1] Lee, S. Z., Tsujii, J., and Rim, H. C. 2000. Lexicalized [2] Fu, G. and Luke, K. K. 2005. Chinese Named Entity [3] Turney, P. D. 2002. Thumbs up or Thumbs Down? [4] Turney, P. D. and Littman, M. L. 2003. Measuring praise [5] Dave, K., Lawrence, S., and Pennock, D. M. 2003. Mining [6] Pang, B., Lee, L., and Vaithyanathan, S. 2002. Thumbs up? [7] Pang, B. and Lee, L. 2004. A sentimental education: [8] Das, S. and Chen, M. 2001. Yahoo! for Amazon: [9] Hu, M. and Liu, B. 2004. Mining and Summarizing [10] Zhuang, L., Jing, F., and Zhu, X. 2006. Movie Review [11] Popescu, A. and Etzioni, O. 2005. Extracting Product [12] Ding, X., Liu, B., and Yu, P. S. 2008. A Holistic Lexicon-Component Entities (%) 
