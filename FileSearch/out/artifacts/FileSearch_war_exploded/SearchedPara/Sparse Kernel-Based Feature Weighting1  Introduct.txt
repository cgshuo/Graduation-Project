 Reducing the dimensionality of the raw data is an important preprocessing step in data mining. It plays a fundamental role in practices for a variety of reasons [4]. In the literature, there are two major types of methods: while feature selection (FS) methods identify a subset of us eful features and discard others, feature extraction (FE) approaches construct new features out of the original ones.
Traditional feature selection algorithms are conducted in the original input space. Therefore they cannot satisfactorily capture the inherent nonlinear struc-tures in the data. Quite r ecently, Cao et al [2] propos ed a kernel-based feature weighting algorithm based on the Relief algorithm [5,7]. By conducting feature selection in the kernel space, Kernel-Relief (K-Relief) [2] actually bridges the gap between FS and FE, i.e., it achieves the purpose of FE through FS in a nonlinear space. Although the idea is quite interesting, the problems occurred in their mathematic derivations and numerical comparisons strongly weakened the reliability of their study. In addition, an practical shortcomings of K-Relief is that the nonlinear features constructed by this algorithm has a non-sparse kernel expression, which could become pr ohibitive in practice especially when large scale problems are concerned (see Section 4 for details).

In this paper, we first revisit the K-Relief algorithm proposed by [2] both theoretically and empirically. In particular, detailed rigorous derivations are pro-vided to produce reliable formulations. And numerical evaluations are carried out under more reliable configurations. Furthermore, an online greedy sparsification method was proposed to achieve very sparse expressions of the features while preserving their effectiv enessasmuchaspossible.

The organization of the rest parts of the paper is as follows. Section 2 briefly introduce the key ideas of K-Relief algorithm. Section 3 revisits the algorithm and provides a detailed rigorous derivation. Section 4 proposes a sparsification procedure to build sparse K-Relief. Section 5 presents the empirical studies, and Section 6 concludes the whole paper. Suppose we are given a set of input vectors { x n } N n =1 along with corresponding tar-1 } is its label, N , D , C denote the training set size, the input space dimension-ality and the total number of categories respectively, and the d -th feature of x is denoted as x ( d ) , d =1,2,. . . , D . The Relief algorithm [5] ranks the features according to the weights w d  X  X  obtained from the following equation [7]: margin for the pattern x n , H ( x n )and M ( x n ) denote the nearest-hit (the near-est neighbor from the same class) and nea rest-miss(the nearest neighbor form different class) of x n respectively.

Traditional linear feature selection methods are not appropriate for non-linearly separable classification problems. Cao et al [2] established an algorithm that allows Relief to be approximately carried out in the kernel space. Suppose the data have been mapped from the original input space R D to a high (usually infinite) dimensional feature space F through an implicit mapping function  X  : R
D  X  X  , which is induced by the kernel function k ( x , x )=  X  ( x ) , X  ( x ) ,Caoet al [2] proposed to implement the kernel-based Relief (K-Relief) by the following procedures: ( b ) Apply Relief to the space spanned by the constructed basis set and select
Though the idea is very interesting, s ome problems occurred in their study: ( i ) They failed to provide a rigorous theoretical derivation of the algorithm. As a result, several key formulas in both procedure ( a )and( b ) are incorrect, making their algorithm unreliable. ( ii ) The configuration of their comparison study is problematic, which makes the empirical performance of K-Relief unclear. Firstly, if one wishes to compare different DR methods using the classification accuracy as the evaluation metric, it is necessary to fix the classifier. However, in their study, this discipline is defied. Secondly, to fairly assess K-Relief, one needs to compare it with some state-of-art feature extraction techniques.
 In this section, we provide a rigorous derivation of K-Relief algorithms as follows: ( a ) Kernel Gram-Schmidt Process (GP). Suppose the l -th orthogonal basis can be expressed as v ( l ) = N n =1  X  ln  X  ( x n ). Denote the design matrix in and e ( l ) be the l  X  th standard basis of the Euclidean space (the vector with l  X  th component be 1 and all others be zero). Applying the Gram-Schmidt orthogo-nalization process we have:
To get an orthogonal basis set, we need to normalize each basis. (b) Applying Relief in Kernel Space. We are now ready to project the data into the space spanned by the basis set that we have constructed. or equivalently:  X  ( x n )= That is, the design matrix in F can be expressed as: where  X  : R D  X  F , F = { L l =1  X  l v ( l ) |  X  l  X  X } is the subspace spanned by the basis set, A =(  X  ln ) L  X  N ,and k n denotes the n -th column of K . This approximate Algorithm 1. Kernel-Relief based on Gram-Schmidt basis (K-Relief) expression of  X  makes it possible to apply Relief algorithm in the kernel space. In particular, Relief in F solves the following optimization problem: H (  X  ( x n )) and M (  X  ( x n )) denote the nearest-hit and nearest-miss of x n in F respectively, which can be found based on the distance d F ( x , x )=  X  ( x )  X   X  ( x ) , X  ( x )  X   X  ( x ) = k ( x , x )+ k ( x , x )  X  2 k ( x , x ).
Eq.(2) has an explicit solution:
The K-Relief algorithm is described as Algorithm 1. Note that several key formulas in [2], such as Eq.(4), (9) and (10), are incorrect.

Once the feature weights are obtained fr om Eq.(9), one can select a preferable subset of M ( M L )features {  X  m ( x ) | m = l 1 ,l 2 , ..., l M } according to the ranking of the weights. Then one either applies the selected feature mapping {  X  m ( x ) based on these features, or else, one can compute a kernel matrix K w and build a kernel machine based on K w ,where K w ( x , x )= M m =1  X  l m ( x )  X  l m ( x ). A fundamental problem with the K-Relief algorithm is that the nonlinear feature has a non-sparse kernel expression, w hich may not be eliminated even when sparse kernel machines, such as SVM and RVM, are applied to the tasks because each feature is expressed as a linear combination of kernel functions centered at all the training data points (see Eq.(6)). In practice, this could cause severe over-fitting and would also become a cruci al computation prohibition especially when we are facing large scale problems or o nline learning requirements, because both the memory for storing the kernel matrix and the training &amp; testing time are typically proportional to the number of support patterns 1 (denote as N sp ).
The non-sparseness stems from the basis construction pro cess. Therefore, if we can construct a set of orthogonal bases which has a sparse kernel expression, i.e., which can be expressed as a linear combination of a small set of the kernel vectors, then we can easily achieve a sparse K-Relief algorithm. Clearly, the sparse kernel PCA (SKPCA, [8]) satisfies this requirement. Here however, we shall establish an online sparsification procedure for the kernel Gram-Schmidt process.

Our goal is to recursively select L ( M L N )outof N kernel vectors to support patterns. Clearly, if we have obtained all of the L support patterns, it would be straightforward to apply Gram-Schmidt process on the sub-matrix K
L =( k ( x j , x k ) proach, an online construction would be more preferable.

The proposed sparse Gram-Schmidt process starts with a randomly chosen comprised by the ind ( l ) -indexed columns of the Gram matrix K . Suppose at l -th step, we have collected a dictionary K ind = K (:, ind ( l ) ), the major operating procedures from l -th to ( l +1)-th step can be summarized as follows. (1) Choose a new pattern. Randomly choose i l from Ind -ind ( l ) . (2) Approximate linear dependence (ALD) test [3]. The new candidate basis k i l can be viewed to be approximately expressed by linear combination of K ind , if the ALD condition is satisfied, i.e.:
There exists a close form solution to Eq.(6). and  X  is a small constant which determines the sparsity degree. If ALD condition and construct a new ba sis vector based on K ind . (3) Construct the l -th orthogonal basis . Suppose the bases form a causal series , i.e., l -th basis can be expressed as the linear combination of the previously Gram-Schmidt process, we have: Algorithm 2. K-Relief based on Sparse Gram-Schmidt basis (SK-Relief)
Solving  X  ik from Eq.(13), we have: where e ( l ) is an l -by-1 sized vector with the last element be 1 and all others equal linear combination coefficients defined in Eq.(6)form a lower triangular matrix:
The derived algorithm is described as Algorithm.2. Note that the computation complexity has been reduced from O ( N 2 )to O ( N sp L ). In this section, we conduct extensive experiments to evaluate the effectiveness of the proposed methods in comparison with several state-of-art kernel-based FE techniques. Six benchmark machine learning data sets from UCI collection are selected because of their diversity i n the numbers of features, instances and classes. The information of each data set is summarized in Table 1. To eliminate statistical deviation, all the results are averaged over 20 random runs. The k NN classifier is tested and the testing accu racy is used to evaluate the performances of different FE methods. In all experiments, Gaussian RBF kernels are used. The hyper-parameters, i.e., the number of nearest neighbors and the kernel width, are both determined by five-fold cross validation.

Baseline methods in our comparison include the famous classical techniques such as KPCA [6] and GDA [1], and recently developed ones like KANMM [10], and KMFA [11]. The average testing error of KNN for each FE method, as a function of the number of projected dimensions, is plotted in Fig.1. As a reference, the best results of each al gorithm, along with the corresponding number of features (the value in bracket) and the number of support patterns (the value in square bracket) used in K-Relief and SK-Relief, are reported in Table 2. From these experimental results, we arrive at the following observations: 1. K-Relief is a competitive feature extraction method. It has much lower com-2. The performance of SK-Relief is very similar to K-Relief. However, the num-In this paper, we provide a rigorous derivation for the K-Relief algorithm. To achieve sparse kernel-based nonlinear f eatures, we assume the kernel bases form a causal series and incorporate a greedy online sparsification procedure into the basis construction process, leading to an SK-Relief algorithm. Gram-Schmidt process is in no way the only method to construct orthogonal bases in the kernel space. An obvious alternative is KPCA . However, besides such unsupervised approaches, there are various supervised approaches. Therefore, an interesting investigation would be to explore how these different basis construction methods affect the performance of K-Relief.

