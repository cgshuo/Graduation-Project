 Hyper-parameter optimization has been receiv-ing an increasingly amount of attention in the NLP and machine learning communities (Thorn-ton et al., 2013; Komer et al., 2014; Bergstra et al., 2011; Bardenet et al., 2013; Zheng et al., 2013). The performance of learning algorithms depend on the correct instantiations of their hyper-parameters, ranging from algorithms such as lo-gistic regression and support vector machines, to more complex model families such as boosted regression trees and neural networks. While hyper-parameter settings often make the differ-ence between mediocre and state-of-the-art per-formance (Hutter et al., 2014), it is typically very time-consuming to find an optimal setting due to the complexity of model classes, and the amount of training data available for tuning. The issue is particularly important in large-scale problems where the size of the data can be so large that even a quadratic running time is prohibitively large.
Recently several sequential Bayesian Op-timization methods have been proposed for hyper-parameter search (Snoek et al., 2012; Eggensperger et al., 2015; Brochu et al., 2010; Hutter et al., 2011; Eggensperger et al., 2014). The common theme is to perform a set of it-erative hyper-parameter optimizations, where in each round, these methods fit a hyper-parameter response surface using a probabilistic regression function such as Gaussian Process (Snoek et al., 2012) or tree-based models (Hutter et al., 2011), where the response surface maps each hyper-parameter setting to an approximated accuracy. The learned regression model is then used as a cheap surrogate of the response surface to quickly explore the search space and identify promising hyper-parameter candidates to evaluate next in or-der to enhance validation accuracy.

While these methods have enjoyed great success compared to conventional random search (Bergstra et al., 2012; Bengio et al., 2013) and grid search algorithms by significantly reduc-ing the number of iterations and trials required during the process, the focus and starting point of these work have largely been on dealing with many dimensions of hyper-parameters, rather than scaling to large amount of data as typical in many NLP tasks, where the efficiency bottleneck stems from the size of the training data in addition to hyper-parameter dimensions. For example, as dataset size grows, even simple models (with few hyper-parameters) such as logistic regression can require more training time per iteration in these algorithms, leading to increased overall time complexity.
 In this work, we introduce a multi-stage Bayesian Optimization framework for efficient hyper-parameter optimization, and empirically study the impact of the multi-stage algorithm on hyper-parameter tuning. Unlike the previous approaches, the multi-stage approach considers hyper-parameter optimization in successive stages with increasingly amounts of training data. The first stage uses a small subset of training data, applies sequential optimization to quickly iden-tify an initial set of promising hyper-parameter settings, and these promising candidates are then used to initialize Bayesian Optimization on later stages with full training dataset to enable the ex-pensive stages operate with better prior knowledge and converge to optimal solution faster.

The key intuition behind the proposed approach is that both dataset size and search space of hyper-parameter can be large, and applying the Bayesian Optimization algorithm on the data can be both expensive and unnecessary, since many evaluated candidates may not even be within range of best final settings. We note our approach is orthogo-nal and complementary to parallel Bayesian Opti-mization (Snoek et al., 2012) and multi-task learn-ing (Yogatama et al., 2014; Swersky et al., 2012), because the improved efficiency per iteration, as achieved by our algorithm, is a basic building block of the other algorithms, thus can directly help the efficiency of multiple parallel runs (Snoek et al., 2012), as well as runs across different datasets (Yogatama et al., 2014; Swersky et al., 2012). The new multi-stage Bayesian Optimization is a generalization of the standard Bayesian Optimiza-tion for hyper-parameter learning (Snoek et al., 2012; Feurer et al., 2015). It is designed to scale standard Bayesian Optimization to large amounts of training data. Before delving into the de-tails, we first describe hyper-parameter optimiza-tion and give a quick overview on the standard Bayesian Optimization solution for it. 2.1 Hyper-parameter Optimization Let  X  = {  X  1 ,..., X  m } denote the hyper-parameters of a machine learning algorithm, and let {  X  1 ,...,  X  m } denote their respective domains. When trained with  X  on training data T train , the validation accuracy on T valid is denoted as L (  X  , T train , T valid ). The goal of hyper-parameter optimization is to find a hyper-parameter setting  X   X  such that the validation accuracy L is maxi-mized. Current state-of-the-art methods have fo-cused on using model-based Bayesian Optimiza-tion (Snoek et al., 2012; Hutter et al., 2011) to solve this problem due to its ability to identify good solutions within a small number of iterations as compared to conventional methods such as grid search. 2.2 Bayesian Optimization for Model-based Bayesian Optimization (Brochu et al., 2010) starts with an initial set of hyper-parameter settings  X  1 ,...  X  n , where each set-ting denotes a set of assignments to all hyper-parameters. These initial settings are then eval-uated on the validation data and their accuracies are recorded. The algorithm then proceeds in rounds to iteratively fit a probabilistic regression model V to the recorded accuracies. A new hyper-parameter configuration is then suggested by the regression model V with the help of acquisition function (Brochu et al., 2010). Then the accu-racy of the new setting is evaluated on validation data, which leads to the next iteration. A common acquisition function is the expected improvement, EI (Brochu et al., 2010), over best validation accu-racy seen so far L  X  : a (  X  ,V ) = where p V ( L |  X  ) denotes the probability of accu-racy L given configuration  X  , which is encoded by the probabilistic regression model V . The acquisi-tion function is used to identify the next candidate (the one with the highest expected improvement over current best L  X  ). More details of acquisition functions can be found in (Brochu et al., 2010).
The most common probabilistic regression model V is the Gaussian Process prior (Snoek et al., 2012), which is a convenient and powerful prior distribution on functions. For the purpose of our experiments, we also use Gaussian Process prior as the regression model. However, we would like to note the fact that the proposed multi-stage Bayesian Optimization is agnostic of the regres-sion model used, and can easily handle other in-stantiations of the regression model.
Algorithm 1: Multi-stage Bayesian Optimiza-tion for Hyper-parameter Tuning
Input : Loss function L , number of stages S ,
Output : hyper-parameter  X   X  for stage s=1 to S do end return  X   X  = arg max 2.3 Multi-stage Bayesian Optimization for The multi-stage algorithm as shown in Algo-rithm 1 is an extension of the standard Bayesian Optimization (Section 2.2) to enable speed on large-scale datasets. It proceeds in multiple stages of Bayesian Optimization with increas-ingly amounts of training data | T 1 train |  X  ...,  X  train | . During each stage s , the k best configu-rations (based on validation accuracy) passed from rent stage X  X  training data T s train , and then the stan-dard Bayesian Optimization algorithm are initial-ized with these k settings and applied for Y s  X  k iterations on T s train (discounting the k evaluations done earlier in the stage), where Y s is the total number of iterations for stage s . Then the top k configurations based on validation accuracy are used to initialize the next stage X  X  run.

We note after the initial stage, rather than only considering candidates passed from the previous stage, the algorithm expands from these points on larger data. Continued exploration using larger Table 1: Hyper-parameters used in SVM and boosted regression trees. data allows the algorithm to eliminate any po-tential sensitivity the hyper-parameters may have with respect to dataset size. After running all S stages the algorithm terminates, and outputs the configuration with the highest validation accuracy from all hyper-parameters explored by all stages (including the initialization points explored by the first stage).

This multi-stage algorithm subsumes the stan-dard Bayesian optimization algorithm as a special case when the total number of stages S = 1 . In our case, for datasets used at stages 1 ,...,S  X  1 , we use random sampling of full training data to get subsets of data required at these initial stages, while stage S has full data. For the number of top configurations k used to initialize each following stage, we know the larger k is, the better results in the next stage since Bayesian Optimization relies on good initial knowledge to fit good regression models (Feurer et al., 2015). However, larger k value also leads to high computation cost at the next stage, since these initial settings will have to be evaluated first. In practice, the number of stages S and the value of k depend on the quantity of the data and the quality of stage-wise model. In our experiments, we empirically choose their values to be S = 2 and k = 3 which result in a good balance between accuracy and speed on the given datasets. We empirically evaluate the algorithm on two tasks: classification and question answering. For classification we use the Yelp dataset (Yelp, 2014) which is a customer review dataset. Each review contains a star/rating (1-5) for a business, and the task is to predict the rating based on the textual in-formation in the review. The training data contains half-million feature vectors, and unique unigrams are used as features (after standard stop-word re-moval and stemming (Manning et al., 2008)). For question answering (QA), the task is to identify correct answers for a given question. We use a commercial QA dataset containing about 3800 unique training questions and a total of 900 , 000 feature vectors. Each feature vector corresponds to an answer candidate for a given question, the vec-tor consists of a binary label (1=correct, 0=incor-rect) and values from standard unigram/bigram, syntactic, and linguistic features used in typical QA applications (Voorhees et al., 2011). Both QA and Yelp datasets contain independent training, validation, and test data, from which the machine learning models are built, accuracies are evalu-ated, and test results are reported, respectively.
We evaluate our multi-stage method against two methods: 1) state-of-the-art Bayesian Opti-mization for hyper-parameter learning (Snoek et al., 2012)), and 2) the same Bayesian Optimiza-tion but only applied on a small subset of data for speed. For experiments, we consider learn-ing hyper-parameters for two machine learning algorithms: SVM implementation for classifica-tion (Fan et al., 2008) and boosted regression trees for question answering (Ganjisaffar et al., 2011) as shown in Table 1. 3.1 Accuracy vs time Figures 1 and 2 compare the test accuracy of our proposed multi-stage Bayesian optimization as a function of tuning time for QA and Yelp, respec-tively. The state-of-the-art Bayesian optimiza-tion (Snoek et al., 2012) is applied on full train-ing data, and the fast variant of Bayesian Opti-mization is applied with 30% of training data (ran-domly sampled from full dataset). The top-1 and classification accuracies on test data are reported on the y-axis for QA and Yelp, respectively, and the tuning time is reported on the x-axis. For fair-ness of comparison, the multi-stage method uses the same 30% training data at the initial stage, and full training data at the subsequent stage.

From these figures, while in general both of the comparison methods produce more effective results when given more time, the multi-stage method consistently achieves higher test accuracy than the other two methods across all optimiza-tion time values. For example, best test accu-racy is achieved by the multi-stage algorithm at time (45 min) for the QA task, while both the full Bayesian Optimization and the subset variant can only achieve a fraction of the best value at the same time value. We also note in general the multi-stage algorithm approaches the upper bound more rapidly as more time is given. This shows that the new algorithm is superior across a wide range of time values. 3.2 Expected accuracy and cost per iteration To investigate the average accuracy and cost per iteration achieved by different methods across dif-ferent time points, we compare their mean ex-pected accuracy (according to precision@1 for QA and classification accuracy for Yelp) in Ta-ble 2, and their average speed in Table 3. In terms of average accuracy, we see that the state-of-the-art Bayesian optimization on full training data and the multi-stage algorithm achieve similar test accuracy, and they both outperform the sub-Table 2: Average test accuracy for QA (preci-sion@1) and Yelp dataset (classif. accuracy). Bayes opt on small subset 3.2 min 0.4 min set variant of Bayesian Optimization. However, in terms of time per iteration, the full Bayesian Optimization is the most expensive, taking more than twice amount of time over subset variant al-gorithm, while the multi-stage is 23% and 50% faster than standard Bayesian Optimization on QA and Yelp (Table 3), respectively, while maintain-ing the same accuracy as full Bayesian Optimiza-tion. This demonstrates the multi-stage approach achieves a good balance between the two baselines and can simultaneously delivers good speedup and accuracy. We introduced a multi-stage optimization algo-rithm for hyper-parameter optimization. The pro-posed algorithm breaks the problem into multiple stages with increasingly amounts of data for effi-cient optimization. We demonstrated its improved performance as compared to the state-of-the-art Bayesian optimization algorithm and fast variants of Bayesian optimization on sentiment classifica-tion and QA tasks.

