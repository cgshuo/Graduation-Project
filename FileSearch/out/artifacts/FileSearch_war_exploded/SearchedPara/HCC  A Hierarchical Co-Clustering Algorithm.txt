 In this poster, we develop a novel method, called HCC, for hierarchical co-clustering. HCC brings together two interre-lated but distinct themes from clustering: hierarchical clus-tering and co-clustering. The goal of the former theme is to organize clusters into a hierarchy that facilitates brows-ing and navigation, while the goal of the latter theme is to cluster different types of data simultaneously by making use of the relationship information. Our initial empirical results are promising and they demonstrate that simultane-ously attempting both these goals in a single model leads to improvements over models that focus on a single goal. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Clustering General Terms: Algorithms, Experimentation Keywords: Hierarchical, Co-Clustering
Hierarchical clustering generates tree-like structures (e.g., dendrograms and hierarchies) which can be utilized to facil-itate data navigation and browsing and has many applica-tions in Information Retrieval(IR) [1]. Most existing hierar-chical clustering algorithms aim at clustering homogeneous single type of data.

In many real world applications, however, a typical task often involves more than one type of data points. For exam-ple, in document analysis, there are terms and documents. Recently, many co-clustering algorithms have been proposed to simultaneously clustering different types of data [2]. How-ever, few algorithms have been developed on simultaneously building hierarchies for different types of data.
In this poster, we develop a novel method, called HCC, for hierarchical co-clustering, which aims at generating dendro-grams for different types of data simultaneously by making use of their relationship information. In this regard, HCC brings together two interrelated but distinct themes from clustering: hierarchical clustering and co-clustering. HCC utilizes the agglomerative hierarchical clustering algorithms as the frame, starting with singleton clusters, successively merges the two nearest clusters until only one cluster re-mains. Different from traditional agglomerative hierarchical
The work is partially supported by NSF grants IIS-0546280 and CCF-0939179.
 Figure 1: An Illustrative Example. Blue rectan-gles denote books and red rectangles denote key-words. The nodes containing both blue and red rectangles denote the clusters having some books and keywords. clustering algorithms, HCC applies on the union of differ-ent types of data instead of on the points of a single data type. At each step of the merging process, HCC can merge a subset of one type of data objects with a subset of another type of data objects based on a measure of cluster internal heterogeneity. Figure 1 shows the hierarchy generated by traditional single-link hierarchical clustering algorithm and the hierarchy generated by HCC on an example dataset of book titles with two data types (books and keywords). It can be easily observed that HCC builds two coupled hier-archies (book hierarchy and keyword hierarchy) simultane-ously. Books in a cluster can be well explained using the keywords in the same cluster. In addition, the relationships between book clusters can be described using their shared keywords.

Our initial empirical results are promising and they demon-strate that simultaneously attempting both these goals in a single model leads to improvements over models that focus on a single goal. In the rest of the poster, we introduce our HCC algorithm and also present some experimental results.
In the typical setting of a dyadic co-clustering problem, there are two types of data objects, A = { a 1 ,a 2 ,a 3 ...a and B = { b 1 ,b 2 ,b 3 ...b n } with size m and n ,andwearegiven a relationship matrix X =( x ij )  X  R m  X  n , such that x represents the relationship between i -th point in A and the j -th point in B . The goal of HCC is to generate hierarchical clustering solutions for A and B simultaneously by making use of X . In the example of Figure 1, A is the set of book titles, B is the set of keywords, and X is the (normalized) book-term matrix indicating the relationship between the book titles and keywords.

HCC has its base in the approach developed in [4]. Simi-lar to agglomerative hierarchical clustering algorithms, HCC starts with singleton clusters and then successively merges the two nearest clusters until only one cluster remains. Dif-ferent from traditional agglomerative hierarchical clustering algorithms, HCC applies on the union of different types of data instead of on the points of a single data type. The out-put of HCC is a tree: each leaf of the tree is a data point of A or B and each internal node is a cluster of data points in A and B . The root of this tree is the largest cluster which contains all data points of A and B . The general procedure of HCC is described in Algorithm 1.
 Algorithm 1 HCC Algorithm Description Create an empty hierarchy H List  X  Objects in A + Objects in B N  X  size [ A ]+ size [ B ]
Add List to H as the bottom layer for i =0to N  X  1 do end for
The core issue in Algorithm 1 is the process of PickUpT-woNodes: picking up two nodes (corresponding to two clus-ters) from the current layer to merge. Different from tra-ditional hierarchical clustering where a node only contains objects of a single data type, a node here can have objects from different data types. Given a cluster C with m 1 objects from A (denoted as A 1 )and n 1 objects from B (denoted as B ), the cluster heterogeneity CH ( C ) can be defined as where  X  = Avg s,t x st is the average value of the correspond-ing entries in X . Once the cluster heterogeneity is defined, we can select two nodes which would result in the least in-crease in cluster heterogeneity to merge [4]. Therefore, at each step of the merging process, HCC can merge two sub-sets of different types of data objects.
We perform experiments on DBLP dataset [5] to evaluate our HCC method. DBLP dataset contains the paper titles published by 552 relatively productive researchers over the last 20 years (from 1988 to 2007, inclusive) from 9 categories.
The input to our HCC method is a normalized author-term matrix where each entry indicating the relationship between the corresponding author X  X  paper titles and the cor-responding keyword. As we discussed before, HCC is able to generate a coupled dendrogram for both authors and terms. Figure 2 shows the dendrogram generated by HCC. In the dendrogram, each leaf represents one author or one term and each internal node contain subsets of authors and terms. Figure 2: The dendrogram generated by HCC on the DBLP dataset Note that authors X  research interests can be clearly described by the associated terms in a hierarchical manner. The more representative are the words for certain authors, the larger possibility for them to be clustered together. For example, at the sixth layer,  X  X ata X  and  X  X ining X  have been merged with  X  X iawei Han X  who is renowned computer scientist in data mining research area.

We also evaluate the clustering performance of our HCC method. Normalized mutual information(NMI) and CoPhe-netic Correlation Coefficient(CPCC) are used as evaluation measures. NMI is the normalized version of mutual informa-tion which measures how much information the two cluster-ings shares. In general, larger NMI indicates better cluster-ing results. CPCC measures how faithfully a dendrogram preserves the pairwise distances between the original data points. Since DBLP dataset contains 9 classes, we make a cut on certain layer of the generated dendrogram, such that we can generate 9 clusters from the tree to compare with the original classes. We compare HCC with K-means, Tri-Factor Nonnegative Matrix Factorization(TNMF) [3], and Single Linkage Hierarchical Clustering(SLHC). Note that TNMF is an effective matrix-based framework for co-clustering. The experimental results presented in Table 1 clearly show that HCC outperforms other rivals. In summary, HCC combines the strengths of hierarchical clustering and co-clustering, si-multaneously builds hierarchies for different types of data utilizing their relationships, and thus leads to better clus-tering performance.
