 The growth of online microblogging services introduced new means of sharing opinions, news, and information such as Twitter. Users of Twitter can exchange messages up to 140 characters, commonly known as tweets . Statistics from 2014 show that the total number of tweets posted per day is about 500-million tweets Interestingly, as of March 2014, an average of 17-million Arabic tweets are posted daily 2 . Moreover, Twitter has no particular restriction on automation; thus, automation programs can play a critical role in creating a huge volume of tweets. Such automation programs are formally known as bots [ 2 ]. The main purpose of bots is to mimic humans and post tweets (periodically in most cases) to the timelines of subscribed users. Tweets posted by bots, referred to as  X  X utomated X  tweets, are sometimes partially-edited by a human, or completely automated (e.g., prayer times or temperature readings). In the Arab world, Arabic bots often use formal or Modern Standard Arabic (MSA) in their messages. Furthermore, tweets generated by bots are not personalized, as they discuss broad topics like news and famous quotes.
 Examples of automated tweets written by bots are given in Table 1 . The first tweet is a verse from Quran, the second is a supplication posted by an Arabic bot, the third tweet is a quote by the famous Greek philosopher Aristotle, and the fourth tweet is an advertisement for an Arabic words application. In terms of tweeting behavior, humans tend to use informal (dialectal) Arabic when tweet-ing. Moreover, they communicate with other users, sometimes misspell words, and often use abbreviations or emoticons in their tweets. The diversity of writing styles used by Arab users on Twitter makes human tweets unique by nature. The bottom half of Table 1 shows examples of  X  X anual X  tweets written by humans. The first manual tweet is written in Egyptian dialect, while the second is written in Gulf dialect. The last two tweets are written informally in mix of different Arabic dialects.
 Unfortunately, bots can be easily abused to spread spam such as advertise-ments and malicious hyperlinks [ 13 ]. Moreover, some bots interfere with infor-mation extraction applications, thus hindering their job. For example, trending topic detection systems can easily confuse actual trending topic tweets with automated tweets. Results of such confusion stem from the interference of auto-mated tweets, which leads to biased result [ 7 ]. When users rely on Twitter to exchange valuable information, automated tweets might obstruct the commu-nication between users, especially in cases that involve dangerous locations or survival needs. At times of need, users seek information from genuine tweets, not from automated sources. Therefore, it is important to distinguish between automated and manual tweets.
 since it noticeably affects users in the Arab region due to the evolving recent political events. To address the problem, we formulate it as a binary classification problem. Besides the typical unigram features (i.e., term occurrences in tweets) that constitute our baseline classifier, we experimented with four feature cate-gories: formality, structural, tweet-specific [ 4 ], and temporal features [ 12 , 14 ]. Our classifier was trained using a set of manually-labeled tweets that were obtained through crowdsourcing. The classification performance was evaluated through a set of experiments that aim to answer the following research questions:  X  RQ1: Would preprocessing of unigram features help improve the baseline uni-gram classifier?  X  RQ2: Can any of the feature categories separately produce a classifier that outperforms the unigram classifier? Would temporal features specifically help classify the automated tweets?  X  RQ3: What is the impact of combining the unigram features with each of the four feature categories? Will combining all feature categories improve the classification compared to individual categories?  X  To our knowledge, this is the first study that focuses on automation behavior in Arabic microblogs. We conducted our experiments on a collection of 1.2-million tweets from 11 k different users that was obtained in 4 days.  X  Our proposed classifier, achieving an accuracy of about 92 %, constitutes a very strong baseline classifier for future studies on the problem.  X  Two lists of Arabic tweets were developed and made publicly available for further research 3 : the first one includes 1.2-million tweets (represented by their tweet ids) that were used in our experiments. The second contains a total of 3503 manually-labeled tweets, where 1944 were labeled as automated tweets and 1559 were labeled as manual tweets.
 related work. Section 3 introduces the proposed solution. Section 4 discusses our experimental evaluation in terms of setup and results. Section 5 concludes this work with some future research directions. In recent years, several researchers attempted to study the implications of auto-mated tweets on the microblogging community. Unfortunately, the literature does not report any work on classifying tweets as either automated or manual. Therefore, all of the systems that will be discussed shortly tackle the problem of classifying Twitter accounts. Chu et al. [ 2 ] proposed a methodology to classify Twitter users based on their posting activity. The classification system comprises of four main components: the spam detection component, the entropy component, the account properties component, and the decision making component. The system applies a set of measurements to classify a collection of over 500 K Twitter accounts. The goal of the proposed classification is to identify accounts that originate from humans, bots, or cyborgs. The study shows that the ratio of Twitter population given the human, cyborg, and bot categories is 5:4:1 [ 2 ]. In another research, Laboreiro et al. [ 7 ] introduced classes of features that aid in identifying automation. The work proposed classes of features that combine chronological and other content-based features to identify automation. The classes of features include chrono-logical features and content-based features. With a collection of 72 K accounts, the authors show that out of the 26 K features that were generated through experiments, stylistic content-based features contribute the most to gaining high classification accuracy [ 7 ].
 Since malicious bots are generally associated with spreading spam, there is an overlap between automation and spam detection. Hence, it was important to look at the work done on spam detection to understand how such systems identify these tweets.
 The work done in [ 2 ] triggered the interest of researchers to further investi-gate the nature of automation. Zhang and Paxson [ 12 ] conducted a study that aims to identify spammers on Twitter. Their research relies on the timestamp associated with each tweet. The authors showed that their approach can analyze Twitter X  X  landscape and identify the exhibited degree of automation. By crawl-ing 19 K accounts from the public timeline, 16 % of the accounts were identified as automated. Moreover, verified Twitter users and accounts with many follow-ers exhibit lower automation rates [ 12 ]. The work done by Lee et al. [ 8 ] studied the nature of content pollution on Twitter. By spending seven months on the study, the authors were able to create 60 honeypots and attract 36 K spamming users on Twitter. The unique features that were used in this study include user friendship network, user demographic, user content, and user history [ 8 ]. Another work by Yang et al. [ 11 ] aimed at discovering features that identify spam accounts. By looking at the relationship between spammer nodes and their neighbors, the study looked at three graph-based features. In addition to other new features that consider the posting time and whether tweets are automated or not. This suggests spam detection systems also consider automated and non-automated tweets to be possible sources of spam. Similarly, the work done by Zhu et al. [ 13 ] aimed at modeling the social network in twitter and identify spammers based on their account associations. By deploying a supervised Matrix Factorization technique, the proposed method relied highly on relations between users and actions performed by users. On the same note, the study conducted by Ghosh et al. [ 3 ] focused on identifying the nature of link farming in Twitter. The authors show that complex features that rely on mutual linking between spammers and non-spammers can aid in the spread of spam. Similarly, the work done in [ 5 , 10 ] studying the communication between users on Twitter to identify spam bots and prevent spread of spam.
 accounts. Martinez-Romo and Araujo [ 9 ] introduced a spam classification sys-tem that detects spam within trending topics by analyzing tweets rather than user accounts. The method introduces two approaches that uniquely distinguish spam. The first approach studies spam tweets without considering their source user. The second approach relies on statistical analysis of language to detect spam. The proposed online content-filtering technique relies on language as a detection tool. Amidst the spam detection research, a study conducted by Aggar-wal et al. [ 1 ] tackled the problem of detecting phishers in Twitter. The method-ology relies on a combination of Twitter based features and URL features to classify tweets. A recent study by Hu et al. [ 6 ] followed an approach similar to the work proposed in [ 1 ] but with a focus on optimizing the process of spam detection.
 the problem of classifying tweets rather than Twitter accounts. Additionally, this work focuses on automated and manual Arabic tweets as opposed to the all the work discussed earlier which only considers English tweets and spam content. We formally define our problem as follows. Given a collection of Arabic tweets, the goal is to classify each of them into one of two main classes: automated tweets and manual tweets. This section describes in detail our proposed solution to the problem which involves an overview of the system architecture and the categories of features extracted from the data collection to help classify the tweets. 3.1 System Architecture The architecture of the proposed system is illustrated in Fig. 1 . It was inspired by Martinez-Romo and Araujo X  X  system in [ 9 ] which was used for filtering spam tweets. The model in Fig. 1 shows the four major components of the automated tweet detection system. The preprocessing component performs punctuation and stop-word removal to process the raw tweets, then several feature categories and unigram features are extracted from the preprocessed tweets. The last compo-nent performs the classification and evaluation of tweets using the extracted feature vectors and the labels obtained through crowdsourcing. 3.2 Feature Extraction Since features play a critical role in the classification process, it is essential to detail the list of features that will be used to detect automation in Arabic tweets. The focus of this section will be on describing the four main feature categories used in the study, which are: formality features, structural features, tweet-specific features, and temporal features . Formality Features. Formality features are features that measure how formal a tweet is. In this category, three features are involved: emoticons ,whichis the total number of emoticons (like smiley faces (:-)) found in Arabic tweets. Diacritics feature checks if a tweet contains a decorating diacritic or not. The third feature is elongation , which checks if tweet words contain more than four consecutive characters, such as , which is equivalent to  X  X ool X  [ 4 ]. Structural Features. Studies show that the structure of a tweet can provide good features for tweet classification [ 4 ]. Such features include the length of the tweet in terms of total number of characters, the total number of question marks , and the total number of exclamation marks .
 Tweet-Specific Features. Observing the data associated with tweets can pro-vide some useful features for classification. Fortunately, a lot of research was done in this area to identify such features [ 4 ]. For classifying automated and manual Arabic tweets, a total of six tweet-specific features were used: retweet ,which checks if the tweet was a retweet or not, reply also checks if the tweet is a reply or not, hashtags computes the total number of hashtags in the tweet, and URLs checks if a tweet contains a hyperlink or not.
 A tweet object (obtained through Twitter API) embeds several fields that provide some information about the tweet. For instance, the  X  X ource X  field can indicate the type of client application used in posting the tweet. A user can post a tweet through different devices, like the Web service, a dedicated mobile application, a third party application, or an API [ 2 ]. For bots, some devices are easier to access than others due to authentication requirements. Moreover, humans can combine different clients to post tweets, such as the Web client and the mobile client. The source feature checks if the tweet contains a source field or not. Checking just the existence of the field (instead of using the source type) was sufficient for this study because most of the crawled tweets did not contain any value in the source field.
 Temporal Features. Temporal features constitute an essential part of this study because they allow us to study the characteristics of tweets in terms of posting nature. For that purpose, the following temporal features were proposed:  X  Activity period on Twitter : Some Twitter accounts exhibit constant usage over time, such as bot accounts. While other accounts focus on a particular time period to be active on Twitter, like human accounts. Therefore, studying the posting time of tweets during a particular time period can be a strong indi-cation of automation. This feature, which was proposed by Zhang et al. [ 7 ] assigns the activity period to be a full day. However, in this study, the period was set to five hours due to the limited amount of tweets per user. For each hour, we count the number of tweets posted in that hour by the author of the tweet, resulting in five features, one per hour.  X  Spread Velocity : This is the sixth temporal feature, adopted from the research done by Zubiaga et al. [ 14 ] on trending topic classification. The reason behind choosing this feature is because it captures temporal characteristics based on the differences in seconds between the posting time of tweets per user. To compute this temporal feature, Eq. 1 was used, where  X t is the time period in seconds between the first and last posted tweet of the author, and the total number of tweets posted by the author in that period.
 4.1 Experimental Setup Dataset. To build the data collection, tweets were first randomly sampled through Twitter streaming API 4 over a period of four days. The query used to collect the random sample is  X  X ang:ar X  using Twitter4j java library. The language query ensures that the crawled tweets were in Arabic. To avoid the case where the collected set of tweets are biased towards specific users, a post-processing step was applied to keep just one tweet for each unique user. The result of this step was a collection of 11,764 tweets from unique users. The second step involved collecting more tweets for each user to measure temporal features. For each user, the most recent 120 tweets were obtained through the REST API. However, some users tend to tweet in English and Arabic simultaneously. There-fore, the most recent 120 for each unique user were filtered again with the query  X  X ang:ar X . The final collection was processed to remove tweets with duplicate content. The result of this step is a collection of 1,202,815 tweets posted by 11,764 different users. Manual Labeling. For any classification problem, it is essential to obtain labeled data that identify each class. For this study, there was no labeled data that is publicly available for identifying automated and manual Arabic tweets. To solve this problem, the crowdsourcing platform CrowdFlower was used. Annota-tors were given the task of labeling Arabic tweets as either automated or manual. Annotators were presented with clear Arabic instructions and a total of 18 test questions that qualified them to take part in the labeling process. This measure ensures the accuracy of the aggregated labels. Due to the difficulty of the task and time constraints, annotators were able to label a total of 3503 tweets out of the 11764 collected tweets, where each tweet was judged by at least three anno-tators. Out of the 3503 labeled tweets, 1944 (55 %) were labelled as automated tweets and 1559 (45 %) were labelled as manual tweets. Annotator agreement of the submitted labeling job was around 93 %, with a Fleiss kappa value of 0.61, which is a good value considering the ambiguity of some tweets to the annotators.
 Classification. We experimented with three classification algorithms: Sup-port Vector Machines (SVM), Na  X   X ve Bayes, and Decision Trees; we used their implementations in Weka 5 , the machine learning library. These algorithms were reported as extremely effective for binary classification problems in the litera-ture [ 13 ]. For each machine learning algorithm, a 10-fold cross validation app-roach was used for training and testing purposes. As for evaluation measures, this study reports the classification accuracy, precision, recall, and F1 measures. Baseline. To evaluate the proposed solution, it was essential to identify and construct a baseline classifier to compare against. Finding a baseline system was a real challenge because there was no previous study in the literature that addresses the same problem. Hence, we selected the basic unigram model to comprise our baseline classifier. This choice was made due to the simplicity of the unigram model and its effectiveness in many classification problems [ 14 ]. 4.2 Experimental Results RQ1 : Baseline Performance. Before conducting experiments on any of the four feature categories, it is important to think about the proper settings of the baseline unigram model. The unigram model does not take into consideration any features other than the frequency of words in tweets. To construct the term frequency feature, terms must be tokenized and preprocessed, then the frequency of each word is computed for each tweet. The problem with this model is that the number of dimensions is not fixed as it changes depending on the number of unique words in the collection. Since Arabic tweets are rich with all types of decorations and symbols, there are many options when it comes to preprocessing. We tried the following three scenarios:  X  uni(raw): Nothing is removed from the unigrams, thus they are raw.  X  uni(emoti): Emoticon symbols are removed from the unigrams.  X  uni(stop): Standard MSA Arabic stop words are removed from the unigrams. of the unigram model. This experiment aims to achieve two goals. The first goal is to get some insights on the impact of stop words and emoticons on the quality of unigrams. The second crucial goal is to select a candidate baseline system that will be compared with in the rest of the experiments.
 racy of the three proposed unigram models. The accuracy values answer RQ1 because the unigrams(stop) clearly outperforms the two other unigram models, which indicates that the best baseline model is the unigrams with stop words removal.
 RQ2 : Performance of Individual Feature Categories. Along with uni-grams(stop) as a baseline model, the remaining models that were used in the fol-lowing experiments are the categories of features that were described in Sect. 3.2 . To answer RQ2 , the right side of Table 2 shows the classification accuracy of each category of features. The results highlighted in bold are the highest average clas-sification accuracy of each model. Results clearly show that the tweet-specific features model outperforms all the models including the baseline. Furthermore, the SVM and Decision Tree classifiers show higher accuracy values when com-pared to the Na  X   X ve Bayes classifier.
 baseline model in the classification accuracy of SVM and Decision Trees algo-rithms only. The results show that for the temporal features, the precision drops in the Na  X   X ve Bayes classifier for the automated class. As for the manual class, both the recall and F1 drop when compared to the baseline. We suspect that the limited number of tweets per user (only 120 tweets) were not enough to compute effective temporal features, and hence the limited classification perfor-mance. Nonetheless, the temporal features are on par with the baseline model, even though they did not outperform it in one classification algorithm. RQ3 : Performance of Combined Features. Results in Table 4 addresses RQ3 . The left side of the table shows the impact of adding the unigram features to each of the other feature categories. No or slight improvements in accuracy are shown when compared to the results in Table 2 . The right side shows the results of combining all feature categories. The column  X  X ll X  indicates the combination of all feature categories in addition to the unigram features. The highest clas-sification accuracy values are highlighted in bold for each classification model. The decision tree classifier seems to have the highest classification accuracy at 91.97 % when the unigram and tweet-specific features are combined, while the Na  X   X ve Bayes shows the lowest classification accuracy overall. Hence, combining all feature categories with the baseline unigram features did not improve classi-fication accuracy.
 This work showed that different feature categories can be leveraged to classify Arabic tweets as either automated or manual. With the aid of conventional clas-sification techniques, it is possible to classify Arabic tweets at a high accuracy of 92 %. In fact, this research shows that combining tweet-specific and unigram fea-tures outperforms all other experimented combinations. Moreover, crowdsourc-ing labelers identified a total of about 2 k automated Arabic tweets and 1.5 k manual tweets in a random sample of tweets used in this study; this indicates that automation plays a huge role in the Arabic space of Twitter.
 some room for improvement. Since this work presented a preliminary study, future work will focus on task-specific features that target different kinds of automated tweets (e.g. propaganda and spam tweets). The temporal features in this work did not manage to fully outperform the baseline model due to the limited number of features and tweets per user. Hence, as future work, it would be interesting to investigate additional temporal features, like Pearson X  X  X which considers minutes-of-the-hour and hours-of the-minute as features [ 12 ]. We also plan to sample more tweets per user to allow for more accurate temporal feature values. As for the tweet-specific features, we plan to extend the source field to account for different types of sources. Moreover, dialect detection and bigram (or generally n-gram) features can be leveraged to extend the list of feature categories. Another direction of future work can focus more on Arabic-specific preprocessing of the tweets, e.g., character normalization, and compare the classification results with an English corpus.

