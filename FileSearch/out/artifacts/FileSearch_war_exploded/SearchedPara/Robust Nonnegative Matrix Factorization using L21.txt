 Nonnegative matrix factorization (NMF) is widely used in data mining and machine learning fields. However, many data contain noises and outliers. Thus a robust version of NMF is needed. In this paper, we propose a robust formulation of NMF using L norm loss function. We also derive a computational algorithm with rigorous convergence analysis. Our robust NMF approach, (1) can handle noises and outliers; (2) provides very efficient and elegant updating rules; (3) incurs almost the same computational cost as standard NMF, thus potentially to be used in more real world ap-plication tasks. Experiments on 10 datasets show that the robust NMF provides more faithful basis factors and consistently better clustering results as compared to standard NMF.
 I.2 [ Artificial Intelligence ]: Learning; I.5.3 [ Pattern Recogni-tion ]: Clustering Algorithms, Theory NMF, L21 norm, Robust, Clustering
Nonnegative Matrix Factorization (NMF) has been popularly stud-ied in data mining and machine learning areas since the initial work of Lee and Seung [13]. Originally proposed as a method for finding matrix factors with parts-of-whole interpretations [13], NMF has been applied to a number of different areas, e.g., envi-ronmetrics [19], chemometrics [25], pattern recognition [14], mul-timedia data analysis [3], text mining [20] and DNA gene expres-sion analysis [1]. Algorithmic extensions of NMF also have been developed to accommodate a variety of objective functions [4, 7] into different data analysis problems, including classification [21] collaborative filtering [22], and constrained clustering [15, 23]. It also can be extended by making a combination with Laplacian em-bedding [2, 11]. One of the key features of NMF is its clustering capabilities. It was shown [5, 8] that NMF essentially solves a ma-trix clustering problem.

Standard NMF uses the least square error function which is well-known to be unstable w.r.t. noises and outliers [24]. However, many real data in various applications probably contain noise and outliers. Potential applications in real world drive us to consider about a robust version of NMF. For this reason, a robust NMF model is studied in this paper.
 To our knowledge, a robust NMF has not been studied so far. In this paper, we propose a novel robust formulation of NMF by using L 2 , 1 -norm loss function 1 . The proposed method is termed as  X  X obust X  because it can accommodate outliers and noises in a better way than standard one. We derive the computational algorithm and provide rigorous analysis on its convergence and correctness. More importantly, the derived solution for robust NMF has very elegantly updating rules, with nearly the same computation cost as standard NMF, and also easy for implementation. We perform experiments on 10 datasets using both robust NMF and standard NMF. On all 10 datasets, robust NMF consistently outperforms standard NMF in terms of clustering results.

The merits of our Robust NMF are in threefold.
The rest of the paper is organized as follows. In section 2 we propose the robust NMF formulation using L 2 , 1 norm, emphasize the advantages of our approach compared with standard NMF. In section 3, we present a rigorous convergence analysis of the algo-rithm. In section 4, we show that the converged solution satisfies the Karush-Kuhn-Tucker condition and thus is a correct optimal so-lution. In section 5 we present experimental results on 10 datasets. We show convergence properties, speed, and clustering results by making comparisons with standard NMF. Finally we discuss the extension of L 2 , 1 NMF to L 1 NMF followed by the conclusion.
In this section we first revisit standard NMF, show the assump-tion of the Gaussian noise leads to the formulation of standard NMF by imposing constraints F  X  0 ,G  X  0 ; then we present our ro-bust NMF formulation, show the assumption of the Laplacian noise leads to the formulation of L 2 , 1 NMF; next we give two illustrative examples both on toy data and real data to show the robustness and effectiveness of L 2 , 1 NMF; finally we present the computational al-gorithm for L 2 , 1 NMF, and highlight the main contribution of our paper.
Given input data vectors X =( x 1 ,x 2 , ...x n ) ,where x represents a data point. Standard NMF is defined as, where X 2 F = ij X 2 ij is the Frobenius form of a matrix. Above problem is usually solved by an iterative updating algorithm, where F and G are updated alternatively using
In general, the input data x i is a p -dimensional column vector contaminated by additional noise, where  X  i is the unobservable true value of the observed x is the additive noise.  X  i can be viewed as a point in a k -dimensional subspace ( k&lt;p ) such that, where g i is the projection of x i on the subspace defined by columns of F .

Suppose the noise  X  i follows zero-mean normal distribution with standard deviation of  X  , thus x i  X  N (  X  i , X  2 ) . Usually the ele-ments of each vector x i in X are independent, thus the probability distribution of x i conditioned on  X  i is,
The data log likelihood can be written as,
To maximize the data log likelihood is equivalent to minimize the term tuting  X  i with Fg i by using Eq. (5), min where g i is the i -th column of G . This means the assumption of i.i.d Gaussian noise model transfers the maximum likelihood problem into a standard NMF problem by imposing constraints F  X  0 ,G 0 . Figure 1: Fit 10 data points with L 2 , 1 NMF and standard NMF by using Fg i . Two data points on upper-left are outliers.
One of the most important drawbacks of standard NMF is that it is prone to outliers. Let X =( x 1 ,  X  X  X  ,x n ) ,G =( g 1 The error function of standard NMF is Here the error for each data point enters the objective function as squared residue error in the form of x i  X  Fg i 2 . Thus a few outliers with large errors easily dominate the objection function be-cause of the squared errors. Note in NMF, both matrices F,G are unknown, the impact of the outliers may be more complicate than the simpler convex case. Thus it is very necessary to present robust NMF formulation and discuss its properties.
 The robust formulation of the error function is
X  X  FG 2 , 1 = In this robust formulation, the error for each data point is x Fg i , which is not squared, and thus the large errors due to out-liers do not dominate the objective function because they are not squared.

For this reason, in this paper, we propose robust NMF ( L NMF) formulated as L , 1 norm of a matrix A is first introduced in [9] and is defined as where a i is the i th column of A . In [9] it is called rotational invari-ant L 1 norm, because the column vectors a i contribute in the form of vector Euclidean norm which is rotational invariant.  X  valid norm because it satisfies the 3 conditions for a norm: (1) posi-tive scalability:  X A 2 , 1 = |  X  | A 2 , 1 where  X  is a real scalar; (2) triangle inequality: A + B 2 , 1  X  A 2 , 1 + B 2 , 1 ; (3) existence of a zero vector: if A 2 , 1 =0 ,then A =0 . These 3 properties can be easily proved.

Note that in general, the L 2 , 1 norm of Eq.(11) is harder to solve than the least squares of the standard NMF of Eq.(1). One main contribution of our paper is to derive an efficient algorithm to solve the L 2 , 1 formulation of Eq.(11). We will present the algorithm in section 2.7. Figure 2: Plot of residue: || x i  X  Fg i || by using standard NMF and L , 1 NMF for each of 10 data points in Fig. 1. Data points #9 and #10 are outliers.
Further, we show the motivation of L 2 , 1 NMF from probability point of view. Different from the normal distribution assumption in section 2.2, if we assume the noise  X  i follows the Laplacian distri-bution with zero mean, we have where  X  is a scale parameter.

Following the strategy of maximizing the data log likelihood, we obtain,
To maximize the data log likelihood is equivalent to minimize the term stituting  X  i with Fg i , min This means the assumption of i.i.d Laplacian noise model transfers the maximum likelihood problem into a L 2 , 1 NMF problem by im-posing constraints F  X  0 ,G  X  0 .
We use 2-dimensional toy data to show the robustness of L NMF in Fig.(1). 10 original 2-dimensional data points are gener-ated, two of which are outliers. For each data point, we use Fg fit the original data point. Here we project all data into 1D subspace (i.e., k =1 ). Both the fitting values from L 2 , 1 and standard NMF are shown in Fig. 1. L 2 , 1 NMF gives much better results while standard NMF is greatly influenced by two outliers. In Fig.(2), we also plot the residue || x i  X  Fg i || corresponding to each data point of Fig.(1). Clearly, L 2 , 1 NMF gives much smaller errors compared with standard NMF. To further illustrate the effectiveness of our L 2 , 1 NMF, we run L , 1 NMF algorithm on real world data sets. Due to space limit, here we only show the results obtained from AT&amp;T face image data set and Yale face data set in Figs. (3-4). On these two datasets, each x is corresponding to an image, which is linearized into a vector. The basis factor f k in the computed F =( f 1 ,f 2 , ..., f an image. After NMF algorithms converge, we can reshape each f into its original size and show each f i as an image.
 Results on AT&amp;T data and Yale data are shown in Fig.(3) and Fig.(4), respectively. Upper images of each row show the robust NMF results. Lower images of each row show the results of stan-dard NMF. Better visual effects indicate better performance of the algorithm. Generally, L 2 , 1 NMF performs much better than stan-dard NMF.

One example is the 6th upper image and the corresponding lower image in the top row of Fig.(3). You can see the clear differences between them. First of all, upper image is much clearer by keep-ing most of the pixel information. It is hard to tell  X  X ho is who X  from the lower one. Next, most of the important pixels (e.g., pixels on eyes, month, nose) are preserved on upper image while some of them(e.g., eyes, month, cheek) are lost with noises of distorted pixels (e.g., anamorphic nose). Finally, compared to the particu-lar category it comes from, the upper image resembles much more than the lower one.

Another example is images from Yale data set. See the 2nd face image in the upper part and the corresponding one in the lower part of top row of Fig.(4). The upper image is much better than the lower one. First of all, the lower image is fuzzy and shaded while the upper one is unshaded and vivid. Secondly, without any distortion and transformation, the upper image preserves most of the pixels while the lower one loses nearly half of the face pixels, and resembles more to the faces from the other categories (e.g., the 8th face in the 3rd row). Finally, the upper one is more similar to the particular category it is originated from.

From above results on real world data, we can see the superior-ity of robust NMF. Due to the noises and outliers existed in real data, it is reasonable for L 2 , 1 NMF to produce better results, which validates the effectiveness of our approach.
The main contribution of this paper is to derive the following iteratively updating algorithm for Eq. (11), where D is a diagonal matrix with the diagonal elements given by The computational algorithm for robust NMF is surprisingly sim-ple. It has almost the same computational cost as standard NMF. It is easy to be adapted for various applications in different context. To our knowledge, this is the first study of the robust NMF with L , 1 norm formulation.

We provide the proof of the convergence of the algorithm in sec-tion 3 and the correctness of the algorithm in section 4. We note that similar algorithmic approach has been used in L 2 , 1 based regres-sion model [17] where no nonnegativity constraints are involved.
In this section, our main goal is to prove the convergence of the algorithm described in Theorem 1. lower images are standard NMF results.
 lower images are standard NMF results.
T HEOREM 1. (A) Updating G using the rule of Eq.(17) while fixing F , the objective function of Eq.(11) monotonically decreases. (B) Updating F using the rule of Eq.(16) while fixing G , the objec-tive function of Eq.(11) monotonically decreases.

We prove (A,B) separately in next two subsections.
We focus on updating G while fixing F . The proof of Theorem 1(A) requires the following two lemmas.
 L EMMA 2. Let G t be the old G [on the RHS of Eq.(17)] and G +1 be the new G [on the LHS of Eq.(17)]. Under the updating rule of Eq.(17), the following inequation holds where D ii =1 / x i  X  Fg t i .
 The proof of Lemma 2 is given in section 3.3.

L EMMA 3. Under the updating rule of Eq.(17), the following inequation holds where D ii =1 / x i  X  Fg t i .
 The proof of Lemma 3 is given in section 3.4.

P ROOF . (Theorem 1(A)). From Lemma 2, the value of expres-sion inside [  X  ] in Eq.(21) is negative or zero. Therefore This proves that the objective function of Eq.(11) decreases mono-tonically.
We now focus on updating F while fixing G . Similarly, the proof of Theorem 1(B) also requires the following two lemmas. L EMMA 4. Let F t be the old F [on the RHS of Eq.(16)] and F +1 be the new F [on the LHS of Eq.(16)]. Under the updating rule of Eq.(16), the following inequation holds where D ii =1 / x i  X  F t g i .

The proof of Lemma 4 is similar to the proof of Lemma 2 and thus is skipped due to space limitations.

L EMMA 5. Under the updating rule of Eq.(16), the following inequation holds where D ii =1 / x i  X  F t g i .

The proof of Lemma 5 is similar to the proof of Lemma 3 and thus is skipped due to space limitations.

P ROOF . (Theorem 1(B)). From Lemma 4, the value of expres-sion inside [  X  ] in line Eq.(25) is negative or zero. Therefore This proves that the objective function of Eq.(11) decreases mono-tonically. P ROOF . Eq.(19) can be re-expressed as where Lemma 2 states that under updating rule of Eq.(17), J ( G ) mono-tonically decreases.
 We prove Lemma 2 using the auxiliary function approach [13]. If a function satisfies we say Z ( G, G ) is an auxiliary function of J ( G ) .Wedefine Then, we have This proves that J ( G ( t ) ) monotonically decreases.
The key steps in the remainder of the proof are: (1) find an appro-priate auxiliary function; (2) find the global maxima of the auxiliary function.
 Now we show that an auxiliary function of J ( G ) of Eq.(27) is First, J ( G ) of Eq.(28) can be expressed as J ( G )= Tr XDX T  X  2 G T F T XD + FGDG T F T . (33) We make use of the following matrix inequality [6] where A, B, H are nonnegative matrices with appropriate sizes and A = A T ,B = B T . The equality holds when H = H .
 In the inequality Eq.(34), setting A = F T F,B = D,H = G, H = G , then the 3rd term of Eq.(33) is always smaller than the 3rd term of Eq.(32). The equality holds when G = G . Thus Z ( G, G ) of Eq.(32) is an auxiliary function of J ( G ) of Eq.(33). Now we need to find the global minima of Eq.(32). Let f ( G )= Z ( G, G ) . The gradient of f ( G ) is The 2nd order derivatives (Hessian matrix) is This implies function f ( G ) is a convex function and there is a unique global minima for f ( G ) .

The global minima is obtained by setting the gradient of f ( G ) to zero and solve for G . Thus we set Eq.(35) to zero and obtain Noting G ( t +1)  X  G and G ( t )  X  G , the above equation recovers the updating rule of Eq.(17). Therefore under this updating rule, the objective function J ( G ) of Eq.(28) decreases monotonically. P ROOF . First we note that
Tr ( X  X  FG t ) D ( X  X  FG t ) T = Similarly, Tr ( X  X  FG t +1 ) D ( X  X  FG t +1 ) T = Thus the right-hand-size (RHS) of Eq.(21) becomes
RHS = 1 by using the definition D ii =1 / x i  X  Fg t i .
 The left-hand-size (LHS) of Eq.(21) becomes Therefore, we have This competes the proof.
In previous section, we proved that the objective function de-creases monotonically under updating of Eqs.(16,17). Here we prove that the converged solution is the correct optimal solution, i.e., the converged solution satisfies the Karush-Kohn-Tucker con-dition of the constrained optimization theory.

First we have Theorem 6 to prove the correctness of the algo-rithm w.r.t. F , and then we have Theorem 7 to prove the correctness of the algorithm w.r.t. G .

T HEOREM 6. At convergence, the converged solution F  X  of the updating rule of Eq.(16) satisfies the KKT condition of the opti-mization theory.

P ROOF . The KKT condition for G with the constraints F jk 0 ,j =1  X  X  X  p, k =1  X  X  X  K ,is
The derivative is Thus the KKT condition for F is On the other hand, once F converges, according to the updating rule of Eq.(16), the converged solution F  X  satisfies which can be written as This is identical to Eq.(47). Thus the converged solution satisfies the KKT condition.

T HEOREM 7. At convergence, the converged solution G  X  of the updating rule of Eq.(17) satisfies the KKT condition of the opti-mization theory.

P ROOF . The KKT condition for G with the constraints G ki 0 ,k =1  X  X  X  K, i =1  X  X  X  n ,is The derivative is
Thus the KKT condition for G is On the other hand, once G converges according to the updating rule of Eq.(17), the converged solution G  X  satisfies which can be written as This is identical to Eq.(52). Thus the converged solution G fies the KKT condition.
In this section, we apply the proposed L 2 , 1 NMF clustering al-gorithm to compare its performance with standard NMF algorithm and k-means algorithm. Extensive experiments are made on ten well known data sets. We use 6 widely used image data sets and also 4 UCI 1 data sets. Table 1 summarizes the characteristics of those data sets.
AT&amp;T 2 . There are totally 400 images belonging to 40 differ-ent subjects. For each subject, the images are in great varieties because of different taking time with changing lighting variance and facial expressions. All the pictures are taken with dark homo-geneous background. The size of each cropped image is 112x92 pixels. We resize each image to 56x46 pixels in our evaluation.
MNIST . This hand-written digits data set consists of 70,000 dig-ital images, which are from digit  X 0" to  X 9 X  [12]. We centralize each image to 28x28 pixels according to the center of the mass of the pixel intensities. We random select 15 images from each class, and there are totally 150 images in our evaluation data set.
UMIST . It is a face-recognition data set frequently used in com-puter vision field. It is challenge to recognize the faces in this data set because the variations for the face from the same class are larger than those in other face recognition problems. Each image is cropped to 28x23 pixels. There are totally 360 images with 20 different classes.

CMU PIE . It is a face data set containing 68 subjects with 41,368 face images. In the preprocessing step, we normalize the images(in scale and direction) to keep the two eyes are aligned at the same http://archive.ics.uci.edu/ml/ http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html position. Each image is resized into 32x32 pixels. In our experi-ment, we random select 10 images from each class with different combinations of pose, face expression and illumination condition. Yale . It is a data set obtained from the combinations of the origi-nal and extended Yale database [10]. There are totally 38 classes (10 subjects in original database with 28 subjects in the extended database) under 576 viewing conditions (9 poses with 64 different illumination conditions). We shrink each image by a factor of 0.25 to size 48x42. We select 64 images in different illumination condi-tions from 31 classes, and therefore there are totally 1984 images.
Bin-alpha . This data set is composed of 1404 binary images of handwritten digits from  X 0 X  to  X 9 X  and also characters from  X  X  X  to  X  X  X , totally 36 classes. The resolution of each image is 20x16 pixels.

For all the image data sets, we use the same original space with-out making any changes and also the raw gray level values as fea-tures. The other four non-image data sets German, Car, Lenses, Vehicle are randomly selected from the UCI Repository. All of them only have non-negative values as features. Robust NMF can be applied to text, web and social network datasets for various ap-plications.
The evaluation metrics [16, 18] we used here are clustering ac-curacy, normalized mutual information and purity. These measure-ment are widely used in the evaluation of different clustering ap-proaches.
 Clustering accuracy(ACC) is defined as, where l i is the true class label and c i is the obtained cluster la-bel of x i , map(.) is the best mapping function,  X  ( x, y ) is the delta function where  X  ( x, y )=1 if x = y ,and  X  ( x, y )=0 otherwise. The mapping function map(.) matches the true class label and the obtained clustering label, where the best mapping is solved by Hun-garian algorithm. A large ACC value indicates a better clustering performance.

Normalized mutual information (NMI) is used to evaluate the clustering quality from information point, and defined by normal-ization on the mutual information between the cluster assignments and the pre-existing input labeling of the classes. The normaliza-tion used is the average of the entropy of the cluster assignment and that of pre-existing input labeling. More formally, where C = { c 1 ,c 2 , ..., c k } is the pre-existing classes, S = is a particular clustering result, I ( S, C ) is the mutual information of clustering assignment with pre-existing class labels, and H ( S ) is the entropy for the clustering assignment. A larger NMI valuealso indicates a better clustering solution.

Purity measures the extent to which each cluster contained data points from primarily one class. The purity of a clustering is ob-tained by the weighted sum of individual cluster purity values, given as, th input class that were assigned to the j -th cluster. k is the number of the clusters and n is the total number of the data points. A large Purity value indicates a good clustering solution. sets UMIST and AT&amp;T. The object functions we demonstrate are log ( ||
X  X  FG || 2 , 1 ) and log ( || X  X  FG || F ) . The convergence criteria is described in Eq. (58).

We make convergence analysis on real world image data sets. We reshape each image into one vector, and then form a large matrix X , where each column is a linearized vector from an image. For example, AT&amp;T data set has 400 images, where the size of each image is 56x46 pixels, thus X is constructed with size 2576x400. With the same input data X , we random generate the same F and G to feed into NMF model and L 2 , 1 NMF model. We compare the convergence properties (e.g., speed, object function values) of L NMF with standard NMF.

We test convergence in each iteration by computing the object function values. The convergence criterion we used is where J t is the object function values ( || X  X  FG || F for NMF and || X  X  FG || 2 , 1 for robust NMF) in the t -thiterationinthe convergence tests. For the fairness of comparisons, we use the non-squared object function || X  X  FG || F in standard NMF instead of the squared object function || X  X  FG || 2 .

We implement our algorithm in Matlab 7.0. All the experiments are done on a AMD Phenom(tm) 2.80GHZ machine with 6GB memory running Windows 7. The initial object functions are com-puted through the random guess of F and G . Fig.(5) shows the log object functions and also the number of iterations needed for the convergence of NMF and L 2 , 1 NMF on the data sets UMIST and Table 2: Comparison on the object functions and time cost on data sets, UMIST and AT&amp;T. We present the normalized convergence sents the time of iterations needed before converges according to the convergence criteria of Eq.(58).
 AT&amp;T. Because data matrix X can be very large, for the conve-nience of demonstration, here we show the log values of the object functions, i.e., log ( || X  X  FG || F ) and log ( || X  X  FG
Our experiment results show that on data set UMIST, L 2 , needs 3906 iterations before convergence while NMF needs 4932 iterations before convergence. However, the computation time cost on L 2 , 1 NMF is higher than NMF due to the updating rule on the diagonal matrix D . As is shown in Table 2, both on UMIST and AT&amp;T data sets, L 2 , 1 NMF takes less iterations to converge than NMF, yet with more computation time.

Also, the object function of L 2 , 1 NMF is always larger than non-squared NMF object function due to the formulation of L 2 defined in Eq.(10). It is worth mentioning that the larger object function values do not necessarily mean the worse matrix factoriza-tion results. As is shown in Table 2, L 2 , 1 NMF has lower normal-ized convergence error compared with NMF on those two testing data, which shows the effectiveness of L 2 , 1 NMF.
We report clustering results by making comparisons with K-means clustering and standard NMF clustering approach. For the initial-izations of F and G , we did not use the random generated matrices. Firstly, we use the principal component analysis to get a subspace with r -dimension. After this, k-means clustering approach is em-ployed on the projection data to get clustering results. We use above clustering results G to initialize G = G +0 . 3 ,andthen F are Table 3: Clustering quality comparison of L 2 , 1 NMF with NMF and k-means on 10 data sets.
 obtained by computing the clustering centroid for each category. Empirically, we run k-means 10 times during initializations of G .
Clustering Analysis on Confusion Matrices. Firstly, we show the confusion matrices constructed from the clustering analysis. The diagonals of the matrices show the number of data points that are clustered into the default subject clusters. The number of data with correct clustering labels can be directly computed by summing over all the diagonals in each matrix. Due to space limit, we did not show all the confusion matrices on all data sets. Figs. (6-9) show the confusion matrices on data sets UMIST, YALE, MNIST, PIE. It is easy to see that the diagonals of confusion matrices in L are much stronger than those of standard NMF approach.

Clustering Results Analysis. Table 3 summarizes the cluster-ing results of our approach by making comparisons with standard NMF and k-means clustering. The metrics we used here are clus-tering accuracy(ACC), normalized mutual information(NMI) and purity(PUR). Extensive experiments are made on 10 data sets, 6 of which are image data sets and the other 4 are from UCI data sets. The experiments can also be easily conducted on other text/web data sets. On each dataset, the clustering number is set to the real number of classes in ground truth (e.g., for data set AT&amp;T, K = 40; for data set MNIST, K = 10). For fairness, we use the same generated F and G to compare the performance of NMF and L NMF in each round. k-means algorithm is run 10 times to get the clustering results with the least object function values. From Table Figure 6: Comparisons of confusion matrices of L 2 , 1 NMF(left in each panel) and NMF(right in each panel) on dataset UMIST. Each column of a matrix represents the instances in a predicted class, and each row represents the instances in an actual class.
 Figure 7: Comparisons of confusion matrices of L 2 , 1 NMF(left) and NMF(right) on dataset Yale.
 Figure 8: Comparisons of confusion matrices of L 2 , 1 NMF(left) and NMF(right) on dataset MNIST.
 Figure 9: Comparisons of confusion matrices of L 2 , 1 NMF(left) and NMF(right) on dataset PIE. 3, we can see, (1). L 2 , 1 NMF approach performs better than stan-dard NMF approach on all data sets. (2). L 2 , 1 NMF outperforms k-means clustering on all data sets.
Here we briefly discuss the extension of NMF to another form of robust NMF: L 1 -NMF. The most robust formulation of the error function is For computational reasons, we replace | ( X  X  FG ) ij | by ( X minimize the objective function Formally, L 1 -NMF is formulated as Extending our approach in deriving the computational algorithm for L 2 , 1 -NMF, we can derive the following updating algorithms for L 1 -NMF: where W is a matrix given by W ij = ( X  X  FG ) 2 ij + 2 and  X  is the Hadamard product, i.e., elementwise product between two matrices. Here we assume Hadamard product has higher op-erator precedence over regular matrix product, i.e., AB  X  A ( B  X  C ) D . Convergence property and correctness analysis can be similarly established. Details will be published in a forthcoming paper.

A striking feature of the L 1 -NMF updating algorithm [Eqs.(62-if we replace D by W ; they are also nearly-identical to the algo-rithm [Eqs.(2,3)] of standard NMF. Analysis of this unified NMF algorithmic framework will be presented in the forthcoming paper. In this paper, we propose a robust formulation of NMF using L21-norm. We also derive a computational algorithm with rigorous convergence analysis. Experiments on 10 datasets show that the robust NMF provides more faithful basis factors and consistently better clustering results as compared to standard NMF.
 Acknowledgements . This work is partially supported by NSF-CCF-0939187, NSF-CCF-0917274, NSF-DMS-15228. [1] J.-P. Brunet, P. Tamayo, T. Golub, and J. Mesirov. Metagenes [2] D. Cai, X. He, X. Wu, and J. Han. Non-negative matrix [3] M. Cooper and J. Foote. Summarizing video using [4] I. Dhillon and S. Sra. Generalized nonnegative matrix [5] C. Ding, X. He, and H. Simon. On the equivalence of [6] C. Ding, T. Li, and M. Jordan. Convex and semi-nonnegative [7] C. Ding, T. Li, and W. Peng. Nonnegative matrix [8] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [9] C. Ding, D. Zhou, X. He, and H. Zha. R1-pca: Rotational [10] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman. [11] Q. Gu and J. Zhou. Co-clustering on manifolds. In KDD , [12] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.
 [13] D. D. Lee and H. S. Seung. Algorithms for non-negative [14] S. Li, X. Hou, H. Zhang, and Q. Cheng. Learning spatially [15] T. Li, C. Ding, and M. I. Jordan. Solving consensus and [16] F. Nie, C. H. Q. Ding, D. Luo, and H. Huang. Improved [17] F. Nie, H. Huang, X. Cai, and C. Ding. Efficient and robust [18] F. Nie, D. Xu, I. W. Tsang, and C. Zhang. Spectral embedded [19] P. Paatero and U. Tapper. Positive matrix factorization: A [20] V. P. Pauca, F. Shahnaz, M. Berry, and R. Plemmons. Text [21] F. Sha, L. K. Saul, and D. D. Lee. Multiplicative updates for [22] N. Srebro, J. Rennie, and T. Jaakkola. Maximum margin [23] F. Wang, T. Li, and C. Zhang. Semi-supervised clustering via [24] N. Z. Weixiang Liu and Q. You. Nonnegative matrix [25] Y.-L. Xie, P. Hopke, and P. Paatero. Positive matrix
