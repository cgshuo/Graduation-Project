 1. Introduction
Recently, the increasing use of temporal data, in particular time series data, has initiated various research and development attempts in the field of data mining. Time series is an important class of temporal data objects, and it can be easily obtained from scientific and financial applications (e.g. electrocardiogram (ECG), daily temperature, weekly sales totals, and prices of mutual funds and stocks). A time series is a collection of observations made chronologically. The nature of time series data includes: large in data size, high dimensionality and update continuously. Moreover time series data, which is characterized by its numerical and continuous nature, is always considered as a whole instead of individual numerical field. Therefore, unlike traditional databases where similarity search is exact match based, similarity search in time series data is typically carried out in an approximate manner.

There are various kinds of time series data related research, for example, finding similar time series ( Agrawal et al., 1993a; Berndt and Clifford, 1996; Chan and Fu, 1999 ), subsequence searching in time series ( Faloutsos et al., 1994 ), dimensionality reduction ( Keogh, 1997b; Keogh et al., 2000 ) and segmentation ( Abonyi et al., 2005 ). Those researches have been studied in considerable detail by both database and pattern recognition communities for different domains of time series data ( Keogh and Kasetty, 2002 ).
In the context of time series data mining, the fundamental problem is how to represent the time series data. One of the common approaches is transforming the time series to another domain for dimensionality reduction followed by an indexing mechanism. Moreover similarity measure between time series or time series subsequences and segmentation are two core tasks for various time series mining tasks. Based on the time series representation, different mining tasks can be found in the literature and they can be roughly classified into four fields: pattern discovery and clustering, classification, rule discovery and summarization. Some of the research concentrates on one of these fields, while the others may focus on more than one of the above processes. In this paper, a comprehensive review on the existing time series data mining research is given. Three state-of-the-art time series data mining issues, streaming, multi-attribute time series data and privacy are also briefly introduced.
 The remaining part of this paper is organized as follows:
Section 2 contains a discussion of time series representation and indexing. The concept of similarity measure, which includes both whole time series and subsequence matching, based on the raw time series data or the transformed domain will be reviewed in
Section 3. The research work on time series segmentation and visualization will be discussed in Sections 4 and 5, respectively. In
Section 6, vary time series data mining tasks and recent time series data mining directions will be reviewed, whereas the conclusion will be made in Section 7. 2. Time series representation and indexing
One of the major reasons for time series representation is to reduce the dimension (i.e. the number of data point) of the original data. The simplest method perhaps is sampling ( Astrom, 1969 ). In this method, a rate of m/n is used, where m is the length of a time series P and n is the dimension after dimensionality reduction ( Fig. 1 ). However, the sampling method has the drawback of distorting the shape of sampled/compressed time series, if the sampling rate is too low.
 each segment to represent the corresponding set of data points.
Again, with time series P  X  X  p 1 , ... , p m  X  and n is the dimension after dimensionality reduction, the  X  X  X ompressed X  X  time series ^
P  X  X  ^ p 1 , ... , ^ p n  X  can be obtained by ^ p k  X  1 e where s k and e k denote the starting and ending data points of the k th segment in the time series P , respectively ( Fig. 2 ). That is, using the segmented means to represent the time series ( Yi and
Faloutsos, 2000 ). This method is also called piecewise aggregate approximation (PAA) by Keogh et al. (2000) . 1 Keogh et al. (2001a) propose an extended version called an adaptive piecewise constant approximation (APCA), in which the length of each segment is not fixed, but adaptive to the shape of the series. A signature technique is proposed by Faloutsos et al. (1997) with similar ideas. Besides using the mean to represent each segment, other methods are proposed. For example, Lee et al. (2003) propose to use the segmented sum of variation (SSV) to represent each segment of the time series. Furthermore, a bit level approximation is proposed by Ratanamahatana et al. (2005) and Bagnall et al. (2006) , which uses a bit to represent each data point. is to approximate a time series with straight lines. Two major categories are involved. The first one is linear interpolation. A common method is using piecewise linear representation (PLR) ( Keogh, 1997b; Keogh and Smyth, 1997; Smyth and Keogh, 1997 ).
The approximating line for the subsequence P ( p i , y , p the line connecting the data points p i and p j . It tends to closely align the endpoint of consecutive segments, giving the piecewise approximation with connected lines. PLR is a bottom-up algo-rithm. It begins with creating a fine approximation of the time series, so that m /2 segments are used to approximate the m length time series and iteratively merges the lowest cost pair of segments, until it meets the required number of segment. When the pair of adjacent segments S i and S i +1 are merged, the cost of merging the new segment with its right neighbor and the cost of merging the S i +1 segment with its new larger neighbor is calculated. Ge (1998) extends PLR to hierarchical structure.
Furthermore, Keogh and Pazzani enhance PLR by considering weights of the segments ( Keogh and Pazzani, 1998 ) and relevance feedback from the user ( Keogh and Pazzani, 1999 ). The second approach is linear regression, which represents the subsequences with the best fitting lines ( Shatkay and Zdonik, 1996 ).
Furthermore, reducing the dimension by preserving the salient points is a promising method. These points are called as perceptually important points (PIP). The PIP identification process is first introduced by Chung et al. (2001) and used for pattern matching of technical (analysis) patterns in financial applications.
With the time series P , there are n data points: P 1 , P data points in P can be reordered by its importance by going through the PIP identification process. The first data point P the last data point P n in the time series are the first and two PIPs, respectively. The next PIP that is found will be the point in P with maximum distance to the first two PIPs. The fourth PIP that is found will be the point in P with maximum vertical distance to the line joining its two adjacent PIPs, either in between the first and second PIPs or in between the second and the last PIPs. The
PIP location process continues until all the points in P are attached to a reordered list L or the required number of PIPs is reached (i.e. reduced to the required dimension). Seven PIPs are identified in from the sample time series in Fig. 3 . Detailed treatment can be found in Fu et al. (2008c) .

The idea is similar to a technique proposed about 30 years ago for reducing the number of points required to represent a line by
Douglas and Peucker (1973) (see also Hershberger and Snoeyink, 1992 ). Perng et al. (2000) use a landmark model to identify the important points in the time series for similarity measure. Man and Wong (2001) propose a lattice structure to represent the identified peaks and troughs (called control points) in the time series. Pratt and Fink (2002) and Fink et al. (2003) define extrema as minima and maxima in a time series and compress the time series by selecting only certain important extrema and dropping the other points. The idea is to discard minor fluctuations and keep major minima and maxima. The compression is controlled by the compression ratio with parameter R , which is always greater than one; an increase of R leads to the selection of fewer points. That is, given indices i and j , where i r x r j , a point p series P is an important minimum if p x is the minimum among p , y , p j , and p i / p x Z R and p j / p x Z R . Similarly, p maximum if p x is the maximum among p i , y , p j and p x p / p j Z R . This algorithm takes linear time and constant memory. It outputs the values and indices of all important points, as well as the first and last point of the series. This algorithm can also process new points as they arrive, without storing the original series. It identifies important points based on local information of each segment (subsequence) of time series. Recently, a critical point model (CPM) ( Bao, 2008 ) and a high-level representation based on a sequence of critical points ( Bao and Yang, 2008 ) are proposed for financial data analysis. On the other hand, special points are introduced to restrict the error on PLR ( Jia et al., 2008 ). Key points are suggested to represent time series in ( Leng et al., 2009 ) for an anomaly detection.

Another common family of time series representation approaches converts the numeric time series to symbolic form. That is, first discretizing the time series into segments, then converting each segment into a symbol ( Yang and Zhao, 1998; Yang et al., 1999; Motoyoshi et al., 2002; Aref et al., 2004 ). Lin et al. (2003; 2007) propose a method called symbolic aggregate approximation (SAX) to convert the result from PAA to symbol string. The distribution space ( y -axis) is divided into equiprobable regions. Each region is represented by a symbol and each segment can then be mapped into a symbol corresponding to the region in which it resides. The transformed time series ^ P using PAA is finally converted to a symbol string SS ( s 1 , y , s W ). In between, two parameters must be specified for the conversion. They are the length of subsequence w and alphabet size A (number of symbols used). Besides using the means of the segments to build the alphabets, another method uses the volatility change to build the alphabets. Jonsson and Badal (1997) use the  X  X  X hape Description Alphabet (SDA) X  X . Example symbols like highly increasing transi-tion, stable transition, and slightly decreasing transition are adopted. Qu et al. (1998) use gradient alphabets like upward, flat and download as symbols. Huang and Yu (1999) suggest transforming the time series to symbol string, using change ratio between contiguous data points.

Megalooikonomou et al. (2004) propose to represent each segment by a codeword from a codebook of key-sequences. This work has extended to multi-resolution consideration ( Megalooi-konomou et al., 2005 ). Morchen and Ultsch (2005) propose an unsupervised discretization process based on quality score and persisting states. Instead of ignoring the temporal order of values like many other methods, the Persist algorithm incorporates temporal information.

Furthermore, subsequence clustering is a common method to generate the symbols ( Das et al., 1998; Li et al., 2000a; Hugueney and Meunier, 2001; Hebrail and Hugueney, 2001 ). A multiple abstraction level mining (MALM) approach is proposed by Li et al. (1998) , which is based on the symbolic form of the time series.
The symbols in this paper are determined by clustering the features of each segment, such as regression coefficients, mean square error and higher order statistics based on the histogram of the regression residuals.

Most of the methods described so far are representing time series in time domain directly. Representing time series in the transformation domain is another large family of approaches. One of the popular transformation techniques in time series data mining is the discrete Fourier transforms (DFT), since first being proposed for use in this context by Agrawal et al. (1993a) . Rafiei and Mendelzon (2000) develop similarity-based queries, using
DFT. Janacek et al. (2005) propose to use likelihood ratio statistics to test the hypothesis of difference between series instead of an
Euclidean distance in the transformed domain. Recent research uses wavelet transform to represent time series ( Struzik and
Siebes, 1998 ). In between, the discrete wavelet transform (DWT) has been found to be effective in replacing DFT ( Chan and Fu, 1999 ) and the Haar transform is always selected ( Struzik and
Siebes, 1999; Wang and Wang, 2000 ). The Haar transform is a series of averaging and differencing operations on a time series ( Chan and Fu, 1999 ). The average and difference between every two adjacent data points are computed. For example, given a time series P  X  (1, 3, 7, 5), dimension of 4 data points is the full resolution (i.e. original time series); in dimension of two coefficients, the averages are (2 6) with the coefficients ( 11) and in dimension of 1 coefficient, the average is 4 with coefficient ( 2). A multi-level representation of the wavelet transform is proposed by Shahabi et al. (2000) . Popivanov and Miller (2002) show that a large class of wavelet transformations can be used for time series representation. Dasha et al. (2007) compare different wavelet feature vectors. On the other hand, comparison between
DFT and DWT can be found in Wu et al. (2000b) and Morchen (2003) and a combination use of Fourier and wavelet transforms are presented in Kawagoe and Ueda (2002) . An ensemble-index, is proposed by Keogh et al. (2001b) and Vlachos et al. (2006) , which ensembles two or more representations for indexing.

Principal component analysis (PCA) is a popular multivariate technique used for developing multivariate statistical process monitoring methods ( Yang and Shahabi, 2005b; Yoon et al., 2005 ) and it is applied to analyze financial time series by Lesch et al. (1999) . In most of the related works, PCA is used to eliminate the less significant components or sensors and reduce the data representation only to the most significant ones and to plot the data in two dimensions. The PCA model defines linear hyperplane, it can be considered as the multivariate extension of the PLR. PCA maps the multivariate data into a lower dimensional space, which is useful in the analysis and visualization of correlated high-dimensional data. Singular value decomposition (SVD) ( Korn et al., 1997 ) is another transformation-based approach.
Other time series representation methods include modeling time series using hidden markov models (HMMs) ( Azzouzi and
Nabney, 1998 ) and a compression technique for multiple stream is proposed by Deligiannakis et al. (2004) . It is based on base signal, which encodes piecewise linear correlations among the collected data values. In addition, a recent biased dimension reduction technique is proposed by Zhao and Zhang (2006) and Zhao et al. (2006) .
 above are incorporated with different indexing methods. A common approach is adopted to an existing multidimensional indexing structure (e.g. R-tree proposed by Guttman (1984) ) for the representation. Agrawal et al. (1993a) propose an F-index, which adopts the R * -tree ( Beckmann et al., 1990 ) to index the first few DFT coefficients. An ST-index is further proposed by ( Faloutsos et al. (1994 ), which extends the previous work for subsequence handling. Agrawal et al. (1995a) adopt both the R * -and R+-tree ( Sellis et al., 1987 ) as the indexing structures. A multi-level distance based index structure is proposed ( Yang and Shahabi, 2005a ), which for indexing time series represented by
PCA. Vlachos et al. (2005a) propose a Multi-Metric (MM) tree, which is a hybrid indexing structure on Euclidean and periodic spaces. Minimum bounding rectangle (MBR) is also a common technique for time series indexing ( Chu and Wong, 1999; Vlachos et al., 2003 ). An MBR is adopted in ( Rafiei, 1999 ) which an MT-index is developed based on the Fourier transform and in ( Kahveci and Singh, 2004 ) which a multi-resolution index is proposed based on the wavelet transform. Chen et al. (2007a) propose an indexing mechanism for PLR representation. On the other hand,
Kim et al. (1996) propose an index structure called TIP-index (TIme series Pattern index) for manipulating time series pattern databases. The TIP-index is developed by improving the extended multidimensional dynamic index file (EMDF) ( Kim et al., 1994 ).
An iSAX ( Shieh and Keogh, 2009 ) is proposed to index massive time series, which is developed based on an SAX. A multi-resolution indexing structure is proposed by Li et al. (2004) , which can be adapted to different representations.
 indexing depends only on the precision of the approximation in the reduced dimensionality space. However in choosing a dimensionality reduction technique, we cannot simply choose an arbitrary compression algorithm. It requires a technique that produces an indexable representation. For example, many time series can be efficiently compressed by delta encoding, but this representation does not lend itself to indexing. In contrast, SVD,
DFT, DWT and PAA all lend themselves naturally to indexing, with each eigenwave, Fourier coefficient, wavelet coefficient or aggregate segment map onto one dimension of an index tree.
Post-processing is then performed by computing the actual distance between sequences in the time domain and discarding any false matches. 3. Similarity measure of time series analysis and data mining tasks. Most of the representation approaches discussed in Section 2 also propose the similarity measure method on the transformed representation scheme. In traditional databases, similarity measure is exact match based. However in time series data, which is characterized by its numerical and continuous nature, similarity measure is typically carried out in an approximate manner. Consider the stock time series, one may expect having queries like: the closing prices of all high-tech stocks .
 for different stock analysis activities. Queries like Query2 in fact is tightly coupled with the patterns frequently used in technical analysis, e.g. double top/bottom, ascending triangle, flag and rounded top/bottom.

In time series domain, devising an appropriate similarity function is by no means trivial. There are essentially two ways the data that might be organized and processed ( Agrawal et al., 1993a ). In whole sequence matching, the whole length of all time series is considered during the similarity search. It requires comparing the query sequence to each candidate series by evaluating the distance function and keeping track of the sequence with the smallest distance. In subsequence matching, where a query sequence Q and a longer sequence P are given, the task is to find the subsequences in P , which matches Q .
Subsequence matching requires that the query sequence Q be placed at every possible offset within the longer sequence P . With respect to Query1 and Query2 above, they can be considered as a whole sequence matching and a subsequence matching, respec-tively. Gavrilov et al. (2000) study the usefulness of different similarity measures for clustering similar stock time series. 3.1. Whole sequence matching
To measure the similarity/dissimilarity between two time series, the most popular approach is to evaluate the Euclidean distance on the transformed representation like the DFT coeffi-cients ( Agrawal et al., 1993a ) and the DWT coefficients ( Chan and
Fu, 1999 ). Although most of these approaches guarantee that a lower bound of the Euclidean distance to the original data,
Euclidean distance is not always being the suitable distance function in specified domains ( Keogh, 1997a; Perng et al., 2000;
Megalooikonomou et al., 2005 ). For example, stock time series has its own characteristics over other time series data (e.g. data from scientific areas like ECG), in which the salient points are important.

Besides Euclidean-based distance measures, other distance measures can easily be found in the literature. A constraint-based similarity query is proposed by Goldin and Kanellakis (1995) , which extended the work of ( Agrawal et al., 1993a ). Das et al. (1997) apply computational geometry methods for similarity measure. Bozkaya et al. (1997) use a modified edit distance function for time series matching and retrieval. Chu et al. (1998) propose to measure the distance based on the slopes of the segments for handling amplitude and time scaling problems.
A projection algorithm is proposed by Lam and Wong (1998) .A pattern recognition method is proposed by Morrill (1998) , which is based on the building blocks of the primitives of the time series.
Ruspini and Zwir (1999) devote an automated identification of significant qualitative features of complex objects. They propose the process of discovery and representation of interesting relations between those features, the generation of structured indexes and textual annotations describing features and their relations. The discovery of knowledge by an analysis of collections of qualitative descriptions is then achieved. They focus on methods for the succinct description of interesting features lying in an effective frontier. Generalized clustering is used for extracting features, which interest domain experts. The general-ized Markov models are adopted for waveform matching in Ge and Smyth (2000) . A content-based query-by-example retrieval model called FALCON is proposed by Wu et al. (2000a) , which incorporates a feedback mechanism.

Indeed, one of the most popular and field-tested similarity measures is called the  X  X  X ime warping X  X  distance measure. Based on the dynamic time warping (DTW) technique, the proposed method in (Berndt and Clifford, 1994) predefines some patterns to serve as templates for the purpose of pattern detection. To align two time series, P and Q , using DTW, an n-by -m matrix M is first constructed. The ( i th, j th) element of the matrix, m ij distance d ( q i ,p j ) between the two points q i and p j Euclidean distance is typically used, i.e. d ( q i ,p j )  X  ( q corresponds to the alignment between the points q i and p warping path, W , is a contiguous set of matrix elements that defines a mapping between Q and P . Its k th element is defined as w  X  ( i k ,j k ) and W  X  w 1 , w 2 , ... , w k , ... , w K  X  2  X  where max  X  m , n  X  r K o m  X  n 1.

The warping path is typically subjected to the following constraints. They are boundary conditions, continuity and mono-tonicity. Boundary conditions are w 1  X  (1,1) and w K  X  ( m,n ). This requires the warping path to start and finish diagonally. Next constraint is continuity. Given w k  X  ( a,b ), then w k 1 where a a u r 1 and b b u r 1. This restricts the allowable steps in the warping path being the adjacent cells, including the diagonally adjacent cell. Also, the constraints a a uZ 0 and b b uZ 0 force the points in W to be monotonically spaced in time.
There is an exponential number of warping paths satisfying the above conditions. However, only the path that minimizes the warping cost is of interest. This path can be efficiently found by using dynamic programming ( Berndt and Clifford, 1996 )to evaluate the following recurrence equation that defines the cell and the minimum of the cumulative distances of the adjacent elements, i.e.  X  i , j  X  X  d  X  q i , p j  X  X  min f g  X  i 1 , j 1  X  , g  X  i 1 , j  X  ,
A warping path, W , such that  X  X  X istance X  X  between them is minimized, can be calculated by a simple method DTW  X  Q , P  X  X  min where d  X  w k  X  can be defined as d  X  w k  X  X  d  X  q i k , p i k  X  X  X  q i k p i k  X  2  X  5  X 
Detailed treatment can be found in Kruskall and Liberman (1983) . As DTW is computationally expensive, different methods are proposed to speedup the DTW matching process. Different constraint (banding) methods, which control the subset of matrix that the warping path is allowed to visit, are reviewed in Ratanamahatana and Keogh (2004) . Yi et al. (1998) introduce a technique for an approximate indexing of DTW that utilizes a FastMap technique, which filters the non-qualifying series. Kim et al. (2001) propose an indexing approach under DTW similarity measure. Keogh and Pazzani (2000b) introduce a modification of DTW, which integrates with PAA and operates on a higher level abstraction of the time series. An exact indexing approach, which is based on representing the time series by PAA for DTW similarity measure is further proposed by Keogh (2002) .An iterative deepening dynamic time warping (IDDTW) is suggested by Chu et al. (2002) , which is based on a probabilistic model of the approximate errors for all levels of approximation prior to the query process. Chan et al. (2003) propose a filtering process based on the Haar wavelet transformation from low resolution approx-imation of the real-time warping distance. Shou et al. (2005) use an APCA approximation to compute the lower bounds for DTW distance. They improve the global bound proposed by Kim et al. (2001) , which can be used to index the segments and propose a multi-step query processing technique. A FastDTW is proposed by Salvador and Chan (2004) . This method uses a multi-level approach that recursively projects a solution from a coarse resolution and refines the projected solution. Similarly, a fast
DTW search method, an FTW is proposed by Sakurai et al. (2005) for efficiently pruning a significant number of search candidates. Ratanamahatana and Keogh (2005) clarified some points about
DTW where are related to lower bound and speed. Euachongprasit and Ratanamahatana (2008) also focus on this problem. A sequentially indexed structure (SIS) is proposed by Ruengron-ghirunya et al. (2009) to balance the tradeoff between indexing efficiency and I / O cost during DTW similarity measure. A lower bounding function for group of time series, LBG, is adopted.
On the other hand, Keogh and Pazzani (2001) point out the potential problems of DTW that it can lead to unintuitive alignments, where a single point on one time series maps onto a large subsection of another time series. Also, DTW may fail to find obvious and natural alignments in two time series, because of a single feature (i.e. peak, valley, inflection point, plateau, etc.).
One of the causes is due to the great difference between the lengths of the comparing series. Therefore, besides improving the performance of DTW, methods are also proposed to improve an accuracy of DTW. Keogh and Pazzani (2001) propose a modifica-tion of DTW that considers the higher level feature of shape for better alignment. Ratanamahatana and Keogh (2004) propose to learn arbitrary constraints on the warping path. Regression time warping (RTW) is proposed by Lei and Govindaraju (2004) to address the challenges of shifting, scaling, robustness and complexity. Latecki et al. (2005) propose a method called the minimal variance matching (MVM) for elastic matching. It determines a subsequence of the time series that best matches a query series by finding the cheapest path in a directed acyclic graph. A segment-wise time warping distance (STW) is proposed by Zhou and Wong (2005) for time scaling search. Fu et al. (2008a) propose a scaled and warped matching (SWM) approach for handling both DTW and uniform scaling simultaneously. Different customized DTW techniques are applied to the field of music research for query by humming ( Zhu and Shasha, 2003; Arentz et al., 2005 ).
 Focusing on similar problems as DTW, the Longest Common Subsequence (LCSS) model ( Vlachos et al., 2002 ) is proposed. The
LCSS is a variation of the edit distance and the basic idea is to match two sequences by allowing them to stretch, without rearranging the sequence of the elements, but allowing some elements to be unmatched. One of the important advantages of an LCSS over DTW is the consideration on the outliers. Chen et al. (2005a) further introduce a distance function based on an edit distance on real sequence (EDR), which is robust against the data imperfection. Morse and Patel (2007) propose a Fast
Time Series Evaluation (FTSE) method which can be used to evaluate the threshold value of these kinds of techniques in a faster way.

Threshold-based distance functions are proposed by ABfalg et al. (2006) . The proposed function considers intervals, during which the time series exceeds a certain threshold for comparing time series rather than using the exact time series values. A T-Time application is developed ( ABfalg et al., 2008 ) to demonstrate the usage of it. Fu et al. (2007) further suggest to introduce rules to govern the pattern matching process, if a priori knowledge exists in the given domain.
A parameter-light distance measure method based on Kolmo-gorov complexity theory is suggested in Keogh et al. (2007b) .
Compression-based dissimilarity measure (CDM) 3 is adopted in this paper. Chen et al. (2005b) present a histogram-based representation for similarity measure. Similarly, a histogram-based similarity measure, bag-of-patterns (BOP) is proposed by
Lin and Li (2009) . The frequency of occurrences of each pattern in the time series is counted and compared by CDM. Lang et al. (2010) develop a dictionary compression score for similarity measure. A dictionary-based compression technique is suggested to compute long time series similarity. 3.2. Subsequence matching time series are given, the task is to find the subsequences in the longer time series, which matches the query sequence. The query sequence is required to place at every offset within the longer time series. Faloutsos et al. (1994) generalizes the work in
Agrawal et al. (1993a) for subsequence searching. Based on this work, many researches are conducted to improve the perfor-mance of the subsequence searching. For example, a DualMatch is proposed by Moon et al. (2001) to divide the time series into disjoint windows and query pattern into sliding windows. Loh and Kim (2001) extend ( Faloutsos et al., 1994 ) using an index interpolation to solve the storage and CPU time overhead. which reduces the window size effect by using large windows by the method in Faloutsos et al. (1994) and exploits point-filtering effect by DualMatch ( Moon et al., 2001 ). Furthermore, Kim and
Jeong (2007) discuss on the potential performance bottleneck during subsequence matching. Four major areas, that time required to process subsequence matching, are identified. They are processing time, disk access time and the corresponding post-processing steps of them. A window ordering method is proposed to eliminate the redundancies of disk access and CPU processing in the post-processing steps. A method based on an index interpolation is further proposed by Lim et al. (2007) to improve the performance of DualMatch.
 segments ( Morinaka et al., 2001 ), anomaly subsequence detection using an SAX ( Keogh et al., 2005 ), online subsequence matching using
PLR ( Wu et al., 2004 ) and weighted subsequence matching using PLR ( Wu et al., 2005 ) can be found in the literature. All these approaches including the DFT approaches are based on lower bounding of the Euclidean distance.
 algorithm for locating subsequences in the transformed domain (e.g. DFT, wavelet) hierarchically. An indexing method, S presented by Wang and Perng (2001) for subsequence matching, which is based on string searching techniques on different time series representation schemes.
 ( Gusfield, 1997 ) is proposed to index the DTW for subsequence matching ( Park et al., 1999; Park et al., 2000; Kim et al., 2002 ).
Other approaches include a segment-based approach based on piecewise time warping ( Park et al., 2001a ), an index-based approach based on prefix querying ( Park et al., 2001b ) and an optimization approach ( Kim et al., 2005 ).
 algorithm to reduce the number of subsequence needs to access by defining the minimum-distance matching-window pair (MDMWP). Directed acyclic graph (DAG) is adopted in Dorr and
Denton (2009) to capture the relationship between subsequences and patterns.
 measure and pattern matching include forecasting by pattern recognition ( Singh and Stuart, 1998; Liu et al., 2004 ) and defining query language. Shape definition query (SDL) is introduced by
Agrawal et al. (1995b) for retrieving objects based on the shapes contained in the histories associated with these objects. Jagadish et al. (1995) propose a framework, which include a pattern language, a transformation rule language and a query language, for defining queries in terms of similarity of objects. Lin and Risch (1998) extend the SELECT operator in SQL that retrieves implicit values from a discrete time sequence under various user-defined interpolation assumptions. Anand et al. (2001) propose a chart-pattern language (CPL) to enable financial analysts to define patterns with subjective criteria and incrementally compose complex patterns from simpler patterns for pattern query. Dong et al. (2009) propose to measure the shape distance of the time series. The shapes are described according to the relative changes of the slopes lines. Finally, detail comparisons or experiments on the existing time series representation and similarity measure approaches can be found in Keogh and Kasetty (2002) and Ding et al. (2008) . 4. Segmentation
Time series segmentation can be considered either as a preprocessing step for variety of data mining tasks or as trend analysis techniques. It is also considered as a discretization problem. Unlike transactional databases with discrete items, time series data is characterized by their numerical and continuous nature. In Das et al. (1998) , a simple discretization method is proposed. A fixed length window is used to segment a time series into subsequences and the time series is then represented by the primitive shape patterns that are formed. This discretization process mainly depends on the choice of the window width.
However, using fixed-length segmentation is an over-simplified approach to solve the problem. There are at least two identified disadvantages. First, meaningful patterns typically appear with different lengths throughout a time series. Second, as a result of the even segmentation of a time series, meaningful patterns may be missed if they are split across time (cutting) points. Thus, it is better to use a dynamic approach, which identifies the time points in a more flexible way (i.e. using different window widths).
This is certainly not a trivial segmentation problem. Common segmentation methods include using the PIP ( Fu et al., 2006; Jiang et al., 2007 ) or detecting special events ( Guralnik and Srivastava, 1999 ) in the time series as the time points, minimum message length (MML) ( Oliver et al., 1998 ) and minimum description length (MDL) segmentation ( Fitzgibbon et al., 2002 ). Fancourt and
Principe (1997) adopt PCA for the segmentation problem. Based on PCA, a fuzzy clustering based segmentation is proposed by
Abonyi et al., (2003, 2005) . A two stages approach which first uses piecewise generalized likelihood ratio (GLR) to rough segmenta-tion and then refines the results is proposed by Wang and Willett (2004) . On the other hand, Keogh et al. (2001c) adopt PLR to segment the time series. They focus on the problem of an online segmentation of time series and a sliding window and bottom-up (SWAB) approach is proposed.

Oliver and Forbes (1997) suggest that the time points are identified at which behavior changes occur in a time series. In the statistical term, this is called the  X  X  X hange-point detection problem X  X . The standard solution involves fixing the number of change-point, then identifying their positions, and finally deter-mining functions for curve fitting the intervals between succes-sive change-points. Chu (1995) presents a sliding test window segmentation procedure which is based on non-stationary detection on fluctuation statistics and change-point localization.
An iterative algorithm is proposed by Guralnik and Srivastava (1999) that fits a model to a time segment and then uses a likelihood criterion to determine if the segment should be partitioned further. Srivastava and Weigend (1996) suggest discovering the underlying switching process in a time series, which entails identifying the number of sub-process and the dynamics of each sub-process. The concept of the nonlinear gated experts derived from statistical physics is proposed to perform the segmentation. In Duncan and Bryant (1996) , dynamic program-ming is proposed to determine the total number of intervals within the data, the location of these intervals and the order of the model within each segment. In ( Srivastava et al., 1999) , the segmentation problem is considered with a tool for exploratory data analysis and data mining called the scale-sensitive gated experts (SSGE), which can partition a complex nonlinear regres-sion surface into a set of simpler surfaces called  X  X  X eatures X  X . An improved annealed competition of experts algorithm (ACE) identifies switching dynamics in time series using on mutual information and false nearest neighbor to determine appropriate embedding dimension and time delay ( Feng et al., 2005 ).
The segmentation problem has also been considered from the perspective of finding cyclic periodicity for all of the segments. In Han et al. (1998, 1999) , the data cube and the Apriori data mining techniques are used to mine segment-wise periodicity, using a fixed length period. An off-line technique for the competitive identification of piecewise stationary time series is described by Fancourt and Principe (1996) . In addition to performing piecewise segmentation and identification, the proposed techni-que maps similar segments of a time series as neighbors on a neighborhood map.

Himberg et al. (2001) propose a global iterative replacement (GIR) method, which approximates the dynamic programming result for minimizing the intra segment variances. The proposed method is applied to context recognition for the mobile phone applications.

Although the approaches described in this section can generally identify a given pattern from a time series, they do not consider the problemofidentifyingasuitablesetoftimepointsinatimeseries, when a set of pattern templates is given; for example, the technical order to form a versatile mining space, a variety of patterns (e.g. in different resolutions) have to b e identified. The aforementioned segmentation task can be regarded as an optimization problem and Chung et al. (2004) propose a solution, which is based on an evolutionary computation. 5. Visualization
Visualization is an important mechanism to present the processed time series for further analysis by users. It is also a powerful tool to facilitate the mining tasks like pattern searching, query-by-example, and pattern discovery afterwards. Current tools for visualizing time series include: (1) cluster and calendar-based visualization tool ( van Wijk and van Selow, 1999 ), which obtains chunks of data with a given interval and then clusters them accordingly and (2) spiral visualization tool ( Weber et al., 2001 ), which maps the periodic section of time series into a ring.
These two tools are focused on periodic time series and a fixed length of period must be provided, say, weekly or monthly.
A financial visual analytics system for pattern-based analysis of 2-dimensional time-vary chart data is proposed by Schreck et al. (2007) . Hao et al. (2007) introduce the notion of degree of interest (DOI) to define and generate multi-resolution layouts of long time series. Non-linear rescaling and space-efficient rendering method are used to visualize the long time series.

Keogh et al. (2002a) and Hochheiser and Shneiderman (2004) developed a tool called TimeSearcher which is a time series exploratory and visualization tool, so that a user can retrieve time series by querying. Based on their previous developed TimeBoxes, which are rectangular, direct-manipulation time series queries, they extend it by introducing variable time timeboxes (VTT), which permits the specification of queries to allow uncertainty in the time axis. Four methods (sample events, aggregated sample events, event index, and interleaved event index) to represent the unevenly space time series data are studied in Aris et al. (2005) .
Furthermore, TimeSearcher2 is developed by Buono et al. (2005) , in which a new search interface combining both filter and pattern search capability is provided. TimeSearcher is focused on multiple time series query based on examples. Specification of the region of interest must be provided.

Recently, another time series visualization tool called VizTree is proposed ( Lin et al., 2005a ). This approach first converts each numeric time series to a symbol string based on the SAX and a set of substrings (with the same number of symbol) extracted from thesymbolstringisencodedbyamodifiedsuffixtreetovisualize the frequency of patterns. That is, the SAX discretizes the original time series into fixed length subsequences, converts each subse-quence to a symbol and the symbols obtained are concatenated to form a symbol string. Given a symbol string, say abccbccbcabcbcc ,the next step is to convert this long string to a set of substrings according to the length of each substring, W (or the number of substring), specified by users. For example, if the preferred length of substring is 3, the given string will be divided into 5 substrings if jumping window is used, i.e. abc , cbc , cbc , abc and bcc . Similarly, 13 substrings will be obtained if the sliding window is used, i.e. abc , bcc , will be constructed as exemplified in Fig. 4 . The length of substring is reflected by the depth of the tree. Each branch of the tree represents a pattern. The frequency of the pattern is represented by the thickness of each branch. As shown in Fig. 4 , frequently appearing patterns discovered by using jumping window are abc and cbc ( Fig. 4 a), while bcc and cbc are discovered by using the sliding window ( Fig. 4 b). Different applications of VizTree are suggested by the authors including subsequence matching, frequently appearing pattern discovery and surprising pattern discovery. a bc ing frequently appearing and surprising patterns in a given resolution, which is suitable for time series applications like an
ECG. The same representation (i.e. SAX) but different visualization tools are also proposed by using bitmap ( Kumar et al., 2005 ) and dot plots ( Yankov et al., 2005 ). On the other hand, Fu et al. (2008b) extend the work to the discovery of interesting patterns across different resolutions by adopting the symbolic representation of
PIP instead of an SAX in the VizTree. 6. Mining in time series knowledge from either the original or the transformed time series data. Indeed, pattern discovery is the most common mining task and the clustering method is the most commonly method. Other time series data mining tasks include classification, rule mining and summarization. 6.1. Pattern discovery and clustering include frequently appearing ( Fu et al., 2001 ) and surprising patterns ( Keogh et al., 2002b ), from time series data. These tasks are also called motif discovery ( Chiu et al., 2003; Tanaka et al., 2005 ) and anomaly detection (Chan and Mahoney, 2005; Wei et al., 2005) or finding discords (Keogh et al., 2007a) , respectively.
The discovery of interesting patterns has become one of the most important data mining tasks, and it can be applied to many domains ( Caraa-Valente and Lopez-Chavarrias, 2000; Lerner et al., 2004 ). Ma and Perkins (2003) present a support vector regression (SVR)-based online novelty detection algorithm. Chan and
Mahoney (2005) present an online anomaly detection approach based on the Gecko algorithm, which creates a sequence of minimal bounding boxes with the training trajectories. group of techniques being employed is distance-based clustering ( Das et al., 1998; Oates, 1999; Wang et al., 2002 ). The general clustering procedure is listed in Fig. 5 . In each iteration, the winner cluster is found and its center is updated accordingly. The initial cluster centers can be chosen in various ways, e.g. chosen arbitrarily or by some sequences. Also, the number of cluster is a critical parameter to be determined. It can be fixed beforehand or can vary during the clustering process. The clustering procedure finally terminates when the number of iteration exceeds the maximum allowed number of iterations or convergence.
 major problem is that time series data mostly increase linearly with time. This will cause the storage needs to increase rapidly and slow down the pattern discovery process exponentially.
Therefore, an effective mechanism for compressing the huge amount of time series data, especially historical data, is needed.
This not only reduces the size of storage, but also maintains an acceptable level of information for the discovery process. Fu et al. (2001) propose to adopt PIP representation to solve these problems. A neural clustering method, the self-organizing map (SOM) ( Kohonen, 1995 ), is used for pattern discovery. An SOM is a special type of clustering algorithm ( Ripley, 1996 ) with an immense discovery power. Spatio-temporal self-organizing fea-ture maps are proposed by Euliano and Principe (1996) . Other researches adopt an SOM for time series data including: Ultsch (1999) and Morchen et al. (2005) adopt emergent feature maps in the medical domain, Kuo et al. (2004) focus on financial domain and use the K-chart (i.e. combining open, high, low and close prices) analysis as the input of the SOM for prediction and Wang et al. (2005b) propose a dimensionality reduction method using global characteristics like trend and seasonality, periodicity, skew for feature selection before using an SOM. Guo et al. (2007) adopt an SOM for stock pattern discovery. An improved version of the rival penalized competitive learning (RPCL) is further introduced.
Moller-Levet et al. (2003) adopt the fuzzy c -means (FCM) algorithm for short time series and unevenly spaced sampling points X  time series clustering. They propose to measure the similarity of short time series based on shapes, which are formed by the relative change of amplitude and the temporal information.
Bargiela and Pedrycz (2003) discuss the notion of granular data, elaborate on the recursive information granulation and access the quality of summarization of information granules through an FCM clustering. Steinbach et al. (2003) present a clustering-based method to discover climate indices that represent regions with relatively homogeneous behavior. Lin et al. (2004) propose an anytime version of partitioned clustering algorithm, which adopts the multi-resolution property of wavelets. Anytime algorithm means trade execution time for quality of results and always has a best-so-far answer available and the quality of the answer improves with an execution time. The user may examine this answer at anytime ( Grass and Ziberstein, 1996 ). Similarly, Lin et al. (2005b) introduce another multi-resolution clustering approach based on multi-resolution PAA (MPAA) for the iterative clustering algorithm of streaming time series.

Autoregressive moving average (ARMA) and autoregressive integrated moving average (ARIMA) models have also been used extensively for time series analysis. Kalpakis et al. (2001) propose to cluster ARIMA time series, using the partition around the medoids method. Xiong and Yeung (2004) focus on the problem of clustering time series of different lengths, using mixtures of ARMA models and expectation-maximization (EM) algorithm. Bagnall and Janacek (2005) focus on clustering data derived from ARMA models, using k -means and k -medoids algorithms. A clipping process, which descretizes data into binary sequences of above and below the media, is adopted. This process is strengthens on the presence of an outlier in the data.
Hidden Markov model (HMM) is a common model-based algorithm adopted in time series clustering ( Panuccio et al., 2002 ). HMMs are defined as stochastic generalizations of finite-state automata, where both transitions between states and generation of output symbols are governed by probability distributions. Oates et al. (1999) present a hybrid time series clustering algorithm that uses DTW for rough initialization and HMM for removing the sequences that do not belong to the clusters. Yin and Yang (2005) propose to transform the sensor time series data into an equal-length vector and model it as a HMM for spectral clustering. Furthermore, a recursive HMM training process is proposed by Duan et al. (2005) .

Finding representative trends is a typical task, which belongs to the time series pattern discovery area. Indyk et al. (2000) identify various representative trends in time series over arbitrary windows of interest. An approximation approach based on replacing each interval by a  X  X  X ketch X  X , which is a low dimensional vector, is adopted. Papadimitrious and Yu (2006) also examine the time series at multiple time scales and discover the key trend in each (i.e. the optimal local patterns). Udechukwu et al. (2004) propose to convert the time series to symbolic form, build the suffix tree and discover the frequent patterns or trends accordingly.

On the other hand, discovering periodic patterns is another common focus for pattern discovery. Han et al. (1998, 1999) propose methods to discover periodicity or partial periodicity segments. Berberidis et al. (2002a) search for weak periodic signals using autocorrelation function and fast Fourier transform (FFT) with no period length are known in advance. This work extends the work from the same authors in ( Berberidis et al., 2002b ). Elfeky et al. (2004) further develop a one-pass algorithm based on convolution. A summary of the work by these authors can be found in Elfeky et al. (2005) . Vlachos et al. (2004) apply power spectral density estimation using DFT and tree index to discover important periods. It is applied on an identification of bursts for online search queries.

Cluster analysis is also applied to the sliding window of the time series for grouping related time series subsequence patterns that are dispersed along the time series. Clustering methods seek out a special type of structure, namely, grouping tendencies in the data. In this regard, they are not as general as the other approaches, but they can provide valuable information when local aggregation of the data is suspected. Das et al. (1998) propose that the pattern templates for matching are not predefined. Instead, the templates are generated automatically by clustering techniques and they will then be used for further matching in the discretization process to produce meaningful symbols. Policker and Geva (2000) describe adaptive methods for finding rules of the above type from time series data. Methods are based on discretizing the sequence by methods resembling vector quantization. Again, they first form subsequences by sliding a window through the time series, and then cluster these subsequences by using a suitable measure of time series similarity. Denton (2005) proposes a kernel-density-based clus-tering for time series subsequences.

However, applying clustering approaches to discover fre-quently appearing patterns is claimed to be meaningless when focusing on time series subsequence recently ( Keogh et al., 2003 ).
It is because when using a sliding window to discretize the long time series into subsequences in a fixed window size, patterns, which are derivations from sine curve, are always resulted no matter how the shape of the given time series is. Theoretical analysis and experiments can be found in Wang et al. (2005a) . Lin et al. (2002) state that the definition of a match is rather obvious and intuitive; but it is needed for the definition of a trivial match as it is easy to observe that the best matches to a subsequence tend to be the subsequence that begin just one or two points to the left or the right of the subsequence. They define the term trivial match as: given a time series P containing a subsequence S beginning at position p and a matching subsequence S 2 beginning at q , S 2 is a trivial match to S 1 if either p  X  q or there does not exist a subsequence S 2 0 beginning at q 0 such that D(S 1 , S 2 either q o q 0 o p or p o q 0 o q . Therefore, it is necessary to prevent the over-counting of these trivial matches.

Lin et al. (2002) and Patel et al. (2002) define the problem of enumerating the most frequently appearing patterns in a time series P ( Lin et al. (2002) referred to as the most significant motifs, 1-motif) is the subsequence S 1 that has the highest count of non-trivial matches. Therefore, the K th most frequently appearing patterns (significant motif, K -Motif) in P is the subsequence S that has the highest count of non-trivial matches and satisfies D ( S , Si ) 4 2 R , for all 1 r i o K .
 In addition, Chiu et al. (2003) address the limitations discussed in
Lin et al. (2002) on poor scalability of the motif discovery algorithm and the inability to discover motifs in the presence of noise by applying a probabilistic model to the algorithm. However, it is still difficult to define a threshold, R , to distinguish trivial and non-trivial matches as it is case dependent and there is no general rule for defining this value.

Furthermore, Keogh et al. (2003) suggest applying a classic clustering algorithm to cluster only the motifs discovered from
K -motif detection algorithm in place of subsequence time series clustering. It is because a subset of the motifs discovered might really be a group that should be clustered together to extract promising subsequences from the data. The motif-based-cluster-ing algorithm is as shown in Fig. 6 .

Chen (2005) shows that time series clustering is not mean-ingless when using delay space method instead of an Euclidean distance for the similarity measure. Fu et al. (2005a) propose an intermediate subsequences filtering process by detecting the change of PIP before the clustering process. Simon et al. (2006) introduce an unfolding (subsampling) preprocess method before the subsequence clustering, using an SOM. Furthermore, another solution is proposed by Chen (2007) , which is based on restricting the clustering space. A disk-aware algorithm is proposed by
Mueen et al. (2009) to find exact motifs in massive time series databases.
 anomalies. Vlachos et al. (2005c) focus on non-parametric methods that extract important periodic features for classification and anomaly detection and Keogh et al. (2006) adopt the SAX representation to improve the performance of finding surprising patterns. Wei et al. (2006) adopt the SAX representation for anomaly detection on the basis of shape. Based on an SAX, suffix tree and Markov model, surprising patterns are discovered in
Lonardi et al. (2006) if the frequency of their occurrence differs substantially from the expected chance of existing patterns. On the other hand, a bitmap approach is also proposed by Wei et al. (2005) . Yankov et al. (2007) focus on discovering surprising patterns under uniform scaling. Yankov et al. (2008) further focus on finding surprising patterns in terabyte sized data sets. Two linear scans through the database are adopted to reduce the memory usage.
 discovering patterns in a fixed resolution (i.e. fixed period), Bettini et al. (1998) and Li et al. (2000b) focus on discovering patterns across different resolutions, which are called multiple granula-rities. Bettini et al. (1998) propose timed automata with granularities (TAGs), while calendar schemata is presented by Li et al. (2000b) .A k -motif-based algorithm is proposed by Tang and
Liao (2008) for discovering patterns with different lengths. 6.2. Classification series domain, special treatment must be considered due to the nature of the data. Geurts (2001) proposes to classify time series data based on combining local properties or patterns in the time series. Zhang et al. (2004) develop a representation method using wavelet decomposition that can automatically select the para-meters for the classification task. They propose a nearest neighbor classification algorithm, using the derived appropriate scale.
Kadous and Sammut (2005) use metafeature approach (i.e. recurring substructure) like local maxima in time series to generate classifiers. Similarly, Yang et al. (2005) focus on feature subset selection (FSS) based on common principal components, which is called CleVer, to retain the correlation information among original features. Classification is employed to evaluate the effectiveness of the selected subset of features.
 ing or developing classifiers for time series data. For example,
Povinelli et al. (2004) present a signal classification approach based on modeling the dynamics of a system as they are captured in a reconstructed phase using Gaussian Mixture models of time domain signatures. Rodriguez and Alonso (2004) study both interval and DTW-based decision trees adequate for the classifi-cation of time series data. Ensembles are used to combine base classifiers, while Wei and Keogh (2006) study the combination of the numerosity reduction, using DTW and nearest-neighbor classifiers for time series classification. Also, Xi et al. (2006) propose a semi-supervised time series classifiers when only a small set of labeled examples is available. 6.3. Rule discovery
Association rule mining ( Agrawal et al., 1993b; Agrawal and Srikant, 1994 ) is one of the most well known algorithms. However, it is mainly focused on symbolic items present in transactions.
Therefore, many researchers pro pose new or modified algorithms for rule mining in the context of time series data.

A trivial approach is first discretizing the time series data into segments and converting each segment to a symbol. Then rules can be discovered in the transformed symbolic domain. Das et al. (1998) cluster the subsequences to find the symbols, and then apply simple rule mining method to discover the hidden rules. Lu et al. (1998) propose n -dimensional inter-transaction association rules for handling spatial and multimedia data mining. Last et al. (2001) focus on discovering fuzzy association rules, which is based on the computational theory of perception and signal processing techniques. Leigh et al. (2002) develop a financial rule discovery method, using  X  X  X ull flag X  X  technical charting heuristics.
Similarly, Ting et al. (2006) propose to representation financial time series based on candle stick charting technique for rule mining.

The research group of Hoppner ( Hoppner and Klawonn, 2001 ) takes durations into account and develops a framework to discover temporal rules, which have been generated out of a set of frequent patterns in a state sequence. The framework represents the segments of time series by attributes (e.g. increasing, high-value, highly convex) and discovers interval relationships described in terms of an interval logic. Another research group of Hetland and Saetrom (2005) present a rule mining method that is based on genetic programming and specialized hardware. They also examine the role of discretization when evolving time series predictor rules.

Besides using association rules, decision tree is another common approach for rule mining. Ohsaki et al. (2003) first discuss on the preprocessing step to discover interesting rules from the medical time series data. Similar to Das et al. (1998) , clustering is adopted to discover typical patterns from subse-quences for the discretization process. The rule mining method is based on pattern extraction and decision tree. Cotofrei and Stoffel (2002) propose formalism based on the temporal first logic-order for rule mining. The approach first transforms sequential raw data into sequences of events, then infers temporal rules using the classification trees.

In addition, Jin et al. (2002) focus on discovering the distribution of temporal rules and Sarker et al. (2003) focus on developing parallel algorithm for time series rule mining. 6.4. Summarization
Some of the researches are focused on summarizing and describing the time series data for analysis, mining or prediction.
Zwir and Enrique (1999) develop an automated identification of significant qualitative features (interesting patterns) in complex objects (time series). Clustering techniques are adopted to summarize and produce a compact description of salient features and their relations.

Boyd (1998) developed a system that integrates knowledge-based signal processing and natural language processing to automatically generate descriptions, and it is tested on the weather data. These descriptions are based on short and long-term trends, which are detected using the wavelet transform.
Guimaraes and Ultsch (1999) propose an approach to transit patterns in multivariate time series to a linguistic description.
Different abstraction levels X  temporal grammatical rules are extracted from the results of neural networks and other exploratory methods. This approach is applied to medical time series, i.e. sleep-related breathing disorders ( Guimaraes et al., 2001 ).

The SumTime project is carried out by Sripada et al. (2001, 2003) , which aims to develop generic techniques for summarizing time series data. The developed system can generate English textual summaries of time series data by selecting the most important trends and patterns, mapping these patterns onto words and phrases and generating actual texts based on these words and phrases. SumTime-Turbine is developed by Yu et al. (2004) , which is focused on the sensor data from gas turbines. Reiter et al. (2005) developed a SumTime-Mousam weather-forecast generator, which uses consistent data-to-word rules.
Ahmad et al. (2004b) present a time series summarization method based on analyzing non-stationary, volatile and high-frequency time series data. Multi scale wavelet analysis is used to separate the trend, cyclical fluctuations and autocorrelation effects. It helps to provide a summary of the data with respect to the  X  X  X hief features X  X  of the data. The framework can generate text signals to describe each effect. Ahmad et al. (2004a) outline a system that comprises modules for summarizing texts and time series to study the link between them. Similarly, previous work by Lavrenko et al. (2000) demonstrate how to use language models to associate stories and trends in time series. They identify trends in time series using PLR and use language models to represent patterns of language that are highly associated with particular trends. All these researches are evaluated using data from financial domain (e.g. stock market, currency rate). A visualization system is developed and described with a financial trading case study by Taskaya and Ahmad (2003) .

Furthermore, Kacprzyk et al. (2008) propose to use the fuzzy quantifier to present a linguistic summarization on the trends of time series which the trends are identified by PLR. Similarly, Batyrshin and Sheremetov (2008) describe a perception-based decision making system which time series is represented by fuzzy sets of perceptions. A linguistic scaling of patterns is used to define the vocabulary. 6.5. Recent research directions
We discussed four major time series data mining tasks so far; they are: pattern discovery (clustering), classification, rule discovery and summarization. Due to the mature development in this field and the significant enhancement on the hardware and communication technologies, three extensions attract more researchers focused on recently. They are mining on multi-attribute time series, mining on time series data stream and also the privacy issue. Some researches discussed above also proposed partial solutions or directions on them.

First, multi-attribute time series data can also be considered as multiple time series manipulation. Povinelli and Feng (1999) propose an approach which temporal clusters from multiple time series are used and a genetic algorithm is adopted. The method reconstructs state space for temporal pattern extraction and adopts an optimal local model for short-term forecasting. The performance of the approach is demonstrated by using financial non-stationary time series (i.e. stock price and volume). Kahveci et al. (2002) consider the problem of shift and scale invariant search for multi-attribute time series. A symmetric distance function and a Cone Slice (CS) index are proposed. Lee et al. (2009) focus on mining of closed patterns in multi-sequence time series by adopting an SAX representation.

Recent researches also focus on mining multivariate time series data. Minnen et al. (2007a, 2007b) propose different algorithms to discover multivariate frequently appearing patterns (i.e. motif discovery). Tatavarty et al. (2007) consider the problem of discovering the temporal dependencies between the frequently appearing patterns in multivariate time series. Wang et al. (2007) and Plant et al. (2009) focus on the clustering issue, while Takashi et al. (2009) focus on the prediction issue.

Second, data streaming is referring to transfer of data at a steady high-speed rate. Handling corresponding time series data received considerable attention recently, because of the increase in network bandwidth and its stability. It differs from traditional time series data on its characteristics of huge amount of data arriving at steady high-speed rate. One of the initial works on time series data streaming mining mainly focuses on the design of system architecture. Miller et al., (1998) propose an I / O system design and implementation targeted at applications, which perform data streaming. Golab and Ozsu (2003) reviewed recent work in data stream management systems with an emphasis on application requirements, data models, continuous query lan-guages and query evaluation. Chen et al. (2002) investigate methods for an online multi-dimensional regression analysis of time series stream data. They show that only a small number of compressed regressions need to be measured instead of a complete stream of data for a multi-dimensional linear regression analysis. Lian and Chen (2008) propose a framework to handle similarity search, values prediction and indexing over data stream.

Based on the well-developed time series data mining algo-rithms in different aspects, they are either applied directly or customized for streaming time series data. Indeed, representation of streaming time series for dimensionality reduction and an online query or matching is a hot topic. It is important that an incoming stream of data is a continually appended time series in a database. Each time when a new data point arrives, the system needs to fetch/get the data from the database, the nearest or the neighboring data of the incoming time series is up to the time position and most researches focus on investigating the correla-tion of the data. Yi et al. (2000) develop a fast method to analyze the co-evolving time series for estimating and forecasting, quantitative data mining and outlier detection. Gilbert et al. (2001) adopt sketch based methods for capturing various linear projections of the data for representing data streams (i.e. wavelet transform) and approximate aggregate query. Gao and Wang (2002) tackle the problem by using an FFT to find the cross correlations of time series in a batch mode efficiently. Gao et al. (2002) focus on continuous nearest neighbor query, using existing indexing methods with pre-fetching. Cole et al. (2005) propose to combine several simple techniques (e.g. sketches, convolution, structured random vectors, etc.) to compute Pearson correlation over uncooperative time series. Vlachos et al. (2005b) examine the problem of monitoring and identifying correlation burst patterns in multi-steam time series data. The solution adopts burst detection and indexing. Ogras and Ferhatosmanoglu (2006) developed a transformation-based framework to reduce the dimension for large-scale and dynamic time series data online.
The framework is focused on DFT-based synopsis generation and a recursive method is introduced to update the highest energy transform coefficients of the series data. Wei et al. (2007) introduce an on-the-fly subsequence matching of streaming time series to a set of predefined patterns, using the filtering approach. The proposed approach merges similar patterns into a wedge, which is an envelope-based lower bounding technique, to speedup the matching process. A stream-DTW (STW) distance measure is proposed by Capitani and Ciaccia (2007) for con-tinuously monitoring DTW distance measure of time series data streams. Boolean representation based on the data-adaptive correlation analysis is proposed by Zhang et al. (2007) . Palpanas et al. (2008) introduce arbitrary user-specified amnesic functions based on PLR to allow an online approximation of streaming time series. This function allows arbitrary, user-defined reduction of quality with time. A tree structure is further proposed by Fu et al. (2008c) for storing the PIPs, which supports various incremental updating approaches ( Fu et al., 2005b ). A multiscale segment mean (MSM) approximation is proposed by Lian et al. (2009) which support incrementally computation and static/dynamic pattern matching.
 correlations, discovering bursts, matching hums and maintaining and manipulating time ordered data stream can be found in
Lerner et al. (2004) . Chen et al. (2007b) propose to handle continuous pattern detection, using spatial assembling distance (SpADe). An SpADe is proposed in this paper to handle both shifting and scaling in temporal and amplitude dimensions. Lim et al. (2008) concentrate on continuous query sequences on time series data stream based on window construction mechanism for supporting variable length queries. Two online segmentation methods, (stepwise) feasible space window (FSW/SFSW), are proposed by Liu et al. (2008) to improve the performance of classic sliding window method. A distortion-free predictive streaming time series matching algorithm is introduced by Loh et al. (2010) . The proposed algorithm performs preprocessing step to remove distortions and predict future search results simultaneously.
 time series streaming data. Yamanishi and Takeuchi (2002) present an online learning framework based on a probabilistic model for outlier detection and change-point detection on the time series data stream. Papadimitriou et al. (2005) introduce a streaming pattern discovery method in multiple time series, which summarizes the key trends in the stream collection based on PCA. An online clustering system is proposed by Rodrigues et al. (2008) which a top-down strategy is adopted to construct a binary tree hierarchy of clusters. Clusters X  diameters are evolved continuously with the stream data.
 with privacy concern ( Agrawal and Srikant, 2000 ). Working on the privacy in time series data mining is a newly research direction.
Zhu et al. (2008) suggest that traditional techniques are not effective in the time series data. Data flow separation attack is identified and possible countermeasures to this attack are further proposed in this paper. To preserve the privacy, cloaked time series data are suggested to be adopted. Lian et al. (2008) propose an approach to deal with cloaked range query (CRQ) based on an R-tree indexing. Furthermore, framework is proposed by Nin and Torra (2009) to evaluate different time series protection methods.
A set of information loss and disclosure risk measures for time series are introduced in this paper. Based on the definition of these kinds of measurements, increasing number of research on time series data protection and privacy issue during the mining process is expected. 7. Conclusion mining. Different research is focused on one or more problems in time series data mining. However, according to the unique behavior of the time series data, existing research is still inadequate and it is considered as one of the 10 challenging problems in data mining ( Yang and Wu, 2006 ). There is still room for us to further investigate and develop. For example, while most of the research communities have concentrated on the mining tasks, the fundamental problem on how to represent a time series has not yet been fully addressed so far. To represent a time series is essential, because time series data is hard to manipulate in its original structure. The high dimensionality of time series data creates difficulties in applying existing data mining techni-ques to it. Therefore, defining a more effective and efficient time series representation scheme is of fundamental importance.
The framework should also support time series pattern matching, including both whole sequence and subsequence matching, between time series of different lengths in an effective manner.
The framework should be compatible to varieties of time series data mining tasks like pattern discovery. In addition, handling multi-attribute time series data, mining on time series data stream and privacy issue are three promising research directions, due to the existence of the system with high computational power.
 References
