
Takehito Utsuro 1 , Mitsuhiro Kida 2 , Masatsugu Tonoike 3 , and Satoshi Sato 4 Lexicons of technical terms are one of the most important language resources both for human use and for computational research areas such as information retrieval and natural language processi ng. Among various research issues regard-ing technical terms, full-/semi-automatic compilation of technical term lexicon is one of the central issues. In various resea rch fields, novel technologies are in-vented every year, and related research areas around such novel technologies keep growing. Along with such invention of technologies, novel technical terms are created year by year. Considering such a situation, it requires a huge cost for manually compiling lexicons of technical terms for hundreds of thousands of technical domains. Therefore, it is inevitable to invent a technique of full-/semi-automatic compilation of technical term lexicons for various technical domains.
The whole task of compiling a technical term lexicon can be roughly decom-posed into two sub-processes: (1) collecting candidates of technical terms of a technical domain, and, (2) judging whether each candidate is actually a techni-cal term of the target technical domain. The technique of the first sub-process is closely related to research on automatic term recognition, and has been relatively well studied so far (e.g., [7]). On the other hand, the technique of the second sub-process has not been studied well so far. Exceptional cases are works such as [1,2], where their techniques are mainly based on the tendency of technical terms appearing in technical documents of limited domains rather than in documents of daily use such as newspaper and magazine articles. Although the underlying idea of those previous works is very interesting, those works are quite limited in that they require existence of certain amount of technical domain corpus. It is not practical for manually collecting technical domain corpus for hundreds of thousands of technical domains. Therefore, as for the second sub-process here, it is very important to invent a technique for automatically classifying the domain of a technical term.

Based on this observation, among several key issues regarding the second sub-process above, this paper mainly focuses on the issue of estimating the domain specificity of a term. In this paper, supposing that a target technical term and a technical domain are given, we propose a technique of automatically estimating the specificity of the target term with respect to the target domain. Here, the domain specificity of the term is judged among the following three levels: i) the term mostly appears in the target domain, ii) the term generally appears in the target domain as well as in other domains, iii) the term generally does not appear in the target domain.

The key idea of the proposed technique is as follows. In the proposed tech-nique, we assume that sample technical terms of the target domain are available. Using such sample terms with search engine queries, we first collect a corpus of the target domain from the Web. In a similar way, we also collect sample pages that include the target term from the Web. Then, the similarities of the con-tents of the documents are measured between the corpus of the target domain and each of the sample pages that include the target term. Finally, the domain specificity of the target term is estimated according to the distribution of the domain of those sample pages.

Firgure 1 illustrates rough idea of this technique. Among the three example (Japanese) terms, the first term ( impedance characteristic )mostlyappearsin the documents of the  X  X lectric engineering X  domain on the Web. In the case of the second term ( electromagnetism ), about half of sample pages collected from the Web can be regarded as in the  X  X lectric engineering X  domain, while the rest are not. On the other hand, in the case of the last term ( response characteristic ), only a few of the sample pages can be regarded as in the  X  X lectric engineering X  domain. In our technique, such difference of the distribution can be easily identified, and the domain specificities of those three terms are estimated.
Experimental evaluation results show that the proposed method achieved mostly 90% precision/recall. 2.1 Outline As we introduced in the previous section, the underlying purpose of this paper is to invent a technique for automatically classifying the domain of a technical term. More specifically, this paper mainly focuses on the issue of estimating the domain specificity of a term t with respect to a domain C , supposing that the term t and the domain C are given.

Generally speaking, the coarsest-grained classification of domain specificity of a term is binary classification, namely, the class of terms that are used in a certain technical domain, vs. the class of terms that are not used in a certain technical domain. In this paper, we further classify the degree g ( t, C )ofthe domain specificity into the following three levels: g ( t, C )= (When we simply classify domain specificity of a term into two classes with the coarsest-grained binary classification above, we regard those with domain specificity  X + X  or  X   X   X  as those that are used in the domain, and those with domain specificity  X   X   X  as those that are not used in the domain.)
The input and output of the process of domain specificity estimation of a term t with respect to the domain C are given below:
The process of domain specificity estimation of a term is illustrated in Fig-ure 2, where the whole process can be decomposed into two sub-processes: (a) that of constructing the corpus D C of the domain C , and (b) that of estimating the specificity of a term t with respect to the domain C . In the process of do-main specificity estimation, the domain of documents including the target term t is estimated, and the domain specificity of t is judged according to the distri-bution of the domains of the documents including t . The details of those two sub-processes are described in the followings. 2.2 Constructing the Corpus of the Domain When constructing the corpus D C of the domain C using the set T C of sample terms of the domain C ,first,foreachterm t in the set T C , we collect into a set D t the top 100 pages obtained from search engine queries that include the term t 1 . The search engine queries here are designed so that documents that describe the technical term t are ranked high. When constructing a corpus of the Japanese language, the search engine  X  X oo X  2 is used. The specific queries that are used in this search engine are phrases with topic-marking postpositional particles such as  X  t -toha, X   X  t -toiu, X   X  t -wa, X  and an adnominal phrase  X  t -no, X  and  X  t . X  Then, union of the sets D t for each t is constructed and denoted as D ( T C ):
Finally, in order to exclude noise texts from the set D ( T C ), the documents in the set D ( T C ) are ranked according to the number of sample terms (of the set T
C ) that are included in each document. Through a preliminary experiment, we decided here that it is enough to keep top 500 documents, and regard them as the corpus D C of the domain C . 2.3 Domain Specificity Estimation of Technical Terms Given the corpus D C of the domain C , domain specificity of a term t with respect to a domain C is estimated through the following three steps: step 1. Collecting documents that include the term t from the Web, and con-step 2. For each document in the set D t , estimating its domain by measuring step 3. Estimating the domain specificity g ( t, C )of t using the document set Details of those three steps are given below: Collecting Web Documents Including the Target Term. For each target term t , documents that include t are collected from the Web. According to a procedure that is similar to that of constructing the corpus of the domain C described in section 2.2, the top 100 pages obtained with search engine queries are collected into a set D t .
 Domain Estimation of Documents. For each document in the set D t ,its domain is estimated by measuring similarity against the corpus D C of the domain C . Then, given a certain lower bound L of document similarity, documents with large enough similarity values are extracted from D t into the set D t ( C, L ).
In the process of document similarity calculation, we simply employ a standard vector space model, 3 where a document vector is constructed, after removing 153 stop words, as a vector of frequencies of content words such as nouns and verbs in a document. Here, the corpus D C of the domain C is regarded as a document d C , and a document vector dv ( d C ) is constructed. For each document d in the set D t , a document vector dv ( d t ) is also constructed. Then, the cosine similarity between dv ( d t )and dv ( d C ) is calculated and is defined as the similarity sim ( d t ,D C ) between the document d t and the corpus D C of the domain C .
Finally, suppose that a certain lower bound L of document similarity is given, documents d t with the similarity value sim ( d t ,D C )aboveorequalto L are regarded as those of the domain C , and are collected into a set D t ( C, L ): In experimental evaluation of section 3, the lower bound L is determined using a development term set for tuning vari ous parameters of the whole process of estimating domain specificity of technical terms.
 Domain Specificity Estimation of a Term. The domain specificity of the term t with respect to the domain C is estimated using the document sets D t and D t ( C, L ). Here, this is done by simply calculating the following ratio r L of the numbers of the documents within the two sets: Then, by introducing the two thresholds a (  X  )and a (+) for the ratio r L ,the specificity g ( t, C )of t is estimated with the following three levels: In experimental evaluation of section 3, as in the case of the lower bound L of the document similarity, the two thresholds a (  X  )and a (+) are also determined using the development term set mentioned above. We evaluate the proposed method with five sample domains, namely,  X  X lectric engineering X ,  X  X ptics X ,  X  X erospace engineering X ,  X  X ucleonics X  ,and  X  X stronomy X  . For each domain C of those five domains, the set T C of sample (Japanese) terms is constructed by randomly selecting 100 terms 4 from an existing (Japanese) lexicon of technical terms for human use. For each of the five domains, we then manually constructed the development term set T dev and the evaluation term set T eva , each of which has 100 terms (those with frequency more than or equal to five, and hits of the search engine within 100  X  10,000.), respectively. For each of the domain specificity  X + X ,  X   X   X , and  X   X   X , Table 1 lists the number of terms of the class. In our experimental evaluation, the development term set T dev is used for tuning the lower bound L of the document similarity, as well as the two thresholds a (  X  )and a (+), where those parameter values are determined so as to maximizing F score (  X  =0 . 75). Here, we chose the value of the weight  X  as 0.75, since we prefer precision to recall in our application such as automatic technical term collection, where automatically collected technical terms are recursively utilized as seed technical terms in the later steps of a bootstrapping process.
Parameter values as well as experimental evaluation results are summarized in the Tables 2 and 3. Generally speaking, the task of discriminating the terms with domain specificity  X + X  against the rest is much harder than that of discrim-inating those with  X + X  and  X   X   X  against those with  X   X   X . In the Table 3, especially, the results with the domains  X  X erospace engineering X  and  X  X ucleonics X  are with lower precision values than other domains. Each of these two domains has closely related another domain (e.g.,  X  military  X  X or  X  X erospace engineering X  ,and  X  X adi-ation medicine X  for  X  X ucleonics X  ), where technical terms of these two domains tend to appear also in the documents of such closely related domains. Most of the errors are caused due to the existence of those close domains. This paper proposes a method of domain specificity estimation of technical terms using the Web. In the proposed method, it is assumed that, for a certain tech-nical domain, a list of known technical terms of the domain is given. Technical documents of the domain are collected through the Web search engine, which are then used for generating a vector space model for the domain. The domain specificity of a target term is estimated according to the distribution of the do-main of the sample pages of the target term. Experimental evaluation results showed that the proposed method achieved mostly 90% precision/recall.
Related techniques on domain specificity estimation of a term include the one based on straightforward application of similarity calculation of language models between the domain corpus and the target term. Although this technique seems mathematically well-defined, it is weak in that it assumes a single language model per a target term. Especially, when a target term appears in the documents of more than one domain, the technique proposed in this paper seems to be advantageous, since it independently estimates the domain of each individual document including the target term.

Figure 3 illustrates a further application of the proposed technique of domain specificity estimation of technical terms. Here, from the corpus D C of the domain C , candidates of technical terms that are not included in any of existing lexicons of technical terms for human use are collected. Then, after excluding terms which do not share constituent nouns against the sample terms of the given set T C , the domain specificity of the remaining t erms are automatically estimated. In the result of experimental evaluation, out of the 1,000 candidates of technical terms per a domain, we discovered about 100  X  200 novel technical terms that are not included in any of existing lexicons of technical terms. This result clearly supports the effectiveness of the proposed technique for the purpose of full-/semi-automatic compilation of technical term lexicons.

