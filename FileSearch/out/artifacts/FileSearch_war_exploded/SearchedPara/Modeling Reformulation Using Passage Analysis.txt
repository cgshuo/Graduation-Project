 Query reformulation modifies the original query with the aim of better matching the vocabulary of the relevant doc-uments, and consequently improving ranking effectiveness. Previous techniques typically generate words and phrases related to the original query, but do not consider how these words and phrases would fit together in new queries. In this paper, we focus on an implementation of an approach that models reformulation as a distribution of queries, where each query is a variation of the original query. This approach considers a query as a basic unit and can capture important dependencies between words and phrases in the query. The implementation discussed here is based on passage analysis of the target corpus. Experiments on the TREC collection show that the proposed model fo r query reformulation sig-nificantly outperforms state-of-the-art methods.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Performance Query Reformulation, Query Substitution, Query Segmen-tation, Passage Analysis, Information Retrieval
Query reformulation is considered as a process of modify-ing or rewriting the original query to better match the vo-cabulary of relevant documents. Many previous models of query reformulation [7, 9, 10] have focused on generating re-lated words and phrases to expand the original query. How-ever, they do not consider how the new words and phrases can be used together to form queries that are variations of the original query, thus the important dependencies between those new terms can be missed. Other research on web query reformulation, on the other hand, has tended to focus on generating a single new query (e.g. [2][6]) by applying a specific reformulation operation. Different operations have been studied such as Query Segmentation [2] and Query Substitution [6]. However, little work considers combining these operations from a unified perspective, thus the impor-tant information about altern ative query reformulations is not captured.

An alternative approach is to t ransform the o riginal query into a distribution of reformulated queries. Since the refor-mulated query that involves a particular choice of words and phrases is explicitly modeled, this approach captures depen-dencies between those query terms. On the other hand, this approach naturally combines different reformulation opera-tions, where all these operations are considered as methods for generating reformulated queries. The detailed analysis of this approach and the comparisons with previous models are provided in Xue and Croft [17].

In this paper, we focus on an implementation of this ap-proach based on passage analysis of the target corpus, avoid-ing the use of proprietary resources. This implementation allows us to use TREC collections that make fair compar-isons with other methods possible.

We make the observation that in the target corpus, pas-sages containing all query words or most of the query words provide a good source of information for reformulating queries. Specifically, passages with all query words provide informa-tion about the common ways that people split the original query into different concepts. For example, given the origi-nal query  X  X il industry history X , the analysis of passages con-taining all these three words in the Gov2 collection 1 shows that the original query is split as  X (oil industry)(history) X  in 51 passages. Similarly, passages only containing some query words can indicate possible ways of substituting miss-ing words. For example, the analysis of passages containing  X  X ndustry history X  on Gov2 shows that  X  X etroleum indus-try history X  appears in 46 passages, which indicates that  X  X etroleum industry history X  is a potential substitution for the original query.
In this section, a passage analysis based implementation is described. This implementation can be divided into three
Gov2 is a TREC collection used for ad-hoc retrieval. Figure 1: The passage analysis based implementa-tion. main steps: first, the original query ( Q ) is substituted through passage analysis and with the help of Wikipedia; second, the candidate queries ( Q c ) consisting of the original query and the substituted queries are segmented based on passage anal-ysis; third, the probabilities of the reformulated queries ( generated from the previous steps are estimated using pas-sage level information. The above framework is illustrated in Fig. 1.
Three different methods are considered to find query sub-stitutions. The first two methods are based on passage anal-ysis and the last uses information from Wikipedia.
Morphologically similar words are a reliable way to sub-stitute the original query words using appropriate morpho-logical variants. In this paper, the morphologically similar words are chosen by considering other query words. Specif-ically, given the original query Q =( q 1 ...q i ...q l ), for each query word q i , we extracted all passages containing all query words except q i . For each extracted passage, if we find a word q i which is morphologically similar to q i , q i will be considered as a substitution of q i and a candidate query is generated as Q c =( q 1 ...q i ...q l ). Note a morphologically sim-ilar word is considered as a substitution only when it appears in passages containing all other query words, which further guarantees the quality of the substitution word.
Two types of patterns are derived from the original query and are then used to match qualified passages to find query substitution.

Adding-word patterns are used to find substitutions which replace a bigram of the original query with an n-gram that adds some words in the middle of the query bigram. Specif-ically, given the original query Q =( q 1 ...q i q i +1 ...q sider substituting any q i q i +1 with q i w 1 ...w s q i the added words and s is the number of added words. Here, we only consider adding one or two words. First, for each bigram q i q i +1 , a pattern is designed as q i q i +1 ,where de-notes any word or any two words. Then, passages containing all query words q 1 ...q l are extracted. For each extracted pas-sage, if the designed pattern q i q i +1 matches this passage, q wq i +1 is collected as a substitution of q i q i +1 . Here, notes the added word/words. Thus, a candidate query is generated as q 1 ...q i wq i +1 ...q l .

Changing-word patterns are used to find substitutions that replace a trigram of the original query with a new trigram where the middle word is different. Specifically, given the tuting q i q i +1 q i +2 with q i wq i +2 ,where w is a different word. First, for each trigram q i q i +1 q i +2 of the original query, a pat-tern is designed as q i q i +2 . Second, passages containing all query words except q i +1 are extracted. For each extracted passage, if the pattern q i q i +2 matches this passage, q is collected as a substitution for q i q i +1 q i +2 and a candidate query substitution is generated as q 1 ...q i wq i +2 ...q
Some query substitutions are difficult to obtain only re-lying on corpus information, thus the third method uses Wikipedia redirect page as an external resource. A redi-rect page in Wikipedia is designed to send the user to the article with an alternative title 2 .

All redirect pages are organized as a set of tuples { ( p p tar ) } , where the phrase p src is redirected to the phrase p tar . Given the original query Q =( q 1 ...q i +1 ...q i + n-grams ( n&gt; 1) are extracted. For each n-gram q i +1 ...q if it matches p src or p tar in any tuple, the corresponding p tar or p src will be collected as a substitution. Then, a candidate query substitution is generated as ( q 1 ...p tar or ( q 1 ...p src ...q n ).
In this step, phrase structures are detected using passage analysis for all candidate queries including both the orig-inal query and query substitutions. The basic idea can be described as follows. Given a candidate query, passages con-taining all query words are extracted. Then, each extracted passage tells us one way to segment the candidate query. Af-ter analyzing all extracted passages, the most frequent ways of segmenting the candidate query can be determined. The probability P ( Q r | Q )for Q r is calculated as follows:
P ( D | Q ) measures the similarity between D and Q ,which is calculated using the language model approach with Dirich-let Smoothing[13, 18]. P ( Q r | D ) measures the probability that Q r is observed in D . P ( Q r | D ) is estimated by dividing the number of passages where Q r appears in by the total number of passages of document D ,whichisshowninEq. 2. # psg ( Q r ,D ) is the number of passages of D where has appeared and # psg ( D ) is the number of passage of D
After this step, we have generated a distribution of re-formulated queries as { ( P ( Q r | Q ) ,Q r ) } .Table1provides examples of the reformulated queries generated by different methods and their associated probabilities for the original query  X  X il industry history X .
The definition of redirect pages can be found at http:// en.wikipedia.org/wiki/Redirects_on_wikipedia Table 1: Examples of the reformulated queries of  X  X il industry history X 
In this section, we incorporate { ( P ( Q r | Q ) ,Q r ) } retrieval function. The basic idea is to combine the orig-inal query Q and the distribution of reformulated queries { (
P ( Q r | Q ) ,Q r ) } together. We assign the probability Q and 1  X   X  to { ( P ( Q r | Q ) ,Q r ) } . Here,  X  is a parameter. The retrieval score of each document is calculated by Eq. 3.
P ( Q | D )and P ( Q r i | D ) are estimated using the language model approach with Dirchlet Smoothing. k is the number of reformulated queries with the highest probabilities. In the case where the total number of reformulated queries is smaller than k , the actual number will be used.
The Gov2 collection is used for experiments. Two indexes are built, one not stemmed and the other stemmed with the Porter Stemmer. For each topic, the title part is used as the query. The reformulated queries are generated from the non-stemmed index and then the retrieval model with reformulated queries is run on both indexes. The size of passage is fixed to 20.  X  is set to 0.8 and k issetto20for the retrieval model using reformulated queries.

For different substitution methods,  X  X rig X  denotes no sub-stitution of the original query.  X  X iki X  denotes using the Wikipedia redirect page.  X  X orph X  denotes the method of finding the morphologically similar words.  X  X at-add X  de-notes the method of using adding-word patterns and  X  X at-chg X  denotes the method of using changing-word patterns.
Several baselines are considered. QL denotes the query likelihood language model [13, 18]. SDM denotes the se-quential dependence model [9]. RM denotes the relevance model [7]. SVM-seg denotes a SVM-based query segmen-Table 3: The effect of combining Orig(o) with other sources including Wiki(w), Morph(m), Pat-add(a), Pat-chg(c). q denotes significantly different with QL, denotes significantly different with SDM and r de-notes significantly different with RM QL 22.00 60.00 58.85 29.99 57.12 54.90 SDM 23.49 60.96 57.79 33.40 61.35 59.81 RM 23.60 64.62 61.06 31.94 57.50 56.92 SVM-seg 21.83 58.46 56.54 30.07 56.35 54.42
QL-psg 22.12 61.35 58.65 31.11 57.69 55.48 o 23.32 q 62.31 59.42 33.52 q 62.31 qr 61.44 qr o,w 23.99 q 64.04 q 60.00 34.00 qr 63.27 qr 61.44 qr o,m 23.91 q 64.04 q 60.10 33.76 qr 60.77 q 61.06 qr o,a 23.85 q 62.69 61.06 d 34.66 qr 63.27 qr 62.69 qr o,w,m,a 24.65 q d 64.81 q d 61.06 d 35.12 qr d 63.65 qr 62.40 o,w,m,a,c 24.67 q d 64.62 q d 61.06 d 35.19 qr d 63.65 qr tation method [1]. QL-psg denotes a passage-augmented language model [8].

Mean average precision (MAP), precision at 5 (P@5) and precision at 10 (P@10) are used as performance measures. The two-tailed t-test is used to measure significance.
Table 2 shows more examples of the reformulated queries generated by our method. Sometimes, the reformulated queries generated from different sources happen to be the same. Also, some sources can not generate reformulated queries for certain queries (denoted as  X  X /a X  in Table 2).
We conducted experiments to explore the effect of com-bining different sources of reformulated queries. Since Orig is the most reliable source of reformulated queries, in or-der to make a fair comparison, the experiment is conducted over queries that Orig can reformulate. Other sources are combined with Orig. Different baselines are compared in-cluding QL, SDM, RM, SVM-seg and QL-psg. The results are reported in Table 3. The best results are bolded.
Table 3 shows that after combining Orig with other sources of reformulated queries, our proposed method beats all base-line methods. In particularly, it performs significantly better than SDM and RM, the state-of-the-art reformulation mod-els, which supports the advantages of the proposed frame-work that models reformulation as query distribution. When Wiki, Morph and Pat-add are combined with Orig respec-tively, some improvement can be observed over Orig and Figure 2: Analysis of relative increases/decreases of MAPoverQLonGov2withpstem. Thebarsfrom left to right indicate SDM, RM and o,w,m,a. when all three sources are used together with Orig, signifi-cant improvement is observed. Pat-chg can bring some slight improvement when it is combined with other sources.
In order to better understand the behaviors of different re-formulation models, we analyzed the number of queries each model increases or decreases over QL. Fig 2 demonstrates the histograms of SDM, RM and o,w,m,a based on the rel-ative increases/decreases of MAP over QL on the Porter-stemmed index. The results on the non-stemmed index are similar.

In Fig 2, the improvement of o,w,m,a over SDM mainly comes from decreasing the number of queries that hurt the performance of QL (SDM has 38 queries in the range (-25%, 0%], while o,w,m,a has 28 queries in it). The improvement of o,w,m,a over RM mainly comes from increasing the number of queries that significantly improve the performance of QL (RM has 0 query in the range &gt; 100%, while o,w,m,a has 11 queries in it).
Besides the work mentioned in Section 1, [3][16] expanded the original query with new words and [11] augmented the query representation with phrases.

Tan and Peng [14] proposed a unsupervised query seg-mentation method using a concept-based language model. Bendersky et al [1] proposed a two-stage query segmenta-tion method.

Wang and Zhai [15] mined the query log to find poten-tial query term substitution and addition patterns. Dang and Croft [4] re-implemented and tested Wang and Zhai X  X  method for TREC collections. The results showed this method does not work well for the well-formed TREC queries.
Peng et al [12] proposed a context-sensitive stemming method for web queries, where query words are stemmed based on the analysis of their context.

Guo et al [5] proposed a CRF-based model for query re-finement, which combined several tasks like spelling correc-tion, stemming and phrase detection. Within their model, different tasks can be solved simultaneously instead of se-quentially.
In order to capture the dependencies of query words and phrases, we discuss a novel refo rmulation framework, where the original query is reformulated as a distribution of queries instead of a bag of components. A passage analysis based implementation is described in this paper and experiments on the TREC collection show that this model significantly improves retrieval performance. Currently, we have focused on short queries. Extending the passage analysis techniques to long queries will be an interesting future issue. This work was supported in part by the Center for Intelli-gent Information Retrieval and in part by NSF grant #IIS-0711348. Any opinions, findings and conclu sions or recom-mendations expressed in this material are those of the au-thors and do not necessarily reflect those of the sponsor.
