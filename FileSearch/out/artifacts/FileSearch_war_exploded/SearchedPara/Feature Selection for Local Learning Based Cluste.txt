 In many pattern recognition and data mining problems, the data is usually represented by a huge number of features. However, the abundance of features often makes the distinction among patterns much harder a nd less accurate. For instance, in genomic data analysis, it is not uncommon to have thousands of gene expression coefficients as features for a single sample, but only a smal l faction is discriminant among different tissue classes. Those irrelevant features for the prediction may largely degrade the per-formance of a learning algorithm. Therefore, developing an effective feature selection algorithm, whose goal is to identify those features relevant to the inference task at hand, is highly necessary for achieving a good performance.

In the literature, most feature selection algorithms have been developed for super-vised learning, rather than the unsupervised learning. It is believed that the unsuper-vised feature selection is more challenging due to the absence of ground truth class labels that can guide the search for relevant f eatures. Until very recently, several al-gorithms have been proposed to address this issue for clustering. In general, they can be categorized as the filter and wrapper methods. The filter approaches [2,3,4] leave out uninformative ones before the clustering. They demonstrate great computational efficiency because they do not involve clustering when evaluating the feature quality. However, the issue of determining how many relevant features should be selected is The wrapper approaches [5,6,7,8,9] first construct a candidate feature subset, and then assess its goodness by investigating the performance of a specific clustering on this sub-set according to some criteria, the two steps are repeated till convergence. In general, the wrapper approaches are computationally demanding, but they are expected to be more accurate than the filter ones, due to the performance feedback of the clustering. Some wrapper approaches, e.g. [6,7], employ the greedy (i.e., non-exhaustive) search through the whole space of all feature subsets, thus cannot guarantee all the relevant features are selected. This drawback, as wel l as the issue of determining how many fea-tures to be selected for filter approaches, can be alleviated by assigning each feature a clustering. By casting the feature selection as a n estimation problem, the combinatorial explosion of the search space can be avoided as well. Nevertheless, all the existing al-gorithms [5,8,9] are constructed by global models built with all the available samples, they may get degenerated on high-dimensional datasets because the similarities among samples may be less discriminable from global view.

In this paper, we propose a novel method of the wrapper manner, which perform the feature selection in a Local Learning base d Clustering (LLC) [1]. The LLC algorithm searches for a solution that ensures the c luster labels in the neighborhood of each point are as pure as possible. Therefore, when the samples seem resembling each other as a whole as in the high dimensi onal space, searching for mo re similar samples locally may help obtaining a more reliable intermediate clustering result to aid the feature se-lection. Furthermore, through extending the ridge regression, a supervised method, to unsupervised learning, LLC has a built-in regularization for the model complexity [1]. Consequently, such a built-in regularization is modified in this paper, in order to take into account the relevance of each feature or k ernel for the clustering. We show the equivalence between the modified penalty term and existing sparse-promoting one sub-ject to some constraints, thus the resulting weights for features are guaranteed to be very sparse. Eventually, the feature weights are estimated iteratively with the local learning based clustering.

The remainder of this paper is organized as follows: Section 2 overviews of local learning based clustering algorithm. We present the proposed feature selection method Section 5, experimental results on several benchmark datasets are reported. Finally, the paper is concluded in Section 6. X = { x ing result can be represented by a cluster assignment indicator matrix P =[ p ic ]  X  { wise. The scaled cluster assignment indicator matrix used in this paper is defined by: Y is the c -th column of Y  X  R n  X  C . y ic = p ic / verify that Y T Y = I ,where I  X  R n  X  n is the identity matrix.

The starting point of the LLC [1] is that the cluster assignments in the neighborhood of each point should be as pure as possible. Sp ecifically, it assumes that the cluster indicator value at each point should be well estimated by a regression model trained locally with its neighbors and their cluster indicator values. Suppose an arbitrary Y c  X  C, 1  X  i, j  X  n ) ,where N including x i itself). The output of the local model is of the following form: f c i ( x )= x bias term is ignored for simplicity, assuming that one of the features is always 1. In [1], i is solved by: where  X  is a trade-off parameter. Denoting the solution to the linear ridge regression problem (1) as  X  c  X  i , the predicted cluster assignment for the test data x i can then be calculated by: and of
N
After all the local predictors (2) have been constructed, LLC aims to find an optimal cluster indicator matrix Y that minimizes the overall prediction error: being the corresponding element in  X  i by (3) if x j  X  X  i and 0 otherwise.
Similar to the spectral clustering [10,11], Y is relaxed into the continuous domain while keeping the property Y T Y = I for (4). LLC then solves: A solution to Y is given by the first C eigenvectors of the matrix T , corresponding to by discretizing Y via the method in [11] or by k-means as in [10]. It can be seen that the key part of LLC is to learn a local regression model and then pre-dict the cluster assignment for each point by (2). Thus one may easily observe that the inclusion of noisy features in the input vector might ruin the inner product in (2). Con-sequently, the prediction error may not be able to truly reflect the purity measurement of cluster assignment in each neighborhood. He nce, we need to exclude those irrelevant features from the prediction.

To this end, we introduce a binary feature selection vector  X  =[  X  1 , X  2 ,..., X  d ] , X  l  X  { 0 , 1 } to the local discriminant function as follows: where diag ( reason for using the square root on  X  will become clear later, but now it does not change input vector x could be turned on and off depending on the value of  X  l .Toavoida restrict its scale by d l =1  X  l =1 , giving them the flavor of probabilities. Consequently the local discriminant function can be solved by: or equivalently, the following problem: which is obtained by applying a change of variables diag ( model is now tantamount to be of the following form: i.e., the second term in the square bracket of (8). Hence a small value for  X  l ,whichis expected to be associated with an irrelevant feature, will result in a large penalization on ( w c i ) l by this weighted norm. In particular, when  X  l =0 , we will prove later that completely eliminated from the prediction. Subsequently, an improved clustering result can be expected. To perform feature selection in LLC, we develop an alternating update algorithm to estimate the clustering captured in Y and the feature weight  X  . 3.1 Update Y for a Given  X  First, the nearest neighbors N i should be re-found according to the  X  -weighted square Euclidean distance, i.e.: With fixed feature weight  X  , analytic solutions for problem (8) can be then easily ob-tained by setting the derivatives to zero. The solution are: satisfying  X  i  X  i =  X  i ,and I i is an n i -by-n i unit matrix.

For high dimensional data, the matrix inver sion in (11) will be very computational inefficient whose time complexity is O ( d 3 ) . Fortunately, by applying the Woodbury X  X  matrix inversion lemma, we could get: n i d , thus the computational cost can be considerably reduced. Besides, from (13), it can be seen that ( w c i ) l (  X  i, c ) goes to 0 as the feature weight  X  l vanishes.
Subsequently the predicted cluster assignment confidence for x i will be obtained as where we define k  X  i = x T i diag (  X  ) X i ,and K  X  i = X T i diag (  X  ) X i .
As in LLC, we construct the key matrix T by (14) and (4). To solve the same opti-mization problem in (5), the columns of Y are simply set to the first C eigenvectors of T corresponding to the smallest C eigenvalues. 3.2 Update  X  for a Given Y With fixed Y and neighborhood determined at each point, a reasonable  X  is the one that can lead to a better local regression model, which is characterized by a lower objective value at the minimum in (8). We will apply this criterion to re-estimate  X  . Firstly, we remove the bias term by plugging (12) in to (8), then it is rewritten as follows: min Subsequently, the estimation of  X  is reformulated as follows: Lagrange of (16) is: where  X  and  X  are Lagrangian multipliers, and  X   X  0 is a scalar,  X   X  0 is a vector. The derivative of L with respect to  X  l ,( l =1 ,...,d ) is computed as: where At the optimality, we therefore have:
By using the Karush-Kuhn-Tucker (KKT) condition, i.e., (23), it is easy to ver-ify the following two cases: Case 1: C c =1 n i =1 ( w c  X  i ) 2 l =0  X   X  l =0 ;Case2: with (22), it follows that the optimal solution of  X  can be calculated in a closed form: It can be seen from (24) that a feature is unimportant if the corresponding element in the regression coefficients has neglectable magnitude for all the clusters at each point. 3.3 The Complete Algorithm The complete local learning based clustering algorithm with feature selection (denoted as LLC-fs) is described in Algorithm 1. The l oop stops when the relative variation of the trace value in (5) between two consecutive iterations is below a threshold (we set it at 10  X  2 in this paper), indicating the partitioning has almost stabilized. Then Y is discretized to obtain the final clustering result with k-means as in [10].
Algorithm 1 : Feature selection for local learni ng based clustering algorithm In this section, it will be shown that the weighted l 2 norm regularization associated with a l 1 norm constraint on these weights, is equivalent to a well-known sparse-promoting regularization penalty. We address this equivalence based upon the fact that the infimum of the weighted l 2 norm, with the weights defined on the standard simplex, is equal to a squared special l 1 norm regularization.
 Theorem 1 Proof. From the Cauchy-Schwarz inequality we have l W l = l  X  (  X  = W l / In fact, l W l can be viewed as a combination of the l 1 -norm regularization on the a simplified case, where the regression model is not built locally, i.e., omitting the sub- X  . Hence, according to Theorem 1, the weighted l 2 norm regularization should be able to produce at least as sparse as that of the squared l W l penalty. Consequently, the sparsity of  X  follows from (24). Five benchmark datasets were used, whos e characteristics are summarized in Table 1. In all the experiments, we evaluated the performance w ith the clustering accuracy (ACC) index [3]. On each dataset, we investigated whether the proposed LLC-fs could improve the LLC algorithm and the baseline k-means clustering, both of which assume all features are equally important. Furthermore, LLC-fs was compared with the state-of-the-art unsupervised feature selection method, Q - X  algorithm 3 [5], which is also a wrapper approach with iterative eigen-decomposition and feature weight estimation, but is global learning based. Because the selection of the optimal number of clusters is beyond the scope of this paper, we simply set the number of clusters equal to the number of classes in each dataset for all the algorithms.

Both the LLC and the LLC-fs algorithms have two parameters: the size of the mu-tual neighbors k , and the trade-off parameter  X  . For each datasets, k and  X  were chosen from pre-specified candidate intervals, respectively. Then both LLC and LLC-fs were performed with each combination of k and  X  , and the whole process was repeated 10 times. For LLC, we only report the performan ce with the best parameter combinations (denoted as LLC-bestk  X  ). Q - X  and k -means have no parameters, the mean and the stan-dard deviation of the ACC index over 10 runs are presented. The parameter sensitivity study for the proposed LLC-fs algorithm will be given at the end of this section. Handwritten Digit Datasets. In this case study, we focused on the task of clustering on the USPS ZIP code 4 handwritten digits, which are 16  X  16 grayscale images. Each image is thus represented by a 256-dimensional vector. In particular, we considered two pairs of digits:  X 4 vs. 9 X  and  X 0 vs. 8 X , which are known difficult to differentiate.
For the USPS datasets, k and  X  were chosen from 30  X  60 and [0 . 1 , 10] , respectively. k has been chosen in such an interval because there are on average more than 800 samples per cluster for both datasets, and there are known heavy overlappings within each pair, a larger neighborhood may help obt ain a more accurate local prediction. For simplicity, only the mean and the standard deviation of the ACC index for LLC-fs, with the combination k =30 , X  =1 over 10 runs, are presented. The results are summarized in Table 2.

It can be seen from Table 2 that LLC-fs significantly improves the performance of other methods on both USPS datasets, whereas the global approach Q - X  performs even worse than the baseline k-means clustering on t he  X 4 vs.9 X  dataset that overlaps heavily. A plausible reason is that, when the points r esemble to each other in a large scale due to the high dimensionality or too many features of similar values, a local search strategy is expected to produce a more accurate intermed iate clustering result than the global based counterpart, for aiding the feature selection.

To get a better understanding of what features have been ranked top by our weighting scheme, Figure 1 shows in the top features in the image domain. Firstly, the sorted  X   X  X  in typical runs on the two datasets are presented in Figure 1(a),1(b) respectively. It can be seen that the both  X  vectors are sparse, and only few of the feature weights are above a very clear threshold. One can find that the 15 top-ranked features have covered almost all the strongly discriminative regions fo r each digit pairs (see Figure 1(c) and 1(d)), thus resulting in more accurate partitions.
 Genomic Datasets. We studied the clustering on public gene expression datasets: colon cancer [13], SRBCT [14], and breast cancer [15].

For these genomic data, the size of the mutual nearest neighbors k should be nei-ther too small (it would be less accurate with deficient local training sets of very high-dimensionality) nor too large (we have limited samples). We chose the k from 20  X  40 for LLC-fs with the combination k =30 , X  =1 over 10 runs.

It can be seen from Table 2 that the superiority over the compared algorithms is remarkable on these high dimensional dat asets with very scarce samples. The typical feature weighting results in the 10 runs are also plotted in Figure 2, all with k =30 , X  = 1 . For each dataset,  X  is quite sparse, only few of them have significant magnitudes while most feature weights are close to zero. The reason why the LLC-fs significantly improves the performance of LLC on these data is that it can jointly identify these most relevant genes for clustering.
 Parameter Sensitivity Study. The effects of parameter k and  X  in LLC-fs algorithm are presented in Figure 3, where each parame ter is varied while th e others are fixed.
In Figure 3(a) and 3(b), on these two USPS datasets which have more than 800 samples per cluster and of high dimensionality, the performance of our method does not vary much when the k and  X  are chosen from 30  X  60 and [0 . 1 , 10] , respectively. Figure 3(c) justifies our setting on of k on these genomic datasets, i.e., the performance is stable and satisfactory when k is neither too large nor too small. From Figure 3(d), it is observed that a large  X  generally would lead to better performance on genomic datasets. The explaination is that the weighted l 2 norm penalty will be very large in (8) because there are many irrelevant features, a large trade-off parameter  X  could balance the fitting error and the penalty. In this paper, a novel feature selection method has been proposed for the local learn-ing based clustering, where an optimal subset of features is jointly learned with the clustering. The proposed approach is developed within a regularization framework. The proposed feature selection method is able to improve the accuracy of the basic local learning clustering. Furthermore, it generally outperforms the state-of-the-art counter-part, especially when the similarities among samples become less discriminable as in the high dimensional space.
 This work was supported by the Faculty Research Grant of HKBU under Projects: FRG/07-08/II-54, and the Research Grant Council of Hong Kong SAR under Grant: HKBU210306.

