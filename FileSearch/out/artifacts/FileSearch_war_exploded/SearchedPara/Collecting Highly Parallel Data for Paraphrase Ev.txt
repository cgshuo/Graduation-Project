 Machine paraphrasing has many applications for natural language processing tasks, including ma-chine translation (MT), MT evaluation, summary evaluation, question answering, and natural lan-guage generation. However, a lack of standard datasets and automatic evaluation metrics has im-peded progress in the field. Without these resources, researchers have resorted to developing their own small, ad hoc datasets (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004; Dolan et al., 2004), and have often relied on human judgments to evaluate their results (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Bannard and Callison-Burch, 2005). Consequently, it is difficult to compare different sys-tems and assess the progress of the field as a whole.
Despite the similarities between paraphrasing and translation, several major differences have prevented researchers from simply following standards that have been established for machine translation. Pro-fessional translators produce large volumes of bilin-gual data according to a more or less consistent spec-ification, indirectly fueling work on machine trans-lation algorithms. In contrast, there are no  X  X rofes-sional paraphrasers X , with the result that there are no readily available large corpora and no consistent standards for what constitutes a high-quality para-phrase. In addition to the lack of standard datasets for training and testing, there are also no standard metrics like BLEU (Papineni et al., 2002) for eval-uating paraphrase systems. Paraphrase evaluation is inherently difficult because the range of potential paraphrases for a given input is both large and unpre-dictable; in addition to being meaning-preserving, an ideal paraphrase must also diverge as sharply as possible in form from the original while still sound-ing natural and fluent.

Our work introduces two novel contributions which combine to address the challenges posed by paraphrase evaluation. First, we describe a frame-work for easily and inexpensively crowdsourcing ar-bitrarily large training and test sets of independent, redundant linguistic descriptions of the same seman-tic content. Second, we define a new evaluation metric, PINC (Paraphrase In N-gram Changes), that relies on simple BLEU-like n-gram comparisons to measure the degree of novelty of automatically gen-erated paraphrases. We believe that this metric, along with the sentence-level paraphrases provided by our data collection approach, will make it possi-ble for researchers working on paraphrasing to com-pare system performance and exploit the kind of automated, rapid training-test cycle that has driven work on Statistical Machine Translation.

In addition to describing a mechanism for collect-ing large-scale sentence-level paraphrases, we are also making available to the research community 85K parallel English sentences as part of the Mi-crosoft Research Video Description Corpus 1 .
The rest of the paper is organized as follows. We first review relevant work in Section 2. Section 3 then describes our data collection framework and the resulting data. Section 4 discusses automatic evalua-tions of paraphrases and introduces the novel metric PINC. Section 5 presents experimental results estab-lishing a correlation between our automatic metric and human judgments. Sections 6 and 7 discuss pos-sible directions for future research and conclude. Since paraphrase data are not readily available, var-ious methods have been used to extract parallel text from other sources. One popular approach exploits multiple translations of the same data (Barzilay and McKeown, 2001; Pang et al., 2003). Examples of this kind of data include the Multiple-Translation Chinese (MTC) Corpus 2 which consists of Chinese news stories translated into English by 11 transla-tion agencies, and literary works with multiple trans-lations into English (e.g. Flaubert X  X  Madame Bo-vary .) Another method for collecting monolingual paraphrase data involves aligning semantically par-allel sentences from different news articles describ-ing the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004).

While utilizing multiple translations of literary work or multiple news stories of the same event can yield significant numbers of parallel sentences, this data tend to be noisy, and reliably identifying good paraphrases among all possible sentence pairs re-mains an open problem. On the other hand, multiple translations on the sentence level such as the MTC Corpus provide good, natural paraphrases, but rela-tively little data of this type exists. Finally, some ap-proaches avoid the need for monolingual paraphrase data altogether by using a second language as the pivot language (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Kok and Brockett, 2010). Phrases that are aligned to the same phrase in the pivot language are treated as potential paraphrases. One limitation of this approach is that only words and phrases are identified, not whole sentences.
While most work on evaluating paraphrase sys-tems has relied on human judges (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Bannard and Callison-Burch, 2005) or indirect, task-based meth-ods (Lin and Pantel, 2001; Callison-Burch et al., 2006), there have also been a few attempts at creat-ing automatic metrics that can be more easily repli-cated and used to compare different systems. Para-Metric (Callison-Burch et al., 2008) compares the paraphrases discovered by an automatic system with ones annotated by humans, measuring precision and recall. This approach requires additional human an-notations to identify the paraphrases within paral-lel texts (Cohn et al., 2008) and does not evalu-ate the systems at the sentence level. The more recently proposed metric PEM (Paraphrase Evalu-ation Metric) (Liu et al., 2010) produces a single score that captures the semantic adequacy, fluency, and lexical dissimilarity of candidate paraphrases, relying on bilingual data to learn semantic equiva-lences without using n-gram similarity between can-didate and reference sentences. In addition, the met-ric was shown to correlate well with human judg-ments. However, a significant drawback of this ap-proach is that PEM requires substantial in-domain bilingual data to train the semantic adequacy evalu-ator, as well as sample human judgments to train the overall metric.

We designed our data collection framework for use on crowdsourcing platforms such as Amazon X  X  Mechanical Turk. Crowdsourcing can allow inex-pensive and rapid data collection for various NLP tasks (Ambati and Vogel, 2010; Bloodgood and Callison-Burch, 2010a; Bloodgood and Callison-Burch, 2010b; Irvine and Klementiev, 2010), includ-ing human evaluations of NLP systems (Callison-Burch, 2009; Denkowski and Lavie, 2010; Zaidan and Callison-Burch, 2009). Of particular relevance are the paraphrasing work by Buzek et al. (2010) and Denkowski et al. (2010). Buzek et al. automati-cally identified problem regions in a translation task and had workers attempt to paraphrase them, while Denkowski et al. asked workers to assess the validity of automatically extracted paraphrases. Our work is distinct from these earlier efforts both in terms of the task  X  attempting to collect linguistic descrip-tions using a visual stimulus  X  and the dramatically larger scale of the data collected. Since our goal was to collect large numbers of para-phrases quickly and inexpensively using a crowd, our framework was designed to make the tasks short, simple, easy, accessible and somewhat fun. For each task, we asked the annotators to watch a very short video clip (usually less than 10 seconds long) and describe in one sentence the main action or event that occurred in the video clip We deployed the task on Amazon X  X  Mechanical Turk, with video segments selected from YouTube. A screenshot of our annotation task is shown in Fig-ure 1. On average, annotators completed each task within 80 seconds, including the time required to watch the video. Experienced annotators were even faster, completing the task in only 20 to 25 seconds.
One interesting aspect of this framework is that each annotator approaches the task from a linguisti-cally independent perspective, unbiased by the lexi-cal or word order choices in a pre-existing descrip-tion. The data thus has some similarities to parallel news descriptions of the same event, while avoiding much of the noise inherent in news. It is also simi-lar in spirit to the  X  X ear Stories X  film used by Chafe (1997). Crucially, our approach allows us to gather arbitrarily many of these independent descriptions for each video, capturing nearly-exhaustive cover-age of how native speakers are likely to summarize a small action. It might be possible to achieve sim-ilar effects using images or panels of images as the stimulus (von Ahn and Dabbish, 2004; Fei-Fei et al., 2007; Rashtchian et al., 2010), but we believed that videos would be more engaging and less ambiguous in their focus. In addition, videos have been shown to be more effective in prompting descriptions of motion and contact verbs, as well as verbs that are generally not imageable (Ma and Cook, 2009). 3.1 Quality Control One of the main problems with collecting data using a crowd is quality control. While the cost is very low compared to traditional annotation methods, work-ers recruited over the Internet are often unqualified for the tasks or are incentivized to cheat in order to maximize their rewards.

To encourage native and fluent contributions, we asked annotators to write the descriptions in the lan-guage of their choice. The result was a significant amount of translation data, unique in its multilingual parallelism. While included in our data release, we leave aside a full discussion of this multilingual data for future work.
To ensure the quality of the annotations being pro-duced, we used a two-tiered payment system. The idea was to reward workers who had shown the abil-ity to write quality descriptions and the willingness to work on our tasks consistently. While everyone had access to the Tier-1 tasks, only workers who had been manually qualified could work on the Tier-2 tasks. The tasks were identical in the two tiers but each Tier-1 task only paid 1 cent while each Tier-2 task paid 5 cents, giving the workers a strong incen-tive to earn the qualification.

The qualification process was done manually by the authors. We periodically evaluated the workers who had submitted the most Tier-1 tasks (usually on the order of few hundred submissions) and granted them access to the Tier-2 tasks if they had performed well. We assessed their work mainly on the gram-maticality and spelling accuracy of the submitted de-scriptions. Since we had hundreds of submissions to base our decisions on, it was fairly easy to identify the cheaters and people with poor English skills 3 . Workers who were rejected during this process were still allowed to work on the Tier-1 tasks.

While this approach requires significantly more manual effort initially than other approaches such as using a qualification test or automatic post-annotation filtering, it creates a much higher quality workforce. Moreover, the initial effort is amortized over time as these quality workers are retained over the entire duration of the data collection. Many of them annotated all the available videos we had. 3.2 Video Collection To find suitable videos to annotate, we deployed a separate task. Workers were asked to submit short (generally 4-10 seconds) video segments depicting single, unambiguous events by specifying links to YouTube videos, along with the start and end times. We again used a tiered payment system to reward and retain workers who performed well.

Since the scope of this data collection effort ex-tended beyond gathering English data alone, we tried to collect videos that could be understood regardless of the annotator X  X  linguistic or cultural background. In order to avoid biasing lexical choices in the descriptions, we muted the audio and excluded videos that contained either subtitles or overlaid text. Finally, we manually filtered the sub-mitted videos to ensure that each met our criteria and was free of inappropriate content. 3.3 Data We deployed our data collection framework on Me-chanical Turk over a two-month period from July to September in 2010, collecting 2,089 video segments and 85,550 English descriptions. The rate of data collection accelerated as we built up our workforce, topping 10K descriptions a day when we ended our data collection. Of the descriptions, 33,855 were from Tier-2 tasks, meaning they were provided by workers who had been manually identified as good performers. Examples of some of the descriptions collected are shown in Figure 2.

Overall, 688 workers submitted at least one En-glish description. Of these workers, 113 submitted at least 100 descriptions and 51 submitted at least 500. The largest number of descriptions submitted by a single worker was 3496 4 . Out of the 688 work-ers, 50 were granted access to the Tier-2 tasks. The success of our data collection effort was in part due to our ability to retain these good workers, building a reliable and efficient workforce. Table 1 shows some statistics for the Tier-1 and Tier-2 tasks 5 . Overall, we spent under $5,000 including Amazon X  X  service fees, some pilot experiments and surveys.

On average, 41 descriptions were produced for each video, with at least 27 for over 95% of the videos. Even limiting the set to descriptions pro-duced from the Tier-2 tasks, there are still 16 de-scriptions on average for each video, with at least 12 descriptions for over 95% of the videos. For most clusters, then, we have a dozen or more high-quality parallel descriptions that can be paired with one an-other to create monolingual parallel training data. One of the limitations to the development of ma-chine paraphrasing is the lack of standard metrics like BLEU, which has played a crucial role in driv-ing progress in MT. Part of the issue is that a good paraphrase has the additional constraint that it should be lexically dissimilar to the source sen-tence while preserving the meaning. These can be-come competing goals when using n-gram overlaps to establish semantic equivalence. Thus, researchers have been unable to rely on BLEU or some deriva-tive: the optimal paraphrasing engine under these terms would be one that simply returns the input.
To combat such problems, Liu et al. (2010) have proposed PEM, which uses a second language as pivot to establish semantic equivalence. Thus, no n-gram overlaps are required to determine the se-mantic adequacy of the paraphrase candidates. PEM also separately measures lexical dissimilarity and fluency. Finally, all three scores are combined us-ing a support vector machine (SVM) trained on hu-man ratings of paraphrase pairs. While PEM was shown to correlate well with human judgments, it has some limitations. It only models paraphrasing at the phrase level and not at the sentence level. Fur-ther, while it does not need reference sentences for the evaluation dataset, PEM does require suitable bilingual data to train the metric. The result is that training a successful PEM becomes almost as chal-lenging as the original paraphrasing problem, since paraphrases need to be learned from bilingual data.
The highly parallel nature of our data suggests a simpler solution to this problem. To measure semantic equivalence, we simply use BLEU with multiple references. The large number of reference paraphrases capture a wide space of sentences with equivalent meanings. While the set of reference sen-tences can of course never be exhaustive, our data collection method provides a natural distribution of common phrases that might be used to describe an action or event. A tight cluster with many simi-lar parallel descriptions suggests there are only few common ways to express that concept.

In addition to measuring semantic adequacy and fluency using BLEU, we also need to measure lexi-cal dissimilarity with the source sentence. We intro-duce a new scoring metric PINC that measures how many n-grams differ between the two sentences. In essence, it is the inverse of BLEU since we want to minimize the number of n-gram overlaps between the two sentences. Specifically, for source sentence s and candidate sentence c : where N is the maximum n-gram considered and n-gram s and n-gram c are the lists of n-grams in the source and candidate sentences, respectively. We use N = 4 in our evaluations.

The PINC score computes the percentage of n-grams that appear in the candidate sentence but not in the source sentence. This score is similar to the Jaccard distance, except that it excludes n-grams that only appear in the source sentence and not in the candidate sentence. In other words, it rewards candi-dates for introducing new n-grams but not for omit-ting n-grams from the original sentence. The results for each n are averaged arithmetically. PINC eval-uates single sentences instead of entire documents because we can reliably measure lexical dissimilar-ity at the sentence level. Also notice that we do not put additional constraints on sentence length: while extremely short and extremely long sentences are likely to score high on PINC, they still must main-tain semantic adequacy as measured by BLEU.

We use BLEU and PINC together as a 2-dimensional scoring metric. A good paraphrase, ac-cording to our evaluation metric, has few n-gram overlaps with the source sentence but many n-gram overlaps with the reference sentences. This is con-sistent with our requirement that a good paraphrase should be lexically dissimilar from the source sen-tence while preserving its semantics.

Unlike Liu et al. (2010), we treat these two cri-teria separately, since different applications might have different preferences for each. For example, a paraphrase suggestion tool for a word processing software might be more concerned with semantic adequacy, since presenting a paraphrase that does not preserve the meaning would likely result in a negative user experience. On the other hand, a query expansion algorithm might be less concerned with preserving the precise meaning so long as additional relevant terms are added to improve search recall. To verify the usefulness of our paraphrase corpus and the BLEU/PINC metric, we built and evaluated several paraphrase systems and compared the auto-matic scores to human ratings of the generated para-phrases. We also investigated the pros and cons of collecting paraphrases using video annotation rather than directly eliciting them. 5.1 Building paraphrase models We built 4 paraphrase systems by training English to English translation models using Moses (Koehn et al., 2007) with the default settings. Using our para-phrase corpus to train and to test, we divided the sen-tence clusters associated with each video into 90% for training and 10% for testing. We restricted our attention to sentences produced from the Tier-2 tasks in order to avoid excessive noise in the datasets, re-sulting in 28,785 training sentences and 3,367 test sentences. To construct the training examples, we randomly paired each sentence with 1, 5, 10, or all parallel descriptions of the same video segment. This corresponds to 28K, 143K, 287K, and 449K training pairs respectively. For the test set, we used each sentence once as the source sentence with all parallel descriptions as references (there were 16 references on average, with a minimum of 10 and a maximum of 31.) We also included the source sen-tence as a reference for itself.

Overall, all the trained models produce reasonable paraphrase systems, even the model trained on just 28K single parallel sentences. Examples of the out-puts produced by the models trained on single paral-lel sentences and on all parallel sentences are shown in Table 2. Some of the changes are simple word substitutions, e.g. rabbit for bunny or gun for re-volver , while others are phrasal, e.g. frying meat for browning pork or made a basket for scores in a bas-ketball game . One interesting result of using videos as the stimulus to collect training data is that some-times the learned paraphrases are not based on lin-guistic closeness, but rather on visual similarity, e.g. substituting cricket for baseball .
 To evaluate the results quantitatively, we used the BLEU/PINC metric. The performance of all the trained models is shown in Figure 3. Unsurprisingly, there is a tradeoff between preserving the meaning and producing more varied paraphrases. Systems trained on fewer parallel sentences are more con-servative and make fewer mistakes. On the other hand, systems trained on more parallel sentences of-ten produce very good paraphrases but are also more likely to diverge from the original meaning. As a comparison, evaluating each human description as a paraphrase for the other descriptions in the same cluster resulted in a BLEU score of 52.9 and a PINC score of 77.2. Thus, all the systems performed very well in terms of retaining semantic content, although not as well in producing novel sentences.

To validate the results suggested by the automatic metrics, we asked two fluent English speakers to rate the generated paraphrases on the following cate-gories: semantic, dissimilarity, and overall. Seman-tic measures how well the paraphrase preserves the original meaning while dissimilarity measures how much the paraphrase differs from the source sen-tence. Each category is rated from 1 to 4, with 4 being the best. A paraphrase identical to the source sentence would receive a score of 4 for meaning and 1 for dissimilarity and overall. We randomly se-lected 200 source sentences and generated 2 para-phrases for each, representing the two extremes: one paraphrase produced by the model trained with sin-gle parallel sentences, and the other by the model trained with all parallel sentences. The average scores of the two human judges are shown in Ta-ble 3. The results confirm our finding that the sys-tem trained with single parallel sentences preserves the meaning better but is also more conservative. 5.2 Correlation with human judgments Having established rough correspondences between BLEU/PINC scores and human judgments of se-mantic equivalence and lexical dissimilarity, we quantified the correlation between these automatic metrics and human ratings using Pearson X  X  corre-lation coefficient, a measure of linear dependence between two random variables. We computed the inter-annotator agreement as well as the correlation between BLEU, PINC, PEM (Liu et al., 2010) and the average human ratings on the sentence level. Re-sults are shown in Table 4.

In order to measure correlation, we need to score each paraphrase individually. Thus, we recomputed BLEU on the sentence level and left the PINC scores unchanged. While BLEU is typically not reliable at the single sentence level, our large number of ref-erence sentences makes BLEU more stable even at this granularity. Empirically, BLEU correlates fairly well with human judgments of semantic equiva-lence, although still not as well as the inter-annotator agreement. On the other hand, PINC correlates as well as humans agree with each other in assessing lexical dissimilarity. We also computed each met-ric X  X  correlation with the overall ratings, although neither should be used alone to assess the overall quality of paraphrases.

PEM had the worst correlation with human judg-ments of all the metrics. Since PEM was trained on newswire data, its poor adaptation to this domain is expected. However, given the large amount of train-ing data needed (PEM was trained on 250K Chinese-English sentence pairs and 2400 human ratings of paraphrase pairs), it is difficult to use PEM as a gen-eral metric. Adapting PEM to a new domain would require sufficient in-domain bilingual data to sup-port paraphrase extraction. In contrast, our approach only requires monolingual data, and evaluation can be performed using arbitrarily small, highly-parallel datasets. Moreover, PEM requires sample human ratings in training, thereby lessening the advantage of having automatic metrics.

Since lexical dissimilarity is only desirable when the semantics of the original sentence is unchanged, we also computed correlation between PINC and the human ratings when BLEU is above certain thresh-olds. As we restrict our attention to the set of para-phrases with higher BLEU scores, we see an in-crease in correlation between PINC and the human assessments. This confirms our intuition that PINC is a more useful measure when semantic content has been preserved.

Finally, while we do not believe any single score could adequately describe the quality of a para-phrase outside of a specific application, we experi-mented with different ways of combining BLEU and PINC into a single score. Almost any simple combi-nation, such as taking the average of the two, yielded decent correlation with the human ratings. The best correlation was achieved by taking the product of PINC and a sigmoid function of BLEU. This follows the intuition that semantic preservation is closer to a binary decision (i.e. a paraphrase either preserves the meaning or it does not, in which case PINC does not matter at all) than a linear function. We used an oracle to pick the best logistic function in our experiment. In practice, some sample human rat-ings would be required to tune this function. Other more complicated methods for combining BLEU and PINC are also possible with sample human rat-ings, such as using a SVM as was done in PEM.
We quantified the utility of our highly parallel data by computing the correlation between BLEU and human ratings when different numbers of refer-ences were available. The results are shown in Fig-ure 4. As the number of references increases, the correlation with human ratings also increases. The graph also shows the effect of adding the source sen-tence as a reference. If our goal is to assess seman-tic equivalence only, then it is better to include the source sentence. If we are trying to assess the overall quality of the paraphrase, it is better to exclude the source sentence, since otherwise the metric will tend to favor paraphrases that introduce fewer changes. 5.3 Direct paraphrasing versus video In addition to collecting paraphrases through video annotations, we also experimented with the more traditional task of presenting a sentence to an anno-tator and explicitly asking for a paraphrase. We ran-domly selected a thousand sentences from our data and collected two paraphrases of each using Me-chanical Turk. We conducted a post-annotation sur-vey of workers who had completed both the video description and the direct paraphrasing tasks, and found that paraphrasing was considered more diffi-cult and less enjoyable than describing videos. Of those surveyed, 92% found video annotations more enjoyable, and 75% found them easier. Based on the comments, the only drawback of the video an-notation task is the time required to load and watch the videos. Overall, half of the workers preferred the video annotation task while only 16% of the workers preferred the paraphrasing task.

The data produced by the direct paraphrasing task also diverged less, since the annotators were in-evitably biased by lexical choices and word order in the original sentences. On average, a direct para-phrase had a PINC score of 70.08, while a parallel description of the same video had a score of 78.75. While our data collection framework yields useful parallel data, it also has some limitations. Finding appropriate videos is time-consuming and remains a bottleneck in the process. Also, more abstract ac-tions such as reducing the deficit or fighting for jus-tice cannot be easily captured by our method. One possible solution is to use longer video snippets or other visual stimuli such as graphs, schemas, or il-lustrated storybooks to convey more complicated in-formation. However, the increased complexity is also likely to reduce the semantic closeness of the parallel descriptions.

Another limitation is that sentences produced by our framework tend to be short and follow simi-lar syntactic structures. Asking annotators to write multiple descriptions or longer descriptions would result in more varied data but at the cost of more noise in the alignments. Other than descriptions, we could also ask the annotators for more complicated responses such as  X  X ill in the blanks X  in a dialogue (e.g.  X  X f you were this person in the video, what would you say at this point? X ), their opinion of the event shown, or the moral of the story. However, as with the difficulty of aligning news stories, finding paraphrases within these more complex responses could require additional annotation efforts.
In our experiments, we only used a subset of our corpus to avoid dealing with excessive noise. How-ever, a significant portion of the remaining data is useful. Thus, an automatic method for filtering those sentences could allow us to utilize even more of the data. For example, sentences from the Tier-2 tasks could be used as positive examples to train a string classifier to determine whether a noisy sentence be-longs in the same cluster or not.

We have so far used BLEU to measure seman-tic adequacy since it is the most common MT met-ric. However, other more advanced MT metrics that have shown higher correlation with human judg-ments could also be used.

In addition to paraphrasing, our data collection framework could also be used to produces useful data for machine translation and computer vision. By pairing up descriptions of the same video in dif-ferent languages, we obtain parallel data without re-quiring any bilingual skills. Another application for our data is to apply it to computer vision tasks such as video retrieval. The dataset can be readily used to train and evaluate systems that can automatically generate full descriptions of unseen videos. As far as we know, there are currently no datasets that contain whole-sentence descriptions of open-domain video segments. We introduced a data collection framework that pro-duces highly parallel data by asking different an-notators to describe the same video segments. De-ploying the framework on Mechanical Turk over a two-month period yielded 85K English descriptions for 2K videos, one of the largest paraphrase data re-sources publicly available. In addition, the highly parallel nature of the data allows us to use standard MT metrics such as BLEU to evaluate semantic ad-equacy reliably. Finally, we also introduced a new metric, PINC, to measure the lexical dissimilarity between the source sentence and the paraphrase. We are grateful to everyone in the NLP group at Microsoft Research and Natural Language Learning group at UT Austin for helpful discussions and feed-back. We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous re-viewers for helpful comments on a previous draft.
