 Mobile videoconferencing is in creasingly being used to bring remote friends or family along to an activity happening outside the home, such as shopping or visiting a tourist attraction. We explored how including contextual information of the event, in addition to audio and video of the person at the event, impacts the shared experience. We studied three kinds of information: a map video showing what was in front of that person, and periodic high quality images showing what wa s in front of the person. We information significantly improve d connectedness and the sense periodic images for  X  X id you see that X  moments and to see greater detail. Together they led to smooth view negotiation, activity input from the couch potato, and high levels of engagement. H.4.3 [Information Systems A pplications]: Communications Applications  X  computer confer encing, teleconferencing, and videoconferencing.
 Human Factors. Telepresence; consumer; mobile; wearable; video; periodic snapshots; map; fiel d study; connectedness; sense of presence. People are increasingly using mobile devices to capture and share For example, they share shopping tr ips with friends to get advice on what to buy, kids X  soccer games with grandma so that she can with loved ones back at home. They share events asynchronously through social media and text me ssages, and synchronously using mobile video chat apps. In this paper, we focus on synchronous shared mobile experiences. Synchronous shared mobile experi ences go beyond the traditional  X  X alking head X  conversations over video chat by enabling people to share rich experiences as they do something together. Two important shared experience factors are connectedness and the sense of presence. Connectedness can be defined as the degree to experiencing and feeling. Meanwhile, the sense of presence can physically side by side during the experience. We were interested in explorin g how to increase connectedness and the sense of presence in shared mobile experiences. Our remotely with additional live views showing the context of the activity in combination with the li ve audio and video of the person at the activity. As people alread y frequently share videos and photos, and sometimes geo-tag th em, we wanted to study the benefits of providing contextual vi ews that convey these types of information. We focused on three views: a map showing the position of the person at the activit y, a second live video showing what was in front of that person, and periodic high quality images showing what was in front of the person. To better understand the impact of adding contextual information to mobile shared experiences, we built and field-tested a prototype system that shared contextual views of an activity, as well as, audio and video of the person at the activity. The field study had information significantly improve d connectedness and the sense orientation and to provide directi ons, the second live video for  X  X o the periodic images for  X  X id you see that X  moments and to see smooth view negotiation, activit y input from the couch potato, and high engagement levels. prior work. Then we describe our prototype and the field study. We end with a discussion, conclusions, and directions for future work. The use of mobile videoconferenci ng is on the rise, and as O X  X ara outside home and work settings. People are increasingly going beyond video calls that primarily focus on conversations (talking share rich experiences. Recently, shared experiences have attracted significant media attention, such as the iPad Bridesmaid Skype [18]. Current mobile devices are well suited to sharing experiences because they can capture both video of the participant at the event (front-facing camera) and contextual video of the activity (rear-facing camera). They also support switching between the cameras during a call, which is important for collaboration scenarios. Prior [15] reporting that people like seeing each other during remote collaborations, and Gaver et al. [7] demonstrating that in some experiences suggests that would be useful for a remote attendee to have a view of the person at the event, additional contextual views of the event, and the ability to switch among these views. remote attendee X  X  engagement and feelings of being together and head-mounted video in addition to audio between two remote geocaching partners. They found that the video was useful for coarse-grained navigation but not for fine-grained search. The issues with using video for fine-grained tasks were low resolution (640x480) and difficulties with framing. The GestureCam by Kuzuoka et al. [12] and other prio r work have addressed the video framing issue by allowing the remote person to direct the camera. The video quality issue remains, however, and is largely dependent on infrastructure. such as websites relevant to the shared activity, as well as, mash-ups of live and synthetic data, such as a live video annotated with digital content. The Chili system by Jo and Hwang [10] supports a mash-up view in which both users can annotate a shared live video. In addition, Stafford et al. [19] created a system in which a non-mobile user places pins on locat ions in a digital map, and the mobile user sees that data supe rimposed on a mobile device when the device X  X  camera sees those locations. Prior work has also enriched the real world with participants X  videos. For instance, Billinghurst and Kato [1] show videos of remote participants overlaid on the real world when looking at the world through a head mounted display. These previous works studied contextual information for shared ubiquitous computing, devices co mpute decisions by sensing and responding to the environment around them, which forms the organizational processes and meetings, context has been used to documents and devices, within the institution [4]. We focus on the notion of context for shared experiences, where context of a shared activity conveys additional awareness of the activity to remote attendees. As a final note on prior work, while we focused on connectedness and the sense of presence, there are other factors that also impact remote shared experiences. At a low level, these factors include aspects such as the richness of the communication channels [2][6], mutual and directional gaze [14][17], and referential awareness [3]. These low-level factors drive high-level experience metrics connectedness. To s ummarize prior work, some sy stems have provided contextual few studies of this information, especially in the wild. Even fewer of these evaluations were carried out for activities that users themselves chose as something they would like to share remotely. Finally, prior work has not studied how users manage multiple types of contextual information when they are available simultaneously. Our work addres ses these outstanding issues. To better understand how contextual activity information impacts connectedness and the sense of presence during a shared shared events because of costs and scale. Thus, a lab could everyday life. As a result, we decided to run a field study in which remote attendees should have joined from their own homes, collection tasks more manageable. particular, we could not rely on super-high resolution cameras, virtually unlimited bandwidth, and high-quality audio that exists both the person at the event and the contextual information of the event. We also had to rely on cellular networks for connecting the undeniably impacted the quality of the shared audio, video, and data. However, and more importantly, this quality was realistic. The preparation complexity was further increased by the fact that activity. Thus, we built our own prototype to support this kind of sharing. To reduce the prototype build time, we leveraged a combination of existing commercial products and added custom components where required. The result was a prototype with a event in the wild. The mobile prototype had to perform three tasks: present the audio and video of the remote attendee; capture the audio and information. Standard video chat applications on a smartphone with a front-facing camera already accomplish the first two tasks. As a result, one part of the mobile prototype was a handheld smartphone (Figure 1 left), in our instan ce a Lumia 920 Windows Phone 8, running Skype mobile. As with most video chat applications and smartphones today, users could switc h to the rear-facing camera if they wanted to use the phone to show a video of something in front of them. To reduce the impact of noise in outdoor settings, a headset was connected to the phone. The headset also made it possible to have audio when users placed the phone in a pocket or purse to free up their hands. capture contextual information in the form of live video, snapshot history, and user location together with the audio and video of the person using the phone. For this, we created a new system. Since a smartphone could capture photos and videos using one of its cameras and user location using th e built in GPS sensor, we used a second, shoulder-mounted, wearable smartphone to capture the contextual information. We again used a Lumia 920. We mounted pressed flush against their bodies. The wearable smartphone captured three types of contextual information: Map  X  GPS location at one second intervals; Video  X  a video showing what was in front of the user wearing the phone; and Images  X  automated high-quality images taken every five seconds and capturing what was in front of the user. Because 920, we used the rear camera to capture Video and screenshots of the camera preview window as Imag es. Thus, the resolution of the images matched the 800x480 resolution of the Lumia 920 screen. To share the contextual information with the remote attendee, we used two separate channels. To sh are Video information, we setup a second Skype session with one-way muted video stream from the wearable phone. To share the Images and Map information, the wearable smartphone uploaded data to an Azure cloud service that could serve that data on demand. The living room prototype space n eeded to look like a living room were in a lab. Thus, we created a space that had a couch, a 55 X  TV placed a comfortable distance from the couch, a coffee table, and some simple living room d X cor, su ch as rug, floor lamps, fake plants, and wall hangings (Fi gure 1 center and left). The prototype had to execute three tasks: capture the audio and video of the person in the room; pr esent the audio and video of the person at the activity; present contextual information of the activity. an HD webcam on top of the TV and digitally zoomed it in on the person. To capture the audio, we placed a ClearOne speakerphone on the coffee table. The speakerphone also played back the audio of the person at the activity. Displaying content is more complex in the living room than in the of the remote attendee, while the room prototype had to show both contextual information and the vide o of the person at the activity. An important question was whether to display contextual display or multiple displays. W ith a single display, both could have been shown at the same time using a tiled or picture-in-picture view. However, this could have made some things difficult on the display. However, switc hing between the views would have been burdensome to users. We instead chose to utilize two contextual information could be shown simultaneously on different displays. We used a TV and one additional display in the living room. increase connectedness and the sense of presence because it this intuition, we explored two dual-display configurations with second display (Figure 1 center). In the Projector-TV condition , we placed an In-Focus IN126ST short-throw projector under the coffee table and it projected a 14 X  diagonal image to the sides and above the TV (Figure 1 right). In both conditions, the video of the contextual views were show n on the larger display. In order to display the participant video and contextual information on the two displays, the living room used two desktop computers. One computer joined the Skype session with the handheld phone in the mobile prototype and showed the participant video from that phone on the smaller display that was being used. The second computer joined the Skype session with the wearable phone and displayed the context video from that phone on the larger display that was being used. This desktop did not transmit any audio or video back to the wearable phone. In information from the Azure cloud service. For the Map data, it 
Figure 2. The context view stretches across the TV and the converted the GPS location to a pin on a Bing map and a line showing the path of the person at the activity. The living room prototype allowed users to choose which context could change the zoom level and clear the movement history. Map tilting, panning, and searching were disabled for simplicity. In the Images view, users could navi gate through image history and jump to the latest image. The system did not provide any ways to interact with the Video view other than to watch it. Finally, the living room prototyp e was mindful of the times when the people at the activity put thei r handheld phone away (e.g., into a pocket or purse) to free up their hands. When the handheld phone was put away, the video capt ured by the phone was simply black and not useful. In the Projector-TV condition, this meant that the TV occluded a part of the projected image for no reason. To improve this experience, when the system detected that the handheld phone was put away, it made things appear as if the TV and the projector were one large contiguous screen (Figure 2). The prototype accomplished this by determining the part of the projected image that was occluded by the TV and sending that manually activate or deac tivate this behavior. With the prototype system in place, we were ready to evaluate the impact of contextual information for activities people want to them with, and their preferred time and location. While affording also added some complexity. ecological validity, it also introduced additional complexity. First, there were cross-session differences since participants were allowed to choose their activity . This enabled us to observe different levels of ambient noise, lighting, and network coverage. Second, our study design introduced a within-session difference asymmetric experiences. Thus, we had to analyze their feedback separately. Third, our use of tw o different display setups and multiple forms of contextual information in the living room added another layer of complexity. Therefore, we had to tease out the this section, we desc ribe our methodology. Otherwise, their feedback could have been influenced by the novelty of using video chat rather than focusing on feedback about contextual information in mobile shared experiences. Therefore, we recruited twelve pairs of people who had used participant pairs were family me mbers, couples, and good friends. During recruitment, the recruits were told that one of them would attend an activity in person while the other would join them remotely. We provided some s uggestions for activities (e.g., visiting a museum, attending a child X  X  soccer game, etc.) but left it up to them to choose the event and who would attend it remotely. At the start of each session, one participant came to our lab while the other participant went to the event. We refer to the participants in the lab and at the event as inside and outside participants , respectively. One study administrator met with the outside participant while two administrators met the inside participant. We started each session by explaining the purpose of the study. Then we demonstrated basic Skype features to the outside cameras (on the handheld phone). We also explained that they can put the phone away in their pocket or purse any time they desired, and that if they did so, they could still talk to their partner through the headset. We then mounted the wearable smartphone on the participant and started the shared experience. Each shared experience was divide d into three sections: warm-up, familiarization, and free play. Warm-up : Each shared experience started with a five-minute warm-up period of regular Skype video chat. Even though the outside user was wearing the second smartphone, we told them that it was not active. Meanwhile, the inside user could see video which living room condition they were using. We used the warm comfortable with since it was just basic Skype video chat. Familiarization : Following the warm-up period, the participants spent three five-minute periods us ing each type of contextual information our system could provi de. We explained each view to not see the living room, the administrator showed them what the current condition looked like using photographs. We used these contextual information. They co uld not choose what information they were seeing or otherwise interact with it. Of the twelve pairs in our study, six used the TV-Tablet condition in the living room and six used the Projector-TV c ondition. Within each condition, the order in which the contextual information views were used during the familiarization stage was counterbalanced. entered a twenty-minute free-play period during which the inside user could control what contextual information was shown. We view; 3) navigate through image history in Image view; and 4) free-play period was the period we were most interested in. We felt that it could help us better understand user preference for the contextual information, inform us of whether they would switch than the others. Most of our data collection, anal ysis, and reported results are with respect to the twenty minute free-play period. To assist in managing the study complexity, we collected multiple forms of data. Questionnaires and interviews : We collected subjective data via two questionnaires and a debrief interview at the end of each session. The first questionnaire was completed at the start of each session and included demographic questions. At the end of the enjoyment, feeling of connectedne ss, feeling of being together, view. It also asked them to rank the views from best to worst and for each view. Once they completed the questionnaire, we conducted a semi-structured intervie w with the inside and outside participants separately. changed their contextual views, their interactions with the Map off. From this data, we can obtai n information such as the number and frequency of view changes and total time spent in each contextual view. Observations, pictures, and screen and video recordings : To help us better understand the questionnai re and log data, we took notes and pictures during the sessions. Mo reover, we video recorded the inside participant and screen captured their screens, while the otherwise, such as whether the front or rear-facing camera was being used on the handheld phone. fairly poor. As a result, Video quality was lower than Images quality. Unfortunately, our participants expected very good video because of two reasons they mentioned in their comments. First, they thought that mobile video on 4G/LTE would have similar quality as on their home Wi-Fi. S econd, they believed that the quality of live mobile video shoul d be similar to that of pre-recorded content, such as videos from a GoPro camera. These expectations influenced the feedb ack about the live video streams. As predicted, our participants chose to share a variety of activities which resulted in many differences across the sessions. To help us activities people shared and how our prototype system was used to investigation into the usefulness of contextual information. the participants did during the sessions. The shared activities included two playtime sessions in a park, walking the dog, visiting a farmers market, and fly fishing. We describe six unique activities in detail as we observed them to be Projector-TV condition in the living room and three used the TV-Tablet condition. We focus on the twenty minute free-play periods in our descriptions. Playtime in park (Group 2) : In this session, a mom used the Projector-TV condition to join her husband as he took their young son to play in a park. The fath er and son walked around, climbed tables, visited a gazebo, and ate some snacks. Mom continuously interacted with them and suggest ed things to do in the park. The mom spent almost the entire time in Map and Images views. She used Map mostly when the fa ther and son were walking and often told them what was nearby:  X  at the end of that little curved road you X  X e going to be on, there X  X  going to be something, I don X  X  know what it is, but it X  X  an oval thing that X  X  green . X  When the dad and son were stationary, she used mostly Images, which worked mother enjoyed watching him stuff food into his mouth. She frequently navigated through the sn apshot history until she found the photo she liked best. Her comments reflected her usage video was useful because  X  that X  X  what Skype was already doing.  X  just the plain video is not as fun as when I have control.  X  The dad enjoyed experience because  X  X the mom] could see around us, and what we were doing, and she could see us  X ; however, he found the system cumbersome to manage while also taking care of his son:  X  it was hard because there was gear strapped to me, and the headphones, and the other camera  X . China Town tour (Group 5) : In this session, one participant gave participant joined using the Projector-TV condition. They talked about nearby restaurants and the history and future of China Town. Figure 3. (left) view of snack time from living room: front cam 
Figure 4. China Town from liv ing room showing full screen 
Figure 5. Shopping from living room: snapshot on TV with frequently switched among them. Overall, he used Video and Images the most, and he used the Map for location references and to ask location specific questions. At times, he surprised his friend Map views. For example, he asked  X  Hey, are you walking towards the big arch thing?  X  when he saw the China Town Gate on the map (Figure 4), or  X  Hey what X  X  that Gossip restaurant on the corner there?  X  when the sign appeared in a snapshot. In both cases, the friend in China Town paused in surprise at these questions because he did not expect his friend could see these participating in the tour, when aske d if the system helped him feel there.  X  He added that  X  I found myself gravitating toward the existing technology, like a basic Skype camera that you can switch comfortable . X  He liked that because  X  X the outside user] could control it and focus my attention on something . X  completely opposite. He felt that providing the additional contextual views automatically from the wearable phone made it feel like his partner was there:  X  when I had regular Skype it didn X  X  feel like he was right next to me. I knew I was talking to him over Skype. No major difference over phone call. When I had the other camera I was more inclined to show him things, because I was able to gesture freely, because I had this one in my pocket. I knew he could see what I was seeing. He is getting the same experience, me . X  Shopping at a mall ( Group 6) : In this session, a boyfriend remotely joined his girlfriend for a shopping trip. He used the TV-Tablet living room condition. They shopped for clothes and vitamins. The boyfriend used all contextual information, although he used Images the most and Video the least. He generally enjoyed the experience. At one point he said  X  you know normally I hate couch.  X  Once she held up a shirt for him to see, and he could see details such as the price tag because he was using the Images view:  X  I can see a white shirt. For $19  X . Later on, when shopping for vitamins, he used the Images view to find them himself. When she looked at the shelf, he said  X  oh yes I can see something there  X  was a bit surprised that I did have a feeling of being present . X  He more things that you can see at th e same time, that gives a better feeling of the activity to the person . X  The girlfriend wanted to see her boyfriend while shopping so she kept the front camera active on the handheld phone most of the time. She explained that she liked that she  X  could see his reaction, if he liked it or not  X . One challenge this pair had was that the she walked very fast, which made the video difficult for him to watch. market and then walked along a beach. The inside participant joined using the Projector-TV condition. The inside participant used all contextual information. Initially, she used Images and Map views interchangeably to get a sense of where her friend was. Later, she used Video and Images to see She used Video when she wanted to see something that was currently happening, such as people walking by or paddle  X  switching between the videos and the images and Skype with her so that I could really see what she was talking about  X . between what worked for her ... which was nice as well for me to feel like she was here X  . Walking the dog (Group 8) : In this session, a teenage girl walked her dog while a friend joined using the TV-Tablet living room condition. They generally just hung out during the session. The teenager outside felt self-conscious about holding the phone out in front of her, so she kept it in her pocket the entire time. She explained that  X  I like being able to see the person, but I don X  X  like when you walk around you have to hold it up X . The inside teenager used only Video and Images, so that she could see what her friend could see. At one point, the girl outside asked responded  X  yeah, I saw it in the pictures  X  and then she switched to Images (Figure 7). When asked how much the system helped her place there ... and it was really cool to talk to her about it while we look at it. X  Farmers market (Group 11) : In this session, two friends visited a farmers market. The one in the li ving room joined using the TV-
Figure 6. Sunset at beach from living room: front cam on TV 
Figure 7. Hanging ou t in the living room: no video on tablet Figure 8. Farmers market from the living room: rear cam on 
TV and snapshot on projector (left); outside view (right). Tablet condition. They visite d booths, tried samples, and generally enjoyed the market. the map, she commented  X  it looks like you X  X e been around the (Figure 8). At the end, she said  X  I felt pretty connect ed with her. It was neat to be able to click the map view to see where she was.  X  what I see.  X  The friend at the market was a nother participant who was self-conscious about wearing the prototype, and as a result, she was more negative about the setup. She also commented on the asymmetry of the experience:  X  my experience was probably really everything and I X  X  just walking around really, I wasn X  X  really watching her as closely on the monitor because she wasn X  X  doing anything . X  Remaining groups : In Group 1, two friends in attended a Viking festival where they walked around booths and a Nordic museum, ate some food, and watched a mock sword fight. In Group 3, a boyfriend and girlfriend went shopping , bought cupcakes, mom. In Group 4, a husband joined his wife at an RC airfield where they walked around and watched a person fly a plane. In Group 9, a husband and his wife went fly fishing and talked about wife as she took their daughter to play at a park . Summary of sessions : As these session observations illustrate, people used contextual informati on in a variety of ways. Each contextual view was useful at least some of the time, and some views were used in specific situ ations. The observations also show from them, and smooth negotiations about what they looked at. Next, we unpack these findings and present higher level insights about the usefulness and impact of contextual information. Based on the observations from the sessions, each of the three kinds of contextual information was useful. We were interested to see which of them had the highe st impact on connectedness and the sense of presence, and if one was more useful than the others. participants X  questionnaire answers. contextual information on inside users X  ratings of connectedness , Table 1). The pairwise differences reveal that the participants rated having the Images view in addition to a pure Skype call ( connectedness: Z=-2.85 p=.004 , being there: Z=-2.84 p=.004 , the Images view significantly higher than the Map view for p=.005 , Z=-2.99 p=.003 ). These results suggest that contextual information increased connectedness and the sense of presence for the inside participant. The Images view was the most effective, while the poor quality of the video worked against the Video view. The debrief interviews corrobo rated the questionnaire results. Nine of the twelve participants reported that the system helped help them feel like they were at the activity, all giving poor video quality as the reason. Meanwhile, the outside users e xperienced only two conditions, which were vanilla Skype (SkypeOnly) and contextual SkypeAndContext condition, they knew the inside user had three types of contextual information available, but they did not see what the inside user saw. Nevertheless, we also found significant differences for having and not ha ving contextual information on Table 1). They rated SkypeAndContext significantly higher than SkypeOnly on all three measures: ( connectedness: Z=-2.26 part of the activity: Z=-2.54 p=.011 ). During the debrief interview, many of the participants commented SkypeAndContext condition compared to the SkypeOnly condition (see earlier comments from Groups 2 and 5). For as good that you can see both the person and the surroundings X  . Like the inside users X  ratings for connectedness and sense of the inside participants rated having the Images contextual view in addition to a Skype call significantly higher than having only vanilla Skype (SkypeOnly) (Wilcoxon: usefulness Z=-3.11 p=.002 , enjoyment Z=-2.83 p=.005 ). In addition, the Images view was rated significantly higher than the Map view for enjoyment (Z=-2.69 p=.007 ). These results suggest that including contextual information was generally benefi cial for the remote attendee. T t h p T c o t h w h T u 7 ( m S s p O i n n w O a p c c v v s d E v a a F w I m l i s s ( r t h M t h t h D i n v T p l o a t h M p T hey also show t h e most useful a p rototype provi d T he results for t c ases, the map p o utside user wa s h e area well. Fo w alk that was w h and, three p arti c T he outside use r u sefulness and e 7 .0 and 7.5, re s median 8.0 f o S kypeAndConte x s ignificant ( use f p =.46 7 ). O verall, these n formation did n n ot see any of w ould not report O verall, our use r a nd helpful wi t p resence. Inter e c onnectedness a n c ould not see t h v iew was found t v ideo quality, w s ituations. In th e d ifferent views w E ven though ou r v iew on all me a a nd were found a nalysis of how e F rom the log da t w as utilize d and mages 47%, an d i ving room se t s ignificant mai n s hown (F 2,20 =6. 6
F 2,20 =3.418, p = evealed that the h e Map view ( p M ap and Imag e h em, which is h ree views (Tab D uring the ses n stances when t h v iew. T he Map view w p articipant was. o cation reading . a tour of China T h e map to see w M ap uses were l p erson is and w p artici p used t h had vi s While users X  c the ou t one in s to revi e Group nearby to the I what h a examp l son as was us poor q u in the price o f Video, happe n a walk and as k p artici p was g o Image s the ins i that ha v p artici p outsid e when t increas contex t phone. We di s driven p hone. p lay p e handh e p artici p during in Gro u which p When t on the views o
Figu r the outside user mostly used the front facing camera. As the session timeline shows (Figure 9 mi ddle), the inside user changed views frequently as they tried to get an angle of the activity. camera, the inside user tended to use the Images more. For instance, during Group 11 X  X  visit to the farmers X  market, the outside user only used the rear facing camera, which lead the combination of views  X  rear-facing camera video combined with Images. event. They also commented that they were more engaged because they had control of the context views. For example, the mom in Group 2 who was joining pl aytime at a park mentioned that having some control is more fun than just watching plain video. In addition to following the activity with contextual views, the inside participants were also able to use the views to contribute to Moreover, the details afforded by the Images view let them drive necessarily related to what the out side person was talking about at the moment. switched among the various contextu al views, none of the outside participants were able to tell wh ich view their partner was looking at unless they explicitly told them or commented on the view. fact, many commented that they liked their partner having the on the beach in Group 7, found that the inside participant X  X  ability to switch among the context views on their own made it feel like tour of China Town in Group 5 was stunned a couple of times when his friend asked him about things nearby that he did not think his friend could see. In summary, each contextual view contributed uniquely to the shared experience. Moreover, the ability of the inside person to outside person to choose how to use the hand-held phone, resulted in smooth negotiation of what view the inside participant was looking at. Finally, the availability of the various contextual views them to offer activity related input back to the outside people. living room. Our participants did not express any concerns about privacy during the study. As our sessions involved family, loved ones, and good friends, we did not expect su ch concerns, except perhaps for times when they went to the bath room (which never happened). However, privacy concerns were raised by people around our participants. For instance, in both of the farmers market sessions, when our participant approached booths selling art pieces, the sellers asked that no pictures of the art were taken. Meanwhile, in one of the mall shopping events, a security guard asked us to stop filming. Even though our participan ts informed the offended party that they were just video chatting, the concerns remained. wearable phone. Two participants commented on the hands-free benefit:  X  When I didn't want to hold up the phone with Skype she could still see where I was  X  P 8out and  X  I could put the phone in my participants whose hands were o ccupied during the activity, such participants who felt self-conscious about having the phone out. As P 12out expressed:  X  I liked that I could put the regular phone in my pocket if I had to, and so the other one was still taking getting something.  X  The main issue with our wearable camera prototype was that it was awkward and cumbersome. Two participants felt extremely self-conscious about wearing it and explained they  X  didn X  X  like things least liked about the whol e experience. Four participants mentioned the bulkiness of the belt to which the phone was attached. When we probed the participants on what type of wearable form factor they would prefer, they generally desired something compact, discreet, sleek, waterproof, rugged, hands-free, and voice-activated. We were also interested in und erstanding how the sizes of the displays in the living room affected the experience. During the debrief interview, we asked each inside participant to compare the two living room conditions. Since each participant experienced only condition one during the study, we verbally described the condition commented that the video was stretched out and may have felt better. This belief is supported by the questionnaire was ranked significantly higher in the TV-Tablet setup (median=1 for both measures) than in the Projector-TV setup (median=3.5 and 3) ( connectedness : Z=-2.19, p=.041 , being there : Z=-2.18, p=.041 ). However, as these results were affected by the poor quality of 4G/LTE video, they may not hold once video quality improves. Expected vs. Actual Video Quality : One of the key observations during our study was the gap between the expected quality of live video when streamed over 4G/LTE and the actual quality. All of our living room participants complained about the low video First, they assumed that mobile video quality is the same on Wi-Fi as it is on 4G/LTE. Second, they thought that live streaming video should have the same quality as videos recorded on a GoPro camera. Unfortunately, because wireless bandwidth is being consumed as quickly as it becomes available, the expectations gap will impact all near future mobile shared experiences. expected and actual streaming vi deo quality on wireless networks show that having a history of periodic images from the activity views. Although one reason for usefulness of Images view was their high what happened. While it is possible to review video streamed difficult to do so without affecting the live conversation. This did not seem to be an issue in our study when the participants reviewed images. The Map view helped the inside participants get a sense of what is in front of a remote part ner helped with navigation tasks. Therefore, it may be useful creat e a system that begins to support very course-grained navigation using our Map view and then switches to a Video view as the ta rget comes into visual range. Cognitive Costs : Although our study shows the benefits of including the contextual information in a mobile video chat system, these benefits do not come for free. An important issue is the cognitive load forced onto the participants with additional channels of communication. In our study, neither the inside nor the outside participants complained about any stress from the of the outside participants seemed to have been reduced as they mentioned that the activity context made them worry less about what their partners could see. Thus, it may be that cognitive load cognitive load is one of many [20] that future systems can leverage. Implementation Costs : Another cost associated with contextual information is the cost of building the software and hardware needed to support it. The system infrastructure will need to be redesigned to make tradeoffs between the performances of the various views for whatever bandwi dth is available. The UX of the additional information in useful ways. Finally, additional sensors (e.g., wearable cameras, etc.) may be needed, and the additional hardware costs will impact both system builders and users. Some of our participants reported that having just a smartphone out for a mobile video chat made them feel self-conscious, let alone the wearable phone. Ideally, the additional hardware should not increase the social stigma bey ond that of regular mobile video possible. Privacy : Related to social costs is the issue of privacy. Although activities had some concerns. These concerns did not seem to be about the people themselves being in the shot (although we believe that this can also be an issue). Instead, the concerns who asked for their pictures not to be photographed, the issue was that they did not want others to digitally reproduce their art. to stop did so because of mall policies about video recordings. Interestingly, even when our users explained that they were not actually recording anything, the artists and the security guard were still concerned. As mobile video chat becomes more pervasive, people to tell when a device is recording video and when it is in a video call. Limitations : Th ere are several limitations with our work that are important to mention. For one, the participants used the system for only twenty minutes, so the novelty effect could have influenced our findings. Also, the quality of 4G/LTE video clearly impacted prototype was cumbersome a nd awkward, which may have impacted our findings. Finally, out study had an element of artificiality because the remote participants did not join from their own living rooms. In this paper, we explored the impact of adding contextual information to mobile shared expe riences in which one person is at an activity and another joins remotely. Through a field study of events happening in the wild , we found that contextual information increased connectedness and the sense of presence for information is not  X  X ne size fit all. X  The three types of information we studied, Map, Video, and Images, all had unique positive to provide directions, Video for  X  X o you see this X  moments and to see that X  moments and to see greate r detail. Together the different sources of context provided add itional benefits. They led to joining remotely, and high levels of engagement. The benefits of including contex tual information are not without costs. Cognitive load, social aw kwardness, and privacy concerns may all increase. Thus, a careful comparison of costs and benefits is needed before adding such information to video chat systems. incorporate shared inking and a ugmented reality into mobile shared experiences. We also plan to study how stabilization of live video impacts the experience even if the video quality is poor like on current 4G/LTE networks. We will also design and evaluate new wearable form factors for these experiences. [1] Billinghurst, M., Kato, H., Real World Teleconferencing. [2] Bos, N., Olson, J., Gergle, D., Olson, G., Wright, Z. Effects [3] Buxton, W. Mediaspace  X  meaningspace  X  meetingspace. In [4] Dourish, P., Bellotti, V., Mackay, W., and Ma, C-Y. [5] Dourish, P. What we talk about when we talk about context? [6] Fussell, S.R., Setlock, L.D., Kraut, R.E. Effects of head-[7] Gaver, W., Sellen, A., Heath, C., Luff, P. One is not enough: [8] Inkpen, K., Taylor, B., Junuzovic, S., Tang, J.C., Venolia, G. [9] iPad bridesmaid attends wedding via FaceTime. [10] Jo, H., Hwang, S. Chili: View point Control and On-Video [11] Junuzovic, S., Inkpen, K., Hegde , R., Zhang, Z., Tang, J., [12] Kuzuoka, H., Kosuge , T., Tanaka, M. GestureCam: Video [13] O X  X ara, K., Black, A., Lipson, M. Everyday Practices with [14] Okada, K., Maeda, F., Ichikawaa, Y., Matsushita, Y. [15] Olson, J.S., Olson, G.M., Mead er, D.K. What mix of video [16] Procyk, J., Neustaed ter, C., Pang, C., Tang, A., and Judge, [17] Sellen, A., Buxton, B., Arnott, J. Using spatial cues to [18] Soldier watch es baby born over Skype. [19] Stafford, A., Piekarski, W., Thomas, B. Implementation of [20] Voida, A., Voida, S., Greenberg, S., and He, H. A. 
