 deduplication function that is easy to interpret and efficient to evaluate when deployed on large record lists. This required evaluating various non-obvious design tradeoffs that arise when using current active learning methods in a practical setting. Experiments on real-life datasets show that our approach reduces the number of labeled training pairs by two orders of magnitude to reach a certain accuracy. After labeling less than 100 pairs selected interactively by our system, the learnt deduplication function can achieve the peak accuracy which a randomly chosen set of pairs cannot achieve even with 7000 pairs. Outline There are two main parts to the paper. The first part (Section 2) is an overall description of our ALIAS interactive deduplication system. The second part starting from Section :3 covers the main novelty of our system, namely, the mechanism for selecting the informative sets of pairs, the foundations of which lie in Active Learning. Section 4 presents an experimental evaluation of the effectiveness of active learning in easing the deduplication task. Section 5 discusses related work. Finally, conclusions appear in Section 6. Figure 1 shows the overall design of our ALIAS system for deduplication. There are three primary inputs to the system: 1. Database of records (D) The original set D of records in which duplicates need to be detected. The data has d attributes al,...ad, each of which could be textual or numeric. The goal of the system is to find the subset of pairs in the cross-product D x D that can be labeled as duplicates. 2. Initial training pairs (L) An optional small(less than ten) seed L of training records arranged in pairs of duplicates or non-duplicates. 3. Similarity functions (.T) A set .~ of n I functions each of which computes a similarity match between two records rl, r2 based on any subset of d attributes. Examples of such functions are edit-distance, soundex, abbreviation-match on text fields, and absolute difference for integer fields. Many of the common functions could be inbuilt and added by default based on the data type. However, it is impossible to totally obviate an expert's domain knowledge in designing specific matching functions. These functions can be coded in the native language of the system (C++ in our case) and loaded dynamically. The functions in the set can be highly redundant and unrelated to each other because finally our automated learner wil perform the non-trivial task of finding the right way of combining them to get the final deduplication function. L into a pair format via a mapper module. The mapper module takes as input a pair of records rl, r2, computes the nf similarity functions ~ and returns the result as a new record with n I attributes. For each duplicate pair we assign a class-label of 'T' and for all the other pairs in L x L we assign a class label of "0". At the end of this step we get a mapped training dataset Lp.These Lp instances are used ALIAS interactive deduplication system. 
Sampling In the active learning phase, when we are learning a deduplication function, we may not need to work on the entire set of records D. Sampling appears like a natural recourse in such cases. However, simple random sampling will not work here because in most cases the number of duplicates will be few and sampling might further diminish such duplicate pairs. For example if only 10% of the records in D have one duplicate each, then a 5% random sample of the data will contain on an average just 0.10 x 0.05 x 0.05 = 0.025% duplicates. We propose an alternative grouped sampling approach that hinges on being able to find a grouping function such as above. In this approach we sample in units of a group instead of individual records. 
Indexing Another option we support is to index the fields of D such that predicates on the similarity function of the attribute can be evaluated efficiently. For example, a predicate on a similarity function of the form "fraction of common words between two text attributes _&gt; 0.4" can be evaluated efficiently by creating an inverted index on the words of the text attribute. Clearly, this cannot be done easily for all possible similarity functions. Edit distance is an example of such a hard to index similarity function. In most cases, however, it is possible to approximate a similarity function f with another function g that is always less than or equal to f. So, a predicate that retrieves all records r with f(r) _&gt; C can be transformed to a looser predicate of the form g(r) _&gt; C. This predicate can be evaluated via the index and later filtered for exact match. For example, [23, 10] show how an inverted index on the NGrams of a text field can used to lower bound the edit-distance function. Such ideas have been used earlier in [19, 10, 5] 
However, we cannot exploit such similarity functions unless we modify our active learning mechanism which as described in the previous section takes as input the materialized pairs Dp = D x D and chooses a subset n for labeling. We need to modify it to not require all of 
D x D at a time. It is possible to design active learners that can first output predicates that characterize the pairs likely to be selected by the learner. The predicates can be evaluated using the indices and only the qualifying pairs will be materialized and passed to the learner for further subseting to the n instances. This is just a rough outline of an approach. In this paper we will not have space to discuss this topic further. We defer further details to a future paper. 
Our main focus here is to first study the efficacy of active learning for deduplication. simple conjuncts and disjuncts on individual similarity functions instead of classifiers like SVMs and naive Bayes that combine the similarity functions in more complicated ways. Sometimes, it might be possible to post-process a classifier, like naive Bayes to extract indexable predicates as shown in [5]. 
Efficient training Finally, we want a method that is fast to train because for the interactive active learning phase, during each iteration we would need to retrain a classifier with a larger training dataset. Fortunately, the training data by all three of the above classifiers. 
An active learner starts with a limited labeled and a large unlabeled pool of instances. The labeled set forms the training data for an initial preliminary classifier. The goal is to seek out from the unlabeled pool those instances which when labeled will help strengthen the classifier at the fastest possible rate. What criteria should we use for picking such instances? The initial classifier will be sure about its predictions on some unlabeled instances but unsure on most others. The unsure instances are those that fall in the classifier's confusion region. This confusion region is large when the training data is small. The classifier can perhaps reduce its confusion by seeking predictions on these uncertain instances. This intuition forms the basis for one major criteria of active learning, namely, selecting instances about which the classifier(s) built on the current training set is most uncertain. We give an example to show how selecting instances based on uncertainty can help reduce a classifier's confusion. 
Example: Consider a very simple learner for separating points from two different classes: positive (P) and negative (N) on a straight line as shown in Figure 2. Assume that the set of points are separable by a single point on the line. The initial training set consists of one positive point b (star) and one negative point r (circle) picked randomly from the line. 
The rest of the points(squares) are unlabeled. The confusion region is the region between r and b. Any unlabeled point reducing this confusion. Hence they will not be selected for active learning. The points in between r and b are the ones about which the classifier is uncertain to varying degree. 
For any point x in this region, assume that the probability that it is negative is inversely proportional to its distance from r. For simplicity, assume r has a coordinate of 0 and b has a coordinate of 1. Thus, if x has a coordinate of d, the probability that its class is negative (N) is Pr(N[x) = 1 -d and Pr(PIx ) := d. If x were negative, the size of confusion margin would reduce by d, if it were positive the size would decrease by (1 -d). Hence, the expected reduction in the size of the confusion region on adding x to the training set is Pr(NIx)d+ Pr(PIx )(1-d) = (1-d)d+ d(1-d) = 2d(1-d). 
This achieves the maximum value when d = 0.5, that is, the point about whose prediction the classifier has the maximum uncertainty. By including m in the training set, the size of the uncertain region will reduce by half no matter what its label. Any other point say s that is close to the negative boundary but far from the positive boundary could reduce the confusion more if its true label is found to be positive in various ways. We used entropy on the fraction of committee members that predicted each of the two classes. 3.1.1 Methods of creating committees 
We next present three different ways of creating commit-tees. 
Randomizing model parameters A common method of creating committees is by making small perturbations on the parameters of the model trained through the given training data [28, 1]. The perturbations are made by sampling from a distribution that the particular training parameter is expected to follow. Some of the previous approaches show how to perturb the parameters of a Naive Bayes classifier[20] and Hidden Markov Model [1]. classifiers. During tree construction, when selecting an attribute for splitting on next, instead of deterministically choosing the attribute with the highest information gain, we randomly pick one with information gain within close range of the best. Secondly, when picking the threshold on which to split a continuous attribute, instead of picking the midpoint of the range within which the information gain remains unchanged, we pick a point uniformly randomly from anywhere in the range. 
Partitioning training data A second model-independent method of creating committees is by partitioning the training dataset in various ways, including disjoint and overlapping partitioning. Disjoint partitioning did not work well in our experiments due to the limitation of training data in the early phase of active learning. We therefore did N-fold overlapping partitioning where (for a committee of size N) we first partition a training dataset 
D into N disjoint sets D1, D2... DN. Then, train the i-th committee with the dataset D -Di. This way each member gets trained on (1 -~)th fraction of the data. 
Attribute Partition When the training data is sparse, but the number of attributes is surplus, another method of constructing a committee is by partitioning the attribute set in various ways. Like for data partitioning, one approach is disjoint partitioning where all attributes used in constructing a model are removed from the attribute set before constructing the next model. We stop constructing models once all attributes exhaust or the accuracy on the training data reduces drastically. Not surprisingly, this method did not work well. We therefore used a second approach that removed only the topmost attribute from each earlier model. This is applicable only for decision trees. 3.1.2 Comparing the three different methods of In Figure 3 we compare the three different methods of creating committee members for the two datasets and settings described in Section 4. The x-axis is the different rounds of active learning and since we select one instance per round this is also equal to the number of training instances, the y axis is F accuracy of the current classifier on the total unlabeled data Dp. parameter method performs the best overall, followed by attribute partitioning. Data partitioning is relatively worse 3.2 The representativeness of an instance Real-life data is often noisy. The most uncertain instance could often be an outlier. An uncertain instance that is representative of a larger number of unlabeled instances is likely to have a greater impact on the classifier, than an outlying instance. Hence, another important factor is how representative an instance is of the underlying data distribution. tiveness factor is figuring out how to combine it with the uncertainty factor. Two different methods have been proposed for this. tiveness of an instance by estimating the density of points around it after clustering the unlabeled instances [20]. The instances are then scored using a weighted sum of its density and uncertainty value and the n highest scoring instances selected. This method requires us to tune several parameters: distance function for clustering, number of clusters and the weights to tradeoff uncertainty with representativeness. to preserve the underlying data distribution [8, 1, 17]. First, each candidate unlabeled instance is weighted by its uncertainty value. Then the n instances for active learning are selected from this using weighted sampling. We chose and experimented with different variations of this approach starting with no-sampling to full-sampling. In no-sampling we simply pick the n highest uncertainty instances. In full-sampling we do weighted sampling on the entire unlabeled set. An intermediate approach is to first pick the top kn (k _&gt; 1) instances based on uncertainty and then do weighted random sampling on them to select n out of these kn instances, k = 1 then corresponds to no sampling, kn = total data size corresponds to the full sampling. We had a default k of 5. learning for the above three sampling schemes with two different base classifiers: decision trees (D-tree), a discriminative classifier and naive Bayes (NB), a generative classifier. For [)-trees we find all three sampling schemes to be comparable. We notice the real benefit of accounting for representativeness for naive Bayes classifier. This is expected because for generative classifiers maintaining the input data distribution is much more important than for discriminative classifiers. Theoretical analysis of this phenomenon can be found in [34]. n instances for labeling is given in Figure 6. We now present overall evaluation figures for our chosen active learning approach. obtained from CiteSeer by searching on the lastnames of the 100 most frequently referred authors. The data consisted of 254 citations, 54 of which were found duplicates (after careful manual searching). In the pair format, this led to 2s4 X 2sz = 32131 instances of which only 169 were duplicates 2 --that is, only 0.5% of the instances were of the positive and page, we had a special number match that tolerated shift by 1. For attributes that are likely to get wrongly segmented as a neighboring field, we created new text fields by concatenating the neighboring fields and defined on them the text matching functions. A special function was also designed for null-matches on each attribute. This is to distinguish cases where two attributes match because they are both nulls, or two attributes mismatch because one of them was null. 
Classification methods We used the following three classification methods: C4.5 decision tree classifier, .M X C++'s naive Bayes classifier [15], and SVMTorch 
Support Vector Machine classifier (SVM) [77]. Our experiments were performed on a three processor Pentium III server running LimLx redhat 77.0 with 512 MB of RAM. ten runs with different seeds of a random number generator that gets deployed in different stages of our algorithm as discussed in Section 3. 
Defaults The defaults for the different parameters of our algorithm were set to the best option found in the experiments of the previous section. The default classification method was decision trees. The number of committees was set to 5. The committees were created using parameter randomization and instances were selected using partial-sampling. The initial training set consisted of one exact duplicate and one clear non-duplicate. In each round of active learning one instance was selected for labeling, i.e., n = 1. The accuracy of the classifier at each round of active learning is measured on the entire unlabeled dataset. 4.1 Running time In Figure 7 we plot total running time with increasing rounds of active learning for both the datasets. This graph establishes that the time per round of active learning is limited to within 3 seconds. The address dataset takes longer since it has more unlabeled instances. The datasets used in this experiment are small and do not use any of the performance enhancements discussed in Section 2.1. This is a topic of our ongoing research. In Figure 8 we plot the performance of active learning under three different classification methods. These graphs show that decision trees provide the best F accuracy overall. In the legend of Figure 8 we show the precision and recall values at the last round of active learning. D-trees dominate SVMs which in turn dominate NB in both the precision and recall values. However, D-trees show a much larger fluctuation in accuracy in the initial stages. This is to be 276 In Section 3 we have already discussed the various ways We find that active learning requires one to two orders Future work include more extensive running time sample selection for probabilistic classifiers. Journal of 
Artificial Intelligence Research, 11:335-360, 1999. Automatic text segmentation for extracting structured records. In Proc. A CM SIGMOD International Conf. on Management of Data, Santa Barabara,USA, 2001. adding relevance information in a relevance feedback environment. In Proc. of SIGIR, pages 292-300, 1994. [20] A. K. McCallum and K. Nigam. Employing EM [21] T. Mitchell. Machine Learning. McGraw-Hill, 1997. [22] A. E. Monge and C. P. Elkan. The field matching [23] G. Navarro. A guided tour to approximate string [24] J. R. Quinlan. C~.5: Programs for Machine Learning. [25] V. Raman and J. M. Hellerstein. Potters wheel: An [26] S. Sarawagi, editor. IEEE Data Engineering special [27] G. Schohn and D. Cohn. Less is more: Active [28] H. S. Seung, M. Opper, and H. Sompolinsky. Query by [29] S. Toney. Cleanup and deduplication of an international [30] S. Tong and D. Koller. Support vector machine active [31] W. E. Winkler. Matching and record linkage. In [32] W. E. Winkler. The state of record linkage [33] B. Zadrozny and C. Elkan. Learning and making [34] T. Zhang and F. J. Oles. A probability analysis on the 
