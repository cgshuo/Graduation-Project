 ORIGINAL PAPER Hi X p Q. Luong  X  Wilfried Philips Abstract In this paper, we present a new approach for reconstructing low-resolution document images. Unlike other conventional reconstruction methods, the unknown pixel values are not estimated based on their local surroun-ding neighbourhood, but on the whole image. In particu-lar, we exploit the multiple occurrence of characters in the scanned document. In order to take advantage of this repe-titive behaviour, we divide the image into character seg-ments and match similar character segments to filter relevant information before the reconstruction. A great advantage of our proposed approach over conventional approaches is that we have more information at our disposal, which leads to a better reconstruction of the high-resolution (HR) image. Experimental results confirm the effectiveness of our propo-sed method, which is expressed in a better optical character recognition (OCR) accuracy and visual superiority to other traditional interpolation and restoration methods. Keywords Repetition  X  Restoration  X  Interpolation  X  Character segmentation  X  Bimodal distribution  X  OCR 1 Introduction Many applications nowadays rely on digital image interpola-tion. Especially improving image text resolution has become important in recognition applications. Some examples are improving the readability (e.g. of license plates provided by surveillance cameras or for office automation) and simple spatial magnification (e.g. printing low-resolution documents on high-resolution printer devices or displaying text in low-resolution pictures on the next-generation e-papers or High Definition televisions (HDTV) screens). Optical character recognition (OCR) is a useful tool in digi-talizing libraries, computer-assisted indexing and retrieval of video archives, etc. However text observed in low resolution (e.g.inpoorqualityvideoorwithverysmallfontsize)reduces the OCR performance heavily. That is why we need docu-ment reconstruction methods in order to improve the OCR accuracy.

Many image interpolation methods, which are not desi-gned specifically for text, have already been proposed in the literature, but all suffer from one or more artefacts especially when applied to text. Linear or non-adaptive interpolation methods cause staircasing (i.e. jagged edges in the upscaling process), blurring and/or ringing effects. Well-known and popular linear interpolation methods are the nearest neigh-bour method, bilinear interpolation, interpolation with higher order (piecewise) polynomials, B-splines, truncated or win-dowed sinc functions, etc. [ 20 , 28 ].

Adaptive interpolation methods incorporate prior know-ledge about images. Some methods focus on reconstructing edges [ 22 , 30 ] and other methods tackle unwanted interpola-tion artefacts such as staircasing, blurring and ringing using isophote smoothing, level curve mapping or mathematical morphology [ 18 , 25 , 29 ]. Another class of adaptive image enlargement methods is the training-based approach, which maps blocks of the low-resolution image into predefined high-resolution blocks [ 14 ]. This has been successfully applied to text images [ 8 , 9 ]. However, the results depend heavily on the used training set and thus the font type (which must be known in advance). Other specific text enhance-ment methods focus on contrast improvement [ 7 ], pixel patterns [ 41 ], fixing broken or touching characters [ 1 , 4 ], deblurring and noise reduction [ 35 , 37 ] and/or incorporate the bimodal property of text images in their reconstruction model [ 11 , 36 ].

When multiple acquisitions of the same document are available, which is the case in camera-based processing for example, conventional multi-frame super-resolution can be applied on the set of low-resolution (aliased) images [ 23 ]. Super-resolution is a signal processing technique that reconstructs the missing high frequencies by aligning these images on sub-pixel level. Several successful attempts have been made for super-resolution text [ 5 , 11 , 21 , 27 ]. It is generally assumed that true motion needs to be recovered for super-resolution. However, many registration methods opti-mize some proposed cost criterion, therefore, the calculated motiondonotnecessarilycorrespondtothetruemotion.With this in mind, we can hypothetically assume that similar struc-tures could serve as multiple noisy observations of the same structure. In order to avoid confusion with the self-similarity property in fractal-based interpolation methods, we will use the term repetition.

The concept of repetition or non-localization has already successfully been used for general image denoising [ 3 ] and our recently introduced image interpolation method for natural images [ 26 ]. Besides repetition in time (e.g. in image sequences), we can also find spatial repetition in images. Obviously in scanned documents, such repetition is more frequent than in general images. In this paper, we exploit the multiple occurrence of characters irrespective of font, character set and even the scan orientation. Note that each occurrence is not exactly the same due to the noise, different spreads of ink and the irregularity of the paper. In [ 16 , 40 ], the authors present a binary document compression scheme based on multiple occurrence of the characters, a similar stra-tegy will be used in this paper. In [ 2 , 15 ], binary text enhan-cement methods cluster instances of the same symbol and compute a reconstructed binary prototype (average) for each cluster. In contrast to these methods, our proposed scheme does not cluster characters (no misclustering possible), ope-rates in greylevel domain and uses robust Bayesian recons-truction to upsample, deblur and denoise the low-resolution characters.

In what follows in this paper, we exploit the repetititive behaviour and propose a robust reconstruction framework for degraded low-resolution document images. Section 2 gives the necessary background, describing the problem of image acquisition and defining problems non-local methods are facing today. In Sect. 3 , we present our proposed robust reconstruction framework. Section 4 presents experimental results and quantitatively compares our technique with other interpolation/reconstruction methods. Section 5 concludes this paper and describes additional possible applications of this algorithm. 2 Problem statement The document image acquisition process consists of making a (discrete) digital image out of a paper document. However in practice, the acquired image is corrupted by noise and blur. Some applications (such as OCR) need images with higher resolution than the acquired images. Re-acquisition is in some cases not possible (if the document or book is lost) or burdensomeduetothemassiveamountofwork(e.g.indigital libraries). Therefore development of resolution enhancement techniques are desired in those cases.

The unknown high-resolution image x is related to the captured low-resolution image y by y = Ax + n . (1) In this equation, A represents some linear degradation ope-ration, which is a combination of blur and decimation in our case, and n describes the additive noise, which is assumed to be zero-mean Gaussian distributed (with a standard deviation  X  ). The relationship between the paper document, the obser-ved low-resolution image y and the desired high-resolution image x is once illustrated in Fig. 1 .

To take advantage of repetition in our restoration frame-work, we define windows and then seek similar patterns. In [ 3 , 26 ], fixed squared windows combined with a full search algorithm are used to find repetition across the image. The computational complexity of these non-local methods is of the order of O ( m 2 M 2 N 2 ) for an M  X  N input image and an m  X  m window. The windows are preferably chosen such that the frequency of occurrences is maximized while the number of possible mismatches is minimized. In case of document images, we have prior knowledge of the structures of interest, i.e. letters and symbols. We exploit this in two ways: on the one hand we use adaptive windows that fit the symbols. On the other hand we take advantage of the fact that the number of different symbols in the document is limited in practice. This helps us to reduce the search space for similar symbols.
Based on this prior knowledge, we can enumerate briefly the three consecutive components in our algorithm: 1. Segmentation of the individual characters. 2. Seeking and aligning repetitive characters. 3. Reconstruction (i.e. upsampling, deblurring and denoi-3 Algorithm description 3.1 Character segmentation In this step, we focus on locating characters and segmenting them. This preprocessing part is typical in OCR algorithms. We will briefly describe the different steps of our character segmentation method. 3.1.1 Background modelling In this paper, we assume that the images have a uniform back-ground. To cope with uneven backgrounds (e.g. due to poor non-uniform illumination), we refer the interesting reader to [ 39 ].

To distinguish ink pixels (symbols) from non-ink pixels (paper), many algorithms use simple thresholds (binariza-tion). Actually, the problem is not trivial as illustrated in Fig. 2 . This figure clearly shows that the peak in the histo-gram corresponding to the foreground colour vanishes at low resolutions. To solve this problem we model the statistical distribution of the colour of the background. We assume that the noise in the non-character regions is normally distribu-ted. In presence of outliers (i.e. printed characters), we can utilize the univariate Student-t distribution to estimate the parameters of the Gaussian density function robustly [ 17 ]. The maximum likelihood (ML) of the parameters of the Student-t distribution can not be obtained in closed form, but the mean  X  b (this is the background colour) and the standard deviation  X  b can be computed by the expectation maximization (EM) algorithm [ 10 ]. We use a parameter expanded scheme to accelerate the convergence of the EM algorithm [ 24 ]. 3.1.2 Segmentation of lines The locations of the lines in a document can be determined by horizontal and vertical projection profiles in greyscale images [ 19 ]. A projection profile is a measure of the contri-bution of a row or column to the foreground or background. In the binary image case, the projection profile is simply coun-ting the foreground/background pixels in a row or column. In the greyscale image case, we propose to use order statistics and only calculate the sum of the n (e.g. n = 5) smallest pixel contributions to the background (using the background den-sity function as described in the previous subsection). That sum is then compared to a threshold. In that way we can robustly distinguish lines with very few characters from noise. It is clear that the lines can only be determined suc-cessfully if the pages are straighten out prior to segmentation, that is why deskewing is needed for rotated images. 3.1.3 Search for nonlinear segmentation paths Now, we segment each strip into individual characters. As in[ 19 ]weformulatethisproblemasfindingthetop-to-bottom path with minimum accumulated cost in a graph defined over a line or segmentation strip. This path is called the nonlinear segmentation path or the nonlinear character boundary. Note that the cost of a background pixel is less than a foreground pixel. All optimal candidate paths are obtained via the Viterbi algorithm, which is a typical dynamic programming method with a linear time complexity [ 13 ]. Among these candidates, the accumulated costs of the nonlinear segmentation paths are compared with a predefined threshold.

We modify the algorithm of [ 19 ] in three ways: we take the background pixel distribution into account instead of the greylevels, we find multiple paths in a segmentation strip in one step and we favour vertical paths by adding additional costs to slanted paths. The latter prevents that character  X  X  X  for example is divided in a upper and lower segment.
Thecharactersegmentationregionwithheight h andwidth w can be represented by a multiline graph as illustrated in Fig. 3 where each row corresponds to a row of pixels in the image and each vertex to a pixel. We choose the cost of an arc between a vertex in one row and a vertex in the previous row inversely proportional to the probability of the pixel being a background pixel: g ( x , y ) = I ( x , y ) represents the greyscale value of the pixel located at the coordinates ( x , y ) .Let f y ( x ) be the minimum accumula-The vertices of row y  X  1 connected to vertex at ( x , y ) ( x  X  1 , y  X  1 ) , ( x , y  X  1 ) and ( x + 1 , y  X  1 ) as illustrated in Fig. 3 . The nonlinear segmentation paths can be found by the following recursive algorithm, motivated by the work presented in [ 19 ]: Modified nonlinear segmentation path search algorithm:  X  Initialization: In the first step, f  X  Recursion: The minimum accumulated cost f  X  Backtracking: We backtrack each path with ending ver- X  Termination: Each starting vertex m  X  Selection: The candidate path ( m In a blank space, multiple parallel (vertical) shortest paths lie near each other. In such a group of paths, we select the one with the smallest accumulated cost f  X  ( m h ) . Figure 4 compares the results produced by our proposed segmentation method and by the unmodified nonlinear segmentation path search algorithm [ 19 ]. Our method has a lower miss rate in boundary detection: for example, in the original algorithm all minimum cost paths between  X  X met X  and  X , X  cross with paths between  X , X  and  X  X onse X  on the first line and thanks to the additional cost c s for slanted paths, we can avoid these crossings and recover the segmentation path.
 3.1.4 Bounding box determination We label each character segment using connected comp-onents labelling. The pixels are connected if there is no nonlinear character boundary in between and if they are loca-ted in the same segmentation strip. Afterwards we reduce each character segment to a bounding box. This is the smal-lest rectangle which entirely encloses the character. The shrinkage is solved with horizontal and vertical projection profiles as discussed earlier in Sect. 3.1.2 .

Remark that the bounding boxes can be overlapping because the segmentation boundaries are not always straight. When we consider a specific bounding box, we set the pixel values which lie inside the bounding box and outside the segment to the background colour  X  b . Also all pixel values outside the bounding boxes are set to the background colour. In that way, the influence of the background (i.e. paper) is minimized. To simplify the explanation we will omit such details in the rest of this paper. 3.2 Exploiting the repetition 3.2.1 Match criterion We define the characters B ref , obtained after character seg-mentation and bounding box determination, as the reference windows or blocks. We use the zero-mean normalized cross correlation (CC) as criterion to find matching windows B (with the same size of B ref ): where  X  contains all the pixels of the window B . B and B are denoted as the mean values of respectively B and B ref The CC criterion emphasizes the similarity of patterns (such as lines, curves, etc.) instead of colours.

Windows are said to be matching if E CC (  X  X  X  1 , 1 ] )is larger than threshold  X  CC : E CC &gt; X  CC . Threshold  X  CC inversely proportional to the noise level of the image (e.g. due to additive noise or due to quantization noise in DCT-based compressed images such as JPEG). The influence of wrong matches will be further reduced by a pixel outlier rejection component discussed in Sect. 3.3.1 . 3.2.2 Reduction of search space For the simplicity, we assume that same characters do not undergo any transformation such as rotation or scale. That means that only translational shift operations are needed, which of course saves a lot of computation time.

A simple exhaustive block matching algorithm across the whole image would require too much computation time. However in this case we exploit prior knowledge: there is no point in matching characters to background areas, so we only have to match with blocks which overlap with segmen-ted characters. We convert the labelled bounding box mask into a binary mask A (with the same size of the image): a pixel value is 1 if it is located in a bounding box and 0 elsewhere. The block matching needs only to be performed at the 1-positions of the mask.

We further reduce the search space by applying binary mathematical morphology operators on the mask [ 34 ]. We start applying a dilation on the mask M with a structuring element S d ( M  X  = M  X  S d ), which is defined as a 3  X  3 squared window with the origin positioned in the centre. The dilation operation enlarges the search areas and pre-vents missing repetitive characters due to boundary effects. Afterwards, we apply an erosion on the mask M  X  with struc-turing element S e ( M  X  S e ), which has the same size as the reference block B ref . The origin of structuring element S e is located at the upper-left corner. The erosion opera-tion optimizes the search space according to the shape of the reference block: there is no point in matching large cha-racters to small characters for example. The final binary mask M  X  = (( M  X  S d ) S e ) represents the enormously reduced search space.

Further improvements on the computation time could be achieved by feature-based prefiltering the characters and exclude dissimilar blocks from matching. A practical approach to this sub-problem is to build a dynamic library on-the-fly:eachrepeatedsymbolisrelatedtothefirstinstance ofeachsymbol.Howeverthisalsobringsalongnewproblems such as possible wrongly clustered characters which cause catastrophic errors [ 2 , 15 ]. 3.2.3 Imperfect character segmentation In case of imperfect character segmentation, i.e. due to over-or undersegmentation, we can apply the split-and-merge strategy, which is common in OCR applications [ 6 ]. In case of oversegmentation, i.e. with too many small seg-ments, it is important that the mask M  X  still contains enough candidates after erosion, this is solved by dilating first with a structuring element S d which results in a loss of perfor-mance. In case of undersegmentation, i.e. multiple charac-ters are segmented in one bounding box, we search for an additional nonlinear character boundary in that specific seg-ment. This split-strategy is applied when not enough similar character segments are found.

In a worst scenario case, imperfect character segmenta-tions can link parts or combination of characters to other different characters. An example of such a confusion is the undersegmentation of the following combination  X  X n X  (this is the combination of the characters  X  X  X  and  X  X  X  and is illustrated in the segmentation of the word  X  X rna X  in Fig. 4 ), which can be easily matched with the single character  X  X  X  in most font types. This type of errors can be solved with a dictionary-based OCR postprocessing by which unknown words are replaced by their most resemblant counterpart found in the dictionary. 3.2.4 Subpixel registration A common way to achieve subpixel registration in the spatial domain, is to interpolate either the image data or the correlation data. In order to save computation time we only resample the reference window B ref ( x ) in subpixel shifts to B  X  ref ( x ) . In this way we represent the downsampling ope-rator in the scan model in Fig. 1 as a simple decimation operator as illustrated in Fig. 5 . We estimate the subpixel shifts by minimizing the mean absolute differences (MAD) criterion: E where  X  contains all the pixels of the window B and  X ( X ) is the cardinality (i.e. the number of pixels) of  X  .Asa simplification of the registration problem, the possible sub-pixel shifts x are limited to multiples of ( 1 / r , 1 / r r the magnification factor in horizontal and vertical direc-tion. 3.3 Robust Bayesian reconstruction Let us have a closer look at the problem stated in Sect. 2 and especially the inverse problem of Eq. ( 1 ). The linear degra-dation operator A is the combination of the blur operator H and the decimation operator D as illustrated in Fig. 1 .We assume that the blur in the scan model can be modelled by a space-invariant point spread function (PSF), which is typi-cally a truncated two-dimensional isotropic Gaussian function characterized by its standard deviation  X  blur .The relationship between the high-resolution character x C and N registered low-resolution observations y k of the same character is given by y where y k is given by a window B that satisfies the CC match criterion in Eq. ( 11 ) and D k is the corresponding shifted decimation operator according to the subpixel registration in Eq. ( 12 ). We break the complex problem of finding x C that best complies with the measurements y k into two separate steps [ 12 ]: 1. fusing the low-resolution repetitive structures y k into a 2. estimating the deblurred and denoised character  X  x C We will discuss these two steps into more details in the next subsection. 3.3.1 High resolution fusion In this step we determine a pixel value for every pixel of the HR grid. With the shifted decimation operators, the pixel values of character y k are mapped onto the HR grid. In that way,weobtainzero(becauseofunderdetermination)ormore observations for these pixels.

In case of multiple observations, the additive noise can not simply be assumed to be Gaussian distributed in Eq. ( 13 ) because of errors due to a combination of noise, misregistra-tion and mismatching. In [ 12 ], the Laplace probability den-sity function (PDF) is suggested in the presence of different sources of outliers. The blurred character  X  z C can be found via the ML principle:  X  z The L 1 -norm or simply norm y k  X  D k z C 1 is very robust against outliers and the minimization of Eq. ( 14 ) corresponds to a pixelwise median of the repetitive structures [ 12 ].
In case there is no observation of a pixel of y k at a given pixel in the HR grid, the pixel will be initialized with the interpolated pixel value of B  X  ref . 3.3.2 Joint deconvolution and denoising The desired HR character  X  x C is obtained by deblurring  X  The inverse problem becomes highly unstable in the pre-sence of noise. This can be solved by imposing some prior knowledge about the image in a Bayesian framework.
Via the Bayes rule, finding the maximum a posteriori (MAP) deconvolution solution becomes the following minimization problem  X  x tionship between the observation and the solution) and p ( denotes the prior knowledge on the high-resolution image. In the next subsections we will describe the prior PDF X  X  and solve this minimization problem. 3.3.3 The image priors A common way to describe the prior PDF p ( x ) is the Gibbs distribution, which has the following exponentional form [ 9 ] p ( x ) = c G  X  exp { X   X  f ( x ) } , (16) where c G is a normalizing constant, guaranteeing that the integral over all x is 1, and the term f ( x ) is a non-negative energy function. The expression of the prior in the negative loglikelihood function (Eq. 15 ) becomes very simple, namely  X  f ( x ) . We will now describe two prior PDF X  X  on the HR image, namely the piecewise smoothness prior p S ( x C ) and the bimodality prior p B ( x C ) .

We assume that document images are locally smooth with sharp edges. The use of the so-called edge-stopping functions is very popular, because it suppresses the noise better while retaining important edge information [ 32 ]. That is why we use a Gibbs prior with the Lorentzian edge penalty function  X 
L which has the following PDF: p where  X  ( x ) denotes the 4-connectivity neighbourhood of x as illustrated in Fig. 6 . The preservation of sharp edges is controlled by the Lorentzian edge-stopping function which is defined by  X  ( x ) = log 1 + 1 where  X  L is called the contrast parameter, which controls the shape of the edge-stopping function [ 32 ]. In Fig. 7 the Lorentzian edge-stopping function  X  L ( x ) is plotted together with the popular Tikhonov X  X  smoothness function x 2 . Since the function  X  L ( x ) increases less quickly than x 2 for large gradients, sharp edges are preserved. Minimizing  X  L ( x ) to the following closed expression:  X 
Since HR document images have generally a bimodal his-togram (illustrated in Fig. 2 ), we use a Gibbs prior with a non-negative fourth-order polynomial [ 11 ]: p where  X  b and  X  f are the means of the background (i.e. paper) and foreground (i.e. ink) pixel distribution respectively and  X  is the standard deviation of these pixel distributions. Note that the parameters  X  b and  X  b are already calculated in Sect. 3.1.1 and  X  f canberetrievedinthesamewaybutiscalculated on the current HR estimate.

The bimodal PDF is shown in Fig. 8 . Note that this prior is only valid when the proportions and variances of the back-ground and foreground pixel distributions are equal, this is approximately true when we only consider the bounding boxes. The derivative of the fourth-order polynomial in Eq. ( 20 ) leads to the following expression:  X  ( x ) = 4 x 3  X  6 ( X  3.3.4 Solution of the MAP estimator We can now rewrite the formula of the MAP estimate of  X  x (Eq. 15 ) by substituting the previously defined priors:  X  x where  X  contains all the pixels on the HR grid. The likelihood term (or also called data fidelity term) is derived again from the Laplace distribution because of the combina-tion of different sources of outliers as discussed in Sect. 3.3.1 . The prior terms are also called the regularization terms where  X  parameters.

To solve this minimization problem, we use the steepest descent algorithm. This results in the following update rule  X  x + where  X  denotes the scalar step size in the direction of the gradient. We also make  X  n adaptive on the HR grid: for pixels we do not have any observations, these pixels are initialized with the interpolated values but are mostly likely noise, we decrease their data fidelity weight  X   X  1 n to 0. 4 Experimental results As a first and simple experiment we scan two sets of 30 A4-documents each at a resolution of 75 dots per inch (dpi) using the HP Scanjet 8250 and Epson Precision 4990 Photo machines. One set contains 177647 characters (in total including punctuation marks) of random non-English text (e.g. coming from the Lorem ipsum -generator) and the other set contains 177972 characters of random English text.

The parameters for our method in the scanning environ-ment are  X  blur = 3 . 0(fora4  X  linear magnification factor),  X  tion process. The regularization parameters  X   X  1 n = 30 (for observed pixels, otherwise the weight is set to 0),  X   X  2 and  X   X  4 b = 0 . 000004 are chosen accordingly to the rough inverse order of magnitude of the regularization terms and the rest of the parameter selection was based on trial and error, i.e. to produce the visually most appealing results. In very bad conditions (e.g. in presence of noise or com-pression artifacts), the regularization parameters  X   X  2 s  X  b should relatively have more weight compared to since the reconstruction will depend more on the prior know-ledge. The proposed algorithm is applied on one page at a time.

We compare our method with the popular cubic B-spline interpolation and the bimodal-smoothness-average (BSA) reconstruction method, which is a nonlinear optimization technique that maximizes the BSA score of the enlarged document image [ 36 ]. As a comparison with the ground-truth data, we have also scanned the same documents at 300 dpi, which also contain a certain amount of blur and noise.

After visual inspection, we can clearly see in Figs. 9 and 10 that our method outperforms traditional linear interpolation techniques (e.g. nearest neighbour and cubic B-spline inter-polation) but also the BSA method: the letters are much better readableandourmethodmanagetoreconstructthecharacters much better, noise is heavily reduced and blurring is much less. In Fig. 10 , we can observe true resolution improvement in the characters  X  X  X  and  X  X  X .
 Concerning the reconstruction time on a 3 . 0Ghz AMD Athlon XP machine with 512Mb RAM, the proposed algo-rithm takes about 12 minutes per page (with approximately 6000 characters in a 616  X  877 image).

As a numerical measurement of the image quality, we calculate the average MAD criterion (see Eq. 12 ) between the enhanced images and the groundtruth images. Because some resolution enhancement techniques may introduce (off-center) shifts, we apply proper image registration on the images by minimizing the MAD criterion on a discrete grid. Table 1 shows us the relative MAD compared to the original image and we can obviously see that our propo-sed method produces an image with the smallest deviation from the groundtruth. Note that the groundtruth image does not fully comply with our models, because we do not take certain effects into account such as the structure of the paper and other artefacts (e.g. different spreads of ink for the same symbol).

Since OCR is highly important in automated text applica-tions, we have tested the usefulness of our algorithm as pre-processing for several popular commercial and open-source page-reading systems: Scansoft OmniPage 15.0, ABBYY FineReader 8.0, Microsoft Office 2003 Document Imaging, gOCR 0.45 and Tesseract 2.01.

The OCR-generated string of characters is compared with the groundtruth data. The similarity of two strings is often expressed in the Levenshtein -distance L , which is the total minimum cost of transforming one string into the other using the following edit operations: insertions, deletions and sub-stitutions [ 31 , 33 ]. We assign the same cost, namely 1, to each edit operation and we calculate the Levenshtein-distance L (which is then the number of errors) based on the Ukkonen X  X  algorithm [ 38 ].
 The character OCR accuracy is then defined by C  X  L where C is the length of the correct string, i.e. the number of characters [ 33 ].

To prove the justification of the use of repetition, we also compare our method with the proposed Bayesian restora-tion without taking advantage of the repetition, i.e. only joint deblurringanddenoising.Figures 11 and 12 showtheaverage OCR accuracy for the different reconstruction techniques over the five OCR software. As we can see in both figures, both scanners produce similar results and the reconstruction method without using repetition produces quite similar OCR accuracy results compared to cubic B-spline interpolation and the BSA method. When we take the repetitive character behaviour into account, we have a significant improvement in the average OCR results. Because some OCR software are optimized for the English language, we investigate the influence of the language aspect on the OCR results. We can observe a small bias of 1  X  2% with preprocessing between the English and non-English sets as illustrated in Figs. 11 and 12 .

Table 2 gives us the percentage of errors (or Levenshtein-distance) recovered by our proposed method averaged over both scanners. When we compare to the original image, we can correct up to 94% of the errors based on a total of 355619 characters. When we only take the multiple occurrence of characters into account, we can correct up to 66% of the errors.

As a second experiment, we test the robustness of our algorithmagainstnoise.WeaddartificialGaussiannoisewith a standard deviation of  X  = 10 to the 60 low-resolution scans obtained by the Epson Precision 4990 Photo machine. We change the parameters  X  CC = 0 . 7 (such that more blocks are matched in presence of noise) and  X   X  1 n = 20 (such that the priors have more weight in the restoration part).
In Fig. 13 , we can clearly see that the noise is removed entirely and the characters are much sharper compared to the traditional interpolation techniques and the BSA reconstruc-tion method. In Fig. 16 , the OCR accuracy results are shown, again, we can observe the same trends as in the previous experiments: our proposed method outperforms the other enlargement techniques and cubic B-spline interpolation, the BSA method and the Bayesian restoration method produce similar results. Compared to the original experiment with no added noise (shown in Fig. 12 ), we can see that our propo-sed method has the smallest loss (only 4%) in performance in presence of noise while other techniques loose 6% up to 34% in OCR accuracy.

As a more representative real-world experiment, we acquire the same documents vi aa5megapixel Sony DSC-P120 digital camera. These images are compressed with a standard lossy JPEG compression scheme (an example is given in Fig. 14 ) and are manually deskewed via control points and bilinear resampling.

WecanseeinFig. 15 that our proposed method remove different artifacts such as noise, ringing and compression effects. The successful removal of ringing artifacts is partly ascribed to the bimodal prior. As shown in Fig. 16 ,thesame conclusions as in the previous experiments can be drawn for the OCR accuracy results. Despite the lower contrast and the more artifacts, the OCR accuracy results are comparable or even better than the simulated noisy data. The reason is that the resolution of the images obtained by the camera is higher than 75 dpi. 5 Conclusion In this paper, we have described a robust reconstruction technique to enhance the quality of low-resolution greyscale document images. Exploiting the multiple occurrence of cha-racters brings more information at our disposal, which leads to much better estimates of the unknown pixel values. In order to take advantage of this repetitive behaviour in a prac-tical way, we divide the image into character segments. The character segmentation reduces the computation time drasti-cally in two ways: the algorithm only has to focus on these regions of interests and the search space for possible mat-ching candidates is enormously reduced. Matching between the character segments filters relevant information before the reconstruction. Information originating from other similar characters are combined and the characters are reconstruc-ted in a Bayesian framework.

Results of different experiments show the effectiveness of our proposed intra-frame super-resolution technique: cha-racters and symbols are reconstructed very well and OCR results show a significant improvement of our method com-pared to other reconstruction methods. A trivial extension to our method is to take multiple pages of the same document, journals or book into account or to combine our method with multi-frame super-resolution techniques (for video applica-tions).Thiswouldproduceevenbetterresultsbecausethereis more repetitive information available. The proposed method can also deal with documents irrespective to their exotic font type, it even preserves the font type and is not res-tricted to characters of a particular alphabet, but can also contain generic symbols such as musical notes, hieroglyphics or mathematical symbols.

The strategy of using the repetitive symbol property is not restricted to the reconstruction of document images which suffer from noise, compression artefacts, low resolution scanning, wear processes (e.g. in old manuscripts), etc., but can also be applied in an example-based search engine and combined with an efficient document compression scheme (such as described in [ 16 , 40 ]) for instance. The latter is use-ful for the storage of large digital libraries or for transmitting documents. Repetitive characters contain redundant infor-mation, this redundancy can be removed for compression by constructing a prototype for each class/cluster of charac-ters and encode the remaining reconstruction errors [ 16 , 40 ]. Example-based search is useful in cases where OCR fails: a symbol, a prototype or a set of symbols can be suggested as a search string after which the search engine reports all similar symbols. OCR typically fails in situations where the characters are degraded too much or in case the alphabet is unknown (e.g. due to an exotic font type, foreign symbols, mathematical equations, etc.).
 References
