 In several applications the main focus of interest is a small proportion of the available data. These unusual cases have a large importance, and as such, antici-pating them is a critical task for these domains. An example of such applications is the prediction of the future returns of a stock. Unusually high (low) returns are rare, but they are the most interesting values for investors and thus they should be the target of any financial prediction model.

A related problem has been receiving great attention i n the data mining community: the construction of classification models based on samples with un-balanced class distributions (e.g. [1]). Predicting extreme values of a continuous variable can be handled through a classification approach by means of a dis-cretization process (e.g. [2]) off the continuous target variable. This would have the advantage of using all work that has been around in the areas of unbal-anced classification problems and evaluation under differentiated misclassifica-tion costs. However, this approach would require to establish the number of classes and, moreover, would lead to an undesirable crisp division between what is an extreme and what is a  X  X ormal X  case. These are some of the major draw-backs of handling regression as a classification problem 1 .

The problem of predicting rare extreme values is a particular case of multiple regression where a targ et continuous variable Y is being modelled using a set of predictor or input variables X 1 ,X 2 ,  X  X  X  ,X p . Any modelling method tries to find the model parameters that minimise an error function over the training sample. Standard functions used in regression setups are the Mean Squared Error, MSE = 1 n n i =1 ( y i  X   X  y i ) 2 , or the Mean Absolute Deviation, MAD = cost) and thus can be regarded as less adequate for our target applications, where errors on extreme values are more important.

One possible method for giving more weight to the errors on extreme values is to use case weights. Some algorithms allow the user to attach a weight to each case of the training sample. Mode l parameters can then be obtained by minimising a criterion that takes into account these weights. Using case weights that depend on the respective Y value being an extreme allows us to bias the obtained model to correct ly predict these extreme cases. The main drawback of this approach is that it only sees one side of the problem, the true values. In effect, this method does not try to avo id (or penalise) the cases where an extreme value is predicted by the model, but the truth value is  X  X ormal X , i.e. false positives according to the classification terminology. This drawback stems from the fact that the weights are dependent solely on the true value of the cases, y , instead of being dependent on both y i and  X  y i . Our proposal builds upon this idea by trying to eliminate this drawback through the use of a weight function that depends on both y i and  X  y i . The overall goal of this work is to have an evaluation metric that is biased towards valuating more the predictions of rare extreme values. Our proposal was developed with the following requirements in mind: i) the cost of a prediction error should depend on both the predicted and the true values, i.e. we should penalise both false positives and false negatives; ii) the cost of the errors should vary smoothly (no crisp divisions between extremes and non-extremes); iii) the method should have reasonable default costs (according to the overall goal) for applications where knowledge about the costs is not available.

We propose an evaluation metric that is basically a weighted average of the errors. Our key contribution lies on the form of calculating the weights. We use a weight function that depends on both the true and predicted values. We propose to use a smooth cost surface, w ( Y,  X  Y ), that can be seen as a continuous version of cost matrixes used in classification tasks. Summarising, our proposed Rare Extremes Error metric is defined as, where L ( y i ,  X  y i ) can be any loss function, e.g. the squared error.
In order to make the use of smooth cost surfaces practical we need to devise an easy way of specifying them. Our propo sal consists of requiring the specifica-tion of the cost values at a small set of properly selected points and then using a function approximation method to interpolate the complete surface. The axes of the surface are the true, Y , and predicted,  X  Y , values of the target variable. These range from low extreme values ( extr L ) to high extreme values ( extr H ). The points selected for specifying the co st surface should be related to the most relevant areas of the surface. These are the areas of lower cost (the model accu-rately predicts and extreme as such), and of the worse performance (the model predicts an extreme high for a tru e extreme low, or vice versa).

For applications where no cost information is available but still extremes are more important, we need to describe means to setup the costs for the key points used for surface approximation. The critical question is to define what is a rare extreme value. We use the same definition as in Torgo and Ribeiro [3]. This means that we set extr L = adj L and extr H = adj H ,where adj L ( adj H ) is the smallest observation that is greater or equal to the 1st quartile minus 1 . 5 r ,with r being the interquartile range. After having defined these two extreme values we artificially create n grid points by diving the interval extr H  X  extr L in n equally spaced bins. This means that we will have a ( n +2)x( n + 2) matrix to fill in with costs. We use an arithmetic progression to setup the co sts from the lowest to the highest cost. Full details and illustrative examples can be found in [4]. We have carried out a series of experiments with the goal of checking the validity of our proposed metric in the task of identifying the models that are better from the perspective of being more accurate at rare extreme values. With this purpose we have designed the following experimental setup for each data set: 1. Draw a stratified test sample with 50% of the cases; 2. Randomly generate a set of prediction errors with the same size as the test 3. We then artificially allocate this se t of generated errors to each case on
A performance metric that is biased towa rds accurate predictions on extremes, should clearly indicate that the per formance of Model A is better than the performance of Model B. Notice that, given that the errors of the two models are exactly the same (only occurring at different test cases), metrics like the MSE or the MAD will show both models as having exactly the same score.
As we are testing on a large set of domains with a quite different range of target variable values, we have used a normalised version of our performance statistic to allow comparisons across domains, where Y is the sample median.
 The goal of our experiments is to assert the score difference between models A and B, when evaluating them using our proposed metric and an alternative measure. With this purpose we have measured the percentual difference of scores for all data sets. Positive values of this difference indicate that our metric is able to identify Model A as performing better than Model B. We obviously want the difference to be as high as possible, as Model A has an  X  X deal X  performance. We have compared our proposed metric, NRExE , against the score obtained by the most similar alternative, an error measure using case weights as mentioned in Section 1. For this competitor we have setup the case weights such that more weight is given to cases with extreme values of the target (details on [4]). Notice that contrary to our approach the weights of this measure only consider the true value of the target, thus not taking into account the predictions of the models. For each data set we have repeated the e xperiment outlined above 10 times. The results shown on Table 1 are the average and standard deviation of the observed percentual differences between Model A and B, when using NRExE and the metric with sigmoid-based case weights. The best scores for each data set are indicated in bold. The used datasets are real world problems with a diverse set of rare extreme values types. For instance, some include both low and high extremes, while others include only one t ype of extremes. Due to space reasons we are not able to present the full characteristics of these problems.
The results reported in Table 1 show the advantages of our proposed metric for domains where the main objective i s to be accurate at rare extreme values. In effect, in most problems our metric corr ectly signals model A as being signif-icantly better than model B, in spite of being compared against a competitor metric that also take extremes into account. Notice that standard measures, like MSE, would signal both models as being equal (difference equal to zero). In this paper we have described the parti cular features of a class of problems with high practical importance: the pre diction of rare extreme values. We claim that existing metrics for evaluating the performance of different models have several drawbacks and perform poorly on identifying the best models in terms of predictive accuracy on the most important cases for these applications. We have presented a new metric that is particularly suited for these applications.
In a set of experiments using real world data we have shown that this measure is able to identify the best model in term s of accuracy on the rare extreme values, even on the most difficult scenario where b oth models have exactly the same error distribution and thus have the same score in  X  X tandard X  metrics like MSE.
One of the main impacts of the results of this work is that our metric can be used to compare different existing models on tasks where the main goal is the accuracy on rare extreme values. The use of our metric should provide better in-formation concerning the merits of alternative models for these important tasks. Another important side effect of this work is the possibility of using the described metric in the search process of any modelling technique, so as to develop models that are built for maximising the predictive performance on extreme values. This work was supported by FCT project MODAL (POSI/SRI/40949/2001) co-financed by POSI and by the European fund FEDER and by a PhD scholarship of FCT (SFRH/BD/1711/2004) to Rita Ribeiro.

