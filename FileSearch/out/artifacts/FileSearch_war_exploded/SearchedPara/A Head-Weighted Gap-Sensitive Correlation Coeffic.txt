 Information retrieval systems rank documents, and shared-task eval-uations yield results that can be used to rank information retrieval systems. Comparing rankings in ways that can yield useful insights is thus an important capability. When making such comparisons, it is often useful to give greater weight to comparisons near the head of a ranked list than to what happens further down. This is the fo-cus of the widely used  X  AP measure. When scores are available, gap-sensitive measures give greater weight to larger differences than to smaller ones. This is the focus of the widely used Pear-son correlation measure (  X  ). This paper introduces a new measure,  X 
GAP , which combines both features. System comparisons from the TREC 5 Ad Hoc track are used to illustrate the differences in emphasis achieved by  X  AP ,  X  , and the proposed  X  GAP . H.3.4 [ Systems and Software ]: Performance evaluation Evaluation Metric; Rank Correlation Coefficient
In information retrieval evaluation, we often wish to compare alternative systems based on some single-valued evaluation mea-sures. For example, we might want to know whether comparing systems using relevance judgments created by one user (to whom we have access) can be used to compare systems in ways that are predictive of what we would have seen had some other user made the judgments [9]. Alternatively, we might want to know whether we can better approximate the system comparison results we could compute with very extensive relevance judgments on a large num-ber of topics by reducing the number of topics or by reducing the number of judgments per topic [2]. In such cases, we can formulate our research question as asking about the correlation between two ranked lists of scores, where the scores result from some evaluation metric such as F 1 , Expected Reciprocal Rank (ERR), Normalized c Discounted Cumulative Gain (NDCG), or Mean Average Precision (MAP).
 When making such comparisons, we focus on two desiderata. First, we prefer that those differences to be large, since small dif-ferences may not reflect any meaningful degree of impact on the user experience [3]. Second, we prefer that those differences be statistically significant, since unreliable measurements of differ-ences could be misleading [7]. When extending our comparison from pairs to large sets of systems, as is common in shared-task evaluations such as TREC, CLEF, NTCIR and FIRE, we often care more about distinguishing systems that are very different from each other, which requires the evaluation metric to be gap-sensitive. We also care more about the comparisons among the best systems than we do about comparisons between, for example, the best and the worst systems, which requires the evaluation metric to be head-weighted.

Perhaps the most widely used measure of rank correlation in information retrieval research is Kendall X  X   X  [4] in which swap-ping systems is penalized. Yilmaz et al. [10] introduced a head-weighted variant of  X  that they call  X  AP that penalizes the mis-ranking of the best systems (i.e., those near the head of the list), and that measure is now also often reported. Buckley and Voorhees have observed, however, that when systems receive very similar scores we should care less about swaps than when those scores are very different [1]. They therefore created a gap-sensitive mea-sure by suppressing the effect of small swaps by treating any swap within a  X  X uzziness value X  (e.g., 5% relative to the smaller value) as not being large enough to be counted. In this paper, we propose to generalize that approach to penalize swaps in proportion to the difference in scores, so that large swaps will gave the greatest influ-ence on the measure, but small swaps are not completely ignored. This approach thus potentially offers greater insight, without the need to commit to a specific  X  X uzziness X  threshold. Moreover, we combine this more nuanced approach with the head-weighted de-sign of  X  AP to produce a new correlation measure for ranked lists of scores that is both head-weighted and gap-sensitive. We call our new measure  X  GAP (for Gap And Position).

The remainder of the paper is organized as follows, Section 2 re-views the prior work on the use of correlation measures for system comparison. Section 3 then define  X  GAP and establishes that it has a number of desirable properties. Section 4 complements this ana-lytic perspective with empirical results for system scores from the TREC 5 Ad Hoc task, and then further analyses the focus of differ-ent correlation coefficient metrics through the heatmap of system pair weights. Section 5 concludes the paper with some remarks on limitations of the  X  GAP measure that may inspire further work on this important problem.
The Pearson correlation coefficient between two items is defined as the covariance of the two items divided by the product of their standard deviations: where X and Y are the vectors of ranked lists; E is the expecta-tion;  X  is the standard deviation; and  X  is the mean [6]. Given two ranked lists of items, the Spearman correlation coefficient [11] is defined as the Pearson correlation coefficient between the ranks (i.e., with the ranks used in place of scores). The most widely used measure in information retrieval research is neither Pearson nor Spearman correlation, however, but rather Kendall X  X   X  [4]. Kendall X  X   X  evaluates the correlation of two lists of items by counting their concordant and discordant pairs.

To fulfill the specific evaluation needs for different tasks, var-ious definitions of correlation coefficients derived from Kendall X  X   X  have been proposed. Kendall X  X  tau rank distance [5] measures the disagreements between two ranking lists by counting the swaps that the bubble sort algorithm needed to sort one list in the same order as the other. The AP Correlation coefficient (  X  AP by Yilmaz et al. [10], focuses and penalizes more on the errors at high rankings. Given a ground-truth list and prediction list,  X  the two lists is defined as: where N is the number of items in the list; C i is the number of items above rank i in the prediction list and correctly ranked with respect to the item at rank i in the ground-truth list. For each item at rank i ,  X 
AP only checks the positions of the i  X  1 items with ranks above i , and calculates the proportion of the correctly ordered items with respect to the item at rank i . Finally, the value of  X  AP combination of all ranks i in the prediction list.  X  GAP is a non-parametric correlation coefficient, particularly sen-sitive to errors at high rankings and errors for item pairs with large score differences (gaps). In this section, we present the definition of  X  GAP . Since the definition of  X  AP and  X  GAP are both based on swapped item pairs counting, we further explore the mathematical properties of  X  GAP comparing with  X  AP . Given two ranked lists, a ground truth list and a prediction list,  X 
GAP is defined as in Equation 3, where N is the number of items the item ranked higher than i in the prediction list; G ji between the item at rank j and item at rank i in ground-truth list; CG ji returns the same value as G ji when the item pair ( j prediction list are ranked in the same order as in the ground-truth list, otherwise, return 0. For each item at rank i in the prediction list,  X  GAP only considers the rank relation of the i  X  1 item pairs ( j , i ) .  X  j &lt; i | G ji | is the sum of all the gaps for the i T HEOREM 1. The value of  X  GAP is always between  X  1 and 1 .
P ROOF . For each item at rank i , there will be i  X  1 pairs of items ( j , i ) taken into the consideration by  X  GAP .  X  j &lt; i | G the gaps of these i  X  1 pairs of items.  X  j &lt; i | CG ji der as the ground-truth list. Therefore,  X  j &lt; i | CG ji 0 and 1. Normalized across all ranks i , 1 always between 0 and 1; 2 0 and 2. Then the value of  X  GAP is always between  X  1 and 1.
T HEOREM 2. If the gaps between the items follow the uniform distribution, then  X  GAP is equal to  X  AP .

P ROOF . Let the uniform gap be g , then  X  j &lt; i | G ji and  X  j &lt; i | CG ji | = C i  X  g . Therefore, Moreover, if the errors are uniformly distributed over the all the ranks in prediction list, then  X  GAP ,  X  AP and  X  are equivalent. T HEOREM 3. For two prediction lists with same number of items N and same number of errors located at the same ranks, if the er-rors of one list have large gaps and the errors of the other list have small gaps, then the value of  X  GAP for the list with large error gaps  X 
GAP  X  Large will be smaller than the  X  GAP for the list with small
P ROOF . If we define FG ji as the gap between items at rank i and rank j when the item pair ( j , i ) in the prediction list are ranked in the converse order as in the ground-truth list, or otherwise 0, then  X 
GAP could be represented as: If two prediction lists have the same number of errors located at the same ranks, then their will have the same N , i , j and CG difference between  X  GAP  X  Large and  X  GAP  X  Small is that  X  has larger values for FG ji , so that the value of  X  GAP  X  Large always smaller than  X  GAP  X  Small .

T HEOREM 4. For a prediction list, if the swapped item pairs always have small gaps, then the value of  X  GAP is larger than  X  if the swapped item pairs always have large gaps, then the value of  X  GAP is smaller than  X  AP .

P ROOF . The expectation of the difference between  X  GAP and  X  AP is:
E [  X  GAP  X   X  AP ]= Let P i be the probability that the items before item i in the predic-tion list are ranked as the same order as in ground-truth list, then we have Then E [ The overall expectation of E [  X  GAP  X   X  AP ] is a linear combination over all ranks i  X  [ 2 , N ] . However, for each rank position i in the predict list, the expectation of the difference between  X  is: Since  X  j &lt; i G ji is always positive, the relation between  X   X 
AP at rank i depends on (  X  j &lt; i CG ji  X  P i  X  j &lt; are ( i  X  1) pairs of items and their corresponding gaps in the pre-diction list taken into consideration, and there are P i ( large gaps, then  X  j &lt; i CG ji &gt; P i  X  j &lt; i G ji gaps, then E i [  X  GAP  X   X  AP ] &lt; 0; ideally, if the P dered pairs are distributed randomly across all levels of gaps, then E [  X  GAP  X   X  AP ]= 0. Since the expectation of E [  X  GAP  X   X  linear combination over all E i [  X  GAP  X   X  AP ] , we could conclude that if the error gaps of a prediction list are relatively small, then  X  large, then  X  GAP is smaller than  X  AP .
In this section, we use 61 participating systems from TREC 5, ranked by MAP, as a case study to compare the different emphases of the correlation coefficients  X  ,  X  AP and our proposed  X 
Figure 1 shows the result of a two-sided paired t -test for each pair of systems based on Average Precision (AP) scores for each of the 50 topics as samples, with both axes sorted in the same order. In that figure, system pairs with p &lt; 0 . 05 are plotted as white (37%) and system pairs with p  X  0 . 05 are plotted as black (63%). This illustrates clearly that swaps among many of the systems (near the main diagonal, where score differences are smallest) would offer little insight into whether one evaluation framework yielded system comparisons that we meaningfully different from another.
Figures 2 and 3 then each illustrate two ways of making ranked lists to compare. Each dot represents a system, with the true (TREC-5) rank of that system plotted on the X-axis, and a randomly per-muted rank on the Y-axis. We produce the random permutations by randomly selecting five system pairs each time and swapping them. In Figure 2, we randomly select systems pairs that are statis-tically significantly different from each other (i.e., five of what were black dots in Figure 1). We do these five random draws 50 times (Figure 2 shows only one of the 50 times). The correlation coef-ficient metrics that result are  X  = 0 . 70;  X  AP = 0 . 68;  X  averaging over 50 such random draws of five pairs to swap. In Figure 3, we randomly select system pairs from among the set of pairs that are not statistically significantly different from each other (i.e., five of what were white dots in Figure 1). We do this 50 times. The correlation coefficient metrics that result are  X   X 
AP = 0 . 92;  X  GAP = 0 . 97, averaging over 50 such random draws of pairs to swap. In general, we can see that swapping system pairs with large gaps yields lower correlation scores with any measure than swapping system pairs with small gaps. But the key obser-vation to make is that, as proven in theorem 4, when the swapped system pairs have relative large gaps,  X  GAP = 0 . 65 is lower than  X 
AP = 0 . 68; whereas when the swapped system pairs have rela-tively small gaps,  X  GAP = 0 . 97 is larger than  X  AP = 0
Figures 4, 5 and 6 show the weight for each system pair in eval-uating the correlation coefficient of two ranked lists under the mea-surement of  X  ,  X  AP and  X  GAP respectively. The systems are also ranked by their ground truth MAP scores. So cell (1, 61) repre-sents the weight of only swapping the systems at rank 1 and rank 61. In detail, taking  X  GAP in Figure 6 as example, the value of cell (1, 61) is calculated by: (1) produce the prediction list by only swapping the systems at rank 1 and 61 in ground truth list; (2) calculate the value of  X  GAP between the ground truth list and the prediction list; (3) fill the value of (1  X   X  GAP ) to cell (1, 61); (4) heatmap. Therefore, in general, darker cells represent system pairs with higher weight in evaluating the correlation coefficient. Figure 4 shows the heatmap of system pair weights by using  X  . Since  X  is only sensitive to the score difference, not the ranks of items, we can observe that the value of  X  is dominated by the swapping pairs with large differences, probably composed of a sys-tem at very high rank and a system at very low rank. The system pairs along the diagonal with non-significant difference get lower weights as expected. However, the swap of top ranked systems also get lower weights because of their relatively small difference. The heatmap of  X  AP in figure 5 shows a progressively decreasing from the top-left corner of (1, 1) to the bottom-right corner of (61, 61) due to its sensitivity over ranks. However, comparing Figure 5 with Figure 1, we can see that the 37% non-significant system pairs still get non-ignorable weights in calculating  X  AP . Figure 6 shows the heatmap using  X  GAP . Since  X  GAP is sensitive to both top ranked systems and the gap between system pairs, we can also observe a decreasing of weights for the system pairs from top-left corner to the bottom-right corner. At the same time, we can see a expansion of the lower weight area (bright-ish area) along the diagonal line comparing with the heatmap of  X  AP . This is due to  X  GAP tivity to the system pairs with small difference. Overall, the value of  X  GAP is dominated by the top ranked system pairs with large difference.
In this paper, we proposed a new metric  X  GAP for evaluating the correlation coefficient between two ranked lists of scores. We have shown that  X  GAP is sensitive to both top ranked items and to swapped item pairs that exhibit larger differences. Through anal-ysis, we have shown that  X  GAP compares favorably with both  X  and  X  AP , and using the TREC 5 Ad Hoc track as a case study we have illustrated that the swaps that  X  GAP is most sensitive to the ones we have argued we should care the most about. Although we have introduced  X  GAP in the context of system comparison, it is of course a general correlation coefficient for ranked lists of scores that could be applied in any case in which both a head-weighted and gap-sensitive measure would be useful. There are, however, some characteristics of  X  GAP that might be further improved upon. For example, it might be useful to completely discount differences in scores that are not statistically reliable indicators of real differ-ences in system behavior. As Figure 1 illustrates, such cases can Figure 1: Significance of the MAP differ-ence on TREC 5. be common. As another example,  X  GAP implicitly presumes that the scores are represented on a meaningful interval scale, meaning that (for example) a user would prefer a difference of 0.2 twice as much as they would prefer a difference of 0.1. User studies have shown that some current evaluation measures do not exhibit any-where near this degree of correlation to extrinsic measures of suc-cess such as task completion rates [8]. Future extensions that more closely model extrinsic measures of satisfaction or success might therefore be useful. Nonetheless, we see the progression from  X  to  X 
AP and now to  X  GAP to be a useful one, and one that can perhaps serve as a basis for future extensions of these and other kinds. This work has been supported in part by NSF award 1065250. Opinions, findings, conclusions and recommendations are those of the authors and may not reflect NSF views.
 [1] C. Buckley and E. M. Voorhees. Evaluating evaluation [2] N. Gao et al. Reducing reliance on relevance judgments for [3] K. S. Jones. Automatic indexing. Journal of Documentation , [4] M. G. Kendall. A new measure of rank correlation.
 [5] M. G. Kendall. Rank correlation methods. Griffin, 1948. [6] K. Pearson. Note on regression and inheritance in the case of [7] M. Smucker et al. A comparison of statistical significance [8] A. Turpin and W. R. Hersh. Why batch and user evaluations [9] E. M. Voorhees. Variations in relevance judgments and the [10] E. Yilmaz et al. A new rank correlation coefficient for [11] G. U. Yule. An introduction to the theory of statistics . C.
