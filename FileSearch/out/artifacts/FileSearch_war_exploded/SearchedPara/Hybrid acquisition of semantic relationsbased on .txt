 1 Introduction
Whatever the domain, specialized texts are char-acterized by terms and relations between terms.
Identifying these relations is crucial in many ap-plications in Natural Language Processing (NLP), such as information retrieval, question-answering systems, information extraction in search engines or specialized automatic translation. For instance, a semantic relation that links the terms sucre (sugar) and saccharose will allow to increase re-call in a retrieval information system.

Those relations may be provided by terminolo-gies, but usually those resources are not tuned to the targeted texts (Bourigault and Slodzian, 1999). Relations may also be automatically ac-quired from specialized corpora, through differ-ent strategies. We can take into account morpho-logical (Grabar and Zweigenbaum, 2000), syn-tactic (Jacquemin, 1997) or semantic informa-tion (Jacquemin, 1999), define lexico-syntactic patterns through observation in corpora (Hearst, 1992; Morin, 1999; Auger and Barriere, 2008), use machine learning techniques (Snow et al., 2005) or distributional analysis (Habert and
Zweigenbaum, 2002), etc. All the methods show various limits. Regarding the quality of the results, they can either get a low recall (methods are too restrictive) or a low precision (ambiguities or pol-ysemy are not well identified). Furthemore, ap-proaches usually aim at acquiring only one rela-tion type (for eg., hyperonymy).
 Let X  X  take the example of two methods :  X 
Lexico-syntactic patterns allow to get a good precision, but are limited by their (very) low recall (Embarek and Ferret, 2008), because of the quite restricted contexts they use.  X 
On the contrary, distributional analysis (DA) is more flexible and allows to put many terms in relation, with a great diversity of relation types (Morlane-Hond X re and Fabre, 2012), but without returning a type of re-lation. Indeed, DA do not seem to offer any obvious way to distinguish between syn-tagmatic (collocations, noun-verb relations) and paradigmatic relations (synonymy, hy-peronymy) (Fabre and Bourigault, 2006).

Furthemore, methods based on DA are generally used with big amounts of data and tend to be less efficient with low frequency words (Caraballo, 1999). Results obtained with general language are promising, but improvement is still required with specialized texts, even if good results have already been achieved (Habert and Zweigenbaum, 2002).
As mentionned above, one of DA X  X  limit is low frequency words. Indeed, for those words, sim-ilarity is computed from very little information (i.e. the one in contexts), that leads to gener-ate poorer quality groupings of terms (Caraballo, 1999). We assume that this information could be increased with semantic information as the one contained in an existing resource or acquired by a relation acquisition method, as for example, us-ing hyperonymy relations acquired with patterns.
Following this idea, we intend to define a hybrid method that switches words in DA contexts for their hierarchical parent or morphosyntactic vari-ant. This method normalizes contexts (Henneron et al., 2005), to increase their frequency.

We first present the related work, then our hy-brid method and we finally describe the different experiments we led. The results we get are then evaluated in terms of precision. 2 Related work
This work uses a DA method, based on the har-rissian hypothesis that states that words appearing in a similar context tend to be semantically close (Harris, 1954). The DA principle has been auto-mated in the 90 X  X , and concepts and procedures used in distributional computations have been well defined (Sahlgren, 2006; Turney and Pantel, 2010;
Baroni and Lenci, 2010). However, this area of research still represents some current issues con-cerning the building, the evaluation and the use of distributional resources 1 . We focus here on the building of distributional resources.

In that respect, during the past few years, re-search has shifted from using DA methods for modelling the semantics of words to tuning them for the semantics of larger units such as phrases or entire sentences (Hermann et al., 2013). Most ap-proaches tackle the problem through vector com-position. Mitchell and Lapata (2008) use lin-ear algebraic vector operations, testing both ad-ditive and multiplicative models, and a combina-tion of these models. Grefenstette and Sadrzadeh (2011) apply unsupervised learning of matrices for relational words to their arguments, in order to compute the meaning of intransitive and tran-sitive sentences. Baroni and Zamparelli (2010) use matrices to model meaning, but only for adjective-noun phrases, whereas Grefenstette and
Sadrzadeh (2011) X  X  work also applies to sentences containing combinations of adjectives, nouns, verbes and adverbs. Recently, the framework pro-posed by Grefenstette et al. (2013) combines both approaches.

An important issue in DA improvement focuses on distributional contexts, and more precisely on weighting contexts. Broda et al. (2009) consider that what matters is not the feature X  X  exact fre-quency. They do not use these frequencies as sim-ple weights but rank contexts and take into ac-count this rank in DA. Influence on contexts may also be done by embedding additional semantic in-formation. With a method based on bootstrapping,
Zhitomirsky-Geffet and Dagan (2009) modify the weights of the elements in contexts relying on the semantic neighbours found with a distributional similarity measure. Based on this work, Ferret (2013) faces the problem of low frequency words by using a set of positive and negative examples selected in an unsupervised way from an original distributional thesaurus to train a supervised clas-sifier. This classifier is then applied for reranking the semantic neighbours of the thesaurus selec-tion. With the same purpose of solving the prob-lem of data sparseness, other methods are based on dimensionality reduction, as Latent Semantic
Analysis (LSA) in (Pad X  and Lapata, 2007), or on a bayesian approach of DA (Kazama et al., 2010).
Above work exploits great collection of texts of general language. However, few works are also interested in applying DA to specialized do-mains where text collections are generally smaller and frequencies lower (Habert and Zweigenbaum, 2002; Embarek and Ferret, 2008). As presented previously, Ferret (2013) attempts to exploit ma-chine learning approaches to face the problem of low frequency of words and contexts. In our work, we propose an approach that exploits relations ac-quired with linguistic approaches in order to nor-malize contexts and increase their frequency. As we work with specialized texts, our approach dif-114 fers in considering nouns, adjectives and both sim-ple and complex terms. 3 Hybrid method
The contexts in which occurs a target word have associated frequencies which may be used to form probability estimates. The goal of our hybrid method is to influence those distributional context frequencies by normalizing contexts. Indeed, nor-malization tends to decrease diversity in contexts in order to increase contexts X  frequency. Our hy-brid method follows the scheme presented in fig-ure 1.

Target and context definition During Step 1, we define target words and contexts. Through the literature, syntactic analysis is mainly used to get dependency relations. But as it is time-consuming and heavy, we choose to use instead graphical windows within a sentence and around the target word. As we work on specialized texts, we also identify terms with the term extractor Y A T E A (Aubin and Hamon, 2006).

We define the following parameters:  X  Target words: words are in relation when  X  Distributional contexts: contexts are made of  X  Fixed window size: we tested two different  X 
Word form: for both contexts and target words we use the lemmas.

Linguistic approaches During the normaliza-tion process described below, we use three exist-ing linguistic approaches: two methods that aim at acquiring hyperonymy relations and one that al-lows to get morphosyntactic variants.  X 
Lexico-syntactic Patterns (LSP): we use the patterns defined by (Morin and Jacquemin, 2004): where NP is a noun phrase and LIST a list of noun phrases.  X 
Lexical Inclusion (LI): uses the syntactic analysis of the terms. Based on the hypoth-esis that if a term (ex:  X pice (spice) ) is lex-ically included in another (ex:  X pice aro-matique (aromatic spice) ), there is a hyper-onymy relation between the two terms gener-ally (Bodenreider et al., 2001).  X 
Terminological Variation (TV): uses rules that define a morpho-syntactic transforma-tion. This transformation may be an inser-tion, as the insertion of the adjective aro-matic in  X pice asiatique (asian spice) - X pice aromatique asiatique (asian aromatic spice) (Jacquemin, 1996).

Context normalization Once targets and con-texts are defined comes the core of the hy-brid method with context normalization. During 115
Step 2, we normalize contexts with the relations acquired by the three linguistic approaches we mentionned.

The relations are integrated in contexts in the following way: a word in context is replaced by its hyperonym or its morphosyntactic variant. We define two rules :  X  If the word in context matches with only one  X  If the context matches with several hyper-
We normalize contexts with each method sep-aretly and sequentially: the first normalization is processed on all contexts before the second nor-malization starts, and so on.

Computation of semantic similarity When contexts have been normalized, similarity between two target words of the same POS tag is computed.
As we decrease diversity in contexts during the normalization step, we choose among the exist-ing measures (Weeds et al., 2004) a measure that favors words appearing in similar contexts com-pared to words appearing in many different con-texts.

The Jaccard Index (Grefenstette, 1994) normal-izes the number of contexts shared by two words by the total number of contexts of those two words. sim  X  JACCARD mn =
Parameter: threshold We filter the relations according to three parameters, two of them ap-plied on the contexts and the third one on the tar-get.  X 
Number of shared contexts: number of lem-matized contexts. For example, if two words share cr X me (cream) , battre (shake) , poivre (pepper) , sel (salt) , cr X mes (creams) , battant (shaking) , the number of shared contexts is 4.  X 
Frequency of the shared contexts: number of occurrences of the same lemma when shared in the context position of two target words. In the previous example, frequencies are cr X me (cream)-2 , battre (shake)-2 , sel (salt)-1 and poivre (pepper)-1 .  X 
Frequency of the target words: number of oc-currences of the lemma in the target position. For each parameter, a threshold is automatically computed, according to the corpus. It corresponds to the mean of the values taken by each parameter on the whole corpus. 4 Experiments
In order to evaluate the contribution and influence of relations acquired by the three methods, we de-fine several sets of experiments and evaluate the relations acquired on existing resources. 4.1 Corpus
We use the merging of the two corpora pro-vided by DEFT 2013 French challenge 2 : the train-ing corpus (2,388,731 words) and the test corpus (1,539,927 words). They are both French corpora and contain cooking recipes. Each text of the cor-pus is made of a title, ingredients and the body of the recipe, and we use all the information.
 We pre-process the corpus within the Ogmios platform (Hamon et al., 2007). We perform mor-phosyntactic tagging and lemmatization with Tree Tagger (Schmid, 1994), and use the term extractor Y A T E
A(Aubin and Hamon, 2006). 4.2 Parameters and models of hybridization
In these different sets we vary two main kinds of parameters (cf. table 1): window size and models of hybridization.
 We test two window sizes. With a large one of 20 words around the target (10 before, 10 af-ter, henceforth W10) we may take into account the highest number of possible relations, because the average size of a sentence in French is 20 words Window size 4 (W2) and 20 (W10) words around the target
Hybridization and we restricted the relation acquisition to the sentence level. But such a large window may face a lack of specificity and get too much noise. We also test a window of 4 words (2 before, 2 after, henceforth W2). Such a size applied after remov-ing the function words is comparable to a 8 word window applied to the original texts (Rapp, 2003).
We test different models of hybridization. We first use DA on its own, without normalizing the contexts (DAonly). This set is a reference to which compare the hybridization sets. As for the models of hybridization, we first separately evalu-ate the contribution of each method (LSP, LI, TV) in distributional context, and then different types of combinations of the methods integrated in DA.
Within these combinations, we first exploit two methods together and then three. Our goal is to evaluate the impact of the order of the methods and the contribution of each method. 4.3 Comparison with existing resources
In order to evaluate the quality of the acquired re-lations, we compare our relations with three dif-ferent resources: Agrovoc 3 , of 75,222 relations [AGRO], and UMLS 4 .

With the UMLS resource, we build two dif-ferent resources: one more general [UMLS] of 2,325,006 relations, and a more specific one re-stricted to terms belonging to the Food concept (semantic type T168) [UMLS/Food] of 1,843 re-lations.

We only use the relations for the nouns and terms of our corpus, because adjectives were not represented in the resources. In that respect, we evaluate our work with 1,551 ([AGRO]), 1,800 ([UMLS]) and 871 ([UMLS/Food]) relations.

We use those three resources because of avail-ability. The comparison with UMLS/Food and
Agrovoc is justified by the presence of relations between food terms in both resources. But in cooking recipes, we may find other types of re-lations, as the relation between a food term and a term belonging to another semantic class. The comparison with the whole UMLS may allow to detect other relations than ingredient relations.
Even if we can not expect an important overlap between these resources and the corpus, the com-parison of our results to the relations issued from these resources gives an indication of the contri-bution of each proposed hybridization model. We compute precision for each target term: semantic neighbours (acquired by our method) found in the resource by the semantic neighbours acquired by our method. For each target term, we sorted the semantic neighbours we obtained ac-cording to their similarity measure, and apply four thresholds: precision after examining 1 (P@1), 5 (P@5), 10 (P@10) and 100 (P@100) neighbours. 5 Results and discussion
We proceed to the analysis and discussion of the results we obtain with our hybrid method. Regard-ing the relations provided by the terminologies, we present here the results obtained for nouns and terms only.
 We evaluate precison after examining four groups of neighbours. The best results are ob-tained with P@1, and decrease when we consider more neighbours: the more neighbours we con-sider, the lower precision is. For instance, for nouns-W10, precision decreases from 0.089 for P@1 to 0.009 for P@100, when compared with
Agrovoc. We make similar observations on all the sets of results. Best results in first position means that the values of the measures rank quite correctly the proposed relations, and therefore that the choice of the measure was a good choice. The table 2 presents the results for P@1, given the two window sizes (W10 and W2). We describe here only those results. The relations produced by DA (DAonly) are considered as our baseline. The low precision of our results was expected and 117 can be explained by the fact that even if the re-sources are relevant for our corpus, they are not fully adapted. However, the comparison of the precision values gives important information on the usefulness of the hybridization models.

Results are better for nouns (between 0.056 and 0.111 for nouns-W10 and between 0.024 and 0.073 for nouns-W2, with Agrovoc) than for terms (between 0 and 0.047 for terms-W10, and between 0 and 0.34 for terms-W2, with Agrovoc). This is not surprising because terms do not match easily with other terms in resources. This can be due to two main factors: terms are less frequent and it is difficult to match terms from the terminologi-cal resources in the corpus. As for the window size, we observe that generally W10 gives good results for nouns and W2 is better with terms. But when we look more in details, we observe that the quality of the results depends on the resource used for comparison. For nouns, with Agrovoc and
UMLS/food, W10 gives the best results, but when compared with UMLS results are better with W2.
The difference is similar with terms, but in this case results are better with W2 when compared with UMLS and UMLS/food, and better with W10 when compared with Agrovoc.

Linguistic approaches Considering the three methods individually, TV seems to have no in-fluence on the computation of semantic simi-larity; the results obtained with DAonly and DA/TV are identical, except for terms W2 and
W10 when compared with UMLS. Also, in all the hybrid sets, exploiting TV in the distribu-tional contexts doesn X  X  influence the results, ex-cept for DA/LI+LSP+TV with nouns-W10 when compared with Agrovoc and UMLS/food, and DA/TV+LSP with term-W2 when compared with
UMLS. This may be because of the small num-ber of relations used and our current way of DA hybridization with TV.
 On the contrary, LSP is the method that most in-fluences the results: most of the time they give bet-ter results than DA. The best hybridization model for terms is the normalization with LSP, whereas for nouns the combination of LI and LSP is the best choice. The order of the methods also mat-ters, but results also differ according to the re-source; DA/LSP+LI (and DA/LSP+LI+TV) give better results when compared with UMLS/food, and DA/LI+LSP (and DA/LI+LSP+TV) give bet-ter results when compared with Agrovoc. What emerge from these results is that generalization with hyperonyms is the best configuration, for both terms and nouns, and that the quality of the hyperonymy relation is important as well. Lexi-cal inclusion used after patterns does not seem to bring new relations but allow to rule out noisy re-lations. By noisy relations, we mean relations not found in the resource. But these relations may be interesting and may be domain relations.

Resources and relation types The relations found by our method in UMLS/Food are co-118 hyponyms (eg: ail (garlic)/oignon ), those found in Agrovoc are both hyperonyms (eg:  X pice (spice)/poivre (pepper) ) and meronyms (eg: miel (honey)/sucre (sugar) ). Relations found in the whole UMLS are the same as those found in
UMLS/food. The identification of terms allow to find more relations, between simple terms and complex terms. For instance, in UMLS/food, our method found the co-hyponyms poivre blanc (white pepper)/poivre noir (black pepper) and miel (honey)/fruit that are not identified by taking into account nouns only. 6 Conclusion
In this work, we present our hybrid method based on normalization of distributional contexts. Our method aims at acquiring semantic relations from specialized texts, and is adapted to low frequency words. We normalize contexts with relations ac-quired by three linguistic approaches; two meth-ods of hyperonymy relation acquisition and a method of morpho-syntactic variant acquisition. We focus on relations between nouns and terms.
We tested our method on a French corpus com-posed of cooking recipes, varying one parameter in our DA method, the window size, and testing different models of normalization. Normalization obtains the best results when realized with hyper-onyms and also depends on the quality of the hy-peronymy relations. In our method, the hyper-onym used for normalization is the one with the highest frequency. Even if precision values pre-sented in this work are currently low and results differ according to the resource used for evalua-tion, it emerges that the best parameters are for nouns a W10 with LSP, and for terms a W2 with
LSP and LI. This set of parameters is to be used for classical types of relations. But other types may be acquired with our DA++ method, especially do-main specific relations. In order to have a better knowledge of the influence of each hybridization model, quality of the results has to be analyzed more deeply by manually checking with the val-idation of a subset of relations, and with a study of relations that are in common or not between the various results sets. For future work, we plan to investigate other strategies of normalization by assigning a weight to the relations proposed by the linguistic methods, or taking into account the level in the hierarchy. In that latter approach, the choice of the hyperonym used for the normaliza-tion could be guided by a distance (Resnik, 1995;
Leacock and Chodorow, 1998). Relations used for normalization can also be issued from terminolog-ical resources. Furthemore, we will intend to com-bine the methods before normalization and exploit other similarity measures.
 References Cet article d X crit une exp X rimentation visant  X  faciliter la reconnaissance automatique des termes (T) en texte int X gral. Elle contribue  X  un objectif final d X  X ndexation de textes via la re-connaissance automatique de termes en textes connaissance en nous focalisant sur l X  X nvironnement textuel des candidats termes, dans une philosophie similaire  X  celle de (Ba-chimont et al. 2005 ; Bourigault et al. 2001 ; Bourigault et al. 2004). Nous nous inscrivons dans le champ de la terminologie textuelle (Bourigault et Slodzian 1999) qui appr X hende les termes dans leur fonctionnement textuel. L X  X ypoth X se que nous testons est celle d X  X ne interaction privil X gi X e entre les unit X s de lexique transdisciplinaire scientifique (ULT) et les termes du vocabulaire de sp X cialit X  d X  X ne discipline. Plus pr X cis X ment,  X  la suite de (Kis-ter et Jacquey 2012), nous faisons l X  X ypoth X se que les co-occurrences port X es par une relation syntaxique entre les unit X s du lexique transdis-ciplinaire (ULT) et les termes peuvent  X tre des indices du statut terminologique des termes. Par exemple, dans le domaine des sciences du lan-gage, les termes diglossie et locution apparais-sent dans les textes avec concept et analyser : le concept de diglossie et nous analysons les locu-tions . 
L'exp X rience de (Kister et Jacquey 2012), qui portait sur un petit corpus contrastif scientifique vs vulgarisation, a montr X  l X  X nt X r X t de mener une exp X rience  X  plus grande  X chelle : extension du corpus, automatisation, structuration s X mantique du lexique transdisciplinaire (toutes les ULT ne sont pas  X quivalentes du point de vue de l'hypo-th X se examin X e).

Notre exp X rience, centr X e sur le discours scienti-fique, automatise deux  X tapes cl X s : (1) la projec-tion du lexique transdisciplinaire, (2) la d X tectio n et la qualification des relations syntaxiques T-
ULT. Cette  X tape d X  X utomatisation permet ainsi d X  X ugmenter la taille du corpus de travail. 
Enfin, nous effectuons une analyse quantitative et qualitative des relations d X tect X es. L'analyse qualitative est r X alis X e en fonction d'une pre-mi X re structuration s X mantique du lexique trans-disciplinaire. Des pistes d'exploration sont pro-pos X es pour affiner les proc X dures dans la pers-pective d'une exploitation automatique du lexique transdisciplinaire. 
Le lexique transdisciplinaire des  X crits scienti-fiques est un lexique particulier qui ne renvoie pas aux objets scientifiques des domaines de sp X cialit X , mais plut X t aux discours sur les objets et les proc X dures scientifiques (Tutin 2007). Il es t mis en  X uvre dans la description et la pr X senta-tion de l X  X ctivit X  scientifique et est ainsi partag  X  par la communaut X  scientifique. Il s'agit donc d'un lexique de genre plus que d X  X ne terminolo-gie propre  X  un domaine. Il est en grande partie partag X  par de nombreuses disciplines, m X me si des diff X rences se font jour entre familles de disciplines. 
Ce lexique concerne ainsi des unit X s lexicales qui renvoient aux proc X dures scientifiques (  X tudier, analyser, recenser ), aux opinions ( de notre point de vue, nous pensons ),  X  l' X valuation ( valide, int X ressant, pertinent ), aux artefacts scienti-fiques ( approche, hypoth X se, mod X le ), aux ob-servables ( donn X es, r X sultats ). Il int X gre  X  la fois des mots simples et de tr X s nombreuses s X -quences lexicalis X es aux fonctions diversifi X es (Tutin,  X  para X tre).
 Pour les mots simples,  X  la suite du Vocabulaire 
G X n X ral d'Orientation Scientifique (Phal 1971), plusieurs listes ont  X t X  propos X es pour l'anglais (Coxhead 2000 ; Bolshakova 2008 ; Paquot 2010) et pour le fran X ais (Drouin 2007) et (Tutin 2007). Ces listes ont  X t X  constitu X es sur la base de crit X res statistiques (distribution dans plu-sieurs disciplines, fr X quences, sp X cificit X ). Dans une perspective d X  X ndexation de textes, nous envisageons d X  X tiliser le lexique constitu X  par fusion de ceux de (Drouin 2007) et (Tutin 2007) de deux mani X res : En approfondissant et en cherchant  X   X valuer cette derni X re hypoth X se, nous esp X rons contri-buer  X  l X  X xtraction et  X  la reconnaissance auto-matique des termes dans les textes, sous l X  X ngle de la d X limitation des unit X s terminologiques d X  X n domaine par le biais du lexique transdisci-plinaire. Comme bri X vement d X crit dans l X  X ntroduction, plusieurs phases segmentent les traitements r X alis X s sur les textes ( cf . Tableau 1). Phase 4 Filtrage et analyse des r X sultats 3.1 Corpus de travail, ressources lexicales et 
Les articles de l'exp X rimentation sont extraits du appartenant tous au domaine des sciences du langage (42 textes r X partis en 22% d X  X rticles et 78% de communications -155 157 occurrences). 
La ressource terminologique de r X f X rence est entr X es). 
Le lexique transdisciplinaire utilis X  est le r X sult at d'une fusion des lexiques de Tutin (2007) et 
Drouin (2007), restreint aux entr X es nominales et verbales afin de r X duire le bruit, ce qui entra X ne l X  X xclusion des adjectifs. Il comporte 390 noms ( cas,  X tude, travail, type ) et 321 verbes ( agir, consid X rer, correspondre) , soit 711 entr X es. 3.2 Annotation terminologique 
L'annotation terminologique est r X alis X e en deux  X tapes principales. Les extracteurs ACABIT (Daille 1996) et TER-
MOSTAT (Drouin 2003a et b) fournissent des listes de candidats termes en appliquant sur les articles donn X s en entr X e, un jeu de r X gles lin-guistiques et de crit X res statistiques. Apr X s avoir  X t X  projet X s dans les articles sources, les candidats termes sont s X lectionn X s manuellement pour ne retenir que les occur-rences terminologiques valides, qu'il s'agisse d X  X nit X s simples ou complexes. Pour d X termi-ner la validit X  des candidats, nous avons eu recours  X  des experts linguistes qui ont pu, si besoin, utiliser les deux ressources terminolo-giques de r X f X rence, Thesaulangue et le vocabu-laire Francis ( cf . section 3.1). Parmi les 43 324 occurrences de candidats, nous avons conserv X  11 772 occurrences jug X es valides (21%). A l X  X ssue de cette s X lection, les candidats valid X s sont consid X r X s comme des occurrences de termes. 3.3 Projection du lexique transdisciplinaire La projection du lexique transdisciplinaire est r X alis X e par un module distinct qui prend en entr X e les articles enrichis en termes et le lexique transdisciplinaire s X lectionn X . A l'issue d'un  X tiquetage morpho-syntaxique (TreeTagger et d'une projection du lexique transdisciplinaire, l'ensemble des co-occurrences d'une m X me phrase est report X  en tenant compte de la com-binatoire possible. Une m X me phrase qui con-tient deux termes (T 1 et T 2 ) et 2 ULT (ULT 1 et ULT 2 ) diff X rents est report X e 4 fois en mettant en valeur  X  chaque fois un couple (T i , ULT j ) diff X rent. A l'issue de ce traitement, nous obte-nons 53 281 relations de co-occurrence impli-quant 1 273 termes diff X rents : corpus 1 417 relations, mots 8 1 156, mot 1 068, verbe 1 067, sens 814, etc. Ensuite, les phrases contenant des co-occurrences T-ULT sont analys X es syntaxiquement ( X  l X  X ide de XIP) afin de d X terminer si la relation de co-occurrence est une relation syntaxique de d X pendance et de pr X ciser la nature de la d X pendance quand il y en a une. 3.4 D X tection et qualification des relations 
Comme indiqu X  pr X c X demment, les phrases trai-t X es sont analys X es en d X pendance avec XIP de fa X on  X  rep X rer automatiquement les relations syntaxiques de d X pendance entre un terme et une ULT. 
Sur les 53 281 co-occurrences T-ULT du corpus, 4 565 relations de d X pendance directes ont  X t X  rep X r X es. Ces relations syntaxiques de d X pen-dance sont class X es par ULT, par nature de la relation de d X pendance et par orientation de cette relation (ULT rectrice ou r X gie). Nous avons ainsi pu identifier les associations lexico-syntaxiques les plus fr X quentes (relation i , ULT j ).
L'analyse quantitative et qualitative des r X sultats obtenus porte sur les seules relations directes et plus particuli X rement sur trois patrons de rela-passives DEEPSUBJ et DEEPOBJ. Ces trois types de relations ont  X t X  choisis, d'une part, pou r leur productivit X  (3 406 sur 4 565) et, d'autre part, pour leur repr X sentativit X . La relation de type NMOD s' X tablit principalement au sein du groupe nominal complexe. Les relations de type 
OBJ et SUBJ apparaissent dans le domaine ver-bal. De cette mani X re, nous esp X rons couvrir tout ou partie des cas les plus int X ressants d'interac-tion entre termes et ULT. 
Nous pr X sentons ci-dessous des exemples de relations entre un [terme] T r X gi et une [unit X  du lexique transdisciplinaire] ULT rectrice : 3.6 Filtrage manuel des r X sultats Nous avons  X cart X  les erreurs d'analyse syn-taxique parmi les 3 406 relations  X tudi X es ce qui nous conduit  X  rejeter 768 analyses erro-n X es : erreur de recteur ou de r X gi, erreur de relation.
 Puis, nous avons class X  les 2 638 couples res-tants (T i , ULT j ) : les cas pour lesquels l X  X LT joue un r X le de d X limiteur de terme et ceux pour couple analys X  est ainsi plac X  dans l X  X ne des trois cat X gories suivantes : La r X partition obtenue pour les 2 638 relations analys X es est pr X sent X e dans le tableau 2. 
Pour certaines occurrences, la configuration est tr X s claire : l X  X LT putative a pour fonction d'ex-poser des proc X dures et des outils de l'activit X  scientifique consid X r X e. 
Le m X me verbe peut avoir une valeur relevant de la langue g X n X rale comme constituant dans l X  X xemple suivant : 
Dans certains contextes, l X  X mbigu X t X  est impor-tante au point de rendre une d X cision spontan X e difficile. Pour ces cas, on s X  X nterroge au sujet de l X  X mploi terminologique ou langue g X n X rale de l X  X LT. 
Dans ce cas, produire a un sens ambigu entre l X  X cception de langue g X n X rale plus proche du verbe support faire et le sens terminologique de production langagi X re par opposition  X  compr X -hension . 
Un dernier type de difficult X s tient  X  notre m X -thode qui isole les phrases et n X  X ffre pas toujours un contexte suffisant pour comprendre le sens de l'occurrence. C X  X st le cas lorsqu X  X n est en pr X -sence d X  X naphores : 
Dans cet exemple, il est difficile de retrouver l X  X nt X c X dent de la premi X re . A c X t X  de l'analyse quantitative, nous avons souhait X  observer plus finement les ULT ou les classes d'ULT les plus susceptibles d'introduire des termes. Nous faisons, en effet, l'hypoth X se que certaines ULT qui ont une fonction m X ta-linguistique ( concept, terme ) ou introduisent des proc X dures scientifiques ( X tudier, analyser ) sont plus propices  X  l'introduction de termes. Pour cette t X che, nous avons effectu X  un typage s X mantique des ULT les plus productives en observant  X galement les ULT projet X es n'en-trant pas dans ces relations. Nous nous sommes pour cela bas X s sur un typage effectu X  par Tutin (2007)  X tendu pour cette exp X rience. 5.1 Les ULT nominales introductrices de Les noms transdisciplinaires ont  X t X  caract X ris X s dans des grandes classes s X mantiques, cons-truites largement  X  partir de crit X res distribu-tionnels et inspir X es de l'approche de Flaux et Van de Velde (2000). Certaines classes sont assez g X n X riques comme les noms de processus ( choix ), les noms humains (personne, individu ), les noms quantitatifs (nombre de, ensemble de ) alors que d'autres sont sp X cifiques de la langue scientifique comme les noms de processus scientifique (  X tude, description, recherche ), les noms d'observables scientifiques avec les objets  X tudi X s par l'activit X  scientifique ( donn X es, pa-ram X tres) , les noms d'artefact scientifique avec les objets construits par la r X flexion scientifique ( approche, m X thode, analyse ). Bien entendu, certains noms peuvent relever de plusieurs ca-t X gories. La cat X gorisation a  X t X  effectu X e en observant l'ensemble des contextes de fa X on syst X matique pour les noms les plus productifs de la liste des 390 noms transdisciplinaires. 
Dans la cinqui X me colonne du tableau 3, on peut constater un tr X s net accroissement de la propor-tion de cas satisfaisants ce qui confirme l X  X ypoth X se selon laquelle certaines classes de noms sp X cifiques de la langue scientifique sont clairement des introducteurs privil X gi X s de termes. C'est en particulier le cas des noms de processus scientifiques : artefacts scienti-fiques ( analyse, construction, description, re-cherche,  X tude ) et artefacts scientifiques purs ( structure, repr X sentation, sch X ma, mod X le ) . 
On observe aussi un ensemble de noms a priori moins sp X cifiques : Contrairement  X  nos attentes, les noms m X talin-guistiques ( terme, notion, concept, mot, nom ) ne sont pas les meilleurs introducteurs de termes bien que les noms terme et mot figurent parmi les ULT les plus fr X quentes du corpus Scientext dans son ensemble. Dans notre  X chan-tillon, seuls notion et concept sont utilis X s  X  cette fin, mot, nom et terme sont surtout  X  ce qui est  X videmment  X troitement li X   X  la disci-pline des textes  X  utilis X s comme des termes. Par ailleurs, on observe que quelques classes de noms transdisciplinaires sont peu employ X es comme introducteurs de termes : En r X sum X , si l'hypoth X se est fortement v X rifi X e pour certaines classes s X mantiques de noms (en particulier pour les processus scientifiques et artefacts scientifiques et pour les processus dans leur ensemble), ce n'est pas le cas d'autres cat X gories bien repr X sent X es dans notre genre, principalement pour deux raisons : La cat X gorisation s X mantique des noms du lexique transdisciplinaire appara X t donc indis-pensable pour effectuer un filtrage plus effi-cace. 5.2 Les ULT verbales introductrices de termes 
Nous avons  X galement observ X  le fonctionne-ment des verbes introducteurs de termes appa-raissant avec les relations SUBJ/DEEPSUBJ et 
OBJ/DEEPOBJ. La liste de verbes utilis X s, peu filtr X e, a  X t X  caract X ris X e  X  l'aide de classes de quasi-synonymes pour les  X l X ments les plus pro-ductifs ( cf. Tableau 4). 
Comme dans le cas des noms, certaines classes de verbes apparaissent plus clairement introduc-trices de termes. La cinqui X me colonne du ta-bleau 4 montre une nette augmentation de la proportion de cas pour lesquels l X  X ypoth X se est v X rifi X e, except X  pour la classe utiliser . Cepen-dant, la t X che de caract X risation est rendue plus difficile par une polys X mie plus grande que pour les noms. 
Les verbes s X mantiquement quasi-vides (  X tre, avoir ) sont largement repr X sent X s, ce qui appara X t facilement explicable par les formules d'identifi-cation, souvent utilis X es pour l'introduction de termes comme :
Viennent ensuite les modaux ( devoir, sembler ), largement repr X sent X s, eux aussi, dont nous pen-sons qu X  X ls peuvent  X tre consid X r X s comme des auxiliaires modaux dans la t X che d'analyse syn-taxique. A c X t X  de ces verbes  X  tout faire , fr X quents dans la langue g X n X rale, on rel X ve plusieurs classes synonymiques productives comme celle de l'identification ( identifier, distinguer, trouver ), description ( d X crire, caract X riser ).
 Comme pour les noms, certaines classes syno-nymiques paraissent peu repr X sent X es : celles du point de vue ou des liens logiques ( cause, cons X quence ). L'examen des classes s X mantiques introduc-trices de termes appara X t comme une piste pro-metteuse, certains champs paraissent clairement privil X gi X s comme introducteurs de termes (processus et artefacts scientifiques, processus de description et d' X tude, formules identifica-toires ). Cela m X rite un approfondissement sur un ensemble de donn X es plus vaste, travail que nous souhaitons entreprendre en extrayant se-mi-automatiquement la combinatoire syn-taxique et s X mantique du lexique transdiscipli-naire  X  partir de corpus arbor X s (sous-cat X gorisation syntaxique, co-occurrents lexico-syntaxiques) de fa X on  X  acc X l X rer et faciliter le processus de cat X gorisation. L X  X xp X rience d X crite dans cet article examine, sur un corpus de 42 articles scientifiques en sciences du langage, la mani X re et la proportion selon lesquelles les unit X s d X  X n lexique trans-disciplinaire scientifique peuvent jouer un r X le de d X limiteurs de termes. L X  X  X aluation quantita-tive de cette hypoth X se montre que celle-ci est v X rifi X e pour environ 74% des cas trait X s ( cf. Tableau 2). Nous analysons manuellement 2 638 couples (T i , ULT j ) parmi 3 406 couples qualifi X s par une relation syntaxique de d X pen-dance (NMOD, OBJ, SUBJ et leurs variantes. Sur le plan quantitatif, l X  X nalyse manuelle des 2 638 cas pour lesquels il n X  X  a aucune erreur d X  X nalyse syntaxique montre que l X  X ypoth X se est majoritairement valide mais insuffisamment r X guli X re pour utiliser en l X  X  X at les ULT pour filtrer automatiquement les candidats termes propos X s par les extracteurs automatiques de termes, ACABIT et TERMOSTAT. L X  X nalyse qualitative des r X sultats les plus fr X -quents fait  X merger une premi X re piste d X  X m X lioration en mettant en valeur plusieurs classes d X  X nit X s nominales et verbales pour les-quelles on constate un net accroissement de la proportion de cas satisfaisants ( cf . Tableau 3 et Tableau 4, section 5). 
Au-del X  de l X  X xp X rience d X crite, plusieurs pistes nous semblent int X ressantes pour am X liorer les r X sultats. 
Tout d X  X bord, l X  X xp X rience men X e dans le do-maine des sciences du langage engendre un biais disciplinaire important : par exemple l X  X mbigu X t X  constat X e pour le verbe produire mentionn X e ci-dessus. Ce biais pourra  X tre r X duit par une exten-sion de l X  X xp X rimentation  X  d X  X utres disciplines scientifiques et la prise en compte des sp X cifici-t X s de chacune d X  X lles avant de projeter le lexique transdisciplinaire. 
Ensuite, l X  X xtension de l X  X xp X rience ne sera que plus productive si on op X re un affinage du typage s X mantique du lexique et en particulier de la combinatoire des ULT.

De plus, il faut envisager de diversifier le rep X -rage des relations syntaxiques. L X  X d X e est de ne plus limiter l X  X xp X rience  X  des relations binaires mais de mettre en place une reconnaissance de patrons identificatoires (Jacques 2011) de la forme un T [est] ULT un T qui [...] , par exemple. 
Enfin, il sera n X cessaire d X  X nalyser les contextes o X  les termes apparaissent sans pour autant  X tre introduits par des ULT. 
