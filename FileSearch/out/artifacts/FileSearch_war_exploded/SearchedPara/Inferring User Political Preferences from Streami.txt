 Inferring latent user attributes such as gender, age, and political preferences (Rao et al., 2011; Za-mal et al., 2012; Cohen and Ruths, 2013) auto-matically from personal communications and so-cial media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data become extremely valuable for studying the un-derlying properties of such informal communica-tions because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013).
The existing batch models for predicting latent user attributes rely on thousands of tweets per author (Rao et al., 2010; Conover et al., 2011; Pennacchiotti and Popescu, 2011a; Burger et al., 2011; Zamal et al., 2012; Nguyen et al., 2013). However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g., the median number of tweets produced by a random Twitter user per day is 10. Moreover, recent changes to Twitter API querying rates further restrict the speed of access to this resource, effectively reducing the amount of data that can be collected in a given time period.
In this paper we analyze and go beyond static models formulating personal analytics in social media as a streaming task. We first evaluate batch models that are cognizant of low-resource predic-tion setting described above, maximizing the effi-ciency of content in calculating personal analytics. To the best of our knowledge, this is the first work that makes explicit the tradeoff between accuracy and cost (manifest as calls to the Twitter API), and optimizes to a different tradeoff than state-of-the-art approaches, seeking maximal performance when limited data is available. In addition, we propose streaming models for personal analytics that dynamically update user labels based on their stream of communications which has been ad-dressed previously by Van Durme (2012b). Such models better capture the real-time nature of evi-dence being used in latent author attribute predic-tions tasks. Our main contributions include: Twitter users interact with one another and en-gage in direct communication in different ways e.g., using retweets, user mentions e.g., @youtube or hashtags e.g., #tcot , in addition to having ex-plicit connections among themselves such as fol-lowing, friending. To investigate all types of social relationships between Twitter users and construct Twitter social graphs we collect lists of followers and friends, and extract user mentions, hashtags, 2.1 Social Graph Definition Lets define an attributed, undirected graph G = ( V,E ) , where V is a set of vertices and E is a set of edges. Each vertex v i represents someone in a communication graph i.e., communicant : here a Twitter user. Each vertex is attributed with a feature vector ~ f ( v i ) which encodes communica-tions e.g., tweets available for a given user. Each vertex is associated with a latent attribute a ( v i in our case it is binary a ( v i )  X  { D,R } , where D stands for Democratic and R for Republican users. Each edge e ij  X  E represents a connec-tion between v i and v j , e ij = ( v i ,v j ) and defines different social circles between Twitter users e.g., follower ( f ) , friend ( b ) , user mention ( m ) , hash-tag ( h ) , reply ( y ) and retweet ( w ) . Thus, E  X  V (2)  X { f,b,h,m,w,y } . We denote a set of edges of a given type as  X  r ( E ) for r  X  X  f,b,h,m,w,y } . We denote a set of vertices adjacent to v i by so-cial circle type r as N r ( v i ) which is equivalent to { v j | e ij  X   X  r ( E ) } . Following Filippova (2012) we refer to N r ( v i ) as v i  X  X  social circle, otherwise known as a neighborhood. In most cases, we only work with a sample of a social circle, denoted by r ( v i ) where | N 0 r ( v i ) | = k is its size for v i
Figure 1 presents an example of a social graph derived from Twitter. Notably, users from differ-ent social circles can be shared across the users of the same or different classes e.g., a user v j can be in both follower circle v j  X  N f ( v i ) ,v i  X  D and retweet circle v j  X  N w ( v k ) ,v k  X  R . 2.2 Candidate-Centric Graph We construct candidate-centric graph G cand by looking into following relationships between the users and Democratic or Republican candidates during the 2012 US Presidential election. In the Fall of 2012, leading up to the elections, we ran-domly sampled n = 516 Democratic and m = 515 Republican users. We labeled users as Demo-cratic if they exclusively follow both Democratic do not follow both Republican candidates  X  Mit-tRomney and RepPaulRyan and vice versa. We collectively refer to D and R as our  X  X sers of in-terest X  for which we aim to predict political prefer-ence. For each such user we collect recent tweets and randomly sample their immediate k = 10 neighbors from follower, friend, user mention, re-ply, retweet and hashtag social circles. 2.3 Geo-Centric Graph We construct a geo-centric graph G geo by col-lecting n = 135 Democratic and m = 135 Re-publican users from the Maryland, Virginia and Delaware region of the US with self-reported po-litical preference in their biographies. Similar to the candidate-centric graph, for each user we col-lect recent tweets and randomly sample user social circles in the Fall of 2012. We collect this data to get a sample of politically less active users com-pared to the users from candidate-centric graph. 2.4 ZLR Graph We also consider a G ZLR graph constructed from a dataset previously used for political affiliation classification (Zamal et al., 2012). This dataset consists of 200 Republican and 200 Democratic users associated with 925 tweets on average per 642 tweets per friend. Sharing restrictions and rate limits on Twitter data collection only allowed us to cratic and 178 Republican users with 1K tweets per user, and 20 neighbors of four types including follower, friends, user mention and retweet with 200 tweets per neighbor for each user of interest. Baseline User Model As input we are given a set of vertices representing users of interest v i  X  V along with feature vectors ~ f ( v i ) derived from con-tent authored by the user of interest. Each user is associated with a non-zero number of publicly posted tweets. Our goal is assign to a category each user of interest v i based on ~ f ( v i ) . Here we focus on a binary assignment into the categories Democratic D or Republican R . The log-linear where features are normalized word ngram counts extracted from v i  X  X  tweets ~ f t ( v i ) : D  X  t ( v i )  X  R .
The proposed baseline model follows the same trends as the existing state-of-the-art approaches for user attribute classification in social media as described in Section 8. Next we propose to ex-tend the baseline model by taking advantage of language in user social circles as describe below. Neighbor Model As input we are given user-local neighborhood N r ( v i ) , where r is a neighborhood type. Besides the neighborhood X  X  type r , each is characterized by:  X  the number of communications per neighbor  X  the order of the social circle  X  the num-
Our goal is to classify users of interest using evidence (e.g., communications) from their local neighborhood cratic or Republican. The corresponding log-linear model is defined as:  X 
To check whether our static models are cog-nizant of low-resource prediction settings we com-pare the performance of the user model from Eq.1 and the neighborhood model from Eq.2. Follow-ing the streaming nature of social media, we see the scarce available resource as the number of re-quests allowed per day to the Twitter API. Here we abstract this to a model assumption where we receive one tweet t k at a time and aim to maximize classification performance with as few tweets per  X  for the baseline user model:  X  for the neighborhood model: We rely on straightforward Bayesian rule update to our batch models in order to simulate a real-time streaming prediction scenario as a first step beyond the existing models as shown in Figure 2.
The model makes predictions of a latent user at-tribute e.g., Republican under a model assumption of sequentially arriving, independent and identi-cally distributed observations T = ( t 1 ,...,t k ) 9 . The model dynamically updates posterior proba-bility estimates p ( a ( v i ) = R | t k ) for a given user v as an additional evidence t k is acquired, as de-fined in a general form below for any latent at-tribute a ( v i )  X  A given the tweets T of user v i : where y is the number of all possible attribute val-ues, and k is the number of tweets per user.
For example, to predict user political prefer-ence, we start with a prior P ( R ) = 0 . 5 , and se-quentially update the posterior p ( R | T ) by accu-mulating evidence from the likelihood p ( t k | R ) : p ( R | T ) =
Q
Our goal is to maximize posterior probability estimates given a stream of communications for each user in the data over (a) time  X  and (b) the number of tweets T . For that, for each user we take tweets that arrive continuously over time and apply two different streaming models:  X  User Model with Dynamic Updates: re- X  User-Neighbor Model with Dynamic Up-We design a set of experiments to analyze static and dynamic models for political affiliation classi-fication defined in Sections 3 and 4. 5.1 Batch Classification Experiments We first answer whether communications from user-local neighborhoods can help predict politi-cal preference for the user. To explore the con-tribution of different neighborhood types we learn static user and neighbor models on G cand , G geo and G ZLR graphs. We also examine the ability of our static models to predict user political prefer-ences in low-resource setting e.g., 5 tweets.
The existing models follow a standard setup when either user or neighbor tweets are available during train and test. For a static neighbor model we go beyond that, and train our the model on all data available per user, but only apply part of the data at the test time, pushing the boundaries of how little is truly required for classification. For example, we only use follower tweets for G test , but we use tweets from all types of neighbors for world prediction scenarios which have not been previously explored, to our knowledge e.g., when a user has a private profile or has not tweeted yet, and only user neighbor tweets are available.
We experiment with our static neighbor model defined in Eq.2 with the aim to: 1. evaluate neighborhood size influence, we 2. estimate neighbor content influence, we alter-100 random restarts for every n and t parame-ter combination. We compare our static neigh-bor and user models using the cost functions from Eq.3 and Eq.4. For all experiments we use LibLinear (Fan et al., 2008), integrated in the Jerboa toolkit (Van Durme, 2012a). Both mod-els defined in Eq.1 and Eq.2 are learned using normalized count-based word ngram features ex-5.2 Streaming Classification Experiments We evaluate our models with dynamic Bayesian updates on a continuous stream of communica-tions over time as shown in Figure 2. Unlike static model experiments, we are not modeling the in-fluence of the number of neighbors or the amount of content per neighbor. Here, we order user and neighbor communication streams by real world time of posting and measure changes in posterior probabilities over time. The main purpose of these experiments is to quantitatively evaluate (1) the number of tweets and (2) the amount of real world time it takes to observe enough evidence on Twit-ter to make reliable predictions.

We experiment with log-linear models defined in Eq. 1 and 2 and continuously estimate the poste-rior probabilities P ( R | T ) as defined in Eq.6. We average the posterior probability results over the users in G cand , G geo and G ZLR graphs. We train streaming models on an attribute balanced subset of tweets for each user v i excluding v i  X  X  tweets (or v  X  X  neighbor tweets for a joint model). This setup is similar to leave-one-out classification. The clas-sifier is learned using binary word ngram features extracted from user or user-neighbor communi-cations. We prefer binary to normalized count-based features to overcome sparsity issues caused by making predictions on each tweet individually. 6.1 Modeling User Content Influence We investigate classification decision probabilities for our static user model  X  v tions on a random set of 5 vs. 100 tweets per user. To our knowledge only limited work on personal analytics (Burger et al., 2011; Van Durme, 2012b) have performed this straight-forward comparison. For that purpose, we take a random partition con-taining 100 users of G cand graph and perform four independent classification experiments  X  two runs using 5 and two runs using 100 tweets per user.
Figure 3 demonstrates that more tweets during prediction time lead to higher accuracy by show-ing that more users with 100 tweets are correctly classified e.g., filled green markers in the right up-per quadrant are true Republicans and in the left lower quadrant are true Democrats. Moreover, a lot of users with 100 tweets are close to 0.5 deci-sion probability which suggests that the classifier is just uncertain rather then being completely off, e.g., misclassified Republican users with 5 tweets (not filled blue markers in the right lower quad-rant) are close to 0. These results follow natu-rally from the underlying feature representation: having more tweets per user leads to a lower vari-ance estimate of a target multinomial distribution. The more robustly this distribution is estimated (based on having more tweets) the more confident we should be in the classifier output. 6.2 Modeling Neighbor Content Influence Here we discuss the results for our static neighbor-hood model. We study the influence of the neigh-borhood type r and size in terms of the number of neighbors n and tweets t per neighbor. In Figure 4 we present accuracy results for G cand and G geo graphs. Following Eq.3 and 4, we spent an equal amount of resources to obtain 100 user tweets and 10 tweets from 10 neighbors. We annotate these  X  X oints of equal number of commu-nications X  with a line on top marked with a corre-sponding number of user tweets.

We show that three of six social circles  X  friend, retweet and user-mention yield better accuracy compared to the user model for all graphs when t  X  250 . Thus, for effectively classifying a given user v i it is better to take 200 tweets each from 10 neighbors rather than 2,000 tweets from the user.
The best accuracy for G cand is 0.75 for friend, follower, retweet and user-mention neighborhoods which is 0.03 higher than the user baseline; for G geo is 0.67 for user-mention and 0.64 for retweet circles compared to 0.57 for the user model; for G
ZLR is 0.863 for retweet and 0.849 for friend circles which is 0.11 higher that the user baseline. Finally, similarly to the results for the user model given in Figure 3, increasing the number of tweets per neighbor from 5 to 200 leads to a significant gain in performance for all neighborhood types. 6.3 Modeling Neighborhood Size In Figure 5 we present accuracy results to show neighborhood size influence on classification per-formance for G geo and G cand graphs. Our re-sults demonstrate that even small changes to the neighborhood size n lead to better performance which does not support the claims by Zamal et al. (2012). We demonstrate that increasing the size of the neighborhood leads to better performance across six neighborhood types. Friend, user men-tion and retweet neighborhoods yield the highest accuracy for all graphs. We observe that when the number of neighbors is n = 1 , the difference in accuracy across all neighborhood types is less sig-nificant but for n  X  2 it becomes more significant. 7.1 Modeling Dynamic Posterior Updates Figures 6a and 6b demonstrate dynamic user model prediction results averaged over users from G cand and G ZLR graphs. Each figure outlines changes in sequential average probability esti-mates p  X  ( R | T ) for each individual self-authored tweet t k as defined in Eq. 6. The average proba-bility estimates p  X  ( R | T ) are reported for every 5 where n is the total number of users with the same attribute R or D . We represent p  X  ( R | T ) as a box and whisker plot with the median, lower and upper quantiles to show the variance; the length of whiskers indicate lower and upper extreme values. We find similar behavior across all three graphs. In particular, the posterior estimates converge faster when predicting Democratic than Republi-can users but it has been trained on an equal num-ber of tweets per class. We observe that average posterior estimates P  X  ( R | T ) converge faster to 0 (Democratic) than to 1 (Republican) in Figures 6a and 6b. It suggests that language of Democrats is more expressive of their political preference than language of Republicans. For example, frequent politically influenced terms used widely by Demo-cratic users include faith4liberty, constitutionally, pass, vote2012, terroristic.

The variance for average posterior estimates decreases when the number of tweets increases for all three datasets. Moreover, we detect that P  X  ( R | T ) estimates for users in G cand converge 2-3 times faster in terms of number of tweets than for users in G ZLR . The lowest convergence is de-tected for G geo where after t k = 250 tweets the average posterior estimate P  X  ( R | t k ) = 0 . 904  X  0 . 044 and P  X  ( D | t k ) = 0 . 861  X  0 . 008 . It means that users in G cand are more politically vocal com-pared to users in G ZLR and G geo . As a result, less active users in G geo just need more than 250 tweets to converge to a true 0 or 1 class. These re-sults are coherent with the outcomes for our static models shown in Figures 4 and 5. These findings further confirm that differences in performance are caused by various biases present in the data due to distinct sampling and annotation approaches.
Figure 7a and 7b illustrate the amount of time required for the user model to infer political pref-erences estimated for 1,031 users in G cand and 371 users in G ZLR . The amount of time needed can be evaluated for different accuracy levels e.g., 0.75 and 0.95. Thus, with 75% accuracy we classify:  X  100 (  X  20%) Republican users in 3.6 hours  X  100 (  X  56%) R users in 20 weeks and 100  X  100 (  X  75%) R users in 12 weeks and 80
Such extreme divergences in the amount of time required for classification across all graphs should be of strong interest to researchers concerned with latent attribute prediction tasks because Twitter users produce messages with extremely different frequencies. In our case, users in G ZLR tweet ap-proximately 800 times less frequently than users 7.2 Modeling Dynamic Posterior Updates We estimate dynamic posterior updates from a joint stream of user and neighbor communications in G geo , G cand and G ZLR graphs. To make a fair comparison with a streaming user model, we start with the same user tweet t 0 ( v i ) . Then instead of waiting for the next user tweet we rely on any neighbor tweets that appear until the user produces the next tweet t 1 ( v i ) . We rely on communications from four types of neighbors such as friends, fol-lowers, retweets and user mentions.

The convergence rate for the average posterior probability estimates P  X  ( R | T ) depending on the number of tweets is similar to the user model re-sults presented in Figure 6. However, for G geo the variance for P  X  ( R | T ) is higher for Democratic users; for G ZLR P  X  ( R | T )  X  1 for Republicans in less than 110 tweets which is  X  t = 40 tweets faster than the user model; for G cand the conver-gence for both P  X  ( R | T )  X  1 and P  X  ( D | T )  X  0 is not significantly different than the user model.
Figures 7c and 7d show the amount of time re-quired for a joint user-neighbor model to infer po-litical preferences estimated for users in G cand and G
ZLR . We find that with 75% accuracy we can classify 100 users for:  X  G cand : Republican users in 23 minutes and  X  G ZLR : R users in 3.2 weeks and D users in  X  G geo : R users in 1.2 weeks and D users in
Similar or better P  X  ( R | T ) convergence in terms of the number of tweets and, especially, in the amount of time needed for user and user-neighbor models further confirms that neighborhood con-tent is useful for political preference prediction. Moreover, communications from a joint stream al-low to make an inference up to 7 times faster. Supervised Batch Approaches The vast major-ity of work on predicting latent user attributes in social media apply supervised static SVM mod-els for discrete categorical e.g., gender and re-gression models for continuous attributes e.g., age with lexical bag-of-word features for classifying user gender (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b), age (Rao et al., 2010; Nguyen et al., 2011; Nguyen et al., 2013) or political orientation. We present an overview of the existing models for political pref-erence prediction in Table 1.

Bergsma et al. (2012) following up on Rao X  X  work (2010) on adding socio-linguistic features to improve gender, ethnicity and political prefer-ence prediction show that incorporating stylistic and syntactic information to the bag-of-word fea-tures improves gender classification.

Other methods characterize Twitter users by ap-plying limited amounts of network structure in-formation in addition to lexical features. Con-nover et al. (2011) rely on identifying strong parti-san clusters of Democratic and Republican users in a Twitter network based on retweet and user mention degree of connectivity, and then combine this clustering information with the follower and friend neighborhood size features. Pennacchiotti et al. (2011a; 2011b) focus on user behavior, net-work structure and linguistic features. Similar to our work, they assume that users from a partic-ular class tend to reply and retweet messages of the users from the same class. We extend this as-sumption and study other relationship types e.g., friends, user mentions etc. Recent work by Wong et al. (2013) investigates tweeting and retweet-ing behavior for political learning during 2012 US Presidential election. The most similar work to ours is by Zamal et al. (2012), where the authors apply features from the tweets authored by a user X  X  friend to infer attributes of that user. In this paper, we study different types of user social circles in addition to a friend network.

Additionally, using social media for mining po-litical opinions (O X  X onnor et al., 2010a; May-nard and Funk, 2012) or understanding socio-political trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For in-stance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol this content (Yano and Smith, 2013).

Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location classification on Twitter. O X  X onnor et al. (2010b) following the work by Eisenstein (2010) propose a Bayesian generative model to discover demo-graphic language variations in Twitter. Rao et al. (2011) suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity. Golbeck et al. (2010) incorporate Twitter data in a spatial model of political ideology.

Streaming Approaches Van Durme (2012b) proposed streaming models to predict user gen-der in Twitter. Other works suggested to process text streams for a variety of NLP tasks e.g., real-time opinion mining and sentiment analysis in so-cial media (Pang and Lee, 2008), named entity disambiguation (Sarmento et al., 2009), statistical machine translation (Levenberg et al., 2011), first story detection (Petrovi  X  c et al., 2010), and unsu-pervised dependency parsing (Goyal and Daum  X  e, 2011). Massive Online Analysis (MOA) toolkit developed by Bifet et al. (2010) is an alternative to the Jerboa package used in this work developed by Van Durme (2012a). MOA has been effec-tively used to detect sentiment changes in Twitter streams (Bifet et al., 2011). In this paper, we extensively examined state-of-the-art static approaches and proposed novel mod-els with dynamic Bayesian updates for streaming personal analytics on Twitter. Because our stream-ing models rely on communications from Twitter users and content from various notions of user-local neighborhood they can be effectively applied to real-time dynamic data streams. Our results support several key findings listed below.

Neighborhood content is useful for personal analytics. Content extracted from various notions of a user-local neighborhood can be as effective or more effective for political preference classifi-cation than user self-authored content. This may be an effect of  X  X parseness X  of relevant user data, in that users talk about politics very sporadically compared to a random sample of their neighbors.
Substantial signal for political preference prediction is distributed in the neighborhood. Querying for more neighbors per user is more ben-eficial than querying for extra content from the existing neighbors e.g., 5 tweets from 10 neigh-bors leads to higher accuracy than 25 tweets from 2 neighbors or 50 tweets from 1 neighbor. This may be also the effect of data heterogeneity in social media compared to e.g., political debate text (Thomas et al., 2006). These findings demon-strate that a substantial signal is distributed over the neighborhood content.

Neighborhoods constructed from friend, user mention and retweet relationships are most effective. Friend, user mention and retweet neighborhoods show the best accuracy for predict-ing political preferences of Twitter users. We think that friend relationships are more effective than e.g., follower relationships because it is very likely that users share common interests and preferences with their friends, e.g. Facebook friends can even mentions and retweets are two primary ways of in-teraction on Twitter. They both allow to share in-formation e.g., political news, events with others and to be involved in direct communication e.g., live political discussions, political groups.
Streaming models are more effective than batch models for personal analytics. The predic-tions made using dynamic models with Bayesian updates over user and joint user-neighbor commu-nication streams demonstrate higher performance with lower resources spent compared to the batch models. Depending on user political involvement, expressiveness and activeness, the perfect predic-tion (approaching 100% accuracy) can be made using only 100 -500 tweets per user.

Generalization of the classifiers for political preference prediction. This work raises a very important but under-explored problem of the gen-eralization of classifiers for personal analytics in social media, also recently discussed by Cohen and Ruth (2013). For instance, the existing models developed for political preference prediction are all trained on Twitter data but report significantly different results even for the same baseline mod-els trained using bag-of-word lexical features as shown in Table 1. In this work we experiment with three different datasets. Our results for both static and dynamic models show that the accuracy in-deed depends on the way the data was constructed. Therefore, publicly available datasets need to be released for a meaningful comparison of the ap-proaches for personal analytics in social media.
In future work, we plan to incorporate itera-tive model updates from newly classified com-munications similar to online perceptron-style up-dates. In addition, we aim to experiment with neighborhood-specific classifiers applied towards the tweets from neighborhood-specific streams e.g., friend classifier used for friend tweets, retweet classifier applied to retweet tweets etc. The authors would like to thank the anonymous reviewers for their helpful comments.
