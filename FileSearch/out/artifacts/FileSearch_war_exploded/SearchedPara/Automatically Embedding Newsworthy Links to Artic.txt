 It is of great interest to news providers such as Yahoo! News to attain higher visitor rates by promoting greater engage-ment with their content. One aspect of engagement deals with keeping users on the site longer by allowing them to navigate through content with enhanced, click-through ex-periences. News portals have invested in ways to provide embedded links within news stories. So far these links have been manually curated by professional editors, and due to the manual effort involved, the use of such links has been limited. In this paper we propose an automated approach to detecting and linking newsworthy events to associated articles. Our analysis, conducted on Amazon X  X  Mechanical Turk, reveals that our system X  X  performance is comparable to that of professional editors, and that users find the auto-matically generated highlights interesting and the associated articles worthy of reading.

News portals have become a very popular destination for web users who read news online. As there is great potential for online news consumption but also serious competition among news portals, providers must find effective and effi-cient strategies to engage users longer in their sites. In this paper, we are interested in one type of strategy promoting engagement; enticing users to browse the site through em-bedded links within news articles . The work was done while at Yahoo! Labs.

Embedded links, or hyperlinks , are a great strategy for prolonging time users spend on a site by sustaining engage-ment through interactive click-through experiences. Once clicked, users are re-directed to another page showing the referenced content. We studied this phenomenon in the con-text of news portals, where the embedded links direct users to other pages within the same domain. However, hyperlinks are mostly created by human editors, making it a manual task that is time-consuming and not scalable. We propose an automatic approach to hyperlinking, where for any news article the goal is to identify newsworthy events as a poten-tial source for links. Newsworthy events are more likely to have a related news article already written in the past.
Work in automatically generating hyperlinks has gained new interest, primarily within the context of Wikipedia, looking at cross-referencing documents [8, 10], automatic ap-proaches for hyperlinking [4, 5], and automatically linking documents to encyclopaedic knowledge [6, 9]. Link gener-ation approaches are also used in disambiguation tasks [2, 3, 7]. Many of these works are not directly comparable to ours because of different aims and a reliance on properties specific to Wikipedia. Our focus is to identify the news-worthy events in a given news article and connect them to the appropriate news articles, where it is not likely that all newsworthy events are, or will be, Wikipedia concepts.
In [6] the aim was to generate links from medical reports to Wikipedia pages for explanations or background infor-mation. They showed that the approaches in [9, 10] did not yield satisfactory results because medical phrases typically have a more complex syntactic and semantic structure than Wikipedia concepts. They therefore developed their own ap-proaches. Events, as phrased in a news article, have also a specific structure and in addition, very few form concepts in Wikipedia. Similarly, we had to design our own automatic link generation approach.

The goal of our research is two-fold: (i) a fully automated system that constructs hyperlinks in news articles using text processing and understanding techniques; and (ii) assessing the system-embedded links against manually-curated ones by professional editors. This paper focuses on assessing the quality of the system-embedded links and conducted an eval-uation using the crowd-sourcing power of Amazon X  X  Mechan-ical Turk (MTurk). Our system, called Linker for Events to Past Articles ,or LEPA , has two main components: an indexer and a linker.
The indexer processes articles over a time period by ex-tracting features from each article, and storing them to fa-cilitate faster retrieval. The indexer runs in two stages: the build stage produces an index for a set of articles over a time period, while the update stage is run periodically to add fresh articles to the existing index. For the build stage we constructed an index from articles that spanned over a month, while for the update stage we processed new docu-ments daily and added them to the index.

In both stages, we implemented a simple inverted index approach. The inverted index stores a list of the documents for each word in the vocabulary derived from the corpus, which is formed from the entire set of news articles being indexed. The frequency of each word in the document is stored in the inverted index. These frequencies are calcu-lated during the feature extraction step of the indexer.
Our retrieval goal is to find the article in the index that exactly discusses the corresponding event. Finding a precise matching between the article and the event can be more easily accomplished if both contexts have comparable sizes. Since an event consists of only a few words, only the title and the abstract sections of the news articles are considered and indexed. 1
The goal of the linker is to find newsworthy events in each article and link them to the previously indexed articles. It first identifies sentences that mention newsworthy events, then, for each event matches and retrieves newsworthy ar-ticles. Finally, the top ranked article is hyperlinked to the event if it satisfies a certain confidence level criterion.
Selecting the Candidate Sentences We identified three important criteria in selecting the candidate sentences: (i) the sentence must contain a named entity; (ii) the sentence must contain a verb in past tense; and (iii) the verb men-tioned in the second criteria must be an action verb .Almost all important observed events are regarding one or more important entities that occur as the subject or the object of a sentence. For example, the sentence  X  X  few days ago Google announced their acquisition of Zagat , the popular publisher of restaurant review guides X  contain the named entities Google and Zagat as part of the events that refer to news in the past. This criterion could be restrictive by ignor-ing otherwise good candidates such as  X  X he company issued a press letter yesterday regarding the new privacy policies X , where e.g.  X  The company  X  is a co-reference, as it refers to a named entity such as  X  X oogle X ). Here, we do not employ a co-reference resolution approach due to its complexity and the additional noise it might introduce to the system.
The second criterion is trivial. Since our goal is to hyper-link the events in the current article to previously published content, we ensure that the candidate sentence contains a verb in past tense. Our last criterion stems from the need to eliminate verbs that usually do not specify any event, thus cannot be linked to any previously published content. Examples include be , become , seem , grow , etc. We are look-ing for verbs that describe an action, which are referred to as action verbs . Therefore we ensure that all identified past
The abstract section corresponds to the first paragraph of a news article. tense verbs are action verbs, otherwise the sentence is elim-inated. To filter the sentences based on these criteria we used the Natural Language Processing Toolkit (NLTK) [1], a freely available application for research purposes. 2
Constructing the Query Once the candidate sentences are identified we extract the events from each candidate sen-tence. An event is contained in a sentence and is determined by a predicate of that sentence. We use the term predicate to describe a function over arguments. The function is formed by the verb and its arguments are the noun phrases that are immediately before and after the verb. Thus, it is pos-sible for one sentence to contain more than one event. For instance, the predicate formed from the sentence  X  X arack Obama announced his candidacy for presidency on Feb. 10. X  would be X  announced(Barack Obama, his candidacy for pres-idency)  X  X stheverb announce forms the function, and the immediate noun phrases  X  Barack Obama  X , and  X  his candi-dacy for presidency  X  form the arguments of the predicate. Hence, the event extracted in this example would be X  Barack Obama announced his candidacy for presidency  X .

The general pattern being identified are subject-verb-object relationships. This has certain disadvantages, as it assumes that the verbs are normal transitive verbs that take a single direct object. This is not always the case. A verb can be intransitive, i.e. it takes no objects, or it could be transitive but take both a direct and an indirect object, as in the case of complex transitive verbs and ditransitive verbs (datives) . Nevertheless, this leads to an approach that is easily scalable to other languages.

We use NLTK X  X  built-in noun phrase chunker to automati-cally identify the noun phrases in the sentence, and the verb is identified through the part of speech tags as mentioned previously. Once the event is identified we again use NLTK to remove the stop words and stem each word in the event, including the verb. The resulting phrase forms our query.
Ranking the Results Once the query is formed the matching articles in the index are retrieved and ranked. The inverted index keeps track of each word and the document it appears in together with its frequency. We form vectors of term frequencies, q and d i , for each query q and document d in the corpus, respectively. The dot product of the query vector with a document vector gives us the importance score of that document for the query. We normalize the dot prod-uct with the length of the document. Thus, the score of document d i for the query q is q  X  d i / | d i | . The documents are ranked according to this score.

When constructing the vectors for the documents, we give more weight to the frequency of a word appearing in the ti-tle of the document. The reason is the more query terms we find in the title, the more confident we are of that docu-ment describing the event. For example, consider the event  X  X  magnitude-8.8 earthquake hit Chile X , and two matching documents with titles  X 8.8-Magnitude Quake Hits Chile X , and  X  X illions are Displaced After the Chile Quake X . Even though both documents are related to the event, the former matching document is devoted to the event, whereas the latter has only tangential relevance. We set the weight of matching terms in the title to 3, by tuning this parameter on a separate validation set. Finally, if the score of the top re-www.nltk.org sult retrieved from the index is above a predefined threshold, the event is linked to the article in the index. This threshold parameter can be set depending on the application. For ex-ample, in a setting where precision is more important than recall one can set the threshold to a high value to make the system very precise while trading-off recall and vice-versa.
In this section we compare the quality of system-embedded against manually-curated links by professional editors.
We used 200 news articles taken from the top-50 most viewed articles from Yahoo! news, on four different dates. We kept our selection random, covering a diversity of topics and a range of document lengths (150-2000 words). The news articles were separately annotated by LEPA and a team of professional editors from Yahoo! news. We repeated this process for our system using four precision settings.
The professional editors, who had no prior knowledge that their work would be evaluated against the proposed system, were asked to read the articles and identify events and enti-ties that were good candidate links. The guidelines indicated this as a routine editing task and instructed them to link ar-ticles that were perceived as related and newsworthy and that would provide interesting insights with respect to the main article. The only limitation was that the linked articles had to reside within Yahoo! news. The editors were allowed to embed as many links as they thought appropriate.
Out of the 200 articles we retained 75 after filtering out the articles for which the system did not detect any events. Our system identified a total of 192 links, while the editors identified 211 links. From the latter we excluded 28 links that were embedded in common by LEPA and the editors, as our focus was to compare the quality of system-embedded against the manually-curated links. As common links we treated those cases of anchored text that appeared in the same article, same paragraph/sentence, and shared at least one common word. This resulted in 164 system-embedded and 183 manually-curated links. From the latter we retained a random selection of 164, to have an equal contribution of both types of link.

We examined the performance of our automated approach using standard metrics, quantitative judgments from hu-man evaluators, and compared the system-embedded links against manually-curated ones. We used the Amazon Me-chanical Turk crowd-sourcing service.
This study used a between-groups design ( X  X roup A X ,  X  X roup B X ) with three independent variables: type of link (two levels:  X  X ystem-embedded X , X  X anually-curated X ), preci-sion configuration (four levels:  X 0.0 X ,  X 0.1 X ,  X 0.2 X ,  X 0.3 X ) and date of publication (four levels:  X 19/10/2011 X , X 20/10/2011 X ,  X 16/11/2011 X ,  X 17/11/2011 X ). The type of link was con-trolled by introducing either system-embedded ( Group A ) or manually-curated ( Group B ) links. The precision config-uration was controlled by adjusting accordingly a threshold value in LEPA. The results were filtered based on this confi-dence level criterion and those that did not make the cut-off were dropped. This allowed some control over the system X  X  levels of precision &amp; recall, producing results that varied between high precision-low recall and low precision-high re-call. The date of publication was controlled by building our experimental dataset with news articles crawled on four dif-ferent dates, thus reducing the dependency of our findings on the temporal factor.

Each participant took part in one condition (one article-link combination) and assessed four different aspects of the system: (i) the main article, (ii) the associated article, (iii) the link, and (iv) the system X  X  performance. With respect to the first category, the dependent variables were: (i) interest, (ii) newsworthiness, and (iii) similarity to other news read online. In terms of the second category, the dependent vari-ables were: (i) type of relation with the main article (five levels:  X  X elated to the main topic of the article X ,  X  X elated to a subtopic of the article X ,  X  X angentially related X ,  X  X nrelated X ,  X  X ther X ), (ii) newsworthiness, and (iii) interesting insights with respect to the main article. Regarding the third cat-egory, the dependent variables were: (i) suitability of the anchored text 3 , and (ii) relatedness with the associated ar-ticle. Finally, the system performance was measured using the standard metrics of precision, recall, and f-measure.
We prepared 328 tasks, each a unique combination of article-link, using the 75 news articles and corresponding 164+164 links. Each participant was assigned a single, ran-domly selected article. While reading the article the partic-ipants were instructed to click on the link that appeared in the text and go through the associated article that it pointed to. To mitigate any unwanted effects stemming from the visual saliency of non-relevant elements of the original con-tent, the articles were presented in the simple html format. To reduce the subjectivity of individual responses, each task was performed by two different participants. Upon complet-ing the task, the participants were redirected to an online questionnaire.
A post-task questionnaire was used to elicit information on several aspects of the task such as the main and associ-ated articles, the quality of the embedded links, the partici-pants X  reading experience. A demographics section gathered background information and inquired about previous expe-rience with online news reading. All questions were forced-choice type using a 5-point Likert scale. Questions asking for user rating on a unipolar dimension have the positive concept corresponding to the value of five and the negative concept corresponding to the value of one.
We designed the task and the questionnaire in such way that completing them accurately and in good faith required approximately the same amount of effort than random or malicious completion. To ensure that the tasks were per-formed by human participants we employed keyword tagging with respect to the theme of the main and associated arti-cles. In addition, we recorded the times required to complete each step of the task. This allowed us to distinguish auto-mated responders from human participants. A final check was to accept as participants only workers who had gained a high reputation from other requestors, by having at least 90% of their responses to previous tasks accepted, as well as
As anchored text we refer to the underlined text (sentence, phrase, etc.) that appeared in each hyperlink. Table 1: Observed frequencies across the five cate-gories of associated articles.
 a number of completed HIT X  X  (Human Intelligence Tasks) greater than, or equal to, 50. The participants were asked to complete the task, including the questionnaire, in a single sitting. They were also informed of their option to opt out from the task at any point without being compensated. The average duration of the reading task was approximated to 10 minutes and the payment for participation was $0.66.
Six hundred and sixty-four participants were recruited through MTurk, from which we reached the expected num-ber of approved assignments from 656 different participants, who spent an average of 17.68 minutes on each task and pro-vided a total of 195 hours of labor. The participants were randomly distributed into two even groups,  X  X roup A X  (fe-male 45.12%, male 54.87%) and  X  X roup B X  (female 38.69%, male 61.3%), and were of mixed ethnicity and educational background. The Mann-Whitney test did not indicate any statistically significant difference between the two groups in terms of age, gender, educational level, proficiency with en-glish, or frequency of reading news.
We evaluate the performance of LEPA against professional editors on the selected 75 news articles, with a total of 328 links. The performance was measured using precision, re-call and f-measure, and as ground-truth we used the partic-ipants X  assessments. Precision was computed as the fraction of links in each article that received, in terms of related-ness, a score equal to, or greater than, 3 on a 5-point Likert scale. The Mann-Whitney test, the Chi-Squared  X  X oodness of Fit X , and the Chi-Squared Test of Association were used to establish the statistical significance ( p &lt; .05) of the differ-ences observed in the experimental results as well as isolate the significant pair(s) through pair-wise comparisons. The Mann-Whitney test is a non-parametric test used for testing differences between groups, when there are two conditions and different participants have been used in each condition. The Chi-Squared  X  X oodness of Fit X  and the Chi-Squared Test of Association are tests for examining the association of different variables when dealing with data frequencies (nom-inal data). To take an appropriate control of Type I errors we applied a Bonferroni correction, and so all effects are reported at a .005 level of significance.

To evaluate the main article we asked our participants to provide scores for the following questions: (i)  X  X id you find the article informative? X , (ii)  X  X ow interesting did you find the article you read? X , and (iii)  X  X as the article you read similar to other news you usually read? X . In this evaluation the results are presented across all groups, since we were in-terested in examining the overall effect of our experimental manipulation on our sample, instead of narrowing it down to specific subgroups. For the first question, the participants reported the main article as somewhat-to-very informative ( M =3.5914, SD =0.6164). Regarding the second question the participants felt that the main article was somewhat-to-very interesting ( M =3.3978, SD =0.6825), while in the third question they rated it as somewhat similar to other news that they usually read ( M =2.8841, SD =0.7832). The results suggest that the news articles we employed was a fair approximation of what users read online. Moreover, the scores assigned to interest and informativeness indicate that these variables did not suffer from any adverse effects intro-duced by the manipulation of the independent variables.
Table 1 presents the frequency scores for all five types of links, in relation to the question:  X  X lease indicate if the as-sociated article is related to the overall theme of the main article, related to a subtopic within the main article, tangen-tially related or unrelated X . Since each link was evaluated by two different participants, we present the scores per group ( X  X ystem Links X ,  X  X ditor Links X ) and per participant ( X  X ar-ticipant A X , X  X articipant B X ). Columns three and six present the sum of counts for both participants. The Chi-Squared  X  X oodness of Fit X  test was applied and revealed a statisti-cally significant variation in the observed distribution across all types: (1)  X  2 (4, N =164) = 99.354, p &lt; .0001, (2)  X  N =164) = 69.293, p &lt; .0001, (3)  X  2 (4, N =164) = 165.634, p &lt; .0001, (4)  X  2 (4, N =164) = 158.134, p &lt; .0001. We, there-fore, reject the null hypothesis that the counts are uniformly distributed across the categories.

We also applied the Chi-Squared Test of Association to examine if there is an association between the participant X  X  group and the type of link. Participants from Group B were significantly more likely to find the associated articles related to the main theme (52.7%) or a subtopic (32.6%) of the main article, compared to participants from Group A (main theme: 45,4%, subtopic: 22.3%). However, the participants from Group A were more likely to perceive the articles as tangentially related (14.0%), compared to participants from Group B (10.4%). The system X  X  performance was found to be comparable to that of the editors X , with only 15.85% of the embedded links having been reported as unrelated. This is a very encouraging finding, suggesting that our system is scalable and efficient in curating the embedded links.
Table 2 shows the means and standard deviations for par-ticipants X  assessments of the system-embedded and manually-curated links. LEPA X  X  performance is presented across all four precision settings in rows 1 to 4. Four aspects of the links are examined here, namely: (i) if the anchored text was a good location for the link in the article, (ii) if the anchored text was related to the associated article (the one that the link points to), (iii) if the associated article was newswor-thy, and (iv) if the associated article provided interesting insight with respect to the main article. As indicated in Table 2 both types of links received average-to-good scores, with variations being more evident in terms of location and relatedness. The editors performed better, especially com-pared to the versions of the system with lower precision. The Mann-Whitney independent groups test also supports this finding for the differences observed between System@0.0 and the editors. A direct comparison between the remaining sys-tems (System@0.1, System@0.2, System@0.3) and the edi-tors was not possible, since the former incorporated only a Table 3: Performance of system across all precision configurations using standard metrics.
 subset of the links embedded by System@0.0; thus contain-ing an uneven number of links, compared to the number of links that the editor curated.

The Mann-Whitney test also revealed that the manually-curated links received statistically significant higher scores than the system links, in terms of location ( U =8213.5, p =.000, r =-0.35), relatedness ( U =7449, p =.000, r =-0.39), newswor-thiness ( U =11498.5, p =0.02, r =-0.12), and interesting in-sights ( U =9998, p =.000, r =-0.22). However, in all cases the observed differences represent a small effect that accounts for less than 10% of the total variance in our sample. Fur-thermore, as we increased the precision threshold LEPA X  X  performance drew nearer to that of the editors.

Looking at the performance of the system in terms of pre-cision, recall, and f-measure in Table 3, we notice that the mean-average precision escalates as the threshold value is increased, although this increase has an adverse effect on recall. Apparently there is a trade-off between introducing fewer but more related links and receiving a larger number of links of heterogenous relevance. In terms of performance column three shows the f-measure scores, which indicate Sys-tem@0.0 as the optimum approach.
We presented the evaluation of LEPA, a fully automated approach to detecting events in news articles and linking them to relevant past articles. One of the main advantages of LEPA over other systems is that it works on sentences as well as events within a sentence in isolation from the main content, whereas other approaches often consider the entire content when generating its related content pool. More-over, unlike other systems that focus on link detection and disambiguation on Wikipedia articles or concepts, LEPA is less domain-dependent.

We conducted an experimentation via MTurk to evaluate our system-embedded links against links manually-generated by professional editors. When we treat these manual links as a gold standard the results indicate that LEPA is com-parable to that across several facets of the news reading experience: relatedness of the anchored text with the asso-ciated article, newsworthiness, offering interesting insights, etc. Our evaluation reveals that the editors had an average-to-good performance, whereas our system had an, overall, average performance. When examining the system perfor-mance using standard metrics we observe that the f-measure scores decline over high precision values. Apparently, assign-ing a high value to the precision threshold acts as a trade off for recall and vice-versa. However, the analysis of the open-ended responses indicates that, for our setting (online news reading), a high precision configuration facilitates a better news reading experiences, contrary to the lower thresholds that result in a larger number of links and provide access to a plethora of information. In other words, less is more .
The manual creation of hyperlinks is a time-demanding and challenging task, especially for large online providers where the editors are expected to process daily a significant number of news articles. Consequently, any effort towards automating the link curation process could go a long way to improve their efficiency and performance. In a real use-case scenario the final decision might ultimately for the editors to make but our experimental findings indicate that the pro-posed system, with its massive scalability being its greatest asset, can support the process of identifying interesting links and fulfil its purpose in reducing manual effort. Contrary to other automated systems, LEPA does not require any train-ing data or human intervention, nor is it limited to a specific domain, making it a generalizable and attractive solution for many real-world applications.
