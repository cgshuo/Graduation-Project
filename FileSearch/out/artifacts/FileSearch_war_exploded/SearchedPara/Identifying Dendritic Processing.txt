 The nature of encoding and processing of sensory information in the visual, auditory and olfactory systems has been extensively investigated in the systems neuroscience literature. Many phenomeno-logical [1, 2, 3] as well as mechanistic [4, 5, 6] models have been proposed to characterize and clarify the representation of sensory information on the level of single neurons.
 Here we investigate a class of phenomenological neural circuit models in which the time-domain linear processing takes place in the dendritic tree and the resulting aggregate dendritic current is en-coded in the spike domain by a spiking neuron. In block diagram form, these neural circuit models are of the [Filter]-[Spiking Neuron] type and as such represent a fundamental departure from the standard Linear-Nonlinear-Poisson (LNP) model that has been used to characterize neurons in many sensory systems, including vision [3, 7, 8], audition [2, 9] and olfaction [1, 10]. While the LNP model also includes a linear processing stage, it describes spike generation using an inhomogeneous Poisson process. In contrast, the [Filter]-[Spiking Neuron] model incorporates the temporal dynam-ics of spike generation and allows one to consider more biologically-plausible spike generators. We perform identification of dendritic processing in the [Filter]-[Spiking Neuron] model assuming that input signals belong to the space of bandlimited functions, a class of functions that closely model natural stimuli in sensory systems. Under this assumption, we show that the identification of dendritic processing in the above neural circuit becomes mathematically tractable. Using simulated data, we demonstrate that under certain conditions it is possible to identify the impulse response of the dendritic processing filter with arbitrary precision. Furthermore, we show that the identification results fundamentally depend on the bandwidth of test stimuli.
 The paper is organized as follows. The phenomenological neural circuit model and the identification problem are formally stated in section 2. The Neural Identification Machine and its realization as an algorithm for identifying dendritic processing is extensively discussed in section 3. Performance of the identification algorithm is exemplified in section 4. Finally, section 5 concludes our work. In what follows we assume that the dendritic processing is linear [11] and any nonlinear effects arise as a result of the spike generation mechanism [12]. We use linear BIBO-stable filters (not necessarily causal) to describe the computation performed by the dendritic tree. Furthermore, a spiking neuron model (as opposed to a rate model) is used to model the generation of action potentials or spikes. We investigate a general neural circuit comprised of a filter in cascade with a spiking neuron model neuron models considered in this paper include the ideal IAF neuron, the leaky IAF neuron and the threshold-and-feedback (TAF) neuron [15]. However, the methodology developed below can be extended to many other spiking neuron models as well.
 erations in the dendritic tree and (ii) identification of spike generator parameters. First, we consider problem (i) and assume that parameters of the spike generator can be obtained through biophysical experiments. Then we show how to address (ii) by exploring the space of input signals. We consider a specific example of a neural circuit in Fig. 1(a) and carry out a full identification of that circuit. A Neuron Identification Machine (NIM) is the realization of an algorithm for the identification of the dendritic processing filter in cascade with a spiking neuron model. First, we introduce several definitions needed to formally address the problem of identifying dendritic processing. We then con-impulse response of the filter and provide conditions for the identification with arbitrary precision. Finally, we extend our results to the [Filter]-[Leaky IAF] and [Filter]-[TAF] neural circuits. 3.1 Preliminaries We model signals u = u ( t ) , t  X  R , at the input to a neural circuit as elements of the Paley-Wiener space  X  = u  X  L 2 ( R ) supp ( F u )  X  [  X   X  ,  X ] , i.e., as functions of finite energy having a finite they belong to the space H = h  X  L 1 ( R ) supp( h )  X  [ T 1 ,T 2 ] .
 Definition 1. A signal u  X   X  at the input to a neural circuit together with the resulting output T Definition 2. Two neural circuits are said to be  X  -I/O-equivalent if their respective I/O pairs are identical for all u  X   X  .
 h with the sinc kernel g , sin( X  t ) / (  X t ) , t  X  R . We say that P h is the projection of h onto  X  . {  X  i } N i =1 , not all zero, and real numbers {  X  i } N i =1 such that 3.2 NIM for the [Filter]-[Ideal IAF] Neural Circuit An example of a model circuit in Fig. 1(a) is the [Filter]-[Ideal IAF] circuit shown in Fig. 1(b). In this circuit, an input signal u  X   X  is passed through a filter with an impulse response (kernel) h  X  H and then encoded by an ideal IAF neuron with a bias b  X  R + , a capacitance C  X  R + and a an observer. This neural circuit is an instance of a TEM and its operation can be described by a set of equations (formally known as the t-transform [13]): a measurement q k of the signal v ( t ) = ( u  X  h )( t ) on the interval t  X  [ t k , t k +1 ] . functional L k :  X   X  R with L k ( P h ) =  X  k , P h , where  X  k ( t ) = 1 [ t u (  X  t ) , t  X  R , denotes the involution of u .
 Proof: Since ( u  X  h )  X   X  , we have ( u  X  h )( t ) = ( u  X  h  X  g )( t ) ,t  X  R , and therefore R is a bounded linear functional L k :  X   X  R with where  X  k  X   X  and the last equality follows from the Riesz representation theorem [16]. To find  X  k , we use the fact that  X  is a Reproducing Kernel Hilbert Space (RKHS) [17] with a kernel K ( s,t ) = Letting  X  u = u (  X  t ) denote the involution of u and using (2), we obtain also interpreted as the measurements of ( P h )( t ) . A natural question then is how to identify P h Theorem 1. Let u be bounded with supp( F u ) = [  X   X  ,  X ] , h  X  H and b/ ( C X  ) &gt;  X  / X  . Then given an I/O pair ( u, T ) of the [Filter]-[Ideal IAF] neural circuit, P h can be perfectly identified as where  X  k ( t ) = g ( t  X  t k ) , t  X  R . Furthermore, c = G + q with G + denoting the Moore-Penrose pseudoinverse of G , [ G ] lk = Proof: By appropriately bounding the input signal u , the spike density (the average number of spikes over arbitrarily long time intervals) of an ideal IAF neuron is given by D = b/ ( C X  ) [14]. Therefore, [18] and ( P h )( t ) = where [ G ] lk =  X  l , X  k = 1 [ t computed as c = G + q . Remark 1. The condition b/ ( C X  ) &gt;  X  / X  in Theorem 1 is a Nyquist-type rate condition. Thus, perfect identification of the projection of h onto  X  can be achieved for a finite average spike rate. Remark 2. Ideally, we would like to identify the kernel h  X  H of the filter in cascade with the ideal is not BIBO-stable and does not have a finite temporal support. Nevertheless, it is easy to show that is sufficiently large.
 input signal u ( t ) , then q l = identify P  X  ( t ) = sin( X  t ) / (  X t ) , the projection of  X  ( t ) onto  X  .
 Corollary 1. Let u be bounded with supp( F u ) = [  X   X  ,  X ] , h  X  H and b C X  &gt;  X   X  . Furthermore, let given an I/O pair ( u, T ) of the [Filter]-[Ideal IAF] neural circuit, ( P h )( t ) can be approximated arbitrarily closely on t  X  [ T 1 ,T 2 ] by where  X  k ( t ) = g ( t  X  ( t k  X   X  + T )) , c = G + q , [ G ] lk = [ q ] l = C X   X  b ( t l +1  X  t l ) for all k,l  X  Z , provided that |  X  1 | and |  X  2 | are sufficiently large. Proof: Through a change of coordinates t  X  t 0 = ( t  X   X  + T ) illustrated in Fig. 2, we obtain W 0 = [  X  1  X   X  + T,  X  2  X   X  + T ]  X  [ T 1 ,T 2 ] and the set of spike times ( t k  X   X  + T ) k : t that W 0  X  R as (  X  2  X   X  1 )  X   X  . The rest of the proof follows from Theorem 1 and the fact that lim t  X  X  X  X  g ( t ) = 0 .
 single temporal window W to identify ( P h )( t ) to an arbitrary precision on [ T 1 ,T 2 ] . This result is not surprising. Since the spike density is above the Nyquist rate, we could have also used a canonical time decoding machine (TDM) [13] to first perfectly recover the filter output v ( t ) and then employ one of the widely available LTI system techniques to estimate ( P h )( t ) . However, the problem becomes much more difficult if the spike density is below the Nyquist rate. Theorem 2. (The Neuron Identification Machine) Let { u i | supp( F u i ) = [  X   X  ,  X ] } N i =1 be a col-lection of N linearly independent and bounded stimuli at the input to a [Filter]-[Ideal IAF] neural the neural circuit in response to the bounded input signal u i . If be identified perfectly from the collection of I/O pairs { ( u i , T i ) } N i =1 . Proof: Consider the SIMO TEM [14] depicted in Fig. 3(a). h ( t ) is the input to a population of N [14], it follows that, if { u i } N i =1 are appropriately bounded and a frame for  X  [14], [18]. Hence for i = 1 ,...,N,l  X  Z . Letting [ G ij ] lk =  X  i l , X  j k , we obtain with [ q i ] l = C X   X  b ( t i l +1  X  t i l ) , [ G ij ] lk = to find the coefficients c k , k  X  Z , we compute c = G + q .
 Corollary 2. Let { u i } N i =1 as before, h  X  H and closely on t  X  [ T 1 ,T 2 ] by  X  h ( t ) = G + q , with [ G ij ] lk = for all k,l  X  Z provided that |  X  1 | and |  X  2 | are sufficiently large.
 Proof: Similar to Corollary 1.
 Corollary 3. Let supp( F u ) = [  X   X  ,  X ] , h  X  H and let W i ,  X  i 1 , X  i 2 P h can be approximated arbitrarily closely on [ T 1 ,T 2 ] by where  X  j k ( t ) = g ( t  X  ( t j k  X   X  j + T )) , c = G + q with [ G ij ] lk = non-overlapping windows N is sufficiently large.
 Proof: The input signal u restricted, respectively, to the collection of intervals W i ,  X  i 1 , X  i 2 plays the same role here as the test stimuli { u i } N i =1 in Corollary 2. See also Remark 9 in [14]. Figure 3: The Neuron Identification Machine. (a) SIMO TEM interpretation of the identification problem Remark 4. The methodology presented in Theorem 2 can easily be applied to other spiking neuron models. For example, for the leaky IAF neuron, we have [ q i ] l = C X   X  bRC Similarly, for a threshold-and-feedback (TAF) neuron [15] with a bias b  X  R + , a threshold  X   X  R + , and a causal feedback filter with an impulse response f ( t ) , t  X  R , we obtain 3.3 Identifying Parameters of the Spiking Neuron Model If parameters of the spiking neuron model cannot be obtained through biophysical experiments, we can use additional input stimuli to derive a neural circuit that is  X  -I/O-equivalent to the original circuit. For example, consider the circuit in Fig. 1(a). Rewriting the t-transform in (1), we obtain where h 0 ( t ) = h ( t ) /b , t  X  R and q 0 k = C X /b  X  ( t k +1  X  t k ) .
 Setting u = 0 , we can now compute C X /b = ( t k +1  X  t k ) . Next we can use the NIM described in Section 3.2 to identify with arbitrary precision the projection P h 0 of h 0 onto  X  . Thus we identify a and a threshold  X  0 = C X /b . This neural circuit is  X  -I/O-equivalent to the circuit in Fig. 1(b). We now demonstrate the performance of the identification algorithm in Corollary 3. We model the t  X  [0 , 0 . 1 s ] , c = 3 and  X  = 200 . The general form of this kernel was suggested in [19] as a plausible approximation to the temporal structure of a visual receptive field.
 depend on the signal bandwidth  X  . In Fig. 4 the signal is bandlimited to  X  = 2  X   X  25 rad/s, whereas in Fig. 5 it is bandlimited to  X  = 2  X   X  100 rad/s. Although in principle the kernel h has an infinite bandwidth (having a finite temporal support), its effective bandwidth  X   X  2  X   X  100 rad/s (Fig. 6(b)). Thus in Fig. 4 we reconstruct the projection P h of the kernel h onto  X  with  X  = 2  X   X  25 rad/s, whereas in Fig. 5 we reconstruct nearly h itself. Next, we evaluate the filter identification error as a function of the number of temporal windows N and the stimulus bandwidth  X  . By increasing N , we can approximate the projection P h of h with arbitrary precision (Fig. 6(a)). Note that the estimate  X  h converges to P h faster for higher average spike rate (spike density D ) of the neuron. At the same time, by increasing the stimulus bandwidth  X  , we can approximate h itself with arbitrary precision (Fig. 6(b)). Previous work in system identification of neural circuits (see [20] and references therein) calls for parameter identification using white noise input stimuli. The identification process for, e.g., the LNP linearity. The performance of such an identification method has not been analytically characterized. In our work, we presented the methodology for identifying dendritic processing in simple [Filter]-[Spiking Neuron] models from a single input stimulus . The discussed spiking neurons include the ideal IAF neuron, the leaky IAF neuron and the threshold-and-fire neuron. However, the methods presented in this paper are applicable to many other spiking neuron models as well. The algorithm of the Neuron Identification Machine is based on the natural assumption that the den-dritic processing filter has a finite temporal support. Therefore, its action on the input stimulus can be observed in non-overlapping temporal windows. The filter is recovered with arbitrary precision from an input/output pair of a neural circuit, where the input is a single signal assumed to be ban-dlimited. Remarkably, the algorithm converges for a very small number of spikes. This should be contrasted with the reverse correlation and spike-triggered average methods [20].
 Finally, the work presented here will be extended to spiking neurons with random parameters. The work presented here was supported by NIH under the grant number R01DC008701-01.
