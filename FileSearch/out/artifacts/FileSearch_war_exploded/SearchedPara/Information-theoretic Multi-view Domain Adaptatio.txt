 Domain adaptation has been shown useful to many natural language processing applications including document classification (Sarinnapakorn and Kubat, 2007), sentiment classification (Blitzer et al., 2007), part-of-speech tagging (Jiang and Zhai, 2007) and entity mention detection (Daum  X  e III and Marcu, 2006).

Documents can be represented by multiple inde-pendent sets of features such as words and link struc-tures of the documents. Multi-view learning aims to improve classifiers by leveraging the redundancy and consistency among these multiple views (Blum and Mitchell, 1998; R  X  uping and Scheffer, 2005; Ab-ney, 2002). Existing methods were designed for data from single domain, assuming that either view alone is sufficient to predict the target class accu-rately. However, this view-consistency assumption is largely violated in the setting of domain adapta-tion where training and test data are drawn from dif-ferent distributions.

Little research was done for multi-view domain adaptation. In this work, we present an Information-theoretical Multi-view Adaptation Model (IMAM) based on co-clustering framework (Dhillon et al., 2003) that combines the two learning paradigms to transfer class information across domains in multi-ple transformed feature spaces. IMAM exploits a multi-way-clustering-based classification scheme to simultaneously cluster documents, words and links into their respective clusters. In particular, the word and link clusterings can automatically associate the correlated features from different domains. Such correlations bridge the domain gap and enhance the consistency of views for clustering (i.e., classifying) the target data. Results show that IMAM signifi-cantly outperforms the state-of-the-art baselines. The work closely related to ours was done by Dai et al. (2007), where they proposed co-clustering-based classification (CoCC) for adaptation learning. CoCC was extended from information-theoretic co-clustering (Dhillon et al., 2003), where in-domain constraints were added to word clusters to provide the class structure and partial categorization knowl-edge. However, CoCC is a single-view algorithm. Although multi-view learning (Blum and Mitchell, 1998; Dasgupta et al., 2001; Abney, 2002; Sridharan and Kakade, 2008) is common within a single domain, it is not well studied under cross-domain settings. Chen et al. (2011) proposed CODA for adaptation based on co-training (Blum and Mitchell, 1998), which is however a pseudo multi-view algorithm where original data has only one view. Therefore, it is not suitable for the true multi-view case as ours. Zhang et al. (2011) proposed an instance-level multi-view transfer algorithm that integrates classification loss and view consistency terms based on large margin framework. However, instance-based approach is generally poor since new target features lack support from source data (Blitzer et al., 2011). We focus on feature-level multi-view adaptation. Intuitively, source-specific and target-specific fea-tures can be drawn together by mining their co-occurrence with domain-independent (common) features, which helps bridge the distribution gap. Meanwhile, the view consistency on target data can be strengthened if target-specific features are appro-priately bundled with source-specific features. Our model leverages the complementary cooperation be-tween different views to yield better adaptation per-formance. 3.1 Representation Let D S be the source training documents and D T be the unlabeled target documents. Let C be the set of class labels. Each source document d s  X  D S is labeled with a unique class label c  X  C . Our goal is to assign each target document d t  X  D T to an appropriate class as accurately as possible.
Let W be the vocabulary of the entire document collection D = D S  X  D T . Let L be the set of all links (hyperlinks or citations) among documents. Each d  X  D can be represented by two views, i.e., a bag-of-words set { w } and a bag-of-links set { l } .
Our model explores multi-way clustering that si-multaneously clusters documents, words and links. Let  X  D ,  X  W and  X  L be the respective clustering of doc-uments, words and links. The clustering functions are defined as C D ( d ) =  X  d for document, C W ( w ) =  X  w for word and C L ( l ) =  X  l for link, where  X  d ,  X  w and represent the corresponding clusters. 3.2 Objectives We extend the information-theoretic co-clustering framework (Dhillon et al., 2003) to incorporate the loss from multiple views. Let I ( X,Y ) be mutual in-formation (MI) of variables X and Y , our objective is to minimize the MI loss of two different views: where  X 
W and  X  L are the loss terms based on word view and link view, respectively, traded off by  X  .  X  bal-ances the effect of word or link clusters from co-clustering. When  X  = 1 , the function relies on text only that reduces to CoCC (Dai et al., 2007).
For any x  X   X  x , we define conditional distribution based on Dhillon et al. (2003). Therefore, for any w  X   X  w , l  X   X  l , d  X   X  d and c  X  C , we can calculate a set of conditional distributions: q ( w |  X  d ) , q ( d q ( l |  X  d ) , q ( d |  X  l ) , q ( c |  X  w ) , q ( c |  X 
Eq. 1 is hard to optimize due to its combinatorial nature. We transform it to the equivalent form based on Kullback-Leibler (KL) divergence between two conditional distributions p ( x | y ) and q ( x |  X  y ) , where D ( p ( x | y ) || q ( x |  X  y )) = Lemma 1 (Objective functions) Equation 1 can be turned into the form of alternate minimization: (i) For document clustering, we minimize where  X  C (  X  W,  X  L ) is a constant 1 and (ii) For word and link clustering, we minimize  X  =  X  where for any feature v (e.g., w or l ) in feature set V (e.g., W or L ), we have
Lemma 1 2 allows us to alternately reorder either documents or both words and links by fixing the other in such a way that the MI loss in Eq. 1 de-creases monotonically. In this section, we present how the consistency of document clustering on target data could be en-hanced among multiple views, which is the key issue of our multi-view adaptation method.

According to Lemma 1, minimizing  X  D ( d,  X  d ) for each d can reduce the objective function value itera-tively ( t denotes round id):
In each iteration, the optimal document cluster-ing function C ( t +1) D is to minimize the weighted sum of KL-divergences used in word-view and link-view document clustering functions as shown above. The optimal word-view and link-view clustering func-tions can be denoted as follows: Our central idea is that the document clusterings closer in each iteration due to the word and link clusterings that bring together seemingly unrelated source-specific and target-specific features. Mean-while, C ( t +1) D combines the two views and reallo-cates the documents so that it remains consistent with the view-based clusterings as much as possi-ble. The more consistent the views, the better the document clustering, and then the better the word and link clustering, which creates a positive cycle. 4.1 Disagreement Rate of Views For any document, a consistency indicator function with respect to the two view-based clusterings can be defined as follows ( t is omitted for simplicity): Definition 1 (Indicator function) For any d  X  D ,  X  Then we define the disagreement rate between two view-based clustering functions: Definition 2 (Disagreement rate)
Abney (2002) suggests that the disagreement rate of two independent hypotheses upper-bounds the er-ror rate of either hypothesis. By minimizing the dis-agreement rate on unlabeled data, the error rate of each view can be minimized (so does the overall er-ror). However, Eq. 5 is not continuous nor convex, which is difficult to optimize directly. By using the optimization based on Lemma 1, we can show em-pirically that disagreement rate is monotonically de-creased (see Section 5). 4.2 View Combination In practice, view-based document clusterings in Eq. 3 and 4 are not computed explicitly. Instead, Eq. 2 directly optimizes view combination and pro-duces the document clustering. Therefore, it is nec-essary to disclose how consistent it could be with the view-based clusterings.

Suppose  X  = {F D |F D ( d ) =  X  d,  X  d  X   X  D } is the set of all document clustering functions. For any F D  X   X  , we obtain the disagreement rate  X  ( F clustering resulting from the overlap of the view-based clusterings.
 Lemma 2 C D always minimizes the disagreement rate for any F D  X   X  such that  X  ( C D , C D Meanwhile,  X  ( C D , C D
Lemma 2 suggests that IMAM always finds the document clustering with the minimal disagreement rate to the overlap of view-based clusterings, and the minimal value of disagreement rate equals to the dis-agreement rate of the view-based clusterings. Data and Setup Cora (McCallum et al., 2000) is an online archive of computer science articles. The documents in the archive are categorized into a hierarchical structure. We selected a subset of Cora, which contains 5 top categories and 10 sub-categories. We used a similar way as Dai et al. (2007) to construct our training and test sets. For each set, we chose two top categories, one as positive class and the other as the negative. Different sub-categories were deemed as different domains. The task is defined as top category classifi-cation. For example, the dataset denoted as DA-EC consists of source domain: DA 1(+), EC 1(-); and target domain: DA 2(+), EC 2(-).

The classification error rate  X  is measured as the proportion of misclassified target documents. In or-der to avoid the infinity values, we applied Laplacian smoothing when computing the KL-divergence. We tuned  X  ,  X  and the number of word/link clusters by cross-validation on the training data.
 Results and Discussions Table 1 shows the monotonic decrease of view dis-agreement rate  X  and error rate  X  with the iterations and their Pearson X  X  correlation  X  is nearly perfectly positive. This indicates that IMAM gradually im-proves adaptation by strengthening the view consis-tency. This is achieved by the reinforcement of word and link clusterings that draw together target-and source-specific features that are originally unrelated but co-occur with the common features.

We compared IMAM with (1) Transductive SVM (TSVM) (Joachims, 1999) using both words and links features; (2) Co-Training (Blum and Mitchell, 1998); (3) CoCC (Dai et al., 2007): Co-clustering-based single-view transfer learner (with text view only); and (4) MVTL-LM (Zhang et al., 2011): Large-margin-based multi-view transfer learner.
Table 2 shows the results. Co-Training performed a little better than TSVM by boosting the confidence of classifiers built on the distinct views in a comple-mentary way. But since Co-Training doesn X  X  con-sider the distribution gap, it performed clearly worse than CoCC even though CoCC has only one view.
IMAM significantly outperformed CoCC on all the datasets. In average, the error rate of IMAM is 30.3% lower than that of CoCC. This is because IMAM effectively leverages distinct and comple-mentary views. Compared to CoCC, using source training data to improve the view consistency on tar-get data is the key competency of IMAM.

MVTL-LM performed worse than CoCC. It sug-gests that instance-based approach is not effective when the data of different domains are drawn from different feature spaces. Although MVTL-LM regu-lates view consistency, it cannot identify the associ-ations between target-and source-specific features that is the key to the success of adaptation espe-cially when domain gap is large and less common-ality could be found. In contrast, CoCC and IMAM uses multi-way clustering to find such correlations. We presented a novel feature-level multi-view do-main adaptation approach. The thrust is to incor-porate distinct views of document features into the information-theoretic co-clustering framework and strengthen the consistency of views on clustering (i.e., classifying) target documents. The improve-ments over the state-of-the-arts are significant.
