 Computing semantic similarity between two terms is essen-tial for a variety of text analytics and understanding appli-cations. However, existing approaches are more suitable for semantic similarity between words rather than the more gen-eral multi-word expressions (MWEs), and they do not scale very well. Therefore, we propose a lightweight and effective approach for semantic similarity using a large scale semantic network automatically acquired from billions of web docu-ments. Given two terms, we map them into the concept space, and compare their similarity there. Furthermore, we introduce a clustering approach to orthogonalize the concept space in order to improve the accuracy of the similarity mea-sure. Extensive studies demonstrate that our approach can accurately compute the semantic similarity between terms with MWEs and ambiguity, and significantly outperforms 12 competing methods.
 H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Miscellaneous; I.2 [ Computing Methodologies ]: ARTI-FICIAL INTELLIGENCE X  Knowledge Representation For-malisms and Methods Term Similarity; Multi-word Expression; Clustering; Seman-tic Network  X 
Peipei Li was a student intern in Microsoft Research Asia when the paper was developed.

Measuring semantic similarity between terms is a funda-mental problem in lexical semantics [12] and it finds many applications in web and document search [29], question and answer systems, and other text analytics and text under-standing scenarios. By terms , we mean either single word-s or multi-word expressions (MWEs). We say two terms are semantically similar, if their meanings are close, or the concept or object that they represent share many common attributes. For example,  X  X merging markets X  and  X  X evelop-ing countries X  are similar because their semantic contents (the subset of countries) are very similar. Another example,  X  X oogle X  and  X  X icrosoft X  are similar because they are both software companies. However,  X  X ar X  and  X  X ourney X  are not semantically similar but related because  X  X ar X  is a transport means for the activity  X  X ourney X . Specifically, semantic sim-ilarity is defined by some measure of distance between two terms on an isA taxonomy. It is clear that  X  X ar X  and  X  X our-ney X  are quite far away from each other in an isA taxonomy from WordNet as shown in Figure 1. Semantic similarity is a more specific relationship and is much harder to model than relatedness (which can be modeled by term co-occurrence).
Recent work on term similarity can be roughly classified into two main categories: knowledge based and corpus based . Knowledge based approaches rely on handcrafted resources such as thesauri, taxonomies or encyclopedias, as the contex-t of comparison. Most work in this space [23, 25, 5] depends on the semantic isA relations in WordNet [20] which is a manually curated lexicon and taxonomy. Corpus based ap-proaches work by extracting the contexts of the terms from large corpora and then inducing the distributional proper-ties of words or n-grams . Corpus can be anything from web pages, web search snippets to other text repositories.
One significant challenge faced by the knowledge-based methods is the limited coverage of taxonomies such as Word-Net (with 155,287 words at last count). It does not cover many proper nouns (e.g.,  X  X icrosoft X  or  X  X oogle X ), or very popular senses (e.g., Apple the company or Jaguar the car make). Another major restriction of WordNet is that it pri-marily covers single words with only a handful of phrases or multi-word expressions. For example, it does not know  X  X eneral Electric X  X r X  X merging markets X . Consequently, the Figure 1: A fragment from WordNet showing se-mantic distance between  X  X ar X  and  X  X ourney X  similarity between X  X eneral Electric X  X nd X  X E X  X s completely ignored.

Corpus-based approaches also face several serious limita-tions. First, such measures are biased because of the index-ing and ranking mechanisms used in search engines. For ex-ample when querying the term  X  X ate X  or  X  X ange X  on Google, none of the first 100 results has anything to do with fruits (a sense for date) or cooking stoves (a sense for range), because these are rare senses of the two terms. With such search re-sults, it is not surprising that a corpus-based method would think  X  X sian pear X  and  X  X ate X  share very little commonali-ty. Second, some search-result oriented similarity methods require interaction with the search engine which has high communication overhead and high index costs, and are not suitable for online applications. Third, statistical distribu-tion based on words or n-grams in the context ignores the fact that i) the semantic units can be MWEs and not words, let alone n-grams; and ii) many words or phrases are am-biguous in meaning. Finally, corpus-based methods focus on surrounding context of a term or the co-occurrence of two terms within a neighborhood, both of which are more suit-able to the calculation of semantic relatedness rather than similarity. Under this approach,  X  X ar X  and  X  X ourney X  would have high semantic relatedness because they co-occur very frequently on web texts.

In this paper, we propose a light-weight but effective frame-work for computing semantic similarity between a pair of terms using a large scale, general purpose isA network ob-tained from a web corpus. It belongs to the knowledge-based method. Below is a small sample of results:
This pair shares the same hypernyms, such as  X  X ctivity X .
This pair shares the same hypernyms, such as  X  X ood X .
The main contributions of this paper are:
The rest of the paper is organized as follows. Section 2 introduces the preliminaries of Probase, our isA semantic network. Section 3 describes a basic algorithm for comput-ing term similarity using Probase. Section 4 proposes an im-portant refinement to the basic algorithm which addresses several key challenges faced by the basic approach. Section 5 gives some experimental results that compare our approach with a whole list of other previous approaches both using knowledge and using external corpora. Finally we discuss some related work in Section 6 and conclude in Section 7.
To compute the similarity between two terms, we com-pute the similarity between their contexts. The context that we use in this paper comes from a large-scale, probabilistic semantic network, known as Probase [30]. Besides other knowledge, Probase contains isA relations between concept-s, sub-concepts, and entities, which is called  X  isA in this paper. We can represent  X  isA as  X  isA = { c, e } ,where c indicates a hypernym and e indicates a hyponym. The isA relationships in Probase are harvested from 1.68 billion web pages and 2 years X  worth of Microsoft Bing X  X  search log us-ing syntactic patterns (e.g., the Hearst patterns [15] and the is-a pattern). For example,  X  X icrosoft is a company X . Here  X  X ompany X  X s a concept and X  X icrosoft X  is an entity . We refer to concepts and entities collectively as terms in this paper. Probase has the following important properties:
Before we deal with similarity between any two terms, we first look at terms that have the same meaning. Intuitively, they should have the highest similarity. A single term may have many surface forms: synonyms:  X  X E X  X nd X  X eneral Electric X ;  X  X orporation X , X  X ir-spelling styles:  X 2d barcode X  vs.  X 2d bar code X  and  X  X c-singular/plural forms:  X  X hoe X  vs.  X  X hoes X ;
We address this issue in two steps. First, we use sources such as Wikipedia Redirects, Wikipedia Internal Links, and synonym data set in WordNet to group terms that are syn-onyms. Second, we use the edit distance function to evaluate the distance between terms as follows. If dis lex ( t 1 ,t 2 ) &lt; X  , the two terms in the current pair are ones with very similar surface forms, and we group them to-gether. In this paper, we set  X  to 0.05 according to empirics, which enables high accuracy (95%) for identifying synony-mous pairs.

At this point, all lexically similar or synonymous terms are grouped into a cluster which is analogous to the notion of  X  X ynset X  in WordNet. As a result, the isA pairs between terms are also mapped logically into isA relations between synsets. The set of all synsets is called  X  ssyn which pro-vides a mapping between any Probase term, to its synset and hence all the other terms in that synset. When com-puting the semantic similarity between two terms which be-long to the same synset, e.g., General Electric and GE, the similarity is set to the highest score, namely 1.
This section presents the basic framework of computing semantic similarity between two terms. In nutshell, given terms, i.e., whether they are concepts or entities, and then finally compute the similarity between the two contexts.
Type checking requires the following data from the se-mantic network: 1) the entity and concept sets; 2) the isA relations between terms and their frequencies in corpus. If the given pair of terms has an isA relation, then the hyper-nym term is said to be a concept term while the hyponym term is an entity term. Otherwise, we decide the type of each term individually: t is a concept if its frequency as a hypernym in the isA network is larger than its frequency as a hyponym; it is an entity otherwise.
We extract the context of a term according to its type and its position in the semantic network. If the term is a concept, its context is all the entities that it subsumes; if it is an entity, its context is all the concepts that it belongs to. Algorithm 1 Basic Approach Output: a similarity score of t 1 ,t 2 ; 2: Let sim ( t 1 ,t 2 )  X  1 and return sim ( t 1 ,t 2 ); 3: end if 4: Judge the type for each term; 8: end if 12: end if 13: if t 1 ,t 2 is a concept-entity pair then 14: Collect top k concepts of the entity term t i from  X  isA 15: for each concept c x in C t i ( c x = t j , i = j ,1  X  16: sim c x  X  get the semantic similarity between c x and 17: end for 19: end if Furthermore, we transform the context into a vector I c or I term and a term in the context: and entity e i , that is, how typical e i is among all the entities c subsumes.
 e and concept c i , that is, how typical c i is among all the concepts e belongs to.
We use the cosine similarity function 3 to evaluate the similarity between two contexts, i.e.,
The complete algorithm for the basic approach is shown in Algorithm 1. We set top k = 5 by the empirical study.
Our experiments reveal that the cosine function outper-forms other similarity/distance evaluation functions, such as Jaccard and the smoothed KL divergence. In addition, to avoid an infinite loop in Algorithm 1, we limit the maximum iteration depth no more than 5, namely
Our preliminary evaluation shows that the basic approach works reasonably well for many pairs of terms, but for am-biguous terms with multiple senses such as apple and orange , the result is less satisfactory.

For example, as shown in Table 1, the basic approach decides that microsoft, google and apple,pear are quite similar whereas apple,microsoft and orange, red are not, because  X  X pple X  and  X  X range X  have multiple senses. The dominant senses of  X  X pple X  and  X  X range X  are a fruit, and we can see when we are comparing similarity using non-dominant senses, the results are less satisfactory.
The baseline approach introduced in Section 3 is not sen-sitive to different senses of a term. A simple solution is to use an existing knowledge database containing sense labels of terms such as the glosses in WordNet. But none of the handcrafted knowledge bases has the sufficient data cover-age. Instead we propose the following refined approach.
Given a term, we define its concept context as the entire set of concepts that the term belongs to in Probase. We perform automatic sense disambiguation by concept clustering. We then prune irrelevant clusters as an optimization. Finally, we define the similarity of two terms as the highest similarity between any sense of the first term and any sense of the second term. Next we present this approach in details.
To identify multiple senses of a term automatically, we first use a k-Medoids clustering algorithm on the concept context of the term, and then we select the center concept in each cluster to represent a sense of this term. Figure 2(a) shows the concept context of the term  X  X pple X , and Fig-ure 2(b) shows the clustered concepts. It is clear that each cluster represents a sense of the term.

In the following, we define the distance measure and present the clustering algorithm.
We first define the semantic distance between two con-cepts c 1 and c 2 as where I c i represents the vector of entity distributions of con-
Our algorithm is a modified k-Medoids clustering algo-rithm that partitions concepts according to their entity dis-tributions. Good initial centers are essential for the success of partitioning clustering algorithms such as K-Medoids. In-stead of using random initial centers, we identify good initial centers incrementally by a refined method from Moore [21]. The first medoid is randomly selected among all candidate points (concepts). Then we select the point that has the maximum of the minimum of the distances from each of the existing medoids to be the next medoid, i.e., where c j indicates the j th point in the candidate points, m i indicates the i th medoid in existing medoids, and  X  indi-cates the threshold in the limit of initial medoid count. This process continues until we do not find any medoids satisfy-ing Eq. (6). In this case, we get k medoids at iteration 0: M 0 = { m 0 1 , ..., m 0 k } . Clearly, the value of k is determined by the threshold  X  . The larger the threshold of  X  ,thesmall the value of k . Since experiments show that the numbers of clusters do not vary much with  X  between 0.7 to 0.8, we set  X  = 0.7 as an optimal value.

With k medoids in the t th iteration, we assign each can-didate concept c i  X  C to its closest medoid m  X   X  M t = { m t 1 , ..., m t k } , namely, a medoid m  X  with the minimum se-mantic distance from c i : When we assign all candidate concepts to the corresponding clusters, we can update the medoid with the most centrally located concept in each cluster. To find such a center con-cept, we first compute the average distance of a cluster K in terms of the semantic distance in Eq. (5) as The clustering process iterates until the following objective function reaches minimum.
 is a known number of centers, n is the count of objects (concepts) to cluster. W =[ w ij ]isa k  X  n binary matrix, M =[ m 1 ,...,m k ] is a set of cluster medoids and m i is the i th cluster medoid.
We use Eq. (8) to calculate the medoid set M .When M is computed, to minimize F ( W, M ), W is given by The convergence condition is that F ( W t ,M t +1 )  X  F ( W is less than a threshold  X  (e.g., 10  X  5 ). Accordingtothe above processing of k-Medoids, we can get k clusters for all given concepts.
The k-Medoids clustering algorithm has a time complex-ity of O ( kn 2 ), where k is the number of centers and n is the number of objects (concepts) to cluster. This is not ac-ceptable if the number of pairs is large. To improve the efficiency, we cluster all concepts in the semantic network offline, and then during online calculation, each concept in a term X  X  context can be quickly mapped to an offline cluster which acts as synset, and this effectively reduces the online clustering complexity to O ( n ).
To cluster the concepts in the semantic network, we use the entity distributions to represent the concepts and eval-uate their similarities by Eq. (5). According to the isA relationships between concepts and entities in  X  isA ,wecan construct a bipartite graph between concepts and entities (Figure 3) and cluster the concepts based on this graph. The basic idea is that if two concepts share many entities, they are similar to each other. From this bipartite graph, we represent each concept c i as an L2-normalized vector as shown in Eq. (2), where each dimension corresponds to an entity in the graph.

Even though the number of concept and entity nodes may be large, the graph is actually very sparse. For example, a concept is connected with an average number of 5.72 entities on average. Therefore, for a concept c , the average size of c , is small. To find the closest cluster to c , we only need to check the clusters which contain at least one concept in S method, the average number of clusters to be checked is small. Furthermore, edges in the graph with low weights (i.e., low typicality scores) are likely to be noises and can be ignored.
In the basic approach, we compute the similarity of two terms by the cosine similarity between their contexts. In the refined approach, we use a new similarity function known as max-max similarity which is useful in identifying rare senses of terms with small sized clusters. Let K = { K 1 ,K 2 , ..., K k concepts that two terms belong to respectively. According to the cluster information in K , we can get the clusters in C and where We then compute the similarity between the contexts of each cluster pair and get the semantic similarity between two terms as: The corresponding value vector of x (or y ) indicates the set (or sup ( t 2 )).

With all concepts clustered offline and the new similarity function based on concept clusters, the refined algorithm is given in Algorithm 2.
The cluster-based refined approach improves the quality of similarity remarkably from the basic algorithm. But there are two problems. First, the max-max similarity function tends to boost the probability of picking a less dominant sense of a term because it is easier for small clusters to look similar by the cosine similarity and hence dominate the max-max similarity score. However, many small clusters in C are usually noises. This leads to incorrect similarity results. Second, with the current concept clustering algorithm, some terms can have both a general sense and a more specific sense. For example, the term  X  X unch X  has a specific sense called X  X ish X  X nd a more general (and also vague) sense called  X  X ctivity X . We know  X  X ctivity X  is more general because it is a super-concept of  X  X ish X  in  X  isA . Such general senses pose problems because they make almost unrelated terms similar. For example, the term  X  X usic X  also has the  X  X ctivity X  sense and thus is deemed similar to  X  X unch X .

To overcome these problems, we adopt an optimization technique called cluster pruning after concept clustering. First, to reduce the negative impact from noisy clusters, we prune away those clusters with only one member or with very small combined weight. The weights of clusters are computed below. Let the concept clusters of the term t be K where w i = c j  X  x i p ( c j | t )and1  X  i  X  m . Second, to avoid the impact from the vague senses, we prune the clusters whose senses are super-concepts of other senses according to hierarchical isA relationships of senses after clustering con-cept contexts of two terms  X  X unch X  and  X  X usic X . Because the senses  X  X ctivity X ,  X  X ost X ,  X  X nterest X  and  X  X rt X  are the super-concepts of the senses  X  X ish X  and  X  X ultimedia X , we Algorithm 2 Refined Approach Output: a similarity score of t 1 ,t 2 ; 1: Install the synset checking and type checking as Steps 3: return sim ( I t 1 c , I t 2 c ) as Steps 6-7 in Algorithm 1; 4: end if 6: sim 1  X  sim ( I t 1 e , I t 2 e ) as Steps 10-11 in Algorithm 1; 8: sim 2  X  sim ( K t 1 ,K t 2 ) computed in Eq. (11); 9: return max ( sim 1 ,sim 2 ); 10: end if 14: for each cluster x in K t i do 15: Select top k concepts to represent t i ,namely C k x = 16: for each concept c y in C k x do 18: end for 20: end for 21: return max { sim x | x  X  K t i } ; 22: end if only keep specific senses like  X  X ish X  and  X  X ultimedia X  and remove the rest.
In this section, we first outline the experimental setup, and then compare the effectiveness of the online and the offline variant of our approach, and also compare our approaches (basic, refined and refined with pruning) with 12 competing methods on three benchmark data sets. Finally, we evaluate the efficiency of our approaches.
We use three data sets in the following experiments, in-cluding two well-known benchmark data sets for word simi-larity and one labeled data set for evaluating MWEs which is created by us. Table 2 shows the descriptions and some examples in each of the three data sets. M&amp;C data set is a subset of Rubenstein-Goodenough X  X  [26] and consists of 28 word pairs. Because of the omission of two word pairs in ear-lier versions of WordNet, most researchers used only 28 word pairs for evaluations in the past. We follow this tradition in this paper. WordSim203 is a subset from WordSim353[1], and has been used as a similarity testing data set by Agirre et. al.[7] It contains 203 pairs which are considered more similar than related.
 Figure 4: Illustration to vague and specific senses of terms lunch and music
Because there are no benchmark data for the semantic similarity between MWEs, we labeled 300 pairs (known as WP) with both words and MWEs. Our labeled data consist of three categories: 100 concept-entity pairs, 100 concept-concept pairs and 100 entity-entity pairs. These 300 pairs contain 84 word pairs and 216 MWE pairs, in which 71 MWE pairs are in WordNet the remaining are not. You can find all 300 labeled pairs at http://adapt.seiee.sjtu. edu.cn/similarity/SimCompleteResults.pdf . Five native speakers of English labeled these pairs according to the label classes, and the labels are then translated into numerical similarity scores in Table 3. These scores are averaged to produce the final rating for each pair.

All experiments are performed on an Intel Core 2 Duo 2.66GHz PC with 4G physical memory, running Windows 7 Enterprise. All timing results are averaged over 10 runs. All competing methods involved in this section are summarized in Table 4. We implemented S  X  an method while adopting the existing implementation [2] of other methods. To e-valuate the effectiveness of each method, we compute the Pearson Correlation Coefficient (PCC in short) to measure the agreement between the machine rating (computed by the semantic similarity measurement approaches) and the human ratings over the data sets as follows, where X is the machine ratings while Y is the human ratings:
Figure 5 reports the PCC in our RCP approach with on-line clustering and offline clustering respectively on the WP data set. From this figure, we can see that the PCC val-ues for online clustering and offline clustering differ only marginally. Therefore, in the following experiments, we use the offline clustering in our refined approach.

Table 5 compares the PCC of our approaches with that of 12 others. Some of these competing methods (from Rad to Agi) rely on WordNet and do not recognize MWEs that are not in WordNet, therefore they are excluded from compari-son in the experiment on  X  X WEs Not in WordNet X , marked with  X - X . From the experimental results, we make the fol-lowing observations.

First, our most advanced approach, RCP, leads the com-petition against the peers by large margins in all data sets, especially in MWE pairs.

Second, in the Hun method, the PCC value is negative, because it depends only on the surface forms of terms. Most terms which are semantically similar are not lexically simi-lar. Thus, some of the computed similarities are incorrect , which leads to the negative correlation.
 Third, methods based on taxonomy structure, such as Rad, Hir and Do, generally fare better than pure syntax-based methods.
 Fourth, information content based methods, such as Res, Jcn, Lin and S  X  an, generally do better than other WordNet based methods. Information content based methods effec-tively combines the knowledge from the taxonomy structure and external corpora. This has certain advantage but the coverage of this knowledge is still limited compared to the knowledge we acquired from the entire web.

Finally, search snippet-based method like Bol works fine with M&amp;C data sets but fares quite badly elsewhere. This is because it considers co-occurrences of two terms which pro-duces more of relatedness than similarity. It works badly with words in WP because word pairs in WP contain many ambiguous terms, and many pairs with transitive isA rela-tionships ( e.g., animal, puppy with  X  X og X  being the child of  X  X nimal X  and parent of  X  X uppy X ) and many pairs with vague senses (e.g., music, lunch with the vague sense  X  X c-tivity X ). Co-occurrence alone is not effective on these pairs.
Figure 6 reports the PCC of six approaches which work with arbitrary MWEs and the experiment is done on all 300 pairs from the WP data set. RCP produces a PCC value of around 0.7 which is much higher than the other peers.
Figure 7 reports the PCC of our approaches on three types of pairs in WP. From the experimental results, we can see that our three approaches have the same PCC value (0.74) on the concept pairs, because they have the same calculation mechanism on these pairs. Our methods generally work bet-Table 4: Competing Methods (IC = Information Content, LCA = Least Common Ancestor) Table 5: Pearson Correlation Coefficient on Three Data Sets with Word or MWE Pairs (WN: Word-Net) ter with concept-entity pairs than entity-entity pairs. The reason is that concept-entity pairs are similar only if they are in a hypernym-hyponym relation so the similarity is clearly defined. In the case of entity-entity pairs, comparing their concept contexts can be difficult due to i) the ambiguity in the senses and ii) the noises in the super-concepts which can be very abstract and vague.

Table 6 shows some examples from each data set along with the computed similarity scores by the RCP approach. Human ratings have been uniformly normalized to [0, 1] in this table. Complete set of results can be found at http: //adapt.seiee.sjtu.edu.cn/similarity/ .
Figure 8 compares the execution time between online clus-tering and offline clustering in our refined approach. Offline clustering, with only a fraction of the cost, is a clear winner.
Figure 9 reports the average computation time on a pair of terms in our approaches compared to the other competi-tors. On average, RCP takes 65 milliseconds to compute the similarity of a pair, which is on par with most of the earlier methods using information content and WordNet. String-based methods are faster for an obvious reason: they need PCC Figure 5: Performance of RCP with online/offline clustering PCC Figure 7: Performance comparison on various types of pairs not collect any context or model the context. Hir is slow because it considers the lexical chain in the taxonomy in the calculation of semantic similarity between terms. Bol takes about 60 times longer than RCP because it requires extracting lexico-syntactic patterns from snippets online.
Figure 10 shows the average computation time on differ-ent types of pairs using our approaches. RCP costs less than half the time of RC due to the pruning. Computing similar-ity between concept-entity pairs is more expensive because in order to catch the concept-entity pairs with potentially transitive isA relationships, e.g.,  X  X nimal X  X nd X  X uppy X (with  X  X og X  being the child of  X  X nimal X  and parent of  X  X uppy X ), we iteratively check the relatio ns between every top ancestor concepts of an entity term and the concept term in RCP. Figure 8: Execution time in online/offline clustering Figure 9: Computation time in different approaches
Contrary to the semantic relatedness which represents the more general relationships such as part-whole and the co-occurrence, semantic similarity measures the degree of tax-onomic likeness between concepts and considers relations such as hyperonymy and synonymy. In this section, we only discuss previous work on semantic similarity, while most of them can be adapted or generalized to deal with semantic relatedness. To compute the semantic similarity between terms, existing efforts mainly follow two approaches: The first approach calculates the semantic similarity based on some distance in a preexisting thesauri, taxonomy or ency-clopedia, such as WordNet. The second approach computes similarity by the terms X  context in large text corpora (such as the search snippets and web documents) and such sim-ilarities are derived from distributional properties of words or n-grams in the corpora.
 Knowledge-based Approach Most methods in this direction use a taxonomy such as WordNet, which is a tree hierarchy, as the knowledge base to compute the similarity between terms. The most straight-forward way to calculating similarity between two terms on the WordNet is to find the length of the shortest path con-necting the two terms in the taxonomy graph [23]. This path-length based approach is very simple, but has a low accuracy because: i) it relies on the notion that all links in the taxonomy represent a uniform distance; ii) it ignores the amount of information hidden in the concept nodes.
More advanced approaches [25, 17, 18, 28, 27] compute the similarity between t 1 and t 2 by the information content of these terms with respect to the taxonomy structure. The pioneer work by Resnik [25] suggests that the similarity mea-sure is the information content of the least common ancestor Figure 10: Computation time on different types of pairs node of the two terms in the taxonomy tree. To compute the information content of a term, it requires a large text corpus to obtain the occurrences of the term. A limitation of this method is that the similarities between all children of a concept are identical, regardless of their individual infor-mation content. The most recent information content based approach [27] calculates the information content of term t by the ratio of the number of hypernyms of t divided by the number of all descendants of t in WordNet. Some of the above measures have been adapted to the biomedical field by incorporating domain information extracted from clinical data or from medical ontologies (such as MeSH or SNOMED-CT ( R ) ) [22, 10].

Other researchers attempted to apply graph learning algo-rithms on term similarity computation. Given two terms t 1 and t 2 , Alvarez and Lim [8] build a rooted weighted graph called Gsim , using the terms hypernyms, other relations, and descriptive glosses from WordNet, and then calculate the similarity score by selecting the minimal distance be-tween any two hypernyms c 1 and c 2 of t 1 and t 2 respective-ly, by random walk. Agirre et. al. subsequently proposed a WordNet-based personalized PageRank algorithm [6, 5]. It first computes the personalized PageRank of each word and aggregates into a probability distribution for each synset. Similarity is then defined by the cosine between two distri-butions.

The above knowledge based approaches depend heavily on the completeness of the underlying taxonomy and the exter-nal corpora. However, the popular taxonomy like WordNet does not have the adequate coverage as it cannot keep up with the development of new terms and phrases everyday.
The framework proposed in this paper is also knowledge based, but is more scalable and effective, because i) the knowledge we use was acquired from the entire Web; and ii) the clustering algorithm detects the senses of the input terms and the max-max similarity function effectively pick-s the senses that are most suitable given the pair of terms. The above methods cannot be easily adapted to use Probase because it is a general network, not a tree structure. Corpus-based Approach
In this space, Chen et. al. proposed a double-checking model using text snippets returned by a Web search engine to compute semantic similarity between words [13]. The proposed method uses the occurrences of terms X and Y in their search snippets to evaluate the semantic similarity. Recently, Bollegala et.al. proposed a new measure using Table 6: Example Pairs and Their Semantic Simi-larity Scores Computed by RCP Approach page counts and snippets from Web search [11]. The search engine based methods are more time-consuming because i) snippets and search results must be obtained online; ii) it requires parsing of the returned text by the patterns.
Radinsky et. al. proposed a new model, Temporal Seman-tic Analysis (TSA) [24], which captures the temporal infor-mation of corpus. TSA uses a more refined representation, where each concept is no longer scalar, but is instead rep-resented as time series over a corpus of temporally-ordered documents. This method can improve the pearson correla-tion coefficient, but it requires massive historical data, which leads to more time-consuming in the handling of texts.
Most corpus based methods are more suitable for the se-mantic relatedness not for the semantic similarity because they make heavy use of the co-occurrence context in the representation of terms or in similarity functions. Mean-while, corpus based methods are more suitable for specific languages.
We presented a lightweight, effective approach for seman-tic similarity between terms with any multi-word expres-sion. It uses an isA semantic network extracted from a large Web corpus to provide contexts for the terms, employs a concept clustering algorithm to disambiguate the senses of the input terms, and finally applies a max-max similarity function to compute the similarity. Extensive studies show that our clustering-based refined algorithm outperforms the state-of-the-art methods as well as our basic algorithm in terms of pearson correlation coefficient on word pairs and MWE pairs. The method is efficient enough to be applied on large scale data sets. In our future work, we will focus on how to learn a more robust similarity function instead of a simple Max-Max similarity function, and how to introduce the supervised algorithms instead of clustering methods for higher Pearson Correlation Coefficients.
This work is supported in part by National 863 Program of China under gra nt 2012AA011005, the National 973 Pro-gram of Chin a under grant 2013CB329604, the Natural Sci-ence Foundati on of China unde r grants (61100050, 61273292, 61229301, 61070131, 61273297), the Postdoctoral Science Foundation of Hefei University of Technology under grant 2013HGBH0025, MOE New Fa culty unde r grant 201100731-20023, and the US National Scie nce Foundation (NSF) un-der grant CCF-0905337. [1] http://www.cs.technion.ac.il/~gabr/resources/ [2] http://wn-similarity.sourceforge.net/ . [3] http://www.math.uwo.ca/~mdawes/courses/344/ [4] http://www.codeproject.com/Articles/11835/ [5] E.Agirre,M.Cuadros,G.Rigau,andA.Soroa.
 [6] E. Agirre and A. Soroa. Personalizing pagerank for [7] E. Agirre, A. Soroa, E. Alfonseca, K. Hall, [8] M. Alvarez and S. Lim. A graph modeling of semantic [9] S. Banerjee and T. Pedersen. An adapted lesk [10] M. Batet, D. S  X  l  X cnchez, and A. Valls. An [11] D. Bollegala, Y. Matsuo, and M. Ishizuka. A web [12] A. Budanitsky and G. Hirst. Evaluating [13] H. Chen, M. Lin, and Y. Wei. Novel association [14] Q. Do, D. Roth, M. Sammons, Y. Tu, and [15] M. A. Hearst. Automatic acquisition of hyponyms [16] G. Hirst and D. St-Onge. Lexical chains as [17] J. Jiang and D. Conrath. Semantic similarity based on [18] D. Lin. An information-theoretic definition of [19] G. Miller and W. Charles. Contextual correlates of [20] G. A. Miller. WordNet: A lexical database for english. [21] A. W. Moore. An intoductory tutorial on kd-trees. [22] T. Pedersen, S. V. S. Pakhomov, S. Patwardhan, and [23] R. Rada, H. Mili, E. Bichnell, and M. Blettner. [24] K.Radinsky,E.Agichtein,E.Gabrilovich,and [25] P. Resnik. Using information content to evaluate [26] H. Rubenstein and J. B. Goodenough. Contextual [27] D. S  X  anchez, M. Batet, and D. Isern. Ontology-based [28] N. Seco, T. Veale, and J. Hayes. An intrinsic [29] Y. Wang, H. Li, H. Wang, and K. Q. Zhu.
 [30] W. Wu, H. Li, H. Wang, and K. Q. Zhu. Probase: a
