 proportions. In an extreme case, labeled data for an entire c lass might be missing (for example, negative experimental results are typically not published ). A naively trained learner may perform bias in the context of cost-sensitive learning and learning from imbalanced data [5, 11, 2]. If the on biased data can be corrected by reweighting [5]. When the la beling bias is unknown, a model is often selected using threshold-independent analysis such as ROC curves [11]. A good ROC curve, however, does not guarantee a low loss on test data. Here, we a re concerned with situations when the labeling bias is unknown and some classes may be missing, but we have access to unlabeled data. We want to construct models that in addition to good ROC -based performance, also yield low test loss. We will be concerned with minimizing joint and conditional log loss, or equivalently, maximizing joint and conditional log likelihood.
 Our work is motivated by the application of modeling species  X  geographic distributions from occur-tribution modeling suffers from extreme imbalance in train ing data: we often only have information about species presence (positive examples), but no informa tion about species absence (negative ex-amples). We do, however, have unlabeled data, obtained eith er by randomly sampling locations from the region [4], or pooling presence data for several spe cies collected with similar methods to yield a representative sample of locations which biologist s have surveyed [13]. Previous statistical methods for species distribution mod eling can be divided into three main ap-proaches. The first interprets all unlabeled data as example s of species absence and learns a rule to discriminate them from presences [19, 4]. The second embe ds a discriminative learner in the EM algorithm in order to infer presences and absences in unla beled data; this explicitly requires knowledge of true class probabilities [17]. The third model s the presences alone, which is known in commonly reweighted so that positive and negative examples have the same weight [4]; this models a quantity monotonically related to conditional probabili ty of presence [13], with the relationship and x to denote a location on the map, then the first two approaches y ield models of conditional stantiation of the third approach, maximum entropy density estimation (maxent) [14] yields a model and necessary for measuring conditional log loss on which we focus here) again requires knowledge We apply robust Bayesian decision theory, which is closely r elated to the maximum entropy prin-one-class estimation and species distribution modeling in particular. Using an extensive evaluation on real-world data, we show improvement in both generative a nd discriminative techniques. the bias can be decomposed into an estimable and inestimable part, the right approach might be to use a combination of techniques presented in this paper and t hose for sample-selection bias. Our goal is to estimate an unknown conditional distribution  X  ( y | x ) , where x  X  X is an example and y  X  Y is a label. The input consists of labeled examples ( x examples x space X and the set of features J to be very large.
 In species distribution modeling from occurrence data, the space X corresponds to locations on the examples are locations that have been surveyed by biologist s, but neither presence nor absence was recorded. The unlabeled examples can be obtained as presenc e locations of species observed by a similar protocol, for example other birds [13].
 revealed with an unknown probability that depends on y and is independent of x . This means that In our example, species presence is revealed with an unknown fixed probability whereas absence is revealed with probability zero (i.e., never revealed). 2.1 Robust Bayesian Estimation, Maximum Entropy, and Logis tic Regression Robust Bayesian decision theory formulates an estimation p roblem as a zero-sum game between a estimate  X  (equivalently, maximizing the worst-case log likelihood r atio) Here,  X  is the simplex of joint densities and E uniform density.
 Gr  X  unwald and Dawid [6] show that the robust Bayesian problem (E q. 1) is often equivalent to the minimum relative entropy problem where RE ( p k q ) = E and measures discrepancy between distributions p and q . The formulation intuitively says that we minimizing relative entropy is equivalent to maximizing en tropy H ( p ) = E the approach is mainly referred to as maximum entropy [10] or maxent for short. The next theorem outlines the equivalence of robust Bayes and maxent for the c ase considered in this paper. It is a special case of Theorem 6.4 of [6].
 Theorem 1 (Equivalence of maxent and robust Bayes) . Let X  X  Y be a finite sample space,  X  continuous w.r.t.  X  . Then Eqs. (1) and (2) have the same optimizers.
 For the case without labeling bias, the set P is usually described in terms of equality constraints versions of f distribution of the golden bowerbird from presence-absenc e data then moment equality constraints average altitude of absence locations (both weighted by the ir respective training proportions). When the number of samples is too small or the number of feature s too large then equality con-lie within a certain distance of sample averages [3].
 The solution of Eq. (2) with equality or relaxed constraints can be shown to lie in an exponential family parameterized by  X  = h  X  y i The optimizer of Eq. (2) is the unique density which minimize s the empirical log loss possibly with an additional  X  [3] for a proof.) on marginals of p . The most common implementations of maxent impose marginal constraints takes form q As before, the maxent solution is the unique density of this f orm which minimizes the empirical log loss (Eq. 3). The minimization of Eq. (3) is equivalent to the minimization of conditional log loss Hence, this approach corresponds to logistic regression. S ince it only models the labeling process  X  ( y | x ) , but not the sample generation  X  ( x ) , it is known as discriminative training. solution has the form q Log loss can be minimized for each class separately, i.e., ea ch  X  y is the maximum likelihood esti-mate (possibly with regularization) of  X  ( x | y ) . The joint estimate q conditional distribution q training when  X  ( x | y ) = Q discriminative training with two additional components: a vailability of unlabeled examples and lack latter the form of constraints P . 2.2 Generative Training: Entropy-weighted Maxent When the number of labeled and unlabeled examples is sufficien tly large, it is reasonable to assume across y , but if some classes are known to be rarer than others then a no n-uniform estimate will perform better. In Section 3, we analyze the impact of this ch oice.
 ify true moments in the presence of labeling bias. However, a s discussed earlier, labeled examples expectations, we constrain conditional expectations E Bayes and maxent problems with the set P of the form P = { p  X   X  : p y the | X | -dimensional vector of conditional probabilities p ( x | y ) and P y p . For example, relaxed constraints for class y are expressed as where  X  y estimates  X  y tion of f When m The next theorem and the following corollary show that robus t Bayes (and also maxent) with the notation p y denoted  X   X  Theorem 2. Let P y If P contains at least one density absolutely continuous w.r.t.  X  then robust Bayes and maxent over P minimize RE ( p y the equivalence of robust Bayes and maxent follows from Theo rem 1. To prove the remainder, we rewrite the maxent objective as Maxent problem is then equivalent to Since RE ( p k q ) is minimized for p = q , we indeed obtain that for the minimizing p , p Theorem 2 generalizes to the case when in addition to constra ining p y p in the theorem, but with p ( y ) minimizing RE ( p probabilities. When sets P y regularized maximum likelihood estimates in an exponentia l family (see, e.g., [3]): Corollary 3. If sets P y maxent are equivalent. The class-conditional densities  X  p ( x | y ) of the solution take form and solve single-class regularized maximum likelihood pro blems One-class Estimation. In one-class estimation problems, there are two classes ( 0 and 1 ), but we we only have access to presence records of the species. Based on labeled examples, we derive a set and RE ( X  p ME k  X   X  tional estimate  X  p ( y = 1 | x ) across all unlabeled examples x : so that  X  X ypical X  examples x under  X  p ME (examples with log probability close to the expected log probability) yield predictions close to the default. 2.3 Discriminative Training: Class-robust Logistic Regre ssion a combination of Theorem 1 and duality of maxent with maximum likelihood [3]. A complete proof will appear in the extended version of this paper.
 Theorem 4. Assume that sets P y equivalent. For the solution  X  p ,  X  p ( x ) =  X   X  ( x ) and  X  p ( y | x ) takes form and solves the regularized  X  X ogistic regression X  problem where  X   X  is an arbitrary feasible point,  X   X   X  P , and  X  y possibly an asymmetric one, since any feasible  X   X  will have |  X  y  X   X  all examples. We remark that the value of the objective of Eq. (10) does not depend on the choice of (influencing the second term) and these differences cancel o ut. To provide a more concrete example and some intuition about Eq. (10), we now consider one-class estimation.
 One-class estimation. A natural choice of  X   X  is the  X  X seudo-empirical X  distribution which views all unlabeled examples as negatives. Pseudo-empirical mea ns of class 1 match empirical averages of class 1 exactly, whereas pseudo-empirical means of class 0 can be arbitrary because they are uncon-Eq. (10) after multiplying by M then becomes discriminating positives from unlabeled examples. We evaluate our techniques using a large real-world dataset containing 226 species from 6 regions of the world, produced by the  X  X esting alternative methodol ogies for modeling species X  ecological niches and predicting geographic distributions X  Working G roup at the National Center for Ecological surveys or incidental records, including those from museum s and herbariums. The test set contains presence-absence data from rigorously planned independen t surveys (i.e., without labeling bias). The regions are described by 11 X 13 environmental variables , with 20 X 54 species per region, 2 X 5822 training presences per species (median of 57), and 102 X 1912 0 test points (presences and absences); for details see [4]. As unlabeled examples we use presences o f species captured by similar methods, known as  X  X arget group X , with the groups as in [13].
 We evaluate both entropy-weighted maxent and class-robust logistic regression while varying the separate optimization for each default prevalence.
 on features and tuning). Class-robust logistic models are c alculated by a boosting-like algorithm SUMMET [3] with the same set of features and the same value  X  as the maxent runs. Eq. (5). Note that the constant Bernoulli prediction has no d iscrimination power (its AUC is 0.5) even though it matches class probabilities perfectly. Figure 1: Comparison of reweighting schemes. Top: Test log loss averaged over species with given determine the range of default prevalence values that achie ve it.
 ate boosted regression trees (BRT), which have the highest p redictive accuracy along with maxent among species distribution modeling techniques [4]. In thi s application, BRT is used to construct a logistic model discriminating positive examples from unla beled examples. Recent work [17] uses a more principled approach where unknown labels are fitted by a n EM algorithm, but our preliminary runs had too low AUC values, so they are excluded from our comp arison. We train BRT using the of unlabeled examples, and then apply Elkan X  X  reweighting s cheme [5]. Specifically, the BRT result  X  p
BRT ( y | x ) is transformed to for two choices of p ( y ) : default, p ( y ) =  X  ( y ) , and entropy-based (using  X  p ME ). All three techniques yield state-of-the-art discriminati on (see [13]) measured by the average AUC: maxent achieves AUC of 0.7583; class-robust logistic regre ssion 0.7451 X 0.7568; BRT 0.7545. Un-yield different AUC for different default prevalence. Howe ver, log loss performance varies broadly according to the reweighting scheme. In the top portion of Fi g. 1, we focus on maxent. Naive weighting by default prevalence yields sharp peaks in perfo rmance around the best default preva-The improvement diminishes as the true prevalence increase s, but entropy-based weighting is never more sensitive. Thanks to smaller sensitivity, entropy-ba sed weighting outperforms naive weight-optimal default values are higher for entropy-based weight ing, because in one-class estimation the entropy-based prevalence is always smaller than default (u nless the estimate  X  p ME is uniform). BRT performing overall slightly better than maxent. Note th at entropy-reweighted BRT relies both on BRT and maxent for its performance. A striking observatio n is the poor performance of class-To correct for unknown labeling bias in training data, we use d robust Bayesian decision theory and developed generative and discriminative approaches that o ptimize log loss under worst-case true class proportions. We found that our approaches improve tes t performance on a benchmark dataset for species distribution modeling, a one-class applicatio n with extreme labeling bias. Acknowledgments. We would like to thank all of those who provided data used here : A. Ford, CSIRO Atherton, Australia; M. Peck and G. Peck, Royal Ontari o Museum; M. Cadman, Bird Stud-Databank and the Allan Herbarium, New Zealand; Missouri Bot anical Garden, especially R. Magill and T. Consiglio; and T. Wohlgemuth and U. Braendi, WSL Switze rland.

