 Graph search, i.e., finding all graphs in a database D that contain the query graph q , is a classical primitive prevalent in various graph database applications. In the past, there has been an abundance of studies devoting to this topic; however, with the recent emergence of large information net-works, it places new challenges to the research community.
Most of the traditional graph search schemes utilize the strategy of graph feature based indexing, whereas the in-dex construction step that often involves frequent subgraph mining becomes a bottleneck for large graphs due to the high computational complexity. Although there have been several methods proposed to solve this mining bottleneck such as summarization of database graphs, the frequent sub-graphs thus generated as indexing features are still unsatis-factory because the feature set is in general not only inade-quate or deficient for the large graph scenario, but also with many redundant features. Furthermore, the large size of the graphs makes it too easy for a small feature to be contained in many of them, severely impacting its selectivity and prun-ing power. Motivated by all the above issues we identify, in this paper we propose a novel CP-Index ( Contact Preser-vation ) for e ffi cient indexing of large graphs. To overcome the low selectivity issue, we reap further pruning opportuni-ties by leveraging each feature X  X  location information in the database graphs. Specifically, we look at how features are touching upon each other in the query, and check whether this contact pattern is preserved in the target graphs. Then, to tackle the deficiency and redundancy problems associated with features, new feature generation and selection methods such as dual feature generation and size-increasing bootstrap-ping feature selection are introduced to complete our design. Experiment results show that CP-Index is much more e ff ec-tive in indexing large graphs.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing methods Algorithms, Performance, Experimentation Graph database, Graph indexing, Graph querying
In recent years, given the expressiveness of graph, we have seen an ever increasing usage of graphs in representing com-plicated structures and schemaless data, such as proteins, software flows, social networks, and the Web. In many graph-related database applications, a common and criti-cal primitive is graph search, i.e., given a graph database D = { g 1 ,g 2 ,...,g n } and a graph query q , find all graphs in D that contain q as a subgraph. Obviously, it is ine ffi cient to perform graph search by checking subgraph containment between the query q and each of the database graphs g i  X  D , because subgraph isomorphism is NP-complete and it has to be executed for all n entries in the database D .

This is where the graph search index comes in with graph feature based indexing being the key idea [8, 25, 3, 23, 28, 20, 29, 26, 27]. To summarize, during o ff -line index con-struction, a set of indexing features are selected carefully. For each feature f , we build an index which points to those database entries that contain f . When an on-line query q is issued, a graph g i in the database can be safely pruned without performing any subgraph isomorphism checking, if feature f is contained in q ( f  X  q ) but is not contained in g ( f  X  g i ). Suppose a good set of features is indexed, the graph feature based scheme described above has been shown to result in a significant query processing speed-up.
However, nowadays more and more graph data come with a large size, and the need to perform search on such databases is urgently expected. For instance, a software system call graph describes the dependencies among various system call procedures in a software package, and one may want to de-termine which programs have been contaminated by viruses or security holes by performing a graph search. Unfor-tunately, as we encounter more real examples, traditional graph feature based indexing, originally designed for graphs with moderate size such as those representing chemical com-pounds, do not fit well in the large graph scenario.
To start with, generating an initial set of indexing features is a bottleneck for the case of large graphs. Historically, people have been using frequent subgraphs for indexing fea-tures which were shown to provide satisfactory performance. However, the time complexity of subgraph isomorphism, the core routine of any frequent subgraph mining algorithms, grows exponentially with the graph size. As shown in [9], all the representative graph indexing schemes are tested for databases of small graphs only. Though not aimed at index-ing, there exist several studies to conduct mining on large graphs, e.g., the one using a summarization technique for frequent subgraph mining [1]. But even that, directly using frequent subgraphs thus mined will not lead to satisfactory indexing performance as will be shown in the experiments of Section 6, with the major reasons analyzed as follows.
The first obstacle we are facing is that the query process-ing in the large graph scenario may be less e ffi cient. Con-sider the following example in Figure 1, given a query q that contains f 1 and f 2 , any graph g i  X  D not indexed by f ( j =1 , 2), i.e., f j  X  g i , can be safely pruned. However, as the database graph g i becomes larger, more diverse in-ternal structures are added. Consequently, it becomes more and more likely for any small size feature f j to appear in g , limiting f j  X  X  selectivity and generating much more can-didates that require direct verification against q . The root cause behind this ine ffi ciency is that, during query process-ing, di ff erent features, e.g., f 1 and f 2 , are always treated separately. Looking at Figure 1, the only instance of f 1 g is at its top-left corner, while the only instance of f 2 the bottom-right. Treating f 1 and f 2 separately leads to the conclusion that g i is a candidate containing q since f 1 and f 2  X  g i , and traditional graph feature based indexing inevitably breaks down here. But does that really mean no pruning opportunities? For the situation shown in Figure 1, we are sure that q is not a subgraph of g i ; otherwise, it would imply a non-disjoint pair of f 1 and f 2 appearing in g , and this is contradicting to what we observe. So, inter-estingly, if we consider f 1 ,f 2 in an interdependent context and remember the way they combine into the full query q , then it provides important hints as to whether g i is a valid candidate containing q . Essentially, the relative positioning of features in g i has to be exactly the same as that in q . Figure 1: The Concept of Contact Preservation
Apart from the low selectivity issue above, another disad-vantage of conventional methods is its inability to provide e ff ective indexing features. There are two issues here: in-adequacy and redundancy. When mining indexing features from large graphs, we want to set the support level some-what high to curb the computational complexity. Because of the high support, this kind of features cannot prune much as they tend to be small and focus more on the commonalities between di ff erent graph instances. We need a mechanism to consider the potentially substantial di ff erences among large graph instances. Meanwhile, given the large size of graphs, it is quite possible that we end up with an explosive set of frequent subgraphs that are quite similar to each other in terms of their indexing utilities. This introduces a huge amount of redundancy.

The key contributions of this paper are outlined next. We are the first to identify the three major issues associated with direct indexing using frequent patterns mined from large graphs (say using the summarization technique): low selectivity, inadequacy and redundancy. Towards the reso-lution of these issues, we propose a novel indexing scheme: CP-Index ( C ontact P reservation Index), which consists of contact preservation filtering , dual feature generation , and size-increasing bootstrapping feature selection to tackle each of them, respectively. Our newly designed CP-Index frame-work is depicted in Figure 2.

The key concept behind our scheme is leveraging con-tact preservation among features for more e ff ective index-ing. Di ff erent from traditional schemes, we record every fea-ture X  X  locations of occurrences in the database graphs o ff -line and check their relative positioning during querying time to make sure that, if two features touch upon each other in a query (e.g., they share some vertices/edges in some partic-ular way), then the same contacts are preserved in any true candidate graph in the database. In another word, these contacts work just like nails, which can correctly glue mul-tiple features together and form a valid occurrence image of the query. Fortunately enough, with contact preservation, we alleviate the need to mine larger subgraph features in search for higher selectivity.

To account for the potentially large variabilities among graphs because of their big size, a new dual feature gen-eration method is proposed to address the inadequacy is-sue. In addition to generating shared features common to the whole database, we decide to mine graph-specific fea-tures that are particular to one single graph because they o ff er better query indexing for individual database entries. Furthermore, regarding the potentially explosive size of the subgraph feature collection, we introduce a strategy called size-increasing bootstrapping to select an optimal subset for index construction, which directly uses the CP-Index query processing framework to decide whether it pays o ff to index an additional feature. Note that our size-increasing boot-strapping strategy is general enough to handle both shared and graph-specific features.

The rest of this paper is organized as follows. Preliminary concepts are reviewed in Section 2. Then the overall index-ing and search framework utilizing contact preservation is outlined in Section 3. We focus on indexing feature gener-ation and selection in Section 4, with Section 4.1 on dual feature generation and Section 4.2 on size-increasing boot-strapping. In Section 5 we discuss some practical aspects of implementing CP-Index, while Section 6 presents exper-imental results. Related work are listed in Section 7, and Section 8 concludes this study.
In this paper, we use the following notations: For a graph and l is a label function mapping a vertex or an edge to a label.

Definition 1. (Subgraph Isomorphism). For two labeled graphs g and g ,a subgraph isomorphism is an injective function f : V ( g )  X  V ( g ) ,s.t.,: first,  X  v  X  V ( g ) ,l ( v )= and l ( u, v )= l ( f ( u ) ,f ( v )) ,where l and l are the labeling functions of g and g , respectively. Under these conditions, f is called an embedding of g in g .

Definition 2. (Subgraph and Supergraph). If there ex-ists an embedding of g in g ,then g is a subgraph of g , denoted by g  X  g ,and g is a supergraph of g .

Definition 3 (Graph Search Problem). Given a graph database D = { g 1 ,...,g n } and a graph query q ,find all graphs g i in D ,s.t., q  X  g i .

As can be seen, the problem definition is exactly identical to normal graph search tasks discussed in the past, except that the graphs we look at have much larger size, usually orders of magnitude larger than those targeted in previous studies, e.g., one typical dataset people used for testing is the graph structures of chemical compounds, which have around 20 vertices and 20-30 edges on average.
As mentioned in the introduction, we propose utilizing the contact preservation property of features to test their compatibility with each other and explore further pruning opportunities, in an attempt to augment feature selectivity. This requires us to first pre-compute each feature X  X  locations in the database graphs, so that its position relative to others can be quickly assessed during on-line query processing.
Given a feature f and a graph g i in the database D ,an embedding of f in g i records one location where f exists in g . We can surely store every embedding of f in g i before queries come in, but in order to keep things compact, we decide to summarize the full information as follows. Suppose f has t vertices, i.e., V ( f )= { v 1 ,v 2 ,...,v t } ; for each v V ( f ), we store a list of vertices in g i which v k can be mapped to. Denote this list as L k f ( g i )  X  V ( g i ), where 1  X  k  X  t = | V ( f ) | , then for each vertex u  X  L k f ( g i ), there must exist a subgraph isomorphism from f to g i under which u is the image of v k  X  V ( f ). In the following, we shall refer to L ( g i ) as the CP-Index of node v k in g i , the set { L k V ( f ) ,k =1 , 2 ,...,t } as the CP-Index of feature f in g Similarly, when all graphs in D are considered, we will obtain the CP-Index of a node or a feature in the whole database.
In Figure 3, feature f is a clique with 6 vertices, while database graph g i happens to have a clique with the same size in it. All vertices in f and g i have the same label. If we record all embeddings of f in g i , there will be 6! = 720 embeddings, and to store each of them, a list of 6 IDs is needed, e.g., ( u 2 ,u 5 ,u 6 ,u 3 ,u 1 ,u 4 ). In comparison, any of v ,v 2 ,...,v 6 has a CP-Index of so in total the space saving of CP-Index is 720  X  6 6  X  6
Let us use an example to demonstrate how the CP-Index could help graph search query processing. Looking at Figure 4, which is redrawn based on Figure 1, if f 1  X  g i , f 2 then the traditional graph feature based indexing paradigm cannot prune g i based on the respective containment rela-tionships between f 1 or f 2 and g i . However, with CP-Index, we know that if the query q is contained by g i , then the sub-graph isomorphism that embeds q in g i must map f 1  X  X  3rd vertex and f 2  X  X  2nd vertex to the same vertex u in g i ,i.e., v  X  V ( q ). The same holds for f 1  X  X  1st vertex and f 2 vertex. Based on this non-empty requirement, g i can now be easily pruned. With f 1 only existing at the top-left corner of g i and f 2 only existing at the bottom-right corner, the CP-Index of v 3 in f 1 is while the CP-Index of v 2 in f 2 is The two intersect to  X  . This clearly shows that no valid embedding of query q should exist in the target graph g i Now we are ready to formally introduce our proposed CP-Index framework. It consists of three steps: index con-struction, search and verification, with enhanced capabilities compared to traditional schemes. 1. O ff -line index construction : Generate and select a 2. Search : Test indexed features in F against the query 3. Verification : Directly check subgraph isomorphism
Before moving forward, let us spend a bit more time on each of the steps above. For index construction, we have as-sumed that a feature set F is already available, after which the CP-Index of each feature f  X  F are computed and stored. However, generating such an F is never a straight-forward task. We will come back to this topic in Section 4 for a more detailed discussion.

To make the framework clear, we have intentionally omit-ted elaborated descriptions in the search step. As can be seen, the first phase of this step is identical to traditional feature based indexing, but since for each f  X  F , we have further saved the CP-Index of f in g i instead of simply a graph ID i that indicates f is a subgraph of g i , more pruning possibilities exist. The whole procedure of leveraging CP-Indices during query processing is outlined below.  X  After getting the set of features contained in q ,welocate  X  Considering all these mappings for all features f  X  F q  X  After multiple intersections, the list mentioned above
Alternatively, we may understand this pruning strategy as follows. Given a query q , we leverage the location informa-tion entailed in the CP-Indices of features in F q to pinpoint q  X  X  location in the graph g i . In a word, the contacts made among features in F q must be fully preserved, as otherwise, they cannot combine into a full query graph q in g i . This contact preservation property among features is checked by examining the compatibility of their associated CP-Indices. Now, every vertex v  X  V ( q ) works like a nail that glues mul-tiple indexed features together: It nails down their relative positioning, which must be held intact as we try to embed q in g i . This is why we could intersect relevant CP-Indices to narrow down the possibilities of mapping q  X  X  vertices to g  X  X , and also prune g i if the intersection result is empty.
Finally, verification works as a last resort by directly exe-cuting a subgraph isomorphism test when contact preserva-tion based query processing still could not prune a candidate graph out of consideration. Hopefully, at the time of veri-fication, the candidate set has been shrunk to a very small portion of the initial database. Note that, for any remain-ing candidate say g i , the contact preservation property can only yield a non-empty subset of vertices for each node v in the query q , denoted as V v  X  ( g i )  X  V ( g i ), which points to all possible nodes in g i which v can map to. We call V v  X  ( g i )as v  X  X  location list in g i . Interestingly, even though the location lists are not null, they still prune away lots of possibilities. In fact, now the subgraph isomorphism test routine could limit its scope by only mapping v to some vertex in V v  X  ( g i ), instead of trying all possible vertices in V ( g i ) without any guidance. In other words, the pruning we have achieved in the search step can to a large extent cut down the computation time that has to be spent in the ver-ification step. Considering an extreme situation, if the set V v  X  ( g i ) for every v  X  V ( q ) contains one single vertex, then there is only one probable way to embed q in g i by mapping each v  X  V ( q ) to that single vertex, and our verification pro-cedure just needs to test this sole potential embedding and verify whether it is true or not.
As we pointed out in the introduction, using frequent sub-graphs as indexing features on a database of large graphs is inflicted by the inadequacy and redundancy issues, even if summarization techniques could help reduce the time com-plexity to mine out an initial feature set. In this paper, we introduce two novel concepts to address these two problems in separate steps: 1) dual feature generation , which summa-rizes the database graphs and then mines both shared and graph-specific features to tackle the inadequacy issue, and 2) size-increasing bootstrapping , which treats the output of step 1 as an initial set, and then performs contact preserva-tion based redundancy analysis to select an optimal subset for index construction. The details of both steps will be given in Section 4.1 and Section 4.2, respectively.
Given the big graph size, most frequent graph pattern mining algorithms would experience a hard time to produce results e ffi ciently due to the high time complexity of sub-graph isomorphism. To solve this dilemma, the authors of [1] proposed a way to summarize graphs in the database by randomly merging groups of nodes with identical labels and collapsing respective edges. With the graph summarization technique, it is feasible to obtain an initial frequent subgraph set F 0 in a given database with large graphs. However, un-fortunately, using this initial set F 0 to build indexing for large graphs as in traditional graph feature based schemes will not lead to satisfactory indexing performance. One is-sue that might stem from it is that, we may have to set the minimum support threshold min sup somewhat high as compared to the regular settings in previous works. Since small structures are very easy to be contained by a large graph, there would be too many patterns if the support threshold is low. Now, adjusting min sup upwards defi-nitely makes the mining much easier, but it also negatively influences the pruning power of obtained features, because they are contained in many entries of the database and thus not very selective. In Section 3, we proposed to leverage contact preservation properties and alleviate this issue by considering multiple features inter-dependently during query processing. Here, we further suggest another round of graph-specific mining to directly tackle the low selectivity problem.
As a large graph o ff ers the opportunity to use its own subgraph features to improve pruning, graph-specific mining generates a feature set specific to a single graph g i in the database D . Considering a g i -specific feature, we simply compute and store its CP-Index for only one graph g i .With the additional graph-specific indexing, this is referred to as the dual feature generation approach. Now, suppose we have a shared feature set F D for the database and n graph-specific features sets F g i for each g i  X  D (details about how F F process of a query graph q proceeds as follows. 1. Shared feature indexing. With the index on shared 2. Graph-specific feature indexing. For each remain-
As one could easily see, this query processing procedure is very similar to what we outlined in Section 3, except that it works in two phases and the pruning only happens for g (instead of the whole database) when doing g i -specific fea-ture indexing. Back to how graph-specific features should be mined in the first place, we still leverage graph summariza-tion techniques, transform g i to its summary s i and mine the database { s i } with min sup = 100%. Strictly speaking, we are only borrowing frequent pattern mining algorithms here to extract subgraphs of g i for use as indexing features, because every subgraph of g i would have a support of 100% in such a single-entry database. So, to avoid pattern explo-sion, we decide to only find patterns up to maxL edges and terminate the mining process beyond. Meanwhile, one can also repeat the summarization-mining workflow for multiple rounds, i.e., transform g i to s 1 i ,s 2 i ,... and merge the result patterns obtained on { s 1 i } , { s 2 i } ,... . Since summarization may cause pattern loss, i.e., a pattern in g i might not neces-sarily be a pattern in s i , it will be exponentially harder for a g -pattern to miss from the output of all mining rounds given that the merging/collapsing of nodes/edges during summa-rization is fully randomized. Consequently, patterns found on summarized graphs can go arbitrarily close to the true ones, which is desirable for indexing purposes. At last, when it is appropriate, either early termination after maxL edges or repeated mining can also be applied in generating the shared feature set for indexing the whole database.
To make things compact, we do not want to keep every-thing output from step 1 in the index. Suppose the initial feature set is F 0 , in this section, we are going to perform redundancy analysis and select a non-redundant set of fea-tures F  X  F 0 for CP-Index construction. The result set should preserve the utility of F 0 to the largest extent. To-gether with the query processing procedure discussed pre-viously, this completes our indexing and search pipeline on large graphs.

As the name implies, if a feature f is redundant with re-gard to a feature set F , it means that most of its utility can be well substituted by those already in F . Referring to our query processing procedure, maintaining f in the index allows us to use f  X  X  CP-Index and directly point to the lo-cations where f occurs in the database graphs. How can we not index f but still achieve almost the same benefit? For an on-line query q , feature f is only useful when f  X  q . Due to the chain e ff ect, each f  X  F, f  X  f must be contained in q , as well, implying that f  X  X  subgraphs in the feature set F can also be used when searching q in the database. Now, a natural question to ask is, can the utility of f be substituted by the set { f | f  X  F, f  X  f } ?
Keeping f out of the index, the impact is low if we can re-construct f  X  X  CP-Index with little e ff ort during on-line query processing. Note that, with an existing set of features F ,we do not have to compute from scratch the locations where f can appear in a target graph, but rather with the help of CP-Indices maintained for { f | f  X  F, f  X  f } , which is exactly what we are going to do when searching f on the graph database D . So, interestingly, we could reuse the search framework proposed above to help decide an opti-mal set of features. This is referred to as the size-increasing bootstrapping approach of feature selection.
 Definition 4. (Redundant Feature). Given a database D ,anexistingfeatureset F and another feature f ,treat f as the query and perform a search of it in D ,with F being the set of indexed features. Now, for each candidate graph g i obtain a set of location lists for vertices in f by intersecting the CP-indices of features in F .Comparingwiththetrueset of f  X  X  location lists in g i (i.e., f  X  X  CP-Index), we say that f is redundant with regard to F on graph g i ,ifthesetwosets are very similar to each other.

There are many possibilities of defining the above simi-larity between location lists and aggregating the metric over all graphs in D , which is the reason why we did not want to stipulate a particular way in Definition 4. In this pa-per, we choose to use the following specification for redun-dancy. Referring back to the end of Section 3, we used V v  X  ( g i ) ,v  X  V ( f ) to denote the location list of v in a tar-get graph g i . Multiplying the length of all location lists for f , we can model the search space of testing whether f is a subgraph of g i as because a potential embedding of f in g i may result from taking one vertex from each of the location lists and using them to comprise a full image of f . If this metric of search space size is further aggregated over the whole database D , we have to model the cost of searching f in D . Note that, if there exists some v  X  V ( f ) such that V v  X  ( g i )=  X  , i.e., the lo-cation list of v in g i is empty after pruning, then the cost entry corresponding to g i is 0, which is inline with our index-ing framework because it means that CP-Index can directly remove g i from the candidate set without checking further.
Obviously, the cost of searching a query in the database depends on the composition of available indices. So, we want to write Cost( f )asCost F ( f ) to signify that F is feature set that has been indexed. We now define the redundancy of a feature f with regard to F on database D as meaning that if the cost of searching f in D decreases a lot after f is integrated into the index, i.e., Cost F f ( f ) and thus R D ( f, F ) is small, then the redundancy of f is low and it is worthwhile to index f in addition to F .

One might have already noticed that Cost F f ( f ) cannot be larger than Cost F ( f ), and thus R D ( f, F )  X  1. This is because, if the index includes f , then the pre-computed CP-Index associated with f would directly point to the true lo-cation lists of searching f in D , which cannot be shrunk any further. In comparison, Cost F ( f ) refers to a larger search space as we need to use features that are: 1) already indexed in F , and 2) contained by f , to deduct f  X  X  location lists. Fi-nally, the ratio between Cost F f ( f ) and Cost F ( f )isagood measure of how redundant f is if one wants to merge it into the indexed feature set.

A formal description of the feature selection process is given in Algorithm 1, where we sequentially go through the set of available features and only retain those non-redundant ones. Note that, since calculating the redundancy of f with regard to F requires all features in F that are subgraphs of f , we want to follow a predetermined order in traversing F 0 so that subgraphs are always visited before supergraphs. This could be easily achieved by checking features that have a smaller number of edges first, because a feature with e +1 edges cannot be contained in a feature with e edges for sure.
As proposed in Section 4.1, in order to enhance index performances, apart from a set of features shared among all graphs in the database D , we want to pick some graph-specific features for each g i  X  D . Interestingly, Algorithm 1 is general enough to handle dual feature generation. The only di ff erence is instead of starting from the empty set, we will use F = F D in line 1 of Algorithm 1, because graph-specific features are picked on top of shared features. Algorithm 1 Size-Increasing Bootstrapping Input: Graph database D , Initial feature set F 0 , Output: A subset of selected features F  X  F 0 .
 Feature Selection( D , F 0 , r ) { 1: F =  X  ; 2: for each f  X  F 0 do 3: Search f in D , obtain its location lists, with F indexed; 4: Calculate f  X  X  true location lists as well as R D ( f, F ); 5: if R D ( f, F ) &lt;r then 6: F = F f ; 7: Integrate f and its true location lists (i.e., CP-Index); 8: return F ; }
In this section, we want to describe how the CP-Index storage is organized. In fact, we could store all indexed features in a tree-like structure: If feature f 2 is a supergraph of feature f 1 , then f 2 is linked under f 1 as one of its direct or indirect children, just as what Figure 5 depicts. With such an indexing feature tree T , the CP-Index of each feature f is stored at the node corresponding to f on T . Since many frequent subgraph mining algorithms, such as gSpan (which is the one we used for experiments), maintain a pattern tree during the mining process, it is often very easy to obtain an initial feature tree as the by-product of mining. After that, we can run Algorithm 1 to filter those redundant tree nodes, leading to the final indexing tree. Note that, since Algorithm 1 is based on size-increasing bootstrapping, one only needs to pass the initial tree level by level, so that smaller features are visited before larger ones.

Both shared and graph-specific features are stored and organized in the way mentioned above, and their respective indexing feature trees are treated similarly during query pro-cessing. When a query q comes, we do a post-order traversal (i.e., parent node is visited after its children) on the index-ing feature tree T to obtain those largest features contained in q . As shown in Figure 5, we get a boundary in T where each node on it is a subgraph of q and all of their children are not. These boundary features comprise the set F q  X  F we use in the search step of Section 3 X  X  framework, whose CP-Indices would help us look for q in the database. There are several reasons for doing this here.  X  We can stop traversing a feature f  X  X  direct or indirect  X  Larger features must have better selectivity, so for ex- X  Having larger size, features may overlap to a larger de-
Interestingly, our CP-Index framework also makes the in-dex we construct a naturally incremental one. First, the deletion of a graph from the database is very easy to handle: Without additional e ff orts, we only need to retain the same set of shared features and their associated indices on the re-maining graphs, together with respective graph-specific in-dices, and all future queries would be handled in exactly the same manner. On the other hand, when new graphs are inserted, we keep the same set of shared features and ex-tend their indices to new graphs by computing correspond-ing CP-Indices on them; meanwhile, Algorithm 1 is executed to add graph-specific features for these new database entries, as well. Usually, the pattern distribution of new graphs is pretty similar to those existing ones since they belong to the same database, so it is expected that the shared fea-ture set picked previously should continue to work well after insertions take place. Moreover, even if some of the new entries are significantly di ff erent, the graph-specific features selected for them can to a large extent compensate for the possible negative e ff ect on query performance. This will keep the built index running e ffi ciently for a long time before we need to decide a new shared feature set and reconstruct ev-erything from scratch. In this section, we will provide empirical evaluations of CP-Index. We have two kinds of databases with graphs of large size: a real program analysis database and a synthetic database. We note that there is no indexing algorithm dedi-cated to indexing a database with large-sized graphs because of the computational complexity involved. Consequently, for comparison purposes, we choose a baseline that follows the framework of traditional graph feature based indexing but conducts mining on summarized database graphs [1] to gen-erate the indexing feature set. To be more specific, in the baseline algorithm, we first summarize each database graph for o ff -line feature selection and index construction, and then apply normal filtering and verification procedures [25] for on-line query processing. This method is referred to as direct indexing on summarized graphs (DI Summarization). All experiments are done on a Debian GNU/Linux server with two dual-core Xeon 3.0GHz CPUs and 16GB main memory, and the program is written in C++. Program Analysis Data. Program dependence graphs appear in software-security applications that perform char-acteristic analysis of malicious programs [6]. In a depen-dence graph, vertices are di ff erent types of system calls and edges representing the dependency relationships among them. For example, an edge with label y = f ( x ) between two ver-tices v 1 and v 2 means that the system call at v 1 assigns the variable x and the system call at v 2 uses the variable y whose value is derived from x .

Graph search is an important analytical task to perform on such system call graphs. It can reveal latent security holes if we search the database using a known malicious pattern. Our testing data has 238 graphs, with an average size of around 300 vertices and 600 edges. In addition, following what traditional schemes did, queries are randomly drawn from the database graphs as connected subgraphs. Note that, the size of the graphs here is 15-20 times larger than those in a typical chemical compound dataset that was used to test various traditional indexing methods. Also, with 238 graphs in the database, it might seem small at a first glance. In fact, this is to keep our experiment more manageable. Given the large size of graphs, many involved graph rou-tines such as isomorphism checking would understandably be much slower; moreover, as we show later in the scalabil-ity analysis of Figure 10, CP-Index has a linearly scalable trend, which means that experiment results achieved on a smaller database is representative, as well. (a) High Support Queries
We set parameters in our algorithm as follows. For min-ing, we summarize each graph to 20 vertices, i.e., the com-pression ratio is about 300 20 = 15, and min sup is set to 50%. To avoid pattern explosion, we only mine features up to maxL = 4 edges, and later we shall have an experiment for the synthetic data (Figure 12(b)) to show how this pa-rameter maxL would impact actual indexing performance. Finally, the redundancy thresholds is r =0 . 5.

Figure 6(a) and Figure 6(b) well demonstrate the useful-ness of CP-Index. We generate five groups of queries, with query size ranging from 20 to 60 (in terms of number of edges), which is drawn on the x -axis. We then process all these queries, and report the average candidate answer set size for each of the groups on the y -axis. Since it repre-sents the number of time-consuming subgraph verification tests that need to be executed, we have chosen to use the candidate set size for performance gauging. To set up an objective standard concerning the index pruning power, we also draw a line with label  X  X ctual Match X  in the figures, which displays the number of true answer graphs that really contain the input query. Since no index could/should prune away any true answer graph, the  X  X ctual Match X  line is a firm lowerbound for all indexing schemes.

We divide all queries into two categories: high support and low support. If a query has an  X  X ctual match X  number that is greater than 5, then it belongs to the high support category. Figure 6(a) and Figure 6(b) display the candidate answer set size for high support and low support queries, respectively. As can be clearly seen, even with shared fea-tures only, CP-Index is already much better than DI Sum-marization; if we further use both types of features, then the candidate set achieved by CP-Index becomes nearly indis-tinguishable from the true answer set. This is regardless of whether the query is small or large and whether the query is frequent or not. It proves the comparative advantage of leveraging contact preservation properties and building CP-Indices, as the pruning power of DI Summarization methods severely degrades in the large graph scenario.

Figure 7 depicts the query performance in a di ff erent way, where x -axis is the true query answer set size, i.e., its sup-port. Still, we could see a much better performance of CP-Index, lying very close to the theoretical optimum. The gap between CP-Index and DI Summarization narrows at the right end of the picture, mainly due to a constant increase of the  X  X ctual match X  number, which is the best our CP-Index can possibly achieve.

Directly watching response time measured in seconds could be a more intuitive way to understand the query answering cost of searching on large graphs. So let us take a look at Figure 8, where the average query response time for each group of queries is drawn against its size. Note that, one
Figure 7: Performance on Query Support Levels may think that the several hundred seconds on y -axis is a little too large; however, considering that at least for each of the  X  X ctual match X  we still need to run an isomorphism test for verification, it does not sound a big surprise since one might need tens of seconds to verify a large, complex graph. As can be seen, with DI Summarization, queries will take much longer to complete. Also, the gap between shared features only and dual features is more prominent here than in Figures 6(a) and 6(b). For example, when the query size is 20, indexing graph-specific features reduces the response time from 270 seconds to 45 seconds. Considering that only 34% more space is needed (1182 shared features, 405 graph-specific features), we can see that dual feature generation is a very cost-e ff ective way to promote index performance.
Lastly, we focus on the sensitivity to redundancy threshold r used in our size-increasing bootstrapping and the result is depicted in Figure 9. Here we set the query size to be 20. Figure 9(a) illustrates the impact of r on the number of output index features. As expected, the number of features decreases monotonously while r becomes smaller. When r is 1, the output set is exactly the same as the one we could get by performing dual feature generation step without any selection. On the other hand, as showed in 9(b), the size of the candidate set after filtering gradually increases when r decreases. It is a natural trade-o ff between the performance and the cost. However, the result of CP-Index is very close to the true answer set anyway no matter how small r is. For example, when r decreases from 1 to 0.6, the feature set size reduces almost six folds from 18,653 to 2,745, but the candidate set size remains nearly the same. Synthetic Generator Description. The synthetic graph generator follows a similar mechanism as the one used to generate itemset transactions, where we can set the number
Figure 9: Sensitivity to Redundancy Threshold of graphs ( D ), average size of graphs ( T ), number of seed patterns ( L ), average size of seed patterns ( I ) and number of distinct vertex/edge labels ( V/E ). To begin with, a set of L seed patterns are generated randomly, whose size is determined by a Poisson distribution with mean I ; then, seeds are randomly selected and inserted into a graph one by one until the graph reaches its size, which is the realization of another Poisson variable with mean T . Due to lack of space, we refer interested readers to [15] for further details.
The synthetic dataset we take is D200T400L100I50V4E1, i.e, 200 transactions with 400 vertices on average, which are generated by 100 seed patterns of average size 50; the num-ber of possible vertex/edge labels is set to 4/1. For scalabil-ity and index size tests in Figure 10 and Figure 11, we vary | D | , the number of graphs in the database, and generate mul-tiple datasets D( | D | )T400L100I50V4E1. Queries are gener-ated similarly as in the real data case. Algorithm parame-ters: Each graph is summarized to 20 vertices, i.e., the com-pression ratio is 400 20 =20; min sup = 70%, which is a bit higher than that in the real data, because the synthetic data generator produces graphs based on a pool of seed patterns, thus tending to have higher similarities among database en-tries; maximum indexed feature length is maxL = 5; redun-dancy threshold is r =0 . 5.
The scalability of CP-Index is shown in Figure 10. We vary the database size, i.e., the number of graphs inside, from 200, 400, ... , to 1,000, and measure the respective index construction time. As demonstrated, the implemen-tation is highly e ffi cient and our method is linearly scalable with regard to the data size. This is in strong contrast to state-of-the-art graph indexing methods. We tested gIndex, due to the mining bottleneck on large graphs, it cannot fin-ish in one day.

Figure 11 displays the index size (in terms of number of features being indexed). The same experiment setting as testing scalability in Figure 10 is used: We vary the num-ber of graphs from 200 to 1,000, and see how the index size responds. The observation is that the index size grows sub-linearly, definitely a desirable property when dealing with a large graph database. Also, we could see that the gap between  X  X ual Features X  and  X  X hared Features Only X  is not very large, meaning that the addition of graph-specific fea-tures does not cost too much index space indeed.
Figure 12(a) shows the performance of CP-Index on syn-thetic data. Trends depicted here are quite similar to those in the corresponding Figure 7 of real data empirical stud-ies. We also tested other datasets generated with di ff erent parameter settings and obtained similar results.

Fixing all other parameters, in Figure 12(b), we want to examine the impact of maxL , maximum indexed feature size, on the performance. Here, x -axis is maxL , y -axis is the candidate set size, and the queries we use have 40 edges. Both the  X  X P-Index X  and  X  X i Summaraization X  lines have a downward trend because the index pruning power increases when more features are considered. The X  X ctual Match X  X ine is horizontal. Interestingly, the performance of CP-Index seems to have almost reached its peak at maxL = 5, as fea-tures with at most 5 edges have provided enough index cov-erage, without resorting to larger ones. Even at maxL =4, CP-Index is already very close to  X  X ctual Match X , while  X  X i Summaraization X  lags behind by a large gap. This suggests from yet another angle the stronger pruning power of CP-Index and contact preservation.
Graph search is a basic primitive in the management and analysis of complex structured data, which has stimulated much interest on indexing techniques from the research com-munity. Most of the traditional graph indexing algorithms are graph feature based, as they follow the strategy of filter-and-verification. These methods achieved great success by exploiting di ff erent types of features and various verification strategies. GraphGrep [8] is the first work of this kind, which uses paths as indexing features. gIndex [25] makes a break-through by indexing frequent subgraphs of the database, o ff ering much better selectivity than paths. gIndex defines a framework representative of many later methods that use graphs as features, and provides very good performance in many di ff erent situations, according to the empirical stud-ies of [9]. Tree+Delta [28] only extracts tree fragments for indexing, since the cost of tree mining is much lower; in addi-tion, it also generates on-demand a small delta of graph fea-tures during query processing to compensate for the limited pruning power of trees. Regarding verification strategies, FG-Index [3] introduces a verification-free concept, while SwiftIndex [20] proposes an encoding plan based on the fre-quencies of vertices/edges, etc. to improve the e ffi ciency of subgraph isomorphism tests. Quite unfortunately, none of the methods here is suitable for the large graph scenario, because of the problems shared by all graph feature based indexing schemes as we mentioned in the introduction.
There are also graph indexing methods that are not filter-and-verification based. Williams et al. store all unique, in-duced subgraphs of the database graphs in a directed acyclic graph for query processing [23]. gCode [29] computes vertex signatures according to local information and adopts spec-tral graph coding to encode the topology of both database graphs and queries in a numerical space. Empirical studies of [9] show that the pruning of gCode is not very powerful for sparse datasets.

Compared to the easier setting that comprises a database of small-sized graphs, there has been much less research fo-cusing on large graph search and indexing. Similar works mainly focus on indexing on a single large graph rather than a database with many large graphs. GraphQL [11] leverages neighborhood subgraphs and vertex profiles for global and local pruning, respectively. GADDI [26] defines NDS (neigh-boring discriminating substructure) distance for every neigh-boring vertex pair in the graph and based on that develops some inequality property for candidate elimination. Both GraphQL and GADDI uses a vertex-at-a-time mechanism when searching for a query in the graphs; essentially, this is like using single vertices as indexing features, which might not be very selective compared to the subgraph features deployed in CP-Index. Making a step forward in this re-spect, [27] proposes a path-at-a-time query processing strat-egy. The low mining cost and more manageable structures all make paths very desirable indexing features. However, as we already pointed out in this paper, feature selectiv-ity is very important in the large graph scenario. Given their better selectivity, one might want to look at subgraph features if the mining and indexing complexity can be well controlled, just like what summarization techniques do in the construction of a compact and e ffi cient CP-Index.
Frequent subgraph mining algorithms exist in abundance, including [15], [24], [12] that work on a graph transaction database and [16], [2] that work on a single large graph, fol-lowed by many others. There are also works such as [17] that focus on the extraction of patterns with specific structural characteristics. Chen et al. suggest summarizing graph in-stances before mining them [1], which is inline with what we do in this paper. Other strategies have also been proposed to reduce either the pattern space [10] or the data space [21] by picking delegates. They are orthogonal to graph summa-rization, and can be integrated together to further accelerate the mining process.
The idea of leveraging summaries to facilitate data pro-cessing on graphs is also not new. For example, it has been used in XML data [18] and computing approximate person-alized PageRank [19], while [22] mines the dense sub compo-nents of a big network to compress and visualize its internal structures. Exploring the technique, these works inspired our design of a compact CP-Index, which nicely summarizes the features X  location information in database graphs.
Quite a few studies, e.g. [4], have been dedicated to the acceleration of subgraph isomorphism testing, which can def-initely help the verification step of CP-Index. Besides the containment queries discussed in this paper, there are works that try to handle other types of queries on graphs, e.g., key-word search [13], correlation search [14], reachability queries [5] and bounded graph simulation [7]. These are all of im-portant use and greatly enhance the primitives available for graph database management.
As a basic primitive, graph search is critical for the success of many graph database management applications. With the recent emergence of large-scale data sources, e.g., bio-logical networks, software programs, and social networks, in this paper, we reconsider the graph indexing problem within the context of large graphs. Given the large size of the tar-get, traditional indexing techniques will break down, and still do not perform well even after advanced strategies such as summarization are utilized to mine frequent subgraphs as indexing features. We identify three challenging issues for the feature set thus selected: low selectivity, inadequacy and redundancy, and propose a new indexing technique, CP-Index, to overcome them. First, a new graph search frame-work is presented, which leverages the contact preservation properties we introduced to enhance the selectivity of small features that have been severely impacted in the large graph scenario. Then, we use this search framework and develop a size increasing bootstrapping scheme to decide the utili-ties of individual features and pick a non-redundant subset of them, resulting in a compact but rather optimized graph index. Accounting for the much bigger variabilities among di ff erent graphs due to their size, we devise a dual feature generation approach to address the inadequacy issue, where our index consists of not only a feature set shared among all database graphs, but also multiple graph-specific feature sets. Experiments on real and synthetic large graph datasets confirm the e ffi ciency and e ff ectiveness of our method.
This work is supported in part by NSF through grants IIS 0905215, DBI-0960443, OISE-0968341 and OIA-0963278.
