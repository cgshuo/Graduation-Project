 What are the computational goals of the retina? The retina has numerous specialized classes of retinal ganglion cells (RGCs) that are likely to subserve a variety of different tasks [1]. An important class directly subserving visual perception is the midget RGCs (mRGCs) which constitute 70% of RGCs with an even greater proportion at the fovea [1]. The problem that mRGCs face should be to maximally preserve signal information in spite of the limited representational capacity, which is imposed both by neural noise and the population size. This problem was recently addressed (although not specifically as a model of mRGCs) in [2], which derived the theoretically optimal linear coding method for a noisy neural population. This model is not appropriate, however, for the mRGCs, because it does not take into account the noise in the retinal image (Fig. 1). Before being projected on the retina, the visual stimulus is distorted by the optics of the eye in a manner that depends on eccentricity [3]. This retinal image is then sampled by cone photoreceptors whose sampling density also varies with eccentricity [1]. Finally, the sampled image is noisier in the dimmer illumination condition [4]. We conjecture that the computational goal of mRGCs is to represent the maximum amount of information about the underlying, non-degraded image signal subject to limited coding precision and neural population size.
 Here we propose a theoretical model that achieves this goal. This may be viewed as a generalization of both Wiener filtering [5] and robust coding [2]. One significant characteristic of the proposed model is that it can make optimal use of an arbitrary number of neurons in order to preserve the maximum amount of signal information. This allows the model to predict theoretically optimal representations at any retinal eccentricity in contrast to the earlier studies [4, 6, 7, 8]. Figure 1: Simulation of retinal images at different retinal eccentricities. (a) Undistorted image signal. (b) The convolution kernel at the fovea [3] superimposed on the photoreceptor array indicated by triangles under the x-axis [1]. (c) The same as in (b) but at 40 degrees of retinal eccentricity. First let us define the problem (Fig. 2). We assume that data sampled by photoreceptors (referred to as the observation) x  X  R N are blurred versions of the underlying image signal s  X  R N with additive white noise  X   X  N (0 , X  2  X  I N ) , where H  X  R N  X  N implements the optical blur. To encode the image, we assume that the obser-vation is linearly transformed into an M -dimensional representation. To model limited neural pre-The noisy neural representation is therefore expressed as where each row of W  X  R M  X  N corresponds to a receptive field. To evaluate the amount of signal information preserved in the representation, we consider a linear reconstruciton  X  s = Ar where A  X  R N  X  M . The residual is given by where I N is the N -dimensional identity matrix, and the mean squared error (MSE) is with E = tr  X  T  X  by definition,  X  X  X  the average over samples, and  X  s the covariance matrix of the image signal s . The problem is to find W and A that minimize E .
 To model limited neural capacity, the representation r must have limited SNR. This constraint is we assume all neurons have the same capacity). It is expressed in the matrix form as where  X  x = H X  s H T +  X  2  X  I N is the covariance of the observation. It can further be simplified to Figure 2: The model diagram. If there is no degradation of the image ( H = I and  X  2  X  = 0 ), the model is reduced to the original robust coding model [2]. If channel noise is zero as well (  X  2  X  = 0 ), it boils down to conventional block coding such as PCA, ICA, or wavelet transforms. and  X  k are respectively the eigenvalues of H and  X  s , and the columns of E are their common eigenvectors 1 . Note that attenuation of the amplitude of the signal along the k -th eigenvector.
 Now, the problem is to find V and A that minimize E . The optimal A should satisfy  X  E / X  A = O , which yields where  X  2 =  X  2 u / X  2  X  (neural SNR), S s = diag (  X  noise; as we will see below,  X  k characterizes the generalized solutions of robust coding, and if there is neither sensory noise nor optical blur,  X  k becomes 1 that reduces the solutions of the current model to those in the original robust coding model [2]). This implies that the optimal A is determined once the optimal V is found.
 With eqn. 7 and 9, E becomes Finally, the problem is reduced to finding V that minimizes eqn. 10.
 Solutions for 2-D data In this section we present the explicit characterization of the optimal solutions for two-dimensional data. It entails under-complete, complete, and over-complete representations, and provides precise insights into the numerical solutions for the high-dimensional image data (Section 3). This is a generalization of the analysis in [2] with the addition of optical blur and additive sensory noise. From eqn. 6 we can parameterize V with where  X  j  X  [0 , 2  X  ) , j = 1 ,  X   X   X  ,M , which yields with  X  k  X   X  k  X  k and Z  X  P j (cos 2  X  j + i sin 2  X  j ) . In the following we analyze the cases when  X  the previous analysis of robust coding [2], these cases depend only on the ratio between  X  1 and  X  2 , i.e., the isotropy of the data. In the current, general model, these also depend on the isotropy of the to the individual cases.) 1). If  X  1 =  X  2 (  X   X  ) : E in eqn. 10 becomes Therefore, E is minimized when | Z | 2 is minimized. 1-a). If M = 1 (single neuron case): By definition | Z | 2 = 1 , implying that E is constant for any  X  1 , Because there is only one neuron, only one direction in the two dimensional space can be recon-structed, and eqn. 15 implies that any direction can be equally good. The first equality in eqn. 14 can be interpreted as the case when W represents the direction along the first eigenvector, and consequently, the whole data variance along the second eigenvector  X  2 is left in the error E . 1-b). If M  X  2 (multiple neuron case): There always exists Z that satisfies | Z | = 0 if M  X  2 , with which E is minimized [2]. Accordingly, where V is arbitrary as long as it satisfies | Z | = 0 . Note that W takes the same form as in M = 1 except that there are more than two neurons. Also, eqn.16 shares the second term with eqn. 14 except that the SNR of the representation  X  2 is multiplied by M/ 2 . It implies that having n times the neurons is equivalent to increasing the representation SNR by the factor of n (this relation generally holds in the multiple neuron cases below). 2). If  X  1 &gt;  X  2 : Eqn. 12 is minimized when Z = Re ( Z )  X  0 for a fixed value of | Z | 2 . Therefore, the problem is reduced to seeking a real value Z = y  X  [0 ,M ] that minimizes 2-a). If M = 1 (single neuron case): Z = Re ( Z ) holds iff  X  1 = 0 . Accordingly, These take the same form as in the case of  X  1 =  X  2 and M = 1 (eqn. 14-15) except that the direction resources (namely, one neuron) are devoted to the largest data variance direction. 2-b). If M  X  2 (multiple neuron case): From eqn. 18, the necessary condition for the minimun d E /dy = 0 yields The existence of a root y in the domain [0 ,M ] depends on how  X  2 compares to the next quantity, which is a generalized form of the critical point of neural precision [2]: 2-b-i). If  X  2 &lt;  X  2 c : d E /dy = 0 does not have a root within the domain. Since d E /dy is always negative, E is minimized when y = M . Accordingly, These solutions are the same as in M = 1 (eqn. 19-20) except that the neural SNR  X  2 is multiplied by M to yield smaller MSE. 2-b-ii). If  X  2  X   X  2 c : Eqn. 21 has a root within [0 ,M ] , with y = M if  X  2 =  X  2 c . The optimal solutions are where V is arbitrary up to satisfying eqn. 25.
 In Fig. 3 we illustrate some examples of explicit solutions for 2-D data with two neurons. The gen-eral strategy of the proposed model is to represent the principal axis of the signal s more accurately as the signal is more degraded (by optical blur and/or sensory noise). Specifically, the two neurons come to represent the identical dimension when the degradation is sufficiently large. Figure 3: Sensory noise changes the optimal linear filter. The gray (outside) and blue (inside) contours show the variance of the target and reconstructed signal, respectively, and the red (thick) bars the optimal linear filters when there are two neurons. The SNR of the observation is varied from 20 to  X  10 dB (column-wise). The bottom row is the case where the power of the signal X  X  while the top is without the blur: (  X  1 , X  2 ) = (1 , 1) . The neural SNR is fixed at 10 dB. We applied the proposed model to a natural images data set [11] to obtain the theoretically optimal population coding for mRGCs. The optimal solutions were derived under the following biological constraints on the observation, or the photoreceptor response, x (Fig. 2). To model the retinal images at different retinal eccentricities, we used modulation transfer functions of the human eye [3] and cone photoreceptor densities of the human retina [1] (Fig. 1). The retinal image is further corrupted by the additive Gaussian noise to model the photon transduction noise by which the SNR of the observartion becomes smaller under dimmer illumination level [4]. This yields the observation at different retinal eccentricities. In the following, we present the optimal solutions for the fovea (where the most accurate visual information is represented while the receptive field characteristics are difficult to measure experimentally) and those at 40 degrees retinal eccentricity (where we can compare the model to recent physiological measurements in the primate retina [12]).
 The information capacity of neural representations is limited by both the number of neurons and the precision of neural codes. The ratio of cone photoreceptors to mRGCs in the human retina is 1 : 2 at the fovea and 23 : 2 at 40 degrees [13]. We did not model neural rectification (separate on and off channels) and thus assumed the effective cell ratios as 1 : 1 and 23 : 1 , respectively. We also fixed the neural SNR at 10 dB, equivalent to assuming  X  1 . 7 bits coding precision as in real neurons [14]. The optimal W can be derived with the gradient descent on E , and A can be derived from W using eqn. 8. As explained in Section 2, the solution must satisfy the variance constraint (eqn. 6). We formulate this as a constrained optimization problem [15]. The update rule for W is given by  X  W  X   X  A T ( AWH  X  I N )  X  s H T  X   X  2  X  A T AW  X   X  diag where  X  is a positive constant that controls the strength of the variance constraint. Our initial results indicated that the optimal solutions are not unique and these solutions are equivalent in terms of MSE. We then imposed an additional neural resource constraint that penalizes the spatial extent of is the spatial distance between the j -th weight and the center of mass of all weights, and  X  is a positive constant defining the strength of the spatial constraint. This assumption is consistent with the spatially restricted computation in the retina. If  X  = 0 , it imposes sparse weights [16], though not necessarily spatially localized. In our simulations we fixed  X  = 0 . 5 .
 For the fovea, we examined 15  X  15 pixel image patches sampled from a large set of natural im-ages, where each pixel corresponds to a cone photoreceptor. Since the cell ratio is assumed to be 1 : 1 , there were 225 model neurons in the population. As shown in Fig. 4, the optimal filters show concentric center-surround organization that is well fit with a difference-of-Gaussian function (which is one major characteristic of mRGCs). The precise organization of the model receptive field changes according to the SNR of the observation: as the SNR decreases, the surround inhibition gradually disappears and the center becomes larger, which serves to remove sensory noise by aver-aging. As a population, this yields a significant overlap among adjacent receptive fields. In terms of spatial-frequency, this change corresponds to a shift from band-pass to low-pass filtering, which is consistent with psychophysical measurements of the human and the macaque [17].
 Figure 4: The model receptive fields at the fovea under different SNRs of the observation. (a) A cross-section of the two-dimensional receptive field. (b) Six examples of receptive fields. (c) The tiling of a population of receptive fields in the visual field. The ellipses show the contour of receptive fields at half the maximum. One pair of adjacent filters are highlighted for clarity. The scale bar indicates an interval of three photoreceptors. (d) Spatial-frequency profiles (modulation transfer functions) of the receptive fields at different SNRs.
 For 40 degrees retinal eccentricity, we examined 35  X  35 photoreceptor array that are projected to 53 model neurons (so that the cell ratio is 23 : 1 ). The general trend of the results is the same as in the fovea except that the receptive fields are much larger. This allows the fewer neurons in the population to completely tile the visual field. Furthermore, the change of the receptive field with the sensory noise level is not as significant as that predicted for the fovea, suggesting that the SNR is a less significant factor when neural number is severely limited. We also note that the elliptical shape of the extent of the receptive fields matches experimental observations [12]. Figure 5: The theoretically-derived receptive fields for 40 degrees of the retinal eccentricity. Cap-tions as in Fig. 4.
 Finally, we demonstrate the performance of de-blurring, de-noising, and information preservation by these receptive fields (Fig. 6). The original image is well recovered in spite of both the noisy representation (10% of the code X  X  variation is noise because of the 10 dB precision) and the noisy, degraded observation. Note that the 40 degrees eccentricity is subject to an additional, significant dimensionality reduction, which is why the reconstruction error (e.g., 34.8% at 20 dB) can be greater than the distortion in the observation (30.5%). Figure 6: Reconstruction example. For both the fovea and 40 degrees retinal eccentricity, two sensory noise conditions are shown ( 20 and  X  10 dB). The percentages indicate the average distortion in the observation or the reconstruction error, respectively, over 60,000 samples. The blocking effect is caused by the implementation of the optical blur on each image patch using a matrix H instead of convolving the whole image. The proposed model is a generalization of the robust coding model [2] and allows a complete char-acterization of the optimal representation as a function of both image degradation (optical blur and additive sensory noise) and limited neural capacity (neural precision and population size). If there is no sensory noise  X  2  X  = 0 and no optical blur H = I N , then  X  k = 1 for all k , which reduces all the optimal solutions above to those reported in [2].
 The proposed model may also be viewed as a generalization of the Wiener filter: if there is no channel noise  X  2  X  = 0 and the cell ratio is 1 : 1 , and by assuming A  X  I N without loss of generality, the problem is reformulated as finding W  X  R N  X  N that provides the best estimate of the original signal  X  s = W ( Hs +  X  ) in terms of the MSE. The optimal solution is given by the Wiener filter: (note that the diagonal matrix in eqn. 29 corresponds to the Wiener filter formula in the frequency domain [5]). This also implies that the Wiener filter is optimal only in the limiting case in our setting. Here, we have treated the model primarily as a theory of retinal coding, but its generality would allow it to be applied to a wide range of problems in signal processing. We should also note several limitations. The model assumes Gaussian signal structure. Modeling non-Gaussian signal distribu-tions might account for coding efficiency constraints on the retinal population. The model is linear, but the framework allows for the incorporation of non-linear encoding and decoding methods, at the expense of analytic tractability.
 There have been earlier approaches to theoretically characterizing the retinal code [4, 6, 7, 8]. Our approach differs from these in several respects. First, it is not restricted to the so-called complete representation ( M = N ) and can predict properties of mRGCs at any retinal eccentricity. Second, we do not assume a single, translation invariant filter and can derive the optimal receptive fields for a neural population. Third, we accurately model optical blur, retinal sampling, cell ratio, and neural precision. Finally, we assumed that, as in [4, 8], the objective of the retinal coding is to form the neural code that yields the minimum MSE with linear decoding, while others assumed it to form the neural code that maximally preserves information about signal [6, 7]. To the best of our knowledge, we don X  X  know a priori which objective should be appropriate for the retinal coding. As suggested earlier [8], this issue could be resolved by comparing different theoretical predictions to physiological data.

