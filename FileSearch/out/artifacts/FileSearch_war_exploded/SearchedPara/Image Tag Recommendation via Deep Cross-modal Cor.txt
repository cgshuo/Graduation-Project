
With the explosive growth of images available both online and offline, especially on some popular websites such as Flickr , how to explore the involved contents in images to achieve more effective image retrieval has become an important research focus. Usua lly, common users are allowed to annotate each image with a series of tags in accordance with their own tendencies. However, existing investigations reveal that only around 50% tags provided by Flickr users are indeed related to the images. The quality of tags is far from satisfactory due to the ambiguity, incompleteness and over -subjectivity. The main reason is that because of the semantic gap, there may exist huge uncertainty on the correspondence relations hips among visual contents and se-mantic tags. Thu s how to integrating multimodal information sources to enable the objective image tag recommendation has become a critical issue for supporting image retrieval.

The general image recommendation approach attempts to describe an image as a set of tags based solely on image contents [4]. However, tailoring the general image recommendation to image retrieval is challenging in two aspects: a) the semantic gap still largely exists, and annota-combine the enough semantics of images, as well as capture the multimodal association of dif-ferent modalities. In the general recommendation framework [8] , the missing of such important semantic information for image description m ay result in the huge uncertainty on the correspond-ence relations hips between visual contents and semantic tags, and the performance of retrieval accuracy, efficiency and effectiveness is not very ideal. Therefore, more efforts are put into ab-stracting ima ges in the deep level and integrating the semantic information from tags to form a novel expression with strong descriptive ability, so as to fully exploit the multimodal attributes in images and tags to support more precise tag recommendation and further improve the retrieval performance [1]. To achieve effective image tag recommendation, two inter -related issues should be addressed simultaneously: 1) the in -depth discovery of valuable multimodal features to char-acterize images and tags more reasonably; an d 2) the cross -modal correlation mining to identify better multimodal correlations between the visual features for images and semantic features for tags. To address the first issue, it is very important to leverage large -scale annotated images for robust v isual and semantic mining to achieve more comprehensive multimodal feature represen-tation. To address the second issue, it is very interesting to develop new algorithms for exploiting multimodal features and efficiently explore the cross -modal correlations between different mo-dality attributes in images and tags [13] .

Based on these observations above, a novel scheme is developed in this paper for facilitating automatic image tag recommendation to enable more effective image retrieval. Our scheme sig-nificantly differs from other earlier work in: a) The meticulous deeper representation for multi-modal feature is constructed to learn more representative attributes for visual images and seman-tic tags; b) The deep cross -modal correlation mining has a st rong ability to characterize the deep multimodal correlations between the visual features in images and semantic features in tags, which can alleviate the problem of semantic gap to a great degree; c) A novel image tag recom-mendation framework is built by fusing the deep multimodal feature representation and cross -modal correlation mining strategies, which enables the most appropriate and relevant tags to be presented on the image. Such an image tag recommendation pattern can be modeled as an inter -related correlation distribution over multimodal visual and semantic representations of images and tags, in which the most important is to create more effective cross -modal correlation and measure what degree they are related. Our experiments on a large number of public data have obtained very positive results.
T he Convolution Neural Network (CNN) is a feed -forw a rd artificial neural network in ma-chine learning [16] , which is biologically -inspired [7]. Recently deep convolution al neural net-works have demonstrated promising results in computer vison tasks, such as single -label image classification, image recognition, etc. We consider utilizing a popular deep neural network, Alex -Net , to extract CNN deep visual features [14] . Alex -Net is a typical convolutional neural network, which consists of 5 convolutional layers and 2 fully connected layers. It X  X  really a simple but high -efficient network. Given an image, we extract a 4,096 -dimensional feature vector using the pre -trained CNN on the ILSVRC -2012 dataset. We explore Alex layered architecture and use the Alex -Net features for all our experiments. In order to making faster training, we use the A uto -E ncoder (AE) strategy to reduce the dimension from 4,096 to 300. Hence, the dimensio n for the feature expression reduces 93% and the amount of information decreases 20%, thus some redun-dant information in image contents can be eliminated and the information representation ability can be also further improved.

T he Skip -gram model is an efficient method to learn the distributed representations of textual words in a vector space from large amounts of unstructured text data [1 5 ] . It learns deep word vector representations that are good at predicting the nearby textual words, which can captu re more precise syntactic and semantic relationships among textual word s and group similar textual words together . Thus we leverage the Skip -gram model to construct the deep text ual word for better represent ing the deep semantic properties in annotation ta g s.

Let D T be the text ual annotation part of the whole multimodal dataset , W denotes all the raw textual words in D T , and V is the textual word vocabulary. F or each textual word w in W , I w and O w are the input and output vector representations for w , Context ( w ) represents the nearby textual words of w , here the context window size is set as 5. We define the set of all the input and output the input or output vector, thus the objective function of Skip -gram can be described as: S ince the computing cost is extremely high for the standard softmax formulation of Skip -gram, the Negative Sampling is u tilize d to compute log P ( w j | w i ) approximatively . where  X  ( X ) is the sigmo i d function ; and m is the number of negative samples, each sample is drawn from the noise distribution P ( w ) based on the textual word frequency. The entire textual word vectors are clustered to acquire the new deep text ual word vocabulary, and then each word /tag in the annotation is projected to this vocabulary. Thus each annotation can be repre-advantage of deep text ual word is the consideration of the semantic relationship s among raw words, which makes the deep words more representative to describe textual annotation s , i.e., semantic tags .
To achieve more effective cross -modal image tag recommendation , we have developed a mul-timodal feature embedding scheme based on Deep Canonical Correlation Analysis ( D CCA ) [3] [9] [11] to exp loit multiple features of annotated images and explore the multimodal associa-tions between deep visual property features and semantic expression features . To make a clear presentation, we first introduce the gen er al C CA [16 ] , and then develop our D CCA -based multi-modal fe atu re embedding patte rn .

The standard CCA algorithm is a classic statistical method to multi -view and multi -scale anal-ysis for multiple data source s, which has received much attention in the field of cross -me-dia/cross -modal processin g [5] . It aims at finding linear projections for different types of data with the maximum correlation. Due to the limitation of standard CCA, the kernel representation is always integrated into CCA to increase the computation power of linear learning machi nes, that is, Kernel CCA (KCCA) [6] . As an extension of standard CCA, KCCA can provide a non-linear function learning method by projecting different types of data into a high dimension feature space. However, it X  X  still difficult for KCCA to find a better c orrelation between different types of data, because there are not simple linear or nonlinear relationships among real data.
To overcome the drawback of KCCA that the representation for real data is limited by the fixed kernel, deep networks can be integrat ed to learn flexible nonlinear representations, that is DCCA [ 2 ] [10] . Deep network is commonly used in the field of feature learning and classification, which plays a great role in learning a complex and nonlinear form to fit real data. DCCA with deep netw orks means a model with multiple hidden layers, which can simultaneously learn two deep nonlinear mappings of two views that are maximally correlated. The key difference between DCCA and other closely related approaches is that DCCA learn s two separate dee p encodings with the objective that the learned encodings are as correlated as possible, and such different objectives may have advantages in different settings. Thus DCCA, which could o btain more ac-curate highly abstract expression of real data via the co mplex linear and nonlinear transformation among layers, is introduced to make a better solution for image tag recommendation via cross -modal correlation mining .

For projecting multi -modal features to cross -modal features with D CCA, it aims at project ing th e multimodal features with different modalities on multiple views into a common subspace and mak ing sure that the correlation between visual features and semantic features could be maxim-ized. We illustrate our architecture in Fig . 1.

Let IMG be the set of annotated images that consists of N samples ; N D V V R X *  X  is the visual feature vector for IMG and N D S S R X *  X  represents the semantic feature vector , D V and D S are the corresponding dimensionality values for these two vectors, generally S V D D  X  . Taking these mul-timodal feature vectors as the input data of t he corresponding deep network s including the visual and semantic networks, that is, a V 1 = X V and a S 1 = X S . The visual and semantic networks ha ve D V and D S units in the input layers, V h and S h units in the hidden layers, and hold V o and S o layers in the final output layers respectively. Thus in each network, the feed -forward from the i th layer to the ( i +1) th layer can be defined as: w here W Vi and W Si represent the weight matrices between the i th layer to the ( i +1) th layer in two layers of two networks; and f is equivalent to a nonlinear transformation func t ion for th e data in the corresponding layer. Hence, the final matrices from the output layer in two networks can be implemented as F o rmula ( 4 ).
 where Vo H works respectively; and r V and r S are two regularization factors, which are usually set as small values to a void the numerically ill -conditioned problem.

T o find a projection relation ship between the visual feature space and semantic feature space that could maximize the correlation betwe en different feature views , the following F ormula ( 5 ) is adopted to achieve such a goal. output layer in the visual and semantic network respectively. Because the maximal correlation value is 1, to convert the above maximization problem into a usual minimization problem, For-mula (5 ) ca n be further slightly transformed into Fo r mula ( 6 ).
To optimize the correlation measure, the gradient -based optimization is introduced to achieve better training for the parameters of W and b . Suppose the singular value decomposition of T is T = UDV T , U and V are singular vectors, the gradient of correlation function can be computed with respect to Vo H Similarly, metric eigenvalue problem . Th us based on the matri ces of o o D D R Q P * ,  X  , we can embed the mul-timodal features ( i.e., visual feature H Vo and semantic feature H So ) into a common subspace that can generate the cross -modal correlation , as shown in Formula ( 8 ).

In fact, for visual features and semantic feature s involved in each annotated image, they be-long to different feature s paces with different dimensions . Our D CCA -based multimodal feature embedding for deep cross -modal correlation mining can p rovide a rela t ively perfect feature rep-resentation with associati ons between various features in different modalities , which can mitigate the problem of semantic gap to a certain extent and achieve better multimodal feature associations. 4.1 Dataset and Evaluation Metrics
The evaluation for image tag recommendation via deep cross -modal correlation mining re-quires an image collection with paired images and annotation texts. Thus our dataset is estab-lished based on three benchmark datasets of Corel 5 k , Corel 30 k and Nus -Wide . The first and sec-ond datasets are two subsets of the Corel database, which contain 5 , 000 images, 260 words or labels and 87 categories, and 31,695 images, 5,587 words or labels and 320 categories respec-tively. The latter is a web image dataset created by NUS  X  s Lab for Media Search, which includes 269,648 images, the associated tags from Flickr with a total of 5,018 unique tags, and 704 cate-gories. Because our tag recommendation method is a supervised task, we follow [ 8 ] [12] to split the dataset to the tra in ing set and test ing set.

Our algorithm evaluation focuses on three criteria: 1) how well our deep multimodal feature representation can identify the valuable multimodal features from the images and annotation texts of large -scale annotated image collecti ons; 2) how well our deep cross -modal correlation mining can measure the deep inter -related correlations between visual and semantic features and make an effective integration to construct cross -modal associations; 3) how well our model can support automat ic image tag recommendation for large -scale images. To evaluate the effectiveness of these criteria for our image tag recommendation, we compare the annotation results with the available ground -truth labels and employ the benchmark metrics of Average Preci sion ( AP ), Av-erage Recall ( AR ) and Average F -measure ( AF ), which are defined as: where TagSet ( Img i ) denotes the semantic annotation set for the image Img i ; RecTag ( Img i ) denotes semantic annotation set recommended for Img i ; | RecTag ( Img i )  X  TagSet ( Img i )| repre-sents the number of correct semantic annotation tags recommended for Img i ; and n is the number of images in the whole dataset. Meanwhile, a specific C ross -modal C orrel ation Score ( CCS ) is introduced to better exhibit the effect of cross -modal correlation measure, which belongs to the range of [0, 1] . 4.2 Experiment on Cross -modal Correlation
The image -tag association mainly focuses on mining t he valid and reasonable cross -modal correlation between deep visual features and semantic features. As the definition for the above evaluation metric of CC A , c ross -modal correlation considers the positive correlation in the com-mon space, that is, the maxim um correlation for each multimodal visual -semantic representation pair in the same common space. Thus to verify the effect of our DCCA -based cross -modal cor-relation mining for acquiring the deep association between different modalities, we make a com-pariso n analysis between different correlation models on three datasets, as shown in Table 1. Metric Cross -modal Correlation Score ( CCS ) Dataset Corel 5 k Corel 30 k Nus -Wide
Top -k CCA KCCA DCCA CCA KCCA DCCA CCA KCCA DCCA
It can be seen from Table 1 that for cross -modal correlation measure on three datasets, we can obtain the best CCS value of 0.9999 for Top -1 and the best CCS sum of 19.8846 for Top -20 on Corel 30 k in the evaluation pattern of our DCCA -based cross -modal correlation mining. In com-parison with the general CC S , the correlation mining performance could be promoted to a grea t degree based on KCCA and DCCA, and DCCA exhibits more significant positive impact to cross -modal correlation, which further confirms the obvious advantage of our deep cross -modal correlation mining mechanism with the deep multimodal feature information. Compared the re-sults on Corel 5 k , Corel 30 k and Nus -Wide , the results on Nus -Wide appear less performant due to the obvious characteristic differences between these three datasets . Corel 5 k and Corel 30 k have more normative and consistent annotations, and the images in the same cluster have the higher visual similarity . Nus -Wide is established based on the social annotated images from Flickr , in which the annotation information is very abundant but with many noisy or error tags, and the images have much higher visual diversity. Although when evaluating on Nus -Wide the cross -modal correlation mining performance maybe slightly influenced by such extra phenomena, the CCS value for Top -1 and the CCS sum for Top -2 0 can still reach a relatively high value. W e can still observe the promising and positive performance exhibition under the complicated tag rec-ommendation environment of Nus -Wide . The best CCS value of 0.9959 for Top -1 and the CCS sum of 1 9. 8071 for Top -2 0 on Nus -Wide approach to those on Corel 30 k a cquired under a rela-tively more pure recommendation environment. It  X  s also evidenced once more that our cross -modal correlation mining mechanism can be effectively applicable for both the unsophisticated case with less interference information and the complex case with more misinformation effect . 4.3 Experiment on Tag Recommendation
T o further explore the applicability and usefulness of our cross -modal correlation mining for supporting image tag recommendation, we particularly carry out four runs to make th e evaluation for the recommendation performance on Corel 5 k [ C 5 ] and Corel 30 k [ C 30] , that is, DMFR &amp; CCA [ C 5] , DMFR &amp; CCA [ C 30] , DMFR &amp; DCCA [ C 5] and DMFR &amp; DCCA [ C 30] ( with Deep Multimodal Feature Representation [ DMFR ] and CCA/DCCA -based Cross -modal Corre-lation Mining) . T he AF curve s with different feature dimension s ettings on two datasets are ex-hibited in Figure 1 . The AP -AR curves and the AF values for such six runs on three datasets are listed in Fig . 2 .

It can be observed from Fig . 2 that for the tag recommendation on Corel 5 k and Corel 30 k , we can obtain the best recommendation performance in the evaluation pattern of fusing DMFR and DCCA . In comparison with the patterns using CCA -based cross -modal correlation measure, the performance could be greatly promoted by integrating the deep multimodal feature representation and the deep correlation mining, which confirms the obvious advantage of our deep represe nta-tion and mining for tag recommendation. Comparing the results on t wo datasets, the results on Corel 30 k appear less performant on the whole A F curves, while the performance difference for Corel 5 k and Corel 30 k is not obvious, which reflects the performanc e advantage to some degree . Meanwhile, compared the results for different feature dimensions, we can observe that with the number of feature dimensions increasing, the recommendation performance gradually becomes pretty good, especially for our approach wi th DMFR &amp; DCCA . These results are consistent with what we expect considering more valuable deep multimodal feature information or given more detailed deep multimodal feature description . It  X  X  worth noting that the in -depth multimodal anal-ysis is available and presents more impactful ability for discovering the meaningful deep multi-modal feature s and cross -modal co rrelations . An instantiation of some recommendation results is shown in Fig . 3 .
 Fig. 3 . An instantiation of some recommendation results with our model, in which the upper line represents the ground -truth tag sequence, the lower line denotes the recommended results , and the r ed word s denote the mis match ed word s .

To give full exhibition to the superiority of our tag recommendation model with deep cross -modal correlation mining , we have also performed a comparison between our approach and the other existing classical method s in recent years. T wo method s developed by Makadia et al . (20 08 ) [ 8 ] and Murthy et al . ( 2014 ) [ 3 ] respectively are analogous with ours to some extent, and then we accomplished them on the same dataset s . The experimental results are presented in Table 2 , which reflect the difference of power between these four patterns .
 It can be found from Table 2 and Fig . 3 that the best performance can be acquired on Corel 5 k , Corel 30 k and Nus -Wide by our approach. When integrating DMFR and DCCA , we can acquire the obviously better performance than the other patterns . Although based on our approach the values of AP , AR and AF on Corel 5 k and Corel 30 k may exhibit the obvious increase over those by the other patterns, we can observe that these values on Nus -Wide have been also dramatically higher than those by th e other patterns. U nder such a complicated learning environment of Nus -Wide, our approach reveals more significant advantage. This further confirms the prominent roles of deep multimodal feature representation and cross -modal correlation mining in tag rec ommen-dation, which implies that our model is exactly a better way for determining deep multimodal associations between images and annotation tags.
Through the analysis for the tag recommendation results, it can be found that our deep corre-lation with deep feature for tag recommendation is effective., but it  X  s quality is highly related to the train ing set. Because correlation method is sensitive to data noise. It X  X  easier to introduce error or nois y detections for visual and semant ic feature information, which will seriously affect the whole clustering performance. (2) There is abundant information connotation involved in visual image. It X  X  empirically realized that only using visual features is not sufficient for well formulating t he distinguishability among image classes. The intensive visual feature expression can be utilized to further improve the multimodal correlation effectiveness and stableness. (3) There are different multimodal attributes among different annotated images. A lthough more ob-vious performance superiority has been exhibited via our CCA -based multimodal feature fusion, it  X  s very beneficial to exploit an adaptive fusion between visual and semantic feature for each annotated image. (4) Some annotated images present an extreme vision with wrong or even with-out any valid annotations . With very limited useful annotation information and too much noises , it X  X  hard for such images to successfully to recommend correct tags. This may be the stubbornest problem .
A new framework is implement ed to exploit deep cross -modal correlation s among deep visual and semantic features to enable more effective image tag recommendation . The in -depth multi-modal feature analysis is established for characterizing the deep multimoda l attributes for image s and annotations . The D CCA -based cross -modal correlation mining is introduced to acquire the specified multimodal association expressions . Our future work will focus on making our system available online, so that more Internet users can benefit from our research .
 Acknowledgments. This work is supported by the N ational K ey R esearch and D evelopment P lan (Grant No. 2016YFC080100 3 ) . Yuejie Zhang is the corresponding author. 1. Venkatesh N. Murthy . 2015 . Automatic image annotation using deep learning representa-2. Wang W . , Arora R . , Livescu K . , et al. 2015. Unsupervised learning of acoustic features via 3. Murthy V . N . , Can E . F . , Manmatha R. 2014. A hybrid model for automatic image annota-4. Guillaumin M . , Mensink T . , Verbeek J . , et al. 2009. Tagprop: Discriminative metric learning 5. Hardoon D . R . , Szedmak S . , Shawe -Taylor J. 2004. Canonical correlation analysis: An over-6. Jin C . , Mao W . , Zhang R . , et al. 2015. Cross -Modal Image Clustering via Canonical Corre-7. Gong Y . , Jia Y . , Leung T . , et al. 2013. Deep convolutional ranking for multilabel image 8. Makadia A . , Pavlovic V . , Kumar S. 2008. A new baseline for image annotat ion[M]//Com-9. Andrew G . , Arora R . , Bilmes J . , et al. 2013. Deep canonical correlation 10. Wang W . , Arora R . , Livescu K . , et al. 2014. On deep multi -view representation 11. Andrew G . , Arora R . , Bilmes J . , et al. 2013. Deep canonical correlation 12. Sigurbj X rnsson B . , Van Zwol R. 2008. Flickr tag recommendation based on collective 13. Murthy V . N . , Can E . F . , Manmatha R.A . 2014. A hybrid model for automatic image 14. Krizhevsky A . , Sutskever I . , Hinton G . E. 2012. Imagenet classification with deep 15. Pennington J . , Socher R . , Manning C. 2014. Glove: Global v ectors for w ord 16. Thompson B. 2005. Canonical correlation analysis[J]. Encyclopedia of statistics in 
