 As speech-to-speech translation systems move from the laboratory into field deployment, we quickly see that mis-match in training data with field use can degrade the per-formance of the system. Retraining based on field us-age is a common technique used in all speech systems to improve performance. In the case of speech-to-speech translation we would particularly like to be able to adapt the system based on its usage automatically without hav-ing to ship data back to the laboratory for retraining. This paper investigates the scenario of a two-day event. We wish to improve the system for the second day based on the data collected on the first day.

Our system is designed for eyes-free use and hence provides no graphical user interface. This allows the user to concentrate on his surrounding environment during an operation. The system only provides audio control and feedback. Additionally the system operates on a push-to-talk method. Previously the system (Hsiao et al., 2006; Bach et al., 2007) needed 2 buttons to operate, one for the English speaker and the other one for the Iraqi speaker. To make the system easier and faster to use, we propose to use a single button which can be controlled by the En-glish speaker. We mounted a microphone and a Wii re-mote controller together as shown in 1.

Since the Wii controller has an accelerometer which can be used to detect the orientation of the controller, this feature can be applied to identify who is speaking. When the English speaker points towards himself, the system will switch to English-Iraqi translation. However, when the Wii is pointed towards somebody else, the system will switch to Iraqi-English translation. In addition, we attac h a light on the Wii controller providing visual feedback. This can inform an Iraqi speaker when to start speaking. The overall system is composed of five major compo-nents: two automatic speech recognition (ASR) systems, a bidirectional statistical machine translation (SMT) sys -tem and two text-to-speech (TTS) systems. The standard data that is available for the TransTac project was collected by recording human interpreter mediated dialogs between war fighters and Iraqi native speakers in various scenarios. The dialog partners were aware that the data was being collected for training ma-chine based translation devices, but would often talk di-rectly to the human interpreter rather than pretending it was an automatic device. This means that the dialog partners soon ignored the recording equipment and used a mostly natural language, using informal pronunciation and longer sentences with more disfluencies than we find in machine mediated translation dialogs.

Most users mismatch their language when they com-municate using an automatic speech-to-speech transla-tion system. They often switch to a clearer pronuncia-tion and use shorter and simpler sentences with less dis-fluency. This change could have a significant impact on speech recognition and machine translation performance if a system was originally trained on data from the inter-preter mediated dialogs.

For this reason, additional data was collected during the TransTac meeting in June of 2008. This data was collected with dialog partners using the speech-to-speech translation systems from 4 developer participants in the TransTac program. The dialog partners were given a de-scription of the specific scenario in form of a rough script and had to speak their sentences into the translation sys-tems. The dialog partners were not asked to actually react to the potentially incorrect translations but just followe d the script, ignoring the output of the translation system. This has the effect that the dialog partners are no longer talking to a human interpreter, but to a machine, press-ing push-to-talk buttons etc. and will change their speech patterns accordingly.

The data was collected over two days, with around 2 hours of actual speech per day. This data was transcribed and translated, resulting in 864 and 824 utterance pairs on day 1 and 2, respectively. This section describes the Iraqi ASR system and how we perform LM adaptation on the day 1 data to improve ASR performance on day 2. The CMU Iraqi ASR system is trained with around 350 hours of audio data collected un-der the TransTac program. The acoustic model is speaker independent but incremental unsupervised MLLR adap-tation is performed to improve recognition. The acous-tic model has 6000 codebooks and each codebook has at most 64 Gaussian mixtures determined by merge-and-split training. Semi-tied covariance and boosted MMI discriminative training is performed to improve the model (Povey et al., 2009). The features for the acoustic model is the standard 39-dimension MFCC and we concatenate adjacent 15 frames and perform LDA to reduce the di-mension to 42 for the final feature vectors. The language model of the ASR system is a trigram LM trained on the audio transcripts with around three million words with Kneser-Ney smoothing (Stolcke, 2002).

To perform LM adaptation for the ASR system, we use the ASR hypotheses from day 1 to build a LM. This LM is then interpolated with the original trigram LM to pro-duce an adapted LM for day 2. We also evaluate the effect of having transcribers provide accurate transcription ref -erences for day 1 data, and see how it may improve the performance on day 2. We compare unigram, bigram and trigram LMs for adaptation. Since the amount of day 1 data is much smaller than the whole training set and we do not assume transcription of day 1 is always available, the interpolation weight is chosen of be 0.9 for the orig-inal trigram LM and 0.1 for the new LM built from the day 1 data. The WER of baseline ASR system on day 1 is 32.0%.
 The results in Table 1 show that the ASR benefits from LM adaptation. Adapting day 1 data can slightly improve the performance of day 2. The improvement is larger when day 1 transcript is available which is expected. The result also shows that the unigram LM is the most robust model for adaptation as it works reasonably well when transcripts are not available, whereas bigram and trigram LM are more sensitive to the ASR errors made on day 1.
Table 2 shows the impact of ASR adaptation on the performance of the translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT com-ponent. There is no obvious difference between unsuper-vised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac pro-gram. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to op-timize LM interpolation weights by minimizing perplex-ity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences simila r to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation ref-erences; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vo-gel, 2005).

The first question we would like to address is whether our adaptation obtains improvements via an unsupervised manner. We take day 1 baseline ASR hypothesis and use the baseline SMT to get the MT hypothesis and a 500-best list. We train a domain LM using the 500-best list and use the MT hypotheses as the reference in MERT. We treat day 1 as a development set and day 2 as an unseen test set. In Table 3 we compare the performance of four systems: the baseline which does not have any adaptation steps; and 3 adapted systems using unigram, bigram and trigram LMs build from 500-best MT hypotheses.
Experimental results from unsupervised adaptation did not show consistent improvements but suggest we may obtain gains via supervised adaptation. In supervised adaptation, we assume we have day 1 translation refer-ences. The references are used in MERT. In Table 4 we show performances of two additional systems which are the baseline system without adaptation but tuned toward day 1, and the adapted system which used day 1 trans-lation references to train a unigram LM (1gramLM MT Ref). The unigram and bigram LMs from 500-best and unigram LM from MT day 1 references perform rela-tively similar on day 2. Using a trigram 500-best LM returned a large degradation and this LM is sensitive to the translation errors on day1 In Sections 3 and 4 we saw that individual adaptation helps ASR to reduce WER and SMT to increase BLEU score. The next step in validating the adaptation frame-work was to check if the joint adaptation of ASR and SMT on day 1 data will lead to improvements on day 2. Table 5 shows the combination of ASR and SMT adaptation methods. Improvements are obtained by us-ing both ASR and SMT adaptation. Joint adaptation con-sistently gained more than one BLEU point improvement on day 2. Our best system is unsupervised ASR adapta-tion via 1gramLM of ASR day 1 transcription coupled with supervised SMT adaptation via 1gramLM of day 1 translation references. An interesting result is that to have a better result on day 2 our approach only requires translation references on day 1. We selected 1gramLM of 500-best MT hypotheses to conduct the experiments since there is no significant difference between 1gramLM and 2gramLM on day 2 as showed in Table 3. The previous results indicate that we require human translation references on day 1 data to get improved per-formance on day 2. However, our goal is to make a better system on day 2 but try to minimize human efforts on day 1. Therefore, we raise two questions: 1) Can we still ob-tain improvements by not using all of day 1 data? and 2) Can we obtain more improvements?
To answer these questions we performed oracle exper-iments when we take the translation hypotheses on day 1 of the baseline SMT and compare them with transla-tion references, then select sentences which have BLEU scores higher than a threshold. The subset of day 1 sen-tences is used to perform supervised adaptation in a sim-ilar way showed in section 5. These experiments also simulate the situation when we have a perfect confidence score for machine translation hypothesis selection. Table 6 shows results when we use various portions of day 1 to perform adaptation. By using day 1 sentences which have smoothed sentence BLEU scores higher than 10 or 20 we have very close performance with adaptation by using all day 1 data. The results also show that by using 416 sen-tences which have sentence BLEU score higher than 40 on day 1, our adapted translation components outperform the baseline. Performance starts degrading after 50. Ex-perimental results lead to the answer for question 1) that by using less day 1 data our adapted translation compo-nents still obtain improvements compare with the base-line, and 2) we did not see that using less data will lead us to a better performance compare with using all day 1 data.
 This work clearly shows that improvement is possible us-ing collected data for adaptation. The overall picture is shown in Figure 2. However this result is only based on one such data set, it would be useful to do such adaptation over multiple days. The best results however still require producing translation references, notably ASR transcrip-tions do not seem to help, but may still be required in the process of generating translation references. We wish to further investigate automatic adaptation based on implici t confidence scores, or even active participation of the user e.g. by marking bad utterance which could be excluded from the adaptation.

