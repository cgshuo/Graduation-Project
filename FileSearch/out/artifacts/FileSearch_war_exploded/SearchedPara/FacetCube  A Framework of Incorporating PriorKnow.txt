 Non-negative tensor factorization (NTF) is a relatively new technique that has been successfully used to extract signifi-cant characteristics from polyadic data, such as data in so-cial networks. Because these polyadic data have multiple dimensions (e.g., the author, content, and timestamp of a blog post), NTF fits in naturally and extracts data charac-teristics jointly from different data dimensions. In the stan-dard NTF, all information comes from the observed data and end users have no control over the outcomes. However, in many applications very often the end users have certain prior knowledge, such as the demographic information about individuals in a social network or a pre-constructed ontol-ogy on the contents, and therefore prefer the extracted data characteristics being consistent with such prior knowledge. To allow users X  prior knowledge to be naturally incorporated into NTF, in this paper we present a novel framework X  FacetCube X  X hat extends the standard non-negative tensor factorization. The new framework allows the end users to control the factorization outputs at three different levels for each of the data dimensions. The proposed framework is intuitively appealing in that it has a close connection to the probabilistic generative models. In addition to introducing the framework, we provide an iterative algorithm for com-puting the optimal solution to the framework. We also de-velop an efficient implementation of the algorithm that con-sists of a series of techniques to make our framework scalable to large data sets. Extensive experimental studies on a pa-per citation data set and a blog data set demonstrate that our new framework is able to effectively incorporate users X  prior knowledge, improves performance over the standard NTF on the task of personalized recommendation, and is scalable to large data sets from real-life applications. H.2.8 [ Database Management ]: Database Applications X  Data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering Algorithms, Experimentation, Measurement, Theory Non-negative Tensor Factorization, Polyadic Data, Prior Knowl-edge, Iterative Algorithm, Sparse Algorithm
Data in many applications are polyadic , i.e., they have multiple dimensions. Data in social networks are such an example X  X or example, data in the blogosphere may have people dimension (the author of a blog post), content di-mension (the body of the post), and time dimension (the timestamp of the post). Documents in a digital library are another example X  X  scientific paper can be described by its authors, keywords, references, publication date, publication venue, etc. To analyze such polyadic data, a very important task is to extract significant characteristics from different data dimensions, where the extracted characteristics can be used either directly for data summarization and visualiza-tion or as features for further data analysis. The extracted characteristics can be in the form of, using the blog exam-ple, salient communities among bloggers, coherent topics in blog posts, and noteworthy temporal trends of these topics and communities. Because these data dimensions affect each other in a joint way, approaches that either analyze each data dimension independently or only consider pairwise re-lations between data dimensions are not able to accurately capture the data characteristics.

Recently, several multiple-dimensional tensor models have been proposed to capture the higher-order correlation (other than the second order correlation) among various data di-mensions. These tensor-based approaches can be catego-rized into two groups. Approaches in the first group (e.g.,[7, 13]) decompose polyadic data by using higher-order linear decompositions, which are extensions of the matrix singular value decomposition. On the other hand, approaches in the second group (e.g., [4, 5, 21]) decompose polyadic data by using non-negative tensor factorizations (NTFs), which are extensions of the non-negative matrix factorization (NMF [18]). The approaches based on NTFs decompose data into additions of non-negative components and therefore have many advantages over those based on linear decompositions. Such advantages include ease of interpretation of the ex-tracted characteristics, close connection to the probabilistic models, and no enforcement on the orthogonality among dif-ferent data characteristics. Because of these advantages, in this paper we focus on the approaches that extract data characteristics by using NTF.

In an approach based on the standard NTF for extract-ing data characteristics, the extracted characteristics can be of arbitrary forms and end users do not have any control over them. Such an approach has some benefits X  X t is sim-ple because it does not require any input other than the observed data. However, such a simple approach also has its weakness: end users have no channel to incorporate their prior knowledge into the process of characteristic extraction. We argue that users X  prior knowledge can greatly benefit the extraction of meaningful data characteristics from dif-ferent perspectives. Cognitively, human concepts such as the content topics usually only occupy a very low dimensional subspace or manifold of the whole space (which usually is of much higher dimensions) and users X  prior knowledge can provide guidance toward the appropriate subspace. Statis-tically, a troublesome issue in the standard NTF-based ap-proach is the problem of overfitting whereas users X  input can alleviate this problem. Application-wise, there are many applications in which the end users already have certain domain knowledge, e.g., a pre-constructed ontology of con-tents, and want to view the data through the lens of such domain knowledge. Therefore, it is beneficial to provide means to allow the end users to incorporate prior knowledge into the process of characteristic extraction.

In this paper, we propose a new framework that extends the standard NTF and allows end users to incorporate prior knowledge in the process of extracting characteristics from polyadic data. We name our framework FacetCube 1 . Our main contributions are summarized as follows. Our FacetCube framework is general in that it can handle polyadic data of arbitrary order. In this paper, we use two applications, personalized recommendation in a paper cita-tion data set and data exploration in a blog data set, to
FacetCube stands for  X  X ac torize dat a using NTF with c o-ordinates being u nconstrained, b asis-constrained, or fixe d X . demonstrate the effectiveness of our framework. Extensive experimental studies on these two data sets demonstrate that our new framework is able to effectively incorporate users X  prior knowledge, improves performance over the stan-dard NTF on the task of personalized recommendation, and is scalable to large data sets from real-life applications.
The rest of this paper is organized as the following. In the rest of this section we survey related work. In Section 2 we provide background information. In Section 3 we intro-duce our FacetCube framework. In Section 4, we offer a probabilistic interpretation of our framework. In Section 5, we describe the techniques for our efficient implementation. We present experimental results in Section 6 and finally give conclusions in Section 7.
 The majority of existing tensor factorization-based studies focus on liner tensor factorizations, where the non-negative constraint is not enforced. Two mostly used linear tensor factorizations are the PARAFAC model [13] and the Tucker model [25], where the latter has recently been generalized by De Lathauwer et al. [7] to the Higher Order Singular Value Decomposition. These models have been used for applications such as Web page ranking [24], cross-lingual information retrieval [3], sensor network analysis [23], etc. We refer interested readers to the comprehensive survey by Kolda and Bader [17]. Although these linear tensor factor-izations show some good properties in terms of minimizing certain losses in the Frobenius norm, they also have some weak points. For example, the negative values in the out-come of the factorization make it difficult to interpret the outcome in terms of probabilistic distributions. In addition, in contrast to NTF where the data are factorized into ad-ditions of components, linear tensor factorizations require subtractions among components which make them not intu-itively appealing. Furthermore, linear tensor factorizations usually require concepts in the outcome to be orthogonal, and this limits the scope of their applications (e.g., we sel-dom enforce different temporal trends to be orthogonal to each other). A special linear tensor factorization, the CAN-DELINC model [2], has certain similarity to our framework in that it allows the data characteristics to be inside a pre-defined linear subspace. However, for linear factorizations, such a goal can be easily achieved by projecting the data into the subspace before applying the tensor factorization.
Our work is an extension to the existing non-negative ten-sor factorizations (NTFs) and the following are some related studies in this area. Hofmann [15] proposed the Probabilis-tic Semantic Analysis (PLSA) for extracting latent topics from documents. Later, Gaussier and Goutte [12] and Ding et al. [9] showed the equivalence between the PLSA and the NMF algorithms by Lee and Seung [18]. Several studies have extended NMFs to NTFs and applied them to applications such as computer vision [22], digital signal processing [11], information retrieval [4], etc. Both Tucker-typed NTF [4, 5, 21] and the PARAFAC-typed NTF [14, 22] have been explored in some of these studies. Our work extends exist-ing NTF approaches by allowing users to incorporate prior knowledge in the factorization process and takes the above standard NTF approaches as special cases. In addition, in this work we focus on the Tucker-typed NTF and the exten-sion to the PARAFAC-typed NTF is straightforward.
Some other related work include the following. Dhillon et al. [8] proposed a co-clustering framework using an informa-tion theoretic framework. Long et al. [20] proposed several optimization-based models for multi-type relational data. These approaches, however, focus more on pairwise relations and do not capture higher order correlations. Banerjee et al. [1] proposed a multi-way clustering framework that can handle multi-dimensional relations. However, the approach by Banerjee et al. is derived from the Bregman divergence and it uses an optimization framework. In comparison, our new model has an underlying probabilistic interpretation. In addition, Chi et al. [6] proposed to incorporate certain reg-ularization, such as the connectivity of subgraphs and the smoothness of temporal trends, in the factorization process. Lin et al. [19] proposed a FacetNet framework for dynamic community detections. These two studies turned out to solve non-negative matrix factorization problems instead of using non-negative tensor factorizations.
First, we introduce some mathematical notations that will be used in this paper. In this paper, unless stated other-wise, values of all variables belong to  X  + , the set of non-negative real numbers. We denote scalars by lower-case let-ters (e.g.,  X  ,  X  ,  X  ,  X  ), vectors by lower-case letters in vector forms (e.g.,  X  X  X  ,  X  X  X  ), matrices by capital letters (e.g.,  X  ,  X  ,  X  ), and tensors by calligraphic letters (e.g.,  X  ,  X  ,  X  ). We reserve  X ,  X ,  X ,  X ,  X ,  X  to indicate the indices of tensors, and  X ,  X ,  X ,  X ,  X ,  X  for the sizes of the corresponding dimen-sions. In the following definitions and the formulations in the rest of the paper, we restrict attention to 3rd-order ten-sors whereas the same definitions and formulations can be easily generalized to tensors of arbitrary orders.
The dot product between tensors  X  and  X  is denoted by Similar dot products are defined on matrices and vectors. For example,  X   X ,  X   X  =  X  X  X  (  X   X   X  ), where  X  X  X  is the trace oper-ator. The element-wise product between tensors  X  and  X  is denoted by with  X   X  X  X  X  =  X   X  X  X  X   X   X  X  X  X  . Similarly, the element-wise division is denoted by with  X   X  X  X  X  =  X   X  X  X  X  /  X   X  X  X  X  . We define element-wise products and divisions on matrices and vectors in a similar way. The KL-divergence between tensors  X  and  X  is defined as  X  X  X  (  X  X  X  X  ) = KL-divergence is defined on matrices and vectors similarly.
We say a tensor  X   X   X   X   X   X   X   X  + is obtained from another tensor  X   X  X  X   X   X   X   X   X  + by a base transform , if we have  X  P In such a transform, we refer to  X  as the core tensor and  X ,  X  and  X  as the facet matrices of the corresponding dimensions. The same notation applies to matrices where [  X ,  X ,  X  ] =  X  X  X  X   X  . In addition, we define shorthand notations for three special base transforms:  X  X  1  X  . = [  X  ,  X ,  X   X  ,  X   X  ],  X  X  [  X  ,  X   X  ,  X ,  X   X  ], and  X  X  3  X  . = [  X  ,  X   X  ,  X   X  ,  X  ], where  X  are the identity matrices of sizes  X  ,  X  , and  X  , respectively. (In the literature,  X  1 ,  X  2 and  X  3 are also called the mode-1, mode-2, and mode-3 multiplications of  X  [7]).

As the partial derivative of the inner product  X  X  , [  X  ,  X ,  X ,  X  ]  X  with respect to  X  is independent to  X  , we denote it by  X  X  , [  X  ,  X  ,  X ,  X  ]  X  . That is, with  X   X  X  X  = we have and  X  X  , [  X  ,  X ,  X ,  X  ]  X  to indicate the partial derivatives of  X  X  , [  X  ,  X ,  X ,  X  ]  X  with respect to  X  ,  X  , and  X  , respectively. Note that we have the following relation  X  X  , [  X  ,  X ,  X ,  X  ]  X  . =  X 
In an NTF-based approach that extracts data character-istics, the first step is to construct a data tensor , where the order of the data tensor is the same as the number of di-mensions of the data. We again use the blogosphere as an example. A third-order data tensor  X   X   X   X   X   X   X   X  + can be used to represent the blog data where the three dimen-sions of the data tensor correspond to blogger, keyword, and timestamp, respectively. Each of  X  ,  X  , and  X  represents the size of the corresponding data dimensions. I.e., there are in total  X  bloggers,  X  keywords and  X  timestamps in the data. Each entry of the data tensor represents the inten-sity of the corresponding entry in the observed data. For example, (  X  )  X  X  X  X  =  X   X  X  X  X  can be the frequency that blogger  X  mentioned keyword  X  at timestamp  X  .

Once the data tensor is constructed, in approaches based on the standard NTF, a non-negative tensor factorization is directly applied to the data tensor. The outcomes of this fac-torization consist of two parts. The first part is a set of facet matrices where each matrix represents the most significant characteristics of one dimension of data. More specifically, the number of facet matrices is the same as the order of the tensor and for each facet matrix, each column of the matrix indicates one facet of the corresponding dimension of data. For our blog example, the facet matrices are  X   X   X   X   X   X  +  X   X   X   X   X   X  + , and  X   X   X   X   X   X  + , where each column of  X  ,  X  , and  X  denotes a salient community of bloggers, a significant topics in posts, and a noteworthy temporal trends, respec-tively. The second part of the NTF decomposition is a core tensor  X  , which has the same order as the data tensor but usually with a much smaller size.  X  represents the correla-tion among all the facets in all the data dimensions. In our blog example,  X   X   X   X   X   X   X   X  + where  X  ,  X  , and  X  are the number of facets in the dimensions of blogger, keyword, and timestamp, respectively.

The target of a non-negative tensor factorization is to find the core tensor  X  , the facet matrices  X  ,  X  , and  X  , so that when put together as [  X  ,  X ,  X ,  X  ], they approximate  X  in an optimal way. A commonly used metric to measure the approximation error is the KL-divergence. That is, the ob-jective of the standard NTF is to find  X  ,  X ,  X  , and  X  to minimize the following error In addition, because of the scale-free issue (e.g., [  X  ,  X ,  X ,  X  ] = [  X   X  , 1  X   X ,  X ,  X  ]), additional constraints are added so that each column of  X  ,  X  , and  X  must sum to 1, as well as the sum of all the entries of  X  must be 1.

The following procedure, which we proposed in [4, 5], can be used to iteratively searching the optimal solutions for  X  ,  X ,  X  , and  X  .

Lemma 1. For a given  X   X   X   X   X   X   X   X  + , we first define an auxiliary tensor  X  . =  X  / [  X  ,  X ,  X ,  X  ] . Then the following up-date rules are guaranteed to converge to an optimal solution to the objective function defined in Equation (1) : where  X  0 denotes the operation of after all updates are com-pleted, normalizing so that all entries sum to one, and  X  denotes the same operation except that all columns are nor-malized so that they sum to ones.
In this section, we describe the details of our FacetCube framework. As we have mentioned in the introduction, in-corporating users X  prior knowledge can potentially benefit the extraction of data characteristics. The benefits can be of multiple-fold X  X he extracted characteristics can be more generalizable because of the alleviation of overfitting, they can be more reasonable because they fit users X  prior knowl-edge, and they can meet the users X  requirement when the users want to enforce the facets on certain data dimensions. To achieve these benefits, we extend the standard NTF by allowing users X  prior knowledge to be incorporated in the process of decomposition. We will discuss two variants in our framework X  X hen the facets in a data dimension are re-stricted to be in a user-given subspace and when the facets are fixed by the user. Together with the case of uncon-strained facets in the standard NTF, our framework allows end users to control the process of data characteristic ex-traction at three different levels.
In the first variant, we assume that the prior knowledge from the end users forms a subspace from which the facets can be located. To illustrate this variant, we first rewrite Eq. (1) as the following equivalent form where  X   X  ,  X   X  , and  X   X  are identity matrices of dimensions  X  ,  X  , and  X  , respectively. We can therefore consider, for example, every facet in  X  as a point located in the sim-dimension  X  (i.e., the  X  -th element being 1 and other ele-ments being zeros). However, because  X  can be very large (i.e., tens of thousands of keywords), this simplex can have too large degree of freedoms, and therefore the search space is too large for us to find reliable facets. In such a case, users X  prior knowledge can be used to reduce the search space. Assuming the prior knowledge is given as a set of space of facets to be in the simplex formed by  X   X  . I.e., each facet is in the form of  X   X   X   X  X  X  for certain  X  X  X   X  X  X   X   X  . To further clarify this, we come back to the blog example. Each facet in the content dimension is a significant topic and in the standard NTF approaches, a topic can be formed by any combination of keywords with arbitrary weights. If the end users provide a set of sub-topics, e.g., obtained from the leaf nodes of a content ontology tree, then our framework can take these sub-topics as a basis for the content facets. That is, each facet (topic) must be a convex combination of the given sub-topics. In this case, we say the data di-mension is basis-constrained . Fig. 1(b) illustrates this point. Another example for basis-constrained data dimension can be the time dimension. Each facet in the time dimension cor-responds to a noteworthy temporal trend. According to our prior intuition, a temporal trend should be smooth instead of noise-like. One way to incorporate this prior knowledge is to form a basis consisting of Fourier series in low frequencies. Then a convex combination of this basis will not contain high frequency components and therefore the facets will be guaranteed to be smooth. 2 Figure 1: Schematic illustrations of the FacetCube framework with (a) unconstrained, (b) basis-constrained, and (c) fixed data dimensions. In (a) and (b), the shaded areas are the simplex in which the facets can locate; in (c),  X  X  X  1 and  X  X  X  2 are fixed facets.
In a mathematical form, to handle basis-constrained data dimensions, we extend the objective function in NTF to the following form subject to each column of  X  ,  X  , and  X  sums to 1. Note that in this objective function, without loss of generality, we assume that  X  is the basis-constrained dimension, that  X   X  is the basis matrix provided by the end users, and that each column of  X   X  sums to 1. As can be observed in Eq. (3), the term  X   X   X  replaces the term  X   X   X  in Eq. (2) and so Eq. (2) can be considered as a special case of Eq. (3).
In some other cases, the end users may require the facets for certain data dimensions to be fixed . Such a requirement may sound overly restricted, but it is not uncommon in real applications. For example, from the top level nodes of an ontology tree, the user may have already determined the top-level topics (in contrast to the sub-topics in Sec. 3.1) such as politics, technologies, sports, etc., as well as the representation of these topics as facets. And the user X  X  goal is to summarize the blog posts through the lens of these pre-constructed facets. As another example, in the time dimension, the user may choose to use a set of facets that correspond to the domain knowledge, i.e., concepts such as year, quarter, month, etc., to detect seasonal trends. In such scenarios, we extend the objective function in NTF to the following form subject to each column of  X  , and  X  sums to 1. Note that without loss of generality, we assume that the facets in the time dimension are fixed as  X   X  and each column of  X   X  sums to 1. Because  X   X  is fixed, we say that  X  is a fixed data dimension . Fig 1(c) illustrates this case.
To summarize the above two variants as well as the third variant which corresponds to the standard NTF case, we give the objective function in its most general form as the following where  X   X  ,  X   X  , and  X   X  are given a priori whereas  X  ,  X  ,  X  , and  X  are to be computed. When  X   X  ,  X   X  , and  X   X  are set to identity matrices, we obtain the standard NTF problem; when any of the  X   X  ,  X   X  , or  X   X  is given and the corresponding  X  ,  X  , or  X  is to be computed, we have the case of the basis-constrained data dimension; when any of the  X   X  ,  X   X  , or  X   X  is given and the corresponding  X  ,  X  , or  X  is fixed to be an identity matrix, we have the case of the fixed data dimension.

In addition, because in our proposed algorithm, as will be described momentarily, the (per iteration) optimization of each data dimension is performed independently of other data dimensions, we can adopt any of the above three vari-ants in our FacetCube framework for any data dimensions independently. For example, when using our framework, the end users may decide to let the blogger communities be dis-covered with no constrains, provide a basis for the contents, and fix the facets in the time dimension.
We propose an algorithm for computing optimal solutions for our FacetCube framework. We present the algorithm in the most general form X  X he basis-constrained form X  X nd then show that the unconstrained and the fixed dimensions can be solved by using the same result.

The iterative algorithm and its correctness are stated in the following theorem.

Theorem 1. For a given  X  X  X  X   X   X   X   X   X  + , the following up-date rules are guaranteed to converge to an optimal solution to the objective function defined in Equation (5) . where  X  0 and  X  1 are the same as those defined in Lemma 1. The proof of this theorem is based on an EM-style argument and is provided in the appendix.

Although the iterative algorithm is given for the case of the basis-constrained dimension, it is straightforward to adopt it to the other two cases. For the case of unconstrained di-mension, say  X   X  =  X   X  is the identity matrix, we directly use  X  instead of  X   X  2  X   X   X  in Eq. (9) of the update algorithm. For the case of fixed dimension, say  X   X   X  =  X   X  , we sim-ply skip the corresponding update step in Eq. (10). Such a skip is safe because in each step of the iterative algorithm, Eqs. (7) X (10) independently improve the objective function in a monotonic way. It is worth noting that another conse-quence of such independent updates is that the end users can simultaneously enforce different levels of preference at each dimension, e.g., by setting  X   X  =  X   X  (unconstrained), pro-viding  X   X  (basis-constrained), and fixing  X   X   X  =  X   X  (fixed) at the same time.
The FacetCube framework turns out to have a natural probabilistic interpretation in that it is equivalent to a spe-cial probabilistic generative model. In the following discus-sion, we focus on the basis-constrained dimension and we use the blog data as an example, assuming the observed data are in a list in the form of { X  blogger  X , word  X , time  X ,  X  We can use the following probabilistic generative procedure to describe how the observed data are sampled: 1. select a community  X  , a topic  X  , a temporal trend  X  , 2. conditioning on the result in step 1, select a sub-community 3. conditioning on the results in step 2, select a blogger Then under this generative model, the log-likelihood of the data can be written as X where the inner sum is computed over all  X ,  X ,  X ,  X   X  ,  X  simple derivation can show that maximizing the log-likelihood in Eq. (11) is equivalent to minimizing the KL loss in Eq. (5). As a consequence, our FacetCube framework is equivalent to this probabilistic generative model.

The probabilistic interpretation, other than giving addi-tional insights to our FacetCube framework, provides the underpinning for certain extensions of our basic framework. We show such an extension which is to consider a maximum a posteriori (MAP) estimation for the basic probabilistic model. Here, we consider a case of using the Dirichlet dis-tribution as the prior distribution of the parameters. First, we consider the prior for  X  . The prior of each column of  X  is a Dirichlet distribution with hyper-parameter  X   X  &gt; 0. The logarithm of the prior probability is where  X   X  is a value irrelevant to  X  . Similarly, we assume the priors for  X  ,  X  and  X  are all Dirichlet distributions, with hyper-parameters  X   X  ,  X   X  , and  X   X  , respectively. The logarithm of the error for the MAP estimation is that for the MLE plus the logarithm of the prior probabilities: With a few derivations, we can obtain the following algo-rithm for solving the MAP estimation. (Due to the limit of the space, we skip the details of the proof).

Corollary 1. The following update rules are guaranteed to converge to an optimal solution to the objective function defined in Equation (12) .

 X   X  X  X  / [  X  ,  X   X   X ,  X   X   X ,  X   X   X  ] ,  X   X  0 [  X  X  X  X  X  X  , [  X  ,  X   X   X ,  X   X   X ,  X   X   X  ]  X  + (  X   X   X  1)]  X   X  1  X   X  1  X   X  1 where  X  is a small positive real number and [  X  ]  X  stands for taking the maximal one between the variable value and  X  . Figure 2: Distribution of variables with different  X  . When  X  &gt; 1 , the mode is at 0 . 5 , and so  X  tends to be non-zeros. When  X  &lt; 1 , the modes are 0 and 1 , which make  X  or 1  X   X  tend to be 0 , an so it results in sparse solutions.

Corollary 1 reveals that the Dirichlet prior can play two different roles in computing the optimal solutions. When  X  &gt; 1, the Dirichlet prior plays a role of smoothing by adding pseudo-counts to each variable. On the other hand, when  X  &lt; 1, the Dirichlet prior plays a role opposite to smoothing X  X t pulls variable with small values into the neg-ative territory (which are then set to  X  by the update rules) and as a result, making the solution more sparse (if we dis-regard the small-valued  X   X  X ). This point is demonstrated in Fig. 2 by using a two-variable Dirichlet (Beta) distribution.
In this section, we first describe some techniques we devel-oped to make our algorithm practical, and then introduce a technique we adopted for fast computation of top- X  queries. The reason for us to give such a detailed description is that we believe these techniques are important by themselves: they can be equally applied to other multiplicative update rules for tensors of arbitrary order and they can be used to solve a family of similar problems.
A naive implementation of the update rules in Theorem 1 is impractical for all but very small data sets. In real ap-plications, the size of a data dimension (e.g., the number of bloggers or the size of the vocabulary) can easily be tens of thousands. When  X  =  X  =  X  = 10 , 000, for example, even the computation of [  X  ,  X ,  X ,  X  ] in Eq. (6) is unmanageable, for [  X  ,  X ,  X ,  X  ] is a dense tensor with 1 trillion entries. handle data from real applications, we designed an efficient implementation of the FacetCube algorithm that exploits three key insights we observed. Part of these insights have been roughly mentioned in a high-level way in [4, 5]. Here we give them a much more rigorous and complete treatment.
In the following discussion, without loss of generality, it is assumed that  X   X   X   X   X  and  X   X   X   X   X  . In ad-dition, we assume that each data record is stored in the are the indices of the data record in the first, second, and third dimensions, respectively;  X  is the value of the data record, which can be, for example, the number of times that a blogger mentions a keyword on a particular day. It is also assumed that the data records have been sorted according to  X  X  X  X  3 as the major key and then  X  X  X  X  2 and then  X  X  X  X  1 as the minor keys. We shall come back to this last assumption later.

The first insight we have exploited is that data in real ap-plications are usually sparse. For example, not every blogger uses every word in every of his or her posts. Because  X  is a sparse tensor, not every entry of [  X  ,  X ,  X ,  X  ] is needed. In our implementation, we take advantage of data sparseness by computing base transforms in an on-demand fashion. For example, it only takes time  X  (  X   X   X   X  3 ) to compute the entries in [  X  ,  X ,  X ,  X  ] that correspond to the non-zero entries in  X  , where  X   X  is the total number of non-zero entries in  X  .
The second insight is that the update rules in Theorem 1 involve a lot of nested summations and multiplications that have a lot of structures. Therefore if we carefully order these nested operations and cache the intermediate results, we can avoid generating them multiple times. More specifically, we rewrite Eq. (6) as the following by pushing certain summa-tions inside the nest In the above expression, for entries with the same  X  index, the term in the inner parentheses can be reused even when the  X  and  X  indices vary; similarly, for entries with the same  X  and  X  indices, the term in the outer parentheses can be reused even when the  X  index varies. We can derive similar expressions for Eqs. (7) X (10). For instance, Eq. (7) can be rewritten as [  X  ,  X   X  ,  X   X  ,  X   X  ]  X  X  X  X  = And for Eq. (8), we first notice that
For the discussion in this part we restrict attention to the case of unconstrained dimensions, for the computations for  X   X   X  ,  X   X   X  , and  X   X   X  do not dominate the time complexity. and for the second part of the above expression, we have  X  X  X  , [  X  ,  X  ,  X ,  X  ]  X   X  X  X  = As a result, we can reuse intermediate computation results in all the update rules, since  X  is stored with  X  X  X  X  3 (the  X  index) as the major key and then  X  X  X  X  2 (the  X  index) and then  X  X  X  X  1 (the  X  index) as the minor keys. The sorting of  X  does affect the time complexity for two reasons. On the one hand, the indices  X  ,  X  , and  X  are positive integers with known upper-bounds and so a linear sorting algorithm, such as bucket sort, can be applied. On the other hand,  X  only has to be sorted once before the iterative process starts and so the sorting cost is amortized among multiple iterations. It is worth mentioning that  X  , which is also sparse, does not have to be explicitly sorted X  X t is automatically sorted because of the way it is derived from the sparse tensor  X  .
The third insight is that different data dimensions usu-ally have different cardinalities. For example, if the time unit is a day, then the size of the time dimension can be much smaller than than of the blogger dimension. We take advantage of this fact by ordering the computation in such a way that the dimension of the smallest size is put in the inner parentheses. A moment of thoughts on Eqs. (13) X (15) can show that this ordering is always possible as long as  X  is sorted accordingly at the beginning. As will be demon-strated in the experimental studies, this re-ordering makes huge difference in the running time of our algorithm.  X  input:  X  as { X   X  X  X  X  1 ,  X  X  X  X  2 ,  X  X  X  X  3 ,  X   X  X  ,  X  ,  X  ,  X  ,  X  ,  X   X  output: updated  X  1:  X   X  X  X  1,  X   X  X  X  1,  X   X  X  X  1,  X   X  0 ; 2: for each entry  X   X  X  X  X  1 ,  X  X  X  X  2 ,  X  X  X  X  3 ,  X   X  of  X  do 3: if  X   X  =  X  X  X  X  3 4:  X   X   X  X  X  X  3,  X   X  X  X  1; 5: construct  X  s.t.  X   X  X  X   X  6: if  X   X  =  X  X  X  X  2 7:  X   X   X  X  X  X  2; 8: construct  X   X  s.t. (  X   X  )  X   X  9:  X   X   X  X  X  X  1; 11:  X   X  1  X   X  (  X   X   X   X  ); 12: return  X  ; Figure 3: Implementation of the update rule for  X  . Those for  X  ,  X  , and  X  are similar.

To summarize, as an example, Fig. 3 gives the high-level pseudo code for our implementation of the update rule for updating  X  , for the case of basis-constrained dimension. The time complexity can be shown to be  X  (  X   X   X   X  +  X   X   X   X  2 +  X   X   X  3 ), where  X   X  is the number of non-zero entries in  X  ,  X   X  is the number of distinct (  X ,  X  ) pairs in  X  , and  X  is the smallest size among all the data dimensions. As can be observed from the time complexity, if we consider the num-ber of factors  X  as a constant, then the per iteration time is linear in the size of raw data  X   X  ; if on the other hand we do not consider  X  to be a constant, because  X   X  &gt;  X   X  &gt;  X  , we assign the lowest coefficient to the most expensive  X  3 term and therefore fully exploit the data characteristics. We will verify these analyses in the experimental studies.
In many recommendation applications, instead of being interested in the ranks of the whole list of candidates, the end users often are only interested in a quick answer to the top- X  queries. For example, the query may be X  X ho are the top 3 bloggers mostly involved in a topic during a given time period? X  or  X  X hat are the top 10 references that are mostly relevant to a group of authors who plan to co-author a pa-per on a set of keywords? X . Because in many applications such queries must be answered in real time, fast computa-tion of top- X  answers becomes crucial. In the implementa-tion of our framework, we develop a technique that derives the exact top- X  answers without computing the scores of all the candidates for the recommendation. Without go-ing into details, we want to point out that the key com-ponent of the technique is the Fagin X  X  algorithm [10] which has been extensively used in the database field for answer-ing fuzzy queries over multimedia databases. For Fagin X  X  algorithm to work, a key requirement is that the score func-tion must be monotonic . A function  X  (  X  1 , . . . ,  X   X  tonic if {  X   X  1  X   X  1 , . . . ,  X   X   X   X   X   X  } implies that  X  (  X   X   X  (  X  1 , . . . ,  X   X  ). It turns out that our ranking function sat-isfies this condition of monotonicity. We illustrate this by using the above query of recommending references to a given set of authors on a given set of keywords. We assume that  X  ,  X  , and  X  correspond to author , keyword , and reference , and  X   X  ,  X   X  , and  X   X  are the corresponding basis.
Recall that  X   X  [  X  ,  X   X   X ,  X   X   X ,  X   X   X  ] and so for a set of authors and a set of keywords, the relevances of the refer-ences are [  X  ,  X  X  X   X  ,  X  X  X   X  ,  X   X   X  ] = (  X   X   X  )  X  [  X  ,  X  X  X  the  X  -th row of the result indicates the relevance of the  X  -th reference. Because  X  X  X   X  and  X  X  X   X  are obtained by aggregating the given set of authors and the given set of keywords, re-Because  X   X   X  is also non-negative, it can be easily shown that the score function  X  [(  X   X   X  )  X  X  X  X   X  ] = (  X   X   X  )  X  X  X  X  monotonic function.
In this section, we present experimental studies on two data sets X  X  paper citation data set and a blog data set. For experiments on the paper citation data, we study the task of personalized reference recommendation, and we focus on the accuracy of recommendations, the running time of our factorization algorithm, the time for computing the top- X  recommendations, as well as the impact of the Dirichlet prior. For experiments on the blog data, we illustrate a real application scenario where the end users have an ontology as the facets for content, and a fixed set of facets for time. The paper citation data set is obtained from the Cite-Seer website 3 . This data set contains the information about papers published in the computer science area. The infor-mation we used includes the authors, the abstract, and the citations of each paper. The abstracts of the papers are pre-processed through the following steps. First, an NLP package 4 is used to detect and only keep the nouns from the abstracts (according to our experiences, non-noun words are not very informative); then a standard set of stop words are http://citeseer.ist.psu.edu/ http://opennlp.sourceforge.net/ Table 1: Performance of different algorithms on the task of recommending references on CiteSeer data FacetCube 0.110 0.326 0.463 0.870 1.088
FacetCube-D 0.110 0.325 0.462 0.868 1.086 removed; finally, the Porter stemming algorithm is applied to normalize the keywords. After these steps, we use cer-tain thresholds to filter out uncommon words, authors with few publications, and references being cited too infrequently. The final data set contains 15,072 papers with 6,821 distinct authors, 1,790 distinct keywords, and 21,894 distinct refer-ences.

We design the task as for a set of authors and a set of keywords, to recommend the most relevant references. Such recommendations can be helpful for authors to select a set of candidate references when writing a paper on a particular topic. The performance is measured in the following way. The 15,072 papers are randomly split evenly into a training set and a testing set. Papers in the training set are used to construct the data tensor where the three data dimen-sions are: author (  X  ), keyword (  X  ), and reference (  X  ). The standard NTF and FacetCube are applied to the data tensor to compute the facet matrices and the core tensor. During testing, for each paper in the testing set, we assume its au-thors and keywords are given. Relevant references are to be recommended by using [  X  ,  X  X  X   X  ,  X  X  X   X  ,  X  ] =  X   X  [  X  ,  X  X  X  where  X  X  X   X  and  X  X  X   X  are obtained by aggregating the authors and keywords, respectively, of the given paper. The perfor-mance is measured by comparing the recommendations with the ground truth, namely the real references cited in the papers in the testing set. The metric we used for the mea-surement is the average Normalized Discounted Cumulative Gain (NDCG [16]) for the top- X  answers, where NDCG is defined as In our performance study, we choose to use a binary rele-vance score for  X  X  X  X  (  X  ), i.e.,  X  X  X  X  (  X  ) = 1 if item  X  occurs in the testing data and 0 otherwise.

For the FacetCube algorithm, we show a case where the prior knowledge is used to improve the personalized recom-mendation. We construct a basis  X   X  for the authors by using the co-authorship relationship. More specially, we set  X   X  to be the normalized version of  X  +  X  X  X  where  X  is the identity matrix and  X  is the adjacency matrix representing the co-authorship relations in the training data, i.e.,  X  if authors  X  and  X  have co-authored a paper and 0 otherwise.  X   X  can actually be considered as the first order approxi-mation to the diffusion kernel (  X   X   X  X  X  )  X  1 and therefore by using  X   X  , we restrict the facets of authors to be embedded in the manifold of this diffusion kernel. In our experiments, we simply set  X  = 0 . 01.

We compare FacetCube with (a) the standard NTF and (b) a baseline in which the counts of reference for each key-word in the training data are stored and used to rank the references in the testing data. The baseline gives global (non-personalized) recommendations. For both the stan-dard NTF and FacetCube, we set the facet numbers to be  X  =  X  =  X  = 50 . Table 1 reports the NDCG scores at top- X  , for  X  = 1 , 5 , 10 , 50 , 100. As can be seen from the results, FacetCube slightly outperforms the standard NTF in all the cases and outperforms the baseline in most of the cases. FacetCube improves over the standard NTF because intu-itively, we enforce the authors who have co-authored papers in the training set to be in similar facets in the factorization.
To investigate the running time of our implementation, we conduct the following experiments. First, we construct a series of data sets with different sizes by randomly removing a portion of references. Fig. 4(a) shows the running time of FacetCube under different data sizes. For the factorizations, we set the condition for convergence as the relative error be-ing less than 10  X  4 , for which all the methods converge within a couple hundreds of iterations. The results in Fig. 4(a) ver-ify that the running time of our implementation is linear in the size of data set. Second, we fix the data set and test a wide range of facet numbers, from 10 to 100. In Fig. 4(b) we show the running time under different facet numbers for two ways of ordering data: one by  X   X  X  X  X  X  X  X  X  X  X  X  X  X ,  X  X  X  X  X  X  X  X  X ,  X  X  X  X  X  X  X  X  X  X   X  (best) and anther by  X   X  X  X  X  X  X  X  X  X  X ,  X  X  X  X  X  X  X  X  X ,  X  X  X  X  X  X  X  X  X  X  X  X  X   X  (worst). The former is the best because in this data set, the number of distinct keywords is much smaller than that of the refer-ences. From the curves we can see that the running time is about  X  (  X  3 ) (note the log-log scale in the figure). Also revealed is that a good order can make the implementation 10 to 50 times faster for this specific data set. Figure 4: Running time vs. number of non-zero entries in the data tensor (a), and vs. the number of facets (b).

In Fig. 5(a), we report the time for computing the top- X  queries by our implementation. Also shown is the lower bound of a naive implementation that computes the scores for all the candidates (for the naive approach, we only mea-sure the time for computing the scores but not for ranking, and so it is a lower bound). As can be seen, our computa-tion is much faster. For example, for the top-1 query, a naive approach will take at least 75 seconds for the testing data while our implementation takes less than 1 second. Next, we study how the Dirichlet prior influences the sparseness of the results. We run FacetCube with  X  = 10  X  100 and  X   X   X  1 to be a wide range of values. We report in Fig. 5(b) the num-ber of entries in the facet matrix  X  that are smaller than or equal to  X  . As can be seen, with smaller values of  X   X   X  1, we makes the facets more sparse. However, this sparseness has its costs. In Table 1, on the row of FacetCube-D, we give the NDCG scores for FacetCube with  X   X   X  1 set to be  X  10  X  50 As can be seen, the performance deteriorates a little bit.
The blog data was collected by an NEC in-house blog crawler. This data set contains 1,161 bloggers, 280,259 blog Figure 5: Time for computing the top- X  answers (a), and the level of sparsity (b). posts, with 19,390 keywords (nouns only), for 28 consecutive weeks, between March 02 to September 09, 2008. In addi-tion, we constructed an ontology for content by using data from the Open Directory Project (http://www.dmoz.org/). The ontology consists of 296 tree nodes, such as  X  X omput-ers Hardware X  and  X  X ociety Politics X . For each of the tree node, a description vector is constructed by aggregating the descriptions of all the Web sites under the given tree node. Figure 6: Two representative communities and their projections on the Open Directory ontology.

In this experiment we demonstrate a scenario where the user X  X  domain knowledge is used to directly control the fac-torization in the FacetCube framework. For this purpose, we fix the facets in the content dimension to be the 296 de-scription vectors and the facets in the time dimension to be 28 windowing functions that correspond to the 28 weeks in our data. Therefore, the free variables in this experiment are the blogger facet matrix  X  and the core tensor  X  . We set the number of blogger facets (i.e., the number of blogger communities) to be 10.
 It turns out that the blogger communities derived by the FacetCube algorithm all have certain focused topics, such as technology, politics, music, etc. In Fig. 6 we illustrate two representative blogger communities and their projections on the Open Directory ontology. The projections are computed by aggregating out the time dimension of the core tensor  X  and from the resulting matrix, identifying for each commu-nity the top 3 ontology tree nodes that have the largest values. As can be seen, community  X  focuses on different issues related to technology while community  X  focuses on political issues (as verified by the top keywords among the top bloggers in the communities).

Finally in Fig. 7, we show the temporal trends for these two communities by aggregating out the content dimension from the core tensor  X  . In addition, we also show some hot keywords among the community members during cer-tain temporal peaks. As can be seen, these hot keywords clearly indicate certain noteworthy events (e.g., the release of 3G iPhone in community  X  and the nomination of the vice presidential candidates in community  X  ). Figure 7: Temporal trends for the two representa-tive communities.
In this paper, we presented a novel framework, FacetCube, that extends the standard non-negative tensor factorization for extracting data characteristics from polyadic data. The FacetCube framework allows end users great flexibility in incorporating their prior knowledge at different levels in the process of characteristic extraction. In addition to describ-ing the new framework from the NTF point of view, we also made connection to the perspective of probabilistic genera-tive models. Furthermore, we provided iterative algorithms for solving the corresponding optimization problems as well as efficient implementations for handling large-scale data from real-life applications. Extensive experimental studies demonstrated the effectiveness of our new framework as well as the efficiency of our algorithm.
 The authors would like to thank Professor C. Lee Giles for providing the CiteSeer data set, and thank Koji Hino and Junichi Tatemura for helping prepare the blog data set. [1] A. Banerjee, S. Basu, and S. Merugu. Multi-way [2] J. D. Carroll, S. Pruzansky, and J. B. Kruskal. [3] P. A. Chew, B. W. Bader, T. G. Kolda, and [4] Y. Chi, S. Zhu, Y. Gong, and Y. Zhang. Probabilistic [5] Y. Chi, S. Zhu, K. Hino, Y. Gong, and Y. Zhang. [6] Y. Chi, S. Zhu, X. Song, J. Tatemura, and B. L. [7] L. De Lathauwer, B. De Moor, and J. Vandewalle. A [8] I. S. Dhillon, S. Mallela, and D. S. Modha. [9] C. Ding, X. He, and H. D. Simon. On the equivalence [10] R. Fagin. Fuzzy queries in multimedia database [11] D. FitzGerald, M. Cranitch, and E. Coyle.
 [12] E. Gaussier and C. Goutte. Relation between plsa and [13] R. A. Harshman. Foundations of the parafac [14] T. Hazan, S. Polak, and A. Shashua. Sparse image [15] T. Hofmann. Unsupervised learning by probabilistic [16] K. J  X  arvelin and J. Kek  X  al  X  ainen. IR evaluation methods [17] T. G. Kolda and B. W. Bader. Tensor decompositions [18] D. D. Lee and H. S. Seung. Algorithms for [19] Y. Lin, Y. Chi, S. Zhu, H. Sundaram, and B. L. Tseng. [20] B. Long, Z. Zhang, and P. S. Yu. A probabilistic [21] M. M X rup, L. K. Hansen, and S. M. Arnfred.
 [22] A. Shashua and T. Hazan. Non-negative tensor [23] J. Sun, C. Faloutsos, S. Papadimitriou, and P. S. Yu. [24] J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen. [25] L. R. Tucker. Some mathematical notes on three-mode
Proof. Assume that the values obtained from the pre-the update rule for  X  . The rules for  X  ,  X  , and  X  can be proved similarly. For the update rule of  X  , we can consider  X  ,  X  , and  X  as fixed (i.e., fixed as their values  X   X  ,  X   X  in the previous iteration). To avoid notation clutters, we define  X   X   X  . =  X   X   X   X  ,  X   X   X  . =  X   X   X   X  ,  X   X  objective function as First define and where obviously we have
Then we have  X   X  (  X  ) =
X  X  = =  X  . =  X  (  X  ;  X   X  ) , where  X  1 ,  X  2 , and  X  3 are constants irrelevant to  X  . Note that in the last step of the above derivation, we used the fact that P columns of  X  all sum to 1.

It can be easily shown that  X   X  (  X  ;  X   X  ) is an auxiliary func-tion of  X   X  (  X  ) in the sense that 1.  X   X  (  X  )  X   X   X  (  X  ;  X   X  ), and 2.  X   X  (  X  ) =  X   X  (  X  ;  X  ).
 So the problem is reduced to minimizing  X   X  (  X  ;  X   X  ) with respect to  X  , under the constraint that all the columns of  X  sum to ones. We define the Lagrangian and by taking its derivative and setting the result to zero, we have which gives the update rule for  X  in Theorem 1.
