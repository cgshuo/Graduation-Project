 Many generative language and relevance models assume con-ditional independence between the likelihood of observing individual terms. This assumption is obviously na  X   X ve, but also hard to replace or relax. There are only very few term pairs that actually show significant conditional dependencies while the vast majority of co-located terms has no implica-tions on the document X  X  topical nature or relevance towards a given topic. It is exactly this situation that we capture in a formal framework: A limited number of meaningful de-pendencies in a system of largely independent observations. Making use of the formal copula framework, we describe the strength of causal dependency in terms of a number of established term co-occurrence metrics. Our experiments based on the well known ClueWeb X 12 corpus and TREC 2013 topics indicate significant performance gains in terms of retrieval performance when we formally account for the dependency structure underlying pieces of natural language text.
 Information Systems [ Information Retrieval ]: Retrieval models Relevance models; Multivariate relevance; Ranking; Proba-bilistic framework; Language models.
Generative n-gram language models are frequently used tools for representing document or collection vocabulary in the form of probability distributions over (spans of) textual tokens. They are popular for a wide array of tasks, includ-ing sentiment analysis, machine translation, content based classification and document retrieval. Most state-of-the-art models assume individual terms to be independently drawn from the underlying distribution. While this independence c  X  assumption greatly simplifies the computation of conditional amples of proper names such as  X  Barack Obama  X  or  X  Hong Kong  X , but also other fixed expressions ( X  tax evasion  X ) as well as non-consecutive constructs ( X  dollar  X  ,  X  stock  X ), that should benefit from an explicit model of term interdepen-dency. It is easy to see that some of these examples go be-yond the capabilities of mere higher-order language models. In the past, there have been a number of attempts to for-mally integrate term dependency structures into generative models.

Van Rijsbergen X  X  early work on dependency trees [19] the-oretically establishes the use of maximum spanning trees and term co-occurrence statistics in order to establish local term dependency structures. Yu et al. [20] present a comparison of tree and cluster-based methods for dependency modelling in information retrieval. Srikanth and Srihari [18] inves-tigate dependency-aware relevance models by using higher order n-gram models and comparing to the unigram setting. They report consistent improvements for the dependency models. Croft et al. [7] propose the use of inference net-works trained on the basis of term proximity information to model the relevance of entire phrases. In a related ef-fort, Lossee [12] confirms the important role of proximity in dependency modelling. The author reports optimal per-formance using context windows of 3-5 terms surrounding each candidate term. In a comparison study of multiple de-pendency models, Bruza and Song [4] achieve best results using a matrix representation of co-occurrence contexts to describe terms. Gao et al. [11] explicitly decouple the de-pendency structure of a sentence from the concrete term generation probabilities in the form of linkages.

Nallapati and Allan [14] relax the independence assump-tion by modelling documents as groups of (still independent) sentences. Within each sentence, however, they condition the probability of observing a term on all previous terms in the same sentence. Their sentence model is based on the maximum spanning tree over the fully connected sentence graph. The individual strength of dependency within term pairs is measured in terms of the Jaccard co  X  efficient. They later refine this model by advancing from dedicated sentence trees to entire forests [15] of trees for each connected com-ponent in the sentence graph. Cao et al. [5] use Bayesian networks to combine two sources of term dependence: co-occurrence and semantic relatedness. The latter is expressed in terms of proximity in the WordNet graph. While most approaches define document language models and compare their respective likelihoods of having generated the query, Bai et al. [1] propose a term-dependency query model in or-der to account explicitly for query expansion. Metzler and Croft [13] learn Markov Random Fields that account for various forms of term dependence on the basis of arbitrary feature vectors. Bendersky et al. [3, 2] propose a learning to rank approach that accounts for higher order term and concept dependencies using hypergraphs. Shi and Nie [17] investigate different dependence weighting schemes based on the concept X  X  utility in the respective retrieval task.
In this paper, we present the use of copulas, a robust statistical model family that is able to explicitly decouple marginal observations (i.e., the individual likelihoods of gen-erating terms) from their underlying dependency structures. Comparing to a number of well-known baseline methods and evaluating on a large-scale standard dataset, we show the competitive performance of this novel approach to depen-dency modelling.
In this section, we will give a brief overview of the for-mal copula framework, introducing the relevant notation and conventions. For a more comprehensive introduction to the topic, please refer to the previous IR applications by Eickhoff et al. [8, 9], or the surveys by Embrechts [10] and Schmidt [16].
 Let X be a k -dimensional random vector of observations that we wish to use as input to our copula model: X k = ( x 1 ,x 2 ,...,x k ) The copula allows us to model the likelihood of observing X by offering computationally efficient approximations to the true joint probability distribution in the high-dimensional space of cardinality k . As a first step, copulas require input scores U k to be uniformly distributed in the [0 ... 1] inter-val. We can achieve this by defining a set of transformations F ( X ) between raw marginal observations X and their nor-malized equivalents on the unit cube U .

U k = ( u 1 ,u 2 ,...,u k ) = F ( X ) = ( f 1 ( u 1 ) ,f 2 An easy example of such a function is the empirical distri-bution function  X  f :  X  f ( t ) = 1 n P 1 { x i  X  t } The cumulative distribution function C for all copulas is fully defined in terms of a generator  X  and its inverse  X 
C ( u 1 ,u 2 ,...,u k ) =  X   X  1 (  X  ( u 1 ) +  X  ( u 2 ) + ... +  X  ( u There are many concrete instantiations of such copula func-tions. Each copula family defines their own generator and inverse. A previous study [8] compared a wide range of cop-ula families for the task of Web retrieval, finding Gumbel copulas to be the most adequate choice in this setting. For reasons of space, this paper builds on the previous findings and concentrates exclusively on Gumbel copulas. Their gen-erators are given in the following form:  X   X  1 ( t ) = exp (  X  t 1  X  )  X  ( t ) = (  X  log ( t ))  X  The resulting distribution function for a 2-dimensional Gum-bel copula is, for example:
C ( u 1 ,u 2 ) = exp (  X  ((  X  log ( u 1 ))  X  + (  X  log ( u Once the choice of copula family is made, we are left with just a single parameter  X  that allows us to control the strength of dependency between the individual marginal observations u . If we, for example, set  X  = 1, our distribution function defaults to the case of conditional independence:
C  X  =1 = exp (  X  (  X  log ( u 1 )) + (  X  log ( u 2 ))) = u Any choice of  X  &gt; 1 results in an increasing degree of con-ditional dependency between the k dimensions of our ob-servation. At this point, we have introduced all relevant components for our original use case of statistical language modelling. A traditional unigram language model describes the likelihood of observing a string of text T under a given class c as the product of the individual likelihoods of each term: P ( T | c ) = P ( t 1 | c ) P ( t 2 | c ) ...P ( t | T | | c ) The same can be achieved under the copula framework by considering the class-conditional probabilities of observing individual terms t 1 ,t 2 ,... as our marginal observations, mak-ing the dimensionality of our copula k = | T | :
P ( T | c ) = C  X  =1 ( P ( t 1 | c ) ,P ( t 2 | c ) ,...P ( t By choosing  X  = 1, we ensure conditional independence be-tween the marginal term observation likelihoods, giving us the standard unigram language model. As we however in-crease  X  , the strength of dependency between the individual terms increases. This ability to account for term depen-dence makes the copula framework a powerful alternative to the standard language modelling scheme. At this point, any setting of  X  globally describes the relationship between all terms. In practice, however, we much rather want a select few terms to depend on each other, while the majority of terms occur indeed independently.

This is easily achieved by using nested copulas. Instead of combining all dimensions in a single step as described earlier, they allow for a nested hierarchy of multiple copu-las that estimate joint distributions for sub sets of the full term space and subsequently combine scores until one global model is obtained. Generally, an example of a fully nested copula with k dimensions is given by: By means of the structure of the nesting  X  tree  X , nested cop-ulas can explicitly model which dimensions depend on each other directly. Instead of the global  X  parameter discussed earlier, each of the constituent copulas defines their respec-tive  X  i , determining the strengths of these (per-dimension) dependencies. This mechanism gives nested copulas a theo-retical advantage in flexibility over their non-nested coun-terparts. Effectively, this allows us to describe formally grounded probabilistic models under which select term pairs show dependencies (  X  &gt; 1) while the majority of terms oc-cur independently of each other (  X  = 1). Figure 1 shows an example of such a situation.

At this point, the final missing component in our language modelling scheme is a way to determine the concrete settings of  X  for a pair of terms. To this end, we define conditional dependency in terms of frequency of co-occurrence in a doc-ument corpus and rely on two widely used co-occurrence metrics. The point-wise mutual information between terms t and t 2 as well as their Jaccard coefficient measure in which fraction of sentences the terms co-occur.
 Finally, the dependency parameter  X  t 1 ,t 2 is defined on the basis of the concrete choice of metric m  X  { PMI ,J } and its collection-wide metric mean (  X  PMI ,  X  J ) across all po-tential term pairs. All those pairs of higher-than-average co-occurrence frequency are assigned values of  X  proportion-ally to their relative co-occurrence rate. Since  X  is defined in the range [1 ,  X  ) and the resulting scores scale in a non-linear fashion, there is no need to further address or remove outlier pairs of extremely high frequency.
To empirically test the performance of the previously pre-sented copula-based language modelling scheme, we investi-gate its performance at the task of adhoc document retrieval. Instead of modelling the likelihood of observing a given doc-ument under a topic specific language model, we will now es-tablish one distinct model per document and compare their respective likelihoods of having generated the query q . P ( rel | q,d )  X  P ( q | d )
P ( q | d ) indep = P ( q | d ) cop = C d ( w 1 ,w 2 ,...,w n ) | w  X  q
For our experimental comparison, we rely on the widely used ClueWeb X 12 corpus, a collection of 730 million authen-tic Web documents. Our 50 topics originate from TREC X  X  2013 Adhoc retrieval task [6]. We contrast our method X  X  Table 1: Retrieval performance on ClueWeb X 12 and TREC 2013 Adhoc topics at a cut-off threshold of 20 retrieved documents.
 Unigram LM 0.31 0.22 0.26 0.41 Bigram LM 0.34 0.26 0.3 0.45
Copula LM 0.41* 0.35* 0.38* 0.52 performance with a number of established as well as state-of-the-art baselines such as standard unigram and bigram language models, Nallapati X  X  sentence trees [14], as well as the Markov Random Field model [13] and apply Laplace smoothing to all LM variants in order to account for pre-viously unseen query terms. Table 1 details the respective performances obtained by the various methods in terms of precision, recall, F 1 and MAP, each computed at a cut-off rank of 20 retrieved documents. Statistically significant im-provements over all baseline methods are indicated by the asterisk character. Statistical significance was tested using a Wilcoxon signed-rank test at  X   X  0 . 05-level. We can note that, due to their wider context, the classification perfor-mance of bigram language models significantly exceeds that of the lower-order model. SenTrees as well as the MRF model which explicitly capture term dependence show even higher classification performance. Finally, our copula lan-guage model yields significant performance improvements across most metrics and baselines. The improvements over the MRF model were only significant for some of the con-sidered metrics.
In this paper, we demonstrated the use of the copula framework, a model family from the field of robust statis-tics, for representing term dependencies in language models. The main advantages of the proposed model are its formal rigour, the low model complexity in terms of training effort as well as disk space requirements and its high degree of flexibility. As an additional advantage, copulas have been previously shown [9] to be beneficial for qualitative manual inspection of results.

Our experiments, based on a sizeable document collection (ClueWeb X 12) confirm the competitive performance of the proposed model in comparison with a number of state-of-the-art baselines.
The present paper describes early results of an ongoing body of research. Consequently, there are numerous direc-tions for future work that are interesting to explore: (1) In this paper, we investigated  X  X ingle-layer X  dependency struc-tures with a nesting depth of 1. The nested copula frame-work, however, is able to capture arbitrarily complex struc-tures. Given a modified dependency estimation scheme, the model can easily account for cases of fine-grained multi-level dependencies. (2) Similarly, the current model regards only dependencies of degree 2. Since the copula framework is able to account for higher-degree dependencies (i.e., be-tween three or more terms) this is another promising alley for continued research. (3) Previous work has investigated different forms of inter-term dependency, including, for ex-ample, semantic proximity. It would be easy to integrate such additional sources of evidence into our  X  estimation step. (4) We would like to draw from the existing wealth of topic modelling techniques in order to describe not merely the dependency structure between individual terms but also between terms and more high-level (latent) concepts, allow-ing for exciting new insights. (5) Finally, we would like to explore means of representing local term context into the de-pendency model. Take for instance the two terms  X  X ew X  and  X  X ork X . In the query [affordable real estate on the US east coast] these terms clearly have some dependency, whereas they probably are less dependent when the query is [afford-able real estate in yorkshire, UK]. The current paper high-lights the applicability of the context invariant method for general queries. In the future we will additionally investigate the importance of the immediate context. [1] Jing Bai, Dawei Song, Peter Bruza, Jian-Yun Nie, and [2] Michael Bendersky and W Bruce Croft. Modeling [3] Michael Bendersky, Donald Metzler, and W Bruce [4] Peter Bruza and Dawei Song. A comparison of various [5] Guihong Cao, Jian-Yun Nie, and Jing Bai. Integrating [6] Kevyn Collins-Thompson, Paul Bennett, Fernando [7] W Bruce Croft, Howard R Turtle, and David D Lewis. [8] Carsten Eickhoff and Arjen P de Vries. Modelling [9] Carsten Eickhoff, Arjen P de Vries, and Kevyn [10] P. Embrechts, F. Lindskog, and A. McNeil. Modelling [11] Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and [12] Robert M Losee. Term dependence: truncating the [13] Donald Metzler and W Bruce Croft. A markov [14] Ramesh Nallapati and James Allan. Capturing term [15] Ramesh Nallapati and James Allan. An adaptive local [16] T. Schmidt. Coping with copulas. Risk Books: Copulas [17] Lixin Shi and Jian-Yun Nie. Using various term [18] Munirathnam Srikanth and Rohini Srihari.
 [19] Cornelis Joost van Rijsbergen. A theoretical basis for [20] Clement T Yu, Chris Buckley, K Lam, and Gerard
