 local communication structure of distrib uted algorithms lik e belief propag ation. 2.1 Secur e Multiparty Function Computation Let f ( x k parties would lik e to jointly compute the value of f ( x circumstances, a third party is surprisingly unnecessary .
 to reveal nothing beyond what it implied by the value of f . ality that each input x f ending with every party learning the value y = f ( x Definition 1 1 Let  X  be any protocol for the k parties to jointly compute the value y = of from being told the final output y of  X  (and their pri vate input x while y itself may  X  X eak X  some information about the other pri vate inputs x and randomized functionalities, which we need for our technical results. Theor em 1 (See e.g. [6]) Let f ( x k in whic h party i learns nothing not alr eady implied by their private input x nothing from the protocol, while the other learns which player is wealthier . cal message-passing structure of belief propag ation and similar algorithms. 2.2 Public-K ey Encryption with Blinding E Informally , the system should have the follo wing security properties: to factoring N , and thus intractable without the secret key. w refer to this operation as blinding an encrypted bit. which the underlying netw ork is a tree. We denote by V ( X and by N ( X function  X  X step, a node X by all possible assignments x Once verte x X to their product. That is, if x variables, we can simply hard-wire this evidence into the potential functions  X  this case it is well-kno wn that the algorithm computes the conditional mar ginals P [ X 3.1 Mask Pr opagation and the Pri vacy-Pr eser ving Pr otocol We assume that at the beginning of the pri vacy-preserving protocol, each node X indi vidual potential function  X  Recall that our fundamental pri vacy goal is to allo w each verte x X distrib ution P [ X In particular , X neighbors. Kno wledge of  X  tion about the mar ginals over X and thus fall in Z we will mak e frequent use of the follo wing simple fact: for any fix ed x  X  Z does not kno w r , while lea ving it readable to a party that does. us focus on the computation of  X  algorithm, we mak e the follo wing inducti ve message and kno wledge assumptions: case for the induction. No w under these informational assumptions, verte x X {  X  `  X  i ( x i ) +  X  j,` ( x i ) mo d N : X `  X  N ( X i ) \ X j , x i  X  V ( X i ) } I = {  X  j,` ( x i ) mo d N : X `  X  N ( X i ) \ X j , x i  X  V ( X i ) } . Let us first consider the case in which X itself. In order to complete the inducti ve step, it will be necessary for each X pro vide a set of masking values  X   X  Verte x X Z  X  inputs I Since this expression can be computed jointly by X allo wing X masking value  X  After this masking process has been completed for all X X  X  satisfied at X uniformly at random in Z It remains to consider the case in which X acceptable for X its final mar ginal. From their joint input I  X  efficiently compute this value in such a way that X At the end of the message-passing phase, each internal (non-leaf) node X messages from each of its neighbors. In particular , for each pair X that X Theorem 1 again, we can construct an efficient protocol for X compute the mar ginals such that X Each leaf verte x X from its neighbor X able X ready computable in polynomial time from only P [ X T We briefly note a number of extensions to Theorem 2 and the methods described abo ve. information about its own mar ginal.
 ple, the mar ginals of other nodes within the same clique.
 NashPr op and Other Message-P assing Algorithms: The methods described here can also be on values in { 0 , 1 } , but this assumption is easy to relax. If a node X neighbors will be represented by a table T range over all 2 d possible assignments ~x to N ( X value P [ X in the d + 1 st column in the row corresponding to the assignment ~x . rent values of their neighbors. More precisely , at each step, a variable X Its current value is replaced by randomly dra wing value 1 with probability T probability 1  X  T so that no verte x is required to communicate with a verte x more than two hops away. assigning each verte x X will hold a second bit b 0 residues described in Section 2.2, though other choices are possible. We begin by describing a sub-protocol for preprocessing the table T S be the 2 d indices of the rows of the table T of such a way that during the protocol, each V {  X  ( ~x ) : V j = 1 } and X i learns nothing.
 The sub-protocol is quite simple. First each neighbor V own public key and passes the encrypted column to X own public key. X version of T key of V X clearte xt and encrypted table entries. For instance, without blinding abo ve, N  X  ( X struct the permutation chosen by X No w from the perspecti ve of N  X  ( X random bits. N  X  ( X In the next step in the protocol, N  X  ( X keys (those of V table back to X j will be encrypted by the public key of V encrypted by both X protocol. We inducti vely assume that at the start of each step, for each X of been learned is the new value of a particular node X Consider a neighbor V to its value being 0 and which rows correspond to its values being 1 . While V what its current value is, V out which rows of the permutation correspond to V V , N  X  ( V to After this secure computation of partitions has been completed for all neighbors of X X decrypt the value in column d + 1 , use this value in order to sample X once again, this means that we can construct an efficient protocol for X complete these computations in such a way that the y only learn the new bits b Each time the value of a node X permuting the rows of T 0 computation between each node X final value.
 updating schedule will also hold for the secure version.

