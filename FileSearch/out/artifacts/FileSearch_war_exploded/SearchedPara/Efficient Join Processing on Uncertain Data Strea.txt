 Join processing in the streaming environment has many practical applications such as data cleaning and outlier detection. Due to the inherent uncertainty in the real-world data, it has become an increasingly important problem to consider the join processing on uncertain data streams, where the incoming data at each timestamp are uncertain and imprecise. Different from the static databases, processing uncertain data streams has its own requirements such as the limited memory, small response time, and so on. To tackle the challenges with respect to efficiency and effectiveness, in this paper, we formalize the problem of join on uncertain data streams (USJ), which can guarantee the accuracy of USJ answers over un-certain data, and propose effective pruning methods to filter out false alarms. We integrate the pruning methods into an efficient query procedure for incrementally maintaining USJ answers. Ex-tensive experiments have been conducted to demonstrate the effi-ciency and effectiveness of our approaches.
 H.2.4 [ Database Management ]: Systems X  Query processing ; G.3 [ Mathematics of Computing ]: [Probability and Statistics]; H.2.8 [ Information Systems ]: Database Management X  Database appli-cations, Spatial databases and GIS Algorithms, Design, Experimentation, Performance, Theory Uncertain data stream, join, USJ
Recently, uncertain data analysis has become an increasingly im-portant issue due to the ubiquitous data uncertainty in many real-world applications such as sensor data monitoring [12, 18], location-based services (LBS) [26], RFID networks [14], object identifica-tion [3], and moving object search [6]. As an example, in sensor networks, sensory data often contain noises, resulting from envi-ronmental factors, packet losses, and/or low battery power. Given another example, in LBS applications, the positions of mobile users can be measured by Global-Positioning System (GPS). However, GPS data are often imprecise for various reasons such as clock er-rors, ephemeris errors, atmospheric delays, and multipathing and satellite geometry. Sometimes, the trajectory data of mobile users are even intentionally blurred by a trustworthy third-party, for the sake of privacy preserving [26]. Therefore, in real applications, it is not uncommon to encounter uncertain and imprecise data, and we have to efficiently and effectively answer queries on such data.
In many scenarios like sensor data monitoring or LBS applica-tions mentioned above, a large volume of uncertain data are often collected at a high speed in a streaming manner [11]. Due to the real-time requirement in many emerging applications, for example, monitoring moving objects or detecting dangerous events by ana-lyzing sensory data, it is quite crucial to efficiently process queries online over continuously-arriving uncertain data streams.
Join processing over uncertain data streams is useful in many practical applications such as data cleaning [14], outlier detection [15], and object tracking and monitoring [11]. For example, Knorr and Ng [15] defined the distance-based outliers as objects o such that fewer than n objects lie within  X  distance from o , where n is a user-specified threshold. Therefore, to detect an outlier, we need to find pairs of objects that are within  X  distance from each other. In the application of object tracking, we may want to detect abnormal behavior of a mobile user (e.g. taking a wrong bus or encounter-ing a sudden slow-down) by comparing his/her real-time trajectory with a normal one, and alert his/her family or elder-care center im-mediately. In this example, the reported position of the user in the trajectory can be considered as a data object at a timestamp, which is uncertain due to the GPS accuracy. We can conduct a join be-tween this user X  X  current trajectory and that of the normal one (both in a streaming manner), and monitor similar pairs of objects within a period. If the position of the user becomes a distance-based out-lier among data objects in his/her normal trajectory, then it indicates that this user might behave abnormally.

As another example, in the application of protecting the public safety, the real-time trajectory of each suspicious criminal (or ter-rorist) can be monitored by the police 24 hours per day. Thus, with these data collected in a streaming fashion, it is important to an-swer questions such as  X  X hether or not two people recently go to the same place within a short period? X  or  X  X hat is the period that they meet with each other possibly planning sth.? X . The positions of suspicious people can be either obtained by GPS or tracked by the police. Due to the GPS accuracy or the delay of the report, the collected position information may not reveal the actual values, and is often noisy and uncertain. Therefore, in this case, we need to on-line conduct a join on such uncertain data streams, and efficiently obtain pairs of suspicious people who satisfy the join predicate (i.e. close to each other during a period) with high confidence, which exactly corresponds to a join operator on uncertain data streams.
Previous works on stream join mainly focus on the equality join , including XJoin [32], hash merge join (HMJ) [27], and rate-based progressive join (RPJ) [31], and these studies always assume that data are precise points. In the context of uncertain data streams, the existing works investigate frequent items [34], top-k query [13], and skyline query [35]. In contrast to previous works, we study the similarity join (with range predicate) over uncertain data streams.
Basically, there are three challenging issues for join processing over uncertain data streams. First, to deal with uncertain data, we have to provide an accuracy guarantee for our join answers. Sec-ond, in the streaming environment, we need to design efficient ap-proaches in order to online process a large volume of uncertain data and report join answers with small response time. Third, since the stream processing is usually conducted in memory without disk ac-cesses, the size of the available memory is often limited, which requires join processing techniques with small memory consump-tion. In addition, due to the time-varying characteristics of uncer-tain stream data, our join processing should be able to adapt to such data changes with respect to the three issues above.

To tackle these important issues, in this paper, we first formally define a query operator, namely join on uncertain data streams (USJ), providing the probabilistic confidence of the join results on uncertain data. Then, we propose efficient approaches to perform USJ processing, including a framework for incrementally maintain-ing the USJ results and effective pruning methods to reduce the USJ search space.

Specifically, we make the following contributions in this paper. 1. We formalize the problem of join on uncertain data streams 2. We provide a general framework for performing USJ in Sec-3. We propose effective pruning methods in Section 5 to filter 4. We conduct extensive experiments in Section 7 to confirm
In addition, Section 2 overviews previous works on stream join processing and uncertain query processing. Section 8 concludes this paper.
In this section, we present the previous works on stream process-ing on precise data, as well as query processing in static uncertain databases and streaming environment.
Query processing on data streams has many important applica-tions such as real-time processing of data collected from sensor networks [5]. Previous works studied query processing techniques on data streams, including the top-k query [10], skyline query [30], join [32, 27, 31], and so on.

Different from join in static databases (e.g. spatial join [4]) where spatial index is allowed to be offline constructed, the stream join is conducted online in memory. The existing works on join over data streams include XJoin [32], hash merge join (HMJ) [27], and rate-based progressive join (RPJ) [31]. These works mainly focus on the equality join over precise data points, which allow disk accesses and can move unprocessed data onto disk for later join operation. In contrast, our USJ problem considers the similar-ity join with range predicate on streams where data are uncertain and imprecise. Moreover, USJ assumes that the entire join process is completed in memory without any I/O involved. Thus, previous methods cannot be directly applied to our USJ scenario.
Data uncertainty is usually classified into two categories [29], tu-ple uncertainty and attribute uncertainty . Specifically, the tuple un-certainty is usually used to model the probabilistic relational data in probabilistic databases [9]. Each object in probabilistic databases is called an x-tuple (or generation rule ), which consists of sev-eral instances (a.k.a. alternatives ) associated with certain proba-bilities. Note that, in a probabilistic database, each x -tuple may have the chance of not belonging to the database. In contrast, the attribute uncertainty is often considered in the context of uncertain databases, where each uncertain object is modeled by an uncer-tainty region , in which object can reside following any distribu-tion. The object distribution can be represented by either probabil-ity density function (pdf) [7] or discrete samples [28]. In our work, we consider the attribute uncertainty with sample representation.
Due to the data uncertainty, queries on uncertain data need to consider the confidence guarantee of query answers. Thus, many query types have to be re-defined in the uncertain scenario. For ex-ample, in literature, previous works studied queries over uncertain databases, including probabilistic range query (PRQ) [7], nearest neighbor (PNN) [7, 17, 1], group nearest neighbor (PGNN) [20], reverse nearest neighbor (PRNN) [22], static skyline (PSQ) [28], reverse skyline (PRSQ) [19], ranked query (PRank) [21], inverse ranking query (PIR) [23], probabilistic top-k dominating query (PTD) [24], similarity join (PSJ) [8, 16, 25], and so on.
In particular, Cheng et al. [8] considered the join over two (static) uncertain databases containing 1-dimensional noisy sensory data, with either of the three join predicates  X  X quality X ,  X  X reater than X , or  X  X maller than X . In order to facilitate the join processing, static in-terval R-trees are constructed over 1-dimensional uncertain data. In contrast, our work focuses on the similarity join (with range predi-cates) on multidimensional uncertain data in the (dynamic) stream-ing environment. Note that, it is not efficient to online construct a tree-based indexing structure in our USJ problem, which would incur high cost of updating/querying, especially when data arrive with fast input rate. Therefore, the proposed techniques for offline join processing in [8] are not suitable for our USJ problem. Kriegel et al. [16] proposed an efficient approach to perform the join by traversing two (static) R-tree indexes built on samples of uncer-tain objects. Ljosa and Singh [25] studied the join over uncertain data with a domain-specific similarity measure used in cognitive science, biogeography, and ecology. In these works, since uncer-tain databases are assumed to contain static uncertain objects, index or pre-computation is allowed. Our USJ problem, however, investi-gates the join on uncertain data streams (instead of static databases) in a Euclidean space (note: we would like to leave the interesting topic of considering other similarity measures as our future work), which is more challenging due to the constraints of stream process-ing such as the limited processing power and the available memory. Thus, techniques proposed in previous works are not suitable for our USJ scenario.

Processing uncertain data streams has recently received much at-tention from the database community due to many of its practical applications such as sensor data monitoring [11]. Recent studies in-vestigated frequent items [34], top-k query [13], and skyline query [35] on probabilistic data streams. Yeh et al. [33] explored approx-imate range queries on an uncertain time-series stream. Singh et al. [29] studied join on two probabilistic tables, which is a direct product of 2 static tables without pruning, whereas our work pro-poses pruning methods to help improve the join efficiency. Diao et al. [11] proposed an architecture of an uncertainty-aware stream system, in which the join operator is mentioned with a nice motiva-tion example in real applications of object tracking and monitoring. The authors, however, did not present specific techniques or algo-rithms in detail to efficiently process the join. In contrast, our work formulates the USJ problem with the confidence guarantee, and de-signs effective approaches (e.g. pruning methods) to speed up the USJ efficiency.
In this section, we formally define the problem of join on uncer-tain data streams (USJ). Without loss of generality, throughout this paper, we consider two uncertain data streams. Nonetheless, the methodology proposed in this work is applicable to the case with multiple uncertain data streams.
 Figure 1: Illustration of Join on Uncertain Data Streams (USJ)
Figure 1 illustrates the scenario of our USJ problem over two uncertain data streams T 1 and T 2 . Each uncertain data stream consists of a sequence of continuously-arriving uncertain objects dimensional uncertain object at timestamp i , and t is the current timestamp. As mentioned in applications such as LBS, we may want to retrieve close pairs of objects within a period. Thus, we adopt the concept of sliding window [35] for the uncertain stream join operator. Specifically, as shown in Figure 1, a USJ opera-tor always considers the most recent w uncertain data in streams, that is, W ( T 1 ) = { X [ t  X  w + 1] , X [ t  X  w + 2] , ..., X [ t ] } and W ( T 2 ) = { Y [ t  X  w + 1] , Y [ t  X  w + 2] , ..., Y [ t ] } at the cur-rent timestamp t . In other words, when a new uncertain object X [ t + 1] ( Y [ t + 1] ) comes in at the next timestamp ( t + 1) , the new object X [ t + 1] ( Y [ t + 1] ) is appended to T 1 ( T while, the old object X [ t  X  w + 1] ( Y [ t  X  w + 1] ) expires and is evicted from the memory. Thus, USJ at timestamp ( t + 1) is conducted on a new sliding window { X [ t  X  w + 2] , ..., X [ t + 1] } ( { Y [ t  X  w + 2] , ..., Y [ t + 1] } ) of size w . Here, we consider the append-only uncertain data streams [35], that is, without any dele-tion involved, and leave the interesting topic that handles USJ in the presence of deletions as our future work.

The uncertain object at each timestamp in uncertain data streams is represented by a number of d -dimensional discrete samples [28] (indicating the discrete object distribution). Each sample of the un-certain object is associated with an appearance probability , provid-ing the probability that object appears at the position of this sample. All the samples of an uncertain object are mutually exclusive (i.e. uncertain object cannot reside at more than one place at the same time in reality). In LBS or object tracking applications, the col-lected position data of targets obtained from different positioning devices can be modeled as samples of an uncertain object 1
For simplicity, in this work, we assume each uncertain object has the same number (denoted as l ) of samples (though our pro-posed approaches do not rely on this assumption). Figure 2 illus-trates an example of the uncertain object X [ i ] at timestamp i in uncertain data stream T 1 . Specifically, uncertain object X [ i ] con-( 1  X  k  X  l ) has the appearance probability x k [ i ] .p  X  (0 , 1] satisfy-ing ..., y l [ i ] } at timestamp i in uncertain data stream T has appearance probability y k [ i ] .p  X  (0 , 1] satisfying = 1 . Note that, here we assume uncertain object X [ i ] always exist in the real world 2 (i.e. the summed appearance probability for all samples of X [ i ] equals to 1).

After we introduce the basic model for our uncertain data streams, we now give the formal definition of USJ operator as follows.
D EFINITION 3.1. ( Join on Uncertain Data Streams, USJ ) Given two uncertain data streams T 1 and T 2 , a distance threshold  X  , and a probabilistic threshold  X   X  (0 , 1] , a join on uncertain data streams (USJ) continuously monitors pairs of uncertain objects X [ i ] and Y [ j ] within sliding windows W ( T 1 ) and W ( T 2 ) , respectively, of size w at the current timestamp t , such that: holds, where t  X  w + 1  X  i, j  X  t , and dist (  X  ,  X  ) is a Euclidean distance function between two objects.

In Definition 3.1, to perform a USJ, users need to register two parameters, distance threshold  X  and probabilistic threshold  X  . In particular, due to the data uncertainty, users can use the probabilis-tic threshold  X  to control the accuracy (or confidence) of our USJ answers. That is, we want to obtain pairs of close uncertain objects from two uncertain streams with a confidence never below thresh-old  X  .

Since each uncertain object at a timestamp consists of l samples, the joining probability Pr { dist ( X [ i ] , Y [ j ])  X   X  } in Inequality (1) can be rewritten via samples as:
Note that, one straightforward method to directly perform USJ over sliding windows is to follow the USJ definition. That is, for every object pair  X  X [ i ] , Y [ i ]  X  from sliding windows W ( T W ( T 2 ) , respectively, we compute the joining probability that X [ i ] is within  X  distance from Y [ i ] (via samples) based on Eq. (2). If the resulting probability is greater than or equal to probabilistic thresh-old  X  , then this pair  X  X [ i ] , Y [ i ]  X  is reported as the USJ answer; otherwise, it is a false alarm and can be safely discarded.
However, this straightforward method requires extensive compu-tation cost, that is, O ( w 2  X  l 2  X  d ) , where w is the size of sliding win-dows, l is the sample size, and d is the dimensionality of data sets. Specifically, in the time complexity above, the term w 2 is due to the nested loop calculation for pairwise uncertain objects within sliding windows; the cost ( l 2  X  d ) results from the probability calculation in Eq. (2) involving distance computations between d -dimensional samples. Taking the case where l = 100 , d = 3 , and w = 1 , 000 as an example, the total cost is as high as (3  X  10 10 ) computation units, which is very costly. In addition, at each timestamp, it is not efficient to conduct the straightforward method on the new sliding windows and re-compute every USJ answer from scratch.

Due to the inefficiency of direct computation, in the sequel, we aim to design an efficient approach to incrementally maintain USJ answers over sliding windows of uncertain data streams. In par-ticular, we propose a framework for conducting USJ processing incrementally, in which we illustrate effective pruning methods to reduce the search space. Table 1 summarizes the commonly-used symbols in this paper. As illustrated in Figure 3, we propose a general framework, namely USJ_Framework , for our USJ processing in uncertain data streams. Initially, when sliding windows W ( T 1 ) and W ( T 2 ) are not full (i.e. with size below w ), we compute the USJ answers between the two windows directly (or we can also consider it as a special case of incremental update without the expired data, as will be described later), and add them to a candidate set S cand (lines 1-2). After the sliding windows become full with size w , procedure USJ_Framework will incrementally update the sliding windows as well as the candidate set S cand , upon the arrival of each new uncertain object from uncertain data streams (lines 3-8). Specif-ically, at a new timestamp ( t + 1) , we can obtain two uncertain objects X [ t + 1] and Y [ t + 1] from each uncertain streams T T , respectively (line 4). First, we maintain the sliding windows by adding new objects X [ t + 1] and Y [ t + 1] to and removing the expired objects X [ t  X  w + 1] and Y [ t  X  w + 1] from W ( T W ( T 2 ) , respectively (line 5). Then, we update the candidate set S cand . That is, we insert USJ candidate pairs  X  X [ t + 1] , Y [ j ]  X  or  X  X [ i ] , Y [ t + 1]  X  satisfying Inequality (1) (obtained by invoking a procedure Obtain_Cand_Pairs ) into S cand and delete those ex-pired pairs from S cand (lines 6-7). Finally, we report the actual USJ answers in S cand , and wait for processing data that come in at the next timestamp (line 8).

Note that, instead of exhaustively checking pairwise objects in sliding windows from scratch for each stream update, our frame-work in Figure 3 is able to incrementally maintain a USJ candidate set S cand at a lower cost, that is, at most O ( w  X  l 2  X  d ) rather than O ( w 2  X  l 2  X  d ) in the straightforward method. This is because the USJ answer set at two consecutive timestamps may share many joining pairs, and we only need to incrementally update the changed pairs.
In order to further reduce the O ( w  X  l 2  X  d ) cost, we have to im-prove the efficiency of an essential step in our USJ framework (line 6), that is, invoking procedure Obtain_Cand_Pairs , which finds candidate pairs that contain at least one newly-incoming object (i.e. X [ t + 1] or Y [ t + 1] ) yet satisfying the join predicate given by In-equality (1). To achieve this goal, below, we will design effective pruning methods in Section 5 to filter out as many false alarms as possible.
In this section, we will present the rationale behind our pro-posed two pruning methods during USJ processing (i.e. used when executing line 6 of procedure USJ_Framework ). For simplic-ity, we focus on the case of retrieving USJ pairs  X  X [ t + 1] , Y [ j ]  X  ( t  X  w + 2  X  j  X  t + 1 ) for a new uncertain object X [ t + 1] at timestamp ( t + 1) ; the other case of finding pairs  X  X [ i ] , Y [ t + 1]  X  ( t  X  w + 2  X  i  X  t + 1 ) is symmetric.

Below, we discuss two effective pruning methods on object-and sample-levels. In particular, the object-level pruning (OP) method views all samples of each uncertain object as a whole, and prunes those object pairs that definitely do not satisfy the USJ predicate (i.e. with joining probabilities in Inequality (1) equals to 0); the sample-level pruning (SP) method utilizes samples to filter out those candidate pairs  X  X [ t +1] , Y [ j ]  X  with joining probabilities given by Inequality (1) below probabilistic threshold  X  .
In this subsection, we illustrate the basic idea of object-level pruning method, OP. Given an uncertain object X [ t + 1] from sliding window W ( T 1 ) and a number of uncertain objects Y [ j ] ( t  X  w + 2  X  j  X  t + 1 ) from sliding window W ( T 2 ) , our OP method would discard those candidate pairs such that the joining probability Pr { dist ( X [ t + 1] , Y [ j ])  X   X  } = 0 . In other words, we want to prune those pairs  X  X [ t + 1] , Y [ j ]  X  such that objects X [ t + 1] and Y [ j ] always have distance to each other greater than distance threshold  X  .

Recall from Section 3 that each uncertain object in a sliding win-dow consists of l random samples. Clearly, if all the pairwise dis-tances between samples from two uncertain objects X [ t + 1] and Y [ j ] are above threshold  X  , then these two uncertain objects will definitely have their distance above  X  , and in turn Pr { dist ( X [ t + 1] , Y [ j ])  X   X  } = 0 holds.

Note, however, that the distance computations between pairwise samples are costly (i.e. O ( l 2  X  d ) for each object pair). Thus, we illustrate how to reduce such cost using an example in Figure 4. For the arrival of a new uncertain object, say X [ t +1] , we bound all the l samples in X [ t + 1] with a hypersphere HS ( X [ t + 1]) centered at the centroid, C X [ t +1] , of samples (in the d -dimensional space) and with radius r X [ t +1] (defined as the maximum distance from C
X [ t +1] to samples of X [ t +1] , or formally max l k =1 x [ t + 1]) } ). The case of uncertain object Y [ j ] is similar, that is, we use a hypersphere HS ( Y [ j ]) to bound all samples of Y [ j ] .
We now summarize our object-level pruning method in the fol-lowing lemma.

L EMMA 5.1. ( Object-Level Pruning, OP ) Given a pair of un-certain objects X [ t +1] and Y [ j ] , and a distance threshold  X  , can-didate pair  X  X [ t + 1] , Y [ j ]  X  can be safely pruned if it holds that: Proof. It is sufficient to prove that Pr { dist ( X [ t + 1] , Y [ j ])  X   X  } = 0 holds under the condition in Inequality (3).

According to the definition of r X [ t +1] and r Y [ j ] , for any samples x 1 [ t +1] and y k 2 [ j ] from objects X [ t +1] and Y [ j ] , respectively, y
Thus, by applying the triangle inequality , it always holds that:
As a result, by combining Inequality (4) and Eq. (2), we can infer that Pr { dist ( X [ t + 1] , Y [ j ])  X   X  } is always equal to 0. Hence, candidate pair  X  X [ t + 1] , Y [ j ]  X  can be safely pruned based on Definition 3.1 (since probability 0 is always smaller than  X  &gt; 0 ). 2
As shown in the example of Figure 4, intuitively, LHS of In-equality (3) corresponds to the minimum possible sample distance between objects X [ t + 1] and Y [ j ] . If this minimum distance is greater than the distance threshold  X  , then Inequality (1) in the USJ definition will never hold (since  X  &gt; 0 ), and thus we can discard this object pair.
 Complexity Analysis. In order to enable the object-level prun-ing, to process each new uncertain object, say X [ t + 1] , we need to compute the hypersphere HS ( X [ t + 1]) . The time complex-ity of computing the centroid C X [ t +1] of HS ( X [ t + 1]) is given by O ( l  X  d ) , and that of computing the radius r X [ t +1] O ( l  X  d ) (note: this cost will be subsumed by the cost of sorting on radii, as later discussed in sample-level pruning method), where l is the sample size per object, and d is the dimensionality. In addition, since we need to keep d -dimensional centroid C X [ t +1] r
X [ t +1] , the space cost of OP is given by O ( d ) for each object.
In this subsection, we propose the second sample-level pruning method, SP. Different from the object-level pruning method that filters out candidate pairs with probabilities of being USJ results equal to 0, SP aims to prune those pairs with probabilities below probabilistic threshold  X  .

First, let us introduce the concept of (1  X   X  ) hypersphere [20] for uncertain object.

D EFINITION 5.1. ((1  X   X  ) -Hypersphere [20] ) Given an uncer-tain object o , a (1  X   X  ) -hypersphere  X  1  X   X  ( o ) is a hypersphere, with center C o and a radius r 1  X   X  o such that o resides in  X  with probability at least (1  X   X  ) .

Figure 5 illustrates an example of (1  X   X  ) -hypersphere for un-certain object Y [ j ] . Specifically, since object Y [ j ] is represented by l discrete samples, we can compute the distances, dist ( y from centroid C Y [ j ] to each sample y k [ j ] ( 1  X  k  X  l ), and sort them in ascending order of distances, denoted as dist ( y dist ( y k 2 [ j ])  X  ...  X  dist ( y k l [ j ]) , where { k ..., l } . Then, we let the center of (1  X   X  ) -hypersphere  X  be the centroid C Y [ j ] of all samples, and radius r 1  X   X  be the smallest distance dist ( y k b [ j ]) satisfying 1  X   X  .

We are now ready to give our sample-level pruning method in the following lemma.

L EMMA 5.2. ( Sample-Level Pruning, SP ) Given a pair of un-certain objects X [ t + 1] and Y [ j ] , a distance threshold  X  , and a probabilistic threshold  X  , candidate pair  X  X [ t + 1] , Y [ j ]  X  can be safely pruned if there exist two hyperspheres,  X  1  X   X  ( X [ t + 1]) and  X  1  X   X  ( Y [ j ]) for uncertain objects X [ t + 1] and Y [ j ] , respectively, such that both conditions below hold. Proof. Similar to the proof of Lemma 5.1, we can prove that once Inequality (6) holds, then for any samples x k 1 [ t +1]  X  X  1]) and y k 2 [ j ]  X  X  1  X   X  ( Y [ j ]) , we have: dist ( x  X  . In other words, the following conditional probability holds:
As a result, we can rewrite the probability in Inequality (1) using the conditional probability below.
 By substituting Inequality (7) into Eq. (8), we obtain:
Hence, from Inequality (9), pair  X  X [ t + 1] , Y [ j ]  X  is not the USJ result (via Inequality (1)). 2 Complexity Analysis. From Lemma 5.2, we can see that if there exists at least one pair of values  X  and  X  satisfying conditions in Inequalities (5) and (6), then candidate pair  X  X [ t +1] , Y [ j ]  X  can be safely pruned. Although it seems that there are totally l combinations of  X  and  X  pair, in the sequel, we will prove that we only need to check the pruning conditions l times in the worst case. We first give a lemma below.

L EMMA 5.3. For 0  X  1  X   X  1  X  1  X   X  2  X  1 , we have r 1  X   X  r  X  ) -hyperspheres, respectively, for uncertain object o .
 Based on Lemma 5.3, we obtain the following lemma.

L EMMA 5.4. Given a value  X  0  X  [0 , 1) , choose (1  X   X  1 smallest value such that 1  X   X  1 &gt; 1  X   X  1  X   X  Then, if Inequality (6) does not hold for  X  =  X  0 and  X  =  X  1  X   X  1  X  1  X   X  2 , Inequality (6) will not hold either when  X  =  X  and  X  =  X  2 .
 Proof. From the lemma assumption that Inequality (6) does not  X  . Moreover, since 1  X   X  1  X  1  X   X  2 , by Lemma 5.3, we can in-r
Y [ j ]  X   X  also holds. Hence, Inequality (6) does not hold either for  X  =  X  0 and  X  =  X  2 . 2
Lemma 5.4 indicates that, given a fixed  X  value, we only need to pruning condition in Inequality (6). If the inequality does not hold under this setting, then candidate pair cannot be pruned even using other  X  values.

Therefore, for each possible  X  value, we only need to check the pruning conditions in Lemma 5.2 once. Since there are at most l possible  X  values, the time complexity of checking pruning condi-tions is given by O ( l ) .

In addition, to apply the sample-level pruning method, for each uncertain object, we need to sort its samples according to their dis-tances to centroid, which requires O ( l  X  log ( l )) cost. Regarding the space cost, each object o needs to keep information including (1  X   X  ) values and radii r 1  X   X  o , requiring O ( l ) space cost.
In this section, we propose an efficient approach to answer USJ queries by seamlessly integrating the pruning ideas mentioned in the last section. Specifically, Section 6.1 discusses the data struc-tures we use for USJ processing, and Section 6.2 illustrates the de-tails of our USJ query procedure.
In this subsection, we present the data structures for facilitating the USJ processing. In particular, we want to apply the object-level (followed by sample-level) pruning method (as mentioned in Section 5) to reduce the search space of finding similar pairs from two uncertain data streams.

As indicated by the object-level pruning in Lemma 5.1, given a pair of uncertain objects X [ t + 1] and Y [ j ]  X  W ( T 2 check the pruning condition in Inequality (3), that is, dist ( C C out of w uncertain objects from sliding window W ( T 2 ) , it is not efficient to check this condition for every Y [ j ] in W ( T reduce the search space, we construct a grid index on the centers of uncertain objects (e.g. C X [ t +1] or C Y [ j ] ), which takes O (1) cost for insertion/deletion update.

Therefore, instead of exhaustive computation, only those uncer-tain objects in grid cells satisfying: are needed to be accessed, where r X [ t +1] is the radius of object X [ t + 1] , and r max ( T 2 ) is defined as the maximum radius among all objects in sliding window W ( T 2 ) (or formally r correctness of Inequality (10) above.
 isfy Inequality (10), then this pair can be safely pruned. Proof. Based on the lemma assumption, we have dist ( C X [ t +1] C r actly the condition in Lemma 5.1. Hence, the lemma holds. 2
Now the only remaining issue that needs to be addressed is how to obtain the maximum radius r max ( T 2 ) in sliding window W ( T as given in Inequality (10). Especially, we want to incrementally compute r max ( T 2 ) upon each stream update.
 Incremental Maintenance of r max ( T 2 ) . In order to correctly ob-tain the value of r max ( T 2 ) at any timestamp, we propose a one-pass algorithm, which incrementally maintains a list, denoted as L max ( T 2 ) , containing candidate radii r Y [ j ] in W ( T be r max ( T 2 ) in future timestamps. Specifically, let r updated radius in list L r max ( T 2 ) . For each uncertain object Y [ j ] that arrives at T 2 , we append r Y [ j ] to list L r max r last holds; otherwise, we first remove all radii in L r max moving radii, say r last , smaller than r Y [ j ] is that, r be the maximum radius in its lifespan due to the existence of r (since r Y [ j ] &gt; r last ). For the expired data, we simply remove it from the list. This way, L r max ( T 2 ) always stores all possible r max ( T 2 ) (in descending order) at current and future timestamps. Thus, at any timestamp, we can obtain r max ( T 2 ) by setting it to the maximum value in list L r max ( T 2 ) .
 Space Complexity. Since we need to store the center of each un-certain object in the sliding window, the space consumption of the constructed grid is given by O ( w  X  d ) for each uncertain data stream, where w is the size of sliding window and d is the dimensionality. Furthermore, the space cost of the list L r max ( T 2 ) for maintaining maximum radius candidate is given by O ( w ) in the worst case.
After we discuss the data structures for our USJ processing, in this subsection, we present the detailed USJ procedure. As men-tioned in the USJ framework (in Section 4), one important step is to invoke procedure Obtain_Cand_Pairs to retrieve candidate pairs. In the sequel, we give the pseudo-code of procedure Ob-tain_Cand_Pairs in Figure 6.

Specifically, given a new uncertain object X [ t + 1] from un-certain stream T 1 , procedure Obtain_Cand_Pairs retrieves candi-date pairs  X  X [ t + 1] , Y [ j ]  X  satisfying Inequality (1), where Y [ j ]  X  W ( T 2 ) . Since we construct a grid for centers C Y [ j ] objects Y [ j ] in the sliding window W ( T 2 ) , we apply the object-level pruning method (in Lemma 5.1) to the grid index. That is, we retrieve those candidate pairs such that Inequality (10) holds (line 1). After that, we can obtain a number of candidates that cannot be pruned on the object level. Thus, we next use the sample-level pruning mentioned in Lemma 5.2 to further filter out those pairs with joining probabilities below threshold  X  (line 2). Then, for the remaining candidate pairs  X  X [ t +1] , Y [ j ]  X  , we check Inequality (1) by verifying the actual probabilities (given in Eq. (2)) using pair-wise samples from uncertain objects X [ t + 1] and Y [ j ] (lines 3-4). Finally, we report all the pairs that satisfy Inequality (1) as our USJ answers (line 5).
 Complexity Analysis. In procedure Obtain_Cand_Pairs , the ma-jor cost is in the two-level pruning methods as well as the final refinement. In particular, we denote P Inequality (10) as the proba-bility that Inequality (10) holds. The retrieval cost in line 1 by using the object-level pruning method is given by O ( d  X  w  X  P Then, we apply the sample-level pruning method on these ( w  X  P
Inequality (10) ) retrieved candidates (i.e. line 2), with time com-plexity O ( l  X  w  X  P Inequality (10) ) . Finally, denote  X  (  X  [0 , 1]) as the percentage of USJ candidates that cannot be pruned by sample-level pruning methods. The time cost of the final refinement (lines 3-4) is thus given by O (  X   X  w  X  P Inequality (10)  X  d  X  l Discussions on USJ Batch Processing. Up to now, we have con-sidered the case where, at each timestamp, only one uncertain ob-ject arrives at an uncertain data stream. In the sequel, we will briefly discuss how to handle the case where  X t ( &gt; 1 ) uncertain objects arrive at the same time. That is, we need to process these  X t new objects in batch. Note that, this case has many real appli-cations, for example, LBS in which data are buffered (or delayed) due to the limited bandwidth (or network latency).

In order to enable fast USJ batch processing, we can divide the newly-incoming  X t objects, say X [ t +1] , X [ t +2] , ..., and X [ t +  X t ] , into several groups, each containing spatially close consecutively arriving objects. Specifically, we perform such grouping in one pass as follows. Denote G 1 , ..., and G g as g groups we obtain on objects { X [ t + 1] , X [ t + 2] , ..., X [ t + k ] } ( 1  X  k  X   X t ). Now we consider the next object X [ t + k + 1] , that is, to decide whether or not adding it to G g or start a new group G g +1 . In particular, we denote HS ( G g ) as a hypersphere bounding all objects in G center C G g and radius r G g . Then, from Inequality (10), we replace C the volume, V ol ( G g ) , of a hypersphere (i.e. the search space on the object level) centered at C G g , with radius (  X  + r G g Next, we also obtain V ol ( G g  X  { X [ t + k + 1] } ) . If the aver-age volume per object does not increase, that is, V ol ( G V ol ( G g  X  X  X [ t + k +1] } ) / ( | G g | +1) , then we include X [ t + k +1] in group G g , where | G g | is the number of objects in the group; oth-erwise, we create a new group G g +1 = { X [ t + k + 1] } . This way, our procedure sequentially processes X [ t + 1] , ..., and X [ t +  X t ] , and heuristically obtain groups of spatially close objects. For each group we access the grid index once, and obtain candidate pairs.
In this section, we demonstrate the efficiency and effectiveness of our proposed approaches for performing the join on uncertain data streams (USJ). Specifically, we test the performance of USJ processing on two real data sets, sensor and GPS . Specifically, sensor data set contains temperature data collected from 54 sen-sors deployed in Intel Berkeley Research lab between February 28th and April 5th, 2004. In order to evaluate the effect of di-mensionality on our USJ performance, we treat sensory data ob-tained from different sensors as different dimensions. Moreover, GPS data set contains 12 days X  3D trajectories (with dimensions altitude, latitude, and longitude) of a person on his way between home and office. Both sensor and GPS data are available online at [ http://www.cse.ust.hk/  X  xlian/cikm09-USJ/index.html ]. Note that, USJ over sensor or GPS data is useful in applications such as sen-sor data monitoring and object tracking [11]. Due to the noises introduced by sensing devices or the inaccuracy of positioning de-vices, these data are often imprecise. In practice, assuming object locations do not change very fast in a short period, we can treat data collected in a short time interval as independently collected samples of an uncertain object. In our experiments, in order to test the robustness of our USJ processing approaches under different uncertainty levels, we simulate the uncertainty of uncertain object o at each timestamp t by synthetically generating random samples. In particular, we first let the real data at timestamp t as center C then produce a radius r o  X  [ r min , r max ] , and finally randomly generate samples within a hypersphere HS ( o ) centered at C radius r o . As mentioned in Section 3, we assume that samples of uncertain objects continuously arrive in a streaming fashion. With-out loss of generality, in the sequel, we consider data with r adius of U niform or G aussian distribution (with mean ( r min + r and variance ( r min + r max ) / 5 ), denoted as rU or rG , respec-tively. Thus, we can obtain four pairs of data sets, GPS _ rUrU , GPS _ rUrG , sensor _ rUrU , and sensor _ rUrG . Note that, the results with other data sets (e.g. GPS _ rGrG , sensor _ rGrG , or that with a different radius distribution) are similar, and thus we omit them.

Table 2 depicts the settings of parameters in our experiments, including the probabilistic threshold  X  , radius range [ r dimensionality d , and size of the sliding window w , where the val-ues in bold font are default values . To evaluate the performance of USJ processing, each time we vary the value of one parameter while setting other parameters to their default values. For the dis-tance threshold  X  , we fix it to a value such that the join selectivity (i.e. the percentage of USJ answers in all possible joining pairs) is around 0.1%  X  0.5%.

Below, we first test the effectiveness of our proposed pruning methods (in Section 5), that is, object-level pruning (OP) and sample-level pruning (SP), in terms of the pruning power (defined as the percentage of joining pairs that can be pruned by OP or SP). Fur-thermore, we investigate the efficiency of our USJ processing ap-proaches, in terms of the average processing time for each times-tamp. Note that, the space cost for USJ processing has been ana-lyzed in Section 5. All our experiments are conducted on a Pentium IV 3.2GHz PC with 1G memory. The reported results are the aver-age over 5,000 timestamps.
In this subsection, we illustrate the effectiveness of our proposed object-level and sample-level pruning methods for USJ, over the two pairs of data sets sensor _ rUrG and GPS _ rUrG . Note that, the results of data sets sensor _ rUrU and GPS _ rUrU are sim-
Figure 8: Comparisons of USJ Performance with WoP and Grid ilar to that of sensor _ rUrG and GPS _ rUrG , respectively, and thus, in subsequent experiments, we omit them. Recall from Sec-tion 6 that, during USJ processing, we first conduct the object-level pruning (OP), followed by the sample-level pruning (SP) on the resulting candidates.

Figure 7 reports the pruning power of each pruning methods dur-ing the USJ processing under different  X  values from 0 . 1 to 0 . 9 , where other parameters are set their default values and each un-certain object consists of 100 samples (i.e. l = 100 in this and subsequent experiments) to represent its distribution. The upper part of columns in figures corresponds to the percentage of joining pairs pruned by OP, whereas the lower part indicates that by SP. From the results, we can see that the OP method can significantly reduce the USJ search space (i.e. pruning 80% -90% false alarms of joining pairs). Moreover, the SP method can further filter out 5% -10% false alarms from the remaining candidates. Note that, when the probabilistic threshold  X  increases, the number of pairs pruned by SP also increases (as confirmed by numbers in figures), which shows the effectiveness of our SP method with different  X  values.
Taking into account both OP and SP, they can totally prune more than 90% candidate pairs, which confirms the effectiveness of our pruning methods. Due to the reduced search space by OP and SP, the USJ processing cost on uncertain data streams can be greatly saved. In the subsequent experiments, we will illustrate the effi-ciency of our USJ processing.
After confirming the effectiveness of our pruning methods, we will next verify the robustness of our proposed USJ processing ap-proaches under different parameter settings. In particular, we test the average cost of USJ processing per timestamp, by comparing two approaches, namely WoP and Grid . Specifically, WoP is an incremental update method without using any pruning methods. That is, for each insertion of uncertain object X [ t + 1] ( Y [ t + 1] ) in T 1 ( T 2 ), we scan each uncertain object Y [ j ] ( X [ i ] ) in sliding window W ( T 2 ) ( W ( T 1 ) ), and check whether or not Inequality (1) is satisfied via samples; Grid is our USJ approach that applies OP and SP pruning methods over grid index (as mentioned in proce-dure Obtain_Cand_Pairs in Figure 6).

Figure 8 shows the comparisons of the two USJ processing ap-proaches, WoP and Grid , over four pairs of data sets, where pa-rameters are set to default values. We find that WoP performs much worse than Grid , since it exhaustively checks Inequality (1) for pairwise uncertain objects within sliding windows, which is very costly. In contrast, Grid applies our two pruning methods, reducing the search space, and thus performs better than WoP by about 2 orders of magnitude. In addition, our Grid approach takes only about 10  X  2  X  10  X  3 second to process data at each timestamp, which indicates that Grid approach can process around 0 . 1 K  X  1 K data per second, and scale well for data with high in-put rates.
 In the sequel, in order to evaluate the robustness of our proposed USJ processing approach, we will compare Grid with WoP by varying different parameters (e.g.  X  , d , and w ).

Figure 9 presents the performance of USJ processing by vary-ing probabilistic threshold  X  from 0.1 to 0.9. From the results, we find that when  X  increases, the time cost per timestamp of our Grid approach slightly decreases. This is mainly because our SP method can prune pairs with joining probabilities below thresh-old  X  . Thus, for larger  X  , the obtained candidates are fewer, and Grid can save the time cost of refining candidates. In contrast, since WoP scans uncertain objects within sliding window at each timestamp, the time cost per timestamp remains the same for dif-ferent  X  values. Similar to previous results, the Grid approach can efficiently handle incremental updates for each timestamp within less than 10  X  2 second, and it performs much better than WoP by about 2-3 orders of magnitude, which shows the efficiency of our proposed USJ processing approaches. We also did experiments by varying radius range [ r min , r max ] . The experimental results show similar query performance of our Grid approach, compared with WoP , and thus we do not present them here.
Figure 10 varies the dimensionality, d , of uncertain data streams from 2 to 5 over sensor data (note: here different dimensions corre-spond to data collected by different sensors; here, we only evaluate sensor data sets, since GPS data have fixed 3 dimensions), where  X  is selected so that the join selectivity remains approximately the same for different d values, and other parameters are set to their default values. From figures, we can see that when the dimension-ality increases, the time cost per timestamp of both WoP and Grid also increases. The reason is that the distance computations involve more dimensions, and thus introduce more computational cost. In addition, for our Grid approach, this is also because, in higher dimensional space, the distances from an object to other objects would become indistinguishable [2]. Therefore, when the dimen-sionality d is higher, more candidate pairs will be retrieved and re-fined, which incurs higher USJ processing cost. Nevertheless, the time cost per timestamp of Grid for all experiments is still low (i.e. below 10  X  1 second).

Finally, Figure 11 demonstrates the efficiency of processing USJ with different sizes, w , of sliding windows. Specifically, we vary w from 500 to 2 , 000 , where other parameters are set to default values. From the results, we can see that the time cost per times-tamp increases with the increasing size of sliding windows. The reason is that, for large w , more uncertain objects (in a wider slid-ing window) are expected to be joined with a newly incoming ob-ject, which will produce more candidate pairs to be refined. Note that, in figures, due to different data characteristics, the increasing slope of Grid in sensor data sets is smaller than that in GPS data sets. Similar to previous results, Grid performs well with small time cost below 10  X  2 second (i.e. about 1-3 orders of magnitude better than WoP ), which confirms the scalability of our proposed approaches on different sizes of sliding windows.

In summary, from the experimental results shown above, we have verified and confirmed the efficiency and effectiveness of our proposed USJ processing approaches, compared with WoP , under various parameter settings.
Join processing has many important real applications in uncer-tain and streaming environment, such as sensor data analysis and object tracking and monitoring. In this paper, we formulate and tackle the problem of join on uncertain data streams (USJ). In particular, we formally define the USJ problem, which can guar-antee the confidence of the returned join results from uncertain and streaming data. Moreover, due to the unique characteristics of streaming and uncertain data, we propose effective pruning meth-ods, object-level and sample-level pruning, to reduce the search space of USJ. Then, we integrate the proposed pruning methods into an efficient procedure to incrementally update the USJ an-swer set. We conduct extensive experiments to demonstrate the efficiency and effectiveness of our proposed USJ processing tech-niques under different parameter settings.

As a future work, we plan to further investigate the interest-ing topic of designing optimization techniques to improve the effi-ciency of USJ processing.
Funding for this work was provided by NSFC / RGC Joint Re-search Scheme N_HKUST602/08, National Grand Fundamental Research 973 Program of China under grant No.2006CB303000, and the NSFC Projects under grant No. 60533110 and No. 60763001. [1] G. Beskales, M. Soliman, and I. F. Ilyas. Efficient search for [2] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. [3] C. B X hm, A. Pryakhin, and M. Schubert. The Gauss-tree: [4] T. Brinkhoff, H-P. Kriegel, and B. Seeger. Efficient [5] D. Carney, U.  X etintemel, M. Cherniack, C. Convey, S. Lee, [6] L. Chen, M. T.  X zsu, and V. Oria. Robust and fast similarity [7] R. Cheng, D. V. Kalashnikov, and S. Prabhakar. Evaluating [8] R. Cheng, S. Singh, S. Prabhakar, R. Shah, J. S. Vitter, and [9] N. N. Dalvi and D. Suciu. Efficient query evaluation on [10] G. Das, D. Gunopulos, N. Koudas, and N. Sarkas. Ad-hoc [11] Y. Diao, B. Li, A. Liu, L. Peng, C. Sutton, T. Tran, and [12] A. Faradjian, J. Gehrke, and P. Bonnet. Gadt: A probability [13] M. Hua, J. Pei, W. Zhang, and X. Lin. Ranking queries on [14] S. R. Jeffery, M. J. Franklin, and M. Garofalakis. An [15] E. M. Korn and R. T. Ng. Algorithms for mining [16] H.-P. Kriegel, P. Kunath, M. Pfeifle, and M. Renz.
 [17] H.-P. Kriegel, P. Kunath, and M. Renz. Probabilistic [18] M. Li and Y. Liu. Underground coal mine monitoring with [19] X. Lian and L. Chen. Monochromatic and bichromatic [20] X. Lian and L. Chen. Probabilistic group nearest neighbor [21] X. Lian and L. Chen. Probabilistic ranked queries in [22] X. Lian and L. Chen. Efficient processing of probabilistic [23] X. Lian and L. Chen. Probabilistic inverse ranking queries [24] X. Lian and L. Chen. Top-k dominating queries in uncertain [25] V. Ljosa and A. K. Singh. Top-k spatial joins of probabilistic [26] M. F. Mokbel, C.-Y. Chow, and W. G. Aref. The new casper: [27] M. F. Mokbel, M. Lu, and W. G. Aref. Hash-merge join: A [28] J. Pei, B. Jiang, X. Lin, and Y. Yuan. Probabilistic skylines [29] S. Singh, C. Mayfield, R. Shah, S. Prabhakar, S. Hambrusch, [30] Y. Tao and D. Papadias. Maintaining sliding window [31] Y. F. Tao, M. L. Yiu, D. Papadias, M. Hadjieleftheriou, and [32] T. Urhan and M. J. Franklin. Xjoin: A reactively-scheduled [33] M.-Y. Yeh, P. S. Yu, K.-L. Wu, and M.-S. Chen. PROUD: A [34] Q. Zhang, F. Li, and K. Yi. Finding frequent items in [35] W. Zhang, X. Lin, Y. Zhang, W. Wang, and J. X. Yu.

