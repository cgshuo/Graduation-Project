 The highly dynamic nature of online commenting environments makes accurate ratings prediction for new comments challenging. In such a setting, in addition to exploiting comments with high pre-dicted ratings, it is also critical to explore comments with high un-certainty in the predictions. In this paper, we propose a novel upper confidence bound (UCB) algorithm called L OG UCB that balances exploration with exploitation when the average rating of a com-ment is modeled using logistic regression on its features. At the core of our L OG UCB algorithm lies a novel variance approxima-tion technique for the Bayesian logistic regression model that is used to compute the UCB value for each comment. In experiments with a real-life comments dataset from Yahoo! News, we show that L OG UCB with bag-of-words and topic features outperforms state-of-the-art explore-exploit algorithms.
 I.2.6 [ Computing Methodologies ]: Learning Algorithms, Experimentation logistic regression, explore-exploit, comment ratings, upper confi-dence bound
User generated content in the form of comments has witnessed an explosive growth on the web. Web sites like Yahoo! and YouTube allow users to comment on diverse content like news articles and Work done while the authors were at Yahoo!.
 videos. These  X  X rowd-sourced X  comments are highly engaging be-cause they reflect the views and opinions of real users, and are a means for users to discuss and debate controversial issues. Further-more, comments can be informative, and are an effective gauge of public sentiment on a topic.

However, a key challenge with user generated comments is that a single news article on a major event can easily trigger thousands of comments. Clearly, it is unrealistic to expect that users will sift through the copious comments for an article. A more realis-tic scenario is that a user will look through the first K comments that are presented to him/her, and ignore the remaining comments. Fortunately, most commenting environments allow users to provide feedback on comments shown to them  X  users can give a thumbs-up rating to indicate that they like, or a thumbs-down rating to express dislike for a comment. Thus, our goal in this paper is to develop algorithms that leverage past comment ratings to rank an article X  X  comments so that the top K comments have the highest chance of being liked by the user.

Recommender systems have been extensively studied in the re-search literature  X  these systems use collaborative filtering, content-based filtering, or hybrid approaches to recommend items to users. Traditional collaborative filtering techniques rely on either item-item/user-user similarity models [26], or matrix factorization mod-els [22] that multiply user-and item-specific factors for ratings prediction. A shortcoming of classical collaborative filtering ap-proaches is that they cannot predict ratings for new users or items (referred to as the cold-start problem ). Content-based filtering meth-ods alleviate the cold-start problem by building predictive models based on user and item features like age, gender, and category. But they do not consider user-or item-specific parameters learnt from past ratings. Recently proposed hybrid approaches [2, 1, 4] com-bine collaborative filtering with content-based filtering by simul-taneously incorporating features and user/item-centric parameters in models  X  this allows them to predict ratings more accurately for both existing user-item pairs as well as new pairs (through fea-tures). The top K items with the highest predicted ratings are then recommended to the user.

Commenting environments are highly dynamic with new com-ments and articles arriving continuously. These new comments have zero or few user ratings, and constantly changing content that may have little overlap with previous comments. Furthermore, of the users who view comments, only a small fraction typically rate comments causing training data to be extremely sparse. In such a scenario, even models that exploit features and past ratings in-formation may be unable to make accurate predictions, and there may be a high degree of uncertainty in the predicted ratings of new comments. Consequently, simply ranking comments based on their predicted rating scores may lead to poor comments being ranked high and good comments being ranked low because of inaccuracies in their rating estimates. Thus, a strategy that only exploits user comments with high predicted ratings may be suboptimal, and it is important to also explore comments with low predicted ratings but high uncertainty since their true ratings may be much higher.
Our comments recommendation problem is an instance of the well-known exploitation-exploration tradeoff that has been studied extensively in the context of the multi-armed bandits problem [12, 6]. Given a set of arms with unknown rewards, the objective in ban-dit problems is to select arms in successive trials (one per trial) so that the expected total reward over a finite number of trials is max-imized. In our comments setting, comments correspond to arms and the ratings they receive are the rewards. Existing bandit algo-rithms [6, 16, 7] balance exploration and exploitation to achieve optimal rewards. Exploration gathers information to refine the ex-pected reward estimates for seemingly suboptimal arms while ex-ploitation selects arms with high reward estimates based on past observations.

A popular class of bandit algorithms are the upper confidence bound (UCB) algorithms [6, 16, 7]  X  these compute an upper con-fidence bound for each arm which is essentially a high-probability upper bound on the expected reward. The UCB for an arm is com-puted by combining the current reward estimate with the uncer-tainty in the estimate. In each trial, the arm with the highest UCB value is then selected. Thus, the selected arms either have a high reward estimate (and so should be exploited more to increase the total reward) or high uncertainty (and so should be explored more since the arm could potentially give a high reward). In recent work, [16] proposes a UCB-based bandit algorithm L IN UCB that estimates the reward of each arm through a linear regression on features. Leveraging textual and other informative features help to overcome data sparsity and improve the accuracy of reward esti-mates. The authors compute the variance in the reward estimated by the linear model and use it to determine the UCB for each arm. Such bandit algorithms that utilize context information (e.g., fea-tures) are called contextual bandits .

In this paper, we propose a UCB-based contextual bandit algo-rithm L OG UCB that estimates average ratings for comments using logistic regression on features. Logistic regression is better suited for modeling thumbs-up/down style binary ratings data compared to linear regression. Unlike linear regression, it ensures that av-erage ratings lie in the fixed range [0, 1], and so estimates have bounded variance (between 0 and 1). Moreover, logistic regression models are more robust to outliers [8]. We adopt a Bayesian ap-proach to estimate average ratings for comments and the variance in the estimates. Since logistic regression models are more com-plex, the mean rating estimates and variance are not available in closed form, and need to be approximated. Similar to UCB-type algorithms, we combine the rating estimate (exploitation) with the variance in the estimate (exploration) to compute a UCB for each comment, and select the top-K comments with the highest UCBs.
Our main contributions can be summarized as follows: 1) We propose a novel UCB-based algorithm called L OG UCB that balances exploration with exploitation when the average rating of a comment is a logistic sigmoid function of its features. Our L algorithm for recommending comments consists of the following core modules: (1) A global logistic regression model that uses past comments to compute a global prior on feature weights, (2) A per-article Bayesian logistic regression model that incorporates the global prior and additional slack variables per comment  X  in the beginning when ratings data is sparse, the model relies on past comments and comment features for rating prediction; but once a comment has sufficient ratings, the model makes a smooth transi-tion and uses empirical rating estimates, and (3) An explore-exploit module that combines the predicted rating with uncertainty in the prediction to determine the comments recommended to the user. 2) To compute the UCB for each comment, we assume a Bayesian setting and employ a host of approximation techniques to efficiently estimate the mean rating and associated variance. Our variance ap-proximation technique for the Bayesian logistic regression model is novel and lies at the core of our L OG UCB algorithm  X  this is a key technical contribution of our work. 3) In addition to the words in each comment, we also include a comment X  X  Latent Dirichlet Allocation (LDA) topics [10] as fea-tures to further improve rating prediction accuracy. 4) We conduct an extensive experimental study on a real-life dataset containing comments for Yahoo! News articles. Our experimental results indicate that our per-article logistic regression models in-corporating global priors and leveraging bag-of-words and topic features predict ratings with hi gher accuracy compared to simple baselines. Furthermore, our L OG UCB explore-exploit algorithm outperforms state-of-the-art techniques like L IN UCB.
Each online article has an associated set of user generated com-ments. Every time an article is viewed by a user, our system selects K comments for the article to display to the user. The user then assigns one of two ratings to a (random) subset of the K com-ments shown to him/her: (1) A thumbs-up rating if the user likes the comment, or (2) A thumbs-down rating to express dislike for the comment. The user can also post additional comments for the article. Thus, the comments for an article and the ratings for each comment are highly dynamic.

Consider an article a at time t . We denote the set of comments posted by users for article a at time t by C ( t ) and the number of comments in C ( t ) at time t by N ( t ) . Thus, N ( t )= comment i  X  C ( t ) has an associated M -dimensional feature vec-tor x i .The M feature values for a comment are computed based on its content and include word occurrence frequency, comment length, presence of numeric values or special symbols like excla-mation marks, etc.

We will use binary 0/1 values for ratings with a 1 corresponding to a thumbs-up, and a 0 to a thumbs-down. We will denote by n the total number of 0/1 ratings provided by users for comment i at time t  X  of these, the number of 0 and 1 ratings are denoted by n ( t ) and n + i ( t ) , respectively. Furthermore, we will represent the vector of observed ratings for comment i by y i ( t ) and the j ing for comment i by y ij . Finally, we will use X ( t )= Y ( t )= { y i ( t ) } to denote the feature and rating vectors, respec-tively, for all the comments of article a in C ( t ) .

Let  X  i be the expected rating of comment i ,thatis,  X  i = E [ y Note that  X  i for comment i is unknown. At time t , our comments recommendation system only has access to the comments C ( t ) and rating information Y ( t ) for the comments  X  based on these, it tries to identify the K best comments with the highest expected ratings, and recommends them to users. These comments with high likeli-hoods of ge tting a thumbs-up are precisely the ones that the average user will like, and thus will lead to increased user satisfaction. Comments Recommendation Problem: Let article a be viewed at times t 1 ,t 2 ,t 3 ... . At each time t l , the problem is to select K comments from C ( t l ) to recommend such that the total expected number of thumbs-up ratings received until time t l is maximum,
As mentioned earlier, our comments recommendation problem above has a natural mapping to the multi-armed bandit problem with comments as arms, and ratings as the corresponding rewards.
L OG UCB is a novel UCB-style contextual bandit algorithm that employs logistic regression on comment features to model the ex-pected rating for a comment. For each article a ,L OG UCB trains a separate logistic regression model. We consider a Bayesian setting in our work because the priors on model parameters help to regular-ize them while the posteriors provide a natural method to compute the variance of estimated ratings.

Below, we give a brief description of the different modules of our L OG UCB algorithm shown in Figure 1.
 Global Rating Model: A newly posted article has very few com-ment ratings initially. Consequently, the per-article model that we train can lead to overfitting due to data sparseness issues. To ad-dress this, we learn a global logistic regression model by pooling comments and user ratings data across past articles. The global model is used to initialize the parameters of the per-article model. When training data is insufficient, the per-article model parameters shrink to the global parameters. Hence, the global model provides a good backoff estimate that helps to improve predictive accuracy for new articles. The global model is updated periodically (every few days).
 Per-Article Rating Model: For an article a , a per-article logis-tic regression model is learnt at regular time intervals; at time t , our training procedure uses the features X (t) and ratings comments in C ( t ) as training data. The rationale for having a local per-article model is that once an article has a sufficient number of comments with ratings, the local model can provide better rating estimates than the global model for both existing as well as unseen comments. This is because it can capture article-specific feature-rating correlations, and deviations of the article from the average behavior captured by the global model  X  these deviations could be due to new article content never seen before, dynamically changing user sentiment, etc.

As mentioned earlier, when an article is new and has very lit-tle training data (cold-start scenario), our per-article model falls back to the global model and prediction accuracy is not adversely impacted. Furthermore, as the article collects more training data, the per-article model parameters start deviating to incorporate the article-specific comments and ratings. Finally, as each comment acquires more ratings, predictions become comment-specific based on the actual ratings the comment receives as opposed to its fea-tures. This is enabled by including a per-comment slack variable Thus, as the training data for a comment grows, our per-article model makes a smooth transition from feature-based predictions to comment-specific ones.

We should point out here that we don X  X  include user features in our model because only a small fraction of users rate comments  X  as a result, (user, comment) ratings data is extremely sparse. Fur-thermore, in many cases, it is difficult to accurately identify users because they may not be logged in, or user cookie information is unreliable. Finally, even if we could identify users, certain infor-mation they may have provided in their profiles (e.g., age, salary, etc.) may be inaccurate, or they may have not been very active in terms of browsing or searching. In case reliable user features are available, these can be easily incorporated into our model to enable greater personalization.
 Explore-Exploit: The explore-exploit module is responsible for selecting the K best comments to be displayed when a user views an article. Specifically, it uses the posterior estimates for the per-article model to obtain mean rating and variance estimates for each comment. These are subsequently combined to compute UCBs for each comment and the top-K comments with the highest UCB val-ues are finally recommended to the user. Thus, L OG UCB exploits comments with high predicted ratings and explores the ones with high uncertainty in the predictions. The user rating feedback is in-corporated into the training data for the article and used to further improve the per-article model.
 Note that it is possible that the top-K comments with the highest UCBs may contain very similar or redundant comments  X  explor-ing/exploiting such comments can be wasteful. Recently, there has been some work [21, 25] on extending multi-armed bandit prob-lems to select multiple arms per trial. However, the modeling as-sumption is that the K selected arms correspond to the K ranked results for a search query and at most one arm (corresponding to the first relevant result) gives a reward. Our comments scenario is somewhat different since ratings are determined by logistic re-gression and a user can give ratings to multiple comments in a sin-gle view. In this paper, we do not address the problem of diverse recommendations although L OG UCB can be used in conjunction with techniques such as Maximal Marginal Relevance (MMR) [11] (with UCB values substituted for relevance scores) to ensure com-ments diversity. Extending our comments recommendation frame-work to address diversity is left as future work.

The goal of bandit algorithms is to minimize the regret ,thatis, the difference between the expected reward of the algorithm and the optimal expected reward. In [16], L IN UCB is shown to have an  X 
O ( sumption, however, does not hold in our dynamic comments en-vironment. Since logistic regression models are much more com-plex, formally proving regret bounds for L OG UCB is difficult and we do not focus on this in our work. However, logistic models are a natural choice (compared to linear models) for modeling binary ratings data, and in our experiments, we empirically show that our L OG UCB algorithm outperforms L IN UCB.

In the following sections, we will provide a detailed description on each of the above three modules. For notational convenience, we will drop the argument t indicating time from C ( t ) , when it is clear from context.
We start by describing our global probabilistic model and train-ing procedure. We denote the comments (belonging to older ar-ticles) used for training the global model by C g , and the corre-sponding features and ratings for the comments by X g respectively. Since our ratings y ij are binary, we assume that they come from a Bernoulli distribution with probability equal to the mean rating  X  Further, we model  X  i as a logistic function of the linear projection of the comment feature vector x i . Hence, where w is an M -dimensional weight parameter that captures the preferences of an average user for the different content features, and  X  (  X  ) is the logistic sigmoid function.
 Data Likelihood: Given our ratings generation model, the likeli-hood of observations y i for comments i is given by: Prior over w : We also assume a standard pr ior Gaussian distribu-tion with zero mean and  X  2 variance over the weight vector Posterior Distribution of w : Using Bayes rule, the posterior dis-tribution of w can be written as the product of the likelihood (in Equation (3)) and prior (in Equation (4)):
The goal of our model training procedure is to find the mode, also known as the maximum-a-posteriori (MAP) estimate, of the posterior distribution of w . To find the mode w g map , we minimize the negative log of the posterior, that is,
We use the standard iterative conjugate gradient descent [18] al-gorithm to obtain w g map .
As discussed earlier in Section 3, we learn a per-article model specifically for article a with the comments C of the article as train-ing data. The set C contains N comments with features and ratings X and Y , respectively.
As in the global model, we assume that the rating y ij for a com-ment i comes from a Bernoulli distribution and the mean rating  X  is a logistic function. However, it is possible that the comment feature space and/or the logistic model are incapable of predict-ing the rating correctly even if sufficient observations are available. Hence, the mean rating estimate  X   X  i has a bias which is indepen-dent of the training data size. Thus, in addition to w x i per-comment slack variable z i to our model for  X  i .
We will use z to denote the vector of slack variables, that is, z =[ z 1 ,z 2 ,...,z N ] .
 Prior over ( w , z ) : We assume a Gaussian prior over the weight vector w with mean equal to the MAP estimate, w g map global model and variance  X  2 w . Thus, P ( w )= N ( w | w g Observe that this prior provides backoff estimates from the global model when an article is new and has very few comments.

We also place a zero-mean Gaussian prior on each z P ( z )= N ( z | 0 , X  2 z ) . Note that if a comment i has no ratings in the training data, then z i is zero because of the prior and  X  predicted using the projection w x i only. On the other hand, if the comment has many ratings, then data likelihood dominates the prior and we precisely estimate the bias z i . Thus, our prediction reduces to the empirical rating estimate based only on the fraction of thumbs-ups.
 Posterior Distribution of ( w , z ) : Using Bayes rule, the posterior distribution of ( w , z ) can be written as P (( w , z ) | Y )  X  P ( Y | w , z ) P ( w ) P ( z )
Observe that the likelihood involves logistic functions and the priors are Gaussians. Hence, a closed form does not exist for the posterior distribution. A popular technique is to approximate the posterior of ( w , z ) by a Gaussian distribution using the Laplace approximation [19]. The mean of the Gaussian is approximated by the mode ( w map , z map ) of the true posterior, while the inverse of the covariance matrix S is approximated by the Hessian matrix which comprises the second order derivatives of the negative log likelihood at the mode. Thus, the posterior P (( w proximated as N (( w , z ) | ( w map , z map ) , S ) .

The mode ( w map , z map ) can be found by minimizing L = P (( w , z ) | Y ) , the negative log of the posterior. Thus, ( w
We use the standard iterative conjugate gradient descent algo-rithm to minimize L where the gradients are given by  X  X   X  w  X  X   X  z
Next, we derive the equations for the covariance matrix S is obtained by inverting the M + N -dimensional Hessian matrix. 1begin 5for each user viewing article a do 6for each comment i  X  C do 9end 13 end 14 end where, Above, P is a N  X  N diagonal matrix, where P [ i, i ]= n i  X  ) . The overall time complexity of computing the covariance ma-trix S is O ( M 2 N +( M + N ) 3 ) . Here, O (( M + N ) 3 complexity of inverting the M + N  X  dimensional Hessian. If the number of dimensions M + N is large, then we simply use the diag-onal approximation of the Hessian. The complexity of finding the inverse then reduces to O ( M + N ) and the overall time complex-ity for computing the covariance becomes O ( MN ) . Thus, model training can be achieved in a matter of a few seconds for thousands of dimensions and comments (see Section 7.5).
Our L OG UCB algorithm (see Algorithm 1) adopts a UCB-based approach to handle the exploitation-exploration trade-off for our logistic regression rating prediction model. For each comment i ,it uses Bayesian estimation methods to compute the mean rating es-timate  X   X  i and its variance  X  2 i . Specifically,  X   X  i the predictive mean and variance, respectively, of the mean rating  X  i over the posterior of the per-article model. Now, by Cheby-shev X  X  inequality, it follows that for an appropriate choice of  X  , |  X  i  X   X   X  i | X   X  X  i holds with high probability (  X  1  X  1  X   X   X  +  X  X  i is a UCB on  X  i , and our algorithm selects the comments with the top-K UCB values to show to users. We collect the user feedback (thumbs-up/down ratings) on the displayed comments as well as any new comments posted by the user. Clearly, since only a small fraction of users typically rate comments, the estimated model parameters will not change (by much) often. Consequently, we keep collecting new observations until we have a sufficient num-ber of new ratings  X  we then perform a periodic batch update of the per-article model parameters (as in [16]). Below, we present ap-proximation techniques for computing the mean rating estimate  X   X  and its variance  X  2 i . The method for estimating  X   X  while the approximation scheme for  X  2 i is novel.
 Mean Rating Prediction: Following the Bayesian approach, we predict the mean rating  X  i by its posterior predictive mean, that is, the expectation of  X  i with respect to the posterior of model param-eters ( w , z ) conditional on observed data. Thus,
Substituting t = w x i + z i in the above equation, we get where  X  t =
Note that the above integral over t represents the convolution of a Gaussian with a logistic function, and cannot be solved an-alytically. However, as pointed out in [19], the logistic function  X  ( t ) can be approximated well by the scaled probit function  X  (  X t ) , where  X  ( t )= ing the slopes of the two functions same at the origin, which gives  X  expressed in terms of another probit function. Specifically, The computational complexity to predict the mean rating is O (( M + N ) 2 ) which is the number of steps needed to compute  X  2 t .How-ever, if S is approximated by a diagonal matrix, then the time com-plexity reduces to O ( M + N ) . Note that the inverse computation needed to compute S is done only once (when the per-article model is trained) for all the comments.
 Variance of Mean Rating Estimate: The variance  X  2 i is given by the predictive variance of the mean rating  X  i which is  X  Above, the final equation is obtained by substituting t = z ,and  X  t and  X  t are as defined earlier.

Again, the convolution of a Gaussian with a squared logistic function cannot be solved analytically. However, we observe em-pirically that a squared logistic function  X  2 ( t ) can be approximated well by a scaled and translated probit  X  (  X  ( t +  X  )) (as opposed to only a scaled probit for the logistic function). We minimize the squared error between the two functions to empirically find the values of parameters  X  and  X  , which gives  X  =0 . 703 and  X  =  X  0 . 937 . The similarity between the squared logistic and pro-bit functions for this choice of parameters is shown in Figure 2. The root mean square error is 0 . 0053 with a maximum error of 0 . 02 . Again, since the convolution of a probit with a Gaussian can be expressed in terms of another probit function, we have,
Note that the variance  X  2 i above is the difference between the squares of two sigmoids and is thus upper bounded by 1. However, such a bound does not hold for the variance of linear regression models. Thus, the UCB values for L OG UCB are more accurate compared to L IN UCB [16].

We should also point out here that [23] presents an alternate vari-ance estimation method for logistic regression which computes the variance of the Taylor approximation of  X  i (that ignores higher-order terms in the Taylor expansion of  X  i ). However, since the probit approximates the squared logistic function so closely (see Figure 2), we expect our variance estimate to be more accurate.
In this section, we perform multiple experiments that demon-strate the effectiveness of our proposed methods on a real-life com-ments dataset from Yahoo! News. Our first experiment shows that our per-article logistic regression models incorporating global pri-ors and leveraging bag-of-words and topic features have higher pre-dictive accuracy compared to other models such as linear regres-sion. In our second experiment, we show that our L OG gorithm outperforms state-of-the-art explore-exploit methods. Fi-nally, we report the running times of our per-article model training and rating prediction procedures.
We obtain the comments rating data between March and June, 2011 from Yahoo! News. The ratings for different comments are available in terms of the number of thumbs-ups and thumbs-downs given by different users. Note that no ground truth per-comment mean rating  X  i is available. Hence, in order to reliably evaluate the performance of our algorithms, we only consider the comments that receive at least 10 ratings and assume that the ground truth mean rating  X  i is the same as the empirical mean given by the fraction of thumbs-ups. In our experiments, we consider 780 articles with 408 , 643 comments and a total of 16 . 8 million ratings.
We randomly select 70% of the articles and consider their com-ments as the global comments set C g used to train the global model. Another 5% of the articles are used as a validation set to learn pa-rameters of the different models. The remaining 198 articles are used to train per-article models for the model accuracy and explore-exploit experiments. We consider two different types of features.
 Bag of words: The text of each comment is cleaned using stan-dard techniques like lower-casing, removal of stop-words, etc. For the articles used to train the global model, we prune all words with very low (present in less than 10 comments) and high (present in greater than 50% of the comments) frequencies. The final vocabu-lary consists of approximately 9,000 words. On the other hand, for articles on which we train a per-article model, the low frequency threshold for pruning words is set to a lower value of 3  X  this is because there are a lot fewer comments per article. We consider both L 2 normalized tf and tf-idf as feature values for words in each comment. The tf feature values give slightly better performance  X  so all the results are shown using them.
 Topics: Latent Dirichlet Allocation (LDA) [10, 13] is a topic model that represents each document as a mixture of (latent) top-ics, where each topic is a probability distribution over words. LDA clusters co-occurring words into topics, and is able to overcome data sparsity issues by learning coarse-grained topic-rating correla-tions. We compute topic assignments to words in a comment using Gibbs sampling as described in [13], and use the topic distribution for a comment as feature values. The topic-word distributions are first learnt on the global comments set, and subsequently used to assign topics to the comments of each article. In our experiments, we set the default number of topics to 50.

We also considered a number of other per-comment features like (1) the fraction of positive and negative sentiment words where the sentiment of a word is obtained using Senti-WordNet 1 , (2) num-ber of special characters like question marks (?) or exclamations (!), and (3) total number of words, fraction of non-stop words, and fraction of upper-case words. However, none of these features had an effect on model performance.
In this experiment, we compare the mean rating prediction accu-racy of different models. Models Compared: We compare the following variants of our per-article model as well as other state-of-the-art models.  X  Global: This is our global logistic regression model described in Section 4 trained on the comments set C g . The features comprise the (unpruned) words in C g and LDA topics.  X 
PA1: This is our per-article logistic regression model described in Section 5 with slack variables z i set to zero and no global prior. It is trained on the article X  X  comments, and features consist of the words in the article X  X  comments and LDA topics.  X 
PA2: This is the same as PA1 but uses the global prior and so has additional features corresponding to the words in C g . We consider http://sentiwordnet.isti.cnr.it/.
Table 1: AUC scores for different models in Section 8.3.1 this model to measure the impact of the global prior on prediction accuracy.  X 
LinReg: This is a linear model,  X  i = w x i , to predict the mean rating scores. As in [16], we use ridge regression to solve for the coefficients w . We choose a simple linear model since this is the underlying model for the state-of-the-art feature-based L explore-exploit algorithm [16]. The model is trained on an article X  X  comments with the comment words and LDA topics as features.
Note that we do not consider models with slack variables since they mainly impact the rating estimates of training comments and not test comments. However, as shown later, slack variables do lead to performance improvements in an explore-exploit setting where there is no separate notion of test data. The variance parameters  X  for the global model, and  X  w and  X  z for the per-article models are tuned using the validation set.
 Evaluation Methodology: For model comparisons, the (non-global) comments in each of the 198 articles are further split into training comments ( 75% ) and test comments ( 25% ). The training com-ments for each article are used to train the per-article models PA1 , PA2 ,and LinReg , and the test comments are used to measure the accuracy of all 4 models.
 Evaluation Metric: Since we are looking to recommend com-ments with high mean rating scores to users, we are primarily con-cerned about the rating prediction accuracy for comments with ex-treme mean rating values. Furthermore, we want the predicted rat-ings for comments at the two extremes to be well separated. We use the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) metric to evaluate the predictive performance of the various models.

AUC is a popular measure of classifier accuracy on data with bi-nary labels. We only choose test comments i with extreme mean bels to test comments with  X  i &gt; 0 . 75 and negative labels to com-ments with  X  i &lt; 0 . 25 . The AUC is then computed based on the correctly and incorrectly classified comments for different classi-fication thresholds on the predicted mean rating. We finally report the average AUC over all the articles. Note that a higher AUC value implies better predictive performance. Also, a random or constant rating prediction yields an AUC of 0 . 5 irrespective of the number of positive and negative examples in the test set.
Our results show that both the global prior and per-article model are critical for achieving high prediction accuracy. Table 1 shows the AUC scores for the models discussed in Section 7.3.1 for dif-ferent training set sizes. First, note that the global model ( Global ) has an AUC score greater than 0.5 and so significantly outperforms a model that makes random predictions. This clearly shows that a global model learned by pooling the articles together is useful. Furthermore, the two per-article models ( PA1, PA2 ) provide better rating estimates than Global when sufficient training data is avail-able. This is because per-article models are able to effectively cap-
Table 2: AUC scores for PA 2 for different number of topics. ture article-specific feature-rating correlations involving new article content. However, since PA1 does not use the global prior, it over-fits and performs worse than Global when the article is new and hence training data is sparse. Introducing the global prior in the per-article model PA2 ensures better predictive performance than both Global and PA1 at all the training set sizes. Note that the AUC scores of the linear LinReg model are similar to those of PA1 but lower than PA2 because of the global prior. However, as we show later, PA1 (and PA2 ) outperforms LinReg in an explore-exploit set-ting due to better variance estimates.

Table 2 shows the AUC scores of the PA2 model for different number of LDA topics and training set sizes. In general, the bag-of-words feature (corresponding to 0 topics) turns out to be the most important for our models. Topic features help to improve the performance of our models when training sets are small. This is because topics are essentially clusters of words that frequently co-occur, and using them as features allows us to overcome data spar-sity issues by generalizing correlations between a few topic words and ratings to the other words within the topic. For large train-ing set sizes and number of topics beyond 50, however, we didn X  X  see an improvement in AUC scores on adding topic features. We should point out here that all the observations made above (Tables 1 and 2) are statistically significant using the one-sided paired t-test with significance level 0 . 01 .
In this experiment, we compare the total expected number of thumbs-up ratings for different explore-exploit algorithms. Explore-Exploit Schemes Compared: We consider the following context-free (without f eatures) as well as contex tual (with features) bandit schemes for evaluation.  X 
Random: This policy chooses the top-K comments randomly.  X 
UCB1: This is a context-free UCB-based policy from [6]. In round t , the mean rating  X  i for a comment i is estimated as the current fraction of thumbs-ups the comment has received so far. The confidence interval for the estimate is given by 2log t n ( t ) is the total number of ratings received by the comment until round t .  X  -greedy1: This context-free policy chooses a random comment to show with probability , and with probability 1  X  the comment with the highest current fraction of thumbs-ups. To obtain the top-K comments, we repeat the process K times (each time excluding the comments that have already been selected).  X  -greedy2: This is the contextual version of -greedy discussed above. With probability 1  X  , it chooses the comment with the highest predicted mean rating given by the per-article logistic re-gression model with global prior and slack variables (see Equation (8)).  X 
LinUCB: This algorithm represents the state-of-the-art and uses the LinReg model to obtain the mean rating estimate  X   X  i our formulation, in [16], a closed form confidence bound is com-puted based on the predictive variance of the mean rating.  X 
LogUCB1: This is our L OG UCB algorithm (Algorithm 1) with per-article model PA1 (without global prior) used for rating predic-tion.  X 
LogUCB2: This is our L OG UCB algorithm with per-article model PA2 (with slack variables set to 0 ) used for rating predic-tion.  X 
LogUCB3: This is our full L OG UCB algorithm (Algorithm 1) with non-zero slack variables.

The parameters for -greedy1 and -greedy2 ,and  X  for LinUCB and LogUCB variants are tuned using the validation set. Also, since PA2 has a large number of features, we use the diagonal approxi-mation of the Hessian in LogUCB2 and LogUCB3 for computing the covariance (see Section 5.2).
 Evaluation Methodology: For evaluating the different explore-exploit schemes, the train-test split is not needed and we use all the comments of the 198 articles. Further, we evaluate performance using an offline simulation over 7000 rounds with new comment arrivals for an article following a poisson distribution. For a given explore-exploit algorithm, in each round t , the simulator takes the top-K comments chosen by the algorithm. For each of the K comments, it generates a binary thumbs-up/down rating from a Bernoulli distribution with proba bility given by t he ground truth mean rating of the comment. These ratings are then provided to the algorithm as feedback, and the underlying models and mean rating estimates are appropriately updated at the end of the round. Evaluation Metrics: We define the Thumbs-up rate at round t (TR@ t ) as the fraction of thumbs-up ratings received until round t . We use T R @ t to evaluate the different explore-exploit algorithms since it is a good proxy for the total expected number of thumbs-up ratings that the algorithms are trying to maximize.
Figure 3 plots TR@ t values for the different explore-exploit al-gorithms discussed in Section 7.4.1 as a function of the number of rounds t .

Figure 3(a) compares the different context-free schemes with our logistic regression-based schemes for K =1 . Note that K =1 corresponds to the standard single-slot multi-armed bandit prob-lem. We do not present the curve for Random since it performs significantly worse than the other schemes with TR@t values be-low 0.65. Observe that -greedy2 performs significantly better than -greedy1 and UCB1 when the number of rounds is small. This clearly shows that using the predictions of our feature-based logis-# Comments 25 50 100 250 500 1000 Time (seconds) 0.87 0.92 1.0 1.13 1.28 1.63 tic model in the exploit phase is more useful than empirical esti-mates of ratings in either -greedy1 or UCB1 . When the number of rounds become large and empirical estimates of ratings become reliable, the performance of -greedy1 and -greedy2 are compara-ble. Also, note that LogUCB3 consistently outperforms -greedy2 . Thus, the UCB value which includes model variance in LogUCB3 is a better exploration strategy than the random one in -greedy2 . Figures 3(b) and 3(c) compare the different variants of our L UCB algorithm and L IN UCB for K =1 and K =10 , respec-tively. Observe that LogUCB3 has a higher thumbs-up rate than both LogUCB1 and LogUCB2  X  this can be attributed to more accu-rate rating estimates due to the use of slack variables in LogUCB3 . Initially, when there are very few user ratings, LogUCB2 does bet-ter than LogUCB1 due to the global prior. However, the diago-nal approximation for computing covariance causes its thumbs-up rate to fall below that of LogUCB1 as the rounds progress. Fi-nally, LogUCB3 and the other LogUCB variants outperform Lin-UCB since as mentioned earlier, logistic regression has bounded mean rating and variance estimates (between 0 and 1), and is thus better suited for modeling binary ratings data. Again, it is worth mentioning here that the difference in performance of the various schemes in Figure 3 was found to be statistically significant with significance level 0 . 01 .
Table 3 depicts the total time (in seconds) taken by the LogUCB3 algorithm to train the per-article model and compute UCB values as the number of comments is increased from 25 to 1000. The running times are measured on a machine with 2  X  2.5 GHz Xeon processors and 16 GB of RAM.

In our experiments, we found that more than 90% of the time is spent on computing the MAP estimates w map and z map (see Sec-tion 5.2). Observe that model training and UCB computation are extremely fast and take only 1.63 seconds for 1000 comments. This is because we use the previous MAP estimates as the starting point in our conjugate gradient descent algorithm and the number of new ratings between two successive model updates is generally small  X  this causes the algorithm to converge within a few iterations. More-over, gradient computations are also quick because comment fea-ture vectors are sparse (comments are short with very few words). This is also the reason why computing the covariance (see Sec-tion 5.2) and the UCB values (mean ratings in Equation (8) and variance in Equation (9)) takes only 26 and 148 msec, respectively, for all 1000 comments. Thus, our per-article models can be trained at frequent time intervals to give accurate rating estimates. We should also point out here that compared to 1.63 seconds taken by LogUCB3 , LinUCB takes 0.95 sec to train models and compute UCB values for 1000 comments. Recommender Systems: Recommender systems have been stud-ied extensively in the literature. Popular methods predict user-item ratings based on neighborhoods defin ed using similarities between users/items [26], latent factors f ound using user-item matrix factor-ization [22], and generalizations of matrix factorization that model latent factors using regression on user/item features [1, 4] or item LDA topics [2]. However, these methods focus only on estimating ratings, and not on balancing the exploitation-exploration tradeoff which is so critical in our setting due to the dynamic nature of com-ments. Furthermore, due to fewer comment ratings per article, the feature weight and slack variable parameters in our per-article mod-els both contribute to variance in the mean rating estimate  X  so we place Gaussian priors on both parameters (instead of only slack variables as is done in [1, 2]). Note that although our modeling framework currently models an average user, it can be easily ex-tended to make personalized reco mmendations if user features are available.
 Explore-Exploit: There exists a large body of work on the multi-armed bandit problem [12, 6, 15, 5, 16, 7]. Context-free bandit algorithms [6, 15, 5] such as -greedy and UCB assume that no side information is available and that arms are independent of each other. Such algorithms have been shown to achieve the lower bound of O (ln T ) on the regret [15] (here, T is the number of trials). [3] develops a Bayesian solution and extends several existing bandit schemes to a dynamic set of items with short lifetimes, delayed feedback, and non-stationary reward distributions. However, the above methods assume that arms are independent, and hence do not model the correlation between the comments of a given article.
Contextual bandit algorithms [16, 7] assume that side informa-tion (e.g., features) is also available. L IN R EL [7] and L are UCB-based algorithms that assume the reward of an arm is lin-early dependent on its features. The algorithms have been shown to have an  X  O ( problem setting, arms are dynamic. Furthermore, linear regression is not suitable for modeling binary ratings data, and cannot ensure that the average rating lies in the range [0 , 1] .

Pandey et. al [20] assume that dependencies among arms can be described by a generative model on clusters of arms. However, these clusters are assumed to be known beforehand and each arm belongs to exactly one cluster. In contrast, our framework does not assume any clustering of comments. Instead, the dependencies among comments are effectively captured by our logistic regression model with comment features.
 User Generated Content: Recently, analysis of user generated content has been an active area of research [24, 17, 28, 27, 11, 4, 14]. [11] uses different user, textual, and network features to an-alyze the credibility of tweets on twitter. [27] extends LDA [10] to model the generation of blog posts, authorship, as well as com-ments on the posts. [28] proposes a variant of supervised LDA [9] called the Topic-Poisson model to identify which blog posts will receive a high volume of comments. However, these methods do not focus on recommending comments to users. [14] uses an SVM regressor on different features such as unigrams, review length etc. to determine how helpful a review is. The helpfulness is defined as the fraction of users who found the review to be helpful. [24] an-alyzes the dependencies between comments, their ratings, and dif-ferent topic categories. An SVM classifier is used to estimate the ratings of new or unrated comments. However, none of these meth-ods employ an explore-exploit strategy, and are thus inadequate for ranking dynamic comments.
In this paper, we proposed a novel UCB-based explore-exploit algorithm called L OG UCB for recommending comments to users. L
OG UCB uses a logistic regression model on word and topic fea-tures to predict the average rating for a comment. At its core, L
OG UCB relies on a novel variance approximation technique un-der a Bayesian setting to deriv e the upper confidence bounds used to select comments. In experiments with a real-life comments dataset from Yahoo! News, L OG UCB outperforms other state-of-the-art context-free and feat ure-based explore-exploit methods like L UCB. Directions for future work include extending our L OG algorithm to ensure diversity of the K recommended comments, and proving formal regret bounds for our LogUCB algorithm. [1] Deepak Agarwal and Bee-Chung Chen. Regression-based [2] Deepak Agarwal and Bee-Chung Chen. flda: matrix [3] Deepak Agarwal, Bee-Chung Chen, and Pradheep Elango. [4] Deepak Agarwal, Bee-Chung Chen, and Bo Pang.
 [5] R. Agrawal. Sample mean based index policies with o(log n) [6] Peter Auer, Nicol X  Cesa-Bianchi, and Paul Fischer.
 [7] Peter Auer and M. Long. Using confidence bounds for [8] Chris Bishop. Pattern Recognition and Machine Learning . [9] David Blei and Jon McAuliffe. Supervised topic models. In [10] David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John [11] Carlos Castillo, Marcelo M endoza, and Barbara Poblete. [12] J. C. Gittins. Bandit Processes and Dynamic Allocation [13] T. L. Griffiths and M. Steyvers. Finding scientific topics. [14] Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco [15] Tze L. Lai and Herbert Robbins. Asymptotically efficient [16] Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. [17] Yue Lu, ChengXiang Zhai, and Neel Sundaresan. Rated [18] David Luenberger and Yinyu Ye. Linear and Nonlinear [19] D J C MacKay. The evidence framework applied to [20] Sandeep Pandey, Deepayan Chakrabarti, and Deepak [21] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. [22] Ruslan Salakhutdinov and Andriy Mnih. Bayesian [23] Andrew Schein and Lyle Ungar. Active learning for logistic [24] Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl, and Jose [25] Aleksandrs Slivkins, Filip Radlinski, and Sreenivas [26] Jun Wang, Arjen P. de Vries, and Marcel J. T. Reinders. [27] Tae. Yano, William. Cohen, and Noah A. Smith. Predicting [28] Tae. Yano and Noah A. Smith. What X  X  worthy of comment?
