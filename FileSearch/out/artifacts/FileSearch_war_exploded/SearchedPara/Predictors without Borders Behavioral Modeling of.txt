 Billions of people around the world live without access to banks or other formal financial institutions. In the past sev-eral years, many mobile operators have launched  X  X obile Money X  platforms that deliver basic financial services over the mobile phone network. While many believe that these services can improve the lives of the poor, in many coun-tries adoption of Mobile Money still remains anemic. In this paper, we develop a predictive model of Mobile Money adoption that uses billions of mobile phone communications records to understand the behavioral determinants of adop-tion. We describe a novel approach to feature engineering that uses a Deterministic Finite Automaton to construct thousands of behavioral metrics of phone use from a concise set of recursive rules. These features provide the foundation for a predictive model that is tested on mobile phone op-erators logs from Ghana, Pakistan, and Zambia, three very different developing-country contexts. The results highlight the key correlates of Mobile Money use in each country, as well as the potential for such methods to predict and drive adoption. More generally, our analysis provides insight into the extent to which homogenized supervised learning meth-ods can generalize across geographic contexts. We find that without careful tuning, a model that performs very well in one country frequently does not generalize to another. Mobile Money; Feature engineering; Gradient boosting; Prod-uct adoption; Supervised learning
The rapid penetration of mobile phones in developing coun-tries is creating new opportunities to provide basic finan-cial services to billions of individuals who have never before had access to banks or other formal financial institutions (Figure 1). In particular, over the last several years, mo-bile phone operators across the globe have launched  X  X obile are colored according to the fraction of Mobile Money users in each region spire on the mobile phone network, including phone calls, text messages, and Mobile Money transactions. For thou-sands of unique individuals in each country, we can thus infer a wealth of information about the structure of their social networks, their daily movements about the country, patterns of communication, and several other behaviors that we discuss in greater detail below. We also know whether each subscribers eventually signs up for Mobile Money, and if so, whether he or she remains an active user on the sys-tem. In the three countries we study -Ghana, Pakistan, and Zambia -each Mobile Money platform is owned and oper-ated by a separate, independent mobile phone operator, and the subscriber population in each country has very different social and economic characteristics.

There are three substantive and one methodological con-tributions of this study. Substantively, we (1) develop a richer understanding of what drives the adoption of Mobile Money, by mining several large databases of transactions data; (2) construct a supervised learner that can predict, to varying degrees of accuracy depending on the prediction task and country context, the likelihood than an individual subscriber will use Mobile Money; and (3) explore the pos-sibility that transfer learning could be used to train models in one country or context and apply them in another. To our knowledge, this is the first study to train and evaluate models of product adoption in three very different contexts. Since Mobile Money adoption is notoriously idiosyncratic, we hope this  X  X ross-cultural X  comparison can provide insight into the generalizability of our results, and increase their broader relevance to the policy and business communities working in developing countries.

Methodologically, we develop a novel framework for ex-tracting behavioral metrics from transaction logs, which pro-duces interpretable features that can provide the input data into standard supervised learning algorithms. This frame-work extends previous efforts described in [2], which used a simplified approach to predict poverty and wealth from mobile phone data. The core of this approach is formal-ized as a Deterministic Finite Automaton, which provides a structured, recursive grammar that relies on relatively few degrees of freedom to generate a comprehensive and inter-pretable set of  X  X ense X  features from sparse log data. This approach is sufficiently generalizable that we hope it can be further extended to a much broader range of contexts where of different types of mobile phone metrics on the adoption of Mobile Money in Africa, using 180 metrics derived from call data [5]. Relative to these studies, our study moves this literature forward by (a) innovating in the method used to generate features, thereby providing a systematic and com-prehensive approach to feature engineering; (b) leveraging data from three different contexts to calibrate the external validity and generalizability of our results; and (c) carefully articulating the experimental protocols and algorithms in a way that will enable other researchers to replicate and ex-tend these methods.
For this study, we worked in collaboration with three mo-bile phone operators in Ghana, Pakistan, and Zambia. All three countries rank in the bottom third of the Human De-velopment Index, a metric developed by the United Nations to capture a broad range of welfare outcomes such as in-come, education, inequality, and life expectancy. As can be seen in Figure 1, penetration of financial services is very low in each country. Geographic patterns of Mobile Money adoption also vary greatly within each country, as shown in Figure 2. Additional information on each country is pro-vided in Table 1. Of course, these country-level statistics mask enormous diversity between and within nations in so-cial and demographic characteristics, religious and political attitudes, and general ways of living.

From each mobile phone operator, we obtained the anony-mized Call Detail Records (CDR) and Mobile Money Trans-action Records (MMTR) of every subscriber on the network. The CDR and MMTR contain basic metadata on every event that occurs on the mobile phone network, including phone calls, text messages, and any form of Mobile Money activity. CDR typically consists of tuples containing {cal-lerID, recipientID, date, time, duration, callerLoca-tion} , where the two ID X  X  are anonymized phone numbers, the date and time indicate when the event transpired, the duration of the call is recorded in seconds, and the location field indicates the cellular tower through which the call was routed, which can be used to pinpoint the approximate lo-cation of the individual at the time of the call. 3 For the mo-
In practice, the cell tower is accurate within several hun-dred meters in urban areas, and tens of kilometers in rural regions. We do not observe the contents of text messages.
As highlighted in the introduction, the CDR contain a wealth of latent information about how people communicate, with whom they interact, the locations they visit, and many other social and behavioral characteristics. Our eventual goal is to leverage this information to better understand why people use Mobile Money, and to develop a predictive model of Mobile Money adoption. However, the raw CDR are not natural inputs to most machine learning algorithms, and interpretable metrics must first be derived from the CDR before inferences can be made.

In the prior literature, the vast majority of studies take a rather ad hoc approach to constructing interpretable met-rics ( X  X eatures X ) from the phone data. The most common approach is to hand-craft a small number of features that correspond to some intuition of the researcher. For instance, [10] focus on 5 topological properties of the static social net-work; [17] use two metrics that quantify airtime purchases; and [14] construct 6 measures of physical mobility. Even the more ambitious approaches, such as [28] and [5], which respectively use 350 and 180 CDR-based metrics, employ a large number of idiosyncratic rules to determine which fea-tures should be considered by the learning algorithm. These approaches have the advantage of producing metrics that are convenient to interpret, but they may systematically over-look non-intuitive features, mis-attribute relationships (if, for instance, feature A is weakly correlated with the tar-get variable only because an omitted feature B is strongly correlated with both A and the target variable), or fail to maximize the predictive power of a classifier that would per-form better with a more comprehensive set of features.
Our approach is different. We develop a method for fea-ture engineering from transactional data that is designed to construct a large and comprehensive set of features from a small number of recursive operations. While the application is to CDR, we believe this method could be used to engineer features from a more diverse class of data including IP logs, social media data, and financial transaction records. each state inside the circle. Valid features are constructed through traversals of the state machine, which start at the start state ( q0 ) and end at the end state ( q3 ) and follow only legal transitions between states (denoted by arrows). For instance, the feature described above would begin with the full CDR in q0 , filter outgoing calls and return to q0 , map (group by)  X  X go X  i and proceed to q1 , map by day of week to q4 , select duration and proceed to q5 , reduce (aggregate) by average -this produces average call duration for each day of week for each i -and proceed to q2 , aggregate by variance and exit at q3 .

Formally, the DFA is specified by: In total, there are several thousand valid traversals of the DFA, each of which produces a different feature. Together, the resulting set of features covers almost all of the hand-crafted metrics used in prior work, as well as many, many more.

An additional advantage of the DFA is that it can be effi-ciently and elegantly implemented. Pseudo-code is provided in Algorithm 1, and the implementation in Spark Python is available from the author X  X  website. 5
The DFA is a convenient abstraction for generating a very large number of features from a small set of rules. To inter-pret the set of features produced by the DFA, we label each generated feature with interpretable tags. These tags are determined by the path taken through the automaton, and indicate whether each feature captures information on, for example, incoming vs. outgoing communications, calls vs. text messages, variance vs. volume, and so on. Specifically, we map each feature onto the tree structure shown in Figure 5, which is designed to encapsulate the substantive behavior captured by each feature. Each level of the tree corresponds to a different partition of the feature space: 1. Actor: Whether the feature relates to activity of the 2. Type: Whether the feature relates to phone calls or 3. Direction: Whether the feature relates to incoming 4. Behavior: Whether the feature relates to movement http://www.jblumenstock.com
We then use a variety of supervised learning algorithms to tackle two classification tasks. First, we seek to differ-entiate between Voice Only subscribers and Registered Mo-bile Money Users (one or more Mobile Money transactions); second, we attempt to differentiate between Voice Only and Active Mobile Money users (at least one transaction per month). In all cases, we report the average accuracy across testing sets from 10-fold cross validation.

Since our data has a large number of features relative to observations, we focus on learners that are robust to over-fitting, such as regularized and elastic net logistic regres-sion [30], gradient boosting [15], and Extremely Randomized Trees [16]. Performance was comparable across these clas-sifiers, although as expected these methods generally per-formed better than unregularized alternatives. To stream-line the analysis that follows, we report only the results from gradient boosting, which outperformed the other classifiers by a small margin. To understand which CDR-based features are related to Mobile Money use, we calculate two metrics: 1. (Unconditional) AUC : We run a (cross-validated) 2. (Conditional) Normalized feature importance : show only the fraction of users of each type, rather than the raw numbers, which are in the millions. use in Ghana. Here, the range of AUC values is significantly higher than in the full set of features, and some sub-classes such as  X  X etwork X  have uniformly high predictive power. 7 Finally, the usefulness of each class of features depends on whether the goal is to identify Registered or Active Mobile Money users. For instance, the  X  X M Alter X  features, which capture information about the characteristics of i  X  X  network who have previously adopted Mobile Money, are bimodally distributed and on average more useful in predicting Reg-istered users than Active users. However, that same class contains a small number of features that are extremely good predictors of Active Mobile Money use.

A similar approach is taken to construct Figure 9, ex-cept here we show the distribution of normalized feature importance values obtained through gradient boosting. The difference between the values in Figure 9 and Figure 8 is that the former are conditional on all features present in the final classification model, which includes several hun-dred features, whereas the latter are unconditional, i.e., they indicate performance in a univariate model with no other features. As in the uncondititional ranking, each class of features in the conditional ranking contains a mass of fea-tures with low predictive power, but closer inspection reveals interpretable patterns.

Perhaps most striking in Figure 9 are the differences be-This particular class, where Actor= X  X oice Alters, X  Type= X  X ll, X  and Behavior= X  X etwork X , corresponds to in-formation about the network structure of i  X  X  network; in other words, 2nd degree properties of i  X  X  network.
As discussed in Section 4.2, we test the ability of sev-eral supervised classification models to discriminate between Voice Only and Mobile Money users, using the CDR-based features constructed from the DFA. Cross-validated results from gradient boosting are reported separately for each coun-try in Figure 10. The best-performing models included sev-eral hundred features, but in practice there was little differ-ence in performance between models in the range of 50-1000 features. We also include results from a baseline classifier, which uses the same model trained on a single  X  X ntuitive X  feature  X  the total number of outgoing calls made by the subscriber, which is the feature shown in Figure 7.
In each country, the DFA-based classifier significantly out-performs the naive baseline, and in all countries, we achieve marginally better success in identifying Active Mobile Money users than Registered Mobile Money users. Across countries, however, there is a great degree of variability in classifier performance, with classification accuracy between 71% and 78% in Ghana and Zambia, but only 58%-59% in Pakistan. We discuss several possible explanations for these results in Section 6.
In the proceeding analysis, we have been careful to stan-dardize the methods and analysis performed across all three countries. In each instance, we use the exact same source data, DFA specification, classification algorithm, experimen-tal sample size, and so forth. In some cases, this meant that we knowingly discarded data that might have improved the performance of the classifier in a single country. For ex-ample, in some countries we had several months of CDR that could be used for training, additional fields in the CDR metadata, or much larger samples of Mobile Money users available for training and cross-validation. However, our approach reduced everything to the lowest common denom-inator in order to maintain comparability across contexts.
A key advantage of this approach is that it makes it possi-ble to answer a question that has been elusive in prior studies of the adoption of new technologies in developing countries: Do the behavioral determinants of adoption identified in one context generalize to another? Based on the analysis we have performed, our short answer to this question appears to be,  X  X o. X 
Figure 11 shows the performance of a classifier trained in one country and evaluated in another. Thus, the first set of six bars shows that the classifiers trained in Ghana perform well in Ghana (the first two grey bars), essentially replicat-ing the results in Figure 10. However, that same Ghana model does quite poorly when it is evaluated in Zambia (the next two blue bars) and Pakistan (the final two green bars). While it is almost certain that a more sophisticated approach to transductive transfer learning would perform better [25], the naive application of a model out of context is quite in-effective. We return to these ideas in the discussion that follows.
Taken in the broader context of research into the deter-minants of Mobile Money adoption and use, the preceding results uncover several unexpected patterns. Superficially, it is not surprising that CDR-based metrics can be used to construct classifiers that predict Mobile Money use, though different countries and cultures in the developing world, no single set of behavioral features is likely to consistently pre-dict Mobile Money adoption and use. This is most clearly evident in Figure 11, which shows that a classifier trained in one country performs very poorly when tested in another country. But the same conclusion may also be drawn from Figure 9, where we see that the same model, when trained in different countries, selects different features and attaches different weights to to those selected features. In results not shown, we further inspect the list of top-ranked features for each country, using both (unconditional) AUC and (condi-tional) normalized feature importance, and note very few features that appear consistently across countries. However, even though there may not be a  X  X olden X  list of features that always predict Mobile Money use, we are optimistic that more generalized insights can be extracted from one context and applied in another. In ongoing work, we are exploring methods for transfer learning that may strike this balance.

More concretely, over the past several months our partner in Ghana has been using the methods we describe to gener-ate  X  X doption Scores X  that indicate the likelihood that any given mobile subscriber will adopt and use Mobile Money. They recently reported that when using these scores to tar-get promotions, response rates were roughly 30% higher than promotions targeted with traditional methods. Such estimates are notoriously unreliable and subject to many possible sources of bias, but their optimism provides an in-dication of the potential for this line of research. At the same time, it should be noted that if the end goal is to in-crease financial inclusion of the poor, further methodological innovation is needed beyond what identifies the  X  X ow hang-ing fruit X  subscribers whose behavior indicates that they are likely to adopt of their own volition.
In this paper, we present a new approach to feature engi-neering that uses deterministic finite automata to construct a very large number of features from a concise set of rules. In applying this technique to mobile phone data from Ghana, Pakistan, and Zambia, we show that the resultant metrics correlate with, and can be used to predict, both active and passive Mobile Money use in three very different contexts. In so doing, we discover several previously undocumented patterns related to the adoption and use of Mobile Money. Superficially, the analysis makes it possible to highlight spe-cific correlates of Mobile Money use, such as the relative importance of network structure in Ghana and Pakistan, and the relative importance of geographic diversity in Zam-bia. More fundamentally, the results provide insight into the extent to which standard predictive models can gener-alize across contexts. Here, it is clear that each population has a unique signature in terms of what metrics are good predictors of adoption, and as a result, models trained in one location do not perform well in another. Retraining the model helps, but does not solve, the underlying issue. De-spite the fact that the data structures, experimental design, and Mobile Money products are nearly identical in the three countries, the performance of each country-specific model varies greatly. [18] W. Jack and T. Suri. Risk sharing and transactions [19] M. R. Khan, J. Manoj, A. Singh, and J. Blumenstock. [20] J. Leskovec, L. A. Adamic, and B. A. Huberman. The [21] I. Mas and D. Radcliffe. Scaling mobile money. Journal [22] W. S. McCulloch and W. Pitts. A logical calculus of [23] I. Medhi, A. Ratan, and K. Toyama. Mobile-banking [24] O. Morawczynski. Exploring the usage and impact of [25] S. J. Pan, I. Tsang, J. Kwok, and Q. Yang. Domain [26] Safaricom. FY14 Presentation, 2014. [27] C. Scharwatt, A. Katakam, J. Frydrych, A. Murphy, [28] P. Sunds X y, J. Bjelland, A. M. Iqbal, Y.-A. [29] J. Ugander, L. Backstrom, C. Marlow, and [30] H. Zou and T. Hastie. Regularization and variable
