 Ranked retrieval has a particular disadvantage in comparison with traditional Boolean retrieval: there is no clear cut-off point where to stop consulting results. This is a serious problem in some setups. We investigate and further develop methods to select the rank cut-off value which optimizes a given effectiveness measure. Assuming no other input than a system X  X  output for a query X  X ocument scores and their distribution X  X he task is essentially a score-distribution-al threshold optimization problem. The recent trend in modeling score distributions is to use a normal-exponential mixture: normal for relevant, and exponential for non-relevant document scores. We discuss the two main theoretical problems with the current model, support incompatibility and non-convexity, and develop new mod-els that address them. The main contributions of the paper are two truncated normal-exponential models, varying in the way the out-truncated score ranges are handled. We conduct a range of experi-ments using the TREC 2007 and 2008 Legal Track data, and show that the truncated models lead to significantly better results. H.3.3 [ Information Search and Retrieval ]: Information Filtering; Retrieval Models Experimentation, Performance, Theory
Ranked retrieval has a particular disadvantage in comparison to traditional Boolean retrieval: there is no clear cut-off point where to stop consulting results. This is hardly a practical problem in set-tings such as Web Search where only the initially retrieved results are consulted. In recall-oriented retrieval setups, such as searching patents or litigation and regulatory documents, the problem sur-faces at full force. It is simply too expensive to give a ranked list with zillions of results to patent experts or litigation support professionals paid by the hour. This may be one of the reasons why ranked retrieval has been adopted very slowly in professional search.
 Figure 1: Score distribution (top); fitted mixture (middle); and the
The  X  X issing X  cut-off remains unnoticed by standard evaluation measures: there is no penalty and only possible gain for padding a run with further results. At TREC 2008 the Legal Track addressed this problem head-on by requiring participants to submit the rank at which precision and recall are best balanced [12]. Our overall aim is to investigate and further develop methods to selecting a rank cut-off value K , per topic, for optimizing the given F 1 Note that the resulting F 1 @ K is as much as a result of the quality of the underlying ranking as of the choice of the cut-off value, but we focus entirely on determining the optimal rank cut-off values. The methods can be applied to a range of effectiveness measures; the measure under optimization is merely a parameter.
 How can one select such a rank threshold given a ranked list? Selecting K is essentially equivalent to thresholding in binary clas-sification or filtering [15]. Provided that there exist appropriate training data, a natural candidate line of approaches would be to apply machine learning [10, 17]. However, we assume here that results for a query are based solely on analysing the system out-put for this query X  X he scores and their distribution X  X nd nothing else. In this context, machine learning methods cannot be applied and native IR methods (or  X  X omegrown X  methods [15]) must be used. Robertson and Callan [15] stress both the importance and the difficulty of threshold setting in homegrown IR methods. We opt for a pure IR approach: score-distributional threshold optimization (illustrated in Figure 1). In a nutshell, we treat the total score distri-bution as a weighted sum of the distributions of relevant and non-relevant scores. If we can recover these two distributions and their mix weight, we can approximate the numbers of relevant and non-relevant documents above and below any rank, and consequently, the total number of relevant documents R . Having these, any of the usual measures based on document counts can be calculated at all ranks, and the optimal rank for the selected measure can be found.
Apart from the practical use to determine a rank cut-off value op-timizing some measure, the score-distributional methods also have theoretical importance. Provided that a model fits well enough, it can reveal the underlying relevance distribution (in much greater detail than through some judged documents), can transform scores to probabilities of relevance, and may reveal suboptimal behavior of the used ranking method. For example, these methods help us understand the underlying IR problem: What is the optimal cut-off point? How does it relate to R ? Hence, advancing the score-distri-butional methods has direct relevance to IR theory. This is exactly why score distributions have attracted a lot of attention since the early days of IR. A range of known distributions have been consid-ered, with the mixture of normal-exponential being the most popu-lar in recent years. This mixture will be our starting point.
Over the years, two main problems have been identified with the normal-exponential mixture, and the main contribution of this pa-per is the development of new models that address these theoretical problems. The mixture, as it has been used in all related literature so far, has a support incompatibility problem: while the exponen-tial is defined at or above some minimum score, the normal has a full real axis support. This is a theoretical problem that we will address by investigating truncated distributional models.
From the point of view of how scores or rankings of IR systems should behave, Robertson [14] formulates the recall-fallout con-vexity hypothesis, a condition on how those two measures should trade-off for good systems. Similar conditions can be formulated on other quantities, e.g. on smoothed precision or probability of relevance which both should be monotonically increasing with the score. The normal-exponential mixture violates such conditions, thus it has a problem of non-convexity . One could seek alternative models without a convexity problem, but here our goals are more modest; we will at least address the convexity problem present in the mixture of normal-exponential distributions.
The rest of this paper is organized as follows. In Section 2, we discuss earlier work on thresholding, in particular the score-distri-butional (s-d) method, and how it can be adapted for rank thresh-olding. The normal-exponential mixture is discussed in Section 3, and its theoretical problems in Section 4. Then, we develop new truncated mixture models in Section 5, and discuss the methods we use for their parameter estimation in Section 6. In Section 7, we conduct experiments on the TREC Legal Track data, both ana-lyzing the resulting fits of the s-d models and the effectiveness of selecting rank cut-off values optimizing the F 1 measure used in the Legal 2008. Finally, we summarize the findings in Section 8.
In this section, we discuss earlier work on thresholding and score distributions, focusing on the score-distributional threshold opti-mization , a method first introduced at the TREC-9 Filtering Track [3, 4]. We re-formulate the method in order to stress variables re-lated to the task we are dealing with, such as the total number of relevant documents R , as well as to clarify the assumptions under which the method works. Finally, we also adapt the method to the task of rank thresholding.
Let us assume an item collection of size n , and a query for which all items are scored and ranked. Let P ( s | 1) and P ( s | 0) be the probability densities of relevant and non-relevant documents as a function of the score s , and F ( s | 1) and F ( s | 0) their correspond-ing cumulative distribution functions (cdfs). Let G n  X  [0 , 1] be the fraction of relevant documents in the collection, also known as gen-erality . The total number of relevant documents in the collection is given by while the expected numbers of relevant and non-relevant documents with scores &gt; s are respectively. The expected numbers of the relevant and non-rele-vant documents with scores  X  s respectively are
Let us now assume an effectiveness measure M of the form of a linear combination the document counts of the categories defined by the four combinations of relevance and retrieval status, for ex-ample a linear utility [15]. From the property of expectation lin-earity, the expected value of such a measure would be the same linear combination of the above four expected document numbers. Assuming that the larger the M the better the effectiveness, the optimal score threshold s  X  which maximizes the expected M is Given n , the only unknowns which have to be estimated are the densities P ( s | 1) and P ( s | 0) (or their cdfs), and the generality G
So far, this is a clear theoretical answer to predicting s even normalizing scores to probabilities of relevance by straight-forwardly applying the Bayes X  rule [3, 11].
The s-d threshold optimization method is based on the assump-tion that the measure M is a linear combination of the document counts of the four categories defined by the user and system de-cisions about relevance and retrieval status. However, measure linearity is not always the case, e.g. the F measure is non-linear. Non-linearity complicates the matters in the sense that the expected value of M cannot be easily calculated. Given a ranked list some approximations can be made to simplify the issue. If G n , F ( s | 1) , and F ( s | 0) are estimated on a given ranking, then Equations 2 X 5 are good approximations of the actual document counts. Plugging those counts into M , we can now talk of actual M values rather than expected.
While M can be optimal anywhere in the score range, with re-spect to optimizing rank cutoffs we only have to check its value at the scores corresponding to the ranked documents, plus one extra point to allow for the possibility of an empty optimal retrieved set. Let s k be the score of the k th ranked document, and define M follows: M Now the optimal rank K is arg max k M k . This allows for K to become 0, meaning that no document should be retrieved.
Let us now elaborate on the form of the two densities P ( s | 1) and P ( s | 0) of Section 2.1. Score distributions have been modeled since the early years of IR with various known distributions. Swets [18] used two normal distributions, and later two exponentials [19]. Bookstein [6] used two Poisson distributions, and Baumgarten [5] used two Gamma distributions. Arampatzis et al. [4] started using a mixture of normal-exponential distributions: normal for relevant, exponential for non-relevant. Since this mixture has become the trend in the last few years [1, 3, 7, 11, 20], it is our starting point.
The normal-exponential model works as follows. Let us con-sider a general retrieval model which in theory produces scores in [ s min ,s max ] , where s min  X  R  X  X  X  X  X } and s max  X  R  X  X  +  X  X  . By using an exponential distribution, which has semi-infinite sup-port, the applicability of the s-d model is restricted to those retrieval models for which s min  X  R . The two densities are given by where  X  ( . ) is the density function of the standard normal distribu-tion, i.e. with a mean of 0 and standard deviation of 1, and  X  ( . ) is the standard exponential density [13]: The corresponding cdfs are given by where erf( . ) is the error function [13]. The total score distribution is written as where G n  X  [0 , 1] . Hence, there are 4 parameters to estimate,  X  ,  X  ,  X  , and G n .

Despite its popularity, it was pointed out recently that the mixture of normal-exponential presents a theoretical anomaly in the context of IR. In practice, nevertheless, it has stood the test of time in the light of a) its (relative) ease to calculate, b) good experimental re-sults, and c) lack of a proven alternative. The reader should keep in mind that the normal-exponential mixture fits some retrieval mod-els better than others, or it may not fit some data at all. As a rule of thumb, candidates for good fits are scoring functions in the form of a linear combination of query-term weights, e.g. tf.idf, cosine sim-ilarity, and some probabilistic models [3]. Also, long queries [3] or good queries/systems [11] seem to help.
 In this paper, we do not set out to investigate alternative mixtures. We theoretically extend and refine the current normal-exponential model in order to address the problems which we will discuss in the next section.
Over the years, two main problems of the normal-exponential model have been identified. Although we already generalized it somewhat above by introducing a shifted exponential , the mix, as it has been used in all related literature so far, has a support incompat-ibility problem: while the exponential is defined at or above some s min , the normal has a full real axis support. This is a theoretical problem which is solved by the new models we will introduce in the next section. In the remainder of this section, we will describe the other main problem: recall-fallout non-convexity.

From the perspective of how scores or rankings of IR systems should be, Robertson [14] formulates the recall-fallout convexity hypothesis: Similar hypotheses can be formulated as a conditions on other mea-sures, e.g. the probability of relevance should be monotonically in-creasing with the score; the same should hold for smoothed preci-sion (which calculates precision only at points where relevant docu-ments are found and interpolates in between). Although, in reality, these conditions may not always be satisfied, they are expected to hold for good systems, i.e. those producing rankings satisfying the probability ranking principle (PRP), because their failure implies that systems can be easily improved. As an example, let us con-sider smoothed precision. If it declines as score increases for a part of the score range, that part of the ranking can be improved by a simple random re-ordering [16]. This is equivalent of  X  X orcing X  the two underlying distributions to be uniform (i.e. have linearly in-creasing cdfs) in that score range. This will replace the offending part of the precision curve with a flat one X  X he least that can be done X  X mproving the overall effectiveness of the system.

Such hypotheses put restrictions on the relative forms of the two underlying distributions. The normal-exponential mixture violates such conditions, only (and always) at both ends of the score range. Although the low-end scores are of insignificant practical impor-tance, the top of the ranking is very significant. The problem is a manifestation of the fact that an exponential tail extends further than a normal one. We focus on the problem at the top scores, and denote the lowest offending score with s c . Since the F -measure we are interested in is a combination of recall and precision (and recall by definition cannot have a similar problem), we find s precision. We force the distributions to comply with the hypothesis only when s c &lt; s 1 , where s 1 the score of the top document; oth-erwise, the theoretical anomaly does not affect the observed score range. If s max is finite, then two uniform distributions can be used in [ s c ,s max ] as mentioned earlier. Alternatively, preserving a theo-retical support in [ s min , +  X  ) , the relevant documents distribution can be forced to an exponential in [ s c , +  X  ) with the same  X  as this of the non-relevant. We apply the alternative.

In fact, rankings can be further improved by reversing the of-fending sub-rankings; this will force the precision to increase with an increasing score, leading to a better effectiveness than the ran-dom re-ordering. However, the big question here is whether the initial ranking satisfies the PRP or not. If it does, then the problem is an artifact of the normal-exponential model and reversing the sub-ranking may be actually damaging to performance. If it does not, then the problem is inherent in the scoring formula producing the ranking. In the latter case, the normal-exponential model can-not be theoretically rejected, and it may even be used to detect the anomaly and improve rankings.
 It is difficult to determine whether a single ranking satisfies the PRP; precision for single queries is erratic, especially at early ranks, justifying the use of interpolated precision. According to interpo-lated precision all rankings satisfy the PRP, but that is due to the interpolation. Consequently, we rather leave open the question of whether the problem is inherent in some scoring functions or in-troduced by the combined use of normal and exponential. Being conservative, we just randomize the offending sub-rankings rather than reversing them. The impact of this on thresholding is that the s-d method turns  X  X lind X  inside the upper offending range; as one goes down the corresponding ranks, precision would be flat, recall naturally rising, so the optimal F 1 threshold can only be below the range.

The models we introduce next, although they do not eliminate the problem, do not always violate such conditions imposed by the PRP (irrespective of whether it holds or not); a modest and con-servative theoretical improvement over the original model which always does.
In this section, we introduce two truncated normal-exponential models, with support restricted to [ s min ,s max ] . The two models differ in the way the out-truncated score ranges are handled. In Sec-tion 6 we provide appropriate estimation methods for these models.
In order to enforce support compatibility, we introduce a left-truncated at s min normal distribution for P ( s | 1) . With this modifi-cation, we reach a new mixture model for score distributions with a semi-infinite support in [ s min , +  X  ) , s min  X  R . In practice, scores may be naturally bounded (by the retrieval model) or truncated to the upside as well. For example, cosine similarity scores are natu-rally bounded at 1. Scores from probabilistic models with a (theo-retical) support in (  X  X  X  , +  X  ) are usually mapped to the bounded (0 , 1) via a logistic function, or by normalizing with the per-query score range. Other retrieval models may just truncate at some maxi-mum number for practical reasons. Consequently, it makes sense to introduce a right-truncation as well, for both the normal and expo-nential densities. Depending on how one wants to treat the leftovers due to the truncations, two new models may be considered.
There are no leftovers (Figure 2). The underlying theoretical densities are assumed to be the truncated ones, normalized accord-ingly to integrate to one: where of the above P ( s | 1) and P ( s | 0) are given by [9, 13, pp.156 X 162]:
The underlying theoretical densities are not truncated, but the truncation is of a  X  X echnical X  nature: the leftovers are accumulated at the two truncation points introducing discontinuities (Figure 3). For the normal, the leftovers can easily be calculated: where  X  ( . ) is Dirac X  X  delta function. For the exponential, while the leftovers at the right side are determined by the right truncation, calculating the ones at the left side requires to assume that the ex-ponential extends below s min to some new minimum score s P ( s | 0) = The cdfs corresponding to the above densities are: The equations in this section simplify somewhat when estimating their parameters from down-truncated ranked lists, as we will see in Section 6.1.
For both models the right truncation is optional. For s max +  X  , we get  X (  X  ) =  X ( s max  X  s 0 min ;  X  ) = 1 , leading to left-truncated models; this accommodates retrieval models with scoring support in [ s min , +  X  ) , s min  X  R . This is the maximum range that can be achieved with the current mixture, since the restriction of a finite s min is imposed by the use of an exponential.
 When s min  X  s max then  X (  X  )  X  0 and  X (  X  )  X  1 .
 If additionally s 0 min = s min , then  X ( s min  X  s 0 min  X ( s max  X  s 0 min ;  X  )  X  1 . Thus we can well-approximate the stan-dard normal-exponential model. Consequently, using a truncated model is a valid choice even when truncations are insignificant.
From a theoretical point of view, it may be difficult to imagine a process producing a truncated normal directly . Truncated normal distributions are usually the result of censoring, meaning that the out-truncated data do actually exist. In this view, the technically truncated model may correspond better to the IR reality. This is also in line with the theoretical arguments for the existence of a full normal distribution [3].

Concerning convexity, both truncated models do not always vi-olate such conditions. Consider the problem at the top score range ( s , +  X  ) . In the cases of s c  X  s max , the problem is out-truncated in both models, while X  X n theory X  X t still always exists in the orig-inal model. The improvement so far is of a theoretical nature. In practise, we should be interested in what happens when s c As we will later see in the experiment X  X  section, truncation helps estimation in producing higher numbers of convex fits within the observed score range. Consequently, the benefits are also practical.
These improvements make the original model more general, and it indeed produces better fits on our data. In fact, the truncated distributions should have been used in the past during parameter estimation even for the original normal-exponential model due to down-truncated rankings.
In this section, we will develop parameter estimation methods for the truncated models. The normal-exponential mixture has worked best under the availability of some relevance judgments which serve as an indication about the form of the component densities [4, 7, 20]. In filtering or classification, usually some training data X  X l-though often biased X  X re available. In the current task, however, no relevance information is available.

A method was introduced in the context of fusion which recovers the component densities without any relevance judgments using the Expectation Maximization (EM) algorithm [11]. In order to deal with the biased training data in filtering, the EM method was also later adapted and applied for thresholding tasks [1]. 1 Nevertheless, EM was found to be  X  X essy X  and sensitive to its initial parameter settings [1, 11].
For practical reasons, rankings are usually truncated at some rank t &lt; n . Even what is usually considered a full ranking is in fact a collection X  X  subset of those documents with at least one matching term with the query. This fact has been largely ignored by all pre-vious research using the standard model, despite that it may affect greatly the estimation. Also, considering that the exponential may
Another method for producing unbiased estimators in filtering can be found in [20], but it requires relevance judgements. not be a good model for the whole distribution of the non-relevant scores but only for their high end, some imposed truncation may help achieve better fits. Consequently, all estimations should take place at the top of the ranking, and then get extrapolated to the whole collection. Let us see how the formulas change.

Let us assume that the truncation score is s t . For both truncated models, we need to estimate a two-side truncated normal at s s max , and a shifted exponential by s t right-truncated at s s max possibly be +  X  , from a set of top-t scores. The formulas that should be used are Equations 13 and 14 but for  X  t instead of  X  : and for s t instead of s min . Beyond this, the models differ in the way R is calculated. If G t is the fraction of relevant documents in the truncated ranking, extrapolating the truncated normal outside its estimation range and appropriately per model in order to account for the remaining relevant documents, R is calculated as: Consequently, Equation 1 must be replaced by one of the above depending on the model, Equations 2 and 3 must be re-written as while Equations 4 and 5 remain the same. F ( s | 1) and F ( s | 0) are now the cdfs either of Section 5.1 or 5.2, depending on the model.
For the choice of the technically truncated model, if there are any scores equal to s max or s min they should be removed from the data-set; these belong to the discontinuous legs of the densities given in Section 5.2. In this case, t should be decremented accordingly.
EM is an iterative procedure which converges locally [8]. Find-ing a global fit depends on the initial settings of the parameters.
For t  X  n observed scores s 1 ,...s t with neither truncated nor shifted normal and exponential densities (i.e. the original model), the update equations are P (1) = G t , P (0) = 1  X  G t , and P ( s ) by Equation 12 (for G instead of G n ).
In practise, while scores equal to s min should not exist in the top-t due to the down-truncation, some s max scores may very well be in the data. Removing these during estimation is a simplifing approx-imation with an insignificant impact when the relevant documents are many and the bulk of their score distribution is below s as it is the case in our experimental setup. As we will see next, while we do not use the s max scores during fitting, we take them into account during goodness-of-fit testing; using multiple such fit-ting/testing rounds, the impact of the approximation is reduced.
We initialize the equations as it will be described in Section 6.2.3, and iterate them until the absolute differences between the old and new values for  X  ,  X   X  1 , and and | G t, new  X  G t, old | &lt; . 001 . Like this we target an accuracy of 0.1% for scores and 1 in a 1,000 for documents. We also tried a tar-get accuracy of 0.5% and 5 in 1,000, but it did not seem sufficient.
If we use the truncated densities (Equations 13 and 14) in the above update equations, the  X  new and  X  2 new calculated at each it-eration would be the expected value and variance of the truncated normal, not the  X  and  X  2 we are looking for. Similarly, 1 / X  would be equal to the expected value of the shifted truncated expo-nential. Instead of looking for new EM equations, we correct to the right values using simple approximations.

Using Equation 21 in the Appendix, at the end of each iteration we correct the calculated  X  new as using the  X  old from the previous iteration as an approximation. Similarly, based on Equations 19 and 20 in the Appendix, we cor-rect the calculated  X  new and  X  2 new as where again using the values from the previous iteration.

These simple approximations work, but sometimes they seem to increase the number of iterations needed for convergence, de-pending on the accuracy targeted. Generally, convergence hap-pens in 10 to 50 iterations depending on the number of scores (more data, slower convergence), and even with the approxima-tion EM produces considerably better fits than when using the non-truncated densities. We cap the number of iterations to 100. The end-differences we have seen between the observed and expected numbers of documents due to these approximations have always been less than 4 in 100,000.
We tried numerous initial settings, but no setting seemed univer-sal. While some settings helped a lot some fits, they had a negative impact on others. Without any indication of the form, location, and weighting of the component densities, the best fits overall were ob-tained for randomized initial values, preserving also the generality of the approach. The randomized approach worked well because we initialize and run EM multiple (up to 100) times, and select the fit with the least  X  2 with the observed score data.

Although randomizing the parameters in their whole possible ranges works well, we used an improved initialization by random-izing into more probable narrower ranges motivated by IR consid-erations. This improves efficiency by reducing the number of EM iterations and runs. Furthermore,  X  2 values largely depend on how the observed scores are binned. Due to length limitations, we do not expand here on these issues but refer the reader to [2]. Table 1: Ranking quality for the Legal 2007 and 2008. The high-
In this section, we apply the new models on the experimental setup based on the TREC 2007 and 2008 Legal Tracks. We discuss the underlying retrieval runs, and analyse the fits resulting from the old and new models. Then, we look at the effectiveness of the mod-els in selecting a rank cut-off value K (per topic) for optimizing the given F 1 -measure. 3 Note that the resulting F 1 @ K is as much as a result of the quality of the underlying ranking as of the choice of the cut-off. Since our focus is the thresholding problem, we use an off-the-shelf retrieval system: the vector-space model of Apache X  X  L For TREC Legal 2007 and 2008 we created the following runs: Legal07 Off-the-shelf L UCENE using the RequestText as query, Legal08 Same run as above, using the RequestText as query.
We first discuss the overall quality of the rankings. The top half of Table 1 shows several measures on the two underlying rankings, Legal07 and Legal08 . We show precision at 5 (all top-5 results were judged by TREC); estimated recall at B (i.e. the size of the returned set of the reference Boolean run); and the F 1 of the esti-mated precision and recall at R (i.e. the estimated number of rele-vant documents).

To determine the quality of our rankings in comparison to other systems, we show the highest, lowest, and median performance of all 2008 submissions in the bottom half of Table 1. 4 As it turns out, Legal08 obtains exactly the median performance for Recall @ B and F 1 @ R , and fares somewhat better than the median at Prec @5 . It is clear that our rankings are far from optimal in comparison with the other submissions. On the negative side, this limits the performance of the s-d methods. On the positive side, our Legal08 ranking is a good representative of the participating systems.
We fit the two new models, as well as the old model, by esti-mating the appropriate parameters using EM as discussed above. Table 2 provides some data on the convexity of the resulting fits. We look at the number of topics where the fit (as measured by the  X  2 with the observed score data) improves over the non-truncated approach, and see that the fit improves for 80% of the topics. We also investigate the number of fits presenting the non-convexity anomaly within the observed score range, i.e. at a rank below rank 1 ( k c &gt; 1 ). We see that the anomaly shows up in a large number of topics; 53-66% for the truncated models, but in almost all topics,
More information about the collection, topics, and evaluation measures can be found in [12].
We include these for 2008 to be able to compare to the threshold-ing task later (for which is there is no comparable data from 2007). 92-95%, for the original non-truncated model. Thus, beyond the theoretical improvement of the truncated models not always vio-lating convexity, truncation also helps in practice during parameter estimation resulting in a higher fraction of convex fits.
What is the impact of non-convexity on thresholding? By ran-domizing the affected ranks rather than re-ranking, the net effect is that the s-d method turns  X  X lind X  at rank numbers &lt; k c the estimated optimal thresholds with K  X  k c . However, the me-dian rank number e k c down to which the problem exists is very low compared to the median estimated number of relevant documents e R (7,484 for 2007 and 32,233 for 2008). Since K &lt; k c is unlikely, thresholding quality should not be affected X  X n average. Never-theless, for a small number of topics (2-10%), the problem appears for k c &gt; e R and non-convexity should have a more significant im-pact. For a good fraction of such topics, a large k c indicates a fitting problem rather than a theoretical one. Figure 4 illustrates this: the Figure 4: For topic 91 (top plot), the fit looks good but has a convexity Table 3: Estimating cut-off K for the Legal 2007 &amp; 2008. The highest, resulting fit (top plot) looks good but has a convexity problem in the whole ranking ( k c  X  25 , 000 ), indicated by having to flatten its estimated precision in the whole range. The fit regards all highest scoring documents as non-relevant, and it could have been rejected on IR grounds: for example, by requiring the expected relevant score to be larger than the expected non-relevant. The next best fit (bottom plot) has no convexity problem. Overall, our data suggest that non-convexity has an insignificant impact on s-d thresholding.
For the threshold optimization we simply use the fitted mixture of normal and exponential, and calculate the rank that maximizes the F 1 measure. Note that a fit may indicate an optimal rank thresh-old beyond the run X  X  length (25k in 2007 and 100k in 2008), in which case we simply select the final rank. We have three runs corresponding to the use of truncation: none Runs using the original non-truncated s-d model. theoretical Runs using the theoretical truncation of Section 5.1. technical Runs using the technical truncation of Section 5.2. Table 3 shows the results for the various thresholding methods. All runs with the truncated s-d models lead to significantly better results than the old s-d model. For 2007, the theoretically truncated model scores better than the technically truncated model. For 2008, the technically truncated model gets a somewhat better score than the theoretically truncated model. Hence, it is not clear which of the truncation models is superior X  X he differences are not significant.
We also show the highest, lowest, and median performance over the 23 submissions to TREC Legal 2008 (the thresholding task is new at TREC 2008, so there are no comparable data for 2007). Note again that the actual value of F 1 @ K is a result of both the quality of the underlying ranking and choosing the right threshold. As seen earlier, our ranking has the median Recall @ B and F With the estimated thresholds of the s-d model, the F 1 @ K is 0.136, well above the median of 0.0974. There is still room for improve-ment. Although this comparison is unrealistic X  X he mean estimated number of relevant items is generally not known X  X e achieve up to 80% of the F 1 @ R of Table 1.
We studied the problem of finding an optimal point to stop read-ing a ranked list, by selecting thresholds that optimize a given mea-sure. Assuming no other input than a system X  X  output for a query X  document scores and their distribution X  X he task is essentially a score-distributional threshold optimization problem. The recent trend in modeling score distributions is to use a normal-exponential mixture: normal for relevant, and exponential for non-relevant doc-ument scores. We discussed the two main theoretical problems with the current model X  X upport incompatibility and non-convexity X  and developed new models that address them.

The main contributions of the paper are two truncated normal-exponential models, varying in the way the out-truncated score ranges are handled. The theoretical truncation assumes that no data exist outside the truncated score range; the technical truncation as-sumes that the  X  X issing X  data are accumulated at the two truncation points. We showed that truncation improves the goodness-of-fit for most topics and reduces non-convexity problems at the top of rank-ings, although the problem remains for a considerable fraction of topics. Our analysis revealed that some of the extreme cases can be attributed to fitting problems rather than problems of the underly-ing ranking or with the normal-exponential mixture, and suggested that such fits can be rejected on IR grounds (e.g. by requiring that the expected relevant score is larger than the expected non-rele-vant score). We also showed that for the overwhelming majority of the remaining topics non-convexity occurs at early ranks, where it has an insignificant impact on the s-d thresholding given the large numbers of relevant documents in the setup. This is confirmed in a range of experiments using the TREC 2007 and 2008 Legal Track data, where we showed that the truncated models lead to signifi-cantly better performance over the standard model.

Assuming that the normal-exponential mixture is a good approx-imation for score distributions and that no relevance information is available, we believe that the improved methods described in this paper a) are as general as possible, b) deal with most known theoretical anomalies and practical difficulties, and consequently, c) bring us closer to the performance ceiling of s-d thresholding. Further improvements of s-d thresholding should come from using training data or alternative mixtures. Although we focused on the normal-exponential mixture, truncated models can also be defined for other distributions. Since all retrieval runs tend to be truncated for practical reasons, truncation is an important factor for fitting any distribution. For completeness, we give here the formulas not given throughout the paper, and the derivations of those not found in the literature.
The moments of a truncated normal can be found in the liter-ature [9]. Let S be a normally-distributed random variable with mean  X  and variance  X  2 , left-truncated at s min and right-truncated at s max . Its expected value is We do not us the  X  sign at the upper limit of S here (and in the equations below) to denote that the right-truncation is an option (i.e. s max can be +  X  ) in the context of this paper. For the variance:
V( S | s min  X  S &lt; s max )
Concerning the expectation of a shifted truncated exponential, we have not found the formula in the literature. Let S be an expo-nentially-distributed random variable with rate parameter  X  , which we shift by s min and right-truncate at s max . From the definition of the expected value of a truncated distribution and Equation 9: E( S | s min  X  S &lt; s max ) = where the shift of the exponential by s min is already taken into account. From lists of integrals of exponential functions: Putting these equations together and working out the calculation: For only shift but no truncation (i.e. s min 6 = 0 and s max  X  ( s max  X  s min ;  X  ) = 0 and  X ( s max  X  s min ;  X  ) = 1 . Equation 21 becomes which without a shift ( s min = 0 ) becomes E( S ) = 1 / X  , as ex-pected [13].
