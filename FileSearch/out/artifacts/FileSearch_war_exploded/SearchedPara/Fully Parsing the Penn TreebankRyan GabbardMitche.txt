 The trees in the Penn Treebank (Bies et al., 1995) are annotated with a great deal of information to make various aspects of the predicate-argument structure easy to decode, including both function tags and markers of  X  X mpty X  categories that represent dis-placed constituents. Modern statistical parsers such as (Collins, 2003) and (Charniak, 2000) however ig-nore much of this information and return only an impoverished version of the trees. While there has been some work in the last few years on enrich-ing the output of state-of-the-art parsers that output Penn Treebank-style trees with function tags (e.g. (Blaheta, 2003)) or empty categories (e.g. (Johnson, 2002; Dienes and Dubey, 2003a; Dienes and Dubey, 2003b), only one system currently available, the de-pendency graph parser of (Jijkoun and de Rijke, 2004), recovers some representation of both these aspects of the Treebank representation; its output, however, cannot be inverted to recover the original tree structures. We present here a parser, 1 the first we know of, that recovers full Penn Treebank-style trees. This parser uses a minimal modification of the Collins parser to recover function tags, and then uses this enriched output to achieve or better state-of-the-art performance on recovering empty categories.
We focus here on Treebank-style output for two reasons: First, annotators developing additional treebanks in new genres of English that conform to the Treebank II style book (Bies et al., 1995) must currently add these additional annotations by hand, a much more laborious process than correcting parser output (the currently used method for annotating the skeletal structure itself). Our new parser is now in use in a new Treebank annotation effort. Second, the accurate recovery of semantic structure from parser output requires establishing the equivalent of the in-formation encoded within these representations.
Our parser consists of two components. The first-stage is a modification of Bikel X  X  implementation (Bikel, 2004) of Collins X  Model 2 that recovers func-tion tags while parsing. Remarkably little modifica-tion to the parser is needed to allow it to produce function tags as part of its output, yet without de-creasing the regular Parseval metric. While it is dif-ficult to evaluate function tag assignment in isola-tion across the output of different parsers, our re-sults match or exceed all but the very best of earlier tagging results, even though this earlier work is far more complicated than ours. The second stage uses a cascade of statistical classifiers which recovers the most important empty categories in the Penn Tree-bank style. These classifiers utilize a wide range of features, including crucially the function tags recov-ered in the first stage of parsing. Function tags are used in the current Penn Treebanks to augment nonterminal labels for various syntactic and semantic roles (Bies et al., 1995). For example, in Figure 1, -SBJ indicates the subject, -TMP indi-cates that the NP last year is serving as a tem-poral modifier, and -LOC indicates that the PP is specifying a location. Note that without these tags, it is very difficult to determine which of the two NPs directly dominated by S is in fact the subject. There are twenty function tags in the Penn Treebank, and following (Blaheta, 2003), we collect them into the five groups shown in Figure 2. While nonterminals can be assigned tags from different groups, they do not receive more than one tag from within a group. The Syntactic and Semantic groups are by far the most common tags, together making up over 90% of the function tag instances in the Penn Treebank.
Certain non X  X ocal dependencies must also be in-cluded in a syntactic analysis if it is to be most use-ful for recovering the predicate X  X rgument structure of a complex sentence. For instance, in the sentence  X  X he dragon I am trying to slay is green, X  it is im-portant to know that I is the semantic subject and the dragon the semantic object of the slaying. The Penn Treebank (Bies et al., 1995) represents such dependencies by including nodes with no overt con-tent (empty categories) in parse trees. In this work, we consider the three most frequent 2 and semanti-cally important types of empty category annotations in most Treebank genres:
Null complementizers are denoted by the sym-bol 0 . They typically appear in places where, for example, an optional that or who is missing:  X  X he king said 0 he could go. X  or  X  X he man (0) I saw. X 
Traces of wh  X  X ovement are denoted by *T* , such as the noun phrase trace in  X  X hat 1 do you want (NP *T*-1) ? X  Note that wh  X  X races are co X  indexed with their antecedents. (NP *) s are used for several purposes in the Penn Treebank. Among the most common are pas-sivization  X  (NP-1 I ) was captured (NP *-1) , X  and control  X  (NP-1 I ) tried (NP *-1) to get the best results. X 
Under this representation the above sentence would look like  X (NP-1 The dragon) 0 (NP-2 I) am trying (NP *-2) to slay (NP *T*-1) is green. X 
Despite their importance, these annotations have largely been ignored in statistical parsing work. The importance of returning this information for most real applications of parsing has been greatly ob-scured by the Parseval metric (Black et al., 1991), which explicitly ignores both function tags and null elements. Because much statistical parsing research has been driven until recently by this metric, which has never been updated, the crucial role of parsing in recovering semantic structure has been generally ignored. An early exception to this was (Collins, 1997) itself, where Model 2 used function tags dur-ing the training process for heuristics to identify ar-guments (e.g., the TMP tag on the NP in Figure 1 disqualifies the NP-TMP from being treated as an argument). However, after this use, the tags are ig-nored, not included in the models, and absent from the parser output. Collins X  Model 3 attempts to re-cover traces of W h-movement, with limited success. Our system for restoring function tags is a modifica-tion of Collins X  Model 2. We use the (Bikel, 2004) Syntactic (55.9%) Semantic (36.4%) Misc (1.0%) CLR (5.8%) DTV Dative NOM Nominal EXT Extent CLF It-cleft CLR Closely-PRD Predicate Adverbial MNR Manner TTL Title PUT LOC of  X  X ut X  BNF Benefactive PRP Purpose Topicalization SBJ Subject DIR Direction TMP Temporal (2.6%) VOC Vocative TPC Topic emulation of the Collins parser. 3 Remarkably little modification to the parser is needed to allow it to produce function tags as part its output, without de-creasing the regular Parseval metric.

The training process for the unmodified Collins parser carries out various preprocessing steps, which modify the trees in various ways before taking ob-servations from them for the model. One of these steps is to identify and mark arguments with a parser internal tag ( -A ), using function tags as part of the heuristics for doing so. A following preprocessing step then deletes the original function tags.
We modify the Collins parser in a very simple way: the parser now retains the function tags af-ter using them for argument identification, and so includes them in all the parameter classes. We also augment the argument identification heuristic to treat any nonterminal with any of the tags in the Syntactic group to be an argument; these are treated as synonyms for the internal tag that the parser uses to mark arguments. This therefore extends (Collins, 2003) X  X  use of function tags for excluding potential argument to also use them for including arguments. 4 The parser is then trained as before. We compare our tagging results in isolation with the tagging systems of (Blaheta, 2003), since that work has the first highly detailed accounting of function tag results on the Penn Treebank, and with two re-cent tagging systems. We use both Blaheta X  X  metric and his function tag groupings, shown in Figure 2, although our assignments are made by a fully inte-grated system. There are two aspects of Blaheta X  X  metric that require discussion: First, this metric in-cludes only constituents that were otherwise parsed correctly (ignoring function tag). Second, the metric ignores cases in which both the gold and test nonter-minals are lacking function tags, since they would inflate the results. We trained the Bikel emulations of Collins X  model 2 and our modified versions on sections 2-21 and tested on section 23. Scores are for all sentences, not just those with less than 40 words.

Parseval labelled recall/precision scores for the unmodified and modified parsers, show that there is almost no difference in the scores: We find this somewhat surprising, as we had ex-pected that sparse data problems would arise, due to the shattering of NP into NP-TMP , NP-SBJ , etc.
Table 1 shows the overall results and the break-down for the different function tag groups. For pur-poses of comparison, we have calculated our over-all score both with and without CLR . 5 The (Blaheta, 2003) numbers in parentheses in Table 1 are from his feature trees specialized for the Syntactic and Se-mantic groups, while all his other numbers, includ-ing the overall score, are from using a single feature set for his four function tag groups. 6
Even though our tagging system results from only eliminating a few lines of code from the Collins parse, it has a higher overall score than (Blaheta, 2003), and a large increase over Blaheta X  X  non-specialized Semantic score (79.81). It also out-performs even Blaheta X  X  specialized Semantic score (83.37), and is very close to Blaheta X  X  specialized score for the Syntactic group (95.89). However, since the evaluation is over a different set of non-terminals, arising from the different parsers, 7 it is difficult to draw conclusions as to which system is definitively  X  X etter X . It does seem clear, though, that by integrating the function tags into the lexi-calized parser, the results are roughly comparable with the post-processing work, and it is much sim-pler, without the need for a separate post-processing level or for specialized feature trees for the different tag groups. 8
Our results clarify, we believe, the recent results of (Musillo and Merlo, 2005), now state-of-the-art, which extends the parser of report a significant modification of the Henderson parser to incorporate strong notions of linguistic lo-cality. They also manually restructure some of the function tags using tree transformations, and then train on these relabelled trees. Our results indicate that perhaps the simplest possible modification of an existing parser suffices to perform better than post-processing approaches. The linguistic sophistication of the work of (Musillo and Merlo, 2005) then pro-vides an added boost in performance over simple in-tegration. Most learning X  X ased, phrase X  X tructure X  X ased (PSLB) work 9 on recovering empty categories has fallen into two classes: those which integrate empty category recovery into the parser (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b) and those which recover empty categories from parser output in a post X  X rocessing step (Johnson, 2002; Levy and Manning, 2004). Levy and Manning note that thus far no PSLB post X  X rocessing approach has come close to matching the integrated approach on the most numerous types of empty categories.

However, there is a rule X  X ased post X  X rocessing approach consisting of a set of entirely hand X  designed rules (Campbell, 2004) which has better results than the integrated approach. Campbell X  X  rules make heavy use of aspects of linguistic repre-sentation unexploited by PSLB post X  X rocessing ap-proaches, most importantly function tags and argu-ment annotation. 10 7.1 Runtime The algorithm applies a series five maximum X  entropy and two perceptron X  X ased classifiers: [1] For each PP , VP , and S node, ask the classifier NPT RACE to determine whether to insert an (NP *) as the object of a preposition, an argument of a verb, or the subject of a clause, respectively. [2] For each node , ask N ULL C OMP to determine whether or not to insert a 0 to the right. [3] For each S node , ask WHXPI NSERT to de-termine whether or not to insert a null wh  X  X ord to the left. If one should be, ask WHXPD ISCERN to decide if it should be a (WHNP 0) or a (WHADVP 0) . [4] For each S which is a sister of WHNP or WHADVP , consider all possible places beneath it a wh  X  X race could be placed. Score each of them using WHT RACE , and insert a trace in the highest scoring position. [5] For any S lacking a subject, insert (NP *) . [6] For each (NP *) in subject position, look at all NP s which c X  X ommand it. Score each of these us-ing PROA NTECEDENT , and co X  X ndex the (NP *) with the NP with the highest score. For all (NP *) s in non X  X ubject positions, we follow Campbell in as-signing the local subject as the controller. [7] For each (NP *) , ask A NTECEDENTLESS to determine whether or not to remove the co X  X ndexing between it and its antecedent.

The sequencing of classifiers and choice of how to frame the classification decisions closely follows Campbell with the exception of finding antecedents of (NP *) s and inserting wh  X  X races, which follow Levy and Manning in using a competition X  X ased ap-proach. We differ from Levy and Manning in using a perceptron X  X ased approach for these, rather than a maximum X  X ntropy one. Also, rather than introduc-ing an extra zero node for uncontrolled (NP *) s, we always assign a controller and then remove co X  indexing from uncontrolled (NP *) s using a sepa-rate classifier. 7.2 Training Each of the maximum X  X ntropy classifiers men-tioned above was trained using M ALLET (McCal-lum, 2002) over a common feature set. The most notable departure of this feature list from previous ones is in the use of function tags and argument markings, which were previously ignored for the un-derstandable reason that though they are present in the Penn Treebank, parsers generally do not produce them. Another somewhat unusual feature examined right and left sisters.

The PROA NTECEDENT perceptron classifier uses the local features of the controller and the con-trolled (NP *) , whether the controller precedes or follows the controlled (NP *) , the sequence of cat-egories on the path between the two (with the  X  X urn-ing X  category marked), the length of that path, and which categories are contained anywhere along the path.

The WHT RACE perceptron classifier uses the fol-lowing features each conjoined with the type of wh  X  trace being sought: the sequence of categories found on the path between the trace and its antecedent, the path length, which categories are contained any-where along the path, the number of bounding cat-egories crossed and whether the trace placement vi-olates subjacency, whether or not the trace insertion site X  X  parent is the first verb on the path, whether or not the insertion site X  X  parent contains another verb beneath it, and if the insertion site X  X  parent is a verb, whether or not the verb is saturated. 11
All maximum X  X ntropy classifiers were trained on sections 2-21 of the Penn Treebank X  X  Wall Street Journal section; the perceptron X  X ased classifiers were trained on sections 10-18. Section 24 was used for development testing while choosing the feature set and other aspects of the system, and section 23 was used for the final evaluation. 8.1 Metrics For the sake of easy comparison, we report our re-sults using the most widely X  X sed metric for perfor-mance on this task, that proposed by Johnson. This metric judges an entity correct if it matches the gold standard in type and string position (and, if there is an antecedent, in its label and string extent). Be-cause Campell reports results by category using only his own metric, we use this metric to compare our results to his. There is much discussion in the litera-ture of metrics for this task; Levy and Manning and Campbell both note that the Johnson metric fails to catch when an empty category has a correct string position but incorrect parse tree attachment. While we do not have space to discuss this issue here, the metrics they in turn propose also have significant weaknesses. In any event, we use the metrics that allow the most widespread comparison. 8.2 Comparison to Other PSLB Methods Category Pres LM J DD Comb. 0 87.8 87.0 77.1 COMP-WHADVP 69.0 NP * 69.1 61.1 55.6 70.3 Comb. wh  X  X race 78.2 63.3 75.2 75.3 Table 2: F1 scores comparing our system to the two PSLB post X  X rocessing systems and Dienes and Dubey X  X  integrated system on automatically parsed trees from section 23 using Johnson X  X  metric.
F1 scores on parsed sentences from section 23 are given in table 2. Note that our system X  X  parsed scores were obtained using our modified version of Bikel X  X  implementation of Collins X  X  the-sis parser which assigns function tags, while the other PSLB post X  X rocessing systems use Charniak X  X  parser (Charniak, 2000) and Dienes and Dubey inte-grate empty category recovery directly into a variant of Collins X  X  parser.
 On parsed trees, our system outperforms other PSLB post X  X rocessing systems. On the most numer-ous category by far, (NP *) , our system reduces the error of the best PSLB post X  X rocessing approach by 21%. Comparing our aggregate wh  X  X race results to the others, 12 we reduce error by 41% over Levy and Manning and by 12% over Johnson.
 Table 3: Comparison of our system with that of Di-enes and Dubey on parsed data from section 23 over the aggregation of all categories in table 2 except-ing the infrequent (WHADVP 0) s, which they do not report but which we almost certainly outperform them on.

Performance on parsed data compared to the inte-grated system of Dienes and Dubey is split. We re-duce error by 25% and 44% on plain 0 s and (WHNP 0) s, respectively and by 12% on wh  X  X races. We increase error by 4% on (NP *) s. Aggregating over all the categories under consideration, the more balanced precision and recall of our system puts it ahead of Dienes and Dubey X  X , with a 6.4% decrease in error (table 3). 8.3 Comparison to Campbell Table 4: A comparison of the present system with Campbell X  X  rule X  X ased system on gold X  X tandard trees from section 23 using Campbell X  X  metric
On gold-standard trees, 13 our system out-performs Campbell X  X  rule X  X ased system on all four categories, reducing error by 87% on 0 s, 14 by 11% on (ADVP *T*) s, by 7% on (NP *T*) s, and by 8% on the extremely numerous (NP *) s. We have shown that a PSLB post X  X rocessing ap-proach can outperform the state X  X f X  X he X  X rt inte-grated approach of Dienes and Dubey. 15 Given that their modifications to Collins X  X  parser caused a de-crease in local phrase structure parsing accuracy due to sparse data difficulties (Dienes and Dubey, 2003a), our post X  X rocessing approach seems to be an especially attractive choice. We have further shown that our PSLB approach, using only sim-ple, unconjoined features, outperforms Campbell X  X  state X  X f X  X he X  X rt, complex system on gold X  X tandard data, suggesting that much of the power of his sys-tem lies in his richer linguistic representation and his structuring of decisions rather than the hand X  designed rules.

We have also compared our system to that of Levy and Manning which is based on a similar learning technique and have shown large increases in perfor-mance on all of the most common types of empty categories; this increase seems to have come al-most entirely from an enrichment of the linguistic representation and a slightly different structuring of the problem, rather than any use of more powerful machine X  X earning techniques
We speculate that the primary source of our per-formance increase is the enrichment of the linguis-tic representation with function tags and argument markings from the parser X  X  first stage, as table 5 at-tests. We also note that several classifiers make use of the properties of aunt nodes, which have previ-ously been exploited only in a limite form in John-son X  X  patterns. For example, A NTECEDENTLESS uses the aunt X  X  head word to learn an entire class of uncontrolled PRO constructions like  X  X t is difficult (NP *) to imagine living on Mars. X  This work has presented a two stage parser that re-covers Penn Treebank style syntactic analyses of new sentences including skeletal syntactic structure, and, for the first time, both function tags and empty categories. The accuracy of the first-stage parser on the standard Parseval metric matches that of the (Collins, 2003) parser on which it is based, despite the data fragmentation caused by the greatly en-riched space of possible node labels for the Collins statistical model. This first stage simultaneously achieves near state-of-the-art performance on recov-ering function tags with minimal modifications to the underlying parser, modifying less than ten lines of code. We speculate that this success is due to the lexicalization of the Collins model, combined with the sophisticated backoff structure already built into the Collins model. The second stage achieves state-of-the-art performance on the recovery of empty cat-egories by combining the linguistically-informed ar-chitecture of (Campbell, 2004) and a rich feature set with the power of modern machine learning meth-ods. This work provides an example of how small enrichments in linguistic representation and changes in the structure of the problem having significant effects on the performance of a machine X  X earning X  based system. More concretely, we showed for the first time that a PSLB post X  X rocessing system can outperform the state X  X f X  X he X  X rt for both rule X  X ased post X  X rocessing and integrated approaches to the empty category restoration problem.

Most importantly from the point of view of the authors, we have constructed a system that recov-ers sufficiently rich syntactic structure based on the Penn Treebank to provide rich syntactic guidance for the recovery of predicate-argument structure in the near future. We also expect that productivity of syn-tactic annotation of further genres of English will be significantly enhanced by the use of this new tool, and hope to have practical evidence of this in the near future.

