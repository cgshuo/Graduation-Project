 typical rendition of this sentiment is the article titled  X  X pplying 
Neural Computing to Target Marketing X  by Zahavi and Levin [ 193. The abstract summarizes the results:  X  X t is shown that, at least for the data used in this study the fit achieved for both methods [logistic regression and neural networks] is approximately the same, but the process of configuring and setting up a neural network for a database marketing application is not straightforward and may require extensive experimentation and computer resources. The results are therefore not encouraging for the neural net approach. X  Poorly specified neural networks, inefficiently fitted to noisy data, are a waste of effort. However, in the domains where the signal is difficult to extract from the noise, such as database marketing, small improvements of predictive power often have great value. 
Three practical difficulties with applying neural networks in predictive data mining are inscrutability, model selection, and troublesome training. Multilayer perceptrons are usually considered black boxes with respect to interpretation. The effects of a particular input on the target can depend in complicated ways on the values of the other inputs. In some pattern recognition applications, such as handwriting recognition, pure prediction is the goal; understanding how the inputs affect the prediction is immaterial. In many scientific applications, the opposite is true. Understanding is the goal, and predictive power is a consideration only to the extent that it validates the interpretive power of the model. This is the domain for formal statistical inference such as hypothesis testing and confidence intervals. Domains such as database marketing often have both goals. Scoring new cases is the ultimate purpose of predictive modeling. However, some understanding, even informal, of the factors affecting the prediction can be helpful in determining how to market to segments of likely responders. Understanding the effects of the inputs can also be useful for decisions about costly data acquisitions. In credit scoring, the opaqueness of the model can have legal ramifications. The US Equal Credit Opportunity Act requires creditors to provide a statement of specific reasons why an adverse action was taken. The regulation considers the statements that the applicant failed to achieve the qualifying score on the creditor X  X  scoring system to be insufficient. The second practical difficulty with neural networks is the enormous number of configurations from which to choose. Trial and error is the most reliable method for determining the best number of layers, number of units, number of inputs, type of activation functions, type of connections, etc. The third practical difficulty is the computational effort required to optimize the large number of parameters in a typical neural network model. This is partially self-inflicted. Data analysts often use inefficient optimization methods such as backpropagation with neural networks. Newton-type methods [5] are usually more robust and efficient. Even with an efficient algorithm, local minima are troublesome. Different starting values can converge to different (faulty) solutions. Often the best remedy is multiple runs from different random starting values. 
The first GANN model had 33 parameters (4 for each of the eight inputs, plus an output bias). It was fitted using the Levenburg-
Marquardt optimization method [5]. Convergence was attained in 71 iterations. The analysis was conducted using the SAS@ procedure NEURAL from the Enterprise MinerTM. Computational details are given in [ 141. 
The empirical partial residual plots (figure 1) show poor tits for several of the inputs. The distributions of the inputs DDATOT, 
ADBDDA, DDADEP, and SAVBAL are highly skewed. The fitted values appear to be overly sensitive to the few large values in the tails of the distributions. Power transformations were applied to DDATOT, ADBDDA, 
DDADEP, and SAVBAL to encourage the GANN to learn the variation in the center of the distributions. The cube-root transformation was chosen because of the positive skewness and the presence of many zero values. The choice of transformation is not as crucial here as it is with linear models. The neural network can accommodate nonlinearity. The purpose of the transformation is merely to reduce data sparsity. In addition to the transformations, the neuron for ATMCT was pruned because the partial residuals showed a linear trend. The second GANN model had 30 parameters (4 for seven of the inputs, 1 for ATMCT, and an output bias). It was fitted using the Levenburg-Marquardt optimization method [5]. Convergence was attained in 39 iterations. The partial residual plots (figure 2) show adequate fits for all the inputs. To aid interpretation, the inputs can be de-transformed to the original scale and the partial residuals can be converted from the logit scale to the probability scale. However, even on the transformed scale, the partial residual plots give an informal profile of likely responders. DDATOT and DDADEP have opposite effects. For small and moderate checking accounts, greater response is associated with a greater amount deposited and less withdrawn. This is supported by the negative trend in ATMCT. Increasing savings (SAVBAL) is associated with greater response except for the largest accounts. The wealthier customers appear to behave differently. This is supported by the effects of INCOME and INVEST, which increase at a decreasing rate. To assess the predictive power of the model the validation data was scored. The area under the receiver operating characteristic (ROC) curve [7] was used as a performance measure. In database marketing the ultimate goal is usually classification, not function estimation. A cutoff on the predicted probability is used to determine which cases receive an offer. The ROC area measures the discriminatory power of the classifier over a range of cutoffs. It can be interpreted as the probability that an actual responder has a greater posterior probability than a nonresponder. X  The ROC curve is not distorted by the separate sampling design (i.e. 50% responders in the sample versus 4% in the population). The first GANN (33 parameters) had a ROC area on. the validation data of .919. The transformations and pruning increased the ROC area to .928. For comparison, several other neural network models were fitted to this data. The 9-parameter linear logistic regression model had a ROC area of .912. To determine the best MLP architecture, 33% of the training data was held out for model selection. Five runs, from different starting values, were used for each network configuration. The best performing single-hidden-layer MLP had 2 neurons and 21 parameters. This network had a ROC area ,920. In addition, a larger 91-parameter (9 neuron) [lo] Hastie, T. J. and Tibshirani, R. J., Generalized Additive 
