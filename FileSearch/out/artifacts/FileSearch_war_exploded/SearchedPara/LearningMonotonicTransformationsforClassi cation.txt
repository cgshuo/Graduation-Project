 Man y elds have dev elop ed heuristic metho ds for prepro cessing data to impro ve perfor-mance. This often tak es the form of applying a monotonic transformation prior to using a classi cation algorithm. For example, when the bag of words represen tation is used in documen t classi cation, it is common to tak e the square root of the term frequency [6, 5]. Monotonic transforms are also used when classifying image histograms. In [3], transforma-tions of the form x a where 0 a 1 are demonstrated to impro ve performance. When classifying genes from various microarra y exp erimen ts it is common to tak e the logarithm of the gene expression ratio [2]. Monotonic transformations can also capture crucial prop erties of the data suc h as threshold and saturation e ects.
 In this pap er, we prop ose to sim ultaneously learn a hyperplane classi er and a monotonic transformation. The solution pro duced by our algorithm is a piecewise linear monotonic function and a maxim um margin hyperplane classi er similar to a supp ort vector mac hine (SVM) [4]. By allo wing for a richer class of transforms learned at training time (as opp osed to a rule of thum b applied during prepro cessing), we impro ve classi cation accuracy . The learned transform is speci cally tuned to the classi cation task. The main con tributions of this pap er include, a novel framew ork for estimating a monotonic transformation and a hyperplane classi er sim ultaneously at training time, an ecien t metho d for nding a Figure 1: Monotonic transform applied to eac h dimension follo wed by a hyperplane classi er. locally optimal solution to the problem, and a con vex relaxation to nd a globally optimal appro ximate solution.
 The pap er is organized as follo ws. In section 2, we presen t our form ulation for learning a piecewise linear monotonic function and a hyperplane. We sho w how to learn this com bined mo del through an iterativ e coordinate ascen t optimization using interlea ved quadratic and linear programs to nd a local minim um. In section 3, we deriv e a con vex relaxation based on Lasserre's metho d [8]. In section 4 syn thetic exp erimen ts as well as documen t and image classi cation problems demonstrate the div erse utilit y of our metho d. We conclude with a discussion and future work. that there is an unkno wn nuisance monotonic transformation ( x ) and unkno wn hyperplane exp ected test error R = R 1 transformation acts elemen twise as can be seen in Figure 1.
 We prop ose to learn both a maxim um margin hyperplane and the unkno wn transform ( x ) sim ultaneously . In our form ulation, ( x ) is a piecewise linear function that we parameterize z j 2 &lt; and m j 2 &lt; + . The transformation can be written as ( x ) = P ( x ) are truncated ramp functions acting on vectors and matrices elemen twise as follo ws: This is a less common way to parameterize piecewise linear functions. The positivit y con-strain ts enforce monotonicit y on ( x ) for all x . A more common metho d is to parameterize the function value ( z ) at eac h knot z and apply order constrain ts between subsequen t knots to enforce monotonicit y. Values in between knots are found through linear interp olation. This is the metho d used in isotonic regression [10], but in practice, these are equiv alen t form ulations. Using truncated ramp functions is preferable for numerous reasons. They can be easily precomputed and are sparse. Once precomputed, most calculations can be done via sparse matrix multiplications. The positivit y constrain ts on the weigh ts ~ m will also yield a simpler form ulation than order constrain ts and interp olation whic h becomes imp ortan t in subsequen t relaxation steps.
 Figure 2a sho ws the truncated ramp function asso ciated with knot z 1 . Figure 2b sho ws a conic com bination of truncated ramps that builds a piecewise linear monotonic function. Com bining this with the supp ort vector mac hine form ulation leads us to the follo wing learn-ing problem: where ~ are the standard SVM slac k variables, ~ w and b are the maxim um margin solution for the training set that has been transformed via ( x ) with learned weigh ts ~ m . Before training, the knot locations are chosen at the empirical quan tiles so that they are evenly spaced in the data.
 This problem is noncon vex due to the quadratic term involving ~ w and ~ m in the classi cation constrain ts. Although it is dicult to nd a globally optimal solution, the structure of the problem suggests a simple metho d for nding a locally optimal solution. We can divide the problem into two con vex subproblems. This amoun ts to solving a supp ort vector mac hine for ~ w and b with a xed ( x ) and alternativ ely solving for ( x ) as a linear program with the SVM solution xed. In both subproblems, we optimize over ~ as it is part of the hinge loss. This yields an ecien t con vergen t optimization metho d. However, this metho d can get stuc k in local minima. In practice, we initialize it with a linear ( x ) and iterate from there. Alternativ e initializations do not yield much help. This leads us to look for a metho d to ecien tly nd global solutions. When faced with a noncon vex quadratic problem, an increasingly popular technique is to relax it into a con vex one. Lasserre [8] prop osed a sequence of con vex relaxations for these types of noncon vex quadratic programs. This metho d replaces all quadratic terms in the original optimization problem with entries in a matrix. In its simplest form this matrix corresp onds to the outer pro duct of the the original variables with rank one and semide nite constrain ts. The relaxation comes from dropping the rank one constrain t on the outer pro duct matrix. Lasserre prop osed more elab orate relaxations using higher order momen ts of the variables. However, we mainly use the rst momen t relaxation along with a few of the second order momen t constrain ts that do not require any additional variables beyond the outer pro duct matrix.
 A con vex relaxation could be deriv ed directly from the primal form ulation of our problem. Both ~ w and ~ m would be relaxed as they interact in the noncon vex quadratic terms. Un-fortunately , this yields a semide nite constrain t that scales with both the num ber of knots and the dimensionalit y of the data. This is troublesome because we wish to work with high dimensional data suc h as a bag of words represen tation for text. However, if we rst nd the dual form ulation for ~ w , b , and ~ , we only have to relax ~ m whic h yields both a tigh ter relaxation and a less computationally intensiv e problem. Finding the dual leaves us with the follo wing min max saddle point problem that will be subsequen tly relaxed and transformed into a semide nite program: where ~ 1 is a vector of ones, ~y is a vector of the lab els, Y = diag ( ~y ) is a matrix with the lab els on its diagonal with zeros elsewhere, and X is a matrix with ~x i in the i th column. We introduce the relaxation via the substitution M = m m T and constrain t M 0 where m is constructed by concatenating 1 with ~ m . We can then transform the relaxed min max problem into a semide nite program similar to the multiple kernel learning framew ork [7] by nding the dual with resp ect to ~ and using the Schur complemen t lemma to generate a linear matrix inequalit y [1]: where ~ 0 is a vector of zeros and 1 is a vector with 1 in the rst dimension and ones in the rest. The variables , ~ , ~ arise from the dual transformation. This relaxation is exact if M is a rank one matrix.
 The above can be seen as a generalization of the multiple kernel learning framew ork. Instead of learning a kernel from a com bination of kernels, we are learning a com bination of inner pro ducts of di eren t functions applied to our data. In our case, these are truncated ramp functions. The terms i ( X ) T j ( X ) are not Mercer kernels except when i = j . This more general com bination requires the stricter constrain ts that the mixing weigh ts M form a positiv e semide nite matrix, a constrain t whic h is introduced via the relaxation. This is a sucien t condition for the resulting matrix P i;j M i;j i ( X ) T j ( X ) to also be positiv e semide nite.
 When using this relaxation, we can reco ver the monotonic transform by using the rst column (row) as the mixing weigh ts, ~ m , of the truncated ramp functions. In practice, 4.1 Syn thetic Exp erimen t In this exp erimen t we will demonstrate our metho d's abilit y to reco ver a monotonic trans-formation from data. We sampled data near a linear decision boundary and generated lab els based on this boundary . We then applied a strictly monotonic function to this sampled data. The training set is made up of the transformed points and the original lab els. A linear al-gorithm will have dicult y because the mapp ed data is not linearly separable. However, Figure 3: a) Original data. b) Data transformed by a logarithm. c) Data transformed by a quadratic function. d-f) The transformation functions learned using the noncon vex algorithm. g-i) The transformation functions learned using the con vex algorithm. if we could reco ver the inverse monotonic function, then a linear decision boundary would perform well.
 Figure 3a sho ws the original data and decision boundary . Figure 3b sho ws the data and hyperplane transformed with a normalized logarithm. Figure 3c depicts a quadratic trans-form. 600 data points were sampled, and then transformed. 200 were used for training, 200 for cross validation and 200 for testing. We compared our locally optimal metho d (L mono), our con vex relaxation (C mono) and a linear SVM (linear). The linear SVM struggled on all of the transformed data while the other metho ds performed well as rep orted in Figure 4. The learned transforms for L mono are plotted in Figure 3(d-f ). The solid blue line is the mean over 10 exp erimen ts, and the dashed blue is the standard deviation. The blac k line is the true target function. The learned functions for C mono are in Figure 3(g-i). Both algorithms performed quite well on the task of classi cation and reco ver nearly the exact monotonic transform. The local metho d outp erformed the relaxation sligh tly because this was an easy problem with few local minima. 4.2 Documen t Classi cation In this exp erimen t we used the four univ ersities WebKB dataset. The data is made up of web pages from four univ ersities plus an additional larger set from miscellaneous univ ersities. These web pages are then categorized. We will be working with the largest four categories: studen t, facult y, course, and pro ject. The task is to solv e all six pairwise classi cation problems. In [6, 5] prepro cessing the data with a square root was demonstrated to yield good results. We will compare our noncon vex metho d (L mono), and our con vex relaxation (C mono) to a linear SVM with and without the square root, with TFIDF features and also a kernelized SVM with both the polynomial kernel and the RBF kernel. We will follo w the setup of [6] by training on three univ ersities and the miscellaneous univ ersit y set and testing on web pages from the fourth univ ersit y. We rep eated this four fold exp erimen t ve times. For eac h fold, we use a subset of 200 points for training, 200 to cross validate the parameter settings, and all of the fourth univ ersit y's points for testing.
 Our two metho ds outp erform the comp etition on average as rep orted in Figure 5. The con vex relaxation chooses a step function nearly every time. This outputs a 1 if a word is in the training vector and 0 if it is absen t. The noncon vex greedy algorithm does not end up reco vering this solution as reliably and seems to get stuc k in local minima. This leads to sligh tly worse performance than the con vex version. 4.3 Image Histogram Classi cation In this exp erimen t, we used the Corel image dataset. In [3], it was sho wn that monotonic transforms of the form x a for 0 a 1 work ed well. The Corel image dataset is made up of various categories, eac h con taining 100 images. We chose four categories of animals: 1) eagles, 2) elephan ts, 3) horses, and 4) tigers. Images were transformed into RGB histograms follo wing the binning strategy of [3, 5]. We ran a series of six pairwise exp erimen ts where the data was randomly split into 80 percen t training, 10 percen t cross validation, and 10 percen t testing. These six exp erimen ts were rep eated 10 times. We compared our two metho ds to a linear supp ort vector mac hine, as well as an SVM with RBF and polynomial kernels. We also compared to the set of transforms x a for 0 a 1 where we cross validated over threshold a = 0 at the other (choosing 0 0 = 0), and the square root transform in the middle. The con vex relaxation performed best or tied for best on 4 out 6 of the exp erimen ts and was the best overall as rep orted in Figure 6. The noncon vex version also performed well but ended up with a lower accuracy than the cross validated family of x a transforms. The key to this dataset is that most of the data is very close to zero due to few pixels being in a given bin. Cross validation over x a most often chose low nonzero a values. Our metho d had man y knots in these extremely low values because that was where the data supp ort was. Plots of our learned functions on these small values can be found in Figure 7(a-f ). Solid blue is the mean for the noncon vex algorithm and dashed blue is the standard deviation. Similarly , the con vex relaxation is in red. 4.4 Gender classi cation In this exp erimen t we try to di eren tiate between images of males and females. We have 1755 lab elled images from the FERET dataset pro cessed as in [9]. Eac h pro cessed image is a 21 by 12 pixel 256 color gra y scale image that is rastorized to form training vectors. There are 1044 male images and 711 female images. We randomly split the data into 80 percen t training, 10 percen t cross validation, and and 10 percen t testing. We then compare a linear SVM to our two metho ds on 5 random splits of the data. The learned monotonic function from L Mono and C Mono are similar to a sigmoid function whic h indicates that useful saturation and threshold e ects where unco vered by our metho ds. Figure 8a sho ws examples of training images before and after they have been transformed by our learned function. Figure 8b summarizes the results. Our learned transformation outp erforms the linear SVM with the con vex relaxation performing best. A data driv en framew ork was presen ted for join tly learning monotonic transformations of input data and a discriminativ e linear classi er. The join t optimization impro ves classi -cation accuracy and pro duces interesting transformations that otherwise would require a priori domain kno wledge. Tw o implemen tations were discussed. The rst is a fast greedy algorithm for nding a locally optimal solution. Subsequen tly, a semide nite relaxation of the original problem was presen ted whic h does not su er from local minima. The greedy algorithm has similar scaling prop erties as a supp ort vector mac hine yet has local minima to con tend with. The semide nite relaxation is more computationally intensiv e yet ensures a reliable global solution. Nev ertheless, both implemen tations were helpful in syn thetic and real exp erimen ts including text and image classi cation and impro ved over standard supp ort vector mac hine tools.
 Figure 8: a) Original and transformed gender images. b) Error rates for gender classi cation. A natural next step is to explore faster (con vex) algorithms that tak e adv antage of the speci c structure of the problem. These faster algorithms will help us explore extensions suc h as learning transformations across multiple tasks. We also hop e to explore applications to other domains suc h as gene expression data to re ne the curren t logarithmic transforms necessary to comp ensate for well-kno wn saturation e ects in expression level measuremen ts. We are also interested in looking at fMRI and audio data where monotonic transformations are useful. This work was supp orted in part by NSF Aw ard IIS-0347499 and ONR Aw ard N000140710507.
 [1] S. Boyd and L. Vanden berghe. Convex Optimization . Cam bridge Univ ersit y Press, [2] M. Bro wn, W. Grundy , D. Lin, N. Christianini, C. Sugnet, M. Jr, and D. Haussler. [3] O. Chap elle, P. Hafner, and V.N. Vapnik. Supp ort vector mac hines for histogram-based [4] C. Cortes and V. Vapnik. Supp ort-v ector net works. Machine Learning , 20(3):273{297, [5] M. Hein and O. Bousquet. Hilb ertian metrics and positiv e de nite kernels on probabilit y [6] T. Jebara, R. Kondor, and A. Howard. Probabilit y pro duct kernels. Journal of Machine [7] G. Lanc kriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the [8] J.B. Lasserre. Con vergen t LMI relaxations for noncon vex quadratic programs. In [9] B. Moghaddam and M.H. Yang. Sex with supp ort vector mac hines. In Todd K. Leen, [10] T. Rob ertson, F.T. Wrigh t, and R.L. Dykstra. Order Restricte d Statistic al Infer enc e .
