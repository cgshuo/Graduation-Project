 In recent years, a number of different types of SMT methods have been proposed, such as the phrase-based, hierarchical phrased-based, and syntax-based models etc., which significantly improve the translation quality. Meanwhile, a lot of effort has been put to apply SMT systems to practical use, e.g. the software localization industry [1 X 3]. However, the translation quality cannot fully satisfy the actual demand of industry yet. For example, the ungrammatical errors and disordered words in the translation often increase human cost. Therefore, high-quality automatic translation error detection or word-level confidence estimation is necessary to further improve the working efficiency of the post-editors or translators in the localization industry.
 tures (e.g. WPP) combining with extra knowledge such as linguistic features to decrease the classification error rate [4 X 9]. As to the system-based features, a number of different algorithms to calculate the WPP were proposed based on the N -best list or word lattice, and had been applied to SMT translation quality estimation. Afterwards, some researchers try to introduce more useful knowledge sources such as syntactic and semantic features to further improve the error de-tection capability [8, 10, 11]. However, these features are not that easy to extract due to their complexity, low generalization capability, and dependency on spe-cific languages etc. Hence, currently the system-based features such as WPP and lexicalized features (e.g. word and part-of-speech (POS)) still play the main role in the error detection task or the confidence estimation task.
 cation task. Thus, the accuracy of the classifier also plays an important role in terms of improving the prediction capability besides adding new features and extra knowledge. This paper mainly focuses on the investigation of different classifiers, and presents an effective and straightforward strategy of combining two different classifiers to improve the classification performance. Firstly, we introduce the features used in our task, which are three typical WPP system-based features and three linguistic features, then employ two different classifiers, namely the MaxEnt classifier and SVM classifier to perform the classification task respectively. Finally, we carry out a combination operation  X  multiplication of the classification probabilities  X  to obtain the final result. Experiments are conducted on NIST Chinese-to-English translation task, and the results show that the combined method outperforms either individual classifier used in our task in terms of the CER.
 work as to the error detection task. In Section 3, three typical WPP and three linguistic features are described. Section 4 firstly describes the MaxEnt and SVM classifiers used in our task, and then the multi-classifier strategy and feature representation are detailed. Experimental settings, implementation and analysis are reported in Section 5. The final section concludes and gives avenues for future work. The question of translation confidence estimation has attracted a number of re-searcher due to its importance in promoting SMT application. In 2004, Blatz et al. improved the basic confidence estimation method by combining the neural network and a naive Bayes classifier to predict the word-level and the sentence-level translation errors [6]. The features they used include WPP calculated from the N -best list, translation model-based features, semantic feature extracted from the WordNet, as well as simple syntactic features. Experimental results show that all among these features, WPP is more effective with strong general-ization capability than linguistic features.
 form confidence measures, and proposed different WPP algorithms to verify the effectiveness in confidence estimation task [5, 7]. In their task, the words in the generated target sentence can be tagged as correct or false to facilitate post-editing or work in an interactive translation environment. Their experiments conducted on different data sets show that different WPP algorithms perform differently, but basically each can reduce the CER. Furthermore, the combina-tion of different features can perform better than any individual features. in the computer-aided translation field [10, 11]. They categorize translations into  X  X ad X  or  X  X ood X  classes based on sentence-level binary scores of the post-edition MT fragments. The features used are called  X  X lack-box X  features, which can be extracted from any MT systems only if the information from the input (source) and translation (target) sentences are given, such as source and target sentence lengths and their ratios, the edit distance between the source sentence and sen-tences in the corpus used to train the SMT system. Their work contributed significantly to SMT translation confidence estimation research and application. to predict translation errors (each word is tagged as correct or incorrect ) by integrating a WPP feature, a syntactic feature extracted from LG parser and some lexical features [8]. The experimental results show that linguistic features can reduce CER when used alone, and it outperforms WPP. Moreover, linguistic features can further provide complementary information when combined with WPP, which collectively reduce the classification error rate.
 tracting more richer set of source-side information features, and combined sen-tence -level and word-level features to estimate translation quality. They predict error types of each word in the MT output with a confidence score, then extend it to the sentence level, and finally apply it to N -best list re-ranking task to improve MT quality [9].
 icantly improve the classification performance by using different kinds of clas-sifiers and combining multiple classifiers over a set of effective features. Specif-ically, this paper 1) verifies the performance of various classifiers, namely the MaxEnt classifier and the SVM classifier on the translation error detection task; 2) presents a probability product combination strategy to fuse two classifier to obtain better results. 3.1 WPP Feature WPP is served as a major and effective confidence estimation feature both in speech recognition and in SMT post-processing. In terms of SMT, WPP refers to the probability of a word occurring in the hypothesis given a source input. Generally speaking, the underlying idea is that if the posterior probability of a word occurring in a hypothesis is high, then the chance that it is believed to be correct is big correspondingly. Based on this consideration, it is reasonable that the more useful information considered in the WPP algorithm, the better the performance would achieve.
 hypothesis of the N -best list as in (1), where a is a hidden variable which indicates an alignment measure; f ( a, e n,i , e ) is a binary sign function as in (2), the posterior probability of a word in a hypothesis can be worked out according to the sentence-level posterior probabilities of hypotheses in the N -best list. The vital information to be considered is the position of the word e which is determined by the alignment measure between the 1-best hypothesis and the rest of the N -best list.
 influence on the error detection performance over different kinds of classifiers. 3.1.1 Fixed Position based WPP The basic idea of fixed position-based WPP is that given an input f J 1 , the poste-rior probability of a word e at position i in the hypothesis e I 1 can be calculated by summing the posterior probabilities of all sentences in the N -best list containing target word e at target position i , which is as in (3), where  X  ( x, y ) is the Kronecker function as in (4), any extra alignment measure between the 1-best and any other hypotheses. 3.1.2 Flexible Position based WPP The potential problem of fixed position based WPP is that generally the hy-potheses in the N -best list have different length that will make the same word occur at different positions so that the WPP would have a large error com-pared to the real probability distribution. Naturally the intuition to improve this method is to make the position flexible, e.g. using a sliding window. i , i.e., the context. Let the window size be t , then the sliding window at position i can be denoted as i  X  t . If the target word e appears inside the window, then we regard it occurring at position i and sum up the probability of the current hypothesis, which is formulated as in (5), where p k ( e | f J 1 , e I 1 ) is as illustrated in Eq. (3). 3.1.3 Word Alignment based WPP The sliding window based method needs to choose a proper window size which can only be determined by experiments. Thus, another straightforward way to improve the fixed position method is to perform the word alignment between the 1-best hypothesis and the rest of hypotheses in the N -best list, i.e., align the rest of hypotheses against the 1-best hypothesis.
 other hypotheses, then the WPP of the word e at position i is as in (6): where N -best list, which is given by the SMT system.  X  ( x, y ) is the Kronecker function as in Eq. (4). 3.2 Linguistic Features 3.2.1 Syntactic Features Xiong et al. extracted syntactical feature by checking whether a word is con-nected with other words from the output of the LG parser. When the parser fails to parse the entire sentence, it ignores one word each time until it finds linkages for remaining words. After parsing, those ignored words which are not connected to any other words to be called null -linked words. These null -linked words are prone to be syntactically incorrect and the linked words are prone to be syntactically correct, then a binary syntactic feature for a word according to its links can be defined as in (8), Refer to detailed description in [8]. 3.3 Lexical Features Lexical features such as the word itself and the POS [12] are common features used in NLP tasks. In this paper, we also utilize the word/pos with its context (e.g. the previous two words/pos and next two words/pos) to form a feature vector as follows,  X  word : ( w  X  2 , w  X  1 , w, w 1 , w 2 )  X  pos : ( pos  X  2 , pos  X  1 , pos, pos 1 , pos 2 ) In this paper, the translation error detection is regarded as a classification task. In this section, we introduce two kinds of classifiers  X  MaxEnt and SVM, and then come up with a multi-classifier combination strategy to perform our translation error detection task.
 or  X  incorrect  X  if it is a wrong translation. Therefore, the label set for the clas-sification task can be denoted as y = { c, i } , where y indicates the label set, c stands for class  X  correct  X  and i represents class  X  incorrect  X . 4.1 Maximum Entropy Classifier The MaxEnt model is the most commonly-used classifier in NLP tasks, which is a generalization of the model used by the naive Bayes classifier. The basic idea of MaxEnt model is to build a consistent model for all known factors without considering any unknown factors. A remarkable characteristic of the MaxEnt classifier is that features are not necessarily required independent. In doing so, features can be arbitrarily added into the model. Denote the binary classification and i stand for correct and incorrect labels respectively in our task), then as in literature [8], the MaxEnt classifier for a word e in a hypothesis can be formulated as in (9), where f i is a feature function,  X  i is the weight of f i , y is the class label, and x is the feature vector. 4.2 SVM Classifier SVM has been widely and successfully used in many NLP tasks, such as word sense disambiguation, name entity recognition etc. The basic principle of SVM is to find an optimal hyperplane to make the distance maximum between two classes. The classification task can be defined as in (10), tion. 4.3 Multi-classifier Combination Multiple classifiers combination method has been applied in many NLP tasks, such as word sense disambiguation etc. Most of these applications have shown a considerable improvement over the performance of individual classifiers. There-fore, it leads us to consider implementing such a multiple classifier combination strategy for the translation error detection task as well.
 in the classification results, so that using classifier combination techniques can potentially achieve a better classification accuracy based on the assumption that the errors made by each of the classifiers are not identical, and if the combination strategy is appropriate, then the outcome might correct some errors. such as probability distribution based method, vote-based method, rank-based method, linear/weighted linear combination method etc. [13 X 15]. Regarding this task, considering that we only have two different classifiers, a straightforward strategy  X  probability product rule  X  is presented to combine the outputs of two individual classifiers to achieve better results.
 task can be formalized as: C = { C 1 , . . . , C n } and the class set c = { c 1 , c 2 } , for a word sequence S = for a word w from each individual classifier C i can be denoted as O i w = { p i c classifier C i , and p i c C classifier combination can be formulated as in (11), where c w indicates the predicted class for the word w . In our task, n = 2. 4.4 Feature Vector Representation As described in previous sections, in our translation error detection task, we have four kinds of features: wpp , pos , word and link (c.f. Section 3). In this section, we introduce how to construct a normalized and unified feature vector format for the MaxEnt and SVM classifiers.
 considered in the process of feature extraction. Therefore, in our task, to build a feature vector for a word e , we look at 2 words before and 2 words after the current word position as well. Thus, the feature vector x that includes four kinds of features can be denoted as, 5.1 Chinese-English SMT System We utilize Moses [17] to provide 10 , 000-best list with translation direction from Chinese to English. The training data consists of 3,397,538 pairs of sentences (including Hong Kong news, FBIS, ISI Chinese-English Network Data and Xin-Hua news etc.). The language model is five-gram built on the English part of the bilingual corpus and Xinhua part of the English Gigaword.
 (1,664 source sentences) and the test sets are NIST MT-05 (1,082 sentences) and NIST MT-08 (1,357 sentences). Each source sentence has four references. During the decoding process, the SMT system exports 10 , 000-best hypotheses for each source sentence, i.e., N = 10 , 000.
 of BLEU4 and other metrics. 5.2 Experimental Settings for Translation Error Detection Task Development and test sets: in the error detection task, we use NIST MT-08 as the development set to tune the classifiers, and NIST MT-05 as the test set to evaluate the classification performance.
 Data annotation: we use the WER metric in TER toolkit [18] to determine the true labels for the words in the development set and the test set. Specifically, we firstly perform the minimum edit distance alignment between the hypothesis and the four references, and then select the one with minimum WER score as the final reference to tag the hypothesis. That is, a word e in the hypothesis is tagged as c if it is the same as that in the reference, otherwise tag it as i . hypothesis of MT-08 set (37.99% ratio of correct words, RCW), 15,179 correct words and 21,318 incorrect words in the 1-best hypothesis of MT-05 set (41.59% RCW). See RCW in Table 1.
 sification task includes CER (classification error rate), precision, recall and F measure. In our translation error detection task, we use CER as the main eval-uation metric to evaluate the system performance that is defined as in (12), Since the RCW is less than 50% (41.59%), i.e., the number of incorrect words is more than correct words, it is reasonable to use the RCW as the baseline of CER to examine the classification performance of classifiers.
 metrics to evaluate some performance of features and classifiers. See definitions in [8]. 5.3 Classification Experiments for Individual Classifiers Results of different features and feature combinations over two different individ-ual classifiers are shown in Table 2.
 the flexible position-based WPP with the window size 2, and WPP Lev repre-sents word alignment-based WPP. com 1 represents the feature combination of WPP Dir + Word + Pos + Link , com 2 stands for the feature combination of WPP Win + Word + Pos + Link , and com 3 indicates the feature combination of WPP Lev+ Word + Pos + Link .
 cantly reduce the CER compared to the baseline; 2) the WPP Win and WPP Lev perform better than WPP Dir which shows that position information is helpful; 3) linguistic features perform better than three WPP features over the MaxEnt (except link ), while worse than those over the SVM classifier in terms of CER. However, they all significantly reduce the CER compared to the baseline; 4) WPP Win + word + pos + link obtains the best performance both on Max-Ent and SVM classifiers. Feature combinations outperform any of the individual features; 5) SVM classifier outperforms the MaxEnt classifier on all features in terms of the CER. 5.4 Classification Experiment on Multi-classifier Combination The results of the Multi-classifier Combination experiment are shown in Table 3. multi-classifier combination method achieved significant improvement by relative 15.10%, 15.99% and 16.09% in terms of CER. 2) compared to the MaxEnt and SVM classifiers over three feature combinations, namely com 1, com 2 and com 3, the proposed multi-classifier combination method achieved significant improve-ment respectively by relative 0.15%, 0.94%, 1.52%, and 1.73%, 1.72%, 2.02% in terms of CER. 3) WPP Lev + word + pos + link and WPP Win + word + pos + link are significantly better than WPP Dir + word + pos + link , which indicates that the flexible position based WPP feature is more useful than the fixed position based WPP on the multi-classifier combination.
 WPP Win performs the best and robust both in the three individual WPP features and the three combined features. The reason we consider is that the sliding window makes the alignment more flexible and considers more context information. 2) linguistic features are helpful to the error detection. 3) multi-classifier strategy is effective to further improve the error detection performance. name, location name, organization name etc.) are prone to be wrongly classified; 2) the prepositions, conjunctions, auxiliary verbs and articles are easier to be wrongly classified due to the factors that they often have an impact on the word orders or lead to empty alignment links; 3) the proportion of the notional words that are wrongly classified is relatively small.
 This paper presents a multi-classifier combination strategy for translation error detection. Firstly three different kinds of WPP features, three linguistic features and two individual classifiers are introduced, then a probability product based multi-classifier combination method is proposed which multiplies the correspond-ing classification probabilities respectively coming from MaxEnt and SVM clas-sifiers for each word in a hypothesis, and then decides the label by the maximum probability. Experimental results on Chinese-to-English NIST MT data sets show that 1) in terms of individual classifiers used in our experiments, SVM classifier outperforms the MaxEnt classifier; 2) the proposed multi-classifier combination method performs the best compared to two individual classifiers.
 task in the respects of 1) introducing paraphrases to annotate the hypotheses so that it can truly reflect the correct or incorrect at the semantic level; 2) introducing new useful features to further improve the detection capability; 3) performing experiments on more language pairs to verify our proposed method. This work is supported by NSF project (61100085), SRF for ROCS, State Edu-cation Ministry, and Research Foundation of Education Department of Shaanxi Provincial Government (11JK1029). Thanks the reviewers for their insightful comments and suggestions.

