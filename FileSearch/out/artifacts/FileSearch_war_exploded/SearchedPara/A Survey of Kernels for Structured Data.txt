 Kernel methods in general and support vector machines in particular have been successful in various learning tasks on data represented in a single table. Much  X  X eal-world X  data, however, is structured  X  it has no natural representation in a single table. Usually, to apply kernel methods to  X  X eal-world X  data, extensive pre-processing is performed to embed the data into a real vector space and thus in a single table. This survey describes several approaches of defining positive definite kernels on structured instances directly. Kernel methods, structured data, multi-relational data min-ing, inductive logic programming Most  X  X eal-world X  data has no natural representation as a single table, yet most data mining research has so far concen-trated on discovering knowledge from single tables. In order to apply  X  X raditional X  data mining methods to structured data, extensive pre-processing has to be performed. Re-search in inductive logic programming and multi-relational data mining [8] aims to reduce these pre-processing efforts by considering learning from multi-relational data descrip-tions directly. Some algorithms developed in that field are upgrades of algorithms, originally developed only with single tables data in mind. Among the so far upgraded algorithms are popular data mining methods such as decision trees, rule learners, and distance-based algorithms.
 Support vector machines [3; 41] are among the most success-ful recent developments within the machine learning and data mining communities. Along with some other learn-ing algorithms like Gaussian processes and kernel principal component analysis, they form the class of kernel methods [32; 37]. The computational attractiveness of kernel meth-ods comes from the fact that they can be applied in high dimensional feature spaces without suffering the high cost of explicitly computing the mapped data. The kernel trick is to define a positive definite kernel on any set. For such functions it is know that there exists an embedding of the set in a linear space such that the kernel on the elements of the set corresponds to the inner product in this space. While the inductive logic programming community has tra-ditionally used logic programs to represent structured data, the scope has now extended and also includes other knowl-edge representation languages. Development of kernels for structured data has mostly been motivated and guided by  X  X eal-world X  problems. Although the structure of these prob-lems is often such that they do not permit a natural repre-sentation in a single table, the full power of logic programs is hardly ever needed. This is one reason, kernel design has concentrated on different and sometimes less powerful knowledge representations.
 This paper is not intended as an introduction to kernel-based learning algorithms, for such an introduction the reader is referred to one of the excellent books [5; 37] or tutorials [2; 32] on kernel methods ([2] is available online). Instead, this paper intends to give an introduction to kernel functions defined on structured data. For that, we assume some basic familiarity with linear algebra.
 In this paper we distinguish kernels according to whether they are defined on the structure of the instances (syntax-driven kernels), or on the structure of the instance space (model-driven kernels). Furthermore, we distinguish accord-ing to the power of the knowledge representation used. The outline of the paper is as follows: Section 2 introduces kernel functions and characterizes valid and good kernels. It also presents a classification scheme for kernel functions. Model-driven kernels are then described in section 3. After that syntax-driven kernels are described in section 4. Finally, section 5 concludes . Two components of kernel methods have to be distinguished: the kernel machine and the kernel function. While the ker-nel machine encapsulates the learning task and the way in which a solution is looked for, the kernel function encapsu-lates the hypothesis language, i.e., how the set of possible solutions is made up. Different kernel functions implement different hypothesis spaces or even different knowledge rep-resentations. Before we give the definition of positive definite kernels, the traditionally used kernels (on vector spaces) are briefly re-viewed in this section. Let x, x 0  X  R n and let  X  X  ,  X  X  denote the scalar product in R n . Apart from the linear kernel and the normalized linear kernel which corresponds to the cosine of the angle enclosed by the vectors, the two most frequently used kernels on vector spaces are the polynomial kernel and the Gaussian kernel. Given two parameters l  X  R , p  X  N + the polynomial kernel is defined as: The intuition behind this kernel definition is that it is of-ten useful to construct new features as products of original features. This way for example the XOR problem can be turned into a linearly separable problem. The p in above definition is the maximal order of monomials making up the new feature space, the l is a bias towards lower order mono-mial. If l = 0 the feature space consists only of monomials of order p of the original features.
 Example : Consider the positive examples (+1 , +1) , (  X  1 ,  X  1) and the negative examples (+1 ,  X  1) , (  X  1 , +1). Clearly, there is no straight line separating positive from negative exam-ples. However, if we use the transformation  X  : ( x 1 , x ( x thonormal to (0 , 1 , 0) as the the sign of x 1 x 2 already cor-responds to the class. To see that this transformation can be performed implicitly by a polynomial kernel, let x = ( x 1 , x 2 ) , z = ( z 1 , z 2 ) and k ( x, z ) =  X  x, z  X  2 Now, let x + ( x  X  ) be either of the positive (negative) exam-ples given above. Any example x can be classified without explicitly transforming instances, as k ( x, x + )  X  k ( x, x responds to implicitly projecting x onto the vector  X  ( x  X  ( x  X  ) = (0 , Given the bandwidth parameter  X  the Gaussian kernel is defined as: Using this kernel function in a support vector machine can be seen as using a radial basis function network with Gaus-sian kernels centered at the support vectors. The images of the points from the vector space R n under the map  X  : R n  X  H with k ( x, x 0 ) =  X   X  ( x ) ,  X  ( x 0 )  X  lie all on the surface of a hyperball in the Hilbert space H . No two images are orthogonal and any set of images is linearly independent. The parameter  X  can be used to tune how much general-ization is done. For very high  X  , all vectors  X  ( x ) are al-most parallel and thus almost identical. For very small  X  , the vectors  X  ( x ) are almost orthogonal to each other and the Gaussian kernel behaves almost like the matching ker-nel k  X  ( x, x 0 ) = 1  X  x = x 0 and k  X  ( x, x 0 ) = 0  X  x 6 = x applications this often causes a problem known as the ridge problem , which means that the learning algorithm functions more or less just as a lookup table. Technically, a kernel k corresponds to the inner product in some feature space which is, in general, different from the representation space of the instances. The computational attractiveness of kernel methods comes from the fact that quite often a closed form of these  X  X eature space inner prod-ucts X  exists. Instead of performing the expensive transfor-mation step explicitly, the kernel can be calculated directly, thus performing the feature transformation only implicitly. Whether, for a given function k : X  X  X  X  R , a feature transformation  X  : X  X  H into the Hilbert space H exists, such that k ( x, x 0 ) =  X   X  ( x ) ,  X  ( x 0 )  X  for all x, x checked by verifying that the function is positive definite [1]. This means that any set, whether a linear space or not, that admits a positive definite kernel can be embedded into a linear space. Throughout the paper, we take  X  X alid X  to mean  X  X ositive definite X . Here then is the definition of a positive definite kernel. ( Z + is the set of positive integers.) Definition : Let X be a set. A symmetric function k : X  X  X  X  R is a positive definite kernel on X if, for all n  X  Z + , x 1 , . . . , x n  X  X , and c 1 , . . . , c n  X  P While it is not always easy to prove positive definiteness for a given kernel, positive definite kernels do have nice closure properties. In particular, they are closed under sum, direct sum, multiplication by a scalar, product, tensor product, zero extension, pointwise limits, and exponentiation [5]. For a kernel method to perform well on some domain, va-lidity of the kernel is not the only issue. To discuss the characteristics of good kernels, we need the notion of a con-cept class. Concepts c are functions c : X  X   X , where X is often referred to as the instance space or problem domain (examples are elements of this domain) and  X  are Boolean labels. A concept class C is a set of concepts.
 While there is always a valid kernel that performs poorly ( k 0 ( x, x 0 ) = 0), there is also always a valid kernel ( k +1  X  c ( x ) = c ( x 0 ) and k c ( x, x 0 ) =  X  1  X  c ( x ) 6 = c ( x performs ideally. We distinguish the following three issues crucial to  X  X ood X  kernels: completeness, correctness, and ap-propriateness.
 Completeness refers to the extent to which the knowledge incorporated in the kernel is sufficient for solving the prob-lem at hand. A kernel is said to be complete if it takes into account all the information necessary to represent the concept that underlies the problem domain. Formally, we call a kernel complete if k ( x,  X  ) = k ( x 0 ,  X  ) implies x = x . With respect to some concept class, however, it is not important to distinguish between instances that are equally classified by all concepts in that particular concept class. We call a kernel complete with respect to a concept class C Correctness refers to the extent to which the underlying semantics of the problem are obeyed in the kernel. Cor-rectness can formally only be expressed with respect to a certain concept class and a certain hypothesis language. In particular, we call a kernel correct with respect to a con-cept class C and support vector machines if for all con-cepts c  X  C we can find  X  i  X  R , x i  X  X ,  X   X  R such that  X  x  X  X : P i  X  i k ( x i , x )  X   X   X  c ( x ). For the remainder of the paper we will use  X  X orrect X  only with respect to support vector machines and other kernel methods using a similar hypothesis language.
This corresponds to the map  X  , with  X   X  ( x ) ,  X  ( x 0 )  X  = k ( x, x 0 ) for all x, x 0 , being injective.
 Appropriateness refers to the extent to which examples that are close to each other in class membership are also  X  X lose X  to each other in feature space. Appropriateness can formally only be defined with respect to a concept class and a learning algorithm. A kernel is appropriate for learning concepts in a concept class if polynomial mistake bounds can be derived for some algorithm using this kernel. This problem is most apparent in the the matching kernel k  X  ( x, x 0 ) = 1  X  x = x and k  X  ( x, x 0 ) = 0  X  x 6 = x 0 which is always complete and correct but (in general) not appropriate.
 Empirically, a complete, correct and appropriate kernel ex-hibits two properties. A complete and correct kernel sep-arates the concept well, i.e., a learning algorithm achieves high accuracy when learning and validating on the same part of the data. An appropriate kernel generalizes well, i.e., a learning algorithm achieves high accuracy when learning and validating on different parts of the data. A useful conceptual distinction between different kernels is based on the  X  X riving-force X  of their definition. We distin-guish between syntax and models as the driving-force of the kernel definition.
 Syntax is often used in typed systems to formally describe the semantics of the data. It is the most common driving force. In its simplest case, i.e., untyped attribute-value rep-resentations, it treats every attribute in the same way. More complex syntactic representations are graphs, restricted sub-sets of graphs such as lists and trees, or terms in some (pos-sibly typed) logic. Whenever kernels are syntax-driven, they are either special case kernels, assuming some underlying se-mantics of the application, or they are parameterized and offer the possibility to adapt the kernel function to certain semantics.
 Models contain some kind of knowledge about the instance space, i.e., about the relationships among instances. These models can either be generative models of instances or they can be given by some sort of transformation relations. Hid-den Markov models are a frequently used generative model. Edit operations on a lists are one example for operations that transform one list into another. The graph defined on the set of lists by these operations can be seen as a model of the instance space. While each edge of a graph only contains local information about neighboring vertices, the set of all edges, i.e., the graph itself, also contains information about the global structure of the instance space. This section describes different kernel functions defined on models describing the instance space. These models are ei-ther constructed form background knowledge about the se-mantics of the domain at hand or learned from data. The first part of this section deals with kernel functions defined on probabilistic, generative models of the instance space. The second part of the this section describes kernel func-tions defined using some kind of similarity relation or trans-formation operation between instances. Generative models in general and hidden Markov models [35] in particular are widely used in computer science. One of their application areas is protein fold recognition where one tries to understand how proteins fold up in nature. An-other application area is speech recognition. One motivation behind the development of kernels on generative models is to be able to apply kernel methods to sequence data. Se-quences occur frequently in nature, for example, proteins are sequences of amino acids, genes are sequences of nucleic acids, and spoken words are sequences of phonemes. An-other motivation is to improve the classification accuracy of generative models.
 The first and most prominent kernel function based on a generative model is the Fisher kernel [17; 18]. The key idea is to use the gradient of the log-likelihood with respect to the parameters of a generative model as the features in a discriminative classifier. The motivation to use this feature space is that the gradient of the log-likelihood with respect to the parameters of a generative model captures the gen-erative process of a sequence rather that just the posterior probabilities.
 Let U x be the gradient of the log-likelihood with respect to the parameters of the generative model P ( x |  X  ) at x : Furthermore, let I be the Fisher information matrix, i.e., the expected value of the outer product U x U &gt; x over P ( x |  X  ). The Fisher kernel is then defined as k ( x, x 0 ) = U &gt; The Fisher kernel can be calculated whenever the proba-bility model P ( x |  X  ) of the instances given the parameters of the model has a twice differentiable likelihood and the Fisher information matrix is positive definite at the chosen  X  . Learning algorithms using the Fisher kernel can be shown to perform well if the class variable is contained as a latent variable in the probability model. In [17] it has been shown that under this condition kernel machines using the Fisher kernel are asymptotically at least as good as choosing the maximum a posteriori class for each instance based on the model. In practice often the role of the Fisher information matrix is ignored, yielding the kernel k ( x, x 0 ) = U &gt; Usually, as a generative model a hidden Markov model is used and as a discriminative classifier a support vector ma-chine is used. The Fisher kernel has successfully been ap-plied in many learning problems where the instances are sequences of symbols, such as protein classification [16; 20] and promoter region detection [34].
 The key ingredient of the Fisher kernel is the Fisher score mapping U x that extracts a feature vector from a generative model. In [39] performance measures for comparing such feature extractors are discussed. Based on this discussion, a kernel is defined on models where the class is an explicit variable in the generative model, rather than only a latent variable as in the Fisher kernel. Empirically, this kernel performs favorably to the Fisher kernel on a protein fold recognition task. A similar approach has been applied to speech recognition [38].
 Recently, a general framework for defining kernel functions on generative models has been described [40]. The so-called  X  X arginalized kernels X  contain the above described Fisher kernel as a special case. The paper compares other marginal-ized kernels with the Fisher kernel and argues that these have some advantages over the Fisher kernel. While, for ex-ample, Fisher kernels only allow for the incorporation of second-order information by using a second-order hidden Markov model [7], other marginalized kernels allow for the use of second-order information with a first-order hidden Markov model. In [40] it is shown that incorporating this second-order information in a kernel function is useful in the prediction of bacterial genera from their DNA.
 In general, marginalized kernels are defined on any genera-tive model with some visible and some hidden information. Let the visible information be an element of the finite set X and the hidden information be an element of the finite set S . If the hidden information was known, a joint kernel k : ( X  X  S )  X  ( X  X  S )  X  R could be used. Usually, the hid-den information is unknown but the expectation of a joint kernel with respect to the hidden information can be used. Let x, x 0  X  X and s, s 0  X  S . Given a joint kernel k z and the posterior distribution p ( s | x ) (usually estimated from a generative model), the marginalized kernel in X is defined as: To complete this section on kernels from generative mod-els, the idea of defining kernel functions between sequences based on a pair hidden Markov model [7] has to be men-tioned. Such kernels have been developed in [15] and [45]. Strictly speaking pair hidden Markov models are not mod-els of the instance space, they are generative models of an aligned pair of sequences [7].
 Recently, [36] has shown that syntactic string kernels (pre-sented in section 4.2) can be seen as a special case of Fisher kernels of a k-stage Markov process with uniform distribu-tions over the transitions. This section describes kernels that are based on knowledge about common properties of instances or transformations between instances. The best known kernel in this class is the diffusion kernel.
 The motivation behind diffusion kernels [24] is that it is often more easy to describe the local neighborhood of an instance than to describe the structure of the whole instance space or to compute a similarity between two arbitrary instances. The neighborhood of an instance might be all instances that differ with this one only by the presence or absence of one particular property. When working with molecules, for ex-ample, such properties might be functional groups or bonds. The neighborhood relation obviously induces global infor-mation about the make up of the instance space. The ap-proach taken in the diffusion kernel is to try to capture this global information in a kernel function merely based on the neighborhood description.
 The main mathematical tool used in diffusion kernels is ma-trix exponentiation. The exponential of a square matrix H is defined as It is known that the limit always exists and that e  X H is a positive definite matrix if H is symmetric. In this case it is also possible to compute e  X H efficiently by first diagonalizing the matrix H such that H = T  X  1 DT and then computing where e  X D can be computed component-wise (as D is diag-onal).
 The matrix H is called the  X  X enerator X . The kernel matrix is defined as the exponential of the generator. In the case of instance spaces that have undirected graph structure, [24] suggests to use the negative Laplacian of the graph as the generator. Let G = ( V , E ) be an undirected graph with vertices V = {  X  i } and edges E  X  2 V . In particular {  X  E if there is an edge between vertices  X  i and  X  j . Furthermore, let  X  (  X  i ) = {  X  j  X  V : {  X  i ,  X  j }  X  E} . Then the negative Laplacian of the graph is given by where H ij denotes the i, j -th component of the generator matrix. To generalize this to the case of graphs with weighted and/or parallel edges, the components of H ij for  X  i 6 =  X  replaced by the sum over the weights of all edges between  X  and  X  j .
 If the instance space is big, the computation of e  X H sug-gested above might still be too expensive. For some special instance space structures, such as regular trees, complete graphs, and closed chains [24] gives closed forms for directly computing the exponential of the generator and thus the kernel matrix. An application of the diffusion kernel to gene function prediction has been described in [43].
 A similar idea of defining a kernel function on the structure of the instance space is described in [42]. In that paper it is described how global patterns of inheritance can be incorpo-rated in a kernel function and how missing information can be dealt with using a probabilistic model of inheritance. The main difference to the diffusion kernel is that the structures considered are directed trees and that instances correspond to sets of vertices in these trees rather than single instances. Trees are connected acyclic graphs where one vertex has no incoming edge and all other nodes have exactly one incom-ing edge. The application considered in that paper is that of classifying phylogenetic profiles. A phylogenetic profile contains information about the organisms in which a partic-ular gene occurs. The phylogenetic information considered in [42] is represented as a tree such that each leaf (a vertex with no outgoing edge) corresponds to one living organism and every other vertex corresponds to some ancestor of the living organisms. To represent genes, every vertex of the tree is assigned a random variable. The value of this ran-dom variable indicates whether the corresponding organism has a homologue of the particular gene or not.
 If the genomes of all ancestor organisms were available, this information could be used to define a kernel function re-flecting the similarity between evolutions. A subtree of the above described tree along with an assignment of indicator values to this tree is called an evolution pattern. Ideally the kernel function on two genes would be defined as the num-ber of evolution patterns that agree with the phylogenetic histories of both genes. An evolution pattern agrees with a phylogenetic history if the assignment of indicator variables is the same for all vertices in the evolution pattern. As the genomes are only known for some ancestor organisms, a probabilistic model that can be used to estimate missing indicator variables is suggested in [42].
 For two given phylogenetic profiles x L , y L the kernel is de-fined as follows. Let T be a tree, L be the leaf nodes and C ( T ) the set of all possible subtrees of T . One particular evolution pattern can be expressed as a subtree S  X  C ( T ) and the corresponding assignment z S of indicator values. p ( z s ) denotes the probability of such an evolution pattern having occurred in nature and p ( x L | z S ) is the probability of observing a particular phylogenetic profile x L given the evolution pattern z S (obtained from the probabilistic model mentioned above). Then the tree kernel for phylogenetic profiles is defined as: In [42] it is shown that this kernel function can be computed in time linear in the size of the tree. This section describes different kernel definitions based on the syntax of the representation. The simplest way to apply kernel methods to multi-relational data it to first transform the data into a single table. This process is called propo-sitionalization [25]. The first application of support vector machines to propositionalized data is described in [26]. We will not consider such approaches in more detail in this pa-per, we will rather concentrate on kernels defined directly on structured data.
 The most prominent kernel for representation spaces that are not mere attribute-value tuples, the convolution kernel, is introduced first. Its key idea is to define a kernel on a composite object by means of kernels on the parts of the objects. This idea is also reflected in many of the kernel functions developed later and described thereafter in this section. Several kernel functions have been defined on se-quences of discrete symbols. The idea is always to extract all possible subsequences (of some kind) and to define the ker-nel function based on the occurrence and similarity of these subsequences. This idea can be generalized to extracting subtrees of trees and defining a kernel function based oc-currence and similarity of subtrees in two trees. A rather different approach is that of representing the instances of the learning task as terms in a typed higher-order logic. These terms are powerful enough to allow for a close modeling of the semantics of different objects by means of the syntax of the representation. The last kernel described in this section is a kernel defined on instances represented by graphs. The best known kernel for representation spaces that are not mere attribute-value tuples is the convolution kernel proposed by Haussler [15]. The basic idea of convolution kernels is that the semantics of composite objects can of-ten be captured by a relation R between the object and its parts. The kernel on the object is then made up from kernels defined on different parts.
 Let x, x 0  X  X be the objects and ~x, ~x 0  X  X 1  X   X   X   X   X  X be tuples of parts of these objects. Given the relation R : ( X 1  X   X   X   X   X  X D )  X  X we can define the decomposition R  X  1 as R  X  1 ( x ) = { ~x : R ( ~x, x ) } . Then the convolution kernel is defined as The term  X  X onvolution kernel X  refers to a class of kernels that can be formulated in the above way. The advantage of convolution kernels is that they are very general and can be applied in many different problems. However, because of that generality, they require a significant amount of work to adapt them to a specific problem, which makes choosing R in  X  X eal-world X  applications a non-trivial task. While the traditional kernel function used for text classi-fication is simply the scalar product of two texts in their  X  X ag-of-words X  representation [19], this kernel function does not take the the structure of the text or words into account but simply the number of times each word occurs. More sophisticated approaches try to define a kernel function on the sequence of characters. Similar approaches define ker-nel functions on other sequences of symbols, e.g., on the sequence of symbols each corresponding to one amino acid and together describing a protein.
 The first kernel function defined on strings can be found in [46; 31] and is also described in [5; 37]. The idea behind this kernel is to base the similarity of two strings on the number of common subsequences. These subsequences need not be contiguous in the strings but the more gaps in the occurrence of the subsequence, the less weight is given to it in the kernel function. This can be best illustrated by an example.
 Consider the two strings  X  X at X  and  X  X art X . The common sub-sequences are  X  X  X ,  X  X  X ,  X  X  X ,  X  X a X ,  X  X t X ,  X  X t X ,  X  X at X . As mentioned above, it is useful to penalize gaps in the occurrence of the subsequence. This can be done using the total length of a subsequence in the two strings. We now list the common subsequences again and give the total length of their oc-currence in  X  X at X  and  X  X art X  as well:  X  X  X :1/1,  X  X  X :1/1,  X  X  X :1/1,  X  X a X :2/2,  X  X t X :2/3,  X  X t X :3/4,  X  X at X :3/4. Usually, an exponential decay is used. With a decay factor  X  the penalty associ-ated with each substring is  X  X  X :(  X  1  X  1 ),  X  X  X :(  X  1  X  1  X  X a X :(  X  2  X  2 ),  X  X t X :(  X  2  X  3 ),  X  X t X :(  X  3  X  4 ),  X  X at X :(  X  function is then simply the sum over these penalties, i.e., k ( X  X at X  ,  X  X art X ) = 2  X  7 +  X  5 +  X  4 + 3  X  2 .
 Let  X  be a finite alphabet,  X  n the set of strings of length n from that alphabet and  X   X  the set of all strings from that alphabet. Let | s | denote the length of the string s  X   X  let the symbols in s be indexed such that s = s 1 s 2 . . . s For a set of indices i let s [ i ] be the subsequence of s induced by this set of indices and let l ( i ) denote the total length of s [ i ] in s , i.e., the biggest index in i minus the smallest index in i plus one. We are now able to define the feature transformation  X  underlying the string kernel. For some string u  X   X  n the value of the feature  X  u ( s ) is defined as: The kernel between two strings s, t  X   X   X  is then simply the scalar product of  X  ( s ) and  X  ( t ) and can be written as: While this computation appears very expensive, recursive computation can be reduced to O ( n | s || t | ) [31]. An alternative to the above kernel has been used in [33] and [27] where only contiguous substrings of a given string are considered. A string is then represented by the number of times each unique substring of (up to) n symbols occurs in the sequence. This representation of strings by their con-tiguous substrings is known as the spectrum of a string or as its n -gram representation. The kernel function is simply the scalar product in this representation. It is shown in [27] that this kernel can be computed in time linear in the length of the strings and the length of the considered substrings. In [33] not all possible n -grams are used but a simple statistical test is employed as a feature subset selection heuristic. This kernel has been applied to protein [27] and spoken text [33] classification. Spoken text is represented by a sequence of phonemes, syllables, or words.
 Many empirical results comparing the above kernel functions can be found in [30]. As shown in [36] these kernels can be seen as a special case of the Fisher kernel (presented in section 3.1). This perspective leads to a generalization of the above kernel that is able to deal with variable length substrings. In [44] and [28] string kernels similar to the n -gram kernel are considered and it is shown how these can be computed efficiently by using suffix and mismatch trees, respectively. The main conceptual difference to the n -gram kernel is that a given number of mismatches is allowed when comparing the n -grams to the substrings.
 Another kernel on strings can be found in [47]. The focus of that paper is on the recognition of translation inition sites in DNA or mRNA sequences. This problem is quite different from the above considered applications. The main difference is that rather than classifying whole sequences, in this task one codon (three consecutive symbols) from a sequence has to be identified as the translation inition site. Each sequence can have arbitrarily many candidate solutions of which one has to be chosen. However, in [47] and earlier work this problem is converted into a traditional classification problem on sequences of equal length.
 Fixed length windows of symbols centered at each candidate solution are extracted from each sequence. The class of one such window corresponds to the candidate solution being a true translation inition site or not. One valid kernel on these sequences is simply the number of symbols that coincide in the two sequences. Other kernels can, for example, be defined as a polynomial of this kernel.
 Better classification accuracy is, however, reported in [47] for a kernel that puts more emphasis on local correlations. Let n be the length of each window and  X  be the set of possible symbols, then each window is an element of  X  n . For x, x 0  X   X  n let x i , x 0 i denote the i -th element of each sequence. Using the matching kernel k  X  on  X  ( k  X  :  X   X   X   X  R is defined as k  X  ( x i , x 0 i ) = 1 if x i = x 0 i and k  X  ( x i , x first a polynomial kernel on a small sub-window of length 2 l + 1 with weights w j  X  R and power d 1 is defined: Then the kernel on the full window is simply the polynomial of power d 2 of the kernels on the sub-windows: This kernel is called the  X  X ocality-improved X  kernel. An em-pirical comparison to other general purpose machine learn-ing algorithms shows competitive results for l = 0 and better results for larger l . These results were obtained on the above mentioned translation inition site recognition task. Even better results can be achieved by replacing the symbol at some position in the above definition with the conditional probability of that symbol given the previous symbol. Let p i, TIS ( x i | x i  X  1 ) be the probability of symbol x i given symbol x i  X  1 at position i  X  1, estimated over all true translation inition sites. Furthermore, let p i, ALL ( x i the probability of symbol x i at position i given symbol x at position i  X  1, estimated over all candidate sites. Then we define and replace x i + j in the locality-improved kernel by s i + j and the matching kernel by the product. A support vec-tor machine using this kernel function outperforms all other approaches applied in [47]. A kernel function that can be applied in many natural lan-guage processing tasks is described in [4]. The instances of the learning task are considered to be labeled ordered di-rected trees. The key idea to capture structural information about the trees in the kernel function is to consider all sub-trees occurring in a parse tree. Here, a subtree is defined as a connected subgraph of a tree such that either all children or no child of a vertex is in the subgraph. The children of a vertex are the vertices that can be reached from the vertex by traversing one directed edge. The kernel function is the inner product in the space which describes the number of occurrences of all possible subtrees.
 Consider some enumeration of all possible subtrees and let h ( T ) be the number of times the i -th subtree occurs in tree T . For two trees T 1 , T 2 the kernel is then defined as Furthermore, for the sets of vertices V 1 and V 2 of the trees T 1 and T 2 , let S ( v 1 , v 2 ) with v 1  X  V 1 , v 2  X  V 2 of subtrees rooted at vertex v 1 and v 2 that are isomorphic. Then the tree kernel can be computed as Let label ( v ) be a function that returns the label of vertex v , let |  X  + ( v ) | denote the number of children of vertex v , and let  X  + ( v, j ) be the j -th child of vertex v (only ordered trees are considered). S ( v 1 , v 2 ) can efficiently be calculated as follows: S ( v 1 , v 2 ) = 0 if label ( v 1 ) 6 = label ( v if label ( v 1 ) = label ( v 2 ) and |  X  + ( v 1 ) | = |  X  wise 2 , This recursive computation has time complexity O ( |V 1 ||V Experiments investigated how much the application of a ker-nelized perceptron algorithm to trees generated by a prob-abilistic context free grammar outperforms the use of the number of children of a vertex is determined by its label. This is due to the nature of the natural language processing applications that are considered. probabilistic context free grammar alone. The empirical re-sults achieved in [4] are promising. The kernel function used in these experiments is actually a weighted variant of the kernel function presented above.
 A generalization of this kernel to also take into account other substructures of the trees is described in [22]. A substructure of a tree is defined as a tree such that there is a descendants preserving mapping from vertices in the substructure to ver-tices in the tree 3 . Another generalization considered in that paper is that of allowing labels to partially match. Promis-ing results have been achieved with this kernel function in HTML document classification tasks.
 Recently, [44] proposed the application of string kernels to trees by representing each tree by the sequence of labels generated by a depth-first traversal of the trees, written in preorder notation. To ensure that trees only differing in the order of their children are represented in the same way, the children of each vertex are ordered according to the lexical order of their string representation. In [14] a framework has been been proposed that allows for the application of kernel methods to different kinds of structured data. This approach is based on the idea of hav-ing a powerful representation that allows for modeling the semantics of an object by means of the syntax of the rep-resentation. The underlying principle is that of represent-ing individuals as (closed) terms in a typed higher-order logic [29]. The typed syntax is important for pruning search spaces and for modeling as closely as possible the seman-tics of the data in a human-and machine-readable form. The individuals-as-terms representation is a natural gener-alization of the attribute-value representation and collects all information about an individual in a single term. The key idea is to have a fixed type structure. This type structure expresses the semantics and composition of indi-viduals from their parts. The type structure is made up by function types, product types, and type constructors. Func-tion types are used to represent types corresponding to sets, multisets, and so on. Product types are used to represent types corresponding to fixed size tuples. Type constructors are used to represent types corresponding to arbitrary size structured objects such as lists, trees, and so on. The set of type constructors also contains types corresponding to symbols and numbers.
 To define, for example, a type corresponding to a subset of { A, B, C, D } one first defines a type constructor of arity zero, corresponding to elements of this set. Then the type corresponding to a sets of these elements is a function type where the function is from the set of elements to the set of Boolean values. The type corresponding to a multi-set of these elements is a function type where the function is from the set of elements to the set of natural numbers. It is important to note that the elements of these sets, lists, etc. are not restricted to be mere symbols but can again be structured objects. Thus one can, for example, define a type corresponding to a set of lists, or a list of sets. Each type defines a set of terms that represent instances of that type, these are called the basic terms. The terms of the logic are the terms of the typed  X  -calculus, which are formed
A descendant of a vertex v is any vertex that occurs in the subtree rooted at v . in the usual way by abstraction, tupling, and application. Abstraction corresponds to building instances of a function type, tupling to building instances of a product type, and application to building instances of a type constructor. To this end the biggest difference to terms of a first-order logic is the presence of abstractions that allows modeling sets, multisets, and so on. For example, the basic terms s, t representing the set { 1 , 2 } and the multiset with 42 occur-rences of A and 21 occurrences of B (and nothing else) are, respectively: Now, we need some additional notation. For a basic ab-straction r , V ( r u ) denotes the value of r when applied to u , i.e., V ( s 2) = &gt; and V ( t C ) = 0. The default term is the value of the abstraction that has no condition, i.e., the default term of s is  X  and the default term of t is 0. The support of an abstraction is the set of terms u for which V ( r u ) differs from the default term, i.e., supp ( s ) = { 1 , 2 } and supp ( t ) = { A, B } . More details of the knowledge rep-resentation can be found in [29]. The basic term kernel [14] is then defined inductively on the structure of terms. If s, t are basic terms formed by application, i.e., instances of a type constructor, then s is C s 1 . . . s n and t is D t where C, D are data constructors and s i , t j are basic terms. The kernel is then defined as If s, t are basic terms formed by abstraction, i.e., instances of a function type, then the kernel is defined as If s, t are basic terms formed by tupling, i.e., instances of a product type, then s is ( s 1 , . . . , s n ) and t is ( t where s i , t j are basic terms. The kernel is then defined as In [14] additional  X  X odifiers X  are described that allow for the modification of the default kernels in order to reflect the semantics of the domain better. We will now describe some applications along with the type definitions.
 In drug activity prediction it is common to represent the shape of a molecule by measuring the distance from the cen-ter of the molecule to the surface in several directions. Thus a shape can be described by a tuple of real numbers. As, however, the shape of the molecule changes as its energy state changes, each molecule can only be described by a set of shapes (conformations). A molecule is active if one of its conformations satisfies some conditions, this is known as a  X  X ulti-instance X  concept. The task of classifying a molecule as active or inactive given the set of its conformations has been introduced in [6]. In [12] each molecule is represented by a set of conformations (i.e., an abstraction mapping a conformation to  X  X rue X  if it is a member of that set and to  X  X alse X  otherwise), and each conformation is represented by a tuple of real numbers. It can be shown that the correspond-ing basic term kernel is correct with respect to support vec-tor machines and multi-instance concepts. In an empirical evaluation, support vector machines proofed competitive to the best results achieved in literature.
 For spatial clustering it is important to gather data points in a cluster that are not only spatially close but also demo-graphically similar. Applying a simple clustering algorithm to vectors containing both the spatial and the demographic information fails to find spatially compact clusters. Model-ing the spatial and demographic information as two different types and defining the type of the individuals as a function type mapping the spatial information to the corresponding demographic information leads to a kernel function that can be used by a simple clustering algorithm to find spatially compact clusters [14].
 To elucidate the structure of molecules, spectroscopic meth-ods such as 13 C NMR are frequently used. A spectrum contains information about the resonance frequency of each (chemically different) carbon atom in the molecule. Addi-tional information can be obtained describing the number of protons directly connected with each carbon atom (the multiplicity). The task of predicting the skeleton structure of molecules from their NMR spectrum has been introduced in [9]. A spectrum is modeled as an abstraction mapping each resonance frequency to the multiplicity of the corre-sponding carbon atom and mapping every other frequency to zero. A support vector machine using the corresponding kernel function and and a nearest neighbor algorithm using the corresponding metric have been shown to outperform all other algorithms applied in literature to this problem. Labeled graphs are widely used in computer science to model data. Some of the work described above can be seen as kernels on some restricted set of graphs, e.g., on strings or on trees. In this section we consider recent work [13] on labeled graphs with arbitrary structure. Such kernel functions can, for example, be useful in learning tasks on molecules. A graph G is described by a finite set of vertices V and a finite set of edges E . For directed graphs, the set of edges is a subset of the Cartesian product of the set of vertices with itself ( E  X  V  X  V ) such that that (  X  i ,  X  j )  X  E if and only if there is an edge from  X  i to  X  j in graph G . For labeled graphs there is additionally a set of labels along with a function assigning a label to each edge and/or vertex.
 The definition of a complete graph kernel is slightly dif-ferent from the definition of complete kernels given above. In general it is not desired that learning algorithms distin-guish between isomorphic graphs, i.e., graphs that only differ in the enumeration of their vertices. Complete graph ker-nels are those positive definite functions on graphs for which k ( G,  X  ) = k ( G 0 ,  X  ) if and only if G, G 0 are isomorphic. Using a complete graph kernel and computing k ( G, G )  X  2 k ( G, G 0 ) + k ( G, G ) one could decide whether G, G morphic [13]. As deciding graph isomorphism is suspected to be computationally hard, one can conclude that no effi-ciently computable complete graph kernel exists. It can also be shown that some complete graph kernels are NP -hard to compute.
 Consider a graph kernel that has one feature  X  H for each possible graph H , each feature  X  H ( G ) measuring how many subgraphs of G have the same structure as graph H . Using the inner product in this feature space, graphs satisfying certain properties can be identified. In particular, one could decide whether a graph has a Hamiltonian path [13], i.e., a sequence of adjacent vertices that contains every vertex exactly once. Now this problem is known to be NP -hard, i.e., it is strongly believed that this problem can not be solved in polynomial time.
 The two results given above motivate the search for alterna-tive graph kernels which are less expressive and therefore less expensive to compute. Conceptually, the graph kernels pre-sented in [10; 13; 21; 23] are based on a measure of the walks in two graphs that have some or all labels in common. In [10] walks with equal initial and terminal label are counted, in [21; 23] the probability of random walks with equal label sequences is computed, and in [13] walks with equal label sequences, possibly containing gaps, are counted. We consider here the work presented in [13]. In this work, efficient computation of these  X  possibly infinite  X  walks is made possible by using the direct product graph and com-puting the limit of matrix power series involving its adja-cency matrix.
 The two graphs generating the product graph are called the factor graphs. The vertex set of the direct product of two graphs is a subset of the Cartesian product of the vertex sets of the factor graphs. The direct product graph has a vertex if and only if the labels of the corresponding vertices in the factor graphs are the same. There is an edge between two vertices in the product graph if there is an edge between the corresponding vertices in both factor graphs and both edges have the same label. To build its adjacency matrix, assume some arbitrary enumeration {  X  i } i of the vertices. Each component of the adjacency matrix E  X  is defined by [ E  X  ] ij = 1  X  (  X  i ,  X  j )  X  E  X  and [ E  X  ] ij = 0  X  (  X  where E  X  denotes the edge set of the direct product graph. With a sequence of weights  X  0 ,  X  1 , . . . (  X  i  X  R ;  X  i  X  N ) the direct product kernel is then defined as if the limit exists. Using exponential series  X  i =  X  i /i ! or geometric series  X  i =  X   X  i , this kernel can be computed in cubic time. Extensions suggested in [13] include a kernel for counting label sequences with gaps, and one for handling transition graphs, i.e., graphs with a probability distribution over all edges leaving the same vertex.
 One interesting application of such graph kernels is an ex-periment in a relational reinforcement learning setting, de-scribed in [11]. In that paper Gaussian processes were ap-plied with graph kernels as the covariance function. Exper-iments were performed in blocks worlds of up to ten blocks with three different goals. In this setting Gaussian processes with graph kernels proofed competitive or superior to all previous implementations of relational reinforcement learn-ing algorithms, although it did not use any sophisticated instance selection strategy. It has often been argued that relational data mining and inductive logic programming approaches should have suc-cessful propositional learning algorithms as a special case. Support vector machines are one of the most successful re-cent developments within the machine learning area. Thus developing algorithms that can be a applied to structured data and have support vector machines as a special case is a promising research direction. This can be achieved by defining positive definite kernels on structured data. Such positive definite kernels allow algorithms to be applied to structured data that solve learning problems so far not considered by the inductive logic programming community. One example is kernel principal component analysis which finds optimal embeddings of structured data into low dimen-sional spaces. Such an embedding can, for example, be used to visualize the relationship among instances, or to find a good attribute-value representation of the data.
 Support vector machines have many computational and learn-ing theoretical properties that make them a very interesting and popular learning algorithm. Many of these properties follow from the kernel function being positive definite. The problem of finding a hyperplane which is maximally distant from points on either side of the hyperplane can be formu-lated as a quadratic program. Such problems are convex if and only if the matrix they are operating on is positive defi-nite. A convex problem has only one local -and thus global -optimum. This optimum can be found efficiently by well known optimization algorithms.
 In this paper we described several approaches to define pos-itive definite kernel functions on various kinds of structured data. On the one hand, kernel functions that are applica-ble to similar kinds of data have been compared and their respective application areas briefly described. On the other hand, conceptual differences between approaches have been clarified and summarized.
 We believe that investigating kernels on structured data is an important and promising research area. While several kernels have been defined so far, this research area is still very young and there is space for more work. Research supported in part by the EU Framework V project (IST-1999-11495) Data Mining and Decision Support for Busi-ness Competitiveness: Solomon Virtual Enterprise and the DFG project (WR 40/2-1) Hybride Methoden und Systemar-chitekturen f  X ur heterogene Informationsr  X aume . The author thanks Peter Flach, Tam  X as Horv  X ath, John Lloyd, and Stefan Wrobel for valuable discussions and comments. [1] N. Aronszajn. Theory of reproducing kernels. Transac-[2] K. Bennett and C. Campbell. Support vector machines: [3] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training [4] M. Collins and N. Duffy. Convolution kernels for nat-[5] N. Cristianini and J. Shawe-Taylor. An Introduction [6] T. G. Dietterich, R. H. Lathrop, and T. Lozano-P  X erez. [7] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. Bio-[8] S. D X zeroski and N. Lavra X c, editors. Relational Data [9] S. D X zeroski, S. Schulze-Kremer, K. Heidtke, K. Siems, [10] T. G  X artner. Exponential and geometric kernels for [11] T. G  X artner, K. Driessens, and J. Ramon. Graph ker-[12] T. G  X artner, P. A. Flach, A. Kowalczyk, and A. J. [13] T. G  X artner, P. A. Flach, and S. Wrobel. On graph ker-[14] T. G  X artner, J. W. Lloyd, and P. A. Flach. Kernels [15] D. Haussler. Convolution kernels on discrete structures. [16] T. Jaakkola, M. Diekhans, and D. Haussler. A discrimi-[17] T. Jaakkola and D. Haussler. Exploiting generative [18] T. Jaakkola and D. Haussler. Probabilistic kernel re-[19] T. Joachims. Learning to Classify Text using Support [20] R. Karchin, K. Karplus, and D. Haussler. Classifying [21] H. Kashima and A. Inokuchi. Kernels for graph classi-[22] H. Kashima and T. Koyanagi. Kernels for semi-[23] H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized [24] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs [25] S. Kramer, N. Lavra X c, and P. A. Flach. Proposi-[26] M.-A. Krogel and S. Wrobel. Transformation-based [27] C. Leslie, E. Eskin, and W. Noble. The spectrum ker-[28] C. Leslie, E. Eskin, J. Weston, and W. Noble. Mis-[29] J. W. Lloyd. Logic for Learning . Springer-Verlag, 2002. [30] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, [31] H. Lodhi, J. Shawe-Taylor, N. Christianini, and [32] K.-R. M  X uller, S. Mika, G. R  X atsch, K. Tsuda, and [33] G. Paass, E. Leopold, M. Larson, J. Kindermann, [34] P. Pavlidis, T. Furey, M. Liberto, D. Haussler, and [35] L. R. Rabiner. A tutorial on hidden Markov models and [36] C. Saunders, J. Shawe-Taylor, and A. Vinokourov. [37] B. Sch  X olkopf and A. J. Smola. Learning with Kernels . [38] N. Smith and M. Gales. Speech recognition using [39] K. Tsuda, M. Kawanabe, G. R  X atsch, S. Sonnenburg, [40] K. Tsuda, T. Kin, and K. Asai. Marginalized kernels [41] V. Vapnik. The Nature of Statistical Learning Theory . [42] J.-P. Vert. A tree kernel to analyze phylogenetic pro-[43] J.-P. Vert and M. Kanehisa. Graph driven features ex-[44] S. Vishwanathan and A. Smola. Fast kernels for string [45] C. Watkins. Dynamic alignment kernels. Technical re-[46] C. Watkins. Kernels from matching operations. Tech-[47] A. Zien, G. Ratsch, S. Mika, B. Sch  X olkopf, T. Lengauer,
