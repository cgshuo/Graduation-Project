 1. Introduction
Most previous information retrieval (IR) models assume that terms of queries and documents are statistically independent from each another. Although conditional independence assumption is obviously and openly understood to be wrong (Cooper, 1991), many IR models based on this assumption have been developed because the assumption leads to a formal representation of the model more easily, and most IR systems have worked well under this assumption.
Many researchers tried to remove the conditional independent assumption and have incor-porated various term dependence models with diverse techniques (Croft &amp; Harper, 1979; Losee, 1994, 2001; Van Rijsbergen, 1977; Yu, Buckley, Lam, &amp; Salton, 1983; Yu, Luk, &amp; Siu, 1979).
However, when a higher order model of term dependence is used, the easily reached formal representation of the model (in fact, the greatest merit of term independence) cannot be main-has been clarified that incorporation of a term dependence model actually improved the perfor-mance (Bollmann-Sdorra &amp; Raghavan, 1998; Croft &amp; Lewis, 1987; Losee, 1994).
Many approaches that have traditionally been regarded as tools for increasing precision or relaxing the conditional independent assumption use phrases for indexing and retrieval of doc-uments. Phrases have been found to be a useful indexing unit by most of the leading groups participating at NIST and DARPA sponsored Text REtrieval Conferences for performance evaluations of IR systems (Spark Jones, Walker, &amp; Robertson, 1998).

In this paper, we propose a new method to incorporate term dependence into a probabilistic retrieval model by adapting a dependency structured indexing system using a dependency parse tree and the Chow Expansion which was originally used in pattern recognition field (Duda &amp; the Chow Expansion to the original probabilistic model (Robertson &amp; Sparck Jones, 1976) and a statistically significant improvement of the performance for our proposed model in English and
Korean IR systems. The experiments for English were conducted on two different TREC testing collections (AP88 and WSJ90-92). For Korean, the experiments were conducted on a standard document collection of Korean, ETRI-KEMONG (Kemong, 1992).

This paper is organized as follows. In Section 2, we discuss previous researches on diverse techniques to incorporate the term dependences in different retrieval models and compare them with our own research. In Section 3, we describe the Chow Expansion theory and the dependency parse tree, and in Section 4, describe our adaptation of the theory to probabilistic IR models and 2-Poisson models, particularly Okapi BM25. In Section 5, we illustrate our retrieval procedure using the model and describe comprehensive experiment results and analysis. In Section 6, we draw some conclusions and plans for future works. 2. Previous researches
Robertson and Sparck Jones originally proposed a probabilistic retrieval model based on the distribution of query terms in relevant and non-relevant documents (Robertson &amp; Sparck Jones, 1976) (as Eq. (1)), and for which Robertson and Walker presented a formula combining prior weights and pure evidence-based estimates (Robertson &amp; Walker, 1997). The estimated formula actually approximates Inverse Document Frequency (IDF) when there is no relevant information. where d is a document description and rel designates the relevance set. Among probabilistic retrieval models, the 2-Poisson model proposed by Bookstein and
Swanson (Bookstein, 1983; Bookstein &amp; Swanson, 1975) assumes that a content word plays two different roles in documents. The model assumes documents can be divided into two classes: those random, though more frequently in the relevant set than in the non-relevant set. In each case, the randomness is modeled by the Poisson distribution. Robertson and Walker presented an IR model approximating this 2-Poisson model, well known as Okapi BM series (Robertson &amp;
Walker, 1994), which integrate within-document term frequency, document length and within-query term frequency. While these models are widely used in IR, they are based on one important assumption, i.e., a linked dependence assumption (Cooper, 1991): where A , B are regarded as properties of documents, and rel designates the relevance set.
The linked dependence assumption is considerably weaker than the unconditional binary independence assumption, so in most cases, IR systems using the formula based on the linked assumption, and much research has tried to address the limitations of the linked dependence assumption by computing term dependences using diverse techniques on the basis of different retrieval models.

Bollmann-Sdorra and Raghavan showed that, for retrieval functions such as dot products or the cosine used in the vector space model, weighted retrieval is incompatible with term inde-pendence in query space (Bollmann-Sdorra &amp; Raghavan, 1998). They also proved that term independence in the query space even turned out to be undesirable.

Croft proposed an approach to integrate Boolean and statistical systems, where boolean queries are interpreted as a method to specify term dependencies in the relevant set of documents (Croft, 1986). Later, he and Lewis presented an algorithm to generate dependent term groups from their own developed representations (Croft &amp; Lewis, 1987). In the same paper, they showed the performance improvements were gained by re-ranking the top 100 documents based on the dependent term groups.

Losee also proposed a probabilistic model integrating a boolean query in CNF (conjunctive
Bookstein, 1988). Other probabilistic models incorporating the term dependences using the maximum entropy techniques (Cooper &amp; Huizinga, 1982; Paul, 1984) were also proposed.
However, these models require a high cost of computing, so the parameter estimation cannot be performed in real time.
 Previously, Losee incorporated term dependence information in estimating Pr  X  d j rel  X  using the Bahadur X  X azarsfeld Expansion (BLE) (Yu et al., 1979), and documents were ranked by Expected Precision (EP) of the documents (Losee, 1991) as follows: where d is the vector of a document and rel is a relevant set. Losee performed experiments using
Cystic Fibrosis (CF) (Shaw, 1991) for spanning the degree of the terms and showed that the best performance was obtained when degree 3 and  X 3 to  X 5 window of the terms were used. Losee also proposed that the Expected Mutual Information Measure (EMIM) is superior to
Inverse Document Frequency (IDF) for the weighting function (Losee, 2001). In fact, the two measures are actually similar to each other on the theoretical ground based on Luhn (1958) and Zipf (1949) model, but it is meaningful to have some empirical experiment results. Van Rijsbergen explored one way of removing the independence assumption using the Chow
Expansion theory (Duda &amp; Hart, 1973; Van Rijsbergen, 1977, 1979). He constructed a proba-bilistic model incorporating dependences between index terms. The extent to which two index terms depend on one another is derived from the distribution of co-occurrences in the entire collection or in the relevant and non-relevant document sets, and used to construct a non-linear weighting function. In a practical situation, the values of some of the parameters of such a function must be estimated from a small document sample. So a number of estimating rules were discussed and one in particular was recommended.

Turtle described a new formal retrieval model which uses probabilistic inference networks to represent documents and information needs (Turtle &amp; Croft, 1990). Retrieval is viewed as an evidential reasoning process in which multiple sources of evidence about document and query content are combined to estimate the probability that a given document matches a query. This model generalizes several current retrieval models and provides a framework within which dis-parate information retrieval research results can be integrated. The chief advantage of the model is that it allows complex dependencies to be represented in an easily understood form and allows networks containing these dependencies to be evaluated without the development of a closed form expression. However, the model makes only limited use of term dependence information (phrase and thesaurus information) and should be extended to incorporate additional dependencies (e.g., term clustering).

Much of the research done within the TREC Programme on the use of phrases and passages can be seen as seeking to capture dependencies by more informal means, although there may be other motivations as well (Spark Jones et al., 1998). Thus limiting candidate query expansion terms to those occurring in the passage neighborhoods of matching terms can be seen as a way of concentrating on the co-occurrence information so that it is more discriminating than the co-occurrence information computed over extended full texts. 3. Chow Expansion theory and dependency parse tree
In this section, we describe the Chow Expansion theory and the dependency parse tree. 3.1. Chow Expansion corresponding vector x  X f x 1 ; x 2 ; ... ; x n g , where x otherwise. The problem of estimating a density becomes the problem of estimating the probability In this case we can write where p i  X  Pr  X  x i  X  1  X  and 1 p i  X  Pr  X  x i  X  0  X  .

It is natural to ask whether or not any compromise positions exist between complete accuracy, which reduce the problem to one of estimating only n probabilities. One answer is provided by finding an expansion for Pr  X  x  X  and approximating Pr  X  x  X  by partial sum, e.g. the Rademacher X  Walsh Expansion and the Bahadur X  X azarsfeld Expansion (Duda &amp; Hart, 1973).
 the identity Suppose the variables are not independent, but we can number the variables so that
Pr  X  x i j x i 1 ; x i 2 ; ... ; x 1  X  is solely dependent on some preceding variable x that and a corresponding dependence tree as Fig. 1. Then it follows from Eq. (2) that Pr  X  x 1 ; x 2 ; x 3 ; x 4 ; x 5  X  can be written as follows: In general, we obtain the product expansion where the function j  X  i  X  exhibits the limited dependence of one variable on preceding variables. Thus we can write the probability of x i given x j  X  i  X  as follows:
By letting p i j  X  Pr  X  x i  X  1 j x j  X  i  X   X  0  X  X  Pr  X  x logarithm, and collecting terms, we obtain the Chow Expansion (Chow &amp; Liu, 1968). independent, p i j X   X  p i j and the last two sums in the expansion disappear, leaving the familiar expansion for the independent case. When dependence exists, we obtain additional linear and quadric terms.

Chow and Liu base their probabilities on a maximum spanning tree based on the mutual information measure (Chow &amp; Liu, 1968), but in this paper, the tree will be created using grammatical connections, as described below. 3.2. Dependency parse tree
A dependency relationship (Hays, 1964) is an asymmetric binary relationship between a word called a head (or governor, parent), and another word called a modifier (or dependent, daughter).
Dependency grammars represent sentence structures as a set of dependency relationships. Nor-mally the dependency relationships from a tree connects all the words in a sentence. A word may have several modifiers, but each word may modify no more than one word. The root of the dependency tree does not modify any word. It is also called the head of the sentence. For example, pairs of dependency relationships, depicted by four arcs, from heads to modifiers.
For English, in order to apply dependency parse trees to the Chow Expansion, we use Minipar as a dependency parser, which is a principle-based English parser (Lin, 1994; Minipar, 1998).
Minipar represents its grammar as a network, where nodes represent grammatical categories and links represent types of dependency relationships. Minipar parses about 300 words per second on a Pentium II 300 with 128 MB memory (Minipar, 1998).

We developed a simple dependency parser for Korean. Our dependency parser uses some heuristics which are generally used in dependency parsing (Kurohashi &amp; Nagao, 1994) (e.g. non-crossing condition, constraint under surface information and nearest modifiee principle). Our dependency parser shows approximately 70% precision.

Chow and Liu propose the construction of a MST using mutual information for a dependence tree which was originally used in the Chow Expansion. However, we propose to use a dependency parse tree which is generated by a linguistic dependency parser instead of mutual information
MST, because a dependency parse tree intuitively and linguistically represents the term depen-dence relations in syntactic structure, which helps to capture the underlying semantics of a document. 4. Adapting to probabilistic IR models
We propose a method of incorporating the term dependence into probabilistic models, spe-cifically 2-Poisson models, using Chow Expansion and a dependency parse tree. We consider the use of dependency relation as one way to relax the independence assumption of the terms. So, we develop a dependency structured indexing system which consists of the dependency parse tree and
Chow Expansion to relax the conditional independence assumption. 4.1. Adapting the Chow Expansion (5)) as follows: are probabilities over whole collection without consideration of relevance in Eq. (5). In the equations above, we note that if the variables are indeed independent, p sums again disappear. From the equations above, we can adapt the Chow Expansion to the probabilistic IR model (Robertson &amp; Sparck Jones, 1976) as follows:
Since Pr  X  x i  X  1 j x j  X  i  X   X  1 ; rel  X  X  Pr  X  x i  X  1 ; x and q i j Pr  X  x i  X  1 j rel  X  X  q i , Eq. (6) is transformed as follows: q term i and term j  X  i  X  given that it is relevant, and q i ; j  X  i  X  document. We define MS prob Chow  X  R  X  , a query X  X ocument scoring function which adapts the Chow
Expansion to the probabilistic model as Eq. (7). This model consists of linear and quadratic terms same as the independence model which consists of only linear terms. And the model is similar to models which use the weight of phrases as a single term weight, if we ignore the second sum; that over-scored phrase weight, because our model subtracts the weight of single terms from the weight of phrases. So we can overcome the well-known anomaly that any document containing the phrase and a single term (Spark Jones et al., 1998). 4.2. Using no relevance information
Consider a collection of N documents and assume that R documents out of N are relevant to a given query and N R documents are non-relevant. In the probabilistic retrieval model, if the relevant information is not available, we can generally assume that R N . Therefore, we assume the following approximation: where n i is the number of documents in which the term t i documents in which the term t i occurs. We also assume that p 1997). Then we can write the coefficient of x i in Eq. (7) (i.e. the original Spark Jones inverse collection frequency weight) as follows:
Furthermore, we assume the following approximations: where n j  X  i  X  is the number of documents in which the term t documents in which t i and t j  X  i  X  occur as a head X  X odifier grammatical relation (i.e. t t in a dependency parse tree for the sentences in documents). Then we approximate the coeffi-cients of x j  X  i  X  in Eq. (7), respectively, as follows: where k 7 is an unknown constant parameter. We also approximate the coefficient of x
Eq. (7) as follows: where k 8 is another unknown constant parameter.

From the above assumption and approximation, we adapt the Chow Expansion into the probabilistic retrieval model without relevance information as follows:
We define MS prob Chow  X  N  X  , a query X  X ocument scoring function which adapts the Chow Expansion to the probabilistic retrieval model without relevance information as Eq. (8). 4.3. Extending to the 2-Poisson model
Our method can be extended to incorporate term dependence into a state-of-the-art 2-Poisson model (Robertson &amp; Walker, 1994), specifically Okapi BM25 (Robertson, Walker, Jones, Han-cock-Beaulieu, &amp; Gatford, 1995), using Chow Expansion. The weight of a term t in a 2-Poisson model is represented in the following equation (10) (Robertson &amp; Walker, 1994): sets for term t respectively, p 0  X  Pr  X  elite document for t j rel  X  , and q non-relevant document. Normally, l is smaller than k ,soastf !1 (to give the asymptotic small, so the approximation will be:
Since we cannot estimate Eq. (10) directly, the alternative way is to use w above results, Eq. (10) can be transformed into a simple formulation, such as BM25 (Robertson et al., 1995), as given below. a average length of the documents, and k 1 and b are constant parameters.
 From Eq. (12), we define MS BM25 Chow  X  N  X  , a query X  X ocument scoring function which adapts the Chow Expansion to the 2-Poisson model without relevance information as follows: where d is a document, t is a query. 5. Experiments and performance evaluation In this section, we will empirically demonstrate that the 2-Poisson model incorporating the
Chow Expansion and dependency parse tree based term dependences (Eq. (13)) actually gives a significantly better performance than the 2-Poisson model under the conventional linked dependence assumption. 5.1. Test collection We used two different TREC testing collections for evaluation: AP88 (Associated Press, 1988),
WSJ90-92 (wall street journal from 1990 to 1992) for English. We used TREC4 queries (202 X 250) and their relevance judgements for evaluation. We excluded the TREC topic 201 from the
For Korean, we used the ETRI-KEMONG test collection which is a Korean encyclopedia published by the Kemong Company (Kemong, 1992). It is published in six volumes with 500 pages per volume. The text data contains 23,113 entries, and its size is about 10 mega-bytes. The content of each entry describes the concept with other entries or more fundamental words. The test set manually constructed by ETRI 1 contains 46 natural language queries and the relevance information of the entry lists related to each query which were manually selected by ETRI human assessors. The average document length of the test set is 56 words. The average number of relevant documents of the test set is 9. We used the 46 ETRI-KEMONG queries and their relevance judgements for our evaluation.

We used relatively small test collections, because of the cost of parsing the entire set of test collections. The dependency parse tree of the user query is obtained by the dependency parser at search time and the dependency relation information between the two terms in a document is obtained by the dependency parser at indexing time. 5.2. Experiments and the results
The goal of the experiments is to validate the proposed model. Fig. 3 shows a scheme of the dependency structured indexing system. Using a dependency structure analysis of the documents, we extract single terms and head X  X odifier relations for indexing. In retrieving, keyword extrac-tion from the query is performed in the same manner as in the indexing process.
The followings are brief descriptions of the two methods tested and compared.  X  BM25: Conventional Okapi BM25 model.  X  BM25-Chow( N ): Proposed method (Eq. (13)) using the Chow Expansion and dependency parse trees.
The 1000 top-ranked documents retrieved by the system for English and Korean were used to evaluate performance. The performance was assessed by  X  trec_eval  X  , the standard evaluation program of SMART project (TREC Eval, 1992). The performance measures used in the result table are average precision (non-interpolated) over all relevant documents (AvgP), Precision at 5, 10, 20, 100, 500 documents (P@5, P@10, P@20, P@100, P@500, respectively) and R-Precision (R-P).
 The results on AP88 and WSJ90-9 2test collection are shown in Tables 1 and 2respectively. Table 3 shows the result on ETRI-KEMONG test collection.

As seen from Table 1, the proposed method shows a significant gain in performance. The proposed method achieved about a 4.3% improvement compared to conventional BM25 in the average precision. The proposed method achieved about 11.4% improvement compared to BM25 in the precision at 5 documents (P@5). One notable fact is that the performance of the proposed method includes the errors of Minipar parsing in the given corpus. We hope we can achieve much better performance if we can improve the dependency parsing accuracy, which is about 85% in the current Minipar system.

Table 2shows that the proposed method has also a significant gain in performance. The proposed method achieved about a 4.8% improvement compared to BM25 in the average pre-cision. The performance of the proposed method again includes the errors of Minipar parsing in the given corpus.

In Table 3, the proposed method also has a gain in performance. The proposed method achieved about a 2.4% improvement compared to BM25 in the average precision. The accuracy of the parsing is about 70% in the current Korean dependency parser system.

Fig. 4 shows the average precision of each topic, from 1 to 25, on the ETRI-KEMONG col-lection. The effect of the proposed method can be shown clearly at topic 2, 3, and 5. For example, the average precision of topic 5 was elevated from 0.9167 to 1.0 by the proposed method.
From the above results, we can confirm that the 2-Poisson model incorporating term depen-dence with Chow Expansion techniques and dependency parse tree improves the performance both in English and in Korean. 6. Conclusion and future work
We presented a new method to incorporate term dependences in a probabilistic retrieval model to compensate for the weakness of the linked dependence assumption. We also presented a new weight function for dependency-based phrase terms using the Chow Expansion and dependency parse tree as applied to the BM25 function, the widely used 2-Poisson model. Co-occurrence between the two terms was obtained from the dependency parse tree.

We carried out some experiments to verify the proposed models. The experiments were per-formed on several test collections (AP89, WSJ90-92, and ETRI-KEMONG) of English and
Korean to make observations of practicality and usefulness. From the results, an improvement of the performance was obtained on the document collections by incorporating term dependences using the Chow Expansion and dependency structured indexing system.

We can conclude that incorporating term dependences using the Chow Expansion and dependency structured indexing system into a 2-Poisson model is a viable and appropriate technique to overcome the weakness of the linked dependence assumption model.

The greatest disadvantage in using the Chow Expansion is that the retrieval cost of dependence tree increases because the dependence tree of the user query is obtained by a dependency parser at the search time and co-occurrence information between the two terms are obtained by a depen-required in the future.

Another future project will be to apply the Chow Expansion to Auto Relevance Feedback (ARF). Many research efforts on query expansions using ARF have verified a significant per-formance improvement. But it is not yet known whether the Chow Expansion techniques work well on the ARF expanded queries or not, and this question is another research direction con-cerning the Chow Expansion based term dependency model.
 References
