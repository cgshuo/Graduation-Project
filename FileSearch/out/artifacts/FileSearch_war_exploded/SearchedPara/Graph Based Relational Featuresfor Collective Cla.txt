 Statistical relational learning (SRL) methods are used when samples are con-nected by one or more relation. These relations are helpful in tasks like sci-entific article classification where patterns ssuch as  X  X onnected samples have similar labels X  are very predictive. Feature based ML methods in contrast often assume that samples are independently, identically distributed (iid). This app-roach is well established, allows for efficient parameter estimation and simpli-fied prediction but ignores the relational information available in SRL settings. Recently, a number of methods have been proposed [ 11 , 13 , 18 , 31 ] that signifi-cantly improves over classical methods by using joint inference. Exact joint infer-ence have high runtime complexity which often requires approximate solutions [ 30 ]. These approximated joint inference techniques introduce new difficulties such as the need for specialized implementations that are expensive to run and difficult to tune [ 30 ].
 features. This allows a straightforward combination of relational and classical (attribute) information. It also renders traditional, feature based ML methods competitive in settings where relational information is available and allows to leverage the large body of classical ML methods and their scalable algorithms. We use three standard collective classification (CC) benchmark datasets to show that classical ML with relational features are strong competitors for state of the art SRL methods on this task. Note that on these datasets, collective clas-sification are considered the best performing methods in the current literature [ 7 , 14 ]. In particular, we make the following contributions:  X  We discuss how joint inference could be avoided by extending the sample description with relational features (Section 2.1 ).  X  We extend relational features to indirect relations (Section 2.2 ). This is new and crucial to achive high accuracy when only few samples are labeled (Section 4.3 ).  X  We introduce a new cluster based relational feature (Section 2.2 ) that provides strong results and is cheap to compute.  X  We show that our approach improves state of the art collective classification even in network only settings (Section 4.2 ). We start by giving the necessary definitions with the traditional setting of sam-ples
D = { ( x describing sample i . We assume that for the first u samples ( the class label y i is known and for the samples with index label is unknown. Relations among samples are represented by weighted, sym-metric adjacency matrices R k  X  R n  X  n and the complete relational information is denoted as R = { R k } K k =1 . While in the general case, each of the could be complete, i.e. provide some similarity between each pair of samples, we explicitly consider the case of sparse, unweighted relations where only a minority of node pairs are connected by an edge. We start with a statistical argument to motivate the representation of relational information in a way that is compatible with the iid assumption. 2.1 Preserving IID Many machine learning algorithms are based on the maximum likelihood prin-cipal to learn the optimal value  X   X  for model parameters given a dataset (c.f. [ 17 ]) where l (  X  ):= p ( D|  X  ) is called the likelihood. A very common assumption in many ML approaches is that the samples in the dataset D are independent and identically distributed (iid). This assumption simplifies the likelihood to One of the central arguments of relational learning is that examples are not iid. In particular for any pair of examples ( x i ,y i ) and ( x j does not hold Note that in this formulation the relational information neglected. However if R is used, it can render the probabilities independent This formulation is close to standard non-relational ML with iid of samples. be encoded in a (usually real-valued) feature vector x . Let us assume that the influence of R on sample i can be described through a finite number of real define  X  x i as the extended feature vector of an example Equation 1 Note that due to the relational information in  X  x i , the iid assumption can be preserved. In the remainder of this section, we discuss several ways to generate relational features. 2.2 Graph Based Relational Features Samples can be linked through multiple relations each of which can be described as a graph. Representing each relation by an independent set of features allows us to integrate an arbitrary number of relations per problem into a standard feature matrix. All of the proposed features have in common that the encoded relational information does not only consist of direct relations but in addition captures indirect relations which we found to be the key to their performance. Neighbor Ids. Encoding the direct neighbors of a sample i can be achieved by treating each sample as a categorical variable which is true when samples are connected and false otherwise [ 1 , 24 , 25 ]. As this information is very local and yields limited information about i  X  X  position in by additionally including indirect neighbors at various distances to refers to the number of edges d k ( i, j ) on a shortest path connecting R k . In particular, for a (small) set of distances R k by the distance-if d Figure 1 . Aggregated Neighbor Attributes. Relational position can also be described by individual features of direct and indirect neighbors [ 20 , 21 ]. As before we extend the idea by calculating individual features at various distances. For a categorical attribute with categories 1 ,...,c , we define an L =1if x i is labeled as j and L i,j = 0 otherwise. Then the count matrix C =: D d k L can be derived as the projection of the corresponding neighborhood matrix to the label matrix L such that ( C d k ) i,j yields the number of category nodes in distance d of sample i . We denote this feature by neighbor class counts (NCC) and provide an illustration in Figure 2 . An additional row normalization yields a probability matrix for the class labels in distance by neighbor class probabilities (NCP).
 Random Walk Similarity. While the features described above are based strictly on shortest paths, random walks with restart (rwr) incorporate a dif-ferent notion of connectivity. They have been proposed as a similarity measure in the context of auto-captioning of images [ 22 ].
 The similarity between two nodes is measured as the probability of a random walk connecting them, i.e. the probability of the random walk process visiting one node when started from the other. To control for locality this includes a restart probability: in each step the walk will jump back to the starting point with probability r . This can be modeled as where the column vector p i of matrix P describes the steady-state probability distribution over all samples for walks starting at i . W transition probabilities, i.e. a L 1 row normalized version of zeros and unit value at position i and the parameter r is the restart probability. P can be determined as the solution of a linear system or approximated effi-ciently [ 32 ], leaving r as free parameter. We derive  X  P normalization and use analogous to the neighbor features row for sample i .
 Clustering Memberships. Clustering methods can be used to identify groups of similar samples. Clustering features encode this information by representing this group membership. Given a clustering of the graph representing relation R k into sample i belongs to cluster m and zero otherwise. Since a single clustering yields limited information about the dense groups in the graph, we create features for various clusterings, i.e. different c . We limit c to c =2 which limits the number of clusterings to log 2 ( n ) while also providing a wide range of cluster sizes. This results in O ( n ) features per relation that are very sparse with only O (log( n )) non-zero features for per sample. The clusters can be calculated with negligible runtime using the METIS clustering framework [ 8 ]. Note that the dense subgroups identified in the clusterings can be directly related to the homophily assumption often exploited in relational learning. We build on two main categories of related work. The first, in Section 3.1 uses fea-tures derived from the network structure to improve iid based inference. The sec-ond, discussed in Section 3.2 is work that views collective classification as a joint inference problem, simultaneously inferring the class label on every instance. The challenges specific to problems with few labeled data points have received spe-relations. 3.1 Relational Features Relational features can be combined with collective inference or directly used with standard ML methods as we argue in Section 2.1 . Models such as Relational Probabilistic Trees [ 20 ], Relational Bayes Classifier [ 21 ] and the Link Based Classifier (LBC, [ 11 ]) concentrate primarily on the aggregation of attributes of connected samples. Others use rows from the (weighted) adjacency matrix as basis for feature construction [ 1 , 24 , 25 ]. We were especially inspired from the suggestion to extend the neighborhood of samples with few neighbors with distance-two neighborhoods [ 26 ] or ghost edges [ 5 ]. In contrast to previous work we keep the information from various neighborhood distances separated and introduce the concept of multiple indirect relations. 3.2 Collective Inference Full relational models such as Markov Logic Networks (MLN, [ 28 ]) or the Prob-for a comprehensive overview of collective inference based CC algorithms. Their strength in high label autocorrelation settings and the problem of error prop-agation has been examined [ 7 , 33 ] and improved inference schemes have been proposed [ 15 ]. Recently, stacking of non relational model has been introduced [ 9 ] as a fast approximation of Relational Dependency Networks [ 19 ]. In our experiments we investigate the following three questions: 1. Are classical ML methods with relational features competitive to MLN and Collective Classification approaches. 2. What are the main ingredients that make relational features effective. 3. Does the combination of relational and attribute information improve results? 4.1 Experimental Setup Datasets. We use three standard benchmark SRL datasets. The Cora and Cite-Seer scientific paper collections have been used in different versions, we chose the versions 2 presented in [ 30 ] and the IMDb dataset 3 include text features in form of bag of words (bow) representations. We give some statistics of these datasets in Table 1 .
 Benchmark Models. As baseline models we use the well established relational learning methods wvRN [ 12 , 13 ], nLB [ 11 ]andMLN[ 3 ]. We chose relaxation labeling [ 2 ] as the collective inference method for wvRN and nLB as it has been shown to outperform Gibbs sampling and iterative classification on our datasets [ 14 ]. For MLN we used the rules HasWord(p,+w)=&gt;Topic(p,+t) and Topic(p,t)^Link(p,p X )=&gt;Topic(p X ,t) together with discriminative weight learning and MC-SAT inference as recommended for Cora and Citeseer in a previous study [ 3 ]. Measures and Protocol. We follow [ 14 ] and remove samples that are not con-nected to any other sample (singletons) in all experiments. Each experiment is repeated 10 times with class-balanced splits into ratios of 0 train-test set (shown as percentage in the figures). The MLN experiments are only repeated 3 times due to their extremely high evaluation time. We calculate multi-class accuracies by micro averaging and plot them on the y-axis of each figure in this section. We used the netkit-srl framework [ 14 ] in version (1.4.0) to evaluate the wvRN and nLB classifiers and the Alchemy 2.0 framework evaluate the MLN model. The graph clusterings were obtained using METIS [ 8 ] version 5.1.0. Relational features are learned with an L 2 sion model 6 included in scikit-learn [ 23 ] version 0.14.01. The penalty hyperpa-rameter C is optimized via grid search over { 0 . 001 , 0 the training set. 4.2 Comparing Relational Features to SRL This section examines whether feature based relational models (without collec-tive inference) are able to compete with the prediction quality of specialized relational learning methods. Consequently, the benchmark is a task where only relational data is available. In the first experiment, we compare wvRN and nLB with two logistic regression models that use only our relational features. The first relational feature model (rwr) is based on a random walk. The second model uses both, neighborhood and aggregated (NCP) features with distances 1,2,3. We exclude distances higher than three, since almost every node can be reached over three edges from every other node and therefore further distances do not provide additional information.
 when labels are sparse and (ii) wvRN is sensitive to violations of its built in assumptions  X  i.e. if label consistency among neighbors is not met, as with the IMDb dataset.
 much affected by the number of labeled samples. The results of the neighbor and NCP feature combination on IMDb illustrate the flexibility of relational features. 4.3 Engineering Relational Features We now examine the different relational features and the influence of their parameters. Two questions will be addressed: (i) How important are indirect relations? (ii) Which of the proposed relational features lead to the best results? All relational features that we consider can incorporate information about indi-rect neighbors. Each method has parameters that adjusts the locality of the resulting features. We first examine the effect of including indirect neighbors (Figure 5 ) and the importance of unlabeled neighbors (Figure 6 ) for relational neighbor features. The influence of the restart parameter the rwr features can be seen in Figure 7 and an informative subset of results for various numbers of clusters is shown in Figure 8 . The results suggest that the inclusion of indirect neighbors in the relational features is beneficial inde-pendently of whether they are used directly or for aggregation. Figure 6 shows that unlabeled neighbors contribute significantly to the overall performance. Together this answers our first question: unlabeled samples and indirect neigh-borhood relations are essential ingredients for relational features. Regarding the second question, the results show that the choice of relational features depends on the particular problem. 4.4 Combining Relational and Local Information In the following, we examine the effect of adding local attributes. Figure 9 shows results with neighborhood count features (NCC) of distances 1,2,3. Interestingly, the bag of words model performs better than network only models on Citeseer but worse on Cora. Combining relational and local attributes on the other hand, improves results in both cases. The figure further shows that our features out-perform MLN on both datasets. In summary, our experiments suggest that the combination of relational features and attributes is beneficial even with a simple model such as logistic regression. 4.5 Discussion Our experiments indicate that relational feature based models compare well to specialized relational learners even in network only and sparse labeling settings. This has been verified on three standard SRL benchmark datasets and with three state of the art SRL methods for comparison. The inclusion of indirect neighbors has proven extremely important, especially in sparse label settings. We have further shown that the combination of relational features and local attributes is both straightforward and has the potential to improve considerably over both, feature only and network only models.
 Note, that our relational features can lead to very high dimensional repre-sentations. Such feature spaces are, however, common in recommender systems, click-through rate prediction and websearch where regularized logistic regres-sion has been shown to be very effective [ 6 ], [ 10 ]. In addition, we use a standard implementation of logistic regression and can consequently employ scalable ver-sions that can be trained with billions of samples of high dimensions [ 16 ][ 27 ]. We are further not committed to the logistic regression model as our features could be used as input for arbitrary vector space models. We have shown that dependencies between samples can be exploited using rela-tional feature engineering. Our method allows to combine relational informa-tion from various sources with attributes attached to individual samples. We tested this on standard SRL benchmark datasets, showing that even on network only data our features are competitive to specialized relational learning mod-els. In addition, our features can outperform them when additional information is available. Note that in contrast to the SRL methods, our proposal achieves these results without collective inference. While we restricted our experiments to logistic regression as prediction model, the proposed features could be used as input to any other feature based learning algorithm such as SVM, neural networks or random forests. Extending the use of relational features to multi relational datasets would be straight forward and a interesting direction for fur-ther research.

