 Recent years have witnessed a persistent interest in generating pseu-do test collections, both for training and evaluation purposes. We describe a method for generating queries and relevance judgments for microblog search in an unsupervised way. Our starting point is this intuition: tweets with a hashtag are relevant to the topic cov-ered by the hashtag and hence to a suitable query derived from the hashtag. Our baseline method selects all commonly used hashtags, and all associated tweets as relevance judgments; we then gener-ate a query from these tweets. Next, we generate a timestamp for each query, allowing us to use temporal information in the training process. We then enrich the generation process with knowledge derived from an editorial test collection for microblog search.
We use our pseudo test collections in two ways. First, we tune parameters of a variety of well known retrieval methods on them. Correlations with parameter sweeps on an editorial test collection are high on average, with a large variance over retrieval algorithms. Second, we use the pseudo test collections as training sets in a learning to rank scenario. Performance close to training on an edi-torial test collection is achieved in many cases. Our results demon-strate the utility of tuning and training microblog search algorithms on automatically generated training material.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models Simulation, pseudo test collections, learning to rank, microblog re-trieval
Modern information retrieval (IR) systems have evolved from single model based systems to intelligent systems that learn to com-bine uncertain evidence from multiple individual models [ 10 , 22 ]. The effectiveness and flexibility of such systems has led to wide adoptation in IR research. A key contributor to the success of such systems is the learning phase, i.e., the training set they are given for learning. Training sets have to be tailored to the task at hand and, in contrast to the systems themselves, do not generalize to other tasks. This characteristic requires compiling task-specific training sets, which is a time consuming and resource intensive process, as it usually involves human labor. Automating the process of com-piling training sets has obvious advantages in reducing costs, while it simultaneously increases the size of the training set. This obser-vation has led to a persistent interest in finding ways for generating so-called pseudo test collections , which consist of a set of queries, and for each query a set of relevant documents (given some doc-ument set). In this paper, we consider the problem of generating pseudo test collections for microblog search.

Microblog search is the task of finding information in microblogs, such as Facebook status updates, Twitter posts, etc. The task be-came popular with the advent of social media and is distinct from web search and from blog search due mainly to its real-time nature, the very limited length of microblog posts and the use of  X  X icro-blog language, X  e.g., hashtags, mentions, which can provide useful information for retrieval purposes. In 2011 the Text REtrieval Con-ference (TREC) launched the Microblog track aimed at developing a test collection from Twitter data and evaluating systems X  perfor-mance on retrieving X  X iven a query and time-stamp X  X elevant and interesting tweets in a simulated real-time scenario. Several par-ticipants approached the task using learning to rank methods for combining evidence from multiple rankers [ 21 ]. This approach to microblog search comes natural because of the many dimensions available for ranking microblog posts, e.g., recency, user authority, content, existence of hyperlinks, hashtags, retweets. For training a learning to rank-based system at the TREC 2011 Microblog track, participants used a traditional supervised method: many manually labeled data for compiling a training set. What if we could generate the required training sets automatically? In 2012 a second edition of the Microblog track was organized. This gives us the opportunity to compare what yields better learning to rank performance: train-ing on the 2011 relevance assessments, or training on automatically generated ground truth?
Our starting point is the following intuition, based upon the ob-servation that hashtags tend to represent a topic in the Twitter do-main: From tweets T h associated with a hashtag h , select a subset of tweets R h  X  T h that are relevant to an unknown query q lated to h . We build on this intuition for creating a training set for microblog rankers. To this end, we take several steps, each raising research questions. First, we select hashtags h and associated rel-evant tweets R h . Can we just select all hashtags and use all their associated tweets? In microblog search, time is important: what is considered relevant to a query may change rapidly over time. A microblog query, then, has a timestamp, and relevant tweets must occur prior to this timestamp. As for a query, the topic a hashtag is associated with may change over time. Can we exploit this analogy, and label hashtags with a timestamp, regarding tweets prior to this timestamp as relevant? Another well-known aspect of microblog posts is that they often contain casual conversation that is unlikely to be relevant to a query. Can we improve generated training sets by selecting interesting tweets and hashtags associated with such tweets? Once we have selected a hashtag h and a set of tweets R how do we generate a query q h related to h ?
The main contribution of this paper is a set of methods for creat-ing pseudo test collections for microblog search. These collections are shown to be useful as training material for tuning well-known retrieval methods from the literature, and for optimizing a learn-ing to rank method. In particular, we contribute: (1) unsupervised pseudo test collection generation methods; (2) a supervised pseudo test collection generation method, where we learn what are inter-esting tweets from TREC Microblog track assessments; (3) insights into the sensitivity of our methods to parameter settings.
Below, we consider a number of instantiations of our pseudo test collection generator. For the purposes of the TREC Micro-blog track, a test collection for microblog search consists of queries with timestamps and a set of relevant documents for these queries. A pseudo test collection for microblog search consists of a set of queries Q , in which each query q  X  Q is associated with a times-tamp q t and a set of relevant documents R q . Given this definition, there are three main steps for generating a pseudo test collection for microblog search: generating (a) the query; (b) the query times-tamp; and (c) a set of relevant tweets for the query.

We start from the following intuition: From the tweets T h contain a hashtag h , we can select tweets R h that are relevant to an unknown query q h related to h . In the next section, we present three methods to generate a pseudo test collection. Each method selects hashtags and for every hashtag h it selects tweets R T h that will act as relevant tweets to a suitable query related to h . In  X 4, we present a technique for generating queries from R
We propose four solutions to selecting hashtags and tweets for inclusion in a pseudo test collection. (A) Random: A sanity check baseline against our hypothesis that hashtags are good sources for generating pseudo test collections. Collections are created by ran-domly sampling a set of relevant tweets for each topic, without replacement. All these random collections are of a fixed size, equal to our largest hashtag-based pseudo test collection. (B) Hashtags: A naive method that serves as baseline in our experiments and that considers all hashtags and tweets to be equally important ( X 3.1). (C) Hashtags-T: A method that creates a test collection in the microblog retrieval sense, in which queries have timestamps ( X 3.2). (D) Hashtags-TI: A method that aims at capturing interestingness in tweets. Interesting tweets should contain good candidate terms for a query. We present a method with which we can estimate from example queries and relevant tweets the probability of interesting-ness of a tweet ( X 3.3).
We select all hashtags, with a single requirement: that they are mentioned in a reasonable amount of tweets, m (Algorithm 1). There are three reasons for this lower bound: (i) it reflects a cer-tain consensus about the meaning of a hashtag; (ii) we generate Algorithm 1: Generating collection Hashtags
H  X  X  X { h : | T h | &gt; = m } ; for h  X  H do end our queries based on word distributions in these tweets: for this to work reliably, we need a reasonable amount of tweets, see  X 4; (iii) we train a learning to rank retrieval algorithm on our pseudo test collection; we hypothesize that it would benefit from a rela-tive large set of positive training examples, see  X 5. We normalize hashtags by lowercasing them and removing any non-alphanumeric characters. In all our experiments, we set m = 50 . The pseudo test collection generated by Algorithm 1 is called Hashtags .
Microblog search is sensitive to the query issue time because of the real time nature of tweets. To generate a timestamp for a query related to a hashtag h , we make an analogy between search volume over time for a query and publishing volume over time for tweets with h . Our assumption is that users often issue queries for trending topics because they want to monitor developments, similar to cer-tain types of blog search [28]. We generate a timestamp for hashtag h just after peaks in publishing volume. In this way, our generated queries will be about trending topics. In addition, we keep a large amount of tweets from T h , while discarding a limited number, after h stops trending. In collections that span a considerable period of time, re-occurring topics, such as Christmas or Super Bowl, may quite likely be observed. In this case, one may want to assign mul-tiple issue times for a query, depending on the number of observed peaks. Our corpus (see  X 5) covers a relatively short period, and we assign only one issue time to every query sampled.

In detail, our query issue time generation works as follows. First, we group the time span of the collection in 8-hours bins. Then, for each hashtag, we count how many relevant documents belong to each bin; this results in generating the hashtag X  X  timeseries. In our setting, timeseries are short and sparse; our peak detection method aims at coping with this challenge. We find the bin with the most counts and resolve ties by taking the earliest date. This approach allows us to return a peak even for very sparse timeseries. We call the pseudo test collection generated by Algorithm 2: Hashtags-T .
Consider the following tweet:  X  X ey follow me here #teamfol-lowback #justinbieber. X  We hypothesize that this tweet would not be useful for sampling terms for topics labeled #teamfollowback or #justinbieber or as a relevant document for these topics. To avoid selecting such tweets, we rank tweets by their probability of in-terestingness and keep the best X percent. We think of a tweet Algorithm 2: Generating collection Hashtags-T
H 0  X  X  X { h : | T h | X  m } ; for h  X  H 0 do end
H  X  X  X { h : h  X  H 0 and | R h | X  m } ; for h  X  H do end as interesting if it carries some information and could be relevant to a query. We use a set of criteria to capture interestingness and present a method to learn from example queries and relevant docu-ments from an editorial collection.

Let C 1 ,C 2 ,...,C n be random variables associated with the cri-teria and let I  X  := I (  X  ) = 1 be the event that a tweet is interest-ing. We estimate P ( I  X  | C 1 = c 1 ,...,C n = c n ) , or, shorthand: P ( I  X  | c 1 ,...,c n ) . Following Bayes X  rule, we have where P ( I  X  ) is the a-priori probability that a tweet is interest-ing, P ( c 1 ,...,c n | I  X  ) is the likelihood of observing the evidence given that a tweet is interesting, and P ( c 1 ,...,c n ) is the proba-bility of observing the evidence. The crucial step is to estimate P ( c 1 ,...,c n | I  X  ) . We hypothesize that tweets that are known to be relevant to a query are interesting and estimate P ( c 1 , . . . , c with P ( c 1 ,...,c n | R  X  ) , where R  X  is the event that a tweet is rele-vant to a query in an editorial collection. We use the TREC Micro-blog 2011 qrels for this estimation. Since we do not have enough relevant tweets to estimate the full joint probability, we assume con-ditional independence of c i given that a tweet is relevant: Since we rank tweets by interestingness we do not have to estimate P ( I  X  ) . Instead, we have: rank  X  ( P ( I  X  | c 1 ,...,c n )) = rank  X  Most of the criteria we use have discrete distributions. For those that do not, we bin their values in B bins; we set B = 10 . To avoid rejecting a tweet on the basis of one measurement that did not occur in any of the relevant tweets, we add one observation to every bin of every P ( c i | R  X  ) distribution. For P ( c use the empirical distribution of all tweets in the collection. After selecting the best X percent of tweets, we again filter out hashtags that have less than 50 interesting tweets. We build this method on top of our pseudo test collection Hashtags-T , only ranking the tweets in this collection and keeping the best 50% of them. We call the pseudo test collection generated by Algorithm 3 Hashtags-TI . Algorithm 3: Generating collection Hashtags-TI
H 00  X  X  X { h : | T h | X  m } ; for h  X  H 00 do end H 0  X  X  X { h : h  X  H 00 and | T h,t | X  m } ; // Rank tweets by probability of being Rank T by P ( I  X  | c 1 ,...,c n (  X  )) ; // See eq. 3
Let T I be the top X percent of this ranking; for h  X  H 0 do end
H  X  X  X { h : h  X  H 0 and | R h | X  m } ; for h  X  H do end
The criteria we use build on textual features ( density and capi-talization ) and microblog features ( links , mentions , recency ). Each criterion is discussed below. The marginal distributions P ( c of three criteria are shown in Fig. 1 as white histograms. They over-lap with black histograms of all tweets in our Hashtags pseudo test collection. These criteria have different distributions over relevant tweets and over tweets that have a hashtag, which motivates our idea to keep tweets with high probability of interestingness. Links. The existence of a hyperlink is a good indicator of the content value of a tweet. TREC Microblog 2011 defines interest-ingness of a tweet as whether a tweet contains a link [21]. Also, a large fraction of tweets are pointers to online news [19]. Tweets with links are likely to include terms that describe the linked web page, rendering them good surrogates for query terms [5]. Mentions. Tweets with mentions (@username) signify discus-sions about the hashtag X  X  topic. This type of tweet is likely to be noisy because of their personal character. They may, however, bring in query terms used by a niche of people.
 Tweet length. Document length has been shown to matter in re-trieval scenarios [35]. Short tweets are less likely to contain terms useful for query simulation, see Fig. 1 (Center) for the distribution of tweet length.
 Density. A direct measure for probing a tweet X  X  content quality is the density score [20]. Density is defined as the sum of tf-idf values of non-stopwords, divided by the number of stopwords they are apart, squared: Density (  X  ) = K where K is the total number of non-stopwords terms in tweet  X  , w and w k +1 are two adjacent keywords in  X  . weight (  X  ) denotes the term X  X  tf-idf score, and distance ( w k ,w k +1 ) denotes the distance between w k and w k +1 in number of stopwords. Fig. 1 (Left) shows the distribution of density scores of tweets.
 Capitalization. The textual quality of tweets can partially be cap-tured through the use of capitalization [41]. Words in all capitals are considered shouting and an indication of low quality. The ratio of capitals may indicate the quality of the text. Fig. 1 (Right) shows the distribution of the fraction of capital letters over tweet length. Direct. A tweet is direct if it is meant to be a  X  X rivate X  message to another user (i.e., the tweets starts with @user ).
Sampling query terms is a challenging step in the process of au-tomatically generating pseudo test collections. Azzopardi et al. [3] propose several methods for sampling query terms from web docu-ments for known-item search, while Asadi et al. [2] avoid the prob-lem by using anchor texts. Neither approach is applicable in the microblog setting due to a lack of both redundancy in the tweets and anchor texts. Terms in tweets usually occur at most once, but if not, this is often a signal for spam [26]. Probabilistic sampling methods that boost terms occurring with high probability are less likely to return good candidates for query terms. Tf-idf methods emphasize rare terms, which are likely to be spelling mistakes or descriptive of online chatter (e.g.,  X  X ooooool X ) in our setting.
The log-likelihood ratio (LLR) is suitable for our problem of sampling terms from (tweets associated with) a given hashtag [25]. LLR is defined as the symmetric Kullback-Leibler divergence of the expected and observed term probabilities in two corpora. Terms are ranked by how discriminative they are for both corpora. For  X  T h ( 447957 ) our purposes, we set one corpus to be a set of tweets associated with hashtag h and the other to consist of the rest of the tweets in the collection. Stopwords, spam terms, or terms indicative of on-line chatter will rank lower because they occur in both corpora with roughly the same frequency.

Let T be a collection of tweets  X  , R h a set of relevant tweets as-sociated with a hashtag h resulting from one of the sampling meth-ods described in  X 3, and w a term in  X  . For every w  X  S we compute LLR( w ), given corpora R h and B = T \ R h : where O R h ( w ) = tf ( w,R h ) and O B ( w ) = tf ( w,B ) are the ob-served term frequencies of w in R h and B , respectively. E and E B ( w ) are the expected values of the term frequency of w in R h and B , respectively.

For every hashtag h , terms are ranked in descending order of their log-likelihood ratio score. We remove terms if one of the following pertains: (1) The term is equal to the hashtag up to the  X # X  character. In this case, all tweets in T h contain the hashtag, making the query very easy for all rankers, which then leads to a learning to rank method having a hard time distinguishing between rankers. (2) The term occurs in fewer than 10 documents. We generate queries that consist of the top-K ranked terms. For all pseudo test collections described in  X 3 we set K = 10 . For our most promising method, we examine the impact of this parameter by generating queries of length 1, 2, 3, 5, and 20.
The main research question we aim to answer is: What is the utility of our test collection generation methods for tuning retrieval approaches and training learning to rank methods?
Parameter tuning. We do parameter sweeps for some retrieval runs on different (pseudo) test collections, see Table 1 for details. We ask: (a) What is better in terms of retrieval performance: tuning on a different editorial test collection or tuning on a pseudo test col-lection? We answer by calculating how far performance obtained by tuning on either collection is from optimal performance for each retrieval model. (b) Do scores between a pseudo test collection and an editorial test collection correlate better than scores between edi-torial test collections? We answer by calculating Kendall X  X  tau and expected loss in effectiveness.

Learning to rank. We compare the utility of test collections as training material for a learning to rank algorithm. We ask: (a) What is better in terms of retrieval performance: training on a different editorial test collection, or training on a pseudo test collection? We answer by calculating the difference in retrieval performance be-tween training on each. (b) Which of our pseudo test collections is most useful? (c) Do learning to rank algorithms trained on pseudo test collections outperform the best individual feature?
Parameter sensitivity. We also analyze parameter sensitivity of our methods, focusing on two parameters. First, in generating Hashtag-TI ( X 3), we keep the best X = 50% percent of tweets. How sensitive are our results to this method to different values for X? We try these values: 20, 40, 60, and 80. In all our experiments,
Algorithm 4: Training an LTR system on a pseudo test collec-tion, and testing it on an editorial collection for i = 1  X  N do end
Indri 5.1
T errier 3.5 when we generate queries, we keep the top 10 terms of the ranking produced by LLR ( X 4). For our best pseudo test collection, we ask how parameter tuning results and learning to rank performance is influenced by different query length. We try query lengths 1, 2, 3, 5, and 20.
We use the publicly available dataset from the TREC 2011 and 2012 Microblog tracks. It covers two weeks of Twitter data, from January 24, 2011 X  X ebruary 8, 2011, consisting of approximately 16 million tweets. We perform a series of preprocessing steps on the content of tweets. We discard non-English tweets using a lan-guage identification method for microblogs [6]. Exact duplicates are removed; among a set of duplicates the oldest tweet is kept. Retweets are discarded; in ambiguous cases, e.g., where comments were added to a retweet, we keep the tweet. Punctuation and stop words are removed using a collection-based stop word list, but we keep hashtags without the  X # X  character. After preprocessing we are left with 4,459,840 tweets, roughly 27% of all tweets. Due to our aggressive preprocessing, we miss 19% of the relevant tweets per topic, on average. Our 10 retrieval models avoid using future evi-dence by using per topic indexes. For completeness, we note that our stopword list and the idf-weights in the density feature were computed on the entire collection. Pseudo test collections and both TREC microblog test collections also contain tweets from the en-tire collection. For generating queries, we index the collection with Lucene without stemming or stopword removal.

Table 2 lists statistics of the pseudo test collections generated with the methods described in  X 3, as well as statistics of the collec-tions generated by choosing different values for X. The Hashtags-TI-QL{1,2,3,5,20} pseudo test collections are of the same propor-tions as Hashtags-TI , apart from the query length. We have listed only one of fifteen Random collections, but these are all of the same proportions: about as large as the Hashtags collection. We follow a two-step approach to learning to rank, outlined in Algorithm 4. First, we run several retrieval algorithms, then we rerank all retrieved tweets. For retrieval, we use the retrieval algo-rithms listed in Table 1, optimized for MAP [24, 34] after tuning on a training collection. In case of ties among parameter vectors for a ranker, we randomly sample a parameter vector. We also use a parameter free retrieval algorithm, DFRee [1]. For re-ranking, we compute three groups of features.
 Query-tweet features. These are features that have different val-ues for each query-tweet pair. We subdivide these as follows. Rank-ers : the raw output of each retrieval algorithm. For LM, BOW and BUW we transform the raw output X by taking the exponent: exp( X ) . Ranker meta features : the number of rankers that re-trieved the tweet, the maximal, average, and median reciprocal rank of the tweet over all rankers. Recency : query-tweet time difference decay, computed as exp( t (  X  )  X  q t ) , where t (  X  ) is the timestamp of the tweet and q t the timestamp of the query. We linearly normalize query-tweet features over all retrieved tweets for the query. Query features. These are features which have the same value for every retrieved tweet within the same query. We use Query clarity , a method for probing the semantic distance between the query and the collection [11]. We linearly normalize query features over the set of retrieved tweets for all queries.
 Tweet features. These are features that have the same value for each tweet independent of the query. We use the Quality criteria listed in  X 3: link , mentions , tweet length , density , capitalization , and direct . We linearly normalize tweet features over the set of retrieved tweets for all queries.
 To build a training set, one needs positive and negative training examples. Let q  X  Q be a query from the training collection, R the set of relevant tweets for query q , and M q the set of all retrieved tweets for q . Then, for each query in the training collection we use R q  X  M q as positive examples. To have a balanced training set, we randomly sample | R q | tweets as negative training examples from M
Next, we feed the training set to four state of the art learners: (a) Pegasos SVM 1 [ 36 , 37 ], (b) Coordinate ascent [ 27 ], (c) Rank-SVM [ 17 ], and (d) RT-Rank [ 29 ]. We set Pegasos SVM to optimize the area under the ROC curve using indexed sampling of training examples, with regularization parameter  X  = 0 . 1 for a maximum of 100,000 iterations. We set coordinate ascent to optimize for MAP with = 0.0001 and maximum step size of 3. We use line search to optimize each feature with uniform initialization and consider only positive feature weights without projecting points on the manifold. For RankSVM we set the cost parameter to 1 per query. In pre-liminary experiments, RT-Rank performed poorly and therefore we choose to leave it out from our report.

Recall that training sets are compiled using tuned rankers and that in case of ties between different parameter vectors for a ranker, a random vector is selected. When compiling test sets for TREC MB 2011 and TREC MB 2012 to evaluate the utility of a training set, we use the exact same parameter vectors, so that the same set of features are used for training and testing.

Algorithm 4 has randomness in several stages: (i) when gener-ating the pseudo test collection (only in the case of the Random collections), (ii) when sampling a winning parameter setting for each feature, (iii) when randomly sampling negative training exam-ples, and (iv) during model learning. To obtain a reliable estimate of of the performance when training on a pseudo test collection, this procedure is repeated N = 10 times, each time generating a new pseudo test collection (in the case of the Random test collec-tion), selecting random parameter vectors, selecting random nega-tive training examples, and training an LTR model.
We report on precision at 30 (P30) on binary relevance judg-ments. We choose P30 because it has been one of the main metrics in both the TREC 2011 and 2012 Microblog track. We also report on MAP, as it is a well understood and commonly used evaluation metric in information retrieval, allowing us to better understand the behavior of our pseudo test collections. Note that in the 2011 task, tweets had to be ordered by their publication date instead of by their relevance. Many top performing systems treated the task as normal relevance ranking and cut off their ranked lists at rank 30 [ 21 ]. In the 2012 track organizers decided to focus on ranking by relevance again, which is what we will focus on.

Testing for statistical significance. For each training collection, we run Algorithm 4 N = 10 times, giving rise to N scores for each topic, for each collection. We report average performance and sam-ple standard deviation over these iterations. To also gain insight if any differences between a pair of training collections would be ob-served on different microblog topics from the same hypothetical population of topics, we proceed as follows. We pick for each col-lection the iteration of Algorithm 4 which had the smallest training error on that collection. Then, we do a paired t-test over differences per topic as usual and report the obtained p-values. Statistically sig-nificant differences are marked as N (or H ) for significant differences for  X  = . 001 , or M (and O ) for  X  = . 05 .
First, we report on our parameter tuning results; then on our learning to rank results. We also analyze parameter sensitivity with regard to the percentage of interesting tweets kept and query length. http://code.google.com/p/sofia-ml/
The main outcomes in this section will be correlations, to an-swer the question whether relative performance of parameter val-ues on pseudo test collections correlates with relative performance of the same values on an editorial collection. We begin with two case studies to gain a better understanding of the behavior of our pseudo text collections. We sweep (a) the document length normal-ization parameter b for Terrier X  X  tf-idf implementation (Fig. 2(a) ), and (b)  X  for Indri X  X  implementation of language modeling with Dirichlet smoothing (Fig. 2(b) ). We only include one of our ten random pseudo test collections; all random collections behave sim-ilarly, for all retrieval systems and metrics.

Fig. 2(a) shows that on the TREC MB 2011 collection there is a general trend to prefer lower values of b , possibly because of the very small average document length, which, in turn, renders the deviation from the average length close to one. All pseudo test collections capture this trend, including the Random-1 pseudo test collection. The curves of the pseudo test collections are smoother than the curve obtained when tuning on the TREC MB 2011 top-ics; this is because the pseudo test collections have far more test topics. Pseudo test collections show differences in absolute scores, but most importantly, we are interested in whether pseudo test col-lection predictions that one parameter vector is better than another correlate to such predictions of editorial collections. Kendall X  X  tau expresses exactly that correlation, see Table 3 . In addition, we want to know the following: if we sample a random parameter vector from those predicted to yield optimal performance on a pseudo test collection, what will be the expected loss with regard to optimal performance on an editorial collection? Table 5 provides these quantities. Correlations are high across the board, and expected loss is low. All pseudo test collections can be used to reliably tune the b parameter of Terrier X  X  TF-IDF, even the Random-1 collection.
Turning to a second case study, Indri X  X  language modeling algo-rithm with Dirichlet smoothing, we see a different picture (Fig. 2(b) ). The TREC MB 2011 topics show a slightly decreasing trend for larger  X  values. We believe this is due to the short document length; tweets after processing are few terms long, and therefore even small  X  values overshadow the document term probability with the back-ground probability. This trend is not entirely captured by the pseudo test collections. All have a short increase for low values of  X  which is much less pronounced in the TREC MB 2011 curve. After that, all except the Random-1 collection show a decline, if only in the third digit. Correlations are fair (Table 4 ), but the Random-1 col-lection fails here. Expected loss is low across the board (Table 6 ).
Looking at the big picture, we average the correlations and ex-pected loss figures in Tables 7 and 8 over all nine retrieval models from Table 1 . For the hashtag based collections, correlations are high, with a large variance over systems. Expected loss is low. This indicates that pseudo test collections can be used to reliably and profitably tune parameters for a variety of well established re-trieval algorithms, with more or less success depending on which model is being tuned. Tuning retrieval models on all hashtag based pseudo test collections is about as reliable as tuning on editorial test collections. For most retrieval algorithms, a Random collec-tion cannot be recommended for tuning. Thus, the idea of grouping tweets by hashtag has value for creating pseudo test collections for tuning retrieval algorithms.
In this section we evaluate the usefulness of our pseudo test col-lections (PTCs) by training several learning to rank algorithms on them. In Tables 9 and 10 , we report P30 and MAP performance on the TREC 2011 Microblog track topics. We compare training on our PTCs with training on the TREC 2012 Microblog track top-ics and indicate significant differences. In Tables 11 and 12, we report P30 and MAP performance on the TREC 2012 Microblog track topics, and compare training on our PTCs with training on the TREC 2011 Microblog track topics.

A first brief glance at all four tables tells us that training on strategy. Still, if training on a PTC is not substantially and signif-icantly worse, we may conclude that in the abscence of training data, pseudo test collections are a viable alternative.
 A second glance at all four tables shows us that training on the Random PTC is always significantly outperformed by training on editorial collections. Still, in most cases, there is a hashtag based PTC on which training yields performance on par with training on editorial collections. This shows that there is added value in our idea of using hashtags to group tweets by topic.

Another phenomenon we can observe in all tables is that Pega-sos has remarkably stable performance over pseudo test collections, compared to the other two learning to rank algorithms. With the exception of Hashtags-TI-QL1 and Random, it seems to be able to exploit the structure in any PTC to learn a function that yields performance comparable to a function learned on editorial training data. RankSVM, on the other hand, has unstable performance. Es-pecially in Table 12 it refuses to work on anything but manually ob-tained ground truth. Coordinate Ascent, a remarkably simple learn-ing to rank algorithm holds the middle ground. When queries are too short, as in Hashtags-TI-QL1, Hashtags-QL2 and Hashtags-TI-QL3 performance detoriates. When subsamples of tweets become too small (Hashtags-TI-X20) the same happens.

So which PTC is the best? All PTCs are significantly outper-formed by training on an editorial collection in at least one of the conditions. Hashtags-TI-X60 and Hashtags-TI-X80 both yield best results in one case. Also, like Hashtags, Hashtags-T and Hashtags-TI are significantly outperformed in only a small number of cases.
Learning to rank only makes sense if improvements over indi-vidual retrieval algorithms can be obtained. Tables 13 and 14 show performance of the retrieval models we use as features. In the great majority of cases our hashtag based PTCs outperform the best fea-ture. The Random collection never achieves this.
We describe two types of related work: (i) searching microblog posts and (ii) pseudo test collections.
 Searching microblog posts. Microblog search is a growing re-search area. The dominant microblogging platform that most re-search focuses on is Twitter. Microblogs have characteristics that introduce new problems and challenges for retrieval [12, 40]. Mas-soudi et al. [26] report on an early study of retrieval in microblogs, and introduce a retrieval and query expansion method to account for microblog search challenges. Efron and Golovchinsky [13] inves-tigate the temporal aspects of documents on query expansion using pseudo relevance feedback. Naveed et al. [30] develop a retrieval model that takes into account document length and interestingness defined over a range of features.

In 2011, TREC launched the microblog search track, where sys-tems are asked to return relevant and interesting tweets given a query [21]. The temporal aspect of Twitter and its characteristics, e.g., hashtags and existence of hyperlinks, were exploited by many participants, for query expansion, filtering, or learning to rank [21]. Whereas teams depended on self-constructed training data to train learning to rank systems during TREC 2011, most of these systems were successfully trained on the 2011 queries during TREC 2012. Teevan et al. [40] find that over 20% of Twitter queries contain a hashtag, an indication that hashtags may be good topical surrogates, which can be leveraged for building pseudo test collections. Pseudo test collections. The issue of creating and using pseudo test collections is a longstanding and recurring theme in IR, see, e.g., [38, 39]. Several attempts have been made to either simulate human queries or generate relevance judgments without the need of human assessors for a range of tasks. Azzopardi et al. [3] sim-ulate queries for known-item search and investigate term weight-ing methods for query generation. Their main concern is not to develop training material, but to determine whether their pseudo test collection generation methods ultimately give rise to similar rankings of retrieval systems as manually created test collections. Kim and Croft [18] generate a pseudo test collection for desktop search. Huurnink et al. [15] use click-through data to simulate rel-evance assessments. Later, they evaluate the performance of query simulation methods in terms of system rankings [16] and they find that incorporating document structure in the query generation pro-cess results in more realistic query simulators. Hofmann et al. [14] try to smooth noise from click-through data from an audio-visual archive X  X  transaction log using purchase information of videos. We extend previous work on pseudo test collection generation [4] with a principled method to obtain good quality training material, using knowledge derived from editorial judgements.

Asadi et al. [2] describe a method for generating pseudo test collections for training learning to rank methods for web retrieval. Their methods build on the idea that anchor text in web documents is a good source for sampling queries, and the documents that these anchors link to are regarded as relevant documents for the anchor text (query). Our work shares analogies, but in the microblog set-ting, there is no anchor text to sample queries. Moreover, the tem-poral aspect of relevance plays a bigger role in microblog search.
Carterette et al. [7] investigate what the minimal judging effort is that must be done to have confidence in the outcome of an eval-uation. Rajput et al. [32] present a method for extending the re-call base in a manually created test collection. Carterette et al. [8] find that test collections with thousands of queries with fewer rel-evant documents considerably reduce the assessor effort with no appreciable increase in evaluation errors. This finding inspired us to come up with pseudo test collection generators that are able to produce large numbers of queries: while the signal produced by an individual query may be noisy, the volume will produce a signal that is useful for learning and parameter tuning.
Following the results of our experiments we list three main ob-servations: (1) The Random pseudo test collection performs signif-icantly worse than editorial collections in the retrieval experiments and shows low correlation to these collections in the tuning phase. (2) The top pseudo test collections are not significantly worse than editorial collections and show high correlation when tuning param-eters. (3) Differences between various pseudo test collections on retrieval effectiveness are small.
Combining the top two observations leads us to conclude that our approach to constructing pseudo test collections works. We can successfully use pseudo test collections, as long as we find ap-propriate surrogate relevance labels. Why are these findings impor-tant? To train learning to rank methods on microblog retrieval tasks we do not have to invest in manual annotations but can use hashtags for creating training examples. Pseudo test collections can also be used successfully for tuning parameters of retrieval models.
The third observation is that the differences between our pseudo test collections are limited. More advanced methods for selecting tweets and hashtags result in performance that is only sporadically better than the naive baseline method, which treats all tweets and hashtags equally. We can look at this from two angles. (i) Col-lection construction: We can limit time spent on constructing a smooth and interesting pseudo test collection by substituting more advanced methods (Hashtags-T and -TI) with the naive Hashtags method. Using the naive method is faster and results in a larger collection, with similar results. (ii) Training volume: Investing in obtaining more interesting tweets and hashtags using our more ad-vanced methods substantially reduces the number of queries in our collections. While the naive Hashtags uses over 1,800 queries and 460,000 relevant tweets, the other methods use only 890 (Hashtags-T) and 480 (Hashtags-TI) queries and equally reduced sets of rele-vant tweets. Training efficiency improves substantially by limiting the number of queries in the collections. In other words, we can choose between spending more time on constructing our collec-tions, while reducing training time, or take a naive collection con-struction approach that results in larger collections and thus longer training times. A similar observation holds for the editorial col-lections, which are the smallest collections (50 X 60 queries), but (supposedly) with the highest quality.
 Summarizing, we have studied the use of pseudo test collections for training and tuning LTR systems for microblog retrieval. We use hashtags as surrogate relevance labels and generate queries from tweets that contain the particular hashtag. These pseudo test col-lections are then used for (1) tuning parameters of various retrieval models, and (2) training learning to rank methods for microblog retrieval. We explore three ways of constructing pseudo test collec-tions, (i) a naive method that treats all tweets and hashtags equally, (ii) a method that takes timestamps into account, and (iii) a method that uses timestamps and selects only interesting microblog posts. We compare their performance to those of a randomly generated pseudo test collection and two editorial collections.

Our pseudo test collections have high correlation with the edito-rial collections in the parameter tuning phase, whereas the random collection has a significantly lower correlation. In the LTR phase we find that in most cases our collections do not perform signifi-cantly worse than the editorial collections, while the random col-lection does perform significantly worse.

Looking forward, we are interested in training on a mixture of editorial and generated ground truth. Our work is related to creating ground truth in a semi-supervised way and we also aim to further explore this relation.
 This research was partially supported by the European Commu-nity X  X  Seventh Framework Programme (FP7/2007-2013) under grant agreements nr 258191 (PROMISE Network of Excellence) and 288-024 (LiMoSINe project), the Netherlands Organisation for Scien-tific Research (NWO) under project nrs 640.004.802, 727.011.005, 612.001.116, HOR-11-10, the Center for Creation, Content and Technology (CCCT), the BILAND project funded by the CLARIN-nl program, the Dutch national program COMMIT, the ESF Re-search Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch Academy of Sciences (KNAW), and the Netherlands eScience Center under project number 027.012.105. [1] G. Amati and C. van Rijsbergen. Probabilistic models of [2] N. Asadi, D. Metzler, T. Elsayed, and J. Lin. Pseudo test [3] L. Azzopardi, M. de Rijke, and K. Balog. Building simulated [4] R. Berendsen, M. Tsagias, M. de Rijke, and E. Meij. [5] M. Bron, E. Meij, M. Peetz, M. Tsagkias, and M. de Rijke. [6] S. Carter, W. Weerkamp, and E. Tsagkias. Microblog [7] B. Carterette, J. Allan, and R. Sitaraman. Minimal test [8] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and [9] S. Clinchant and E. Gaussier. Bridging language modeling [10] B. Croft, D. Metzler, and T. Strohman. Search Engines: [11] S. Cronen-Townsend and W. B. Croft. Quantifying query [12] M. Efron. Information search and retrieval in microblogs. J. [13] M. Efron and G. Golovchinsky. Estimation methods for [14] K. Hofmann, B. Huurnink, M. Bron, and M. de Rijke. [15] B. Huurnink, K. Hofmann, and M. de Rijke. Simulating [16] B. Huurnink, K. Hofmann, M. de Rijke, and M. Bron. [17] T. Joachims. Training linear SVMs in linear time. In KDD [18] J. Kim and W. B. Croft. Retrieval experiments using [19] H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter, a [20] G. G. Lee et al. SiteQ: Engineering high performance QA [21] J. Lin, C. Macdonald, I. Ounis, and I. Soboroff. Overview of [22] T.-Y. Liu. Learning to rank for information retrieval. Found. [23] C. Lundquist, D. Grossman, and O. Frieder. Improving [24] C. Macdonald, R. Santos, and I. Ounis. The whens and hows [25] C. D. Manning and H. Sch X tze. Foundations of statistical [26] K. Massoudi, E. Tsagkias, M. de Rijke, and W. Weerkamp. [27] D. Metzler. A Feature-Centric View of Information Retrieval . [28] G. Mishne and M. de Rijke. A study of blog search. In ECIR [29] A. Mohan, Z. Chen, and K. Q. Weinberger. Web-search [30] N. Naveed, T. Gottron, J. Kunegis, and A. C. Alhadi. [31] J. Peng, C. Macdonald, B. He, V. Plachouras, and I. Ounis. [32] S. Rajput, V. Pavlu, P. B. Golbus, and J. A. Aslam. A [33] S. Robertson and K. Jones. Simple, proven approaches to [34] S. Robertson and H. Zaragoza. On rank-based effectiveness [35] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, [36] D. Sculley. Large scale learning to rank. In NIPS Workshop [37] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [38] J. Tague and M. Nelson. Simulation of user judgments in [39] J. Tague, M. Nelson, and H. Wu. Problems in the simulation [40] J. Teevan, D. Ramage, and M. R. Morris. #twittersearch: a [41] W. Weerkamp and M. de Rijke. Credibility-based reranking [42] C. Zhai and J. Lafferty. A study of smoothing methods for
