 We argue that the abilit y to iden tify pairs of related terms is at the heart of what mak es spectral retriev al work in prac-tice. Schemes suc h as laten t seman tic indexing (LSI) and its descendan ts have this abilit y in the sense that they can be view ed as computing a matrix of term-term relatedness scores whic h is then used to expand the given documen ts (not the queries). For almost all existing spectral retriev al schemes, this matrix of relatedness scores dep ends on a xed low-dimensional subspace of the original term space. We in-stead vary the dimension and study for eac h term pair the resulting curve of relatedness scores. We nd that it is actu-ally the shap e of this curv e whic h is indicativ e for the term-pair relatedness, and not any of the individual relatedness scores on the curv e. We deriv e two simple, parameterless algorithms that detect this shap e and that consisten tly out-perform previous metho ds on a num ber of test collections. Our curv es also shed ligh t on the e ectiv eness of three fun-damen tal types of variations of the basic LSI scheme. H.3.3 [ Information Searc h and Retriev al ]: Clustering, Retriev al Mo dels Algorithms, Performance, Theory , Exp erimen tation Spectral Analysis, Laten t Seman tic Indexing, Documen t Ex-pansion, Curv e of Relatedness Scores
Laten t seman tic indexing (LSI) [4] became famous as one of the rst information retriev al techniques for fully auto-matically and with surprising e ectiv eness dealing with the problems of synon ym y (web, internet) and polysem y (surf-ing the web, sur ng at Waikiki beac h), whic h mak e searc hing by pure text matc hing, where only documen ts are returned that con tain all or at least some of the words in the query , so frequen tly a frustrating exp erience.

LSI and its man y successors are based on what is usually referred to as the vector space mo del [18], where documen ts as well as queries are represen ted as vectors, with eac h di-mension corresp onding to a word or term that occurs in the documen t collection; the similarit y between a documen t and a query is then measured by the similarit y between the cor-resp onding vectors, typical measures being the dot pro duct or the cosine of the angle (whic h coincide if the vectors are normalized).

The key idea of LSI is to map both queries and documen ts from the m -dimensional space, where m is the num ber of terms, to a space of signi can tly lower dimension k , and compute vector similarities in the reduced space instead of in the original one. The hop e is that while the dimensions of the original space corresp ond to the speci c terms used, the dimensions of the lower dimensional space corresp ond more to the (relativ ely few) conc epts underlying this term usage. We will therefore refer to the original m -dimensional space as term space and to the reduced k -dimensional space as conc ept space .

LSI deriv es the low-dimensional represen tation by a spec-tral decomp osition of the term-do cumen t matrix, more pre-cisely , by pro jecting the high-dimensional vectors on the k most signi can t left singular vectors, i.e., pertaining to the k largest singular values. Follo wing LSI, man y other related schemes have been prop osed [12] [1] [2] [16] [6] [13] [7], whic h all do (or can be used for) what we call spectral retrieval in this pap er: the documen ts are pro jected from the origi-nal term space to some lower-dimensional space spanned by eigen vectors related to the term-do cumen t matrix. 1 All of these schemes can be view ed as to augmen t the basic LSI metho d by mere normalizations and rescalings, whic h can have a signi can t impact on retriev al qualit y. This will be explained and discussed in Section 6.
 There have been man y attempts to explain the success of LSI and spectral retriev al in general, most notably the an-alyzes of [17], [5], [3], and [2]. The bottom line of all these results is that if the term-do cumen t matrix is a sligh t per-turbation of some rank-k matrix | and the results di er in what kind of perturbation is permitted | then spectral retriev al will succeed in reco vering that rank-k matrix. On the one hand, this is of course an interesting and non-trivial 1 The various concept-based retriev al schemes based on prob-abilistic mo delings, e.g. [11], are of a rather di eren t kind, and their investigation lies outside the scop e of this pap er. prop erty. On the other hand, any statemen t of this kind is much closer to the mathematics of spectral retriev al (ap-pro ximating a set of points from a high-dimensional space by a set of points in a lower-dimensional space) than to what real documen t collections actually look like. For example, any of the man y term-do cumen t matrices we have seen so far | be they small, medium, or large | can be seen as \appro ximately rank k " for about any k with equal justi -cation. Subtler points are that there is usually more than one reasonable way to divide a set of documen ts into (a given num ber of) categories, and that natural categorizations are often not at but rather hierarc hical.

In this pap er, we tak e a novel approac h to understanding and analyzing spectral retriev al, and to making use of its poten tial. We rst work out the details of LSI, the most basic implemen tation of spectral retriev al, as a document expansion (and not query expansion) pro cess. This leads us to the insigh t that spectral retriev al essen tially works by assigning a relatedness score to eac h pair of terms. We then introduce our cen tral notion: the curve of relate dness scores , whic h for a given term pair sho ws the men tioned relatedness score as the dimension k of the concept space is varied from 1 to the full rank of the term-do cumen t matrix. We then pro vide complemen tary theoretical and empirical evidence that the relatedness of two terms in a collection corresp onds to a certain characteristic shap e of the curv e of relatedness scores and not to any of the individual scores on the curv e.
Our ndings imply a surprising answ er to the question of the appropriate choice of dimension, namely that every xed choic e is inappr opriate for a signi c ant fraction of the term pairs . But our ndings also help us out of this dilemma, by naturally leading to two simple, parameterless algorithms that assess the shap e of the curv e of relatedness scores for eac h term pair and thus overcome the limitations of meth-ods committing to a concept space of xed dimension. We nd that on three test collections | one small, one medium-sized, and one large | our algorithms consisten tly outp er-form LSI and a variet y of its successors.

Our ndings also shed new ligh t on the e ectiv eness of the man y variations of spectral retriev al. In Section 6, we form ulate three fundamen tal types of variations of the basic LSI scheme, whic h together capture almost all varian ts of spectral retriev al whic h we found in the literature. We com-men t on the e ects of eac h of these variations from the point of view of our curv es of relatedness scores. In particular, we will see that none of these variations can overcome the problems asso ciated with considering only individual scores of the curv es, instead of their overall shap e.

Parts of our work were inspired by the setup from Dupret [6], who exp erimen tally investigated the in uence of the choice of k on particular terms (not term pairs). In a statis-tician's approac h, Efron [8] pro vided a num ber of accurate form ulas for predicting that k whic h yields the best retriev al performance (among the possible choices of a xed k ). In a very recen t work, Dupret [7] presen ted a heuristic that avoids committing to a xed k , with promising, yet incon-clusiv e results and without much emphasis on theoretical foundation and explanation.
It has been remark ed (but nev er explored further in much detail) by sev eral authors that LSI is related to a pro cess kno wn as query expansion , where terms related to those ini-tially presen t are added to a query . As we sho w next, LSI is in fact doing exactly what could be called document ex-pansion . The most widely used varian t of LSI maps vectors from the m -dimensional term space to the k -dimensional concept space by the map x 7! U T k x , where U k is the ma-trix con taining the k left singular vectors pertaining to the k largest singular values. The cosine-similarit y of a query q with documen t A i in the LSI space is then given by the form ula If our goal is to rank the documen ts by their similarities to a given query , we can drop the division by j U T k q j (it is the same for every documen t), and obtain the 1 n row vector of all similarities by where N is an n n diagonal matrix, with N ii = 1 = j U T k Now a simple but imp ortan t observ ation is that and Instead of mapping both queries and documen ts to the k -dimensional concept space via U T k and computing the cosine similarit y there, we may therefore as well transform the doc-umen ts via the m m matrix U k U T k , and compute cosine similarities in the original term space. According to the pre-ceding calculations, both pro cedures will yield exactly the same ranking.

Now the e ect of multiplying a documen t or any vector in the m -dimensional term space by an m m matrix, as U k U T is one, has a simple intuition. Let us call the matrix T and the vector d , and rst assume that both have entries either zero or one. Then if T ij = 1 and d j = 1, the i th comp onen t of T d will also be (at least) 1, that is, the e ect of T to \add" term i to the documen t whenev er term j is presen t. More generally , for arbitrary values in T and d , the entry in T ij determines to whic h exten t the weigh t for term i should be increased when term j is presen t. Note that for LSI, T may also have negativ e values, in whic h case a term can also e ect the \subtraction" of other terms.
As we said, almost all previous spectral retriev al schemes commit to a particular dimension, and from that viewp oint it app ears natural to study the entries of the documen t ex-pansion matrix T = U k U T k for some xed value of k . The cen tral idea of this pap er is now to shift to a di eren t view-point, and study how the entry at a xed position (term pair) changes when the dimension is varied, that is, inste ad of looking at all the entries for a xed dimension, we now look at a xed entry for all dimensions .

Definition 1. Given an m n term-do cument matrix of rank r , let U V T be its singular value decomp osition, wher e U = ( u ij ) is an m r matrix and the singular values in are sorte d in desc ending order. Then the curv e of relatedness scores for terms i and j is a plot of the function
By looking at the curv es of man y term pairs, one quic kly recognizes three characteristic types of shap es, irresp ectiv e of the documen t collection under consideration. Figure 1: Curv es of relatedness scores for pairs of related terms (a) + (b), and for a pair of unrelated terms (c), all from a collection of computer science abstracts.

Figure 1 sho ws a typical curv e for eac h type. Curv es of type (a) go up relativ ely steadily from beginning to end. Curv es of type (b) go up steadily in the beginning, but then from some point on start to go down again. Curv es of type (c) are quite di eren t in that they do not have a clear direc-tion upward or downward; they are also less smo oth. There are a few intermediate cases, but most curv es quite natu-rally fall into one of these three categories. It turns out that curv es of type (a) and (b) typically belong to intuitiv ely more or less related terms (\v oronoi" and \diagram", \job" and \scheduling"), while most curv es of type (c) belong to intuitiv ely unrelated terms (\geometry" and \logic"). So far, these are empirical observ ations, but we will soon see theoretical evidence for what we have describ ed here.
It is imp ortan t to note that for our categorization above we did not use the order of magnitude of the relatedness scores. It is true that curv es of types (a) and (b) on aver age reac h larger scores than those of type (c). But it will become clear by the theory that follo ws that individual scores on the curv es | and almost all previous schemes are based on these | are much less reliable indicators for term relatedness than the overall shap e of the curv es.
We next explain why intuitiv ely related terms give rise to curv es of the types (a) and (b) that we have seen in Figure 1. To this end, we introduce a notion of perfe ctly relate d terms , whic h are terms that have iden tical co-o ccurrence patterns in the documen t collection; this de nition was inspired by the exp erimen tal setup of Dupret [6].

Definition 2. Two terms (indexe d i and j ; without loss of gener ality assume i = m 1 , j = m ) in an m n term-document matrix A are called perfectly related if, for some permutation of the columns of A , wher e A 1 is a sub-matrix with dimension ( m 2) n 1 , A 2 a sub-matrix with dimension ( m 2) n 2 , A 3 is a sub-matrix of dimension ( m 2) n 3 , a 1 and b 1 are row vectors of length n 1 each and a 2 is a row vector of length n 2 (conse quently, 2 n 1 + n 2 + n 3 = n ).

The follo wing lemma says that a pair of perfectly related terms gives rise to a very particular substructure in the left singular vectors, whic h, as we will explain afterw ards, im-plies an equally particular kind of curv e of relatedness scores.
Lemma 1. For a matrix A as in De nition 2, (a) the vector v = (0 ; ; 0 ; 1 = gular vector of A ; (b) the corresponding singular value is j a 1 b 1 j ; (c) for all other left singular vectors u of A , u m 1 = u that is, the last two entries are equal.

Proof. If A = U V T is the singular value decomp osi-tion of A , and we de ne C = AA T , we have
C = ( U V T )( U V T ) T = U ( V T V ) T U T = U 2 U T ; because V T V is the iden tity matrix. The left singular vec-tors of A are therefore just the eigen vectors of C , and the singular values of A are the square roots of the eigen values of C , so that we may as well consider C instead of A . Now if A is as stated in (2), then where C 1 = 2 A 1 A T 1 + A 2 A T 2 + A 3 A T 3 is an ( m 2) ( m 2) matrix, c 1 = a 1 A T 1 + b 1 A T 1 + a 2 A T 2 is a 1 m vector, x = a v = (0 ; ; 0 ; 1 = eigen value x y because Cv = ( x y ) v , and Moreo ver, since all other eigen vectors u of C are orthogonal to v , the dot pro duct u T v = u m 1 = zero, hence u m 1 = u m .

Lemma 1 implies the follo wing particular shap e of the curv e of relatedness scores for two perfectly related terms. If k is the rank of the special singular vector from Lemma 1(a), then because of Lemma 1(c) the relatedness scores will steadily increase until k , then at dimension k fall o by 1 = For an example, see the left curv e of Figure 2. By Lemma 1(b), the dimension k of fall-o is exactly 2 the num ber of singular values whic h are greater or equal to j a 1 b 1 j .
The dimension of fall-o dep ends on the co-o ccurrence pattern of the term pair in an interesting way. For an intu-itiv e explanation, let us assume that b 1 = 0, whic h, accord-ing to De nition 2, means that when the terms co-o ccur, they do so in the same frequency . If then also j a 1 j = 0, the two terms co-o ccur whenev er they occur. Then j a 1 b 1 j is 2 assuming that all non-zero singular values are di eren t, whic h is true for any \real" term-do cumen t matrix; see the remark follo wing Lemma 2. zero, whic h means that the relatedness scores will increase on the whole range from 1 to the full rank of the matrix. This corresp onds to curv es of the type sho wn in Figure 1(a). When j a 1 j is non-zero, then the larger it is compared to other terms in the collection, the more singular values will be smaller than j a 1 b 1 j , and the earlier the dimension of fall-o will come. This leads to graphs of the type sho wn in
The follo wing lemma sho ws that our de nition of per-fectly related terms is robust under small perturbations of the term-do cumen t matrix. In view of another application in Section 3.3, the lemma is form ulated sligh tly more generally than would be necessary for this section. In the follo wing we will write m ij for the entries of an arbitrary matrix M , and j M j F for the Frob enius norm, whic h is the square root
Lemma 2. Let A be a term-do cument matrix and let k be an inte ger such that the matrix U of the k most signi c ant left singular vectors of A has two identic al rows i and j . Let E be any matrix with jE j F bounde d by some fraction f &lt; 1 = 4 of the gap betwe en the singular values k and k +1 , and let U 0 be the matrix of the k most signi c ant left singular vectors of A + E . Then, Remark. In every real term-do cumen t matrix we have seen so far, a plot of the sorted singular values sho ws a very smo oth curv e, in particular, there is a gap between eac h pair of neigh boring (non-zero) singular values. Under these cir-cumstances the lemma above implies that sucien tly small perturbations change the curv e of relatedness scores only little at any dimension befor e it falls o .

Proof. By an adaption of Stew art's theorem on the per-turbation of symmetric matrices [9, Theorem 8.6.5 on page 450] to arbitrary rectangular matrices, similarly as done in [17] and [3], it can be sho wn that U 0 = U R + H , where R is an orthogonal k k matrix, and j H j F 4 f .
 Let U 00 = U R and let u T i , u T j , and u 00 T i , u 00 i th and j th row of U and U 00 , resp ectiv ely. Then U 00 shares two prop erties with U . First, by assumption, rows i and j of U are iden tical, and this prop erty is invarian t under righ t multiplication by any matrix, so that U 00 has this prop erty as
RR T u j = u T i u j , that is, the dot pro duct of rows i and j is the same for U 00 as for U .
 j = u 00 T j + h T j , and we now sho w that because H has small norm, the dot pro duct of rows i and j of U 0 is close to that for U . For that, rst write 3 A generic example for term pairs with a very early dimen-sion of fall-o would be two transcriptions of the same name, e.g., Cherno v and Cherno : these will occur in very related con texts but rarely co-o ccur together.
 for the iden tical u 00 i and u 00 j , this becomes where the inequalit y follo ws from Cauc hy's inequalit y. Now j u 00 j 1 because u 00 is part of a row of an orthogonal matrix, and j h i j and j h j j are eac h bounded by j H j , and, by the triangle inequalit y, j h i + h j j is bounded by j H j , too. This, nally , pro ves that j u 0 T i u 0 j u T i u j j is bounded by 2 j H j whic h, in turn, is at most 8 f , as desired.

Figure 2 gives a typical example of the e ect of adding perturbation for two perfectly related terms. It is imp ortan t to note that the absolute values of the curve can and do actually change quite a lot on the way from two perfectly related terms to a pair as it is found in a real collection, however, the basic shap e of the curve remains unchange d : there is a phase of ascend in the beginning, and exactly one intermediate phase of descend, and dra wn on its scale the curv e looks rather smo oth. (Whether another phase of ascend follo ws after the phase of descend or not will not be imp ortan t for us.) Figure 2: Curv es of relatedness scores for two per-fectly related terms (left), after a relativ ely small perturbation of the term-do cumen t matrix (mid-dle), and after a relativ ely large suc h perturbation (righ t).

The three curv es from Figure 2 were obtained as follo ws: we took the term-do cumen t matrix of the collections of ab-stracts men tioned in the caption of Figure 1, and mo di ed two of its rows (terms) to obey the perfect-relatedness cri-terion of De nition 2. The curv e for the in suc h a way mo d-i ed matrix is sho wn on the left of Figure 2. For the curv es in the middle and on the righ t, we added to this mo di ed matrix a 0 : 05 and a 0 : 5 fraction, resp ectiv ely, of the di er-ence between the original and the mo di ed matrix (adding the whole di erence would give the graph for the original matrix).
In the previous section, we de ned an ideal notion of per-fectly related terms, found a very characteristic shap e of their curv es of relatedness scores, and veri ed that related terms from real collections do actually have curv es of a simi-lar shap e. It remains to argue that unr elate d terms give rise to curv es of a (measurably) di eren t kind.

For that consider the co-occurr ence graph of a collection, whic h is an unweigh ted undirected graph that has a vertex for eac h term in the collection, and that has an edge between two vertices if and only if these two terms co-o ccur in at least one documen t (that is, the corresp onding entry in the term-do cumen t matrix is non-zero).

Definition 3. Two terms are called completely unrelated in a collection, if and only ther e is no path from the one term to the other in the co-occurr ence graph of that collection.
Lemma 3. The curve of relate dness scores for two com-pletely unr elate d terms is all zero. A smal l perturb ation of the underlying term-do cument matrix a e cts that curve in a way that can be bounde d exactly as state d in Lemma 2.
Proof. The rst part of the lemma is an easy conse-quence of Theorem 1 in [14]. The second part is implied by Lemma 2.

Indeed, the curv es whic h we observ e for pairs of intuitiv ely unrelated terms (the vast ma jorit y of all term pairs) on real collections are of that kind: they vary around zero, change their direction man y times and look zig-zagged. An example was given in Figure 1(c).
One consequence of our ndings in the last section is a surprising answ er to one of the fundamen tal questions of spectral retriev al, as to what the appropriate choice of di-mension (num ber of concepts) is. We can say that every choic e is inappr opriate for a signi can t fraction of the term pairs. Figure 3 gives an example to illustrate this point. Figure 3: No single xed dimension can do justice to both term pairs.

Both term pairs are intuitiv ely strongly related (singular and plural of the same word) and both curv es have the shap e characteristic for related terms. However, the rst curv e reac hes its peak at a very small k , where the relatedness scores of the second curv e are still very low, but for larger k , when the scores of the second curv e become large, the scores of the rst curv e actually become negativ e. We therefore cannot nd a single xed k suc h that both term pairs get the high relatedness score whic h they would deserv e.
Another problem of any metho d that (implicitly or ex-plicitly) works with relatedness scores at a xed dimension is illustrated by an example in Figure 4. With resp ect to the underlying collection (still the CS abstracts), the two terms on the righ t are not related, whic h indeed sho ws in the shap e of the curv e of relatedness scores. For a signi can t and interesting range of dimensions, however, that curv e reac hes scores of the same order as those reac hed by curv es for strongly related terms, like that on the left hand side of Figure 4. Figure 4: For man y xed dimensions these term pairs look indistinguishable from eac h other, al-though the di eren t shap es of the two curv es clearly tell them apart.

But our ndings from the previous section also suggest a way to overcome these two problems. Algorithms for spec-tral retriev al should not build on relate dness scores compute d for a xed dimension , but inste ad assess the over all shap e of the curve of relate dness scores for eac h term pair.
The follo wing three steps describ e one particularly sim-ple suc h algorithm, where U denotes the matrix of the left singular vectors of the given term-do cumen t matrix A . 1. Normalize the rows of A to length 1, compute the SVD 2. For eac h pair of terms i; j compute the size z ij of the 3. Perform documen t expansion 4 with the 0 1 matrix T , Let us quic kly verify that this algorithm is in accordance with our theoretical ndings from Section 3. By Lemma 1, we have that for all perfectly related terms z ij = 0 and hence t ij = 1. By Lemma 3, completely unrelated terms have all-zero curv es, in whic h case z ij = r &gt; 0, and hence t ij = 0. By Lemma 2, these assignmen ts to T are invarian t under small perturbations of the underlying term-do cumen t 4 To coun ter over-expansion e ects, we actually split T into its diagonal part (whic h is the iden tity matrix) and the rest, do standard expansion as describ ed in Section 2 for eac h of the matrices (whic h means no expansion for the diagonal part), and then add the two similarit y scores. 5 Strictly speaking, Lemma 2 does not rule out the possibilit y that the small change a ected in an all-zero curv e by a small perturbation is suc h that all values become sligh tly above zero, in whic h case our algorithm would set t ij = 1. It is plausible, however, and we have veri ed it exp erimen tally , that for a random perturbation this happ ens with negligible probabilit y.
As pointed out already in the paragraph follo wing Figure 1, curv es of type (a) and (b) could also be distinguished from those of type (c) by the smo othness of the curv es. This idea gives rise to the follo wing alternativ e algorithm. 1. Compute the same matrix U as for TN. 2. For eac h pair of terms i; j , compute the smo othness 3. Perform documen t expansion 4 with the 0 1 matrix T , In con trast to TN, TS has a parameter: the smo othness threshold s . This can be seen as an adv antage as well as a disadv antage. The disadv antage is that we have to nd a good value for this parameter. The adv antage is that it is a very intuitiv e parameter: it speci es how man y related terms we want to consider. This is in sharp con trast to the choice of dimension in previous metho ds, whic h, as we pointed out already in the introduction, has no intuitiv e or natural setting.
The computational complexit y of our two dimensionless algorithms TN and TS is essen tially that of basic LSI. All three require the computation of a singular value decomp o-sition up to a certain dimension k . After that, LSI needs to map the term-do cumen t matrix to the k -dimensional la-ten t space, whic h tak es O ( k nz ) basic numerical opera-tions, where nz is the num ber of nonzero entries in the term-documen t matrix. The construction of TN/TS requires O ( k ) operations per term pair, and actually expanding the term-documen t matrix requires O ( l nz ) operations, where l is the average num ber of related terms of a term, giving a total of O ( k m 2 + l nz ) operation. We can save on the m 2 by dis-carding pairs of terms that do not co-o ccur in at least one documen t, because, in practice, TN and TS nev er assign a 1 to suc h term pairs.
We tested our algorithms on three test collections: the small Time collection [18] (3882 425), the signi can tly larger Reuters collection [15] (5701 21578), and the still larger Ohsumed collection [10] (99117 233445); in paren-theses, the dimensions of the resp ectiv e term-do cumen t ma-trices are given. In all cases we used stemming (Porter) and remo ved common stop-w ords as well as words that oc-curred in less than a certain num ber of documen ts. We measured average precision for 83 queries for Time, 120 queries for Reuters, and 63 queries for Ohsumed. For Time and Ohsumed, the available relev ance rankings were used. Reuters comes only with topic lab els, and we syn thesized a query from eac h topic by taking the most signi can t terms of a random sample of documen ts from that topic, just as done, for example, in [6].

As comp etitors of our algorithms we chose four ma jor spectral retriev al schemes from the literature: the basic la-ten t seman tic indexing scheme (LSI) from [4], the term-normalized varian t (LSI-RN) prop osed in [12], the correla-tion metho d (CORR) from [6], and iterativ e residual rescal-ing (IRR) from [2]. These are among the most well-kno wn varian ts of LSI and moreo ver, eac h of the three fundamen tal types of variation of spectral retriev al, discussed in the next section, is covered by this selection.

As a baseline metho d we took ranking by plain cosine similarit y (COS). The required singular vectors and values were computed from a standard tf.idf matrix for COS, LSI, IRR, and LSI-RN, and from a row-normalized matrix for CORR and our TN and TS, because that is what the theory of the latter metho ds asks for (note that row-normalization undo es tf.idf normalization). All computations were done in Matlab.
On all three collections, our two dimensionless algorithms consisten tly outp erformed their four comp etitors. Moreo ver, the performance of those four varied signi can tly between di eren t collections and di eren t choices of dimension, while TS and TN consisten tly gave very similar results, although the num bers they are based on are mathematically quite di eren t (for eac h term pair, the num ber of times its curv e becomes negativ e versus the curv e's smo othness). This fur-ther adds to the evidence built up in the previous sections that spectral retriev al works by iden tifying pairs of related terms and that the overall shap e of the curv e of relatedness scores is a more reliable indicator for term-pair relatedness than the score at any xed dimension can be.
 LSI-RN 58.6% 60.9% CORR 59.1% 60.5% Figure 5: Average precision gures for the Time collection. For the four varian ts of LSI, gures are given for dimension 300 and 400. The righ t gure sho ws the averaged precision-recall graphs of TN versus IRR at its best dimension.
 Figure 6: Average precision gures for the Reuters and Ohsumed collections. For the three varian ts of LSI, gures are for dimensions 800 and 1000 for Reuters and dimensions 1000 and 1200 for Ohsumed. For TN and TS, the relatedness scores were computed up to dimension 1000 for Reuters and up to dimension 1200 for Ohsumed.
We remark that dimension 400 is close to the optimal dimension for eac h of the four LSI varian ts in Figure 5. In Figure 6, IRR does not app ear because it is computationally too exp ensiv e for collections of this size; indeed, all exp er-imen ts from [2] were done on less than a thousand docu-men ts. Note that the low average precisions of around 15% for Ohsumed are actually quite substan tial given that on average 40 documen ts from over 200,000 were relev ant for eac h query . To mak e the SVD computation for the Ohsumed collection feasible, we kept only the 12464 most signi can t terms. For the LSI-st yle metho ds, all other terms were sim-ply discarded. For TN and TS, we computed the expansion matrices for the restricted num ber of terms, but then used it to actually expand the originally matrix (whic h is possible only by the sparseness of our expansion matrices, another adv antage of our approac h).
One surprising asp ect of our exp erimen tal ndings is that algorithms whic h do a simple binary classi cation into re-lated and unrelated term pairs outp erform schemes whic h seem to have additional power by giving a fractional assess-men t for eac h term pair.

An intuitiv e explanation is that while in principle it is of course reasonable to deem some term pairs more related than others, it is plausible that deducing suc h ne distinc-tions from mere co-o ccurrence data can add more noise than precision.
 A more formal explanation comes from the histogram in Figure 7, whic h illustrates how man y curv es have scores at or below zero at how man y dimensions (before the earliest possible dimension of fall-o according to our theory). The main observ ation here is that most curv es are at or below zero at either very few dimensions (and the vast ma jorit y of these nev er touc hes zero) or at quite a lot of dimensions. In fact, this conforms well with our theoretical ndings: if all term pairs were either perfectly related or completely unre-lated, we would have non-zero coun ts only at both endp oints of the histogram (and an in nitesimal perturbation would move the coun t for the unrelated terms to the middle of the histogram). Figure 7: Histogram of the num ber of dimensions where the curv es of relatedness scores are at or be-low zero (for Reuters).
Our exp erimen tal results con rmed the observ ation of sev-eral previous authors that the impro vemen ts entailed by the various variations of LSI are not consisten t over collections of di eren t sizes and types. As the authors of a recen t SIGIR publication remark, "to the best of our kno wledge, [so far] no study systematically evaluated these fundamen tal choices" [19].

In this section we will characterize how our curv es of re-latedness scores are a ected by three fundamen tal types of variations found in the literature: rescaling of the terms or documen ts, scaling by the singular values, and iterativ e residual rescaling. This covers, in particular, the selection listed in [19].

The bottom line of this section will be that on the one hand, relatedness scores for individual dimensions can in-deed change a lot, whic h re ects the abilit y of eac h of these variations to boost retriev al qualit y. On the other hand, we will see that the characteristic of the shap e of the curv es whic h we found indicativ e of term relatedness remains ba-sically una ected by any of these normalizations. This is further evidence that the shap es we iden ti ed lie at the bot-tom of what mak es spectral retriev al work in practice.
Man y authors have normalized the rows or columns of the term-do cumen t matrix prior to computing the singular vectors. For the various metho ds listed in [6], for example, the rows of the term-do cumen t matrix are rst cen tered and then normalized to length 1. The follo wing lemma says that our de nition of perfectly related terms is invarian t under any com bination of suc h normalizations.

Lemma 4. Let A be a matrix of the form as in De nition 2. Then A remains a matrix of the same form after nor-malizing the rows or columns with respect to an arbitr ary norm, and after centering of the rows or columns, or any combination of these.

Proof. It suces to observ e that the entries in the two rows of the perfectly related terms synon yms are the same up to perm utation (hence their norms and means are equal), and that the same applies to columns j and n 1 + j of A , for j = 1 ; : : : ; n 1 , where n 1 is the num ber of documen ts where the, say, rst term occurs without the second.
After the matrix U k of the top k singular vectors has been computed, there is an option of also rescaling the rows or columns of that U k , or both. A widely used type of rescaling here is to multiply the columns of U k by the corresp onding singular values raised to some power . For example, the original work on LSI [4] sets = 1, while [12] and [6] adv ocate = 1. The most frequen tly used setting is = 0.
The e ect of on our curv es can be characterized as fol-lows. Since the singular values are sorted in descending or-der, positiv e values of will stretch the curv es towards the beginning but shrink it towards the end. For negativ e val-ues of , the opp osite happ ens, whic h, given the shap e of the curv e for related terms, seems less favorable for LSI and its varian ts. Indeed, the inventors of LSI used = 1 only in their very rst work [4], while they used = 0 in all of their man y follo w-up works.

The overall shap e of the curv es remains basically unc hanged by a change of , however, and no particular choice of can therefore overcome the inheren t problems illustrated by Fig-ures 3 and 4. This sho ws in the inconsisten t results rep orted in [6], as well as in our exp erimen tal results from Section 5.1.
Husbands et al. [12] observ ed that basic LSI gives undue preference to frequen t terms and suggested to normalize the rows of U k (after scaling its columns by the singular values). The results were again inconclusiv e, however. In [19], the in uence of the size of the collection on this phenomenon was discussed, however without a theoretical underpinning and also without a conclusiv e answ er. From the point of view of our work, row-normalization of U simply brings all curv es to the same order of magnitude, whic h partially overcomes the problem addressed in Figure 4 but does not address the problem addressed in Figure 3.
Ando and Lee [1] [2] prop osed iter ative residual rescaling (IRR) to obtain, from an m n term-do cumen t matrix A , a sequence of pairwise orthogonal m -vectors u 1 ; u 2 ; : : : , where u 1 is just the top left singular vector of A , u 2 is the top left singular vector of a column-normalized version of A u 1 u and so on. The rationale of this metho d is to balance the bad e ects when the assumed underlying concepts are presen t in the collection in widely di eren t prop ortions.

Due to the pairwise orthogonalit y of the u 1 ; u 2 ; : : : , IRR can be view ed as documen t expansion just like any of the other spectral retriev al metho ds. It is also not hard to see that two perfectly related terms, according to De nition 2, lead to the special singular vector from Lemma 1, and hence to the characteristic shap e of the curv e from Figure 2. The main observ ation needed is that if u is any vector with iden-tical entries at the indices corresp onding to the two perfectly related terms, then for any matrix ~ A of the form of De nition 2, ~
A uu T ~ A will again be of the same form. The problems illustrated in Figures 3 and 4 therefore a ect IRR as much as they do a ect any of the other metho ds.
We have introduced the curves of relate dness scores as a new angle of looking at retriev al schemes based on spectral analysis. We have given strong evidence, both theoretical and exp erimen tal, that iden tifying related terms on the basis of the shap e of these curv es is at the heart of what mak es spectral retriev al work in practice.

Our new dimensionless algorithms do not only outp erform previous schemes that work with relatedness scores com-puted from individual dimensions, but they are also more intuitiv e: they do nothing but iden tify pairs of related terms (in a 0-1 fashion), and expand documen ts via these relations. Note that this immediately gives a thesaurus-lik e function-alit y, whic h could be used, for example, in an interactiv e scenario to prompt a user with a list of possible synon yms for eac h word of her query (possibly rank ed by the smo oth-ness scores computed by our algorithm TS).

Our view of spectral retriev al as a documen t expansion also mak es it straigh tforw ard to incorp orate external kno wl-edge into the retriev al pro cess. In [13], rst ad-ho c steps have been tak en in that direction. Our ndings app ear to be a good foundation for a more principled approac h.
All spectral retriev al schemes so far, including our new algorithms, give rise to a symmetric documen t expansion matrix T , but word pairs like \nucleic" and \acid" actu-ally call for an asymmetric suc h matrix: whenev er there is \nucleic" there is \acid", but not vice versa. This is actu-ally a frequen t phenomenon and we would exp ect algorithms whic h are able to consider this asymmetry to give a further boost to the e ectiv eness of spectral retriev al. [1] R. K. Ando. Laten t seman tic space: Iterativ e scaling [2] R. K. Ando and L. Lee. Iterativ e residual rescaling: [3] Y. Azar, A. Fiat, A. Karlin, F. McSherry , and J. Saia. [4] S. C. Deerw ester, S. T. Dumais, T. K. Landauer, [5] C. H. Q. Ding. A similarit y-based probabilit y mo del [6] G. Dupret. Laten t concepts and the num ber of [7] G. Dupret. Laten t seman tic indexing with a variable [8] M. Efron. Eigenvalue-b ased Estimators for Optimal [9] G. H. Golub and C. F. Van Loan. Matrix [10] W. Hersh, C. Buc kley , T. Leone, and D. Hic kam. [11] T. Hofmann. Unsup ervised learning by probabilistic [12] P. Husbands, H. Simon, and C. H. Q. Ding. On the [13] S. D. Kam var, D. Klein, and C. D. Manning. Spectral [14] A. Kon tostathis and W. M. Pottenger. Detecting [15] D. Lewis. Reuters-21578 text categorization test [16] A. Ng, M. Jordan, and Y. Weiss. On spectral [17] C. H. Papadimitriou, H. Tamaki, P. Ragha van, and [18] G. Salton, A. Wong, and C. Yang. A vector space [19] C. Tang, S. Dw ark adas, and Z. Xu. On scaling laten t
