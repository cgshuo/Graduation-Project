 For many applications measuring the similarity between doc-uments is essential. However, little is known about how users perceive similarity between documents. This paper presents the first large-scale empirical study that investigates per-ception of narrative similarity using crowdsourcing. As a dataset we use a large collection of Dutch folk narratives. We study the perception of narrative similarity by both experts and non-experts by analyzing their similarity ratings and motivations for these ratings. While experts focus mostly on the plot, characters and themes of narratives, non-experts also pay attention to dimensions such as genre and style. Our results show that a more nuanced view is needed of narrative similarity than captured by story types, a concept used by scholars to group similar folk narratives. We also evaluate to what extent unsupervised and supervised models correspond with how humans perceive narrative similarity. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Narratives; folktales; similarity; crowdsourcing
Measuring the similarity between documents is essential in many applications. For example, clustering systems are inherently dependent on the used similarity measure. How-ever, for many tasks it is unclear what an appropriate simi-larity measure should be. Multiple dimensions might play a role (e.g. topic, genre), and di ff erent users might not agree on which dimensions are important. So far, most research on text similarity has focused on topical or semantic similarity, thereby ignoring dimensions that might be important from a user X  X  perspective. Research investigating how humans per-ceive similarity between documents has been scarce so far.
Understanding how humans perceive similarity is useful in many situations. It could guide the development of simi-larity metrics to correspond better with human perception. Clustering systems could benefit by knowing along which di-mensions documents should be clustered. And, it could aid in the creation of more suitable datasets [2] and evaluation metrics [13].

In this paper, we study perception of similarity in the domain of folk narratives (such as fairy tales and urban leg-ends). With the increasing digitization of folk narratives [1, 16, 19], there is a need for better search and clustering systems [12]. However, so far little is known about how hu-mans perceive narrative similarity .Forexample,takethe following two narratives (summaries are shown):
The characters in both narratives are witches, humans and cats. Although the exact events are di ff erent, both narra-tives share a story line: The cats are actually witches, who are recognized by their wounds in their human form. Some people might even recognize that these narratives served a common purpose: demonstrating that witches are real.
Folktale researchers could recognize these narratives as belonging to the same story type (titled  X  Witch hurt as ani-mal; woman turns out to be wounded the next day  X , SINSAG 0640). A story type represents a collection of similar stories. Story types are used by scholars to organize folk narratives and defined in catalogues such as SINSAG and ATU. For ex-ample, a well-known story type is  X  Little Red Riding Hood  X  (ATU 333). The many variations of this story (e.g., with di ff erent endings) are classified with the same story type.
The above example illustrates that similarity between nar-ratives can be based on various dimensions (e.g. characters, plot, theme/purpose, story types). The goal of our study is to shed light on how narrative similarity is perceived. Which dimensions do people consider when judging narrative simi-larity, and do non-experts pay attention to di ff erent dimen-sions than experts?
Empirical studies on narrative similarity have only been done on a small scale so far (e.g., [9, 15]). This study is the first large-scale empirical study on narrative similarity. We collect data from a large number of non-experts using crowd-sourcing by asking them to rate similarity between narra-tive pairs. Data on how experts judge narrative similarity was collected in two ways: 1) By asking experts directly to rate similarity, in the same way as data obtained from non-experts. 2) By using the story types that the narratives are (manually) classified with.

To summarize, our contributions are as follows:
In this section we discuss related work on empirical studies of similarity, narrative similarity, and crowdsourcing. research on the human perception of similarity in various domains, for example images [21], style of paintings [14], text [2], multimedia files [31], music [17, 29] and videos [4]. Some of these studies also investigated which dimensions play a role in human judgement of similarity, for example of multimedia files [31] and preference judgements of search result lists [13]. The influence of structure, style, and con-tent on text similarity have been studied by B  X  ar et al. [2]. Compared to these previous studies, we collect and compare judgements from both experts and non-experts.
 Narrative similarity. Narratives have traditionally been studied by focusing on their plot structure. This is also reflected in research focusing on narrative similarity [20]. Scholars have typically approached this by developing for-mal systems to represent and find analogies between plot structures of narratives. The approaches rely on in-depth annotations of story structures by humans and as a result have stayed either theoretical [20] or have only been tested on small amounts of data (e.g. 1 narrative pair [8], or 26 Aesop fables [7]).

A di ff erent line of work involves a more computational approach but uses shallower features. For example, auto-matic classification of folk narratives [23] or jokes [10]. These methods focus on lexical similarity and do not study which dimensions play a role in perception of similarity. In addi-tion, their ground truth labels provide only a binary view of similarity.

Two recent studies investigated perception of narrative similarity, but on a very small scale (16 narrative pairs [15], variations of two stories [9]). Their results have suggested that non-experts also focus on dimensions other than struc-tural similarity [9], and that humans are more likely to rate narratives as similar if they have a common summary [15]. Crowdsourcing. Crowdsourcing enables the collection of large amounts of data with low costs using platforms such as Amazon Mechanical Turk and Crowdflower. We target Dutch workers in our study, who are fast and of high qual-ity [24]. Recent studies have explored how the crowd can be used to infer taxonomies [6] and clusterings from data [11]. However, such approaches need judgements for each item. An alternative approach is to use the crowd to learn a sim-ilarity metric, which can then be applied on large, growing collections (e.g., [32]). Our study follows the latter line of thought, by aiming to obtain insight into perceived similar-ity and develop automatic methods to measure similarity. Dutch Folktale Database. In this study, we use narratives from the Dutch Folktale Database. The database contains over 40.000 folk narratives [19], collected through various methods, including fieldwork and from social media. All nar-ratives have been manually annotated with metadata such as a summary, keywords, language, story type and named entities.
 Genres. In this study, we confine ourselves to the most fre-quent genres in the Dutch Folktale Database [22]:
Narratives belonging to the same story type do not nec-essarily occur in only one genre. For example,  X  Little Red Riding Hood  X  can be told as a fairy tale or as a joke. Story types. Story types are used by scholars to categorize similar folk narratives. A story type represents a collection of similar stories often with recurring plot, motifs or themes [27]. For example, the story type  X  Little Red Riding Hood  X  (ATU 333, [26]) is about a young girl who visits her grand-mother, but then is eaten by a wolf disguised as her grand-mother. Variations of a story emerge as stories are retold in di ff erent cultures and by di ff erent narrators. For example, in some variations the girl manages to escape from the wolf, and in other variations the story is transformed into a joke.
Story types are defined in catalogues created by various scholars. For example, the SINSAG catalogue focuses specif-ically on legends. While story types have been useful to or-ganize narratives, they also su ff er from limitations [5]. They provide a simplified (binary) view of similarity and story types do not always group narratives on the same level of specificity. Some story types are very specific, grouping nar-ratives that share a common plot (e.g.  X  Little Red Riding Hood  X ). Other story types group narratives that share a common structure (e.g. repetition), and the broadest cate-gory of story types only share a common theme (e.g.  X  Anec-dotes about Lawyers  X ).
In this section we describe the collection of the narrative similarity judgements. We selected a subset of the narratives from the Dutch Folktale Database. We restricted the set to narratives that were easily readable (based on writing style and length) and had all the required metadata to support our analyses. More specifically, we only kept narratives with the following re-quirements: 1) Written in Standard Dutch [28] 2) With an annotated story type, genre and collector 3) Of intermediate length (between 10 and 250 tokens).
To collect data, we designed a human intelligence task (HIT). The task was given to both experts and non-experts. Data from non-experts was collected by posting the HITs on Crowdflower, a crowdsourcing platform. We asked workers to judge the similarity between pairs of stories. We provided as few instructions as possible (e.g. by not mentioning terms like plot), so that workers were not influenced by us to pay attention to certain dimensions. Small pilot experiments were carried out while developing the design of the HIT. Each HIT consisted of 6 pairs of narratives (5 pairs + 1 pair with gold labels), and several survey questions. Each HIT was initially judged by 3 workers. We collected additional judgements for narrative pairs with large standard devia-tions of the judgements, such that all HITs received 3 to 5 judgements. We paid 40 US dollar cents for each HIT. Survey. To study the influence of characteristics of people on how they perceive narrative similarity, we included sev-eral survey questions: Similarity judgements. Workers were presented with pairs of narratives for which they were asked to rate the similarity on a scale from 1 (no similarity) to 5 ((almost) the same). A similar scale was used in related studies [18, 33]. Workers were also asked to provide a short motivation for their rating in a free text field for each narrative pair. The order of the displayed narrative pairs within a HIT was randomized. Gold labels. To improve the detection of spammers, we manually created 12 narrative pairs with  X  X old labels X . Work-ers who provided ratings deviating from these labels were
No education, Elementary school, Pre-vocational sec-ondary education, Senior general secondary education/pre-university secondary education, Secondary vocational edu-cation, Higher professional education, University education
Daily, several times a week, several times a month, never
Fiction, non-fiction, both identified as potential spammers. We created pairs with high similarity by copying an existing story and making small ed-its in spelling, punctuation, word order, etc. For such pairs, we expected a similarity judgement of 4 or 5. We also se-lected pairs with very low similarity by manually selecting stories that had nothing in common (e.g. plots and charac-ters are completely di ff erent). For such pairs, we expected a similarity judgement of 1 or 2. We did not inform workers about their performance on the pairs with gold labels.
Selecting pairs at random would generate many pairs with little similarity. Therefore, we control the selection of pairs as follows: 1. Similarity between narratives classified with the same 2. Similarity between narratives classified with the same 3. Similarity between narratives with the same genre, but
Under conditions 1 and 2, the narratives are the same based on their story types. We include pairs by varying the lexical similarity of these pairs based on cosine similarity. A threshold (based on data analysis, see below) was calculated to distinguish between low, mid and high similarity. We include an equal number of pairs from each bin.

Under condition 3, we only include pairs that have a high cosine similarity. We assume that pairs with low or mid similarity are less interesting, since they have little lexical similarity and are also not similar based on their story types. Thresholds. We first group all narratives by story type. For each story type, we randomly select a pair of narratives and calculate the cosine similarity. Based on the samples, we take their 33% and 67% boundaries to define the thresholds to distinguish between low, mid and high cosine similarity. ratives that are classified under the same story type but under di ff erent genres. We first generate candidate pairs: For each story type:
The final selection is made by sampling from all the can-didate pairs, given the desired distribution for the cosine similarity bins and the number of pairs to include. tween pairs belonging to the same genre and story type, but with varying levels of cosine similarity.
 For each genre:
The final selection is made by sampling from the candidate pairs ensuring an equal distribution across cosine similarity bins, given a desired genre distribution and the total number of pairs needed. belonging to di ff erent story types but with a high cosine similarity. We create candidate pairs as follows: For each genre:
The final selection is made by sampling from the candidate pairs given a desired genre distribution.
The designed HITs were given to two di ff erent groups: crowdworkers and folk narrative researchers.
 Crowdworkers. We posted the tasks on CrowdFlower and targeted workers from the Netherlands. The jobs ran be-tween April 4, 2014 and April 27, 2014. We launched the jobs in several batches, to prevent workers from doing the task too many times and to ban spammers in between. Po-tential spammers were identified by the following criteria:
We manually checked if workers identified using these cri-teria were spammers. Such workers were excluded from the dataset and blocked for all next HITs. We collected in total 923 HITs (150 workers). 619 HITs (80 workers) were kept after filtering spammers. Figure 1 shows the average times spent on a HIT for workers (median: 677.5 seconds). Folktale Researchers. We also asked three senior folktale researchers (all with a researcher/lecturer position) to do the same task. We selected 40 narrative pairs, ensuring that we included at least 2 pairs from each bin according to our sampling method described above. HITs were the same as presented to the crowdworkers, but without the pairs with gold labels (thus resulting in 5 narrative pairs per HIT). The statistics of the collected data are shown in Table 1.
Statistic Crowdworkers Experts # unique narrative pairs 1002 40 # completed HITs 619 24 # persons 80 3
In this section, we analyze the collected data. We start with studying the demographics of the workers and then continue with an analysis of their similarity judgements. Demographics. Workers are mostly men (66%), but are relatively spread across di ff erent ages and education levels. The workers are spread throughout the Netherlands, but most workers come from the west of the Netherlands (where the population density is higher as well).
 users X  reading and movie-watching behaviour. Most peo-ple read both fiction and non-fiction (52, 65%), and some read only fiction (20, 25%). A small fraction only reads non-fiction (8, 10%). We code the education responses, and movies and reading behaviour by converting each category to an integer. We find that the education level is highly correlated with frequencies of reading a book (Spearman X  X   X  = . 424, p &lt; 0.001). The education level is negatively cor-related with the frequency of watching movies (Spearman X  X   X  =  X  . 229, p &lt; 0.05). Watching movies and reading books is not correlated (Spearman X  X   X  =  X  . 086, not significant).
Frequency Figure 1: Average time spent on task
Workers also indicated how well they understood the pair of narratives on a scale from 1 (not understandable) to 5 (well understandable) (Figure 2). Manual inspection of pairs with lower ratings, revealed that crowdworkers had di ffi culty with language use that was less standard (e.g., dialects, slang, uncommon words), unconventional style and struc-ture. Narratives from more modern genres, urban legends and jokes, are understood better than narratives from the older genres, legends and fairy tales (Table 3).
 Figure 2: Understand-ability ratings
For each worker, we calculate the worker X  X  understandabil-ity bias, by calculating the average di ff erence between the worker X  X  score and the average of the scores. We find that higher educated workers tend to give lower understandabil-ity scores (Spearman X  X   X  =-.249,p &lt; 0.05). While this may seem counterintuitive, they also vary more in their un-derstandability ratings (Spearman X  X   X  =.278,p &lt; 0.05). We found no significant correlations with reading or movie watching behaviour. In the remainder of this paper, we only keep narrative pairs that received an average understand-ability rating of 3.5 or higher (removing 104 pairs). We first analyze the agreement between the judgements. Next, we study the similarity judgements for di ff erent con-ditions. Finally, we study the similarity dimensions by ana-lyzing the free-text motivations. Crowd Workers. We first analyze the agreement between crowdworkers. We have in total 80 workers and 898 pairs. For each pair, we have 3 to 5 judgements.

Figure 3 shows a histogram of the standard deviations of the judgements for each narrative pair. We find that 85% of the pairs have a standard deviation less than 1. We also calculate a user bias. For each worker, it is the average over the di ff erences between a judgement made by a worker and the overall mean of the judgements for each narrative pair. Only 17.5% of the workers are on average more than 0.5 points o ff the mean.

We calculate several agreement metrics (see Table 5). Fol-lowing [18], we calculate a measure of inter-rater correlation. For each narrative pair, we select at random a judgement and correlate it with the average of the other similarity judgements. We also calculate a pairwise agreement. For each narrative pair, we check the agreement between all pairs of workers. The reported pairwise agreement is the number of pairs workers agreed on divided by the total number of pairs, and is similar to the value reported in [33]. We also find that mapping the scores to a lower number of categories (1-2, 3, 4-5) leads to a higher pairwise agreement of 0.517.
We also analyzed the influence of demographics on agree-ment. We find that people who read books daily (3) or never (0) tend to agree more within their group (pairwise agreement of 0.433 and 0.444, see Table 6). We also tested excluding groups with lower reading frequencies. Only in-cluding workers who often read (  X  2) leads to higher agree-ment (0.374) than including all workers. No clear trends were observed with watching movies or education.
 Table 6: Pairwise agreement and reading frequency Experts. We calculated the agreement between experts in the same way as with the crowd. Table 5 shows the cal-culated metrics. We find that experts achieve higher inter-annotator agreement than the crowd, probably because their reasoning involves story types and they agree more on which dimensions are important (see also the next section). We also study to what extent individual experts and the average of the experts correspond with the crowd judgements (Ta-ble 7). Averaging the expert judgements leads to a higher correlation with the crowd judgements.
 Table 7: Spearman correlation of individual experts (E1-3) and average expert with the crowd
The average similarity for each condition (see Section 4.3) is shown in Table 4.
 Story types. We first investigate how story types corre-spond with judgements by the crowd. We would expect narratives belonging to the same story type to receive higher ratings than narratives belonging to di ff erent story types.
Narrative pairs (with high cosine similarity) with the same story type indeed receive higher ratings than pairs (with high cosine similarity) with di ff erent story types (Table 4).
Figure 4 shows a histogram of the similarity ratings for narrative pairs belonging to the same story type. If story types would correspond strongly with perceived similarity by non-experts, we would see a skewed distribution with most of the ratings being a 4 or 5. Instead, most of the ratings are in the middle and the perceived similarities of the narratives belonging to the same story type vary widely. Figure 4 also shows a histogram of the similarity ratings for narrative pairs belonging to di ff erent story types. Here, we do see a skewed distribution, with most scores being low (e.g. 1 or 2).

Thus, although narratives belonging to the same story type tend to be perceived as more similar than narratives belonging to di ff erent story types, story types do not explain all of the observed variation in similarity judgements by non-experts. This suggests that story types ignore dimensions that non-experts do find important.
Figure 5 shows figures based on expert judgements. The figures reflect that experts use story types in their research and give more extreme scores than non-experts. Pairs of narratives belonging to the same story type are mostly rated with a 5, narratives belonging to di ff erent story types often with a 1 or 2. However, we do observe variation indicating that other aspects influence their judgements as well. For example, based on their feedback, we find that experts tend to rate pairs of narratives from broad story types (e.g., based on theme) lower than story types defined based on plots. Genres. Table 4 also shows the scores for narratives with the same story type but classified under di ff erent genres . We find that narrative pairs with di ff erent genres tend to receive a lower similarity judgement than pairs belonging to the same genre. In the next section we study the influence of genre using the provided free-text motivations. Cosine similarity. In Table 4 we observe that within each genre, a higher cosine bin results in a higher average sim-ilarity judgement. In a later section, we experiment how the similarity judgements correspond with various super-vised and unsupervised similarity metrics.
For each similarity judgement, we also asked for a free-text answer with a motivation. In this section, we study the importance of di ff erent similarity dimensions (e.g., plot, characters) based on these motivations.
 Crowdworkers. Motivations given for the narrative pair in the introduction are shown in Table 9. The first worker only mentions a similarity in the characters, the other workers also see a similarity in the plot. Note that other dimensions could have (unconsciously) influenced the workers as well, but they did not mention them.
 Table 9: Translated motivations by crowdworkers
We randomly selected 192 narrative pairs and included all motivations for these pairs (total: 589). The most frequent dimensions were identified after annotating subsets of the data. Each motivation was then manually annotated (Table 8) by one coder. A second coder annotated a subset of 64 narratives. Cohen X  X   X  ranged from moderate (e.g., plot:  X  = 0.59, style:  X  =0.66,theme:  X  = 0.59) to high (e.g., genre:  X  = 0.80, characters:  X  = 0.88, number of details:  X  =1.00).
The characters, plot, genre and theme were mentioned the most. However, a variety of other dimensions were men-tioned as well (e.g., style, number of details). No explanation was given in 18% of the motivations.

For each dimension (except  X  X one X  and  X  X ther X ), we anno-tated whether a di ff erence and/or similarity was mentioned, e.g.  X  X lots are di ff erent X  (Table 8). When workers mentioned the characters, plot or theme, they tended to focus on the similarities between the narratives. However, when they re-ferred to the amount of detail, workers only stated di ff er-ences. Dimension Description MSimDi ff P MSimDi ff P
Characters The characters or important objects in a narrative None E.g.,  X  they are not the same  X  .18 --.51 .13 --.67 Table 10: OLS model (weights and standard errors).  X  X  X  X  p &lt; 0.001;  X  X  X  p &lt; 0.01;  X  p &lt; 0.05; ! p &lt; 0.1
For most dimensions, whether they are mentioned is in-fluenced by the presented narrative pair. For example, the probability of a random motivation mentioning theme is 0.28. However, knowing that another worker has mentioned theme for the same pair, the probability goes up to 0.51. To study the importance of these dimensions, we fitted an Ordinary Least Squares model (OLS) with the given score as the dependent variable (Table 10). We find that plot, genre and theme are the most important. Characters are not sig-nificant after including the other dimensions. Maybe sur-prisingly, mentioning di ff erences between style and number of details receives a positive weight. From manual inspec-tion, we find that when narratives are already very similar on other dimensions, workers tend to mention these more superficial di ff erences.

We also analyzed the correlation between characteristics of workers (education, frequency of watching movies/reading books). For most of the dimensions, we did not observe a relation with the characteristics of workers. People who read more books more often mention the theme of a narrative (  X  =.223,p &lt; 0.05). We also found that people who watch more movies more often pay attention to whether narratives di ff er in number of details or length (  X  =.210,p &lt; 0.1). Experts. Motivations given by the experts for the narra-tive pair in the introduction are shown in Table 11. Statis-tics based on manual annotation are shown in Table 8. The dimensions  X  X tory types X  and  X  X otifs X  are used in folk narra-tive research. Motifs are small elementary building blocks of plots of narratives (e.g.,  X  X isease caused by witchcraft X ). As expected, motifs and story types were only mentioned by the experts. Story types were mentioned in many of the motivations (46%).

Other dimensions important to experts are the plot, char-acters and theme of the narratives. Style, whether true facts are recounted, and setting are not important to experts.
In this section we present preliminary experiments on how well unsupervised and supervised methods correspond with the crowd judgements. Studies on document similarity in other domains found low to moderate correlations between automatic measures and human judgements. For example, acorrelationoflessthan0.2wasobservedusingcosinesimi-larity [33] and between 0.5-0.6 using di ff erent binary, count-based and LSA-based measures [18]. To our knowledge, we are the first to perform such experiments on narrative simi-larity.
For each narrative pair, we take the mean of the received similarity judgements by the crowdworkers. We experiment with two di ff erent setups: 1) Classification, where the goal is to classify the pairs into low ( &lt; = 3) and high ( &gt; 3) sim-ilarity. The performance is reported using the F-score. 2) Regression, where the goal is to predict the mean of the re-ceived judgements. We evaluate the performance using the Spearman correlation and Mean Squared Error (MSE).
We randomly divided the dataset into a training and test set. Feature development and parameter tuning was done using cross-validation on the training set. Like in the previ-ous sections, we excluded the narrative pairs that received a low score for understandability. Statistics of the dataset are shown in Table 12. The documents were parsed using the Frog parser [30] and a stop word list of 76 frequent Dutch words was used.
 Set # Pairs Mean Low High Train 498 2.674 344 (69.08%) 154 (30.92%) Test 400 2.683 271 (67.75%) 129 (32.25%)
We experiment with both unsupervised similarity met-rics (e.g., cosine similarity) and supervised machine learning models. We use linear regression and logistic regression with Ridge (L2) regularization to prevent overfitting.
We evaluate a variety of features, most of them based on the dimensions we identified in the previous section. In addition, we explore features based on manually annotated metadata.

First, we study the e ff ectiveness of features that only mea-sure lexical similarity. We experiment with di ff erent metrics (cosine similarity and Jaccard index) and representations (e.g., words versus character ngrams).

We also extract features from the narratives to approx-imate elements such as the plot, characters and theme in narratives. Plot elements are approximated by extracting subject-verb pairs. They are extracted by searching on sub-ject (  X  X u X  ) and verb complement (  X  X c X  ) relations from the Frog parser. Each  X  X lot element X  is a character + root of a verb (e.g.,  X  X awyer answer X  or  X  X irl disappear X ).
We extract the characters of a narrative by searching on subject (  X  X u X  ) relations from the Frog parser. Only tokens classified as nouns, pronouns, or as  X  X pecial X  are included. Unfortunately, the narratives are noisy because they come from a variety of sources, and therefore the Frog parser some-times missed relations or incorrectly extracted them.
Themes are extracted using LDA [3]. We train a model on the training documents with 20 topics using the Gensim library [25]. We measure the similarity between the topic distributions using the Jensen-Shannon divergence.
Crowdworkers also pay attention to style .Wetherefore experiment with features that capture stylistic similarities based on statistics such as the length of words and sentences, and similarities in POS structures.

Our analyses also revealed that di ff erences in the amount of detail in narratives play a role. We use the di ff erence in length of the narratives to approximate this dimension.
We also study the usefulness of manually annotated meta-data . They also capture dimensions identified in the previ-ous section, such as whether the narratives have the same genre (mentioned by the crowd and experts), or story type (only mentioned by experts). In addition, we study whether manually annotated keywords and named entities are useful. Below is an overview of the used features: 1. Cosine similarity 2. Jaccard index 3. Plot 4. Theme (LDA) 5. Characters 6. Absolute di ff erence between average word length 7. Absolute di ff erence between average sentence length 8. 1-3 ngram POS patterns (Jaccard) 9. Absolute length di ff erence 10. Same story type (boolean) 11. Keywords (Jaccard) 12. Same genre (boolean) 13. Named Entities (Jaccard)
We first study the individual features. Next, we study the performance achieved by combining them.
 Individual features. We first evaluate the individual fea-tures in the regression setup. We report the Spearman cor-relations and MSEs (Table 13).

For the lexical features, we experimented with using the cosine similarity and Jaccard index. We also experimented with using word unigrams, word unigrams + bigrams, or character ngrams (of lengths 2-5). We find that using n-grams consistently achieves a better performance. In ad-dition, the Jaccard index performs better than the cosine similarity.
We find that the stylistic features (POS patterns, word and sentence length) only obtain a low correlation. The fea-tures that aim to capture the story elements (e.g., theme) perform moderately. The features based on manually an-notated metadata perform well, in particular the features based on story types and keywords.
 using supervised machine learning models. We evaluate them in regression and classification tasks (Table 14). (Lexical + story elem. + (Lexical + story elem. +
We find that a reasonable performance is obtained using only the lexical features. Although the story elements fea-tures alone (plot, characters, theme) obtained a moderate performance, they do not help improve on the performance using the lexical features. We suspect this has several rea-sons. First, the story elements features are directly derived from the text as well and therefore highly correlated with the lexical features. For example, we find a Spearman correla-tion of .468 between the characters feature and the Jaccard n-grams feature. In addition, manual inspection shows that the extracted story elements are noisy, and thus the extrac-tion of the features itself can be improved.

The metadata alone are already very e ff ective. However, one should keep in mind that for new narratives no metadata will be available.

Using only lexical + stylistic features a good performance is achieved. Adding the remaining features does not lead to improvements. However, the best performance is ob-tained using both the automatically extracted features and the metadata. While the obtained correlation is moderate (.592), we should keep in mind that it is a di ffi cult task. For example, when we randomly selected a judgement for each narrative pair and correlated that with the average of the remaining judgements, a Spearman correlation of .556 was obtained (see the section on agreement analysis).
We analyzed the relationship between story types and hu-man perception of similarity. While most narrative pairs from di ff erent story types are indeed perceived as not sim-ilar, within a story type there may be much variation. Di-mensions such as genre and style that do not play a role in the definition of story types, do play a role in perception of similarity. This suggests that a more nuanced view of narrative similarity is desired.

Our results highlighted that non-experts and experts dif-fer in how they judge narrative similarity. Therefore, how similarity between narratives is estimated should depend on the intended users and goal of the application.

We also found that non-experts vary in which dimensions they consider. Therefore, e ff orts to personalize systems that deal with narrative similarity could be an interesting direc-tion of research. In addition, to help users understand the output of an automatic system, explicit explanations of how narratives are related would be useful as well.

Our study has limitations. First, free-text motivations were used to study the importance of dimensions. Users only mentioned dimensions they considered relevant, but (uncon-sciously) they may have also been influenced by other di-mensions. Second, the mentioned dimensions and provided ratings may also have been influenced by the previous pairs a user has seen. We randomized pairs within a HIT to re-duce possible e ff ects of displaying order. However, further research is needed to study the influence of sampling and displaying order on the user judgements. Third, to enable a large-scale experiment, we included a large number of nar-ratives from the Dutch Folktale Database. While we posed several restrictions to the final set to improve readability and also asked workers to indicate whether they understood the narratives, unclear or noisy narratives may have led to noise in the obtained judgements and mistakes in the au-tomatic extraction of the features in the prediction exper-iments. Fourth, our experiments were performed on one specific dataset. Although we expect that our experimental setup can be used in other domains as well, other datasets (for example, movie reviews) should be used to verify this.
This paper presented a study on how humans perceive narrative similarity. A better understanding of narrative similarity is a first step towards better clustering and re-trieval systems dealing with narrative collections. Data was collected by asking crowdworkers and folktale experts to rate the similarity between narrative pairs. We analyzed the pro-vided similarity scores as well as their provided motivations. Our results showed that non-experts pay attention to more dimensions than experts, and that story types only give a limited view of narrative similarity.

Many of the identified dimensions can currently only be approximated in a shallow way using automatic methods. Further work is needed on automatically extracting dimen-sions such as style, structure, plot etc. of narratives to improve the automatic estimation of narrative similarity. Based on the findings in this paper, we plan to develop bet-ter clustering systems for narratives.

While this paper focused on a particular domain (narra-tives), we expect that the setup of the experiment and the types of data analyses performed can also be used to shed light on how similarity is perceived in other domains. This research was supported by the Folktales as Classifi-able Texts (FACT) project, part of the CATCH programme funded by the Netherlands Organisation for Scientific Re-search (NWO). We would also like to thank the folktale ex-perts for participating in the experiment.
