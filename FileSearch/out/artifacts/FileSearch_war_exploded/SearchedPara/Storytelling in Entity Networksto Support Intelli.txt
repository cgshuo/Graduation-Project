 Intelligence analysts grapple with many challenges, chief among them is the need for software support in storytelling, i.e., automati-cally  X  X onnecting the dots X  between disparate entities (e.g., people, organizations) in an effort to form hypotheses and suggest non-obvious relationships. We present a system to automatically con-struct stories in entity networks that can help form directed chains of relationships, with support for co-referencing, evidence marshal-ing, and imposing syntactic constraints on the story generation pro-cess. A novel optimization technique based on concept lattice min-ing enables us to rapidly construct stories on massive datasets. Us-ing several public domain datasets, we illustrate how our approach overcomes many limitations of current systems and enables the an-alyst to efficiently narrow down to hypotheses of interest and reason about alternative explanations.
 H.2.8 [ Database management ]: Database applications X  Data min-ing ; H.3.3 [ Information storage and retrieval ]: Information search and retrieval X  Search process, Retrieval models Storytelling, connecting the dots, redescriptions, intelligence anal-ysis Intelligence analysts today are faced with many challenges, chief among them being the need to fuse disparate streams of data, and rapidly arrive at analytical decisions and quantitative predictions for use by policy makers. Although there are available catalogs of problem solving strategies suitable for intelligence analysis (e.g., see [1, 2]) and several visual analytic tools (e.g. [3 X 8]), our under-standing of underlying user needs is constantly evolving.
After a thorough user study, Kang and Stasko [9] suggested sev-eral design implications for systems supporting intelligence analy-sis. Along with several suggestions related to information manage-ment, two of the suggested design requirements are: (1) help an-alysts create a convincing production by supporting insight prove-nance and sanity checks, and (2) help analysts continuously build a conceptual model. In our work, we design an algorithmic frame-work to help intelligence analysts make connections by providing evidence and explanations to build a mental model.

Here, we focus on the task of storytelling [10], i.e., connect-ing the dots between disparate entities in an attempt to discover relationships between hitherto different concepts and suggest hy-potheses. For example, what is the connection between Osama Bin Laden and Igor Kolokov? What other entities relate them and what documents provide evidence for the underlying connections? Current tools in this space X  X .g., Entity Workspace [5], Jigsaw [8], NetLens [11], and Sentinel Visualizer [4]. Entity Workspace X  X se entity recognizers to infer a graph of relationships between enti-ties (see Fig. 1). (There are other tools that we do not survey here, e.g., Palantir [7], but their characteristics are similar.) Entity Workspace focuses more on information representation goals and does not provide graph exploration or summarization capabilities, whereas Jigsaw, NetLens, and Sentinel Visualizer provide explo-ration techniques with different emphases. In Jigsaw and NetLens, the user can traverse from entities to documents (in which the en-tities appear), and vice versa. Sentinel Visualizer supports explo-ration of the entity network and can hide document level details by using concepts of degree centrality, betweenness centrality, and closeness. In all cases, however, manual exploration becomes dif-ficult as networks get dense with increasing numbers of entities. More broadly, significant shortcomings of the above tools are:
A. Lack of support for  X  X vidence marshaling X  : The tools de-
B. Lack of support for explanation: Extracted and inferred rela-
C. Directed search: Current tools do not support automated di-
D. Syntactic constraints: Analysts bring in significant domain
E. Lack of support for entity disambiguation: A person named
F. The curse of large datasets and massive networks: All the The above shortcomings motivate our story generation methodol-ogy. A collection of documents is mined and indexed to provide efficient access to nearest-neighbor queries, and a storytelling al-gorithm is used to direct searches toward desired destination enti-ties. Various pre-and post-processing techniques are utilized to discover stories and explain them. Our contributions are: 1. Cliques: We introduce a new network model that employs 2. Explanation Chain (Clique Chain): To provide effective ex-3. Efficient Automated Search for Storytelling: We propose a 4. Syntactic Storytelling: User can provide syntactic constraints 5. Coreferencing: We use coreferencing to disambiguate pronom-6. Induced Similarity Networks: Our algorithms operate directly Intelligence analysis: Research work into software tools for intel-ligence analysis can be grouped into model-guided systems [12 X  14], multi-agent systems [15], graph-based analytic tools [16 X 19], and collaborative systems [5,20,21]. Our work falls in the category of graph-based analytic tools. We support exploration of the entity-entity graph (without materializing it) to help intelligence analysts to marshal evidence, explain connections, and form hypotheses.
Visual analytic tools: A different way to classify software tools is in terms of the activity they support, i.e., information foraging vs. sensemaking [22]. Some of the tools in the former category are IN-SPIRE [6], and NetLens [11]; they leave the reasoning processes to the analyst. Other tools, such as Analyst X  X  Notebook [3], Sentinel Visualizer [4], Entity Workspace [5], Jigsaw [8], and Palantir [7] focus more on the the sensemaking process, and while many of them ostensibly support information foraging, some of these tools are primarily for late stage sensemaking and presentation.
Connecting the dots: The  X  X onnecting the dots X  problem is not new and has appeared before in a variety of contexts: entity net-works [23], image collections [24], cellular networks [25], social networks [26], and document collections [10, 27, 28]. While some of these works can be adapted toward our problem context, the no-tion of a story in intelligence analysis often deals with relating enti-ties sequentially such that neighboring entities share commonality, whereas the above projects typically require a stronger connecting thread through all entities, not just neighboring entities. Swanson refers to the notion of neighboring commonality as complementary but disjoint (CBD) structures [29], whereby two arguments may exist separately that when considered together lead to new insights, but the objects exhibiting these two arguments are unaware of each other.

This paper builds upon our prior work in [10,30] in many impor-tant ways. First, we build stories through entity networks extracted Sufaat, and Seashore Hotel.
 connection.
 between Jose Escalante and Omar Hanif.
 from documents whereas these prior projects build stories through documents directly. This emphasis on entity networks is closer to how intelligence analysts reason about connections. Second, the present paper significantly generalizes the work in [10] by incor-porating the notion of cliques in stories, thus supporting evidence marshalling and explanation generation. Third, we present a novel optimization technique for large databases based on concept lattice mining to support faster story construction. A story between entities e 1 and e t is a sequence of intermediate entities e 2 , e 3 , ..., e t  X  1 such that every neighboring pair of entities satisfies some user defined criteria. We model entities and the doc-uments they occur in using a traditional vector space model. The problem of finding a story then can be modeled as a path search problem in the induced entity-entity graph E , but direct material-ization of E with hundreds of thousands of entities and billions of edges is infeasible. What is needed is support for directed explo-ration of the graph toward desired entities.

Given a start and end entity, our algorithm induces the network on the fly from the vector space model and finds a path. We allow the analyst to influence the story construction using two distinct criteria: clique size and distance thresholds. Given a story connect-ing a start and an end entity (see Fig. 2(a)), analysts can perform one of two tasks: they can either aim to strengthen the individual connections resulting in a longer path (see Fig. 2(b)), or they can organize evidence around the given connection (see Fig. 2(c)). We use the notions of distance threshold and clique size to mimic these behaviors. The distance threshold refers to the maximum accept-able distance between two neighboring entities in a story. Lower distance thresholds impose stricter requirements and lead to longer paths. The clique size refers to the minimum size of the clique that every pair of neighboring entities must participate in. Greater clique sizes provide more evidence and tend to provide longer sto-ries.

We use the term edge to refer to the basic unit of a direct link between two entities. A k -clique has k entities and is composed of k ( k  X  1) / 2 edges where every edge satisfies the distance threshold  X  .A clique chain is composed of a number of consecutive cliques connecting as start and end entity. Clique chains of Fig. 2(a, b, and c) are respectively composed of 2-, 6-, and 6-cliques. Applied distance thresholds to these three clique chains are 0.99, 0.99, and 0.93. Each clique chain provides alternative explanations for the re-lationship between the same pair of entities. We use the term story to refer to a relation between two entities via the junction entities of the corresponding clique chain. The stories are highlighted by thick lines in the clique chains of Fig. 2.

We use the Soergel distance between two entities e 1 and e measure the strength between them: where V ( e,f ) indicates the weight of feature f for entity the features are the different documents in which the entity appears. Let e ( f ) be the set of entities associated with feature f, and be the set of features associated with entity e . Soergel distance is a true distance measure: it is exactly 0.0 when the entities and e 2 have exactly the same features is symmetric, and obeys the triangle inequality. For entities in a document collection, the weight V ( e,f ) can be defined as where n e,f is the frequency of entity e in document f , | e ( f ) | number of entities in document f , | e ( j ) | is the number of entities in document j , and |E| is the total number of entities. Note that this is a variant of tf-idf modeling with cosine normalization in the entity-document space. Figure 3 summarizes our storytelling framework. The framework takes a document corpus as input, applies algorithmic approaches to handle the research issues in story generation, and outputs stories at the end of the pipeline. We describe the entire process in detail in this section.

Our overall methodology is based on using a concept lattice frame-work to structure the search for stories. A concept lattice [31] struc-tures the membership of entities in documents into sets of overlaps and relationships between these sets. Recall that the two parameters influencing the quality of the path X  X istance threshold and clique size X  X mpose a duality. The distance threshold is posed over fea-ture sets whereas the clique size is over entities. We use the clique size to prune the concept lattice during construction (by incorpo-rating it as a support constraint) and the distance threshold to select candidates for dynamic construction of paths. There are three key computational stages: (i) construction of the concept lattice, (ii) generating promising candidates for path following, and (iii) eval-uating candidates for potential to lead to destination. Of these, the first stage can be viewed as a startup cost that can be amortized over storytelling tasks.

We adopt the CHARM-L [31] algorithm of Zaki for constructing concept lattices. The second and third stages are organized as part of an A* search algorithm that begins with the starting entity, uses the concept lattice to identify candidates satisfying the distance threshold and clique size requirements, and evaluates them heuris-tically for their promise in leading to the end entity. In practice, we will place a limit on the branching factor ( b ) of the search, thus sacrificing completeness for efficiency. We showcase these steps in detail below, including the construction of admissible heuristics. We employ the notion of a concept lattice [31] to capture similar-ities between entities. Each concept/closed set is a pair: (entity set, document set). Concepts capture a maximal co-occurrence be-tween entity sets and document sets, i.e., it is not possible to add more entities to a concept without losing some documents, and vice versa. We order the entity list for each concept by the number of documents. Note that we can find an approximate set of nearest neighbors for an entity e from the entity list of the concept con-taining e and the longest document set. The concept lattice is a data structure that models conceptual clusters of entities and feature overlaps and is used here as a quick lookup of potential neighbors that will satisfy the distance threshold and clique constraints. Successor generation is the task of, given an entity, using the dis-tance threshold and clique size requirements, to identify a set of possible successors for path following. Note that this does not use the end entity in its computation. We study three techniques for successor generation: 1. Cover Tree Nearest Neighbor, 2. Nearest Neighbors Approximation (NNA), and 3. k -Clique Near Neighbor ( k CNN).

Among these three techniques, NNA and k CNN approaches are our contributions and the cover tree approach is used for compari-son purpose only. The cover tree [32] is a data structure for fast nearest neighbor operations in a space of entities organized alongside any distance Algorithm 1 NNA( e ) Input: An entity e  X  E fringe  X  ( e ) order by feature set size prospects  X  X  X  while fringe =  X  do metric (here, we use the Soergel distance [33]). The space com-plexity is O ( E ) , i.e., linear in the entity size of the database. A nearest neighbor query requires logarithmic time in the entity space O c 12 log ( n ) where c is the expansion constant associated with the feature set dimension of the dataset (see [32] for details). The second mechanism we use for successor generation is to ap-proximate the nearest neighbors of an entity using the concept lat-tice. We use the Jaccard coefficient between two entities as an in-dicator to inversely (and approximately) track the Soergel distance between the entities. In order to efficiently calculate an entity X  X  nearest neighbors, however, we cannot simply calculate the Jac-card coefficient between it and every other entity. We harness the concept lattice to avoid wasteful comparisons.

A formal description of our NNA algorithm is shown in Algo-rithm 1. The set ( e ) is the set of all redescriptions [31] which form the upper edge in the concept lattice where e appears. of Algorithm 1 refers to Jaccard coefficient between two entities is defined as which is a measure of how similar two entities are based upon how many features they share in proportion to their overall size. NNA returns better approximate redescriptions of an entity first. This is still, however, an approximation since it uses the Jacccard coeffi-cient rather than the Soergel distance. e The basic idea of the k CNN approach is, in addition to finding a good set of successor nodes for a given entity e , to be able to have sufficient number of them so that, combinatorially, they contribute a desired number of cliques. With a clique size constraint of is not sufficient to merely pick the top k neighbors of the given en-tity, since the successor generation function expects multiple clique candidates. Given that this function expects b clique candidates, minimum number m of candidate entities to identify can be cast as the solution to the inequalities:
The entity list of each concept of the lattice is ordered in the number of features and this aids in picking the top m candidate en-tities for the given entity e . We pick up these m candidate entities for o from the entity list of the concept containing the longest fea-ture set and redescription set containing e . Note that, in practice, the entity list of each concept is much larger than m and as a re-sult k CNN does not need to traverse the lattice to obtain promising candidates. k CNN thus forms combinations of size k from these entities to obtain a total of bk -cliques. Since m is calculated using the two inequalities, the total number of such combinations is equal to or slightly greater than b (but never less than b ). Each clique is given an average distance score calculated from the distances of the entities of the clique and the current entity e . This aids returning a priority queue of exactly b candidate k -cliques. We now have a set of candidates that are close to the current entity and must determine which of these has potential to lead to the des-tination. We present two operational modes to rank candidates: (i) the normal mode and (ii) the mixed mode. The normal mode is suitable for the general case where we have all the entities and features resident in the database. The primary criteria of optimality for the A* search procedure is the cumulative Soergel distance of the path. We use the straight line Soergel dis-tance for the heuristic. This can never overestimate the cost of a path from any entity e to the goal (and is hence admissible), be-cause the Soergel distance maintains the triangle inequality. The mixed mode distance measure is effective for large datasets where only important information is stored while other informa-tion is removed from the system after recording some of their ag-gregated information to save on space and cost. With the mixed mode approach, for simplicity, we assume that all the information about items outside the concept lattice are absent but some of their aggregated information like number of features truncated are pro-vided. Figure 4 shows the distribution of common and uncommon features of entities e 1 and e 2 inside and outside a concept lattice.
Figure 5 shows our formula for the mixed mode approach. Con-sider the set of features T O that do not appear in the lattice due to the support threshold of the concept lattice, minsup . Some fea-tures of T O can be common to both entities e 1 and e 2 . | U 2 O | are the numbers of uncommon features in entities e 1 which are thus outside the frequent concept lattice. Length a known variable due to the recorded aggregated information, but | U 1 O | and | U 2 O | are unknown. This is why | U 1 O | and not appear in D mixed ( e 1 ,e 2 ) . For D mixed ( e 1 ,e all the features of T O (i.e., features outside the lattice) are com-mon in both entities e 1 and e 2 and all these features have the same weight which is max( maxw ( e 1 ) , maxw ( e 2 )) .

To be able to use D mixed ( e 1 ,e 2 ) as a heuristic, it should be men-tioned that D mixed ( e 1 ,e 2 ) must never overestimate the original So-ergel distance D ( e 1 ,e 2 ) . The proof is omitted due to space con-straints. The aim of the algorithms we have described so far is to support in-telligence analysts in marshaling thoughts and evidence in order to generate hypotheses and then to generate defensible and persuasive arguments on hypotheses that are most favored by the evidence. Note that significant domain knowledge is still required to analyze a dataset to find a set of good stories. This raises some implementa-tion issues regarding pre-and post processing, starting points, and exploration constraints. This subsection describes these implemen-tation issues. We used a number of named-entity recognition (NER) APIs to ex-tract entities from a document collection. We used multiple NER APIs because we observed that some named entities are better ex-tracted by one tool but missed by another. The NER APIs we used are LingPipe [34], OpenNLP [35], and Stanford NER [36]. We combined the entities extracted by these three named-entity recog-nizers and modeled the entities in the vector space model (entities are objects and documents are features). The k CNN approach already uses two constraints: clique size and distance threshold. It is possible to include a syntactic constraint to the successor generation process (Figure 6). For example, we can restrict the exploration in such a way that the junction nodes can be of certain types of entities, e.g., people or organizations. Other entity types such as places, dates, money amount, phone number, etc. are better used as surrounding evidence, and hence can be part of a clique but not as a junction point.

Figure 6(a) shows that the connection between  X  X sama Bin Laden X  and  X  X gor Kolokov X  has a place  X  X airo X  as a junction node. One way to interpret this connection is that Laden and Kolokov vis-ited Cairo. This interpretation becomes unimportant if the dates of their travel to Cairo are far apart. On the other hand, Figure 6(b) shows that Laden and Kolokov are connected via  X  X aeed Hasham X . This new junction node warrants further investigation about  X  X aeed Hasham X . In the Atlantic Storm dataset discussed later, Saeed Hasham is a direct recruitee of Osama Bin Laden and Igor Kolokov is recruited by Saeed Hasham. There is no direct connection be-tween Laden and Kolokov. Therefore, a syntactic constraint ap-plied on the successor generation module can provide better con-nections. In the experimental results section we provide perfor-mance and quality comparison between two strategies: with and without person/organization as junction nodes (Section 5.4). Coreference is the art of determining when two entity mentions in a text refer to the same entity. We use LingPipe X  X  heuristic coref-erence package [34] to resolve pronominal references to entities. LingPipe X  X  coreference package is based on a greedy algorithm called CogNIAC [37] that visits each entity mentioned in a doc-ument in order, and for each mention either links it to a previous linked chain of mentions, or begins a new chain consisting only of the current mention. The resolution of a mention is guided by matchers and anti-matchers that score candidate antecedent men-tion chains based on properties such as the closest matching alias (using a complex comparison allowing for missing tokens), known alias associations, discourse proximity (how far away the last men-tion in a chain is and how many are intervening), and entity type. Figure 7 shows a simple example where  X  X olokov X  and some pro-nouns are replaced by  X  X gor Kolokov X  using this approach. The question of how to find a starting point is very subjective and depends on the analyst X  X  objectives. Some systems choose use graph-theoretic metrics such as betweenness centrality to identify key players in an entity network. We found significant benefits in classifying documents and using entities extracted from specific classes as candidate starting points. We use the AlchemyAPI [38] to assign each document to its most likely topic category (news, sports, business, law and crime, etc.). The analyst X  X  further perusal of documents in each category helps narrow down the number of documents that might contain potential start and end entities. In section 5.6.2, we describe how we can reduce the start set of docu-ments to solve a particular task with the VAST 2011 dataset. So far, we have discussed our framework in the entity space. That is, the network we consider has entities as nodes and each entity is modeled as a vector of documents. Storytelling provides differ-ent explanations of a relationship between two entities as form of clique chains where each node of the clique chains is an entity. It is possible to reverse this model and traverse in the document space so that we can obtain clique chains of entities. In this reverse case, documents are modeled as vectors of entities and we can find ex-planation clique chains for a pair of documents instead of a pair of entities. This reverse modeling is sometimes useful where the in-vestigation is less evidence based and the task is to find a small plot from a large volume of documents. In section 5.6.2, we explain how when we used documents as objects and entities as features, we can identify an imminent threat to a hypothetical city. Due to confidentiality reasons, our group is unable to disclose many intelligence analysis datasets that we have applied our algorithm on. For this paper, we demonstrate results on many public domain datasets, including i) the Atlantic Storm dataset developed in the Joint Military Intelligence College [39], ii) two datasets from the annual VAST challenge contests in 2010 and 2011, and iii) a dataset of politicians that we harvested from Wikipedia text. These datasets range from having 779 to 230,627 entities. We use both quantitative and qualitative measures for our evalua-tion. To compare the performance of different successor genera-tion strategies, we assess the number of nodes explored against the length of the story, and whether this growth is linear or exponen-tial. For a numerical measure of story quality, we assess the pair-wise Soergel distances between entities in a story (both consecutive and non-consecutive ones) and distill these distances into a disper-sion coefficient. The dispersion coefficient assesses the overlap of the features of the entities of a story (not the graph structure of the clique chain) and, thus, evaluates the story, not the clique chain. An ideal story is one that meets the Soergel distance threshold only be-tween consecutive pairs whereas a non-ideal story  X  X versatisfies X  the distance threshold and meets it even between non-consecutive pairs. If n entities of a story are e 0 , e 1 , ... , e n  X  1 dispersion coefficient as: where disp
Thus, the dispersion coefficient is 1 for an ideal story and 0 in the worst case when every pair of entities of the story satisfies the distance threshold. Finally, we describe below the analysts inter-pretations and conclusions from the discovered stories. All the re-sults presented in this paper were obtained using a regular desktop computer with Intel Core2 Quad CPU Q9450 @ 2.66GHz and 8 GB physical memory. The goal of this experiment is to assess the number of nodes ex-plored by the A* search and the time taken as a function of the dis-covered path length, and as a function of different successor gen-eration strategies. For this purpose, we used the VAST11 dataset and aimed to generate 10,000 stories between randomly selected entity pairs, with a distance threshold of 0.95. Figure 8 depicts the results of the successful searches. As Figure 8 (left) shows, the cover tree and NNA approaches require much more number of node exploration than k CNN with k =2 and 7. The runtime trends shown in Figure 8(right) also mirror the number of nodes explored in Figure 8(left). This result is not surprising, as the cover tree al-gorithm does not factor into the clique constraint, thus preventing it from taking advantage of the search space reduction that this con-straint provides. NNA does take advantage of this constraint, how-ever, and it generates a strict ordering on the Jaccard X  X  coefficient over the cliques, whereas k CNN simply generates some b candidate cliques. In practice, the k CNN relaxation results in the discovery of candidate cliques more rapidly than the NNA algorithm, while still remaining accurate. In both cases a post processing step is neces-sary to determine if a given candidate does in fact meet the search threshold. Through the remainder of this paper, we thus use the k
CNN algorithm as it provides the best performance of the three algorithms. We use the VAST11 dataset to study the effects of varying distance and clique size thresholds. As expected, the number of possible sto-ries decreases monotonically with stricter distance and clique size requirements. Figure 9 shows the relationship between the largest available clique size as distance thresholds become progressively stricter. To explore this phenomena, we apply apply our algorithms on a dataset of politicians harvested from Wikipedia.org (contains 230,627 entities and 49,612 documents). We allow only person/organization type of entities as junction points of a clique chain in syntactic sto-rytelling. On the other hand, non-syntactic storytelling can use any type of entities as junction nodes. To evaluate the performance of these two approaches, we generated 10,000 random pairs of politi-cians and invoked syntactic and non-syntactic storytelling with a range of distance threshold and clique size. Figure 10 compares the results of syntactic and non-syntactic storytelling with those start and end entity pairs for which at least one story was found by both the approaches. Figure 10 (left) shows that the average dispersion is better (i.e., higher) for syntactic storytelling with any distance threshold. This plot also depicts an increase in average dis-persion with stricter (lower) distance thresholds for both syntactic and non-syntactic approaches.

Figure 10(middle) compares syntactic and non-syntactic versions of storytelling in terms of average clique size as a function of dis-tance threshold. It shows that the average clique size is smaller with the syntactic approach at any distance threshold. This indicates that the extra syntactic constraint applied to the successor generator re-duces the size of the neighborhood. As a result, the explanations (with cliques) are generally smaller than the non-syntactic version.
Figure 10(right) shows the distributions of the costs of the paths of the discovered stories for both syntactic and non-syntactic strate-gies. It shows that the syntactic approach discovers costlier paths than the non-syntactic approach. The extra syntactic restrictions at the junction nodes result in costlier and hence rare stories that the non-syntactic approach might not discover. Around 60% of the discovered stories for the non-syntactic approach have cost be-tween 4.0 to 8.0 where only 36.5% of the stories with the syntactic approach fall into this range. Note that both the approaches yield a right skewed distribution, but the syntactic approach has a longer right skew than the non-syntactic approach indicating generation of costlier stories. We used the VAST 2011 dataset to compare storytelling against an uninformed search. Figure 11 (a) shows that the use of Soergel distance heuristic improves the average branching factor over the vanilla BFS ( h =0). Overall improvement of average branching fac-tor is more than 50%. Figure 11(b) shows that the use of the straight line Soergel distance for the heuristic exhibits lower runtime than exploration by the A* procedure over BFS (h=0), for a range of different clique sizes. The average time saved is more than 60%, even with the smallest clique size k =2. The heuristic tends to save additional time as larger clique size requirements are imposed.
Figure 11 (c) and (d) respectively depict the average effective branching factor and average time to generate stories as functions of clique size, for different approaches. Despite the use of the trun-cated dataset, Figure 11 (c) and (d) show that the mixed mode gains due to the heuristic over the BFS have a similar trend to the normal mode of Figure 11 (a) and (b). Therefore, the mixed mode analy-sis offers a practical mechanism to provide the best possible gains from lossy datasets without time consuming remodeling of the vec-tor space (e.g., [10] uses a costly remodeling for post processing). In this subsection we describe some of the illustrative results we obtained using some benchmark intelligence analysis datasets. The Atlantic Storm dataset was developed in the Joint Military In-telligence College as part of an evidence-based case study. We ex-tracted 779 entities from the Atlantic Storm text dataset. An illus-trative example of some stories are already shown in Figure 2 which provides multiple explanations of the relationship between two per-sons, Jose Escalante and Omar Hanif, who play an important role in transferring biological agents from Europe to the Caribbean.
After analyzing some stories, the user obtains a mental model of the attack. An illustrative mental model is shown in Figure 12. It depicts that the conspiracy starts from Osama Bin Laden in Afghanistan. Two of his close contacts are Saeed Hasham and Fahd al Badawi. Once these two contacts are discovered it be-comes easy to find stories involving Igor Kolokov, Pyotr Safrygin, and Boris Bugarov. All these three people are former employees of Russian State Research Center of Virology and Biotechnology (Vector). Among them Boris Bugarov is known to be a bioweapon scientist. Another person named Abdellah Atmani, who works for Holland Orange Ship Lines, helps in smuggling the biologi-cal agents to the Caribbean from Morocco. After discovering the plot up to this point, a story involving four key entities (Escalante, Arze, Morales, and Sufaat) reveals that the biological agents will be transferred to the USA from the Bahamas. Omar Hanif and Adnan Hijaji are two key players who recruited Al Qaeda field agents to tansport the biological agents to the USA by several cruise ships. The VAST11 dataset contains 4,447 documents. The task is to find any imminent threat to the imaginary city named  X  X astopolis X . We extracted a total of 55,109 entities from 4,447 documents. To find a starting set to work with, we first classify each of the documents using AlchemyAPI (Section 4.4.4). We reduced the set of 4,474 documents down to 122 by selecting the articles that were classi-fied as  X  X aw and crime X  or were considered  X  X nclassifiable X . This narrows down the potential number of terminal entities (start and end) to a few hundred from 55,109 entities. (Note that once given a start and end point, our storytelling algorithm still uses the entire collection.)
Since the number of entities is too large for an analyst to work with and the possible imminent threat is hidden in the news articles, we modeled the documents as vectors of entities (as described in Section 4.4.5). In the actual solution plot there are only around ten entities and thirteen documents involved. There are other stories that can be discovered from this dataset but they do not pose any imminent threats to Vastopolis.

An example story from the dataset is: [03435.txt  X  00783.txt  X  02566.txt  X  01785.txt  X  03212.txt]. The start document con-tains an entity named  X  X aramurderers of Chaos X  which is the name of a terror group and the end document contains a report regard-ing a talk of professor (Edward Patino), another important entity in this plot. The algorithm selects documents 00783.txt, 02566.txt and 01785.txt as junction documents. The first two junction docu-ments (00783.txt, 02566.txt) are related to computer viruses, worm threats, and security flaws in the Vast University. The next docu-ment (01785.txt) is related to a Robbery at Vast University (equip-ments of Professor Edward Patino were stolen). 01785.txt is one of the key documents in the solution which is properly picked by our algorithm. Note that document 00783.txt is considered a  X  X alse lead X  in the solution planted by the VAST 2011 contest organizers. This study demonstrates that out algorithm was able to identify sev-eral key elements hidden inside a large corpora of news articles as well as a few false leads related to the actual plot. Several stories combined together and explanations provided by the cliques lead an analyst to the mental model of the whole plot. Unlike the VAST 2011 dataset, VAST 2010 dataset is composed of transcribed phone calls, email messages, and reports from fields agents. The task is to investigate arms dealing. We extracted a total of 621 entities from this dataset, of which 102 are person en-tities. After reading a few documents, the user becomes interested in two people: Igor Sviatoslavich and Viktor Bout. The user first uses syntactic storytelling so that the junction points are people or organizations, but syntactic storytelling reported that there is no story between the given entities. A broader search reveals the result shown in Figure 13, where the junction nodes are locations. Both the two intermediate cliques of the chain are validated in the solu-tion provided by the VAST 2010 Contest organizers. This story is part of an arms deal in the Iran region of the entire plot (called Iran network). Some places used to transport arms from North Korea to Iran were Colombo (Sri Lanka), Pyongyang, and Don Mueang. The story successfully reveals these locations. The story also re-veals another person  X  X oltanzadeh X  who is part of the Iran arms dealing network. We have presented a novel approach to storytelling between en-tities in large document collections, with applications to intelli-gence analysis. Our algorithms are both efficient at handling large datasets and effective at teasing out complicated plots from textual corpora. In the future, we aim to explore ways to automatically incorporate user feedback about presented stories, so as to dynami-cally adjust the weightings of documents and/or the similarity func-tions. This will enable analysts to preferentially explore certain types of stories featuring preferred entity types or subplots. We will also explore incorporating additional sources of data, besides text documents, e.g., social media. Finally, we plan to continue our dialog with intelligence analysts to further develop compelling software tools for analytics. This work is supported in part by the Institute for Critical Tech-nology and Applied Science, Virginia Tech, and the US National Science Foundation through grant CCF-0937133. [1] R. J. Heuer, Psychology of Intelligence Analysis . Center for [2] R. Clark, Intelligence Analysis: a Target-centric Approach . [3] i2group,  X  X he Analyst X  X  Notebook, X  Last accessed: May 26, [4] FMS Advanced Systems Group, FMS Inc.,  X  X entinel [5] E. Bier, E. Ishak, and E. Chi,  X  X ntity Workspace: An [6] PNNL,  X  X acific Northwest National Laboratory, INSPIRE [7] H. Khurana, J. Basney, M. Bakht, M. Freemon, V. Welch, [8] Information Interfaces Research Lab., Georgia Tech, [9] Y. Kang and J. Stasko,  X  X haracterizing the Intelligence [10] D. Kumar, N. Ramakrishnan, R. Helm, and M. Potts, [11] H. Kang, C. Plaisant, B. Lee, and B. B. Bederson,  X  X etLens: [12] R. Alonso and H. Li,  X  X odel-guided Information Discovery [13] A. Koltuksuz and S. Tekir,  X  X ntelligence Analysis Modeling, X  [14] K. Chopra and C. Haimson,  X  X nformation fusion for [15] E. Lindahl, S. O X  X ara, and Q. Zhu,  X  X  Multi-agent System [16] J. Gersh, B. Lewis, J. Montemayor, C. Piatko, and R. Turner, [17] T. Coffman, S. Greenblatt, and S. Marcus,  X  X raph-based [18] P. Crossno, B. Wylie, A. Wilson, J. Greenfield, E. Stanton, [19] G. Chin, O. A. Kuchar, P. D. Whitney, M. Powers, and K. E. [20] E. Bier, S. Card, and J. Bodnar,  X  X rinciples and Tools for [21] M. Chau, J. J. Xu, and H. Chen,  X  X xtracting Meaningful [22] P. Pirolli and S. Card,  X  X he Sensemaking Process and [23] L. Fang, A. D. Sarma, C. Yu, and P. Bohannon,  X  X ex: [24] K. Heath, N. Gelfand, M. Ovsjanikov, M. Aanjaneya, and [25] J.-P. Brassard and J. Gecsei,  X  X ath Building in Cellular [26] C. Faloutsos, K. S. McCurley, and A. Tomkins,  X  X ast [27] M. S. Hossain, J. Gresock, Y. Edmonds, R. Helm, M. Potts, [28] D. Shahaf and C. Guestrin,  X  X onnecting the Dots between [29] D. R. Swanson,  X  X omplementary Structures in Disjoint [30] M. S. Hossain, C. Andrews, N. Ramakrishnan, and C. North, [31] M. J. Zaki and N. Ramakrishnan,  X  X easoning About Sets [32] A. Beygelzimer, S. Kakade, and J. Langford,  X  X over Trees [33] A. Leach and V. Gillet, Introduction to Chemoinformatics . [34] Alias-i.,  X  X ingPipe 4.1.0, X  Last accessed: Jan 31, 2012, [35] Apache Software Foundation,  X  X penNLP, X  Last accessed: [36] Stanford Natural Language Processing Group,  X  X tanford [37] B. Baldwin,  X  X ogNIAC: High Precision Coreference with [38] AlchemyAPI.,  X  X lchemyAPI: [39] F. J. Hughes,  X  X iscovery, Proof, Choice: The Art and
