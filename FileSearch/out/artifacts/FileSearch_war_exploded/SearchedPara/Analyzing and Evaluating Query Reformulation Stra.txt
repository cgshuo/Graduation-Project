 Users frequently modify a previous search query in hope of retrieving better results. These modifications are called query reformulations or query refinement s. Existing research has studied how web search engines can pr opose reformulations, but has given less attention to how peopl e perform query reformulations. In this paper, we aim to be tter understand how web searchers refine queries and form a theoretical foundation for query reformulation. We study users X  reformulation strategies in the context of the AOL query logs. We create a taxonomy of query refinement strategies and bu ild a high precision rule-based classifier to detect each type of reformulation. Effectiveness of reformulations is measured us ing user click behavior. Most reformulation strategies result in some benefit to the user. Certain strategies like add/remove wo rds, word substitution, acronym expansion, and spelling correction are more likely to cause clicks, especially on higher ranked results. In contrast, users often click the same result as their previous query or select no results when forming acronyms and reordering words. Perhaps the most surprising finding is that some re formulations are better suited to helping users when the current results are already fruitful, while other reformulations are more effective when the results are lacking. Our findings inform the design of applications that can assist searchers; examples are described in this paper. H.3.3 [ Information Search and Retrieval ]: Query formulation Algorithms, Measurement, Human Factors Query reformulation, search effectiveness, query log analysis. Of the roughly 2 billion daily web searches made by internet users [8], approximately 28% are modifications to the previous query [29], also known as query reformulations or query refinements . For example, a user may search for  X  X izza Seattle X , but alter their query to  X  X ausage pizza Seattle X  if they are unsatisfied with the results from the initial query. Reformulations make up a large portion of web search activity. In a study of Dogpile.com logs, Jansen et al. [16] reported that 37% of search queries were reformulations when ignoring same queries. A study of Altavista logs [17] identified that 52% of users reformulated their queries. Search engines and humans both try hard to come up with appropriate query reformulations. Many web search engines today offer query reformulation sugges tions by, for example, mining query logs. Users are manually re formulating their queries based on the search results from the initial query, and their knowledge and experience of how search engines work. The reformulation process is an iterative endeavor between users and search engines in getting a satisfactory set of results. While the search engine side of query reformulation has been studied extensively by the search companies and in prior information retrieval resear ch, how users perform query reformulations has received less a ttention. Among the benefits to understanding how people search is being able to automatically propose query reformulations. If many users searching for  X  X ummus X  reformulate their query to  X  X ummus recipe X , the search engine can be proactive and suggest  X  X ummus recipe X  when the user searches for  X  X ummus X . Users can also benefit from an improved search experience when performing reformulations. Currently, search engines present the same interface regardless of whether the user gives it a new query, same query, or query reformulation. Being able to accurately detect when a user is making a query reformulation gi ves the search engine an opportunity to present an improved interface. The goal of this work is to look at the types of query reformula-tion users perform and evaluate th em using effectiveness metrics such as click data. In order to study these metrics, we first construct a taxonomy of query refo rmulation strategies adopted by users. Next, we build a classifier for these different types of reformulations. While there are some existing classifiers that determine whether a query is a refo rmulation, ours is the first to separate them into reformulation types. Our work makes three specific contributions: Much of the work on query reformulation for web search has focused on offering automatically generated query suggestions to the user. The suggestions are typi cally shown on the same page as the search results. These query suggestions are built into every major search engine today. Prior re search in this vein has explored computer-generated suggestions using query expansion [26], query substitution [22], and other refinement techniques [6][23]. Implicit relevance feedback from users is a common data source for computer-generated reformulations. For example, work by Baeza-Yates et al. [6] uses query logs to discover new query reformulations, finding similar que ries using a cosine function over a term-weighted vector built from the clicked documents. A study by Anick [3] showed that these automatically generated reformulations were as effectiv e as human constructed reformula-tions, using metrics such as uptake and click behavior. We process query logs containing raw search queries; therefore, to classify a query reformulation, we must first determine whether a query is indeed a reformulation instead of a new query. This is similar to the problem of det ecting query sessions and their boundaries. Jansen et al. define a session as  X  X  series of interac-tions by the user toward addressing a single information need X  [16]; Silverstein et al. [33] note,  X  X  session is meant to capture a user's attempt to fill a single information need. X  Therefore, sessions can be considered as a single query, followed by any number of reformulated queries. From this, our definition of a query reformulation is: a modifi cation to a search query that addresses the same information need. Further deriving from these definitions, we can conclude that if we were able to correctly identify the boundaries of all query sessions, we would know which queries are initial queries and which are reformulations. Conversely, by identifying which queries are reformulations, we would be able to accurately group query sessions together. Therefore, the problem of iden tifying query reformulations is similar to the problem of detecting session boundaries. Most existing work identifies sessions using a simple temporal strategy, where a specific time interval of inactivity represents a boundary. This method is simple to implement and the definition is unambiguous. He et al. [15] and Ozmutlu [28] used time and common words to determine sessi on cutoffs. Comparing several session detection algorithms, He et al. attained 73% precision and 62% recall using time only, and 60% precision and 98% recall using time and common words together. Arlitt [4] found session boundaries using a calculated timeout threshold. Murray et al. [27] extended this work by using hierarchical clustering to find better timeout values to det ect session boundaries. Their method had 97% precision and 76% recall on a human-classified dataset. More recently, Jones and Klinkner [21] presented evidence that any temporal cutoff is arbitrary and detects session boundaries no better than a random cutoff time. They evaluated the existing session boundary detection methods alongside their own. Their study reviewed these methods w ithout considering same queries. Using the optimal cutoff time, 5 minutes, query reformulations were accurately identified 63% of the time. Combining the optimal features from prior work, that is, common word + prisma (see [3]) + time, they achieved 84% accuracy. Using only Levenshtein edit distance resulted in 85% accuracy. Lastly, their own combination of methods resulted in the best accuracy, 87%. Many researchers have studied clic k data as indicators of search relevance. An early inquiry by Joachims [19] reveals that click data can indeed be used to improve search relevance. Several later studies agree that click data are indicators of search result preferences and discuss best me thods of analyzing click data [2][12]. Joachims et al. also find that analyzing clicks over query reformulations similarly provides useful information [20]. This data has also been shown to be helpful for improving search relevance [1][9]. Our study applies lessons learned from these reports of click data analysis. While they study the effectiveness of analyzing different click patterns, we study the effectiveness of reformulation strategies usi ng different click patterns. Taxonomies of query reformulati on have been developed for different types of search. A more comprehensive review of query reformulation in traditional inform ation retrieval can be found in [10]. Here we focus only on the taxonomies developed by analyzing query logs. These are generally constructed by examining a small set of query logs . Some studies are out of date or incomplete. None have built an automatic classifier distin-guishing reformulation strategies, as we have. Table 1 presents a mapping between our taxonomy of query reformulation strategies and the terminology for these strategies from prior work. Anick [3] classified a random sample of 100 reformulations by hand into elev en categories. Lau and Horvitz [24], Jansen et al. [16], and He et al. [15] used the same reformulation categories X  X erms taken from linguistics [18]. As part of a study of re-finding behavior, Teevan et al. [34] con-structed a taxonomy by looking through query logs, and implemented algorithms to detect a subset of the reformulation strategies. Whittle et al. [36] modeled some reformulation strategies using a graphical network. Bruza and Dennis [7] manually classified 1,040 queries into their own taxonomy. Guo et al. [13] also constructed a small taxonomy and used a conditional random field model to pr edict query refinements. Rieh and Xie [32] constructed conceptu al reformulation categories like content, format, resource; these are not included in the table because their abstract nature makes them difficult to map against concrete reformulation techniques. We constructed our own taxonomy by combining the types of query reformulation identified in prior work (Table 1). We implemented a matching rule for each strategy, which was iteratively improved to find the best unsupervised algorithm. For instance, the  X  X dd words X  rule was modified to detect added words even when the other words were reordered. To determine if we were missing any other rules or n eeded to adjust existing rules, we ran our classifier over th e AOL query logs and randomly checked the output. We optimized for reducing false positives while keeping false negatives low since we wanted a high precision classifier. From this, we tweaked several rules and added one that would detect a number of query reformulations that other rules did not, namely substring 2 . A few categories from prior work were either vague or difficult to detect, marked not detected in the table. For example, determining whether a query was a location reformulation as defined in [3] is subjective and would reduce the precision of our classifier. Categories marked not in data could not be classified because the queries were normalized (via lowercasing and punctuation removal) in our dataset. The query reformulation strategies (ordered by rule precedence) from Table 1 are described below in formal language notation. Let an underscore _ denote the space character; punctuation, represented by P comprises the three punctuation characters left in the query logs: the apostrophe, dash, and period; i.e. Let  X  be the alphabet of letters, digits, and punctuation, w is a word in that alphabet  X   X   X  i w , and z i composed from that alphabet or space character  X   X   X  including the empty string. Includes form acronym and expand acronym Includes substring and superstring In a word reorder , the words in the first (initial) query are reordered but unchanged othe rwise, producing the second (refomulated) query. This transformation can be defined formally using a recursive definition, Explicitly, either both queries contain the same two words but reversed, or removing the same word from both queries makes the second query a word reorder of the first query. The first condition is the base case and second c ondition is the recursive step. Example: seattle pizza palace  X  pizza seattle palace The second query is a whitespace and punctuation reformulation of the first query if only whitespace and punctuation are altered in the reformulation. This can be defined recursively, A whitespace and punctuation reformulation occurs when after removing a whitespace or punctuation character, the remaining queries are the same or the remaining second query is a whites-pace and punctuation reformulation of the second query. Example: wal mart, tomatoprices  X  walmart tomato prices A remove words reformulation is when any number of words is removed from the first query resulting in the same words in both queries. This reformulation neglects word order. Present Study Anick [3] Teevan [34]
Jansen [16], He [15], Lau [24] Whittle [36] Bruza [7] Guo [13] word reorder syntactic variant word order whitespace and punctuation remove words remove words / duplicates generalization D(k) DEL add words head, modifier add words, add stopwords specialization C(k) ADD url stripping domain stemming morphological variant stemming and pluralization M(k) DER word stemming acronym 1 acronym abbreviations ABR expansion substring 2 abbreviation spelling correction spelling misspellings M(k) SPE spelling correction * not detected elaboration, location reformulation S(k), s(k) * not in data capitalization, extra whitespace J(k) CAS Words are recursively removed from the first query until it is a word reorder or equal to the second query. The first and second conditions are base cases where eith er the two queries are equal or a word reorder. The third condition removes words along with the surrounding spaces from the first que ry and replaces them with spaces. Spaces are temporarily added to the left and right of the query to account for the leftmost and rightmost words. Example: yahoo stock price  X  price yahoo An add words reformulation occurs when one or more words are added to the first query. This reformulation applies even if words are reordered in the second query . It is easily defined as the reverse transformation of remove words , Example: eastlake home  X  eastlake home price index Users often append components from a URL into the query, mistaking the search box with their browser X  X  address bar. When they realize this, they will strip these strings from their query. This also happens in reverse, wh ere the user copies the target URL into the search box after searching. A url stripping reformu-lation occurs when the first and second queries are the same after removing  X .com X ,  X  X ww. X , a nd  X  X ttp X  from both sides. Let {} .com , www. , http_ , _http =  X  , This rule applies if there is some permutation of removing URL components from both queries that makes them the same. Example: http www.yahoo.com  X  yahoo A stemming reformulation involves ch anging the word stems in the first query. The rule stems every word in both queries using Porter X  X  stemming algorithm [30] and compares them. Let ) ( w P be the stem of the word w , Example: running over bridges  X  run over bridge A form acronym transformation occurs when the second query is an acronym formed from the first query X  X  words. Example: personal computer  X  pc An expand acronym transformation occurs when the first query is an acronym and the reformulation is a query consisting of the words that form the acronym. Example: pda  X  personal digital assistant A substring is defined as an instance where the second query is a strict prefix or suffix of the first query. Unlike the traditional definition of substring, this doe s not include instances where only inside characters of the first query are extracted. Example: is there spyware on my computer  X  is there spywa A superstring is defined as an instance where the second query contains the first query as a prefix or suffix. Example: nevada police rec  X  nevada police records 2008 An abbreviation reformulation is when corresponding words from the first and second queries are pref ixes of each other. This differs from substring which considers suffixes and only compares the entire queries. Example: shortened dict  X  short dictionary A word substitution occurs when one or more words in the first query are substituted with semantically related words, determined from the Wordnet database [10]. Two words are related if one is a semantic relation (synonym, hy ponym, hypernym, meronym, or holonym) of the other after both are converted to their base morphological form. This rule is im plemented in two steps. First, if the queries in their entirety are related, they are considered a word substitution; this detects substitutions of the entire query. Second, if every corresponding pair of words is the same or related, this is also a word substitution. Let the  X  operator represent a semantic relation between two words, including the case when the words are the same. Synonym: The two words have the exact same meaning. Example: easter egg search  X  easter egg hunt Hyponym: The first word is a specific instance of the second word. These are also referred as broad terms. Example: crimson scarf  X  red scarf Hypernym: The second word is a specific instance of the first word. These are also referred as narrow terms. Example: personal computer  X  laptop Meronym: The first word is a constituen t part of the second word. Example: finger  X  hand Holonym: The second word is a constituen t part of the first word. Example: automobile  X  wheel A spelling correction is detected using a conservative Levenshtein edit distance function [25]. This function maps well to a spelling correction a user would typically make, because it tracks the number of character edits between two queries. The queries are classified as a spelling correction reformulation if the Levenshtein distance is 2 or less. A threshold of 2 matches character swaps and missing characters. In the expression below, ) , ( b a z z L is the Levenshtein edit distance between strings z a and z b , Example: reformualtion  X  reformulation There are a few categories of reformulations which are not included in our taxonomy. They are difficult for our classifier to detect, and may even be difficult for a human to detect. We randomly sampled 200 of the 962 mi ssed reformulations from our evaluation data to get a general se nse of which reformulations our classifier missed. Three types of missed reformulations emerged, described in the next three subsections and quantified in Table 2. Humans can rephrase their queries in complex ways. Many rephrasings are difficult for even a smart algorithm to detect, requiring sophisticated semantic a ssociation at minimum. Context or pop culture knowledge may be needed. Example: easy raspberry mousse  X  cool whip mousse Example: how to calculate nutritional values  X  weight watchers calculator Users often perform more than a single reformulation strategy. For example, they may correct spelling and replace one word with a synonym. While a classifier can theoretically try combinations of reformulation strategies, this is difficult or even impossible because reformulation strategies do not have a commutative property. In other words, a differ ent ordering of strategies gives different results. For example, trying to detect spelling corrections after stemming will yield different results than doing so before stemming. Additionally, many refo rmulations obviously cannot be combined, such as word reorder and acronym. Add words and remove words together were not considered a multi-reformulation since any query can be transformed to any other query. The most common combinations of reformul ations in our sample were add words &amp; spelling correction , remove words &amp; spelling correction , url stripping &amp; whitespace and punctuation . Exploring the challenge of multi-reformulations is planned as future work. The following example demonstrates a multi-reformulation involving two reformulations: add words and spelling correction. Example: lane county gabrage  X  lane county garbage disposal Some instances of reformulation strategies were insufficiently matched by a classifier rule. Howe ver, fixing the rules to detect these reformulations would have introduced new complications. Our rule for detecting spelling correction used a Levenshtein edit distance of 2. While this achieve d high precision, the rule missed spelling correction involving three or more character edits. For example,  X  X metuer X  changed to  X  X mateur X . This is an example of the classic trade-off between precision and recall. We chose a lower threshold to optimize for a high precision classifier. Word substitutions are dependent on the Wordnet database. Substitutions absent from the databa se cannot be detected by our classifier. This limitation will likely be solved over time. Our rule for url stripping currently only removes the .com top-level domain from the query. Some queries involve other top-level domains or second-level domains which are not stripped. The list of top-level domains is not cons tant and there are an infinite number of second-level domains so capturing these reformula-tions requires a more sophisticated rule. The abbreviation detection rule only checked for a substring prefix for each word. There are cases in the English language where an abbreviation is not a substring prefix such as  X  X ept X  for  X  X epartment X . Table 2: Missed reformulations in sample evaluation data Classifiers commonly learn from a set of training data, which we refer to as machine learning classifiers. We developed a rule-based classifier instead of a machine learning classifier because our query reformulation strategies fit a procedural rule model better than a learning model. No prior work has developed a machine learning classifier that distinguishes different query reformulation strategies. Furt hermore, using a rule-based classifier allowed us to make deta iled adjustments to our classifier for special cases. An implementation of the classifier is freely available to the research community 3 . Source code: http://jeffhuang.com/reformulationClassifier.py The classifier reads the query log starting from the top and compares pairs of consecutive queries ( z a , z b ) from the same user. query z b is potentially a reformulated query. The query pairs are matched against the ordered re formulation rules defined in Section 3.1. If there is a match, th e second query is classified as a reformulation of the first query . Figure 1 shows the flow of queries into the classifier and se gmented into query types. Using the notation z n as the n th query in the query log example from Figure 1, we can see that ( z 1 , z 2 ), ( z 2 , z 3 ), and ( z but not ( z 3 , z 4 ) or ( z 4 , z 5 ) because z 4 was from a different user. Accuracy is the percentage of query pairs correctly detected as a reformulation. Existing measures of accuracy in most query reformulation research do not diff erentiate between precision, the percentage of query reformulations identified that are actually reformulations, and recall, the per centage of query reformulations identified. Our goal is to create a rule-based classifier with high precision, but not necessarily high recall. We deemphasize recall because we are studying the properties within each reformulation rather than between each reformulation. In other words, we are interested in inter-reformulation, rather than intra-reformulation, comparisons. For example, the pr oportion of URL clicks within each reformulation helps us unders tand the reformulations better than comparing the absolute counts of URL clicks between each reformulation. The magnitude of query logs provides sufficient events, so the analys is will still be generalizable and compelling even with lower recall. We manually classified every query from 100 users in the AOL query logs for evaluation. Esse ntially, this was a session boundary detection task. In total, there were 9,091 query pairs where we determined whether the second que ry was a reformulation of the first. Same queries were remove d (40.8% of queries), to avoid inflating classifier performance because they can be detected trivially. Of these pairs, we found 2,483 reformulations and 6,608 new queries, or 27.3% reformulati ons. This is very close to the 28% reformulations reported for this dataset [29]. Our classifier was evaluated on th is test data, marking the second query of each query pair as a reformulation if the query pair matched a reformulation strategy . Table 3 presents the results, comparing our classifier with machine learning classifiers. Our focus on precision rather than recall resulted in 98.2% precision which is 38% higher than reported in He et al and slightly higher than Radlinski and Joachim X  X  96.5%. Note that each study used a different set of query logs, so results can not be directly compared. Certain query logs are easier to classify than others because of the nature of the search engine and their users. Looking closer at the 1.8% (28 actual) queries that our classifier incorrectly determined to be a reformulation, we only found one case that was a true mistake. The other 27 were difficult to judge and debatable whether these were reformulations or not (see Section 5.3.3 for discussion). Therefore, we propose that our precision is even better than the 98.2% reported. Our results are extracted from the AOL query logs, which were released on August 3, 2006 [29]. The logs contain 36,389,567 queries from which our classi fier identified 16,069,421 new queries, 14,861,326 same queries, and 3,411,706 reformulations. Each line in the logs contains five fields: the query string, timestamp, the rank of the item selected (if any), the domain portion of the selected item X  X  URL path (if any), and a unique identifier for each user. We use effectiveness metrics to in fer the quality of search results. Past studies found that clickthr ough data and time spent predicted users X  satisfaction with the results [12]. Whether users clicked during the initial query and the reformulated query, which we call a click pattern, can be a predictor of search relevance [20]. We apply metrics learned from previous research to study the effectiveness of different reformulation strategies. These metrics are mostly based on click behavior and help show the usage pattern and effectiveness of specific reformulations. In our analysis, we also included new and same queries for comparison. Some reformulations are misidentified as new queries due to our classifier X  X  lower recall, but identifying new queries has no effect on our study of reformulated queries. Differences between reformulation strategies were all statistically significant, due to the large number of events in our dataset. A reformulation is composed of an initial query followed by a reformulated query. For each query, the user can decide to click or not click (skip) a result, creati ng 2 X 2=4 possible click patterns, presented in Table 4. Same queries, which inflate pr ecision, may have been included Table 4: Click patterns for queries and their reformulation Initial Query Click Skip A click pattern of Skip followed by Click (SkipClick) means the user did not click any result from their initial query, then reformulated their query and clicke d a result. This is an indicator that the user found the query reformulation to be effective. A Click followed by a Skip (ClickSkip) suggests that the reformula-tion did not help [20]. Similarly, two consecutive Clicks can be taken as successful searches, while two consecutive Skips as failed searches. Over all queries in the query logs, the ratio of clicks to skips was approximately 5:4. Figure 2 shows the proportions of the click patterns for each type of reformulation. A chi-squared an alysis verifies that the query reformulation type has a statistically significant effect on click The results show that different reformulation strategies have significantly different proportions of Clicks vs. Skips in the initial query. We can see this by looking at the ratios of SkipSkip + SkipClick to ClickClick + ClickSkip. Spelling correction , expand acronym , and superstring have high ratios, meaning people attempt these reformulations when they are unsatisfied with their initial query, perhaps due to a misspelled query or ambiguous acronym. In contrast, form acronym , remove words , word reorder , and word substitution have lower ratios, indicating the initial results may be somewhat relevant and users are further refining their query. Same queries have the lowest ratio as expected since users are unlikely to repeat a search using the same query if the initial results were unsatisfying; in fact, same queries usually have ClickClick patterns probably because they are re-finding queries [35]. These proportions are cons istent with our current under-standing of users. Comparing the proportions of Clicks vs. Skips in the reformulated query gives insight to whether the reformulation was helpful. Looking at the SkipSkip + ClickSkip to SkipClick + ClickClick ratios, we can see that reformula tion results were clicked about as often as new queries. This is a positive indicator for reformula-tions because it suggests users are as successful with reformula-tions as with new searches. The substring and superstring reformulations were least helpful, possibly because many of those reformulations were mistakes by users. Add words , word substitution , stemming , spelling correction , and expand acronym were most helpful under this comparison. We can also compare the proportions of Clicks vs. Skips in the reformulated query given a specific action in the initial query. We control the action variable in the initial query and regard the action in the reformulated query as the dependent variable. For example, we compare the ratio of SkipSkip to SkipClick to see whether a user is more likely to click if the initial action is Skip. Same queries behave as expected: if the initial query was Skip, the user is significantly more lik ely to skip the second query as well; if the initial query caused a click, the user is about 10 X  more likely to click than skip after searching with the same query. When the initial query causes a Skip, the spelling correction , expand acronym , and add words reformulations have the highest likelihood that the user will click. Likely explanations are that spelling correction and expand acronym fix incorrect queries and disambiguate acronyms, while add words narrows the search to make the results more relevant. In contrast, superstring , url stripping , and substring are least likely to help when the initial query results in a Skip. Different reformulations are effective when looking at initial queries that result in a Click. Wo rd substitutions , word reorder , and add words are the three most helpful reformulations in this condition. When a search provides relevant queries, users that subs titute words for related words, reorder their words, and add new words get better follow-up results. On the other hand, substring , superstring , abbreviation , and spelling correction are not useful when the initial query results in a Click. This is interesting because spelling correction is one of the most helpful reformulations when the initial action is Skip, but one of the least helpful reformulations when the initial action is Click. The next two metrics, Click UR L and Rank Change of Clicked Results, only apply in the case of a ClickClick pattern because rank and URL from corresponding clicks are used in the analysis. Users may be re-finding rather th an reformulating queries to retrieve better results. This can be observed by checking if the URL is the same between queries. We hypothesize that users click on the same URL in same queries (re-finding). There are some limitations to the analysis because the URLs in the AOL logs are truncated at the domain level for privacy. Figure 3 shows the proportions of clicked URLs which were the same for each reformulation type. A chi-squared analysis shows that reformulation type has a statistically significant effect on this new queries which resulted in the same URL is small as expected. The same URLs were often selected before and after url stripping from the query X  X his is also obviously expected. Users substitut-ing related words in their query, i.e. word substitution , seemed to select different results. The marked difference between forming and expanding acronyms may be because users form acronyms to return to the same URL and are simply using a shortcut query, while users expand acronyms to look for new results, perhaps to disambiguate a common acronym. Also notable is that spelling correction caused few same URL clicks, suggesting that the correction helped fetch new, improved results. Query Reformulation Type
Figure 3: Proportions of URLs Clicked which were the Same A rank change is the difference between the rank of the result clicked in the initial query subtracted from the rank of the result clicked in the reformulated que ry. Successful reformulations should have a positive effect on rank change. Table 5 shows that all reformula tions have positively affected the rank of the selected result. The ra nk change is positive if the user clicked a higher ranked result in the query reformulation. The most positive rank changes occurred with the reformulation types word substitution and add / remove words . Url stripping , changing whitespace and punctuation , and forming acronyms resulted in a small positive rank change. We suspect url stripping only had a small rank change effect because most clicks were for the same URL (see Section 5.1.2) which w ould likely have the same or similar rank. Calculated rank ch anges were found to be signifi-cantly different (F 14 , 34342438 = 116,670.58, p &lt; 0.001). This metric measures how quickly users performed each type of reformulation. The average time was computed for each reformulation strategy. Our results in Table 5 show that complex reformulations such as word substitutions and forming acronyms took users longer than simple ones like spelling correction . Surprisingly, the median time for same query was 1 second; this suggests that some same queries may be made by computers rather than humans. As expected, new queries took the longest time since they are often part of different query sessions. Calculated times were found to be significantly different according to an ANOVA (F 14 , 13813144 = 48,235.05, p &lt; 0.001). 
Table 5: The median time (in seconds) between queries and Reformulation Type Median Time (s) word substitution 73 +4.04 add words 63 +3.19 remove words 68 +3.02 word reorder 85 +2.86 expand acronym 42 +2.02 spelling correction 22 +1.03 form acronym 103 +.64 whitespace &amp; punctuation 27 +.54 url stripping 57 +.29 Most findings were consistent with our expectations, evidence that the general approach of analyzing effectiveness metrics of reformulation strategies is usef ul. A surprising finding was that different reformulation strategies were effective depending on the action from the initial query. This emerged when comparing the ratios of actions in a reformulated query while controlling for the initial action. Word substitution reformulations were more likely to result in a Skip than a Click when the initial action was Skip, but result in Click 3 X  as often as Skip when the initial action is a Click. This is supported by metrics that show word substitution is correlated with different URL clicks as well as higher ranked clicks, suggesting that the user is interested in related but better results. In contrast, spelling correction is one of the least effective reformulations when the initial action is a Click, but becomes one of the most effective reformulations when the initial action is a Skip. This demonstrates the prio r action needs to be considered when determining the effectiveness of reformulation strategies. Grimes et al. [14] note that while a vast amount of information can be discovered from aggregating data, query logs are the least rich source of data for individua l events. Query logs only show the recorded actions and not the intent behind the queries. Identifying the user intent can be difficult or impossible without context, which is absent from logs. For example, query logs cannot tell whether a user did not click because the information they were looking for was found on the results page, or because the results were unsatisfying. Co mplementing this research with survey and user studies could address the lack of context. The AOL query logs were released with normalized data, which may skew the results. Some queries were removed or modified for privacy reasons. The paths in the click URLs were stripped leaving only the domains. Lastly, a ll queries were lowercased and most punctuation was removed, preventing us from detecting when the user performed a capitalization query reformulation. Baeza-Yates et al. [5] note that even humans have difficulty manually classifying some queries and the subjectivity involved can lead to errors. When manua lly separating query sessions, we encountered queries where it was ambiguous whether they were a reformulation. Queries can be re lated, but whether they fit the definition of a reformulation,  X  X  s part of the same information need X , may still be unclear. If a human cannot accurately classify a query, a computer programmed by a human, subject to their limitations, will not be more successful. An example is a first query  X  X merican airlines X  and a second query  X  X elta airlines X ; would they be considered part of the same information need? The intent behind the queries could be different (e.g. the user wants to find information about each airline), or part of the same informa-tion need (e.g. comparing prices between the airlines). The findings in this paper are influenced by the AOL search engine X  X  implementation. Studying a different search engine X  X  query logs may affect the reformulations used because of the different results displayed or the way it handles queries. A reformulation may work better for a different search engine. For example, users may learn over time not to reorder words in their query if they find it is ineffective due to the search engine X  X  ignoring of word order. During the period when these logs were collected, AOL Search returned results from Google [Chowdhury, personal communication] and query suggestions were offered for some queries. Despite these effects, the results here are uncom-promised because we study inter-reformulation rather than intra-reformulation effectiveness metrics. Current search engines have inte grated automatically generated query reformulation suggestions into their interface. However, they do not distinguish between new and reformulated queries. Users often perform a query reformulation because they are dissatisfied with the results from their initial query. One possible interface change would be displaying differently the overlapping search results between the refo rmulated search and the initial search. For example, when a user searches for  X  X aptop X  and then  X  X idescreen laptop X , the search engine can gray out the results in the second query that were also presented in the  X  X aptop X  query because it knows the user was not interested in those results. A related interface has been already built into web browsers since their inception X  X isited links turn purple while unvisited links are blue, which helps users avoid sel ecting results already visited. Email applications show the conversational history between recipients which reminds them of past discussion. We suspect that query session history is not shown in search pages because existing reformulation detec tion methods are error prone. However, with our high precision classifier, many previous queries can be determined with confidence. While we may miss some previous queries, that is le ss crucial in this application. Showing query history fits the need of a high precision, low recall classifier. We can design and evaluate a search interface that shows a user X  X  query history when the user is in a query session, i.e., performing a query reformulati on. It may help the user to see prior queries while they are reformulating. Anick [3] notes that query reformulations exist in 56% of sessions; while Pass et al. [29] find the typical session contains 2.6 reformulations on average. A classifier like ours that identifies query reformulations solves the same problem as classifiers that identify query session boundaries (s ee Section 2.2 for discussion). Generally, when orthogonal classifiers are combined, the result is one that is better than either of its components. Since our classifier is rule-based, operating orthogonally to existing classifiers, it can theoretically be combined with an existing temporal or machine learning classifier. This will produce a reliable overall classifier fo r detecting session boundaries. Understanding how users are refo rmulating queries and their effectiveness can help search engines provide better automatic query assistance. For example, a search engine should propose different reformulation strategi es depending on the user X  X  action after a query. Our findings have shown that expanding acronyms and spelling corrections are helpfu l reformulations when a user does not click on any result, but word substitutions and query expansion are more helpful when a user has clicked. Reformulation strategies also greatly vary between users. Search engines can react differently depending on the user performing the search. A search engine that has a history of a user X  X  queries will be able to offer query assistance suited for that user, or offer helpful suggestions about how the user can improve their searching and reformulating. For example, the search engine can suggest stemmed queries to a user who would benefit from stemming reformulation, or it mi ght display a message like  X  X e noticed you have been using future tenses in your searches, we suggest changing to present tense for better results. X  This paper describes the human si de of query reformulation and contributes to our understanding of us ers in search interaction. We created a taxonomy of query reformulation strategies, built a high precision rule-based classifier to detect each type of reformula-tion, and analyzed query reformul ations in the AOL query logs using metrics which are indicators of effectiveness. We found that different reformulation strategies have distinct characteristics when studied through the lens of click data. Certain reformula-tions like add/remove words, word substitution, acronym expansion, and spelling correction seem most effective. On the other hand, acronym formation a nd reordering words may be less beneficial to the user. We discovered that different reformulation strategies are useful depending on the user X  X  behavior in response to the initial set of results. Thes e findings benefit research in query session boundary detection, improve query assistance and personalized search, and propose design implications for user interfaces supporting reformulation. The authors would like to thank the anonymous reviewers, and Eytan Adar, Nicholas J. Belkin , Edie Rasmussen, Jacob O. Wobbrock for helpful comments on earlier drafts. We also thank Xueming Huang for help with formal language notation. [1] Agichtein, E., Brill, E., and Dumais, S. (2006). Improving [2] Agichtein, E., Brill, E., Dumais, S., and Ragno, R. (2006). [3] Anick, P. (2003). Using terminological feedback for web [4] Arlitt, M. (2000). Characterizing Web user sessions. ACM [5] Baeza-Yates, R., Calder X n-Benavides, L., and Gonz X lez-[6] Baeza-Yates, R., Hurtado, C., and Mendoza, M. (2004). [7] Bruza, P.D. and Dennis, S. (1997). Query Reformulation on [8] comScore. (2008). Baidu Ranked Third Largest Worldwide [9] Dou, Z., Song, R., Yuan, X., and Wen, J. (2008). Are click-[10] Efthimiadis, E.N. (1996). Quer y Expansion. Annual Review [11] Fellbaum, C. (1998). WordNet: An Electronic Lexical [12] Fox, S., Karnawat, K., Mydland, M., Dumais, S., and White, [13] Guo, J., Xu, G., Li, H., and Cheng, X. (2008). A unified and [14] Grimes, C., Tang, D., and Russe ll, D.M. (2007). Query Logs [15] He, D., G X ker, A., and Harper, D.J. (2002). Combining [16] Jansen, B.J., Spink, A., Blakely, C., and Koshman, S. (2007). [17] Jansen, B.J., Spink, A., and Pe dersen, J. (2005). A temporal [18] Jansen, B.J., Zhang, M., and Spink, A. (2007). Patterns and [19] Joachims, T. (2002). Optimizing search engines using [20] Joachims, T., Granka, L., Pan, B., Hembrooke, H., Radlinski, [21] Jones, R. and Klinkner, K. L. (2008). Beyond the session [22] Jones, R., Rey, B., Madani, O., and Greiner, W. (2006). [23] Kraft, R. and Zien, J. (2004). Mining anchor text for query [24] Lau, T. and Horvitz, E. (1999). Patterns of search: analyzing [25] Levenshtein, V.I. (1996). Binary codes capable of correcting [26] Mitra, M., Singhal, A., and Buckley, C. (1998). Improving [27] Murray, G.C., J. Lin, and A. Chowdhury,. (2006). Identifica-[28] Ozmutlu, S. (2006). Automatic new topic identification using [29] Pass, G., Chowdhury, A., and Torgeson, C. (2006). A picture [30] Porter, M.F. (1980). An algorithm for suffix stripping, [31] Radlinski, F. and Joachims, T. (2005). Query chains: learning [32] Rieh, S.Y. and Xie, H. (2006). Analysis of multiple query [33] Silverstein, C., Marais, H., Henzinger, M., and Moricz, M. [34] Teevan, J., Adar, E., Jones, R., and Potts, M.A. (2007). [35] Teevan, J. (2007). The re:search engine: simultaneous [36] Whittle, M., Eaglestone, B., Fo rd, N., Gillet, V. J., and 
