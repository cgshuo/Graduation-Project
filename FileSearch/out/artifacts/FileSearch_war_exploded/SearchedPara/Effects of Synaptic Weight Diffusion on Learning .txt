 been developed [3, 4, 5, 6, 7]  X  matching phenomenon, which has been observed in behavioral experiments. as to the average reward per choice a , and the income, I from the choice a and I trials, this equals  X  r | a  X  p behavior. The matching law states that I Then, the matching law states that this quantity equals p satisfy  X  if this identity. the N -neuron population, x A = ( x A vectors x A and x B independently obeys a gaussian distribution with mean X (these quantities can be spike counts during stimulus presentation). determined as With the synaptic efficacies (or weights) J A = ( J A to the output units are given by We assume that J a is scaled as O (1 /N ) instead, the individual fluctuations in x a ically incorporate it as follows.
 Using the order parameters we find h a  X  N ( and variance  X  2 . We assume u C
Var [ u a ] +  X  2 C
A = C B = 1 as u a = h a  X  h a rec +  X  p  X  with h a rec = (1  X  1 / gaussian random variable with unit mean and unit variance. Then, u distributions whose means and variances are respectively given by J probability that the network will choose alternative A can be described as where erfc (  X  ) is the complementary error function, erfc ( x ) = 2  X   X  the weight norm, l differences between the growth of the second order moment of weight distribution l 2 square of its mean J 2 learning rule: Reward-modulated (RM) Hebb rule: Delta rule: where  X  is the learning rate,  X  denotes the expected value and c ( y simplify the following analysis 2 . rule in a vector form: where for the RM-Hebb rule, F square norm of each side of equation 7, we obtain l O (1 /N 2 ) , where we have defined ~ h a = on both sides of equation 7, we obtain J ~ x form of the ensemble averages are obtained for reward-dependent Hebbian learning as and for the delta rule, The conditional averages  X  ~ h a | a  X  and  X  ~ x  X  ~ h a | a  X  = J a X 0 + where we have defined L = given in the supplementary material and [12].
 modifying the learning rule in the following way [22]: with F  X  F With  X  X  X  =  X  F when l 2 Parameters were X main text) with  X  small.  X  Here, we use  X  behavior (denoted as p match ( p match The weight standard deviation,  X  value below p match reaching p match diffusion effect than that of the RM-Hebb rule. 5.1 Matching Behavior Is Not Necessarily Steady State of Learning of learning. In Figure 2, the order parameters are initialized so that p Equations 8 and 11 are numerically solved. We see that p toward the uniform choice ( p evolves toward p match condition, initial conditions were first set at J so that p condition was met.  X  r | and  X  F always takes the same value. Therefore, when p i.e., d X  2 behavior, p contrast with the N = 1 case [7] where the average changes stop when p With weight normalization, determines choice probability is the difference between J Equation 11, only term  X  F 2 J absolute difference, | J approaches unbiased choice behavior due to the diffusion effect. cally converges to p match choice probability as From this expression, we find that the larger the magnitude of J in  X  a . The  X  X iffusion term X ,  X  F 2 a  X  , which moves p A away from p the magnitude of J magnitudes of J p Figure 2A the p is imposed, the magnitude of J p of  X  a  X  X . Thus, p A saturates before it reaches p  X   X  X  are balanced. 5.2 Learning Rate Dependence of Learning Behavior  X 
F same as those in Figure 1. Initial conditions were set at  X  non-normalization condition and J a whole, as  X  is decreased, the asymptotic value, p large learning rate (  X  = 1000 ). At the beginning of learning when J fusion term,  X  F 2 as the magnitude of the differences J p 5.3 Deviation from Matching Law (Figure 4B). In this condition, if the updates makes J a J max / to a finite range, as predicted by our theory. RM-rule with weight normalization. We used VI schedules with  X  choice, p (the synaptic weights are restricted to the interval [0 , J max / convergence for the simulations in (B) indicated by the gray arrows. behavior in large-scale neural circuits.
 havior, regardless of specific task settings.
 realize stochastic random choice behavior perhaps than previously thought. [9] D. Saad. On-line learning in neural networks . Cambridge University Press, 1998.
