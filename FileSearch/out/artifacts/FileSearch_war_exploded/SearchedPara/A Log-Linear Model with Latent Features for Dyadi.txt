
In dyadic prediction, the training set consists of pairs of objects { ( r i , c i ) } n i =1 , called dyads , with associated labels { y i } n i =1 . The task is to predict labels for unobserved dyads, that is for pairs ( r  X  , c  X  ) that do not appear in the training set. In a well-studied special case of dyadic prediction, the dyads are (user, item) pairs, and the labels are a user X  X  numeric rating of an item, often on a scale from 1 to 5 . The task of predicting the ratings of unobserved (user, item) pairs is collaborative filtering , where the ultimate goal is to recommend to users new items they might like.

Existing methods for dyadic prediction are limited in several ways. One common issue is that they assume the observed labels have a numerical scale. This assumption is reflected in the loss function used for training, often mean absolute error or mean square error. But in many applications of dyadic prediction, the outcomes are nominal , that is discrete and unordered. For example, an online store may have information about customers X  interactions with prod-ucts, with possible outcomes including { viewed, purchased, returned } . On such datasets, imposing a numerical or ordinal structure is inappropriate. A dyadic prediction model should be flexible enough to handle nominal labels.

Another issue is that many models cannot exploit addi-tional attributes of dyad members, called side-information or covariates. A model that only uses members X  unique identifiers is limited in the cold-start setting, where the test set contains a dyad ( r  X  , c  X  ) where at least one of r is not present in any training set dyad. An example of this setting is predicting how a user will rate a movie that is yet to be released, and thus has no existing ratings.
In this paper, we propose a new model that addresses the above issues, and more. An appealing property of the new model is its conceptual and technical simplicity: it is a log-linear model where the log-odds are approximated by a low-rank matrix. Based on this, the new model is named LFL for latent feature log-linear .

In this section, we relate the dyadic prediction task to matrix completion. We discuss six important desirable properties for a dyadic prediction model, and explain how no existing method possesses all these properties. A. The dyadic prediction problem
In dyadic prediction, the training set is { (( r i , c i ) , y where the pairs ( r i , c i ) are called dyads. Usually, but not necessarily, the only information available about each el-ement in each pair is a unique identifier. the goal is to predict the label y for an unobserved dyad ( r  X  , c  X  ) . We can interpret this task as a form of matrix completion by associating the training set with a matrix M  X  Y |R| X |C| , where r i  X  R , c i  X  C , y i  X  Y . Each row of M is associated with some r  X  R and each column with some c  X  C , so the training data is a subset of observed entries in M . The task is to fill in the missing entries of M . Based on this interpretation, we refer to r i as a row object and c j as a column object .

The dyadic prediction framework encompasses many real-world problems where the input is naturally modeled as interactions between entities. Important examples include recommending items to users, predicting links in a social network, predicting whether users will click on ads, and predicting the contamination in food production plants [1]. In the absence of side-information, a powerful approach to the problem is based on learning latent features . Ignoring missing entries for the moment, and supposing that Y = R , the classical way to learn latent features from M is the sin-gular value decomposition (SVD) [2]. This expresses M as the product of matrices U and V , which are representations of the row and column objects in a latent feature space. In the case of collaborative filtering, the latent space can be interpreted as scores for user and item characteristics (e.g. whether the item is a status symbol, whether the user likes foreign films). In matrix completion we do not know the entire matrix M , which means the SVD cannot be applied as-is. In this setting we can restate the task as finding a low-rank approximation that agrees with M only on its observed entries; the hope is that with suitable priors on U, V , this approximation will generalize to the missing entries as well. Such latent feature approaches have been very successful in real-world applications of dyadic prediction, in particular collaborative filtering [3].
 B. Desirable properties of a dyadic prediction model
As mentioned in the introduction, there are several desiderata for a dyadic prediction model:
Agnostic to nature of labels . Labels in a dyadic pre-diction task may be ordinal, for example star ratings for a (user, movie) pair, or nominal, for example one of { viewed, purchased, returned } for a (user, product) pair. Ideally, a model should handle both types of labels.

Exploiting side-information . If a model cannot use side-information, when it is provided, then the model is severely limited in the aforementioned cold-start setting.
Learning latent features . It is desirable for a model to induce latent features for dyad elements just from the identities of row/column objects; otherwise, we are severely limited when we have no side-information. Aside from endowing the model with stronger predictive power, it also helps one understand the data better. (See the experimental results in Section V-D, for example, where we use the latent features to cluster the data.)
Resisting sample selection bias . Sample selection bias is the situation where the training and test sets follow different distributions. This is manifested in movie rating prediction for example, where users are generally more likely to provide ratings for movies that they like. Sample selection bias poses a challenge for generative models, because they need to model the joint distribution of examples and labels. A discriminative model, in contrast, only focuses on conditional probabilities of labels, and so is inherently robust against sample selection bias [4].

Calibration of probabilities . Often, a classifier is only a sub-model in a larger framework. In this setting, having the classifier output well-calibrated probabilities [5] rather than just ranking scores helps one make decision-theoretically optimal choices. For example, in online advertising, pub-lishers would like to display ads that bring about the maximum expected revenue. To estimate this revenue, one needs accurate estimates of the probability that a user will click on an ad [6].

Fast training . Large datasets are common for practical applications of dyadic prediction such as collaborative fil-tering, so scalable training algorithms are essential.
To the best of our knowledge, the log-linear model pro-posed in this paper, which we call LFL, is the first method to meet all the criteria above. We now briefly discuss existing methods for dyadic prediction, pointing out which issues they fail to address.
 C. Existing dyadic prediction methods
We focus here on models for two well-studied special cases of dyadic prediction, namely collaborative filtering and link prediction. Note that the focus of this paper is on models for the general dyadic prediction task; as such, we are not interested in tuning a model to the specifics of a collaborative filtering problem, which is the focus of many published results in the literature. Nonetheless, it is conceptually simple to incorporate many of the tricks for collaborative filtering into the LFL model. This is discussed further in Section IV-A.

Matrix factorization . The idea of learning latent features through matrix factorization has been successful in collab-orative filtering. A popular example of this is maximum margin matrix factorization (MMMF) [7], where the idea is to approximate the input matrix M (containing missing entries) by a matrix X whose complexity is controlled by a convex approximation to the rank: where I is the set of observed entries in M ,  X  is a modified hinge-loss, and ||||  X  is the trace norm of a matrix (the sum of its singular values, also known as the nuclear norm). Pe-nalizing the trace norm favors matrices X that are explained by a few latent factors i.e. if X = U T V for rank k matrices U, V , then || U || 2 F + || V || 2 F is small. The above formulation requires solving an SDP (semidefinite program) and so is not scalable. A faster alternative is to learn directly U, V with a gradient-based method [15], although this sacrifices convexity. Probabilistic extensions of MMMF such as [8], [16] give a Bayesian treatment of the problem, and obtain higher accuracy. These were further extended in [10], which interprets the problem in a Gaussian process framework to learn a nonlinear matrix factorization. A related result is [9], where the focus is on a nonparametric model with the number of latent factors k determined automatically.
As noted earlier, a significant amount of research has fo-cused on improving the performance of matrix factorization methods for the collaborative filtering setting. One such line of work has studied how to combine neighborhood models with standard matrix factorization. A neighborhood model is based on the idea that similar users tend to rate movies similarly. These models are good at capturing local effects in collaborative filtering data, and have been shown to improve performance of standard matrix factorization models. Recent models based on this hybrid approach are [11] and [12]. The latter also gives a way to incorporate implicit feedback in collaborative filtering, where we exploit information about which movies a user rated, even if we do not know the actual rating. However, it does not address the issue of incorporating more general forms of side-information. We note that it is conceptually simple to use neighborhood information in the LFL model using techniques similar to these papers; we emphasize once more that our goal is to address a broader set of concerns than just improving accuracy of collaborative filtering methods.

All matrix factorization methods assume that the input labels are numerical. Most of them do not incorporate side-information, although recent exceptions are [10], [17]; to our knowledge the latter has not been tested extensively on a number of datasets.

Boltzmann machines . Restricted Boltzmann machines (RBMs) have enjoyed some success for collabora-tive filtering [13]. The probability model is generative : p ( x, y, h ; w )  X  exp( X ( x, y, h )) , where x, y are the inputs and labels, h a vector of binary-valued hidden units, and  X  some linear function of its inputs. Since the RBM is a gener-ative model, exact training is intractable; also, as mentioned earlier, this makes it susceptible to sample-selection bias. A discriminative form of the RBM was proposed in [18], but it has not been applied to collaborative filtering. Further, to our knowledge, RBM-based models have not been extended to incorporate side-information, and have not been applied to datasets with nominal labels.

Models that are mathematically similar to the discrim-inative RBM have been successfully applied to structured prediction problems [19]. Unlike our model, these methods are not based on matrix factorization, and are not obviously applicable to dyadic prediction tasks. A closer study of this point would be valuable future research.

Link prediction models . In link prediction, the input is the adjacency matrix M of a graph with some missing entries, which we want to fill in. This is a dyadic prediction problem where both objects in the dyad belong to the same space, e.g. people in a social network. Additional (optional) constraints are that the graph is undirected and unweighted i.e. M is binary and symmetric. Two link prediction models that are relevant for our work are [20] and [14]. [20] handles the case of binary M using logistic regression, where the log-odds are modeled by a low-rank matrix approximation. This is similar to the model we propose, but our training pro-cedure is considerably simpler than the MCMC scheme used in the paper, and also our model addresses the general dyadic prediction task, with binary link prediction as a special case. [14] also uses logistic regression, but with two important distinctions from [20]: (i) the matrix decomposition involves binary matrices, indicating the presence of a particular latent feature, and (ii) the decomposition is nonparametric, so the number of latent factors need not be specified a priori . Training in this model is involved, and it is not clear that it scales to large datasets.

Summary . We close this section with Table I, which summarizes various existing methods in the literature in terms of whether they possess each of the six desiderata we listed in Section II-B. We see that the LFL model proposed in this paper is the first method that meets all six desiderata.
In this section, we describe a new log-linear model for dyadic prediction. We then extend the model by adding latent features, yielding the latent feature log-linear (LFL) model. We explain how to make predictions with and train the LFL model for both nominal and ordinal labels.
 A. A simple log-linear model
Given an observation x  X  X and label y  X  Y , a log-linear model represents the conditional distribution p ( y | x ) via Here, w is a vector of real-valued weights to be learned. The functions f i : X  X  Y  X  R are called feature functions , and measure interactions between the inputs and labels.
Recall that each example in a dyadic prediction task is (( r, c ) , y ) , where ( r, c ) is the dyad and y the label. We define x = ( r, c ) , and use r ( x ) and c ( x ) to denote the respective elements in x . Suppose there is no side-information; then, r, c are just indices into sets R , C denoting the space of row and column objects respectively. We apply a log-linear model for p ( y | x ; w ) with three sets of feature functions:  X  (  X  r  X  R , y  X   X  Y ) f (1) ry  X  ( x, y ) = 1 [ r ( x ) = r, y = y  X  (  X  c  X  C , y  X   X  Y ) f (2) cy  X  ( x, y ) = 1 [ c ( x ) = c, y = y  X  (  X  y  X   X  Y ) f (3) y  X  ( x, y ) = 1 [ y = y  X  ] where 1 [ ] denotes an indicator function. We can think of each feature function as inducing a weight for each object-label pair. Specifically, split the weight vector into three Then, the corresponding log-linear model is This model is conceptually simple, but limited in expres-siveness: specifically, it only learns propensities of the row and column objects towards a particular outcome. To see this limitation, fix a row object r 1 and consider dyads of the form ( r 1 , c ) . From Equation 1, for some fixed outcome y , the ranking of all c  X  X  in decreasing order of p ( y | ( r depends only on  X  cy . That means we get the exact same ranking of c  X  X  for a different row object r 2 .
 B. A richer latent feature model
To capture interactions between row and column objects, we propose the following model: where k is a constant that is the number of latent factors learned from the training data. (The  X  y term from the previous model is dealt with in Section IV-A.) For each outcome y ,  X  y is a matrix whose entries represent weights for each row object and one of k latent features, and similarly for  X  y . So, we are learning tensors  X   X  R |Y| X |R| X  k and  X   X  R |Y| X |C| X  k . It is easy to check that this model is not subject to the aforementioned ranking limitation.
Before discussing the intuition for these weights, we take care of a technical point to make further exposition clearer. In multinomial logistic regression, it is standard to fix one category to be the base, by fixing the weights for that category to be 0 . This defines a scale for the other categories X  weights, and thus improves the identifiability of the model. Here, we fix the first outcome to be the base category: if Y = { y 0 , y 1 , . . . } , then we make  X  y 0 ,  X  y 0  X  0 .
Now we see why we call  X ,  X  latent weights. To do this, focus on the case |Y| = 2 i.e. a logistic regression model. Let the outcomes be y = 1 and y = 0 without loss of generality. Following the above discussion, let y = 0 be the base class. In a slight abuse of notation, let  X ,  X  denote the matrices  X  1 ,  X  1 . Then, in the LFL model the log-odds are where  X  r ( x ) denotes row r ( x ) of matrix  X  , and similarly for  X  c ( x ) . Defining a matrix P with entries P rc = p ( y = 1 | ( r, c ); w ) gives the decomposition P =  X  (  X  T  X  ) , where  X  ( ) is the sigmoid function applied elementwise. That is, we model the log-odds by a rank k matrix approximation, where each dimension for  X ,  X  conceptually corresponds to a latent feature. As a result, we call this model the latent feature log-linear model or LFL . When |Y| &gt; 2 , for the base class y = y 0 , we model the pairwise log-odds by a low-rank approximation:
Now suppose that we have side-information in the form of a vector s ( x ) for a dyad x . Then, we augment the low-rank approximation with a linear discriminator, yielding Here, w s represents the weights used for side-information. The model can use either or both of latent features and side-information to make predictions. The idea of extending multinomial logistic regression to incorporate information beyond the linear discriminator w T x has been studied in random effects models in statistics, e.g. [21]. Ours is the first contribution that applies a low-rank approximation of the log-odds for dyadic prediction, with the explicit aim of targeting both nominal and ordinal prediction tasks.
To complete the specification of the model, we now address two important issues: how to make predictions using the model, and how to train the model. The answers to both these issues depend on the nature of the labels.
 C. Making predictions
Let F ( x ; w ) denote the model X  X  prediction for the dyad x . A sensible choice for F ( x ; w ) depends on the whether the outcomes in Y are nominal or ordinal. The mode argmax y p ( y | x ; w ) is the most reasonable choice when the outcomes are nominal. The median F ( x ; w ) = median ( p ( y | x ; w )) is sensible when the labels are ordinal. If we further assume that the labels have a numeric structure, then the mean E [ y ] = P y yp ( y | x ; w ) is a sensible alternative to the median.
 D. Objective function for training
When the y values are nominal, a sensible objective function is the conditional log-likelihood (CLL) of the data. Since we are learning a large number of weights, it is impor-tant to regularize the objective function with an  X  2 penalty. Thus, the objective function we minimize is regularized negative CLL, which for a training set { ( x i , y i ) } n Since this objective function is differentiable, we can apply stochastic gradient descent (SGD) for training, which makes the model scalable to large datasets.

For numeric labels, one can train the model using f nom , but this ignores the structure in the labels and reduces the model X  X  predictive power. Instead, if we assume the labels have a numeric structure, it is more sensible to minimize the discrepancy between the true label and the model X  X  prediction F ( x ; w ) . A simple measure of discrepancy often used in collaborative filtering is the mean absolute error (MAE), where the error for predicting the true label y as  X  y function is If we use MAE as the training objective, ideally we would like to use F ( x ; w ) = median ( p ( y | x ; w )) since the median is the solution to argmin t P i | x i  X  t | . However, this makes the objective non-differentiable, which means we cannot train using SGD. The simplest recourse here is to instead use F ( x ; w ) = E [ y ] , which is differentiable.

Note that MAE can be replaced with other loss functions, such as MSE (mean squared error, defined as  X  ( y,  X  y ) = ( y  X   X  y ) 2 ), or even the modified hinge loss of MMMF [7]; this does not change the underlying model, only the objective function. The choice of loss function is determined by two factors, which can be at odds with each other. One point is that it makes sense to use the same loss for training as for testing: if test error is measured by MAE, then it is sensible to optimize MAE during training. The second point is that we would like to return well-calibrated probabilities. If we train using a proper loss function, then it is guaranteed that we obtain well-calibrated probability estimates [22]. Examples of proper losses include the previously discussed CLL and MSE. MAE is not a proper loss function, so if we optimize it on the training set, we need a post-processing step to return calibrated probability estimates, such as isotonic regression [23]. Therefore, the choice of loss for ordinal labels potentially requires making a trade-off between directly optimizing the test set objective and automatically generating calibrated probabilities.
There are other options when modeling ordinal data with a log-linear or logistic regression model, but these require modifying the underlying probability distribution [24], [25]. the focus here is on using the same model as the nominal case, but modifying the training procedure to exploit the ordinal (or numeric) structure of labels.
 E. Strengths and weaknesses of the LFL model
The LFL model meets all the goals for dyadic prediction listed in Section II-B. First, it can handle both nominal and ordinal Y , by changing the training objective. Second, it is easy for the model to handle side-information when present, which helps address the cold-start problem. Crucially, the model does not assume that only side-information or only unique row and column object identifiers are relevant: it can make use of just one, or both of these pieces of information. Third, by virtue of being a discriminative probabilistic model, it can produce well-calibrated probabilities and is resistant to sample-selection bias. Finally, the objective function is differentiable, so one can use SGD for training, which scales to large datasets.

However, unlike for the basic log-linear model, the LFL objective function is not convex; it is only convex in  X  with  X  fixed, and vice versa. (The same is true of all exist-ing matrix factorization methods for collaborative filtering.) Moreover, the experimental results below demonstrate that it is easy to find good local optima for the LFL model.
Another observation is that a separate set of weights is learned for each y  X  Y . If |Y| is large, then we will be learning many parameters, which increases the risk of overfitting. In nominal settings where there is truly no order to the various outcomes, then it is sensible to have separate weights for each outcome. This is plausible even in the ordinal setting: e.g., the characteristics that make a user rate a movie 1 star may be quite different from those that make the user rate it 5 stars. However, it is still desirable to exploit the ordinal structure to reduce the number of parameters. We address this issue in Section IV-C.

We now explain some important variations of the new latent feature model. Specifically, we discuss adding bias weights, regularization, and reducing the number of param-eters in the ordinal setting. We then show how one can apply the model for the special case of link prediction. A. Adding bias weights
In Equation 2, there is no explicit bias term  X  y . We include biases by forcing one component of each of  X  y r and  X  y c to have the constant value 1 , for each y, r, c . This is equivalent to having a separate bias for each row and column object, and can be thought of as capturing baseline effects, e.g. whether a user tends to give high ratings, and whether a movie is popular. Note that in each vector  X  y r components are related to bias terms: one is the constant 1 and the other is the row bias. Therefore, henceforth, when we speak of a rank k latent feature model, we mean one trained with k general parameters plus these additional two parameters.

The LFL model can incorporate other suggestions that have been used in standard matrix factorization models. For example, in the context of collaborative filtering, [8] suggests imposing a prior on a user X  X  latent weight vector that takes into account the identity of the movies she has rated. B. Regularization
Earlier, we proposed regularization that penalizes all com-ponents of w equally. But it is plausible that the row and column weights have different penalties, meaning separate regularization parameters  X   X  ,  X   X  . It is especially plausible that the weights for side-information be penalized differ-ently. Also, as suggested in [26], it can be beneficial to apply the regularization inversely proportional to the (square root of the) number of observed entries for a particular row or column object, i.e. the number of times that row object r or column object c appears as part of a dyad in the training set. This ensures that the penalty for objects that appear only infrequently is larger than that for those that appear often, which is sensible because we expect to overfit more for objects for which we have only limited data. We applied all these extensions when training the LFL model. C. Reducing parameters in the ordinal setting
The LFL model keeps a separate set of weights for each y  X  Y , but it is plausible that these weights share some structure in the ordinal setting. One way to reduce the num-ber of weights is to have a low-rank approximation of the latent weights themselves. This is similar to the stereotype model for multinomial logistic regression [24], but we apply it to the latent weights rather than the discriminator w . Essentially, we assume that for every label y the weights can be decomposed as a linear combination of some set of base weights that are independent of y . Specifically, for any y  X  Y , we assume there are  X  scalars  X  i such that Here, the weights (  X   X  i ,  X   X  i ) are shared among all outcomes y , and only the  X  i values vary. If  X   X  |Y| , this dramatically reduces the number of parameters to be learnt.
 D. Application to link prediction
In standard link prediction the row and column objects be-long to the same space. Consider the setting where the input graph is unweighted and undirected, so that the adjacency matrix is symmetric and binary. To apply the LFL model, we need to enforce symmetry: we need p ( y = 1 | ( r, c ); w ) = p ( y = 1 | ( c, r ); w ) . One way to achieve this is to maintain just one set of weights  X  , for both the row and column objects. Setting y = 0 to be the base class, we get the model
For a directed graph, it is no longer appropriate to have a symmetric decomposition inside the exponent. There are at least two options here. One is to keep three sets of weights: one for the incoming edges,  X  , one for the outgoing edges,  X  , and another for the global node information (i.e. weights that depend only on the identity of the node),  X  . Then, we use the decomposition  X  T  X  +  X  T  X  . The other approach is to use the decomposition  X  T  X   X  , where  X  is a full non-symmetric matrix [27].
 E. Multi-relational data
In an important extension of the basic dyadic prediction task, there are a series of outcomes. For example, in link prediction, there may be different types of links:  X  X s-family-member-of X ,  X  X s-colleague-of X , etc . This is not the same as having nominal links, because every dyad can have many links of different types, rather than just one link with many possible outcomes. It is akin to multi-label prediction rather than multi-class prediction. In this setting, we can extend the LFL model to capture the structure underlying the various relations. Suppose we have R binary relations, with outcomes y 1 , . . . , y R . Then, for any t  X  { 1 , . . . , R } , we can model That is, we share the row and column object weights among all relations, but we give a different scaling factor depending on the relation.
 The experiments here demonstrate the flexibility of the LFL model, and its competitiveness with state-of-the-art methods on a range of different tasks. We present results for four important regimes: nominal data, ordinal collabo-rative filtering with and without side-information, and link prediction. In the process, we show that the LFL model scales to large datasets. We also present results on a matrix completion task involving handwritten digits. We emphasize that no existing dyadic prediction method is applicable for all these problems.

In all experiments, we pick the regularization parameters using three-fold cross-validation on the training set. We use different regularization for side-information and latent weights. We also report results for a varying number of latent features, k . In general increasing k increases accuracy, but there is a greater risk of overfitting. Accuracy measures are chosen to be sensible for each of the regimes listed above. In particular, we report MAE for ordinal prediction tasks, and AUC for link prediction tasks. We report the mean and standard deviation of test set performance from five runs of the training algorithm, where each run uses a different initialization of the weight vector. Different initializations lead to local optima of differing quality, since the optimization problem is nonconvex.
For the medium-sized datasets, we train using LBFGS [28]. Unlike first-order gradient methods, this does not require tuning of a learning rate, but at the price of higher computational cost. To demonstrate that the LFL model is scalable, results on large datasets are with stochastic gradient descent (SGD) as the training optimizer.

For the collaborative filtering tasks, we compare the LFL method to MMMF, which is representative of most matrix factorization methods. We use the MATLAB code for this provided by the author at http://people.csail.mit.edu/jrennie/ matlab/. Note that this code does not include the suggestions in [26]. The results reported for all other methods are the ones that appear in previously published experiments. All experiments were run in MATLAB 2008b on a 2.67GHz Core i7 machine with 8 GB of RAM.
 A. Results on synthetic nominal data
We run experiments on a synthetic dataset to check that we can learn from nominal data, and that it is possible to find good local optima of the objective function despite its non-convexity. Another question is how well a method for ordinal labels like MMMF performs on this dataset. Intuitively, because such a method imposes an artificial structure on the outcomes, it will be difficult to learn a good model.
We constructed a matrix M  X  R n  X  n with entries in { 1 , 2 , 3 } . These can be thought of as indices into a nominal set such as { viewed, purchased, returned } ; the numeric encoding is just for convenience. We picked the entries of M by sampling M rc  X  p ( y | r, c )  X  exp((  X  y r ) T  X  y c are k = 5 latent factors for the  X  and  X  matrices. The entries of  X ,  X  were drawn uniformly at random from the interval [  X  3 , 3] . We set some fraction of entries to be unobserved, which were used for testing, and let the remaining entries form the training set. The goal of training is to choose  X ,  X  that maximize the log-likelihood. Two parameters that influence the quality of the learned model are the size of the matrix, n , and the retention rate, that is the fraction of entries kept for training.

The simplest measure of quality of a model is 0-1 accu-racy. But this is meaningful only given a base accuracy to measure against. Since we know the underlying probability distribution p ( y | x ; w ) , we can compute the Bayes error for a single matrix entry as 1  X  max y p ( y | r, c ) . A good model should approach the mean Bayes error.

Results are presented in Table II. For varying choices of n and the retention rate, the LFL model has high 0-1 accuracy. The error rate is closest to the Bayes error when the training set is large: this is intuitive, because we expect the learning task to be simpler with more samples. Another promising result is that the accuracy of the LFL model increases with the size of the training data, despite the increase in missing data. As expected, MMMF does significantly worse than the LFL model, demonstrating that an ordinal encoding is not sensible for this task.
 B. Results on ordinal datasets
An important question is how the LFL model compares with existing methods for the ordinal setting. We focus on the canonical case of this problem, collaborative filtering. We emphasize that the goal is not to be the single best method for a collaborative filtering task, but rather to be competitive with existing methods while being more general than them.
Results on bookcrossing . The bookcrossing dataset-consists of 1 , 149 , 780 ratings given by 278 , 858 users for 271 , 379 books [29]. Following [30], we pre-process the dataset to remove all ratings with the value 0 , users with fewer than 3 ratings, and books with fewer than 6 ratings. This leaves 342 , 464 ratings over 35 , 689 users and 138 , 660 books. Unlike [30], we did not use just the books with Amazon reviews; we also did not use any side-information, and only learned latent features.

Given the size of the dataset, we trained using SGD. We could not run the MMMF code, as it required too much memory. We performed three-fold cross-validation where each fold contained 1 / 3 of each user X  X  ratings. The rank 5 LFL model attains an MAE of 1 . 0580  X  0 . 0028 , which is only slightly worse than the reported MAE for the fLDA method ( 1 . 0317 ) proposed in [30]. fLDA only uses side-information for making predictions: this information is hard to duplicate as it involves mining Amazon customer reviews. It is possible that the LFL model would achieve higher accuracy with this extra side-information. The training time of fLDA is not reported in [30]; the LFL method processes the large dataset efficiently, and trains in 10 minutes.
Results on 1 M movielens . The 1M movielens dataset consists of 1 , 000 , 209 ratings for 6040 users and 3900 movies. Following [7], we randomly selected 5000 users and for each user, picked a random rating and put it in the test set. The other ratings are the training set. We ended up with a training set of 836 , 865 examples, each a user-movie rating dyad, and 5000 test examples. We trained the LFL model using SGD on this dataset. Table III presents the MAE and training time for the LFL model and MMMF for various choices of latent factors k . We see that the LFL model trains much faster than MMMF, while achieving competitive accuracy. In particular, LFL results for ranks 1 and 5 are statistically indistinguishable from MMMF results. C. Results in the cold-start setting
Here, we present results showing that the LFL model is able to learn to use side-information, and make useful predictions in the cold-start setting. We took the 100K movielens dataset, and randomly discarded 50 users from the training set. These people are cold-start users when they appear in the test. In the experiments, we considered the following scenarios: (i) the standard setting, where there are no cold-start users or movies, (ii) there are cold-start users, but movies are known, (iii) full cold-start, where both users and movies are unobserved. For (ii), we took the test set for (i) and then discarded all movies that occur in it from the training set. The side-information we used was the user X  X  age, gender and occupation, and the movie X  X  genre.
We compare to a baseline method of predicting the mean from a latent feature model trained without side-information. That is, for the dyad ( u, m ) where u was not present during training, the prediction is 1 n n m is the number of users in the training set T who have rated m , and F ( u  X  , m ) is the standard latent feature based prediction for dyad ( u  X  , m ) . When both u, m are not present in training, we use the mean predicted rating for the entire training set.

When training with side-information, the following heuristic helps in avoiding bad local optima: first train the model without any side-information to learn latent weights. Then, use this as initialization to the model with side-information weights included, that is initialize the latent weights for the second model to those learned by the first one. For the second optimization, we obtained better results when the latent weights were frozen. This is a form of block coordinate descent, where we optimize over two groups of variables by optimizing over each one in turn.
 All models use rank k = 5 and are optimized with LBFGS. Table IV summarizes the results. Learning with side-information significantly improves accuracy in the cold-start setting. Also, with side-information we do almost as well in the cold-start and standard settings; by contrast, the standard latent feature model does much worse in the cold-start setting. Comfortingly, side-information gives better MAE than the basic model when tested in the standard setting, i.e. no cold-start users or movies. This means that, as expected, taking side-information into account can give slightly better predictions than just learning latent features. D. Results on link prediction tasks We use two datasets for the link prediction experiments. The coauthor dataset [31] is a binary matrix indicating whether two authors have written a paper for NIPS together. The alyawarra dataset [32] contains kinship relations between people of the Alyawarra tribe in Australia. For both datasets, we use a random 80 -20 train-test set split: this means that for training, we assume that 20% of entries in the matrix are missing, and we predict these at test time. We report area under the ROC curve (AUC) as the performance measure, since this is commonly used in the link prediction literature. Because both datasets are symmetric, we use the  X  T  X  decomposition discussed in Section IV-D.

Results on coauthor . Following [14], we focus on the 234  X  234 submatrix of authors who collaborated with the most number of people. We compare our results to those given in [14] for an Indian Buffet Process (IBP) model, the infinite relational model (IRM), and the mixed membership stochastic block model (MMSB). We also compare the LFL method to MMMF, which is not explicitly designed for link prediction X  X t does not exploit the fact that the row and column spaces are the same X  X ut helps gauge the improve-ment that methods designed for link prediction provide over collaborative filtering methods.

Results are shown in Table V. The MMMF method is outperformed by all other methods, which suggests that exploiting the structure in link prediction tasks is essential to achieve good performance. (MMMF gives slightly worse results, not shown, for ranks over 5, perhaps due to overfit-ting.) For all ranks, the LFL model is more accurate than the MMSB and IRM models, but the IBP method is slightly better overall. It is important to note that the training/test split often has a significant impact on the accuracy of a learned model. We do not know the dataset splits used in [14] for the MMSB, IRM, or IBP methods, so this issue needs to be kept in mind when interpreting the results.
The learned weights for each author are useful for clus-tering. We applied k -means clustering on the user weights with 7 clusters, and sorted the authors according to the learned clusters. Figure 1 shows the resulting coauthor graph. Clearly the user weights have identified significant cliques in the coauthor graph.

Results on alyawarra . It is not sensible to run MMMF on this dataset, because the task is not binary, and it is not sensible to impose an ordinal scale on the matrix entries. We can view this dataset as multi-relational , so that each possible kinship relation defines a separate matrix, or as multi-category, where the number of classes is the number of kinship relations. We chose the latter because there is only one kinship relation observed between any two people. The reported results on this dataset [14] use the multi-relational viewpoint. With this choice, each relation can have separate weights (a  X  X ingle X  model), or shared weights (a  X  X lobal X  model). We ran our method for a number of ranks k , with log-likelihood as the objective function. The results in Table VI show that the rank 20 LFL model gives the best results of all methods.
 E. Results on matrix completion task
We ran an experiment on the usps dataset of handwritten digits, following [33]. We pick 100 random images each of digits 1 , 2 or 3 . The images are binarized so that the pixel values are  X  1 . For 16 of these images, we occlude their bottom half by setting the pixel values to be  X  X issing. X  The goal is to fill in the missing entries, or equivalently, to reconstruct the bottom half of the 16 partial images. This is a dyadic prediction problem where each digit is a row object, and each pixel position is a column object. We applied the LFL model on this dataset with rank k = 3 , optimizing MAE using LBFGS. Results are shown in Figure 2. The top row shows the original versions of the 16 occluded images, the middle row shows the data as presented to the training algorithm, and the last row shows the predictions made by the LFL model. (For this row, even the top half is the model prediction.) We see that the trained LFL model accurately reconstructs most of the images. One exception is an image whose true digit is 3 , but which the LFL model reconstructs as 1 ; this behavior is understandable, because there is not enough information in the top half for even a human to see the right answer.

Existing methods for dyadic prediction are limited in one or more ways, most commonly in that they assume the labels lie on a binary or numerical scale, and cannot exploit both dyad members X  unique identifiers and side-information. An ideal model should overcome these limitations. Other desirable properties are resistance to sample-selection bias and the generation of well-calibrated probabilities. Last but not least, given the size of datasets in real-world dyadic prediction tasks like collaborative filtering, it is essential for a model to be scalable. In this paper, we have presented a latent feature log-linear model (LFL) that addresses all these issues. The new model learns latent features based on dyad identifiers, but can easily use side-information also when it is available. We can apply it to large datasets using stochastic gradient descent. It is a discriminative probabilistic method, and so has resistance to sample-selection bias and produces well-calibrated probabilities. Experiments show success in learning from both nominal and ordinal labels, and that the model can use dyad identifiers and side-information to achieve competitive accuracy to existing methods for collaborative filtering and link prediction.

