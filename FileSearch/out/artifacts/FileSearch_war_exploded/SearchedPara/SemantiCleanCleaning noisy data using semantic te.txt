 Chris Welty  X  J. William Murdock  X  James Fan Abstract In our research on using information extraction to help populate semantic web resources, we have encountered significant obstacles to interopera-bility between the technologies. We believe these obstacles to be endemic to the basic paradigms and not quirks of the specific implementations we have worked with. In particular, we identify five dimensions of interoperability that must be addressed to successfully employ information extraction systems to populate semantic web resources that are suitable for reasoning . We call the task of trans-forming IE data into knowledge-based resources knowledge integration and we report results of experiments in which the knowledge integration process uses the deeper semantics of OWL ontologies to improve by between 8% and 13% the precision of relation extraction from text.
 Keywords Information extraction OWL reasoning Ontologies 1 Introduction Lexical resources come in many flavors, and range from conventional dictionaries with very little structure to case-frame descriptions with a high degree of structure. Some lexical resources are generated automatically or semi-automatically from background sources such as news articles, reports, etc., some are generated entirely manually. In all cases, as a resource grows in size, error is introduced as a consequence of scale, automation, or both.

In the more structured resources, the structure is usually there to make some semantics more accessible to automated tools. However, in many cases the actual semantics of the structure remains implicit, or implemented in software and not directly accessible to other tools. We have found that making these semantics explicit using declarative logical languages can help in a variety of ways such as maintenance of the resource (e.g., ensuring internal consistency), production of the resources (e.g., improving precision), access to the resource (e.g., improving recall), etc. In this article, we focus specifically on a tool called SemantiClean that reduces error in resources by processing axiomatic constraints. This paper is an extended version of an earlier one (Welty and Murdock 2006 ), which introduces several new elements and more comprehensive experimental results. 2 Background Ontologies describe the kinds of phenomena (e.g., people, places, events, relationships, etc.) that can exist. Reasoning systems typically rely on ontologies to provide extensive formal semantics that enable the systems to draw complex conclusions or identify unintended models. In contrast, lexical resources typically use much lighter-weight ontologies to encode their results, because those systems are generally not designed to enable complex reasoning. The same is true of natural language processing systems in general X  X heir ontologies, if present at all, are extremely lightweight and not intended for formal reasoning.
 We focus our attention on the automatic production of lexical resources using NLP. The techniques we discuss generalize to manually created resources as well, though we would expect the overall improvement to be less, as the manual processes for these kinds of resources are more reliable than automated ones. In general, automatic methods that produce such artifacts are extremely noisy , and empirically we have observed that the amount of noise goes up with the depth of the semantics that is extracted. The central idea is that the produced resource should have some structure which is intended to reflect the semantics of the data in a machine processable form, and by capturing that semantics formally we can find data that violates it and identify it as noise. We have been specifically focused on the use of semantic technologies, including but not limited to OWL and RDF, to augment the semantic power and automation in producing and using these resources. In this article, we discuss a technique called SemantiClean that removes noisy data from the resource by checking semantic constraints. In the experiments described here, we used axiomatic constraints specified in OWL-DL.

Our interest is less in theoretical properties of NLP or KR systems in general, and more in the realities of these technologies today , and how they can be used together to produce lexical resources. In particular, we have reused off-the-shelf components (e.g., Luo et al. 2004 ; Chu-Carroll et al. 2005 ) for information extraction embedded in the open-source Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally 2004 ), and open-source OWL and RDF components such as BOCA (IBM 2007 ), Pellet (Sirin et al. 2007 ), and Swoop (Kalyanpur et al. 2005 ). 3 Generating resources from text We use in our work components implemented within the Unstructured Information Management Architecture (UIMA). UIMA is an open-source middleware platform for integrating components that analyze unstructured sources such as text documents. UIMA-based systems define  X  X  X ype systems X  X  (i.e., ontologies with extremely limited semantic commitments) to specify the kinds of information that they manipulate (Go  X  tz and Suhre 2004 ). UIMA type systems include no more than a single-inheritance type/ subtype hierarchy. To do substantive reasoning over the results of UIMA-based extraction, one needs to convert results into a more expressive representation.
The general dataflow of our system is shown in Fig. 1 . A selected body of text is analyzed, extracting entity and relation annotations. The annotations are merged within and across documents using coreference resolution. This is a critical step for applying our technique, as the propagation of constraints requires that annotations referring to the same entity be merged to produce a graph. The end product of our analysis is a knowledge-base in RDF instantiating an ontology in OWL. 3.1 Text to knowledge pipeline In our evaluation prototype, we produce knowledge-bases from text in a pipeline that proceeds through several stages:  X  Keyword Indexing . The simplest and most scalable processing is the generation  X  Information Extraction . Information extraction (IE) in general can be viewed as  X  Coreference Across Documents . The annotations produced in the IE stage are  X  Knowledge Integration . Although it is not required, the data produced in the first 3.2 Knowledge integration challenges Knowledge Integration aligns the type system of the analytic components with the ontology of the reasoning components, such that the data produced by the analytic components can  X  X  X nstantiate X  X  the ontology. This process can be a trivial format mapping (e.g., from a UIMA CAS to RDF), or a more complex process of semantic alignment necessitated by the common practice of reusing off-the-shelf analytic components.

Knowledge integration can be extremely difficult due to the vastly different requirements and expectations of information extraction and knowledge represen-tation. As a result, what seems on the surface to be a natural connection (producing structured representations from unstructured information and then reasoning over those structures) and a simple transformation (mapping from an OO representation to OWL and RDF), turns out to be a difficult challenge. Below we list the five dimensions of interoperability we have identified and brief notes on how we are addressing them:  X  Precision . Formal reasoning systems are notoriously intolerant of errors, and IE  X  Recall . Imperfect recall is another significant obstacle to interoperability. The  X  Relationships . Simple IE systems that produce only entity type annotations  X  Annotations versus Entities . In our experience, relation annotation by itself  X  Scalability . IE techniques scale far better than KR techniques, and as a result we
Due to space considerations we will not discuss all these dimensions in this paper, and will focus mainly on the experiments we have performed to use deeper semantics expressed in OWL-DL to improve precision. The most important lesson we have learned from this integration effort is that researchers must be realistic about their data. It is not productive at all to build systems that expect perfect input from some other system. 4 Improving annotator precision and recall using owl One particularly promising result of our knowledge integration efforts has been using the kind of deep, axiomatic, semantics that OWL enables, to help improve precision and recall in the results. We present here our technique and two separate evaluations of its effectiveness with large UIMA-based applications that include dozens of  X  X  X ff the shelf X  X  analytic components. 4.1 SemantiClean technique for improving precision The most problematic kind of extraction produced by analytic components we have experienced is relation extraction. A common type of error we see in extracted relations is the violation of simple domain and range constraints. For example, in the following sentence: ... the decision in September 1991 to withdraw tactical nuclear bombs, missiles and torpedoes from US Navy ships ... our analytics extract an ownership relation in the underlined text between  X  X  X uclear X  X  (annotated as a weapon), and  X  X  X ombs X  X  (also a weapon), which maps to a ownerOf relation in the ontology. The ownerOf relation has a restriction limiting the domain to Person or Organization or GPE and a disjointness constraint between each of these and Weapon .

Our approach is a simple one. During knowledge integration, we construct an intermediate knowledge base (in fact, a Jena model) consisting of only the mapped entities and their type information. Then, during the mapping process producing relations, we add resulting triples to this KB one at a time. With each addition, we run the KB through a consistency check using Pellet. If the KB is not consistent, we  X  X  X rop X  X  the triple, if it is consistent, we add the triple to the output of the transformation. Obviously this technique does not scale particularly well and is entirely dependent on the degree to which the ontology is axiomatized. In preliminary experiments, however, the technique appears promising and does quite well X  X ffering a clear improvement in precision by dropping incorrect triples. We are still exploring how these results generalize, but we present here some concrete examples, analysis, and discussion. 4.2 Experiments We performed two experiments of this technique on data from two different corpora with different knowledge integration steps. 4.2.1 Intelligence analysis corpus and expressive ontology In our first experiment, we used a corpus of abstracts from a nuclear arms proliferation website extracted using mainly off-the-shelf UIMA components developed within IBM Research for other projects, and then through knowledge integration transformed the data into an expressive ontology specified by our project partners in the domain of nuclear arms proliferation. Other than a few specific classes for weapons, the classes and properties are fairly generic (people, places, facilities, etc.) and were specified to suit the needs of reasoning components developed in the project. As a result, the ontology was not a very close match to the data we extracted from text using off the shelf components, as it made distinctions familiar to knowledge representation but difficult to extract using shallow NLP. Most notable among these differences is the treatment of time.

The components of our experiment in more detail were:  X  Ontology . The ontology we tested consists of 56 classes and 62 object  X  Analytics . Our analytics are 42 off-the-shelf components that were developed for  X  Corpus . The corpus contains over 30 K documents that average about a page  X  Results . Our technique dropped 67 (object property) triples of the 834 4.2.2 Press release corpus and text analysis ontology In our second experiment, we used a corpus of corporate press releases and an ontology based entirely on the text analysis type system (each relevant annotation type has a corresponding OWL class or property). In this case the knowledge integration step was much simpler, as the ontology was already closely aligned with the data.  X  Ontology . The ontology we tested consists of 201 classes and 82 object  X  Analytics . In this experiment we ran newer versions of the same set of analytics  X  Corpus . The corpus contains over 500 documents of about 2 X 3 pages in length  X  Results . Our technique dropped 1,173 (object property) triples of the 8,898 4.3 Analysis combination of multiple independent annotators to determine the type of an entity. An example of this occurred in the following phrase:
With the use of these pits, landmines, and guerrilla attacks, Khmer Rouge forces allegedly drove off the personnel sent to repair the road.
 One of our entity and relation annotators incorrectly determines that  X  X  X hmer Rouge X  X  is a person who is the leader of the  X  X  X orces. X  X  However, the combination of annotators concludes that  X  X  X hmer Rouge X  X  is actually an organization. Since the OWL ontology indicates that an organization can X  X  be the leader of another organization, this triple is correctly dropped.

Of the 67 triples we reported dropped in the first experiment, 2 should not have been dropped. This appears to be the only source of error in the SemantiClean approach we have observed. The error was due to a combination of weak typing of entities and errors in another relation that did not manifest as inconsistencies until the triple in question was added to the KB. For example, consider the phrase: ... of countries like Pakistan, India, Iran, and North Korea, ...

A comma between two geopolitical entities often indicates a subPlace relation (e.g.,  X  X  X elhi, India X  X ), and one of our annotators incorrectly extracts a subPlace relation between India and Iran. The cross-document coreference process is unable to authoritatively assign the  X  X  X ountry X  X  label to the entity corresponding to  X  X  X ndia X  X , so it ends up as a GPE (geopolitical entity), a superclass of Country. The entity corresponding to  X  X  X ran X  X , however, is correctly typed as a Country. In the ontology, there is a local range restriction on the Country class that prevents it from being a subPlace of another country. So, if the entity corresponding to  X  X  X ndia X  X  had been correctly labeled as a country, our technique would have dropped the  X  X  X ndia subPlace Iran X  X  relation when it was mapped, however since some countries are subplaces of GPEs (e.g., France subPlace EU), the weaker GPE assignment for India allows the erroneous triple through. By happenstance, a subsequent triple in the mapping process results from this passage, ... were seized by Indian authorities after a raid ... where our analytics correctly extract a citizenOf relation in the underlined text between  X  X  X uthorities X  X  and  X  X  X ndian X  X , correctly coreference  X  X  X ndian X  X  with the entity for  X  X  X ndia X  X  in the previous passage, and correctly assign the type Person to the entity corresponding to  X  X  X uthorities X  X . The ontology contains a global range restriction for the citizenOf relation to instances of Country. Since the erroneous subPlace triple added previously prevents India from being a country (since a country cannot be a subPlace of a country), adding this correct triple causes an inconsistent KB. This shows the technique has some order dependences, had these triples been added in a different order the proper one would have been dropped. Fortunately our initial results indicate these circumstances to be rare (2 erroneous drops out of 834 triples in the first experiment, and none in the second experiment). Still, we are working on an approach using blame sets (Kalyanpur et al. 2007 ) that promises to remove this source of error.

A further 11 triples in the first experiment and 57 in the second fell into a special category in which the original annotated relations were correct, but the coreference resolution or type assignments for relation arguments were wrong, so a more robust solution would have been to amend the coreference or typing instead of dropping the triple. For example: ... and the increase in organized criminal groups in the FSU and Eastern
Europe.
In this case, the analytics produce a correct basedIn relation between  X  X  X roups X  X  and  X  X  X SU X  X  in the underlined text, but multiple annotators disagree on the type of  X  X  X SU X  X  (some correctly say GPE, some incorrectly say Organization), and the incorrect label (Organization) ends up winning. Overall our technique for combining annotations does improve precision, but like all IE techniques it isn X  X  perfect, as in this case. Therefore we end up with an organization being basedIn an organization, and the ontology requires organizations to be basedIn GPEs, and specifies that GPEs and Organizations are disjoint.

It is somewhat debatable whether dropping this triple is a mistake X  X learly it would be better to fix the type, but the entity corresponding to  X  X  X SU X  X , as presented to the KB, is an organization and cannot be the object of a basedIn relation. Thus the KB does end up cleaner without it. 4.4 Techniques for improving recall Our initial motivation for combining IE with semantic technology in general was the possibility of improving information access beyond keyword-based approaches through inference. For example, in the passage  X  X  X oe arrived in Paris X  X , no keyword search or search enhanced by semantic markup, would retrieve this passage in response to the query,  X  X  X ho is in France? X  X  Clearly with some world knowledge (that Paris is in France) and the ability to accurately recognize the relation in the passage (&amp; query), we could employ reasoning to catch it.

OWL-DL is not particularly strong in its ability to perform the kinds of  X  X  X -box X  X  reasoning that would be needed to make a significant improvement in this kind of recall. Other choices are RDF rules and translating the KBs into more expressive languages (like KIF). A semantic web rules language would obviously help here as well.

An interesting challenge is in measuring the impact of this kind of reasoning. It makes sense to call this an improvement in recall; in the simple example above clearly the passage in question contains an answer to the query, and clearly keyword search would not find it. However, it is a different sense of recall than is typically used in IE measurements. Recall measurements are based on comparison to a  X  X  X round truth X  X  (i.e., a human annotated corpus), in which implicit information does not appear. In textual entailment (Dagan et al 2005 ) the measurement problem is similar, however they address this in evaluations by always making the determi-nation based on pairs of text passages. So we can show improvement in recall by selecting meaningful queries and determining if and how reasoning improves the recall for each query, but measuring recall improvements in the KB itself is more difficult. 5 Related work Research on extraction of knowledge from text (e.g., Dill et al. 2003 ; Fikes et al. 2005 ; Murdock et al. 2006 ) is a mature and established field. Much of the work in this field has been using shallow, informal ontologies that make minimal semantic commitments (e.g., Marsh 1998 ; Byrd and Ravin 1999 ; Liddy 2000 ; Miller et al. 2001 ; Doddington et al. 2004 ; Vanderwende et al. 2005 ). More recently, especially with the development and rise of the semantic web, more ontology-based methods have been developed, such as those that compete in the ACE program (e.g., Cunningham 2005 ; Bontcheva 2004 ) and other semantic-web approaches (e.g., Maynard et al. 2005 ; Cimiano and Vo  X  lker 2005 ; Maynard 2005 ; Popov et al 2004 ). These systems directly populate small ontologies that have a rich and well-thought out semantics, but very little if any formally specified semantics (e.g., axioms specified in a language for which there are implemented reasoning tools) -the ontologies are extensively described in English, and the results are apparently used mainly for evaluation and search, with little (e.g., subclass inclusion) or no reasoning.

Among the very few exceptions are Cycorp, who published a white paper (Schneider 2004 ) describing a set of similar ideas to that of SemantiClean, in which deep commonsense knowledge in the Cyc knowledgebase combined with reasoning could be used to improve information extraction. The white paper does not discuss results or general impact of the techniques.

In OntoWordNet (Oltramari et al 2005 ), formal ontologies were used to improve the quality of WordNet. OntoWordNet, while ultimately different in scope since it did not focus at all on automated reasoning, has a very similar flavor. Axiomatized knowledge based on principles of formal ontology provided a deeper semantics to the resource than was originally provided. The deeper semantic precision exposed errors and problems with the resource that would not otherwise have been detected. In the case of SemantiClean, errors are found at the instance level of the resource, whereas OntoWordNet finds errors at the ontological level. 6 Conclusions In our research to use information extraction to help populate the semantic web, we have encountered significant obstacles to interoperability between the technologies. We believe these obstacles to be endemic to the basic paradigms, and not quirks of the specific implementations we have worked with. In particular, we identified five dimensions of interoperability that must be addressed to successfully populate semantic web knowledge bases from information extraction systems that are suitable for reasoning . We called the task of transforming IE data into knowledge-bases knowledge integration , and briefly presented a framework called KITE in which we are exploring these dimensions. Finally, we reported on the initial results of an experiment in which the knowledge integration process used the deeper semantics of OWL ontologies to improve the precision of relation extraction from text. By adding a simplistic consistency-checking step, we showed substantial improvements in precision over a very robust IE application without that checking.
This work is still in the beginning stages, but we do have results and conclusions, the most important of which is to address a long-standing problem that presents an obstacle to interoperability: being realistic. IE and NLP systems do not produce perfect output of the sort that KR systems deal with, and KR systems are not capable of handling the scale, precision, and recall that NLP and IE systems produce. These are not criticisms but realities. We cannot just sit back and wait for the two technologies to eventually meet, rather we must begin exploring how to realistically integrate them.

We should also point out that none of the implemented systems we used were baseline  X  X  X trawman X  X  systems, but reportedly state-of-the-art systems in each area. It is not our intention to advance research in information extraction nor in knowledge representation and reasoning, but rather in the combination of the two. We believe that the combination will be better than either individually, and have demonstrated one example of how this is so, using deeper semantics and reasoning to improve precision of relation extraction.
 References
