 Early research in spok en document retrie val (SDR) was spurred by a new way to overcome the high cost of producing metadata (e.g., human assigned topic labels) or manual transcripts for spok en doc-uments: lar ge vocab ulary continuous speech recog-nition. In this sense, SDR research has always been about making do with the available evidence. With the adv ent of automatic speech recognition (ASR), this available evidence simply gre w from being only expensi ve human annotations to comparati vely low-cost machine producible transcripts.

But today even more evidence is available for re-trie ving speech: (1) Using ASR text as input fea-tures, text classification can be applied to spok en document collections to automatically produce topic labels; (2) vocab ulary independent spok en term de-tection (STD) systems have been developed which can search for query words falling outside of an ASR system X  s fix ed vocab ulary . These evidence sources can be thought of as two book ends to the spectrum of domain dependence and independence. On one end, topic labels can significantly impro ve retrie val performance but require the creation of a (presumably domain-dependent) topic thesaurus and training data. Furthermore, classification accu-rac y will be poor if the ASR system X  s vocab ulary is badly matched to the collection X  s speech (e.g., we shouldn X  t expect a classifier to sensibly hypothesize automoti ve topics if the ASR system can not out-put words about car s or driving ). On the other end, STD systems offer the most promise precisely when the ASR system X  s vocab ulary is poorly matched to the domain. If the ASR system X  s vocab ulary already includes every word in the domain, after all, STD can hardly be expected to help.

The primary goal of this dissertation is (1) to ex-plore the combination of these new evidence sources with the features available in ASR transcripts or word lattices for SDR and (2) to determine their suitability in various domain-matching conditions. Secondarily , I X  X l explore impro ving the production of these new resources themselv es (e.g., by classify-ing with temporal domain kno wledge or more rob ust term detection methods).

Research in SDR has been inhibited by the ab-sence of suitable test collections. The recently avail-able MALA CH collection of oral history data will, in lar ge part, mak e this dissertation research possible (Oard et al., 2004). The MALA CH test collection contains about 1,000 hours of con versational speech from 400 intervie ws with survi vors of the Holo-caust 1 . The intervie ws are segmented into 8,104 documents with topic labels manually assigned from a thesaurus of roughly 40,000 descriptors. The collection includes rele vance assessments for more than 100 topics and has been used for several years in CLEF X  s cross-language speech retrie val (CLSR) track (Oard et al., 2006).

Participants in the CLEF CLSR evaluations have already begun investigating evidence combination for SDR, through the use of automatic topic labels X  although label texts are presently only used as an ad-ditional field for inde xing. In monolingual English trials, this topic classification represents a significant effort both in time and mone y (i.e., to produce train-ing data), so that these evidence combination studies have so far been rather domain dependent. Partici-pants have also been using what are probably un-naturally good ASR transcripts. The speech is emo-tional, disfluent, hea vily accented, and focused on a some what rare topic, such that the ASR system re-quired extensi ve tuning and adaptation to produce the current word error rate of approximately 25% . In this setting, we X  d expect STD output and topic la-bels to have low and high utility , respecti vely . To investigate the domain mismatch case, I will apply an off-the-shelf ASR system to produce new, com-parati vely poor , transcripts of the collection. In this setting, we X  d expect STD output and topic labels to instead have high and low utility , respecti vely . I will investigate impro ving SDR performance in both the poorly and well matched domain conditions through: (1) multiple approaches for utilizing auto-matically produced topic labels and (2) the utiliza-tion of STD output.

Throughout this paper , completed work will be denoted with a  X  ?  X , while proposed (non-complete, future) work will be denoted with a  X   X   X . 2.1 Speech Classification for SDR I outline three methods of incorporating evidence from automatic classification for speech retrie val. Cr eating Additional Indexable Text ?
The simplest way to combine classification and speech retrie val is to use the topic labels associ-ated with the classes as inde xable text. As a par -ticipant on the MALA CH project, I produced these automatic topic labels ( X  X  eyw ords X ) for the collec-tion X  s speech segments. These keyw ords were used in this way in both years of the CLEF CLSR track. For a top system in the track, using solely automat-ically produced data (e.g., ASR transcripts and key-word text), inde xing keyw ord text gave a relati ve impro vement in mean average precision of 40 . 6% over an identical run without keyw ords (Alzghool and Inkpen, 2007).
 Runtime Query Classification for SDR  X 
Simply using keyw ord text as an inde xing field is probably suboptimal because information seek-ers don X  t necessarily speak the same language as the thesaurus constructors. An alternati ve is to clas-sify the queries themselv es at search time and to use these label assignments to rank the documents. We might expect this to be superior , insof ar as infor -mation seek ers use language more lik e intervie wees (from which classification features are dra wn) than lik e thesaurus builders.
 Class Guided Document Expansion  X 
A third option for using classification output is as seed text for document expansion. The intuition here is that ASR text may be a strong predictor for a particular class label even if the ASR contains few terms which a user might consider for a query . In this sense, the class label text may represent a more semantically dense representation of the segment X  s topical content. This denser representation may then be a superior starting source for document centered term expansion. 2.2 Unconstrained Term Detection for SDR  X  It is not yet clear how best to combine a STD and topical rele vance IR system. One dif ficulty is that IR systems count words (or putati ve occurrences of words from an ASR system), while STD systems report a score proportional to the confidence that a word occurs in the audio. As a solution, I propose normalizing the STD system X  s score for OO V query terms by a function of the STD system X  s score on putati ve occurrences of in-v ocab ulary terms. The intuition here is that the ASR transcript is roughly a ground truth representation of in-v ocab ulary term occurrences and the score on OO V query terms ought to reflect the STD system X  s confidence in pre-diction (which can be modeled from the STD sys-tem X  s score on  X  X round truth X  in-v ocab ulary term occurrences). In this way, the presence or absence of in-v ocab ulary terms and their associated STD confi-dence scores can be used to learn a normalizer for the STD system X  s scores. In this section, I highlight both completed and pro-posed work to impro ve the production of evidence for combination. 3.1 Classifying with Temporal Evidence ? In spok en document collections, features beyond merely the automatically transcribed words may ex-ist. Consider , for example, the oral history data con-tained in the MALA CH collection. Each intervie w in this collection can be thought of as a time ordered set of spok en documents, produced by the guided intervie w process. These documents naturally arise in this conte xt, and this temporal information can be used to impro ve classification accurac y.

This work has so far focused on MALA CH data, although we expect the methods to be generally ap-plicable to speech collections. For example, the top-ical content of a tele vision episode may often be a good predictor of the subsequent episode X  s topic. Lik ewise, topics in radio, tele vision, and podcasts may tend to be seasonally dependent (based on Hol-idays, recurring political or sporting events, etc.). Time-shifted classification ? One source of tem-poral information in the MALA CH data is the fea-tures associated with temporally adjacent segments. Terms may be class-predicti ve for not only their own segment, but for the subsequent segments as well. This intuition may be easily captured by a time shifted classification (TSC) scheme. In TSC, each training segment is labeled with the subsequent seg-ment X  s labels. During classification, each test seg-ment is used to assign labels to its subsequent seg-ment.
 Temporal label weighting ? We can also benefit from non-local temporal information about a seg-ment. For example, because intervie wees were in-structed to relate their story in chronological order , we are more lik ely to find a discussion of childhood at an intervie w X  X  beginning than at its end. We can estimate the joint probability of labels and segment times on held-out data and use this to bias new label assignments. We call this approach tempor al label weighting (TL W).

In Olsson and Oard (2007), we sho wed that a combined TSC and TL W approach on MALA CH data yields significant impro vements on two sep-arate label assignment tasks: conceptual and geo-graphic thesaurus terms, with relati ve impro vements in mean average precision of 8 . 0% and 14 . 2% re-specti vely . 3.2 Classifying acr oss languages ? In multilingual collections, training data for meta-data creation may not be available for a particular language X  X  good example of domain mismatch. If howe ver, training examples are available in a sec-ond language, the metadata may still be produced through cross-langua ge text classification. In Ols-son (2005), we used a probabilistic Czech-English dictionary to transform Czech document vectors into an English vector space before classifying them with k -Nearest Neighbors and English training exam-ples. In this study , the cross-language performance achie ved 73% of the monolingual English baseline on conceptual topic assignment. 3.3 Vocab ulary Independent Spok en Utterance In Olsson (2007), we examined a low resource ap-proach to utterance retrie val using the expected pos-terior count of n -grams in phonetic lattices as inde x-ing units. A query X  s phone subsequences are then extracted and matched against the inde x to produce a ranking on the lattices. Against a 1-best phone sequence baseline, the approach was sho wn to sig-nificantly impro ve the mean average precision of re-trie ved utterances on five human languages. 3.4 Impr oving Spok en Term Detection  X  Phonetic lattices impro ve spok en term detection per -formance by more accurately encoding the recog-nizer X  s uncertainty in prediction. Ev en so, a cor -rect lattice may not always contain a path with the query X  s entire phone sequence. This is so not only because of practical constraints on the size (i.e., depth) of the lattice, but also because speak-ers don X  t always pronounce words with dictionary precision. We X  X  lik e to allo w approximate matching of a query X  s phone sequence with the phonetic lat-tices, and to do this as quickly as possible. This time requirement will pre vent us from linearly scanning through lattices for near matches. I am currently in-vestigating two solutions to this problem: phonetic query degradation and query expansion.
 Phonetic query degradation  X  The idea in pho-netic query degradation is to build an error model for the phone recognition system and to then degrade the query phone sequence such that it, hopefully , will more closely resemble recognized sequences. This approach incurs only a very slight cost in time and is query independent (in the sense that any term can be pushed through the degradation model X  X ot, for example, only terms for which we can find rec-ognized examples).
 Phonetic query expansion  X  The idea of phonetic query expansion is, again, to transform the clean phone sequence of the query into the degraded form hypothesized by a recognizer . Instead of using a degradation model howe ver, we simply run a first pass at STD with the non-de graded query term and use the putati ve occurrences to learn new, alterna-tive, degraded forms for a second search pass. This can be thought of as blind rele vance feedback or query by (putati ve) example.

The adv antage of this approach is that we are not required to explicitly model the degradation pro-cess. Disadv antages are that we (1) require exam-ples which may not be available and (2) assume that the degradation process is well represented by only a few examples. This dissertation will significantly contrib ute to speech retrie val research in several ways. Can we impr ove SDR by evidence combination? By exploring evidence combination, this dissertation will adv ance the state of the art in speech retrie val systems and their applicability to diverse domains. I will investigate multiple methods for combining the evidence presented by both STD and classification systems with con ventional ASR output (transcripts or word lattices). This work will develop upon pre-vious research which studied, in depth, the use of only one evidence source, e.g., (Ng, 2000). Can evidence combination decr ease domain de-pendency? I will investigate how combining evi-dence sources can increase their applicability to new content domains. This will include, for example, un-derstanding how (vocab ulary independent) STD sys-tems can be paired with fix ed vocab ulary ASR. Ho w can these evidence sour ces be impr oved? Lastly , I will explore how these new evidence sources may themselv es be impro ved. This will in-clude utilizing temporal domain kno wledge for clas-sification and impro ving the rob ustness of phone-based STD systems.

