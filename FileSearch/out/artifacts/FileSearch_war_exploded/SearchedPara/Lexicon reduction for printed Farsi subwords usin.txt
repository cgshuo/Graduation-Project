 ORIGINAL PAPER Homa Davoudi  X  Ehsanollah Kabir Abstract In this paper, we present a method to reduce the lexicon size of printed Farsi subwords, which utilizes the holistic shape along with the key character information to dynamically reduce the size of lexicon organized accordingly in two approaches: (1) based on the global shape description (to build a pictorial dictionary) and (2) based on constitutive character information (to build a textual dictionary). Given an input word image, the reduction procedure is accomplished in two successive stages. First, characteristic loci features are extracted and compared with the pictorial dictionary to select the candidate subwords based on their shapes similarity. The lexicon is further reduced in the second stage by determining the key character in the input image and comparing it with the textual dictionary. The key characters are defined as the ones which can be segmented and recognized rather easily and also, together with global descriptors, characterize the word image efficiently. A method for optimal selection of key characters is also proposed which is based on the mutual information of pictorial and textual dictionaries. The final candidate subwords are those sharing the same key charac-ter with the input image. The performance of the proposed method was studied experimentally on a set of 5,000 sub-word samples. The results obtained show a reduction rate of 97.83 % on a lexicon of 6,900 printed Farsi subwords. Keywords Lexicon reduction  X  Textual dictionary  X  Pictorial dictionary  X  Printed subwords  X  Key characters  X  Persian 1 Introduction Significant growth of textual information in pictorial forms and the high cost of managing them, create a great demand for effective and automatic methods to access this type of information. Automatic text reading is employed in different applications and requires speed and accuracy enhancement as well as cost reduction.

Word recognition can be implemented in two different approaches: segmentation-based and holistic. In segmenta-tion-based approach, the recognition of a word is carried out through the recognition of its characters. It should be noticed that the resulting sequence of characters may not form a valid word. Therefore, the result should be looked up in a dictionary. On the other hand, the holistic approach con-siders a word as an inseparable entity and tries to recognize it as a whole. The most challenging problem here is the high number of classes to be recognized. It is related to the lexi-con size and depending on the application may change from fewer than 50 words in the recognition of the legal amount in a bank check to more than 30,000 words in unconstrained applications. Although the small number of character classes makes it more desirable to choose the segmentation-based approach, but complexities raised in segmentation of cursive scripts, such as Farsi, direct the researchers toward the holis-tic approach. In Farsi/Arabic scripts, the term  X  X olistic X  may refer to either word or subword shapes. Subwords are the iso-lated components of the words, consisting a set of connected characters. (More detail can be found in Sect. 3 ).
Despite the usefulness of holistic approach in the recog-nition of cursive scripts, its application has been limited to the domains where the size of lexicon is relatively small. For a satisfactory performance in a large vocabulary application, a lexicon reduction process is usually performed prior to the final decision making. When a word image is submitted to the recognition system, the lexicon is pruned by eliminat-ing the unlikely words [ 16 ]. The recognition problem is then reduced to choosing the most likely word class between the remaining words.

The lexicon reduction system gets a word image, x , as its input and gives a set of most similar words, Q , from the lexi-con, L , as a reduced lexicon Q  X  L . In order to measure the performance of this system on a set of N t test word images, three criteria have been introduced in [ 21 ] as follows. All these three measures take values between 0 and 1.  X  Accuracy of reduction : The probability that the input sub- X  Degree of reduction : The decrease in the size of the lex- X  Reduction efficacy: A combination of the two above mea-Lexicon reduction can be carried out in different ways. Side-information available in some applications may limit the search space. Reduction of region name lexicon in postal applications is of this kind [ 9 , 29 ]. Postal code provides suf-ficient information to dynamically generate a list of possible city/street names. Linguistic knowledge is another source of information which can be used for recognizing words in phrases or sentences. The language models can be used not only as a source of information to limit the lexicon, but also to improve the overall recognition/retrieval accuracy through a post-processing stage [ 5 , 22 ].

As it is not always the case that such additional knowledge exists, e.g. in isolated word recognition, any characteristic of the input image is a critical alternative. Word shape features are valuable sources of information which can distinguish between various word classes. A simple and yet efficient characteristic of a word image is its length which discrimi-nates short and long words. Although it is not so easy, some methods have been proposed to estimate the length of a word [ 10 , 12 , 13 , 15 , 26 ]. In addition to the word length, other word shape features such as ascenders, descenders, loops, t-bars and i-dots are also used to reduce the lexicon [ 1 , 4 , 18  X  20 , 25 ].
While an efficient coarse description of the word shape is likely to be highly time efficient, it may not cause sig-nificant reduction in the vocabulary list and so, more refined word representation should be used. A recognition based lex-icon reduction have been proposed in [ 13 ],whereanHMM based recognition is done to create a ranked list of word hypothesis according to the distance between their models and input. The lexicon is reduced by considering the N -best results as the most expected candidates. This recognition is performed with less parameters and lower number of features than the main recognition to preserve the time efficiency. A similar method is performed in [ 11 ], for online handwriting recognition, using a two-stage system where the first stage is responsible for the lexicon reduction which works similar to the second stage but uses a more simplified feature set. In [ 6 ] a holistic lexicon reduction method for handwritten Arabic words is proposed which is based on the graph representa-tion of word images. An encoding method called weighted topological signature vector (W-TSV) is also introduced to transform each graph into a feature vector. The lexicon reduc-tion is achieved by a nearest neighbors search. They tested their framework on the historical and modern handwritten documents where the reduction of 92.9 and 83.5 % obtained respectively for an accuracy of 90 %.

In several other holistic approaches for lexicon reduction, feature vectors extracted from lexicon words are clustered to construct groups of visually similar words. Each cluster is represented by a prototype which is used as a lexicon entry to be compared with the input word and select the nearest clusters to form the reduced dictionary. A method presented in [ 23 ] maps the dictionary in to a 2D space through a self-organizing feature map to build word clusters. The neigh-borhood relationships in this space are then used to define a short list of hypotheses.

Beside holistic word representation, there are some other methods that annotates each lexicon word by a code (or a sequence of codes) extracted from their salient shape parts. An example is character shape coding where codes corre-spond to some topological characteristics of character shapes (e.g., ascenders or descenders and basic geometry) [ 4 , 17 , 28 ]. In more complicated approaches, a feature vector is com-puted for each character and classified to the corresponding code [ 30 ]. As the number of codes is fewer than the number of alphabet letters, lexicon words are grouped based on their codes. The reduction stage consists in encoding the input image in the same way as used during indexing and selecting the lexicon words having the same code as the input image. A method presented in [ 21 ] applies a heuristic strategy to detect downward pen-strokes in word images and obtains a string-based descriptor. The input descriptor is then matched with ideal descriptors extracted from the lexicon X  X  ASCII string. Another sort of approaches in this category relies on the con-cept of key characters to represent lexicon words according to their textual content. They attempt to identify some key characters in each word and use them to describe the word. In [ 31 ], a key character has been defined as a prominent part of a cursive word that can be reliably segmented and recog-nized. They used key characters in conjunction with word length estimation for lexicon reduction.

Many different shape-based lexicon reductions have been proposed in the literature; nevertheless, there are relatively few researches done in the field of Farsi word recognition [ 2 , 8 , 14 , 24 ]. In [ 8 ], a pictorial dictionary has been proposed to be used for eliminating search space in word recognition systems. To create this dictionary, characteristic loci features are used to cluster printed Farsi subwords, based on their holistic shape features. The minimum mean-distance classi-fier is then used to limit the lexicon words to those belong to more likely clusters. Testing on a set of 5,000 subwords, they reported an accuracy of 78.71 and 100 % by selecting the first and first ten closest clusters, respectively. In another research carried out on Arabic/Farsi lexicons, dots and dia-critical marks information have been used to encode words into strings. Next, the lexicon reduction is done based on the string edit distance [ 24 ]. The reported results of experiments performed on a set of 12,000 handwritten word images show a lexicon reduction of 93 % with accuracy of 85 %. In [ 2 ], a recognition approach for Farsi printed subwords has been developed based on the recognition of salient characters of the subword. The transition probabilities and the recogni-tion scores of salient characters are employed in a modified Viterbi algorithm to find the most probable combination of them in subwords to encode each word. 2 Proposed work In this paper, we present a lexicon reduction method for printed Farsi subwords (Fig. 1 ) which relies on two lexicon organization: (1) based on the global shape description, a pic-torial dictionary, and (2) based on key character information, a textual dictionary. The first one uses a global shape descrip-tion to partition the lexicon into groups of subwords having similar holistic shape. Characteristic loci method is used to represent each subword and feature vectors are clustered by k-means algorithm. This yields a pictorial dictionary. In the second organization, lexicon words are indexed on the basis of their key characters to make a textual dictionary. The key characters are a subset of characters have been set during the training.

The main idea of combining these two representations is to enrich the global property of the characteristic loci descriptor by considering the structural properties of characters. Since it is not so reliable to exactly locate all characters in a cursive word, we focus on some characters which can be segmented and recognized with less error. But, as the outstanding shape features of these characters is very likely to be already cap-tured by the global shape descriptor, it may not add much information to recognize them in an an additional step. To avoid this redundancy, the concept of key characters is pro-posed. Key characters, in our definition, are those making the most discrimination between words similar in global shape.
Given an input word image, reduction algorithm is per-formed in two stages: first, global shape features are extracted from the input image to classify it to a set of pre-determined word clusters (first stage in Fig. 1 ). Members of selected clusters are examined in the second stage to find the most relevant words based on their key characters. At this point, the key characters of the input image are recognized and the result serves to choose the words with the same key charac-ters from the selected clusters (second stage in Fig. 1 ). As it is very important to determine a proper set of key charac-ters being able to discriminate between the visually similar words, we also develop a method to choose the key char-acters. This method, which is performed earlier during the lexicon organization, finds the key characters considering not only their recognition simplicity but also the mutual infor-mation they share with the holistic descriptor of the word. The chosen key characters are utilized in the second stage of the proposed lexicon reduction to find the final relevant words.

In general, the novelties of our work can be outlined as follow:  X  Despite most of previous holistic methods, which depends  X  We propose a method to examine the diversity of word  X  We define the concept of key characters, as the ones that
The rest of the paper is organized as follows. In Sect. 3 , we present some specifications of Farsi script. Two lexicon organizations for building pictorial and textual dictionaries are explained in Sect. 4 . Section 5 defines the concept of key characters and presents a method to select them. The overall lexicon reduction system is described in Sect. 6 .The details of performed experiments and the results is presented in Sect. 7 . Section 8 concludes the paper. 3 Farsi script specifications Some characteristics of Farsi script is shown in Fig. 2 . Farsi is a right to left script containing 32 letters each may take dif-ferent shapes according to its position in the word. Each char-acter has an isolated form and may take up to three joining forms: beginning, middle and ending. The inherent cursive property of Farsi script is due to the possibility of character connections. A single character or a set of connected char-acters form a subword. Each word consists of one or more separated subwords. Although, all characters can join to their right neighbors in their ending form, but only 25 of them can also join to their left neighbors as well, and so take two other forms of middle or end. For example, a letter with an iso-lated form  X   X  have three other joining forms of  X   X ,  X   X  and  X   X  placed in the beginning, middle and end of a sub-word, respectively. Whereas, the letter with an isolated form of  X   X  just have one other end form of  X   X , which, unlike the previous example, is similar in shape to its isolated form. A number of Farsi letters have dots appearing in groups of one, two or three, above or below their main bodies. Some of these letters have similar bodies and only differ in the number and position of their dots. For instance, letters  X   X ,  X   X ,  X   X  and  X   X  are of this kind and have common bodies in all isolated and joining forms. However, there are also letters like  X   X  and  X   X  that have identical body in their beginning or middle forms, while their body shapes differ when used as an isolated or ending letter of a subwords.

The database used in this paper is part of the one gener-ated in [ 8 ] by taking the most commonly used Farsi words [ 27 ]. 12,700 subwords were obtained from these words which reduce to 6,985 bodies after removing dots and diacritics. These subwords are in Lotus font, one of the most popular Farsi fonts, with size 14 and scanned in 400 dpi. 4 Lexicon organization The goal of this section is to explain the two independent organization applied to construct two separate dictionaries: pictorial dictionary and textual dictionary. These dictionaries will be used later in the ultimate reduction system to partition the search space.

In order to gain the intuition of what these two organiza-tions do, Fig. 3 provides an example of a lexicon containing 12 subwords organized in two ways: (1) based on the sub-words global shape properties to make the pictorial dictio-nary. The subwords with similar holistic shape are located in the same clusters. (2) based on the ASCII code of some of their constitutive characters, here the first character, to build the textual dictionary. As an example, the first character of a group starting with  X   X  is marked. 4.1 Pictorial dictionary The objective is to group words with similar holistic shape in different categories. This categorization depends on the implemented shape descriptor. Each lexicon word is rep-resented by its corresponding feature vector in the feature space. These feature vectors are then clustered and make sub-lexicons with their means considered as the entries of pictorial dictionary. Later, in the reduction procedure, the feature vector of the input image is compared to all these entries in order to select a list of nearest clusters. Members of selected clusters will be examined in more detail in the textufal dictionary stage.

To construct the lexicon based on global shape descrip-tion, we use the method presented in [ 8 ] (Fig. 4 ). Charac-teristic loci, which is the most commonly used descriptor in printed Farsi word recognition, has been selected. To com-pute the characteristic loci features, a code is assigned to each background pixel that specifies its position relative to the foreground pixels. Coding of each background pixel is done according to the number of intersections of two hori-zontal and vertical lines passing through that pixel and the subword body in four directions: right, up, left and down. Figure 5 shows how this code is computed for a background pixel. As recommended in [ 8 ], we limit the maximum num-ber of intersection to 3. Concatenating the numbers associ-ated with four directions yields a four digit number of base 4. This number in base 10 is considered as the locus code of that background pixel. So, the locus codes are numbers between 0 and 255. The feature vector is considered a 256-dimensional vector, where each element represents the total number of background pixels that have locus number corre-sponding to that element. For example, the 176th element of this vector represents the number of background pixels with locus number of 176.

Due to the inherent characteristics of Farsi subwords, there may be some infeasible locus codes which produce some always-zero features. we omit these 33 features from the fea-ture vectors. Features are then normalized to have unity sum. Principle component analysis (PCA) technique is employed to reduce the dimension of feature space. The first 25 features were selected.

Feature vectors are then clustered applying k-means algo-rithm with Euclidian distance. According to [ 8 ], the proper number of clusters obtained based on the entropy criterion is 300. We used the same number of clusters in our work. The mean of feature vectors in each cluster is used as an entry of the pictorial dictionary. In order to make sense of how the words are distributed in different clusters based on their global shapes, some members of three clusters are shown in Fig. 6 . Note that, members of each cluster are almost simi-lar in their beginning and ending letters and have the same number of ascending/descending parts. 4.2 Textual dictionary In this section, we will explain how to build the textual dic-tionary. The goal of textual dictionary in our work is to group lexicon subwords based on the ASCII code of their constitu-tive characters. This grouping is done in three types: based on the beginning characters, based on the middle characters and based on the ending characters. These three types give us three different textual dictionaries, named B-Type, M-type, and E-type respectively. To construct the B-type (M-type; E-type) textual dictionary, every lexicon subword is placed in a group, G Bi ( G Mi ; G Ei ) , according to Fig. 7 . For instance, subwords  X   X  is located in the first group of the B-type dictionary G B 1 , according to its beginning character,  X   X . Similarly, this subword is placed in the group G E 12 of E-type dictionary. However, building the M-type dictionary is a little different which will be described later.

In Fig. 7 , beginning, middle and ending characters are sep-arately divided in to several groups according to their body shape. In addition, various characters similar in their salient parts are also placed together. For instance, as the ending circular part of the letters  X   X ,  X   X  and  X   X  are alike in shape, these letters are put together in the ninth group of ending characters, G E 19 . As we ignore the dots and diacrit-ics in our work, the common body shapes which are more important to us are also shown in this figure.

As it is clear from the this figure, the beginning and end-ing characters include all Farsi letters that take these two forms, but only five letters appear in the middle type set. The reason why we left out other letters in middle group was to avoid the segmentation problems that may happen during their recognition. Unlike beginning and ending letters which are more likely to be segmented correctly, precise extraction of a middle character is not so simple. Hence, in order to decrease the computational complexities of the subsequent reduction procedure, we confined the middle characters to the ascending/descending ones that can be detected easily even without word segmentation. As none of the Farsi letters have any descending component in their middle types, only five ascending letters of  X   X ,  X   X ,  X   X ,  X   X  and  X   X  X re considered.

Although due to the different length of subwords, a mid-dle character may locate anywhere in a subword image from second to the next to last position, here we do not distinguish between various positions and focus solely on the presence of any ascending characters regardless of their exact posi-tions. However, there may be more than one ascending mid-dle character in a subword image. To deal with this condi-a c : N u mber of ascendin g characters in the tion, the number of ascending characters is also considered in building the M-type dictionary. Given a lexicon subword, the M-type dictionary is created in the following way. First, the number of middle ascending characters, N ac , is deter-mined. Then, this number is applied to determine the associ-ated group of the subword, according to Fig. 7 . As a result, in the case of N ac = 1, the words are grouped based on the class of their ascending characters. In other cases, all sub-words with no ascending characters ( N ac = 0 ) are grouped in G
M 4 , whereas subwords with more than one middle ascend-ing character ( N ac &gt; 1 ) are placed in G M 5 . Building the textual dictionaries, the size of different groups for three B-, M-and E-type dictionaries are shown in Fig. 8 . Accordingly, G type, M-type and E-type dictionaries. 5 Key characters In this section, we define the concept of key characters and then present our method for selecting a set of characters as the key characters.

As our lexicon reduction system is based on the combi-nation of pictorial and textual information, we aim to find out which part of the textual information can provide the best result when combining with the pictorial information. For better explanation, notice to Fig. 9 , which shows some subwords partitioned regarding their groups in three textual dictionaries (as described in Sect. 4.2 ). The subwords are the members of cluster (c) in Fig. 6 . Based on this figure, the members of this cluster are partitioned to three groups in B-type and M-type dictionary and four groups in E-type dictionary. Hence, it can be concluded that E-type dictio-nary will make the most distinction among the members of this cluster. But is this the same for all other clusters? Which textual dictionary yields the most reduction when combined with the pictorial dictionary? We will discuss these issues in this section by defining the concept of key characters. 5.1 Definition In our definition, key characters are those which effec-tively discriminate the subwords similar in global shape. The idea comes from the fact that because of the global nature of the characteristic loci description, it is highly probable that some local shape features remain neglected in pictorial dictionary. To make use of this information, we are going to find characters whose shape characteris-tics have not been captured well by the global descrip-tion and hence are probable of providing some supplemen-tary information which can be efficiently exploited along with the global description to cause more reduction in the lexicon.

Our approach to evaluate different characters, and select some as the key characters, is based on the comparison of pic-torial and textual dictionaries. Comparing these dictionaries, we investigate how each textual group is distributed in vari-ous clusters of pictorial dictionary. If subwords with similar characters (i.e. of the same group) are scattered in various clusters, it can be concluded that the corresponding char-acter can provide additional information about the subword images in conjunction with the global shape features.
Figure 10 shows an example to clarify the concept of key character in our study. In this figure, bolded circular bound-aries indicate different clusters being created based on global shape features. Here, there are N c = 5 clusters labeled C C . The second textual organization of the lexicon subwords are shown by different colors (and also different geometri-cal shapes, to be legible in black and white printing), where they are partitioned into various groups based on their end-ing characters (i.e. their groups in E-type dictionary). Here, red (wave shape) for G E 1 , green (rectangle) for G E 3 ,blue (oval) for G E 13 and yellow (diamond) for G E 15 . Hence, as an example, cluster C 1 contains seven subwords similar in holistic shape though differing in their ending characters. It is clear from the figure that these two partitioning are somehow correlated. For instance, knowing that a word is ended up with letter  X   X  that put it in G E 3 , assures its existence in one of the clusters C 1 or C 2 . On the other hand, being a member of C , the word is most likely of group G E 3 . Hence, G E 3 does not provide much more information about the word shape characteristics than the shape clustering results. For another example, consider blue samples scattered in various clusters. In this case, there is not any specific cluster corresponding to G
E 13 as no cluster can be found that is mostly populated with blue samples. Therefore, it can be concluded that the global shape descriptor used in building the pictorial dictionary is inconsiderate of the local shape properties of ending letter  X   X . So, for a given input image assigned to a cluster, the subwords in that cluster may be further pruned by eliminating not relevant members if the input is of group G E 13 . 5.2 Key character selection As mentioned earlier, our approach in selection of key char-acters is to measure the association between the pictorial and various groups of textual dictionaries. The characters corre-sponding to the textual groups with lowest relevance to pic-torial dictionary is selected as the key characters. For quanti-tative measurement of this association, we were inspired by a kind of clustering evaluation technique refers to as  X  X xter-nal X , that measure the association between two partitioning results for a set of objects. The main concept in external clus-tering evaluation is to compare the resulting clusters with a ground truth consisting of real class labels. As our desired goal is to compare two partitioning of the lexicon and exam-ine their relation, this approach seems to match well with our purpose.

Selection of key characters is strictly depends on the global descriptor employed former to cluster globally similar sub-words (i.e. the pictorial dictionary). Considering that we employed the characteristic loci features, the key characters are found so that if combined with the characteristic loci fea-tures yields a better overall reduction results. However, the proposed technique works with any global descriptor and select key characters appropriate for that descriptor.
The scheme we used here [ 7 ] takes the advantages of entropy concept to determine that having an input image, how much informative is knowing the corresponding cluster in predicting the corresponding textual group. While know-ing the proper cluster for a given image increases our knowl-edge about its constitutive character, it can be realized that these two dictionaries are somehow correlated and their com-bination may not enhance the overall reduction significantly. On the other hand, if the initial knowledge about the clus-ters does not guide through its textual group, the latter can be considered as a valuable source of knowledge to be used along with the former. Accordingly, to select the most infor-mative characters as the key characters, a score is computed for each group of the textual dictionary, based on its con-ditional entropy X  X r equally mutual information X  X ith the clusters of the pictorial dictionary. 5.2.1 Score computation For a typical group of the textual dictionary, G l , a score is computed as follows. First, lexicon subwords are divided in two parts according to whether they are of group G l or not,  X  G l . This partition is shown in Fig. 11 for the sample lexicon of Fig. 10 for G E 13 and  X  G E 13 . Next, the con-ditional entropy of the partitioning given lexicon clusters C = c H ( G
Where N c is the number of clusters, p c j is the prob-ability of a subword x being in cluster c j , and p G l denotes the joint probability of a subword being in cluster c and group G l . Replacing the probabilities with their maxi-mum likelihood estimation, the conditional entropy will be rewritten as in Eq. 2 . H ( G
Where, N is the total number of subwords. The minimum value of H ( G l | C ) is equal to 0 which is reached when G l subwords are completely determined by lexicon clusters, i.e. the pictorial dictionary is a perfect predictor of G words. Whereas its maximum value occurs when pictorial clusters provides no useful information about the subwords ended with G l characters. This maximum value is equal to the entropy of G l without using the cluster information, which is given by Eq. 3 .
 H ( G
In this case, grouping the subwods according to whether they are in G l not, is a source of additional information to supplement the global shape description. Therefore, we con-sidered the value of conditional entropy computed for group G , H ( G bined with the pictorial dictionary. Those characters with the highest scores are then selected as the key characters to be employed in conjunction with our global descriptor. Consid-ering that the number of subwords in various textual groups differs, the conditional entropy measure gets incomparable values for different groups. So, we normalize the scores by the maximum expected value to obtain the final score, S n ( G for each textual group, G l (Eq. 4 ).
 S ( G 5.2.2 Results Given the pictorial dictionary built on the main lexicon, the proposed score, S n , was calculated for all groups of 3 dif-ferent textual dictionaries, and the results are presented in Fig. 12 .

According to this figure, it is clear that the reported scores for the B-type groups are relatively higher than the two other groups. The minimum score among the B-type groups obtained for group G B 1 (letter  X   X ) is 0.39, which is greater than most scores of M-type or E-type groups. In E-type groups, the scores that exceed this value belong to those letters that are fairly similar to at least one other letter. For instance, three groups of G E 9 , G E 10 and G E 14 have the high-est scores among the E-type groups. However, all characters in these groups contain an almost similar circular descend-ing part. Reminding that the ascending/descending parts of subword shapes are properly captured by characteristic loci, the subwords of these three groups are frequently located in the same clusters of pictorial dictionary. Although presence of various groups of subwords in one cluster leads to high conditional entropy, but as the ending characters of these subwords are highly similar, it is hard to recognize them pre-cisely. Now, if we merge these three groups to one, the score of this new group will decrease to 0.28. So, the relative higher scores reported for these groups are of little significance.
Since most of Farsi subwords are ended with an ascend-ing/descending letter which are mainly captured by charac-teristic loci descriptor, the relatively low scores of E-type characters seem reasonable. This is also true for the M-type groups as well, since we limit the participating characters to ascending ones, which has salient shape features. Based on M-type categorization of subwords, the third group associ-ated with the middle letter  X   X  has the highest score. How-ever, the number of words containing this letter is too small for significant reduction of lexicon size.

According to the scores obtained for different subword groups, we chose the beginning letters as the key characters to accompany the global shape descriptor in our lexicon reduc-tion scheme. It seems that making use of other high score characters of M and E type groups is beneficial to achieve finer and yet accurate description of the subword images. However, since one of the main goals of lexicon reduction is to speed up the recognition process, the computational com-plexity is of great importance. Based on this, in order to avoid further computational complexity introduced by searching whole subword image to discover the middle and ending key characters, we have ignored M-type and E-type dictionaries in this work. 6 Lexicon reduction In this section, the entire proposed lexicon reduction system is presented. Given an input subword image and a lexicon organized in two ways, pictorial dictionary and textual dic-tionary, the main objective is to choose a number of well-matched lexicon subwords to the input subword as a reduced dictionary. This reduction is performed in two consecutive steps taking advantages of both global and local information to assess the similarity of input image and lexicon entries. First, characteristic loci features are extracted from the input image and compared with all clusters of pictorial dictionary. Minimum mean-distance classifier is used to find the closest clusters among the set of 300 clusters of pictorial dictionary. A test set of 5,000 subword images has been used to evaluate the accuracy of classification. The results are represented in Table 1 .

Regarding this results, perfect classification is achieved by selecting the first 20 clusters. Therefore, for any input image, to make sure that the target subword is not missed out in the first step, we choose the 20 closest clusters and consider all their containing subwords as the candidates to be fed into the next stage, when these subwords are explored based on their key characters. At this point, the first letter of the input image is recognized and compared with the subwords using B-type textual dictionary. Those subwords starting with this letter are selected as the final reduced lexicon.
 6.1 Key character recognition As it is clear, one step of our reduction system is to recog-nize the beginning letter of every input image. In addition to its reliability, the first letter recognition must be fast enough, not to cause a large increase in the overall processing time. However, considering the properties of key characters, it is also clear that the problems in recognition of key characters are considerably smaller than the problems of unconstrained character recognition. One of these properties is the ease of segmentation of first character, and the other is the relatively low number of classes to be recognized. For recognition of the key characters, we utilize a method described in [ 14 ] with minor changes removing the unnecessary details based on our work. This method, while not complicated, provides us with a satisfactory recognition performance. Given a sub-word image, its beginning character is first extracted from the image. This character images is then transferred to the recognition module to be classified into one of the beginning character groups (Fig. 7 ).

Generally, segmentation is an important part of any ana-lytical recognition system which strongly impacts on the overall performance, as any failure in this step irrecover-ably propagates to the subsequent steps. So, there are sev-eral algorithm presented for segmentation of cursive scripts, like Farsi. Among various methods, we use a profile-based segmentation algorithm, a quite fast and reliable segmenta-tion algorithm, formerly implemented for Farsi script in [ 14 ]. Simplifying this algorithm to the extent that fits our need, the beginning character of an input image is extracted as follow. First, the top-profile of the image is found by determining the first cross of background to foreground pixels of the image, viewing from the top (Fig. 13 ). This profile is then scanned from right to left to find a right most horizontal stroke with length of three or more. The first point of such a stroke is considered as the segmentation point of the first character. To improve the segmentation, in addition to the stroke length, some other points are also considered: One, the distance of the segmentation point from the right border of the image should not be smaller than a threshold value(the pen-width); and the other, the middle broken of a horizontal stroke with one pixel deep is neglected (Fig. 13 ).

Designing of this method in [ 14 ] is done in a way that allows the over segmentation, while no under segmentation is occurred. However, as we deal only with the beginning character, the only effect of this over segmentation is that for some characters a simpler shape is extracted, which still can be recognized precisely. Figure 14 shows the bodies of ten groups of beginning characters extracted through our seg-mentation method. After extracting the beginning character, in the second phase, it should be recognized. To do this, we implement an multi-layer perceptron (MLP) classifier, which is rather fast in comparison with other classifiers, to classify the image among ten groups of beginning characters. The features applied for the recognition are the combination of characteristic loci features and the chain code histogram. Considering that the loci codes are previously extracted for global subword description, computing the characteristic loci features of the character images does not impose many cal-culations. But in order to improve the description, we also compute the chain code histogram and concatenate it to the characteristic loci features to form the ultimate feature vector of the beginning character image.

The classifier is trained using the features of beginning characters extracted from the lexicon subwords. The trained classifier is then used to recognize the character images. Con-sidering the high importance of recognition accuracy in this step, our ultimate decision is made based on the classification confidence, which is a number between 0 and 1 comes from the maximum class output of MLP and indicates how reliable the classification result is. If the confidence of classifying an input image is not high enough, the classifier does not return any class. In this case, the overall lexicon reduction system does not proceed to the second reduction step and outputs the initially selected subwords as the result. These recognition method is tested on the 5,000 test subwords and, rejecting any result with less than 60 % of confidence, the accuracy of 99.84 % has been achieved. Accuracy rates obtained under different rejection rates are reported in Table 2 . 7 Experimental result The overall performance of our proposed system is affected by the two reduction stages used for comparison of input images with the pictorial and textual dictionaries. The accu-racy of each stage was separately discussed in Sect. 6 , where we found that the pictorial-based reduction gives an accu-racy rate of almost 100 %, which guarantee the existence of the correct subword in the resultant reduced subwords of first stage, and, accepting the 1.26 % of rejection, the textual based reduction is nearly 100 % accurate, as well. In this section, more complete experiments are carried on to investigate the performance of each stage and, most importantly, their com-binations. For this purpose, another set of 5,000 test images have been generated,which is different from the test set used in Sect. 6 to define the parameters of pictorial dictionary and key character recognizer. To generate these samples, 1,000 subwords have been randomly selected among the lexicon entries. These subwords were printed in Lotus font with three sizes of 10, 12, and 14 and scanned at different resolutions of 200, 300 and 400 dpi. After removing dots and diacritics, a total number of 5,000 test images were obtained which were used to explore the reduction performance.

In order to evaluate the paper suggestions, different exper-iments are performed in two phases. First, we assume that the key character recognizer has an ideal behavior and works perfectly accurate. Having evaluated the lexicon reduction performance under this assumption, the proposed key char-acter recognizer is then added in the second phase of exper-iments, in order to demonstrate the system functionality in real applications. 7.1 Phase 1: Assuming perfect recognition for the key Most of our experiments in this part are focused on the degree of reduction. Our goal of assuming ideal key character recog-nition is to specifically evaluate the proposed idea of combin-ing pictorial and textual dictionary on the lexicon reduction, ignoring the key character recognition errors. Regardless of how well we can recognize the key characters, this experi-ment shows how useful is knowing the key character of the input image in reducing the lexicon size. Table 3 reports the obtained results of lexicon reduction in terms of degree of reduction in three cases: using pictorial dictionary, using var-ious types of textual dictionary and combination of pictorial dictionary and textual dictionaries. The first two cases show how much each dictionary, as its own, contributes in lexi-con reduction. In the case of pictorial dictionary, this result indeed shows the degree of lexicon reduction in the first stage of our proposed system. To study the impact of B-type textual dictionary on the overall reduction compared to the M and E-type dictionaries, we replace the B-type dictionary with M and E-type, respectively, and show the result in this table, along with the B-type dictionary. As expected, the higher degree of reduction is achieved when these two approaches are combined together, and as well, B-type textual dictio-nary results in the most reduction when combined with the pictorial dictionary. Note that although the E-type textual dic-tionary, on its own, outperforms the B-type, but combining with the pictorial dictionary, B-type dictionary causes more reduction.

For more detailed study, in the second experiment, we compute the average degree of reduction for subwords of different textual groups. The results are shown in Fig. 15 for various groups of three types of dictionaries. As is clear, despite the relatively high scores achieved for some groups in Sect. 5.2.2 , the reported degree of reduction is not similarly high. For example, the group G B 10 has the minimum degree of reduction in B-type dictionary, while its score has reported as one of the 3 top scores in Fig. 12 . This is due to the diver-sity of the number of subwords in different groups. As G B 10 is the largest B-type group (refer to Fig. 8 ), its corresponding reduction has a significant effect on the total degree of reduc-tion. Similarly, the high degree of reduction made by G E 13 subwords in E-type textual dictionary, cannot be interpreted as its advantage, since the total number of subwords in the lexicon ended with  X   X  is less than 200.
As another experiment, still ignoring any error in recog-nition of key characters, the lexicon reduction performance was evaluated for different reduction methods varying the number of clusters selected in the first stage and the results are shown in Fig. 16 . According to this figure, the perfor-mance of B-type Textual dictionary, when combined with the pictorial dictionary, is better than the two other types, as it achieves a higher degree of reduction for a given accuracy of reduction. 7.2 Phase 2: Applying the proposed key character In the second phase of our experiments, we drive our pro-posed key character recognition in and do the experiments again, to practically demonstrate our reduction system. Per-forming the lexicon reduction on the test subwords, the per-formance result is reported in Table 4 . As here, in addition to the degree of reduction, we also deal with the accuracy, this table has two more columns, compared to Table 3 , reporting the values of  X  X ccuracy of reduction X  and  X  X eduction effi-cacy X  (see Sect. 1 ). The results reported in this table are computed once by accepting all the key character recognition results, without any rejection, and once again by considering the nearly 1.25 % of rejection, as described in Sect. 6.1 .As mentioned there, in the case of rejecting a key character, the lexicon reduction is limited to the pictorial-based reduction in the first stage. Based on the reported result, applying this rejection improves the accuracy while keeps the reduction degree in a constant level. Thus, considering that in addition to the discriminant power of key characters, we were also concerned of their ease of recognition, the reported result is almost close to the first phase. 8 Conclusion In this paper, we presented a lexicon reduction method for printed Farsi subwords, which captures both the global and local features making use of two pictorial and textual dictio-naries. Pictorial dictionary is constructed by clustering the characteristic loci features extracted from lexicon subwords, while the textual dictionary is built according to the subwords key characters. Key characters are defined as those which dis-criminate between subwords similar in global shape. Based on this definition, we proposed a method to select the key characters using conditional entropy. As a result, we select the beginning letter of each subword as its key character and use these key characters to build the B-type textual dictio-nary. Given an input image, a lexicon reduction was then performed in two stages where a minimum mean distance classifier was used first to classify the input to the nearest clusters of the pictorial dictionary. Members of selected clus-ters were then examined in the second stage using the textual dictionary to find the most relevant words based on their key characters. To evaluate the lexicon reduction scheme, a test on 5,000 subword images in different sizes was performed and a degree of reduction of approximately 98 % was achieved. References
