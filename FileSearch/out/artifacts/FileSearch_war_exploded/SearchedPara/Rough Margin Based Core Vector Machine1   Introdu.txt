 People in computer science societies ha ve been questing for faster algorithms since long before. When come to mind the solving techniques of SVMs, there are several approaches ranged from the chunking method [1] to the sequential minimal optimization [2], as well as scaling down the training data and low-rank kernel matrix approximations [3]. Eventually, the Core Vector Machine (CVM) algorithms [4, 5, 6, 7] have gone to an extreme that they have linear asymptotic time complexity and constant asymptotic s pace complexity, since they transform the quadratic programming (QP) involved in SVMs to the minimum enclosing ball (MEB) problems. In order to perform this transformation the CVM takes the 2-norm error, which may cause it less robust and thus hurt the accuracy. Fortunately the notion of the rough margin in the Rough Margin based Support Vector Machine (RMSVM) [8] could make SVMs less sensitive to noises and we propose our Rough Margin based Core Vector Machine (RMCVM), which unites the merits of the two aforementioned methods.
After brief introductions to the CVM and the RMSVM in Section 2 and 3, we will first of all define the primal problem of the RMCVM. Next we shall elaborate how to solve an RMCVM through the approximate MEB finding algorithm. We will also investigate the loss functions used by the RMSVM and RMCVM. In the end experimental results are shown in Section 5. Given a set of points S = { x 1 ,..., x m } ,where x i  X  R d for some integer d , the minimum enclosing ball of S (denoted MEB( S )) is the smallest ball which contains all the points in S [5]. Formally, let  X  be a kernel induced feature map,
Let B ( c  X  ,R  X  )betheexactMEB( S ). Given an &gt; 0, a (1 + )-approximation The approximate MEB finding algorithm [9] uses a simple iterative scheme: at A surprising property is that the number of iterations, and thus the size of the final core-set, depend only on but not on d or m [9, 5].

The dual of (1) is max  X   X  diag( K )  X   X  K  X  s . t .  X   X  0 ,  X  1 =1,where  X  is the Lagrange multiplier and K is the kernel matrix. Conversely, any QP of this form can be regarded as an MEB problem [4]. In particular when where  X  is a constant (this is true for many popular kernels), we can drop the linear term  X  diag( K ) and obtain a simpler QP, Definition 1 (CVM [4]) min where C is a regularization parameter and  X  i are slack variables. The dual of (4) is analogous to the dual (3), in which K is replaced with  X  K 0exceptthatthe i -th component is 1). Hence the CVM is an MEB if (2) is true. To deal with the situation that (2) is not satisfied, Tsang et al. extend the MEB to the center-constrained MEB [10], and propose the generalized core vector machine [11] which is applicable for any kernel and can also be applied to kernel methods such as SVR and ranking SVM.

Note that the 2-norm error is used here. It could be less ro bust in the presence of outliers in theory to some extent [5]. The rough set theory, which is based on the concept of the lower and upper approximation of a set, is a mathematical tool to cope with uncertainty and incompleteness [12]. The rough margins [8] are expressed as a lower margin 2  X  l w andanuppermargin 2  X  u w where 0  X   X  l  X   X  u . They are corresponding with the lower and upper approximations of the outlier set, such that the samples in the lower margin are considered as outliers, the samples outside the upper margin are not outliers, and the samples between two rough margins are possibly outliers.
When the training procedure takes place, the RMSVM tries to give major penalty to samples lying within the lower margin, and give minor penalty to other samples [8]. Notice that the Universum SVM [13] uses a similar strategy. In practice, the RMSVM intr oduces slack variables  X  i and penalize them  X  times Definition 2 (RMSVM [8]) 3.1 Justification of the Rough Margin Apparently the RMSVM should encounte r severer overfitting problem since it emphasizes more on outliers than the  X  -SVM. However, the dual of (5) is RMSVM is, and ultimately the RMSVM would become an underfitting classifier.
Likewise the loss function, which was an absent topic in [8], could justify the rough margin well. Let f ( x i )= w  X  ( x i )+ b , Proposition 1. The loss function of the RMSVM is  X  arrives at  X  u  X   X  l ,since  X  i suffers less penalty in (5).
 y f ( x i )  X   X  in the  X  -SVM. Therefore the notion of the rough margin is a tool and technique to avoid the overfitting problem.

There is a side effect that  X  u is usually larger than  X  , which makes the RMSVM lies between two rough margins with tiny  X  i . This phenomenon slows down the speed and increases the storage of the RMSVM. For the sake of using the approximate MEB finding algorithm to solve the rough margin based SVM, we use 2-norm error because it allows a soft-margin L2-SVM to be transformed to a hard-margin one. Subsequently we have Definition 3 (RMCVM) The dual of (7) is where y =[ y 1 ,...,y m ] and the operator  X  denotes the Hadamard product. Remark 1. We omit the constraint  X  i  X  0 since it is dispensable for L2-SVMs. We omit the constraints  X  l , X  u  X  0 based on the fact that certain inequality constraints in the dual problem can be replace by the corresponding equality in the objective and additional constraint  X   X  0 , and the optimal  X   X  = 0 obviously. The constraint  X  u  X   X  l is indeed implicated by (7) already. Remark 2. Note that the regularization parameter of the original SVM [16] and the  X  -SVM [17] is C and  X  respectively. In the CVM it is C through which we w and  X  i are equal in (4), their coefficients would change simultaneously under scaling, which means that the coefficient of  X  does not influence very much. Remark 3. It is obvious that there is only  X  -RMSVM insofar as it applies 1-norm error. Similarly the RMSVM using 2-norm error would be C -RMSVM inherently. Therefore we demonstrate that Definition 3 is proper. Furthermore, Proposition 2. The loss function of the RMCVM is L 2 ( x i ,y i ,f )= derivatives of the objective function of (7) w.r.t.  X  i and  X  i respectively. 4.1 Solving Rough Margin Based CVM From now on we will proof that (7) can be solved approximately by the CVM. Proposition 3. The RMSVM cannot be transformed to an MEB problem unless we drop the group of constraints  X  i  X   X  m in (6) .
 Lemma 1. Given a non negative vector  X   X  R m , the optimal value of is between 1 4 m (  X  1 ) 2 and 1 4  X  2 .
 Proof. (Sketch) The upper bound is quite straightforward by setting  X  = 1 2  X  . Let V = {  X  :  X  1 = 1 2  X  1 } ,then V consists of a hyperplane in R m .Thelower Actually  X  i =0iffcot(  X  , e i )  X   X  , which is consistent with that there are less outliers than support vectors. Theorem 1. The optimum of (8) is bounded by max max  X  min  X  ,  X  h 1 (  X  )+ h 2 (  X  ,  X  )where h 1 (  X  )=  X  K  X  yy + yy + 1  X C I  X  and h (  X  ,  X  )= 1 C  X   X   X  2 .Then Corollary 1. The RMCVM can be solved approximately using the approximate MEB finding algorithm.
 Proof. Let Q (1) , X  1 , Q (2) , X  2 be Then (10) and (11) can be regarded as MEB problems with parameters The convergence of (10) and (11) are as same as the CVM but we omit the proof here. Hence the approximate RMCVM algorithms based on (10) and (11) have linear asymptotic time complexity and constant asymptotic space complexity. Recall that the RMSVM is slower than the  X  -SVM since it generates more sup-port vectors. Surprisingly the RMCVM most often generates less core vectors and are faster than the CVM, even though we solve two MEB problems. We implement the RMSVM using LibSVM [18] and the RMCVM using LibCVM . The parameters are fixed to  X  = 5 as [8] suggested, and =10  X  6 as [5] rec-Gaussian and its width is the better one computed by the default methods of LibSVM and LibCVM . The computation method of kernel width is kept unchanged over one dataset.
 and RC u for f u ( x ) from (11), where b = y  X  respectively. We denote RC avg as the average solution of them. For multi-class tasks the default one-versus-one strategy is used. The datasets 2 are listed in Table 1 .
 To begin with, we conduct experiments on a small dataset shown in Table 2 . Our accuracy is almost always higher than the CVM. We find that RC avg is not always the best and RC u is usually better than RC l . The next are results on large datasets in the top of Table 3 , where the time for reading input and writing output files is excluded from the training time. Note that the CVM and the RMCVM are very fast on extended-usps , on which all the RMSVM fail to give a solution in 24 hours. Moveover, the RMCVM is usually better than the CVM and even beat the RMSVM once on web . At last we display results on the extremely noisy SensIT Vehicle in the bottom of Table 3 . Perhaps the RMCVM could always choose the right core vector and have less iteration before convergence as a result. When C is too small the RMCVM is inferior to the CVM since it is underfitting. Motivated by the rough margin, we propose the rough margin based core vector machine and demonstrate that it can be solved efficiently. Experimental results show that the derived algorithms can handle very large datasets and the accuracy is almost comparable to the rough margin based SVM.

