 The deep web [1] is the content that is dynamically generated from data sources such as databases or file systems. Unlike the surface web, where pages are col-lected by following the hyperlinks embedded inside collected pages, data from the deep web are guarded by search inter faces such as HTML forms, web services, or programmable web API, and can be retrieved by queries only. The deep web contains a much bigger amount of data than the surface web [2,3]. This calls for deep web crawlers to collect the data so that they can be used, indexed, and searched in an integrated environment. With the proliferation of publicly available web services that provide programmable interfaces, where input and output data formats are explicitly specified, automated extraction of deep web data becomes more practical.

Deep web crawling is the process of co llecting data from search interfaces by issuing queries. Sending queries and retrieving data are costly operations because they occupy network traffic. More importantly, many deep web data sources impose daily quota for the queries to be sent. In addition, most data sources paginate the matched results into many pages. All these restrictions call for the judicious selection of the queries.

The selection of queries can be modelled as a set covering problem. If we regard all the documents in a data source as the universe, each query is a subset of the documents it can match, the quer y selection problem is to find the subsets (the queries) to cover all the documents with minimal cost. Since the entire set of documents is not available, the queries have to be selected from a sample of partially downloaded documents [4,5,6,7]. In particular, [7,8] demonstrates that the queries selected from a sample set of documents can also work well for the entire data set. This paper will focus on the set covering algorithm on the sampled documents.

The set covering problem is NP-hard, and approximate algorithms have to be used. For large problems, the greed y algorithm is often recommended [9]. The greedy algorithm trea ts every document equally important. In deep web crawling, not every document is the sam e. A very large document containing virtually all possible terms is not an important document in the sense that it will be matched sooner or later by some terms. The query selection algorithm can almost neglect such documents sin ce they can be covered by many terms. Therefore, the weight of a document is in versely proportional to the document size (IDS), or the distinct number of terms. In [10], we proposed and evaluated IDS approach.

This paper reveals that the document importance not only depends on the number of terms it contains, but also the sizes of these terms. The size of a term is the document it can cover, or it s document frequency. A document that contains a small term can be covered with less redundancy, therefore they are of less importance in query selection. A document that is comprised of large termsonlyiscostlytocover,sinceon ly large terms can be used. This kind of documents is more important, and the importance should be proportional to the minimal term size (TS) within the document.

Based on the above analysis, we propose the TS-IDS algorithm to select queries. It outperforms both greedy and IDS algorithms, and is extensively veri-fied on a variety of datasets. We also ex am the query selection process, and find that TS-IDS fundamentally differs from the other two approaches: both greedy and IDS methods prefer to use small terms (the terms with low document fre-quency) first, while TS-IDS tries to use fr equent terms first even though it causes redundancy in the initial stage. In the final stage it uses the small terms to pick up remaining documents.

Our work also makes a contribution to the classic set covering problem by leveraging the distributions of the terms and documents in natural languages. Most of the set covering research assumes that the data are of normal or uniform distribution. For instance, the classic benchmark for set covering algorithms is the famous Beasley data, all are of normal distribution. However, most real-world data follows power law, including natural language texts [11]. We are the first to use data distribution to improve optimization algorithms as far as we are aware of.

In the following we will first give an overview of the related work on deep web crawling, focusing on the query select ion task. Then we describe the problem formally in the context of set covering and bipartite graph. After explaining the motivations for the TS-IDS method, we present the detailed algorithm and its comparison with the greedy algorithm. S ection 4 compares th e three approaches, i.e., greedy, IDS, TS-IDS on four corpora. In deep web crawling, the early work sel ects terms according to the frequencies of the terms [12], in the belief that high frequency terms will bring back more documents. Soon people realize that what is important is the number of new documents being retrieved, not the number of documents. If queries are not selected properly, most of the documents may be redundant. Therefore, query selection is modelled as a set covering [ 4] or dominating vertex [5] problem, so that the queries can return less redundancies. Since set covering problem or dominating vertex problem is NP-hard, the optimal solution can costly be found, especially because the problem size is very big that involves thousands or even more of documents and terms. Typically, a greedy method is employed to select the terms that maximize the n ew returns per cost unit. We realized that not every document is equal when selecting the queries to cover them. Large documents can be covered by ma ny queries, no matter how the queries are selected. Therefore the importance of a document is inversely proportional to its size. We proposed IDS (inverse document size) algorithm in [10]. Our further exploration in this problem finds that the importance of the document depends not only on the number of the terms it contains, but also the sizes of those terms.
In addition to the optimization problem, query selection has also been mod-elled as reinforcement learning problem [13,14]. In this model, a crawler and a target data source are considered as an agent and the environment respectively. Then its selection strategy will be dyna mically adjusted by learning previous querying results and takes account of at most two-step long reward.

Query selection may have goals other than exhaustive exploration of the deep web data. For instance, in Google deep web crawling, the goal is to harvest some documents of a data source, preferably  X  X ood X  ones. Their focus is to look into many data sources, instead of exhausting one data source. In this case they use the traditional TF-IDF weighting to sel ect the most relevant queries from the retrieved documents [6]. For another instance, data sources may be ranked and only return the top-k matches per query. [15] studies the method to crawl the toprankeddocuments.
In addition to query select ion, there are other deep web crawling challenges that are out of the scope of this paper. The challenges include locating the data sources [16,17], learning and understanding the interface and the returned results so that query submission and data extraction can be automated [18,17,19,20].
The set covering is an extensively studied problem [21,22,23]. The state of art commercial Cplex optimizer can find optimal solutions for small problems. When the matrix contains hundreds of elements, it often keeps on running for hours or days. The greedy algorithm is believed to be the better choice for large problems. Although many other heuristics are proposed, such as genetic algorithms[24], the improvements are very limited. The cl assic test cases are Beasley data [24]. Most of the data are synthesized from normal distribution, i.e., in the context of our document-term analogy, the document size and term size follow normal distributions. Some datasets even have uniform size for all the documents and terms. Such datasets cannot reflect the almost universal power-law in real world, and prohibit the discovery of the algorithms such as TS-IDS. We also tested the TS-IDS algorithm on the Beasley datasets. The result is almost the same as the greedy algorithm. The reason is obvious X  X here is little variation of TS and DS, therefore it reduces to the greedy algorithm.

The inception of TS-IDS algorithm is inspired by the classic TF-IDF weight-ing. TS-IDS can be regarded as dual concept of TF-IDF. TF-IDF measures the importance of terms in a document in th e presence of a collection documents, while TS-IDS measures the importance of a document covered by a term among asetofterms. 3.1 The Query Selection Problem Given a set of documents D = { D 1 ,D 2 ,...,D m } and a set of terms T = { T 1 , T , ...,T n } , each document contains a set of terms. In turn, each term covers a set of documents. Documents and terms form an undirected bipartite graph G ( D,T,E ), where the nodes are D and T , E is the set of edges between T and D ( E  X  T  X  D ). There is an edge between a document and a term iff the term occurs in the document. This graph can be represented by the document-term matrix A =( a ij )where Let d D i and d T j denote the degrees of the document D i (the size of document) and term T j (the size of term) respectively. Note that d D i = n k =1 a ik ,d T j = trieval. We call it term size to be cons istent with document size. The query selection problem can be modelled as the set covering problem [25]. In our con-text, it is a set covering problem wher e the cost for each term is the term size d : Definition 1. (Set Covering Problem) Given an m  X  n binary matrix A = ( a ij ) . The set covering problem is to find a binary n -vector X =( x 1 ,...,x n ) that satisfies the objective function subject to Here the first constraint requires that e ach document is covered at least once. The second constraint says that the solut ion is a binary vector consists of either one or zero, i.e., a term can be either sel ected or not selected. The solution cost (
One may wonder that the cost of retrieving documents over a network depends on the total size of the documents rather than their number. In fact, for a crawler, when its goal is to download documents instead of URLs, it would be efficient to separate the URLs collection f rom the document downloading itself. More importantly, the cost for downloading documents is constant, for example, downloading all documents inside the target data source. Thus, the cost for downloading can be ignored. However, the cost for retrieving URLs is different from method to method. The cost of a query can be measured by the number of matched documents by it because of pagination function, i.e., web sites paginate the matched URLs into many query sessions (usually 10 URLs per session) not returning a long list. Each session needs one additional query (ordinarily it is a  X  X ext X  link). So the number of queries grows linearly with the number of matched documents. Hence, the cost for retrieving documents can be measured by the total number of documents retr ieved by the selected queries The set covering is an NP-hard probl em. The greedy method described in Fig. 2 is proved to be an effective approximation algorithm [9]. It iteratively
Fig. 2: The greedy algorithm selects the best term that maximizes the new harvest per cost unit, as described in Fig. 3. Initially, all x j = 0, meaning that no query is selected. Then it selects the next best query until all the documents are covered. The best term is the one that returns the most new documents per cost unit (  X  j /d T j ). Note that m is number of rows in the new matrix after the all covered documents are removed. 3.2 Motivation for the TS-IDS Algorithm The greedy algorithm trea ts every document equally when it selects the best query using  X  j /d T j . Every document contributes unit one to  X  j ,aslongasit is a new document not being covered by other queries yet. However, not every document is of the same importance in t he query selection process. This can be explained using the example in Fig. 1.

Document 7 and 6 have degrees 3 and 1, respectively, meaning that document 7 has more chances being captured t han document 6. In this sense, D 6 is more important than D 7 . If we include all the terms in the solution, D 7 is captured three times, while D 6 is captured only once. When we include a term, say T 4 ,in a solution, D 7 contributes only one third portion of the new document, because other terms could also be selected and D 7 will be covered again. Therefore, the importance of a document D i is inversely proportional to document size d D i ( IDS ).
 Furthermore, the document importance depends also on the term size (TS). Small (or rare) terms, whose degrees are small relative to other terms, are inher-ently better than large terms in achieving a good solution of set covering problem. Take the extreme case when all the terms have degree one. Every document will be covered only once without any redundancy. In general, when every term cov-ers k documents, The greedy algorithm can approximate the optimal solution large terms prone to cause high cost. Documents containing only large terms are costly to cover, thus they are more imp ortant in the query selection process.
Looking back at our example again in Fig. 1. Both document 5 and 4 have degree two. Document 5 has a query whose degree is 1, while document 4 has two queries whose degrees are both 5. This m eans that document 5 can be covered by query 2 without any redundancy, while document 4 has to be covered by either T 6 or T 7 , either one of them will most probably resulting in some duplicates. Therefore, we say that D 4 is more important than D 5 in the query selection process, even though thei r degrees are the same. 3.3 The TS-IDS Algorithm For each document D i , we define its document weight as follows: Definition 2 (Document weight). The weight of document D i , denoted by w , is proportional to the minimal term size of the terms connected to D i ,and inversely proportional to its document size, i.e.,
With this definition, we give the TS-IDS as described in Fig. 3. Note that m is the number of documents in the new matrix after covered documents are removed. The weighted new documents of a term T j , denoted by  X  j ,isthesum of the document weights containing term T j , i.e.,  X  j = m i =1 a ij w i . Compared with the  X  j in the greedy algorithm, where  X  j = m i =1 a ij , the difference in TS-IDS algorithm is the weight w i for each document. Compared with the IDS algorithm where the weight is 1 /d D i , TS-IDS weights a documents not only by its length (1 /d D i ), but also by the terms it contains. It gives a higher priority to short documents (1 /d D i ) that contain popular terms only (min T j  X  D i d T j ). 4.1 Data To demonstrate the performance of our TS-IDS algorithm, the experiment was carried out on four data collections that cover a variety of forms of web data, including regular web pages (Gov), wikipedia articles (Wikipedia), newswires (Reuters), and newsgroup posts (Newsgroup). 10 , 000 documents are selected uniformly at random from the original corpora. Table 1 summarizes the statis-tics of the four datasets, including the numbers of documents ( m ), the number of terms ( n ), and the degree properties of th e documents and terms. Figures 5 plots in log-log scale the distributions of the document size and term size respec-tively. As expected, document size follows log-normal distribution, while term size follows power-law [27]. The highly skewed data distribution is the basis of the success of our algorithm. In tra ditional set covering studies, the benchmark test data, called Beasley data [28], are uniformly at random. For such data, term size ( d T j ) and document size ( d D i ) are mostly the same. We have also carried experiments on these data sets, and found that, as expected, TS-IDS and IDS algorithms perform very closely to the greedy algorithm. Due to space limitation, this paper focuses on the document-term matrix data only. 4.2 Results and Discussions We run three algorithms, the Greedy (Fig. 2), the IDS algorithm in [10], and the TS-IDS algorithm (Fig. 3). Fig. 6 shows the comparison of these algorithms in terms of the solution quality, the average solution cost. The result is the average from running each algorithm 50 times for the same data. Each run may find a different solution because there may be a tie when selecting the best queries. When this happens, we select a random o ne from a set of equally good queries.
From Fig. 6, we can see that performance improvement differs from data to data. TS-IDS achieve better improvement for Reuters and Gov, but less for Wiki and Newsgroups, although all the four datasets have similar document size and term size distributions. For Reuters dataset, the TS-IDS outperforms the IDS method around 24%, and outperforms the Greedy method around 33% in average. On the other hand, for Wiki and Newsgroup datasets, TS-IDS is better than IDS method and Greedy method a round 6% and 10% respectively. This is because the solution costs for Wiki an d Newsgroups are already close to one. Note that the best solution is one, whose redundancy is zero. Therefore there is little room for the TS-IDS algorithm to improve.
To gain the insight into these algorithms, we plot the redundancy rate ( d T j / X  j ) for each query that is selected on Reuters data set in the first row of Figure 7. Note that the redundancy rate is the reciprocal of the new documents per re-turned document  X  j /d T j used in the greedy algorithm. Here the redundancy rate d / X  j is used to analyse query selection process since we want to measure the quality of a query from the viewpoint o f overlap. The x-axes are the number of queries selected. The y-axes are the redundancy rate of each query in log scale, i.e., the number of duplicates per new document in log10 scale. The re-dundancy rate is smoothed with moving window size 100. The second row shows the corresponding query size d T j , also in log10 scale and smoothed with the same window size. The data is obtained from the Reuters corpus. Other three datasets demonstrate similar patterns.

For the greedy algorithm, the first 5785 terms have redundancy rate one, meaning that all those terms have zero duplicates. After 5785 queries, the re-dundancy rate increases rapidly in exponential speed. For the last a few hundreds queries, the average redundancy rate is above 30. In the corresponding query size plot, we see that the first 5785 queries are mostly small ones. It starts with the average term size 3.2, then decreases beca use only smaller queries (i.e., small subsets) can be found that do not overlap with others already used. When over-lapping occurs after first 5785 queries, the query size increases greatly, causing high redundancy rate. Because most of the small queries are already used in the first stage, it has to use large queries to cover the remaining documents.
The IDS algorithm improves the greedy algorithm by distinguishing docu-ments according to their sizes. Long doc uments can be covered by many queries. Hence, in each iteration the IDS algorithm prefers the queries t hat cover smaller documents, even if the redundancy rate of the query is not the smallest. Thus, we can see that in the second column of Fig. 7 the redundancy rate is not al-ways one for the first 5000 queries. However, the overall pattern is similar to that of the greedy algorithm: it tends to select small terms first, and it suffers the same problem of a surge in redundancy rate and query size when small and  X  X ood X  queries are exhausted. A closer inspection on the dataset reveals that short documents normally contain popular words only. These documents can be picked up only by large queries, causing significant overlapping when most of the documents are already covered.

The TS-IDS algorithm solves this problem by giving higher priority to such documents. Since the document weight is proportional to term size, it starts with large queries to cover documents only containing high frequency terms, as we can see from column 3 of Fig. 7. Because of the large query, the redundancy rate is high at the beginning. The benefit of this approach is to save the small queries to fill in the small gaps in the final stage. These terms are not the best in terms of redundancy rate, but along the process of query selection, the redundancy rate decreases, and the overall performance is better. Surprisingly enough, the process is the inverse of the greedy algorithm: instead of selecting the best for the current stage, its current selection is i n average worse than later selections.
The greedy algorithm not only has the highest redundancy rate here (1.94 compared with 1.81 for IDS and 1.31 for TS-IDS), but also uses more queries than other two methods. It selects 7256 queries, while IDS uses 6738 queries, and TS-IDS uses 4051 queries. This paper presents the TS-IDS method to address the query selection prob-lem in deep web crawling. It is extensively tested on textual deep web data sources whose document sizes and docum ent frequencies follow the log-normal and power law distribution. By utilizing the distributions, TS-IDS method con-sistently outperforms the greedy and ID S algorithms. The success of the method is due to the highly skewed distributions of term size (Zipf X  X  law) and document size (log-normal).

Without loss of generality, this pape r discuss the set covering problem as-suming each query is a single term. Our bipartite graph model can be extended to allow nodes representing multiple terms. Although this will greatly increase the graph size, such queries of multiple terms also follow power-law distribution. Therefore, our result can be extended to such queries as well.

In real deep web crawling scenario, usually it is impossible to directly apply a set covering algorithm to all the documents. Those documents are not known yet by the algorithm. Besides, a data source is usually so large that even approximate algorithms such as the ones discussed in this paper cannot handle it. The only option is to run the set covering algorithm on a sample subset of the data source. In [7], we showed that a solution that works well on a sample is also a good solution for the entire data source.

Our method is restricted to textual data sources that returns all the matched documents. Many data sources, especially large ones such Google, rank the matched documents and return only the top-k matches. This kind of data sources demand a different query selection str ategy. One approach is to select and con-struct the low frequency queries, so that the number of matched documents is within the k limit.
 Acknowledgements. This work is supported by NSERC, BSSF(No.14JGA001) and the 111 Project under Grant Number B14020.

