 1. Introduction
Document clustering, dealing with the problem of grouping unlabeled text documents into a set of clusters, is of consid-ters. For example, in the news document clustering task, a user can choose to group news documents according to topics of news events, such as  X  X  X he 2008 Olympic Game X ,  X  X  X outh Korea Hostage X , and  X  X  X obel Prizes Awarded X . Alternatively, another
Traditional document clustering methods cannot detect the user preference automatically. However, users can provide some type of user-provided information. Pairwise constraints contain must-link or cannot-link information between two docu-vised document clustering, which groups unlabeled documents into clusters taking into account the user-provided information, is a different problem from ordinary document clustering and has received much attention recently.
Existing semi-supervised document clustering approaches tackle the problem from the perspective on how to use the user-provided information. The user-provided information is generally useful for improving the performance of document information. Most existing semi-supervised document clustering approaches involve user-provided information in a passive approach play an active role in the process. Informative documents can be actively selected rather than chosen at random. mative constraints and finds a good document partition as well.

The first contribution of our approach is to automatically select informative document pairs for obtaining user judgments so that the clustering performance can be improved with as few supervised data as possible. Active learning, extensively so that the most informative document pairs can be chosen to form pairwise constraints. The active learning approach and semi-supervised document clustering are processed in an iterative manner. In the semi-supervised document clustering pro-mal solution rather than a global one. Therefore, the active learning approach is incorporated to select document pairs according to the current intermediate clustering result so that the semi-supervised document clustering can find a better document pairs in our active learning process. This gain function is designed to measure how much information we can learn by revealing judgments of document pairs.

There has been little work on investigating active learning for semi-supervised document clustering. Basu et al. [3] pro-
The active learning process and the semi-supervised document clustering process are not iteratively conducted. Pairwise constraints are discovered before the semi-supervised document clustering process and are not affected by the nature of does not deal with text document data. Active learning applied to semi-supervised document clustering is closely related of labeled documents or training documents are needed for every classifier. In our approach, we discover a small number of pairwise constraints for some, but not necessarily, all clusters.

The second contribution of our approach is that we investigate language modeling for representing clusters. Currently, most existing semi-supervised document clustering approaches are model-based clustering and can be treated as parametric tering algorithm [2], a recent semi-supervised document clustering approach, is derived from the K -Means document clus-distributions. These parametric models may not work well for the semi-supervised document clustering problem. The main reason is that the underlying clusters may not follow the distribution assumption of the parametric model. Language mod-foundations and it often achieves promising results. One key idea of language modeling is to use a non-parametric proba-bility distribution as the document representation.

Another contribution of our approach is to relax the  X  X  X ag-of-words X  assumption. In most of the semi-supervised cluster-the documents are assumed to be independent. However, this  X  X  X ag-of-words X  assumption may not hold in reality. Different terms can be used to capture similar ideas due to different sources and different writing styles of authors. For example, tageous if we can relax this assumption. Term-to-term dependence relationships can be captured by the term co-occurrence statistics. User-provided pairwise constraints contain useful information for estimating the term-to-term dependence rela-tionships especially for the terms related to the same cluster. Terms co-occurred frequently in the documents labeled with the same cluster should have a higher probability of cohesiveness. Terms co-occurred frequently in the documents labeled to be helpful for estimating term co-occurrence probabilities. Pairwise constraints are then updated by the selected docu-ment pairs and are used to guide the subsequent semi-supervised document clustering process.

We have conducted extensive experiments on our proposed approach to evaluate: (1) whether our active learning ap-ument clustering approach proposed by Basu et al. [3] is investigated for comparison as it is a recent method on active learning in semi-supervised document clustering. We also compared our approach with a model-based semi-supervised doc-ument clustering approach [3] which uses centroids to represent clusters and does not learn constraints. Experimental re-sults demonstrate that our framework is more effective.

The remaining parts of this paper are organized as follows. Section 2 reviews related work on document clustering and semi-supervised document clustering. In Section 3, we present our semi-supervised document clustering approach which uses the discovered pairwise constraints to aid the document partition. In Section 4, we describe our proposed active learning approach for discovering informative document pairs. In Section 5, we briefly discuss the overall system design of our framework. Empirical results are presented in Section 6. We finally present conclusions and future work in
Section 7. 2. Related work
Document clustering, from the perspective of finding a method for partition documents, can be divided into hierarchical document clustering methods and partitional document clustering methods [20,26] . Hierarchical document clustering meth-chical document clustering techniques are composed of two basic approaches, namely, agglomerative document clustering approaches and divisive document clustering approaches. Agglomerative document approaches discover the hierarchical tree structure in a bottom up manner. In contrast to agglomerative document clustering approaches, divisive document clus-tering approaches discover clusters from top down. Partitional document clustering methods group text documents into a flat partition of a fixed number of document clusters. The most widely used partitional document clustering approach is the K -Means method. In [26], Zhao and Karypis experimentally evaluated nine agglomerative algorithms and six partitional algorithms. They also introduced a new class of agglomerative algorithms by constraining the agglomeration process using dimensional document space. Li et al. [15] treated the text document as a sequence of words. Whether documents share fre-quent word sequences or not is used as the measurement of document closeness.

Typical document clustering problems do not involve user-provided information for guiding the document partition and ument clustering problem where a small amount of user-provided information is available.

Semi-supervised clustering, which provides a way of incorporating the user preference or requirement in the form of con-straints are used.

Two typical types of constraints are used. One type of constraint is labeled documents [2,12,27] . The second type of con-straint is a pairwise constraint indicating whether two documents should or should not belong to the same cluster
Cohn et al. [8] considered more general types of constraints such as a requirement that a document should not belong to a mation. Kumar et al. [14] designed a triplet constraint which considers relative comparisons of the data points.
The user-provided information can be supplied at different times. A user can provide the information before the cluster-clustering algorithm. Cohn et al. [8] proposed a feedback-based semi-supervised clustering approach which incorporates feedbacks in the form of constraints to guide the next round clustering process. Different from active learning approaches in which the learning process selects data points for user judgments, the feedback-based methods let users choose data points for generating constraints. Huang et al. [12] investigated the idea of incorporating incremental user feedbacks and a small amount of sample documents for some, not necessarily all, clusters into text clustering. For the modeling of each cluster, a local weight metric is used to reflect the importance of the features for a particular cluster.
Regarding how constraints are used, existing semi-supervised clustering methods fall into three categories, namely, con-straint-based, distance-based, and a combination of the previous two. Constraint-based methods [22,2,10] directly use the must be satisfied. In [22], Wagstaff and Cardie incorporated must-link and cannot-link constraints into a clustering algo-types of constraint. A constrained version of the K -Means algorithm was developed.

Distance-based methods [1,8,23,14,7,12,24] improve the clustering quality by learning a more accurate distortion mea-presented an algorithm to learn a distance metric representing the examples of similar points. Their method is based on global Mahalanobis metric using component analysis (RCA) algorithm. Chang and Yeung [7] proposed a metric learning method which performs nonlinear transformation globally and linear transformation locally. In [14], Kumar et al. learned the underlying dissimilarity measure while finding compact clusters in the given dataset. Yan and Domeniconi [24] ad-dressed the high dimensionality problem by projecting the data and constraints in multiple subspaces and learning positive semi-definite similarity matrices therein.

Some approaches use constraints in more than one way. Basu et al. [4], as well as Bilenko et al. [5], combined the con-straint-based and distance-based approaches in a unified model. Basu et al. [4] proposed a probabilistic model for semi-supervised clustering based on Hidden Markov Random Fields. Bilenko et al. [5] presented a semi-supervised clustering [27] presented deterministic annealing (DA) extensions of three semi-supervised clustering approaches, namely, seeded clustering, constrained clustering, and feedback clustering, under a model-based clustering framework and compared their performance with multinomial models on several document corpora. In these methods, all the constraints are provided once before the clustering. All of these methods do not conduct clustering in an active feedback manner.
There has been little work on investigating active learning for semi-supervised document clustering. Basu et al. [3] pro-ter, while each cluster corresponds to one non-null neighborhood. The second phase consolidates the neighborhoods. How-ever, in this method, active learning is only a part of the preprocessing phase of the clustering and is not effectively ever, this method does not deal with text document data and focuses more on exploiting spatial implications of the con-straints. Cluster-level constraints, which indicate whether two clusters should be combined or not, are discovered in the active learning process rather than instance-level constraints. Moreover, this method employs complete-link hierarchical agglomerative clustering. The computation time is too costly for text data even without active learning. 3. Semi-supervised document clustering 3.1. Pairwise constraints
We use pairwise constraints as the type of user-provided information. The pairwise constraints can be represented as a set of document pairs with user-provided judgments. Given a pair of documents selected, two possible judgments can be and j c , representing the judgment that the two documents should be in different clusters. Let X pairs in must-link constraints. Let X c denote the set of document pairs in cannot-link constraints. Let J  X f j all possible judgments that a user may assign to a pair of documents and X provided judgments b J , we denote the pairwise constraints, W ,as f X f X  x  X  d 1 ; d 2  X  ; ^ j  X j x  X  d 1 ; d 2  X 2 X mc g , where x  X  d constraints W m and cannot-link constraints W c are denoted as follows: 3.2. Neighborhood
We attempt to discover the document-to-cluster relationship from pairwise constraints. The document-to-cluster rela-tionship is described by means of a set of skeleton structures of neighborhoods. A neighborhood is a set of documents that should be in the same cluster as derived from pairwise constraints. Neighborhoods are generated covering some, not neces-sarily, all clusters. Documents in must-link constraints are grouped together in the same neighborhood. Documents in can-not-link constraints are assigned to different neighborhoods. There is at least one cannot-link constraint between the documents in two different neighborhoods. For example, given a set of must-link constraints  X  d  X  d ; d 6  X  , as well as a set of cannot-link constraints  X  d represents a cannot-link constraint. Neighborhood n 1 includes three documents d straints  X  d 1 ; d 2  X  and  X  d 1 ; d 3  X  . Similarly, d 4 link constraints  X  d 1 ; d 4  X  and  X  d 1 ; d 5  X  , d 1 should not be in the neighborhood to which d hood n 2 is discovered and it contains documents d 4 , d 5 3.3. Cluster representation
We design a language model for each cluster, called cluster model M composed of all distinct terms. The cluster model is estimated by a variant of maximum likelihood estimation smoothed by the Jelinek X  X ercer smoothing method [25]. The formulation involves a background corpus which can be easily obtained by assembling a reasonably large collection of text documents. Specifically, the cluster model is given as follows: where M c is the cluster model representing cluster c ; P term frequency of t in the background corpus bg ; j bg j is the total number of terms in the background corpus bg ; k is a smoothing parameter in the range of 0 and 1.
 assigned to each cluster c , denoted as P  X  M c j d  X  . The estimation of P  X  M 3.4. Term-to-term dependence relationships
Term-to-term dependence relationships are captured by term co-occurrence probabilities which are calculated from the user-provided pairwise constraints. In semi-supervised document clustering, pairwise constraints contain useful informa-tion for estimating term-to-term dependence relationships especially for the terms related to the same cluster. The terms quently in cannot-link constraints should have a lower probability of cohesiveness.
 We estimate the term co-occurrence probability, , from pairwise constraints W . Let P probability for term t and t 0 , can be written as a set of P co-occurrence probability estimated from must-link constraints. Let P mated from cannot-link constraints. Here, we assume that t and t 0 are two different terms. P by the frequency of term co-occurrences within pairs of documents in pairwise constraints as follows: where f m  X  t ; t 0  X  is the term frequency of t and t 0 appeared within all document pairs in must-link constraints; f in the dataset. The overall term co-occurrence probability, P as follows: where a is a smoothing parameter in the range of 0 and 1. 3.5. Semi-supervised clustering
The goal of the semi-supervised document clustering process is to find a set of clusters which can maximize the gener-ation probability of documents with the consideration of a set of user-provided information. We assume that the generation probability of a set of documents can be expressed by the summation of the generation of a single document. For a single document d , it can be generated by first selecting a cluster c and then generating d from c . Moreover, each document is formed by a set of terms. Therefore, the generation probability of a document from a certain cluster c can be further estimated by the product of the probability of those terms appearing in the document. We also consider term-to-term the document set D is formulated as follows: probability of a single document d , M c is a cluster language model; p supervised document clustering process. When b is set to 0, the semi-supervised document clustering does not consider term-to-term dependence relationships. When b is set to 1, only the term-to-term dependence relationships are involved in the semi-supervised document clustering process. The real occurrences of terms in clusters are not considered.
We use an iterative process to locally maximize the objective function presented in Eq. (9). Cluster models and probabil-probability that the document d should be assigned to the cluster c , P  X  M n , we set P  X  M c j d  X  to 1 for the corresponding cluster c to which n
If d is not in any neighborhoods, we initialize P  X  M c j d  X  with 1 = K . P  X  M the documents in neighborhoods. P  X  M c j d  X  is estimated as follows: calculated. In the denominator, the overall generation probability of document d from all possible clusters is calculated.
In the second phase, the cluster models are estimated using P  X  M mated as shown in Section 3.3. The cluster prior probabilities p semi-supervised document clustering converges to a local maximum. 4. Gain-directed document pair selection
A major component in our framework is the design of gain-directed document pair selection. Our framework attempts to choose the best set of document pairs from the current discovered cluster assignments. Judgments provided by users for se-following optimization problem: where X is a set of selected document pairs; H is the current cluster assignments discovered by the semi-supervised docu-we can learn by revealing judgments of selected document pairs. We attempt to choose the document pairs that contain most informative information when revealing the judgments of document pairs.

The selection of the set of document pairs depends on the current cluster assignments, the current term co-occurrence probabilities, and on the user judgments of document pairs. The gain function can be written as where s is the number of document pairs that can be selected from the current cluster assignments; possible judgments of X and j i is a possible judgment of the i th document pair in X ; g  X  X ; H ;; which indicates how much we can learn from the set of judgments set of judgments ~ J would be assigned to X .

We assume that the information gained by revealing the judgment of a document pair is independent and does not rely on other selected document pairs. Therefore, the judgment gain function g  X  X ; H ;; where x  X  d 1 ; d 2  X  is a document pair in X ; g  X  x  X  d x  X  d 1 ; d 2  X  ; j is the judgment of the document pair x  X  d selected documents, the user can easily provide judgment mainly based on the nature of the two documents. Therefore, the abilities. We further assume that judgments of document pairs can be provided independently without considering the judg-ments of other selected documents. Thus, we can express P  X  where P  X  j j x  X  d 1 ; d 2  X  X  is the probability that the judgment j would be assigned to the document pair x  X  d
Given these underlying assumptions on g  X  X ; H ;; ~ J  X  and P  X 
Eq. (16). The information gained by revealing judgments for a set of document pairs can be expressed by the summation of the information gained by a single document pair. Let G  X  x  X  d how much we can learn by revealing the judgment of a single document pair x  X  d X thus can be obtained by ranking all the document pairs and selected the top s ones with the highest value on the G  X  x  X  d 1 ; d 2  X  X  : mulated via a document gain function. Given the current cluster assignments, we select document pairs from each currently constraints in the dataset. The documents with the highest values are selected and paired with one of the documents in where d 0 is a document randomly selected from the neighborhood of c belongs; x  X  d ; d 0  X  is a document pair formed by the document d and d ticular, either must-link j m or cannot-link j c indicating that d belongs to or does not belong to the cluster c probability that the document pair x  X  d ; d 0  X  would be assigned the judgment j ; g  X  x  X  d ; d cates how much we can learn from the judgment j of the document pair x  X  d ; d (17) and obtain g  X  x  X  d ; d 0  X  ;; j  X  since each document d is selected from the cluster to which it currently belongs.
As term co-occurrence probabilities play an important role in the semi-supervised document clustering, we attempt to choose the document pair which is helpful for better estimating term co-occurrence probabilities. The judgment gain func-changed by added the document pair which judgment j into pairwise constraints. g  X  x  X  d ; d P x  X  d ; d 0  X  with judgment j is added; j d j is the length of the document d .

We use the probability that document d should be assigned to the cluster c , P  X  M document is must-link or cannot-link to a document in a neighborhood. P  X  M ument clustering process through an iterative process discussed in Section 3.5. Suppose c ument d currently belongs and d 0 is a document randomly selected from the neighborhood of c
P  X  j j x  X  d ; d 0  X  X  are calculated as follows: where M c is the cluster model for the cluster c . 5. Overall system design The overall system design of our active learning framework on semi-supervised document clustering is depicted in Fig. 2 . depict data stores and data flows, respectively.

As shown in Fig. 2 , semi-supervised document clustering process and pairwise constraint discovery process are conducted pairwise constraints are discovered. The semi-supervised document clustering process, as described in Section 3, groups the documents into clusters taking into consideration of pairwise constraints, neighborhoods, and term co-occurrence prob-abilities. In the pairwise constraint discovery process, the gain-directed document pair selection component and the con-straint generation component are cooperated with each other to discover pairwise constraints as shown in Fig. 3 . In the ponent is discussed in Section 4. Selected document pairs are passed to the constraint generation component to discover pairwise constraints.

Fig. 3 depicts the pseudo-code of the pairwise constraint discovery process of our framework. We use s to indicate the number of pairwise constraints that can be generated and to record the number of pairwise constraints that has been dis-neighborhoods, document pairs are formed by taking a document which is currently assigned to c and not involved in any is currently assigned to the cluster c and not involved in any constraints, a document pair d and d ment. When a cannot-link constraint is obtained, document pairs will be formed for d and other neighborhoods until a must-borhood. The set of pairwise constraints and term co-occurrence probabilities are updated by the newly discovered con-straint and used in the subsequent semi-supervised document clustering process.

The time complexity of our approach is highly related to the nature of the dataset and the number of pairwise constraints the time complexity of our active learning component in worst case is O  X  BV ument pair in the dataset. For each document pair, we need to calculate the improvement on every term co-occurrence prob-abilities. The time complexity of semi-supervised document clustering is O  X  c KVA in Section 6.6.1, our semi-supervised document clustering converges quickly within eight iterations. Our approach requires more time than the method proposed by Basu et al. [3]. One reason is that we consider the term-to-term dependence rela-tionship. The other reason is that semi-supervised document clustering process and pairwise constraint discovery process are conducted in an interactive manner. Nevertheless, the computational time is worthwhile in exchange for significantly improved performance as shown in experiments. Besides, the execution time of our proposed framework is able to handle a large volume of documents in practice. 6. Experimental results 6.1. Datasets
The summary of datasets used in this paper is shown in Table 1 . Three real-world text corpora were used for generating datasets for conducting extensive experiments. The first corpus is the 20-Newsgroups collection contains text messages from 20 different Usenet newsgroups, about 1000 messages from each newsgroup. Following the ones larity. News-Similar-3 consists of 300 messages from 3 similar newsgroups (comp.graphics, comp.os.ms.windows.misc, and comp.windows.x) where cross-posting often occurs. News-Different-3 consists of 300 messages posted by three newsgroups of quite different topics (alt.atheism, rec.sport.baseball, and sci.space).

The second corpus, Reuters RCV1 corpus [19], is an archive of over 800,000 manually categorized newswire stories. News clusters would be in a reasonable range. Under such conditions, 6000 news stories were randomly selected. Two datasets were derived, namely RCV1-Region and RCV1-Topic . As a result, RCV1-Region and RCV1-Topic datasets contain the same set of 6000 news documents but organized in different kind of categories. RCV1-Region dataset organizes news stories in 12 re-gion categories, whereas RCV1-Topic dataset organizes news stories in four topic categories.

The third corpus is the TDT3 news corpus 2 provided by the Topic Detection and Tracking (TDT) evaluation project. rived a dataset, namely, TDT3 dataset, from the TDT3 news corpus by selecting native English newswire news documents. The is commonly preferred by real users.

We pre-processed all the datasets by stop-word removal. High-frequency and low-frequency terms were removed fol-lowing the methodology presented in [11]. The purpose of such processing is to eliminate terms which do not help in dis-criminating a cluster. The thresholds for removing high-frequency and low-frequency terms for the News-Similar-3 and
News-Different-3 datasets were set to 100 and 1, respectively. The thresholds for removing high-frequency and low-fre-olds were set to 400 and 1, respectively. 6.2. Evaluation metric
We use two commonly used metrics to evaluate the quality of a clustering solution, namely, normalized mutual informa-tion (NMI) and pairwise F-measure. NMI and pairwise F-measure evaluate the clustering performance from different aspects.
NMI measures how much the discovered clusters recognize the underlying user-labeled cluster structure. Pairwise F-mea-supervised clustering with pairwise constraints. Both of the two evaluation metrics are in the range of 0 and 1.
The formula for NMI is presented in [21]. It considers the information shared between the random variable H represent-ing the cluster assignments and U representing the user-labeled cluster assignments as follows: H  X  X  X  are calculated as follows: where X and Y are random variables.

The pairwise F-measure F is defined similar to the one used in [3]. It is calculated as a combination of recall R and pre-cision P for the document pairs without user-provided constraints as follows: where q c is the number of document pairs that are correctly predicted as in the same cluster; q 6.3. Experimental setup
We use the set of manually annotated cluster assignments to simulate the user judgments on the proposed document pairs chosen by the active learning model. If the two documents belong to the same user-labeled cluster, a must-link con-straint is assigned to the document pair. Otherwise, a cannot-link constraint is assigned.

To determine the value of the parameters of our framework, we conducted a parameter tuning process. We varied differ-eters which achieve the best performance are chosen for conducting all the experiments. The number of constraints was set mation. The best performance was obtained when a was equal to 0.6. The third parameter is b in Eq. (9) which is used to clusters are useful for semi-supervised clustering.

Another parameter in our framework is the number of constraints, s , to be discovered from each intermediate clustering result. We set a value for s according to the number of clusters in the corpus so that each cluster with neighborhood can possibly have some newly discovered constraints. We set s to 20 for the News-Similar-3 , News-Different-3 , RCV-Region ,
The background corpus for estimating cluster models as described in Eq. (4) is assembled by combining the 20-Newsgroups corpus, the Reuters RCV1 corpus, and the TDT3 corpus. 6.4. Performance of active learning
We conducted experiments for our framework on active learning labeled as Active-Semi on the plots investigation, we also ran experiments for a recent semi-supervised document clustering approach with active learning pro-actively learn constraints, is treated as the baseline.

From Figs. 5 X 9 , we present experimental results on our proposed active learning framework. Figs. 5 and 6 depict the re-Topic datasets. Fig. 9 depicts the experimental results on the TDT3 dataset.

From the experimental results, it shows that our proposed framework (labeled as Active-Semi) on actively selecting doc-uments is effective for improving the clustering performance. The NMI and pairwise F-measure scores are strongly corre-lated. Compared with the Explore-Consolidate model and the baseline model, the improvement of our proposed active 7 and 8 , our framework achieves good performance for the same set of documents with different clustering criteria. Gener-of constraints is equal to zero, the Explore-Consolidate and baseline models are reduced to ordinary K -Means clustering.
Moreover, as shown in all sets of experiments, our proposed framework performs better than the K -Means clustering with uments and clusters. We conducted experiments for further evaluating the performance of our semi-supervised document clustering framework. Detailed explanations are presented in Section 6.5.

For each evaluation metric, namely, NMI and pairwise F-measure, we also calculate the improvement ratio of our frame-work taking the clustering performance without constraints as benchmark. The performance of Explore-Consolidate model is also shown taking the K -Means clustering as benchmark. The improvement ratio is computed as the relative value of improvements to the value of the benchmark. Tables 2 and 3 depict the experiment results for the RCV1-Region , the
RCV1-Topic , and the TDT3 datasets. Our proposed approach performs better than the Explore-Consolidate model. Therefore, estimation of the term co-occurrence probabilities is useful for determining the document pair quality.
As shown in Fig. 6 , a perfect clustering result can be achieved for the News-Different-3 dataset with less than 300 con-sults. For the News-Similar-3 , RCV1-Region , RCV1-Topic , and TDT3 datasets, perfect partitions can be achieved when the numbers of constraints were set to 484, 43,392, 12,290, and 13,486, respectively. However, as shown by the experimental straints are most useful when the document partition is not very good. Therefore, when the document clustering perfor-mance is good according to the user preference, there is no need to ask for more user-provided information for guiding the document clustering process. 6.5. Performance of the semi-supervised document clustering
We also conducted experiments for our semi-supervised document clustering framework without active learning labeled as LM-Semi on the plots. Pairwise constraints were selected randomly. The experimental results of the baseline model, a model-based semi-supervised document clustering approach using centroids for the cluster representation, are also depicted for comparison. The clustering performance is evaluated by varying the number of constraints. For each number of con-straints, the same set of constraints are selected randomly and applied to the LM-Semi model and the baseline model.
We used RCV-Region , RCV-Topic , and TDT3 datasets for conducting experiments since these are relatively large datasets for evaluating the effectiveness of our language modeling method. The experimental results are shown from Figs. 10 X 12 .
The experimental results depict that our proposed framework performs better than the baseline model. Therefore, our pro-posed language modeling method is effective due to the fact that it is useful for representing clusters. 6.6. Additional experiments
Besides directly evaluating our proposed approach, we also conducted experiments investigating three issues that are tering. The second issue is the effect on initialization. The last issue is the effect of document preprocessing. 6.6.1. The convergence of the semi-supervised document clustering
We investigate the convergence of the iterative process in the semi-supervised document clustering empirically as shown constraints was set to 200. We calculated the logarithm values of the objective function of semi-supervised document clus-tering as given in Eq. (9) with increasing number of iterations. In our semi-supervised document clustering, the objective 6.6.2. The effect on initialization ues in the semi-supervised document clustering process. The performance is computed by taking the average of these 10 experiments. We show the statistics of these 10 experiments of our framework measured by pairwise F-measure in Table values do not have much effect on the clustering performance with 200 constraints. The clustering partition is stable with just a small number of user-provided documents. 6.6.3. The effect of document preprocessing
Recall that we pre-processed text documents by removing stop-words. Another step is to remove the high-frequency and low-frequency terms [11]. We also conducted some experiments to investigate the effectiveness of this high-frequency and low-frequency term removal step by comparing the experiments on our framework without removing high-frequency and low-frequency terms. The number of constraints was set to 200. The results, measured by pairwise F-measure, are shown in for reducing large sparse text data and speeding up the computation. 7. Conclusions and future work
We have presented an active learning framework that actively selects informative document pairs for user feedback in the by measuring how much we can learn by revealing judgments of document pairs. Experimental results show that our pro-posed active learning approach is effective. The clustering performance is dramatically improved with selected document pairs. Our proposed approach outperforms the recent method mentioned in [3]. Our approach also clearly achieves better performance than the one without active learning.

There are several directions for further study. One direction is to incorporate the document distribution into the gain function. In our current framework, we attempt to choose document pairs which are useful to estimate the term co-occur-dataset. The documents can be weighed by the document density. A selected document with low density is not indicative of the other documents.

Another direction is to discover the hierarchical relationship of the documents. Currently, we consider pairwise con-can be actively learned during the clustering process.
 Acknowledgements The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Nos: CUHK4193/04E and CUHK4128/07), the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050363 and 2050391), and the Hong Kong Polytechnic University Internal Competitive
Research Grants 2007 X 2008 (Project Code: A/C code G-YG39). This work is also affiliated with the Microsoft-CUHK Joint Lab-oratory for Human-centric Computing and Interface Technologies.

References
