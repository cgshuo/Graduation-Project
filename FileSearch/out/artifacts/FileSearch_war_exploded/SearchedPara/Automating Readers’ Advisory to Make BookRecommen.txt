 The academic performance of students is affected by their reading ability, which explains why reading is one of the most important aspects of school curriculums. Promoting good reading habits among K-12 students is essential, given the enormous influence of reading on students X  development as learners and members of society. In doing so, it is in-dispensable to provide readers with engaging and motivat-ing reading selections. Unfortunately, existing book recom-menders have failed to offer adequate choices for K-12 read-ers, since they either ignore the reading abilities of their users or cannot acquire the much-needed information to make recommendations due to privacy issues. To address these problems, we have developed Rabbit, a book recom-mender that emulates the readers X  advisory service offered at school/public libraries. Rabbit considers the readability levels of its readers and determines the facets, i.e., appeal factors, of books that evoke subconscious, emotional reac-tions on a reader. The design of Rabbit is unique, since it adopts a multi-dimensional approach to capture the reading abilities, preferences, and interests of its readers, which goes beyond the traditional book content/topical analysis. Con-ducted empirical studies have shown that Rabbit outper-forms a number of (readability-based) book recommenders. Information Systems [ Retrieval tasks and goals ]: [Rec-ommender systems] Recommendation system; books; K-12; readers X  advisory
Besides watching TV, text messaging, and playing com-puter games, children and teenagers these days often spend spare time browsing through the Internet, looking for some-thing fun to do. YouTube and Facebook are examples of popular sites among young audiences that offer free enter-tainment on demand anytime and anywhere. Recent statis-tics 1 have shown that children/teenagers spend an average of seven hours a day on (smart) devices. This is alarming, since a significant number of children/teenagers are under-achieving at school, especially in reading . According to the 2013 National Assessment of Educational Progress, 2 only 32% of American 4 th graders are proficient in reading. Kids should allocate some of their free time on reading to enhance their educational experience. To turn the tide, educators, parents, government agents, and private organizations must join force to encourage kids to read. Unfortunately, very few existing websites/(non-)government agents are equipped with the resources/technologies to cope with the problem.
To motivate K-12 readers, it is necessary to avoid present-ing these readers with books that are either too easy/difficult to read or involve topics unappealing to them which could diminish their interest in reading. In fact, finding the right books for the right audience is not easy. Even though exist-ing recommenders can assist readers in finding books, they rely either on large historical data, e.g., personal tags/ratings, (which might not be available) or readers X  interactions on so-cial sites (which may not be accessible involving K-12 read-ers due to privacy issues). Furthermore, these systems ig-nore the reading abilities of the respective readers in recom-mending books. To address the shortcomings of these de-sign methodologies, we have developed Rabbit, a R eaders X  a dvisory-b ased b ook recommendati on t ool that makes per-sonalized, appealing suggestions on books to K-12 readers.
Rabbit is unique, since it simulates readers X  advisory (RA), a service offered at school/public libraries which are estab-lished to champion and encourage reading. RA involves knowledgeable professionals in finding reading materials of interest for their patrons [14]. During the search process, librarians identify the topics , contents ,and appeal factors , i.e., literary elements, appealing to each reader and suggest books accordingly. By offering this service, which is in high demand even in the era of the Web 2.0, libraries provide  X  X  vital link between library materials and readers X  [14]. Un-fortunately, (young) readers may not approach readers X  ad-visors to ask for suggestions, feel their interest are obscure or low-brow , or not even visit libraries in person [5]. By au-tomating the RA process, we replace RA in finding books appealing to K-12 readers, which eliminates the interaction with professionals and at the same time handles any num-nytimes.com/2010/01/20/education/20wired.html? r=0 http://goo.gl/Ad1VbE. ber of readers simultaneously that cannot be achieved by traditional RA.

Rabbit is novel, since besides analyzing the reading ability of a reader R , it examines appeal factors to capture the rea-sons why a book Bk is appealing to R . Rabbit determines the facets of Bk that instigate a reaction to Bk ,whichin turn impacts the perception of R on Bk . Rabbit explores the literary elements of books that identify the rate in which the stories unfold (pace), their overall structure (storyline), the feelings that these stories evoke on a reader (tone), and subject matters that some readers might find unpleasant or offensive (special topics), in addition to the qualities of the characters (characterization) and the language and level of details (writing style) of the stories. Rabbit is neither af-fected by the cold-start problem nor requires feedback from its users, and its design surpasses the content or reading patterns explored by state-o f-the-art recommenders.
While topics and content descriptions of books are freely and publicly available from reputable online sources, such as the Library of Congress, 3 appeal-factor/-term descriptions, which are fundamental for our recommendation strategy, are either determined by professionals on-the-fly or accessed through a paid subscription to RA databases. To automate the process of extracting appeal-term descriptions of books, we have developed ABET (A ppeal-b ased e xtraction t ool), a component of Rabbit, which relies on book reviews re-trieved from book-related websites, such as Amazon(.com) and Powells.com. ABET is based on rules that examine typed dependencies and part-of-speech tags of words in re-views to identify appeal factors and their terms. ABET relies on reviews for extracting appeal-term descriptions on books, since they are readily available online and capture readers X  varied opinions on the literary elements of a book.
Numerous approaches have been developed to identify and extract either features, (the polarity of) opinions, or feature-opinion pairs from reviews based on bootstrapping, natural language processing, machine learning, extraction rules, la-tent semantic analysis, statistical analysis, and information retrieval. (An in-depth review of state-of-the-art approaches adopted for opinion mining and extraction can be found in [11].) A product review describes the product X  X  actual fea-tures, such as the  X  X oom X  of a camera, which is unlike a book review that evaluates  X  X rganization and writing style, possible market appeal, and cultural, political, or literary significance X  of a book [15]. A book review is a form of lit-erary criticism in which the work is analyzed based on its content, style, and merit. We have observed that existing information extraction approaches on product reviews are ill-equipped for book reviews, since sentences expressed in book reviews tend to be more elaborated than the ones used to describe products. For this reason, ABET is not designed using any existing extraction approach. Instead, it relies on simple rules to perform linguistic and semantic analysis of the content and writing style of book reviews.

A number of book recommendation systems have been de-veloped in the past. Amazon X  X  recommender suggests books based on the purchase patterns of its users [8], whereas Yang et al. [17] analyze users X  access logs to infer their preferences and apply the traditional collaborative-filtering (CF) strat-catalog.loc.gov/ egy to make book recommendations. Givon and Lavrenko [4] combine CF and social tags to capture the content of books for recommendation. Sieg et al. [16] rely on the standard user-based CF framework and incorporate semantic knowl-edge in the form of a domain ontology to capture the topics of interest to a user. The hybrid-based recommenders in [4, 16, 17], in addition to Rabbit, overcome the cold-start problem. Unlike Rabbit, however, the others require (i) his-torical data on the users in the form of ratings, which may not always be available, or (ii) an ontology, which can be labor-intensive and time-consuming to construct.

Unlike Rabbit, PReF [12] examines users X  connections as part of the recommendation process, which may not be ac-cessible as they involve K-12 readers due to privacy imposed on children. BReK12 [12], which is the closest book recom-mender compared with Rabbit, is based on content and read-ability analysis. The former, however, relies on the availabil-ity of bookmarking information offered by social bookmark-ing site users to analyze reading patters of users. Further-more, with the exception of BReK12, neither of the afore-mentioned recommenders considers the readability level of their users as part of their recommendation strategies. Al-though Rabbit is not a recommendation system for learn-ing, its design goal is to enhance reading selections for K-12 users. (An in-depth description of existing recommenders in the educational domain can be found in [9].)
Rabbit emulates the readers X  advisory (RA) service avail-able at public libraries since the late 1800 X  X  [3, 14]. RA offers (non-)fiction materials of potential interest with  X  X he help of knowledgeable and non-judgmental library staff X  X 14]. While traditional RA involves face-to-face discussions between pa-trons and librarians, a number of public libraries, such as Williamsburg Regional, 4 take advantage of existing tech-nologies and replace human interactions with online forms filled out by patrons to capture their interests [5].
Either through face-to-face conversations or filled-out on-line forms, a RA librarian X  X  task is to identify the type of books preferred by readers based on the reasons behind their preferences. Besides analyzing the topical areas and content descriptions of books favored by a reader R , during the RA process, librarians examine the appeal factors of books that R is interested in [14]. Appeal factors, such as the pacing or description of characters in books, are  X  X he elements of a book X  X hether definable or just understood X  X hat make readers enjoy the book X  [14]. These factors capture general traits of a book that attract the attention of a reader and are considered in answering one of the most important RA ques-tions,  X  X hy is the reader interested in a given book? X  X or ex-ample, some readers might enjoy the Harry Potter books (by J.K. Rowling) because of the established friendships among students and the boarding school setting, whereas others like the fantasy aspect of the story.

To the best of our knowledge, there is no consensus on the set of appeal factors that must be considered during the RA process. The most prominent appeal factors, as articulated in RA-related literature [3, 5, 14] include: (i) characteri-zation, (ii) frame, (iii) pacing, (iv) storyline, (v) language and writing style, (vi) tone, and (vii) special topics. The first six appeal factors are well-known literary elements of wrl.org/books-and-reading/adults/looking-good-book Table 1: Sample appeal terms associated with each of the appeal factors considered by Rabbit Appeal Factors Appeal Terms Characterization Believable, distant, dramatic Frame Bittersweet, contemporary, descriptive Language and Candid, complex, conversational, Writing Style extravagant, poetic, prosaic Pacing Easy, fast, slow Special Topics Addiction, bullying, violence Storyline Action-oriented, character-centered
Tone Dark, happy, surreal (non-)fiction books [2], whereas the latter identifies subjects addressed in a book that can cause emotional stress to some readers but tolerated/enjoyed by others [3]. Each appeal factor is associated with a vocabulary, which is a set of key-words, called appeal terms , employed to describe the factor, which we have defined based on well-known RA literature [3, 14]. The appeal factors considered by Rabbit and a sample of their respective appeal terms are shown in Table 1.
Based on the contents, topics, and appeal terms that de-scribe the appeal factors of books preferred by a reader R , librarians suggest other books matching (to a certain degree) the interests/preferences of R . However, due to the amount of books being published on a regular basis these days, it is an impossible task for a librarian to be familiar with every existing book to determine if it could be a potential rele-vant recommendation for R . For this reason, librarians turn to RA databases, which are available at NoveList, Fiction Connection, Which Book, and Readers X  Advisory Online, to conduct fact-based, appeal factor-oriented, and read-alike searches in locating books to suggest to a reader [3].
While Rabbit conducts topical and content analysis of books using data from book-related websites, appeal-term descriptions are only available through RA databases or determined by professionals. Unfortunately, accessing rep-utable RA databases, such as NoveList Plus, comes with a price tag, i.e., paid subscription, whereas professionals might not have read a particular book and thus it would not be possible for them to infer the corresponding appeal-term description on-the-fly. To address these constraints, we have developed ABET, a tool that automatically ex-tracts appeal-term descriptions of books from reviews avail-able at well-known book-related websites, such as Amazon, Bertrams, Bookfinde r4u, Bookmooch, Dogobooks and Fish-pond.As reading is a personal experience, it is anticipated that a book is (not) appealing to a reader for various reasons. By analyzing reviews, we extract diverse readers X  opinions on a book based on appeal terms that describe the corre-sponding appeal factors of the book, which in turn facilitates the task of identifying why books are appealing to a reader based on their literary elements.
 To generate the appeal-term description for a given book, ABET relies on the taxonomy defined in Section 3, which despite being comprehensive, cannot account for a given ap-peal factor/term being specified differently in readers X  re-views. For example, a reviewer may refer to the  X  X toryline X  of a book as  X  X tory X  or  X  X arrative X , and (s)he may also use either  X  X uick X  or  X  X ast X  to describe its  X  X ace X . For this rea-son, we extend the appeal factors/terms by including the (stemmed) synonyms of each term/factor, which are identi-fied using WordNet. 5 (The complete list of appeal terms for each appeal factor can be found at goo.gl/BSwuPw.)
While the taxonomy can serve as an aid to identify po-tential appeal factors/terms in reviews, it is imperative to properly associate these appeal terms and appeal factors in the reviews so that appeal factor-appeal term pairs can be correctly extracted to generate an accurate appeal-term description for a given book. To accomplish this task, we have defined a number of extraction rules 6 (as given in Ta-ble 2) for ABET based on typed dependency relations be-tween word pairs in sentences extracted from reviews. It is natural for ABET to turn to typed dependencies, since they capture the semantic connection , i.e., association, be-tween words in sentences. (A detailed discussion on typed dependencies is available at http://goo.gl/31Puwm.)
RulesusedbyABETarebasedonwrittenpatternsiden-tified in book reviews and capture the semantic link between appeal factors and their corresponding terms. Consider sen-tence S A ,  X  X he narrative of the book is dramatic X , and sen-tence S B ,  X  X e creates believable characters X . In S A the sub-ject of the sentence, i.e.,  X  X arrative, X  is characterized as be-ing  X  X ramatic X , whereas in S B its object , i.e.,  X  X haracters X , is described as  X  X elievable X . As illustrated by the aforemen-tioned examples, if the subject/object of a sentence is an appeal factor, then a word in the sentence that is semanti-cally directly linked to the mentioned object/subject is often an appeal term. ABET captures these connection patterns using Rules 1 and 2 as defined in Table 2.

In a sentence, an appeal terms can also be indirectly con-nected with an appeal factor. Consider sentence S C , X  X he descriptions included are extravagant X .  X  X xtravagant X  is in-directly related to the subject of S C , i.e.,  X  X escriptions X , through the word  X  X ncluded X . Using Rule 3, ABET exam-ines pairs of grammatical relations that involve indirect con-nections among words. Now consider sentence S D , X  X he characters are not simple X . Based on Rule 1, ABET would mistakenly connect  X  X haracterization X  with  X  X imple X . This example reveals the need to examine pairs of grammatical relations in the presence of negated terms. ABET applies Rule 4, which identifies a negated term as a modifier of an appeal term t and then extracts as the appeal term for the corresponding factor the antonym of t (if it is included in the vocabulary defined in ABET X  X  taxonomy for the factor).
Rules 3 and 4 take precedence over Rules 1 and 2, since once a typed dependency in a sentence is used by either of the former rules, it cannot be considered by the latter ones.
While majority of other relations (beyond the ones cap-tured by Rules 1 to 4) seldom appeared in reviews, we ob-served three special cases that facilitate the extraction of appeal terms for  X  X pecial Topics X  and  X  X rame X , respectively, which we defined in Rules 5 to 7. Consider S E ,  X  X t is about violence at schools X , S F ,  X  X ullying is depicted in the book X , and S G ,  X  X he action is set in a school X , which include special written patterns pertaining to the X  X rame X  X nd X  X pecial Top-ics X  appeal factors that are based on prepositions, subjects, and objects identified in sentences in reviews. The prepo-sition  X  X bout X  in S E captures an appeal term employed to describe the factor  X  X pecial Topics X , i.e.,  X  X iolence X , whereas wordnet.princeton.edu
ABET performs linguistic/semantic analysis on sentences in reviews using Stanford Parser (http://goo.gl/Pxj2QW). dependent or modifier ,word( B ) L list of appeal terms, respectively w f is an appeal factor in L F ,and w t is an appeal term in L w w f ( w w t , respectively) denotes that w is a synonym of w ( w t , respectively) Rule Objective Conditions Identified 1 To capture the written patterns A  X  EL T ,B  X  EL F ,rel  X  X  nn, nsubj } ,(If A is a B w f 2 follows the subject or object of A  X  EL F ,B  X  EL T ,rel  X  X  advmod, amod, A w f } B w t 3 To identify an appeal term that rel  X  X  nn, nsubj } , B  X 
EL F ,and  X  rel 2 ( C, D )  X  B w f } , A = C, D  X  EL T D w t 4 To explicitly consider negated B  X  EL F ,rel  X  X  nn, nsubj EL
T ,D is a negation term  X  A w t 5 To account for the multiple A  X  EL T ,rel  X  X  prep about 7 special cases of  X  X pecial Topic X  A  X  X rame X   X  EL ,B (a synonym of an appeal term w f = X  X rame X   X  EL T ,rel  X  X  prep in } B w t Figure 1: ABET-generated appeal-term description for  X  X he Hunger Games X   X  X n X  in S G is connected with an appeal term, i.e.,  X  X chool X , that describes the factor  X  X rame X . Moreover,  X  X ullying X  in S , which is assigned a X  X B X  X art-of-speech tag, is an appeal term describing the factor  X  X pecial Topics X .

ABET creates the appeal-term description for a book Bk by applying rules defined in Table 2 on (up to) 500 dis-tinct reviews of Bk , if they are available. In generating the appeal-term description of Bk , ABET considers not only the appeal terms extracted from reviews on Bk , but also their frequency of occurrence . The latter captures the rela-tive degree of significance of an appeal term in describing its corresponding factor based on reviewers X  varied opinions on appeal factors that apply to Bk . (A sample of the appeal term description generated using ABET for the book  X  X he Hunger Games X  by Suzanne Collins is shown in Figure 1.)
In making suggestions, Rabbit first analyzes the profile of a reader R , which consists of a set of N (  X  1) books either given or bookmarked 7 by R on a social bookmark-ing site. Based on the profile, Rabbit identifies books that are compatible with the readability level of R ,whichare treated as candidate books to be considered for recommen-dation. These books are selected among those available at a book repository, including (i) reputable websites ,such as OpenLibrary.org or WorldCat.org, which are two of the largest online library catalogs, (ii) school/public libraries , and (iii) book-related bookmarking sites , such as BiblioNa-sium.com, which is a website that encourages reading among children/teenagers. Rabbit computes a ranking score, which quantifies the degree of relevance , of each candidate book with respect to (books in) the profile of R usingaregression model applied to the analytical results of the book using diverse publicly available information.
It is imperative for Rabbit to locate books with grade levels adequate for each individual reader, since  X  X eading for understanding cannot take place unless the words in the text are accurately and efficiently decoded X  [10]. To accomplish this task, Rabbit first determines the readability level of a reader R by analyzing the grade levels of books in R  X  X  profile. The readability level of R is determined by averaging the grade level of each book P B in R  X  X  profile, computed using TRoLL [12], a t ool for r egression analysis o fl iteracy l evels,
Only books bookmarked by a user during the most recent academic year are examined to account for the fact that the users enhance their reading comprehension skills over time. which captures the central tendency of the grade levels of books that have been read by R . Unlike popular prediction formulas/tools (such as Flesch-Kincaid, Lexile, and ATOS [1]), which rely on text of a book to compute its grade level (a severe constraint, since text is not always freely accessible due to copyright laws), TRoLL computes the grade level of any book using book metadata publicly accessible from reputable online sources, even in the absence of sample text.
Rabbit applies Equation 1 to determine the set of candi-date books considered for recommendation.
 SCB ( R )= { CB | CB  X  Rep  X  TRoLL ( CB )  X  [ P B  X  P where CB is a candidate book available at a book reposi-tory Rep , | P | denotes the number of books in R  X  X  profile, and TRoLL ( CB )( TRoLL ( P B ), respectively) is the grade level of CB ( P B , respectively) determined by TRoLL. By selecting books within half a grade level of the mean read-ability level of R , Rabbit considers books for recommenda-tion within an appropriate level of (text) complexity for R based on the grade levels of books in R  X  X  profile.
Rabbit suggests books that not only readers can compre-hend, but also they are interested in by analyzing diverse publicly accessible metadata as described below
Rabbit examines the topical description (i.e., topic) of a book defined by Library of Congress Subject Headings (LCSH) assigned to the book by professional cataloguers. LCSH, a de facto controlled vocabulary, constitute the largest general indexing vocabulary in the English language. Sub-ject headings, which are terms or phrases that denote con-cepts, events, or names, are used by librarians to index books according to their themes. Examples of LCSH include  X  X u-venile Fiction, X  and  X  X rchaeology X  X istory X 18th century X . Rabbit, which explores the topical resemblance between CB and books in R  X  X  profile P , examines the degree to which the distribution of topics in CB matches the distribution of topics of books in P .This topical similarity measure, which is defined using the vector space model (VSM) and computed in Equation 2, prioritizes candidate books that have been assigned LCSH which match the LCSH favored by R , i.e., most frequent LCSH assigned to books in P . where CB and P are represented as n -dimensional vectors CB = &lt;W CB 1 ,...,W CB n &gt; and P = &lt;W P 1 ,...,W respectively, n is the number of distinct subject headings assigned to CB and books in P , W CB i ,theweightof CB i (1  X  i  X  n ) is  X 1 X  if CB i is a subject heading of CB ,and is  X 0 X  otherwise, and W P i ,whichistheweightof P i (1  X  i  X  n ), is computed as a proportion between the number of books in P that have been assigned P i and the total number of books in P , i.e., | P | .

In computing the topical similarity measure, in addition to other similarity measures presented below, we rely on VSM, since VSM handles frequency distributions, which is essential in comparing candidate books based on multiple perspectives with the profile of a reader. We have empiri-cally verified that given the short length of the descriptions for each book based on its topics, content, and appeal factors (which include very few LCSHs, words, and terms, respec-tively), VSM is a more reliable distance measure compared with its probabilistic counterparts, such as KL-divergence.
Besides determining the topics that are of interest to a reader R , Rabbit also identifies written matters covered in books that are generally appealing to R by analyzing the content description of each book in P (and each candidate book CB ), which is extracted from reputable book-related websites, such as Amazon and the Library of Congress.
Rabbit computes the content similarity of each CB with respect to P based on the  X  X ag-of-words X  representation of the brief descriptions of CB and (books in) P . Rabbit favors candidate books with contents compatible with the contents that are most commonly addressed in books be-longed to R  X  X  profile. To compute CSim ( CB , P ), the con-tent similarity score defined in Equation 3, Rabbit employs an enhanced version of the cosine similarity measure based on word-correlation factors (WCF) [6], which relaxes the ex-act matching constraint imposed by the cosine measure by exploring words in the content description of CB that are analogous to, besides the same as, words in the content de-scription of books in P . Rabbit relies on WCF, as opposed to other popular similarity metrics applied to WordNet, since we have empirically verified that word-similarity scores pre-dicted by using WCF correlate with human assessments on word similarity. Using WordSim353, 8 which is a test collec-tion for measuring word relatedness, and STS, 9 which pro-vides human assessments on sentence similarity for 750 pairs of sentences, we compared the performance of WCF with FaITH 10 [13], which is a feature and information theoretic-based similarity measure. (Only FaITH was considered, since as reported in [13], it outperforms other well-known information-theoretic, ontology, and hybrid-based approaches that exploit word-related information available at WordNet to estimate the degree of similarity between pairs of words.) The results of the experiments verified that the performance of WCF is comparable to that of FaITH.
 where the content of CB and P are represented as vectors CB = &lt;W CB 1 , ... , W CB n &gt; and P = &lt;W P 1 , ... , W respectively, n is the number of distinct non-stop, stemmed keywords in the brief descriptions of CB and each book in P ,and W P i and W CB i are the weights of keywords P i and CB i , respectively, which are calculated using Equations 4 and 5 such that the frequency distributions of words in CB and P are determined based on the frequency distributions of non-stop, stemmed words among the brief descriptions of CB and all the books in P , respectively.
 cs.technion.ac.il/  X  gabr/resources/data/wordsim353/ goo.gl/KzMHgn
Available at grid.deid.unical.it/similarity where | HS w | is the size of HS w , f w,D is the frequency of occurrence of w in D ,and HS w is the set of words that are highly similar to, but not the same as, a given word w in the brief description D of either CB or P .(Awordishighly similar if it is included in a reduced WCF matrix [12].)
Rabbit also examines the appeal elements of books pre-ferred by R and captures the overall degree of resemblance between the appeal-term description of CB and (each book in) the profile P of R , which are generated using ABET. In calculating ATSim ( CB,P ), the appeal-term similarity score of CB with respect to P , Rabbit adopts the cosine measure asdefinedinEquation6.
 where F is the set of appeal factors in the appeal-term de-scriptions for CB and P , | F | is the size of F , CB f and P the n -dimensional vector representations of the appeal-term distribution of an appeal factor f for CB and P , respectively, n is the number of distinct appeal terms in the distributions of the corresponding appeal factor for CB and P , W CB f i of the i th term in P f .
To predict the ranking score of CB , Rabbit employs mul-tiple linear regression, which is a statistical technique for building estimation models that accounts for the influence of multiple contributing factors, i.e., the topical, content, and appeal-term descriptions of CB in our case. The top-10 ranked books are recommended to R .
 where Y is the dependent variable, i.e., ranking score of CB ,  X  0 is the intercept parameter,  X  1 ,..., X  n are the coefficients of regression, X 1 ,...X n are the independent variables (pre-dictors), i.e., the scores defined in Section 5.2 for CB ,and n is the number of predictors in the regression analysis.
In Equation 7, the intercept and coefficients of regres-sion are estimated through a one-time training process using the Ordinary Least Squares method and the Tset dataset (introduced in Section 6.2.1). Tset consists of 1,663 in-stances, each of which is a book B that is either a rel-evant or non-relevant recommendation for a given reader R . Each instance is represented as a vector of the form &lt;B 1 ,B 2 ,B 3 ,rel R &gt; ,where B i is the (value of the) i (1  X  i  X  3) predictor computed for B ,and rel R is the target, which is  X 1 X  if B is a relevant recommendation for R , and is  X 0 X  otherwise.
In this section, we present the results of the empirical studies conducted to assess the performance of ABET and Rabbit. To perform these studies, we relied on a number of sample sets of books, i.e., SB 1 and SB 2 . (Due to space con-straints, we posted the sample sets under goo.gl/PWE9u2)
Due to the lack of existing benchmark datasets for vali-dating the performance of tools that automatically extract appeal factor-appeal term pairs, we have assessed the per-formance of ABET by (i) computing the precision and recall of appeal factor-appeal term pairs extracted from book re-views, (ii) analyzing the correctness of appeal-term descrip-tions created by ABET, and (iii) comparing appeal-term descriptions generated by ABET with respect to the ones extracted from NoveList on the same set of books.
We randomly selected a set of 100 books written for K-12 readers, and for each book we randomly examined a re-view. We manually annotated the appeal factor-appeal term pairs in each of the 100 examined reviews and compared the annotated pairs in each review with the ones extracted by ABET. The precision, recall, and F-measure achieved by ABET, which are 0.85, 0.82, and 0.83, respectively, verify the high accuracy of the rules defined for ABET in identi-fying appeal factors and their corresponding appeal terms in reviews. We have observed that majority of the pairs excluded by ABET were due to keywords used by review-ers to describe a given factor which are not included in the pre-constructed vocabulary of the corresponding factor de-fined for ABET (as described in Section 4). We have also observed that poor phrasing in reviews, which in turn yields nonsensical grammatical relations between pairs of words, and improper anaphora resolution in ABET have caused the majority of the extraction errors.

To further evaluate the appeal-term descriptions created by ABET, we relied on SB 1 , a sample set of eight books ,and conducted two surveys on Amazon Mechanical Turk.

In both surveys, we asked appraisers to select the key-words, i.e., appeal terms, that best describe each appeal factor for one of the books in SB 1 . While the first survey in-cludes the appeal terms considered by ABET for each appeal factor as the corresponding possible keyword choices, the second survey contains appeal terms defined by either ABET or NoveList. In the two surveys, we treated the appeal terms selected for each factor by appraisers as the  X  X old standard X  for the factor and computed the accuracy of ABET (Nov-eList, respectively) based on the proportion of terms in the gold standard of a given factor defined by appraisers which match its counterpart identified by ABET (NoveList, respec-tively) for the factor. We relied on Mechanical Turk, since it is a  X  X arketplace for work that requires human intelligence X  that allows individuals/businesses to programmatically ac-cess thousands of diverse workers and has been used in the past to collect user feedback for information retrieval tasks [7]. Furthermore, we considered NoveList for the compari-son purpose, since NoveList is a premier database for RA [3] and, to the best of our knowledge, is the only RA database that includes appeal-term descriptions for books.
As shown in Figure 2, based on the 200 responses collected in September 2013, ABET achieves an overall 94% accu-racy in identifying appeal terms (in reviews) that describe a book. More importantly, the accuracy on the identified appeal terms for each appeal factor considered by ABET is in the upper eighty percentile or higher. Figure 3, on the other hand, shows the accuracy ratios of ABET and Nov-Figure 2: Performance evaluation of ABET con-ducted using Amazon Mechanical Turk Figure 3: Performance evaluation of ABET and NoveList using Amazon Mechanical Turk eList calculated using the 200 responses provided by Me-chanical Turk appraisers for the second survey. NoveList describes books using only four appeal factors, as opposed to the seven considered by ABET. For this reason, we have compared the performance of ABET and NoveList based on their common appeal factors (as shown in Figure 3). Based on the appraisers X  assessments, we claim that appeal-term descriptions provided by ABET are favoured over the ones defined by professionals who maintain the RA database at NoveList. Note that the improvement in overall accuracy ratio achieved by ABET over NoveList, in addition to the improvement on X  X anguage and Writing Style X , X  X acing X  X nd  X  X toryline X  factors, are statistically significant (p &lt; 0.01).
In this section, we discuss the empirical studies conducted to assess the design and performance of Rabbit.
Even though BookCrossing 11 has been employed to eval-uate book recommenders tailored to a general audience, it is not specifically designed for assessing the performance of recommenders for K-12 readers. We used data provided by BiblioNasium, which is a secure social networking site on books that targets children and teenagers, to evaluate Rab-bit instead. The dataset consists of books that have been bookmarked by each one of the 5,580 BiblioNasium users who joined the site within the first four months of its es-tablishment. A portion of the dataset, called Tset ,which consists of 10% of the 5,580 users and their profiles, was em-
Informatik.unifreiburg.de/  X  cziegler/BX ployed for training Rabbit X  X  regression model, whereas the remaining users and their profiles, called Eset ,wereused for evaluation purposes. The design methodology of Rabbit relies on topical , brief content ,and appeal-term descriptions, in addition to the predicted grade levels of books. Thus, we retrieved the brief book descriptions and LCSH from rep-utable book-related websites, the appeal-term descriptions from book reviews using ABET, and the book readability levels using TRoLL.

We adopt the popular 5-fold cross validation strategy to evaluate recommenders. Since only withheld books are con-sidered relevant, it is not possible to account for potentially relevant books a user has not bookmarked, a well-known lim-itation of this evaluation protocol. As the limitation applies to all the recommenders evaluated in the empirical studies, the results are consistent for the comparison purpose.
We assessed the performance of Rabbit using Eset in terms of Normalized Discounted Cumulative Gain (nDCG), which determines the overall (ranking) performance of a recommender and penalizes relevant recommendation posi-tioned lower in the recommendation list. We observed that ranking candidate books solely based on appeal factors or topical information yields the lowest nDCG scores, i.e., 0.19 and 0.18, respectively This is anticipated, since LCSH and appeal-term descriptions mainly identify the types of books preferred by a reader R from a general perspective. Even though the content-based approach yields a relatively high nDCG score (i.e., 0.24), the experimental results show that the multi-dimensional strategy of Rabbit, of which content-based analysis is a component, locates more relevant books, which is justified by the statistically significant improvement in nDCG (i.e., 0.32) generated by the latter ( p&lt; 0.001).
Linearly combining different similarity scores considered by Rabbit yields a lower nDCG value (i.e., 0.27) than the one obtained by using the regression model, which is statis-tically significant ( p&lt; 0.001). The results validate the ne-cessity of accounting for the impact, i.e., weight, of each in-dividual similarity score. Furthermore, we have verified that bypassing the candidate selection step would have a nega-tive impact on the overall performance of Rabbit, since rec-ommending books that only match the interests or general traits of books preferred by a reader R , without considering R  X  X  readability level, increases the number of non-relevant suggestions (as demonstrated by the 0.19 nDCG achieved by Rabbit if no candidate selection step is applied.)
We also compared the performance of Rabbit with BReK12 (as introduced in Section 2). We perform the comparison, since to the best of our knowledge BReK12 is the only avail-able recommender that explicitly considers the readability level of its users in making personalized book recommen-dations. Furthermore, other state-of-the-art approaches for (book) recommendations are excluded for the comparison purpose using Eset , since they require either personal rat-ings on books provided by individual users or social connec-tions established by social bookmarking site users, neither are available on social websites for K-12 readers nor in the Eset dataset. Rabbit achieves a statistically significant im-provement ( p&lt; 0.001) over BReK12 in terms of nDCG, which are 0.32 and 0.18, respectively.
To further validate the performance of Rabbit, we con-ducted a survey using Mechanical Turk on 10 sample books in another test set SB 2 , which evaluated the degree to which books recommended by Rabbit are preferred over those sug-gested by recommendation modules at well-known book-related websites. We have selected recommenders that adopt diverse strategies in making suggestions: (i) Amazon, which considers purchasing patterns of its users [8], GoodReads, which combines multiple proprietary algorithms to  X  X nalyze 20 billion data points, X  and (iii) NoveList, 13 which examines book-related information, including title, publication date, and appeal factors for recommending books.

Each survey included the top-2 (for which some of them can be identical) recommendations made by the aforemen-tioned recommenders for a given sample book Bk .Ap-praisers were asked to select, to the best of their knowl-edge, the top-two books most closely related to Bk ,which were treated as the gold standard for Bk . Based on the 500 responses collected during November 2013, we com-puted the accuracy of the top-2 recommendations made by Amazon, GoodReads, NoveList, and Rabbit, which are 0.50, 0.18, 0.24, and 0.44, respectively. Recommendations made by Rabbit and Amazon are preferred over the suggestions made by GoodReads and NoveList. Furthermore, the im-provement, in terms of accuracy ratios, achieved by Rab-bit over GoodReads and NoveList is statistically significant ( p&lt; 0.001). In terms of the overall accuracy, Amazon out-performs Rabbit. However, their differences in nDCG are not statistically significant ( p&lt; 0 . 001).

Rabbit can make suggestions regardless of the number of books of interest to R . It differs from the recommendation modules employed at Amazon and NoveList, since the latter can only examine books of interest to a user one at the time. Rabbit is also different from GoodReads, since GoodReads processes either a given book or the entire profile of a user. Furthermore, Rabbit can treat a book as a candidate sug-gestion immediately after the book is published, whereas Amazon requires the existence of a number of purchasing transactions involving the new book in order to suggest it to a user. In addition, in making recommendations for children and teenagers, Rabbit considers books provided directly by K-12 readers to generate personalized suggestions. Recom-mendations generated by Amazon that target children and teenagers, on the other hand, are the result of extensive analysis of the purchasing patterns of adults.
We have introduced Rabbit, a recommender which makes personalized suggestions on books that match the interests and reading abilities of its K-12 users. Rabbit emulates the readers X  advisory process offered at public/school libraries to recommend books that are similar in contents , topics ,and literary elements of other books appealing to a reader, with the latter based on extracted appeal-term descriptions.
Rabbit can be a stand-alone tool used by readers (ed-ucators/parents, respectively) or adopted by (K-12) social bookmarking sites for providing suitable reading selections. Even though we have developed Rabbit with K-12 readers in mind, the readers X  advisory-based methodology of Rabbit goo.gl/99me5f support.epnet.com/knowledge base/detail.php? id=4772 can also be used to suggest books for adults (with reading levels below/above the 12 th grade level) as well.
The results of the conducted experiments have (i) vali-dated the correctness of the design methodology of Rabbit and (ii) demonstrated the superiority of Rabbit over other recommenders that either explicitly consider or ignore the reading ability of its users by using data from BiblioNasium and Mechanical Turk appraisers. [1] R. Benjamin. Reconstructing Readability: Recent [2] C. Coulter and M. Smith. The Construction Zone: [3] A. Cox and K. Horne. Fast-Paced, Romantic, Set in [4] S. Givon and V. Lavrenko. Predicting Social-Tags for [5] N. Hollands. Improving the Model for Interactive [6] J. Koberstein and Y.-K. Ng. Using Word Clusters to [7] M. Koolen, J. Kamps, and G. Kazai. Social Book [8] G. Linden, B. Smith, and J. York. Amazon.com [9] N.Manouselis,H.Drachsler,K.Verbert,and [10] J. Oakhill and K. Cain. The Precursors of Reading [11] B. Pang and L. Lee. Opinion Mining and Sentiment [12] M. Pera. Using Online Data Sources to Make [13] G. Pirro and J. Euzenat. A Feature and Information [14] J. Saricks. Readers X  Advisory Service in the Public [15] R. Shaban. A Guide to Writing Book Reviews.
 [16] A. Sieg, B. Mobasher, and R. Burke. Improving the [17] C. Yang, B. Wei, J. Wu, Y. Zhang, and L. Zhang.
