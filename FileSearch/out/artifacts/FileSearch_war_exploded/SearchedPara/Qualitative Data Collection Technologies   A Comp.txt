 With the growing body of qualitative research on HCI and social computing, it is natural that researchers may choose to conduct that research in a mediated fashion X  X ver telephone or computer networks. In this paper we compare three different qualitative data collection technologies: phone, instant message (IM), and email. We use quantitative analysis techniques to examine the differences between the methods specifically concerning word count and qualitative codes. We find that there are differences between the methods, and that each technology has affordances that impact the data. Although phone interviews contain four times as many words on average as email and IM, we were surprised to discover that there is no significant difference in number of unique qualitative codes expressed between phone and IM. H5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous. Qualitative research, Methods, Data collection, Internet studies. Qualitative interviewing is an invaluable tool for understanding human behavior. If quantitative metrics can give us immediate access to what people do and with whom , qualitative methods can help us understand why . As researchers increasingly turn attention to people X  X  behavior online, an intriguing methodological question arises: Should we use online media to conduct those interviews? Qualitative interviewing is a technique that has its roots in several disciplines, especially anthropology [2], psychology [12], and sociology [9]. Traditionally, interviews are conducted face-to-face. However, when we study online communities and social computing environments, our participants may be distribut ed around the globe where face-to-face interviews are ofte n impractical. Fortunately, multiple communication media provide convenient alternatives, including telephone (synchronous), instant message (semi-synchronous), and email (asynchronous). In a social computing class at the Georgia Institute of Technology, the course project is to research an online community, including conducting interviews with members. To emphasize that interviews over the phone result in more data than if conducted with instant message (IM), the instructor Amy Bruckm an took a transcribed copy of a sample phone interview and an online interview (conducted by the same student, about the same online site), and laid them on the floor si de by side. The differences were striking. The transcript obtained via instant messaging crossed the desk, but the phone transcript crossed the room. This theatrical demonstration makes the point well, but on further reflection led us to wonder: Does a longer transcript actually make the phone the best interview method? More specifically, which of these methods is more practical, and how does the choice of interaction medium affect the quality of data? In order to answer these questions, we first reviewed prior work that examines the effect of the technological medium on the collection of qualitative data. We then conducted forty-eight interviews divided across three interview media (phone, instant message, and email) and compared the quality and quantity of data obtained. As other researchers have pointe d out, interviews are in fact a cornerstone of human-computer interaction research, serving both as one of the more valuable and more challenging methods [16]. The issue of the impact of medium on collected data is not a new one, and has been taken up previously in the context of face-to-face versus phone interviews [1] in the area of psychology, as well as face-to-face versus phone versus postal mail [14] in the field of public health. In this methods literature, there are a few reasons that researchers cite for using online interviews (IM or email) rather than face -to-face interviews. Some of these are logistical: (1) difficu lty of in-person meetings due to distance, time, and/or cost [10,15]; (2) difficulty of transcription due to time constraints [16]; and (3) the convenience of automatic transcription [3,10]. Other reasons relate to the context of the study itself. Voida cites the possibility of metadata when a study is about a technology that could be used also as an interview medium, such as IM [16], and Crichton points out that online techniques may  X  X onor the field X  in which participants are engaged X  X he online environment [3]. Interestingly, while many researchers cite the advantages of IM interviews in terms of the infeasibility or cost prohibitive nature of travel as compared to face-to-face in terviews, they often do not mention phone interviews as an alternative [3,10,15,16]. As noted above, it is possible that face-to-face is considered the  X  X efault X  interview method for most researchers even with respect to studying behavior online, and therefore this would be the baseline to co mpare alternative media. However, it is also true that phone interviews mitigate the same distance-related issues. Th ere has also been research taking up reflective comparisons between all four of these interview techniques X  X ace-to-face versus phone versus email versus IM. Kazmer and Xie X  X  methods paper presents the relative advantages and drawbacks of each technique within the specific context of Internet-based research [7]. Their discussion is based on observations collected during their own research on other topics, and focuses primarily on functional effects (such as scheduling, logistics, and data management) and methodological effects (such as probing and affective data). They provide examples of where one medium may be superior (such as email interviews being easier to schedule, or phone interviews having more conversational flow), and conclude that qualitative interviews can be successful in any medium, particularly when researchers give attention to practical issues involved. These comparison studies that have touched on logistical differences do not always address the issue of quality of data in depth. In a review of other studies that use email interviewing in particular as a technique, Meho suggests that the quality of data gained through online research is much the same as traditional methods, evidenced by similar results from research that conducted both types of interviews [11]. In one comparison of email versus face-to-face techniques, Curasi designed a study to compare the methods by collecting interviews using both and comparing the resulting data [4]. She compared the datasets for response rate, response speed, and depth of information (though it is unclear from discussed methods how depth was judged). With respect to quality of data, Curasi concluded that quality may be dependent more on the identity of the interviewer/interviewee than the data collection method. Motivated by our own experiences and related work we ask: In what ways does the technology medium (phone, IM, or email) affect the data collected in qualitative interviews? Because you cannot conduct an interview without talking about something, we took this as an opportunity to extend our work studying video game play practices. The following are our two hypotheses: H1 . Word Count : The phone method will have the highest word count over IM and email. H2 : Qualitative Codes : The phone method will have the highest number of unique qualitative codes, over IM and email. Qualitative analysis methods like grounded theory, thematic analysis, and discourse analysis generate codes and themes from the data. To understand the quality of data produced, we chose to compare the number of qualitative codes generated, as well as the number of words generated. In order to investigate different interviewing methods, we piggy-backed onto an actual study examining video game play practices. We used an ongoing research project by Betsy DiSalvo that explores cultural aspects of these practices. Game play practices are the strategies and ways in which people approach playing digital games. We recruited participants by emailing game email lists such as women in gaming, college game email lists, Latinos in gaming, as well as our own gaming social networks. We then used snowball sampling, where we asked participants if they knew of anyone who would be interested in participating. This constitutes a convenience sample. Participants did not receive any compensation for their participation. We used three different kinds of data collection technologies: phone, IM, and email. We developed a structured interview guide that could be used both in an email format and as questions read or typed. We asked the following questions: Participants were randomly assigned to phone, IM, or email as part of the consent form. Participants did not know in advance that we were res earching different interview mediums, to minimize any selection bias. In order to minimize potential bias introduced by the personal style of a particular interviewer, we had four researchers each conduct 12 interviews (four phon e, four IM, and four email) for a total of 48 interviews. For the email interview, the interview text was sent, and then participants typed in their responses and sent it back. For the IM interviews, we used whatever chat client the partic ipant chose, including GChat, Skype, and AIM. We tried to stick to the interview guide; however, as is appropriate for qualitative interviewing [13], we did ask some probing questions if we felt that the response did not completely answer the question. We used this approach with the phone and IM interviews, which were conducted through Skype and recorded. The median length of the phone and IM interviews was 25 minutes. We are interested in the number of unique ideas or codes that each method generates. By unique ideas, we mean relevant codes, or the generation of labels that associate a concept with the data, as used in qualitative methods such as grounded theory. Grounded theory is a common data analysis method for qualitative data across HCI and social computing methodologies [5]. Specifically, we approached coding the data with an inductive, open coding approach or  X  X laserian X  approach [5,6]. We hypothesized that the phone method would produce the highest number of unique codes that were relevant to the topic of inquiry. In order to evaluate this, we first transcribed the phone interviews. We then had two researchers code the interviews, where the unit of analysis was one sentence. Concepts that represented the codes did span sentences and reoccurred throughout the interview. As a result, coders used the sentence to mark the first time a unique idea was introduced. We only coded for concepts that were relevant to the topic of inquiry. Table 1 illustrates examples of how we coded the interviews. Each sentence was entered into a spreadsheet line, where we then identified how many new codes appeared in the co rresponding column. Codes represent concepts such as th e type of game they like to play, the reason why they like to play, what cheating is, the first mention of a gaming genre, and various points in their life when they have had different gaming habits. First, two researchers did a test run through one interview together. Then, the researchers independently coded a subsequent interview where they achieved a 79% inter-rater reliability using Cohen X  X  Kappa [8]. They talked over the differences, set some additional rules, and coded another interview independently and achieved 87% reliability. They then discarded the two test interviews and divided the rest of the interviews and coded them independently. First, we tested for equality of variances to determine what statistical test we can use. Using Levene X  X  test, we found p=0.529, and thus failed to reject the null hypothesis of equal variances which allows us to run an ANOVA. We ran an omnibus ANOVA which showed that there was a significant difference (we use significance at p&lt;0.05) between the groups, F(2,45) = 7.513, p=0.0016. Post hoc comparisons using the Tukey HS D test indicated that the mean score for the email condition (M=18.73, SD=8.84) was significantly different from phone (M=33.2, SD=12.07), at p=0.001. Email was not significantly different from IM (M=24.0, SD=9.72) at (p=0.06) and IM was not significantly different from phone (p=0.267). A post hoc power analysis using GPower indicates that we have an effect size of 0.51 that gives us a power of 0.92. This means that email produces a lower number of unique codes compared to phone. However, we cannot make any claims with respect IM. Again, when we first ran Levene X  X  test to compare the variances among groups, we found a significant difference in variances (p=0.0038). Th is means we cannot assume equal variances and must use a non-parametric test, Kruskal-Wallis. Using this test, we find a significant effect of the method on word count F(2,45) = 19.347 (p=8.7e-6). Post hoc comparisons using th e Tukey HSD test indicated that the mean score for the Phone condition (M = 2339.875, SD=2396) was significantly different from both IM (M = 660.250, SD=233.47) and email (M = 473.563, SD=523.71). There was no significant difference between IM and email. A post-hoc power analysis indicates that for word count, we have an effect size of .51 that gives us a power of .88. Phone had a significantly higher word count than IM or email, but there was no difference between IM and email. Word Count 2339.88 660.25 473.56 
Qual Codes 33.2 24.0 18.73 In summary, We accept H1 . Word Count : The phone method will have the highest word count over IM and email. We reject H2 : Qualitative Coding : The phone method will have the most unique number of qualitative coded concepts, over IM and email. The dramatic demonstration of spreading a phone interview transcript across the classroo m was intended to prove a point: Phone is better. However, through our more systematic investigation of the issue, we find that the reality is more complicated. Our findings verify that the transcripts used in class were not a fluke X  X hone interviews are significantly longer than IM or email. However, our coding of transcripts shows that phone interviews do not contain substantially more unique ideas than IM. How could this be possible? Looking at our data, it X  X  clear that the phone transcripts contain more repetition. Speaking out loud, people use more words to say the same thing. Although our data here indicate that email interviews provide less detailed and rich data than phone, email interviews also require less effort for both the researcher and subject. The issue is not which mode is better, but which is better for which kind of study. For a study where responses do not need to go into great depth and it is not necessary to follow up on users responses with questions eliciting more depth, email has substantial practical advantages. Those advantages make it possible to gather many more responses with the same amount of researcher effort. In appropriate situations, this may improve the quality of the total pool of data collected, if many short responses are desired over fe wer in-depth responses. One limitation that may affect the generalizability of this study is the context that we studied: game-playing practices. Game players may be more comfortable with digital mediums, and may be fine using different mediums such as IM. Our subjects were also relatively well educated because we recruited many of our participants from college emails lists. The participants X  level of education may affect which modality is preferable. Despite our data showing only minor differences between IM and phone interview methods , our research team have a strong subjective impression that we still prefer phone interviews X  X e feel we get better data that way. One explanation is that this impression is an illusion. An alternative explanation is that our analysis here has omitted key factors that explain the preference. We will continue to explore this issue in future work. Based on these findings, we intend to allow students in future offerings of the social computing class to conduct IM interviews if they wish. Many members of online communities are more comfortable doing interviews online, and the convenience of not needing to transcribe interview tapes is non-trivial. One hour of interview tape can take up to eight hours to transcribe [13], depending on the typist X  X  skill and complexity of the discourse. Transcribing audio files also may put the typist at risk for repetitive strain injury (RSI). If an IM interview is of comparable quality, then it can appropriately be used in more situations. 
