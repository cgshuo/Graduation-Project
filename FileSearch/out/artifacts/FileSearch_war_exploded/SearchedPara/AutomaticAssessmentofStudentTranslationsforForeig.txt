 @csail .mit.edu It is widely recognized that one of the best ways to learn a foreign language is through spok en di-alogue with nati ve speak ers (Ehsani and Knodt, 1998). Ho we ver, this is not a practical method in the classroom setting. A potential solution to this prob-lem is to rely on computer spok en dialogue systems to role play a tutor and/or a con versational partner . Ideally , a voice-interacti ve system can pro vide the learner with endless opportunities for practice and feedback. Ho we ver, while a number of dialogue sys-tems have been developed (or adapted) for language learning purposes (Senef f et al., 2004; Johnson et al., 2004), the issues of speech understanding of the accented and disfluent utterances of a foreign lan-guage student typically lead to unacceptable perfor -mance (Esk enazi, 1999).

A relati vely successful application of speech pro-cessing technology is in the area of pronunciation training (Esk enazi, 1999; Witt, 1999; Hardison, 2004). In this case, a learner repeats words or sen-tences prompted by the computer , and recei ves feed-back on the segmental and suprase gmental quality of their speech. While such systems can be useful, the y do not help the student learn how to formulate sentences within the language on their own.
In this paper , we introduce a novel speech trans-lation game which aims to pro vide a fun and reli-able environment for language learners to gain pro-ficienc y in communication. Our application is com-plementary to pronunciation training in that we pro-vide the opportunity for the students to practice ex-pressing themselv es in the new language. While we don X  t explicitly evaluate their pronunciations, we pro vide implicit feedback in that the y must speak well enough for the speech recognizer to success-fully process their utterances. The translation exer-cise can also serv e as a preparation stage in which students can practice rele vant vocab ulary and sen-tence constructs, in order to prepare themselv es for the more challenging task of interacting with a dia-logue system in the same domain.
Our prototype centers on the task of translating phrases and sentences from English into Chinese, in the flight reserv ation domain. As illustrated by the example dialogue in Figure 1, the system role plays a language tutor interacting with a Mandarin learner . The system prompts the student with randomly gen-erated English sentences to elicit spok en Chinese translations from the learner . The system para-phrases each user utterance in both languages, to im-plicitly inform the user of the system X  s internal un-derstanding, and judges whether the student has suc-ceeded in the task. The system keeps track of how man y turns a user tak es to complete all the sentences in a game session, and rewards good performance by adv ancing the student towards higher dif ficulty lev-els. A con venient  X  X elp X  button allo ws the student to request a translation of the current game sentence, to help them overcome gaps in their kno wledge of the linguistic constructs or the vocab ulary . The stu-dent can also type any English sentences within the domain to obtain a reference translation. The sys-tem utilizes an interlingua-based bidirectional trans-lation capability , described in detail in (W ang and Senef f, 2006; Senef f et al., 2006). Both Chinese and English sentences are parsed into a common mean-ing representation, which we loosely refer to as an  X  X nterlingua,  X  from which paraphrases in both lan-guages can be automatically generated using formal generation rules.

The key to a successful tutoring system lies in its ability to pro vide immediate and pertinent feed-back on the student X  s performance, similar to a hu-man tutor . A central focus of this paper is to ad-dress the challenging problem of automatically as-sessing the appropriateness of a student X  s transla-tion. At first glance, our task appears to share much in common with machine translation (MT) evalua-tion (Ho vy et al., 2002). Indeed, both are trying to assess the quality of the translation output, whether it is produced by a computer or by a foreign lan-guage student. Ne vertheless, there also exist sev-eral fundamental distinctions. Automatic MT eval-uation methods, as represented by the well-kno wn Bleu metric (Papineni et al., 2001), assume the avail-ability of human reference translations. The algo-rithms typically compare MT outputs with reference translations with the goal of producing a quality in-dicator (on a numeric scale) that correlates with hu-man judgement. In contrast, our algorithm operates in the absence of human generated reference trans-lations 1 . Furthermore, our application requires the evaluation algorithm to mak e accept/reject decisions on each individual translation, in the same way as a language tutor determines whether a translation is acceptable or not. While our task is more demand-ing, it is made possible by operating in restricted do-mains.

The remainder of the paper is organized as fol-lows. In Section 2, we present an interlingua-based approach for verifying the correctness of the stu-dent X  s spok en translation. Section 3 describes the evaluation frame work, follo wed by results and dis-cussions in Section 4. Finally , we discuss future plans for extending our work. The two most important aspects in the human eval-uation of translation quality are fluency and fi-delity (Ho vy et al., 2002). In our case, we con-sider a student X  s translation to be acceptable if it is well-formed (high fluenc y) and con veys the same meaning as the input sentence (high fidelity). We designed our interlingua-based evaluation algorithm follo wing these two principles. The algorithm uses parsability to verify fluenc y. Fidelity is examined by extracting and comparing semantic information from the translation pairs. In the follo wing, we begin by describing the basic steps involv ed in our transla-tion verification algorithm. We then discuss dif fer -ent strate gies for inte grating with the speech recog-nition system. 2.1 Parsing Our frame work depends strongly on an ability to parse both the English and Chinese sentences into a common interlingual meaning representation. Pars-ing is critical both for producing the two paraphrases of the student X  s utterance and for judging the qual-ity of their pro vided translation. Both English and Chinese grammars are needed to analyze the source and tar get sides of each translation pair . The gram-mars have been carefully constructed so that mean-ing representations deri ved from both languages are as similar as feasible.

We utilized a parser (Senef f, 1992) that is based on an enhanced probabilistic conte xt-free gram-mar (PCFG), which captures dependencies beyond conte xt-free rules by conditioning on the external left-conte xt parse cate gories when predicting the first child of each parent node. While we use a spe-cific grammar for analyzing flight domain sentences, we emphasize domain portability of the grammar by using mainly syntactic information in the majority of the parse tree rules. Semantics are introduced near the terminals, mostly involving adjecti ves, verbs, nouns and proper noun classes. Rules for general semantic concepts such as dates and times are orga-nized into sub-grammars that are easily embedded into any domain. We have successfully applied the same strate gy in developing both the Chinese and English grammars. Once a parse tree is obtained, se-lected parse cate gories are extracted to form a hier -archical meaning representation encoding both syn-tactic and semantic information. 2.2 Semantic Inf ormation Comparison In principle, we can directly compare the meaning representations deri ved from the source and tar get sides of the translation pair to determine their equi v-alence. In practice, the meaning representation still captures too much language-specific detail, which mak es the comparison prone to failure. Ev en the pair of English utterances,  X  X o w much is the second flight? X  and  X  X hat is the price of the second flight? X  have essentially the same meaning, but would not produce identical meaning representations. Across languages, this situation becomes much worse.
We adopted two complementary strate gies to in-crease the chance of a match between the English prompt and the student translation. First, the English prompt is translated into a reference Chinese trans-lation using the existing interlingua translation capa-bility . This extra step aims at reducing discrepancies caused by syntactic structure dif ferences between the two languages. Secondly , we abstract from the original meaning representation into a simple en-coding of key-v alue (KV) pairs. This is accom-plished using a language generation system (Baptist and Senef f, 2000), with generation rules determin-ing what information to extract from the original hi-erarchical meaning representation. Figure 2 sho ws a couple of examples of the KV representation that we used for scoring.

Another important role of the KV generation step is to bring in a flexible mechanism for defining equi valence, which is a trick y task even for human evaluators. For example, while it is some what ob-vious that  X (1) Give me flights lea ving around nine p m X  is equi valent to  X (2) Give me flights depart-ing around nine p m, X  it is unclear whether these two sentences are equi valent to  X (3) Give me flights around nine p m X  or even  X (4) I would lik e to lea ve around nine p m. X  From a pragmatic point of vie w, the same speak er intention can be inferred from the four sentences. On the other hand, it can be ar-gued that (1) and (2) are completely interchangeable Figure 2: Frame representation of the key-v alue in-formation for two example Chinese sentences. while (3) and (4) could not substitute for (1) or (2) in some circumstances. Criteria for equi valence can be controlled by what is extracted from the mean-ing representation. If only a departure time key is generated for the sentences, then all four sen-tences will be equi valent. Ho we ver, if more infor -mation is preserv ed in the KV pairs, for example, a topic key with value flight , then sentence (4) will not be considered as equi valent to sentences (1)-(3). Considering that our intended application is lan-guage tutoring, we lean towards a stricter criterion for defining equi valence. The KV generation rules are developed manually , guided by human-rated de-velopment data. The KV inventory includes over 80 unique keys.

Once the KV pairs are obtained from the prompt (reference) and the student translation (hypothesis), a recursi ve procedure is applied to compare all the keys in the reference and hypothesis KV frames. Mismatches are tab ulated into substitutions (dif fer -ent values for the same key), deletions (extra keys in the reference), and insertions (extra keys in the hy-pothesis). A perfect match is achie ved if there are no mismatch errors. Figure 3 summarizes the proce-dure to evaluate students X  spok en translations.
Partial match for a good student translation is a common problem caused by speech recognition er-rors, particularly on dates and times. It is natural for the student to just repeat the  X  X ncorrect X  piece after noticing the error in the system X  s paraphrases. Hence, in the tutoring application, we added a sub-match mode to the comparison algorithm, which works in a divide-and-conque r manner . All match-ing KV pairs in each turn are check ed off from the reference, and a subsequent submatch succeeds once there are no remaining KV pairs unaccounted for . One limitation of the incremental comparison algo-rithm is that it ignores insertion errors. The tutoring system pro vides a special reply message when a sen-tence is translated via partial matches accomplished over a series of utterances, to distinguish from the case of a perfect match in a single turn, as illustrated in the example dialogue. 2.3 Integration with Speech Recognition A user X  s utterance is first processed by the speech recognizer to produce word hypotheses. The recognizer is configured from a segment-based speech recognition system (Glass, 2003), using Chi-nese acoustic models trained on nati ve speak ers X  data (W ang et al., 2000a; Wang et al., 2000b). Tone features are ignored in the acoustic models; how-ever, the language model implicitly captures some tone constraints. This is preferred over modeling tone explicitly , considering that non-nati ve speak-ers typically mak e man y tone errors. The language model was initially trained on Chinese translations of English sentences generated from the templates used in the game, and later augmented with addi-tional data collected from users. The recognizer can output multiple hypotheses in the form of an N -best list. The parser is able to con vert the N -best list into a lattice, and re-select a best hypothesis based on a combination of recognition and parsing scores.
Poor recognition on non-nati ve speech is a ma-jor performance issue for CALL application. In our domain, dates, times, and flight numbers are particu-larly challenging entities for the recognizer . Recog-nition error typically results in false rejection, caus-ing frustration to the user . Since the system has explicit kno wledge of the sentence the student is trying to produce, it should be feasible to exploit this kno wledge to impro ve speech understanding. A plausible strate gy is to dynamically adjust the rec-ognizer X  s language model in anticipation of what the user is lik ely to say , as exemplified by dialogue conte xt dependent language models (Solsona et al., 2002).

In theory , we could use the automatically gener -ated reference translation to explicitly bias the lan-guage model. Ho we ver, one has to tak e care not to bias towards the correct response so strongly that the student is allo wed to mak e mistak es with impunity . Furthermore, this strate gy would not generalize to cover all the possible legitimate translations a stu-dent might produce for that prompt. Instead, we de-vised a simple strate gy that overcomes these issues. We select a preferred hypothesis from the N -best list if its KV representation matches the reference. Thus the student has to speak well enough for a correct an-swer to appear some where in the N -best list, with-out any manipulations of the recognizer X  s language model. If the parser fails to find a perfect match in the N -best list, it will choose the hypothesis with the best score, or fall back to the recognizer X  s top hy-pothesis if no parse theory could succeed. Given a translation pair , the goal of our algorithm is to mak e the same accept/reject decision as a human evaluator . Hence, we can evaluate our algorithm in a classification frame work. In this section, we first present the data collection and labeling effort. We then describe a baseline system based on a variant of the Bleu metric. Finally we briefly describe the metrics we used to evaluate our algorithms. 3.1 Data Collection and Labeling During the course of developing a prototype game system, two developers and two student testers inter -acted extensi vely with the system. A total of 2527 Chinese waveforms, recorded during this process, became development data for finding gaps in the interlingua-based matching method and for tuning parameters for the baseline method.

For evaluation, we use 1115 utterances collected from 9 users with varying degrees of Chinese expo-sure. These subjects were ask ed to play the transla-tion game over the Web and fill out a surv ey after -wards. The y came from a rich background of Chi-nese exposure, include adv anced  X  X eritage X  speak-ers of Chinese (including dialects such as Cantonese and Shanghainese), as well as novices who just completed two semesters of a colle ge-le vel Chinese class.

The speech waveforms recorded from the interac-tions were manually transcribed with orthography , gender , and speak er information. The transcriber was instructed to transcribe spontaneous speech ef-fects, such as false starts and filled pauses. Ho we ver, tonal mispronunciations are completely ignored, and segmental errors are lar gely ignored to the extent that the y do not result in a dif ferent syllable.
The translation pairs (the English prompt and the orthographic transcription of the student translation) were rated independently by two bilingual speak ers to pro vide reference labels for evaluating the verifi-cation algorithm. The two raters, both nati ve in Chi-nese and fluent in English, labelled each translation with either an  X  X ccept X  or a  X  X eject X  label. Transla-tions can be rejected because of bad language usage (including false starts) or because of mismatches in meaning. One labeller rated both development and test data, while the second labeller only rated the test data. The interlabeller agreement on the test data has a kappa score (Uebersax, 1998) of 0.85. The subset of data for which there was disagreement were rela-belled by the two raters jointly to reach a consensus. 3.2 Baseline The Bleu metric has been widely accepted as an effecti ve means to automatically evaluate the qual-ity of machine translation outputs (Papineni et al., 2001). An interesting question is whether it would be useful for the purpose of assessing the appro-priateness of translations produced by non-native speak ers at a sentence by sentence granularity level. We developed a simple baseline algorithm using the NIST score, which is a slight variation of Bleu 2 . Given an English prompt, the interlingua-based ma-chine translation system first produces a reference translation. The student X  s translation is then com-pared against the machine output to obtain a NIST score. The translation is accepted if the score ex-ceeds a certain threshold optimized on the develop-ment data.

Figure 4 plots the Recei ver Operating Character -istics (ROC) curv e of the baseline algorithm, ob-tained by varying the NIST score acceptance thresh-old. Each point on the curv e represents a tradeof f between accepting an erroneous translation (False Accept) and rejecting a good one (False Reject). As sho wn in the plot, the NIST score based ROC curv e is far from reaching the ideal top-left corner . For language tutoring purposes, it is desirable to oper -ate in the low false acceptance region. Ho we ver, a 20% false acceptance rate will result in the system rejecting over 35% of correct student translations. The operating point that minimizes overall classifi-cation error turns out to be biased towards lenienc y, falsely accepting over 60% of translations that are rejected by human raters. The resulting minimum error rate on development data transcripts is 23.0%, with a NIST score threshold of 3.16. The thresh-old for automatic speech recognition (ASR) outputs was optimized separately using the 1 -best hypothe-ses of utterances in the development data. The opti-mal threshold on ASR outputs is 1.60, resulting in a classification error rate of 24.1%. The majority clas-sifier , corresponding to the (1 , 1) point on the curv e, translates into a 31.6% error rate on the development data. 3.3 Ev aluation Metrics We evaluated the overall system performance on test data using human decisions as ground truth. Al-Figure 4: ROC curv e by changing acceptance threshold on the NIST score on transcriptions of de-velopment data. though we can not generate an ROC curv e for our proposed algorithm (because it is a non-parametric method), we plot its performance along with the ROC curv e of the baseline system for a more thor -ough comparison.

We evaluated the dif ferent ASR inte gration strate-gies ( 1 -best hypothesis, 10 -best hypotheses, using conte xtual constraints from reference KV) based on sentence classification error rates as well as speech recognition performance. Table 1 summarizes the false accept, false reject, and overall classification error rates on unseen test data. With manual transcripts as inputs, the baseline al-gorithm using the NIST score achie ved a classifica-tion error rate of 19.3%, as compared with 25.0% for the trivial case of always accepting the user sen-tence (Majority classifier). The KV -based algorithm achie ved a much better performance, with only a 7.1% classification error rate. This translates into a kappa score of 0.86, which is slightly abo ve the level of agreement initially achie ved by the two la-bellers. Note that the performance dif ference com-pared to the baseline system is mostly attrib uted to a lar ge reduction in the  X  X alse Accept X  cate gory .
Interestingly , the NIST method degrades only slightly when it is applied to the speech recognition 1 -best output rather than the transcript. Ho we ver, this result is decepti ve, as it is now even more bi-Transcript Reject Accept Error Majority 0.0% 100% 25.0% NIST 8.0% 54.5% 19.6% KV 7.3% 6.8% 7.2% ASR Reject Accept Error NIST 4.2% 77.1% 22.4% KV 1 -best 32.1% 4.3% 25.1% KV 10 -best 27.0% 7.2% 22.1% KV Conte xt 13.5% 14.7% 13.8% Table 1: Classification results for various evaluation systems, on both transcripts and automatic speech recognition (ASR) outputs. Note that the  X  X V Con-text X  condition favors a hypothesis that matches the prompt KV . ased towards a  X  X alse Accept X  strate gy, causing over three quarters of the students X  erroneous utterances to be accepted. The KV method is much more sus-ceptible to speech recognition error because of its deep linguistic analysis. For instance, any recog-nition errors causing a parse failure will result in a  X  X eject X  decision, which explains the high error rate when only the 1 -best hypothesis is used. Ho w-ever, the KV algorithm can impro ve substantially by searching the full N -best list ( N = 10) for a plau-sible analysis. When conte xtual information (KV Conte xt) is used, our simple strate gy of favoring the hypothesis matching the reference KV reduces the classification error rate dramatically .

A plot of the recei ver operating characteristics of these methods in Figure 5 reveals a clear picture of the performance dif ferences. All of the KV points are clustered in the upper left corner of the plot, abo ve the ROC curv e of the NIST -based method. The NIST -score based classifier (represented by the square mark er on the ROC curv e) is hea vily biased towards making the acceptance decision (the major -ity class). In contrast, the KV method operates in the low  X  X alse Accept X  area. It achie ves a much lower false rejection rate when compared with the NIST method operating at an equi valent false acceptance point.

Although the classification error rate clearly im-Figure 5: Comparison of ROC of dif ferent methods. 1 -best 11.6 -40.4 -10 -best 10.7 7.8 38.7 4.2 Conte xt 8.7 25.0 30.0 25.7 Table 2: Comparison of speech recognition per -formance in syllable error rates and sentence error rates, for three dif ferent strate gies of utterance selec-tion from an N -best list. (ER stands for error rate, RR stands for relati ve reduction.) pro ves when the KV method mak es use of the N -best list and incorporates conte xtual constraints, the ROC plot seems to suggest that the error reduction might simply be attrib uted to a shift in the operat-ing point: the impro vements are caused by a bias towards making the majority class decision. We use impro vements in speech recognition to demonstrate that this is not the case (at least not entirely). Table 2 summarizes the syllable and sentence error rates on the test data, for the three configurations discussed pre viously ( 1 -best, 10 -best, and Conte xt). By using a tighter inte gration with the parser with conte xtual constraints, we greatly impro ved speech recognition performance, mark ed by reductions of syllable and sentence error rates by 25% and 25.7% respecti vely . In this paper , we have presented an algorithm for au-tomatically assessing spok en translations produced by language learners. The evaluation results demon-strated that our method involving deep linguistic analysis of the translation pair can achie ve high con-sistenc y with human decisions, and our strate gy of incorporating conte xtual constraints is effecti ve at impro ving speech recognition on non-nati ve speech. While our solution is domain specific, we emphasize domain portability in the linguistic analysis mod-ules, so that similar capabilities in other domains can be quickly developed even in the absence of train-ing data. Our interlingua frame work also mak es the methodology agnostic to the direction of source and tar get languages. A similar application for nati ve Mandarin speak ers learning English could be instan-tiated by using the same components for linguistic analysis.

A major challenge in our problem is in determin-ing equi valence between the meanings of a transla-tion pair . While our approach of using a rule-based generation system gives the developer great flexibil-ity in deri ving an appropriate KV representation, the comparison algorithm is some what primiti ve: it re-lies entirely on the generation rules to produce the right KV representation. In future work, we plan to apply machine learning techniques to this prob-lem. With the data we have collected and labelled (and the effort is ongoing), it becomes feasible to examine the use of data-dri ven methods. As alluded to in our evaluation methodology , we can cast the problem into a classification frame work. Le xical, n -gram, and alignment based features can be ex-tracted from the translation pairs, which can be fur -ther enhanced by features obtained from deep lin-guistic analysis. This will relie ve the burden on the semantic analysis component, and impro ve the over-all portability of our approach.

We also plan to expand our application to man y other domains appropriate for language learning, and test the effecti veness of the translation game as a means for language learning. This research is supported in part by ITRI and the Cambridge MIT Initiati ve. The authors would lik e to ackno wledge Yushi Xu for annotating the data. We are also grateful to Michael Collins and the anon y-mous revie ws for their helpful comments and sug-gestions.

