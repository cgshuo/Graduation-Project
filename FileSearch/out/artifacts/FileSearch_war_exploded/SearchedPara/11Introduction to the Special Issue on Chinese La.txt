 The Chinese language has a number of characteristics that make Chinese language processing particularly challenging and intellectually rewarding. For example, written Chinese text does not have conventionalized word boundaries like English and other Western languages, and researchers have devoted an enormous amount of energy to figuring out the best way to identify words, which is generally considered the first step for more advanced language processing tasks. The Chinese language is also characterized by the lack of formal devices such as morphological tense and number that often provide important clues for fundamental language processing tasks such as part-of-speech tagging and syntactic parsing. As a result, solutions to Chinese language processing problems often requi re more sophisticated language processing techniques that are capable of drawing inferences from more subtle information. The 10 articles appearing in this special issue cover a wide range of topics in Chinese Language Processing (CLP), and they include fundamental CLP technologies such as treebank conversion and semantic role labeling (SRL), as well as more applied topics such as Information Extraction (IE ), Machine Translation (MT), Information Retrieval (IR), and Question Answering (QA).

Syntactic parsing is an urgent topic in Chinese language processing, where parsing accuracy still lags behind languages such as English. Zhu et al. attempts to improve Chinese POS tagging and parsing accuracy by pooling together syntactically anno-tated Chinese resources created based on different standards. They propose a method on converting the Tsinghua Treebank to the Penn Chinese Treebank format. The con-version is achieved in two steps, POS tagging and parsing, in a unified procedure that they call  X  X nformed decoding X , where information derived from the original annotation in a source treebank is used to guide a conversion algorithm trained on a target tree-bank. They show significant improvement in conversion accuracy over several baseline systems, especially in situations where the target treebank is small in size.
Semantic role labeling has been a very active area of research in recent years fueled by the availability of large-scale semantically annotated corpora. Li and Zhou explore a unified semantic role labeling (SRL) framework for both verbal and nominal predicates in Chinese. By careful feature engineering, they are able to achieve signif-icant improvement in both verbal and nominal semantic role labeling. In particular, they improve the performance of nominal SRL by exploiting the connections between verbal and nominal predicates. Specifically, they investigate ways of adding selected verbal predicate instances to the training d ata for nouns and extracting features from the semantic role labels for verbal predicates to improve nominal SRL. While prior work in Chinese SRL generally assumes that the predicates are given, Li and Zhou also attempt to address the problem of identifying nominal predicates automatically, which is a nontrivial problem in itself.

This special issue also includes two articles that represent widely different ap-proaches to entity relation extraction. Zhang et al. proposed a feature-based method that exploits the structural configurations of the two entities involved in a relation. The structural configurations are defined purely based on the text span of the entity mentions, without resorting to the use of a syntactic parser. In contrast, Qian et al X  X  kernel-based approach makes heavy use of syntactic and semantic dependencies pro-duced by syntactic parsers. The two articl es make very different trade-offs between the use of deep syntactic/semantic processing and the error propagation that comes with it. Having the two articles in the same issue helps to illuminate the issues in relation extraction.

Machine Translation, the Chinese-English pair in particular, is a very dynamic research area, and this has been reflected by the fact that it has the most submis-sions and accepted articles in this special issue. Three articles cover various topics in Chinese-related machine translation. Huang et al. study the translation of out-of-vocabulary (OOV) words and find that sublexical information is quite helpful in addressing this issue. Du and Way focus on the translation of structures involving the Chinese function word DE and propose a DE classification and reordering approach that significantly improves the translation quality. Xiao et al. investigate the effect of language models on syntax-based machine translation. They propose a tree substitu-tion grammar-based language model and show that it leads to a modest improvement in Machine Translation performance.

Li et al. X  X  work bridges Information Extra ction and Machine Translation. They pro-pose an English-Chinese named entity pair mining approach that combines features from various information sources, which include a transliteration model, English-to-Chinese string match, Chinese-to-English string match, a translation model, a length model and a context vector. They demonstrate the effectiveness of these features by integrating them into one model with linear combination and a minimum sample risk (MSR) algorithm.

Liu et al. investigate the advantage of exploiting user behaviors and collaborative learning in defining semantic relatedness, and then use this measure to perform re-lated word retrieval and new word detection tasks. Wang et al. tackle the problem of non-factoid Question Answering. So far almost no automatic QA system works well on non-factoid questions. Wang et al. take advantage of the huge number of user-generated QA pairs on Internet social me dia and use a deep belief network to model the semantic relationship between questions and their answers. Using only word fea-tures, the model outperforms traditional methods in detecting answers.
 Out of 33 total submissions, 10 papers were selected to appear in this special issue. We would like to thank the authors for their overwhelming support. The large number of submissions attests to a vibrant Chinese language processing research community. The authors had to work with a very tight publication schedule for a fast turnaround demanded by this journal. Our special thanks, however, go to the reviewers who contributed their service to the community during a busy conference season when their review assignments competed directly with their own research for their time. Their names are listed here in alphabetic order.
 Yee Seng Chan, Wanxiang Che, Berlin Chen, Boxing Chen, Hsin-Hsi Chen, Tee Kiah Chia, David Chiang, Daniel Dahlmeier, Jinhua Du, Kevin Duh, Donghui Feng, Daqing He, Xiaodong He, Yulan He, Zhongjun He, Wen-Lian Hsu, Yunhua Hu, Changning Huang, Xuanjing Huang, Heng Ji, Jing Jiang, Lun-Wei Ku, June-Jie Kuo, Sadao Kurohashi, Wai Lam, Gary Lee, Yoong Keok Lee, Bo Li, Chi-Ho Li, Junhui Li, Shoushan Li, Zhifei Li, Tyne Liang, Chuan-Jie Lin, Ziheng Lin, Yang Liu, Wei Lu, Yajuan L  X  u, Yanjun Ma, Haitao Mi, Alessandro Moschitti, Seung-Hoon Na, Preslav Nakov, Hoifung Poon, Sameer Pradhan, Sindhu Raghavan, Jun Sun, Chew Lim Tan, Kiyotaka Uchimoto, Haifeng Wang, Houfeng Wang, Kun Wang, Chung-Hsien Wu, Hua Wu, Shih-Hung Wu, Rui Xia, Deyi Xiong, Muyun Yang, Kun Yu, Dongdong Zhang, Min Zhang, Ruiqiang Zhang, Ying Zhang, Hai Zhao, Jun Zhao, Guodong Zhou, Ming Zhou, Jingbo Zhu, and Chengqing Zong.

 The Chinese language has a number of characteristics that make Chinese language processing particularly challenging and intellectually rewarding. For example, written Chinese text does not have conventionalized word boundaries like English and other Western languages, and researchers have devoted an enormous amount of energy to figuring out the best way to identify words, which is generally considered the first step for more advanced language processing tasks. The Chinese language is also characterized by the lack of formal devices such as morphological tense and number that often provide important clues for fundamental language processing tasks such as part-of-speech tagging and syntactic parsing. As a result, solutions to Chinese language processing problems often requi re more sophisticated language processing techniques that are capable of drawing inferences from more subtle information. The 10 articles appearing in this special issue cover a wide range of topics in Chinese Language Processing (CLP), and they include fundamental CLP technologies such as treebank conversion and semantic role labeling (SRL), as well as more applied topics such as Information Extraction (IE ), Machine Translation (MT), Information Retrieval (IR), and Question Answering (QA).

Syntactic parsing is an urgent topic in Chinese language processing, where parsing accuracy still lags behind languages such as English. Zhu et al. attempts to improve Chinese POS tagging and parsing accuracy by pooling together syntactically anno-tated Chinese resources created based on different standards. They propose a method on converting the Tsinghua Treebank to the Penn Chinese Treebank format. The con-version is achieved in two steps, POS tagging and parsing, in a unified procedure that they call  X  X nformed decoding X , where information derived from the original annotation in a source treebank is used to guide a conversion algorithm trained on a target tree-bank. They show significant improvement in conversion accuracy over several baseline systems, especially in situations where the target treebank is small in size.
Semantic role labeling has been a very active area of research in recent years fueled by the availability of large-scale semantically annotated corpora. Li and Zhou explore a unified semantic role labeling (SRL) framework for both verbal and nominal predicates in Chinese. By careful feature engineering, they are able to achieve signif-icant improvement in both verbal and nominal semantic role labeling. In particular, they improve the performance of nominal SRL by exploiting the connections between verbal and nominal predicates. Specifically, they investigate ways of adding selected verbal predicate instances to the training d ata for nouns and extracting features from the semantic role labels for verbal predicates to improve nominal SRL. While prior work in Chinese SRL generally assumes that the predicates are given, Li and Zhou also attempt to address the problem of identifying nominal predicates automatically, which is a nontrivial problem in itself.

This special issue also includes two articles that represent widely different ap-proaches to entity relation extraction. Zhang et al. proposed a feature-based method that exploits the structural configurations of the two entities involved in a relation. The structural configurations are defined purely based on the text span of the entity mentions, without resorting to the use of a syntactic parser. In contrast, Qian et al X  X  kernel-based approach makes heavy use of syntactic and semantic dependencies pro-duced by syntactic parsers. The two articl es make very different trade-offs between the use of deep syntactic/semantic processing and the error propagation that comes with it. Having the two articles in the same issue helps to illuminate the issues in relation extraction.

Machine Translation, the Chinese-English pair in particular, is a very dynamic research area, and this has been reflected by the fact that it has the most submis-sions and accepted articles in this special issue. Three articles cover various topics in Chinese-related machine translation. Huang et al. study the translation of out-of-vocabulary (OOV) words and find that sublexical information is quite helpful in addressing this issue. Du and Way focus on the translation of structures involving the Chinese function word DE and propose a DE classification and reordering approach that significantly improves the translation quality. Xiao et al. investigate the effect of language models on syntax-based machine translation. They propose a tree substitu-tion grammar-based language model and show that it leads to a modest improvement in Machine Translation performance.

Li et al. X  X  work bridges Information Extra ction and Machine Translation. They pro-pose an English-Chinese named entity pair mining approach that combines features from various information sources, which include a transliteration model, English-to-Chinese string match, Chinese-to-English string match, a translation model, a length model and a context vector. They demonstrate the effectiveness of these features by integrating them into one model with linear combination and a minimum sample risk (MSR) algorithm.

Liu et al. investigate the advantage of exploiting user behaviors and collaborative learning in defining semantic relatedness, and then use this measure to perform re-lated word retrieval and new word detection tasks. Wang et al. tackle the problem of non-factoid Question Answering. So far almost no automatic QA system works well on non-factoid questions. Wang et al. take advantage of the huge number of user-generated QA pairs on Internet social me dia and use a deep belief network to model the semantic relationship between questions and their answers. Using only word fea-tures, the model outperforms traditional methods in detecting answers.
 Out of 33 total submissions, 10 papers were selected to appear in this special issue. We would like to thank the authors for their overwhelming support. The large number of submissions attests to a vibrant Chinese language processing research community. The authors had to work with a very tight publication schedule for a fast turnaround demanded by this journal. Our special thanks, however, go to the reviewers who contributed their service to the community during a busy conference season when their review assignments competed directly with their own research for their time. Their names are listed here in alphabetic order.
 Yee Seng Chan, Wanxiang Che, Berlin Chen, Boxing Chen, Hsin-Hsi Chen, Tee Kiah Chia, David Chiang, Daniel Dahlmeier, Jinhua Du, Kevin Duh, Donghui Feng, Daqing He, Xiaodong He, Yulan He, Zhongjun He, Wen-Lian Hsu, Yunhua Hu, Changning Huang, Xuanjing Huang, Heng Ji, Jing Jiang, Lun-Wei Ku, June-Jie Kuo, Sadao Kurohashi, Wai Lam, Gary Lee, Yoong Keok Lee, Bo Li, Chi-Ho Li, Junhui Li, Shoushan Li, Zhifei Li, Tyne Liang, Chuan-Jie Lin, Ziheng Lin, Yang Liu, Wei Lu, Yajuan L  X  u, Yanjun Ma, Haitao Mi, Alessandro Moschitti, Seung-Hoon Na, Preslav Nakov, Hoifung Poon, Sameer Pradhan, Sindhu Raghavan, Jun Sun, Chew Lim Tan, Kiyotaka Uchimoto, Haifeng Wang, Houfeng Wang, Kun Wang, Chung-Hsien Wu, Hua Wu, Shih-Hung Wu, Rui Xia, Deyi Xiong, Muyun Yang, Kun Yu, Dongdong Zhang, Min Zhang, Ruiqiang Zhang, Ying Zhang, Hai Zhao, Jun Zhao, Guodong Zhou, Ming Zhou, Jingbo Zhu, and Chengqing Zong.

