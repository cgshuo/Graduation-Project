 A major challenge in developing models for hypertext retrieval is to effectively combine content information with the link struc-ture available in hypertext collections. Although several link-based ranking methods have been developed to improve retrieval results, none of them can fully exploit the discrimination power of contents as well as fully exploit all useful link structures. In this paper, we propose a general relevance propagation framework for combining content and link information. The framework gives a probabilis-tic score to each document defined based on a probabilistic surfing model. Two main characteristics of our framework are our proba-bilistic view on the relevance propagation model and propagation through multiple sets of neighbors . We compare eight different models derived from the probabilistic relevance propagation frame-work on two standard TREC Web test collections. Our results show that all the eight relevance propagation models can outperform the baseline content only ranking method for a wide range of param-eter values, indicating that the relevance propagation framework provides a general, effective and robust way of exploiting link in-formation. Our experiments also show that using multiple neigh-bor sets outperforms using just one type of neighbors significantly and taking a probabilistic view of propagation provides guidance on setting propagation parameters.
 H.3.3. [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval Models Algorithms, Experimentation Content and link ranki ng, hypertext retrieval model, probabilistic relevance propagation, web information retrieval Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00.
Hypertext Retrieval, the task of searching for information in a hypertext collection, has been around for a while. A key charac-teristic that distinguishes the search task in a hypertext collection from a traditional retrieval task is the existence of link information in the former one. Although the primary goal of creating links is to guide a user to other parts of the collection, the link information can also be exploited to improve the search accuracy. The existence of this extra information makes it inappropriate to use traditional in-formation retrieval methods, which do the retrieval task based on the content only, to do the search task.

The early works on the hypertext retrieval task were more on the literature side. Some researchers have used bibliographic cita-tion methods to determine relationships among documents in sci-entific papers [32, 16, 37]. They have used different citation meth-ods in this direction, namely direct citation, bibliographic coupling -the sharing of one or more references by two documents -and co-citation. Modha and Spangler [25] have proposed a clustering algorithm that clusters hypertext documents using words, out-links and in-links, Chakrabrti et al. [6] have developed a technique called  X  X pectral filtering X  for discovering high-quality topical resources in hyperlinked corpora and Ray Larson [22] has applied co-citation analysis methods to the World Wide Web to produce clusterings of the WWW sites that have topical similarities.
 Currently with the fast growth and popularity of the World Wide Web, the search task on this huge collection of hypertext data has gained much attention. The problem of hypertext retrieval on the Web has been studied extensively. Several link-based ranking meth-ods have been developed to improve retrieval results [20, 27, 2, 5, 3, 26, 18, 30, 14, 36, 4, 39, 24, 38, 41, 43, 19, 40, 1, 29].
Although these algorithms have been shown to improve the per-formance over some baseline approaches, it remains a challenging research question what is the best way to exploit the content in-formation and the link information to maximize search accuracy. These works appear to have adopted five strategies for combining content and link information: (1) Using the query as a filter to se-lect documents and rank them according to link-based scores (e.g., PageRank [27] and HITS [20]); (2) Computing a weighted com-bination of topic-specific PageRank scores, where the weights are determined by the query (Topic-sensitive PageRank [18]); (3) Us-ing the query to compute the relevance value of each document and regulating the influence of nodes in HITS using these value (e.g. ARC [5], Bharat and Henzinger[2]); (4) Using the query to compute the relevance value of each document and propagate these values through links (Intelligent S urfer [30], [36], [29]). (5) Us-ing sitemap links to propagate term frequencies ([38], [29]). Un-fortunately, none of these combination methods can fully exploit the discrimination power of contents as well as fully exploit all useful link structures. Despite the importance of link information, the contents of documents are clearly the most direct evidence re-garding whether a document is relevant to a user X  X  interest. Thus presumably, contents of the documents should be the main basis for ranking them. In this sense, among the five strategies, only the last two are close to fully exploiting the content information to improve ranking. However, the intelligent surfer only consid-ers the in-links of a document, the  X  X elevance propagation X  method only considers direct in-links or out-links and the  X  X erm propaga-tion X  method only considers parent-child links in a sitemap. Each of these methods only considers one type of explicit neighbors and none of them fully take advantage of all the available link informa-tion. But intuitively, all neighbors can be potentially exploited; for example, both out-links and in-links may be useful as we will show in our experiments. Besides, for the propagation methods, there exist no principled framework to do the propagation. For example, the content scores can be transformed using any monotonic func-tion without affecting the ranking, but such transformation would presumably affect the propagation. How should we transform the scores to achieve the best propagation results?
In this paper, we propose a general probabilistic relevance prop-agation framework for combining content and link information, which can fully take advantage of content information and the link structure in a principled way and can unify most existing link-based ranking algorithms. The basic idea of probabilistic relevance prop-agation is to first compute a content-based relevance probability score for each document using the query, and then propagate the probabilitie s through differen t groups of neighbor s. We exploit the content information as a basis for finding the probability of the relevance of a document to a query and use the link structure to define different groups of neighbor s to propagate th e probabilities through.

After propagation, unlike [29], our model gives us a probabilis-tic score for each document defined based on a probabilistic surf-ing model. Moreover, our model supports using multiple types of neighbors, which is shown to outperform the results of using a sin-gle type of neighbor. On the other hand, the probabilistic interpre-tation of the model suggests that we should transfer the content-based retrieval scores to probabilities of relevance, which is shown to be beneficial in our experiments. The probabilistic interpreta-tion also provides guidance on how to set various parameters in the propagation model.

In some sense, our work resembles previous work on spreading activation [7, 33, 12, 13, 34, 15, 28, 35, 23, 11] as both involve propagating values through a network/graph. The main difference, however, is that in these spreading activation methods, the number of steps for propagating the weights is predefined and is a small value in most of the cases, while our framework is an iterative pro-cess which iterates until the ranks converge to a limit.
We derive several special instances of the general probabilistic relevance propagation framework and show that probabilistic rel-evance propagation is a very general mechanism that allows us to recover most of the major existing algorithms as special cases. Moreover, it also naturally suggests several new algorithms that can combine content and link information.

In our experiments, we evaluated several propagation algorithms and the experiment results show that: (1) Using relevance prop-agation to combine link information and content information for scoring can improve retrieval accuracy over using only content for scoring. (2) Using multiple sets of neighbors for pr opagation out-performs using a si ngle neighbor set. (3) U sing proba bilities to control the effect of different groups of neighbors helps. (4) Using probabilities to control the influence of each document in a neigh-bor set helps.

The rest of the paper is organized as follows: We present our relevance propagation framework and derive several special cases in section 2. We discuss the experiment results in section 3 and conclude in section 4 finally.
Given a query, intuitively, a good result document is one whose content is related to the query topic and which is surrounded by other good documents; i.e. located in the center of a subset of the collection relevant to the query. Thus in order to maximize ranking accuracy, we need to consider the relevance of the document to the query as well as the relevance of its neighbors.

In this section, we propose a general probabilistic relevance prop-agation framework for combining relevance values of different groups of neighbors in a principled way. The basic idea of probabilistic rel-evance propagation is to first use the query to compute a content-based self relevance probability score for each document and then propagate the scores through the neighbors.
In this framework, we allow different types of neighbors to in-fluence the quality score of a document. Figure 1 shows a sample document d surrounded by k different groups of neighbors. The neighbor sets are not necessarily mutually exclusive. In-links, Out-links, the single document itself and the whole set of documents are a few examples of potential neighbor sets.

Think of a random surfer surfing the Web looking for documents related to a given query q . At each step, the surfer being in a doc-ument, selects a group of neighbors surrounding the document and jumps to a document in that group. The surfer keeps doing this iteratively, jumping to neighbor documents looking for documents relevant to query q . The final score of each page is equal to the stationary probability of the surfer visiting the page.
Formally, the probability of the surfer being in each document is defined as: where D is the set of all documents,  X  i indicates the probability of choosing a particular type of neighbor set when leaving the current document and p i is the probability of visiting a particular page in the chosen neighbor set. Note that p i ( a  X  b ) is positive only if b is a neighbor of a ,otherwise p i ( a  X  b )=0 .

In order to compute the scores, for each neighbor set, we con-struct a matrix M i where M i ( m, n )= p i ( d m  X  d n ) . The prob-ability scores can be computed using matrix multiplication: P = M T P where P is the vector of the probability values and M = =1  X  i M i . The probability valu es are computed iteratively through matrix multiplications in a very similar way as any of the existing link-based scoring algorithms. Clearly, efficient matrix multiplica-tion methods can be used to further speed up the scoring. The final scores will be the values of the stationary probability distribution. To ensure reachability to each document, we generally would in-clude the whole set of documents as one special set of neighbors in our propagation framework. Thus by the Ergodicity theorem for Markov chains [17], we know that the Markov chain defined by such a transition matrix M must have a unique stationary probabil-ity distribution.

In this framework, we identify three groups of probabilistic pa-rameters:
By setting  X  i s to different values and instantiating p i cific functions, we can easily obtain many special cases of our gen-eral relevance propagation framework. In particular, the framework can recover most existing link-based algorithms. Table 1 shows a family of relevance propagation algorithms which are covered by our general framework.

As can be seen from the table, PageRank and its extensions are special cases of the framework. The HITS algorithm is not directly a special case, since it does not satisfy the probability property. But with minor changes, i.e. normalization of the weights, it will be a special case. In this table, we include the normalized version of HITS as well as the normalized version of its extensions.
In our framework, we have identified three groups of probabilis-tic parameters: content relevan ce probab ilities, ne ighbor set selec-tion probabilities and navigation probabilities. The content rele-vance probability of a document can be estimated based on its rel-evance score given by any content-based retrieval method. Neigh-bor set selection probabilities and navigation probabilities can be either set to uniform or estimated based on content-based relevance scores. In this subsection, we show how to estimate the parameters. We compare different estimation methods in the following section.
In our probabilistic framework we should convert the original content scores to probabilities. The specific conversion method is inevitably dependent on the specific content scoring method, but with some training data, we may use techniques such as logistic regression to do the conversion. If the original retrieval model is a probabilistic model, we have some natural analytical way to trans-form the scores. As an example of this transformation, here we show how we compute relevance probabilities from Okapi scores and from language model(LM) scores.  X  Okapi
Having the Okapi scores, our goal is to normalize the scores to find the relevance p robab ilities of the documents to the query. We use logistic regression to normalize the scores [31]:
The Okapi score is a  X  X + b ,where X is the log odds of rele-vance, i.e., log( p ( rel ) / (1  X  p ( rel )) . So, to recover the probability p ( rel ) ,wehave p ( rel )=exp( x ) / (1 + exp( x )) . Given a score s , we have s = aX + b ,or X =( s  X  b ) /a . Thus, the normalization formula should be p ( rel )=exp(( s  X  b ) /a ) / (1+exp(( s
In order to set a and b , we assume that the minimum score min corresponds to a very small probability  X  . We also assume that the maximum score max corresponds to p ( rel )= X  . Solving these equations will give us values for a and b :  X  Language Modeling Approach
In the language modeling approach, we score a document D w.r.t. a query Q by s =log p ( Q | D ) [42]. Thus we can do an expo-nential transformation to recover the probability of relevance. That is, p ( rel )  X  p ( Q | D ) p ( D )  X  exp ( s ) (assuming uniform p(D)).
The easiest way to estimate neighbor set selection probabilities is uniform estimation, counting all the neighbor sets to be equal, i.e.  X  i = 1 k .

But obviously this is not the best we can do. Our framework suggests to use relevance score s for Neighbor set p robability es-timation. We get our intuition for defining neighbor set selection probabilities from the surfer model. In the surfer model, in each step, the surfer should decide on the neighbor set he wants to jump to. Intuitively, the surfer will select the neighbor set based on the average relevance of the documents in the neighbor set, the higher the average relevance, the more probable the surfer will select that group. Using this intuition, we set  X  i using:
Like neighbor set selection proba bilities, the navig ation proba-bilities are most easily estimated th rough uniform estimation. But intuitively, estimating the probab ilities using releva nce values should give better results.

We define navigation probabilities based on the content rele-vance probabilities of target pages. The higher the probability of the relevance of the target page, the higher the probability of navi-gating the link: p ( d  X  x )  X  p ( x ) .
The proposed framework provides a general probabilistic inter-pretation of relevance-based p ropagation through multiple sets of neighbors. It can unify most existing link-based ranking algorithms, making it possible to compare the assumptions made in each spe-cific algorithm. It also makes it possible to systematically explore the algorithm space and compare different components of algo-rithms. Moreover, taking a strict probabilistic view of propagation provides guidance on how to normalize content scores and how to set other propagation parameters to optimize retrieval accuracy, as will be shown later in the paper.
We have done some experiments to evaluate the performance of our proposed models. In this section, we present our experiment results.
As our data set, we used the  X .GOV X  test collection, which is an 18 gigabyte, 1 . 25 million doc ument 2002 partial crawl of the .gov domain used in TREC-2002, TREC-2003 and TREC-2004 experi-ments for topic distillation [8, 9, 10]. We used two sets of queries in our experiments: (1) 50  X  X opic distillation X  topics created by NIST for TREC-2003 and (2) 75  X  X opic d istillation X  topics created by NIST for TREC-2004. The topics are keyword queries for which key resources exist within the .GOV collection.

An important advantage of using this set of data is that it is created carefully for the purpose of evaluating Web retrieval algo-rithms with a significant number of judgments available for quanti-tatively comparing different methods.
 In our experiments, we used two baseline methods: Okapi and Language Modeling approach. Since our exploration is orthogo-nal to the use of anchor text and many other heuristics which are known to improve the performance, we preferred not to enter these heuristics in our baseline. Despite this, we already have a very strong baseline compared to the reported results in TREC2003 [9] and TREC2004 [10]. We expect the performance to be further im-proved when we use other heuristics on top of our method.
In our experiments, we compare the performance of using two types of neighbors: The set of documents which have links to the document(IN) and the set of documents which are linked from the document(OUT). There also exist a universal neighbor set N contains all the documents in the collection. Selecting this univer-sal neighbor set to jump to is equivalent to jumping to a random page.
The probabilistic relevance propagation framework allows us to use any content-based retrieval algorithm from which we can com-pute the relevance probabilities. In our experiments, we try two baseline methods: Okapi and language modeling approach and com-pute the relevance probabilities from these relevance scores.
As mentioned earlier,  X  i s are the parameters which indicate the probability of choosing a particular t ype of neighbor when leaving the current document. In our experiments, we follow one of the two approaches: either manually set  X  i to different values from 0 to 1 or automatically set  X  i using neighbor set sel ection pr obab ilities.
Navigation probabilities on the other hand indicate the probabil-ity of visiting a particular page in a group. In our experiments, we use two different estimations of these probabilities: Uniform esti-mation ( X  X ni X ) and relevance based estimation ( X  X t X ).
The first research question we want to answer is whether apply-ing probabilistic relevance propagation on top of a content-based retrieval method would improve the performance. Most existing studies of link-based scoring algorithms focus on comparing differ-ent link-based algorithms without comparing link-based algorithms with scoring using only contents. The Web Track of TREC has seen some evaluation of effectiveness of exploiting link information to improve content-based scoring, but the results are not quite conclu-sive due to the many uncontrolled factors.

To answer the first research question, we compare the perfor-mance of using two types of neighbors: in-links and out-links. (Note that we also have the universal neighbor set). We will have: where  X  0 is the probability of randomly jumping to a page,  X  the probability of jumping to an in-link and  X  O is the probability of jumping to an out-link. Jumping probabilities can either be uni-form (considering all the members to be equal) or weighted based on relevance probabilities. We also consider the combination of the two types of neighbors. This gives us eight combinations, which we compare with the content-only baseline in tables 2 and 3. The numbers in parenthesis show the percent of improvement over the baseline methods. We use precision at 10 and average precision for comparison. The shown results are the best performances achieved by these methods through tuning the parameter  X  manually in the probabilistic relevance propagation model; we will analyze the sen-sitivity later.
 From tables 2 and 3, we can make the following observations: 1. On both query sets, both types of neighbors can outperform 2. Weighted propagation of probabilities outperform uniform 3. The combination of different types of neighbors outperform 4. We get significant improvement using both Okapi and LM
Overall, we see that the probabilistic relevance propagation frame-work is reasonable and all these specific derived algorithms can help improve search results.

In tables 4 and 5, we compare the results of using only one type of neighbor with the results wh en we consider multiple groups of neighbors. We did a Wilcoxon signed rank test to see if the im-provement on Average Precision is statistically significant. In these tables we compare the best results for each type of neighbor. Sta-tistically significant improvements are distinguished by a star(
As the tables show, combining different groups of neighbors im-proves the performance over using a single set of neighbors. Po-tentially, we can improve the performance by adding new types of neighbors, e.g. co-citations (documents which have at least one common parent with the document) and co-references (documents which have at least one common child with the document).
The probabilistic framework suggests that we should convert the original content scores to probabilities. In our experiments, we use Okapi and LM methods as our baseline and transform the scores to probabilitie s using logistic regression and exponential transforma-tion respectively.

Figures 2 and 3 compare the performance of probabilistic trans-formation with the performance of the original raw score propaga-tion as done in all the previous work. As can be seen, the perfor-mance is much better when we use probabilistic transformation.  X 
Relevance-Based Estimate of  X  Improves over Uniform Esti-mate.

In one set of experiments, we tried to set  X  s automatically based on the average relevance values of neighbors. Table 6 compares the results of relevance-based estimation of  X  with uniform esti-mation. As the table shows, in most of the cases relevance-based estimation gives better results. Note that these results are com-pletely automatic; i.e. we do not have to tune any parameters. Thus these improvements are very encouraging. These results also con-firm that using multiple neighbor sets improves over using just a single neighbor set.  X 
Relevance-Based Estimate of p i ( d  X  x ) Improves over Uni-form Estimate.

In table 7, we compare the results of uniformly setting the navi-gation weights vs. estimating them based on relevance scores. As the table shows, relevance based estimation improves the perfor-mance in most of the cases.
We have so far only looked at the best performance using each method. We now turn to the question about how sensitive each method is to the setting of the parameter  X  , which controls the amount of influence from the neighbors. To answer this research question, we compute an  X  X ptimal range X  of parameter values for each method, which is defined as the interval of parameter values for which a method outperforms the baseline. Table 8 shows the optimal ranges for four of our algorithms.

We see that, in general, the optimal range is wide for most meth-ods, indicating that exploiting t hese groups of neighbors for rel-evance propagation is useful. The uniform methods are gener-ally more sensitive to the setting of  X  , which indicates that using weighted methods is more robust.

In figures 2 and 3, we show the complete picture of the sensitivity of these methods with prec@10 and average precision.
In this paper, we proposed a general probabilistic relevance prop-agation framework for combining content and link information in a principled manner to fully take advantage of query-based content scoring and link structures. The framework can unify most existing link-based ranking algorithms and can also suggest several inter-esting new algorithms through different propagation strategies.
Following the probabilistic relevance propagation framework, we systematically compared eight specific relevance propagation mod-els on two TREC test collections for Web retrieval.

Our results show that all the eight relevance propagation mod-els that we tested can outperform the baseline content only rank-ing method for a wide range of parameter values, indicating that Table 4: Using Multiple Neighbors vs. a Single Neighbor Set-TREC-2003 Table 5: Using Multiple Neighbors vs. a Single Neighbor Set-TREC-2004 the relevance propagation framework provides a general, effective and robust way of exploiting link i nformation to improve hypertext search accuracy.

While the previous work all uses just one type of neighbor for propagation, we have shown that u sing multiple neighbor sets out-performs using just one type of neighbors significantly. We have also shown that taking a probabilistic view of propagation pro-vides guidance on setting propagation parameters, that using con-tent scores to estimate the probabilities of relevance improves the performance and that relevance based estimation of the parameters helps us improve the results.

There are several interesting directions for further research: 1. Our framework naturally accommodates the use of anchor 2. We have shown that in-links and out-links are useful for rel-Table 6: Effectiveness of Neighbor Set Selection Probability Es-timation (  X  ) 3. Other than the neighbor sets derived from the explicit link Table 7: Navigation Probability Estimation -TREC-2003 4. The probabilistic relevance propagation framework is a gen-
This work is in part supported by the National Science Founda-tion under award numbers 0425852, 0347933, and 0428472. [1] V. Anh and A. Moffat. Melbourne university 2004: Terabyte [2] K. Bharat and M. R. Henzinger. Improved algorithms for [3] K. Bharat and G. A. Mihaila. When experts agree: using [4] A. Borodin, G. O. Roberts, J. S. Rosenthal, and P. Tsaparas. [5] S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, [6] S. Chakrabarti, B. Dom, D. Gibson, S. Kumar, P. Raghavan, [7] P. R. Cohen and R. Kjeldsen. Information retrieval by [8] N. Craswell and D. Hawking. Overview of the trec-2002 web [9] N. Craswell and D. Hawking. Overview of the trec-2003 web [10] N. Craswell and D. Hawking. Overview of the trec-2004 web [11] F. Crestani and P. L. Lee. Searching the web by constrained [12] W. B. Croft, T. J. Lucia, and P. R. Cohen. Retrieving [13] W. B. Croft, T. J. Lucia, J. Cringean, and P. Willett. [14] B. D. Davison. Toward a unification of text and link analysis. [15] H. P. Frei and D. Stieger. The use of semantic links in [16] E. Garfield. Citation indexes for science. Science , 129, 1955. [17] G. Grimmett and D. Stir zaker. Probab ility and r andom [18] T. H. Haveliwala. Topic-sensitive pagerank: A [19] J. Kamps, G. Mishne, and M. de Rijke. Language models for [20] J. M. Kleinberg. Authoritative sources in a hyperlinked [21] O. Kurland and L. Lee. Pagerank without hyperlinks: [22] R. Larson. Bibliometrics of the world wide web: An [23] M. Marchiori. The quest for correct information on the Web: [24] F. Mathieu and M. Bouklit. The effect of the back button is a [25] D. S. Modha and W. S. Spangler. Clustering hypertext with [26] A. Y. Ng, A. X. Zheng, and M. I. Jordan. Stable algorithms [27] L. Page, S. Brin, R. Motwani, and T. Winograd. The [28] P. Pirolli, J. Pitkow, and R. Rao. Silk from a sow X  X  ear: [29] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma. A [30] M. Richardson and P. Domingos. The intelligent surfer: [31] S. Robertson. Threshold setting and performance [32] G. Salton. Associative document retrieval techniques using [33] G. Salton and C. Buckley. On the use of spreading activation [34] J. Savoy. Bayesian infer ence networks and spreading [35] J. Savoy. Ranking schemes in hybrid boolean systems: a new [36] A. Shakery and C. Zhai. Relevance propagation for topic [37] H. Small. Co-citation in the scientific literature: a new [38] R. Song, J. R. Wen, S. M. Shi, T. Y. Xin, G. M. abd Liu, [39] M. Sydow. Random surfer with back step. In Proceedings of [40] T. Tomiyama, K. Karoji, T. Kondo, Y. Kakuta, and T. Takagi. [41] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and [42] C. Zhai and J. Lafferty. Model-based feedback in the [43] Z. Zhou, Y. Guo, B. Wang, X. Cheng, H. Xu, and G. Zhang.
