 Chao Liu  X  craigliu@tencent.com Tencent Inc, 38 Haidian St, Beijing, 100080, P. R. China Yi-Min Wang ymwang@microsoft.com Microsoft Research, 1 Microsoft Way, Redmond, WA 98052, USA Recent advent of online crowdsourcing services ( e.g. , Amazon X  X  Mechanical Turk) excites the machine learn-ing community by making large amount of labeled data practical. Because of the low cost, crowdsourc-ing labels are usually given by anonymous lowly-paid non-experts, which sparks recent interest in recover-ing the true labels from noisy (or even malicious) labels (Whitehill et al., 2009; Welinder et al., 2010; Welinder &amp; Perona, 2010; Raykar et al., 2009). In this paper, we study the same problem of analyzing multi-ple ratings, but in quite a different setting. We are in a major Web search engine company, and train search rankers using human ratings on the rele-vance of tens of millions of (query, URL) pairs. As it is too risky to bet the search engine on crowdsourc-ing ratings, we have to carefully recruit human judges, rigorously train them, and continually monitor their quality during the work. Since these judges are well-trained and the rating task is considerably hard, the cost of each label becomes so expensive that even two ratings per (query, URL) pair are economically infea-sible: note that we have millions of pairs to rate and the number keeps increasing. Instead, we hope that a human judge would function satisfactorily once quali-fied, and each (query, URL) pair is only rated by one judge.
 A key component in controlling the judge quality is to blend a small set of  X  X onitoring X  (query, URL) pairs into judges X  regular work without their knowl-edge. This set of (query, URL) pairs are rated by all judges under monitoring. By analyzing the multiple ratings on (query, URL) pairs in this monitoring set, we hope to correctly score the quality of each judge, and more importantly, to gain insights into what con-fusions each judge makes so that we could plan tar-geted tutoring and revisions to the rating guidelines. Therefore, different from previous work that focuses on recovering the true labels from low-cost noisy la-bels, we are more interested in diagnostic information about judge confusions. For this reason, this paper emphasizes on probabilistic models that use a confu-sion matrix to quantify the competency of each judge. The DawidSkene model (Dawid &amp; Skene, 1979) is a good candidate for this purpose. It pioneers the  X  X rueLabel + Confusion X  paradigm: each item has a true label, and the rating each judge assigns to it is the true label obfuscated through the judge X  X  confusion matrix. Suppose the rating is on a K -level scale, a confusion matrix is a K  X  K matrix with the ( k, t ) element being the probability that the judge would rate the item t when the true label is k . Because each confusion matrix entails K  X  ( K  X  1) free parameters, DawidSkene possesses at least J  X  K  X  ( K  X  1) free parameters when J judges are involved. While this large number of free parame-ters renders big model capacity, it could also lead to overfitting easily, as will be seen in the experiments. For this reason, we propose a simplified model, called SingleConfusion , which forces all judges to share the same confusion matrix. It significantly reduces the number of free dimensions, but unfortunately proves too rigid for real-world data, i.e. , underfitting. As a tradeoff between the two, we further propose a hier-archical Bayesian model, called HybridConfusion , which allows each judge to have her own confusion matrix, but at the same time regularizes these ma-trices through Bayesian shrinkage. Effectively, the three models form a spectrum of probabilistic mod-els under the  X  X rueLabel + Confusion X  paradigm. In summary, we make the following two contributions in this paper: (1) we study the problem of analyzing multiple ratings with an emphasis on the diagnostic aspect of different models, which complements pre-vious work on recovering true labels from noisy rat-ings, (2) we generalize the well-known DawidSkene model to a spectrum of probabilistic models under the same paradigm, and show the newly proposed model, HybridConfusion , consistently outperforms DawidSkene and SingleConfusion on both syn-thetic and real-world data sets.
 The rest of this paper is organized as follows. We elaborate on the spectrum of probabilistic models in Sections 2, and report on experiments on synthetic and real-world data in Sections 3 and 4, respectively. With related work discussed in Section 5, Section 6 concludes this study. Suppose we have N items rated by J judges on a K -level metric. The metric can be either ordinal or cate-gorical. Let r i,j be the rating of the i th item assigned by the j th judge: r i,j  X  X  1 , 2 , , K } if the j th judge indeed rates the i th item and r i,j = 0 otherwise. We use t i  X  X  1 , 2 , , K } to denote the true label of the i th item. The competency of the j th judge is modeled by a confusion matrix  X  ( j )  X  R K  X  K with its ( k, t ) el-ement  X  ( j ) k,t being the probability that the j th judge will give a rating of t when t i = k . Collectively, we  X  = {  X  (1) ,  X  (2) , ,  X  ( J ) } , and use the hat notation ( c ) to denote the estimated value of the correspond-ing parameter.
 The spectrum of probabilistic models are plotted in Figure 1. We start with a brief review of the DawidSkene model (Figure 1(a)). It assumes that the true rating of each item is sampled from a multi-nomial distribution parameterized by  X  . Suppose the sampled true label is k , then the rating assigned by the j th judge is regarded as being sampled from an-other multinomial distribution parameterized by the Algorithm 1 : Inference for DawidSkene Input: Observed ratings r Output: Estimated b  X  , b  X  and b t
Initialize : Iterate until convergence :
Compute : k th row of the j th judge X  X  confusion matrix, namely,  X  ( j ) ( k, :), using the Matlab notation. The goal of the inference is to recover the model parameters (  X  and  X  ) and the true ratings t . To be self-contained and to compare with other models, the inference algorithm is reproduced in Algorithm 1 from (Dawid &amp; Skene, 1979). The particular way of initialization in Algo-rithm 1 is to be consistent with HybridConfusion , as to be discussed soon.
 DawidSkene model imposes no regularization on the individual confusion matrices, and hence possesses ( J  X  K + 1)  X  ( K  X  1) free parameters. In the first place, the large number of free parameters endows DawidSkene with high model capacity, but in the second place, it means DawidSkene could easily over-fit and suffer from not having enough data to fit. This observation, as supported by experiments in Sections 3 and 4, prompts us to reduce the capacity by reducing the number of free parameters. We therefore propose the SingleConfusion model, which forces all judges to have the same confusion matrix. Its graphical model and inference algorithm are presented in Figure 1(b) and Algorithm 2, respectively. SingleConfusion ef-fectively reduces the number of free parameters to K 2  X  1, but unfortunately, proves to be too rigid to model the variations across judges.
 We can view DawidSkene and SingleConfusion as two extremes under the same  X  X rueLabel + Confusion X  paradigm: one has too many parameters while the other has too few. We therefore propose the Algorithm 2 : Inference for SingleConfusion Input: Observed ratings r Output: Estimated b  X  , b  X  and b t
Initialize : Iterate until convergence :
Compute : HybridConfusion model, whose graphical model is depicted in Figure 1(c). HybridConfusion makes tradeoffs between DawidSkene and SingleConfusion by allowing each judge to still have her own confusion matrix but at the same time regularizing them through Bayesian shrinkage. Explicitly, HybridConfusion imposes that the k th row of all confusion matrices are sampled from a Dirichlet distribution parameterized by  X  k . Explicitly, let and it explains the choice of initialization in Algo-rithm 1 and 2. To complete the Bayesian model, a Dirichlet prior is imposed on  X  as well with parame-ter  X  . We used Markov Chain Monte Carlo (Gibbs sampling in particular) to perform the inference on HybridConfusion using the BUGS software pack-age (Lunn et al., 2000). We run 3 Gibbs sampler with 1000 burn-in and obtain 100 examples with a thin-ning interval of 10 in all experiments. After collect-ing the samples, we take the mode rating from the true label samples as the recovered true label, and the mean values of samples about  X  and  X  as the esti-mates b  X  and b  X  . As the K labels are mutually ex-changeable, all of the three models need to deal with the un-identifiability issue, and we tackle this using a method similar to (Stephens, 1999). In all the exper-iments,  X  = [1 , 1 , , 1] and  X  = 3 unless otherwise noted.
 The usage of  X  = 3 imposes a moderate prior that implies judges are more likely to rate correctly in gen-eral. Depending on the applications, one could put a stronger prior by using a bigger  X  or impose con-fusion patterns based on prior knowledge ( e.g. , a big-ger value for  X  1 , 3 if judges tend to misjudge  X 1 X  as  X 3 X ). In the case of ordinal rating, one could even put on a diagonal-decaying prior ( i.e. ,  X  i,j | j 1  X  i | &lt; | j 2  X  i | ), indicating the belief that a judge is more likely to confuse adjacent levels than nonad-jacent ones. In short, HybridConfusion provides a flexible way to encode different prior knowledge, but the best setting, as always, depends on the application and prior knowledge. In this section, we use synthetic data to examine the accuracy of different models in recovering the ground truth, i.e. , estimating the true model parameters and labels. We synthesize the data using the DawidSkene model for three judges ( J = 3) on a three-level met-ric ( K = 3). For convenience, we denote the three levels by  X  X  X ,  X  X  X , and  X  X  X , whose prior probabili-ties are assumed to be 0 . 05, 0 . 15 and 0 . 8, respectively. The three confusion matrices  X  are shown in Fig-ure 2. As can be seen, the three matrices are not cho-sen to favor any models, and in fact,  X  (1) even disfa-vors HybridConfusion because HybridConfusion never estimates any cells to be 0.
 Throughout the experiments in this section, we exam-ine the accuracy of recovering the ground truth for different models by varying the number of items. In-tuitively, the more rated items, the more accurately these models would recover the ground truth. The number of items is varied from 3000 down to 5, which renders a complete view of the model efficacy w.r.t. the data size. The data is synthesized in each run, and all the reported numbers are the average across 100 runs. We also include the majority voting (de-noted by MajorityVote ) algorithm in the compar-ison. MajorityVote takes the mode rating as the true rating, and breaks ties randomly when there are multiple modes. Once the true label is determined, b  X  and b  X  are obtained through simple counting. 3.1. Experimental Results Figure 3(a) plots the accuracy in recovering the true labels when the number of items decreases from 3000 to 5, using the default parameter  X  = 3. First, we observe that since the data is synthesized through DawidSkene , the best recovery is indeed achieved by DawidSkene , provided abundant data is avail-able. But when data becomes smaller, DawidSkene quickly deteriorates, as a result of overfitting. On the other hand, we see that HybridConfusion is con-sistently above 0.9, only being slightly weaker than DawidSkene when N = 2000 , 3000, and significantly outperforming DawidSkene otherwise. This consis-tent performance should be attributed to the Bayesian shrinkage. Third, we see MajorityVote is pretty flat, and effectively saturates after N  X  20. Finally, SingleConfusion is the weakest and twisted with MajorityVote when N  X  50. The inferior perfor-mance of SingleConfusion is likely due to its strict constraint to have all judges share the same confusion matrix whereas in fact they are quite different (see Figure 2). Figure 3(b) presents the same experiments with  X  = 10, which deliver similar messages. The accuracy of recovering  X  by HybridConfusion , DawidSkene , and SingleConfusion with varying numbers of items are plotted in Figure 4. Each bar represents the recovered  X   X  1 ,  X   X  2 and  X   X  3 for a given al-gorithm when a certain of number of items are given. The recovered  X  by MajorityVote is consistently around (0.1, 0.15, 0.75) regardless of the number of items, and hence not plotted in Figure 4. We see that both HybridConfusion and DawidSkene can ex-actly recover the true  X  with enough data whereas SingleConfusion misses the target even when 3000 items are provided.
 Finally, Figure 5 plots the accuracy in recovering the confusion matrices, as measured by the Mean Absolute Error (MAE), Figure 5(a) plots the MAE of different models in recovering  X  (1) when the number of items varies. Again, we have seen that DawidSkene gives the best recovery, but only when enough ratings are available, and it clearly deteriorates with fewer and fewer data. HybridConfusion , on the other hand, is very accurate, and barely decays when N gets smaller. SingleConfusion remains the worst per-forming model but MajorityVote appears very competitive. The same trend on relative performance of different models is confirmed in Figures 5(b) and 5(c) as well.
 As a short conclusion, we observe that HybridConfusion is comparable to DawidSkene when abundant data is available even if the data is synthesized by DawidSkene . DawidSkene quickly deteriorates when data becomes scarce, but HybridConfusion remains very accurate in recovering the ground truth, which clearly demon-strates the efficacy of Bayesian shrinkage. But, to be fair, DawidSkene is about 4 times faster than HybridConfusion . In this section, we report on the experimental results on a real-world data set that motivates the study. As explained in Section 1, we use a set of  X  X onitoring X  (query, URL) pairs to gauge judge quality. For each (query, URL) pair, a judge is expected to assign it into one of the five categories Bad, Fair, Good, Excellent, Perfect (denoted by 1 to 5, respectively), based on her assessment of the relevance according to a set of writ-ten guidelines. This  X  X onitoring X  set is chosen to be  X  X ard X  to maximally differentiate judge qualities. A  X  X uper-judge X , who supposedly best understands the judgment guidelines, gives the gold rating to each pair in the set. The quality of each judge is then determined based on the deviation from gold ratings. Because only a few super-judges are available, quality calibra-tion based on existing gold labels usually suffers from data scarcity. For example, a lot of cells in the confu-sion matrices would be zero. Therefore, in this section, we explore to what extent each model would recover the gold rating based on individual ratings from reg-ular judges, and furthermore whether the estimated parameters by each model could help further lift the prediction accuracy.
 One monitoring set consists of 6008 ratings from 148 judges on 48 (query, url) pairs, as visualized in Fig-ure 6(a). Each row corresponds to a distinct (query, url) pair, and each column to a distinct human judge. For eligibility, (query, url) pairs and judges are sorted based on the average rating across judges and items, respectively. The gold labels are listed in the last col-umn. There are only 6 colors of interest in the color-bar: blue for 1 ( Bad ) to red for 5 ( Perfect ) with white for 0 denoting unrated item by judges. As can be seen, some judges only rated a few (query, URL) pairs, and they are generally new judges hired into the system. They are not excluded in order to keep the fidelity to real-world data.
 We report experiment results using the average across 100 runs. In each run, we first sample R ratings for each item, and then randomly partition the 48 (query, URL) pairs into training and testing sets with a ra-tio of 2:1. We compare the accuracy of predicting the gold label on the testing set with different models. In order to see if any models could recover the unknown true parameters to some extent from the training set, we test the prediction accuracy in two different modes. The first is the  X  X emoryless X  mode where the true la-bel is predicted solely based on the testing set, and the second is the  X  X emory X  mode where each (query, URL) pair is predicted using the estimated parameters obtained from the training set. Explicitly, the  X  X em-ory X  mode computes the posterior of the rating based on b  X  , b  X  and r i using p ( z i = k | r i ; b  X  , b  X  ) = and assigns the i th item to the most probable rating. The  X  X emoryless X  model corresponds to the cold-start scenario where no knowledge about the judges is avail-able, whereas the  X  X emory X  mode mimics the case where the confusion matrices and label distributions are known a priori from previous data. However, in neither mode is the gold label of the training set used, and the gold label of testing data is merely used in measuring the prediction accuracy. The goal of this study aims to comparing the recovery accuracy of the true label using different models, and to what extent the estimated parameters could help; the goal is not to build a state-of-the-art prediction model for true labels.
 Figures 6(b) and 6(c) plot the prediction accuracy w.r.t. the number of ratings per item in the two modes, respectively. In the  X  X emoryless X  mode (Figure 6(b)), all models deteriorate as fewer ratings are available to each item, and DawidSkene deteriorates the most, yet another piece of evidence of overfitting. In con-trast, HybridConfusion performs very well and does not deteriorate much even when R = 5. Surprisingly, MajorityVote is very close to HybridConfusion , and the performance gap is no longer as wide as that shown in Figure 3. A likely reason for the strong per-formance of MajorityVote is that our judges are all well-trained and understand that their quality is continually monitored; hence their consensus is usu-ally much stronger than that between crowdsourcing judges. Nevertheless, we still see HybridConfusion beats MajorityVote with a small but consistent margin with 30 or more ratings per item ( p -value &lt; 0 . 05 using one-sided Fisher Sign Test). Figure 6(c) compares the performance in the  X  X em-ory X  mode, where we see that HybridConfusion sig-nificantly beats all other three models and is visibly better than HybridConfusion in the  X  X emoryless X  mode. This suggests that HybridConfusion indeed recovers the true parameters to some extent, which helps lift the prediction accuracy. In contrast, we see the accuracy of MajorityVote drops significantly from the  X  X emoryless X  mode, indicating the inferior quality of the estimated parameters due to data spar-sity.
 Finally, we peek into the estimated confusion matri-ces by DawidSkene and HybridConfusion to un-derstand what the estimates look like. Figure 7 plots the average confusion matrices for DawidSkene and HybridConfusion , as averaged across all judges over the 100 runs with R = 5. At the first glimpse, we see the confusion matrix from DawidSkene is much more chaotic than that from HybridConfusion , which vi-sually demonstrates the efficacy of Bayesian shrink-age in HybridConfusion . Within each cell, we also mark out the probability value with the standard de-viation in the parenthesis. Clearly, we see that confu-sion matrices from DawidSkene are more divergent from each other (bigger standard deviations as shown in Figure 7(a)). In contrast, the confusion matrix from HybridConfusion (Figure 7(b)) exhibits nat-ural diagonal-decaying phenomena, and the standard deviations are generally much smaller. Now that our judges are well-trained, we believe the latter is closer to the truth than the former, although we have no way to solicit the ground truth. The need of large amount of labeled data, as inher-ent in many machine learning algorithms and appli-cations, cultivates the advent of online crowdsourcing services ( e.g. , Amazon X  X  MechanicTurk). Readers in-terested in a detailed survey in this area are referred to (Ipeirotis &amp; Paritosh, 2011). While the low cost of crowdsourcing renders multiple labels on numer-ous items practically feasible, it also calls for princi-pled approaches to distilling true labels from the less-than-expert ratings, among other issues (Sheng et al., 2008). Because of the importance of this problem, recent years have seen increasing interests in this problem, e.g. , (Whitehill et al., 2009; Welinder et al., 2010; Welinder &amp; Perona, 2010; Raykar et al., 2009). Specifically, Whitehill et al. (2009) models the prob-ability that a judge would rate an item correctly as a logistic function of the product between the qual-ity of the judge and the difficulty of the item. This leads to a model that not only recovers the true la-bel, but estimates the judge quality (represented by a single number) and item difficulty at the same time. But since the model deals with the probability that a judge hits the right rating, it does not provide de-tailed diagnostic information about judge confusions. (Welinder et al., 2010; Welinder &amp; Perona, 2010) later generalizes (Whitehill et al., 2009) by introducing a high-dimensional concept of item difficulty, and shows a small but consistent improvement, but again it fails to provide the confusion matrices. In this paper, we focus on models under the  X  X rueLabel + Confusion X  paradigm for diagnostic insights into judge confusion. In addition to the different focus, the judges in our setting are also different from previous work: our in-house judges are well trained, decently paid, and be-nign in general, which directly contrasts to the anony-mous non-expert or even malicious judges in crowd-sourcing services. This entails two consequences: first, we do not worry about malicious judges, and second, MajorityVote becomes much more competitive, al-though it is still inferior to our proposed model. The spectrum of models as presented here are not re-stricted to ratings from human judges, and in fact they can be effectively used to combine any ratings from any sources, be it human judges or predictive models. Previously, Ghahramani and Kim presented some preliminary results using a model similar to HybridConfusion (Ghahramani &amp; chul Kim, 2003), but failed to examine how the performance varies when the numbers of items and judges change. This paper fills in the gap, and to the best of our knowledge, this is the first piece of work generalizing DawidSkene to a spectrum of models with a comparative study of their pros and cons. Finally, another piece of work worth mentioning is (Raykar et al., 2009), which per-forms supervised learning with the true labels recov-ered as a by-product. The method is shown superior to the conventional two-stage alternative ( i.e. , training models after recovering the true labels, as practiced by (Smyth et al., 1994)). Their focus is on building ac-curate predictive models rather than diagnosing judge qualities. In this paper, we generalize the DawidSkene model into a spectrum of probabilistic models under the  X  X rueLabel+Confusion X  paradigm. Our proposed models, SingleConfusion and HybridConfusion , complement the well-known DawidSkene model to overcome its overfitting drawbacks. We study their pros and cons using both synthetic and real-world data, and demonstrate the advantages of HybridConfusion in various settings. In the future, we would optimize the judgement pipeline based on the recovered confusion matrices, e.g. , through targeted training and guideline revisions.
 We would like to thank John Platt for the helpful dis-cussion, and the anonymous reviewers for the insight-ful and constructive comments.
 Dawid, A. P. and Skene, A. M. Maximum likelihood estimation of observer error-rates using the em al-gorithm. Applied Statistics , 28(1):20 X 28, 1979. Ghahramani, Zoubin and chul Kim, Hyun. Bayesian classifier combination. Technical report, University College London, 2003.
 Ipeirotis, Panagiotis G. and Paritosh, Praveen K.
Managing crowdsourced human computation: a tu-torial. In WWW , pp. 287 X 288, 2011.
 Lunn, David J., Thomas, Andrew, Best, Nicky, and
Spiegelhalter, David. Winbugs a bayesian modelling framework: Concepts, structure, and extensibility. Statistics and Computing , 10:325 X 337, 10 2000. Raykar, V. C., Yu, S., Zhao, L .H., Jerebko, A., Florin,
C., Valadez, G. H., Bogoni, L., and Moy, L. Su-pervised learning from multiple experts: Whom to trust when everyone lies a bit. In International Con-ference on Machine Learning (ICML) , pp. 889 X 896, June 2009.
 Sheng, Victor S., Provost, Foster, and Ipeirotis, Pana-giotis G. Get another label? improving data quality and data mining using multiple, noisy labelers. In KDD , pp. 614 X 622, 2008.
 Smyth, Padhraic, Fayyad, Usama M., Burl, Michael C., Perona, Pietro, and Baldi, Pierre.
Inferring ground truth from subjective labelling of venus images. In Neural Information Processing Systems Conference (NIPS) , pp. 1085 X 1092, 1994. Stephens, Matthew. Dealing with multimodal posteri-ors and non-identifiability in mixture models. Tech-nical report, Department of Statistics, University of Oxford, 1999.
 Welinder, P. and Perona, P. Online crowdsourcing: rating annotators and obtaining cost-effective labels. In CVPR , pp. 0 X 1, 2010.
 Welinder, Peter, Branson, Steve, Belongie, Serge, and
Perona, Pietro. The multidimensional wisdom of crowds. In Neural Information Processing Systems Conference (NIPS) , pp. 0 X 1, 2010.
 Whitehill, Jacob, Ruvolo, Paul, fan Wu, Ting,
Bergsma, Jacob, and Movellan, Javier. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in
Neural Information Processing Systems (NIPS) , pp.
