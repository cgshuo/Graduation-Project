 Latent semantic analysis (LSA) has been intensively stud-ied because of its wide application to Information Retrieval and Natural Language Processing. Yet, traditional models such as LSA only examine one (current) version of the doc-ument. However, due to the recent proliferation of collabo-ratively generated content such as threads in online forums, Collaborative Question Answering archives, Wikipedia, and other versioned content, the document generation process is now directly observable. In this study, we explore how this additional temporal information about the document evo-lution could be used to enhance the identification of latent document topics. Specifically, we propose a novel hidden-topic modeling algorithm, temporal Latent Semantic Anal-ysis (tLSA), which elegantly extends LSA to modeling doc-ument revision history using tensor decomposition. Our ex-periments show that tLSA outperforms LSA on word relat-edness estimation using benchmark data, and explore appli-cations of tLSA for other tasks.
 Algorithm, Experimentation
Mapping words to semantic concepts has been an active area of research in Information Retrieval (IR) and Natu-ral Language Processing (NLP). Many approaches, such as LSA [3], pLSA, and LDA, have been introduced to identify semantic concepts by analyzing static documents. However, with the proliferation of Collaboratively Generated Content (CGC), such as the questions and answers posted in Ya-hoo Answers, blog posts and comments, as well as pages in Wikipedia, additional temporal information about the doc-ument evolution is available. This information has been shown to be helpful for fundamental IR and NLP tasks, such as ranking [1] and word similarity analysis [4]. It is then natural to ask, whether temporal document evolution information, such as revision history, can be used to enhance the identification of semantic concepts, by extending LSA to take advantage of the temporal document evolution.
We propose a new hidden-topic modeling algorithm, tLSA, to utilize the content authoring process of CGC. tLSA rep-resents the collection as a tensor , with the dimensions cor-responding to the documents, words, and time, respectively, and uses tensor decomposition to identify latent concepts in a process inspired by LSA. We show that incorporating tem-poral information in tLSA can be valuable for word similar-ity computation, which could in turn improve search quality over CGC. We also explore another application of tLSA, tracing the change of hidden-topic importance over time, which cannot be accomplished by static analysis.

Specifically, our contributions include: Introduction of a novel temporal latent semantic analysis method, tLSA (Sec-tion 2); Improved performance over LSA for predicting word relatedness, and an exploration of applying tLSA for tem-poral topic evolution analysis (Section 3).
Our tLSA algorithm extends LSA, which we briefly re-view, and use to provide intuition for our approach. A word-by-document matrix can be built to represent the collection of static documents. Matrix decomposition algorithms, such as Singular Value Decomposition (SVD) or Non-negative Matrix Factorization (NMF), can be performed onto the ma-trix to obtain semantic representation. By considering the generation process of the collection, we have time dimension upon the flat word-by-document matrix. Therefore, the ma-trix becomes a word-document-time cuboid, more precisely, an 3rd-order tensor (as shown in Figure 1). Intuitively, ten-sor decomposition algorithms could be applied to this 3rd-order tensor, with the goal of identifying important latent topics similar to the idea of LSA or SVD.

LSA decomposes the word-by-document matrix into the sum of rank-1 matrices, therefore captures the correlation/co-occurrence information between words. CANDECOMP/PA-RAFAC (CP) decomposition introduced in [2] decomposes a tensor in the same way. CP-decomposition factorizes word-document-time tensor into the sum of weighted rank-1 ten-sors. Each rank-1 tensor is associated with one hidden-topic and can be rewritten as the outer product of three vectors (word, document and time vector respectively). More for-mally, for CP-decomposition, we have:
T is the 3rd-order tensor,  X  r is the r-th  X  X ingular value X , w , d r and t r are the word, document and time vector re-spectively. R is the number of rank-1 tensors used to recover the original tensor T . We can also construct a matrix by ag-gregating the vectors, for example, [ t 1 , t 2 , . . . , t illustrated in Figure 1. Tensor T is decomposed into three Figure 1: Ten sor decomposition for the word-document-matrices and a core tensor, the core tensor  X  is diagonal and the non-zero entries have the value of  X  1 ,  X  2 , . . . ,  X 
The original CP-decomposition could be done by itera-tively solving a least square problem. However, we notice that the time dimension is different from document and word dimension. The continuity of time dimension should be con-sidered when decomposing the tensor, while document and word dimension does not have this issue. Intuitively, two ad-jacent time slices in the tensor should have similar weights if we assume the weight of hidden-topic consistently changes over time. The loss function of time vector is redefined as:
Basically, loss function (2) adds regularization on time di-mension when solving the least square problem, resulting in a smooth time vector. e T is the recovered tensor as defined in (1). L is the Laplacian matrix which penalizes the difference between  X  X eighborhoods X  along the time vector t . to evaluate tLSA. To construct the tensor, we retrieve top 100 articles through Wikipedia default search engine for each word in WS-353 and 39183 articles in total are obtained. For each retrieved article, the revisions from Jan 1st, 2008 till Jan 31st, 2011 are collected. We take weekly snapshot for those articles since Jan 1st, 2008, resulting in 161 time slices. The baseline method, LSA, uses the last snapshot of this col-lection only. Word relatedness computed by LSA and tLSA are compared on WS-353. Pearson Correlation is employed here to estimate the performance of these models. tLSA with time regularization 0.4593 (+4.4%) tLSA without time regularization 0.3752 Table 1: Cor relation with WS-353 word relatedness.
Table 1 shows that tLSA with time regularization per-forms better than LSA by 4.4% on word relatedness esti-mation. However, the words in the WS-353 dataset are rel-atively common, and primarily related to static concepts, such as  X  X ar X  and  X  X ove X . This may explain the relatively small absolute improvement of tLSA over LSA.

We now explore another application of tLSA -tracing hidden-topic evolution and importance over time (which is not possible with LSA or other static analysis methods). Furthermore, to show the applicability of tLSA, we run this algorithm on another corpus, 20-year New York Times ar-ticles. A section in this corpus is considered a  X  X ocument X , Figure 2: Per iodic topic: A topic of weekly news asso-Figure 3: Eve nt associated topic: A topic of U.S. Senate and all the articles of that section on a single day form a  X  X e-vision X  of this  X  X ocument X . This pseudo-document reflects the daily changing topics while these topics are connected by the theme of the section.

After the CP-decomposition, a time-by-topic matrix is ob-tained and the topic trend can be observed. We find two interesting patterns in the topic trend of New York Times corpus. One is periodic pattern as shown in Figure 2. The period of this topic is exact one week. The representative words for this topic are  X  X eek X ,  X  X onday X  and  X  X hurch X  etc. We can interpret this topic as weekly news, such as weekly report and weekly events. The other topic trend pattern is  X  X vent driven X  topic as shown in Figure 3. Nov 7th, 2006 is the date of Unite States Senate Election and the Democrats regained control of the House. The top words for the topic in Figure 3 are  X  X enate X  and  X  X ontrol X  etc.
In this study, we introduced a novel algorithm, tLSA, which extends latent semantic analysis by incorporating tem-poral information using tensor decomposition. The results of our preliminary experiments using tLSA for word relat-edness estimation and for analyzing topic evolution in col-lections over time are encouraging and suggest promising directions for future work.
