 Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) typically employ a pipeline of two stages: a syntactic parser for the source lan-guage, and a decoder that translates source-language trees into target-language strings. Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chinese-to-English translation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest , that is, a representation of a whole distribution over possible parse trees (Mi et al., 2008). In this paper, we investigate two hypotheses to explain why.
One hypothesis is that forest-to-string translation selects worse parses . Although syntax often helps translation, there may be situations where syntax, or at least syntax in the way that our models use it, can impose constraints that are too rigid for good-quality translation (Liu et al., 2007; Zhang et al., 2008). For example, suppose that a tree-to-string system encounters the following correct tree (only partial bracketing shown): (1) [ NP j  X   X ngj `  X  Suppose further that the model has never seen this phrase before, although it has seen the subphrase z  X engzh X ang de s`ud`u  X  X rowth rate X . Because this sub-phrase is not a syntactic unit in sentence (1), the sys-tem will be unable to translate it. But a forest-to-string system would be free to choose another (in-correct but plausible) bracketing: (2) j  X   X ngj `  X  and successfully translate it using rules learned from observed data.

The other hypothesis is that forest-to-string trans-lation selects better parses . For example, if a Chi-it might consider two structures: (3) [ VP c  X  anji  X  a (4) c  X  anji  X  a The two structures have two di ff erent translations into English, shown above. While the parser prefers structure (3), an n -gram language model would eas-ily prefer translation (4) and, therefore, its corre-sponding Chinese parse.
Previous work has shown that an observed target-language translation can improve parsing of source-language text (Burkett and Klein, 2008; Huang et al., 2009), but to our knowledge, only Chen et al. (2011) have explored the case where the target-language translation is unobserved.

Below, we carry out experiments to test these two hypotheses. We measure the accuracy (using labeled-bracket F1) of the parses that the translation model selects, and find that they are worse than the parses selected by the parser. Our basic conclusion, then, is that the parses that help translation (accord-ing to B leu ) are, on average, worse parses. That is, forest-to-string translation hurts parsing.
But there is a twist. Neither labeled-bracket F1 nor B leu is a perfect metric of the phenomena it is meant to measure, and our translation system is op-timized to maximize B leu . If we optimize our sys-tem to maximize labeled-bracket F1 instead, we find that our translation system selects parses that score higher than the baseline parser X  X . That is, forest-to-string translation can help parsing. We provide here only a cursory overview of tree-to-string and forest-to-string translation. For more details, the reader is referred to the original papers describing them (Liu et al., 2006; Mi et al., 2008).
Figure 1a illustrates the tree-to-string transla-tion pipeline. The parser stage can be any phrase-structure parser; it computes a parse for each source-language string. The decoder stage translates the source-language tree into a target-language string, using a synchronous tree-substitution grammar.
In forest-to-string translation (Figure 1b), the parser outputs a forest of possible parses of each source-language string. The decoder uses the same rules as in tree-to-string translation, but is free to se-lect any of the trees contained in the parse forest. The simplest experiment to carry out is to exam-ine the parses actually selected by the decoder, and see whether they are better or worse than the parses selected by the parser. If they are worse, this sup-ports the hypothesis that syntax can hurt translation. If they are better, we can conclude that translation can help parsing. In this initial experiment, we find that the former is the case. 3.1 Setup The baseline parser is the Charniak parser (Char-niak, 2000). We trained it on the Chinese Treebank (CTB) 5.1, split as shown in Table 1, following Duan et al. (2007). 1 The parser outputs a parse forest annotated with head words and other information. Since the decoder does not use these annotations, we use the max-rule algorithm (Petrov et al., 2006) to (approximately) sum them out. As a side bene-fit, this improves parsing accuracy from 77.76% to 78.42% F1. The weight of a hyperedge in this for-est is its posterior probability, given the input string. We retain these weights as a feature in the translation model.

The decoder stage is a forest-to-string system (Liu et al., 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are listed in Ta-ble 1. We generated word alignments with GIZA ++ and symmetrized them using the grow-diag-final-and heuristic. We parsed the Chinese side using the Charniak parser as described above, and per-formed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. We used the same features as Mi and Huang (2008). The language model was a trigram model with modi-fied Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained on the target side of the training data. We used minimum-error-rate (MER) training to optimize the feature weights (Och, 2003) to maximize B leu .

At decoding time, we select the best derivation and extract its source tree. In principle, we ought to sum over all derivations for each source tree; but the approximations that we tried ( n -best list crunch-ing, max-rule decoding, minimum Bayes risk) did not appear to help. 3.2 Results Table 2 shows the main results of our experiments. In the second and third line, we see that the forest-to-string system outperforms the tree-to-string sys-tem by 1.53 B leu , consistent with previously pub-lished results (Mi et al., 2008; Zhang et al., 2009). However, we also find that the trees selected by the forest-to-string system score much lower according to labeled-bracket F1. This suggests that the reason the forest-to-string system is able to generate better translations is that it can soften the constraints im-posed by the syntax of the source language. We have found that better translations can be ob-tained by settling for worse parses. However, trans-lation accuracy is measured using B leu and pars-ing accuracy is measured using labeled-bracket F1, and neither of these is a perfect metric of the phe-nomenon it is meant to measure. Moreover, we op-timized the translation model in order to maximize B leu . It is known that when MER training is used to optimize one translation metric, other translation metrics su ff er (Och, 2003); much more, then, can we expect that optimizing B leu will cause labeled-bracket F1 to su ff er. In this section, we try optimiz-ing labeled-bracket F1, and find that, in this case, the translation model does indeed select parses that are better on average. 4.1 Setup MER training with labeled-bracket F1 as an objec-tive function is straightforward. At each iteration of MER training, we run the parser and decoder over the CTB dev set to generate an n -best list of possible translation derivations (Huang and Chiang, 2005). For each derivation, we extract its Chinese parse tree and compute the number of brackets guessed and the number matched against the gold-standard parse tree. A trivial modification of the MER trainer then optimizes the feature weights to maximize labeled-bracket F1.

A technical challenge that arises is ensuring di-versity in the n -best lists. The MER trainer re-quires that each list contain enough unique transla-tions (when maximizing B leu ) or source trees (when maximizing labeled-bracket F1). However, because one source tree may lead to many translation deriva-tions, the n -best list may contain only a few unique source trees, or in the extreme case, the derivations may all have the same source tree. We use a variant of the n -best algorithm that allows e ffi cient genera-tion of equivalence classes of derivations (Huang et al., 2006). The standard algorithm works by gener-ating, at each node of the forest, a list of the best subderivations at that node; the variant drops a sub-derivation if it has the same source tree as a higher-scoring subderivation. 4.2 Results The last line of Table 2 shows the results of this second experiment. The system trained to opti-mize labeled-bracket F1 ( max-F1 ) obtains a much lower B leu score than the one trained to maximize B leu ( max-B leu ) X  X nsurprisingly, because a single source-side parse can yield many di ff erent transla-tions, but the objective function scores them equally. What is more interesting is that the max-F1 system obtains a higher F1 score, not only compared with the max-B leu system but also the original parser.
We then tried various settings to investigate what factors a ff ect parsing performance. First, we found that increasing the maximum rule height increases F1 further (Table 3a).

One of the motivations of our method is that bilin-gual information (especially the language model) can help disambiguate the source side structures. To test this, we varied the size of the corpus used to train the language model (keeping a maximum rule height of 5 from the previous experiment). The 13M-line language model adds the Xinhua portion of Giga-word 3. In Table 3b we see that the parsing perfor-mance does increase with the language model size, with the largest language model yielding a net im-provement of 0.82 over the baseline parser.
To test further the importance of bilingual infor-mation, we compared against a system built only from the Chinese side of the parallel text (with each word aligned to itself). We removed all features that use bilingual information, retaining only the parser probability and the phrase penalty. In their place we added a new feature, the probability of a rule X  X  source side tree given its root label, which is essen-tially the same model used in Data-Oriented Parsing (Bod, 1992). Table 3c shows that this system still outperforms the original parser. In other words, part of the gain is not attributable to translation, but ad-ditional source-side context and data that the trans-lation model happens to capture.

Finally, we varied the size of the parallel text (keeping a maximum rule height of 5 and the largest language model) and found that, as expected, pars-ing performance correlates with parallel data size (Table 3d). We set out to investigate why forest-to-string trans-lation outperforms tree-to-string translation. By comparing their performance as Chinese parsers, we found that forest-to-string translation sacrifices pars-ing accuracy, suggesting that forest-to-string trans-lation works by overriding constraints imposed by syntax. But when we optimized the system to max-imize labeled-bracket F1, we found that, in fact, forest-to-string translation is able to achieve higher accuracy, by 0.82 F1%, than the baseline Chinese parser, demonstrating that, to a certain extent, forest-to-string translation is able to correct parsing errors. We are grateful to the anonymous reviewers for their helpful comments. This research was sup-ported in part by DARPA under contract DOI-NBC D11AP00244.
 Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) typically employ a pipeline of two stages: a syntactic parser for the source lan-guage, and a decoder that translates source-language trees into target-language strings. Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chinese-to-English translation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest , that is, a representation of a whole distribution over possible parse trees (Mi et al., 2008). In this paper, we investigate two hypotheses to explain why.
One hypothesis is that forest-to-string translation selects worse parses . Although syntax often helps translation, there may be situations where syntax, or at least syntax in the way that our models use it, can impose constraints that are too rigid for good-quality translation (Liu et al., 2007; Zhang et al., 2008). For example, suppose that a tree-to-string system encounters the following correct tree (only partial bracketing shown): (1) [ NP j  X   X ngj `  X  Suppose further that the model has never seen this phrase before, although it has seen the subphrase z  X engzh X ang de s`ud`u  X  X rowth rate X . Because this sub-phrase is not a syntactic unit in sentence (1), the sys-tem will be unable to translate it. But a forest-to-string system would be free to choose another (in-correct but plausible) bracketing: (2) j  X   X ngj `  X  and successfully translate it using rules learned from observed data.

The other hypothesis is that forest-to-string trans-lation selects better parses . For example, if a Chi-it might consider two structures: (3) [ VP c  X  anji  X  a (4) c  X  anji  X  a The two structures have two di erent translations into English, shown above. While the parser prefers structure (3), an n -gram language model would eas-ily prefer translation (4) and, therefore, its corre-sponding Chinese parse.
Previous work has shown that an observed target-language translation can improve parsing of source-language text (Burkett and Klein, 2008; Huang et al., 2009), but to our knowledge, only Chen et al. (2011) have explored the case where the target-language translation is unobserved.

Below, we carry out experiments to test these two hypotheses. We measure the accuracy (using labeled-bracket F1) of the parses that the translation model selects, and find that they are worse than the parses selected by the parser. Our basic conclusion, then, is that the parses that help translation (accord-ing to B leu ) are, on average, worse parses. That is, forest-to-string translation hurts parsing.
But there is a twist. Neither labeled-bracket F1 nor B leu is a perfect metric of the phenomena it is meant to measure, and our translation system is op-timized to maximize B leu . If we optimize our sys-tem to maximize labeled-bracket F1 instead, we find that our translation system selects parses that score higher than the baseline parser X  X . That is, forest-to-string translation can help parsing. We provide here only a cursory overview of tree-to-string and forest-to-string translation. For more details, the reader is referred to the original papers describing them (Liu et al., 2006; Mi et al., 2008).
Figure 1a illustrates the tree-to-string transla-tion pipeline. The parser stage can be any phrase-structure parser; it computes a parse for each source-language string. The decoder stage translates the source-language tree into a target-language string, using a synchronous tree-substitution grammar.
In forest-to-string translation (Figure 1b), the parser outputs a forest of possible parses of each source-language string. The decoder uses the same rules as in tree-to-string translation, but is free to se-lect any of the trees contained in the parse forest. The simplest experiment to carry out is to exam-ine the parses actually selected by the decoder, and see whether they are better or worse than the parses selected by the parser. If they are worse, this sup-ports the hypothesis that syntax can hurt translation. If they are better, we can conclude that translation can help parsing. In this initial experiment, we find that the former is the case. 3.1 Setup The baseline parser is the Charniak parser (Char-niak, 2000). We trained it on the Chinese Treebank (CTB) 5.1, split as shown in Table 1, following Duan et al. (2007). 1 The parser outputs a parse forest annotated with head words and other information. Since the decoder does not use these annotations, we use the max-rule algorithm (Petrov et al., 2006) to (approximately) sum them out. As a side bene-fit, this improves parsing accuracy from 77.76% to 78.42% F1. The weight of a hyperedge in this for-est is its posterior probability, given the input string. We retain these weights as a feature in the translation model.

The decoder stage is a forest-to-string system (Liu et al., 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are listed in Ta-ble 1. We generated word alignments with GIZA ++ and symmetrized them using the grow-diag-final-and heuristic. We parsed the Chinese side using the Charniak parser as described above, and per-formed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. We used the same features as Mi and Huang (2008). The language model was a trigram model with modi-fied Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained on the target side of the training data. We used minimum-error-rate (MER) training to optimize the feature weights (Och, 2003) to maximize B leu .

At decoding time, we select the best derivation and extract its source tree. In principle, we ought to sum over all derivations for each source tree; but the approximations that we tried ( n -best list crunch-ing, max-rule decoding, minimum Bayes risk) did not appear to help. 3.2 Results Table 2 shows the main results of our experiments. In the second and third line, we see that the forest-to-string system outperforms the tree-to-string sys-tem by 1.53 B leu , consistent with previously pub-lished results (Mi et al., 2008; Zhang et al., 2009). However, we also find that the trees selected by the forest-to-string system score much lower according to labeled-bracket F1. This suggests that the reason the forest-to-string system is able to generate better translations is that it can soften the constraints im-posed by the syntax of the source language. We have found that better translations can be ob-tained by settling for worse parses. However, trans-lation accuracy is measured using B leu and pars-ing accuracy is measured using labeled-bracket F1, and neither of these is a perfect metric of the phe-nomenon it is meant to measure. Moreover, we op-timized the translation model in order to maximize B leu . It is known that when MER training is used to optimize one translation metric, other translation metrics su er (Och, 2003); much more, then, can we expect that optimizing B leu will cause labeled-bracket F1 to su er. In this section, we try optimiz-ing labeled-bracket F1, and find that, in this case, the translation model does indeed select parses that are better on average. 4.1 Setup MER training with labeled-bracket F1 as an objec-tive function is straightforward. At each iteration of MER training, we run the parser and decoder over the CTB dev set to generate an n -best list of possible translation derivations (Huang and Chiang, 2005). For each derivation, we extract its Chinese parse tree and compute the number of brackets guessed and the number matched against the gold-standard parse tree. A trivial modification of the MER trainer then optimizes the feature weights to maximize labeled-bracket F1.

A technical challenge that arises is ensuring di-versity in the n -best lists. The MER trainer re-quires that each list contain enough unique transla-tions (when maximizing B leu ) or source trees (when maximizing labeled-bracket F1). However, because one source tree may lead to many translation deriva-tions, the n -best list may contain only a few unique source trees, or in the extreme case, the derivations may all have the same source tree. We use a variant of the n -best algorithm that allows e cient genera-tion of equivalence classes of derivations (Huang et al., 2006). The standard algorithm works by gener-ating, at each node of the forest, a list of the best subderivations at that node; the variant drops a sub-derivation if it has the same source tree as a higher-scoring subderivation.
 4.2 Results The last line of Table 2 shows the results of this second experiment. The system trained to opti-mize labeled-bracket F1 ( max-F1 ) obtains a much lower B leu score than the one trained to maximize B leu ( max-B leu ) X  X nsurprisingly, because a single source-side parse can yield many di erent transla-tions, but the objective function scores them equally. What is more interesting is that the max-F1 system obtains a higher F1 score, not only compared with the max-B leu system but also the original parser.
We then tried various settings to investigate what factors a ect parsing performance. First, we found that increasing the maximum rule height increases F1 further (Table 3a).

One of the motivations of our method is that bilin-gual information (especially the language model) can help disambiguate the source side structures. To test this, we varied the size of the corpus used to train the language model (keeping a maximum rule height of 5 from the previous experiment). The 13M-line language model adds the Xinhua portion of Giga-word 3. In Table 3b we see that the parsing perfor-mance does increase with the language model size, with the largest language model yielding a net im-provement of 0.82 over the baseline parser.
To test further the importance of bilingual infor-mation, we compared against a system built only from the Chinese side of the parallel text (with each word aligned to itself). We removed all features that use bilingual information, retaining only the parser probability and the phrase penalty. In their place we added a new feature, the probability of a rule X  X  source side tree given its root label, which is essen-tially the same model used in Data-Oriented Parsing (Bod, 1992). Table 3c shows that this system still outperforms the original parser. In other words, part of the gain is not attributable to translation, but ad-ditional source-side context and data that the trans-lation model happens to capture.

Finally, we varied the size of the parallel text (keeping a maximum rule height of 5 and the largest language model) and found that, as expected, pars-ing performance correlates with parallel data size (Table 3d). We set out to investigate why forest-to-string trans-lation outperforms tree-to-string translation. By comparing their performance as Chinese parsers, we found that forest-to-string translation sacrifices pars-ing accuracy, suggesting that forest-to-string trans-lation works by overriding constraints imposed by syntax. But when we optimized the system to max-imize labeled-bracket F1, we found that, in fact, forest-to-string translation is able to achieve higher accuracy, by 0.82 F1%, than the baseline Chinese parser, demonstrating that, to a certain extent, forest-to-string translation is able to correct parsing errors. We are grateful to the anonymous reviewers for their helpful comments. This research was sup-ported in part by DARPA under contract DOI-NBC D11AP00244.

