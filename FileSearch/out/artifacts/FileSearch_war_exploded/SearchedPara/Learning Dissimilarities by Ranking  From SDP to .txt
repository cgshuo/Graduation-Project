 Hua Ouyang houyang@cc.gatech.edu College of Computing, Georgia Institute of Technology Alex Gray agray@cc.gatech.edu College of Computing, Georgia Institute of Technology Ranking or sometimes referred as ordinal regression , is a statistical learning problem which gained much at-tention recently (Cohen et al., 1998; Herbrich et al., 1999; Joachims, 2002). This problem learns from rel-ative comparisons like  X  X  ranks lower than B X  or  X  X  ranks higher than D X . The goal is to learn an explicit or implicit function which gives ranks over an sampling space X . In most of these tasks, the sampled instances to be ranked are vector-valued data in R D , while the ranks are real numbers which can be either discrete or continuous. If the problem is to learn a real valued ranking function, it can be stated as: given a set S of pairs ( x i , x j )  X  X  (which indicates that the rank of x is lower than x j ), learn a real valued f : X  X  R that x .
 In this paper we investigate a special ranking prob-lem: dissimilarity ranking ( d-ranking ). Unlike rank-ing, this problem learns from instances like  X  X  is more similar to B than C is to D X  or  X  X he distance be-tween E and F is larger than that between G and H X . Note that the dissimilarities here are not neces-sarily distances. Other than real vectors in conven-tional ranking problems, the data to be ranked here are dissimilates of pairwised data vectors. This prob-lem can be stated as: learning an explicit or implicit function which gives ranks over a space of dissimilar-ities d ( X , X )  X  R . Based on different requirements of applications, this learning problem can have various formulations. We will present some of them in Section 2.
 D-ranking can be regarded as a special instance of dis-similarity learning (or metric learning). Different dis-similarity learning methods have different goals. We highlight some previous work as below.  X  In metric learning methods (Hastie &amp; Tibshirani,  X  In kernel learning methods (Lanckriet et al., 2004;  X  Multidimensional scaling (MDS) (Borg &amp; Groe-In our d-ranking problems, the purpose of learning a proper dissimilarity is to preserve the ranks of dissim-ilarities, not the absolute values of them (which is the case in MDS and Isomap). For example, if we know that  X  X he distance between A and B is smaller than that between C and D X , the problem can be formu-lated as: find a dissimilarity function d , such that d ( A, B ) &lt; d ( C, D ).
 Unlike conventional learning and ranking problems, d-ranking hasn X  X  received intensive studies in previous research. One of the most important related work is the nonmetric multidimensional scaling (NMDS) (Borg &amp; Groenen, 2005). Given a symmetric prox-imity (similarity or dissimilarity) matrix  X  = [  X  mn ], NMDS tries to find a low dimensional embedding space R L such that  X  x i , x j , x k , x l  X  R L , k x i  X  x j k x k  X  x l k 2 2  X   X  ij &lt;  X  kl . NMDS was recently ex-tended to the generalized NMDS (GNMDS) (Agarwal, 2007). GNMDS does not need to know the absolute values of proximities  X  mn . Instead it only need a set S of quadruples ( i, j, k, l )  X  S , which indicate that  X  Both NMDS and GNMDS learn an embedding space instead of learning an explicit ranking function, thus they are unable to handle out-of-sample problems. Schultz et. al. gave a solution to these problems by proposing to learn a distance metric from relative com-parisons (Schultz &amp; Joachims, 2003). They choose to learn a Mahalanobis distance which can preserve ranks of distances. Since the learned distance functions are parameterized, they can be used to handle new sam-ples. The proposed formulation was solved in a similar manner as SVM. Nonetheless, the regularization term was not well justified.
 Many applications in biology, computer vision, web search, social science etc. can be put into the frame-work of d-ranking problems. Take document classifica-tion as an instance. Without adequate domain knowl-edge, it is hard to accurately determine the quantita-tive dissimilarities between two documents. However, comparing the dissimilarities between every three or four documents can be easily done, either automat-ically or manually. Generally speaking, d-ranking is especially useful when the quantized dissimilarities are not reliable.
 In Section 2, we propose three formulations of d-ranking problems. Section 3 gives the numerical so-lutions for solving d-ranking by SDP. Section 4 shows how to solve d-ranking by QP. The proposed methods are evaluated in Section 5. Section 6 concludes the paper. D-ranking problems can have various formulations de-pending on specific requirements or settings of appli-cations. Next we will give three formulations. Formulation 2.1. (F1) Inputs : a set S of ordered quadruples ( i, j, k, l )  X  S , indicating that d ( x i , x ilarity function; Outputs : coefficients of embedded As proposed by Agarwal et. al. (Agarwal, 2007), in F1 we neither assume any geometry of the input space, nor assume any form of dissimilarities in it. We do not need to know the coefficients of input samples. Only ordering information is provided. Nonetheless we assume a Euclidean metric in the embedding space, which is often of low dimensions (e.g. L = 2, or 3). As shown in Section 3, F1 can be formed as a problem of semidefinite programming (SDP).
 Formulation 2.2. (F2) Inputs : a set S of ordered quadruples ( i, j, k, l )  X  S , indicating that d ( x i , x larity function; corresponding coefficients in the input Euclidean space x i , x j , x k , x l  X  R D ; Outputs : dissim-ilarity functions  X  d (  X  ,  X  ) : R D  X  R D  X  R ; Criteria : ( i, j, k, l )  X  X   X   X  d ( x i , x j )  X   X  d ( x k , x l ) . Unlike learning an embedding space as in F1 , F2 learns an explicit dissimilarity function  X  d (  X  ,  X  ) which preserves the ranks of dissimilarities. We will show in Section 4 that F2 can be handled in a very sim-ilar manner as support vector machines, where the quadratic programming (QP) problem can be solved efficiently by specialized sequential optimization meth-ods. If in some cases we need to find a low dimensional Euclidean embedding of the input samples, we can then use the classical multidimensional scaling (MDS) to preserve the learned dissimilarities.
 Formulation 2.3. (F3) Inputs : a set S of ordered quadruples ( i, j, k, l )  X  S , indicating that d ( x i , x larity function; corresponding coefficients in the input Euclidean space x i , x j , x k , x l  X  R D ; Outputs : pro-jection function f : R D  X  R L , x 0 i , x 0 j , x 0 k , x Criteria : ( i, j, k, l )  X  X   X  X  x 0 i  X  x 0 j k 2 2  X k x 0 Although we formulate F3 as a function learning prob-lem, currently we have not found any efficient method to solve it. This formulation will remains as our future work. F1 was studied by Agarwal et. al. (Agarwal, 2007). The authors proposed GNMDS which can be solved as a SDP, as shown in Eq.(1).
 The main idea of GNMDS is to learn a positive semidefinite Gram matrix K = X T X which can be eigen-decomposed to recover the embedded sam-ples. The relation between Euclidian distances and the Gram matrix is used: Nonetheless, the constraints that contain order infor-mation of dissimilarities are not sufficient to determine a unique K , since any rotation, translation or scaling can also satisfies these constraints. To reduce these ambiguities, they use embedded samples at the origin.
 It is preferable in many applications to find low di-mensional embedding spaces, e.g. 2D or 3D Euclidean space. Thus a low-rank K is desired. Unfortunately, minimizing rank( K ) subject to linear inequality con-straints is NP-Hard (Vandenberghe &amp; Boyd, 1996). Thus the objective function is relaxed heuristically as minimizing trace( K ), which is a convex envelope of the rank.
 Figure 1 shows the result of GNMDS on a toy problem. The inputs are 990 pairwise ranks of distances between 10 European cities. The outputs are the recovered 2D coefficients. It can be observed that the recovered locations of the cities do not correspond to the true locations. Actually only 789 out of 990 pairs of ranks are preserved by the learned 2D embedding, i.e. 20 . 3% error rate. Figure 2 shows the 10 sorted eigenvalues of the Gram matrix K . Although the original space is a 2D Euclidean space, the first 2 eigenvalues only account for 49 . 4% of the total variation.
 There are at least two reasons that account for the poor performance of GNMDS. Firstly, there is no guar-antee on the quality of the solution of the relaxed prob-lem compared with the original problem. There may exist some higher dimensional spaces which satisfy all the constants while have smaller traces than lower di-mensional spaces. Secondly, due to the introduction of slack variables  X  ijkl in the inequality constraints, the learned embedding tends to push all the samples to the same point. The first problem can be solved by introducing better heuristics of the convex envelope of the rank. The second problem can be solved by the following slight modification of GNMDS: The modified GNMDS just changes the slack variables from +  X  ijkl to  X   X  ijkl . This simple trick can ensure that all the differences between distances k, l and i, j are larger than 1, thus pulls the embedding samples apart. Figure 3 shows toy problem solved by the modified GNMDS. The recovered samples are closer to the true locations than those in Figure 1. There are 850 out of 990 pairs of ranks correctly preserved, i.e. 14 . 14% error rate, which is 6% lower than GNMDS. Figure 4 shows the 10 eigenvalues of K learned by modified GNMDS. The first 2 eigenvalues account to 69 . 8% of the total variant, which is 20% higher than GNMDS. As introduced in Section 2, instead of learning an em-bedding as in F1 , a dissimilarity function d ( x i , x j X  X  X  X  R is learned in F2 , such that all the training ranks between d (  X  ,  X  ) are preserved, and can generalize to new samples. This is indeed a dissimilarity learning problem.
 Many previous metric learning methods (Hastie &amp; Tib-shirani, 1996; Goldberger et al., 2004; Kwok &amp; Tsang, 2003) try to learn an alternative dissimilarity function by replacing the Euclidean metric with an properly learnt Mahalanobis metric, either globally or locally. In this section we propose the d-ranking Vector Ma-chine ( d-ranking-VM ) method. Unlike metric learn-ing methods, d-ranking-VM is explicitly regularized. Thus we can have a full control over the complexity of d ( x i , x j ). D-ranking-VM utilizes the technique of hy-perkernel learning (Ong et al., 2005) which was origi-nally proposed for learning a proper kernel.
 D-ranking-VM is formulated as the following optimiza-tion problem: where N = |S| . H is a hyper-reproducing kernel Hilbert space (hyper-RKHS) from which the function d : X  X  X  X  R is drawn.
 Like the representer theorem in RKHS (Kimeldorf &amp; Wahba, 1971), there is also a representer theorem in hyper-RHKS (see (Ong et al., 2005) or (Kondor &amp; Je-bara, 2006) for the theorem and proofs): where K is a semidefinite hyperkernel, x denotes a pair of samples ( x i , x j ), and M is the number of distinct dissimilarity pairs provided by the training rank data S . We denote the set of dissimilarity pairs as D , and M = |D| . Normally we have M &gt; N (the discussion of M and N is given in Section 5.1).
 Substitute Eq.(5) into (4), we can change the primal problem to the following form: where C  X  R M is a vector with the i th element being c The dual problem of (6) can be derived by using the Lagrangian technique. The solution to this optimiza-tion problem is given by the saddle point of the La-grangian function: L ( C,  X  p ,  X  p ,  X  p ) = + where  X  p and  X  p are non-negative Lagrange multipli-ers. The primal problem is convex, thus there exist a strong duality between the primal and the dual. Uti-lizing the KKT optimality condition, we have: and where A  X  R N is a vector with the p th element be-the rank information. Each column p x  X  ( x = 1 . . . N ) of P and each column q x  X  ( x = 1 . . . N ) of Q only con-tain one 1 and M  X  1 0s. For example, if the r th training quadruples in S is ( i, j, k, l ), which means that element in D , while ( k, l ) is the n the element in D , then p rm = 1 and q rn = 1.
 From Eq.(8) and (9) we have: and Substitute Eq.(10) and (11) into (6), we arrive at the following dual problem for d-ranking-VM: This problem is a quadratic programming (QP) prob-lem which shares a similar form as SVM. Thus the se-quential optimization techniques of SVM can be read-ily employed for d-ranking-VM. To perform testing, we can use the learnt dissimilarity function in Eq.(5) and make pairwise comparisons.
 An important problem for kernel learning methods is the selection of proper kernels. This problem also ex-ists in hyperkernel learning methods. Here we pro-pose some examples of hyperkernels, which are hyper-extensions of Gaussian RBF kernels and polynomial kernels. The construction of these hyperkernels are based on the following proposition.
 Proposition 4.1. Let k a (  X  ,  X  ) and k b (  X  ,  X  ) be posi- X   X ,  X  &gt; 0 ,  X k b ( x Proof. See appendix.
 Example 4.2. (Gaussian symmetric product hy-perkernel) Let k a and k b be the same Gaussian RBF kernel k ( x , x 0 ) = exp the Gaussian symmetric product hyperkernel is given by: Example 4.3. (Gaussian symmetric sum hyper-kernel) Under the same conditions as Example 4.2, we can construct the Gaussian symmetric sum hyper-kernel as: Example 4.4. (polynomial symmetric product hyperkernel) Let k a and k b be the same polynomial kernel k ( x , x 0 ) = can construct the polynomial symmetric product hyper-kernel as: k  X  Example 4.5. (polynomial symmetric sum hy-perkernel) Under the same conditions as Example 4.4, we can construct the polynomial symmetric sum hyperkernel as: k  X  The proposed d-ranking methods in Section 3 and Sec-tion 4 are evaluated by several experiments. 5.1. Obtaining Ranks of Pairs from Data To test our methods, we need to obtain pairwise dis-tance ranks. This can be done in many ways. Gen-erally speaking, for a problem of n data samples, the total number of available distance pairs are M = C 2 n = distance). The total number of pairwise distance ranks take d ( x i , x j ) &lt; d ( x k , x l ) and d ( x k , x the same rank pair). Table 1 gives some examples of the relation between n and N . When n grows, the n 2 3 4 10 20 50 100 1000
N 0 3 15 990 17955 749700 12248775 1.2475e+11 number of rank constraints will increase dramatically. Even solving a problem of n &gt; 100 will be impossible for some optimization solvers.
 Here we reduce N by considering order transitivities, i.e. if A &gt; B and B &gt; C , then the rank pair A &gt; C can be ignored (automatically satisfied) in the optimiza-tion constraints. The method is very simple. Firstly we sort the M distances decreasingly. Then we take the adjacent two distances to form one distance rank pair. By doing this, N can be reduced to n 2 2  X  n 2  X  1. This is the maximum number of N which carries full rank information of all the distances. Of course in some applications, the rank information is not be fully given, and N &lt; n 2 2  X  n 2  X  1.
 We test our method on three data sets: 1) 2D locations of 109 largest cities in the continental US; 2) 100 im-ages of handwritten digits  X 3 X  and  X 5 X  from the USPS database, each of size 16  X  16; 3) 126 face images of 4 people from the UMist database, each of size 112  X  92. For GNMDS and modified GNMDS methods, all ranks of distance pairs are fed to a SDP solver, and the re-covered 2D embeddings are plotted. We used SeDuMi (Strum, 1999) to get the results given in the follow-ing subsection. For d-ranking-VM, LibSVM (Chang &amp; Lin, 2001) is employed as our QP solver. It is im-plemented by employing the sequential minimal opti-mization (SMO) technique. The learned dissimilarities are used as a  X  X seudo-distances X , and are fed to the classical MDS. The recovered 2D embeddings are then plotted. 5.2. Results of US Cities In this data set, n = 109 and N = 5885. Every loca-tion of the cities is given by a 2D coefficient. Figure 5 shows the true locations. Figure 6 shows the results given by GNMDS.
 It can be observed that GNMDS cannot correctly re-cover the embedding based on distance ranks. Most of the embedded samples are pushed to a line. 50 . 3% of the distances ranks are preserved, which are the results of randomness. This gives an evidence to the analysis in Section 3. Figure 7 shows the result given by the modified GNMDS. The recovered embedding roughly reflects the geometry of the cities. 74 . 5% of the dis-tances ranks have been preserved. Since the distance information is not provided, there is no hope to match the true locations exactly.
 Figure 8 shows the results given by d-ranking-VM, where  X  = 10, and the Gaussian symmetric product hyperkernel is used, with  X  = 15. 97 . 9% of the dis-tances ranks are preserved.
 Table 2 shows the runtime of the above experiments. 5.3. Results of USPS Handwritten Digits In this data set, n = 100 and N = 4949. The dimen-sion every data sample is 16  X  16 = 256. Figure 9 shows the recovered 2D results given by RnakD-VM. 5.4. Results of UMist Human Faces In this data set, n = 126 and N = 7874. The dimen-sion every data sample is 112  X  92 = 10304. Figure 10 shows the recovered 2D results given by RnakD-VM. We have presented three d-ranking formulations, and give numerical solutions for two of them, namely solv-ing d-ranking by SDP and solving d-ranking by QP. Each of them has its advantages and shortcomings. We list some pros and cons from different perspectives:  X  Pros for d-ranking by SDP (GNMDS and the  X  Cons for d-ranking by SDP (GNMDS and the  X  Pros for d-ranking by QP (d-ranking-VM): Solv- X  Cons for d-ranking by QP (d-ranking-VM): It can-To our knowledge, this is the first work which brings out-of-sample prediction capability and large-scale scalability to d-ranking problems. Note that the tech-nique of d-ranking-VM can also be employed in solv-ing distances preserving problems. We will investigate the regularization properties and evaluate the perfor-mances of different hyperkernels in the following re-search. Finding a numerical solution for formulation F3 will also be our future work. We need to prove that k is a kernel on X 2 .
 Denote and define where 1 is a matrix of all ones.
 Denote the kernel matrix of k a ( x 1 , x 2 ) as K a , and that k as K .
 It is easy to verify that we can construct K = K eigenvalues  X  a and  X  b are positive. Thus the eigenval-ues of K : v ij =  X  X  X  ai  X  bj are also positive. A symmet-ric matrix K with positive eigenvalues is positive def-inite. Thus k = hyperkernel.
 We can also verify that  X K a  X  1 inite and  X ,  X  &gt; 0, K =  X K a semidefinite. Thus k =  X k a ( x 1 , x 2 ) +  X k b ( x 0 1 a valid hyperkernel.

