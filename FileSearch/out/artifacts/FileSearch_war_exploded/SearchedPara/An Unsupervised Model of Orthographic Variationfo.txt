 Optical Character Recognition (OCR) for historical texts, a challenging problem due to unknown fonts and deteriorating documents, is made even more dif-ficult by the fact that orthographic conventions in-cluding spelling, accent usage, and shorthands have not been consistent across the history of printing. For this reason, modern language models (LMs) yield poor performance when trying to recognize characters on the pages of these documents. Further-more, transcription of the actual printed characters may not always be the most desirable output.
Greg (1950) describes two types of transcription: one that preserves variants and typographical er-rors, and another that records the substantive con-tent, with this noise removed. Though in 1950 the substantive version was the norm, today these have become two distinct but equally valid tasks. Diplo-matic transcription, the standard in contemporary OCR, preserves the variants of the document valu-able to book historians and linguists. Normalized or modernized transcription recovers the substantive content, producing a text that adheres to modern standards. Normalized transcriptions are easier for users to read, and make large collections of histori-cal texts indexable and searchable (Driscoll, 2006).
The current ideal for digital editions of histori-cal texts has been described as a combination of diplomatic and normalized transcription (Pierazzo, 2014). This is generally achieved with a pipeline: first OCR is used to transcribe the document, then an (often manual) post-hoc normalization is performed. However, such a pipeline will result in cascading er-rors from OCR mistakes, and fails to make use of knowledge about modern language during the initial transcription. Additionally, post-processing tools are typically cumbersome language-specific, hand-built systems (Baron and Rayson, 2008; Burns, 2013; Hendrickx and Marquilhas, 2011).

In this work, we introduce a novel OCR model designed to jointly produce both diplomatic and nor-malized transcriptions. The model is an extension of Berg-Kirkpatrick et al. X  X  (2013) Ocular , the state of the art in historical OCR. Ocular X  X  innovative abil-ity to handle the material challenges of OCR (un-known fonts, uneven inking, etc.) depends on its use of a character n -gram LM. Our model improves the quality of Ocular X  X  transcriptions by automatically learning a probabilistic mapping between the LM, which is trained on modern text, and the unique or-thography of the document. This results in both an improved orthographically-correct diplomatic tran-scription and a modern-style normalized transcrip-tion. To our knowledge, this represents the first OCR system that jointly produces both diplomatic and normalized transcriptions.

We evaluate our model on a multilingual collec-tion of books exemplifying a high degree of ortho-graphic variation. For diplomatic transcription, our unsupervised joint model achieves an error reduc-tion of 35% over the baseline Ocular system with-out support for orthographic variation, and nearly matches the error rate of an approach proposed by earlier work that uses a hand-constructed ruleset of orthographic rewrites. However, for the new task of normalized transcription, we achieve a 46% error re-duction over the baseline, as well as a 28% reduction over the hand-built ruleset approach. The Primeros Libros corpus dataset introduced in our previous work consists of multilingual (Span-ish/Latin/Nahuatl) books printed in Mexico in the 1500s (Garrette et al., 2015). The original dataset in-cludes gold diplomatic transcriptions of pages from five books of different time periods, regions, lan-guages, and fonts. We additionally include two new monolingual Spanish Primeros Libros books anno-tated with both diplomatic and normalized transcrip-tions. Spanish-only texts were needed in order to find language-competent annotators skilled enough to create the more challenging normalized transcrip-tions. We used each of the seven books in isolation since they each have a different font. For each book, we used 20 pages for training and 10 for testing. For two of the books, an additional 10 pages were held out for tuning hyperparameters with grid search.
To produce the Spanish and Latin LMs, we used texts from Project Gutenberg; these documents were written during the target historical period, but all fol-low modernization standards including substitution for obsolete characters and expansion of shorthand. These texts were chosen because they are a realis-tic sample set that is freely and publicly available. In the Nahuatl case, scarce online resources made it necessary to supplement Project Gutenberg text with that from a private collection. Language LM char Glyph char Typesetting Rendering Figure 1: Our generative OCR model with the new glyph layer (bolded). The Spanish ( sp. ) n -gram lan-guage model (LM) generates a sequence of charac-ters according to standard Spanish spellings, uran in this case, from the word proc uran do which may be written proc vr  X  a do . Language-specific character-replacement probabilities are used to generate a glyph char from each LM char, producing vr  X  a and a zero-width (elided) n . Finally, the model generates a bounding box and right-side padding (the typeset-ting) and a pixel-rendering of the glyph character. We extend Ocular, the generative model of Berg-Kirkpatrick et al. (2013), and its EM training pro-cedure, to support our unsupervised approach to jointly modeling both diplomatic and normalized transcription. Ocular works by modeling the op-eration of a hand press in order to learn unknown fonts in the presence of the visual noise of the print-ing process: uneven inking and spacing in particular. Briefly, Ocular X  X  generative story is as follows. First, a sequence of language states ` i is generated accord-differ on the start of a word. For each state, a charac-ter c i is generated according to its language-specific state X  X  typesetting t i , consisting of a character X  X  bounding box, vertical offset, and between-character Finally, the character image is rendered as a pixel-
A major downside to the Ocular model is that ren-Original image dered image x i is always generated directly from LM character c i , resulting in transcription errors when printed characters don X  X  follow the spellings in the LM. Our model (Figure 1) adds an additional layer to the generative model that de-couples the LM from the rendering by allowing the LM-generated character c i to be replaced by a possibly different glyph character g i which is rendered instead.
For the generative story of our new model, we again begin by generating pairs ( ` i ,c i ) . However, instead of typesetting c i , we generate a distinct glyph character g i as its replacement, according to nally, we typeset and render g i (instead of c i ) using
We follow the previous Ocular work for the defi-ifying the probability of rendering g when the LM generated c , given that the language is ` , as follows: ` ( g | c ) = Constant  X  defines a Bernoulli parameter specifying the fixed probability of deterministically choosing to render c i directly (i.e., g i = c i ). We set  X  = 0 . 9 to bias the model away from substitutions in general. The remaining (1  X   X  ) probability mass is then di-vided among all potential output glyph characters.
Table 1 shows some of the common substitution patterns that our model addresses. For a direct ren-dering of c i , a letter substitution, or the dropping of an accent, g i will be a simple character drawn from the language X  X  set of valid characters (each language may have a different set of permitted characters, e.g. accented letters). In order to support the tilde-elision shorthand, we permit g i to be a tilde-annotated ver-sion of c i , and for doubled letters, we permit g i to be c c i , for which we typeset and render c i twice. Fi-nally, to allow for elided letters, including the drop-ping of a line-break hyphen, we allow g i to be a spe-cial ELISION glyph that renders only as a zero-width space character.

The parameters of the glyph substitution model are learned in an unsupervised fashion as part of Oc-ular X  X  EM procedure via a hard parameter update: where freq ( `,c,g ) is the number of times in the training iteration that the model chose to replace c with g in a word (automatically) determined to be of language ` . The +1 term is Laplace smoothing.
To guide the model and improve efficiency, we employ a number of constraints governing which kinds of substitutions are valid. Among these, we stipulate that substitutions must be letter-to-letter, diacritics may only be added to lowercase letters, only s can replace long-s , and elision-tilde-marked letters must be followed by one or more elisions. As a first baseline, we compare against Ocular with no orthographic variation handling, in which char-acters generated by the LM are rendered directly.
As a second baseline, we compare to our previ-ous work, which improved Ocular X  X  diplomatic tran-scription accuracy by introducing orthographic vari-ation directly into the LM with hand-constructed language-specific orthographic rules to rewrite the LM training data prior to n -gram estimation (Gar-rette et al., 2015). However, this rule-based pre-processing approach is inadequate in many ways. First, annotators do not know the full range of or-thographic variations, or their frequencies, in each document, and it is impossible to write rules to han-dle typos. Furthermore, a highly language-proficient
Orthographic Diplomatic Normalized No handling 13.2 45.7 17.4 47.6 Hand-written rules 8.5 30.8 13.1 37.9 Unsupv. joint model 8.6 32.7 9.5 27.6 Table 3: Experimental results for both Diplomatic (preserving variation) and Normalized (modern or-thography) transcription tasks. Results given as both character error rate ( CER , including punctuation) and word error rate ( WER , without punctuation). annotator is required, which is not always feasible with, e.g, rare indigenous languages.

Each baseline model has a single output, evalu-ated against both diplomatic and normalized gold.
Our source code and evaluation data are freely available at https://github.com/tberg12/ocular and https://github.com/dhgarrette/ocr-evaluation-data . Our results are shown in Table 3 and some correct example system outputs can be seen in Table 1.
Our model X  X  accuracy in producing diplomatic transcriptions is substantially better than baseline Ocular performance, yielding a 35% relative char-acter error rate reduction. Further, our model X  X  diplomatic transcription accuracy is comparable to the LM replacement-rule approach of our previous work, but achieves this result with only unsuper-vised learning, as opposed to expert-produced rules. We can see in Table 4 that our model is able to discover and apply appropriate probabilities to rel-evant substitution rules. For the new normalized-transcription task, even larger gains were achieved: a 46% relative error reduction over Ocular, and a 28% reduction over the rule-based approach. Table 4: A sample of high-probability Spanish sub-stitution rules learned by our unsupervised model. 5.1 Error Analysis Table 2 displays a sample of errors in our system output. (1) The word d  X   X as is printed without an ac-cent though it has one in modern Spanish. Our sys-tem is unable to distinguish between a dot and an accent above the i , and thus it opts to output the accented version since it is preferred by the LM. (2) The word suplicar does not have any accents in modern Spanish, but the model is over-eager in this case and attempts to revive an accent where there should not be one. (3) The word salvo is printed here as the variant saluo , but its under-inking leaves the u in disconnected pieces, resembling a pair of i characters. The LM model believes this to be the (valid) word sali  X  o with the accent dropped and the i doubled. (4) The model guesses incorrectly that the elision at the end of alab  X  a is an r since alabar is a valid word. (5) The model correctly recognizes that the letters ti are printed, but the LM believes the nor-malized form is te even though te pues is not valid Spanish, perhaps because te puedo is a very com-mon phrase and the six-gram context isn X  X  enough to make that distinction. (6) Finally, there are some special idiomatic shorthands that our model is sim-ply unable to understand because they have no clear Table 5: A document excerpt along with actual system outputs for both diplomatic and normalized transcriptions. Note that the normalization recovers the first line X  X  missing line-break hyphen, allowing the full word aprovecha to be reassembled. connection to what they are replacing. Here, x  X  po (from Greek letters chi rho ) is shorthand for Cristo . In this paper we presented a novel unsupervised OCR model for the joint production of diplomatic (variant-preserving) and normalized transcriptions of historical documents. The model is able to auto-matically learn a probabilistic mapping from a LM trained on modern text to the orthographic variants present in the document. This has the dual result of both considerably improving diplomatic transcrip-tion accuracy, while also enabling the model to si-multaneously produce a normalized transcription.
Our model has the potential to significantly im-pact the work of scholars and librarians who wish to make digital texts easier to read, index, search, and study. Our approach also has the fortunate side-effect of producing metadata about orthographic variation in printed documents that may be valu-able to scholars of book history. With our model, we are able to automatically induce sets of variation patterns used by printers, and the locations in the texts where those variants appear, without the need for labor-intensive page-by-page reading. Further, these induced mappings have probabilities attached, and are not simple rulebanks like those used in exist-ing normalization work (Garrette et al., 2015; Baron and Rayson, 2008). Table 4, above, shows a sam-ple of rules and their frequencies that resulted from training our model on Spanish documents.

Finally, our work helps to bridge the gap between historical text and mainline NLP. Orthographic vari-ation lowers the accuracy of NLP tools due to high out-of-vocabulary rates and mismatched morpho-logical features (Piotrowski, 2012). This is espe-cially true when these tools are trained on the mod-ern texts of the standard corpora used in NLP (Yang and Eisenstein, 2016). Normalization of historical texts have been shown to improve the quality of, for example, taggers (Rayson et al., 2007; Yang and Eisenstein, 2016) and parsers (Rayson et al., 2007). These techniques mirror those applied to the processing of text in social media, such as Twitter, where there is a high degree of slang and shorthand (Gimpel et al., 2011; Eisenstein, 2013; Yang and Eisenstein, 2013). Most approaches train off-the-shelf NLP tools on modern text and then apply nor-malization techniques to historical texts to transform them into something resembling the modern train-ing input (Scheible et al., 2011). A joint model such as ours that automatically learns orthographic varia-tions while training the NLP model might overcome some of the limitations of using such a pipeline ap-proach.
 We would like to thank Stephanie Wood, Kelly McDonough, Albert Palacios, Adam Coon, Sergio Romero, and Kent Norsworthy for their input, ad-vice, and assistance on this project. We would also like to thank Taylor Berg-Kirkpatrick, Dan Klein, and Luke Zettlemoyer for their valuable feedback on earlier drafts of this paper. This work is supported in part by a Digital Humanities Implementation Grant from the National Endowment for the Humanities for the project Reading the First Books: Multilin-gual, Early-Modern OCR for Primeros Libros.
