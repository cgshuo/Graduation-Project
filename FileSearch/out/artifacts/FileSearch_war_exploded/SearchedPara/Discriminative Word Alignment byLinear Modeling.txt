 Institute of Computing Technology Chinese Academy of Sciences Institute of Computing Technology Chinese Academy of Sciences Institute of Computing Technology Chinese Academy of Sciences performance for various statistical machine translation systems. 1. Introduction machine translation (Brown et al. 1993).
 correspondences in a parallel text.
 304 word sense disambiguation.
 texts. Among them, generative alignment models (Brown et al. 1993; Vogel and Ney to the maximum posterior predictions of the model.
 erative models usually impose strong independence assumptions between sub-models, which often makes the resulting system quite complex.
 significantly.
 approach and the article closes with a conclusion in Section 6. 2. Approach 2.1 The Model Given a source language sentence f = f 1 , ... , f j , ... , f e , ... , e i , ... , e I , we define a link l = ( j , i ) to exist if f product of the word positions: a for a given sentence pair f , e . 2.2 Training  X  measure alignment quality, because we will optimize  X  M 1 translations.
 alignment quality is.
 scores do not necessarily lead to better translation quality. measure called the balanced F-measure : 306 higher, whereas  X  greater than 0 . 5 weights precision higher. ating translation performance (Section 4 . 2). 2.2.2 Minimum Error Rate Training. Suppose we have three candidate alignments: a to score each candidate. Table 1 lists the feature values for each candidate. to { 1 . 0,  X  2 . 0, 2 . 0 } , the model scores become  X  73, model chooses a 2 as the best candidate correctly.
 that minimize AER or maximize F-measure on a representative hand-aligned training corpus.
 1  X  F-measure. Given a bilingual corpus f S of K different candidate alignments C s = { a s ,1 ... a s , K corpus: parameters until the overall loss on the training corpus does not decrease. variable: where  X  denotes the parameter being tuned (i.e.,  X  m constants with respect to  X  : alignments in Table 1, suppose we only tune  X  2 and keep  X  set of parameters { 1 . 0, 1 . 0, 1 . 0 } . According to Equation (10), a 4  X   X  75, a loss at  X  . How do we find values of  X  that could generate different loss values? (  X  308 predicts with various values of  X  . Instead of computing all possible K a has the highest score. Similarly, the best candidates are a value. Please refer to Och (2003) for more details. 2.3 Search best candidate alignment with the highest model score: e e to a 3 . a 2 and a 3 can be further extended to produce more alignments. constitute a level . There are 2 J  X  I possible nodes and J
The maximum level width is given by J  X  I J  X  I in a greedy way. In Figure 3, starting from a 1 , we add single links to a score than a 1 ,say a 3 , and discard the others. Then, we add single links to a three new alignments: a 7 , a 9 ,and a 11 . After choosing a next candidates are a 12 and a 14 . Suppose the model scores of both a than that of a 9 . We terminate the search process and choose a alignment.
 current best alignment a will result in a new alignment a that is a link l to the current alignment a : we retain at most b candidates at each level.
 310
Algorithm 1 A beam search algorithm for word alignment 1: procedure A LIGN ( f , e ) 2: open  X  X  X  a list of active alignments 3: N  X  X  X  n -best list 4: a  X  X  X  begin with an empty alignment 5: A DD ( open , a ,  X  , b ) initialize the list 6: while open =  X  do 7: closed  X  X  X  a list of promising alignments 8: for all a  X  open do 9: for all l  X  J  X  I  X  ado enumerate all possible new links 10: a  X  a  X  X  l } produce a new alignment 11: g  X  G AIN ( f , e , a , l ) compute the link gain 12: if g &gt; 0 then ensure that the score will increase 13: A DD ( closed , a ,  X  , b ) update promising alignments 14: end if 15: A DD ( N , a ,0, n ) update n -best list 16: end for 17: end for 18: open  X  closed update active alignments 19: end while 20: return N return n -best list 21: end procedure worse than: 1.  X  multiplied with the best score in the list, or 2. the score of b -th best alignment in the list.
 produce a new alignment a (line 10) and calculate the link gain is
O ( bJ 2 I 2 ). 3. Feature Functions of the treatment of the IBM models as features can be found in Appendix B. the one-to-one assumption to simplify the modeling problem (Melamed 2000; Taskar,
Lacoste-Julien, and Klein 2005), many translation phenomena cannot be handled and the recall cannot reach 100% in principle.
 a number of symmetric features used in our experiments. 3.1 Translation Probability Product ities of each link in two directions.
 sentence f 1 f 2 and a target language sentence e 1 e uct is where t ( e | f ) is the probability that f is translated to e and t ( f is translated to f , respectively.
 multiplied with t ( e 2 | f 2 )  X  t ( f 2 | e 2 ): in the current example alignment { (1, 2) } , the unaligned target word e  X  X ligned X  to the empty cept f 0 . As our model is symmetric, we use f respectively. 312
Similarly, the product for { (1, 2), (2, 2) } now becomes
In this case, the new product divided by the old product is t ( f cepts on the other side in two directions (i.e., t ( e i | the fertility of a source word f j as the number of aligned target words: { { words. The fertility of the first English word The is  X  1 zero fertilities (e.g., The ,  X  X  ,and a in Figure 1) are unaligned. words. Table 2 shows the feature values for some word alignments. translation probability product is 4 where  X  j and  X  i are the fertilities before adding the link ( j , i ). t ( e | f ) by training the IBM models using GIZA++ (Och and Ney 2003). 3.2 Exact Match feature in our model: 314 3.3 Cross Count of word orders between two languages.
 argument: 3.4 Neighbor Count line, indicating that the alignment is approximately monotonic. such that neighbors . Similarly, (5, 13) and (6, 14) are also neighbors. 3.5 Fertility Probability Product the symmetric scenario in Equation (20) and Equation (21). fertility probabilities in two directions.
 fertility probability product is where n (  X  j | f j ) is the probability that f j has a fertility of  X  that e i has a fertility of  X  i , respectively. 5 For example, n (1 that one target word is  X  X ligned X  to the source empty cept f probability that one source word is aligned to e 2 .
 { (1, 2), (2, 2) } , the resulting product will be
The new product divided by the old product is
Formally, the feature function for fertility probability product is given by 316
The corresponding feature gain is where  X  j and  X  i are the fertilities before adding the link ( j , i ). obtain all fertility probabilities n (  X  j | f j )and n (  X  directly. 3.6 Linked Word Count We observe that there should not be too many unaligned words in good alignments. the number of aligned words:
In Equation (33),  X  j and  X  i are the fertilities before adding l . 3.7 Sibling Distance { { introduce a feature that sums up the distances between siblings. target word e i . For example, jianzhuye is the second source word (i.e., f
As the first target word aligned to f 2 is construction (i.e., e  X  2,1 = 6. Similarly,  X  2,2 = 7 because industry (i.e., e 7 to jianzhuye . Obviously,  X  j , k + 1 is always greater than  X  source word duiwaikaifang . The sum of distances between them is calculated as Therefore, the distance sum of f j can be efficiently calculated as
Accordingly, the distance sum of e i is
Formally, the feature function for sibling distance is given by
The corresponding feature gain is where  X  j and  X  i are the fertilities before adding the link ( j , i ). 3.8 Link Count
J  X  318 where | a | is the cardinality of a (i.e., the number of links in a ). 3.9 Link Type Count languages, we use features to count different types of links. 1. one-to-one links, in which neither the source nor the target word 2. one-to-many links, in which only the source word participates in other 3. many-to-one links, in which only the target word participates in other 4. many-to-many links, in which both the source and target words one-to-many links.

Note that the feature gains of siblings will not change if  X  3.10 Bilingual Dictionary encourage linking word pairs that occur in a dictionary D : 3.11 Link Co-Occurrence Count predictions as features and obtain substantial improvements. links of the intersection of a and a as a feature: 4. Experiments
In this section, we try to answer two questions: 1. Does the proposed approach achieve higher alignment quality than 2. Do statistical machine translation systems produce better translations if 320
Algorithm 2 Calculating gains for the link type count features 1: procedure GAIN L INK T YPE C OUNT ( f , e , a , j , i ) 3: if  X  j = 0  X   X  i = 0 then consider ( j , i )first 5: else if  X  j &gt; 0  X   X  i = 0 then 7: else if  X  j = 0  X   X  i &gt; 0 then 9: else 11: end if 12: if  X  j = 1 then consider the siblings of ( j , i ) on the target side 13: for i = 1 ... I do 15: if  X  i = 1 then ( j , i ) was a one-to-one link 17: g o 2 m  X  g o 2 m + 1 ( j , i ) now becomes a one-to-many link 18: else ( j , i ) was a many-to-one link 20: g m 2 m  X  g m 2 m + 1 ( j , i ) now becomes a many-to-many link 21: end if 22: end if 23: end for 24: end if 25: if  X  i = 1 then consider the siblings of ( j , i ) on the source side 26: for j = 1 ... J do 28: if  X  j = 1 then ( j , i ) was a one-to-one link 30: g m 2 o  X  g m 2 o + 1 ( j , i ) now becomes a many-to-one link 31: else ( j , i ) was a one-to-many link 33: g m 2 m  X  g m 2 m + 1 ( j , i ) now becomes a many-to-many link 34: end if 35: end if 36: end for 37: end if 39: end procedure native alignment models.
 phrase-based, and tree-to-string SMT systems. 4.1 Evaluation of Alignment Quality 1. HLT/NAACL 2003 shared task (Mihalcea and Pedersen 2003). As part of 2. ACL 2005 shared task (Martin, Mihalcea, and Pedersen 2005). As part of 3. HTRDP 2005 shared task. As part of the 2005 HTRDP (National High ally aligned and marked with both sure and possible labels. Although the Canadian 322 original test set of 447 sentences. The results are shown in Table 11.
Consortium and a bilingual dictionary containing 415, 753 entries. system named Vigne based on the linear modeling approach. As we mentioned before, effectiveness of our search algorithm, we compare Vigne with GIZA++ by using the same models.

GIZA++. For example, the training scheme for Model 4 is 1 sult, the AER scores in Table 6 look quite high.
 identical AER scores because there are no search errors.
 models. All differences are not statistically significant. Cross-EM are unsupervised alignment methods.
 predictions from refined Model 4 and jointly trained HMM. 324 is prone to produce one-to-one alignments by encouraging agreement, symmetrizing from GIZA++ and Cross-EM.
 similar to those of Och and Ney (2003).
 respectively. features in both directions dramatically decreased. can model many-to-many alignments directly without any postprocessing symmetriza-tion heuristics.
 326 and results in more significant improvements than for asymmetric IBM models. One word count still hold. By further including predictions from GIZA++ and Cross-EM, on the Chinese X  X nglish task.
 metric model produces many-to-many alignment in a more natural way. and Chinese X  X nglish tasks are 3 . 6% and 14 . 3%, respectively. takes only two minutes on average.
 corpus. 447 sentences. For the Romanian X  X nglish language pair, we follow Fraser and Marcu (2006) in reducing the vocabulary by stemming Romanian and English words down to model and yields better results than the other participants. 328 et al. 2006; Moore, Yih, and Bode 2006). 4.2 Evaluation of Translation Quality translation systems: 1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT 2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system; 3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that on the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation length for brevity penalty.
 using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser and Marcu 2007b) as the optimization criterion.
 used seven generative alignment methods based on IBM Model 4 and HMM as baseline models on the word-aligned FBIS corpus.
 330 recall higher because F-measure decreases when  X  increases. On the other hand, E systems significantly at all values of  X  .
 oriented alignments yield better translation performance. systems significantly. 5. Related Work
The first generative alignment models were the IBM Models 1 X 5 proposed by Brown
Ney (2003) re-implement the IBM models and the HMM model and compare them with combination of data likelihood and agreement between the models. Fraser and Marcu unsupervised and semi-supervised training methods.
 log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003) in SMT (Och and Ney 2002) to word alignment and report significant improvements alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to (ITG) constraints.
 332 more important than discriminative training method. 6. Conclusions and Future Work syntax-based.
 aligned consists of parallel texts from different domains. tion criterion.
 investigate an algorithm for automatic feature selection.
Appendix A: Table of Notation f source sentence f 1 sequence of source sentences: f 1 , ... , f s , ... , f f source word
J length of f j position in f , j = 1, 2, ... , J f j the j -th word in f f 0 empty cept on the source side e target sentence e 1 sequence of target sentences: e 1 , ... , e s , ... , e e target word
I length of e i position in e , i = 1, 2, ... , I e i the i -th word in e e 0 empty cept on the target side a alignment l a link ( j , i )in a  X  j number of positions of e connected to position j of f  X  i number of positions of f connected to position i of e  X  j , k position of the k -th target word aligned to f j  X  i , k position of the k -th source word aligned to e i
 X  ( j ,  X  j ) sum of sibling distances for f j  X  ( i ,  X  i ) sum of sibling distances for e i  X  a the best candidate alignment  X  feature weight  X  the feature weight being optimized h ( f , e , a ) feature function
G ( f , e , a , l ) link gain after adding l to a g ( f , e , a , l ) feature gain after adding l to a  X  ( f , e , a ) value of the feature being optimized  X  ( f , e , a ) dot-product of fixed features t ( e | f ) the probability that f is translated to e t ( f | e ) the probability that e is translated to f n (  X  | f ) the probability that f has a fertility of  X  n (  X  | e ) the probability that e has a fertility of  X  r reference alignment
C s set of candidate alignments for the s -th training example a s , k the k -th candidate alignment for the s -th training example
E ( r , a ) loss function that measures alignment quality  X  the precision/recall weighting factor in balanced F-measure  X  pruning threshold in the beam search algorithm b beam size in the beam search algorithm  X  ( x , y ) the Kronecker function, which is 1 if x = y and 0 otherwise expr an indicator function taking a boolean expression expr as the argument Appendix B: Using the IBM Models as Feature Functions 334 and Ney 2003).
 target words, and t ( f j | e i ) is a translation sub-model. Note that a f is connected to e i .
 words are  X  X ligned. X  alignment probability distribution with an alignment sub-model a ( i sentence lengths I and J .

The corresponding feature gain is where f j and e i are linked by l and 0 is the index of the empty cept e
Model 2, Model 3 uses a fertility sub-model n (  X  i | e i
Formally, the feature function of Model 3 is given by ter p 0 :
Note that we follow Brown et al. (1993) in replacing I i = 1 with an empty alignment (see Algorithm 1), for which  X  h by adding a smoothing parameter p n  X  (0, 1):
Therefore, the feature gain for Model 3 is where f j and e i are linked by l ,  X  i is the fertility before adding l ,and g
B to capture movement of phrases. The feature function for Model 4 is where 336 the first position to the left of i for which  X  i &gt; 0, c of the words in  X   X  ,  X  ik denotes the k -th French word aligned to e position of the k  X  1-th French word aligned to e i ,and A more details.
 where f j and e i are linked by l and  X  i is the fertility before adding l . p
D ( a ) every time.
 Acknowledgments References 338
