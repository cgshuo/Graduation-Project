 allows us to formulate and understand the underlying smooth ness assumptions of a particular SSL R d with a smooth boundary. Let y :  X   X  X  be the unknown function we wish to estimate. In case of supervised learning problem is formulated as follows: Give n l labeled points, ( x with y the goal is to construct an estimate of y ( x the regime where l is fixed and u  X  X  X  . We first consider the following graph-based approach formul ated by Zhu et. al. [15]: where W given similarities.
 In contrast, our focus here is on the more typical scenario wh ere the points x p ( x ) is the method (1) sensible? The answer, of course, depends on how the matrix W is constructed. We consider the common situation where the similarities are obtained by applying s ome decay filter to the distances: where G : R +  X  R + is some function with an adequately fast decay. Popular choi ces are the with a softer loss-based data term, which is balanced agains t the smoothness term I Our analysis and conclusions apply to such variants as well.
 Limit of the Laplacian Regularization Term where the summation is replaced by integration w.r.t. the de nsity p ( x ) : at least when  X   X  0 slowly enough 1 , namely when  X  =  X  ( d p log n/n ) . If  X   X  0 as n  X   X  , a density weighted gradient penalty term [7, 8]: where C = R the associated limit problem: b order a 6 x satisfies inside each interval ( x Performing two integrations and enforcing the constraints at the labeled points yields with y ( x ) = x the regularizer J ( y ) can be interpreted as a Reproducing Kernel Hilbert Space (RK HS) squared semi-norm, giving us additional insight into this choice of regularizer: Theorem 1. Let p ( x ) be a smooth density on  X  = [ a,b ]  X  R such that A Then, J ( f ) can be written as a squared semi-norm J ( f ) = k f k 2 with a null-space of all constant functions. That is, k f k the RKHS induced by K If squared semi-norm induced by the kernel with a null-space spanned by indicator functions 1 Proof. For any f ( x ) = P When x are never non-zero together and J component [ a,b ] , and assuming w.l.o.g. a 6 x Substituting J any variant where the hard constraints are replaced by a data term) is of the form: where i ranges over the connected components [ a Viewing the regularizer as k y k 2 tion (1), by interpreting K no dependence between points in disjoint components separa ted by zero density regions. of the labeled points.
 p tinuous functions y that J ( y the first labeled point is at x k x 1 k = 1 the origin is contained in  X  = { x  X  R d | p ( x ) &gt; 0 } .
 We first consider the case d &gt; 2 . Here, for any  X  &gt; 0 , consider the function which indeed satisfies the two constraints y where V constraints, but for d &gt; 2 , inf For d = 2 , a more extreme example is necessary: consider the function s and y is performed. In particular, an interpretation in terms of a density-based kernel K dimensional case, is not possible.
 constraints, as in In the limit of infinite unlabeled data, functions of the form y zero with functions that do not generalize at all to unlabele d points. 4.1 Numerical Example We illustrate the phenomenon detailed by Theorem 2 with a sim ple example. Consider a density  X  = 0 . 4 .
 that the solution  X  y ( x ) of (1) satisfies: by Gaussian elimination (here invoked through MATLAB  X  X  backslash operator). This is the method unlabeled points and iterate (14), while keeping the known l abels on x used in the bottom panels of Figure 1.
 setting the threshold to zero yields a generalization error of 45% .
 with the ill-posedness of the limit problem: the solution is numerically very un-stable. performance (this is a  X  X heating X  approach which provides a lower bound on the error of the best method for selecting the bandwidth). As the number of unlabe led examples increases the squared minimizing the squared loss, i.e. when the predictor is even flatter. 4.2 Probabilistic Interpretation, Exit and Hitting Times transition matrix M = D  X  1 W where D is a diagonal matrix with D binary classification case with y of measure zero). With more and more unlabeled data the rando m walk will fully mix, forgetting uniform across the entire graph, independent of the startin g location x  X  is the case, then it may not necessarily make sense to take val ues of  X  &lt;  X  the problem. On the contrary, even the one-dimensional case becomes ill-posed in this case. To a sequence of continuous functions y two labeled points at the centers of the Gaussians and an incr easing number of randomly drawn Before we conclude, we discuss a different approach for SSL, also based on the Graph Laplacian, candidate predictors y ( x ) non-parametrically to those with small I The similarity matrix W is computed as before, and the Graph Laplacian matrix L = D  X  W is considered (recall D is a diagonal matrix with D spanned by the first p eigenvectors e coefficients a Eigenvector based methods were shown empirically to provid e competitive generalization perfor-ture, the success of eigenvector based methods critically d epends on how well can the unknown multiplicity. Consider learning three different function s, y y ( x ) = 1 x separating boundary between the classes on the manifold, as shown in the experiment described in The reason for this behavior is that y by the very few leading eigenfunctions X  X ven though they seem  X  X imple X  and  X  X mooth X , they are significantly more complicated than y cian and its eigenfunctions have the form  X  making it hard to represent simple decision boundaries whic h are not axis-aligned. The empirical success might be due to two reasons.
 unlabeled points grow while l/u is fixed, has recently been analyzed by Wasserman and Laffert y right X  number of unlabeled points without any theoretical g uidance.
 mance. We do not know of any theoretical explanation for such behavior, nor how to characterize with flat predictors, and perhaps also make it appropriate fo r regression. appropriateness for SSL.
 about the  X  X moothness X  of y ( x ) relative to p ( x ) remain unclear.
 Acknowledgments The authors would like to thank the anonymous referees for va luable sugges-tions. The research of BN was supported by the Israel Science Foundation (grant 432/06).
