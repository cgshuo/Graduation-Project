
We investigate the problem of counting the number of frequent (item)sets X  X  problem known to be intractable in terms of an exact polynomial time computation. In this pa-per, we show that it is in general also hard to approximate. Subsequently, a randomized counting algorithm is devel-oped using the Markov chain Monte Carlo method. While for general inputs an exponential running time is needed in order to guarantee a certain approximation bound, we em-pirically show that the algorithm still has the desired accu-racy on real-world datasets when its running time is capped polynomially.
Frequent pattern mining is a technique that is used for data mining within several combinations of local pattern types and application domains. A few examples are asso-ciation rules for market basket data, frequent subgraphs for molecule prediction, and sequential patterns for time series data (see [7] for an overview). Their unifying property is that patterns are only considered interesting if they satisfy a (minimum) frequency constraint, i.e., a certain number of records of the input dataset have to  X  X ontain X  the pattern. In contrast to most other work devoted to frequent pattern mining which deals with the development and evaluation of algorithms that list all frequent patterns, in this paper, we are interested in counting them quickly. Knowing the relationship between frequency threshold and the resulting number of frequent patterns can for instance be used for computing a frequency-plot , i.e., a plot showing all possi-ble thresholds (x-axis) against the corresponding number of frequent sets (y-axis). Having such a plot prior to the actual mining process can for instance be used within intelligent discovery assistants (IDAs) [2] in order to either provide user guidance for setting the frequency threshold or to tune it automatically.

For this purpose it is an essential requirement that the involved computation is performed quickly.  X  X ndeed, the problem is precisely to predict a combinatorial explosion without suffering from it [. . . ] X , as Geerts et al. [4] have put it in their related study of bounding the number of candidate patterns that have to be processed within a BFS-listing of all frequent patterns. In particular for dense datasets this re-quirement prohibits the use of any of the known exhaustive data mining algorithms that list all frequent patterns. Even though these algorithms have been optimized to an impres-sive level in recent years, they have the number of frequent sets as an inherent lower bound of their time complexity. While this does not pose a problem for sparse datasets, in dense ones this number behaves in essence exponentially (for low frequency thresholds). For that reason we are aim-ing at an algorithm counting the number of frequent sets in a time that does not depend on that number. Since it is well known that no exact deterministic algorithm with this property can exist (unless P = NP ), we are aiming at a ran-domized approximation algorithm using the Markov chain Monte Carlo method. Moreover, we restrict ourselves to the case of plain frequent (item)sets, which are used, e.g., for generating association rules. In summary, we are aim-ing at an input polynomial algorithm solving the following computational problem: Problem 1 (#-FREQUENT SETS) . Given a transactional dataset and a frequency threshold, compute the cardinality of the corresponding frequent set family.

We discuss the question of why all this is a worthwhile venture in some more detail in Section 2. After some formal definitions (Section 3) that are needed for the subsequent technical content we analyze theoretical limits in Section 4. As the main result of this section we show that the num-ber of frequent sets is hard to approximate. In addition we interpret some known complexity results in the context of our problem. We then develop a randomized approxima-tion scheme in Section 5 that is integrated into a hybrid al-gorithm with an approximative and an exhaustive counting phase. As indicated by the hardness result, we show that for general inputs the algorithm X  X  correctness cannot be guar-anteed or its time complexity is not bounded polynomially. However, experiments we present in Section 6 constitute its applicability on several real-world and synthetic datasets. A concluding discussion is given in Section 7.
In order to raise business value of data mining or gen-erally to make it more accessible to non-expert users, ap-proaches like Intelligent Discovery Assistants (IDAs) [2] or Mining Mart [12] have been proposed. Both assist a user in selecting a valid data mining process for their data. Still, once a user has decided on a valid process the next problem is to find good parameter settings, and in case a frequent pat-tern mining step is part of the process this usually involves a minimum frequency threshold. Knowledge about the re-lation between frequency threshold and the corresponding number of frequent sets is very helpful in this context, as it allows to control the output size and thus indirectly the out-put time, because frequent set mining algorithms usually exhibit a time complexity that is roughly linear in the out-put. Consequently, quickly counting the frequent sets helps to make optimal use of the available time budget. Moreover, as we will argue below, a computed frequency plot can also help semantically to setup and interpret the whole process. It should be noted that the following discussion is illustra-tive and motivating. A thorough investigation of how to choose a minimum frequency threshold justifies a complete study in its own right.
In order to explain the possible use of a frequency plot let us assume we have a dataset generated by the following illustrative and absolutely idealized underlying process. Example 2 (Beginner X  X  Guide Process) . There is a hobby shop carrying items for k different hobbies. In particular there is a  X  X eginner X  X  guide X  a i for every hobby and a cor-responding  X  X tarter kit X  consisting of items c i, 1 ,...,c ( 1  X  i  X  k ). A usual (senior) customer purchases every item independent from one another with probability p c ex-cept for the beginner X  X  guides, which he will never purchase. However, with probability p a (per hobby independent from one another) a customer will pick up a new hobby and buy the corresponding beginner X  X  guide a i . In this case he will always also purchase items c i, 1 to c i,l ( i ) and behaves like a usual customer otherwise.

Clearly, the most (if not the only) interesting association rules for this underlying process are the rules for i  X  { 1 ,...,k } . All of these rules will have the maxi-mum confidence 1 of 1 , which measures the actual semanti-cal value of a rule. Whereas their expected support is equal to the probability p a . We generated three datasets using the above process for p a = 0 . 1 , 0 . 13 , 0 . 16 and p c = 0 . 6 . Each of the datasets was generated with k = 5 implicant items, 10 different consequence items for each implicant, and 5000 transactions. Figure 1 shows their resulting frequency plots. Depending on initial assumption on the generative process there can be different strategies of inferring a good thresh-old from this plot. First, however, it is necessary to clarify the motivation of introducing a minimum frequency thresh-old at all.
The reasons for introducing a frequency constraint can roughly be subsumed under three purposes: (i) suppress statistically insignificant results that are a (ii) raise output pattern value in the sense that they are ap-(iii) allow additional pruning of search space thus making In particular for the last purpose, the frequency threshold is often set to higher values than it would be necessary for rea-sons (i) and (ii) alone. Clearly, this can harm the analytical value of the resulting patterns, because patterns that are in-teresting according to a primary criterion (e.g., confidence) might be pruned without any statistical reason purely for the sake of performance. Thus, it would be desirable to set the frequency threshold to the minimum value that is reason-able with respect to reasons (i) and (ii).

For the above example observe that for all three datasets the interesting rules have an expected support that lies be-fore that point of when the plot finally turns to a purely exponential behavior. Thus, a user setting the frequency threshold to this point is expected to not miss most of them. Although this must not always be the case, this strategy gen-erally aims at finding the most conservative threshold that preserves most interesting rules while it suppresses most of the statistically insignificant sets. Moreover, the plots lead to a small set of reasonable candidate frequency points, namely the sockets before steep exponential slopes. Clearly, this is an improvement when compared to the uninformed approach of trial-and-error parameter twiddling.
An important related approach to raise user control in frequent set mining is listing only the top-K frequent sets proposed by Wang et al. in [16]. While this approach takes care of output size control and scheduling, it fails to provide the global overview of computing a frequency plot for all possible frequency values X  X  task that cannot be done using a top-K miner (in particular on dense datasets) because the same restrictions apply to them as stated in the introduction for ordinary exhaustive miners.

This may inspire a general objection to the above moti-vation: Why is it useful to compute regions of the plot that correspond to frequent set families so large that they are impossible to list in a subsequent mining step? The answer to this lies in the fact that frequency is usually only a sub-sidiary interestingness criterion. While the frequency plot shows the effect of that criterion alone, the final pattern set may result from a conjunction of several constraints, e.g., confidence, many of which can be used for pruning already during frequent set listing as pointed out by Pei and Han [13]. Thus, the resulting output family, albeit its low fre-quency threshold, can in fact be listed effectively.
In this section we recall and fix notions from frequent set mining, randomized approximation algorithms, and Markov chains that are needed in the subsequent discussion. Frequent Set Mining Let E be a finite set. A dataset D over E is a finite multiset with D  X  E for all D  X  D . In the context of frequent pattern mining the elements of E are often called items and the elements of D transac-tions . For a set F  X  E we define its support multiset as D [ F ] = { D  X  D : D  X  F } . Frequency thresholds can be specified as absolute or relative thresholds depending on what is more convenient in a given situation. For an (ab-solute) integer threshold f , F is called f -frequent (or fre-quent) in D if |D [ F ] |  X  f respectively for a (relative) real threshold f  X  (0 , 1) , if |D [ F ] |  X  f |D| . The family of all f -frequent sets in D is denoted F ( D ,f ) or just as F when D and f are clear from the context.
 Probabilistic Approximation Algorithms A bounded probability (BP) algorithm for a problem with instances X and possible solutions Y specified by a correctness relation R  X  X  X  Y is a probabilistic algorithm A such that it holds that P [( x, A ( x ))  X  R ]  X  3 / 4 . The constant 3 / 4 appearing in the definition has no significance other than being strictly between 1 / 2 and 1 . Any two success probabilities from this interval can be reached from one another by a small number of repetitions of the corresponding algorithm and comput-ing a combined result depending on the nature of Y (see [9]). Now we consider the case when Y is the set of natu-ral numbers N . A randomized approximation scheme for a mapping f : X  X  N is a BP-algorithm A taking arguments x  X  X and  X  (0 , 1) satisfying the relaxed correctness predicate R = { ( x,y ) : (1  X  ) f ( x )  X  y  X  (1 + ) f ( x ) } , i.e., A satisfies P [(1  X  ) f ( x )  X A ( x, )  X  (1 + ) f ( x )]  X  3 / 4 . (1) Such an algorithm is called fully polynomial if its time com-plexity is bounded in a polynomial in size ( x ) and 1 / .
A weaker notion of approximation is given by the fol-lowing definition: An algorithm A is called an  X  -factor approximation of f (or said to approximate f within  X  ) if it satisfies the correctness relation R = { ( x,y ) : may grow in the size of x . Clearly, an efficient approxima-tion scheme can act as an efficient c -factor approximation algorithm for all constants c &gt; 1 .
 Markov Chains A (discrete) Markov chain on state space  X  is a sequence of discrete random variables M = X 1 ,X 2 ,... with domain  X  satisfying the Markov condi-tion, i.e., P [ X n +1 = x | X 1 = x 1 ,...,X n = x n ] is equal to P [ X n +1 = x | X n = x n ] for all n  X  N and x,x 1 ,...,x  X  . Thus, given a probability distribution on the initial state, M is completely specified by the state transition proba-bilities P ( x,y ) = P [ X n +1 = y | X n = x ] of all x,y  X   X  that do not depend on n . The |  X  |  X  |  X  | -matrix contain-ing P ( x,y ) in column x and row y is a stochastic matrix we denote by P . The t -th power of this matrix contains the probability of going from x to y in t steps P t ( x,y ) = P [ X n + t = y | X n = x ] . We call a state y  X   X  reachable from a state x  X   X  if there is a t  X  N such that P t ( x,y ) &gt; 0 . A Markov chain M is called aperiodic if for all x,y  X   X  with x is reachable from y there is a t 0  X  N such that for all t  X  t 0 it holds that P t ( x,y ) &gt; 0 , and it is called irreducible if any two states are reachable from one another. Finally, M is called ergodic if it is irreducible and aperiodic.
Any ergodic Markov chain has a unique limiting station-ary distribution  X  :  X   X  [0 , 1] , i.e., for all states x,y  X   X  it holds that lim t  X  X  X  P t ( x,y ) =  X  ( y ) . Moreover, if there is a function  X  0 :  X   X  [0 , 1] satisfying the detailed balance condition  X  x,y  X   X  , X  0 ( x ) P ( x,y ) =  X  0 ( y ) P ( y,x ) then  X  is a stationary distribution. It follows that ergodic Markov chains with symmetric transition probabilities always con-verge to the uniform distribution . The distance from the t -step distribution of a Markov chain to its stationary dis-tribution can be measured by the total variation distance this definition we can define the mixing time of M by as the minimum number of steps one has to simulate M until the resulting distribution is guaranteed to be -close to its stationary distribution. For more details and results about Markov chains and their mixing time we refer to Randall X  X  survey [14]. Gunopulos et al. proved in [6] #P -hardness 2 of #-FRE-QUENT SETS implying that there is no exact algorithm for that problem unless P = NP . They did this using a reduc-tion from the #P -complete problem of computing the num-ber of satisfying truth assignments of a given monotone 2-CNF formula, i.e., a conjunctive normal form formula con-taining only two positive literals per clause. It was shown by Zuckerman [17] that this problem and in fact even its logarithm is hard to approximate within a factor of n for instances of size n .

The reduction in [6], however, transforms a 2-CNF for-mula into a transaction dataset with n items such that the number of satisfying truth assignments corresponds to the number of sets that are not 1 -frequent, and then it uses the fact that the number of infrequent sets is equal to 2 minus the number of frequent sets. Hence, the construc-tion is highly non-parsimonious, i.e., the numbers usually change drastically without any reasonable bound. As a consequence relative approximation guarantees are not pre-served by that reduction and it does not lead to a hardness result for approximating #-FREQUENT SETS. Still, it is an important side note that the two aforementioned theorems together do imply the strong result that there is no efficient approximation algorithm for counting the number of infre-quent sets even if the (absolute) frequency threshold is fixed to 1 . This is an interesting difference to the same restric-tion for #-FREQUENT SETS: When restricted to frequency threshold 1 approximating the number of frequent sets be-comes equivalent to approximating the number of satisfy-ing assignments of a given DNF-formula, and for this prob-lem there is a fully polynomial randomized approximation scheme [10].

Now, for acquiring a hardness result for the number of frequent sets we have to choose a different starting point, namely the hardness of approximating a frequent set of maximum cardinality. With this approach we can show the following result: Theorem 3. Unless for all &gt; 0 and for all problems in NP there is a BP-algorithm that runs in time 2 n for in-stances of size n , the following holds: There is a constant  X  #F such that there is no polynomial time BP-algorithm that, given a dataset D over n items and a frequency threshold f , approximates log |F ( D ,f ) | within n  X  #F . Proof. It was shown in [3] that under the same assumption as in the claim there is no polynomial time algorithm ap-proximating a frequent set of maximum cardinality within n imability result for Bipartite Clique, which in fact ruled out BP-algorithms under the above assumption (see [11] where you can also find more information about the magnitude of  X 
BC ). Furthermore, it is easy to prove that approximating only the maximum number k such that there is a frequent set of size k is polynomially equivalent to the actual con-struction of a corresponding set. Thus, it is sufficient to show that an algorithm for approximating the logarithm of |F| can be used to approximate this number k .

Since all subsets of a maximizing frequent set F  X  F with | F | = k are also frequent, it holds that |F|  X  2 k the other hand, all frequent set are of size at most k and thus Now suppose a BP-algorithm A approximates log |F| within n  X  . It follows that Now observe that for any  X  &lt;  X  BC , the expression n  X  constant n (  X  ) . Choose  X  #F to be any number strictly be-tween 0 and  X  BC . Then modify A such that it looks up the true result of all (finitely many) instances of size less than n (  X  #F ) in a hard-coded table. Then A is a BP-algorithm approximating the maximum cardinality of a frequent set
Although the complexity assumption of this theorem is stronger than P 6 = NP it is still a widely believed standard as-sumption. Moreover, non-existence of an  X  -approximation of the logarithm of a number implies non-existence of an 2 -approximation to the actual number. Thus, we have strong evidence that there is no reasonable approximation algorithm for the general #-FREQUENT SETS problem and in particular no fully polynomial approximation scheme.
That said, there may still be algorithms allowing a good approximation for a wide range of practical relevant datasets. With this in mind, we are going to derive a ran-domized approximation algorithm in the next section.
The perhaps simplest Monte Carlo approach for counting the number of frequent set would be the following: Uni-formly generate an element F  X  E , return 1 if F  X  F , and return 0 otherwise. The expected value of this exper-iment is |F| / 2 | E | . Thus, taking the mean of sufficiently many independent repetitions and multiplying it by 2 | E | a correct randomized approximation scheme. It is, however, not fully polynomial. This is due to the fact that |F| / 2 can be as small as 1 / 2 n for an instance of size n . For such instances the expected number of trials before the first 1 turn-out appears is not bounded by a polynomial in n . But as long as the returned result is 0 the solution does not sat-isfy any relative approximation guarantee and in particular not Equation 1.

The standard solution to this problem is to decompose the result into a number of factors, each of which having a reasonable lower bound. In our case this can be done as follows. From now on we fix some order of the items E = e ,...,e n . Then, for i  X  X  1 ,...,n } let be the family of frequent sets containing only elements from the first i items. With this we can rewrite the quantity to compute |F| = |F n | as the product with some starting index s  X  { 1 ...,n  X  1 } . It follows di-rectly from the definition that F i  X  X  i +1 . Since, moreover, for each element F  X  X  i +1 \F i the set F \{ i + 1 } must be in F i , it holds that |F i +1 \F i | X |F i | and thus With this we can design a Monte Carlo algorithm as fol-lows: Approximate the reciprocal of each factor separately, count |F s | for an appropriate s exhaustively, and then compute |F| through Equation 2. For an F drawn uniformly at random from F i let Z i de-note the random variable that takes on value 1 if F  X  X  i  X  1 and 0 otherwise. Then Z i is a Bernoulli experiment with P [ Z i = 1] =  X  i . Moreover,  X  Z i = ( Z (1) i +  X  X  X  + Z t  X  1 and Z ( j ) i independent copies of Z i is an unbiased esti-mator of  X  i with variance Var  X  Z i =  X  i (1  X   X  i ) /t . Then an algorithm A returning Z  X  1 |F s | with Z =  X  Z s +1 ... a randomized algorithm approximating |F| . Once we have a procedure that can draw an element from the sets F i uni-formly at random the implementation of A is straightfor-ward. This problem is approached in Section 5.2.

For the rest of this subsection we discuss the conditions under which the above algorithm is a correct randomized approximation scheme for |F| as specified by Equation 1. This analysis closely follows [8] but is nevertheless given here for the sake of completeness. In order to derive the re-quired number of repetitions for each Bernoulli experiment, one can instantiate Chebycheff X  X  inequality as follows So a bound on this probability can be established by appro-priately bounding the ratio of Z  X  X  variance to the square of its expectation. For the estimator of each factor  X  Z i we know that where the last inequality follows from Equation 3. Thus, if we set the number of trials t to trials ( s, ) with we can deduce for the product: Plugging this bound into Equation 4 it follows that Z satis-fies Equation 1 and thus that an algorithm simulating it is a randomized approximation scheme as required.
In the naive Monte Carlo algorithm sketched in the be-ginning of Section 5.1 the necessary number of trials was prohibitive, while the required uniform sampling from the power set did not pose a problem. Now the situation is different: The required number of trials is polynomially bounded, but it is unclear how to sample uniformly from the frequent set families F k ( D ,f ) for k = s + 1 ,...,n as required for estimating the factors  X  k . We approach this problem by a repeated application of the following proce-dure: Let F old be the current element of F k . Then 1. with probability 1 / 2 set F new to F old ; otherwise: 2. uniformly draw an i  X  X  1 ,...,k } 3. if e i  X  F old set F new to ( F old \{ e i } ) ; otherwise: 4. if ( e i  X  F old )  X  X  then set F new to ( F old  X  X  e i This procedure simulates one step of a Markov chain M F k on F k with state transition probabilities where  X  denotes symmetric difference. All  X  X emaining X  probability is assigned to the self-loops, i.e., P ( F,F ) = 1  X  X { F 0  X  X  : | F  X  F 0 | = 1 }| / 2 k . So the transition proba-bilities and thus the corresponding state transition matrix P as well as the reachability relation of M F k are symmetric.
Together with the fact that F is closed under taking sub-sets this implies that M F k is irreducible because all states are reachable from  X  . Moreover, there are non-zero self-loop probabilities for every state. This implies that M is also aperiodic and together with irreducibility this means that M F k is ergodic.

For an ergodic Markov chain we know that there is a unique distribution  X  that it converges to and that is station-ary, i.e., P X  =  X  . Since P is symmetric,  X  must be the uni-form distribution. Hence, simulating M F k for sufficiently many steps can be used to sample a frequent set from F k uniformly at random as required.

The question is, however, for how many steps we have to simulate M F k until it is  X  X lose enough X  to the uniform distribution. Let M t F responding to simulating M F k for t steps starting in state F . The mixing time  X  ( 0 ) is defined as the smallest number such that the total variation distance between the distribu-tion of M t 0 F all possible starting states F  X  F . Here, we use the sym-bol 0 in order to set it apart from the accuracy parameter of the algorithm. Since we will switch to a heuristic for the number of steps below, we will not explain what values of 0 would preserve the overall approximation guarantee. Instead we refer to [9] for the connection between count-ing and almost uniform sampling . The reason for using a heuristic is that in line with Theorem 3, a general polyno-mial bound in size ( D ) to the mixing time  X  should not be expected. Indeed, we can observe the following: Proposition 4. For n  X  N the Markov chain M F n with fre-quency threshold f = 1 and D = {{ 1 } , { 2 ,...,n } , { n + 2 ,..., 2 n }} on items E = { 1 ,..., 2 n } has mixing time  X  ( 0 ) of at least 2 n  X  1 log(1 / 2 0 ) .
 Proof. Let P ( X ) denote the power set of a set X . For n  X  N the 1 -frequent sets of the dataset given in the claim are with the cardinality |F| = 2 n . The conductance of M F n defined as with  X  S = (1 / X  ( S )) P x  X  S,y  X  X \ S  X  ( x ) P ( x,y ) . It is a well-known fact (see for instance [14]) that the mixing time of a Markov chain is bounded from below by the conduc-tance as follows: Now choose S = {{ 1 }} X  X  ( { 2 ,...,n } ) \{ X  X  . Then | S | = 2 n  X  1 and consequently  X  ( S ) = 1 / 2 .
 Plugging this bound into Equation 6 yields the claim.
Intuitively, the reason for the slow mixing time on these instances is that the probability of crossing over from one of the two  X  X locks X  P ( { 2 ,...,n } ) and P ( { n +1 ,..., 2 n  X  1 } ) to the other is very low compared to their sizes.

This situation is obviously rather artificial and it is a rea-sonable assumption that most real-world datasets do not possess such strictly separated blocks. For that reason we use for the rest of this paper as a heuristic for the required number of steps. This is of the same order as the expected number of steps until each item has been drawn at least once (coupon collector X  X  theorem) X  X  reasonable minimum requirement. As we will see in the experimental section, this heuristic works abso-lutely fine for different test datasets. Clearly, there are other possible choices for steps ( k, ) ; in particular when there is prior knowledge of the input dataset. Plugging together the Monte Carlo framework with the Markov chains M F algorithm. As indicated in Section 5.1 we have to count a starting factor  X  s exhaustively. It remains to decide to what value s should be set X  X  decision that requires some anal-ysis of the involved time complexities. For that we regard one frequency check as unit step. We will drop from the parameter lists of trials (  X  ) and steps (  X  ) for ease of notation as it is constant throughout one call of the algorithm.
Estimating  X  i requires at most trials ( s ) steps ( i ) fre-quency checks. On the other hand, counting |F s | exhaus-tively can take up to 2 s frequency checks in case all subsets of E s are frequent. This rough bound can be used to make an initial choice for s by choosing it such that it minimizes the estimated overall running time 2 s + T apx ( s ) with denoting the expected time for approximating all re-maining factors. Clearly, more knowledge about the used implementations X  X n particular that of the exhaustive miner X  X s likely to lead to an improvement of this choice.
Additionally, we can further improve our choice of the starting index s . Denote the index found by the consider-ations above as s  X  . The loose a priori bound used there can be improved once we have counted F s  X  . We know that |F we can increase s to s  X  + 1 and add |F s  X  +1 \F s  X  | counted exhaustively to |F s | . Clearly, as long as the above condition remains true for the new s it amortizes to repeat this step. The resulting algorithm counts F s for the final s  X  X hunk-wise X  as For the exhaustive counting tasks it is desirable to use one of the existing highly optimized frequent set listing algo-rithm. Let D| i denote the dataset in which all transac-tions have been restricted to the first i items. Observe that |F ( D [ { e s } ] | s  X  1 ) | is equal to |F s ( D ) \F one external call of the exhaustive miner with the dataset D [ { e s } ] | s  X  1 suffices to compute the latter quantity.
This concludes the presentation of techniques used in our counting procedure. A final pseudocode incorporat-ing all ideas is given with Algorithm 1. Using the heuristic steps ( k ) =  X  1 k ln k it performs O  X  3 n 2 ln n frequency checks.
 Algorithm 1 Hybrid Frequent Set Counting Input : dataset D on items E = { e 1 ,...,e n } , Require: M F k mixes in steps ( k ) Output : q with P [(1  X  ) |F| X  q  X  (1+ ) |F| ]  X  3 / 4 1. exhaustive phase: 3. S  X  exhaustive ( D| s ,f ) 4. while S &lt; T apx ( s )  X  T apx ( s + 1) do 5. S  X  S + exhaustive ( D [ { e s +1 } ] | s ,f ) 6. s  X  s + 1 7. approximative phase: 8. for k = s + 1 ,...,n do 10. for i = 1 ,..., trials ( s, ) do 12. if e k 6 X  F then  X  k  X   X  k + 1 13.  X  k  X   X  k / trials ( s, ) In this section we present experiments contrasting the Markov chain Monte Carlo algorithm with counting via exhaustive enumeration. The experiments are performed with respect to performance as well as accuracy ( = 0 . 5 was used throughout all experiments). As a representa-tive exhaustive miner, we used the modified FPgrowth al-gorithm by Grahne and Zhu [5], whose C++ implementa-tion is publicly available. This implementation has shown to rank among the fastest exhaustive miners in the com-petitive workshop FIMI [1]. In the following, we will refer to this implementation as  X  X  P Z HU  X . The bench-mark datasets are also taken from the FIMI repository supplemented by synthetic datasets generated according to the beginner X  X  guide process (Example 2) with different choices for the probabilities p a and p c . For Algorithm 1 we used a Java implementation that is available online together with the artificial datasets ( http://www-kd.iai.uni-bonn.de/index.php?page=people details&amp;id=16 or correspond-ing main page). The experiments were performed on an In-tel Core 2 Duo E8400 with 3 GB of RAM running Windows XP. All figures below use relative frequency thresholds. The central part of Algorithm 1 is a large number of Markov chain simulations. As a first optimization it is possible to do better than just simulating M F k for k = s + 1 ,...,n naively. Instead, we first buffer the required number steps ( k ) of random item indices for a single ran-dom walk. Since the result of each random walk F is only used to evaluate the Bernoulli experiment of whether item e is an element of F , we can simply stop the Markov chain simulation after the last occurrence of k in the buffer. Simi-lar, if e k was put into F due to the next to last occurrence of k we can also stop the chain simulation at that point. Since in this case the last occurrence of k will surely cause the item e k to be removed from F again, we can directly report that result.

The really dominant operation, however, is the test of whether a set I = F  X  { e } that is an augmentation of a frequent set F with a single item e remains frequent. This corresponds to step 4 of the Markov chain (see Section 5.2). Since this test has to be performed roughly every second step of each random walk, it is crucial for the overall per-formance of our algorithm. In order to decrease the cost of this operation, our implementation makes use of FP-trees [7]. Basically, an FP-tree is a compact representation of the dataset, where the transactions in the original dataset are represented as paths in a prefix-tree. Since overlapping transactions share a branch-prefix, this usually achieves a significant compression. In addition, there are node lists connecting all nodes representing the same item.

The frequency test for a set is done by selecting its least frequent item and following its node list in the FP-tree, adding the counts of all nodes whose parent nodes include all items in the set I . As a minor optimization, we have stored, at each node, a hash set containing the items in the parent nodes. While this information is redundant, it al-lows to efficiently determine if a node contributes to the frequency of I .
We now report a series of accuracy experiments that are summarized in Figure 2. Our algorithm was applied 100 times to each combination of one of eight test datasets with one of four frequency thresholds, resulting in a total of 3200 runs per dataset. A run  X  X ails X  if the reported result deviates from the true number of frequent sets by more than 50% as we used = 0 . 5 as accuracy parameter. If the overall ap-proximation guarantee holds we expect the fraction of failed runs to be below 1 / 4 (Equation 1). As desired, this was the case on all of the eight test datasets. In fact, in our experi-ments the highest ratio of failed runs was 0 . 07 for the pumsb dataset. Thus, the observed success rate was consistent with the probabilistic approximation guarantee for all datasets, and in fact we experienced much better error bounds. The table also shows median ( X  X edian X ), lower quartile ( X  X ow. quart. X ), and maximum ( X  X ax. dev. X ) of the experienced relative deviation.
 Figure 3 shows accuracy results on chess in more detail. For different frequency thresholds, it shows on a logarith-mic scale the exact number of sets ( X  X xact count X ) com-puted exhaustively, the upper and lower 50% deviation lim-its ( X  X pper limit X  and  X  X ower limit X ), as well as the result of the randomized algorithm in a series of 16 runs ( X  X cmc count X ). The figure shows that in most randomized runs, the approximated result lies in the desired deviation inter-val. The figure does not contain the exact number of sets for the lowest thresholds, because these could not be com-puted by the exhaustive miner.

We close this subsection by presenting an approximated frequency plot computed for a dataset generated by the ex-ample process from Section 2. We refer to this dataset as PC60PA10 , because it is an instantiation of this general pro-cess with probabilities p c = 0 . 6 and p a = 0 . 1 . The result is shown in Figure 4 (together with the exact curve) and illus-trates that the randomized algorithm provides an adequate approximation of the exact plot.
Next, we compare the runtime of our randomized algo-rithm with that of F P Z HU on several datasets. We start with the results on chess , which are presented in Figure 5(a). The diagram shows that while on higher thresholds the exact algorithm is faster than our randomized algorithm, the latter outperforms F P Z HU when the threshold becomes smaller. A similar behavior can be observed for connect (Figure 5(b)) and PC60PA10 (Figure 5(c)). On PC90PA10 (Figure 5(d)), which is extremely dense and as a result has a very high number of frequent sets, the randomized ap-proach outperforms the exhaustive miner on all threshold. In fact, for all but the highest thresholds the runtime of the exhaustive miner becomes unacceptable.

However, on the sparse datasets used in the accuracy study above the randomized algorithm never catches up with the exhaustive miner, or only at extremely low thresh-olds. Consequently, it heavily depends on the dataset X  X  den-sity (the number of frequent sets) whether the randomized or the exhaustive approach performs better. This can be ex-plained and summarized as follows: While the complexity of the Monte Carlo algorithm, in contrast to any exhaustive miner, does not depend on the output size, it does not scale as well as the latter in the input size. Summary In this paper, we developed a randomized ap-proximation algorithm for counting the number of frequent sets. As we pointed out there are worst-case examples on which the algorithm either does not possess a provable ap-proximation ratio or its runtime is not polynomial. We have shown, however, by giving a negative complexity result that a general polynomial algorithm with a good approxima-tion guarantee is unlikely to exists. Moreover, we experi-enced very good approximation rates on real-world and arti-ficial dataset when the runtime is capped polynomially. We demonstrated that for dense datasets/low frequency thresh-olds our method remains well applicable while exhaustive counting is infeasible. That said, on sparse datasets it is hard to compete with the sophisticated and highly efficient frequent set mining algorithm and implementations the data mining community has emerged during the last decade. Future Work There are several possible optimizations for Algorithm 1 we did not investigate in this paper. For instance, in order to speed up frequency tests in subsequent Markov chain simulations, it appears to be promising to store maximal frequent sets as well as minimal infrequent sets as they are visited.

Furthermore, the method proposed in this work is not di-rectly optimized towards drawing a frequency-plot, i.e., it does not benefit from the situation when there is more than one frequency threshold for which #-FREQUENT SETS has to be solved. This setting has room for extensive per-formance improvements that should be explored in future research. Some straightforward observations are: Exhaus-tive miners clearly do benefit from this situation as they can build up the whole plot by just one run with the lowest in-volved frequency threshold. Similar, the Monte Carlo al-gorithm can acquire a lower bound for F i ( f ) / F i +1 ( f ) al-ready while sampling from F i +1 ( f  X  1) during the run for threshold f  X  1 . Improved lower bounds can then be used to reduce the number of trials for their corresponding factor  X  . There is another possible speed-up along these lines that is applicable in the single threshold case as well: Deriving empirical bounds on the variance of each factor would also allow to reduce the required number of trials.

On a more global scale, an important next step is to in-vestigate whether the randomized counting approach can be extended to other pattern classes like closed sets or graph mining. This extension promises to be both, challenging and beneficial, as the cost of exhaustive mining generally increases with the pattern complexity.

The authors wish to thank Thomas G  X  artner for the help-ful insights he provided them into Markov chains.

