 of text mining [15,16]. A major method used in earlier days for Japanese summarizing tasks is based on the extraction of important sentences. Recent years, a methodologi-the deletion of unnecessary words, or a hybrid model of the former two is used. How-ever, neither seems to have made semantic analysis. As a result, some important words are neglected, while many unnecessary words remain in the summary. 
For instance, Hatayama etc. [4] extracted the important words using surface infor-mation and structural information, and selected the words for generating sentences word pairs that are considered important [18]. Oguro etc. made a natural summary etc. [6] and Ohtake etc. [14] put their focus on the deletion of words and the extrac-tion of importance sentences. 
On the other hand, there are some summarizing systems for non-Japanese docu-ments that have employed semantic information. For instance, the work of Hahn and Reimer [1], and that of Hovy and Lin [5]. 
As denoted above, except several summarizing systems for non-Japanese docu-ments, no summarization for Japanese has taken account of semantic information up to now. In this paper we describe an automatic summarizing system ABISYS devel-oped on the basis of a Japanese semantic analysis. We aimed to extract semantically important words and generate correct and natural Japanese sentences. tences. However, the difference is that we use word meanings and the deep cases 1 among the words in the output from a semantic analysis system, rather than the widely used surface information or structural information in other works. 2.1 System Input We take the output case-frames from the semantic analysis system SAGE Harada etc. have built [2,3] as the input to ABISYS. SAGE determines the word meanings and the deep cases among words according to the definition in the EDR Dictionary 2 . Each case-frame is composed of 14 elements including  X  X umber X ,  X  X enotation X ,  X  X eading X ,  X  X DR-POS 3 from Chasen-POS X ,  X  X DR-POS from Juman-POS X ,  X  X ictionary entry denotation X ,  X  X hasen-POS X ,  X  X uman-POS X ,  X  X nflection X ,  X  X article X ,  X  X oncept ID X  (meaning),  X  X eep case information X ,  X  X entence number X  and  X  X hrase number X . 
The summarizing process starts when a file containing a group of case-frames is and the summarizing rate (e.g., 30%) are given. 2.2 System Output The output of ABISYS includes 3 parts: the original document, the rewritten docu-Fig. 2. For the readers X  better understanding, we give their English translation as well. 2.3 Summarizing Process ABISYS summarizes a document in 4 steps. First, the important words are extracted. Then the element words to compose a summary are selected according to the impor-element words, correct and natural sentences are generated. section, we give detailed description for each of them. Generally, indeclinable words 4 tend to show the key point of a document. Along this line in this paper, we take indeclinable words as the candidates for selecting important tively on repetition information, context information, position information, opinion be important is calculated by consolidating the five scores. All indeclinable words are ranked in a list by their probabilities and important words are extracted from the list in a descending sequence until the summarizing rate is reached. 3.1 Repetition Information Score It is said that a repetition word tends to show the entire content of a document and to remain in the summary [17]. Here, a repetition word indicates a word or its synonym words from the case-frame set, then calculate their frequencies of appearance. Here in the calculating process, we count the numbers of the case-frames that have the same limit the distance to be 0 or 1). Finally, a score of 20, 30, 40, or 50 points is given to the repetition word if its frequency of appearance is 2-3, 4-5, 6-7, or 8 and over. case-frames with the  X  X umber X  of 2 and 76. Further more, we know that the word  X  into account the case-frame of  X   X  as well when assigning scores to the repeti-tion word  X   X . 3.2 Context Information Score In most cases, semantic relation holds between two adjacent sentences. Kawabata etc. have built a system InSeRA, which determines the semantic relation between two inter-timingCondition(S1,S2) time condition sentences [7] using the 21 relations defined in Table 1. We made an investigation and it turned out that the former or the latter sentence tends or not to remain in the sum-mary for some certain semantic relation between them. Along this line, we assign the indeclinable words in the former or the latter sentence some adequate scores as shown in Table 1 according to the particular semantic relation between the two sentences. 3.3 Position Information Score the final sentence talks mostly about the future of the current theme. The same thing can be said about a leading article too, where both the lead sentence and the last sen-tence tend to act as a summary. Many summarizing studies adopt this simple but ef-sentence. We assign the indeclinable words in the first sentence 30 points each, the ones in the second sentence 10 points each, and the ones in the last sentence 20 points each. 3.4 Opinion Information Score For a leading article, the argument, suggestion, or wish of the author is regarded most important, and ought to be left in the final summary. Ohtake collected 55 patterns for induction from Ohtake X  X  efforts. We consolidate all the expressions Ohtake collected into 3 opinion concepts ( X  X onsider X , 30f878), ( X  X hink X , 444dda), and its upper concept in EDR Concept Dictionary is called an opinion word. A word holding deep case with an opinion word will be assigned an opinion information score (10 points). 
For instance, (444dda) is the upper concept of the word in Fig. 5 implying that is an opinion word. As a result, the two indeclinable words and are assigned 10 points as they are linked to in deep case. 3.5 Topic-Focus Information Score The topic and the focus of a document generally remain in the summary. However, it is not easy to identify neither the topic nor the focus strictly. Here we take the follow-ing steps to find the topic and the focus of a document, and assign the corresponding words the topic-focus information scores. The method for consolidating a number of scores so far is to multiple each score with its weight, and then to add all the products together to obtain the general score. How-reliable. We propose here a method to calculate the general score using a pan-distance of Mahalanobis. We first select 1000 sample words from documents of the same cata-according to the content of documents. Then we set up a five-dimensional space with the repetition information, context information, position information, opinion infor-nate of each sample word. 
The general score of an important word candidate is calculated using the distance between the candidate word and each of the 2 sets in the five-dimensional space. The procedure of calculation is described below. 
Let denote the repetition information, context information, position information, opinion information and topic-focus information, and get the distribu-tion-covariance matrix for the valuable set and the non-valuable set respectively. 
Let the distribution-covariance matrix and its inverse matrix be and the average vector of the valuable set be and the score vector of the important word candidate be Then the Mahalanobis distance from the valuable set to the important word candi-date will fulfill the following equation, 
The Mahalanobis distance from the non-valuable set to the important word candi-date is calculated in the same way. Finally, we calculate and as shown below, This is the final score we have been seeking so far. 
Here, we switch the number of dimensions to 4 when processing a newspaper arti-cle for the reason that the opinion information score is employed only in the summa-matrix obtained in the experiment for summarizing leading articles that repetition information score is the most crucial one among the 5 variables used, and the impor-information, context information, and opinion information. Sentences are generated using words from the valuable word list that has been achieved in section 4. We employ two strategies to guarantee the correctness and the naturalness of the generated sentences. 5.1 The Use of Indispensable Case Indispensable case is a necessary element in order to generate a meaningful and cor-identify such an indispensable case is to find the postpositional particle or around the verb and take the word in front of or as the indispensable case. How-Sometimes, an indispensable case does exist for a verb in a sentence while no surface information exposes this fact. 
In a study to summarize newspaper articles, Hatayama etc. used a case-frame dic-tionary to compensate indispensable cases for a verb [4]. As their method compen-sates for the primary verb only, in case that multiple verbs appear in a sentence, indis-threshold values are assigned to the agent-case and the object-case with 0.08 and 0.15 respectively [11]. Threshold values used here are empirically set and accordingly untrustworthy. 
In this paper we propose a method to determine the threshold values for each deep Shimbun 5 , then search the EDR Co-occurrence Dictionary for the co-occurrence of each declinable word and each of its deep cases, x , finally calculate the average ap-called .. When , the appearing ratio of a deep case for a declinable word is larger than , we assign to as one of its indispensable cases. 5.2 Extraction of Summary Element Words After compensating indispensable cases fo r the declinable words, we select the sum-mary element words in accordance with the valuable words extracted in section 4. This process is composed of 4 steps. Every sentence here is represented as a depend-ency tree holding the primary predicate verb as the root. 
By Step1, we get the proper expression to show how the valuable word is men-tioned. Then in Step2 and Step3, we compensate the words that hold important rela-tions with the valuable words. Finally, we make up for the words that can hardly ex-press anything meaningful individually in Step4. In this way, we attempt to extract the necessary summary element words to generate the sentence as correct as possible. The final process of the summarizing task is to generate sentences using the summary element words extracted in Section 5. However, even among the element words, there exist some duplicated ones or the ones which are not that valuable as they have been assessed in Section 5. Here, we are to find them and delete them in order to generate shorter and briefer sentences. Specifically, we focus on the linguistic phenomena, quoting verb. A quoting verb is a verb leading a quotation, and often appears in a leading article. We follow the steps shown below to delete the quoting verbs. Fig. 6 and Fig. 7 are an example of deleting the quoting verb in the expression (I think it is unbelievable). With the procedure in mind, let us see the sentence . First we get that the upper concept of is (444dda), and that holds a logical-case with . Then, we find that is a constituent of the compound word whose POS is JPR implying a declinable word. Fi-nally, we see that is followed by the postpositional particle . In other words, we make a conclusion that is a quoting verb, and accordingly de-lete all deep cases of it except the quotation as shown in Fig. 7. fered in the NTCIR Workshop 8 2 (NTCIR-2) held in 2001 for Japanese documents. In this paper, we evaluate ABISYS using the subjective evaluation method designed in former evaluation was made by 5 graduate students and the latter was by the authors. 7.1 Subjective Evaluation Human judges evaluate and rank 4 types of summaries in 1 to 4 scale (1 is for the best and 4 is for the worst) in terms of two points of views, the covering rate in content of each summary and the readability of it [12]. The first two summaries are human-produced summaries. The third is the one produced by the baseline method of sen-tence extraction, and the fourth is the system output. 
We use the data prepared in NTCIR-2 for the subjective evaluation. In Table 2, we compared the evaluating results between ABISYS and the systems that participated in ability of ABISYS is on a par with other systems, while the point of content covering is much better than others. 
The summary produced by merely deleting the unnecessary parts tends to be highly evaluated in its readability. However, it is impossible to cover the total content of the regard, ABISYS behaves much better as it produces the summary by generating sen-tences from semantically important words. 
Further examination reveals that most unreadable cases are due to the incorrect output from the semantic analysis system SAGE, where the indispensable cases have not been compensated appropriately. Currently, the precision of morphological analy-sis and dependency analysis have reached about 90% [8,10], and SAGE also shows a precision of about 90% for the analysis of both word meaning and deep case [9]. We believe that the readability of the summary produced by ABISYS will improve further if the mistakes occurred in these preceding analyses could have been removed. 7.2 Evaluation of Linguistic Correctness sentence-generation based summarizing work. Here a linguistically correct sentence means a sentence holding all the necessary and correct deep cases, and being easy-to-and produce summaries for them using ABISYS. The result of the evaluation shows that about 95% of the summarized sentences have been acknowledged as correct Japanese sentences. We have developed a summarizing system ABISYS. Specifically, using the case-frames output from the semantic analysis, we assign scores to corresponding words in accordance with repetition information, context information, position information, opinion information and topic-focus informa tion. Then we determine the valuable correct and natural summary by compensating the indispensable deep cases and ex-tracting the necessary summary element words. 
The comparison with the subjective evaluations made for other summarizing sys-tems in NTCIR-2 indicates that our system is on a par with other systems in regard to the readability, while the point of content covering is much better. And about 95% of the summary sentences generated by ABISYS are acknowledged as correct Japanese. Science and Technology, under Grants-in-Aid for Scientific Research (C) 13680461. 
