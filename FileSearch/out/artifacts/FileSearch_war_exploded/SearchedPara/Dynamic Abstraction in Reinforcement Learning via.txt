 Shie Mannor shie@mit.edu Ishai Menache imenache@tx.technion.ac.il Amit Hoze amithoze@alumni.technion.ac.il Uri Klein uriklein@alumni.technion.ac.il Faculty of Electrical Engineering, Technion, 32000 Israel Keywords: Improving the performance with experience is the hall-mark of Reinforcement Learning (RL). There are quite a few successful RL algorithms that theoretically solve any problem that can be cast in the Markov Deci-sion Process (MDP) framework. Unfortunately, many RL techniques are not able to solve moderately large problems in reasonable time. The difficulty in solving such tasks is usually a result of the combination of the size of the state space with the lack of immediate re-inforcement signal (the so-called  X  X emporal credit as-signment problem X ). To date, there are three prin-cipal approaches for addressing these problems. The first approach is to consider a limited space of con-trol strategies and search this space directly (Moriarty et al., 1999), or within an actor-critic framework (see Barto et al., 1983; Baxter &amp; Bartlett, 2001). The sec-ond approach is to apply low order approximations of the value function (e.g., Bertsekas &amp; Tsitsiklis, 1995). The third approach is to decompose the control to a hi-erarchy of several simpler tasks and learn each of them independently (e.g., Dayan &amp; Hinton, 1993, Dietterich, 2000, Sutton et al., 1999, see Barto &amp; Mahadevan, 2003 for a detailed recent review). This decomposition simplifies the learning problem in two ways. First, the effective size of the state space is reduced since every sub-task considers only a smaller number of relevant states. Second, learning is accelerated since every sep-arate task is easier to learn.
 In this paper we consider the problem of automati-cally discovering subtasks and hierarchies. Since in many cases the environment is unknown (or partially known), we cannot assume that the agent has the abil-ity to identify desired subtasks beforehand. Even if the agent gathers knowledge of the environment, the de-composition to subtasks may not be straightforward. Moreover, the collection of advantageous subtasks may change and evolve throughout the learning process. We refer the reader to Barto and Mahadevan (2003) for further discussion on subtask discovery during the learning process, termed  X  X ynamic abstraction X , and its impact on RL.
 A common approach is to define subtasks in the state space context. The learning agent identifies important states, which are believed to possess some  X  X trategic X  importance and are worthwhile reaching. The agent learns sub-policies for reaching those key states. One approach is to look for states with non-typical rein-forcement (a high reinforcement gradient, for exam-ple, as in Digney, 1998). This approach may not prove useful in domains with delayed reinforcement. An-other approach is to choose states based on their fre-quency of appearance (see McGovern &amp; Barto, 2001). The motivation here is that states that have been vis-ited often in the past are likely to be a part of the agent X  X  optimal path. Exploration time may be saved by having local policies for reaching those states di-rectly. McGovern and Barto (2001) added the suc-cess condition to the frequency measure X  X tates serve as potential subgoals if they are frequently visited on successful paths but are not visited on unsuccessful ones. A problem with frequency based solutions is that the agent may need excessive exploration of the environment in order to distinguish between  X  X mpor-tant X  and  X  X egular X  states, so that options are defined at relatively advanced stages of the learning process. A different approach was introduced in Menache et al. (2002) where bottleneck states were defined as states that separate well the initial and target states. Our ap-proach deviates from these approaches by considering clusters of states as intermediate stages in the learn-ing process, rather than unique states, which leads to a more robust and versatile learning procedure. We de-fine each option as a sub-policy that allows the agent to efficiently shift from one cluster of states to the other. Our approach is to let the agent roam around the en-vironment and then after some, possibly inaccurate, information was collected perform a clustering algo-rithm, and generate options for reaching the clusters. This approach relies on the assumption that the path towards achieving a complex goal passes through inter-mediate stages (each consists of multiple states) that occupy different parts of the state space. If those stages are discovered, and a policy that reaches each of them is separately learned, the overall learning proce-dure may become simpler and faster. An additional advantage of the clustering approach is that explo-ration may become more efficient since the agent can quickly wander to states that would be otherwise less explored, since they may be harder to reach from the initial states. The input for the clustering algorithm consists of the agent X  X  recorded state transitions, which may be seen as a topological representation of the learning task dynamics. We then generalize the clus-tering algorithm by suggesting to use not only the state-transition map as an input, but also the cur-rent value estimates. The suggested algorithm encour-ages creating clusters with small deviation in the value function. The idea is to encourage the agent to travel between homogeneous clusters, increasing its proba-bility to reach clusters with  X  X nteresting X  values. In essence, the process of creating clusters may be consid-ered as bootstrapping. The clusters are formed early in the learning process, much before convergence, and are based on a rough estimate of the environment. Using this rough estimate we improve the exploration, and focus on promising areas.
 The paper is organized as follows: In Section 2 we describe the RL setup, extended to use options. The clustering problem is formally defined in Section 3. We consider two types of information that can be used by the clustering procedure. First, we limit our at-tention to clustering using topological state transition information. We then suggest to incorporate the pre-liminary value function estimation as well. Three ex-periments are described in Section 4: a simple maze world, the car-hill problem, and a more complicated maze. Section 5 contains concluding remarks. In this section we define the setup and survey the RL with options. See McGovern et al. (1997) for further details. We will consider a discrete time MDP with a finite set of states S and a finite set of actions A . At each time step t , t = 1 , 2 , . . . , the learning agent is in some state s t  X  S . The agent can choose an action a t from the set of available actions at state s t , A ( s t causing a state transition to s t +1  X  S . The agent ob-serves a scalar reward r t which is a (possibly random) function of the current state and the action performed by the agent. The agent X  X  goal is to find a map from states to actions, called a policy, which maximizes the expected discounted reward over time, IE { where  X  &lt; 1 is the discount factor and expectation is taken with respect to the random transitions, ran-dom rewards, and possibly random policy of the agent. A popular RL algorithm is the Q-Learning algorithm (Dayan &amp; Watkins, 1992). In Q-Learning the agent updates the Q-function at every time epoch. The Q-function maps every state-action pair to the expected reward for taking this action at that state, and follow-ing an optimal strategy from that point on.
 We now recall the extension of Q-Learning to Macro-Q-Learning (or learning with options, see McGovern et al., 1997). An option is a sequence of (primitive) actions that are executed by the agent (governed by a  X  X ocal X  policy) until a termination condition is met. Formally, an option is defined by a triplet  X  I,  X ,  X   X  , where: I is the options input set, i.e., all the states from which the option can be initiated;  X  is the op-tion X  X  policy, mapping states belonging to I to a se-quence of actions;  X  is the termination condition over states (i.e.,  X  ( s ) denotes the termination probability of the option when reaching state s ). When the agent is following an option, it must follow it until it termi-nates. When not following an option, the agent can choose, at any given state, either a primitive action or to initiate an option, if available (we shall use the notation A 0 ( s t ) for denoting all choices , i.e., the col-lection of primitives and options available at state s t ). The update rule for an option o t , initiated at state s t is:
Q ( s t , o t ) := Q ( s t , o t ) +  X  ( n ( t, s t , o t where  X  is the actual duration of the option o t ,  X  ( n ( t, s t , a t )) is the learning rate function which de-pends on n ( t, s t , o t ), the number of times o t was ex-ercised in state s t until time t . The update rule for a primitive action is similar with  X  = 1.
 We now add the automatic option generation pro-cedure and show how to combine it with Macro-Q-Learning. The outline of the learning procedure is de-scribed in Fig. 1. After some initiation conditions are met a clustering algorithm is invoked and options are created. The options in the context of clusters consist of supplying the shortest-path policies from each clus-ter to neighboring clusters to which it is connected(one option per neighboring cluster). We now supply addi-
Repeat: 1. Interact with environment and learn using 2. Save state transition history. 3. If clustering conditions are met, and the clus-tional details on the different steps of the algorithm. Activating clustering conditions: The timing of activating the clustering algorithm introduces a trade-off. On the one hand, we would like the clustering algorithm to be performed early in the learning pro-cess, where the impact on the exploration would be the most significant. On the other hand, if clustering is performed too early, the information obtained may not suffice for finding meaningful clusters, and the re-sulting options would contribute less to the learning effort. We found that a decent activation condition for the domains we experimented with is to wait un-til no new states were encountered for T time steps (or N episodes), indicating a stable state-transition model, with T (or N ) being a task-dependent parame-ter. Conditions for activating the clustering algorithm should in general be task dependent and call for fur-ther study.
 Translating the graph from history: Each visited state becomes a node in the graph. Each observed transition s  X  s 0 ( s, s 0  X  S ), is translated to an edge ( s, s 0 ) in the graph.
 Learning an option: After clusters have been cho-sen, a local policy for reaching neighboring clusters is learned by an Experience Replay (Lin, 1992) proce-dure. Dynamic Programming (DP) is performed sepa-rately on each cluster, in order to determine the short-est paths to the neighboring clusters. The inputs for the DP are as follows: All border states belonging to neighboring clusters are assigned an artificial positive reward; the recorded experience serves as an estima-tion for the state transition probabilities. The termi-nation probability of the option  X  ( s ) is set to 1 for border states and 0 for internal states (in addition, we limit the maximal number of actual time steps of an option to some  X  max  X  1). The details of the cluster-ing algorithm itself are the topic of the next section. In order to explain the use of a clustering algorithm in the context of option discovery in RL, we first briefly review the graph theoretic problem it solves. Consider a directed graph G = ( V, E ) ( V is the set of nodes and E is the set of edges). We denote by f a function mea-suring the quality of a cluster. The function f takes as input a subset of nodes C  X  V and the set of cor-responding edges { ( s, s 0 ) | s, s 0  X  C } X  E and returns a real number representing the cluster X  X  quality. Denote by E C minate in C j and by E C of edges that connect C i to C j or vice versa. In addi-tion, let us define the inter-cluster quality function g . The function g ( C i , C j , E C ( C i and C j ) and the set of edges between them, and returns a number representing the separation quality between the clusters. The clustering problem is to de-termine the best partition of the (encountered) states for a given graph. S The choice of functions f and g is important to the success of the algorithm. However, it turns out that even without laboriously tuning the parameters of f and g , reasonable results were obtained.
 The maximization problem posed in Eq. (1) is not triv-ial. In fact, the problem is often NP-hard even for  X  X imple X  f and g . We refer the reader to Jain and Dubes (1988) and Hochbaum (1996) for a detailed dis-cussion on the complexity of various variants of clus-tering algorithms and to Jain et al. (1999) for a survey of the vast number of applications. Given a choice of f and g we applied the so-called agglomerative approach (Anderberg, 1973; Jain &amp; Dubes, 1988). According to this approach one starts with more clusters than de-sired and then merges the clusters by selecting the pair whose merging improves Eq. (1) by most. The algo-rithm (which is polynomial in the number of nodes, assuming that a calculation of f and g can be done in polynomial time) is described in Fig. 2.

Begin with n clusters. We selected each initial clus-ter to contain one state (i.e., n = | S | ).
Repeat until stopping conditions are met: 1. Go over all pairs of neighboring clusters (i.e., 2. Calculate the utility in Eq. (1) with merging 3. Choose the pair whose union contributes the There are several possible stopping conditions for the clustering algorithm. For example, the agent can wait until the total utility in Eq. (1) reaches a predeter-mined value. Another possible stopping condition is to wait until the difference in utilities between two consecutive clustering iterations is less than some pre-determined threshold. In the experiments reported in this paper we assumed that the number of clusters is predetermined, so that the stopping condition is sim-ply to stop when the number of clusters reaches some predetermined k .
 We now introduce and motivate two possible cluster-ing methodologies, whose solution is likely to produce useful options. We start with clustering based on topo-logical information in Section 3.1. We then present in Section 3.2 an extension that considers reward related information in addition to topological information. 3.1. Clustering by Topology As the learning agent interacts with the environ-ment, information is gathered regarding the topolog-ical structure of the environment. We consider two desired properties of the partition of states to clus-ters. First, the size of the clusters should be roughly the same. If a cluster is too large then reaching it might be meaningless, and the options that reach it would probably not contribute much to the learning process. On the other hand, if the cluster is too small then the options reaching it may not play a signifi-cant role in the overall exploration effort. Second, the clusters should be well separated. It is preferable that neighboring clusters are distinct and have minimal in-teraction. This requirement captures the bottleneck notion of McGovern and Barto (2001) and Menache et al. (2002), where it was argued that bottlenecks between well separated clusters of states make useful intermediate subgoals.
 Various choices for f and g may satisfy the above prop-erties. We explored several heuristic possibilities for f , but none proved significantly better than using a triv-ial f = 0. We used g ( C i , C j , E C In words, g is proportional to the size of the smaller cluster, log the size of the larger, and inversely pro-portional to the number of edges that connect the two clusters (also g  X  0 if E C choice of g is justified as follows. The general optimiza-tion procedure (Eq. (1)) tries to maximize the sum of g between clusters. As a result, a small cluster would cause small value of g and is therefore likely to be  X  X wallowed X  by some neighboring cluster. In addition, a term of the form log(max( | C i | , | C j | )) encourages big clusters to swallow small ones. Since the total size of the clusters with the partition, the numerator encourages all the clusters to have roughly the same size. A different interpretation of the numerator in Eq. (2), is to con-sider the entropy of the clustering, the fraction of states in cluster i ) which achieves its maximum when P is a uniform distribution. The de-nominator decreases for well separated clusters (e.g., if there is a  X  X ottleneck X  between them) so that g in-creases for clusters that are distinctively apart. 3.2. Clustering by Value In addition to the topological information (state tran-sitions) there is some reward related information, which the learning agent gathers. When the clustering process is initiated, this information may be far from being complete, however it can be used in order to look for  X  X nteresting X  regions in the state space. For example, an area in the state-space with a dense con-centration of distinct rewards should not be contained in a large cluster, since careful control is required to maximally exploit it. On the other hand, an area with a few rewards may be regarded as one cluster, since the only interest of the agent is to exit it and explore other areas. Thus, we consider the current estimate of the value function and use it in order to encour-age separating clusters with an inhomogeneous value estimation. Specifically, we use a different g , where g was specified in Eq. (2) and  X ( E C P differences of the value function (that is V ( s ) = max a  X  A ( s ) Q ( s, a ), where the current estimates of the Q-function are considered) of the edges connecting the two clusters. The constant  X  &gt; 0 is chosen by trial and error. The additional term is intended to encourage uniting clusters with a small value gradient. In this section we describe three experiments. We start with a simple maze-world with a single initial state and a single goal and compare standard Q-Learning with Q-Learning with options that are discovered us-ing clustering based on topology. We then consider the classical car-hill problem and perform a similar comparison. We finally consider a larger maze with multiple positive and negative rewards, and show the advantage of clustering by value. 4.1. A Simple Maze The first experiment is with a maze environment de-scribed in Fig. 3. This is a maze with approximately 1,000 states. In each state the agent can move to one of the four directions, unless there is an obstacle in that direction. The agent starts from the upper left corner and its goal is to reach the bottom right cor-ner as quickly as possible. The immediate reward the agent obtains is 0 except for the goal state, where it is +20. Each trial starts in the upper left corner and terminates in the goal state. The discount factor was set to  X  = 0 . 9. There is a probability of 0.1 that the agent fails to move in the desired direction. We tested both standard Q-Learning and Q-Learning with op-tions using clustering by topology (see Section 3.1). The initial Q-values were set to 0 and the learning rate was a constant  X  = 0 . 1. An  X  -greedy exploration was used for both algorithms, with  X  = 0 . 3. The num-ber of clusters was set in advance to 19 (there are 19 rooms). The clustering was initiated if no new state was observed in the last N = 10 episodes. The maze is essentially comprised of rooms that are  X  X ell sepa-rated X , so the clustering algorithm worked particularly well, matching clusters to rooms in almost every run. Fig. 4 presents the performance of the greedy policy derived from the current Q-function as a function of the trial number for both algorithms. This graph rep-resents the expected number of steps to the goal (on a log scale) of the greedy policy w.r.t. to the Q-function learned at the end of the trial. It can be observed that Q-Learning with clustering approaches optimal-ity much earlier than standard Q-Learning. The ad-vantage of using the clustering algorithm is further demonstrated in Fig. 5. Here we show the Q-value of the initial state as a function of time, where each in-teraction with the environment takes one unit of time. Learning is accelerated on average by a factor of more than two, with a lower variance at the initial learning phase (a smaller variance). 4.2. The Car-Hill Problem The second experiment considers the well known car-hill problem. In this problem a car tries to climb a one dimensional hill. In order to climb it, the car has to move in the opposite direction first, to build mo-mentum, and then climb the hill. The state space of this problem is the position ( | p |  X  1) and velocity ( | v |  X  3). The agent tries to get to the right, i.e., to p = 1. The control decision is the direction of accel-eration (either left or right), see Ernst et al. (2003) for the exact specifications that were used here. We considered continuous state space in discrete time, and discretized the space uniformly to a 50  X  50 space. The discretization was performed only with respect to the learning process -the dynamics were kept continuous (we used the closest point when referring to Q-values). We tested standard Q-Learning and Q-Learning with clustering by topology on the car-hill domain. For the second algorithm we used k = 14 clusters (in practice, the number of clusters k had little effect on performance; using k  X  [10 , 20] gave similar results). The clustering algorithm was invoked after N = 10 episodes where new states were not observed. The learning parameters for the two algorithms remain the same as in the previous experiment (Section 4.1), with the reward being +1 to get to the goal,  X  1 to be thrown away at the left side, and 0 otherwise. In Fig. 6 we show the clusters generated by a typical run. It can be seen that there is a single large cluster (noted by  X  ) and then small clusters marked by the letters  X  X  X - X  X  X . The optimal policy turned up to be gaining height on the opposite direction (e.g., reaching cluster  X  X  X ), and then moving to cluster  X  X  X , possibly through cluster  X  X  X . We note that a significant part of the state space was never visited (the white area), indicating efficient exploration. In Fig. 7 we compare the Q-values of the initial state ( p =  X  0 . 5 and v = 0), showing a signifi-cant improvement made by the clustering approach. 4.3. A Multi-Reward Maze We now consider a larger (50  X  50 states) and more complicated maze with multiple positive (+20) and negative (  X  20) rewards (see Fig. 8 for the maze map). The maze follows similar rules to the simpler maze con-sidered in Section 4.1, however multiple starting states are possible (the agent starts from one of the possi-ble starting states, at random). We ran Q-Learning, Q-Learning with clustering by topology (utility func-tion of Eq. (2) with k = 40 clusters), and Q-Learning with clustering by value (utility function of Eq. (3) with  X  = 10 and k = 40 clusters). The rest of the learning parameters remained as in Section 4.1. In Fig. 9 we compare the average Q-value of the initial states using the three algorithms. Clearly, our cluster-ing approach has a significant advantage over standard Q-Learning. Moreover, the performance of clustering by value is better than the performance of clustering only by topology, as the steep rise in performance (the  X  X nee X ) starts earlier. In Fig. 8 we present the differ-ent clusters generated by the two clustering algorithms (clusters boundaries are marked with a dotted line). Evidently, the clustering by value concentrates more on the  X  X nteresting X  region (with high reward density) in the center of the maze. In this paper we explored a clustering approach to automatically generating options in RL. We consid-ered two alternative clustering heuristics and showed that the concept works well in experiments. Our learn-ing algorithm has the ability to define useful clusters, which can not be easily figured out, even when the model of the environment is known. As a consequence, learning is enhanced in tasks, which do not possess a natural hierarchical structure (e.g., the car-hill prob-lem).
 The clustering approach is not a panacea for finding options in all types of MDPs. We expect it to enhance learning when the state space is well separated. The identification of conditions under which the generated options are useful requires further study.
 The most important research direction, in our opinion, involves scaling up the method. The examples pre-sented in this paper are of moderate size, and while the performance of the clustering method is encourag-ing, some effort should be devoted to scaling it up to real-world applications. For example, when the state space is continuous the construction of the graph it-self is an issue as multiple states should probably be aggregated before using the clustering algorithm. In addition, the condition for activating the clustering al-gorithm should be modified.
 Additional directions, which we believe are promis-ing include: performing the clustering multiple times, modifying the available options during the learning process as more information becomes available;  X  X or-getting X  options that are not useful or even damaging over time; using multiple alternative clustering func-tions, and learning to use the functions that produced the most useful options (i.e., meta-learning the cluster-ing objective function); and finally an extension of the ideas presented here to the partial observed domains (cf. Theocharous &amp; Kaelbling, 2003).
 Acknowledgements We are grateful for the helpful suggestions made by three anonymous reviewers. The work of S.M. was partially supported by the National Science Founda-tion under grant ECS-0312921.

