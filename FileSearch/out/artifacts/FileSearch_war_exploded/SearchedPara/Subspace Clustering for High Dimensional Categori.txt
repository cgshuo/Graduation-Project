 Data clustering has been discussed extensively, but almost all known conventional clustering algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the data points. Existing subspace clustering algorithms for handling high-dimensional data focus on nu-merical dimensions. In this paper, we designed an iterative algorithm called SUBCAD for clustering high dimensional categorical data sets, based on the minimization of an ob-jective function for clustering. We deduced some cluster memberships changing rules using the objective function. We also designed an objective function to determine the subspace associated with each cluster. We proved various properties of this objective function that are essential fo r us to design a fast algorithm to find the subspace associated with each cluster. Finally, we carried out some experiments to show the effectiveness of the proposed method and the algorithm.
 Subspace Clustering Clustering, Subspace Clustering, Categorical Data Clustering has been used extensively as a primary tool of data mining. Many clustering algorithms have been de-signed [15; 14]. Unfortunately, most of these conventional clustering algorithms do not scale well to cluster high di-mensional data sets in terms of effectiveness and efficiency, because of the inherent sparsity of high dimensional data. In high dimensional data sets, we encounter several prob-lems. First of all, the distance between any two data points becomes almost the same [5], therefore it is difficult to differ -entiate similar data points from dissimilar ones. Secondly , clusters are embedded in the subspaces of the high dimen-sional data space, and different clusters may exist in differ-ent subspaces of different dimensions [3]. Because of these problems, almost all conventional clustering algorithms f ail to work well for high dimensional data sets. One possi-ble solution is to use dimension reduction techniques such as PCA(Principal Component Analysis) [27] and Karhunen-Lo`eve Transformation [3], or feature selection technique s. In dimension reduction approaches, one first reduces the di-mensionality of the original data set by removing less im-portant variables or by transforming the original data set into one in a low dimensional space, and then applies con-ventional clustering algorithms to the new data set. In fea-ture selection approaches, one finds the dimensions on which data points are correlated. In either dimension reduction a p-proaches or feature selection approaches, it is necessary t o prune off some variables, which may lead to a significant loss of information. This can be illustrated by considering a 3-dimension data set that has 3 clusters: one is embedded in ( x, y )-plane, another is embedded in ( y, z )-plane and the third one is embedded in ( z, x )-plane. For such a data set, an application of a dimension reduction or a feature selec-tion method is unable to recover all the clustering structur es, because the 3 clusters are formed in different subspaces. In general, clustering algorithms based on dimension reducti on or feature selection techniques generate clusters that may not fully reflect the original clusters X  structure. This difficulty that conventional clustering algorithms en-counter in dealing with high dimensional data sets motivate s the concept of subspace clustering or projected clustering [3] whose goal is to find clusters embedded in subspaces of the original data space with their own associated dimensions. Almost all the subspace clustering algorithms proposed so far are designed for clustering high dimensional numerical data sets. In this paper, we present SUBCAD(SUBspace clustering for high dimensional CAtegorical Data), a sub-space clustering algorithm for clustering high dimensiona l categorical data sets. We shall develop a method to deter-mine the subspace associated with each cluster, and we shall design an iterative method to cluster high dimensional cat-egorical data sets by treating the clustering process as an optimization problem. Since a subspace clustering algorithm CLIQUE [3] was first proposed by Aggarwal et. al., several subspace clustering a l-gorithms have been designed[3; 9; 11; 19; 1; 2; 25; 8; 22; 17]. The recent subspace clustering algorithms can be roughly divided into three categories: Grid-based algorithms such as CLIQUE [3], MAFIA [11; 19], Partitioning and/or hier-archical algorithms such as ORCLUS [2],FINDIT [25], and Neural Network-based algorithms such as PART [8](See Ta-ble 1).
 AT Algorithms DT H/P Grid-based ENCLUS [9] Num O Partitioning FINDIT [25] Num P Neural Network PART [8] Num H Other CLTree[17] Num O Table 1: A list of some subspace clustering algorithms. Data Type( DT ) indicates the type of data sets which the algo-rithm can be applied to, AT refers to Algorithm Type, Num refers to Numerical, H/P refers to Hierarchical/Partition ing, O refers to Other.
 In the algorithm of CLIQUE, it first partitions the whole data space into non-overlapping rectangular units, and the n searches for dense units and merges them to form clusters. The subspace clustering is achieved due to the fact that if a k -dimension unit ( a 1 , b 1 )  X  ( a 2 , b 2 )  X  X  ( a dense, then any k  X  1-dimension unit ( a i 1 , b i 1 )  X  ( a of the unit in the i -th dimension, 1  X  i 1 &lt; i 2 &lt; &lt; i k  X  1  X  k . ENCLUS(Entropy-based CLUStering) [9] and MAFIA(Merging of Adaptive Finite Intervals) [19; 11] are also Grid-based subspace clustering algorithms.
 PROCLUS [1] is a variation of k -Medoid algorithm [15] for subspace clustering. The PROCLUS algorithm finds out the subspace dimensions of each cluster via a process of eval-uating the locality of the space near it. FINDIT(a Fast and INtelligent subspace clustering algorithm using DImen -sion voTing)[25], ORCLUS(arbitrarily ORiented projected CLUSter generation) [2], FLOC [26] and DOC(Density-based Optimal projective Clustering) [22] are also partitioning sub-space clustering algorithms.
 PART [8](Projective Adaptive Resonance Theory) is a new neural network architecture that was proposed to find pro-jected clusters for data sets in high dimensional spaces. In PART, a so-called selective output signaling mechanism is provided in order to deal with the inherent sparsity in the full space of the high dimensional data points. PART is very effective to find the subspace in which a cluster is embedded, but the difficulty of tuning some parameters in the algorithm of PART restricts its application. CLTree(CLustering base d on decision Trees)[17] is an algorithm for clustering numer -ical data based on a supervised learning technique called decision tree construction. The resulting clusters found b y CLTree are described in terms of hyper-rectangle regions. The CLTree algorithm is able to separate outliers from real clusters effectively, since it naturally identifies sparse a nd dense regions. To describe the algorithm, we start with some notations. Given a data set D , let Q be the set of dimensions of D , i.e. Q = { 1 , 2 , ..., d } , where d is the number of dimensions of D , let Span( Q ) denote the full space of the data set, then by a subspace cluster, we mean a cluster C associated with a set of dimensions P such that (a) The data points in C are  X  X imilar X  to each other in the subspace Span( P ) of Span( Q )(i.e. the data points in C are compact in this subspace); (b) The data points in C are sparse in the sub-space Span( R ), where R = Q \ P (i.e. the data points in C are spread in this subspace).
 For convenience of presentation, we will use a pair ( C, P )( P 6 =  X ) to denote a subspace cluster, where P is the non-empty set of dimensions associated with C . In particular, if P = Q , then this cluster is formed in the whole space of the data set.
 Therefore if we have a cluster C with the associated set of dimensions P , then C is also a cluster in every subspace of Span( P ). Hence, a good subspace clustering algorithm should be able to find clusters and the maximum associated set of dimensions. Consider, for example, a data set with 5 data points of 6 dimensional(given in Table 2). In this data set, it is obvious that C = { x 1 , x 2 , x 3 } is a cluster and the maximum set of dimensions should be P = { 1 , 2 , 3 , 4 } . A good subspace clustering algorithm should be able to find this cluster and the maximum set of associated dimensions P .
 Table 2: A sample data set illustrates clusters embedded in subspaces of a high dimensional space.
 We will introduce an objective function for subspace cluste r-ing, and then treat the clustering process as an optimizatio n problem with the goal to minimize the objective functions. The objective function for clustering and the objective fun c-tion for determining the subspace of each cluster are defined in terms of compactness and separation.
 Let C be a cluster with associated set of dimensions P . We define the compactness of C in Span( P ) and the separation of C in Span( R )(where R = Q \ P ) as follows: where | P | and | R | denote the number of elements in the sets P and R , respectively, and k x  X  y k 2 P = P with  X  ( x, y ) = 0 if x = y , 1 otherwise, x j and y j are the j th dimension values of x and y , respectively, k x  X  y k defined similarly. We shall also drop the index if the whole space is involved. Then from the definition, we have A natural criterion for the effectiveness of a subspace clus-tering is to simply sum up the compactness of each cluster and then to minus the sum of separation of each cluster. This leads to the following objective function Therefore, given the number of clusters k , our goal is to partition the data set into k non-overlapping groups such that the objective function F obj defined in Equation (3) is minimized. Our algorithm is to find an approximation of the optimal partition.
 In practice, we can simplify the formulas in Equation (1), Equation (2), and therefore simplify the objective functio n. To do this, we need to define the symbol table of a data set and the frequency table for each cluster according to the symbol table. Let A j be the categorical variable of the j th dimension(1  X  j  X  d ). We define its domain by DOM ( A j ) = { A j 1 , A j 2 , ..., A jn j } and we call A n ) a state of the categorical variable A j . Then a symbol table T s of the data set is defined as follows: where s j is a vector defined as s j = ( A j 1 , A j 2 , ..., A Since there are possibly multiple states(or values) for a va ri-able, a symbol table of a data set is usually not unique. For example, for the data set in Table 2, Table 3 is one of its symbol tables. Table 3: One of the symbol tables of the data set in Table 2. The frequency table is computed according to a symbol table and it has exactly the same dimension as the symbol table. Let C be a cluster, then the frequency table T f ( C ) of cluster C is defined as where f j ( C ) is a vector defined as where f jr ( C )(1  X  j  X  d, 1  X  r  X  n j ) is the number of data points in cluster C which take value A jr at the j th dimension, i.e. where x j denotes the j th dimension of x .
 For a given symbol table of the data set, the frequency table of each cluster is unique according to that symbol table. For example, for the data set in Table 2, let ( C, P ) be a subspace use the symbol table presented in Table 3, then the corre-sponding frequency table for the subspace cluster ( C, P ) is given in Table 4.
 From the definition of frequency f jr in Equation (6), we have the following equalities: Table 4: The frequency table computed from the symbol table in Table 3. for any subspace cluster ( C, P ).
 We can now use the frequency table to simplify the formulas of compactness and separation. Let C be a cluster with the set P of dimensions associated, T f ( C ) be its frequency table. Since the square of the simple matching distance is equal to itself and using Equation (7), after a simple manipulation, we have where f j ( C ) is defined in Equation (5) and kk is the usual Euclidean norm. Note that the same notation is used for the Euclidean norm of a point in an Euclidean space and for the matching distance (defined below equation (2)) of two points in the original data space, this should be easily distinguished from the context.
 Thus from Equation (8), we obtain the following simplified formulas of compactness and separation: We have defined the objective function (3), and introduced a simple way to calculate this function. Our goal is to parti-tion the data set into k non-overlapping groups such that the function in Equation (3) is minimized. We now introduce our algorithm.
 In the first step of this algorithm, we need to initialize the partition, i.e. given the number of clusters k , we need to par-tition the data set into k non-overlapping groups. There are many ways to do this. One way is to partition the data set into k non-overlapping groups randomly, but this is not effi-cient for the clustering task in the next steps. Another way, that we shall take, is to compute the proximity matrix of the data set, then choose k most dissimilar data points(See Section 3.3) as seeds according to the proximity matrix, and then assign the remaining data points to the nearest seed. Since computing the proximity matrix for large data set is impractical, we can first draw a sample(usually of small size ) from the data set, and then compute the proximity matrix for the sample. We will discuss the initialization phase, in Section 3.3 and Section 3.4, in detail.
 After the initialization phase, we begin to optimize the par -tition such that the objective function (3) is minimized. To optimize the partition, the algorithm will move a point from its current cluster to another cluster if the movement can de -crease the objective function. In practice, the algorithm w ill stop if there is no further change of cluster memberships. We will discuss the optimization phase in Section 3.5 in detail . Algorithm 3.1 The pseudo code of SUBCAD.
 Require: D -Data Set, k -Number of Clusters; Ensure: 2  X  k  X | D | ; 1: if D is a large data set then 2: Draw a sample from D ; 3: end if 4: Compute the proximity matrix from the whole data set 5: Pick k most dissimilar data points as seeds; 6: Assign the remaining data points to the nearest seed; 7: repeat 8: for i = 1 to | D | do 9: Let ( C l , P l ) be the subspace cluster that contains 10: for m = 1 , m 6 = l to k do 11: if Inequality (18) is true then 12: Move x from C l to C m ; 13: Update subspaces P l and P m ; 14: end if 15: end for 16: end for 17: until No further change of the cluster memberships; 18: Output results.
 In summary, the algorithm consists of two phases: the ini-tialization phase and the optimization phase. In the follow -ing sections, we will discuss the criteria of moving a data point from its current cluster to another cluster and the criteria of determining the subspace associated with each cluster. In the initialization phase, we initialize the partition fo r the optimization. In partitioning clustering algorithms, the ini-tial partition is very important. Good initial partition le ads to fast convergence of the algorithm. Some initialization methods have been proposed in the literature of clustering, such as cluster-based method [7], kd -tree based method [21]. Denote by k the number of clusters in this algorithm, we first pick k most dissimilar data points as seeds, then we assign the remaining data points to the nearest seed. To make the algorithm clear, we introduce:
Definition 1. Let D be a data set, k be a positive integer such that k  X | D | . We say that x 1 , x 2 , ..., x k  X  D are k most dissimilar data points of the data set D if the following condition is satisfied where F is the class that contains all subsets E of D such that | E | = k , i.e.
 and k x  X  y k is the distance between x and y . For conve-nience, we use X ( k, D ) ( k  X | D | ) to denote the set of k most dissimilar data points of the data set D .
 Note that the set of k most dissimilar data points of a data not unique. For example, in the data set listed in Table 2, let D = { x 1 , x 2 , x 3 , x 4 , x 5 } and k = 2, then according to Definition 1, X (2 , D ) can be { x 1 , x 4 } or { x 2 , x if k = | D | , then X ( | D | , D ) = D is unique. Since there are total n k elements in F , when n is large, it is impractical to enumerate all sets in F to find a set of k most dissimilar data points. Thus we use an approximation algorithm to find a set of k data points that is near the set of k most dissimilar data points. The basic idea is to choose k initial data points, then continuously replace the bad data points with good ones until no further changes.
 More specifically, let D = { x 1 , x 2 , ..., x n } be a data set or a sample from a data set. First, we let X ( k, D ) be { x 1 , x 2 , ..., x k } , let x r , x s (1  X  r &lt; s  X  k ) be such that Secondly, for each of the data point x  X  D \ X ( k, D ), let Then if S r &gt; k x r  X  x s k , we let X ( k, D )  X  X  x }\{ x X ( k, D ); if S s &gt; k x r  X  x s k , we let X ( k, D )  X  X  x }\{ x replace X ( k, D ). In the initialization phase, we need to select k seeds from the data set. If the data set is very large, then to compute the proximity matrix of the whole data set is impractical. To make the algorithm scale to large data set, we will draw samples from the original data set and choose seeds from the samples. In this section, we will discuss sampling methods for large data sets in the initialization phase.
 Many algorithms for drawing a sample randomly from data sets have been designed, such as density biased sampling [20 ], random sampling with a reservoir [16; 23]. Also as for how to choose the sample size, we can use Chernoff bounds [18; 12] to determine the sample size such that the sample con-tains as least a certain amount data points from an arbitrary cluster with a high probability.
 Let C b ( k, n ) be the minimum size of sample S such that every cluster has more than  X  data points in the sample with probability 1  X   X  , then C b ( k, n ) can be computed from the following equation [25; 12]:
C b ( k, n ) =  X k X  + k X  log 1 where  X  is given by and C min is the smallest cluster in the partition. Note that for a small data set, sampling is not necessary. When sampling is necessary depends on the machine on which the algorithm runs. But in general, we can determine whether or not to draw a sample as follows. If n &gt; C b then we take sample of size C b ( k, n ); if n &lt; C b ( k, n ), we just use the whole data set to compute proximity matrix. If n &gt; C b ( k, n ), from Equation (15) and notice that  X   X  1, we have Thus, if inequality (16) is true, we draw samples of size C ( k, n ) from the original data sets. We take  X  = 50 and  X  = 0 . 01 for default values In the optimization phase of the algorithm, we need to re-assign the data points in order to minimize the objective function (3) and update the subspaces associated with these clusters whose memberships are changed.
 set D , let x be a data point in the subspace cluster ( C To achieve the membership changing rules, we use  X  X xact assignment test X  [24] technique in our algorithm. We will move x from subspace cluster ( C l , P l ) to another subspace cluster ( C m , P m ) ( m 6 = l ) if the resulted cost function de-creases, i.e. if the following inequality is true: where C l  X  x means C l \{ x } , C m + x means C m  X  X  x } . Inequality (17) is equivalent to Hence if Inequality (18) is true, the data point x will be moved from C l to C m . After this movement, the sets of sub-space dimensions P l and P m will of course be updated(See Section 4).
 Let the symbol table of the data set D be T s , and let r 1 , 2 , ..., d ) be the subscript such that x j = A jr j . Then the frequency table of C l  X  x is the same as the frequency table of C l except for the terms f jr j ( C l  X  x ) = f jr j ( C 1 , 2 , ..., d ); the frequency table of C m + x is the same as the frequency table of C m except for the terms f jr j ( C x ) = f jr j ( C m ) + 1. These relationships enable us to rewrite Inequality (18) in a more compact form, omitted here due to limitation of spaces. Let ( C, E ) be a subspace cluster of a d -dimensional data set D . In order to determine the set P of subspace dimensions associated with C , we define an objective function whose domain is all the subsets of Q = { 1 , 2 , ..., d } as follows:
F ( C, E ) = Cp( C, E )+1  X  Sp( C, Q \ E ) ,  X  6 = E  X  Q, (19) where Cp( C, E ) and Sp( C, Q \ E ) are the compactness and separation of cluster C under the subspace dimensions set E .
 Our general idea to determine the set P associated with clus-ter C is to find a P such that the objective function defined in Equation (19) is minimized. Also from Equation (8), if P 6 =  X  or P 6 = Q , we can write the objective function in Equation (19) as F ( C, E ) = 1  X  where R = Q \ P .
 We now establish some useful properties of the objective function.

Theorem 1 (Condition of Constance). The objec-tive function defined in Equation (19) is constant for any subset E of Q if and only if In addition, if the objective function is a constant, then it is equal to 1.  X From the definition of the objective function F ( C, E ), the proof of the above theorem is straightforward and is thus omitted. By Theorem 1, if the objective function is constant , then the objective function is minimized at any subset of Q . In this case, we define the subspace dimensions associated with C to be Q . If the objective function is not constant, then we define the subspace dimensions associated with C to be the set P  X  Q that minimizes the objective function. In fact, we can prove late that such a set P is unique if the objective is not constant. Hence, we have the following definition of subspace associated with each cluster.
Definition 2. Let C be a cluster, then the set P of sub-space dimensions associated with C is define as follows: 1. If the objective function F ( C, E ) is constant for any 2. If the objective function F ( C, E ) is not constant, then
Remark 1.  X From Definition 2, the set P defined in Equa-tion (22) is non-empty. Moreover, if the objective function F ( C, E ) is not constant, then the set P is a true subset of Q , i.e. P $ Q .
 Below we will prove that if the objective function F ( C, E ) defined in Equation (20) is not constant, then the set P defined in Equation (22) is unique. To do this, we first derive some properties of the set P in Equation (22). Theorem 2. Let ( C, P ) be a subspace cluster of data set D in a d -dimensional space( d &gt; 2 ), let P be defined in Equa-tion (22). Then 1. for a given r  X  P , if there exists a s (1  X  s  X  d ) such 2. for a given r  X  R , if there exists a s (1  X  s  X  d ) such Theorem 2 can be proved by way of contradiction. To prove the first part, for example, suppose s /  X  P and let P new P  X  X  s }\{ r } , R new = R  X  X  r }\{ s } , then one can show that F ( C, P new ) &lt; F ( C, P ), a contradiction. A detailed proof can be found in [10] Let ( C, P ) be a subspace cluster, where P is defined in Equa-tion (22). Then from Theorem 2, there exists no r, s (1  X  r, s  X  d ) such that r  X  P, s  X  R and k f r ( C ) k &lt; k f Hence we have the following corollary:
Corollary 3 (Monotonicity). Let ( C, P ) be a sub-space cluster of D , where P is the set of subspace dimensions defined in Equation (22) and let T f ( C ) be the frequency table of C , then for any r  X  P and s  X  R ( R = Q \ P ) , we have Now we consider the case where there exist r  X  P and s  X  R such that k f r ( C ) k = k f s ( C ) k .

Theorem 4. Let ( C, P ) be a subspace cluster of d di-mensional data set D ( d &gt; 2 ), where P is defined in Equa-tion (22), let T f ( C ) be the frequency table defined in Equa-tion (4), let r, s (1  X  r, s  X  d ) be given so that k f s k f ( C ) k . Then either r, s  X  P or r, s  X  R , i.e. r, s must be in the same set of P or R , where R = Q \ P .
 Theorem 4 can be proved using a similar argument for The-orem 2, details can be found in [10].  X From Corollary 3 and Theorem 4, we have the following:
Corollary 5. Let ( C, P ) be a subspace cluster of D , where P is the set of subspace dimensions defined in Equa-tion (22), and let T f ( C ) be the frequency table of C . If the objective function F ( C, E ) is not constant, then for any r  X  P and s  X  R ( R = Q \ P ) , we have Now using Corollary 5, we can prove the uniqueness of the set P defined in Equation (22).

Theorem 6 (Uniqueness of Subspace). Let F ( C, E ) be the objective function defined in Equation (19) and let P be the set defined in Equation (22). If the objective function F ( C, E ) is not constant, then the set P is unique. Also from the Corollary 5 and the Theorem 6, we have the following theorem, based on which we can design a very fast algorithm to find the set of subspace dimensions associated with each cluster. The detailed proof of the following theo-rem can also be found in [10].

Theorem 7 (Contiguity). Let ( C, P ) be a subspace cluster of D , where P is the set of subspace dimensions de-fined in Equation (22). Let T f ( C ) be the frequency table of Finally, let G s be the set of subscripts defined as If the objective function defined in Equation (19) is not con-stant, then the set of subspace dimensions P defined in Equa-tion (22) must be one of the P k  X  X ( k = 1 , 2 , ..., | G as follows: where g 1 &lt; g 2 &lt; &lt; g | G s | are elements of G s Based on Theorem 7, we can find the set of subspace dimen-sions P for a cluster C very fast. There are totally 2 d  X  1 non-empty subsets of Q , it is impractical to find an optimal P by enumerating these 2 d  X  1 subsets. Based on Theo-rem 7, we can design a fast algorithm for determining the set of subspace dimensions. To evaluate the performance of the algorithm, we imple-mented our algorithm on Sun Blade 1000 workstation using GUN C++ compiler. In this section, we shall use experi-mental results to show the clustering performance of SUB-CAD. We chose not to run our experiments on synthetic datasets, not only because synthetic data sets may not well repre-sent real world data [13], but also because there is no well established categorical data generation method. Therefor e, instead of generating synthetic data to validate the cluste r-ing algorithm we choose three real world data sets obtained from UCI Machine Learning Repository [6]. All these data sets have class labels assigned to the instances. The soybean data set has 47 records each of which is de-scribed by 35 attributes. Each record is labelled as one of the 4 diseases: diaporthe stem rot, charcoal rot, rhizocton ia root rot and phytophthora rot. Except for the phytoph-thora rot which has 17 instances, all other diseases have 10 instances each. Since there are 14 attributes that have only one category, we only selected other 21 attributes for the purpose of clustering. The Wisconsin breast cancer data set has total 699 records, each of which is described by 10 categorical values. There are 16 records that have missing values. Since our algo-rithms do not deal with missing values, we delete the 16 records from the data set and use the remaining 683 records for testing. The Congressional voting data set includes votes for each of the U.S. House of Representatives Congressmen on the 16 key votes identified by the CQA. It has 435 instances(267 democrats, 168 republicans) and some of the instances have missing values, we denote the missing value by  X ? X  and treat it as the same as other values.
 Table 5: The misclassification matrix of the result obtained by applying SUBCAD to the soybean data, where D,C,R,P denote four different diseases. To measure the qualities of clustering results of our cluste r-ing algorithms, we use clustering accuracy measure r defined as follows [13]: where a i is the number of instances occurring in both clus-ter i and its corresponding class, and n is the number of instances in the data set.
 Table 5 shows the misclassification matrix of the cluster-ing result by applying our algorithm to the soybean data. According to the clustering accuracy measure r defined in Equation (29), the clustering accuracy is Table 6: The set of subspace dimensions associated with each cluster for the soybean data, where Q = { 1 , 2 , ..., 21 } . Table 6 gives the subspace dimensions associated with each cluster in Table 5. Cluster 1 is formed in a 20-dimensional space, Cluster 2 and Cluster 4 are formed in the same sub-space which is 19-dimensional, Cluster 3 is formed in a 18-dimensional subspace. The result shows that the clusters are formed almost in the whole space. Hence if apply con-ventional clustering algorithms(i.e. not subspace cluste ring algorithms), such as k -Modes[13], to the soybean data, one will get almost the same results.
 Table 7 shows the misclassification matrix of the clustering result by applying our algorithm to the Wisconsin breast cancer data. Similarly, the clustering accuracy is Table 7: The misclassification matrix of the result obtained by applying SUBCAD to the Wisconsin breast cancer data. Table 8: Sets of subspace dimensions associated with each cluster for the Wisconsin breast cancer data, where Q = { 1 , 2 , ..., 10 } .
 Table 8 gives the subspace dimensions associated with each cluster of the Wisconsin breast data. One cluster is formed in a 1-dimensional subspace, while the other one is formed in a 9-dimensional subspace. Under our objective function for determining subspaces, one cluster tends to have low dimen-sionality while the other tends to have high dimensionality . Table 9 shows the misclassification matrix of the clustering result by applying our algorithm to the congressional votin g data. Similarly, the clustering accuracy is Table 9: The misclassification matrix of the result obtained by applying SUBCAD to the congressional voting data. Table 10: Sets of subspace dimensions associated with each cluster for the congressional voting data, where Q = { 1 , 2 , ..., 16 } .
 The subspace dimensions associated with the clusters of con -gressional voting data is given in Table 10. Similar to the clustering results of Wisconsin breast cancer data, one clu s-ter of the congressional voting data has a low dimensionalit y while another has a high dimensionality. In this paper we presented SUBCAD, an algorithm for sub-space clustering high dimensional categorical data. We tre at both the process of clustering and the process of determin-ing the subspaces of clusters as a process of optimizing a certain cost function. The idea of optimization in deter-mining the subspace of each cluster enables us to rapidly identify the subspaces in which the clusters are embedded. We tested the algorithm using various real world data sets from UCI Machine Learning Repository [6], with very good clustering accuracy. It should be mentioned that for some data sets, the algorithm tends to find some clusters in high-dimensional subspaces in conjunction with other clusters in low-dimensional subspaces. The rate of convergence de-pends on the size of the date set, as shown in our simulation on the Connect-4 Database (67557 records, each of which is described by 42 attributes). Furthermore, SUBCAD re-quires the number of clusters as an input parameter, and hence how to incorporate the existing methods of selecting this parameter into SUBCAD remains an interesting and challenging problem.
 This research was partially supported by NSERC(Natural Sciences and Engineering Research Council of Canada), by CRC(Canada Research Chairs) Program, and by Generation 5. [1] C. Aggarwal, J. Wolf, P. Yu, C. Procopiuc, and J. Park. [2] C. Aggarwal and P. Yu. Finding generalized projected [3] R. Agrawal, J. Gehrke, D. Gunopulos, and P. Ragha-[4] M. Anderberg. Cluster analysis for applications . Aca-[5] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. [6] C. Blake and C. Merz. UCI repository of machine [7] P. Bradley and U. Fayyad. Refining initial points for [8] Y. Cao and J. Wu. Projective ART for clustering [9] C. Cheng, A. Fu, and Y. Zhang. Entropy-based sub-[10] G. Gan. Subspace clustering for high dimendional cate-[11] S. Goil, H. Nagesh, and A. Choudhary. MAFIA: Ef-[12] S. Guha, R. Rastogi, and K. Shim. CURE: an efficient [13] Z. Huang. Extensions to the k -means algorithm for clus-[14] A. Jain and R. Dubes. Algorithms for Clustering Data . [15] L. Kaufman and P. Rousseeuw. Finding Groups in [16] K. Li. Reservoir-sampling algorithms of time complex-[17] B. Liu, Y. Xia, and P. Yu. Clustering through decision [18] R. Motwani and P. Raghavan. Randomized Algorithms . [19] H. Nagesh, S. Goil, and A. Choudhary. A scalable paral-[20] C. Palmer and C. Faloutsos. Density biased sampling: [21] D. Pelleg and A. Moore. Accelerating exact k-means [22] C. Procopiuc, M. Jones, P. Agarwal, and T. Murali. [23] J. Vitter. Random sampling with a reservoir. ACM [24] D. Wishart. k -means clustering with outlier detection, [25] K. Woo and J. Lee. FINDIT: a fast and intelligent sub-[26] J. Yang, W. Wang, H. Wang, and P. Yu.  X  -clusters: cap-[27] K. Yeung and W. Ruzzo. Principal component analy-
