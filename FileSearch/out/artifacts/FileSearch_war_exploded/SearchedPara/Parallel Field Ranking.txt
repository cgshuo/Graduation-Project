 Recently, ranking data with respect to the intrinsic geo-metric structure (manifold ranking) has received consider-able attentions, with encouraging performance in many ap-plications in pattern recognition, information retrieval and recommendation systems. Most of the existing manifold ranking methods focus on learning a ranking function that varies smoothly along the data manifold. However, beyond smoothness, a desirable ranking function should vary mono-tonically along the geodesics of the data manifold, such that the ranking order along the geodesics is preserved. In this paper, we aim to learn a ranking function that varies linearly and therefore monotonically along the geodesics of the data manifold. Recent theoretical work shows that the gradient field of a linear function on the manifold has to be a parallel vector field. Therefore, we propose a novel ranking algo-rithm on the data manifolds, called Parallel Field Ranking. Specifically, we try to learn a ranking function and a vector field simultaneously. We require the vector field to be close to the gradient field of the ranking function, and the vec-tor field to be as parallel as possible. Moreover, we require the value of the ranking function at the query point to be the highest, and then decrease linearly along the manifold. Experimental results on both synthetic data and real data demonstrate the effectiveness of our proposed algorithm. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Algorithms, Theory Manifold, Ranking, Vector field
Ranking is a fundamental problem in many areas includ-ing data mining, machine learning and information retrieval [17, 10, 6]. Given a query, we aim to learn a real-valued function f that ranks the data points according to their rel-evance to the query. In other words, for any two data points x and x j , f ( x i ) &gt;f ( x j )if x i is more relevant to the query than x j ,andvice-versa.

In many real-world applications, one is often confronted with high dimensional data, such as images, documents and videos [21, 22]. However, there is a strong intuition that the high dimensional data may have a lower dimensional intrin-sic representation. Various work in literature have consid-ered the case where the data is sampled from a submanifold embedded in the ambient Euclidean space [2, 19, 23]. In this paper, we are interested in the ranking problem on the data represented by vectors in Euclidean space, under the manifold assumption [28, 1, 29].

Most of the existing ranking algorithms on data manifolds are based on the Laplacian regularization framework [28, 29], with promising performance observed on various data types such as image [9], video [27], and text [25]. These methods usually construct nearest neighbor graphs over the data to model the intrinsic geometric structure, and use the Graph Laplacian [5] to ensure that the ranking function varies s-moothly along the manifold. They essentially spread the ranking scores via the graph iteratively until a stationary state is achieved. The Laplacian-based ranking framework has some nice interpretations including close relationships to personalized PageRank [28, 17] and HITS [10]. However, recent theoretical analysis [12, 13] shows that the Laplacian regularizer is way too general for measuring the smoothness of the function. Although it is ensured that data points con-nected by edges in the graph have similar ranking scores, the actual variation of the ranking function along the geodesics of the data manifold is unknown. Ideally, beyond smooth-ness, a desirable ranking function should vary monotonically along the geodesics of the manifold. Therefore, the ranking order of the data points along the geodesics of the manifold could be well preserved. Moreover, according to the nature of the ranking problem, no sample in the data set should be more relevant to the query than the query itself. So the ranking function should have the highest value at the query point, and then decrease to other points nearby.

In this paper, we propose a novel ranking algorithm on the data manifolds termed Parallel Field Ranking (PFRank), which learns a ranking function that has the highest value at the query point, and varies linearly and therefore mono-Figure 1: We aim to design a ranking function that has the highest value at the query point marked by red, and then decreases linearly along the geodesics of the manifold, which is equivalent to its gradient field being parallel along the geodesics. The arrows above denote the gradient field of the ranking func-tion, and the green lines denote the geodesics of the data manifold. tonically along the geodesics of the data manifold. A func-tion that varies linearly along the geodesics of the manifold is called a linear function on the manifold [18], which could still be nonlinear in the Euclidean space. It was shown that the gradient field of a linear function on the manifold has to be a parallel vector field (or parallel field in short) [18]. Be-sides, in order to ensure that the query has the highest rank-ing score, the gradient field of the ranking function should point to the query at the neighborhood around the query, since the value of a function increases towards the direction pointed by its gradient field. Figure 1 shows an example of the ranking function that we aim to learn on a data set sampled from a 2D sphere manifold in the 3D Euclidean s-pace, where the red point denotes the query, the green lines denote the geodesics, and the arrows denote the gradient field of the ranking function. The size of each point denotes the ranking order generated by the ranking function, where large points are ranked higher than small ones. As can be observed, the ranking results generated by a function that has the highest value at the query point and varies linearly along the geodesics are quite reasonable. The gradient field of the function points to the query, and is parallel along the geodesics which pass through the query.

However, it is very difficult to design constraints on the gradient field of a function directly [13]. Instead, motivated by the recent progress in semi-supervised regression [13], we propose to learn a vector field to approximate the gradient field of the ranking function, and then design constraints on the vector field. In this paper, we propose to learn a ranking function and a vector field simultaneously based on three intuitions: 1. The vector field should be close to the gradient field of 2. The vector field should be as parallel as possible. 3. The vector field at the neighborhood around the query
By encoding the above three intuitions, the gradient field of the learned ranking function should be as parallel as pos-sible, and is forced to point to the query at the neighbor-hood around the query. Hence the ranking function learned by our method decreases linearly from the query to other points along the geodesics of the data manifold.

The rest of this paper is organized as follows. We briefly review some background knowledge related to ranking under the manifold assumption and vector fields in Section 2. Our Parallel Field Ranking algorithm encoding all the three in-tuitions proposed above is introduced in Section 3. Section 4 presents an optimization scheme that solves our objec-tive function. Section 5 provides the experimental results on both synthetic data and real data. Finally, we conclude this work in Section 6.
Our algorithm is fundamentally based on vector fields in geometry and vector calculus. Also, for ranking methods under the manifold assumption, the most related work is the Laplacian-based manifold ranking [28]. In this section, we give a brief introduction of [28], and review some background knowledge related to vector fields.
Let M be a d -dimensional submanifold in the Euclidean space R m .Given n data points { x 1 ,...,x n } X  R m on M where x q is the query (1  X  q  X  n ),weaimtolearnaranking function f : M X  R , such that  X  i, j  X  X  1 ,...,n } , f ( x f ( x j )if x i is more relevant to the query x q than x j vice-versa.

The intuition behind [28] is that if two points x i and x are sufficiently close to each other, then their ranking scores f ( x i )and f ( x j ) should be close as well. Let W  X  R n  X  n a symmetric similarity matrix, and y =[ y 1 ,...,y n ] T be an initial ranking score vector which encodes some prior knowl-edge about the relevance of each data point to the query. Then [28] minimizes the following objective function: J
MR ( f )= where w ij denotes the element at the i -th row and j -th col-umn of W ,  X &gt; 0 is a regularization parameter and D is a di-agonal matrix used for normalization with D ii = n j =1 w The first term in the above function aims to learn a ranking function that varies smoothly along the data manifold. The second term ensures the final ranking results to be close to the initial ranking score assignment y .
 Let f =[ f ( x 1 ) ,...,f ( x n )] T . The closed form solution of Eq. (1) is the following: where I is an identity matrix of size n  X  n . L = I  X  D  X  1 / 2 WD  X  1 / 2 is the normalized Graph Laplacian [5]. We can also obtain the same solution via an iterative scheme: where each data point spreads the ranking score to its neigh-bors iteratively.

The above ranking framework based on Laplacian regu-larization has enjoyed long-lasting popularity in the commu-nity, with successful extensions in content-based image re-trieval [9], ranking on both undirected and directed graph-s [1], ranking on multi-typed interrelated web objects [8], ranking tags over a tag similarity graph [14], etc. Howev-er, for the Laplacian-based ranking framework, we do not exactly know how the ranking function varies, and whether the ranking order of the data points is preserved along the geodesics of the data manifold. [29] points out some draw-backs of [28] and proposes a more robust method by using an iterated graph Laplacian. But [29] still works under the Laplacian-based ranking framework, addressing the smooth-ness of the function. Besides, the Laplacian-based ranking framework requires the final ranking results to be close to the initial ranking score assignment y . In the situation when there is no prior knowledge about the relevance of each data point, people usually assign y i =1if x i is the query and y = 0 otherwise. This might not be very reasonable, and the final ranking results are likely to be biased towards the initial ranking score assignment.
In geometry and vector calculus, a vector field is a map-ping from a manifold M to tangent spaces [18]. We can think of a vector field on M as an arrow in the same way as we think of the vector field in Euclidean space, with a given magnitude and direction, attached to each point on M ,and chosen to be tangent to M .

As discussed before, we aim to learn a ranking function f : M X  R that varies linearly along the manifold M . We first review the definitions of parallel fields and linear functions on the manifold [13].

Definition 1. Parallel Field [18]. A vector field X on manifold M is a parallel field if where  X  is the covariant derivative on M .

Definition 2. Linear Function [18]. A continuous func-tion f : M X  R is said to be linear if for each geodesic  X  .

In this paper, a function f is linear means that it varies linearly along the geodesics of the manifold. This definition is a natural extension of linear functions on the Euclidean space.

Then the following proposition reveals the relationship be-tween a parallel field and a linear function on the data man-ifold:
Proposition 1. [18] Let V be a parallel field on the man-ifold. If it is also a gradient field for function f , V = then f is a linear function on the manifold.
In this section, we propose the objective function which learns a ranking function that decreases linearly from the query to other points along the data manifold.
As discussed before, we aim to design regularization terms that ensure the linearity of the ranking function with respect to the data manifold, which is equivalent to ensuring the parallelism of the gradient field of the function. However, it is very difficult to design constraints on the gradient field of a function directly [13]. Following the above analysis, we propose to learn a vector field to approximate the gradient field of the ranking function, and require the vector field to be as parallel as possible. Let C  X  ( M ) denote smooth functions on M . Following [13], we learn a ranking function f and a vector field V on the manifold simultaneously with two constraints, which correspond to the first two intuitions proposed in Section 1:  X  V measures the change of the vector field V .If  X  V van-ishes, then V is a parallel field. Then the following objective function learns a f that varies linearly along the manifold, from the vector field perspective [13]: where  X  1 &gt; 0and  X  2 &gt; 0 are two regularization parameters. R 0 is a loss function that ensures the predicted ranking score for the query x q to be close to a constant positive number y .Ifweremove R 0 from Eq. (7), then if f is an optimal easy to check that f + c is also an optimal solution, where c could be any constant number. Therefore, by adding the R 0 term, we can get a unique solution of our ranking function f . In principle, y q could be any positive number. Following [28, 13], we set y q = 1, and use the squared loss R 0 ( x q ,y ( f ( x q )  X  y q ) 2 for simplicity.
Since the manifold M is unknown, the function f which minimizes Eq. (7) can not be directly solved. Following [13], we introduce how to discretize the continuous objective func-tion (7) in this subsection.

For simplicity, let f i = f ( x i ), i =1 ,...,n . Then our goal is to learn a ranking score vector f =[ f 1 ,...,f n ] T .Wefirst build a nearest neighbor graph G among the data. Let W be the corresponding affinity matrix of G . There are many choices of the affinity matrix W . A simple definition is as follows: where w ij denotes the element at the i -th row and j -th col-umn of matrix W . N k ( x i ) denotes the set of k nearest neigh-bors of x i .Thenforeach x i , we can estimate its tangent space T x i M by performing PCA on its local neighborhood. We choose the eigenvectors corresponding to the d largest eigenvalues since T x i M is d -dimensional. Let T i  X  R m  X  d the matrix whose columns constitute an orthonormal basis for T x i M . It is easy to show that P i = T i T T i is the unique or-thogonal projection from R m onto the tangent space T x i [7]. That is, for any vector a  X  R m ,wehave P i a  X  T x i and ( a  X  P i a )  X  P i a .
Let V be a vector field on the manifold M .Foreach point x i ,let V x i denote the value of the vector field V at x and  X  V | x i denote the value of  X  V at x i .Accordingtothe definition of vector field, V x i should be a vector in tangent space T x i M . Therefore, it can be represented by the local coordinates of the tangent space, V x i = T i v i ,where v We define V = v T 1 ,...,v T n T  X  R dn .Inotherwords, V is a dn -dimensional column vector concatenating all the v i  X  X .
Then according to the analysis in [13], R 1 reduces to the following: And R 2 can be discretized to the following:
As discussed in Section 1, a function that varies linearly along the manifold is good for ranking. However, merely ensuring the linearity is not enough. Furthermore, we hope that the ranking function has the highest value at the query point, i.e., no sample in our data set is more relevant to the query than the query itself. Therefore, the ranking function should decrease from the query to neighboring data points, which is equivalent to that the gradient field of the ranking function at the neighborhood around the query should all point to the query. Figure 2 presents a simple toy problem to illustrate this idea. Suppose we are given a set of da-ta points sampled from a 2D rectangle as shown in Figure 2(a). The query is marked by  X + X . Figure 2(b) shows the ranking results without employing a third regularizer R 3 which addresses our third intuition, where the arrows de-note the gradient field of the ranking function, and the size of the circle at each point denotes the ranking order. Points marked by large circles are ranked higher than those marked by small circles. It is easy to see that the ranking function learned in Figure 2(b) varies linearly, and its gradient field is a parallel field. However, some data points are even ranked higher than the query itself, and points far from the query could be ranked higher than those close to the query, which is not reasonable. By addressing the third intuition via R we force the gradient field at the neighborhood around the query to point to the query while still requiring the gradi-ent field to be parallel along the geodesics, generating good ranking results as shown in Figure 2(c).

Since we approximate the gradient field of the ranking function by the vector field V through R 1 , we can easily control the gradient field using V . Therefore, we require V at the neighborhood around the query to point to the query. Let i  X  j denote that x i and x j are neighbors. We now propose the third regularization term R 3 , which addresses the third intuition proposed in Section 1:
Recall that V x j denotes the value of the vector field V at x , which is a vector in tangent space T x j M .( x q  X  x j a vector pointing from a neighboring point x j to x q ,and P ( x q  X  x j ) is its projection to T x j M . Therefore, ensuing V x j and P j ( x q  X  x j ) to be similar forces the vector field at x j to point from x j to x q .

Let I denote a n  X  n matrix where the entry at the q -th row and q -th column is 1, and all the other entries are 0. Let y  X  R n be a column vector where the q -th entry is y q (= 1), and all the other entries are 0. Then we have:
Combining R 0 in Eq. (12), R 1 in Eq. (9), R 2 in Eq. (10) and R 3 in Eq. (11), our final objective function can be for-mulated as follows: J ( f, V )= R 0 ( x q ,y q ,f )+  X  1 R 1 ( f, V )+  X  2 R 2
The trade-off among the three regularization terms is con-trolled by the parameters  X  1 ,  X  2 and  X  3 in the range of (0 , +  X  ).
In this section, we discuss how to solve our objective func-tion (13).
 Let D be a diagonal matrix where D ii = n j =1 w ij .Let L = D  X  W denote the Laplacian matrix [5] of the graph. Then we can rewrite R 1 as follows: where s ij  X  R n is a selection vector of all zero elements except for the i -th element being  X  1andthe j -th element being 1. We further construct a dn  X  dn block diagonal matrix G ,anda dn  X  n block matrix C =[ C T 1 ,...,C T n ] Let G ii denote the i -th d  X  d diagonal block of G ,and C denote the i -th d  X  n block of C , we define:
With some algebraic transformations, it is easy to check that R 1 in Eq. (14) can be written as follows:
Similarly, in order to rewrite R 2 into a simplified form, we define Q ij = T T i T j , i, j =1 ,...,n .Let I denote the identity matrix of size n  X  n . We further construct a dn sparse block matrix B .Let B ij denote each d  X  d block, i, j =1 ,...,n , then we define Then we can rewrite R 2 as follows:
For R 3 ,weconstructa dn  X  dn block diagonal matrix D , and a dn  X  1blockvector H =[ H T 1 ,...,H T n ] T .Let D jj denote the j -th d  X  d diagonal block of D ,and H j denote the j -th d  X  1blockof H . We define: where I d is an identity matrix of size d  X  d .Nowwecan rewrite R 3 as follows:
Combining R 1 in Eq. (17), R 2 in Eq. (20) and R 3 in E-q. (23), we get the following simplified form of our objective function:
From Eq. (24), it is easy to compute the partial derivative of J ( f, V )withrespectto f and V as follows:
Requiring that the derivatives vanish, our solution could be obtained by solving the following linear equation system: which is further equivalent to solving the following linear system:
In this section, we empirically evaluate the effectiveness of our proposed Parallel Field Ranking (PFRank) algorith-m with comparison to several existing ranking algorithms. Since we focus on learning a ranking function under the manifold assumption, the ma in comparative method is the Laplacian-based manifold ranking algorithm (MR) [28]. We use the same graph structure according to Eq. (8) and em-pirically set the number of nearest neighbors to be 15 for both PFRank and MR. The parameter setting of MR is the same as the original paper [28, 9]. We empirically set  X  1 =  X  2 =  X  3 =0 . 01 in our PFRank algorithm.

We also compare with the SVM method, which has been successfully applied to image retrieval [24]. With the spec-ified relevant/irrelavant information, a maximal margin hy-perplane could be built to separate the relevant data points from the irrelevant ones. Then we can return the data points farthest from the SVM boundary as the relevant ones. We use the LIBSVM toolbox [4] in our experiments. However, with a single query, there are no irrelevant data specified by the user. Following [26], we adopt the pseudo relevance feed-back strategy [15]. Specifically, the nearest 10 data points to the query measured by the Euclidean distance are con-sidered to be relevant, and the farthest 10 are treated as irrelevant. Then we can run SVM for ranking.

In the following, we begin with a simple synthetic example to give some intuition about how PFRank works.
A simple synthetic example is given in Figure 3. We ran-domly sample 1500 data points from a Swiss roll with a hole as shown in Figure 3(a). This data set lies on a 2D manifold in the 3D Euclidean space. The query is marked by  X + X . Figure 3(b)  X  (d) show the ranking order generated by PFRank, MR and SVM, respectively, where the data points marked by warmer color (such as red) are ranked higher than those marked by colder color (such as blue). We can see that SVM does not take the manifold structure of the data in-to consideration. The ranking functions generated by both PFRank and MR vary smoothly along the data manifold, and PFRank better preserves the ranking order of the data points along the geodesics of the manifold. For example, the geodesic distance between the query and the triangle point is smaller than that between the query and the square point. Therefore, the triangle point should rank higher than the square point. However, MR ranks the square point higher than the triangle point, which is counterintuitive.
Note that once the ranking function f is obtained, we can estimate its gradient field reversely. Specifically, the gradi-ent field at each point x i (1  X  i  X  n ), denoted by  X  f | be computed by minimizing the following objective function at the local neighborhood of x i :
We further plot the vector field learned by PFRank in Fig-ure 3(e), and plot the gradient fields of the ranking functions learned by PFRank, MR and SVM (computed via Eq. (30)) in Figure 3(f)  X  (h), respectively. For SVM, many data points have the same ranking scores (i.e., same distance to the SVM boundary), therefore are ranked randomly. And the gradi-ent field of the ranking function has 0 values at many places, therefore are left blank in Figure 3(h). Note that the value of a ranking function increases towards the direction pointed by its gradient field. Therefore, the ranking orders of data points get higher towards the direction pointed by the ar-rows of the gradient field. It can be observed that both the vector field and the gradient field of the ranking function learned by PFRank point to the query along the geodesics of the manifold, and are quite parallel. On the contrary, the gradient fields of the ranking functions learned by M-R and SVM do not vary smoothly along the manifold, and therefore cannot preserve the ranking order.
In this subsection, we apply our proposed ranking algo-rithm to the image retrieval problem in real world image databases. Given an image as a query, we hope to rank the images in our database according to their relevance to the query. We begin with a description of the data preparation.
Two real world data sets are used in our experiments. The first one contains 5,000 images of 50 semantic categories, from the COREL database. For each image, we extract a 297-dimensional feature vector which combines the following information:
The second data set is from the CMU PIE face database [20]. This database contains 68 subjects with 41,368 face images as a whole. The face images were captured by 13 syn-chronized cameras and 21 flas hes, under varying pose, illu-mination and expression. In this experiment, we choose the frontal pose (C27) with varying lighting conditions, which leaves us 21 images per subject. Preprocessing to locate the faces were applied. Original images were normalized (in s-cale and orientation) such that the two eyes were aligned at the same position. Then the facial areas were cropped into the final image for matching. The size of each cropped image in all the experiments is 32  X  32 pixels, with 256 gray levels per pixel. Therefore, each image can be represented by a 1024-dimensional feature vector in the image space. No further preprocessing is done. We describe our experimental setup in this subsection. Both of the two image data sets we use have category labels. In the COREL data set, images from the same category belong to the same semantic concept, such as eagle, bird, elephant, etc. In CMU PIE data set, images from the same category belong to the same person (subject). Therefore, given a query, images that belong to the same category as the query are judged relevant. For each data set, we randomly choose 10 images from each category as queries, and average the retrieval performance over all the queries. It is worth noticing that both PFRank and MR need to construct a nearest neighbor graph among the data and invert a matrix whose size is at least n  X  n , which is time consuming. In order to make the ranking scheme more efficient, given a query image, we first rank all the images according to the Euclidean distance to the query. Then we choose the top 500 images as candidates and use different ranking algorithms to re-rank them.

We use precision, recall, Mean Average Precision (MAP) [15] and Normalized Discount Cumulative Gain (NDCG) to evaluate the ranking results of different algorithms. Preci-sion is defined as the number of relevant presented images divided by the number of presented images. Recall is de-fined as the number of relevant presented images divided by the total number of relevant images in our database. Given aquery,let r i be the relevance score of the image ranked at position i ,where r i = 1 if the image is relevant to the query and r i = 0 otherwise. Then we can compute the Average Precision (AP):
MAP is the average of AP over all the queries. And NDCG at position n is defined as: n is also called the scope, which means the number of top-ranked images presented to the user. Z n is chosen such that the perfect ranking has a NDCG value of 1.

For our PFRank algorithm, the dimensionality of the man-ifold ( d ) in the real data is unknown. We perform cross-validation and choose d = 2 for the COREL data set, and choose d = 9 for the CMU PIE data set, respectively. All the other parameter settings are the same as in the previous experiment.
Figure 4 shows the average precision-scope curves of vari-ous methods on the two data sets, respectively. The precision-scope curve describes the precision with various scopes, and therefore providing an overall performance evaluation of the algorithms. As can be seen, our proposed PFRank algo-rithm consistently outperforms the other two algorithms on both data sets. MR generally achieves higher precision than SVM, indicating that considering the manifold structure in the data is useful for image retrieval.

In order to have a comprehensive view of the ranking per-formance, we present the NDCG, recall and MAP scores of different algorithms on the COREL and CMU PIE data set in Table 1 and Table 2, respectively. Overall, our PFRank method performs the best on both data sets. Although the precision of MR is generally higher than SVM in Figure 4, the NDCG scores of MR are lower than that of SVM on the CMU PIE data set. This is because the precision@ n of MR is lower than that of SVM when n&lt; 20, and NDCG computes the cumulative relevance scores of the top ranked images. Moreover, the recall and MAP of MR are lower than SVM on the COREL data set. MAP provides a single figure measure of quality across recall levels. Our PFRank achieves the highest MAP on both data sets, indicating re-liable performance over the entire ranking list.
Model selection is a critical problem in most of the learn-ing problems. In some situations, the learning performance may drastically vary with different choices of the parameter-s.  X  1 ,  X  2 and  X  3 are essential parameters in PFRank which control the trade-off among the three regularization terms. We empirically set  X  1 =  X  2 =  X  3 =0 . 01 in all the previous experiments. In this subsection, we try to study the impact of the parameters on the performance of our PFRank al-gorithm. Specifically, we fix other parameters the same as before, and let one of {  X  1 , X  2 , X  3 } vary.

In general, it is appropriate to present 20 images on a screen. Putting more images on a screen might affect the quality of the presented images. Therefore, precision@20 is especially important. Figure 5 and Figure 6 show the precision@20 scores of PFRank with respect to different val-ues of  X  1 ,  X  2 and  X  3 , respectively. As can be seen, our PFRank algorithm is generally not very sensitive to the pa-rameters, and outperforms the other two methods over a wide range of parameters.
In this paper, we propose a novel ranking algorithm on the data manifold from the vector field perspective. Motivated by recent study about the relationship between vector fields and functions on the manifold, we employ vector fields to ensure the linearity of the ranking function with respect to the manifold, as well as requiring the predicted ranking score of the query to be higher than that of its neighboring data points. In this way, the ranking function learned by our method decreases linearly, and therefore monotonically from the query to other points along the geodesics of the data manifold. Hence the ranking order along the geodesics is well preserved. Experimental results on both synthetic data and real data demonstrate the superiority of our method over existing ranking schemes.

There are several interesting directions for extending this work. One important problem is how to efficiently learn a ranking function from the vector field perspective for large-scale retrieval problems. The algorithm proposed in this paper needs to build a nearest neighbor graph among the data, and involves the matrix inversion operation. Given a query that is out of our database, how to update the n-earest neighbor graph and compute the ranking scores effi-ciently is critical for introducing this technique to real search engines. We may also consider employing distributed com-puting techniques to further improve the efficiency. Finally, vector fields are very useful to study the geometry and topol-ogy of the data manifold. It will be interesting to explore the application of vector fields in many other real-world prob-lems under the manifold assumption, such as face recogni-tion, manifold alignment, recommendation systems and so on.
The work was supported in part by the U.S. Army Re-search Laboratory under Cooperative Ag reement No. W911NF-09-2-0053 (NS-CTA), NSF IIS-1017362, U.S. Air Force Of-fice of Scientific Research MURI award FA9550-08-1-0265, and National Basic Research Program of China (973 Pro-gram) under G rant 2009CB320801. The views and conclu-sions contained in this paper are those of the authors and should not be interpreted as representing any funding agen-cies.
