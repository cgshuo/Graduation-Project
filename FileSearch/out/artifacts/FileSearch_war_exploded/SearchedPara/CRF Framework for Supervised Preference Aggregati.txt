 We develop a flexible Conditional Random Field framework for supervised preference aggregation, which combines pref-erences from multiple experts over items to form a distribu-tion over rankings. The distribution is based on an energy comprised of unary and pairwise potentials allowing us to effectively capture correlations between both items and ex-perts. We describe procedures for learning in this model and demonstrate that inference can be done much more ef-ficiently than in analogous models. Experiments on bench-mark tasks demonstrate significant performance gains over existing rank aggregation methods.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models ; I.2.6 [ Artificial Intelligence ]: Learning Preference Aggregation, Meta-search, Crowdsourcing
Preference aggregation is the task of combining prefer-ences from multiple experts over items into a single consen-sus ranking. This problem is crucially important in many applications. For instance, in meta-search an issued query is sent to several search engines and the (often partial) rank-ings returned by them are aggregated to generate more com-prehensive ranking results. In crowdsourcing, tasks often in-volve assigning ratings to objects or pairs of objects ranging from images to audio and text. The ratings from several users are then aggregated to produce a single labeling of the data.

Existing approaches to preference aggregation can be di-vided into two categories: permutation-based and score-based. Permutation-based models work directly in the per-mutation space and the majority of these methods are based on the Mallows model [23, 31, 21]. Score-based approaches infer a set of real valued scores that are then used to rank the items. A number of heuristic score-based methods for pref-erence aggregation have been proposed. For example, Bor-daCount [1], Condorcet Fusion [27] and Reciprocal Rank Fu-sion [8] derive the item scores by averaging (weighted) ranks across the experts, or counting the number of pairwise wins. Several probabilistic models have also been proposed, the majority of which are based on the Bradley-Terry and/or Plackett-Luce models [3, 28, 13].

The vast majority of the proposed methods in both cat-egories are developed for unsupervised preference aggrega-tion, where the aim is to produce an aggregate ranking that satisfies the majority of the preferences. However, many of the recent aggregation problems are amenable to supervised learning, as ground truth preference information is available. For example, in meta-search the documents retrieved by the search engines are often given to annotators who assign rele-vance labels to each document, which provides ground truth ranking information. Similarly, in crowdsourcing, a domain expert typically labels a subset of the data shown to the  X  X rowd X . These labels are then used to evaluate the quality of annotations submitted by each worker. In these settings aggregating methods that aim to always satisfy the major-ity often produce suboptimal results. Consequently, we need the aggregating function to be able to  X  X pecialize X  and infer when to use the majority preference versus when to concen-trate only on a small subset of preferences; this specializa-tion property is impossible to achieve without referring to the ground truth labels.

The supervised problem has received considerable atten-tion in recent years and a number of supervised approaches have been proposed [20, 36, 37, 29]. Notably, [37] has re-cently shown that by applying SVD factorization to pair-wise preference matrices effective item features can be ex-tracted. The features transform the problem into a standard learning-to-rank one allowing to apply any of the existing learning-to-rank methods to optimize the aggregating func-tion for the target metric. While the authors of that work have shown superior empirical accuracy of this approach to many existing aggregation methods, it also has a ma-jor drawback in that it requires computing SVD factors at test time. For large problems with many items per instance, such as those in crowdsourcing applications, applying SVD at test time can be prohibitively expensive, limiting the ap-plication of this method. A number of other popular super-vised aggregation methods share the same disadvantage and also require applying complex optimization procedures such as semidefinite programming [20] at test time.

In this paper we address the complexity problem by de-veloping a flexible Conditional Random Field (CRF) frame-work for supervised preference aggregation. Our framework uses preference matrices directly thus avoiding costly opti-mization and only requires computing simple sums during the inference step. We then show how ideas from learning-to-rank and related literature and be used to effectively op-timize our model for most existing metrics. Experiments on rank aggregation tasks with Microsoft X  X  LETOR4.0 [18] data sets show that our model achieves performance comparable to state-of-the art aggregation methods while requiring only a small fraction of computational time.
A typical supervised preference aggregation problem con-sists of a set of N training instances D = { r n , R n } N R n is a set of (partial) preferences expressed by the K n  X  X x-perts X  for the M n items, and r n is a set of (partial) ground truth preferences for the items. Note that the number of experts and items varies across the instances, and there is no information available about the items beyond R n r . Across different domains the preferences in R n can be in the form of full or partial rankings, top-T lists, ratings, relative item comparisons, or combinations of these. More-over, the form of ground truth preferences can also vary across domains. For instance, in social choice, ground truth preferences often come in the form of pairwise comparisons and/or (partial) rankings. On the other hand in meta-search and collaborative filtering ground truth preferences are typ-ically expressed via ratings and/or relevance labels. In this work we concentrate on the rank aggregation instance of this problem from the information retrieval (IR) domain. How-ever, the framework that we develop is general and can be applied to any supervised preference aggregation problem in the form defined above. In rank aggregation the experts X  preferences are summarized in an M n  X  K n matrix R n where R n ( i,k ) denotes the rank assigned to item i by the expert k . Furthermore, R n can be sparse, as experts might not assign ranks to every item; we use R n ( i,k ) = 0 to indicate that item i was not ranked by expert k .

Irrespective of the preference type the goal in supervised preference aggregation is to use R n to predict a ranking  X  y of the items with highest  X  X greement X  with the ground truth preferences r n . We use y n ( i ) = j to denote that rank of item i in y n and i = y  X  1 n ( j ) to denote the reverse. Depending on the preference type, agreement between  X  y n and r n can be measured using different metrics.

In social choice the common evaluation metric is Kendall X  X  tau, which measures the number of pairwise disagreements between  X  y n and r n . Popular evaluation metrics in IR in-clude Normalized Discounted Cumulative Gain (NDCG)[14] and Expected Reciprocal Rank (ERR)[6]. Both NDCG and ERR take relevance labels as input: the value of r ni is pro-portional to the relevance of item i . NDCG is the most commonly used metric in IR and is given by: where r n (  X  y  X  1 n ( i )) is ground truth preference for item in po-sition i in  X  y n , and G ( r n ,T ) is a normalizing constant such that the maximum of NDCG (  X  y n , r n )@ T is 1. As is common in CRFs, the learning problem optimizes average training loss 1 N P N n =1 l (  X  y n , r n ), where l (  X  y n , r for predicting ranking  X  y n under r n . For NDCG we simply define this loss as: l (  X  y n , r n ) = max Other metrics can be converted into a loss in a similar fash-ion.

The variable number of preferences per expert and the variable number of experts across the instances make it dif-ficult to apply the majority of supervised methods to this problem, since they require fixed-length item representa-tions. CRFs (formally defined in Section 4) on the other hand are well suited for tasks with variable input lengths, and have successfully been applied to problems that have this property, such as natural language processing [35, 33, 32], computational biology [34, 19], and information re-trieval [30, 38]. Moreover, CRFs are very flexible and can be used to optimize the parameters of the model for the tar-get loss. For these reasons we develop a CRF framework for preference aggregation.
Before delving into our model, we give a brief overview of existing methods for preference aggregation. Most of the existing approaches in this area can be divided into two cate-gories: permutation-based and score-based. In the following sections we describe both types of models.
Permutation-based models work directly in the permuta-tion space. The most common and well explored such model is the Mallows model [23]. Mallows defines a distribution over permutations and is typically parametrized by a cen-tral permutation  X  y n and a dispersion parameter  X   X  (0 , 1]; the probability of a permutation y is given by: where d ( y ,  X  y n ) is a distance between y and  X  y aggregation problems inference in this model amounts to finding the permutation  X  y n that maximizes the likelihood of the observed rankings. For some distance metrics, such as Kendall X  X   X  and Spearman X  X  rank correlation, the partition function Z (  X ,  X  y n ) can be found exactly. However, finding the central permutation  X  y n that maximizes the likelihood is typically very difficult and in many cases is intractable [26].
Recent work extends the Mallows model to define distri-butions over partial rankings [21]. Under partial rankings the partition function can no longer be computed exactly, so these authors introduced a new sampling approach to estimate it. When the number of items is large, however, this sampling approach is typically very slow, which makes the model impractical for many large scale online problems such as meta-search where aggregation has to be done very quickly. Furthermore, both the proposed pairwise model and the sampling approach rely on the assumption that all pairwise preferences are consistent, which is often violated in real-world preference aggregation problems.

A number of other generalizations of the Mallows model have been proposed [17, 16, 31]; however, to the best of our knowledge none of these extensions address learning and/or inference complexity of this model. In general, due to the extremely large search space (typically M ! for M items) and the discontinuity of functions over permutations, exact infer-ence in permutation-based models is often intractable. Thus one must resort to approximate inference methods, such as sampling or greedy approaches, often without guarantees on how close the approximate solution will be to the target opti-mal one. As the number of items grows, the cost of finding a good approximation increases significantly, which makes the majority of these models impractical for many real world ap-plications where data collections are extremely large. The score-based approach described next avoids this problem by working with real valued scores instead.
In score-based approaches the goal is to infer a set of real valued scores (one per item) s n = { s n 1 ,...,s nM n } which are then used to sort the items. Working with scores avoids the discontinuity problems of the permutation space.

A number of popular score-based aggregation methods in meta-search are heuristic based. For example, BordaCount [1], Condorcet [27] and median rank aggregation [10] derive the item scores by averaging ranks across the experts or counting the number of pairwise wins. In statistics a very popular pairwise score model is the Bradley-Terry [3] model: that item i beats item j in the pairwise contest. The key assumption behind the Bradley-Terry model is that the pair-wise probabilities are completely independent of the items not included in the pair. A problem that arises from this assumption is that if a given item i has won all pairwise con-tests, the likelihood becomes larger as s ni becomes larger. It follows that a maximum likelihood estimate for s ni is  X  [24]. As a consequence the model will always produce a tie amongst all undefeated items. Often this is an unsat-isfactory solution because the contests that the undefeated items participated in, and their opponents X  strengths, could be significantly different.

To avoid some of these drawbacks, the Bradley-Terry model was generalized by Plackett and Luce [28, 22] to a Plackett-Luce model for permutations: y . The generative process behind the Plackett-Luce model assumes that items are selected sequentially without replace-ment. Initially item y  X  1 (1) is selected from the set of M items and placed first, then item y  X  1 n (2) is selected from the remaining M n  X  1 items and placed second and so on until all M n items are placed. Note that here inference can be done quickly by doing simple gradient descent on scores, which is a clear advantage over most permutation based models. The Plackett-Luce generalization relaxes the pairwise inde-pendence assumption of the Bradley-Terry model but this model is only applicable to consistent full or partial rankings (or consistent pairwise preferences) which significantly limits its application. Moreover, for 2-item rankings the Plackett-Luce model reduces to the Bradley-Terry model and thus suffers from the same infinite score problem. To overcome this problem a Bayesian framework was also recently in-troduced for the Plackett-Luce model by placing a Gamma prior on the selection probabilities [13]. The authors of that work demonstrated that the Bayesian approach prevented overfitting and produced aggregate rankings that better fit-ted the observed preference data. This improvement how-ever, comes at the cost of computational overhead required during score inference.

Both Bradley-Terry and Plackett-Luce models are unsu-pervised and are typically fitted via maximum likelihood. This makes them ill-suited for supervised aggregation prob-lems as they are unable to capture the correlations between observed preferences and the ground-truth ones. To over-come this disadvantage a number of supervised methods have recently been proposed. Several of these methods have explored weighted aggregation rules [36, 29], where a well explored social choice aggregation rule, such as Borda or Kemeny, are applied to weighted expert preferences. The weights are tuned on the training data to reflect each experts  X  X greement X  with the ground truth preferences. While these methods have empirically been shown to give improvements, the weights typically have to be tuned by hand making the models inflexible and expensive to optimize.

Other supervised methods explore pairwise item-item preference matrices. A supervised Markov Chain model based on this framework was recently introduced [20]. In this model ground truth preferences are used to create pairwise constraint matrices and a scoring function is then trained to satisfy as many of these pairwise constraints as possi-ble. This method was recently extended by [7] to a semi-supervised setting where ground truth preferences are avail-able only for a subset of the documents. Another method based on pairwise matrices is the SVD approach [37]. In this model pairwise expert matrices are factorized using low-rank SVD factorization and the resulting SVD representations are then used as item features to train the aggregating function. This approach allows to optimize the model for any target metric, but similarly to the Markov-Chain method which requires solving semi definite programming problem, suffers from expensive inference requiring SVD factorization for ev-ery test instance.

Our proposed framework is also based on the pairwise ma-trix approach which as demonstrated by the strong empirical results of the above methods, is a promising way to approach this problem. Unlike the existing methods however, we fo-cus on making the inference as efficient as possible without affecting the accuracy. We describe our approach in the next section.
In a typical supervised problem we are interested in learn-ing a relationship between input x and a target y for a given training set of instantiated pairs D = { x n , y n } . More specif-ically, we are interested in learning a predictive mapping for x to y .

CRFs tackle this problem by defining the conditional dis-tribution p ( y | x ) through some energy function E ( y , x ;  X  ) as follows: where  X  is the model X  X  parameter vector. The parametric form of the energy function E ( y , x ;  X  ) depends on the na-ture of the problem and typically consists of weighted unary and/or higher order potentials. As mentioned above this framework is very flexible and has successfully been applied to a wide range of diverse problems.

In this work we show that CRFs can also be used to build an effective model for preference aggregation. Supervised aggregation can be put into above framework by noting that our goal is to also learn a predictive mapping from an expert matrix R  X  x to a ranking y that has the highest agreement with the ground truth preferences r . Our goal is thus to define a conditional distribution p ( y | R ) through an energy E ( y , R ;  X  ) and optimize it for the target metric. In the following sections we show that effective unary and pairwise potentials can be derived directly from the expert preference matrix, and use these potentials to define a smooth energy function over the space of rankings.
Given the expert matrix R n our aim is to convert it to a set of pairwise preference over the items. There are a num-ber of pairwise functions that we can use here, however, for consistency we chose to use the functions that were utilized in the SVD-based aggregation method [37]: 1. Binary Comparison : 2. Normalized Rank Difference 3. Log Rank Difference Here I [] is an indicator function; when either R n R that  X  k ( i,j, R n ) is also zero if i = j . Normalization by the maximum (log-)preference assigned by the expert k ensures that  X  k ( i,j, R n ) has a comparable range across experts.
Working with pairwise comparisons has a number of ad-vantages, and models over pairwise preferences have been extensively used in areas such as social choice [9, 21], in-formation retrieval [15, 4], and collaborative filtering [21, 12]. First, pairwise comparisons are the building blocks of Variable Description
D = { r n , R n } N n =1 training instances r n ground truth preferences R n M n  X  K n expert preference matrix:
R n ( i,k ) ranking for item i by expert k  X  y n ranking predicted by the model l (  X  y n , r n ) target loss  X  k ( i, R n ) unary potential from expert k  X  k ( i,j, R n ) pairwise potential from expert k almost all forms of evidence about preference and subsume the most general models of evidence proposed in literature. A model over pairwise preferences can thus be readily ap-plied to a wide spectrum of preference aggregation problems and does not impose any restrictions on the input type. For instance, preferences in the form of ratings can be treated like rankings and the same pairwise difference/comparison functions can be applied. Moreover, top-T lists (and their variations) can also be converted into this framework using the binary comparison function and setting  X  k ( i,j, R n if item i is in the top-T and item j is not. These examples demonstrate the flexibility and wide applicability of a model over pairwise preferences.

Second, pairwise comparisons are a relative measure and help reduce the bias from the preference scale. In meta-search for instance, each of the search engines that receives the query can retrieve diverse lists of documents significantly varying in size. By converting the rankings into pairwise preferences we reduce the list size bias emphasizing the im-portance of the relative position.
The main idea behind our approach is based on an obser-vation that the pairwise preference functions defined above naturally translate to pairwise potentials in a CRF model. Using these function we can evaluate the  X  X ompatibility X  of any ranking y by comparing the order induced by the rank-ing with the pairwise preferences from each expert. This leads to an energy function: E ( y , R n ;  X  ) =  X  1 where y  X  1 ( i ) is the item in position i in ranking y . This energy function contains a binary unary potential  X  k ( i ) = I [ R n ( i,k ) = 0], where I [] is an indicator function. This potential is active only when item i is not ranked by the expert k , in which case  X  k is 0, and  X  k provides a base preference score for the item.

The energy also contains pairwise potentials  X  k . Note that from the definition of  X  in Section 4.1 it follows that only one of  X  k ( y  X  1 ( i ) ,j, R n ) or  X  k ( j, y  X  1 ( i ) , R for any pair of items. Consequently, if  X  k ( y  X  1 ( i ) ,j, R on (non-zero) then expert k  X  X grees X  with the relative order induced by y (lowering the energy) and the strength of this agreement is given by  X  k . Similarly, if  X  k ( j, y  X  1 ( i ) , R then expert k  X  X isagrees X  with the relative order, and raises the energy. The weights  X  P k and  X  N k thus control how much emphasis is given to positive and negative relative prefer-ences from expert k . 1 / log( i + 1) is the rank discount func-tion similar to the one used in NDCG and other IR metrics, which emphasizes items at the top positions in the rank-ing. Finally, normalizing by 1 /M 2 n ensures that the energy ranges are comparable across instances with different num-bers of items.

Using the energy we define a conditional probability for a ranking y : where the partition function Z ( R n ) sums over all M n ! valid rankings y . In the proposed model a separate set of weights {  X  k , X  P k , X  N k } is learned for each expert k , which allows the model to effectively capture the correlations between individual expert preferences and the ground truth ones. The proposed framework can easily handle training/test in-stances with missing experts by simply dropping the corre-sponding pairwise potentials from the energy and only using the base scores  X  k for those experts. Moreover, while exist-ing models rely exclusively on pairwise matrices, our model can be straightforwardly extended to handle any available side information on both items and experts. For instance, by adding extra pairwise and higher order potentials we can go beyond item interaction and, for instance, model inter-actions between experts correlating them to ground truth preferences.

This framework however, cannot be applied when the ex-pert identity is unknown or when new experts, unseen during training, are introduced at test time. This is often the case in domains like crowdsourcing where the experts must be anonymized due to privacy considerations, and the number of experts is large so new experts are often introduced at test time. To generalize the model to these settings we can sim-ply share the same parameters  X  ,  X  P and  X  N , removing the dependence on k . The resulting consensus model only takes into account the net preference across all K n experts, ignor-ing the individual preferences. Though this makes it pos-sible to apply the model to arbitrary expert sets, this may weaken it since preference information from individual ex-perts can contain very useful information, especially in cases where the majority of experts are wrong. When a subset of the experts is known, it is possible to take an intermediate approach and learn individual weights {  X  k , X  P k , X  N k known experts k , and consensus-based weights {  X , X  P , X  N for the unknown experts. This demonstrates the flexibility of the proposed CRF framework which allows us to effec-tively learn to aggregate preferences in the settings where both item and expert sets can vary in length.
Given the model our aim is to learn the parameters  X  = {  X  k , X  P k , X  N k } K k =1 that minimize the average training loss 1 N P N n =1 l (  X  y n , r n ). Unfortunately, direct minimization Algorithm 1 Learning Algorithm Input: { r n , R n } N n =1
Parameters: learning rate  X  , cut-off initialize weights:  X  repeat { CRF optimization } until convergence
Output:  X  function of the CRF parameters  X  . Specifically, the loss it-self l (  X  y n , r n ) is not a smooth function of the prediction  X  y and  X  y n itself is also not a smooth function given the model parameters  X  . Such non-smoothness makes it impossible to apply gradient-based optimization directly.

To solve this problem recent work explored different ap-proximation methods to incorporate the target loss into CRF training [38, 11, 25]. The most related of these ap-proaches is the learning-to-rank method BoltzRank [38]. The authors of BoltzRank also dealt with a parametrized distribution over permutations and optimized it for the tar-get IR metric. Inspired by this work we follow this approach and use the expected loss as the target objective to minimize: Note that even in cases where l is non-smooth (e.g., NDCG, ERR) the above objective remains smooth with respect to  X  and can be minimized using standard gradient-based proce-dure. However, to optimize this objective for a given train-ing instance we need to calculate l ( y , r n ) and p ( y | R all M n ! rankings. This computation very quickly becomes intractable since even for M n = 15 one needs to sum over more than 10 12 permutations. Standard MCMC and varia-tional techniques can be used here to estimate the gradients, however, these methods are typically too slow to be applied to the IR domain where data sets often contain thousands of queries. To deal with this problem the authors of [38] sug-gested pre-computing a fixed sample set for every instance and reusing it throughout learning. While this approach is computationally efficient, it might miss important regions of model X  X  probability space and can thus be ineffective at optimizing the target distribution.

To avoid these problems we opted to use an approach suggested by [5], which we empirically found to work very well. Every time an instance n is visited and the number of items is greater than , we sample a subset of items and use the corresponding expert preferences R n and targets r to compute the gradients for  X  . When selecting the items we ensure diversity by sampling from different relevance groups. This is especially important for imbalanced datasets, which are common in IR, where most items are irrelevant. For MQ2008-agg CPS 26.52 31.38 34.59 37.63 40.04 31.63 32.27 32.27 31.66 30.64 41.02 SVP 32.49 36.20 38.62 40.17 41.85 38.52 36.42 34.65 32.01 30.23 43.61 RRF 38.77 40.73 43.48 45.70 47.17 44.89 41.32 38.82 36.51 34.13 47.71 SVDsup 42.81 44.53 47.02 49.00 50.69 48.85 44.13 41.84 39.09 36.50 50.32 MQ2007-agg CPS 31.96 33.18 33.86 34.09 34.76 38.65 38.65 38.14 37.19 37.02 40.69 SVP 35.82 35.91 36.53 37.16 37.50 41.61 40.28 39.50 38.88 38.10 42.73 RRF 41.93 42.66 42.42 42.73 43.13 48.70 47.20 44.84 43.52 42.52 46.72 these datasets random sampling often leads to subsets where all items are irrelevant thus providing very little learning signal to the model. Choosing sufficiently small allows the gradients to be computed exactly by enumerating all possible ! permutations of the items. Unlike static sample sets, repeated re-sampling together with full enumeration of all permutations allows us to explore all regions of the model X  X  probability space throughout learning albeit for the reduced item sets.

To make the learning more efficient both unary and pair-wise potentials can be precomputed a priori and re-used throughout learning. This reduces the complexity of com-puting the model X  X  energy from O ( M 2 n K n ) to O ( M n the cost of additional storage requirement of O ( M training instance. The complete learning algorithm is sum-marized in Algorithm 1.

Once the model is learned, at test time, given a new in-stance with corresponding experts X  preferences R our goal is to produce a single aggregate ranking  X  y of the items that has the highest probability (lowest energy) under the model. Fortunately, such inference can be done very efficiently in this CRF. We note that the energy can be rewritten as a sum of discounted  X  X eights X : where the weights are given by:  X  i =  X  Since 1 / log( i + 1) is a monotonically decreasing function it is easy to verify that the ranking with highest probability is obtained by sorting the items according to the weights: It is important to note here that this inference procedure only requires computing simple sums and can thus be done very efficiently . This is a significant advantage over existing aggregation methods based on pairwise matrices that require complex optimization procedures such as semidefinite pro-gramming [20] or SVD [37] to be run at test time. Moreover, while the inference procedure is simple the learning in our model takes full advantage of the target loss function and optimizes the aggregating function for that metric.
For our experiments we use the LETOR4.0 benchmark datasets [18]. These data sets were chosen because they are publicly available, include several baseline results, and pro-vide evaluation tools to ensure accurate comparison between methods. In LETOR4.0 there are two rank aggregation data sets, MQ2007-agg and MQ2008-agg.

MQ2007-agg contains 1692 queries (instances) with a total of 69623 documents (items), and MQ2008-agg contains 784 queries and a total of 15211 documents. Each query contains partial expert rankings of the documents under that query. There are 21 experts in MQ2007-agg and 25 in MQ2008-agg. Consequently, for every query n in MQ2007-agg with M documents we have a sparse M n  X  21 ( M n  X  25 for MQ2008-agg) expert preference matrix R n . In addition, in both data sets, each document is assigned one of three relevance levels: 2 = highly relevant, 1 = relevant and 0 = irrelevant. These relevance levels correspond to the ground truth preferences r . Finally, each dataset comes with five precomputed folds with 60/20/20 splits for training/validation/testing. The results shown for each model are the averages of the test set results for the five folds.

The MQ2007-agg dataset is approximately 35% sparse, meaning that for an average query the partial ranking ma-trix R n will be missing 35% of its entries. MQ2008-agg is significantly more sparse with the sparsity factor of approx-imately 65%. area of each square is proportional to weight magnitude.
The goal is to use the training data to learn a map-ping from R n to an aggregate ranking  X  y n that has max-imal agreement with the ground truth preferences r n LETOR4.0 this agreement is evaluated using NDCG (N@ T , see Equation 2), Precision (P@ T ) and Mean Average Pre-cision (MAP) [2]. Unlike NDCG, MAP only allows binary (relevant/not relevant) document relevance, and is defined in terms of average precision (AP): where P @ i is the precision at i : MAP is then computed by averaging AP over all queries. To compute P@ k and MAP on the MQ datasets the relevance levels are binarised with 1 converted to 0 and 2 converted to 1. All presented NDCG, Precision and MAP results are averaged across the test queries and were obtained using the evaluation script available on the LETOR website 1 .
To the best of our knowledge the SVD approach of [37] currently has the best published results on the MQ-agg datasets so in experiments we concentrate on comparing our approach with this method. To train our model we use stochastic gradient descent (one query at a time) and do 300 full passes through the training data. We set = 6 (see Algorithm 1) and ensure that at least one document of every relevance level appears in the same for each query. To choose the type of pairwise potential to use (see Section 4.1) we train separate models with each type and use validation MAP to select the best one. We found that the log rank difference potential generally produced the best results for both datasets.
 We compare our model to the best method listed on LETOR website, namely the CPS (combination of Mal-lows and Plackett-Luce models) [31] on each of the MQ-agg datasets. In addition, we compare with the established meta-search standards Condorcet Fusion [27] and Reciprocal Rank Fusion (RRF) [8] as well as the Plackett-Luce model. Finally, we also compare with two SVD-based approaches that use the same pairwise matrices: unsupervised method http://research.microsoft.com/en-us/um/beijing/ projects/letor/ SVP [12] and the supervised SVD approach (SVDsup) [37] described above. These models cover all of the primary lead-ing approaches in the rank aggregation research except for the Markov Chain model [20].

NDCG, Precision and MAP results for both datasets are shown in Table 2. From the tables we see that our model has very strong performance producing similar results to the best baseline SVDsup. It is important to note here that we use the same pairwise matrices as SVDsup dur-ing both learning and inference. These results indicate that our model is able to achieve highly competitive performance without using expensive optimization procedures during in-ference. In the following section we quantify the difference in runtimes between the two models.

An additional advantage of using preference matrices di-rectly is model interpretability. By analyzing the learned potential weights we can gain insight into which experts are useful and how their preferences are combined. Figure 1 shows an example weight matrix learned by our model on the training Fold 1 of MQ2008-agg. Before delving into the figure we note that negative  X  k raises the energy (lowering the probability). Hence large negative values indicate that when preference from expert k is missing for a given docu-ment it is pushed down in the aggregate ranking i.e. expert k is important for aggregation. Similarly, positive  X  P k the energy (upping the probability) while positive  X  N k raise the energy. Consequently, when both weights are positive for a given expert k , documents i strongly preferred by k in the ranking while those not preferred get pushed down.
Taking these relationships into account we see from Figure 1 that preferences from experts 14, 15, 17, 18 and 21 are good indicators of document relevance. The importance of these experts is shown by large negative values of  X  k . Moreover, large positive values for both  X  P k and  X  N k indicate that strong net preference from each of these experts correlates closely with high relevance.

We also see that some experts are not useful for aggrega-tion. For instance experts 24, and 25 all have positive  X  meaning that when their preferences are absent the rank of a document actually improves. Each of these experts also has near-zero  X  P k and  X  N k indicating that when their preferences are present the model does not use them. Table 3: MQ2008-agg NDCG@1-5 results; CRF is trained on the full data, CRF* is trained on a subset of the data with experts 13, 20, 24 and 25 removed.
Finally, some experts are used for aggregation even though their preferences correlate inversely with ground truth. For instance, experts 10, 11 and 12 all have negative  X   X  k weights indicating that documents strongly preferred by these experts will be pushed down in the ranking while those strongly opposed will be pushed up. Moreover, most weights for these experts are large indicating that they play an im-portant role in the aggregation process. The model thus learned that these experts often give wrong relative order-ings reversing which can still lead to useful predictions. It is worth emphasizing here that this kind of inverse relationship is impossible to capture with unsupervised methods. To further validate the utility of analyzing experts through CRF X  X  parameters we removed experts whose preferences were found not to be useful by the CRF and retrained the model. Specifically, from Figure 1 we see that experts 13, 20, 24 and 25 are not being used by the model and when pref-erences from these experts are missing, the corresponding document actually gets a boost in ranking. These experts are clearly not useful for ranking so we removed them and re-trained the model on the remaining 21 experts. The results are shown in Table 3, from the table we see that retrained model CRF* either performs comparably or outperforms the original model. This further support the conclusion that use-ful insight into expert quality can be gained by analyzing the weights learned by our model. Such analysis can be particu-larly useful in crowdsourcing and related domains where the goal is often to identify the most accurate/reliable labelers from the crowd.
In the previous section we demonstrated that our model has comparable performance to the state-of-the-art model SVDsup. Moreover, inference in our model only requires computing simple sums and can thus be done considerably more efficiently than in SVDsup which requires SVD factor-ization. In this section we quantify this difference.
We use test Fold 1 of the MQ2008-agg dataset and con-duct two sets of experiments. In the first experiment we repeatedly increase the number of experts. Starting with the initial expert matrix at iteration 1: R (1) n = R n , we con-catenate it with the original matrix to get an expanded one for iteration 2: R (2) n = [ R (1) n , R n ]. Thus, after t iterations the resulting matrix R ( t ) n = [ R ( t  X  1) n , R n ] contains M and t  X  K n columns. Concatenating expert matrices allows us to test the inference procedure of each method on an in-creasingly larger data while preserving sparsity. In the sec-ond experiment we repeat this procedure but this time we append the matrices increasing the number of documents. Here, the ranking matrix at iteration t contains t  X  M n rows and K n columns. The first experiment thus tests for sce-narios where the expert set is large (expert expansion), that typically arise in domains like crowdsourcing. While the the second experiment tests for large item sets (item expansion) that often arise in domains like meta-search.

Figures 2 and 2(b) show, averaged across queries, runtimes (in seconds) for both methods at each expansion iteration. Figure 2(a) shows runtimes for the expert expansion while Figure 2(b) shows runtimes for the item expansion. From the figures we see significant differences in runtimes between the two methods. The difference is especially large for the expert expansion (Figure 2(a)) where SVDsup is on average almost 80 times slower than our CRF method at the tenth iteration. This difference is due to the fact that SVDsup has to run SVD factorization for every expert. Consequently, the number of SVD factorizations grows linearly with the num-ber of experts significantly slowing down SVDsup. For the item expansion (Figure 2(b)) the number of experts stays constant while the dimension of the preference matrix in-creases. Since no additional SVD factorizations are required we found the speed of SVDsup to not increase as significantly as in the first experiment. However, even in this setting our approach is more than 3.5 times faster. Moreover, we found that for very large matrices (not shown on this plot) SVD factorization dominated the calculation significantly slowing down SVDsup. From these results we can conclude that our approach is considerably more efficient than SVDsup espe-cially in cases where the number of experts is large.
We presented a fully supervised CRF approach to pref-erence aggregation. Unlike existing methods our approach uses observed preferences directly and does not require any expensive optimization procedures at test time. The direct use of preferences also allows us to analyze learned models and draw valuable conclusions about preference quality of each expert. Experimental results show that our approach has very competitive performance outperforming existing methods on two supervised rank aggregation tasks.

Going forward a promising direction would be to explore different types of potentials. Specifically, we plan to ex-periment with adding expert cross correlations and incorpo-rating side information for items and/or experts. Another promising area of research would be to explore CRF mod-els that in addition to experts also condition on queries. In meta-search and other applications it is often the case that different experts perform well for different queries. Adding this extra conditioning can thus help the model to distin-guish when to use each expert making it more powerful. [1] J. A. Aslam and M. Montague. Models for [2] R. Baeza-Yates and B. Ribeiro-Neto. Information [3] R. Bradley and M. Terry. Rank analysis of incomplete [4] C. J. C. Burges. From RankNet to LambdaRank to [5] T. S. Caetano, L. Cheng, Q. V. Le, and A. J. Smola. [6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [7] S. Chen, F. Wang, Y. Song, and C. Zhang.
 [8] G. V. Cormack, C. L. A. Clarke, and S. B  X  uttcher. [9] H. A. David. The method of paired comparisons . [10] R. Fagin, R. Kumar, and D. Sivakumar. Efficient [11] K. Gimpel and N. A. Smith. Softmax-margin CRFs: [12] D. F. Gleich and L.-H. Lim. Rank aggregation via [13] J. Guiver and E. Snelson. Bayesian inference for [14] K. Jarvelin and J. Kekalainen. IR evaluation methods [15] T. Joachims. Optimizing search engines using [16] A. Klementiev, D. Roth, and K. Small. Unsupervised [17] G. Lebanon and J. Lafferty. Cranking: Combining [18] T. Liu, J. Xu, W. Xiong, and H. Li. LETOR: [19] Y. Liu, J. Carbonell, P. Weigele, and [20] Y.-T. Liu, T.-Y. Liu, T. Qin, Z.-M. Ma, and H. Li. [21] T. Lu and C. Boutilier. Learning Mallows models with [22] R. D. Luce. Individual choice behavior: A theoretical [23] C. L. Mallows. Non-null ranking models. Biometrika , [24] D. Mase. A penalized maximum likelihood approach [25] D. McAllester and J. Keshet. Generalization bounds [26] M. Meila, K. Phadnis, A. Patterson, and J. Bilmes. [27] M. Montague and J. A. Aslam. Condorcet fusion for [28] R. Plackett. The analysis of permutations. Applied [29] M. Pujari and R. Kanawati. Supervised rank [30] T. Qin, T.-Y. Liu, X.-D. Zhang, D.-S. Wang, and [31] T. Quin, X. Geng, and T.-Y. Liu. A new probabilistic [32] D. Roth and W.-Y. Yih. Integer linear programming [33] S. Sarawagi and W. W. Cohen. Semi-Markov [34] K. Sato and Y. Sakakibara. RNA secondary structural [35] F. Sha and F. Pereira. Shallow parsing with [36] K. Subbian and P. Melville. Supervised rank [37] M. N. Volkovs, H. Larochelle, and R. S. Zemel. [38] M. N. Volkovs and R. S. Zemel. Boltzrank: Learning
