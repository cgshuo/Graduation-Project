 binary case.
 versus-all approach, which combine a set of binary machines to a multi-class decision maker. At canonical multi-class machine proposed by Vapnik [8] and in dependently by Weston and Watkins Wahba [4].
 Theorem 2 by Tewari and Bartlett [7] establishes the relatio n for the terms set such that S B holds. in the machines by Crammer and Singer [2] and by Weston and Wat kins [9] are not classification ( S
C = false ), although it can be deduced only that the implication S B  X  S C does not hold. This tells us nothing about S C , even if S B can be established per construction. [2] proposed by Crammer and Singer: Theorem 2. Let X  X  R d be compact and k : X  X  X  X  R be a universal kernel with 1 P on X  X  Y and all  X  &gt; 0 it holds is wrong. This important learning machine is indeed univers ally consistent. examples are supposed to be drawn i.i.d. from a probability d istribution P on X  X  Y . Let k : X  X  X  X  R be a positive definite (Mercer) kernel function, and let  X  : X  X  H be d X by the kernel k .
 notion of a universal kernel: on a compact subset X  X  R d is called universal if the set of induced functions is dense in the function f with k g  X  f k  X  &lt;  X  .
 we refer to [6].
 with component functions f u : X  X  R , u  X  Y (sometimes restricted by the so-called sum-to-zero w hypothesis by h =  X   X  f : X  X  Y .
 The multi-class SVM variant proposed by Crammer and Singer u ses functions without offset terms ( b program The slack variables in the optimum can be written as this problem by f = f T,k,C = ( h w 1 , i , . . . , h w q , i ) T .
 E h ( x )  X  s ( x ) and thus  X  h ( x )  X  0 up to a zero set. introduce the covering number of the metric space ( X, d k ) as importance for the proofs of diverse lemmas in the next secti on.
 is simply a multiple of  X  , which we can think of as an arbitrarily small positive numbe r. We split the simplex  X  into a partition of  X  X lassification-aligned X  subsets for y  X  Y , on which the decision function  X  decides for class y . We define the grid of half-open cubes. Then we combine both constructions to th e partition X  X  := x  X  X P ( y | x )  X   X  .
 probabiliy mass, resulting in with M := D N (( X, d k ) ,  X  ) . We summarize these sets in K  X  = S A  X  X  These sets cover nearly all probability mass of P X in the sense from the definition of  X  K  X  .
 lower and upper bounds
L y ( X  X  ) = L y (  X  ) := inf X the following properties: With properties (P2) and (P6) it is straight-forward to obta in the inequality for the risk.
 A  X  X  , we define L of training examples in all sets A  X  X  , and for all classes u  X  Y . to Lemmas 1-6 in [6].
 Lemma 1. (Lemma 1 from [6]) Let k : X  X  X  X  R be a universal kernel on a compact subset X or R d and  X  : X  X  X  be a feature map of k . The  X  is continuous and is finite for all  X  &gt; 0 .
 Lemma 2. Let X  X  R d be compact and let k : X  X  X  X  R be a universal kernel. Then, for all induced function such that for all u  X  Y .
 is completely analogous.
 Lemma 3. The probability of the training sets F  X  is lower bounded by F inequality. The inequality, applied to the variables z i , states can use the relation in order to obtain Hoeffding X  X  formula for the case of strict inequality obtain  X  P  X  inequality results in which also holds in the case L u ( A ) = 0 treated earlier. Finally, we use the union bound and properties (P4) and (P5) to prove the assertion.
 Lemma 4. The SVM solution f and the hypothesis h = f  X   X  fulfill because it is the direct counterpart to the (stronger) Lemma 4 in [6]. fulfills with ( w  X  1 , . . . , w  X  q ) as defined in Lemma 2.
 Proof. The optimality of the SVM solution for the primal problem (2) implies 1 +  X  for P ( y | x i ) 6 X   X  y The definition of F  X  yields where the last line is due to inequality (5). We obtain which proves the claim.
 to the SVM solution fulfills Proof. Problem (2) takes the value C in the feasible solution w 1 = . . . , w q = 0 and  X  1 = =  X  = 1 . Thus, we have P u  X  Y k w u k 2  X  C in the optimum, and we deduce k w u k  X  The proof works through the following series of inequalitie s. The details are discussed below.
X F  X  together with | f u ( x )  X  f u ( x 1  X  E h ( x ) = 1  X  s ( x )  X   X  h ( x ) , which can be deduced from properties (P1) and (P2). detailed but technical auxiliaury result.
 such that for all C  X  C  X  and all  X   X  1 we have N (( X, d k ) ,  X  / (2 Proof. According to Lemma 3 it is sufficient to show R ( f T,k,C )  X  R  X  +  X  for all T  X  F  X  . Lemmas 5 and 6 to (1  X   X  ) 2  X  . With the choice C  X  = 1  X  P u  X  Y k w  X  u k 2 and the condition C  X  C  X  we obtain R ( q + 5)  X  =  X  .
 Because of the importance of the statement and the brevity of the proof we repeat it here: Theorem 1 yields where M  X  = D N (( X, d k ) ,  X  / (2 ( X, d k ) we obtain M 2  X   X  X  ((  X  C  X  ) 2 ) and thus  X  M  X  2  X   X  X  X  . We have proven the universal consistency of the popular mult i-class SVM by Crammer and Singer. [1] C. Cortes and V. Vapnik. Support-Vector Networks. Machine Learning , 20(3):273 X 297, 1995. [3] S. Hill and A. Doucet. A Framework for Kernel-Based Multi -Category Classification. Journal [5] Y. Liu. Fisher Consistency of Multicategory Support Vec tor Machines. Journal of Machine [8] V. Vapnik. Statistical Learning Theory . Wiley, New-York, 1998. [9] J. Weston and C. Watkins. Support vector machines for mul ti-class pattern recognition. In
