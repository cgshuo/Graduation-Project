 There is a great demand for information retrieval systems which are able to han-dle reputations behind documents such as customer reviews of products on the web. Since sentiment analysis technologies which identify sentimental aspects of a text are necessary for such a system, the number of researches for them has been increasing. As one of the problems of sentiment analysis, there is a docu-ment sentiment classification task to label a document according to the positive or negative polarity of its opinion (favorable or unfavorable). A system using document sentiment classification technology can provide quantitive reputation information about a product as the number of positive or negative opinions on the web.
 machine learning (e.g., [ 1 ], [ 6 ], [ 10 ]), which have been successful in other docu-ment classification tasks, showed higher performance than rule-based classifiers. Pang et al. [ 2 ] reported 87% accuracy rate of document sentiment classification of the movie reviews by their classifier using word unigram as feature for support vector machines (SVMs).
 text is regarded as a set of words. Therefore, the document representation ig-nores word order and syntactic relations between words appearing in a sentence included in the original document. However, not only a bag-of-words but also word order and syntactic relations between words in a sentence are intuitively important and useful for sentiment classification. Thus, there appears to remain considerable room for improvement by incorporating such information. propose to use a word sequence and a dependency tree as structured repre-sentations of a sentence (we call simply  X  X  sentence X  below) and mining fre-quent sub-patterns from the sentences in a document dataset as features for document sentiment classification. We believe that the extracted set of frequent sub-patterns includes subjective expressions and idioms in the domain. pendency tree. We then extract frequent sub-patterns from these structured representations of sentences.
 lated works on sentiment classification. In Section 3, we describe our approach to handle word order and syntactic relations between words in a sentence included in a document. In Section 4, we report and discuss the experimental results of sentiment classification. Finally, Section 5 gives conclusion. Sentiment classification is a task of classifying a target unit in a document to pos-itive (favorable) or negative (unfavorable) class. Past researches mainly treated three kinds of target units: a word, a sentence and an overall document. to pos-itive or negative.
 Word Sentiment Classification. Hatzivassiloglou et al. [ 11 ] used conjunctive expressions such as  X  X mart and beautiful X  or  X  X ast but inaccurate X  to extract sentiment polarities of words.
 ber of results returned by web searches. The relationship between a polarity-unknown word and a set of manually-selected seeds was used to classify the polarity-unknown word into a positive or negative class.
 Sentence Sentiment Classification. Kudo et al. [ 9 ] used subtrees of word dependency trees as features for sentence-wise sentiment polarity classification. They used boosting algorithm with the subtree-based decision stamps as weak learners.
 Document Sentiment Classification. Pang et al. [ 1 ] attempted to classify movie reviews. They applied to document sentiment classification a supervised machine learning method which had succeeded in other document classification tasks (e.g., on the task classifying articles of Reuters to 10 categories, Dumais et al [ 8 ] achieved F-measure of 0.92 with SVMs. ). They used a word N-gram in the dataset as bag-of-words features for their classifier. A word N-gram is a set of N continuous words extracted from a sentence. The best results came from word unigram-based model run through SVMs, with 82.9% accuracy.
 sentences in the review. But accuracy of their method is less than that of the classifier using full reviews, which was introduced in their former study [ 1 ]. kinds of products. Unlike Pang X  X  research, they obtained the best accuracy rate with word bigram-based classifier on their dataset. This result indicates that the unigram-based model does not always perform the best and that the best settings of the classifier is dependent on the data.
 tempted to use the semantic orientation of words defined by Turney [ 7 ] and several kinds of information from Internet and thesaurus. They evaluated on the same dataset used in Pang et al. X  X  study [ 1 ] and achieved 84.6% accuracy with the lemmatized word unigram and the semantic orientation of words.
 sentence have not been used for the document sentiment classification. We propose to use word order and syntactic relations between words in a sen-tence for a machine learning based document sentiment classifier. We give such information as frequent sub-patterns of sentences in a document dataset: word subsequences and dependency subtrees. 3.1 Word Subsequence As shown in Figure 2, a word sequence is a structured representation of a sen-tence. From the word sequence, we can obtain ordered words in the sentence. We define a word subsequence of a word sequence as a sequence obtained by removing zero or more words from the original sequence. In word subsequences, the word order of the original sentence is preserved.
 sentence, word subsequences cover cooccurrences of an arbitrary number of non-continuous words as well as continuous words. Therefore incorporating the oc-currences of subsequences into the classification appears to be effective. another word appears between the two words as in Figure 2 . On the contrary, subsequences cover the pattern  X  film-good  X , denoted by s in the figure. 3.2 Dependency Subtree As shown in Figure 3 , a dependency tree is a structured representation of a sen-tence. The dependency tree expresses dependency between words in the sentence by child-parent relationships of nodes. We define a dependency subtree of a de-pendency tree as a tree obtained by removing zero or more nodes and branches from the original tree. The dependency subtree preserves partial dependency between the words in the original sentence. Since each node corresponding to a word is connected by a branch, a dependency subtree would give richer syntactic information than a word N-gram and a word subsequence.
 and  X  film  X , a dependency subtree t (denoted as (is(film)(good)) ) does not only show the cooccurrence of  X  good  X  and  X  film  X , but also guarantees that  X  good  X  and  X  film  X  are syntactically connected by the word  X  is  X . 3.3 Frequent Pattern Mining The number of all sub-patterns of sentences in a document dataset tends to be very large. Thus we consider not all sub-patterns but only all frequent sub-patterns in the dataset. A sentence contains a pattern if and only if the pattern is a subsequence or a subtree of the sentence. We then define the support of a sub-pattern as the number of sentences containing the sub-pattern. If a sup-port of a sub-pattern is a given support threshold or more, the sub-pattern is frequent .
 algorithms.
 Frequent Subsequence Mining : Prefixspan [4] Prefixspan introduced by Pei et al. [ 4 ] is an efficient algorithm for mining all the frequent subsequences from a dataset consisting of sentences. First, the algorithm starts with a set of frequent subsequences consisting of single items (in this paper, corresponding to words). Then the algorithm expands each already-obtained fre-quent subsequence of size k by attaching a new item to obtain frequent sequence of size k + 1. By repeating the latter step recursively, the algorithm obtains all frequent subsequences.
 position leads to duplicated enumeration of the same candidate subsequence. To avoid such enumeration, the algorithm restricts the position to attach a new item to the end of newly-obtained subsequence in left-to-right order. Frequent Subtree Mining : FREQT [5] FREQT introduced by Abe et al. [ 5 ] is an efficient algorithm to mine all fre-quent subtrees from a dataset consisting of trees. First, the algorithm starts with a set of frequent subtrees consisting of single nodes (in this paper, corre-sponding to words). Then the algorithm expands each already-obtained frequent subtree of size k by attaching a new node to obtain frequent tree of size k + 1. By repeating the latter step recursively, the algorithm obtains all frequent sub-trees.
 position of the subtree leads to duplicated enumeration of the same candi-date subtree. To avoid such enumeration, the algorithm restricts the position to attach a new node to the end of newly-obtained subtree in depth-first order. 4.1 Movie Review Dataset We prepared two movie review datasets.
 690 positive and 690 negative movie reviews. Following the experimental settings presented in Pang et al. [ 1 ] and Mullen et al. [ 10 ], we used 3-fold cross validation with this dataset for the evaluation.
 negative movie reviews. Following the experimental settings presented in Pang et al. [ 2 ], we used 10-fold cross validation with this dataset for the evaluation. 4.2 Features We extract word unigram, bigram, word subsequence and dependency subtree patterns from the sentences in the dataset for features of our classifiers. Each type of features is defined as follows.  X  word unigram : uni  X  word bigram : bi  X  frequent word subsequence : seq  X  frequent dependency tree: dep We also extract another feature set whose elements are features consisting of lemmatized words. As in the extraction of the features uni , bi , seq , dep described above, we extract these lemmatized features ( uni l , bi l , seq l , dep l ). fier, we define a feature vector representation of the document. Each dimension corresponds to a feature. The i th dimension X  X  value d i is set to 1 if the i th feature appears in this document, otherwise 0. 4.3 Classifiers and Tests We used support vector machines (SVMs) with the linear kernel as a classifier and the feature vector representation of each document normalized by 2-norm. The linear kernel has a learning parameter C (called a soft margin parameter), which needs adjustment. Since the results of the preliminary experiments in-dicated the performance of the classifier are dependent on this parameter, we carried out three kinds of cross-validation tests: select the best combination of bag-of-words features (we denote these features as bow ) according to the accuracy rate of test 2. We then evaluate the classifier using combinations of the bow and word subsequence and/or dependency subtree pattern features. We finally discuss the improvement of performance by adding the sub-pattern features with the accuracy rate of test 1 and test 3. 4.4 Result and Discussion The results of the experiment on the dataset 1 are shown in Table 2 . The results of the experiment on the dataset 2 are shown in Table 3 .
 92.9% accuracy on the dataset 2. The comparison between these results and the for document sentiment classification than the past researches.
 and 93.2% accuracy on the dataset 2. The comparison between these results and the results obtained by the best bag-of-words classifiers in the test 3 indi-cates that our classifiers are more effective for document sentiment classification than the bag-of-words based classifiers. The contribution of dependency subtree feature is large. Only using this feature with the best bag-of-words feature, we obtained obviously better performance than the preceding bag-of-words based classifiers.
 with bag-of-words features. Since our word pruning strategies is naive, there may exist a more sophisticated strategy which gives higher performance. the best bag-of-words features yields almost the same performance as the classi-fier using dependency trees and the best bag-of-words features. It suggests that there exists large overlap between these two types of pattern features. the classification performance. We consider that the main reason of the dif-ference is the setting of support threshold used to extract bigram patterns. While our method used all bigram patterns which occur at least twice in the dataset, Pang et al [ 1 ] used only patterns which occur at least 7 times in the dataset.
 original ones. If the dataset is large, lemmatizing words may be harmful because it ignores information in the conjugated forms. If the dataset is small, sub-patterns consisting of unlemmatized words tend to be infrequent. Thus there is a risk of missing sub-patterns which are useful for classification. 4.5 Weighted Patterns A classifier, obtained by SVMs with the linear kernel, labels either of two distinct classes to examples based on a weighted voting, where each voter corresponds to a feature of SVMs. The absolute value of each weight indicates how large the contribution of the feature is.
 the following features: uni, bi, seq and dep l 4 . We used all reviews in the dataset 2 as training data. The value of C was set to 1. In Table 4 , we show some pat-terns along with their weights. We could find several heavily-weighted patterns which appear to be effective to detect sentiment polarity (e.g.,  X  stern  X ,  X  pull off  X ,  X  little-life  X ,  X  (without(doubt))  X  in Table 4 ). We also found an overlap of patterns used for the classification. For instance, unigram pattern  X  bad  X  and  X  movie  X , bigram pattern  X  bad movie  X , word subsequence pattern  X  bad-movie  X  and dependency subtree pattern  X  (movie(bad))  X  overlapped each other. In this paper, we have shown the methods for incorporating word order and syntactic relations between words in a sentence into the classification. We have obtained sub-pattern features as information of word order and syntactic rela-tions between words in a document by mining frequent sub-patterns from word sequences and dependency trees in the dataset. In the experiments on the movie review domain, our classifier with a bag-of-words feature and sub-pattern fea-tures showed better performance than past classifiers. In future work, we would like to incorporate discourse structures in a document into the classifier.
