 Sentence simplification can be defined as the process of producing a simplified version of a sentence by changing some of the lexical material and grammat-ical structure of that sentence, while still preserving the semantic content of the original sentence, in or-der to ease its understanding. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Car-roll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chan-drasekar et al., 1996), semantic role labeling (Vick-rey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008).

The goal of simplification is to achieve an im-provement in readability, defined as the ease with which a text can be understood. Some of the factors that are known to help increase the readability of text are the vocabulary used, the length of the sentences, the syntactic structures present in the text, and the usage of discourse markers. One effort to create a simple version of English at the vocabulary level has been the creation of Basic English by Charles Kay Ogden. Basic English is a controlled language with a basic vocabulary consisting of 850 words. Accord-ing to Ogden, 90 percent of all dictionary entries can be paraphrased using these 850 words. An exam-ple of a resource that is written using mainly Basic English is the English Simple Wikipedia. Articles on English Simple Wikipedia are similar to articles found in the traditional English Wikipedia, but writ-ten using a limited vocabulary (using Basic English where possible). Generally the structure of the sen-tences in English Simple Wikipedia is less compli-cated and the sentences are somewhat shorter than those found in English Wikipedia; we offer more de-tailed statistics below. 1.1 Related work Most earlier work on sentence simplification adopted rule-based approaches. A frequently ap-plied type of rule, aimed to reduce overall sentence length, splits long sentences on the basis of syntactic information (Chandrasekar and Srinivas, 1997; Car-roll et al., 1998; Canning et al., 2000; Vickrey and Koller, 2008). There has also been work on lexi-cal substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al., 2003).

Zhu et al. (2010) examine the use of paired doc-uments in English Wikipedia and Simple Wikipedia for a data-driven approach to the sentence simplifi-cation task. They propose a probabilistic, syntax-based machine translation approach to the problem and compare against a baseline of no simplification and a phrase-based machine translation approach. In a similar vein, Coster and Kauchak (2011) use a parallel corpus of paired documents from Sim-ple Wikipedia and Wikipedia to train a phrase-based machine translation model coupled with a deletion model. Another useful resource is the edit his-tory of Simple Wikipedia, from which simplifica-tions can be learned (Yatskar et al., 2010). Woods-end and Lapata (2011) investigate the use of Simple Wikipedia edit histories and an aligned Wikipedia X  Simple Wikipedia corpus to induce a model based on quasi-synchronous grammar. They select the most appropriate simplification by using integer lin-ear programming.
 We follow Zhu et al. (2010) and Coster and Kauchak (2011) in proposing that sentence simpli-fication can be approached as a monolingual ma-chine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. We differ from the approach of Zhu et al. (2010) in the sense that we do not take syntac-tic information into account; we rely on PBMT to do its work and implicitly learn simplifying para-phrasings of phrases. Our approach differs from Coster and Kauchak (2011) in the sense that instead of focusing on deletion in the PBMT decoding stage, we focus on dissimilarity, as simplification does not necessarily imply shortening (Woodsend and Lap-ata, 2011), or as the Simple Wikipedia guidelines state,  X  X impler does not mean short X  1 . Table 1.1 shows the average sentence length and the average word length for Wikipedia and Simple Wikipedia sentences in the PWKP dataset used in this study (Zhu et al., 2010). These numbers suggest that, al-though the selection criteria for sentences to be in-cluded in this dataset are biased (see Section 2.2), Simple Wikipedia sentences are about 17% shorter, while the average word length is virtually equal. Simple Wikipedia 20.87 4.89 Wikipedia 25.01 5.06
Statistical machine translation (SMT) has already been successfully applied to the related task of para-phrasing (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010). SMT typically makes use of large parallel corpora to train a model on. These corpora need to be aligned at the sentence level. Large parallel corpora, such as the multilingual proceedings of the European Parlia-ment (Europarl), are readily available for many lan-guages. Phrase-Based Machine Translation (PBMT) is a form of SMT where the translation model aims to translate longer sequences of words ( X  X hrases X ) in one go, solving part of the word ordering problem along the way that would be left to the target lan-guage model in a word-based SMT system. PMBT operates purely on statistics and no linguistic knowl-edge is involved in the process: the phrases that are aligned are motivated statistically, rather than lin-guistically. This makes PBMT adaptable to any lan-guage pair for which there is a parallel corpus avail-able. The PBMT model makes use of a translation model, derived from the parallel corpus, and a lan-guage model, derived from a monolingual corpus in the target language. The language model is typically an n -gram model with smoothing. For any given in-put sentence, a search is carried out producing an n -best list of candidate translations, ranked by the decoder score, a complex scoring function includ-ing likelihood scores from the translation model, and the target language model. In principle, all of this should be transportable to a data-driven machine translation account of sentence simplification, pro-vided that a parallel corpus is available that pairs text to simplified versions of that text. 1.2 This study In this work we aim to investigate the use of phrase-based machine translation modified with a dissim-ilarity component for the task of sentence simplifi-cation. While Zhu et al. (2010) have demonstrated that their approach outperforms a PBMT approach in terms of Flesch Reading Ease test scores, we are not aware of any studies that evaluate PBMT for sen-tence simplification with human judgements. In this study we evaluate the output of Zhu et al. (2010) (henceforth referred to as  X  X hu X ), Woodsend and La-pata (2011) (henceforth referred to as  X  X evILP X ), our PBMT based system with dissimilarity-based re-ranking (henceforth referred to as  X  X BMT-R X ), a word-substitution baseline, and, as a gold standard, the original Simple Wikipedia sentences. We will first discuss the baseline, followed by the Zhu sys-tem, the RevILP system, and our PBMT-R system in Section 2. We then describe the experiment with human judges in Section 3, and its results in Sec-tion 4. We close this paper by critically discussing our results in Section 5. 2.1 Word-Substitution Baseline The word substitution baseline replaces words in the source sentence with (near-)synonyms that are more likely according to a language model. For each noun, adjective and verb in the sentence this model takes that word and its part-of-speech tag and retrieves from WordNet all synonyms from all synsets the word occurs in. The word is then re-placed by all of its synset words, and each replace-ment is scored by a SRILM language model (Stol-cke, 2002) with probabilities that are obtained from training on the Simple Wikipedia data. The alter-native that has the highest probability according to the language model is kept. If no relevant alterna-tive is found, the word is left unchanged. We use the Memory-Based Tagger (Daelemans et al., 1996) trained on the Brown corpus to compute the part-of-speech tags. The WordNet::QueryData 2 Perl mod-ule is used to query WordNet (Fellbaum, 1998). 2.2 Zhu et al.
 Zhu et al. (2010) learn a sentence simplification model which is able to perform four rewrite op-erations on the parse trees of the input sentences, namely substitution, reordering, splitting, and dele-tion. Their model is inspired by syntax-based SMT (Yamada and Knight, 2001) and consists of a language model, a translation model and a de-coder. The four mentioned simplification opera-tions together form the translation model. Their model is trained on a corpus containing aligned sen-tences from English Wikipedia and English Simple Wikipedia called PWKP. The PWKP dataset con-sists of 108,016 pairs of aligned lines from 65,133 Wikipedia and Simple Wikipedia articles. These ar-ticles were paired by following the  X  X nterlanguage link X  3 . TF*IDF at the sentence level was used to align the sentences in the different articles (Nelken and Shieber, 2006).
 Zhu et al. (2010) evaluate their system using BLEU and NIST scores, as well as various read-ability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n -gram language model perplexity. Although their system outperforms several baselines at the level of these readability metrics, they do not achieve better when evaluated with BLEU or NIST. 2.3 RevILP Woodsend and Lapata X  X  (2011) model is based on quasi-synchronous grammar (Smith and Eisner, 2006). Quasi-synchronous grammar generates a loose alignment between parse trees. It operates on individual sentences annotated with syntactic infor-mation in the form of phrase structure trees. Quasi-synchronous grammar is used to generate all pos-sible rewrite operations, after which integer linear programming is employed to select the most ap-propriate simplification. Their model is trained on two different datasets: one containing alignments between Wikipedia and English Simple Wikipedia (AlignILP), and one containing alignments between edits in the revision history of Simple Wikipedia (RevILP). RevILP performs best according to the human judgements conducted in their study. They show that it achieves better scores than Zhu et al. (2010) X  X  system and is not scored significantly dif-ferently from English Simple Wikipedia. In this study we compare against their best performing sys-tem, the RevILP system. 2.4 PBMT-R We use the Moses software to train a PBMT model (Koehn et al., 2007). The data we use is the PWKP dataset created by Zhu et al. (2010). In gen-eral, a statistical machine translation model finds a best translation  X  e of a text in language f to a text in language e by combining a translation model that finds the most likely translation p ( f | e ) with a lan-guage model that outputs the most likely sentence p ( e ) :
The GIZA++ statistical alignment package is used to perform the word alignments, which are later combined into phrase alignments in the Moses pipeline (Och and Ney, 2003) to build the sentence simplification model. GIZA++ utilizes IBM Models 1 to 5 and an HMM word alignment model to find statistically motivated alignments between words. We first tokenize and lowercase all data and use all unique sentences from the Simple Wikipedia part of the PWKP training set to train an n -gram lan-guage model with the SRILM toolkit to learn the probabilities of different n -grams. Then we invoke the GIZA++ aligner using the training simplifica-tion pairs. We run GIZA++ with standard settings and we perform no optimization. This results in a phrase table containing phrase pairs from Wikipedia and Simple Wikipedia and their conditional proba-bilities as assigned by Moses. Finally, we use the Moses decoder to generate simplifications for the sentences in the test set. For each sentence we let the system generate the ten best distinct solutions (or less, if fewer than ten solutions are generated) as ranked by Moses.

Arguably, dissimilarity is a key factor in simpli-fication (and in paraphrasing in general). As output we would like to be able to select fluent sentences that adequately convey the meaning of the original input, yet that contain differences that operational-ize the intended simplification. When training our PBMT system on the PWKP data we may assume that the system learns to simplify automatically, yet there is no aspect of the decoder function in Moses that is sensitive to the fact that it should try to be different from the input  X  Moses may well trans-late input to unchanged output, as much of our train-ing data consists of partially equal input and output strings.

To expand the functionality of Moses in the in-tended direction we perform post-hoc re-ranking on the output based on dissimilarity to the input. We do this to select output that is as different as possi-ble from the source sentence, so that it ideally con-tains multiple simplifications; at the same time, we base our re-ranking on a top-n of output candidates according to Moses, with a small n , to ensure that the quality of the output in terms of fluency and ade-quacy is also controlled for. Setting n = 10 , for each source sentence we re-rank the ten best sentences as scored by the decoder according to the Leven-shtein Distance (or edit distance) measure (Leven-shtein, 1966) at the word level between the input and output sentence, counting the minimum num-ber of edits needed to transform the source string into the target string, where the allowable edit op-erations are insertion, deletion, and substitution of a single word. In case of a tie in Levenshtein Distance, we select the sequence with the better decoder score. When Moses is unable to generate ten different sen-tences, we select from the lower number of outputs. Figure 1 displays Levenshtein Distance and Flesch-Kincaid grade level scores for different values of n . We use the Lingua::EN::Fathom module 4 to calcu-late Flesch-Kincaid grade level scores. The read-ability score stays more or less the same, indicating no relation between n and readability. The average edit distance starts out at just above 2 when selecting the 1 -best output string, and increases roughly until n = 10 . 2.5 Descriptive statistics Table 2 displays the average edit distance and the percentage of cases in which no edits were per-formed for each of the systems and for Simple Wikipedia. We see that the Levenshtein distance be-tween Wikipedia and Simple Wikipedia is the most substantial with an average of 12.3 edits. Given that the average number of tokens is about 25 for Wikipedia and 21 for Simple Wikipedia (cf. Ta-ble 1.1), these numbers indicate that the changes in Simple Wikipedia go substantially beyond the aver-age four-word length difference. On average, eight more words are interchanged for other words. About half of the original tokens in the source sentence do not return in the output. Of the three simplifica-tion systems, the Zhu system (7.95) and the RevILP (7.18) attain similar edit distances, less substantial than the edits in Simple Wikipedia, but still consid-erable compared to the baseline word-substitution system (4.26) and PBMT-R (3.08). Our system is clearly conservative in its edits.

On the other hand, we observe some differences in the percentage of cases in which the systems de-cide to produce a sentence identical to the input. In 22 percent of the cases the RevILP system does not alter the sentence. The other systems make this decision about as often as the gold standard, Sim-ple Wikipedia, where only 3% of sentences remain unchanged. The word-substitution baseline always manages to make at least one change. 3.1 Participants Participants were 46 students of Tilburg University, who participated for partial course credits. All were native speakers of Dutch, and all were proficient in English, having taken a course on Academic English at University level. 3.2 Materials We use the test set used by Zhu et al. (2010) and Woodsend and Lapata (2011). This test set consists of 100 sentences from articles on English Wikipedia, paired with sentences from corresponding articles in English Simple Wikipedia. We selected only those sentences where every system would perform min-imally one edit, because we only want to compare the different systems when they actually generate al-tered, assumedly simplified output. From this sub-set we randomly pick 20 source sentences, result-ing in 20 clusters of one source sentence and 5 sim-plified sentences, as generated by humans (Simple Wikipedia) and the four systems. 3.3 Procedure The participants were told that they participated in the evaluation of a system that could simplify sen-tences, and that they would see one source sentence and five automatically simplified versions of that sentence. They were not informed of the fact that we evaluated in fact four different systems and the orig-inal Simple Wikipedia sentence. Following earlier evaluation studies (Doddington, 2002; Woodsend and Lapata, 2011), we asked participants to evalu-ate Simplicity, Fluency and Adequacy of the target headlines on a five point Likert scale. Fluency was defined in the instructions as the extent to which a sentence is proper, grammatical English. Adequacy was defined as the extent to which the sentence has the same meaning as the source sentence. Simplic-ity was defined as the extent to which the sentence was simpler than the original and thus easier to un-derstand. The order in which the clusters had to be judged was randomized and the order of the output of the various systems was randomized as well. 4.1 Automatic measures The results of the automatic measures are displayed in Table 3. In terms of the Flesch-Kincaid grade level score, where lower scores are better, the Zhu system scores best, with 7.86 even lower than Sim-ple Wikipedia (8.57). Increasingly worse Flesch-Kincaid scores are produced by RevILP (8.61) and PBMT-R (13.38), while the word substitution base-line scores worst (14.64). With regard to the BLEU score, where Simple Wikipedia is the reference, the PBMT-R system scores highest with 0.43, followed by the RevILP system (0.42) and the Zhu system (0.38). The word substitution baseline scores low-est with a BLEU score of 0.34.
 4.2 Human judgements To test for significance we ran repeated mea-sures analyses of variance with system (Sim-ple Wikipedia, PBMT-R, Zhu, RevILP, word-substitution baseline) as the independent variable, and the three individual metrics as well as their com-bined mean as the dependent variables. Mauchlys test for sphericity was used to test for homogeneity of variance, and when this test was significant we applied a Greenhouse-Geisser correction on the de-grees of freedom (for the purpose of readability we report the normal degrees of freedom in these cases). Planned pairwise comparisons were made with the Bonferroni method. Table 4 displays these results.
First, we consider the 3 metrics in isolation, be-ginning with Fluency. We find that participants rated the Fluency of the simplified sentences from the four systems and Simple Wikipedia differently, F (4 , 180) = 178 . 436 ,p &lt; . 001 , X  2 p = . 799 . The word-substitution baseline, Simple Wikipedia and PBMT-R receive the highest scores (3.86, 3.84 and 3.83 respectively) and don X  X  achieve significantly different scores on this dimension. All other pair-wise comparisons are significant at p &lt; . 001 . Rev-ILP attains a score of 3.18, while the Zhu system achieves the lowest mean judgement score of 2.59.
Participants also rated the systems significantly differently on the Adequacy scale, F (4 , 180) = 116 . 509 ,p &lt; . 001 , X  2 p = . 721 . PBMT-R scores highest (3.71), followed by the word-substitution baseline (3.58), RevILP (3.28), and then by Simple Wikipedia (2.91) and the Zhu system (2.82). Sim-ple Wikipedia and the Zhu system do not differ sig-nificantly, and all other pairwise comparisons are significant at p &lt; . 001 . The low score of Simple Wikipedia indicates indirectly that the human edi-tors of Simple Wikipedia texts often choose to devi-ate quite markedly from the meaning of the original text.

Key to the task of simplification are the hu-man judgements of Simplicity. Participants rated the Simplicity of the output from the four sys-tems and Simple Wikipedia differently, F (4 , 180) = 74 . 959 ,p &lt; . 001 , X  2 p = . 625 . Simple Wikipedia scores highest (3.68) and the word substitution base-line scores lowest (2.42). Between them are the RevILP (2.96), Zhu (2.93) and PBMT-R (2.88) sys-tems, which do not score significantly differently from each other. All other pairwise comparisons are significant at p &lt; . 001 .

Finally we report on a combined score created by averaging over the Fluency, Adequacy and Simplic-ity scores. Inspection of this score, displayed in the leftmost column of Table 4, reveals that the PBMT-R system and Simple Wikipedia score best (3.47 and 3.46 respectively), followed by the word substi-tution baseline (3.39), which in turn scores higher than RevILP (3.13) and the Zhu system (2.78). We find that participants rated the systems signifi-cantly differently overall, F (4 , 180) = 98 . 880 ,p &lt; . 001 , X  2 p = . 687 . All pairwise comparisons were sta-tistically significant ( p &lt; . 01 ), except the one be-tween the PBMT-R system and Simple Wikipedia. 4.3 Correlations Table 5 displays the correlations between the scores assigned by humans (Fluency, Adequacy and Sim-plicity) and the automatic metrics (Flesch-Kincaid and BLEU). We see a significant correlation be-tween Fluency and Adequacy (0.45), as well as be-tween Fluency and Simplicity (0.24). There is a neg-ative significant correlation between Flesch-Kincaid scores and Simplicity (-0.45) while there is a posi-tive significant correlation between Flesch-Kincaid and Adequacy and Fluency. The significant correla-tions between BLEU and Simplicity (0.42) and Flu-ency (0.26) are both in the positive direction. There is no significant correlation between BLEU and Ad-equacy, indicating BLEU X  X  relative weakness in as-sessing the semantic overlap between input and out-put. BLEU and Flesch-Kincaid do not show a sig-nificant correlation. We conclude that a phrase-based machine trans-lation system with added dissimilarity-based re-ranking of the best ten output sentences can suc-cessfully be used to perform sentence simplifica-tion. Even though the system merely performs phrase-based machine translation and is not specif-ically geared towards simplification were it not for the dissimilarity-based re-ranking of the output, it performs not significantly differently from state-of-the-art sentence simplification systems in terms of human-judged Simplification. In terms of Fluency and Adequacy our system is judged to perform sig-nificantly better. From the relatively low average numbers of edits made by our system we can con-clude that our system performs relatively small num-bers of changes to the input, that still constitute as sensible simplifications. It does not split sentences (which the Zhu and RevILP systems regularly do); it only rephrases phrases. Yet, it does this better than a word-substitution baseline, which can also be considered a conservative approach; this is reflected in the baseline X  X  high Fluency score (roughly equal to PBMT-R and Simple Wikipedia) and Adequacy score (only slightly worse than PBMT-R).
The output of all systems, the original and the simplified version of an example sentence from the PWKP dataset is displayed in Table 6. The Simple Wikipedia sentences illustrate that significant por-tions of the original sentences may be dropped, and parts of the semantics of the original sentence dis-carded. We also see the Zhu and RevILP systems resorting to splitting the original sentence in two, leading to better Flesch-Kincaid scores. The word-substitution baseline changes  X  X eceive X  in  X  X ave X , while the PBMT-R system changes the same  X  X e-ceive X  in  X  X et X ,  X  X lightly X  to  X  X  little bit X , and  X  X axi-mum X  to  X  X ighest X .
 In terms of automatic measures we see that the Zhu system scores particularly well on the Flesch-Kincaid metric, while the RevILP system and our PBMT-R system achieve the highest BLEU scores. We believe that for the evaluation of sentence sim-plification, BLEU is a more appropriate metric than Flesch-Kincaid or a similar readability metric, al-though it should be noted that BLEU was found only to correlate significantly with Fluency, not with Ad-equacy. While BLEU and NIST may be used with this in mind, readability metrics should be avoided altogether in our view. Where machine translation evaluation metrics such as BLEU take into account gold references, readability metrics only take into account characteristics of the sentence such as word length and sentence length, and ignore grammatical-ity or the semantic adequacy of the content of the output sentence, which BLEU is aimed to implic-itly approximate by measuring overlap in n -grams. Arguably, readability metrics are best suited to be applied to texts that can be considered grammati-cal and meaningful, which is not necessarily true for the output of simplification algorithms. A disrup-tive example that would illustrate this point would be a system that would randomly split original sen-tences in two or more sequences, achieving consid-erably lower Flesch-Kincaid scores, yet damaging the grammaticality and semantic coherence of the original text, as is evidenced by the negative cor-relation for Simplicity and positive correlations for Fluency and Adequacy in Table 5.

In the future we would like to investigate how we can boost the number of edits the system performs, while still producing grammatical and meaning-preserving output. Although the comparison against the Zhu system, which uses syntax-driven machine translation, shows no clear benefit for syntax-based machine translation, it may still be the case that ap-proaches such as Hiero (Chiang et al., 2005) and Joshua (Li et al., 2009), enhanced by dissimilarity-based re-ranking, would improve over our current system. Furthermore, typical simplification oper-ations such as sentence splitting and more radical syntax alterations or even document-level operations such as manipulations of the co-reference structure would be interesting to implement and test We are grateful to Zhemin Zhu and Kristian Woods-end for sharing their data. We would also like to thank the anonymous reviewers for their comments.
