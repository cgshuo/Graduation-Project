 We present a novel approach for classifying documents that combines different pieces of evidence (e.g., textual features of documents, links, and citations) transparently, through a data mining technique which generates rules associating these pieces of evidence to predefined classes. These rules can contain any number and mixture of the available evi-dence and are associated with several quality criteria which can be used in conjunction to choose the  X  X est X  rule to be applied at classification time. Our method is able to perform evidence enhancement by link forwarding/backwarding (i.e., navigating among documents related through citation), so that new pieces of link-based evidence are derived when nec-essary. Furthermore, instead of inducing a single model (or rule set) that is good on average for all predictions, the pro-posed approach employs a lazy method which delays the inductive process until a document is given for classifica-tion, therefore taking advantage of better qualitative evi-dence coming from the document. We conducted a system-atic evaluation of the proposed approach using documents from the ACM Digital Library and from a Brazilian Web directory. Our approach was able to outperform in both collections all classifiers based on the best available evidence in isolation as well as state-of-the-art multi-evidence classi-fiers. We also evaluated our approach using the standard WebKB collection, where our approach showed gains of 1% in accuracy, being 25 times faster. Further, our approach is extremely efficient in terms of computational performance, showing gains of more than one order of magnitude when compared against other multi-evidence classifiers. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval; I.5.2 [ Pattern Recognition ]: Clas-sifier design and evaluation Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. Algorithms, Experimentation Classification, Data Mining, Lazy Algorithms
Automatic document classification has become a central research topic in Information Retrieval due to the increasing number of large document collections, the heterogeneity of the documents (for instance, papers and Web pages), and the need to organize them for users in a uniform fashion, so that they are easy to find [15]. Utilization scenarios include constructing online Web directories and digital libraries, im-proving the precision of Web searching, and even helping users to interact with search engines [20].

The task of building a (machine learning) document clas-sifier may be divided into three main steps: 1) gathering of the best evidence from any available source (e.g., textual and linkage); 2) determining one or more sets of evidence, or their combinations, to induce a classification model used to predict classes for documents; and 3) ranking of the sets of evidence within the induced model towards determining the document category. Document classification is a well known challenge, since classifiers must be robust to noisy, conflicting, insufficient, and absent evidence, as well as able to evolve, accounting for novel evidence.

In this paper, we present new techniques for improving the effectiveness of all three main document classification steps. Regarding evidence gathering, a novel lazy associative in-duction approach is proposed. It not only takes advantage of more focused and probably better qualitative evidence to induce a classification model, but is also much more efficient. Instead of inducing a single (and typically very large) model that is good on average for all predictions 1 , in the proposed lazy approach the inductive process is delayed until a docu-ment is given for classification. Then, a specific (and much smaller) model is induced, since it focus on the evidence present in the document to be classified. Further, to deal with the issue of poor or absent evidence, we also perform ev-
A typical decision tree classifier, for example, uses a stored decision tree to classify documents by tracing the document through the interior nodes until a leaf containing the cate-gory is reached. idence enhancement by progressively finding new link-based evidence using a link forwarding/backwarding process (i.e., documents citing the same documents, documents cited by the same documents etc.). Different evidence are combined transparently and naturally through an associative tech-nique, which generates a model composed of a set of rules, where each rule X X  c denotes a strong association between a combination of different pieces of evidence ( X )andacate-gory ( c ). These rules may contain any number and mixture of the available evidence, given that they satisfy some prede-fined quality criteria. Also, rules are generated on demand, following a level-wise adaptive process that stops as soon as sufficient rules are generated. Once a rule set is induced, the evidence within the rules are ranked according to one or more statistical criteria that quantify their quality. We empirically conclude that error rate is reduced if multiple criteria are combined, and we propose a multi-criteria ap-proach, so that misclassification is minimized.

The techniques and strategies proposed here result in a novel and enhanced classification approach. To evaluate its effectiveness, we performed experiments using three differ-ent collections: one derived from the ACM Digital Library, one based on a Brazilian Web directory, and also the stan-dard WebKB collection. In the first case, our classification approach was able to achieve more than 90% accuracy, out-performing the best previously known classifiers. Similar re-sults were obtained for the second collection, with gains up to 17% over the baseline results. Gains of 1% are observed in the WebKB collection, but in this case, classification is extremely fast compared with the baseline approach. In fact, we also show that our approach is extremely efficient in terms of computational performance, showing gains of more than one order of magnitude when compared to other existing multi-evidence classification approaches.
Several works have explored the combination of different pieces of evidence, more notably, textual and linkage-based evidence, to boost the performance of automated classifiers. We can divide these in three main approaches: contextual, mixture of experts, and mixture of evidence.

In the first one, the context in which the linkage informa-tion occurs (e.g., terms extracted from linked pages, anchor text describing the links, paragraphs surrounding the links) is used either in isolation as textual evidence or to expand the text of the document being classified. [9, 18] achieved good results by using anchor text together with the para-graphs and headlines that surround the links, whereas [22] shows that the use of terms from linked documents works better when neighboring documents are all in the same class.
In the mixture of experts approach, the output of several classifiers based on the available evidence is combined. By using a combination of link-based and text-based methods, [5] improved classification accuracy over a text-based base-line. This work was extended in [7], which shows that link information is useful when the documents have a high link density and most links are of high quality. In [4] the authors explored similar ideas by combining the decisions of linkage and text classifiers using a belief network strategy.
Finally, in the mixture of evidence approach, the available evidence is explicitly combined to be used within standard classifiers. [11] studied the linear combination of support vector machine kernel functions representing co-citation and textual information. [25] discovers non-linear similarity func-tions through Genetic Programming techniques to combine 14 different types of textual and linkage evidence and used these discovered functions within kNN-based classifiers[21].
The techniques proposed in this paper share similarities and differences with both mixture-based approaches. Each quality criteria used in our multi-criteria approach can be thought of as a particular classification expert. The dif-ferences are exactly in the precise nature of these quality criteria as well as in the fact that these experts are respon-sible not only for classifying a given document but also for choosing the best classification rules to be applied in this process. Regarding mixture of evidence, differences reside in the way the available evidence is explored. In previous works, each type of evidence produces a different classifica-tion space; these needed to be explicitly combined with oth-ers, through linear or non-linear methods. In our approach, all evidence is treated within a unique classification space in a transparent way. The pieces of evidence that will enter in our rules, independently of their types, will be the ones that are more discriminative of the categories in which they occur according to our quality c riteria. The generated rules that mix these evidence will be directly used to classify new documents, instead of being used within other algorithms.
We take decision tree induction [2, 14] as the represen-tative of rule-based classifiers. The induction of decision trees is based on a local search which attempts to append most promising pieces of evidence to rules. Rules are col-lectively generated, and the worth of rules is measured by their contribution to the overall accuracy. Since the induc-tion is based on a local search, the rule set is usually very limited. As a consequence, decision tree induction usually suffers from the missing rule problem [13] (i.e., when no rule matches a test document), which must be handled using a default category. Differently from decision tree induction, associative classifiers [12, 13, 23] search globally for all inter-esting rules. Therefore, associative classifiers usually cover the training set better than decision trees. On the other hand, associative classifiers usually generate very large rule sets, and most of the rules are not used during classification.
In [8] the authors proposed a lazy algorithm for decision tree induction which alleviates the missing rule problem, since a specific decision tree is induced for each test doc-ument. However, this approach still suffer from the miss-ing rule problem, inherent from the decision tree induction approach. Our proposed approach is similar to other lazy approaches, such as kNN[21], in the sense that they sample the training set at classification time. However, those lazy approaches (especially the kNN-based ones) just reduce the number of instances considered, employing all attributes, and thus may be affected by the curse of dimensionality. On the other hand, our approach samples based on a set of attributes, which is usually much smaller than the attribute domain. In summary, kNN reduces the numerosity of the training set while our approach reduces the dimensionality of the training set. There is a lack of studies for lazy asso-ciative classification, which is one of our contributions.
In this section we present our lazy inductive approach, which consists of generating combinations of pieces of ev-idence, and mapping them to predefined categories. This mapping is done through the discovery of association rules X X  c , where the antecedent X is a combination of different pieces of evidence, and the consequent c is a category.
Definition 1. [ Documents ]Let D denote the set of m documents { d 1 ,d 2 ,...,d m } ,whereeach d i is composed of a category (or class) c along with a set of pieces of evidence.
Definition 2. [ Esets ]Let E denote the set of all unique pieces of evidence. We consider two main sources of evi-dence: textual and linkage-based. Textual pieces of evidence (t) are basically words that occur within the documents. Linkage evidence is composed of incoming (i) and outcom-ing (o) links or citations. An eset is simply a non-empty subset of E , and it may contain different types of evidence. We use |X| to denote the cardinality or size of eset X .
Definition 3. [ Dsets ] For an eset X there is a corre-sponding document set, called dset , denoted as s ( X ), which is the set of all document identifiers in the training set which contain X . Each category c also has a corresponding dset, s ( c ), which is the set of all document identifiers belonging to category c . The support (or frequency) of X is the frac-tion of documents in the training set that contain X ,given frequent if  X  ( X )  X   X  min ,where  X  min is a user-specified min-imum support threshold.

Consider the collection shown in Table 1, used as a run-ning example in this paper. There are ten documents in the training set, and three documents in the test set. The textual evidence set consists of all the words that appear in the documents (except stop words), t= { algorithm(s), ap-plication(s), approach(es), association, challenge(s), data, database(s), digital, exception(s), expert(s), filtering, hyper-text, information, large, library(ies), logic, mining, perfor-mance, perspective, retrieval, rule(s), system(s), term(s), text, weighting, workflow } . The linkage-based evidence con-sists of incoming links, i= { 1,3,4,5,6,7,8,10,11,12 } ,andout-going links o= { 1,2,4,5,6,7,8,9,10,11 } . Notice, for instance, that document d 1 contains the linkage-based evidence i =12 and o =2, since it is pointed to by document d 12 , and it points to document d 2 . The dset for t=mining is s ( t = mining )= 4,5,6 } , while the dset for i=6 is s ( i =6)= { 4 } . Different pieces of evidence can be combined by performing the intersection of their dsets, for example, the dset of { t=mining, i=6 } be obtained as s ( { t = mining, i =6 } )= s ( t = mining ) { 4 } .

Definition 4. [ Association Rules ]Therule X  X   X   X  c associates an eset X to a category c . The support of the rule given in terms of its confidence, defined as the conditional probability of the consequent when the antecedent is known: The rule X  X   X   X  c is strong if  X   X   X  min ,where  X  min is a user-specified minimum confidence threshold. The size of X X  c is given by |X| +1.

In general, the associative classification approach consists of three major steps: 1) generating frequent esets (i.e., com-bining pieces of evidence), 2) inducing strong rules, and 3) ranking best rules. There are two main approaches for asso-ciative classification: the traditional eager approach [13, 23] or the novel lazy approach we employ in this paper. Eager Approach: In the eager approach, rules are in-duced from the frequent esets obtained from all documents in the training set. For instance, consider the example in Table 1, and suppose that  X  min =0 . 30 (i.e., at least 3 occurrences are required in the training set of 10 docu-ments) and  X  min =0 . 75. The set of all frequent esets in the training set is given by t=database(6), t=information(3), t=mining(4), t=retrieval(4), t= { information,retrieval } i=3(4), i=7(3) . The number of occurrences of an eset is giveninbrackets(e.g., i=3(4) means that i =3 occurs 4 times in the training set). For each eset X we have to check the rule X X  c for each class c . The rule must pass both the  X  min and  X  min thresholds to be retained. The rule set that would be generated by the eager approach is then given by: 1 t=mining  X  =0 . 75  X   X  X  X  X   X  c=data mining 2 t=information  X  =1 . 00  X   X  X  X  X   X  c=inf. retrieval 3t= { information,retrieval }  X  =1 . 00  X   X  X  X  X   X  c=inf. retrieval 4 t=retrieval  X  =1 . 00  X   X  X  X  X   X  c=inf. retrieval
The other rules are either not frequent or do not have enough confidence, and are thus discarded. Note that, in general textual information is much more frequent than link-based information, and therefore, important rules based on link-based evidence will be possibly missed if  X  min is set high (as shown in the example above, there were no linkage-based rules). On the other hand, the number of rules drastically increases by lowering  X  min value. For instance, if we drop  X  min to 0 . 20 the number of rules goes to 38, and if we drop  X  min to 0 . 10 the number of rules, in this simple example, surpasses 1,000. Thus, the number of rules mined in the eager approach can get very large, especially if there is a skew in the class frequencies, and if we consequently have to lower the minimum support threshold.
 Lazy Approach: In the novel lazy approach that we adopt in this paper, a different set of rules is induced from the frequent esets obtained from the training set, for each test document . Whereas eager approaches induce a single rule set from the training set, lazy approaches induce a specific rule set for a given test document. The lazy approach projects the training set only on those attribute-values (pieces of ev-idence) relevant to classifying a given test document, and then it induces the rules from this projected training set. The projected training set is composed of documents in the training set that share at least one attribute with the test document. That is, only the relevant portion of the training set is used to induce the rules. For instance, suppose we want to predict the category for document d 11 . The first step is to project the training set for d 11 ,whichiscomposed of documents { d 1 , d 2 , d 3 , d 4 , d 5 , d 6 , d 7 , d 8 , d 10 not in the projected training set, since it has no attribute in commonwithtestdocument d 11 , and thus d 9 is not relevant for classifying d 11 . Once the projected training set is calcu-lated, the rule induction starts. Since the number of strong rules is bounded by the number of possibly frequent esets within a specific test document (and not on all frequent es-ets in the training set), we can employ lower values of  X  without generating an overwhelming number of rules. Training 1 Databases Rules in Database Systems 12 2 Unlike the eager case, there is no explicity training phase. Each test document is classified directly in the testing phase using the rules generated on demand from the training set. Our lazy approach consists in generating the top k rules that are the most general and which satisfy the thresholds,  X  min and  X  min .Thevalueof k is chosen suitably to ensure that there is enough information to classify the test document.
The rule induction proceeds in a level-wise manner, which first induces all (most general) rules of size two (i.e., hav-ing a single piece of evidence as the antecedent). If k rules have been generated the process stops. Otherwise, rules of size three are induced, looking at combinations of evidence as antecedent. The rule support is obtained by perform-ing the intersection of the dsets of the corresponding esets. This process continues generating longer (and more specific) rules until at most k rules are induced or there are no more rules to induce. For instance, consider the example in Ta-ble 1, and suppose we want to predict the category for doc-ument d 11 . Also, suppose that we use a (lower)  X  min =0 . 10,  X  min =0 . 75, and k =3. First, rules of size two are generated by looking at each piece of evidence in document d 11 in iso-lation. For example, we find t=database(s)(6) , t=mining(4) , i=7(3) , o=9(2) as the only frequent esets; the other evidence in the test case ( t=challenge(s), t=digital, t=library(ies) )do not occur even once in the training set. The rule set that is finally induced is given by: 1o=9  X  =1 . 00  X   X  X  X  X   X  c=inf. retrieval 2 t=mining  X  =0 . 75  X   X  X  X  X   X  c=data mining
No more rules can be generated, and the process stops, even though we were not able to generate three rules. No-tice that, even after lowering  X  min value, the number of rules generated by the lazy approach is smaller than the number of rules generated by the eager approach. Further, all rules generated by the lazy approach match document proach matches this document. This shows the advantages of inducing specific rule sets.
After induction, the mined rules are sorted/ranked in as-cending order of  X  . Ties are broken by also considering their  X  values, again in ascending order. Any remaining ties are broken arbitrarily. The resulting ranking, given by R  X  ,is then used to assign a numerical weight to the rules; the weight being the rank/position of the rule in R  X  ,givenby R  X  ( X X  c ). Thus each rule X X  c  X  X   X  is interpreted as a(weighted)votebyeset X for category c .Higherranked rules thus count for more in the voting process. Formally, the weighted vote given by eset X for category c is given by: Finally, the score of a category is the sum of the weighted votes assigned to it, represented by the function:
The category with highest score is chosen as the predicted class (if two or more categories receive the same score, the most frequent is chosen). For instance, consider the rule set induced by our lazy approach. In this case, R  X  = { 2 , 1 sorting in ascending order of  X  . Thus rule 1 has rank 2 and weights more heavily than rule 2. Rule 1 gives the predicted class:  X  X nf. Retrieval X , which happens to be correct. Note that, for this example, the eager approach leads to a wrong is t=mining  X  =0 . 75  X   X  X  X  X   X  c=data mining .
Having presented our basic lazy induction approach, we now consider the effect of linkage-based evidence in more detail. Different meanings from links and citations between documents can be inferred. If two documents are linked, their subjects are likely to be related. Similarly, if two doc-uments are pointed by (or point to) common documents, their subjects also can be related. Additional degrees of relationships can also be explored. For example, one can assume that if document A points to document B , and doc-ument B points to document C , then documents A , B and C are somewhat related [10]. These relationships can be used to enhance link-based evidence. Consider again the exam-ple shown in Table 1, and now assume we want to classify document d 12 .Ifweset k =3,  X  min =0 . 10, and  X  min =0 . 50, only two rules will be induced by our lazy approach: 1t=system(s)  X  =0 . 50  X   X  X  X  X   X  c=databases 2t=system(s)  X  =0 . 50  X   X  X  X  X   X  c=inf. retrieval
Notice that document d 12 points to document d 1 , but no rule can be induced from this information, because o =1 does not occur in the training set. In our enhanced approach, we derive other link-based evidential information by performing a link forwarding approach, which replaces the piece of evi-dence o =1 by the outlink of document d 1 (i.e., o =2). Now, the following rule also will be induced: 3o=2  X  =1 . 00  X   X  X  X  X   X  c=databases Ranking these rules we have R  X  = { 1 , 2 , 3 } , and thus cate-gory  X  X atabases X , which receives votes from the esets within rules 1 and 3, will score 4 according to Eq. (3), and will be the (correctly) predicted category.
Processing a rule has a significant computational cost, since it involves performing the intersection of several dsets. Different documents may induce different rule sets, but dif-ferent rule sets may share commo n rules. In this case, caching is very effective in reducing work replication.

Our cache is a pool of entries, and it stores rules of the form X  X   X   X  c . Each entry has the form &lt;key, data&gt; ,where key = {X ,c } and data = {  X  } . Our implementation has a lim-ited storage and stores all cached rules in main memory. Before generating a rule X  X   X   X  c , the classifier first checks whether this rule is already in the cache. If an entry is found with a key matching {X ,c } , the rule in the cache en-try is used instead of processing it. If it is not found, the rule is processed and then it is inserted into the cache.
The cache size is limited, and when the cache is full, some rules to discard to make room for other rules. The best procedure would be to always discard the rules that will not be used for the longest time in the future. Since it is impossible to predict how far in the future a specific rule will be used, we choose the LFU (Least Frequently Used) heuristic (which counts how often a rule is used, and those that are used least are discarded first). Consider our running example of Table 1. Notice that rules t=system(s)  X  =0 . 50  X   X  X  X  X   X  c=databases t=system(s)  X  =0 . 50  X   X  X  X  X   X  c=inf. retrieval are common in the rule sets induced for documents d 12 and We show empirically that rule caching is extremely effective in reducing the computation time for lazy induction.
The use of a single statistical measure usually does not capture all relevant characteristics of a rule [19]. Confidence (  X  ), for instance, is often used to measure the accuracy of arule X  X   X   X  c . However, this information may be mislead-ing, especially when  X  ( c ) &gt; X  [3]. Notice that confidence is not affected by the class frequency, that is, it is not pos-sible to correlate the class frequency to the confidence of rules associated with that class. On the other hand, an interesting property of  X  (which is useful in sparse data, such as text documents) is that it is null-invariant, in the sense that adding documents that contain neither X nor c to the collection does not change its value (i.e., co-presence is more important than co-absence). This short discussion just illustrates that, in practice, all measures imply some trade-off and the best measure for sake of classification is a function of the application and the input data. One charac-teristic that is relevant to text classification is the asymme-try under permutation of antecedent and consequent (i.e.,  X  (
X X  c ) =  X  ( c  X  X  )). Regarding this characteristic, confi-dence is a weak measure of the reciprocal correlation be-tween an eset and a class, and it is desirable to use other criteria that better quantify the classification accuracy of a rule. It is also important to remark that these criteria may produce different rule rankings, making it necessary to define a combination strategy to generate the final class assignment. We discuss these issues in the next sections.
There are several measures that can be used as criteria to select and rank rules. In our case, we are looking for measures that emphasize, at various degrees, the strength of the correlation between an eset and a class. Towards this end, we adopt and evaluate three statistical measures: Weighted Confidence [24]: An eset X may appear too frequently in some categories, and too rarely in others. Rel-ative support is the support of ( X ,c ) divided by the support of the category, and it is given by: We define the weighted confidence of the rule X  X   X   X  c as which varies from 0 to 1. While confidence uses absolute supports (i.e.,  X  ), weighted confidence uses relative sup-ports (i.e.,  X  ). The higher the weighted confidence, the more strongly X is associated only to category c .
 Two-way Confidence : The two-way confidence of the rule X  X   X   X  c ,isgivenas and varies from 0 to 1. Two-way confidence is symmetric under antecedent/consequent permutation, and thus higher values of  X  truly indicates perfect implications ( X X  c and c  X  X  ). Notice that rules with high  X  values may have low  X  values if  X  ( c )  X  ( X ).
 Jaccard Coefficient [19]: The Jaccard coefficient of the rule X  X   X   X  c is given by and varies from 0 to 1. It measures the degree of strength in which X implies, and only implies, c .

In terms of rule generation, we generalize the strategy we used for confidence as follows. First, we have to decide which criteria we are going to use. Second, for each chosen criterion, we define the proper minimum threshold (  X  min  X  ,  X  min for  X  ,and  X  min for  X  ). Then we generate the rules, which are considered strong if their chosen measures are all above the respective threshold.
Once we generate all strong rules for the given criteria, it is necessary to determine the class that is best associated with those rules. The issue here is that the criteria may generate different rankings that must be combined. Each ranking is, in fact, viewed as an expert (i.e., R  X  , R  X  , R  X  ,or R knows the k best rules according to its corresponding mea-sure (i.e.,  X  ,  X  ,  X  ,or  X  ). Given a set of measures V ,westart by calculating the weight for each criterion, as defined in Eq.(2); replacing R  X  with R  X  etc. Our strategy to combine different experts is based on an elementary principle: the error risk of a consensual decision of multiple experts tends to be lower than the risk of individual experts [6]. Thus, we redefine the function score as:
For instance, consider again the example in Table 1, and suppose we want to predict the category of document d 13 . Also, suppose that rules are evaluated using measures V =  X ,  X  } ,andthat  X  min =0 . 20,  X  min =0 . 30,  X  min =0 . 25, and  X  =0 . 25. In this case, the following rule set is induced:
Three rankings are built, R  X  = { 1,2,3 } , R  X  = { 1,3,2 R  X  = { 1,3,2 } . Rule 1 has weight 1 in all experts. Rule 2 has weight 2 in R  X  and weight 3 in R  X  and R  X  . Rule 3 has weight 3 in R  X  ,andweight2in R  X  and R  X  .

If only  X  were used, then category  X  X nf. Retrieval X  would have score 3, and would be the (wrongly) predicted category. On the other hand, if  X  and  X  are also used, then category  X  X atabases X  would have score 8 (according to Eq.(8)), and therefore it would be the (correctly) predicted category.
There may be cases in which no strong rules are found. A simple way to deal with these cases is to pick the majority class in the training set. The problem with this method is that documents coming from frequent classes are usually the ones that provide better rules for classification and, there-fore, choosing the majority class as the prediction when no strong rules are found will possibly lead to misclassification. The approach we adopt is to choose the most frequent class within the projected training set. In this way, the prediction is made based on the attributes within the test document, instead of a fixed default class.
In this section we describe the experimental results for the evaluation of the proposed approach in terms of both classification effectiveness and computational efficiency. Our evaluation is based on a comparison against current state-of-the-art classification approaches, whether single or multi-evidence based. We first present the collections employed, and then we discuss the effectiveness and the computational efficiency of our approach in these collections.

Three collections were used in our experiments. The first collection, Cade12, consists of a set of classified Web pages indexed by the Cad X  edirectory( http://www.cade.com.br/ ). Cad X  e is a Brazilian Web directory pointing to Web pages that were classified by human experts. The pages pointed by entries of Cad X  e are also indexed by the TodoBr search engine [16]( http://www.todobr.com.br/ ). The content of each document of Cade12 is made of the text contained in the body and title of the Web page (excluding HTML tags and stop words). The collection is a set of 44,099 pages labelled using the 12 first level categories of Cad X  e (Comput-ers, Culture, Education, Health, News, Internet, Recreation, Science, Services, Shopping, Society, and Sports). Cad X  ehas a vocabulary of 191,962 unique words. Information about the links related to the Cad X  e pages was also collected from the TodoBR collection. We refer to pages in TodoBr col-lection not classified in Cad X  eas external pages. Cad X  ehas a total of 3,830 internal links, 554,592 links pointed to by external pages, and 5,584 links pointing to external pages.
The second collection, which is called ACM8, was ex-tracted from the first level of the ACM Computing Classifi-cation System ( http://portal.acm.org/dl.cfm/ ). ACM8 is a set of 6,682 documents (metadata records) labelled us-ing the 8 first level categories of ACM (General Litera-ture, Hardware, Computer Systems Organization, Software, Data, Theory of Computation, Mathematics of Computing, Information Systems, Computing Methodologies, Computer Applications, Computing Milieux) 2 . Only 55.80% of these documents have abstracts, which makes it very hard to clas-sify them using traditional content-based classifiers. For the remaining documents, the only available textual content is title. But titles contain normally only 5 to 10 words. As a result, ACM8 has a vocabulary of just 9,840 unique words. ACM8 has a total of 11,510 internal citations and 40,387 citations for papers outside of ACM DL.

The third collection, WebKB, consists of 8,282 Web pages collected from computer science departments of various uni-versities in January 1997 by the World Wide Knowledge Base project ( http://www.cs.cmu.edu/  X  webkb/ ). All pages were manually classified into seven categories (student, fac-ulty, staff, department, course, project, and other). MIME headers, HTML tags, and tokens that only occur once were discarded. Further, for each train/test split, we performed feature selection by removing all but the 2,000 words with highest mutual information with the class variable. WebKB has a total of 10,919 links between pages of the universities.
Figure 1 shows the category distribution for the three col-lections. As we can see, the three collections have very skewed distributions. In Cade12, the three most popular categories represent more than 50% of all documents. The ACM8 collection has similar features, with the two most popular categories counting for more than a half of all doc-uments in the collection. The more skewed collection, how-ever, is WebKB. In this collection, the most popular class contains half of the pages. Note that, for the three collec-tions, each document is classified into just one category.
In all experiments with the aforementioned collections, we used 10-fold cross-validation and the final results of each experiment represent the average of the ten runs. We quan-tify the classification effectiveness of the various approaches through the conventional precision, recall and F 1 measures. Precision p is defined as the proportion of correctly classified documents in the set of all documents. Recall r is defined as the proportion of correctly cla ssified documents out of all the documents having the target category. F 1 is a combination of precision and recall defined as the harmonic mean 2 pr Macro-and micro-averaging [22] were applied to F 1 to get single performance values over all classification tasks. For individual categories and then averaged over all categories. For F 1 micro-averaging ( MicF 1 ), the decisions for all cate-
The three remaining categories of the ACM taxonomy, namely General Literature, Data and Computer Applica-tions, had too few documents which prevented us to use them in our experiments. gories were counted in a joint pool. The computational ef-ficiency is evaluated through the total execution time, that is, the processing time spent in training and classifying all documents. We set  X  min =0 . 001,  X  min =0 . 975,  X  min =0 . 800,  X  min =0 . 750, and  X  min =0 . 700. The experiments were per-formed on a Linux-based PC with a Intel Pentium III 1.0 GHz processor and 1.0 GBytes RAM. All the results to be presented were found statistically significant at the 99% con-fidence level when tested with the two-tailed paired t-test.
We start our analysis by evaluating the effectiveness of our method for different combinations of evidence (text, inlinks, and outlinks) and criteria (  X  ,  X  ,  X  ,and  X  ). Table 2 shows of the WebKB collection, textual information (t) performs poorly when compared against in-links (i) and out-links (o). In the Cade12 collection, text-based classifiers presented a very weak performance when compared with the results from ACM8 and WebKB collections. This is due to the fact that Web pages are usually noisy and contain little text. Further, due to issues such as multiple authorship, Web pages lack coherence in style, language, and structure. These prob-lems are less common in the papers of a Digital Library. As a result, the quality of the textual evidence in ACM8 is better than in Cade12, but still not as good as the cita-tion based information. In the case of WebKB, the good performance of textual classifiers is due to the skewness of the collection. A simple strategy of assigning the majority category to the test document is enough to ensure accuracy figures greater than 70%. In the three collections, textual evidence is a valuable source of information when combined with other types of evidence (i+t, o+t). Combining all avail-able evidence (i+o+t) showed to be the best strategy for all collections. The combination of multiple evidence may lead to deeper understanding of the document than when only taking a single evidence into account. However, not all ev-idence are relevant and of good quality. Taking irrelevant or poor quality evidence into account may deteriorate the accuracy. For instance, in some cases the combination of different linkage evidence (i+o) did not show good accuracy results for the WebKB collection.

As expected, different results were obtained from each cri-terion. In general, two-way confidence (  X  ) gives the best re-sults, while weighted confidence (  X  ) generally gives the worst results. The main reason for this is that only  X  is the one that uses the complement factor ( c ). However, this factor helped in classifying documents provenient from very low frequent classes in the WebKB collection. Combining dif-ferent criteria also shows great improvements. For Cade12 Table 4: Comparison against Different Approaches. and ACM8 collections, the best configuration uses criteria  X  ,  X  ,and  X  , while for WebKB collection, the best configu-ration uses  X  ,  X  and  X  . Hence, for now on we will use the configuration (i+o+t  X   X  +  X  +  X  )(forCade12andACM8col-lections), and (i+o+t  X   X  +  X  +  X  ) (for WebKB collection), for the remaining experiments in this section.

We continue our analysis by evaluating the link forward-numbers for different degrees of relationships between the documents. For instance, degree 0 means that only textual information within the given document was used. Degree 1 means that we are also allowed to use in-links and out-links of the given document. Degree 2 means that we are also allowed to use in-links/out-links of documents that point to (or that are pointed by) the given document. Differ-ent results were obtained for the three collections. For the Cade12 and WebKB collections, the bests results (80.41%, and 83.52%) were achieved using only one degree of rela-tionship, while for the ACM8 the best result (90.55%) was achieved when the second degree of relationship was also explored. This was due to the inherent differences between links and citations. Citations are used to provide back-ground information, give credit to other authors, report or criticize similar ideas, among others. Besides all the func-tionality of citations, links have extra roles such as adver-tising, providing access to databases, navigation etc. Such roles can make them a less reliable source of evidence leading to noise in the classification process
We used the best results obtained by our method and compare them with the best results obtained by carefully hand-tuned state-of-art single and multi-evidence methods. SVM[18] is the best known text-based classifier and there-fore a standard baseline. Co-citation[17] and Amsler[1] are two bibliographic similarity measures that, when applied within a k -Nearest-Neighbor (kNN)[21] algorithm, produce classifiers whose performance is far superior than any text-based classifier in these collections, the former being the best measure for Cade12 and the latter the best one for ACM8. Bayesian[4] and Multi-Kernel [11] are two state-of-the-art representatives of the multiple experts and multiple evidence approaches. Given the huge size of the combined kernel matrix (about 500 million points for a symmetric sparse representation) we were not able to compute the multi kernel method for Cade12. Finally, since our method deals with text and link evidence in the same way, we also rep-resent the documents to be classified as bags of features in which the features can be words and links taken indistinctly. Our last two multi-evidence approaches, SVM(multi) and kNN(multi), consist in classifying these documents using kNN and SVM classifiers, respectively.

Table 4 depicts MicF 1 and MacF 1 numbers obtained for each method. Co-citation, Amsler, and SVM(text), were used as our baselines because they give good results when considered in isolation. The first entry for each collection is used as the baseline, and subsequent entries are sorted ac-cording to MicF 1 . Text-based (SVM) and link-based classi-fiers present very distinct classification performance. Multi-evidence methods (Bayesian, multi-evidence kNN, multi-evidence SVM, Multi-Kernel and Associative) show the best classification performance, our method being clearly the best performer in all collections. The gains over the baseline are up to 17% depending to the collection. On the other hand, its eager counterpart performed poorly in all collections, and this is mainly due to the missing rule problem (many irrel-evant rules, and only few relevant, were generated). Table 5: Comparison against Bayesian Combination.

Table 5 shows detailed comparisons between our approach and the Bayesian combination approach (which was the most competitive performer). The table presents recall and preci-sion figures for each category, considering all documents in the three collections. For Cade12, we observe a gain of 27% in precision in the most frequent category, which is respon-sible for the overall gain in classification effectiveness. This is mainly due to the fact that the Bayesian combination ap-proach uses the most frequent category in the training set as the fixed default prediction, which results in high recall and low precision numbers for more frequent categories. On the other hand, our approach uses the most frequent category in the projected training set. As a consequence, several doc-uments that were misclassified by the Bayesian combination approach were correctly classified by our approach. Simi-larly to Cade12, the Bayesian combination approach prefers to assign documents to most frequent categories in WebKB. However, due to the skewness observed in this collection, this simple strategy leads to high gains. For ACM8 we ob-serve more impressive gains in both precison and recall. The less frequent the category, the more is the gain in recall, and consequently, the opposite trend is observed in precision. In this case, the gains are mainly due to the link forward-ing/backwarding technique, as previously observed in Ta-ble 3. Thus, instead of applying a fixed default prediction, our method performs link enhancement, which is succesful in this case because of the citation regularity observed in the ACM8 collection.
The computational performance of our method was also evaluated. Table 6 depicts the execution times obtained by employing different cache sizes. We allowed the cache to store from 0 to 100,000 rules (approximately 82 MBytes), and for each storage capacity we obtained the corresponding execution time. As expected, execution time is sensitive to cache size, showing improvements of about 200% for larger cache sizes. Similar trends were observed in all collections. Further, higher execution times were observed when textual evidence is used. This is explained by the fact that there is much more textual evidence than link-based evidence, and thus the number of rules based on textual information is much higher than the number of rules based on linkage in-formation. Classification is extremely fast if only linkage ev-idence is used. For instance, our method is able to perform all ten folds of the ACM8 collection within two minutes. Table 7 shows the comparison between different methods. Lazy approaches learn quickly but classify slowly, while ea-ger appoaches learn slowly but classify quickly. However, the use of caching is extremely useful for speeding up lazy classification. Only the Co-citation method was faster than ours in the Cade12 collection. Its effectiveness, however, was much worse than ours. Our method was the best performer in the ACM8 and WebKB collections. Its eager counter-part, on the other hand, spent much time generating a large number of irrelevant rules (i.e., rules that were not used to classify any document in the test set), hurting the compu-tational performance.

Finally, we analyzed the sensitivity of our method by vary-ing the ranking size, k (i.e., the number of rules within each ranking). The analysis was carried in terms of execution time and accuracy, which are depicted in Table 8. As ex-pected, the execution time increases for larger ranking sizes, since more rules have to be generated in order to complete the ranking. Further, we notice that large increases and decreases of the ranking lead to low accuracy. This is due to the fact that larger rankings require more rules to be in-duced, and the direct consequence of applying our level-wise rule induction is that longer (and more specific) rules will be induced. We can see this by analyzing the average rule size column in Table 8, which clearly increases with the ranking size. By using a large number of specific rules, the classifier tends to overfit the data hurting the accuracy. On the other hand, by using only a low number of general rules, the clas-sifier will underfit the data also hurting the accuracy. Thus, the choice of the proper ranking size is a trade-off between underfitting and overfitting.
In this paper we propose and evaluate a novel document classification method which introduces innovations in all main steps of the automatic classification task. First, we propose a lazy method, which delays the model induction process until a new document is given for classification, in-curring not only in classification accuracy gains, but also in performance improvement. Our lazy approach also performs evidence enhancement by looking forward/backward new pieces of link-based evidence in the citation/link graph. Sec-ond, we present a technique in which all evidence is treated within a unique search/classification space in a natural and transparent way. The pieces of evidence that enter in our classification model, independently of its type, are the ones that are more discriminative of the categories in which they occur according to our quality criteria. Finally, multiple quality criteria are combined in order to choose the best rules to be used to predict the correct category. Experimen-tal results demonstrated that these innovations combined
Table 8: Ranking and Classification Performance. together produce more effective and faster classifiers than state-of-the-art approaches in the collections used, achiev-ing more than 90% accuracy in some cases.

As future work, we intend to design and evaluate novel rank criteria and combination strategies for these rankings. We want also to consider evidence weighting strategies, dur-ing the whole process. Finally, we will explore application scenarios such as bioinformatics and spam detection.
This research was sponsored by UOL (www.uol.com.br) through its UOL Bolsa Pesquisa program (process number 20060519184000a). Additional support was provided by the 5S-QV project (grant MCT/CNPq/CT-INFO 55.1013/2005-2), Fucapi Technology Foundation, NSF Career Award IIS-0092978, DOE Early Career Award DE-FG02-02ER25538 and NSF grants EIA-0103708, and EMT-0432098. [1] R. Amsler. Application of citation-based automatic [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone. [3] S. Brin, R. Motwani, and C. Silverstein. Beyond [4] P. Calado, M. Cristo, E. Moura, N. Ziviani, [5] D. Cohn and T. Hofmann. The missing link -A [6] S. Dasgupta, M. Littman, and D. McAllester. PAC [7] M. Fisher and R. Everson. When are links useful? [8] J. Friedman, R. Kohavi, and Y. Yun. Lazy decision [9] J. Furnkranz. Exploiting structural information for [10] D. Gibson, J. M. Kleinberg, and P. Raghavan. [11] T. Joachims, N. Cristianini, and J. Shawe-Taylor. [12] W.Li,J.Han,andJ.Pei.CMAR:Efficient [13] B. Liu, W. Hsu, and Y. Ma. Integrating classification [14] J. Quinlan. C4.5: Programs for machine learning. [15] F. Sebastiani. Machine learning in automated text [16] A. Silva, E. Veloso, P. Golgher, B. Ribeiro-Neto, [17] H. Small. Co-citation in the scientific literature: A [18] A. Sun, E.-P. Lim, and W.-K. Ng. Web classification [19] P. Tan, V. Kumar, and J. Srivastava. Selecting the [20] L. Terveen, W. Hill, and B. Amento. Constructing, [21] Y. Yang. Expert network: Effective and efficient [22] Y. Yang, S. Slattery, and R. Ghani. A study of [23] X. Yin and J. Han. CPAR: Classification based on [24] M. Zaki and C. Aggarwal. XRules: An effective [25] B. Zhang, Y. Chen, W. Fan, E. Fox, M. Gon  X  calves,
