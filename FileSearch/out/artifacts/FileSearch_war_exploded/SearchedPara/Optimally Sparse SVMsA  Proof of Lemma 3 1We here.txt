 We here prove a lower bound on the number of support vectors to achieve generalization bounds of the form of a more general form than those by our sparsification procedure (Section 4).
 distribution D and a reference vector u such that k u k = R , L hinge ( g u ) = L  X  , and any w which satisfies: fication rules that predict 1 with probability  X  ( g u ( x )) for some  X  : R  X  [0 , 1] .
 Proof. We define D such that i is sampled uniformly at random from the set { 1 ,...,d } , with d = R 2 , and the feature vector is taken to be x = e i (the i th standard unit basis vector) with corresponding label distributed that k u k = R and L hinge ( g u ) = L  X  .
 that: which concludes the proof.
 We rely on the following compression bound (Theorem 2 of Shalev-Shwartz (2010)).
 Theorem B.1. Let k and n be fixed, with n  X  2 k , and let A : R d  X { X  1 } k  X  X  be a mapping which receives empirical loss on the training set, respectively. Then, with probability 1  X   X  , for all S : Proof. Consider, for some fixed  X  0 , the probability that there exists a S  X  X  1 ,...,n } of size k such that: stein X  X  inequality that, for a particular S , the above holds with probability at most  X  0 . By the union bound: Let  X  = n k  X  0 . Notice that ( n  X  k )  X  L test ( g w S )  X  n  X  L ( g w S ), so: r Using the assumption that n  X  2 k completes the proof.
 In this section, we will prove a bound comparable to that of Theorem 4.4, but using a proof technique based on a smooth loss, rather than a compression bound. In order to accomplish this, we must first modify the objective of Problem 4.1 by adding a norm-constraint: every iteration. Despite this change, an -suboptimal solution can still be found in k w k 2 / 2 iterations. The concentration-based version of our main theorem follows: Theorem C.1. Let R  X  R + be fixed. With probability 1  X   X  over the training sample, uniformly over all pairs w,  X  w  X  X  such that k w k X  R and  X  w has objective function f (  X  w )  X  1 / 3 in Problem C.1: Proof. Because our bound is based on a smooth loss, we begin by defining the bounded 4-smooth loss ` smooth ( z ) in Figure C X  X otice that it upper-bounds the slant-loss, and lower-bounds the hinge loss even when shifted by . Applying Theorem 1 of Srebro et al. (2010) to this smooth loss yields that, with probability 1  X   X  , uniformly over all  X  w such that k  X  w k X  R : proof of Lemma 4.1, this follows directly from Problem C.1, and the definition of the smooth loss. Combining the proof.
 It X  X  worth pointing out that the addition of a norm-constraint to the objective function (Problem C.1) is only are found using subgradient descent with the suggested step size and iteration count, then applying the triangle different constant hidden inside the big-Oh notation).
 In Section 4.6, we discussed a simple extension of our algorithm to a SVM problem with an unregularized bias this section, we discuss an alternative, in which we optimize over  X  b during our subgradient descent procedure. The relevant optimization problem (analogous to Problem 4.1) is: In other words,  X  b will be chosen such that the maximal violation among the set of positive examples will equal positive and one negative example, and then take a step on both elements. The resulting subgradient descent algorithm is: 2. Take the subgradient step  X  w ( t )  X   X  w ( t  X  1) +  X  ( X ( x i + )  X   X ( x i  X  )).
 Once optimization has completed,  X  b may be computed from Equation D.2. As before, this algorithm will find a -approximation in 4 k w k 2 iterations.
 (2012b), which is the long version of Cotter et al. (2012a). This result, which follows almost immediately from Theorem 1 of Srebro et al. (2010), establishes the sample complexity bound claimed in Section 2. Lemma E.1. (See Lemma D.1 of Cotter et al. (2012b)) Let u be an arbitrary linear classifier, and suppose that Let  X  w  X  = argmin To prove this, we will first prove two helper lemmas: Lemma E.2 is a direct application of Theorem 1 of Srebro Theorem 5 of Srebro et al. (2010)); Lemma E.3 analyzes the empirical error of a single hypothesis by a direct application of Bernstein X  X  inequality. Combining these two lemmas (Section E.2) then gives the claimed result. E.1. Helper Lemmas parameters L,B, &gt; 0 and  X   X  (0 , 1) : w satisfying: we have that L 0 / 1 ( g w )  X  L + .
 Proof. For a smooth loss function, Theorem 1 of Srebro et al. (2010) bounds the expected loss in terms of the empirical loss, plus a factor depending on (among other things) the sample size. Neither the 0/1 nor the hinge losses are smooth, so we will define a bounded and smooth loss function which upper bounds the 0/1 loss and lower-bounds the hinge loss. The particular function which we use doesn X  X  matter, since its smoothness parameter and upper bound will ultimately be absorbed into the big-Oh notation X  X ll that is needed is the existence of such a function. One such is: uniformly over all w such that k w k X  B : Because  X  is lower-bounded by the 0/1 loss and upper-bounded by the hinge loss, we may replace L  X  ( g w ) with L 0 / 1 ( g w ) on the LHS of the above bound, and and solving for n then gives the desired result.
 Lemma E.3. Let u be an arbitrary linear classifier, and suppose that we sample a training set of size n , with n given by the following equation, for parameters &gt; 0 and  X   X  (0 , 1) : L follows that Var x,y ( ` ( y  X  u,x  X  ))  X k u kL hinge ( g u ). Hence, by Bernstein X  X  inequality: Setting the LHS to  X  and solving for n gives the desired result.
 E.2. Proof of Lemma E.1 L hinge ( g u ) + and B = k u k , and observe that  X  w  X  satisfies Equation E.3 because L provided that Equation E.2 is also satisfied. Equation E.1 is what results from combining these two bounds and
