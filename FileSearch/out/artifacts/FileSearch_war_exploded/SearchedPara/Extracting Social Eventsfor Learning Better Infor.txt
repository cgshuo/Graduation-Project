 Learning of the information diffusion model is a fundamental problem in the study of information diffusion in social net-works. Existing approaches learn the diffusion models from events in social networks. However, events in social networks may have different underlying reasons. Some of them may be caused by the social in uence inside the network, while others may re ect external trends in the \real world". Most existing work on the learning of diffusion models does not distinguish the events caused by the social in uence from those caused by external trends.

In this paper, we extract social events from data streams in social networks, and then use the extracted social events to improve the learning of information diffusion models. We propose a LADP (Latent Action Diffusion Path) model to incorporate the information diffusion model with the model of external trends, and then design an EM-based algorithm to infer the diffusion probabilities, the external trends and the sources of events efficiently.
 H.2.8 [ Database Management ]: Database Application| Data Mining Algorithms, Experimentation social event; information diffusion; social in uence
Recently, online social networks have become a major medium for the spread of information. News, rumors, and opinions propagate in social networks. These events are usu-ally explained by information diffusion processes driven by social in uences between users of a social network. Yet each social network is not a closed world. Users obtain informa-tion not only from the social network itself, but also from other sources, such as mass media, lectures in universities, friends in real life, etc.

Most existing work on the information diffusion processes assumes that the model has been learned somehow, and fo-cuses on exploring the properties of the learned models. A less studied, but very important topic is the learning of infor-mation diffusion models. Work on this topic usually learns the information diffusion model from events in the social network. However, it is questionable to use all the events without any distinction, since the information diffusion pro-cesses are not the only reason triggering events in social net-works [2, 18, 1, 15]. Some of the events are results of social in uence or information diffusion processes inside the net-work, while others may re ect external trends in the world outside the network. For example, #DidYouKnow was a trending hashtag in the Twitter network in the last three weeks of 2011. The hashtag was used in tweets where peo-ple talked about surprising facts. It became popular because of social in uence among users in the Twitter network, while #JapanEarthquake , another trending hashtag in the Twitter network at the same time, re ects a major event in the out-side world. Most previous approaches [9, 17, 16, 8] on the learning of information diffusion models do not distinguish the two different types of events, which makes the learned models inaccurate.

In this paper, we study a problem of learning information diffusion models. We propose a new approach that can dis-tinguish the two different sources of events, and then use the identi ed social events to improve the learning of informa-tion diffusion models. Although the basic idea is straightfor-ward, it is not easy to design a solution based on this idea. There are three key challenges:
While the sources of some events are easy to be classi ed as external trends or the social in uence, for most events the sources are not easy to determine. For example, when the earthquake hit Japan, a great many of Twitter users prayed for people in Japan. Some of the users did that after they saw the sad news of earthquake on TV, while others did that because they saw other users in the Twitter network do that. In this case, we cannot simply classify this event to be an externally sourced event or a socially sourced event, but need to decide the source with ner granularity. As we will show in Section 2, we de ne the in uence source on the action level.

In order to distinguish the socially sourced actions from externa lly sourced actions, we need the information diffusion model as well as the model of external trends. But both of them are unknown. The model of external trends can only be inferred from the externally sourced actions, while the diffusion model can only be learned from the socially sourced actions. In other words, only when we are able to decide the sources of actions, we can learn the external trend model and the information diffusion model accurately. This leads to an inherent \chicken and egg" problem. We refer to it as \ inference dependency ".

We need to consider both external trends and informa-tion diffusion processes at the same time. It is not trivial, since the external trends are time-related, while the diffu-sion model depends on the structure of the social network. It requires us to integrate a temporal model and a structural model into one joint model. Besides, both of them contain plenty of parameters. It may lead to high complexity in the inference of the joint model.
 In this paper, we propose a novel LADP (Latent Action Diffusion Path) model to extract social events and learn dif-fusion models with better accuracy. Rather than classify events into external events and social events, we determine for each action in an event whether it is caused by external trends or the social in uence inside the network. We use a mixture model framework to combine the external trend model and the information diffusion model together, and decide the class of each action. As the learning algorithm of the model involves the inference of external trends, dif-fusion probabilities, and the sources of actions at the same time, a naive implementation can lead to prohibitively high computational cost. An inference algorithm based on the expectation maximization (EM) is devised to overcome the difficulty of inference dependency, while avoiding the high computational overhead on repeated invocation of the diffu-sion model.

The improved accuracy can result in better performance on many applications based on information diffusion models, such as in uence maximization [10] and outbreak detection [14]. As we will show in the experiment, for the DBLP network, the top authors suggested by the LADP have an average H-index and number of citations up to 20% higher than the top authors suggested by the state-of-the-art ap-proaches.

Organization. The rest of this paper is organized as fol-lows: Section 2 formally de nes the problem. We proposed the LADP model in Section 3, and present the learning al-gorithm in Section 4. In Section 5, we present experiments. We discuss related work in Section 6, and conclude in Sec-tion 7.
In this section, we formally de ne the task of information diffusion model learning. We begin with a few key concepts as follows. The notations are summarized in Table 1.
Definition 1. Social Network A social network is a graph G = ( V ; E ) , where a vertex v 2V corresponds to a user, and an edge e = ( v i ; v j ) 2E stands for a connection between the users v i and v j . Edges in a social network can either be directed or undirected.

The social network itself provides nothing more than struc-tural information. To learn the diffusion model, we also need the contents created by users in the network, for example, the tweets created by users of Twitter network, or the pub-lications of authors in the DBLP network. We de ne the collection of contents as \data stream".

Definition 2. Data Stream A data stream S on a so-cial network G is de ned as a chronological sequence of doc-ument sets C t , i.e. S = fC t g T t =1 . A textual document d contains a set of terms. Each document is associated with a node in V , denoted by v d , and has a time stamp, denoted by t d . The t-th document set C t 2 S contains the documents created at time step t , i.e. C t = f d ; t d = t g .
If a document d is contained in one of the sets C t , we say that the document d is contained in the data stream S With a little abuse of notation, we denote it by d 2S . We denote with L the set of terms in the data stream S . Terms in the documents can be de ned in various ways. For example, we can de ne each word in a document as a term. We can also de ne each hashtag in a tweet as a term. More generally, we can de ne any tags or labels as terms, so that the streams are not limited to sequences of textual documents. In this paper, we focus on the analysis of textual streams. Nevertheless, the proposed LADP model can be applied to more general types of streams.

In the LADP model, we regard the generation of a doc-ument as a process that, for each term l 2L , the author makes a decision whether to include it in the document or not. We call this decision an \action".

Definition 3. Action For each document d i 2S , for each term l 2L , there is an action ( i; l ) taken by the author of the document. If the document d i contains the term l , the action is a positive action , denoted by x i;l = 1 . Otherwise, the action is a negative action , denoted by x i;l = 0 .
For the positive action, we also introduce the concepts of socially sourced action , and externally sourced ac-tion . A socially sourced action is a positive action that taken by a user because she is in uenced by an information diffusion process inside the social network, while an exter-nally sourced action is a positive action that is triggered by an external trend. It is true that a positive action may sometimes be triggered by both the social in uence and the external trend at the same time. But in most cases, the ma-jor source of an action can be identi ed, since at one point of time the user usually gets information from only one source. In this paper, for simplicity of the model, we assume that a positive action can either be a socially sourced action or an externally sourced action.

Definition 4. Event An event in a social network is a sequence of positive actions of the same term l 2L . Each event may include a socially sourced portion (or a so-cial event ), and an externally sourced portion . The so-cially sourced portion contains socially sourced actions, while the externally sourced portion contains externally sourced ac-tions.

We de ne the sources on the action level, rather than on the event level, on the user level, or on the document level. Although the event, the document and the user of an action are all important factors of it, each factor alone cannot perfectly capture the reasons for triggering the ac-tion. As we have discussed in Section 1, we cannot simply classify an event as a socially sourced event or an exter-nally sourced event, since actions in the event may have different sources. We cannot de ne the classes on user level either, because each user usually obtains information from both inside and outside the network, and actions taken by a user can be triggered by the social in uence or external trends. Even the classi cation on document level is not good enough, since each document may contain several different terms or topics. For example, a tweet in 2011 said \#DidY-ouKnow that #JapanEarthquake affected the underground water in Florida?" It involves both the social event of using the hashtag\#DidYouKnow"in Twitter community and the external trend of \Japan earthquake".

By de ning the classes on the action level, our approach has greatest exibility and can infer the underlying reasons precisely. By classifying the actions as socially sourced or externally sourced actions, the inference algorithm can split socially sourced portion and the externally sourced portion of an event. In another sense, it can extract the socially sourced portion from the event. We refer to the extracted externally sourced portion as social event and the extract-ing procedure as social event extraction .

Definition 5. Information Diffusion Process The in-formation diffusion process is the process that actions of terms propagate along the edges of the social network. The process is the result of in uence among users in a social network.

A diffusion model aims to predict diffusion processes. Typ-ically, given the actions at time step t , the diffusion model predicts the probability for each user in the social network of taking a positive or a negative action in the next time step t + 1. The IC (Independent Cascade) model [10] is a widely used information diffusion model. In the IC model, when a user becomes active, she has an independent chance to make each of her neighbors become active. In the proposed LADP model, we de ne a mechanism of positive action propagation that extends the IC model to the action level.

Task. Based on the de nitions of the above concepts, we can formalize the task of information diffusion model learn-ing: Given a social network G and data stream S on it, we aim to learn the diffusion model on the network G . In vari-eties of information diffusion models, including the IC model and our model, the parameters of a diffusion model are the diffusion probabilities along edges, so we focus on the learn-ing of diffusion probabilities in this paper. Different from existing approaches, we extract social events from the data stream, and learn the diffusion model from the extracted social events.
The LADP model extracts social events from data stream, and learns the information diffusion model from the ex-tracted social events. The block diagram of the LADP model is shown in Figure 1(a).

To extract social events, the LADP model infers the in-uence source for each positive action. The distribution of socially sourced actions is decided by the information diffu-sion model, and the diffusion probabilities are the param-eters that need to be estimated. The distribution of the externally sourced actions is decided by the external trend model, and the trend pro les are the parameters of the in-formation diffusion model that need to be estimated. A mixture framework is proposed to integrate the information diffusion model and the external trend model.

The inference of action sources depends on trend pro les and diffusion probabilities, while the inference of trend pro-les and diffusion probabilities depends on the sources of ac-tions. We design an EM-based inference algorithm to solve this \inference dependency" problem. The algorithm esti-mates the parameters iteratively. In each iteration, it rst infers the sources of actions based on the current estimates of trend pro les and the information diffusion probabilities, and then infers the trend pro les and the diffusion probabil-ities based on the probability of each action being socially caused or externally caused.

Now we formally de ne the LADP model. As shown in graphical representation of the LADP model in Figure 1(b), the observation variables x i;l are central to the model. Each x i;l indicates whether the corresponding action ( i; l ) is pos-itive or negative. It is drawn from a mixture distribution, which integrates the external trend and the information dif-fusion process. The label z i;l decides from which component distribution the variable x i;l is drawn. The left part of the graphical representation ( l and l;t ) shows the model of external trends, while the right part ( p u;v ) represents the information diffusion model. The two of them form the two components of the distribution of x i;l .

Notice that, the external trend model is a temporal model, while the information diffusion model based on the structure of the network. Both of them are complicated models with a lot of parameters. It is not easy to combine these two models together without making the inference algorithm in-tractable. We therefore make an assumption that each ac-tion can only be generated from either the external trend model or the information diffusion model. This assumption makes it possible to integrate the two models together under a mixture model framework.

Formally, x i;l is de ned as:
The hidden variables Z indicates whether the action is drawn from the external trend model or from the informa-tion diffusion model. z
Notice that z i;l is de ned for all actions, whether they are positive or not, though the concepts of socially sourced action and externally sourced action are de ned for the pos-itive actions only. When z i;l = 1 and x i;l = 1, the action is a socially sourced action. When z i;l = 0 and x i;l = 1, the actions is an externally source action.

For each i , z i;l is a random variable drawn from Bernoulli distribution with mean l , i.e. z i;l Bernoulli ( l ). The mean l is different for each term, since different term has different potential probability of being related to an infor-mation diffusion process or an external trend. We use a Beta prior for the parameter l , i.e. l Beta ( ), where = ( 1 ; 0 ) are xed parameters. We choose Beta distri-bution because (1) it has a great exibility of the shape, and (2) it is the conjugate prior distribution for Bernoulli distribution.

Given the corresponding hidden variable, the distribution of x i;l is given as: discuss the two models in the following sections.
We use a diffusion model that can be regarded as an ex-tension to the widely used Independent Cascade (IC) model [10]. In the IC model, information propagates along the edges in the network. When a node becomes active, it at-tempts to activate its neighbors. For each node, the at-tempts to activate it from all its active neighbors are inde-pendent. Similarly, in the LADP model, q l;t;v , the probabil-ity that a user v uses a term l is predicted from the actions that v 's neighbors took in the last time step. For each pos-itive action about term l taken by in-neighbors of v at last time step t 1, there is an independent chance to make the action of v at time t to be positive. Formally, for the term l , the probability of an action taken by user v at time t being positive is given as: where C t 1 is the set of documents that were created at the last time step t 1. p v d with the edge ( v d i ; v ). For the convenience of notation, we de ne p v d ;v = 0, if there is no edge between the nodes v and v . The product in the formula is the probability that all of the in-neighbors of v fail to make the action of v at time t to be positive. Under the independent assumption, this probability could be calculated by multiplying together the probabilities that each attempt fails. We then can get q l;t;v , the probability that at least one attempt succeeds, by subtracting the product from 1.
For the actions that are generated by external trend model, the probabilities of being positive are not decided by the so-cial network structure and previous actions in the network. A reasonable assumption with these actions is that the prob-ability of being positive only depends on the time and the term, but does not depend on the user takes the action. (A similar assumption was made in [15].) The reason under-lying the assumption is that, if an action is decided by an external trend outside the network, we cannot make any pre-diction on whether the action is positive or not, based on the network structure, so the best assumption we can make is that each action is a random variable independently drawn from the same distribution.

Nevertheless, the probability of an action being positive should be depends on the term l and the time t . That is because different terms have different levels of popularity in the external world. The more popular a term is, the more likely that an action with regard to it is positive. For a given term, its popularity changes over time. For each term l , the parameters l;t forms a time sequence which we call pro le of the external trend.

We therefore assume an Beta prior for l;t the probability of being positive: l;t Beta ( l ) = Beta ( l; 1 ; l; 0 ). Similar to the prior of l , we choose Beta distribution because its shape is exible and it is the conjugate prior for Bernoulli distribution. For each term l , we set the parameter l in the prior distribution according to the number of positive and negative actions over all the time steps. Speci cally, l; 1 set to the average number of positive actions for the term l in all the steps, while l; 0 is set to the average number of negative actions in all the steps.
We have discussed the difficulty of inference dependency in the introduction. We design an EM-based algorithm to solve this difficulty by iteratively estimating the conditional distribution of hidden variables z i;l and the parameters. In the E-step of each iteration, the estimate for the conditional probability of z i;l is updated, while the estimates for param-eters l , l;t and p u;v are updated in the M-step. The parameters l and l;t are easy to estimate using the EM algorithm of the maximum a posteriori (MAP) estimate. However, it is difficult to de ne and calculate the MAP es-timate of the parameters p u;v , due to the complexity of the diffusion model. To solve that difficulty, we modify the PCB model in [9] so that it can be integrated into the EM frame-work to provide estimate of p u;v efficiently. We rst assume that p u;v are known (consequently, q l;t;v are known), and show the EM algorithm. Then, we discuss the estimation of p u;v and add it into to the EM framework.
Given the LADP model above, we want to maximize ex-pectation of the logarithm of the posterior: of the equation.

The rst term in the formula comes from the distribution l Beta ( ). The second term comes from the distribution l;t Beta ( l ). The last term comes from the distribution of X and Z , given the parameters , and q .

E-step. In the E-step, we calculate the conditional prob-ability of hidden variables Z , given the observed variables X and estimate of parameter : function of x i;l , given it is drawn from the information dif-fusion component: and p ( j z ( n ) i;l = 0 ; ) is the probability mass function of x given it is drawn from the external trend component:
M-step. By taking partial derivatives of the expectation of log-likelihood, we get the new estimation of parameters. and
Estimate of Diffusion Probabilities. We now discuss the estimate of diffusion probabilities. It is possible to for-mulize it as an inference problem for maximum likelihood or MAP estimate. Saito at el. de ned a likelihood func-tion for the IC model, and proposed an EM algorithm for inference problem[17]. However, the number of parameters in the diffusion model is very large (one parameter for each edge in the network). The inferring of model is very time-consuming, even on a xed set of actions. In the LADP model, the set of socially sourced actions changes in each it-eration of the EM algorithm. Getting maximum likelihood estimation for the diffusion probabilities in each iteration of the EM algorithm will be an enormous computational challenge. Therefore, we follow the Partial Credit Bernoulli model (PCB) [9] to estimate the diffusion probabilities.
The original PCB model is not based on the maximum likelihood or MAP estimate, so it does not work together with the EM algorithm for inferring the LADP model. We change it so that it can be incorporated into the M-step of the EM algorithm. We will rst describe the PCB model, and then show that how we change it so that it can be incorporated into the EM algorithm.

The idea of the PCB model is as follows: if user v takes a positive action about a term l at time t after his in-neighbor u 's positive action at time t 1, we regard there is a suc-cessful diffusion from u to v . If there are more than one in-neighbors of v take positive action at the previous step, they share the credit for the one successful diffusion equally. The diffusion probability p u;v is then given by the ratio of the number of successful diffusion from u to v to the number of positive actions taken by u .

Formally, the diffusion probabilities can be estimated by: where A v = f ( i; l ) : x i;l = 1 ; v d i = v g is the set of all the positive actions taken by the node v and I ( ) is the indicator function. Can ( i; l ) is the candidate set of documents that share the credit for the successful diffusion. A document is in the candidate set if and only if it is posted by a friend of v d i and it is posted in the time step right before t Can ( i; l ) = f d j j t d j = t d i 1 ; x j;l = 1 ; u 2 IN ( v IN ( v ) is the set of in-neighbors of v .

According to Equation 2, all the positive actions are used for the learning of the diffusion model. Since in the LADP model we have the hidden variable z i;l representing whether an action is drawn from diffusion processes or not, we should train the model with the socially source actions only. We then can add the hidden variable z i;l to Equation 2, and replace it with the following equation:
We th en replace z i;l in the above function with P ( z ( n ) j jX ; ( n 1) ), and integrate the learning of diffusion proba-bilities into the EM algorithm.
We compared the proposed LADP model with three base-lines: two for the task of learning diffusion model, and the other for the analysis of extracted social events. the PCB algorithm: We compare the LADP model against the Static PC Bernoulli algorithm (PCB) [9] for the task of learning diffusion model. The PCB algorithm is sim-ilar to the inference method described in Section 4, but all positive actions are regarded as socially sourced actions and are used for the probability learning. It is equivalent to the LADP model with = 1.

Saito's Algorithm The Saito's algorithm [17] is another baseline that we used for evaluating the task of learning diffusion probabilities. It is an EM algorithm for maximum likelihood estimation of the IC model.
 Myers's Algorithm: For better understanding of the LADP model, we also analyze the social events extracted by the mo del. The Myers's algorithm [15] is used for compari-son in the analysis of extracted social events. This algorithm is not designed for the same purpose as the LADP model. But it can divide the in uence to the users into two parts: the internal part and the external part. The internal part can roughly be aligned with the socially sourced portion in our model, so we use it for a comparison in the analysis of extracted social events.
The algorithms are tested on four real world datasets and one semi-synthetic dataset. Each real world dataset con-sists of a social network and a data stream on the network. The semi-synthetic dataset is based on real world network, but we generate the data stream synthetically. The semi-synthetic dataset is used to evaluate the accuracy of the in-ference algorithm, while the real world datasets are used to test how well the LADP model works for real applications.
Twitter-UIC dataset: This dataset consists of 974,382 tweets on a network of 2,180 users and 14,572 links. The users in the dataset are the followers of the \UIC news" ac-count on twitter.com. Most of them are students in Uni-versity of Illinois at Chicago. Directed links in the network correspond to who-follows-whom relationships. They are di-rected from the one being followed to the follower. The stream data on the network are generated from the tweets posted by the users over the 52 weeks of the year 2011. Hashtags in the tweets are used as the terms. Each week is regarded as a time step.

Twitter semi-synthetic dataset: We rst randomly crawled a network from twitter.com that consists of 40,000 users and 544,936 links, and then generate the semi-synthetic dataset using steps as follows. The diffusion probabilities with edges are randomly picked from a Beta distribution with parameters (1 ; 30). We use the same collection of terms as those in the Twitter-UIC dataset, and the Google Trend pro les of the corresponding terms are used as the external trends pro le. The socially sourced actions are generated using the diffusion probabilities with the diffusion model de-scribed in 3.2, and the externally sourced action are gener-ated from external trends. The synthetic data contains 223 events, and 14,170,112 positive actions. 68.1% of the pos-itive actions are socially sourced actions, while others are externally sourced actions.
 DBLP datasets: We extract three datasets from the DBLP database. Each of them contains the stream of pub-lications in a certain area and the co-author network of that area. In the co-author network, each node corresponds to one author, while each undirected edge corresponds to a co-author relationship between two authors. The titles of publications are used as the textual stream. We remove stopwords from the titles, and use bigrams as terms. The three datasets we used for the evaluations are as follows.
Data mining community dataset: This community con-tains 14,011 publications and their authors in 10 data min-ing conferences over 15 years (1995-2010). Each year is re-garded as a time step. Conferences are listed in Table 2. The co-author network contains 6,948 nodes and 39,797 edges. Authors that have less than 3 publications are ltered out.
Machine learning community dataset: It is similar to the data mining community, but are based on 10 machine learn-ing conferences, as listed in Table 2. This community con-tains 24,184 publications, 6,845 nodes and 34,254 edges.
Mixed dataset (data mining + machine learning): We want to test the algorithms on a more complicated com-munity, because the above two simple communities share several desirable properties: Authors in each community are from the same area, and are strongly connected to each other; Documents in each community are from similar top-ics, so the events are easier to be detected. By extracting publication in the 20 listed conference and ltering out au-thors with less than 5 publication, we get a network that consists of 7664 nodes and 47,158 edges.
 T abl e 2: Conference Lists of Two Communities in DBLP dataset
To test the accuracy of the inference algorithm, we evalu-ate the LADP model on the semi-synthetic dataset. As the diffusion probabilities and external trends for this dataset are known, we can evaluate the inferred values directly.
Result of diffusion model learning. The result is shown in Figure 2(a), for the diffusion probability with each edge in the network, we calculate the prediction error as the difference between the real value and the inferred value. We plot the empirical cumulative distribution function of the prediction error in Figure 2(a). Each point in the curve shows a value and the percentage of the prediction errors that are below the given value. As shown in the gure, the diffusion probabilities inferred by LADP model have much smaller error than the ones inferred by the baselines. For ex-ample, for 92.0% edges, the prediction errors of the LADP model are smaller than 0.05, while only for 79.8% and 50.8% edges respectively the prediction errors of the PCB algo-rithm and the Saito's algorithm are within this range. The only difference between the LADP model and the PCB algorithm is that the LADP model extracts the so-cial events from the entire events, and learns the diffusion probabilities from the social events, while the PCB algo-rithm learns the diffusion probabilities from all actions in the events. It implies that the improvement of accuracy by the LADP model is the result of extracting social events.
Extraction of social events. For better understanding of how the LADP model makes the improvement, we show the difference between inferred socially sourced portion and the ground truth in Figure 2(b). To calculate the difference, we rst de ne the time sequence f a t g T t =1 of a set of actions. For each time step t = 1 ; : : : ; T , there is an element a the time sequence, which is the number of actions in the set that are created at time step t . We then calculate for each event the L2 distance between the time sequence of the extracted socially sourced portion and that of the ground truth, and plot the empirical distribution of the distance in Figure 2(b). (a) Erro rs of Diffusion Prob-ability Figure 2: Evaluation on Synthetic Stream Data
For the proposed LADP model, only the extracted socially sourced portion are used for the learning of diffusion model, while for the PCB and Saito's algorithms the entire events are used for the learning, in other words, they consider the entire events to be social events, so we also show the L2 distance between the time sequence of the entire event and that of socially sourced portion.

Since the time sequence of entire events is always an over-estimation of the time sequence of social sourced portion, we can easily get a better estimation by simply scaling it. We then scale the time sequence for each event and make it have the same mean value as the time sequence of the social sourced portion, and calculate the L2 distance between the scaled time sequence and that of socially sourced portion as well.

As shown in Figure 2(b), the distance between the time sequence of socially sourced events inferred by LADP and that of the ground truth is smaller than the distance be-tween the time sequence of the entire events and that of socially sourced portion, even when we scale the time se-quence of entire event. This re ects that the LADP model can extract social events from the entire events, and the ex-traction is more than divide the entire events into two parts according to the proportion of socially sourced portion and externally sourced portion. The extraction of social events by the LADP model results in better accuracy in learning the diffusion model.
Result of diffusion model learning. For the real stream data, the diffusion probabilities are unknown, so we are not able to evaluate a learned diffusion model directly. Instead, we evaluate it by evaluating the most in uential node suggested by the models. In the Twitter commu-nity, users can re-post tweets of other users, which is called \retweet". The more retweets a tweet gets, the more widely it spreads in the network. We can expect that users with larger number of retweets per tweet are more in uential in the social network.

Given the learned diffusion model, by sampling the diffu-sion process for 50,000 times, we calculate the average in-uence of each node, i.e. the average number of activations when using each single node as the seedset. The nodes are then sorted according to the descending order of their in u-ences. In this way, we nd out the most in uential nodes in the models learned by the LADP, PCB and Saito's al-gorithms. We then evaluate the most in uential node by the average number of retweets for each tweet. Figure 3(a) shows the comparison between the LADP method and the baselines for nding the most in uential nodes. For each number k on the x-axis, we calculate the average number of retweets for the top k users and plot it in the gure. In T abl e 3: Top events and top social events identi ed by LADP and Myers's in UIC Twitter network the most range of x-axis, the LADP model achieves a larger average number than the baselines. Besides, the curve of the LADP model is a monotone decreasing curve. It is more desirable than the non-monotonic curve of the PCB model, because the monotone decreasing curve suggests that higher-ranking nodes inferred by the algorithm are really more in-uential than lower-ranking nodes. The better performance in nding top in uential users suggests that the diffusion model learned by LADP is better than those learned by the baselines.

Analysis of top social events. We analyze the top social events extracted by the LADP model in order to un-derstand how the LADP model improves the learning of dif-fusion model. First, we rank the events according to the numbers of all actions in the events, and list the top events in the network. Then, we rank the events according to the number of actions in the inferred socially sourced portion, i.e. we rank the social events extracted by the LADP model. After excluding the externally sourced actions, the list of top social events extracted by the LADP model is different from the list of top events with the largest number of actions. For a comparison, we also list the top events with the largest in-ternal portion inferred by the Myers's algorithm [15].
The results are shown in Table 3. The rst column in the table is the list of keywords of events with the largest number of actions. Some keywords in the list are closely related to the UIC community (Chicago, UIC, higherEd 1 ), but others are not (FF 2 , energy). The second column is the list of top ve internal events returned by the Myers's algorithm. For this dataset, the lists in the second column happens to be the same as the list in the rst column, but this is not always the case, as we will show in the follow-ing experiment. In the third column, we list the top social event extracted by the LADP model. Comparing with the rst column, the keywords \UIC", \higherEd", and \Illinois" move upward in the rankings, while the keywords \FF", \en-ergy" move downward. It is obvious that keywords that are related to the community get better rankings in the list re-turned by the LADP model. Since the keywords that are closely related to the community is more likely to suggest social events inside the network, the top events extracted by the LADP model are more likely to re ect information diffusion processes inside the social network. (Notice that though the keyword \FF" re ects an event on the Twitter website. It is not unique to the UIC community, and its propagation does not suggest an information diffusion pro-cess inside the Twitter-UIC network. Further analysis on the keywords \UIC" and \FF" will be provided in next sec-tion.) This explains that the improvement of LADP in the learning of diffusion probabilities.
In Figures 3(b) and 3(c), we show the results of\UIC"and h igh erEd is short for higher education in this context.
FF is short for FollowFriday in Twitter. It is an online event that people recommend friends for other users. \FF" inferred by the LADP model. For each of the events, we show the number of actions over time for the socially sourced portion, externally sourced portion, and the entire event. As shown in the gure, for the event of \UIC", the socially sourced portion is the majority, while for the event of \FF", the size of the two portions are similar. Beside, for the event of \UIC", the socially sourced portion explains most of the peaks in the full event, while for the event of \FF", some peaks are explained by the in uence while others are explained by the external trends. This is the reason why the ranking of \UIC" moves upward in the list of top social events returned by LADP, while \FF" moves downward. Result of diffusion model learning. Similar to the Twitter-UIC dataset, since there is no ground truth for the diffusion probabilities, we evaluate them by looking at the most in uential nodes. For the most in uential users sug-gested by each algorithm, we evaluate the top-k in uential nodes using their H-index and the number of citations. The H-index and the number of citations of author are obtained from arnetminer.org.
 On all the three datasets, we run the LADP model, the PCB algorithm and Saito's algorithm respectively to learn the diffusion probability with each edge. These three sets of diffusion probabilities can then be used for deciding most in uential nodes. By sampling the independent cascade pro-cess for 50,000 times, we calculate the average in uence of each node, i.e. the average number of activations when using each single node as the seedset. We then sort nodes to the descending order of in uence and plot the average H-index and the average number of citations for top-k authors.
Figures 4(a)-(f) show the comparison of the LADP model and the baselines on three datasets. The x-axes of these gures are the number of top authors, while the y-axes are the average H-index or the average number of citations of these top authors. On all but one gure, the curves of the LADP method are obviously above those of the baselines, which means the top-ranking authors reported by the LADP model are more in uential than those reported by the base-lines. Even for that Figure 4(b), LADP performance on the top 10 authors is signi cantly better than the baselines. It re ects that the diffusion model learned by LADP is more accurate than the one learned by the baselines.

Analysis of top social events. Similar to the exper-iment on Twitter dataset, in order to understand how the LADP model improves the learning of diffusion model, we analyze the top social events extracted by the LADP model. Since we are more familiar with topics in the data mining, we use the data mining community for the analysis.
The results are shown in Table 4. For each network in the dataset, the rst column in the table is the list of keywords of events with the largest number of actions. The second column is the list of top ve internal events returned by the Myers's algorithm. The top ve social events extracted by the LADP model are listed in the third column.

The rankings of keywords related to speci c research top-ics in the data mining community are moved upward by the LADP algorithm (data streams, time series, association rules), while keywords that are less related to speci c top-ics move downward. That is desirable because speci c top-ics are more likely to be connected to information diffusion processes in the network, and represent social events. The (e) Data Mi ning + Machine
Learning Figure 4: (a)(c)(e) H-index for Top Authors, (b)(d)(f) Number of Citations for Top Authors.
 Tabl e 4: Top events and top social events identi ed by LADP and Myers's in data mining community Myers's algorithm also tends to give speci c topics higher ranking, but it undesirably gives higher ranking to the top-ics \query processing" and \xml data", which are related to the database community, rather than the data mining com-munity speci cally.
Information diffusion has been intensively studied in so-cial network analysis [3, 5, 4, 12, 1]. Earlier work on infor-mation diffusion model does not consider the time dynamic of diffusion processes. Recent work in [14, 11, 19] consider the diffusion processes that unfold along the time, so that temporal events in the social network can be explained by the information diffusion processes. Independent Cascade (IC) model and its variants [10, 14, 7, 6, 13] form most widely used class of information diffusion models. Models in this class share two features: (1) the in uences from the neighbors of a user are independent; (2) there is a diffusion probability along with each edge in the network.

The problem of estimating the diffusion probabilities has been studied in [9, 17, 16, 8]. The diffusion probabilities are learned from events in social networks. Models in [9] esti-mate the diffusion probability for general threshold models which include the IC model and almost all of its variants. [17, 16] propose a likelihood maximization approach for the learning of diffusion probabilities. However, due to the large number of parameters and the complexity of the likelihood function, the inference algorithm is time-consuming.
Although it has long been argued that the information diffusion process is not the only reason triggering events in social networks [2, 18, 1], most existing work on the learning of diffusion probabilities neglects the propagation of infor-mation from external trends. Work in [15] explicitly models the external trends and incorporates it with the information diffusion model. While their approach adopts a simple infor-mation diffusion model and focuses on inferring the external trends, our model aims to learn the diffusion probabilities with edges in the networks, and the learned diffusion prob-abilities can be used in the IC model and its variants.
In this paper, we study the problem of learning informa-tion diffusion models on social networks. We propose an LADP model that improves the learning by extracting so-cial events from data streams. The LADP model integrates the external trends and the information propagation process inside the social network.
 Evaluation on real and synthetic datasets shows that the LADP outperforms existing method on the task of learn-ing information diffusion models. Analysis shows that the improvement is due to the extraction of social event.
A possible future work is to use more sophisticated model for the external trends instead of the simple Beta distri-bution. We also expect a further extension to the LADP model that uses topic modeling methods, instead of consid-ering each keyword independently. This work is supported in part by NSF through grants IIS-0905215, CNS-1115234, IIS-0914934, DBI-0960443, and OISE-1129076, and US Department of Army through grant W911NF-12-1-0066. [1] A. Anagnostopoulos, R. Kumar, and M. Mahdian. [2] S. Aral, L. Muchnik, and A. Sundararajan.
 [3] E. Bakshy, B. Karrer, and L. A. Adamic. Social [4] M. Cha, H. Haddadi, F. Benevenuto, and [5] M. Cha, A. Mislove, and K. P. Gummadi. A [6] W. Chen, C. Wang, and Y. Wang. Scalable In uence [7] W. Chen and Y. Wang. Efficient In uence [8] L. Dickens, I. Molloy, J. Lobo, P.-C. Cheng, and [9] A. Goyal, F. Bonchi, and L. Lakshmanan. Learning [10] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing [11] G. Kossinets, J. Kleinberg, and D. Watts. The [12] M. Lahiri, A. Maiya, R. Sulo, Habiba, and T. Y. B. [13] T. Lappas, E. Terzi, D. Gunopulos, and H. Mannila. [14] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, [15] S. Myers, C. Zhu, and J. Leskovec. Information [16] K. Saito, M. Kimura, K. Ohara, and H. Motoda. [17] K. Saito, R. Nakano, and M. Kimura. Prediction of [18] C. R. Shalizi and A. C. Thomas. Homophily and [19] T. Wang, M. Srivatsa, D. Agrawal, and L. Liu.
