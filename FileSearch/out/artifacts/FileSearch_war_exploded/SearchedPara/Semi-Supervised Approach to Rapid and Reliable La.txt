 In this paper, we propose a method, where the labeling of the data set is carried out in a semi-supervised manner with user-specified guarantees about the quality of the labeling. In our scheme, we assume that for each class, we have some heuristics available, each of which can identify instances of one particular class. The heuristics are assumed to have reasonable performance but they do not need to cover all instances of the class nor do they need to be perfectly reli-able. We further assume that we have an infallible expert, who is willing to manually label a few instances. The aim of the algorithm is to exploit the cluster structure of the problem, the predictions by the imperfect heuristics and the limited perfect labels provided by the expert to classify (la-bel) the instances of the data set with guaranteed precision (specificed by the user) with regards to each class. The spec-ified precision is not always attainable, so the algorithm is allowed to classify some instances as dontknow . The algo-rithm is evaluated by the number of instances labeled by the expert, the number of dontknow instances (global coverage) and the achieved quality of the labeling. On the KDD Cup Network Intrusion data set containing 500,000 instances, we managed to label 96.6% of the instances while guaranteeing a nominal precision of 90% (with 95% confidence) by having the expert label 630 instances; and by having the expert la-bel 1200 instances, we managed to guarantee 95% nominal precision while labeling 96.4% of the data. We also provide a case study of applying our scheme to label the network traffic collected at a large campus network.
 H.2.8 [ Database Management ]: Database Applications  X  Data Mining Algorithms, Theory, Experimentation
In this paper, we propose a semi-supervised approach to label large (unlabeled) data sets. The goal of the approach is to label as many instances of the data set with the correct label as possible, under the constraint that the achieved pre-cision 1 of the labeling must exceed the user-specified nom-inal precision  X  with 1  X   X  confidence (  X  is also specified by the user). This means that if the algorithm is run 100 times, we expect that the achieved precision of the labeling will exceed  X  at least 100(1  X   X  ) times, but it may fall be-low  X  in the remaining cases. We call this constraint the precision requirement . Under this constraint, the algorithm may not be able to assign a label to all instances. The label dontknow is used to indicate that an instance is left unla-beled. If we allow many instances to be labeled dontknow , the problem becomes trivial 2 , hence we aim to reduce the number of dontknow labels. We assume that we have access to an infallible expert who can manually determine the true label for a few instances selected by the algorithm. Allow-ing too many instances to be labeled manually by the expert renders the algorithm infeasible in practice, hence we aim to reduce the number of instances labeled by the expert. The algorithm will therefore be evaluated based on the achieved precision of the labeling (w.r.t. to the non-dontknow labels), the number of dontknow labels and the number of instances manually labeled by the expert.

Supervised classification methods have been shown to be very effective for a large number of applications [12], in-cluding network traffic analysis [18]. Some of these areas operate on very large data sets and frequently experience substantial changes in the class composition. As a result, a training data set can, with time, become non-representative of the test data sets causing the classification performance to suffer. For this reason, supervised classifiers in such do-mains need to be frequently re-trained. Frequent re-training, in turn, requires rapid generation of a training set. While obtaining an unlabeled data set is inexpensive, labeling its instancesisverycostly.

We present a method for the rapid labeling of unlabeled data sets that works in two phases. In Phase I, the instances in the data set are heuristically assigned a temporary label the fraction of instances with true label l among the in-stances whose label is predicted to be l by the algorithm
E.g. label only one instance of each class, but label it cor-rectly. The achieved precision will be 100%, yet this labeling is useless. which will exceed the nominal precision  X  with 1- X  confi-dence (possibly dontknow ). This temporary labeling provides no guarantees, but we aim to achieve high precision. Based on these temporary labels, the goal of Phase II is to produce a final labeling that satisfies the precision requirements, that is, the precision of the final labeling exceeds the nominal precision  X  w.r.t. each class (with 1  X   X  confidence). For Phase II, we provide a statistical framework based on sam-pling theory that ensures that the precision requirements are met regardless of the quality of the temporary labeling. While the quality of the temporary labeling does not affect the correctness (the precision requirements will be met), it affects the efficiency of the Phase II framework. If the pre-cision of the temporary labeling is low, Phase II becomes costly: the number of instances labeled by the expert be-comes large. If the precision of the temporary labeling is high, the framework is efficient, i.e. the expert has to label only relatively few instances.

Phase I generates a temporary labeling based on some su-pervision information, hence Phase I is a semi-supervised method on its own. While any semi-supervised algorithm that can generate a labeling is applicable as an implemen-tation of Phase I (provided that it can achieve sufficiently high precision), in this paper, we present our own implemen-tation of Phase I. We assume that for each class, we have a heuristic that can identify that particular class (which we call the target class of the heuristic). These heuristics are crude classifiers that are neither exhaustive (not all instances are classified) nor mutually exclusive (the predictions by the heuristics, called heuristic labels , may conflict for some in-stances). Such heuristics can often, although not always, be found [14]. Phase I produces the temporary labeling by making use of the cluster structure, the heuristic labels and the true labels from the expert for a select few instances (selected by the proposed algorithm).

We evaluated our scheme on the KDD Cup Network In-trusion data set [1] containing 500,000 instances. By having an expert manually label 630 instances, we managed to label 96.6% of the instances while guaranteeing a nominal preci-sion of 90% (with 95% confidence); and by having the expert label 1200 instances, we managed to guarantee 95% nomi-nal precision while labeling 96.4% of the data. The achieved precision in both cases was substantially higher than the nominal precision. We also provide a case study of applying our scheme to label the network traffic collected at a large campus network.
In this paper, we make the following contributions:
Due to limitations in space, we are only able to discuss the key differences between our scheme and existing semi-supervised methods. The reader is referred [17] for a more detailed discussion of semi-supervised learning methods [6, 15, 24], selective sampling [2, 5, 8, 16], active learning [15, 8], expert setting [4, 13, 23], label efficient learning [9], and how they differ from our proposed scheme.

In general, semi-supervised learning (SSL) [6, 15, 24] is po-sitioned between supervised and unsupervised learning. SSL algorithms are given the unlabeled data and some supervi-sion information, such as labels, to some of the instances. In the typical setting, the supervision information is the class label for a very small number of instances. Such a setting is particularly useful in practice, when a large number of un-labeled data instances can be easily obtained, but labeling them is expensive.

Since our scheme is given an unlabeled data set and some supervision in the form of imperfect ( heuristic )labelspro-vided by the heuristics and (relatively) few true labels ob-tained from an expert, it also falls under the umbrella of SSL. Our scheme differs from existing semi-supervised schemes in that it gives probabilistic guarantees about the precision of the labeling. While some of the existing semi-supervised methods also provide guarantees, these guarantees cannot be specified by the user. In contrast, the user of our scheme has direct control over the per-class precision.

The first phase of our scheme produces a temporary label-ing (with no guarantees) based on some supervision informa-tion (heuristic labels and true labels from an expert). Con-sequently, our first phase by itself is also a semi-supervised algorithm. It differs from existing semi-supervised scheme in its ability to use heuristic (unreliable) labels and it can achieve high precision (although such guarantees do not ex-ist) due to its ability to label instances whose label is unclear as dontknow .

Another difference between typical semi-supervised learn-ing methods and our scheme is the ability to select labels. Active learning and selective sampling[2, 5, 8, 16] methods also have the ability to choose samples to obtain the label for, but their goal is different from ours. The goal of se-lecting samples in case of selective sampling algorithms is to reduce the number of training examples and maximize the amount of information learned. In our case, the goal of sampling is to provide guarant ees; we are not learning from the examples.
We consider a c -class classification problem with a set of labels L = { l 1 ,l 2 ,...,l c } . The data set D consists of un-labeled instances x i .Foreach x i ,atruelabel y i exists, y i  X  L , but it is unknown. We need to build a labeling scheme M , that can classify most observed instances x i cor-rectly. The model can be evaluated by its precision (w.r.t. class l ,  X  l  X  L ) for all observed instance x i  X  D and for all label l  X  L , where #( cond ) denotes the number of instances satisfying condition cond . The goal is to build a model M ,which satisfies the precision requirement of where  X  denotes the nominal precision and  X  is the signifi-cance level; both  X  and  X  are user-supplied parameters. In other words, for the instances M classified into one of the c classes, we give the guarantee that with (1  X   X  ) confidence, the achieved labeling precision will not be less than  X  .For our experiment, we chose  X  = . 05, which is commonly used in statistics.

Before we outline the method, let us define some proper-ties of the clusters in the data set.

Definition 1 (Tight). A cluster, or more generally, any set of instances, is tight , if its mean squared error (MSE) is less than a pre-defined threshold minM SE .

Essentially, a cluster is tight, if the instances in the clus-ter exhibit sufficiently high intra-cluster similarity in the ap-propriate feature space. The definition of tight -ness can be extended so that it can handle categorical attributes.
Definition 2 (Homogeneity). A cluster (or set S of instances) is homogeneous with respect to the heuristic la-bels if there exists a 100  X  (1  X  )% subset S of the instances such that (a) the instances in S have only one heuristic label and (b) the instances in S have identical heuristic label (let us cal l it  X  ) OR if none of the instances in S has a heuristic label.
Note that if S has some heuristic labels and it is homoge-neous, then  X  is the majority heuristic label in S .
Similarly, homogeneity can also be defined with respect to the true labels if some or all of the true labels in S are known.

Definition 3 (Small). A set of instances is small, if the number of instances in the set is less than a pre-defined threshold n small .

As an extreme, consider a set consisting of a single in-stance. Such a set is tight and homogeneous, yet this set is not useful for our labeling scheme. The intent behind the definition of a small set is to define a minimum size which is useful for labeling.
Figure 1 illustrates the process of labeling. Labeling is carried out in two phases. The goal of the first phase is to produce a temporary labeling based on the cluster struc-ture of the problem, the imperfect labels assigned by the heuristics (called heuristic labels ) and the true labels deter-mined manually by the expert for a select few instances. Temporary labels are assigned on a per-cluster basis. The desired clusters are tight and homogeneous with respect to
Figure 1: Illustration of the labeling procedure the heuristic labels. If the cluster is tight and homogeneous with  X  being the majority label, all instances in the clus-ter are assigned the temporary label  X  .Otherwise,wehave indication that either the heuristics or the cluster structure are inconsistent with the classification problem in the given cluster (subspace). Instances in such clusters are assigned the temporary (and final) label dontknow and are not consid-ered any further. The temporary labeling in the first phase provides no quality guarantees.

The second phase takes the temporary labels as input and generates a final labeling satisfying the precision require-ments. The precision requirements are ensured via sampling: a strategically chosen sample is drawn from the instances with the same temporary label and the expert is consulted to determine the true label(s) for the instances in the sam-ple. We will use the term sample size to refer to the number of instances that the expert is asked to determine the true label for. From this sample, the precision of the temporary labeling can be estimated. If the labeling precision meets the precision requirements, the temporary labeling becomes the final labeling, otherwise some of the temporary labels need to be corrected before they become the final labels. As shown in Section 4.1, we can estimate the number of labels to be corrected. If the number of corrections is too large to make the approach practical, the user may elect to lower the precision requirements with respect to any of the classes. Phase I. (1) Labeling by heuristics. We apply the heuristics to all instances. The heuristics are not complete and not mutually exclusive, hence, each instance is assigned zero, one or more heuristic label(s) . (2) Partitioning. We cluster the data set into partitions : partitions are recursively divided using bisecting k-means [20] until they become tight and homogeneous ;oruntilthey become small 4 . Partitions resulting from this step can be of three kinds: (i) partitions that are tight, homogeneous and some instances have heuristic labels, (ii) partitions that are tight, homogeneous but have no heuristic labels and (iii) partitions that are small. Instances in the small clusters are assigned the temporary (and also final) label dontknow and are removed from further consideration. For details, see [17]. as defined in Section 3. Paremeter settings: minMSE=.5, n (3) Initial Sampling. From each (non-small) partition, we draw an initial sample and have the expert determine the true labels of the instances in the initial sample 5 .
If a partition is tight, homogeneous w.r.t. the heuristic labels having a majority heuristic label  X  , and the initial sample drawn from this partition is also homogeneous w.r.t. the true labels having the same label  X  as the majority true label in the initial sample, then we assign the temporary label  X  to all instances in the partition. If a partition is tight, homogeneous but no instance has a heuristic label, and the initial sample drawn from this partition is homo-geneous w.r.t. the true labels having the label  X  as the majority label, then we assign the temporary label  X  to all instances in the partition. In all other cases, all instances of the partition are labeled dontknow and are removed from further consideration.

Since the temporary labels are determined from a small sample 6 , the temporary labels can be incorrect. These errors will be discovered and compensated for in Steps 5 and 6 in Phase II.
 Phase II. (4) Strata Formation. We pool the instances into c strata based on their temporary labels. The stratum of class  X  con-tains all instances whose temporary label is  X  .(Recallthat instances with the label dontknow has already been removed in Steps 2 and 3.) Although all instances in stratum  X  have temporary label  X  , some instances may have a different true label. These instances are the labeling errors . (5) Verification Sampling. We draw a verification sample from each stratum. The purpose of the verification sample is to probabilistically verify that the labeling precision in the stratum meets the user-specified guarantee.
 Method. We draw a sample of size n v , Let  X   X  denote the (unknown) true error rate in the stratum. Then for each instance in the verification sample, the prob-ability of finding an e rror (an instance whose true label is different from the temporary label) is  X   X  .Ifthenumber e of errors found in the verification sample is 0, then the preci-sion of the temporary labeling is sufficient high, because For more details, refer to [17]. (6) Correctional Sampling. Correctional sampling is performed when the true labels in the verification sample (in Step 5) do not coincide. During the correctional sam-pling, the error rate in the stratum is estimated iteratively. When an error is discovered, it is corrected: the temporary label is replaced by the true label. The utility of the correc-tional sampling depends on the unknown true error rate  X   X  in the stratum.

If  X   X  &lt;  X  z  X  normal quantile for 1  X   X / 2, =1  X   X  and n v is verification
In the followings,  X  X ample X  means  X  X raw a sample and have the instances in the sample evaluated by the expert X 
In our implementation, the initial sample is of size 1 for tight, homogeneous partitions that have some heuristic la-bels and 2 for tight, homogeneous partitions with no heuris-tic label. sample size, then the verification sampling will very likely succeed and no correctional sampling will be required.
If  X   X  is higher, but  X   X  &lt; , then the verification sampling can fail. In such cases, the goal of the correctional sample is to show through iteratively refined estimates of the labeling error rate that the true error rate is less than .Asmall sample suffices to achieve this.

If  X   X  &gt; , then by taking larger samples (and by correcting errors discovered in the sample), the user corrects enough labels to demonstrably reduce  X   X  below .

If, however,  X   X  then attempting to correct the la-bels in the above manner can result in a very large sample. In such cases, the user is advised to dynamically lower the precision requirement w.r.t. the class in question. Since the sample size can be estimated (see Section 4.1), the user receives sufficient notice of this situation.

The term correctional sample size refers to the cumulative sample taken in this step (Step 6). Figure 2 depicts the correctional sample size as a f unction of the true error rate.
Table 1: Verification sam-plesizeforsomecommon  X  and  X  paris  X   X  0.010 182 90 44 0.025 146 72 36 0.050 119 59 29 0.100 91 45 22
Phase II, that is, verification and correctional sampling, ensure that the user-specified guarantees are met. These guarantees are met regardless of the quality of the temporary labels generated in Phase I. The temporary labels only affect the efficiency of the scheme: the number of instances labeled dontknow and the number of instances manually classified by the expert.

From the verification and correctional sample, we can es-timate the error rate (fraction of instances whose true label is different from the temporary label) in the stratum. As-suming that we found e errors in a sample of n instances, the upper limit of the 1  X   X  Agresti Coull confidence interval [3] around the estimated error rate  X   X  = estimate, [3]) is where z 1  X   X / 2 is the appropriate standard normal quantile. If  X   X  + &lt; ,( =1  X   X  ) then the stratum is labeled with the user specified  X  precision with 1  X   X / 2 confidence (which is higher than the required 1  X   X  confidence).

The efficiency of this scheme, that is, the ability to label a large portion of the instances from a small sample, depends on (i) the extent to which the cluster structure is consistent with the classification problem and (ii) the performance of the heuristics. Cluster structure. If the cluster structure is largely incon-sistent with the classification problem, then most partitions will neither be homogeneous with respect to the heuristic labels (provided that the heuristic labels have good perfor-mance) nor with respect to the true labels in the initial sam-ple (irrespective of the perfo rmance of the heuristics). Ac-cordingly, most partitions will be labeled dontknow .Ifthe cluster structure is largely inconsistent with the classifica-tion problem, most semi-supervised schemes including ours break down.

However, when the cluster structure is largely consistent with the classification problem and the heuristics have rea-sonable performance, then the class-focused partitioning scheme will discover tight and homogeneous partitions. Discovering homogeneous partitions makes the scheme efficient in two ways. First, if all partitions are homogeneous w.r.t. the true labels then a small verification sample suffices to verify that the nominal classification precision is met. Table 1 de-picts the verification sample sizes for a number of commonly used  X  and pairs. Second, even if the partitions are not homogeneous w.r.t the true labels and corrections need to be made, the lower the error rate in the startum, the smaller the correctional sample. Figu re 2 depicts the correctional sample size 7 required to attain the user-defined precision of 95% as a function of the true error rate  X   X  .
 Performance of the heuristics. If the heuristics have both high precision and high recall, then the problem is trivial and the user only gains guarantees from the use of our scheme. If the heuristics have both poor precision and poor recall, then it is almost equivalent to not having any heuristics  X  the condition that most semi-supervised schemes operate under. If the heuristics have high precision but poor recall, then our scheme helps by superimposing the precise labels to the entire partition thereby increasing the recall. If the heuristics have poor precision but (relatively) high re-call, then the performance of the scheme depends on how the errors in the heuristic labels ar e distributed. If the errors are concentrated into a few partitions (instances that are easily confused with some other class), then the instances in these partitions will be labeled dontknow (either because the par-titions become too small or because the heuristic and true labels do not match in the initial sample) and the instances in partitions with less errors will be labeled correctly. In such situations, our scheme improves the precision. If, however, the errors are uniformly distri buted across the partitions, our scheme will be of no help, since most instances of the target class will be labeled dontknow .
We have shown earlier that from a sample of size n drawn from a stratum, we can compute the 1  X   X / 2 confidence in-terval around the estimated error rate in the stratum. If the upper limit of this confidence interval  X   X  + (Eqn. 3) is less than the target error rate , =1  X   X  , then the label-ing precision in the stratum is higher than  X  with 1  X   X / 2 confidence.

Let us consider the case when  X   X  + &gt; . In this case, we need to correct some of the temporary labels, until the precision of the labeling exceeds  X  , or equivalently, the error rate fall below (with at least the required 1  X   X  confidence). We
The verification sample size is constant 59 can estimate the number of errors to correct as
Originally, E is estimated from the verification sample, which is very small. Estimating the error rate from small samples leads to large variance, potentially resulting in a vast over-or underestimate of the number of errors to be corrected (in Eqn 4). For this reason, instead of correcting all E errors at once, correction is c arried out iteratively, in say I iterations. In iteration i , a sample is drawn such that the cumulative sample size becomes n ( i ) c = i I N  X   X  + sample is expected to contain approximately i I N ( X   X  +  X  errors. From this sample, we re -estimate the error rate (let us call it  X   X  + , ( i ) ) and subsequently the number of errors to be corrected (we denote it by E ( i ) ). The estimates  X   X  and E ( i ) are expected to be more precise (of less variance) than the same estimates in previous iterations. Due to the reduced variance of  X   X  + , ( i ) , we may find that  X   X  + , ( i ) this case the precision requirements are met. Otherwise, we continue iterating until we have discovered E (+ , ( i ) errors. Once these errors are corrected , the resulting final labeling will have a precision in excess of  X  (with 1  X   X  confidence). The reader is referred to [17] for additional details and an example. We provide results of applying our scheme to the KDD Cup 1999 data set. We also present a case study of labeling the network traffic of a large campus network.
The KDD Cup 1999 data set consists of 494,020 instances and the task is to classify these instances into one of 5 classes: normal , corresponding to normal traffic, and the remaining 4 classes dos, probe, r2l ,and u2r corresponding to attacks. The class distribution of the data is as given in Table 2. Table 2: The class distribution of the KDD Cup data set
The difficulty of this classification task lies in the ex-tremely skewed class size distribution: the two smallest classes account for less than 1% of the data set.

For simplicity, of the 40 features, we used the 37 con-tinuous and binary features with Euclidean distances. As preprocessing, we standardized 8 the data set.

Our scheme needs one heuristic for each class that can be used to identify instances of this target class. Since we did not have heuristics available for the KDD Cup data set and nor did we have an expert who could give us reasonable heuristics, we used Ripper to produce binary rule-based clas-sifiers that we used as heuristics. We manually removed con-ditions from the Ripper generated rules until the precision of the heuristics (w.r.t. to the larger classes) fell to approx. 90%. We refer to this set of heuristics as H90 and the exact performance of the heuristics are presented in Table 3. normalized to 0 mean and variance of 1 Table 3: The performance of the heuristic set (H90)
There is a tradeoff between the number of instances man-ually labeled by the expert ( sample size for short; this in-cludes the initial, verification and correctional samples), the precision requirement  X  , and the number of instances left unlabeled. To study this tradeoff, we run our scheme (using the H90 heuristics in Table 3) for a number of nominal preci-sions:  X   X  X  . 8 ,. 85 ,. 9 ,. 95 ,. 99 } . For each nominal precision, the scheme was run 50 times. Figure 3 depicts the minimal, maximal and median sample size across the 50 trials. In the right pane, the minimal, maximal and median number of instances that were left unlabeled are depicted. Figure 3: The sample size (left) and number of unla-beled instances (right) for nominal precisions of 80, ..., 99%.

The left pane in Figure 3 shows that the sample size in-creases as the nominal precision increases. The relatively low median sample size (5 / 1000 th of the data set) sufficed to la-bel 98% of the data set with a guaranteed 9 precision of 90%. This indicates that the cluster assumption mostly holds for the data set. The sharp increase in the sample size between 90% and 99% nominal precisions suggests that correctional sampling was needed to reach 95+% precision. This is due to inconsistencies in the cluster structure as well as errors in the heuristic labels. The number of unlabeled instances is relatively flat because they primarily result from conflicts among the heuristic labels in phase one; they depend on the nominal precision to a much smaller extent 10 .

Table 4 presents the median sample size and the num-ber of unlabeled instances for nominal precisions of 90, 95 and 99%. It also presents the achieved precision and re-call with respect to all five classes. The achieved precision always meets and often exceeds the nominal precision. In most cases, the achieved precision exceeds the precision of with (1  X   X  ) confidence
Changing the nominal precision from  X  to  X  +  X  influences the number of unlabeled instances only if there exists a par-tition where the majority heuristic label is assigned to f fraction of the instances that have any heuristic label and f  X  (  X ,  X  +  X  ) , X &gt; 0.
 Table 4: Performance of the scheme in conjunction with the H90 heuristics. SS: sample size (number of instances labeled by the expert), U: number of unlabeled instances (i.e. labeled dontknow ) normal 0.9849 0.9056 0.9921 0.9058 0.9971 0.9051 probe 0.9644 0.8061 0.9816 0.8066 0.9997 0.8051 the heuristics (see Table 3). Clusters where the heuristic la-bels were inconsistent with the classification problem, were discarded (labeled as dontknow ), which led to increased pre-cision but reduced recall. The achieved recall is particularly low for the u2r class. The u2r class is very small (1/10,000 of the data set) and does not form clear clusters, hence clas-sifying it is very difficult. Using our scheme is not necessary for such small classes; the expert can manually label all 26 instances predicted to be of class u2r by the heuristics.
To shed some light on the variability of these results from trial to trial, Figure 3 also depicts the 5th and 95th percentile of the sample size required to label the data set with specific nominal precisions (horizontal axis). While the maximal sample size can be considerably higher than the mean, the mean is much closer the 5th percentile sample size than to the 95th percentile sample size. This indicates that having to use a very large sample size occurs infrequently.
So far, we have demonstrated that our scheme has the ability to compensate for the lack of precision by reducing the recall. What happens if the heuristics have such high precision that further improvement on the precision is un-necessary? To answer this question, we also created a set of heuristics called H99, which has very high precision. The performance of these heuristics is depicted in 5.
 Table 5: The performances of the heuristics H99 in isolation.

Table 6 depicts the median performance of our scheme across 100 trials using the H99 heuristic set. The results show that when the heuristics have high precision, our scheme improves the recall. The notable exception is the u2r class. As we discussed before, there is no need to use our scheme to label a class as tiny as the u2r .

Although these results are not presented here, we found that the sample size as well as the achieved performance is more stable when we use the H99 heuristics instead of the H90 heuristics. For additional results and experiments, please refer to [17]. Table 6: Performance of the scheme in conjunction with the H99 heuristics. SS: sample size, U: number of unlabeled instances. normal 0.9974 0.9410 0.9988 0.9406 0.9991 0.9407 probe 0.9800 0.8628 0.9901 0.8593 0.9968 0.8616
As we discussed before, Phase I of our scheme produces a temporary labeling based on the cluster structure and some heuristics labels. In this experiment, we created a hybrid scheme , where Phase I is replaced by Transductive SVM [10]. Based on heuristic labels produced by the H90 heuristics, TSVM generates a temporary labeling. We use this TSVM generated temporary labeling as the input to our Phase II. The key difference between our Phase I algorithm and TSVM is that our scheme has the ability to label instances as dontknow when the heuristic labels are inconsistent with each other or with the cluster structure. In contrast, TSVM will assign one of the five labels. Accordingly, the goal of this comparison is to study the utility of the dontknow label. Method. Many of the instances receive multiple (conflict-ing) labels from the heuristics in H90. For this reason, we apply the heuristics in sequential order, specifically in in-creasing order of the class prevalence. First, we assign the heuristic label u2r to all instances classified as such by the respective heuristic. The instances that received the u2r heuristic label are set aside. Next, we assign the heuristic label r2l to each instance for which the respective heuristic predicts it to be r2l . Again, instances that received the r2l label are set aside. We proceed similarly with the probe , normal and dos labels in this order.

We used SVMlin [19] as our TSVM implementation. We gave SVMlin the heuristic labeling as the true labels and ran it once to produce a temporary labeling. SVMlin in this case is a replacement of our Phase I method. Next, we run our Phase II algorithm 50 times on the temporary labeling produced by SVMlin and measured the sample size and the achieved performance.
 Table 7: The median sample size and the median number of unlabeled instances Nominal prec. 80 90 95 99 Results. Table 7 presents the results in terms of the median sample size (verification and correctional sample only  X  there is no initial sampling involved) and the number of unlabeled instances. The number of unlabeled instances is constant across the trials, because it o nly depends on the temporary labeling.

In general, the sample size required by the hybrid scheme is higher than the sample size required by the proposed scheme. This is due to the lower achieved precision of the temporary labeling. On the other hand, the number of unla-beled instances is lower for the hybrid scheme, since it does not make use of the dontknow label.

Due to the large number of dontknow instances by the proposed scheme, the use of hybrid scheme is beneficial for lower nominal precisions. For higher precisions, the pro-posed scheme is more advantageous.
 Discussion. Table 8 depicts the achieved precision and recall of the temporary labeling produced by the proposed scheme (Phase I) at 90 and 99% nominal precision and also the achieved precision and recall of the TSVM-produced temporary labeling. It shows that the achieved precision of the TSVM-produced temporary labeling is less than 95% for the normal class. Correctional sampling was required, which caused the sample size to surge at 95% nominal pre-cision. On the other hand, the proposed Phase I scheme managed to meet even the 99% precision for the same class, hence the correctional sampling (if utilized at all) could be completed using a small sample size. In return, the number of dontknow instances is much larger than for the hybrid scheme.
 Table 8: Performance of the temporary labeling pro-duced by our scheme in conjunction with the H90 heuristics dos 0.9987 0.9798 0.9995 0.9798 0.9995 0.9899 norm 0.9847 0.9055 0.9969 0.9049 0.9428 0.9961 prob 0.9633 0.8052 0.9997 0.8050 0.9131 0.4505 r2l 1.0000 0.2549 1.0000 0.2549 0.7444 0.2691 u2r 0.9725 0.2777 0.9738 0.2773 0.4000 0.0385
We applied the proposed scheme to label a full day X  X  worth of traffic trace data of a large campus network consisting of 5 class-B networks. The data is massive and imperfections are abound starting from the data collection (approx. 15% of the connections are not recorded) through computing the feature values (e.g. it is not always possible to say whether a connection attempt was successful) to manually determining the label for a record. The lack of ground truth is our biggest challenge in providing a careful evaluation. Consequently our goal is only to show that by applying our scheme it is possible to produce higher quality label than those that can be generated from some available heuristics.
 Goal. We aim to label as many of the external source IPs that made connection attempts to our network on the day of the observation as we can with exactly one of the four labels defined in Table 9 based on the traffic they generate. The remaining IPs will be labeled dontknow 11 . The nominal precision is set to 95% with respect to the four classes.
Table 9 provides a technical description of the four traffic categories. FLP loosely corresponds to traditional client/server
In addition, some IPs generate more than one type of traf-fic. We label them as dontknow for simplifying the evalua-tion. traffic and the service port is descriptive of which applica-tion generated the traffic. RLP loosely corresponds to P2P traffic on random ports. By running the P2P detector pro-posedin[11]for20days,weconstructedalistofinternal hosts that were involved in P2P traffic. We expect that a high fraction of the internal IPs contacted by source IPs from the RLP category is on this list. Scanning traffic is also verifiable based on a list of ports with known exploits [21]. FLP Fixed Listening Port RLP Random Listening Port scan Random horizontal scanner noise Internet background noise, result of misconfigura-Data. The data set is massive, so to simplify our task, we filtered out IPs that made less than 20 connection at-tempts during this day. After the filtering we were left with 20,885,066 connection attempts by 115,400 distinct external IPs.

Since we do not have ground truth, we withhold from the labeling scheme information regarding the service ports, ports with known exploits and the list of internal P2P hosts. This information is not used by the features or by the heuris-tics. It is only used for determining the  X  X rue X  label. Features and heuristics. We created 9 features, each re-lated to the communication pattern of the source IPs and the success rate of the said communication pattern. Ex-amples include the randomness [22] of the destination port (across connections) and the success rate of the connection attempts. The features do not have access to the list of service or exploit ports. The heuristics are also based on the communication pattern and success rates. For example, scan source IPs are expected to make connection attempts on a very few ports to randomly chosen internal destina-tion IPs. The meaning of the ports or the list of internal P2P hosts is of course not available to the heuristics either. The reader is referred to [17] for the full list of features and heuristics.
 Results. Table 10 depicts the progression of our scheme. For each label, the second column shows the number of source IPs that were assigned the respective heuristic label. 11,193 instances were left unlabeled by the heuristics.
The third column of Table 10 presents the distribution of the temporary labels across the classes (in terms of num-ber of external source IPs). By comparing the third column to the second, we can see the effectiveness of our scheme. The number of source IPs temporary label RLP and scan is higher than the number of source IPs with RLP and scan heuris-tic label (second column). This resulted from the fact that for some clusters, the RLP and scan heuristics assigned the heuristic label to some but not all instances. In such cases, the label was superimposed to all instances of the cluster Table 10: Temporary and heuristic label distribu-tion. Sample size and number of errors found in Phase II.
 Stratum Heuristic Temp. Sample Errors dontknow 11,193 9,999 and hence the cluster structure allowed us to assign a label to some instances not labeled by the heuristics. The num-ber of source IPs with temporary label FLP is lower than the number of source IPs with FLP heuristic labels. This is because some of the instances with FLP heuristic label formed clusters that became too small for one of the follow-ing two reasons: (i) instances with different heuristic labels were nearby (in the feature space) or (ii) these instances correspond to traffic modes that are infrequent. Due to the imperfections in the data collection mechanism, we believe that the precision of the FLP heuristic was not high. The fourth and fifth columns presents the results in Phase II. The fourth column denotes the sample size in the form initial plus verification plus correctional sample size; the fifth column shows the number of errors found during the verifi-cation/correctional sampling.

For the verification sample, 59 instances (from each stra-tum) are sufficient, but we also wanted to estimate the la-beling precision using the Agresti Coull confidence interval, so we took a verification sample of 90 instances from each stratum.
 All true labels in the verification sample coincide for the FLP and scan strata. No correctiona l sampling was required for these strata.
 Two incorrect temporary labels were found in each of the RLP and noise stratum during the verification sampling step. The correctional sample size for the noise stratum is approximately the entire stratum, so all 160 instances needed to be manually corrected.

In case of RLP , the correctional sample size was initially estimated to be approximately 10,000 instances. By set-ting the number of iterations ( I ) to 20, the first correctional sample contained 412 instances. Among the 412 instances, 4 additional errors were discovered (port 110 and port 25 traffic). With a total of 6 errors among 412+90 instances, the error rate estimate at the end of the first iteration is in the range of (.003999263, .02957684). The nominal pre-cision requirement is satisfied at this point. We estimate the achieved precision to be between .970 and .996, so the precision requirements are indeed met.
In this section, we justify our results through the use of the domain knowledge we withheld from the algorithm during labeling. This information was the list of service ports, the list of ports with known exploits and a list of P2P hosts.
We found that out of the 70,744 external source IPs la-beled as FLP , only 910 made connection attempts on ports that are not well known service ports. This corresponds to no less than 98.7% achieved precision. See [17] for a list of these ports and the distribution of the source IPs across these ports.
Out of the 8849 IPs labeled as scan , we found that only 94 made connection attempts to ports not listed as frequently scanned ports by [21]. This corresponds to a precision of 98.94%. The list of ports (and the number of source IPs scanning for them) is listed in [17].

Recall that the traffic labeled as RLP loosely corresponds to P2P traffic on random listening ports. To assess the la-beling precision of the RLP label, we used the list of internal hosts that the P2P detection algorithm in [11] detected over an observation period of 20 days. The list contains 85,343 internal IPs, which corresponds to 26% of the campus IP space. 25637 of 25648 external source IPs made connection attempts to a set of internal destination IPs, in which at least 90% were on the list. The fact that 90% of the des-tination IPs were P2P hosts on a network which has only 26% P2P hosts is strong evidence that these source IPs were involved in P2P traffic.

IPs with the noise label had to be manually labeled during the correctional sampling. 7 of the 160 IPs were found incorrectly labeled.
In this paper, we presented a scheme that can label un-labeled data sets with guaranteed precision specified by the user. We found that for lower-precision high-recall heuris-tics, our scheme improves precision at the cost of recall. Conversely, for high-precision low-recall heuristics, it im-proves recall. It is also a noteworthy property of our scheme that, theoretically, it becomes increasingly effective as the number of instances increases. The verification sample size does not depend on the number of instances. Theoretically, the initial sample size, which is proportional to the num-ber of large partitions does not depend on the number of instances in the data set, either, but in practice, larger data sets can contain a larger number of noise instances, which can cause the algorithm to increase the number of partitions, which, in turn, can lead to increased initial sample size.
Our scheme has the following main limitations. First, we have no direct control over the recall. The user can neither explicitly specify a recall requirement with respect to each class, nor can he explicitly specify a maximum number of dontknow labels. In general, higher precision leads to lower recall and possibly higher number of dontknow instances, but direct control over the recalls does not exist. Second, in order for phase two to be effective, the quality of the temporary labeling must be sufficiently high. The quality of the temporary labeling in part depends on the heuristics. Therefore the heuristics must have reasonable performance in terms of recall and precision. Some classes may have mul-tiple modes. While our scheme works well, when the modes are only partially covered, if the heuristics fail to cover some mode in its entirety, we may not be able to label that mode correctly and the recall of th eschememaysuffer. Asdis-cussed before, imprecise heuristics can also lower the recall. Another factor that influences the quality of the temporary labeling is the extent to which the classification problem is consistent with the cluster structure. If the cluster structure is largely inconsistent, our scheme is not helpful.
As we discussed before, our first phase algorithm is re-placeable by existing semi-supervised algorithms as long as they can achieve high precision by identifying less reliable labels and making them dontknow . Our phase one imple-mentationisjustanexampleandweexpectthatmoreef-ficient methods utilizing existing semi-supervised methods could be devised. [1] Kdd cup  X 99 data. [2] L. Atlas and et al. Training connectionist networks [3] L. D. Brown and et al. Interval estimation for a [4] N. Cesa-Bianchi and et al. How to use expert advice. [5] N. Cesa-Bianchi and et al. Worst-case analysis of [6] O. Chapelle and et al. Semi-Supervised Learning .MIT [7] M. de Vivo and et al. A review of port scanning [8] Y. Freund and et al. Selective sampling using the [9] D. Helmbold and et al. Some label efficient learning [10] T. Joachims. Transductive inference for text [11] T. Karagiannis and et al. Transport layer [12] V. Kumar. Data Mining for Scientific and Engineering [13] N. Littlestone and et al. The weighted majority [14] A. W. Moore and et al. Toward the accurate [15] M. Seeger. Learning with labeled and unlabeled data. [16] H. S. Seung and et al. Query by committee. In [17] G. J. Simon. Data Mining Techniques in Network [18] G. J. Simon and et al. Scan detection -a data mining [19] V. Sindhwani and et al. Large scale semi-supervised [20] M. Steinbach and et al. A comparison of document [21] The SANS Institute. Internet storm center. [22] K. Xu, Z.-L. Zhang, and S. Bhattacharya. Profiling [23] R. Yaroshinsky and et al. How to better use expert [24] X. Zhu. Semi-supervised learning survey. Technical
This work was supported by NSF grants #CRI-0551551, #IIS-0308264, and #ITR-0325949. Access to computing facilities was provided by the Minnesota Supercomputing Institute.
