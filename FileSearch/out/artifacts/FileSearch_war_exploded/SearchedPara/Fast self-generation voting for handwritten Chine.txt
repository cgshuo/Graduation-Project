 ORIGINAL PAPER Yunxue Shao  X  Chunheng Wang  X  Baihua Xiao Abstract Inthispaper,afastself-generationvotingmethod is proposed for further improving the performance in hand-written Chinese character recognition. In this method, firstly, a set of samples are generated by the proposed fast self-generation method, and then these samples are classified by the baseline classifier, and the final recognition result is determined by voting from these classification results. Two methods that are normalization-cooperated feature extraction strategy and an approximated line density are used for speed-ing up the self-generation method. We evaluate the proposed method on the CASIA and CASIA-HWDB1.1 databases.
 High recognition rate of 98.84% on the CASIA database and91.17%ontheCASIA-HWDB1.1databaseareobtained.
 These results demonstrate that the proposed method outper-forms the state-of-the-art methods and is useful for practical applications.
 Keywords Handwritten Chinese character recognition  X  Fast self-generation voting  X  Line density equalization  X  Normalization-cooperated feature extraction  X  Modified quadratic discriminant function 1 Introduction The problem of handwritten Chinese character recognition (HCCR) has been investigated over a long time for its potential in many applications. Many methods have been proposed and very high recognition rate has been obtained on most of the databases. In the character normalization stage, many methods have been proposed to reduce the within-class shape variation, including the nonlinear normalization meth-ods [ 1  X  3 ], the bi-moment method [ 4 ] and the pseudo 2D variety of feature extraction methods are proposed [ 6 ]. The stroke direction feature which can be measured from skeleton used due to the high performance and the ease of implemen-tation. Previous studies have shown that the gradient feature [ 11 ] outperforms the other stroke direction features and the Gabor filter feature [ 12 ]. After character normalization and feature extraction, classifiers based on quadratic discriminant functions (QDFs) are usually applied to HCCR. The most popular one is the modified quadratic discriminant functions (MQDF1 and MQDF2) proposed by Kimura et al. [ 8 ]. The MQDF improves the generalization performance via replac-ing the eigenvalues in the minor subspace of each class with a constant. Besides MQDF1 and MQDF2, the pseudo Bayes classifier [ 13 ], asymmetric Mahalanobis distance (MD) [ 14 ], determinantnormalizedQDF[ 15 ]andMQDF3[ 16 ]aresome other improvements of QDF and comparable performance is achieved.

Many pairwise classifiers are proposed for further improving the recognition rate. The compound Mahalanobis function (CMF) method, proposed by Suzuki et al. [ 17 ], combinespairdiscriminationmeasureswithclasswiseMaha-lanobis distance. Gao and Liu [ 18 , 19 ] proposed a LDA-based compound distance to discriminate similar character pairs. They showed that under restrictive assumptions, the previ-ous CMF is a special case of the LDA-based compound distance method. Their experiments demonstrated that the LDA-based compound distance method outperforms the previous CMF methods. Leung and Leung [ 20 ] proposed the critical region analysis method to tackle the problem of similar character classes. Additional features are extracted classifier. Xu et al. [ 21 ] proposed an average symmetric uncertainty-based critical region selection method, and they showed that the critical regions selected by their method contain more discriminative information than by the method proposed in [ 20 ]. Shao et al. [ 22 ] proposed a self-adaptive critical region-based method for similar pair discrimination. This method tries to tackle the problem that the critical region X  X  scale and position may change due to the wide vari-ability of writing styles.

The combination of multiple classifiers can overcome the limitations of individual classifiers and many works have been performed on it [ 23  X  26 ]. An effective method which uses a single classifier to classify multiple deformations of the input image and combines the decisions of these defor-mations as the final decision is proposed for handwritten digit to the writing habits and writing instruments are modeled via a set of geometric transformations (rotation, slant, per-spective and shrink) and two morphological operations of dilation and erosion, respectively. However, the variations of handwriting Chinese character are more complex, and these variations usually tend to be local; hence, these meth-ods are not suitable for HCCR problem. In this paper, a fast self-generation voting method is proposed for HCCR. This method first generated a set of virtual samples from the input sample by the proposed fast self-generation method, and then the generated samples are classified by the baseline classi-fier, and the final decision is given by voting. For each input sample x , the center of the generated samples is denoted as c x . Experiment on two similar classes shows that the space constructed by c x is easier to discriminate by PCA and LDA than the space constructed by x . This partially proves that the generated samples lie on the original manifold, and the voting from these generated samples is reasonable and effective.

Speeding up of the self-generation method is inspired by the normalization-cooperated feature extraction (NCFE) [ 29 ] method. The NCFE method is proposed for solving the problem that nonlinear normalization may deform orig-inal character X  X  shape and stroke direction. In this paper, we find that not only the stroke direction but also the line density can be calculated by the normalization-cooperated method.TwomethodsthatareNCFEstrategyandtheapprox-imated line density are used in the fast self-generation method.

Unlike other similar character discrimination methods, the proposed method has a number of desirable properties. The first one is that this method need not determine the confus-ing pairs for training and testing. This makes the proposed method well adapted to the extension of the classes. The second one is that the proposed method need not extract new features or to train new classifiers for similar character discrimination. Finally, this method is very effective, easy to implement and suitable for parallel processing. The pro-posed method achieves the best recognition accuracy on the CASIA database. Experimental results show that the pro-posed method is effective and useful for practical appli-cations. The rest of the paper is organized as follows. An overview of the recognition system is given in Sect. 2 .The fast self-generation method is proposed in Sect. 3 . Section 4 presents the voting and the parameter learning method. The experimental setup and results are given in Sect. 5 . Section 6 gives concluding remarks. 2 Overview of the system The diagram of the proposed HCCR system is shown in generation (FSG) method is used for extracting N g fea-ture vectors from each input sample. Then, the Fisher linear discriminant analysis (LDA) is used for feature dimen-calculated. In the recognition stage, firstly, FSG is used for extracting feature vectors. Then, a coarse classifier and the MQDF are used to selected N c candidate classes. Finally, the proposed fast self-generation voting (FSGV) method is used to give the final recognition result. Details of these meth-ods in this framework will be introduced in the following sections.
 3 Fast self-generation method In this section, a short introduction to the self-generation method for HCCR is first given in Sect. 3.1 . Then, the fast self-generation method is proposed in Sect. 3.2 . 3.1 Self-generation method A distortion model is proposed in [ 30 , 31 ] for generating a large number of virtual training samples from existing ones. The distorted samples are then normalized by a non-linear normalization method, and stroke direction features are extracted on the normalized image. Their experimental results show that the generated samples are helpful for solv-ing the problem of insufficient training samples. This method
Let I ( i , j ) be the original character image and D ( r the distorted image. The distortion is implemented by the mapping functions. r = u 1 ( i , j ) = w n ( d 1 , b 1 ( i )) + k 1 b 2 ( j ) + c = v 1 ( i , j ) = w n ( d 2 , b 2 ( j )) + k 2 b 1 ( i ) + where k 1 and k 2 are shearing slopes, d 1 and d 2 control the and w n is a nonlinear warping function for producing local variations on the size of subpatterns.
 and [  X  0 . 20, 0.20], respectively, d 1 = 0 and d 2 = 0are randomly taken from the interval [  X  1 . 6, 1.6]. The following twononlinearwarpingfunctionsareusedforgeneratingmore virtual samples. The probabilities of using w 1 and w 2 are fixed at 0.8 and 0.2, respectively. w ( d , t ) = 1 w
Figure 2 shows some examples of distorted samples. The first column is the original images and the other columns are distorted images. We can see that the distorted images belong to the same character class with the original one while the shapes are different.

In this paper, the nonlinear normalization method based inal and distorted image. Denote the input image and the malization is implemented by coordinate mapping  X   X   X   X   X   X   X   X   X  s = u 2 ( i , j ) = t = v 2 ( i , j ) = where  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  sum =
Line intervals L x along x-direction and L y along y-direction at ( i , j ) are computed. The line density  X  culated as the maximum of the inverse values of L x and L  X ( i , j ) = where W is the image X  X  width.

On each normalized image N ( s , t ) , the eight-direction gradient feature [ 11 ] is extracted. The gradient at pixel is calculated by  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  g x ( s , t ) = N ( s + 1 , t  X  1 ) + 2 N ( s + 1 , t ) g y ( s , t ) = N ( s  X  1 , t + 1 ) + 2 N ( s , t + 1 )
The gradients are then decomposed into eight chain-code directions as shown in Fig. 3 . Each direction is extracted 8 values by Gaussian blurring and down-sampling. The feature dimensionality is 512. 3.2 Fast self-generation method Suppose we need to generate N g samples, the time used by the self-generation method described in Sect. 3.1 is T ple. As we can see from the experiments, this method is very time consuming. In this paper, the proposed fast self-generation method tries to find the relationships between the N g samples to reduce the generation time. Speeding up of the self-generation method is inspired by the normalization-cooperated feature extraction (NCFE) method. The NCFE method is proposed in [ 29 ] for solving the problem that non-linear normalization may deform original character X  X  shape and stroke direction. This method can extract orientation fea-tures whose orientations are not distorted by the nonlinear normalization but whose locations are adjusted by it.
Denote I ( i , j ) as the input image and N ( s , t ) as the normalized image. Nonlinear mapping functions are s = u ( i , j ), t = v by the gradient of the input image (  X  I  X  x ,  X  I  X  y ) . In the self-generation method, for each input image I ( linear mapping function s = u 1 ( i , j ), t = v 1 ( i , j defined in Eq. 1 . The NCFE method can be used to calculate the gradient of the distorted image from the gradient of the input image. Similarly, the gradient of the normalized image N ( s , t ) can be calculated by the gradient of the distorted image. Joining these two steps together, the gradient of the normalized image can be calculated by the gradient of the input image directly. Denote the input image X  X  eight orienta-image X  X  eight orientation planes as G N { G N 1 ,..., G N orientation planes are calculated by the decomposition of the gradient. Therefore, the orientation planes of the normalized images can be calculated by the orientation planes of the input image directly.

In the mapping from the input image to the distorted image, G D is calculated by G where r = u 1 ( i , j ) = w n ( d 1 , b 1 ( i )) + k 1 b 2 ( j ) + c = v 1 ( i , j ) = w n ( d 2 , b 2 ( j )) + k 2 b 1 ( i ) +
The distortion mapping functions u 1 and v 1 are only deter-mined by the parameter set p { d 1 , d 2 , k 1 , k 2 ,w n p can be designed for calculating the orientation planes of the distorted image quickly. i = LT i ( r , c ), j = LT j ( r , c ), From Eqs. 9 and 11 , the orientation planes of the distorted image can be calculated quickly by the lookup tables. G
In the mapping from the distorted image to the normalized image, G N can be calculated by G where the mapping functions u 2 and v 2 are the nonlinear normalization mapping functions defined in Eq. 4 . Joining Eqs. 12 and 13 together, we get G For mapping from the normalized direction planes G N m to the input direction planes G I m directly, the nonlinear map-first according to the predefined lookup tables LT r ( r , image needs to be computed. The nonlinear normalized method used in this paper uses the line interval along x-direction ( L x ) and y-direction ( L y ) to approximate the diameter of the inscribed circle at each point. And the line density  X  is calculated as the maximum of the inverse values of L x and L y . In this paper, we assume that the line density to ( r , c ) by the distortion. This assumption is based on the statistics of the line density on the CASIA databases. image and distorted image, respectively. We want to use the  X   X  Define ( r , c ) as the rate of change of the line density at ( r , c ) =
Figure 4 shows the histogram of over all training images on the CASIA databases and its corresponding cumulative distribution. From this figure, we can see that about 97% is small than 0.26. These pixels with bigger than 0.25 are referred to as the unstable pixels. Figure 5 shows these unstable pixels with pink.

However, as shown in Fig. 6 , these unstable pixels have little effect on the normalization result. The value in  X  visualization. From these results, we can see that the approx-imated line density  X  D can be used to calculate the mapping function from the distorted image to the normalized image. This will save a lot of computing time because we only need to calculate the line density of the input image, and the line density of the distorted images can be got by the lookup tables.

Figure 7 shows the framework of the FSG method. In the preprocessing step, the maximum width or height of the input image is normalized to 64 with aspect ratio unchanged and the normalized image is placed in the center of a 72*72 back-ground image. And then the line density L 0 and blurred ori-entation planes P of the preprocessed image are calculated. The feature vector x 0 can be calculated by NCFE using L 0 and P . Finally, for each pair of lookup tables LT i and LT the line density L i is calculated from L 0 and the orientation planes P i is calculated by Eq. 14 using L i and P . The gener-and in the recognition stage, x 0 will be used for candidate class selection by the coarse classifier and MQDF, and all these samples are used by the fine classifier FSGV to give the recognition result.

Samples in class 2 and 3 in the CASIA database are used to illustrate the effectiveness of the generated samples. These samples are illustrated in Fig. 8 . The center of the gener-ated set from each original sample x is denoted as x c .The generation parameters are learned by Algorithm 1 which is given in Sect. 4 .InFigs. 9 and 10 , the space constructed by all x c is referred to as the new space. PCA and two-class LDA are applied on the original space and the new space for visualization. From Figs. 9 and 10 , we can see that the within-class scatter of the new space is smaller than the scat-ter of the original space and some samples which cannot be discriminated by PCA and LDA in the original space can be discriminated by them in the new space. This phenomenon is caused by two reasons. The one is that some generated samples tend to enhance the critical region which is very useful for discriminating similar characters. The other one is that the parameter learning algorithm helps to improve class separability. 4 Fast self-generation voting The generated samples are only used for training in [ 31 ]. In this paper, the generated samples are not only used for training, but also used for voting. The fast self-generation method makes this strategy useful for practical applications.
The MQDF has been widely applied to HCCR with great success. Denote the input character pattern by a  X  i is assumed to have a Gaussian density p N ( X  ance matrix, respectively. Assuming equal a priori class probabilities, the discriminant function is given by the log-likelihood  X  2log p ( x |  X  To alleviate the problem of inaccurate estimation of the covariance matrix, modified QDFs (MQDF2) is adopted. where h 2 is a constant, and  X  i and  X  i , i = 1 ,..., n are the eigenvalues and eigenvectors of the covariance matrix of class  X  j .
 Denote { x 1 ,..., x N v } as the generated samples, the MQDF distance between the generated samples and class  X  j is tance h j ( x ) as the distance between the generated samples and class  X  j . h ( x ) = g where x 0 is the original feature vector extracted from the input character image. This method which uses the fast self-generation method to generate a set of samples and uses the voting of the generated samples X  MQDF distances as the fine classifier is referred to as the fast self-generation vot-ing (FSGV) method. As showing in the framework of the system, a coarse classifier and the MQDF are first used to select N c candidate classes, and then the FSGV is used to give the final result. The coarse classifier used in our system is the Euclidean distance (ED) between the test sample and the class means.

In the FSGV method, the parameters for generating the samples { x 1 ,..., x N v } and the corresponding weights need to be determined first. From the FSG, we can see that a ated sample x i . p The goal of the learning method is to learn a set of parameter sets P { p 1 ,..., p N v } from the candidate set S and its corresponding weights  X  {  X  1 ,..., X  N v } that perform best on the validation set. In this paper, just for testify-ing the effectiveness of the proposed method, we randomly generated 74 parameter sets S p { p 1 { d 1 1 , d 1 2 , k p learning.

Algorithm 1 gives the learning method used in our exper-{ 0 . parameter sets for FSGV, and is the null set. f 1 ,..., f parameter set selected in the iteration t . best _ p = means that no extra parameter sets are helpful to the already learned parameter sets.

In the parameter learning stage, the training samples are divided into the training and validation set. In each database, the last 10 samples in the training set of each class are used for validation. In Algorithm 1, for calculating the recognition rate on the validation set, the parameter k in MQDF2 is set to 50 and the candidate set number N c is set to 10. The learned parameter sets P and  X  are then used in our experiments. 5 Experimental results We evaluated the proposed method on the CASIA database and the CASIA-HWDB1.1(DB1.1 in brief) [ 32 ]. The CASIA database, which is collected by the Institute of Automation, Chinese Academy of Sciences, contains 3,755 Chinese char-acters, 300 samples per class. Two hundred and fifty samples per class are used for training and the remaining 50 sam-ples for testing. The CASIA-HWDB1.1 database is built by the National Laboratory of Pattern Recognition, Institute of Automation of Chinese Academy of Sciences. It contains 3,755 Chinese characters, almost 300 samples per class. As suggested by the organizers, the first 240 samples are used for training and the remaining 60 are used for testing. Some samples in the two databases are given in Fig. 11 . The first three rows are taken from CASIA and the remaining is taken from CASIA-HWDB1.1.

Firstly, the recognition rate and recognition time of FSG and SG were compared. Table 1 presents the time used for generating varying number of samples by FSG and SG. In this table, times for extracting the original feature vector x and generating N g samples ( x 1 ,... x N g ) were clocked. We can see that the SG method needs about 1.6ms for generating one sample, while the FSG method only needs about 0.38ms. Table 2 presents the recognition rate and recognition time for each character with N v = 20 , k = 100 and Fisher X  X  dimension 350. These results show that FSG method not only runs faster but also performs better than SG method. The NCFE strategy brings these benefits which is identical to results reported in [ 33 , 34 ]. The program ran on a normal PC with 2 CPU and 4G memory.
The recognition rate improves from 88.02 to 89.87% on the CASIA-HWDB1.1 database and from 98.21 to 98.52% on the CASIA database, respectively, by using the generated samples to train the MQDF classifier (Fisher X  X  dimension is 350). These results and results in [ 20 , 31 ] show that the generated samples are helpful to train a more steady clas-sifier. In this experiment, we evaluated the performance of the FSGV and the MQDF trained using the generated sam-ples to show the effectiveness of these generated samples in recognition. In this experiments, all the generated sam-ples were used in FSGV and all the voting weights were set to 1. The candidate classes N c were set to 10. Table 3 and Table 4 presenttherecognitionratesofthesemethodswithout applying LDA and applying LDA (Fisher X  X  dimension is set to 350), respectively. We can see that FSGV performs better than MQDF. This demonstrates that the generated samples are not only helpful to train a more steady classifier but also useful in recognition. Furthermore, using the Fisher X  X  dis-criminantalsoleadstosomeimprovement.Arecognitionrate of 98.83% on CASIA and 91.10% on CASIA-HWDB1.1 is achieved.

Then by applying the parameter learning method, the recognition rates with varying number of N v are presented in Tables 5 and 6 . A recognition rate of 98.84% on CASIA and 91.17% on CASIA-HWDB1.1 is achieved. From these a comparable rates to the results in Tables 3 and 4 . Figure 12 shows the recognition rate of different N v with k = 125 and Fisher X  X  dimension 350 on the two databases. It can be observed that the recognition rate increases slowly or even decreases while N v reaches 20. This tells us that we do not need to generate too many samples to get a better perfor-mance.

Finally, the results of state-of-the-art methods on the two databases are listed in Tables 7 and 8 , respectively. The linear discriminant analysis (LDA)-based compound dis-tances is an extension of previous compound Mahalanobis function (CMF). This method projects two similar classes onto a discriminant vector and use the compound distance of MQDF and the Euclidean distance (ED) or Mahalanobis distance (MD) in the projected one-dimensional subspace to discriminate the similar pair. It achieves 98.57% on the CASIA database. The average symmetric uncertainty (ASU)-based critical region selection method achieves the highest recognition rate 98.70% on the CASIA data-base. The proposed method performs a little better on this database, with the best recognition rate 98.84% on this database. In paper [ 33 ], some state-of-the-art methods are evaluated on the CASIA-HWDB1.1, which establishes a benchmark for further research. The implemented meth-ods include advanced character normalization methods [ 5 ], feature extraction methods [ 11 , 34 ], as well as classifier CASIA-HWDB1.0 is another database released at the same time with CASIA-HWDB1.1. The accuracy on the test set of CASIA-HWDB1.1 is further improved from 89.55 to 90.71% using merged training data of databases CASIA-HWDB1.0 and CASIA-HWDB1.1. The proposed method obtains a recognition rate of 91.17% on this database; how-ever, the recognition performance on this database reveals a big challenge to attack. 6 Conclusion A fast self-generation voting method is proposed for further improving the recognition rate in handwritten Chinese char-acter recognition. In this method, firstly, a set of samples are generated by the proposed fast self-generation method, and then these samples are classified by the baseline classi-fier, and the final recognition result is determined by voting from these classification results. The proposed method was evaluated on the CASIA and CASIA-HWDB1.1 databases. A high recognition rate of 98.84% on CASIA and 91.17% on CASIA-HWDB1.1 is obtained. These results demonstrate that the proposed method is effective and useful for practical applications.

In our future work, three aspects will be concerned: firstly, other normalization methods will be tried to add into the fast self-generation framework to check which is better. Sec-ondly, in our experiments, the candidate parameter set S p was randomly selected, and we will try to determine the best combination of these parameters. Finally, the self-generation method in this paper is unsupervised, and the supervised self-generation method will be concerned.
 References
