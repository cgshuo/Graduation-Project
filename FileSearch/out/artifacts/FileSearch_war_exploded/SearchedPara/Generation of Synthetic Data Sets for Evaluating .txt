 Information Discovery and Analysis Systems (IDAS) are designed to correlate multiple sources of data and use data mining techniques to identify potential significant events. Application domains for IDAS are numerous and include the emerging area of homeland security. Developing test cases for an IDAS requires background data sets into which hypothetical future scenarios can be overlaid. The IDAS can then be measured in terms of false positive and false negative error rates. Obtaining the test data sets can be an obstacle due to both privacy issues and also the time and cost associated with collecting a diverse set of data sources. In this paper, we give an overview of the design and architecture of an IDAS Data Set Generator (IDSG) that enables a fast and comprehensive test of an IDAS. The IDSG generates data using statistical and rule-based algorithms and also semantic graphs that represent interdependencies between attributes. A credit card transaction application is used to illustrate the approach. D.2.5 [ Testing and Debugging ]: Testing Tools . D.2.4 [ Software/Program Verification ]: Statistical Methods, Validation. I.2.4 [ Knowledge Representations Formalisms and Methods ]: Semantic Graphs. Algorithms, Design, Verification Information Discovery, Data Mining, Data Generation. Modern data collection methods in combination with the rapid increases in information technology have made possible the assembling of extensive data sets. Information Discovery and Analysis Systems (IDAS) form an important tool for turning large quantities of data that have been collected into information that can be used. IDASs are designed to correlate multiple sources of data and use data mining techniques to find relationships within disparate data sets that could be used to predict events. An IDAS extracts information from data by finding patterns, threads, and relationships. To do this, IDASs use a hybrid of statistical and artificial intelligence methodologies such as pattern recognition, classification, categorization, and learning, and employ data mining tools based on neural networks, Bayesian networks, classifications schemes, and regression models, among others, to cull out and aggregate information across different data sources. IDASs have been a major asset to business applications such as fraud prevention [2,7] and are in use in the medical field in a wide variety of applications including help in diagnosis [5] and analysis of medical videos [13].A recent survey by the US General Accounting Office found that 52 Federal agencies are conducting or plan to conduct 199 separate data mining efforts, with 131 of these currently operational [4]. It is believed that IDASs could be equally effective for intelligence applications such as providing leading indicators of terrorist acts. A critical technical issue with IDASs is their ability to provide accurate inference. Given the diversity of techniques used to develop an IDAS, it is desirable to have a baseline approach for testing their ability to make accurate inference as well as their ability to deal with large input data sets with varying degrees of accuracy. An important part of testing of IDASs is the generation of synthetic data sets for use in test cases. Developing test cases for an IDAS requires background data sets into which hypothetical future scenarios can be overlaid. The IDAS can then be measured in terms of false positive and false negative error rates. Obtaining the test data sets can be an obstacle due to both privacy issues and also the time and cost associated with collecting and customizing multiple instances of a diverse set of data sources. In this paper, we give an overview of the design and architecture of an IDAS Data Set Generator (IDSG) that will enable a fast and comprehensive test of an IDAS. This IDSG is currently under development for the Department of Homeland Security (DHS) by Lucent Technologies and the University of California, Riverside The specific goal is to design and develop a prototype system to explore the feasibility of synthesizing data sets for testing the effectiveness of IDASs designed to uncover potential threat scenarios. Such a data synthesis capability is an attractive alternative for testing an IDAS when realistic data sets are not readily available due to privacy or restrictive access issues, or the challenge and costs of amassing diverse data from multiple sources is imposing. Further, a synthetic data capability allows various features and capabilities of an IDAS to be exercised in multiple test cases, thus permitting a meaningful figure of merit of its efficacy to be established. There are several challenges in the generation of IDAS test data in general, and for the DHS project in particular. General challenges include: fields with non-trivial correlations between fields. continuous, nominal, ordinal, textual, or image data. available data, the type needed to quickly develop test cases, typically are presented in two or at most three dimensions. DHS specific difficulties include: the Inspector General issued a report finding that the Defense Advanced Projects Agency (DARPA) had not adequately considered privacy concerns associated with the Total Information Awareness Program terrorism data mining project [3]. DHS precludes the direct interaction with the client to identify relevant data fields. training data for evaluating the effectiveness of the data synthesis tools are not available. The problem of developing a data set generator that can create realistic data for all possible IDAS data input types for all possible IDASs is formidable, and is not our goal. Instead, our approach is to synthesize data sets of sufficient quality to enable IDAS evaluation. Our vision is an experimental platform that generate data sets containing user-designed threat scenario information as well as background data whose quality can be increased as needed by imposing relationships, correlations, and other statistical conditions on the data attributes within or across data sets. Examples of generation of synthetic data through various means for testing purposes are available in many areas of research. For example, synthetic data sets were generated for testing a Robust/Resistant crystallographic refinement procedure using a mixture model for different types of random errors [8]. Synthetic data has been generated for use in  X  X round truth X  testing of different protein spot detection software packages through the use of a Gaussian mixture model obtained from training data [9]. Synthetic handwriting data have been generated using random perturbations of actual handwriting for use in testing cursive handwriting recognition systems [10]. The US Census Bureau is exploring the use of synthetic data as a means of protecting the confidentiality of the public while maintaining statistical quality [1]. In multivariate cases, particularly where there are complex covariance structures requiring an understanding of the interdependencies between variables, the data synthesis process becomes more complex. Central to the approach in this paper is the development of an object-oriented information model to represent data objects that readily permit imposing additional data structure and conditions on data attributes. Our synthetic data generation design is based on the use of semantic graphs. For this project, a semantic graph is defined as a structural representation of the interrelationships between data fields of interest. Semantic graphs have been used in various fields for summarizing complex relationships between multiple factors. For example, document summaries have been produced using semantic sub-graphs to represent the various characteristics of the document but in a vastly reduced size [6]. Complex relationships between pairs of nouns in sentences have been summarized using automatic graph algorithms [11] as well. A key aspect of our synthetic data generation research is to provide a scalable design that can be adapted to increase data quality, add data types, and address computational constraints dictated by our users. The concept of a semantic graph for knowledge representation has shown to be very helpful in this regard. The rest of this paper is organized as follows. Section 2 describes the general architecture used in our data generation approach. Section 3 provides a few illustrative applications of our proposed design, with an example drawn from generating credit card transaction data. Section 4 summarizes the principal findings to-date of this project. To better demonstrate the functionality of IDSG, we present a  X  X ypical X  user scenario in Figure 1. The example of Figure 1 is for a credit card holder/transaction data set. The user initially enters information on the type of data set, and the tables, steps 1-5 in Figure 1. For example, the user may select two tables, CardHolder and CardTransaction, with 1-to-many relationship between the CardHolder and the CardTransaction. In Steps 6-13 the system provides the user with a list of pre-defined fields/attributes from which the user can select. For example, the table CardHolder has fields: Name, Address, Telephone Number, Date-of-Birth, Gender, Income, Card Number, and Customer since Date. The CardTransaction table includes fields Card Number, Transaction Date, Transaction Amount, and Purchased Item. In addition to the pre-defined fields, an IDSG Wizard will allow the user to define new fields. On the Wizard screen, a partially constructed semantic graph will appear (Figure 2). The graph is partially constructed since some of the attributes or relationships may not have been predefined and thus need to be specified by the user. For example, the relationship between Age and Income may already exist, but the user may decide to add an additional relationship between Gender and Income. This will add an additional link to the semantic graph between the Gender and Income attributes and open a window for specification of the type of relationship. Alternatively, the user may determine to maintain the structure of the graph (nodes and edges) and only modify parameters. For example, the user may decide to change the parameters for Gender distribution from 50-50 to 40-60, if the ratio of Male-Female credit card holders is 40-60. Once the above two sets of information are provided, IDSG will build the Data Set Schema. For the schema, the user needs to input the multiplicity relationship between the tables. In this example, the user needs to specify how many transaction records per CardHolder/card to generate. For this, IDSG will provide a list of probability distributions to select from. Once the schema and the semantic graph are constructed, the user provides the information on other parameters. For example, the number of records in each table, the duration of time for the events, the geographic area (for addresses), the nationality (for names), and the type of items purchased (eg, airline, hotels, and car rentals only). The above parameters further constrain the type of data generated. Once the parameters are input, the IDSG data generator module can generate the data sets and store them in some specific format. Initially, this format is a comma separated file. A key step in our methodology is the use of a semantic graph to represent dependency relationships among different data sets. While semantic graphs typically are used to summarize pre-existing data, in our methodology the semantic graph serves as a guide for the creation of the synthetic data. The semantic graph is also used to illustrate the data generation sequence to be used in the synthesizing process. The first phase of the data generation process is the construction of the semantic graph. A key assumption in our methodology is that semantic graphs can adequately capture the relationships within the data. The IDSG tool uses a semantic graph that describes these data relationships. The basic elements of our semantic graphs are data attributes and directed dependencies. Ovals in the semantic graph represent various data attributes, and arrows indicate the dependency relationship between two data attributes. The number of arrows coming into a data attribute dictates the dimension of an inner table that holds information about conditional probability distributions for the data attribute. The conditional distribution of the data attribute is theoretically specified once the values of all of the upper level data attributes are known. Data attributes without incoming arrows as illustrated in Figure 3a will be assigned a distribution from either the system or the user. Since this type of data object does not have incoming arrows, they are independent of other data attributes. The generation of this type of data attribute is done with simple random sampling. 
Figure 3. a) No incoming dependencies, b) single incoming To generate a data object with a single incoming arrow, such as Figure 3b we will assume its upper level data attribute A is already generated and takes the value a . Then we can generate a realization of data attribute B according to the conditional probability distribution of B , given Aa = , which is specified in the inner table associated with data attribute B . To generate a data attribute with multiple incoming arrows, such as Figure 3c we need the conditional distributions of C , given all the possible combinations of values for A and B . The set of conditional distributions for C , given A and B , is shown in Table 1. Table 1 would, in theory, be the inner table associated vector associated with the k levels of C , given i Aa = and Unfortunately, high dimensional distribution tables are difficult to obtain in practice without doing extensive customized survey work. The type of information that is more easily found is the set of (weighted) row and column averages shown in Table 1 as 
V distributions of C , given A and given B , respectively. In Section 2.3, we describe an approximation to ij  X  (1 ) YV W  X  X  =+ X  , for 0 1 ij  X   X  X  X  . Clearly  X  ij Y is also a probability vector, and the fact it depends only on the more easily obtained row and column averages of Table 1 make the estimator useable. Given i Aa = and j B b = , a value of for C is then generated from the distribution  X  ij Y . In the next section, we elaborate on what value to use for the mixing parameter ij As described in the previous section, we propose to approximations are an important part of our IDSG since obtaining tables such as Table 1 is difficult in practice, where as obtaining information such as {} motivation for approximations of the form  X  (1 ) ij ij i ij j increases when higher dimensional semantic graphs are utilized. then a 3-dimensional table with entries ijk Y would be needed. Here, ijk Y would correspond to the probability distribution for D , given i Aa = , j B b = and k Cc = . Such a table would be extremely difficult to obtain. However, is is quite conceivable that row, column and planar averages {}
X  X  (1 ) YVW X  X  X   X  X  =+ + X  X  X  could be used to approximate ijk Y . For the 3-dimensional case, there are two mixing parameters 0 , 1 ijk ijk  X  X   X  X  X  . In the next section, we describe our methodology for determining the values of mixing parameters, describing the approach for the case two dimensions where a single parameter 0 1 ij  X   X  X  X  is required. Extensions to the higher dimensional cases are straightforward. The weighted row and column averages shown in Table 1 are functions of weights derived from the joint distribution for A and B , which is shown in Table 2. Here, Pr ( , ) ij i j pAaBb === . It follows that the weighted row and column averages shown in Table 1 can be expressed as In general applications, the mn probability vectors ppp p = K will not be known. We model the ij vectors as independent identically distributed k -dimensional uniform Dirichlet random vectors, and p as a mn -dimensional uniform Dirichlet random vector that is independent of all the In some sense, our model corresponds to a Bayesian non-informative prior, though our use of the model is not within the typical Bayesian paradigm. Rather, we use this approach to define a structural characterization of the ij Y and we can optimize against. While we cannot expect the Dirichlet assumptions to hold across all applications, it can be argued that it is the most reasonable thing to do in the absence of any other information. value of ij  X  by minimizing the total error sum of squares QY V W  X  X  = X   X  X  X  . It is easy to show that the minimizing ij  X  , over the interval (0,1) is the value The quantity  X  ij  X  is a random variable, varying through the Dirichlet distributions used for the ij Y and p . The distribution of  X   X  depends on m , n and k , but does not depend on either i or j . Our choice for the mixing parameter is the mean value of  X  which we obtain numerically by the following algorithm: 1) Simulate mn independent realizations of 2) Simulate a one realization of p from an mn -dimensional 3) Compute  X  ij  X  4) Repeat steps (1)-(3) a large number (e.g., 10,000) times 5) Compute the average of the generated values of  X  ij Table 3 shows the mean values for  X  ij  X  for selected choices of m , n and k . Similar tables can easily be built for higher dimensional problems. For example, in the case where three inputs A , B and C drive the value of a 4 th variable D , the table would have columns and k is the number of levels of D . The algorithm described above is easily extended to compute the mean values of the two mixing parameters  X  ijk  X  and  X  ijk  X  . We now illustrate the procedure described in the previous section with a simple example involving expenditure patterns as a function of Age and Gender. In this example, Age and Gender correspond to the variables A and B , respectively. The variable C , expenditure type, is a two-level categorical variable that identifies an expenditure as either  X  X ecessary X  (N) or  X  X nnecessary X  (U). Figure 3c is an appropriate semantic graph for this example, and we have m =3, n =2, and k =2. Our objective is to simulate a value for C , given we have already simulated values for A and B . In order to implement our methodology, we need the two tables shown in Table 4 and Table 5. Table 4 gives the proportion of N and U expenditures for each gender, and Table 5 gives the proportion of N and U expenditures for each of the three age groups. Table 4. Probability of N and U Expenditures For Gender Table 5. Probability of N and U Expenditures For Age Group Suppose we wanted to simulate expenditure type for a Male 
Aa = ) in Age Group 1 ( 1 B b = ). Referring to Table 3, we find the mixing parameter for (,,) (2,3,2) mnk = is 0.414 . We would thus estimate 11 Y using A simulated value of C , given 1 Aa = and 1 B b = , is then obtained by randomly according to the probabilities given by 111 The IDSG must be capable of outputting generated synthetic datasets in portable formats. These data sets may be stored in volumes of CDs or DVDs. The datasets will be dynamically generated based on user specified parameters with potentially very large size which means that portability, simplicity, and flexibility are the three main requirements. Storage of output data in a database, in flat files/CSV, or XML are some of the main choices. Storage of the output in a database has enough flexibility to allow the user to preserve all data relationships and also to optimize the dataset size. But an Application Programming Interface (API) will be required for the IDAS, which increases the complexity and time of system development. We have chosen comma separated values (CSV) as the initial choice of IDSG for the output format since it is portable, simple and flexible. However, this requires the output of all data in flat format to preserve the data relationships. This will increase the dataset size significantly. This advantage can be offset by cheap storage media and requiring only one-time importing to an IDAS. Future versions of IDSG may include database formats as output. This decision will be made at some point in the future when we have more knowledge of user X  X  IDAS and more development time. In order to generate names, IDSG uses a names database to sample from. One of our sources for names is the Census Bureau. The names are then stored in the master database as male/female first name, and last names in separate files. Separate files are created for each nationality. When the user runs the generator, IDSG will go through the database and randomly selects a last name, then combine it with either a first name from the male list or the female list to generate a full name. The user can further constrain the names to specific ethnicity by appropriate selection. The generator will go through the database and combine last name files from specific ethnicity with first name files to generate the desired data. This process can be illustrated by using a semantic graph (Figure 4). To generate realistic credit card numbers, we use the semantic graph shown in Figure 5. The first digit on a credit card is the Major Industry Identifier (MII) which represents the source from where the credit card was issued. There are ten issuer categories, but for our purpose we will only use four of them for randomly generating a credit card number. For example, a credit card number starting with 6 is assigned for merchandising and banking purposes, such as in the case of The Discovery card. Credit card numbers starting with 4 and 5 are used for banking and financing purposes, as in the case of Visa and MasterCard. The last issuer category we used is 3 to represent travel and entertainment cards, for instance the American Express card. Table 9 is an overview of the rules for numbering credit card. The first six numbers including the MII represents the issuer identifier. The rest of the digits on the credit card represent the cardholder X  X  account number. The credit card number generation process begins by randomly generating the card type (Master, Visa, American Express, or Discovery) depending on the probability distribution for the type of cards in circulation. After a card type is picked, an appropriate MII number is assigned to the card, which includes a randomly sampled number, varying in length from 2 to 5 digits long, depending on the issuer type. The final step in the process is to generate a random string of numbers with length depending on the card type is generated. IDSG uses the Luhn algorithm to check whether or not the credit card number we randomly generated is a valid number or not. To use this algorithm, IDSG first doubles the value of alternating digits of the card number, starting with the first digit. Next, it subtracts 9 from those numbers that are greater than 9. Finally, it sums the transformed numbers together and then divides by 10. If there are no remainders, the card number is considered valid. In this section, we illustrate the generation of one particular aspect of a credit card purchase transaction. Our example makes use of Figure 3c which shows how expenditure category (variable C ) depends on age group (variable A ) and income level (variable B ). Expenditure category has 7 levels: Food, Housing, Apparel, Transportation, Health Care, Entertainment, and Other. Table 7 shows the probability of each expenditure category as a function vectors in Section 2. Table 8 shows the probability of each expenditure category as a function of Income Level. The columns 7and 8 were obtained from the U.S. Census Bureau. To simulate an expenditure category for a 30-year old person with an Income Level of $50,000, we use the algorithm described in parameter is 0.47. The estimated probability distribution for expenditure category is thus 28 2 8  X  0.47 (1 0.47) YV W =+ X  . The elements of i V represent the estimated probabilities of each expenditure category for a 30-year old person with an income of $50,000. A uniform random number generator can be used to randomly pick one of the categories according to the probabilities 
Y . Table 7. Detailed Joint Distribution of Expenditure by Age We have described a design and architecture for a synthetic data generation tool that can facilitate building test cases for IDAS tools. Our tool enables an IDAS developer to overcome time and cost issues associated with gathering real data to build test cases by making synthetic data available. Synthetic test data of sufficient quality is a viable economic alternative to gathering real data sets, which in many cases might not even be possible. The development interval for our tool extends through the calendar year 2005. We anticipate our data generation platform will play an important role in comparing the effectiveness of different IDASs. By overlaying specific scenario on our background data sets, an IDAS evaluator can measure competing IDASs in terms of false negative and false positive error rates. [1] Abowd, J.M. and Lane, J.I. Synthetic Data and [2] Chan, P.K., Fan, W., Prodromidis, A.L., and Stolfo, S.J. [3] Department of Defense, Office of the Inspector General. [4] General Accounting Office, Data Mining: Federal Efforts [5] Kusiak, A., Kernstine, K.H., Kern, J.A., McLaughlin, K.A., [6] Leskovec, J. Grobelnik, M., and Millic-Frayling, N. Learning [7] Ormerod, T., Morley, N., Ball, L., Langley, C., and Spenser, [8] Prince, E., and Nicholson, W.L. A Test of a Robust/Resistant [9] Rogers, M. Graham, J., and Tonge, R.P. Using Statistical [10] Varga, T. and Bunke, H. Generation of Synthetic Training [11] Widdows, D. and Dorow, B. A Graph Model for [12] Yun, W.T., Stefanova, L., Mitra, A.K., and Krishnamurti, [13] Zhu, X., Aref, W.G., Fan, J., Catlin, A.C., and Elmagarmid, 
