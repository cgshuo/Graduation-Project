 Entity disambiguation is the task of mapping ambiguous terms in natural-language text to its entities in a knowledge base. It finds its application in the extraction of structured data in RDF (Resource Description Framework) from textual documents, but equally so in facilitating artificial intelligence applications, such as Seman-tic Search, Reasoning and Question &amp; Answering. We propose a new collective, graph-based disambiguation algorithm utilizing semantic entity and document embeddings for robust entity disam-biguation. Robust thereby refers to the property of achieving better than state-of-the-art results over a wide range of very different data sets. Our approach is also able to abstain if no appropriate entity can be found for a specific surface form. Our evaluation shows, that our approach achieves significantly (&gt;5%) better results than all other publicly available disambiguation algorithms on 7 of 9 datasets without data set specific tuning. Moreover, we discuss the influence of the quality of the knowledge base on the disambigua-tion accuracy and indicate that our algorithm achieves better results than non-publicly available state-of-the-art algorithms.  X  Information systems  X  Information extraction; Information systems applications; Entity Disambiguation; Neuronal Networks; Embeddings
Entity disambiguation refers to the task of linking phrases in a text, also called surface forms, to a set of candidate meanings, referred to as the knowledge base (KB), by resolving the correct semantic meaning of the surface form. It is an essential task in combining unstructured with structured or formal information; a prerequisite for artificial intelligence applications such as Semantic Search, Reasoning and Question &amp; Answering. While entity dis-ambiguation systems have been well-researched so far [19, 14, 5, 12, 15], most approaches have been optimised to work on a partic-ular type of disambiguation task, like for example on short Twitter messages [2], web pages [12, 28], news documents [10, 15], ency-clopedias [16, 11, 5], RSS-Feeds [9] etc. While most authors report to outperform other entity disambiguation algorithms on their do-main/data set, they do not achieve comparable accuracy on other domains. So their approaches could be considered as not being very robust against different types of data sets.

In our work, we focus on robust entity disambiguation, where robustness is defined as achieving high accuracy over a large set of different domains. The only assumption we make is to disam-biguate entities collectively, i.e. we disambiguate all entities in a given textual document at once. Our approach is based on creat-ing a k-partite relatedness graph between all entity candidates for all given surface forms in a document. The relatedness between entity candidates is determined using semantic embeddings, i.e. real-valued n-dimensional vectors capturing the semantics of en-tities. We use two types of semantic embeddings, namely embed-dings capturing the meaning on the word (entity) level and semantic embeddings capturing the meaning on the document level. As we show, both embeddings are important for our approach.

In particular, we provide the following contributions:
The remainder of the paper is structured as follows: In Section 2, we review related work. Section 3 introduces the problem formally and outlines our approach. Sections 4 and 5 explain the process of generating semantic embeddings and our approach. In Section 6 and 7, we describe the experimental setup and the results achieved on 9 data sets. We conclude our paper in Section 8.
Entity disambiguation has been studied extensively in the past 10 years. One of the first works defined a similarity measure to com-pute the cosine similarity between the text around the surface form and the referent entity candidate X  X  Wikipedia page [4]. The pub-licly available framework DBpedia Spotlight [19] for disambiguat-ing Linked Data Resources is also based on the vector space model and cosine similarity. Cucerzan et al. introduced topical coherence for entity disambiguation [7]. The authors used the referent entity candidate and other entities within the same context to compute topical coherence by analyzing the overlap of categories and in-coming links in Wikipedia. Milne and Witten [22] improved the ex-ploitation of topical coherence using Normalized Google Distance and unambiguous entities in the context only. A well-known pub-licly available system is Wikifier [25, 5] from 2013. It incorporates, along with statistical methods, richer relational analysis of the text. In 2014, the authors Guo et al. [10] proposed the use of a proba-bility distribution resulting from a random walk with restart over a suitable entity graph to represent the semantics of entities and documents in a unified way. Their algorithm updates the semantic signature of the document as surface forms are disambiguated.
Other works explicitly make use of topic models to link sur-face forms to a KB. For instance, Kataria et al. [16] proposed a topic model that uses all words of Wikipedia to learn entity-word associations and the Wikipedia category hierarchy to capture co-occurrence patterns among entities. The authors of [11] also pro-pose a generative approach which jointly models context compati-bility, topic coherence and its correlation.

Several other work focus on graph-based algorithms. For in-stance, publicly available, graph-based disambiguation approaches are AIDA [14], Babelfy [23], WAT [24] and AGDISTIS [28]. AIDA is based on the YAGO2 KB and relies on sophisticated graph al-gorithms. This approach uses dense sub-graphs of the underlying KB to identify coherent surface forms using a greedy algorithm. In 2014, Babelfy [23] has been presented to the community. It is based on random walks and densest subgraph algorithms. In contrast to our work, Babelfy differentiates between word sense disambiguation, i.e., resolution of polysemous lexicographic enti-ties like play, and entity disambiguation. The WAT [24] system is a redesign of TagMe [8] components and introduces two dis-ambiguation families: graph-based algorithms for collective entity disambiguation and vote-based algorithms for local entity disam-biguation [29]. Finally, AGDISTIS [28] is based on string similar-ity measures and the graph-based Hypertext-Induced Topic Search algorithm. AGDISTIS disambiguates named entities only and ex-clusively relies on RDF-KBs like DBpedia or YAGO2. All these collective disambiguation approaches rely on graph algorithms but mostly compute the coherence measure with the help of relations between entities within KBs (i.e. DBpedia, YAGO2).

Another graph-based approach was presented by Alhelbawy et al. [1], who applied the PageRank algorithm to a disambiguation graph. To compute the edge weights between entity candidates the authors either used a boolean relation whether two entities refer to each other or estimated a probability of both entities appearing in the same sentence. The authors Han et al. [12] proposed the graph-based representation called Referent Graph, which models the global interdependence between different disambiguation deci-sions. Then, they proposed a collective inference algorithm, which can jointly infer the referent entities of all name surface forms by exploiting the interdependence captured in the Referent Graph.
Semantic embeddings have also been used for entity disambigua-tion. In 2013, He et al. [13] proposed an entity disambiguation model, based on Deep Neural Networks. The model learns a context-entity similarity measure for entity disambiguation. The intermedi-ate representations are learned leveraging large-scale an-notations of Wikipedia. The most recent approach of semantic embeddings is presented by Huang et al. [15] in 2015. The authors present a new entity semantic relatedness model for topical coherence mod-eling. Similar to our approach the model can be directly trained on large-scale knowledge graphs. It maps heterogeneous types of knowledge of an entity from knowledge graphs to numerical fea-ture vectors in a feature space such that the distance between two semantically-related entities is minimized. Unfortunately, the ap-proach is only evaluated on two data sets and, thus, the robustness properties of this approach are debatable.
The goal of entity disambiguation is to find the correct semantic mapping between surface forms in a document and entities in a KB. More formally, let M = &lt; m 1 ,...,m K &gt; be a tuple of K surface forms in a document D , and  X  = { e 1 ,...,e |  X  | of target entities in a KB. Let  X  be a possible entity configuration &lt; t 1 ,...,t K &gt; with t i  X   X  , where t i is the target entity for surface form m i . Here, we assume that each entity in  X  is a candidate for surface form m i . The goal of collective entity disambiguation can then be formalized as finding the optimal configuration  X  Different to [25] we do not pose the optimization problem as max-imizing the sum of the scores of a locality function  X  and a coher-ence function  X  (cf. Equation 1), which has been proven to be NP-hard [7]. We approximate the solution using the PageRank (PR) algorithm with priors [3, 30] on specially constructed graphs which encompass the locality and the coherence function (still NP-hard). Our locality function reflects the likelihood that a target entity t the correct disambiguation for m i , whereas the coherence function computes a score describing the coherence between entities in  X 
The PR algorithm is a well-researched, link-based ranking algo-rithm simulating a random walk on graphs and reflecting the impor-tance of each node. It has been shown to provide good performance in many applications [31], also in disambiguation tasks [12, 1].
The graphs we construct consist of nodes for all entity candidates per surface form and one node, the topic node, that represents the current predominant topic of already disambiguated entities. This topic node allows us to include a-priori information from previous steps into the structure of the graph and, hence, influence the PR al-gorithm. The edge weights are based on similarities between entity embeddings as well as similarities between entity-context embed-dings and the surface forms X  surrounding context. In our work, an entity embedding is a trained vector that is used to compute the semantic similarity between entities. Moreover, an entity-context embedding is a trained context vector of an entity to compute a matching how good this entity fits to the context of a surface form.
Abstaining is an important task if no appropriate entity can be found in the entity set  X  . In this case, our disambiguation algorithm returns the pseudo-entity NIL . In our work, a surface form is linked to NIL if one of the following cases occurs: 1. when the surface form has no candidate entities, or 2. the algorithm is uncertain about the relevant entity after the last PR application.
Embeddings are n-dimensional vectors of concepts which de-scribe the similarities between these concepts using the cosine sim-ilarity. This has already been well researched for words [20, 21] and documents [18] in literature. In this work, we make use of both embedding types in form of entity embeddings (Word2Vec) and entity-context embeddings (Doc2Vec) to improve entity dis-ambiguation. First, we briefly introduce Word2Vec, a set of models that are used to produce word embeddings, and Doc2Vec, a mod-ification of Word2Vec to generate document embeddings, in Sec-tion 4.1. Second, we describe how we create corpora that serve as input for the Word2Vec and Doc2Vec algorithms in Section 4.2.
Word2Vec is a group of state-of-the-art, unsupervised algorithms for creating word embeddings from (textual) documents [20]. To train these embeddings, Word2Vec uses a two-layer neural network to process non-labeled documents. The neuronal network architec-ture is based either on the continuous bag-of-words (CBOW) or the skip-gram architecture. Using CBOW, the input to the model for a word w i are the words preceding and succeeding this word, e.g. w i  X  2 ,w i  X  1 ,w i +1 ,w i +2 when using two words before and af-ter the current word. The output of the network is the probability of w i being the correct word. The task can be described as pre-dicting a word given its context. The skip-gram model works vice-versa: the input to the model is a word w i and Word2Vec predicts the surrounding context words w i  X  2 ,w i  X  1 ,w i +1 ,w trast to other natural language neuronal network models, Word2Vec models can be trained very fast and can be further significantly im-proved by using parallel training [26].

An important property of Word2Vec is that it groups the vectors of similar words together in the vector space. If sufficient data is used for training, Word2Vec makes highly accurate guesses about a word X  X  meaning based on its context words in the training corpus. The resulting word embeddings capture linguistic regularities, for instance the vector operation vec (  X  X resident X  )  X  vec (  X  X ower X  )  X  vec (  X  X rime Minister X  ) . The semantic similarity between two words, which is important in the context of our work, denotes the cosine similarity between the words X  Word2Vec vectors.

Since our approach considers only the semantic similarity be-tween entities (and not words), we treat entities similar to words. This means, we build entity embeddings with the help of an entity corpus instead of a textual corpus containing sentences and para-graphs (cf. Section 4.2). Our evaluation of the influence of the specific architecture in Section 7.3 shows that the skip-gram model outperforms CBOW in our disambiguation setting. Thus, for all other experiments in this paper, the skip-gram model is used.
Doc2Vec , a modification of Word2Vec, learns fixed-length em-beddings from variable-length pieces of texts like documents [18]. It addresses some of the key weaknesses of bag-of-word models by incorporating more semantics and considering the word order within a small context. As an example for the semantic embedding, the Doc2Vec model embeds the word  X  X owerful X  closer to  X  X trong X  than to  X  X aris X , which is not the case in bag-of-word models.
The architecture is either based on the distributed memory model (PV-DM), which is similar to the CBOW model of Word2Vec, or on the distributed bag-of-words model (PV-DBOW), which is sim-ilar to the skip-gram model. Using PV-DM, the context words X  corresponding document vector d i (vector of the document which contains the context words) is added as an input in the neural net. Thus, the input becomes d i ,w j  X  1 ,w j +1 , meaning that the docu-ment vector with the two context vectors is used to predict the word w . For more details on calculating the document vector see [18]. Similar to the skip-gram model in Word2Vec, the intention of PV-DBOW is to ignore the context words in the input, but force the model to predict words randomly sampled from the document in the output. A disadvantage is, that it ignores the word sequence.
The authors of Doc2Vec report consistently better results with the PV-DM architecture. PV-DM also outperforms PV-DBOW in the context of our disambiguation algorithm (cf. Section 7.3). Since we want to compute the similarity between an entity-context em-bedding and the surrounding context of a surface form, one needs to perform an inference step to compute the surrounding context vector. The Doc2Vec model is trained on the entity-context corpus yielding the entity-context embeddings (see next section), and the same model is later used to generate the surface-form-context em-beddings. For a detailed introduction to Word2Vec and Doc2Vec, we refer the reader to the original works [20, 21, 18].
Word2Vec typically accepts a set of corpora containing natural language text as input and trains its word vectors according to the words X  order in the corpora. Since we want to learn entity repre-sentations (entity embeddings) only, we have to create an appropri-ate Word2Vec input corpus file that exclusively comprises entities. The entities X  order in the corpus file should be retained as given in the entity-annotated document KBs. For this purpose, we iterate over all documents in these corpora and replace all available, linked surface forms with its respective target entity identifier. Further, all non-entity identifiers like words and punctuations are removed so that all documents consist of entity identifiers separated by whites-paces only. However, the collocation of entities is maintained as given by the original document, but the distance between the an-notations is ignored. All resulting documents are concatenated to create a single Word2Vec corpus file. Details are provided in [32].
To generate entity-context embeddings with Doc2Vec, any nat-ural language source can be used that offers sufficient descriptions of the entities. To generate embeddings, the source needs to be ei-ther already a single document, or needs to be aggregated to a sin-gle document. These documents describe the respective entities as detailed as possible. A well-known example for entity describing documents are Wikipedia pages, which are used in our experiments.
Our disambiguation algorithm accepts documents that contain one or multiple surface forms that should be linked to entities. It disambiguates all surface forms within a document using a graph-based collective approach. Algorithm 1 gives an overview of the disambiguation process, which is explained in the following. The first step in the disambiguation chain is the Candidate Generation . The goal is to reduce the number of possible entity candidates for each input surface form m i by determining a set of relevant target entities, the candidate set CS m i . Details of our candidate genera-tion process are described in Section 5.1. Given these candidates we disambiguate surface forms with none or one candidate and ini-tialize the entity set E d with the entities of unambiguous surface forms or already disambiguated surface forms (Lines 2-7).
Our second step Semantic Embedding Candidate Filter filters en-tity candidates that fit to the general topic described by the already disambiguated entities (Lines 8-17) requiring at least 3 already as-signed entities. The underlying assumption is, that all entities in a paragraph are somehow topically related. To infer this general topic, we create a topic vector tv = P e  X  E the set of already disambiguated entities, and v ( e ) the entity em-bedding of entity e (Word2Vec vector). Next, we compute the se-mantic similarity (cosine similarity) between the general topic vec-tor tv and the entity candidates of all not yet disambiguated surface forms. If the similarity exceeds the a-priori given CandidateFilter threshold, the entity candidate remains in the candidate list of the respective surface form. If no candidate of a specific surface form
Algorithm 1: Our graph-based disambiguation algorithm exceeds the threshold, the candidate set for this surface forms re-mains unchanged. We note that this filter is a crucial step towards fast and accurate entity disambiguation. Omitting this step results in a significantly lower performance combined with decreasing re-sults (  X  2 to 5 percentage points F1, depending on the data set).
The third step High Probability Candidate Disambiguation com-prises the PageRank (PR) application on a disambiguation graph to disambiguate high probability candidates (Lines 18-26). Detailed information for graph construction and PR can be found in Sec-tion 5.2. Next, we rank the entity candidates for each surface form according to their relevance score given by PR in descending order. Additionally, we select the highest PR score h , second-highest PR score s and average PR score avg across all entities that belong to the same surface form. Given these parameters, we define a thresh-old dynT for determining the certainty in the ranking based on the differences between the first and the second ranked candidate: whereas details on the parameter margin 1 are discussed in Sec-tion 6.2. We use this threshold as a certainty criterion, indicating whether the top-ranked entity candidate of a surface form is the cor-rect disambiguation target. More specific, if the PR score s of the second ranked candidate does not exceed the threshold dynT , the highest ranked entity denotes the target entity of its surface form. In other words, if the relevance score margin between the highest ranked candidate and the other candidates is large, then the likeli-hood of the top-ranked candidate being the correct target entity is also high. If the threshold is exceeded, we reduce the candidate set of the respective surface form to the top 4 ranked entity candidates.
The last step Final Disambiguation and Abstaining disambiguates the remaining entities or abstains if the algorithm is uncertain about the correct target entity (Lines 27-36). We first create a disambigua-tion graph (cf. Section 5.2) and, then, iteratively disambiguate the entities of the remaining surface forms. For this purpose, every iteration applies the PR to the underlying graph and ranks the can-didate entities of each surface form in descending order. The scores h , s , and avg are calculated as in the previous step. The abstaining threshold abstainingThreshold is calculated using formula 2 with a different margin parameter ( margin 2 ). If the second ranked en-tity candidate exceeds the abstaining threshold abstainingThresh-old , the algorithm returns the NIL identifier for the respective sur-face form. Otherwise, the top ranked entity candidate denotes the target entity. After every iteration, we update the graph according to the changes in candidates and disambiguated entities and pro-ceed until all surface form have been processed.

We note, that we apply the PR only once in step 3 due to perfor-mance reasons. The disambiguation graph in step 4 usually does not include many entity candidates and, thus, we apply the PR in every iteration, also to provide the maximum accuracy in the ab-staining task. The margin parameter to compute the high proba-bility threshold and abstaining threshold varies in both steps. In-formation about the parameter choice is presented in Section 6.2.
In the first step, the goal is to reduce the number of possible entity candidates for each input surface form m i by determining a set of relevant target entities. We proceed as follows:
First, we search for all those entities that have already been anno-tated with m i in a corpus. All entities that provide an exact surface form matching serve as entity candidate. To perform this task our algorithm relies on a pre-built index, which has to be created before entities can be disambiguated (cf. Section 6.1). If the candidate set is empty, we additionally use the candidate generation approach proposed by Usbeck et al. for AGDISTIS [28], which includes String normalization and String comparison via trigram similarity. The corresponding parameters are adopted from the default settings in the AGDISTIS framework 1 .

Gathering all relevant entity candidates might result in a long list of candidates. To keep the list short and to improve the efficiency, we prune noisy candidates according to the following three criteria: 1. Prior probability: Some entities (e.g. Influenza) occur more 2. Context similarity: We select the top x entities ranked by http://aksw.org/Projects/AGDISTIS.html 3. Entity-topic similarity: If a document contains at least two For all criteria we use x = 8 , which is enough to capture the rel-evant entity candidates. An experimental increase of x , results in a negligibly higher recall of the candidate generation task, but sig-nificantly decreases disambiguation performance.
In our approach, we generate a disambiguation graph twice in order to disambiguate high probability candidate entities first and to perform abstaining afterwards. On this graph we perform a random walk and determine the entity relevance which can be seen as the average amount of its visits. The random walk is simulated by a PR algorithm that permits edge weights and non-uniformly-distributed random jumps [3, 30].

First, we create a complete, directed K -partite graph whose set of nodes V is divided in K disjoint subsets V 0 ,V 1 ,...,V notes the amount of surface forms and V i is the set of generated entity candidates { e i 1 ,...,e i | V m 0 as pseudo surface form and use the subset V 0 = { e 0 1 tain the topic node. The topic node represents the average topic of all already disambiguated entities in E d . Hence, the edge weight between an entity e i a and the topic node e 0 1 represents the related-ness between e i a and all already disambiguated entities. Since our graph is K -partite, there are only directed, weighted edges between entity candidates that belong to different surface forms. Connecting the entities that belong to the same surface form would be wrong since the correct target entities of surface forms are determined by the other surface forms X  entity candidates (coherence).

The edge weights in our graph represent entity transition proba-bilities (ETP) which describe the likelihood to walk from a node to the adjacent node. We compute these probabilities by first comput-ing the Transition Harmonic Mean (THM) between two nodes. The THM is the harmonic mean between two nodes X  semantic similar-ity and the context similarity of the target entity (cf. Equation 3).
The semantic similarity between two nodes is the cosine similar-ity ( cos ) of the entities X  semantic embeddings (vectors) v ( e v ( e j b ) . The semantic embedding of our topic node e of all entity embeddings in E d (i.e. v ( e 0 1 ) = P e  X  E context similarity between the target entity e j b and the surrounding context of its surface form m j is the cosine similarity of e context embedding cv ( e j b ) , and the inferred surrounding context vector cv ( m j ) of m j . In case, the target entity is our topic node the context similarity equals 0. The ETB is computed by normaliz-ing the respective THM value (cf. Equation 4).

THM ( e i a ,e j b ) = Given the current graph, we additionally integrate a possibility to jump from any node to any other node in the graph during the ran-dom walk with probability  X  . Typical values for  X  (according to the original paper [30]) are in the range [0 . 1 , 0 . 2] . We compute Figure 1: Entity candidate graph with candidates for the sur-face forms  X  X S X  and  X  X ew York X . a probability for each entity candidate being the next jump target. We employ the prior probability as jump probability for each node (entity). The probability to jump to or from the topic node equals 0. Combining the prior probability and context similarity instead leads to decreasing results of  X  3 percentage points F1 on average.
Figure 1 shows a possible entity candidate graph. The surface form  X  X S X  has only one entity candidate and consequently has al-ready been linked to the entity  X  X ime Square X . The surface form  X  X ew York X  is still ambiguous, providing two candidates. The topic node e 0 1 comprises the already disambiguated surface form  X  X ime Square X . We omit the edge weights and jump probabilities in the figure to improve visualization.

After constructing the disambiguation graph, we apply the PR algorithm and compute a relevance score for each entity candidate. Depending on the disambiguation task, our approach decides which entity candidate is the correct target entity or abstains if no appro-priate candidate is available (cf. Algorithm 1).
Our disambiguation algorithm is fully-implemented in Java and is embedded in our publicly available disambiguation system DoSeR ( D isambiguation o f Se mantic R esources) which is being developed continuously. For the Word2Vec and Doc2Vec algorithms we chose Gensim [26], a robust and efficient framework to realize unsuper-vised semantic modeling from plain text. Before we report the dis-ambiguation results in detail, we first describe the preprocessing details in Section 6.1, important parameter settings and evaluation measures in Section 6.2 and 9 test data sets in Section 6.3.
Before our algorithm is able to disambiguate entities, we first have to perform some preprocessing steps. First, we choose a KB whose entities define our target entity set  X  . In the context of this work, we make use of the current version of DBpedia 2015-04 as entity data base, which reflects information from the last years Wikipedia version. Overall, we extract  X  4 . 1 M entities (all enti-ties belonging to the owl:thing class) out of DBpedia that we would like to disambiguate in our work.
 Next, we select Wikipedia (  X  81 M annotations) and the Google Wikilinks Corpus 3 (  X  40 M annotations) as entity-annotated doc-ument KB that serve as training data for our semantic entity em-beddings (Word2Vec). To create the Doc2Vec entity-context em-beddings, we parse the entities X  Wikipedia pages and remove all Wikipedia syntax elements as well as tables. The resulting natural-language texts serve as input for the Doc2Vec algorithm. https://github.com/quhfus/DoSeR https://code.google.com/p/wiki-links/
In the following, we learn entity embeddings and entity-context embeddings with Word2Vec and Doc2Vec. To train the entity em-beddings with Word2Vec, we define a feature space of d = 400 dimensions and employ the skip-gram architecture that performs better with infrequent words [20]. In terms of Doc2Vec we use a feature space of d = 1000 dimensions and employ the PV-DM learning architecture. An experimental comparison between the ar-chitectures and various settings for parameter d is presented in Sec-tion 7.3. The Word2Vec training time takes  X  90 minutes on our personal computer with a 4x3.4GHz Intel Core i7 processor and 16 GB RAM (1 corpus iteration). The training time for Doc2Vec takes  X  11 / 2 days since we performed 5 iterations overall.

Next, we create a disambiguation index with each entry defining an entity. The index comprises the following three entity describing information which are relevant for our disambiguation algorithm: 1. Labels: By default, we extract the rdfs:label attributes of 2. Surface Forms: We gather and generate surface forms from 3. Semantic Entity Embeddings: We store the semantic em-Our manually-constructed disambiguation index is publicly avail-able on the GitHub page.
Our approach offers several parameters to tweak the disambigua-tion results. In the following, we will mention only those that have the most impact on the results. An overview of all parameters can be found on the GitHub page.
In our evaluation, we use the well-known standard measures: re-call  X  , precision  X  , F1 and accuracy a . Given the ground truth G and the output of an entity disambiguation system O , in which G ent and O ent are the sets of surface forms that link to entities, and G nil and O nil are the sets of surface forms that link to NIL ( G = G ent  X  G nil , O = O ent  X  O nil , and | G | = | O | ):  X  = a =
An often occurring problem in evaluating entity disambiguation systems is that authors often download available data sets and ig-nore those entity ground truth annotations that are not available in their underlying KB [12, 14, 13]. Thus, results often slightly differ due to different data set queries across literature. To avoid this problem, the authors of Uzbeck et al. [29] proposed GERBIL -General Entity Annotator Benchmark, an easy-to-use platform for the agile comparison of annotators using multiple data sets and uni-form measuring approaches. Being a web-based platform it can be also used to publish the disambiguation results. The reported re-sults of our approach and competitive systems are based on this platform and serve as comparable results for future systems.
In the following, we present nine well-known and publicly avail-able data sets which are integrated in GERBIL and are used in our evaluation. All data sets provide different characteristics in form of surface form frequency and length of surrounding context (cf. Ta-ble 1). We evaluate our algorithm on all these data sets to demon-strate the robustness across different documents/data sets. 1. ACE2004: This data set from Ratinov et al. [25] is a sub-2. AIDA/CO-NLL-TestB: The original AIDA data set [14] was 3. AQUAINT: Compiled by Milne and Witten [22], the data set 4. DBpedia Spotlight: The DBpedia Spotlight corpus was re-5. MSNBC: The corpus was presented by Cucerzan et al. [7] 6. N3-Reuters128: This corpus is based on the Reuters-21578 7. IITB: This manually created data set by Kulkarni et al. [17] 8. Microposts-2014 Test: The tweet data set [2] was intro-9. N3 RSS-500: This corpus has been published by Gerber et All ground truth annotations in these data sets either refer to enti-ties in DBpedia or Wikipedia. Both KBs contain the same entities whose URLs can be easily converted to the other KB URL. Thus, we simply always return DBpedia URLs to GERBIL, which auto-matically processes the URLs according to the underlying data sets. We note that the GERBIL version that we use does not consider NIL annotations when computing the F1, recall and precision values. If our service returns a NIL annotation, GERBIL treats it like  X  X ot annotated X . Thus, for our abstaining experiment we manually eval-uate the accuracy on the IITB data set, which also considers NIL annotations.
In the following, we compare the results of our approach with those attained by other disambiguation frameworks. To this end, we use GERBIL v1.1.4 and evaluate the approaches on the D2KB (i.e. link to a KB) task. In Section 7.1 we directly compare the approaches on the basis of its results achieved with GERBIL. In Section 7.2 we discuss our results in contrast to other works that are not publicly available. Section 7.3 provides a parameter study of the semantic embeddings and in Section 7.4 we evaluate the ab-staining mechanism of our approach.
In the following, we directly compare our approach to publicly available, state-of-the-art entity disambiguation systems, which dis-ambiguate Wikipedia, DBpedia or YAGO entities, via GERBIL. These are the currently available versions of DBpedia Spotlight [19], AIDA [14] (new disambiguation index), Babelfy [23], WAT [24] and Wikifier [25, 5]. To the best of our knowledge, Wikifier is the current state-of-the-art entity disambiguation system which is pub-licly available. Wikifier and WAT use Wikipedia as underlying KB and link surface forms directly to Wikipedia pages. Babelfy also returns Wikipedia entities but uses BabelNet as KB, which was au-tomatically created by linking Wikipedia to WordNet. In contrast, DBpedia Spotlight and AIDA rely on the RDF-KBs DBpedia and YAGO2, while additionally making use of Wikipedia knowledge. Table 2: Micro-averaged F1,  X  and  X  of DoSeR on 9 data sets. Since entities within these three KBs (DBpedia, Wikipedia and YAGO2) provide sameAs relations, GERBIL maps the systems X  output to the corresponding ground truth URLs. For all systems we choose the best configurations according to the authors. In addi-tion to these frameworks, we define the strong baseline PriorProb which links surface forms to the entities with the highest prior prob-ability (cf. Section 5.1). We also present the results when excluding entity-context embeddings ( DoSeR -Doc2Vec ). We investigate how well the approach performs with entity-embeddings as semantic re-latedness feature only. In this case, we use the entity embeddings directly to compute the ETP (cf. Section 5.2). Table 2 lists the re-sults on nine data sets in terms F1, precision and recall, aggregated across surface forms (micro-averaged). Table 3 shows the F1 val-ues in comparison to the competitor systems on all data sets. The corresponding GERBIL result sheet is available on the GERBIL website 4 and can be used to make comparisons to our approach in future evaluations.

Overall, our approach attains the best averaged F1 value of all systems. Thereby, it outperforms Wikifier by 5 F1 percentage points on average. Additionally, we significantly outperform the other systems as well as the PriorProb baseline by up to 25 F1 percentage points on average. On the data sets ACE2004, MSNBC, Micro-posts2014-Test and N3-Reuters128 our approach performs excep-tionally well (up to 12 F1 percentage points in advance). We note that our PriorProb baseline outperforms Wikifier on the Microp-osts-2014-Test data set because of using a newer version of DB-pedia/Wikipedia. The Micropost2014-Test data set was released in 2014 and obviously queries some very new (or changed) en-tities. On the DBpedia Spotlight and N3 RSS-500 data sets our approach also performs best with F1 values of  X  0 . 81 (DBpedia Spotlight) and  X  0 . 75 (N3 RSS-500) respectively. Considering the AIDA/CONLL-TestB data set, our approach performs slightly bet-ter than Wikifier but performs comparatively poor with a F1 value of  X  0 . 78 compared to  X  0 . 84 by the WAT system. The reasons for this are two-fold: First, the underlying data set is still annotated with entities whose identifiers have been changed over the years with updates. Thus our service returns wrong entity URLs accord-ing to the ground truth. The same problem occurs in the AIDA sys-tem when using the newer AIDA entity index. In this case, the F1 value drops from 0.82 to 0.77. In an experiment where we disam-biguate the original AIDA entities, our system achieves a F1 value of  X  0 . 84 . Second, a more detailed analysis of the surface forms X  textual context is necessary to perform even better. Nevertheless, our algorithm outperforms the other systems and also AIDA which was optimized on this data set. Regarding AQUAINT and IITB, Wikifier leads DoSeR by 2 percentage points F1 on both data sets. http://dx.doi.org/10.5281/zenodo.51250
In summary , we state that our approach significantly outperforms other publicly available disambiguation approaches. Overall, our approach disambiguates the entities highly accurate and attains state-of-the-art or nearly state-of-the-art results on all nine data sets. Hence, our approach is very well suited for all kinds of documents available in the web (e.g. tweets, news, etc.). We emphasize that despite the huge amount of training data used to improve disam-biguation robustness, our approach attains nearly identical results on all data sets with entity embeddings trained on Wikipedia only. Our results proposed in [32] show that a simple graph algorithm using entity embeddings based on Wikipedia already performs ex-ceptionally well. In terms of performance, our approach annotates roughly as fast as the Wikifier and WAT annotation system but is slower compared to Spotlight and AIDA. The Babelfy system is the slowest and takes too much time, especially on the IITB data set. Our system has the advantage to accept multiple queries in parallel, but is not yet optimized for high-performance disambiguation.
Comparing our results to those of other state-of-the-art approaches that are not publicly available is not an easy task. Reimplementing the respective algorithms is not an absolutely fair method to com-pare the approaches with our KB: Usually crucial implementation details remain unknown in the original publications, since the focus mostly lies on the algorithm instead of the implementation.
Anyhow, we use the work of Guo et al. [10] as an entry point in the following. Their approach was exclusively evaluated and optimized on the ACE2004, MSNBC and AQUAINT data sets on which the authors achieve state-of-the-art results. A direct com-parison of our results and the results of [10] shows that both works perform equally well on the MSNBC data set. Furthermore, our ap-proach performs better on the ACE2004 data set (0.906 vs. 0.877 F1) but loses on the AQUAINT data set (0.842 vs. 0.907 F1). The problem with a pure number-based comparison, however, lies in the uncertainty in the underlying KB used in the experiments. If the un-derlying KB has a lower number of entities, the average likelihood of a wrong disambiguation is also reduced. In order to compare our algorithm with the approach in [10], we introduce the con-cept of the Surface Form Ambiguity Degree (SFAD). The SFAD is based on two assumptions: First, both approaches are able to disambiguate all entities in the ground truth data set, i.e. the KB covers the entities in the data sets and contains similar entity oc-currences resulting from a given corpus (important for prior com-putation). Second, the candidate entities retrieved from the KB contain the correct entity, i.e. the error introduced by candidate selection is zero. Under these assumptions, a varying prior prob-ability of an entity defines the degree of ambiguity, the SFAD, for that surface form. So the SFAD describes how many entities are potentially relevant for a specific surface form. Since our approach has a (significantly) lower prior probability on these data sets, the SFAD is higher, respectively. In Table 4 we compare the differ-ences of the best result and the result achieved with the prior alone for our approach and the Guo et al. approach. Overall, our disam-biguation index contains more entities that are relevant for a surface form on average and hints that our core-algorithm (without KB and candidate selection) is more robust than the approach from Guo et al. [10]. Another evidence is that the authors reimplemented the ap-proach used in Wikifier and achieved significantly better results on their KB as we achieved with GERBIL with Wikifier X  X  original KB. Guo et al. also report the results of former, well-known state-of-the-art approaches (e.g. Cucerzan [7], Han et al. [12], Glow [25]), but we do not discuss the results in detail because these approaches perform worse than Wikifier and the approach of Guo et al.
Considering the IITB data set, the system by Han et al. [11] per-forms best with a micro F1 value of 0.80. The authors did not eval-uate their system on other data sets. However, their topic model approach is fully-trained on Wikipedia and takes all words into ac-count. Since, the IITB data set consists of long documents very similar to those in Wikipedia, the system performs best on it.
In 2014, the Micropost2014-Test data set was created in the con-text of the workshop challenge Making Sense of Microposts [2]. The best system in the workshop was proposed by Microsoft which attains a micro F1 value of 0.70. To the best of our knowledge, this has been the best disambiguation approach on this data set so far, but is outperformed by our approach by  X  5 percentage points.
Considering the CONLL-TestB data set, the current state-of-the-art approach has been presented by Huang et al. [15] and attains a micro F1 value of 0.866. Similar to our approach, the authors learn semantic embeddings with a deep neuronal network approach from DBpedia and Wikipedia (but not with Word2Vec and Doc2Vec). Again, the approach was only evaluated on the CONLL-TestB data set as well as on a tweet data set. Experiments show that we can also further improve our results on this data set to a micro F1 value of 0.850 by (i) reducing entity candidates (lower SFAD), (ii) train-ing the semantic embeddings on DBpedia instead of Wikipedia and (iii) using an older entity index. However, since our main goal was to create a robust disambiguation approach which performs well on several data sets with varying underlying document properties, we do not optimize the DoSeR algorithm on a single data set. Table 4: Differences of the best result and the prior DoSeR and Guo et al. [10] on 3 data sets.

In summary , we state that a pure number-based result compar-ison is not always easy since multiple factors play an important role (e.g. disambiguation index, data set queries). However, the re-sults give the hint that our approach achieves comparable accuracy, while being robust on other data sets at the same time.
The accuracy of our approach depends on a number of parame-ters, foremost the parameters of the semantic embeddings. In order to analyze this sensitivity, we conducted experiments in which we vary the dimension number of our semantic embeddings and report the results for both Word2Vec and Doc2Vec architectures (CBOW vs. Skip-gram and PV-DM vs. PV-DBOW).

Figure 2 (top) depicts the average micro F1 value (across all data sets) of our approach when using either the CBOW or skip-gram Word2-Vec architecture and a specific amount of dimensions. Dur-ing this experiment the corresponding Doc2Vec architecture is set to PV-DM since it is better suited as we will see in the following. Basically, in our experiments, the skip-gram architecture consis-tently creates better entity embeddings then CBOW. This might be due to skip-gram performs better with infrequent words (entities) in the training corpus [20]. However, the difference between both ar-chitectures is  X  1  X  2 percentage points F1. On d = 400 the result margin between both architectures is maximized and the average F1-value reaches its peak. It is interesting to see that even more dimensions slightly decrease the result values. We assume that this leads to some kind of overfitting and, thus, the optimal number of dimensions for entity embeddings probably depends on the number of entities and amount of training data.

We conduct the same experiment for our entity-document em-beddings (Doc2Vec). In this particular case, we use the skip-gram architecture as baseline training algorithm for the entity embed-dings (Word2Vec). Figure 2 (bottom) depicts the corresponding average micro F1 values for various dimensions and both Doc2Vec architectures. The PV-DM architecture for Doc2Vec performs bet-ter if the number of dimensions is higher than d = 400 . We assume that the context consideration in the PV-DM architecture leads to an advance. However, the best average F1 value is achieved with d = 1000 , whereby the difference between PV-DBOW and PV-DM is maximal  X  2 percentage points F1.
 In summary , we state that the skip-gram architecture for Word2-Vec and the PV-DM architecture for Doc2Vec perform best. It is interesting to see that the number of optimal dimensions for entity embeddings must be geared to the underlying corpora.
Abstaining is an important task in disambiguation algorithms when it comes to disambiguating surface forms whose referent en-tity is not in the entity set  X  . It is also used if there is uncertainty about the correct entity due to insufficient context information.
In this experiment, our algorithm returns the pseudo-entity NIL in the following situations: (i) if no entity candidates can be found during the candidate generation step (cf. Section 5.1), and (ii) if the algorithm is uncertain about the correct entity after the last PR Figur e 2: Comparison of Word2Vec and Doc2vec architectures with various feature-space dimensions. iteration (cf. Algorithm 1). For experimental purpose, we down-loaded the original IITB data set (information on our GitHub page), which additionally contains 7652 NIL annotations in addition to the default annotations (18897 annotations overall), and report the dis-ambiguation accuracy . We also rerun the GERBIL experiments with abstaining to investigate to what extent the results decrease on data sets which do not provide NIL ground truth annotations.
Conducting the experiment on the manually downloaded IITB data set results in a disambiguation accuracy of 0.757 (micro-aver-aged). With returning 6120 NIL annotations overall, our algorithm does not find candidates for surface forms in 3823 cases (  X  62 . 5 %) and abstains 2297 surface forms (  X  37 . 5 %). When we tune our ab-staining parameter to abstain more aggressive, our overall accuracy slightly decreases. Unfortunately, the authors of the topic-model, state-of-the-art approach [11] on this data set do not provide ab-staining results for comparison in their work.

However, Table 5 reports the micro F1 values of our algorithm with abstaining on all data sets in the GERBIL evaluation. As a re-sult of GERBIL not querying surface forms with NIL annotations in the ground truth, our results (slightly) decrease. Nevertheless, the amount of abstained surface forms is very limited and, thus, our approach still outperforms Wikifier on 6 out of 9 data sets. On the Microposts2014-Test data set the F1 decrease is the highest with 7 percentage points F1. Obviously, our algorithm is sometimes un-certain about the correct entity and abstains, which is due a small amount of surface forms per document. In other words, our algo-rithm lacks sufficient evidence about the correct entity and, hence, abstains due to exceeding the abstaining threshold. In summary , we state that our algorithm is able to successfully abstain entity anno-tations if evidence of the correct entity is missing. Our abstaining mechanism performs well even if data sets do not provide NIL an-notations (as simulated by GEBRIL).
In this work, we present a new collective, graph-based entity disambiguation approach that utilizes semantic entity and entity-context embeddings for robust entity disambiguation. Robust there-Table 5: Micro F1 values of our approach with abstaining on data sets without NIL annotations.
 by refers to the property of achieving (better than) state-of-the-art results over a wide range of very different data sets. Our approach is also able to abstain if no appropriate entity can be found for a specific surface form. We evaluate our approach against 5 strong, publicly available entity disambiguation systems on 9 data sets and show that our approach outperforms all other system by a signifi-cant margin on nearly all data sets. We also discuss the influence of the quality of the underlying knowledge base on the disambiguation accuracy and compare our results to those of other non-publicly available state-of-the-art algorithms. We also provide our approach as well as the underlying knowledge base as open source solution.
Future work includes the evaluation of non-collective disambigua-tion. This is important if queries provide only a single surface form to disambiguate. We also want to optimize the disambiguation per-formance and provide in-depth performance tests.
The presented work was developed within the EEXCESS project funded by the European Union Seventh Framework Programme FP7/2007-2013 under grant agreement number 600601. Addition-ally, it was partially funded by the IRYXYS Research Center.
