 We describe the challenges of characterizing, constructing and managing the usage profiles of System S applications. A running System S application is a directed graph with soft-ware processing elements (PEs) as vertices and data streams as edges connecting the PEs. The resource usage of each PE is a critical input to the runtime scheduler for proper resource allocation. We represent the resource usage of PEs in terms of resource functions ( RF s) that are used by the System S scheduler, with one RF per resource per PE. The first challenge is that it is difficult to build good RF s that can accurately predict the resource usage of a PE because the PEs perform arbitrary computations. A second set of challenges arises in managing the RF s and performance data so that we can apply them for PEs that are re-run or reused by the same or different applications or users. We report our experience in overcoming these challenges. Specifically, we present an empirical characterization of PE RF s from several real streaming applications running in a System S testbed. This indicates that our simple models of resource usage that build on the data-flow nature of the underly-ing application can be effective, even for complex PEs. To illustrate our methodology, we evaluate and analyze the per-formance of these applications as a function of the quality of our resource profile models. The system automatically learns the models from the raw metrics data collected from running PEs. We describe our approach to managing the metrics and RF models, which allows us to construct gener-alizable RF s and eliminates the learning time for new PEs by intelligently storing and reusing the metrics data. D.4.8 [ Operating Systems ]: Performance X  Measurements, Modeling and prediction Measurement, performance Streaming, resource model, performance characterization
System S [2, 7, 17, 18] and other distributed stream pro-cessing systems [1, 10, 20] are geared toward processing long-running queries on continuous streams of data. Streaming applications in such systems are typically organized as data-flow graphs with the software processing elements (the PEs) as verticies and the data streams as directed edges connect-ing the PEs.

A key resource allocation problem faced by the runtime scheduler in such systems is to map the PEs in the applica-tions to compute resources in a way that utilizes the avail-able CPU and network resources efficiently without over-loading any individual node or network link. A critical in-put to the runtime scheduler for solving such a problem is the  X  X ize X  of a PE, that is, the resource usage of a PE. The resource demand r p of a PE p for resource r is given by the functional form r p = f p,r ( d 1 ,d 2 ,... ) where d 1 ,d factors on which the resource usage depends. The function f p,r is called a resource function ( RF ) and is a model of the PE X  X  resource usage.
 There are several challenges that arise in the context of RF s. First and foremost, accurately predicting the resource usage of PEs can be difficult. A PE can be the result of combining multiple simpler streaming operations, or it can directly implement a user-defined, arbitrarily complex data analytic algorithm. Thus, the PE sizes are not fixed or even known a-priori. Furthermore, the resource usage of a PE can depend on dynamic characteristics of the input streams (such as data volume and data content). As these charac-teristics change, the resource usage of a PE may increase or decrease.

A second set of challenges arises in the context of manag-ing the RF s, driven by how the PEs are used. A PE from a job may be resubmitted at a later time, either as part of the same job or a different job. Users may share PEs (eg, a classifier) in their own applications. The same PE may run in the system under a different set of circumstances, such as different parameterizations or context (i.e., with different upstream or downstream PEs). Since even PEs that have never been run before do need to be scheduled, it is very helpful for good resource allocation to have some initial es-timate of the PE X  X  resource usage. Hence, one question is to identify a PE so we can associate an RF with it. In a similar vein, how should RF s be shared between instances of a PE, and how can observing one instance of a PE yield clues about the resource usage of another, slightly different instance? For PEs that may not have been run before  X  what sensible initial RF can be provided?
In this paper, we describe our experience in addressing these challenges. Specifically, we present our practical ap-proach to characterizing, constructing and managing the resource usage profiles of stream PEs for the purpose of providing a critical input to the SODA scheduler [16, 17] in System S . ( SODA , the Scheduling Optimizer for Distributed Applications, is the runtime scheduler for System S .) Our approach is not the only one; other alternatives certainly exist, and we continue to explore some of them. Moreover, we focus only on learning the resource usage of CPU and network.

With inaccurate models, the scheduler may place PEs in ways that reduce application throughput. To highlight the impact of RF inaccuracy on system performance, we com-pare in Figure 1 the total ingest rate achieved for two ap-plications: DAC [18] and SKA [5] when SODA has increas-ingly incongruous RF s. The higher the total ingest rate, the better the system performance. The applications are de-scribed in Section 2.2 and the experiments are detailed in Section 3.5.1. An incongruity level of 1 represents the appli-cation performance when it is scheduled with SODA using the actual trained RF s, i.e., the most congruous/accurate RF s. Higher incongruity levels represent RF s which are in-creasingly divergent from these trained RF s, and therefore more inaccurate. We see that the RF s need not be com-pletely accurate to achieve good performance in practice (see the incongruity levels of 3 and 5 for DAC and the incongruity level of 1.5 for SKA). However, they cannot be too far off, either. For example, with severely incongruous RF s, the ap-plication performance can reduce significantly, e.g., by over 30% for DAC and over 50% for SKA. In extreme cases, the application may even fail to start.

Specifically, based on our experience, we make the follow-ing contributions in this paper:
The rest of the paper is organized as follows. In Section 2 we introduce System S and describe the testbed and appli-Figure 1: Scheduling effects of bad resource profiles cations used in the experiments in this paper. Next, we present in Section 3 the resource model for CPU and net-work that we use, along with an evaluation of the resource model sensitivity. A description of our approach to model management is given in Section 4. Related work is reviewed in Section 5, and we conclude in Section 6.
System S is a large-scale distributed stream processing middleware being developed at IBM Research. It is de-signed for supporting complex analytics on large volumes of streaming data, both structured and unstructured. System S has two main components: SPADE and the System S run-time. SPADE is a rapid application development front-end for System S [7]. It consists of a language, a compiler, and auxiliary support for building distributed stream process-ing applications. The SPADE language provides a stream-centric, operator-level programming model, and composes these operators into logical data-flow graphs.

The operator logic can optionally be implemented in a lower-level language like C++. Generally, a SPADE opera-tor implementing a simple logic, like filtering, would be too  X  X mall X  to be efficiently deployed to a compute node at run-time. The SPADE compiler can fuse multiple operators into a processing element (PE), which is the unit of deployment in System S . Thus, the logical (operator-based) data-flow graphs are coalesced into physical (PE-based) graphs that are more appropriate for deployment.

At runtime, the processing of stream applications is orga-nized in terms of one or more jobs that consist of PEs orga-nized into data-flow graphs. Physically, a PE is a process, Figure 2: Illustration of a PE showing input ports i , output ports o k and streams. and may contain of one or more threads of execution. The PEs of an application are distributed across the compute nodes. Each compute node can run multiple PEs and di-vides its CPU resource between them according to fractions dictated by the SODA scheduler (Section 2.1.1). The PE can be a generic program that uses the streaming API or it may be composed from several fused operators, as above. In the latter case, its behavior depends on that of the individ-ual operators and the manner in which they are connected inside the PE.

A general PE is depicted in Figure 2. PEs consume and produce streams which consist of a series of strictly-typed tuples. A PE receives and sends data through ports , which represent attachment points for streams. A PE can read from multiple ports, write to multiple ports, and multiple streams may originate or end in a single port.
Resource allocation in System S is performed by a cen-tralized, epoch-based scheduler called SODA . In streaming systems, the jobs are usually long-running, and often con-tinue to run until they are terminated by the user. There-fore, metrics such as completion time and response time, traditional in batch processing systems, are not relevant. In-stead, SODA maximizes a utility-theoretic measure known as importance subject to a variety of real-world constraints. Because the best way of running a job may depend on the available resources and competing jobs, SODA allows a user to submit multiple job templates for a job, only one of which will be chosen to run. To schedule, SODA analyzes a vast number of PE resource allocation alternatives for different job admission and template choices, using importance as a black box objective function.

The importance metric is defined quite generally [17], but for our purposes in this paper it is sufficient to think in terms a highly common special case. Specifically, importance can be regarded a weighted sum of rates at the terminal streams in the data-flow graphs. The notion is that these streams represent the final  X  X roducts X  of the various jobs, and the weighted value functions translate these stream rates into measures of goodness of the work done in System S . In or-der to perform its job, SODA must therefore have a way to accurately predict the resource usage and output rates of the PEs in the system. In fact, any scheduler could benefit from accurate resource prediction. The RF s are the mech-anism for the scheduler to generate these predictions under various resource assignment scenarios.

A streaming scheduler such as SODA must also consider flow-balance when computing resource allocations. By flow balance we mean that PEs are given the right level of re-source allocation relative to their predecessors. To under-stand this, consider one particular PE. Giving relatively too many resources to the PE X  X  predecessors will  X  X lood X  the PE, while giving relatively too few resources will starve the PE. The allocation levels need to be balanced throughout the entire data-flow graph.

For the scheduler to do its job, the RF s must provide es-timates of the CPU requirements of the PEs and the net-work traffic between PEs. A static estimate of resource us-age would perform poorly, because these requirements could (and do) vary according to several factors. For example, a classification PE will likely need resources that depend upon its input rate, and will have an output rate that also de-pends on its input rate. Resource usage could also depend on properties of the input to the PE  X  if the classifier above is classifying speech based on audio clips, the length of the audio clips could affect the required resources.

Without going into further details of the scheduler algo-rithm (see [16]), there are some requirements imposed on the RF s:
For our tests, we study PEs from four applications running on System S ; these represent different but typical uses of streaming systems.
We use DAC and SKA for most of our application-level experiments, while Fab and VWAP provide some interesting operators which are useful to study on their own.
The experiments and data discussed in this paper are col-lected using a System S deployment on a cluster consisting of IBM BladeCenters running Linux 2.6.9. We run our ap-plications on 14 blades with dual-CPU, dual-core 3GHz In-tel Xeon processors with 8GB RAM. The blades are in the same rack; they communicate over 1GBit/s links and are inter-connected using a high-speed 20GBit/s backplane.
The input data for DAC is provided by a set of workload generators, while the other applications use source operators that read data from the filesystem. In both cases, the data enters the system as fast as the system can ingest it. The system is operating in reliable transport mode (no tuples dropped between PEs in the system). The applications are configured to run for 30 minutes, each run (for a particular setting) is repeated 4 times, and averages are collected. We collect infrastructure metrics such as CPU and network traf-fic rates in terms of averages for 1 minute intervals. Metrics are discussed further in Section 3.5.

For the specific set of applications and system software used in our experiments, the network, backplane or network interface card (NIC) is almost never a bottleneck. Thus, the only disadvantage of placing two PEs that communicate with each other on separate blades is the additional process-ing overhead involved in sending data to a different node. Although SODA allocates PEs to the nodes while trying to minimize the traffic across blades, this feature is not critical for the specific combination of infrastructure and applica-tions described here. Workloads that will stress this aspect of the system are currently being developed. Nevertheless, accurate RF s are a key factor in SODA s ability to balance the load on the processing nodes. A PE X  X  RF represents a resource usage model for that PE. In this section we present a model for streaming PEs, and validate it against actual PEs from the test applications described above. Our RF models focus on the usage of CPU and network by the PE.
In order to predict the resource usage of a PE, we must consider two broad categories of factors: dynamic and static , depending on whether they vary at runtime. Dynamic fac-tors include the input data rates to the PE, the distribution of input data types, and data content. Static factors include the PE code, the PE arguments, stream flowspecs and sys-tem configuration such as the communication model. One factor which could be regarded as dynamic but is treated as static for modeling purposes is the nature of physical resources given to the PE, for example, whether it is run-ning on generic x86 hardware or special-purpose processors. Also, the data content is difficult to characterize/represent in the general case, so in the interest of simplicity we do not consider it. PEs whose behavior is adaptive to available resources may further depend on the time history of system load and traffic. The use of the static PE attributes are discussed later, in Section 4.

We also remark that RF s are further classified as source , sink , or transform RF s, depending on whether the PE is a source PE (feeding from primal streams only), a sink PE (not writing any streams, except, perhaps, to disk), or a transform PE (everything else), respectively. Our discus-sion in this section assumes that the PE is a transform PE (with both input and output streams), but the methodology and techniques naturally carry over to the other types. The notion of PE types will re-appear when we address RF model management in Section 4.
A challenge in modeling the CPU needs of a program is that this demand will vary depending on the specific CPU being used. To enable us to construct a general model that can be used across all nodes in a heterogeneous cluster, we use MIPS as a measure of CPU demand. The MIPS used here is the processor BogoMips [15] reported by the Linux kernel. The MIPS consumed by a PE is calculated by multiplying (a) the CPU time fraction used by a PE on a specific node (reported by the OS) and (b) the total BogoMips of the cores on a processor. In spite of the known limitations [15] of the BogoMips measure, this approach is still useful as it allows us to generalize across CPU speed variability within the same processor family.

For processing resources, a natural model is to consider a processing cost per incoming data object. In a queueing theory sense, this is the  X  X ervice time X  per request. Even though there is not necessarily a well-defined  X  X ervice time X  for each incoming data object in streaming systems, in the aggregate, one may expect the CPU requirements of a PE to scale proportionally with input rate. For simplicity we consider a linear scaling based on the total data rate into the PE; we do not distinguish the rates on individual ports.
To validate this model, we study the behavior of PEs in our test applications. Some examples are shown in Figure 3. For a majority of PEs, the MIPS -per-datum model seems to hold true. This seems to be the case for both heavier, com-putationally more demanding PEs (Figure 3(a), Figure 3(b)) Figure 4: Internal structure and RF of a fused PE from VWAP Figure 5: Maximum MIPS profile (DAC LoadDiffu-sion.JoinOperator) as well as lighter PEs that use few MIPS per datum (Fig-ure 3(c), Figure 3(d)). In the case of the SPADE -based applications, this model applies even for those PEs which are composed (fused) from simpler SPADE operators. As an example, the PE shown in Figure 4(a) is composed of an aggregator , a functor and a join . Yet, the MIPS profile follows the same linear MIPS -per-tuple pattern as seen in Figure 4(b). Surprisingly, even though a join operation may produce output proportional to the product of its inputs, many PEs containing joins show this linear behavior.
Some PEs, on the other hand, notably from the DAC ap-plication, consume a lot of CPU even when the input rates are not very large, effectively saturating the processing node. One example is shown in Figure 5. The PEs shown are single-threaded, and the BogoMips capacity of one core of the node is 4000 MIPS . However, even such PEs can be mod-eled using the linear model described in (1). Some PEs in our example applications do not show a linear relationship between CPU and input rate, but ultimately, we get good predictions in spite of this fact.

Based on our experience, we present our initial candidate model for CPU. As a function of the input rates r i k on all input ports i of PE p , the model (1) predicts the PE X  X  MIPS usage ( m p Here, IPorts is the set of input ports to PE p , and M p rep-resents the maximum MIPS that can be allocated to the PE p . This limit often occurs due to system considerations: the maximum MIPS on any processing node in the system, for instance. Even when this is not the case, the limit allows us to place bounds on the search space explored by SODA . In this model, the term a p represents the best-fit slope of the MIPS needed by the PE as a function of its input rate.
To illustrate, consider the DAC Load Diffusion PE illus-trated in Figure 3(a). For this PE, the coefficients M p a p in (1) can be derived from fitting the best linear model to the data, yielding M p = 3000, and a p = 200. Also con-sider the Join PE from DAC, as illustrated in Figure 5. By recognizing that the resource usage of such PEs is largely determined by the maximum MIPS available, we can set a p sufficiently large (in this case larger than 50000), and set M p = 4000 (from the observed data). Now the first term in (1) dominates the second, resulting in a MIPS prediction m p of 4000 for any sufficiently large data input rate.
An alternate approach to modeling PE-level RF s is to build RF s for each operator and then combine them to get whole-PE RF s. We have found that for any non-trivial op-erator topology within a PE, it is not obvious how the RF s should be combined. The PE-level RF seems to depend on the details of the operators X  implementations (such as their multi-threaded behavior) as well as their connection topol-ogy. In any case, we need a whole-PE approach to model PEs which may not be created from operators. Hence, we use an approach that does not depend on the intra-PE op-erator graph.
Output rates for most streaming PEs are relatively easy to model, since in many cases they have a 1:1 relationship to the inputs, meaning that for every incoming tuple, an outgoing tuple is produced. Operations that do not fit this pattern include filtering, aggregation and timer-based data output. We use the following model for output rate: of PE p . Thus, for PEs that have a 1:1 relationship to the inputs, b p is equal to 1, and R p is set to the largest data rates seen at the output port. Consider the PLLTest PE from Fab, illustrated in Figure 6. From the data, we observe that R p = 5. Filtering PEs will have b p &lt; 1. Tuple count-based operations (that produce an output for every k input tuples) will have the slope b p = 1 /k . PEs that produce output based on timers are the exception, requiring a different model since they are independent of the input rate. For such output ports of PEs (output every T time units), the model takes the form r = 1 /T . A parametric RF can be built for each output port of a PE. Such an RF first describes the type of model considered, followed by all the relevant parameters.

This study also reveals a methodological challenge: the stable flow balance during this workload generator driven data collection can result in very narrow input rates observed at the PEs. This can result in skewed data for the resource function learning. One way to address this is to explicitly  X  X weak X  the input rates to each PE to ensure coverage over some reasonable range, so that the function fitting will be valid and generalizable outside the range observed during the calibration step. Figure 6: Linear output rate profile (Fab PLLTest PE)
To evaluate the RF s, we run the applications as described in Section 2.3, and look at the following application and infrastructure level metrics: The first metric (ingest rate) is the most tangible measure of system performance for streaming systems. The latter two metrics, in conjunction, illustrate the quality of the place-ment of PEs to processing nodes. Given the same stream affinity, smaller the maximum node utilization the better. Given the same maximum node utilization, the higher the stream affinity the better. These metrics are computed from the raw system metrics such as CPU usage per PE and traf-fic consumed and produced by each PE.
This section describes the experiments shown in Figure 1 and presents additional results. We deem as  X  X ood X  the RF s which are derived from data collected from a run (denoted HAND) in which the PEs are placed by an expert. We then start systematically degrading these RFs. The expert place-ment is an allocation of PEs to processing nodes determined via a trial-and-error process. To illustrate the effectiveness of these learned RF s, we compare them to the performance of the HAND placement. Note that the RF s are not tuned by hand; their parameters are learned automatically by the system using the models described in Sections 3.2 and 3.3.
To analyze the deterioration of RF s carefully, we parame-terize the de-tuning of the RF s using a parameter  X  . (When  X  = 1, the RF s are not modified.) Since our modifications involve generating random numbers, to ensure that these modifications are consistent across runs and values of  X  , we pre-generate a sequence of random numbers, and use the same sequence for all the runs. This is equivalent to seeding our random number generator with the same value for each run.

We now describe precisely how each term in an RF is changed, given  X  . Fix a particular value of  X  . We mod-ify each term t in an RF by a factor  X  , as follows. We draw two random numbers, say r 1 and r 2 . The first number r determines how much the term t gets modified, and the sec-ond number determines whether t is increased or decreased. Let round ( a,b ) denote the value of a rounded to the nearest multiple of b . We set  X  = 1 + round ( r 1  X  (  X   X  1) , 0 . 1); Thus,  X  is some multiple of 0 . 1 between the values 1 and  X  , deter-mined by the value of the random number r 1 . If r 2 &lt; 0 . 5, we multiply t by  X  . Otherwise we divide t by  X  . Observe that when  X  = 1,  X  = 1, and t is not modified. Further-more, as  X  is increased, the amount of de-tuning increases probabilistically. This allows us to modify each term in the RF s in a controllable fashion, analyzing the performance of SODA as the RF s degrade. Since we use the same sequence of random numbers, each term is modified using the same r ,r 2 in all the runs. Different  X  values will result in different values of  X  that are strongly correlated with  X  , as desired. As we increase  X  to  X  ,  X  also increases to  X  (for positive r ), and the RF term t either increases to  X  or decreases to 0, depending on whether r 2 is smaller or larger than 0 . 5. We illustrate, using RF modeling, the CPU usage of the DAC Load Diffusion PE (see Figure 3(a)). The RF for this PE has two terms, M p = 3000 and a p = 200. Consider the first term. Suppose the two random numbers r 1 r 2 are 0 . 84 and 0 . 39. When  X  = 3,  X  = 2 . 7, and M modified to 8100. On the other hand, when  X  = 5,  X  = 4 . 4, and M p is modified to 13200. If  X  = 1, the value of M is unchanged. Observe that larger values of r 1 result in a larger perturbation for a given  X  , and also that if r 2 &gt; 0 . 5, the value of M p would have decreased instead. The effect of making the RF s less congruous is to make SODA  X  X  estimates of the resource requirements of the vari-ous PEs more inaccurate. SODA first determines the CPU requirements of the PEs (and traffic between PEs) in the submitted job in the process of maximizing the net impor-tance of the system. These numbers are then used as input to a second phase to allocate the PEs to nodes in an in-telligent manner (load-balancing the nodes and minimizing inter-node traffic simultaneously). Less congruous RF s re-sults in worse estimates and thus less intelligent placement.
These effects are shown in Figure 1 and Figure 7 for two applications: DAC and SKA. We also show the performance of the expert placement (HAND) and the RND scheduler that assigns PEs to nodes in the cluster randomly. Given our cluster size, the Fab and VWAP applications did not present enough work, so we do not use them in the evalua-Figure 7: Effect of scheduling with bad resource pro-files on node utilization and intra-node traffic tion here. We see that for SKA there is a systematic drop in performance (ingest rate) as the RF s are made more in-congruous, whereas for DAC, the performance is relatively insensitive to RF perturbation. In both cases, we see that the ingest rate of congruous RF s is comparable to that of HAND. This illustrates that our simple RF s encapsulate all the necessary information for the runtime scheduler to place the application as well as a domain expert. In other words, our RF s are effective in practice, with not much room for improvement in performance, at least for the applications in our testbed. In SKA, we see that RND performance is considerably worse than SODA with congruous RF s. On the other hand, the performance of the RND placement on DAC is close to that of SODA with congruous RF s (see Figure 1), even though RND uses more nodes than SODA with congru-ous RF s, thus sending a larger traffic load on the network (smaller stream affinity).

This behavior of the RF sensitivity as well as effectiveness of RND placement is explained by considering the overall node utilizations of the various scenarios (see Figure 7). We see that in general, DAC has low average utilization, imply-ing it is very over-provisioned. In this case, the effect of a poor placement (in terms of network traffic load and node utilization) on ingest rate is not going to be significant until the placement gets dramatically tweaked. For SKA, on the other hand, some PEs are quite computationally intensive. Here, the nodes are well utilized in a placement computed with congruous RF s, and the effect of making the RF s less congruous is to create bottlenecks. This increases the max-imum node utilization while decreasing average utilization. For SKA, the RND case also seems to utilize the nodes well, but because it does not account for intra-node traffic, its performance is much lower than SODA with congruous RF s.
Thus, we see that in the under-utilized case, the perfor-mance can be less sensitive to RF incongruity than under higher node utilizations. On the other hand, with larger workloads (higher node utilizations as in SKA), and with mixed application workloads, the performance of the sched-uler SODA deteriorates significantly when using incongruous RF s as input data.
In this section, we describe our experience and lessons learned with more advanced models than the simple models presented in Section 3.2 and Section 3.3. We also consider the impact of alternate data transport mechanisms, from the perspective of both the RF s and the scheduler itself.
In the preceding discussion, the RF was a function from the input rates to either MIPS or to the output rate. This was because System S , by default, operates in reliable data transport mode. In other words, no tuples are dropped, and queues get backed up if the PEs do not get sufficient re-sources. As a result, in these RF s, input rates are the only independent variables. We also refer to this mode of opera-tion, and the corresponding RF s, as the  X  X o-drop X  model.
On the other hand, System S can also operate in an un-reliable mode, in the sense that reducing the MIPS allocation to a PE effectively forces tuple drops. We call this the  X  X rop X  model. It affects the RF and the scheduler primarily in the following way. The model we described previously is a parametric model. It assumes a particular formula and then determines param-eters that best fit the data. If the formula is wrong, then even the best fit model will be a bad fit. An alternative is to use a non-parametric model. Non-parametric models, as the name suggests, do not assume a particular form of the resource function.

In particular, we elaborate on our experiences using one particular kind of non-parametric model for the RF s: a de-cision tree [9], with the  X  X o-drop X  mode mentioned above. Although the resulting RF s for a decision tree based model cannot be described as compactly as the parametric RF s, we hoped that using these more accurate RF s would result in better scheduling results. The decision tree is trained using training data, with the CPU usage and input rate as the classification attributes. The output of the decision tree is an output rate, which is used by the scheduler. Given a set of training data, the output would be one of the values from the training set.
However, our initial decision-tree-based RF s resulted in bad scheduling decisions because all the PEs were allocated low amounts of resources. Upon investigation, we learned that the data were noisy. The noisy data resulted in de-cision tree outputs that did not satisfy the implicit form assumptions by the scheduler (listed in Section 2.1.1).
As a general trend, when MIPS increases, the output rate also increases. However, because the data were noisy, there were cases where MIPS 1 &lt; MIPS 2 but orate 1 &gt; orate When the SODA scheduler makes RF queries and encounters a point where increasing the MIPS decreases the output rate, the scheduler naturally  X  X ssumes X  that further increasing the MIPS will not produce a higher output. As a consequence, the source PEs (and correspondingly, the downstream PEs) end up getting too few MIPS . The question becomes how to deal with this mismatch in the data and the scheduling algorithm.

Our first response was cleaning up the data by discarding points that caused the problem. Once those outlier points had been removed, the resulting RF had a stair-step shape as in Figure 8. These RF s still had similar problems as the previous ones, for a very similar reason. When the SODA scheduler encounters a flat portion of the function, where MIPS 1 &lt; MIPS 2 and orate 1 = orate 2 , it also does not probe further. (Essentially, it treats the RF as if the first flat spot lasted forever.) Thus, the MIPS assignments were again too small.

As a next step, we tried interpolating between the outer points (see the envelope in Figure 8), so long as there was only one input. This model is effectively piecewise-linear and not truly non-parametric. The results achieved were not significantly better, so we discarded that approach as well. If there is more than one input, it is difficult to see how to ensure that the surface is increasing in both directions, and we did not test models of that type.

We also considered a parametric model that did not in-clude the maximum function. The problem encountered there was quite interesting: Unbounded, the SODA sched-uler probed too far, finding improvements that could never be realized. Though the optimization was not affected, the running time became much longer.

From these different scenarios, we learned two lessons about the RF s. First, the RF s must initially be strictly in-creasing. Once they stop increasing, SODA will stop further exploration. Second, they must eventually reach threshold where they stop increasing. While these lessons are spe-cific to the SODA scheduler, they should be applicable to other schedulers which use RF s as their inputs. Further-more, because of the fundamentally different way in which SODA works in the no-drop case, it may be possible for other model types to be effective in the no-drop case; this is a topic of future research.
The study of real PEs in the previous section indicates that an empirical approach based on collecting a few ob-servations from each PE and constructing a model could be a simple and practical approach to obtaining useful RF s. While the previous section addresses the mathematics issues, it does not touch the management issues. It leaves several questions unanswered:
Our observation is that even though there are a myriad of factors (in addition to input rate) that affect a PE X  X  be-havior, a PE will not be run in every possible configuration. Thus, rather than building a model that explicitly tries to model all these factors, we build and manage separate mod-els for different combinations of those factors. This multi-plicity of models raises the need for managing the models as well as the raw metrics data that is collected from the sys-tem. Specifically, two aspects must be addressed: (a) given a specific instance of a PE in a specific job that is submitted to the system, which model should be used by the scheduler? (b) given an observation of the resource usage of a specific PE instance, which models should be updated, and how?
To facilitate the model management, each PE is associated with a multi-part signature . We learn and maintain an RF for each signature. For a specific PE, the scheduler uses the signature to decide which RF should be used. In our system, we use the following four parts of the signature, in order of increasing specificity:
Figure 9: Hierarchical PE signature management The signature captures some key attributes of the PE which are knowable at job submission time . This allows the sched-uler to choose an RF before the PE even begins execution. After execution, the PE RF may be refined further based on ongoing observations, but the initial lookup addresses the bootstrap problem.

The collection of signatures constitutes a hierarchical or-ganization, as depicted in Figure 9. There is an RF for each node in this hierarchy. The node labeled  X  bGlm  X  represents all PEs whose executable hashes to that string. Its two child nodes represent that PE executable being run with two dif-ferent command-line arguments. Thus each node in the tree represents a generalization of all its children. To lookup the RF for a specific PE, we find the most specific node which matches the PE signature. In the figure, the lookup for  X  bGlm |
ABCD | EFGH  X  stops at the PE level node  X  bGlm  X  because the rest of the signature represents entries that are not in the database (yet). Thus, if specific information about the PE in that context was available, that is the information that would be used. If no specific information was available, the system uses information based on only the executable. In the case of a brand new PE that has never been run before in any configuration, the system uses either (a) a general-ized RF based on the root node (PE type), or (b) a default type-based RF which is hand-populated by us based on our calibrations from some earlier System S applications.
Analogously, when a new data point is collected for a PE, we update the model at each of these three levels, starting at the most specific node and propagating up the tree. Thus, an observation &lt; r I j ,m p ,r O k &gt; for PE p with input port j and output port k , whose signature is  X  bGlm | BTI5 | WMor  X  can update not only the model at the most specific node, but at every generalization above it in the path to the root, namely  X  bGlm | BTI5  X  and  X  bGlm  X . Thus, a leaf model reflects observations about the PE in the most specific context, while an internal tree node generalizes data across the sub-tree rooted at that node. When a known PE is encountered in a new context, we can obtain some information about that PE X  X  behavior by using this generalized information. To highlight the possibility of reuse in actual applications, Table 1 shows the number of unique nodes at each level in the tree for our applications. SKA and VWAP (and most SPADE -based applications) do not use flowspecs on their streams, so each argument-level node has only one child node (the  X  X ull X  flowspec). However, these applications con-tain several replicas of the same PE, executing with different arguments. This indicates that maintaining the executable-level information is likely to be useful if we encounter a new PE with a different set of arguments than seen before.
In the literature on resource allocation and scheduling in distributed systems, the resource requirements are typically assumed to be known or given. Much of the other known modeling work has occured in the context of single and multi-tier distributed systems. In [12], the authors develop and use linear models for CPU, disk and memory demands based on incoming workload rates, similar to our models. A more complex, analytical queueing model of multi-tier ser-vices is developed in [13]. However, this model is difficult to apply to streaming systems which are not neatly organized into tiers  X  in general they are directed graphs, may have cycles and little identifiable substructure.

In general cluster environments, [14] use kernel-based mon-itoring tools to learn application profiles in terms of stochas-tic token-bucket models of CPU and network usage. This approach builds an application-level workload-independent usage profile, and schedules to the tail of the distribution. In our case, for streaming applications, the workload data rates are expected to vary widely, and the system is expected to be quite dynamic. As we see, PEs are very sensitive to the incoming data rates, so a rate-sensitive model is needed to ensure responsiveness to changes in resource demands.
For streaming systems, a cost-per-tuple model is also pro-posed [19] in the context of the Borealis system. Their op-erators, however, are much simpler (like SPADE operators) compared to the PEs in our system. We improve on their work by showing that even complex PEs can be modeled using a similar approach, and further we propose a scheme for managing and generalizing these models. curate due to interference caused by colocation of PEs on a node. They mention that developing models in heteroge-neous clusters is a challenge, but others [8] have suggested a solution involving parametric cross-architecture models.
Model management is an even less discussed topic. For the profiling step, some authors [4, 12] suggest running the applications on idle nodes for accurate measurements.
In this paper, we have presented the challenges of predict-ing the CPU and network usage of PEs in System S appli-cations. An empirical study of PEs from applications in a System S testbed reveals that simple piecewise linear models based on their input rates are sufficient for modeling these PEs, which is encouraging given that some of these PEs per-form relatively complex analytics. We have also presented an approach based on hierarchical PE signatures for man-aging and updating these models that addresses the issues of getting usable models for new PEs and allowing one PE X  X  model to be used effectively for another PE. Although not discussed in this paper, our RF s are dynamically updated based on new observations from the running PEs, this al-lows the SODA scheduler to respond to dynamic changes in resource demands of the applications.

Although we find that many PEs can be modeled using these linear models, there will be PEs that do not fit into this scheme. Although our initial attempts with non-parametric approaches were not very successful, we aim to refine them and pursue other advanced techniques that can capture a larger set of PEs. Our current models also are limited to handling CPU and network, and may even be generalizable to disk resources [12]. However, modeling memory consump-tion is still an open issue. In general, since PEs can arbi-trarily allocate memory at runtime, this is a difficult issue. Currently SODA relies on PE developers to provide hints about the memory needs of their applications, but an auto-mated RF -based approach would allow even the memory de-mands to be taken into account during scheduling. Finally, our use of the MIPS metric does not generalize well across architectures. For clusters of truly heterogeneous nodes, es-pecially ones containing specialized resources (such as the Cell processor), a more generalizable metric and model (for example, [8]) would be very useful. [1] D. J. Abadi, Y. Ahmad, M. Balazinska, U. Cetintemel, [2] L. Amini, H. Andrade, R. Bhagwan, F. Eskesen, [3] H. Andrade, B. Gedik, K.-L. Wu, and P. S. Yu.
 [4] M. Aron. Differentiated and Predictable Quality of [5] A. Biem. Imaging for next-generation radio telescopes. [6] T. J. Cornwell, K. Golap, and S. Bhatnagar. W [7] B. Gedik, H. Andrade, K.-L. Wu, P. S. Yu, and [8] G. Marin and J. Mellor-Crummey. Cross-architecture [9] T. M. Mitchell. Machine Learning . McGraw-Hill, 1997. [10] R. Motwani, J. Widom, A. Arasu, B. Babcokc, [11] D. Sankus, R. Redburn, D. S. Turaga, M. C. Johnson, [12] C. Stewart and K. Shen. Performance modeling and [13] B. Urgaonkar, G. Pacifici, P. Shenoy, M. Spreitzer, [14] B. Urgaonkar, P. Shenoy, and T. Roscoe. Resource [15] W. van Dorst. BogoMips mini-Howto. [16] J. Wolf, N. Bansal, K. Hildrum, S. Parekh, D. Rajan, [17] J. L. Wolf, N. Bansal, K. W. Hildrum, S. Parekh, [18] K.-L. Wu, P. S. Yu, B. Gedik, K. W. Hildrum, C. C. [19] Y. Xing, J.-H. Hwang, U.  X Cetintemel, and S. B. [20] S. Zdonik, M. Stonebraker, M. Cherniack,
