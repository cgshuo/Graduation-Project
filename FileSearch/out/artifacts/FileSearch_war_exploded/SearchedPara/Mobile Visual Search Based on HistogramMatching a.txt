 Content-based image retrieval (CBIR) is always a hot topic to study these years [1]. With the steadily growing amounts of mobile devices, a new type of CBIR technique, mobile visual search [2], is attracting keen attention of researchers. In mobile visual search, there are several significant challenges: limited wireless network bandwidth, small mobile battery capacity, allowing little memory space to store features and the requirement of feature interoperability [3]. To address these challenges, the ISO/IEC moving pictures experts group (MPEG) drafts the compact descriptors for visual search (CDVS) [4].
 visual descriptors are extracted and compressed on the mobile devices, and the retrieval is performed on the remote server using the received descriptors from the mobile devices. In this paper, we focus on the highly efficient server-end image retrieval algorithm design. query object by selecting a region of the query image, and then the feature detec-tion module extract local features such as SIFT [5] and SURF [6]; the retrieval system returns a ranked list of images that contain the same object based on the Bag-of-Words (BoW) [7] signature or global descriptors such as fisher vec-tors (FV) [8] and vector of locally aggregated descriptors (VLAD) [9]. In this paper, the above word  X  X eature X  refers to the original uncompressed key point, and the word  X  X escriptor X  means the encoded local feature or the aggregated global feature. However, the BoW signature and global descriptors are gener-ated based on the orderless local features, and thus lead to the disregarding of information about the spatial layout of the features. To further improve image retrieval performance, Philbin et al. [10] propose to add an efficient spatial ver-ification to rerank the results returned from the BoW model. Similarly, in work [11] the authors first retrieval videos using the weighted frequency vector and then rerank them based on the spatial consistency measure. The mobile visual search requires low memory which makes BoW based approaches unsuitable for mobile search tasks [3]. Thus, the CDVS standard adopts the Scalable Com-pressed Fisher Vector (SCFV) [12] to conduct image retrieval, and then reranks the returned results by using geometric consistency check (GCC) which includes a ratio test and a fast geometric model estimation [3]. However, the statistically information of image database, such as  X  X nverse document frequency X  ( idf ), is ignored.
 First, a short visual codebook is generated based on the database descriptors to represent the statistical information of the dataset. Then, an accurate local descriptor similarity cost (LDSC) is computed by merging the  X  X erm frequency-inverse document frequency X  ( tf-idf ) weighted histogram matching and the ratio based weighting strategy in CDVS. At last,both the global descriptor match-ing score (GMS) and the LDSC are summed up to rerank the retrieval results according to the learned zone weights.
 the mobile visual search techniques from both mobile-end and server-end. In Section 3, we present details of our proposed image retrieval algorithm. In Section 4, we discuss the performance comparisons and then in Section 5 we conclude our paper. The CDVS standard defines the feature extraction process and 6 different query descriptor lengths (512 bytes, 1KB, 2KB, 4KB, 8KB and 16KB) to support different scenarios [3]. The standardized CDVS bitstream makes the interoper-ability of the descriptors from different devices possible, and CDVS standard provides a matching mechanism for descriptors with different coding rates. For a typical mobile visual search application, there are two stages to perform: the mobile-end compact descriptor extraction and the server-end image retrieval. 2.1 Mobile-end Compact Descriptor Extraction The compact descriptor of the query image is extracted by the mobile device, and then transmitted to the remote server to conduct visual search. Figure 2 outlines the work flow of the mobile-end compact descriptor extraction, which includes seven building blocks: interest point detection, local feature selection, local feature description, local feature compression, local feature aggregation, local feature location compression and CDVS encoding.
 (ALP) detector [3] to find the interest points by approximating the result of the Laplacian of Gaussian (LoG) filter. Secondly, a subset of local features is selected to meet the bandwidth limitation according to a statistically learned relevance measure, which indicates the priori probability of a feature from query image matching a feature of database image correctly. After local feature selection, each picked local feature is described as original 128-dimensional (1024 bits) SIFT vector [5]. Then, on the one hand, CDVS standard adopts a SCFV to aggregate the local features to build a global descriptor for the query image; on the other hand, the CDVS standard adopts a transform coding scheme followed by a scalar quantization and entropy coding to compress the selected local SIFT features. Besides, the local feature location compression is performed to record the x and y location information, which will be used in the GCC step in server-end image retrieval. At last, in CDVS encoding module, the global descriptor, the compressed local features and the coded locations are merged to produce the CDVS bitstream. 2.2 Server-end Image Retrieval In the remote server, with the same extraction flow as the mobile-end, a set of global and local descriptors of the database images are extracted, and then they are indexed to build a global database and a local database, respectively. With the received query bitstream, which is extracted by the mobile device, the server will perform image retrieval according to the framework in Figure 3.
 stream by the CDVS decoding module. Second, the query global descriptor is compared with each global descriptor in the global database, and based on the GMS a shortlist with the top ranked N images, such as 500, is returned. In the GMS comparison, the Hamming distance-based similarity score S GMS [3] is computed according to where X and Y are the SCFVs of two images. In CDVS, SCFV is built based on a selected subset of Gaussian components (512 in total) from the Gaussian Mixture Model (GMM). In Equation (1), b X i = 1 if i th Gaussian is selected, if the correlation weight of i th Gaussian between X and Y , respectively. Third, an exhaustive pairwise comparison is performed to find all matched local descriptor pairs between the query image and each candidate image in the shortlist. Then, the ratio test and GCC are conducted to remove the wrong matched pairs, which are also called outliers. The left P inlier good match pairs are used to generate the matching score L between two images. At last, the reranked final image retrieval results are generated according to L .
 In Equation (2), cos is the cosine transform function, and min i / smin i is the ratio between the distance of the closest neighbor and that of the second-closest neighbor in computing the i th inlier matching pair [5]. In this section, we first present our proposed image retrieval architecture, and then show our designed LDSC computing scheme. Finally, we illustrate our zone weight learning (ZWL) based image reranking algorithm, which takes both GMS and LDSC into consideration. 3.1 Image Retrieval Architecture illustrated in Figure 3: first generate shortlist and then perform image rerank-ing. As shown in Figure 4, the main differences between our proposed image retrieval architecture and the typical CDVS image retrieval system come from the following three aspects.
 retrieval system. The local database is a collection of local descriptors, and we cluster them to create a l size, such as 300, codebook C = ( c 1 ,c 2 ,...,c l ) by using k-means algorithm. Based on the codebook, we can quantize and represent the local descriptors of database images as visual words and then compute the idf for each visual word. The idf is calculated according to Equation (3). where c i ( i from 1 to l ) is the i th visual word of the codebook, N is the number of database images and N i is the number of images containing visual word c i . Then the idf weighting table is in the image reranking stage.
 including the GMS, the pre-calculated idf weighting table and the local descrip-tor codebook. It is noteworthy that the GMS corresponding to each image in the shortlist is generated in the global matching stage.
 we add tf-idf weighted histogram matching to compute the LDSC. Moreover, we utilize both the GMS and the LDSC to rerank the shortlist based on the ZWL. 3.2 LDSC Computing Based on tf-idf Weighted Histogram LDSC represents the similarity between the query image local descriptor (QLD) and the reference image local descriptor (RLD) of the database. We use the following Equation (5) to compute LDSC.
 where K is the tf-idf weighted histogram matching score (HMS), L is the rerank-ing criteria used in original CDVS reference model as shown in Equation (2), b is a constant to balance K and L . The key of computing LDSC is to find K . matching for the QLD and a RLD of the database first, and then conduct GCC to remove the wrongly matched pairs. The left inlier pairs are used to compute K . Finally, we compute LDSC based on K and L .
 where q i ( i from 1 to m ) and r j ( j from 1 to n ) are two sets of local descriptor points in CDVS standard corresponding to the query image and a database reference image, respectively. After GCC, we get h number of good matching pairs (the inlier) M , as shown in Equation (7).
 where q io and q jo ( o from 1 to h ) are from Q QLD and R RLD , respectively. Then, q io and q jo are vector quantized to create two weighting histograms H ( Q ) and H ( R ), according to the codebook and idf weighting table. H ( Q ) and H ( R ) are histograms with l bins, corresponding to the l visual words of the codebook. Based on Equation (4), we have where N q ( i ) and N r ( i ) ( i from 1 to l ) denote the counts of query and reference inlier points, contained in Equation (7), which fall into the i th bins of H ( Q ) and H ( R ), respectively.
 in two sets that fall into the same bin, and we use this criteria to evaluate the matching degree of H ( Q ) and H ( R ). Then, we get the tf-idf weighted histogram matching score K where min is the minimum function. score K and CDVS reranking criteria L for Equation (10). 3.3 Image Reranking Based on Zone Weight Learning Generally, image retrieval first sorts the database images based on BoW [7] or global descriptor [12] and yields a shortlist with top ranked images. Then image reranking algorithm will be followed to refine the results based on just local descriptor matching. In this paper, we propose to perform image reranking by using both LDSC and GMS.
 look similar both on the whole and on the part at the same time. In statistical text retrieval system [13], a document can be split into different zones, such as title , abstract and body . In each zone, the document is viewed as a sequence of terms. We usually conduct text retrieval by combining the information of differ-ent zones. For example, we often retrieval a document like this: find documents with  X  X omputer X  in the title and  X  X dam X  in the author list and the phrase  X  X ys-tem architecture X  in the body . The matching score of different zones are weighted and added together to sort the documents. Similarly, in this paper we treat the global information of an image as global zone (similar to the title or abstract zones in text retrieval) and local information as local zone (similar to the body zone in text retrieval). We take both global zone and local zone matching scores into consideration for image reranking.
 where S GMS is the GMS presented in Equation (1) and S LDSC is LDSC illus-trated in Equation (10). In Equation (11) ,  X  and (1  X   X  ) are the weights for global zone and local zone, respectively. In the following, we will describe the proposed zone weight learning method to find  X  .
 training set images are used to learn the zone weights. For the training set, we match each image with the others, and generate a series of image pairs. Then, we assign each image pair that containing the same targets a label  X  and the image pair containing different targets a label  X  . We select P image pairs with label  X  and N image pairs with label  X  to train  X  . We first need to normalize S GMS and S LDSC of the pairs to a range of [0 , 1] by using min-max normalization, as shown in Equation (12). where x is the input value and y is the normalized output. MIN and MAX are the minimum and maximum of the input values. Then, let Pa  X  and Pa  X  as pairs. For explanation convenience, we randomly select 300  X  label pairs and 300  X  label pairs, and depict the corresponding reranking scores S with different  X  , as shown in Figure 6. We can see that different  X  generate different distributions of S . Then, the question becomes how to choose a proper  X  which generates a S distribution that can be easily classified by a fixed threshold value. We propose to use Equation (14) to train  X  . where C inner represents the concentration degree sum of each class, D inter de-notes the degree of separation between class  X  and class  X  . We will formulate Equation (14) in detail in the following. Corresponding to Pa  X  and Pa  X  , we have with where i from 1 to P and j from 1 to N . Let m  X  (  X  ) and m  X  (  X  ) are the expec-tively. Then we can get the expectation of the whole, including both S 0  X  (  X  ) and S (  X  ). where R  X  = P/ ( P + N ) and R  X  = N/ ( P + N ). Finally, we yield C inner and D ing Equation (19). One simple solution for  X  is directly checking all the results according to Equation (19) by increasing  X  with a small step, such as 0.01, from 0 to 1. Once we obtain the best  X  , we will rerank the shortlist according to where S 0 GMS and S 0 LDSC are the normalized values. Dataset. We evaluate our proposed method on two benchmark datasets: INRIA Holidays [14] and University of Kentucky Benchmark (UKBench) [10]. INRIA Holidays dataset includes 1491 images, and 500 ground truth queries can be used for testing. UKBench dataset contains 2550 tagged ground truth groups, and each group contains 4 pictures of the same object with different views. used to evaluate search performance. The mAP is shown in Equation (21). where i is the i th query index and N is the total queries. M i relevant is the number of relevant images corresponding to the i th query, r is the r th relevant image and P ( r ) is the precision at the cut-off rank of r .
 parts. One small part is used to learn zone weights, and the other part is used to evaluate the visual search performance. 100 images and 300 images are used in  X  training for INRIA Holidays and UKBench, respectively. In CDVS, there are six specified bitrate modes (from 0.5K to 16K), thus we learn six different weights for them. Figure 7 depicts the learned  X  for the two datasets. With the increase of bitrate, more local features are encoded into the CDVS bitstream and thus the reliability of S 0 LDSC becomes strong, which results in the decrease of  X  . art performance. The image reranking method based on spatial verification in [10] and the image retrieval algorithm in CDVS are tested to make comparisons with our work. All the methods are integrated into CDVS reference software test model framework 11 (TM 11.0) [15]. Test results are tabulated in Table 1. The results show that our proposed method achieves the best searching performance among all 3 methods: it outperforms both the method in work [10] and the state-of-the-art method in CDVS reference model TM 11.0 among all bitrate modes. Our method can achieve maximum up to 3% gain in some specific bitrate modes, such as 1K mode for UKBench, compared to the other algorithms. In this paper, we propose a server-end retrieval algorithm for mobile visual search application. First, a short visual codebook is generated based on the database de-scriptors and then an accurate LDSC is computed by merging the tf-idf weighted histogram matching and the weighting strategy in CDVS. At last, both the GMS and the LDSC are summed up to rerank the retrieval results according to the learned zone weights. The results show that the proposed approach outperforms the state-of-the-art performance of CDVS.

