 In many widely-used recommender systems [1], users are provided with a ranked list of items in which they will likely be int erested in. In these systems, which are referred to as top-N recommendation systems, the main goal is to identify the most suitable items for a user, so as to encourage possible purchases. In the last decade, several algorithms for top-N recommendation tasks h ave been developed [12], the most popular of which are the neighborhood-based (which focus either on users or items) and the matrix-factorization methods. The neighborhood-based algorithms [6] focus on identifying similar users/items based on a user-item purchase/rating matrix. The matrix-factorization algorithms [5] factorize the user-item matrix into lower rank user factor and item factor matrices, which represent both the users and th e items in a common latent space.

Though matrix factorization methods have been shown to be superior for solving the problem of rating prediction, item-based neighborhood methods are showntobesuperiorforthetop-N recommendation problem [3,6,9,10]. In fact the winning method in the recent million song dataset challenge [3] was a rather straightforward item-based neighborhood top-N recommendation approach.
The traditional approaches for developing item-based top-N recommendation methods ( k -Nearest Neighbors, or k -NN) [6] use various vector-space similarity measures (e.g., cosine, ext ended Jaccard, Pearson corre lation coefficient, etc.) to identify for each item the k most similar other items b ased on the sets of users that co-purchased these items. Then, given a set of items that have already been purchased by a user, they derive their recommendations by combining the most similar unpurchased items to those already purchased. In recent years, the performance of these item-based neighborhood schemes has been significantly improved by using supervised learning methods to learn a model that both cap-tures the similarities (or aggregation coefficients) and also identifies the sets of neighbors that lead to the best overall p erformance [9,10]. One of these methods is SLIM [10], which learns a sparse aggregation coefficient matrix from the user-purchase matrix, by solving an optimization problem. It was shown that SLIM outperforms other top-N recommender methods [10].

However, there is an inherent limitation to both the old and the new top-N recommendation methods as they capture on ly pairwise relations between items and they are not capable of capturing higher-order relations. For example, in a grocery store, users tend to often buy it ems that form the ingredients in recipes. Similarly, the purchase of a phone is often combined with the purchase of a screen protector and a case. In both of these examples, purchasing a subset of items in the set significantly increases the likelihood of purchasing the rest. Ignoring these types of relations, when present, can lead to subopti mal recommendations.
The potential of improving the performance of top-N recommendation meth-ods was recognized by Mukund et al. [6], who incorporated combinations of items (i.e., itemsets) in their method. In that work, the most similar items were identified not only for each individual item, but also for all sufficiently frequent itemsets that are present in the active user X  X  basket. This method referred to as HOKNN (Higher-Order k -NN) computes the recommendations by combining itemsets of different size. However, in most datasets this method did not lead to significant improvements. We believe th at the reason for this is that the recom-mendation score of an item is computed simply by an item-item or itemset-item similarity measure, which does not take into account the subtle relations that exist when these individual predictors are combined.

In this paper, we revisit the issue of utilizing higher-order information, in the context of model-based methods. The research question answered is whether the incorporation of higher-or der information in the recently developed model-based top-N recommendation methods will improve the recommendation quality fur-ther. The contribution of this paper is two-fold: First, we verify the existence of higher-order information in real-world datasets, which suggests that higher-order relations do exist and thus if prop erly taken into acco unt, they can lead to performance improvements. Second, we develop an approach referred to as Higher-Order Sparse Linear Method, (HOSLIM) in which the itemsets capturing the higher-order information are treated as additional items and their contri-bution to the overall recommendation score is estimated using the model-based framework introduced by SLIM. We conduct a comprehensive set of experiments on different datasets from various applications. The results show that this combi-nation improves the recommendation quality beyond the current best results of top-N recommendation. In addition, we show the effect of the support threshold chosen on the quality of the method. Finally, we present the requirements that need to be satisfied in order to ensure that HOSLIM computes the predictions in an efficient way.

The rest of the paper is organized as fo llows. Section 2 introduces the nota-tions used in this paper. Section 3 presen ts the related work. Section 4 explains the method proposed. Section 5 provides the evaluation methodology and the dataset characteris tics. In Section 6, we provide the results of the experimental evaluation. Finally, Section 7 contains some concluding remarks. In this paper, all vectors are represente d by bold lower case letters and they are column vectors (e.g., p , q ). Row vectors are represented by having the transpose superscript T ,(e.g., p T ). All matrices are represented by upper case letters (e.g., R , W ). The i th row of a matrix A is represented by a T i . A predicted value is denoted by having a  X  over it (e.g.,  X  r ).

The number of users will be denoted by n and the number of items will be denoted by m .Matrix R will be used to represent the user-item implicit feedback matrix of size n  X  m , containing the items that the users have purchased/viewed. Symbols u and i will be used to denote individual users and items, respectively. An entry ( u, i )in R , r ui , will be used to represent the feedback information for user u on item i . R is a binary matrix. If the user has provided feedback for a particular item, then the corresponding entry in R is 1, otherwise it is 0. We will refer to the items that the user has bought/viewed as purchased items and to the rest as unpurchased items.
 Let I be the set of sets of items that are co-purchased by at least  X  users in R ,where  X  denotes the minimum support threshold. We will refer to these sets as itemsets and we will use p to denote the cardinality of I (i.e., p = |I| ). Let R be a matrix whose columns correspond to the different itemsets in I (the size of this matrix is n  X  p ). In this matrix r uj will be one, if user u has purchased all the items corresponding to the itemset of the j th column of R and zero otherwise. We refer to R as the user-itemset implicit feedback matrix. We will use I j to denote the set of items that constitute the itemset of the j th column of R . In the rest of the paper, every itemset will be of size two (unless stated otherwise) and considered to be frequent, even if it is not explicitly stated. In this paper, we combine the idea of high er-order models introduced by HOKNN with SLIM. The overview of these two methods is presented in the following subsections. 3.1 Higher-Order k -Nearest Neighbors Top-N Recommendation Mukund et al. [6] had pointed out that the recommendations could potentially be improved, by taking into account higher-order relations, beyond relations between pairs of items. They did that by incorporating combinations of items (itemsets) in the following way: The most similar items are found not for each individual item, as it is typically done in the neighborhood-based models, but for all possible itemsets up to a particular size l . 3.2 Sparse LInear Method for top-N Recommendation (SLIM) SLIM computes the recommendation score on an unpurchased item i of a user u as a sparse aggregation of all the user X  X  purchased items: where r T u is the row-vector of R corresponding to user u and s i is a sparse size-m column vector which is learned by solving the following optimization problem: The l 1 regularization gets used so that sparse solutions are found [13]. The l 2 regularization prevents ov erfitting. The constants  X  and  X  are regularization parameters. The non-negativity constraint is applied so that the matrix learned will be a positive aggregation of coefficients. The s ii = 0 constraint makes sure that when computing the weights of an item, that item itself is not used as this would lead to trivial solutions. All the s i vectors can be put together into amatrix S , which can be thought of as an item-item similarity matrix that is learned from the data. So, the model introduced by SLIM can be presented as  X  R = RS . The ideas of the higher-order models can be combined with the SLIM learning framework in order to estimate the various item-item and itemset-item similari-ties. In this approach, the likelihood that a user will purchase a particular item is computed as a sparse aggregation of both the items purchased and the itemsets that it supports. The predicted score for user u on item i is given by where s i is a sparse vector of size m of aggregation coefficients for items and s i is sparse vector of size p of aggregation coefficients for itemsets.
Thus, the model can be presented as: where R is the user-item implicit feedback matrix, R is the user-itemset implicit feedback matrix, S is the sparse coefficient matrix learned corresponding to items (size m  X  m )and S is the sparse coefficient matrix learned corresponding to item-sets (size p  X  m ). The i th columns of S and S are the s i and s i of Equation 3.
Top-N recommendation gets done for the u th user by computing the scores for all the unpurchased items, sorting them and then taking the top-N values.
The sparse matrices S and S encode the similarities (or aggregation coeffi-cients) between the items/itemsets and the items. The i th columns of S and S can be estimated by solving the following optimization problem:
The constraint s ii = 0 makes sure that when computing r ui , the element r ui is not used. If this constraint was not enforced, then an item would recommend itself. Following the same logic, the constraint s ji = 0 ensures that the itemsets j for which i  X  X  j will not contribute to the computation of r ui .
The optimization problem of Equation 5 can be solved using coordinate de-scent and soft thresholding [7]. 5.1 Datasets We evaluated the performance of HOSLIM on a wide variety of datasets, both synthetic and real. The datasets we used include point-of-sales, transactions, movie ratings and social bookmarking. Their characteristics are shown in Table 1. The groceries dataset corresponds to transactions of a local grocery store. Each user corresponds to a customer and the items correspond to the distinct products purchased over a period of one year. The synthetic dataset was gen-erated by using the IBM synthetic datase t generator [2], which simulates the behavior of customers in a retail environment. The parameters we used for gen-erating the dataset were: average size of itemset= 4 and total number of itemsets existent= 1 , 200. The delicious dataset [11] was obtained from the eponymous social bookmarking site. The items on this dataset correspond to tags. A non-zero entry indicates that the corresponding user wrote a post using the cor-responding tag. The ml dataset corresponds to MovieLens 100K dataset, [8] which represents movie ratings. All the ratings were converted to one, showing whether a user rated a movie or not. The retail dataset [4] contains the retail market basket data from a Belgian retail store. The bms-pos dataset [14] con-tains several years worth of point-of-sales data from a large electronics retailer. The bms1 dataset [14] contains several months worth of clickstream data from an e-commerce website. The ctlg3 dataset corresponds to the catalog purchasing transactions of a major mail-order catalog retailer. 5.2 Evaluation Methodology We employed a 10-fold leave-one-out cross-validation to evaluate the perfor-mance of the proposed model. For each fold, one item was selected randomly for each user and this was placed in the tes t set. The rest of the data comprised the training set. We used only the data in the training set for both the itemset discovery and model learning.

We measured the quality of the recommendations by comparing the size-N recommendation list of each user and the item of that user in the test set. The quality measure used was the hit-rate (HR). HR is defined as follows, where  X #users X  is the total number of users ( n ) and  X #hits X  is the number of users whose item in the tes t set is present in the size-N recommendation list. 5.3 Model Selection We performed an extensive search over the parameter space of the various meth-ods, in order to find the set of parameters that gives us the best performance for all the methods. We only report the performance corresponding to the param-eters that lead to the best results. The l 1 regularization  X  was chosen from the the regularizations were. The number of neighbors examined lied in the interval [1 support threshold  X  took on values { 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1500, 2000, 2500, 3000 } . The experimental evaluation consists of two parts. First, we analyze the various datasets in order to assess the extent to which higher-order relations exist in them. Second, we present the performance of HOSLIM and compare it to SLIM as well as HOKNN. 6.1 Verifying the Existence of Higher-Order Relations We verified the existence of higher-order relations in the datasets, by measuring how prevalent are the itemsets with strong association between the items that comprise it (beyond pairwise associations). In order to identify such itemsets, (which will be referred to as  X  X ood X ), we conducted the following experiment. We found all frequent itemsets of size 3 with  X  equal to 10. For each of these itemsets we computed two quality metrics. The first is which measures how much greater the probability of a purchase of all the items of an itemset is than the maximum probability of the purchase of an induced pair. The second is which measures how much greater the probability of the purchase of all the items of an itemset is than the minimum probability of the purchase of an induced pair. These metrics are suited for identifying the  X  X ood X  itemsets, as they discard the itemsets that are frequent just because their induced pairs are frequent. Instead, the above-mentioned metrics discover the frequent itemsets that have all or some infrequent induced pairs, meaning that these itemsets contain higher-order information.

Given these metrics, we then selected the itemsets of size three that have quality metrics greater than 2 and 5. The higher the quality cut-off, the more certain we are that a specific itemset is  X  X ood X .

For these sets of high quality itemsets, we analyzed how well they cover the original datasets. We used two metrics of coverage. The first is the percentage of users that have at least one  X  X ood X  items et, while the second is the percentage of the non-zeros in the user-item matrix R covered by at least one  X  X ood X  itemset (shown in Table 2). A non-zero in R is considered to be covered, when the corresponding item of the non-zero value participates in at least one  X  X ood X  itemset supported by the associated user.

We can see from Table 2 that not all datasets have uniform coverage with respect to high quality itemsets. The groceries and synthetic datasets contain a large number of  X  X ood X  itemsets that cover a large fraction of non-zeros in R and nearly all the users. On the other hand, the ml , retail and ctlg3 datasets contain  X  X ood X  itemsets that have signi ficantly lower coverage with respect to both coverage metrics. The coverage characteristics of the good itemsets that exist in the remaining datasets is som ewhere in between these two extremes. These results suggest that the potential gains that HOSLIM can achieve will vary across the different datasets and should perform better for groceries and synthetic datasets. 6.2 Performance Comparison Table 3 shows the performance achieved by HOSLIM, SLIM, k -NN and HOKNN. The results show that HOSLIM produces recommendations that are better than the other methods in nearly all the datasets. We can also see that the incorpora-tion of higher-order information improve s the recommendation qu ality, especially in the HOSLIM framework.

Moreover, we can observe that the greater the existence of higher-order rela-tions in the dataset, the more significant the improvement in recommendation quality is. For example, the most significant improvement happens in the gro-ceries and the synthetic datasets, in which the higher-order relations are the greatest (as seen from Table 2). On the other hand, the ctlg3 dataset does not benefit from higher-order models, since there are not enough higher-order re-lations. These results are to a large ex tent in agreement with our expectations based on the analysis presented in the previous section. The datasets for which HOSLIM achieves the highest improvement are those that contain the largest number of users and non-zeros that ar e covered by high-quality itemsets.
Figure 1 demonstrates the performance of the methods for different values of N (i.e., 5, 10, 15 and 20). HOSLIM outperforms the other methods for all different values of N as well. We choose N to be quite small, as a user will not see an item that exists in the bottom of a top-100 or top-200 list. 6.3 Performance only on the Users Covered by  X  X ood X  Itemsets In order to better understand how the ex istence of  X  X ood X  itemsets affects the performance of HOSLIM, we computed th e correlation coefficient of the per-centage improvement of HOSLIM beyon d SLIM (presented in Table 3) with the product of the affected users coverage and the number of non-zeros cov-erage (presented in Table 2). The correlation coefficient is 0 . 712, indicating a strong positive correlation between the c overage (in terms of us ers and non-zeros) of higher-order itemsets in the dataset and the performance gains achieved by HOSLIM. 6.4 Sensitivity on the Support of the Itemsets As there are lots of possible choices for support threshold, we analyzed the performance of HOSLIM, with varying support threshold  X  . The reason behind this is that we wanted to see the trend of the performance of HOSLIM with respect to  X  . Ideally, we would like HOSLIM to perform better than SLIM, for as many values of  X  , as possible; not for just a few of them.

Figure 2 shows the sensitivity of HOSLIM to the support threshold  X  .We can see that there is a wide range of support thresholds for which HOSLIM outperforms SLIM. Also, a low support threshold means that HOSLIM benefits more from the itemsets, leading to a better HR. 6.5 Efficient Recommendation by Controlling the Complexity Until this point, the model selected was the one producing the best recommen-dations, with no further constraints. However, in order for HOSLIM to be used in real-life scenarios, it also needs to be applied fast. In other words, the model should compute the recommendations fast and this means that it should have non-prohibitive complexity.

The question that normally arises is the following: If we find a way to control the complexity, how much will the performance of HOSLIM be affected? In order to answer this question, we did the following experiment: As the cost of computing the top-N recommendation list depends on the number of non-zeros in the model, we selected from all learned models the ones that satisfied the constraint: With this constraint, we increased the complexity of HOSLIM little beyond the original SLIM (since the original number of non-zeros is now at most doubled). Table 4 shows the HRs of SLIM and constrained and unconstrained HOSLIM. It can be observed that the HR of the constrained HOSLIM model is close to the optimal one. This shows that a simple model can incorporate the itemset information and improve the recommendation quality in an efficient way, making the approach proposed in this paper usable, in real-world scenarios. In this paper, we revisited the research question of the existence of higher-order information in real-world datasets and whether its incorporation could help the recommendation quality. This was done i n the light of recent advances in the top-N recommendation methods. By coupling the incorporation of higher-order asso-ciations (beyond pairwise) with state-of-the-art top-N recommendation methods like SLIM, the quality of the recommendations made was improved beyond the current best results.

