 Random Walk with Restart (RWR) has become an appeal-ing measure of node proximities in emerging applications e.g., recommender systems and automatic image captioning. In practice, a real graph is typically large, and is frequently updated with small changes. It is often cost-inhibitive to re-compute proximities from scratch via batch algorithms when the graph is updated. This paper focuses on the incremental computations of RWR in a dynamic graph, whose edges often change over time. The prior attempt of RWR [1] deploys k -dash to find top-k highest proximity nodes for a given query, which involves a strategy to incrementally estimate upper proximity bounds. However, due to its aim to prune needless calculation, such an incremental strategy is approximate :in O (1) time for each node. The main contribution of this paper is to devise an exact and fast incremental algorithm of RWR for edge updates. Our solu-tion, IRWR , can incrementally compute any node proximity in O (1) time for each edge update without loss of exact-ness. The empirical evaluations show the high efficiency and exactness of IRWR for computing proximities on dynamic networks against its batch counterparts.
 H.3.3 [ Information Search and Retrieval ]: Information Storage and Retrieval Random Walk with Restart; Proximity; Dynamic graph
Measuring node proximities in graphs is a key task of web search. Due to various applications in recommender systems and social networks, many proximity metrics have come into play. For instance, Brin and Page [2] invented PageRank to determine the ranking of web pages. Jeh and Widom [3] proposed SimRank to assess node-to-node proximities. Random Walk with Restart (RWR) [4] is one of such useful proximity metrics for ranking nodes in order of relevance to a query node. In RWR, the proximity of node u w.r.t. query node q is defined as the limiting probability that a random surfer, starting from q , and then iteratively either moving to one of its out-neighbors with probability weighted by the edge weights, or restarting from q with probability c , will eventually arrive at node u . Recently, RWR has received increasing attention ( e.g., for collaborative filtering [1] and image labeling [5]) since it can fairly capture the global structure of graphs, and relations in interlinked networks [6].
Prior RWR computing methods are based on static graphs, which is costly: Given a graph G ( V,E ), and a query q  X  k -dash [1] yields, in the worse case, O ( | V | 2 ) time and space, which, in practice, can be bounded by O ( | E | + | V | ), to find top-k highest proximity nodes. B LIN and NB LIN [4] need O ( | V | 2 ) time and space for computing all node proximities. In general, real graphs are often constantly updated with small changes. This calls for the need for incremental algo-rithms to compute proximities. We state the problem below: Problem ( Incremental Update for RWR ) Given a graph G ,proximities P for G , changes  X  G to G , Compute changes to the proximities w.r.t. q exactly. Here, P is a proximity matrix whose entry [ P ] i,j denotes the proximity of node i w.r.t. query j , and  X  G is comprised of a set of edges to be inserted into or deleted from G .
In contrast with the existing batch algorithms [1, 4] that recompute the updated proximities from scratch, our incre-mental algorithm can exploit the dynamic nature of graphs by pre-computing proximities only once on the entire graph via a batch algorithm, and then incrementally computing their changes in response to updates. The response time of RWR can be greatly improved by maximal use of previous computation, as shown in Example 1.
 Example 1. Figure 1 depicts a graph G , taken from [1]. Given the query u 2 , the old proximities P for G ,and c = 0 . 2 , we want to compute new proximities w.r.t. u 2 when there is an edge ( u 1 ,u 5 ) inserted into G , denoted by  X  G . The existing methods, k -dash and B LIN , have to recompute the new proximities in G  X   X  G from scratch, without using the previously computed proximities in G ,whichiscostly. However, we observe that the increment [  X P ] ,u 2 1 to the old [ P ] ,u 2 is the linear combination of [ P ] ,u 1 and [ P ] with  X  =0 . 25 , X  =  X  0 . 32 , X  =0 . 06 . Hence, there are op-portunities to incrementally compute the changes [  X P ] ,u 2 by fully utilizing the old proximities of P .Asopposedto k -dash and B LIN involving matrix-vector multiplications , computing [  X P ] ,u 2 via Eq. (1) only needs vector scaling and additions , thus greatly improving the response time.
As suggested by Example 1, when the graph G is updated, it is imperative to incrementally compute new proximities by leveraging information from the old proximities. However, it is a grand challenge to characterize the changes [  X P ] ,q terms of a linear combination of the columns in old P ,since it seems hard to determine the scalars  X ,  X ,  X  for Eq.(1). Worse still, much less is known about how to extract a subset of columns from the old P ( e.g., why [ P ] ,u 1 and [ P ] chosen from P in Eq.(1)), to express the changes [  X P ] ,q Contributions. This paper aims to tackle these problems. To the best of our knowledge, this work makes the first effort to study incremental RWR computing in evolving graphs, with no loss of exactness. 1) We first consider unit update , i.e., a single-edge insertion or deletion, and derive an elegant formula that characterizes the proximity changes as a linear combination of the columns from the old proximity matrix. 2) We then devise an incremental algorithm for batch update , i.e., a list of edge deletions and insertions mixed together, and show that any node proximity can be computed in O (1) time for every edge update, with no sacrifice in accuracy. 3) Our empirical study demonstrates that the incremental approach greatly outperforms k -dash [1], a batch algorithm that is reported as the best for RWR proximity computing, when networks are constantly updated.
 Organization. Section 2 overviews the background of RWR. The incremental RWR is studied in Section 3. Section 4 gives empirical results, followed by open issues in Section 5. Related Work. Incremental algorithms have proved useful in various node proximity computations on evolving graphs, such as the personalized PageRank [7] and SimRank [8]. However, very few results are known on incremental RWR computing, far less than their batch counterparts [1, 4, 9]. k -dash [1] is the best known approach to finding top-k high-est RWR proximity nodes for a given query, which involves a strategy to incrementally estimate upper proximity bounds. Nevertheless, such an incremental strategy is approximate : in O (1) time for each node, which is mainly developed for pruning unnecessary computation. In contrast, our incre-mental algorithm can, without loss of exactness, compute any node proximity in O (1) time for every edge update. Moore et al. [10] leveraged a sampling approach with branch and bound pruning to find near neighbors of a query w.h.p. . However, their incremental algorithm is probabilistic .Later, Zhou et al. [9] generalized the original RWR by incorporat-ing node attributes into link structure for graph clustering. Basedonthis,anincrementalversionof[9]wasproposedby Cheng et al. [11], with the focus to support attribute update . It differs from this work in that our incremental algorithm is designed for structure update . Thus, [11] cannot cope with hyperlink changes incrementally in dynamic graphs. [ X ] ,j denotes the j -th column vector of matrix X . e i is the | V | X  1unitvectorwitha1inthe i -th entry.
We formally overview the background of this paper. Graphs studies here are directed graphs with no multiple edges. RWR Formula [1]. In a graph G ( V,E ), let A be the tran-sition matrix ( i.e., column normalized adjacency matrix) of G ,whoseentry[ A ] u,v = 1 d v if ( u, v )  X  E , and 0 otherwise. Here, d v denotes the in-degree of v . Given query node q and restart probability c  X  (0 , 1), the proximity of node u w.r.t. q ,denotedby[ P ] u,q , is recursively defined as follows: where [ P ] ,q is the | V | X  1 proximity vector w.r.t. q ( i.e., the q -th column of matrix P ), whose u -th entry equals to [ P ] e q is the | V | X  1 unit query vector ,whose q -th entry is 1.
Intuitively, [ P ] u,q is the limiting probability, denoting the long-term visit rate of node u , given a bias toward query q .
The RWR proximity defined in Eq.(2) can be rewritten as where I is the | V | X | V | identity matrix.

Existing methods of computing RWR are in a batch style, with the aim to accelerate the matrix inversion in Eq.(3). For instance, k -dash [1] uses LU decomposition and an incre-mental pruning strategy to speed up the matrix inversion.
We now study the incremental RWR computation. Given the old P for G , changes  X  G to G , query q ,and c  X  (0 , 1), the goal is to compute [  X P ] ,q for  X  G . The key idea of our approach is to maximally reuse the previous computation, by characterizing [  X P ] ,q as a linear combination of the columns from the old P . The main result is as follows.
Theorem 1. Any node proximity of a given query can be incrementally computed in O (1) time for each edge update.
To prove Theorem 1, we first consider unit edge update, and then devise an incremental algorithm for batch updates, with the desired complexity bound.
 Unit Update. The update (insertion/deletion) of an edge from G may lead to the changes [  X P ] ,q of the proximity. We incrementally compute [  X P ] ,q based on the following.
Proposition 1. Given a query q , and the old proximity matrix P for G , if there is an edge insertion ( i, j ) into G , then the changes [  X P ] ,q w.r.t. q canbecomputedas y = where d j is the in-degree of node j in the old G ,and [ y ] the j -th entry of vector y .

If there is an edge deletion ( i, j ) from G ,then [  X P ] also be computed via Eq. (4) with y being replaced by y = As opposed to the traditional methods, e.g., k -dash and B LIN , that requires matrix-vector multiplications to com-pute new proximities via Eq.(2), Proposition 1 allows merely vector scaling and additions for efficiently computing [  X P ]
The proof of Proposition 1 is attained by combining the three following lemmas.
Lemma 1. Let A be the old transition matrix of G .If thereisanedgeinsertion ( i, j ) into G , then the new transi-tion matrix  X  A is updated by
If there is an edge deletion ( i, j ) from G , then the new is also updated as Eq. (5) with a being replaced by
Proof. Due to space limits, we shall merely prove the insertion case. A similar proof holds for the deletion case. (i) When d j =0,[ A ] ,j = 0 . Thus, for an inserted edge ( i, j ), [ A ] i,j will be updated from 0 to 1, i.e.,  X  (ii) When d j &gt; 0, all the nonzero entries of [ A ] ,j Thus, for an inserted edge ( i, j ), we first update [ A ] entries of [ A + 1 d j e i e T j ] the elementary matrix property that multiplying the j -th column of a matrix by  X  = 0 can be accomplished by using I  X  (1  X   X  ) e j e T Hence, scaling [ A + 1 d j e i e T j ] Combining (i) and (ii), Eq.(5) is derived.

Lemma 1 suggests that each edge change will incur a rank-one update of A . To see how the update of A affects the changes to P , we have the following lemma.

Lemma 2. Let P be the old proximity matrix for G .If thereisanedgeupdate ( i, j ) to G , then the new proximity  X  P w.r.t. a given query q is updated as where the vector a is defined by Lemma 1.

Proof. By RWR definition in Eq.(3), [  X  P ] ,q satisfies where  X  A is the new transition matrix that is expressed as  X  A = A + ae T j by Lemma 1. Thus, Eq.(8) is rewritten as To solve [  X  P ] ,q and  X  in Eq.(9), we apply block elimination, by using block elementary row operations, and starting with the associated augmented matrix: The final array represents the following equations: Back substitution, along with Eq.(3), yields Eq.(7). Algorithm 1: IRWR ( G, P ,q,  X  G, c ) Lemma 2 tells that for each edge update, the changes to P are just associated with the scaling operation of vector z . However, it is costly to compute z via Eq.(7) as it involves the inversion of a matrix. Lemma 3 provides an efficient way of computing ( I  X  (1  X  c ) A )  X  1 a from a few columns of P .
Lemma 3. Suppose there is an edge update ( i, j ) to G , and a is defined by Lemma 1. Then, ( I  X  (1  X  c ) A )  X  1 with y being defined in Proposition 1.

Proof. Due to space limits, we shall only prove the edge insertion case. A similar proof holds for the deletion case. (i) When d j =0, a = e i . Then, Eq.(3) implies that (ii) When d j &gt; 0, a = 1 d j +1 ( e i  X  [ A ]  X  ,j ). Then,
To solve ( I  X  (1  X  c ) A )  X  1 [ A ] ,j , we apply the property that ( I  X  X )  X  1 =  X  k =0 X k (for X 1 &lt; 1) and obtain
Thus,wehave( I  X  (1  X  c ) A )  X  1 [ A ] ,j = 1 1  X  c ( 1 c Substituting this back produces the final results. Combining Lemmas 1 X 3 together proves Proposition 1. Algorithm for Batch Updates. Based on Proposition 1, we devise IRWR , an incremental RWR algorithm to handle aset X  G of edge insertions and deletions (batch update).
IRWR is shown in Algorithm 1. Given the old P for G w.r.t. query q , and the batch edge updates  X  G , it computes new proximities w.r.t. q in G  X   X  G without loss of exactness. It works as follows. For each edge ( i, j )tobeupdated,itfirst computes the auxiliary vector y from a linear combination of only a few columns in P (lines 2 X 10). Using y ,itthen (i) removes ( i, j ) from  X  G (line 11) and (ii) updates the proximities w.r.t. each remaining node in  X  G (lines 12 X 14). After all the edges are eliminated from  X  G , IRWR finally calculates the new proximities [  X  P ] ,q from y (line 15).
Example 2. Recall P and G of Figure 1. Consider batch updates  X  G ,whichinsertedge ( u 1 ,u 5 ) and delete ( u 4 where ( u 1 ,u 5 ) is given in Example 1. IRWR computes the new proximities [ P ] ,u 2 w.r.t. query u 2 in G + X  G as follows:
For the edge insertion ( u 1 ,u 5 ) , since d u 5 =3 and c =0 . 2 , y =1 . 25  X  [ P ] ,u 1  X  1 . 56  X  [ P ] ,u 5 +0 . 31  X  e 5
Before proceeding with the edge deletion, let us look at the changes [  X P ] ,u 2 (via line 14) for the inserted ( u 1 ,u which explains why the values of  X ,  X ,  X  are chosen for Eq. (1) .
IRWR then removes ( u 1 ,u 5 ) from  X  G (line 11). Using y , it updates proximities w.r.t. u 4 ,u 6  X   X  G (lines 12 X 14). Thus, [ P ] ,u 4 =( . 17 ,. 05 ,. 23 ,. 28 ,. 23 ,. 05 , 0)
Likewise, for the edge deletion ( u 4 ,u 6 ) , d u 6 =1 implies  X  G (line 11). Since  X  G =  X  , the changes [  X P ] ,u 2 for the deleted ( u 4 ,u 6 ) is obtained (via line 15):  X  Correctness &amp; Complexity. To complete the proof of Theorem 1, we notice that (i) IRWR can correctly com-pute RWR proximities, which is verified by Proposition 1. Moreover, IRWR always terminates, since the size of  X  G is monotonically decreasing. (ii) One can readily verify that foreachedgeupdate, IRWR involves only vector scaling and additions, which is in O (1)timeforeachnodeproximity.
We present an empirical study on real and synthetic data to evaluate the efficiency of IRWR for incremental computa-tion, as compared with (a) its batch counterpart B LIN [4], (b) k -dash [1], the best known algorithm for top-k search, and (c) IncPPR [7], the incremental personalized PageRank. Two real datasets are adopted: (a) p2p-Gnutella , a Gnutella P2P digraph, in which nodes represent hosts, and edges host connections. The dataset has 62.5K nodes and 147.9K edges. (b) cit-HepPh , a citation network from Arxiv, where nodes denote papers, and edges paper citations. We extracted a snapshot with 27.7K nodes and 352.8K edges.
 GraphGen 3 is used to build synthetic graphs and updates. Graphs are controlled by (a) the number of nodes | V | ,and (b) the number of edges | E | ; updates by (a) update type (edge insertion or deletion), and (b) the size of updates All the algorithms are implemented in Visual C++ 10.0. We used a machine with an Intel Core(TM) 3.10GHz CPU and 8GB RAM, running Windows 7.
 We set restarting probability c =0 . 2 in our experiments. Experimental Results. We next present our findings. 1) Incremental Efficiency. We first evaluate the computa-tional time of IRWR on both real and synthetic data.
Figures 2(a) and 2(b) depict the results for edges inserted to p2p-Gnutella ( | V | =62.5K) and cit-HepPh ( | V | =27.7K), re-spectively. Fixing | V | ,wevary | E | as shown in the x -axis. Here, the updates are the difference of snapshots w.r.t. the collection time of datasets, reflecting their real-life evolution. We find that (a) IRWR outperforms k -dash on p2p-Gnutella for 92.7% ( resp. cit-HepPh for 97.5%) of edge updates. When the changes are 61.9% on p2p-Gnutella (83.8% on cit-HepPh ), http://www.cse.ust.hk/graphgen/ (a) p2p-Gnutella (d) edge deletions IRWR improves k -dash by over 5.1x ( resp. 4.4x). This is because IRWR reuses the old information in G for incremen-tally updating proximities via vector scaling and additions, without the need for expensive LU decomposition of k -dash . (b) IRWR always is better than B LIN by nearly one order of magnitude as B LIN requires costly block matrix inversions. (c) IRWR outperforms IncPPR for over 95% of insertions, due to the extra cost of IncPPR for doing short random walks. (d) IRWR is sensitive to |  X  G | as the larger |  X  G | is, the larger the affected area is, so is the computation cost, as expected.
Fixing | V | =50K on synthetic data, we varied | E | from 280K to 350K ( resp. from 350K to 280K) in 10K increments ( resp. decrements). The results are shown in Figures 2(c) and 2(d), respectively, analogous to those on real datasets. 2) Exactness. To measure IRWR accuracy, we adopted NDCG l (Normalized Discounted Cumulative Gain) for ranking top-l node proximities with l =20 , 40 , 60, and chose the ranking results of k -dash as the benchmark, due to its exactness. The results on p2p-Gnutella and cit-HepPh are reported in Figures 2(e) and 2(f), indicating that IRWR never scarifies accuracy for achieving high efficiency, superior to other approaches.
We showed how RWR proximities can be computed very efficiently in an incremental update model, where the edges of a graph are constantly changed. We also empirically eval-uated that IRWR greatly outperforms the other approaches on both real and synthetic graphs without loss of exactness. Our future work will further predict up to what fraction of updated edges IRWR is faster than its batch counterparts.
