 We consider the task of automatically phrasing and comput-ing top-k rankings over the information contained in com-mon knowledge bases (KBs), such as YAGO or DBPedia. We assemble the thematic focus and ranking criteria of rankings by inspecting the present Subject, Predicate, Object (SPO) triples. Making use of numerical attributes contained in the KB we are also able to compute the actual ranking content, i.e., entities and their performances. We further discuss the integration of existing rankings into the ranking generation process for increased coverage and ranking quality. We re-port on first results obtained using the YAGO knowledge base.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.3.1 [ Information Storage and Retrieval ]: Content Anal-ysis and Indexing; H.2.8 [ Database Management ]: Database Applications X  X ata mining Entity Rankings; Knowledge Bases
Ranking information based on different criteria is one of the most natural and widely used techniques to condense a potentially large amount of information into a concise form. Cars are compared by gas per mile, websites by page rank, students based on GPA, scientists by number of publica-tions, and celebrities by beauty or wealth. Some rankings are highly subjective, for instance, the ranking of the top-10 movies of all times and need to be gathered through crowd sourcing user votes in public websites such as ranker.com or rankopedia.com. On the other hand, there exists a large amount of rankings that can be derived from publicly avail- X 
This work has been supported by the Excellence Cluster on Multimodal Computing and Interaction (MMCI).
 able data like published census data, meteorological obser-vations, stock market data, product reviews, etc.

In both cases X  X omputed or crowdsourced X  X he topic of the ranking needs to be carefully determined to be seman-tically meaningful. In this paper, we propose the usage of knowledge bases to phrase semantically meaningful rank-ings. With the term phrasing we refer to specifying the topic (aka. name or headline) of the ranking as opposed to actually compute it, cf., Figure 1. We investigate how existing facts of knowledge bases can further be used to compute the full rankings once the topics are phrased. The computed rank-ings can be complemented with existing rankings contained in websites or publicly available (open) databases to improve coverage and information quality of the purely knowledge-base X  X riven rankings.

Applications of automatically derived entity rankings are manifold, in particular, since formulated rankings can be continuously re-computed in case of underlaying data changes. This enables rich services to users and applications, for ad-hoc information demands, real-time event delivery, and deeper analytical insights. For instance, a ranking of the best flue vaccines can be compiled and maintained, based on criteria such as the time to take effect, price, or number of adverse effects. Rankings can also be embedded to websites such as Wikipedia and could trigger the attention of the commu-nity to enable crowdsourced enrichment/completion of the contained information.

We consider the problem of phrasing interesting entity rankings in an automated fashion using facts contained in common knowledge bases. We present the architecture and algorithms of a system that can compute the actual rank-ings of entities and discuss techniques to further blend this information with existing rankings. First results on ranking characteristics based on the YAGO ontology are presented.
Entity rankings are formulated for arbitrary domains and for various ranking criteria. For instance, one could be in-terested in the following rankings:
From these rankings we can identify the domains: US-born physicists, NBA basketball players, and the countries of the world. At the same time, we identify the ranking cri-teria: number of awards, points scored, and number of in-habitants. However, not all combinations between domains and ranking criteria make sense, ranking US-born physicists by the number of points scored makes little or no sense at all. This shows that phrasing ranking topics/headlines in an automated fashion is not a trivial task. Common attempts include crowdsourcing techniques to obtain the topic and content of rankings, implemented in specialized portals like ranker.com and rankopedia.com.
 In this work, we use knowledge bases like YAGO[11] or DBPedia[3] to automatically formulate semantically mean-ingful rankings. The facts are further used to compute the actual entity ranking once the ranking topic is formulated. Specifically, we work with YAGO, but the presented tech-niques are applicable to the general concept of harnessed data facts in form of Subject, Predicate, Object (SPO) triples.
YAGO is a knowledge base that contains over 120 million facts for more than 10 million entities. For example, it pro-vides answers to questions like  X  X hen was Albert Einstein born? X  and  X  X hich prizes did Albert Einstein win? X . Inter-nally, information (facts) in YAGO are stored as Subject, Predicate, Object (SPO) triples, like the following Subject: Predicate: Object: Max Planck bornInCountry Germany Isaac Newton bornInCountry England Albert Einstein bornInCountry Germany Albert Einstein hasWonPrize Nobel Prize Albert Einstein TYPE Scientist
The above listing shows four so called facts, using the predicates bornInCountry, and hasWonPrize and further spec-ifies Einstein to be of TYPE Scientist. With this informa-tion, we can, for instance, find all scientists of German ori-gin. In addition, using the thematic domains specified in YAGO, entity rankings can be grouped into different topics, like  X  X usic X ,  X  X rt X  or  X  X cience X  for instance, thus allowing tailoring rankings to specific interest groups or data analy-sis tasks. These  X  X roup-by X  expressions, are also a key part of data warehousing solutions for online analytical process-ing (OLAP) [6] that are frequently used for well specified and, by domain experts, precisely understood areas, such as business intelligence task of large companies.
 Given a knowledge base in form of Subject, Predicate, Ob-ject triples, the task is twofold, namely to:
Rankings consists of entities that share common proper-ties, for instance, being a scientist. The more constraints are imposed on these properties the less competitive the rank-ing becomes. With a recent study on crowdsourced entity rankings [2] we have shown that there are barely any rank-ings that have 4, or more constraints X  X nd that the interest of users in rankings vanishes drastically with each added constraint. Keeping this in mind, we aim at phrasing rank-ings that contain at most 3 constraints. Similar to frequent itemset mining [1], we introduce a minimum support thresh-old  X  , that puts focus on entity rankings where at least  X  entities qualify. Only with a substantially large  X  , a top-k ranking is competitive enough to be of interest. Consider for instance a ranking of all companies in the world based on revenue. There are several hundreds of thousands compa-nies, and being in the top, say top-50, is quite a distinction. If we put constraints on the company type, say electronics manufacturer headquartered in a small town, it becomes less attractive to be in the top-50. The parameter  X  has immedi-ate affect of the number of rankings to be considered. On the other hand, the length of the individual rankings is defined on the per ranking basis, depending on characteristics like number of qualifying entities.
Similar to the famous a priori principle of frequent item-set mining [1], we iteratively generate rankings in a bottom up fashion X  X tarting from rankings with one constraint and consecutively adding constraints. In each iteration, to cre-ate rankings with n constraints, we take a ranking from the previous round of ( n  X  1) constraints and add an additional constraint. The a priori principle means, in our case, that rankings with n constraints can not have more qualifying entities than any of its n n  X  1 = n sub constraints (of size n  X  1). This stems from the fact that we utilize constraints that are combined using conjunctions (e.g., bornIn USA and TYPE scientist). Although TYPE is syntactically different than predicates, such as bornIn or hasWonPrize, they are treated the same when rankings are assembled.

To efficiently implement the mining of meaningful con-straints from the facts stored in YAGO, we first transform the SPO triples into the following data structure/collection:
This allows us to easily associate a set of subjects S to a specific constraint represented with a (Predicate, Object) pair. For instance, the facts shown in Section 2 will be trans-formed into the following form:
To start the incremental generation of entity rankings, we initialize the number of constraints n  X  1. Having the above data structure already created, the generation of entity rank-ings with one constraint boils down to verifying which con-straints would produce a ranking competitive enough, or with other words, which (Predicate,Object) pairs have as-sociated set of subjects S with size larger than  X  . Those constraints that do not satisfy this criteria are discarded.
In the next iteration we pair up each individual constraint with the constraints of the existing rankings aiming at cre-ating constraints of size 2 and 3. At each pairing step we check if the size of the newly created constraint combina-tion equals the required size. To guarantee uniqueness of the rankings we only allow for the lexicographically increas-ing combinations of the constraints, that is, we check if the constraint to be added is lexicographically larger than the last constraint in the ranking before adding it. We further compute the intersect of the corresponding set of subjects for the ranking and the newly added constraint. Only those constrain combinations that result in competitive ranking are written into the final set of entity rankings.
 A pseudocode of this procedure is shown in Algorithm 1. The algorithm takes a set of predicates P as input, where each predicate represents some entity characteristic. The function transform facts( P ) transforms the YAGO facts for each predicate p  X  X  into the described data format. In the subsequent steps of the algorithms the entity rankings with n constraints are generated. function create rankings( P ,  X  ) ( rankings  X  empty set
C  X  transform facts( P ) for each c in C | if not isCompetitive(c,  X  ) ||C .remove(c) rankings  X  C for each n in { 2, 3 } | for each { c,r } in {C , rankings } || if r.size+1 == n and r.last constraint &lt; c ||| S  X  compute intersection(c,r) ||| if isCompetitive(S,  X  ) |||| rankings .add(r+c) return rankings
The above techniques to phrase rankings lead to a set of rankings skeletons (i.e., descriptions) that need to be filled with the actual ranking content, that is, entities and their performances (e.g., number of received awards). There are three ways to obtain such full-fledged rankings. First, the ranking can be filled using crowdsourcing tools like Ama-zon X  X  Mechanical Turk, or using tailored portals such as ranker.com. Second, with known entity types and constraints, available relational (or RDF) data bases can be queried to obtain entity performance numbers used for ranking. This might further demand data integration techniques to match constraints and entity names to corresponding columns of a database. Third, for certain, important properties, knowl-edge bases (KBs) already contain information that can be used to rank entities. For Wikipedia-based KBs, such infor-mation refers to data specified in the so called  X  X nfo boxes X . For instance, for the New York City (USA) police depart-ment we find information such as the number of police cars (8,839), the number of horses (120), next to information about the annual budget ($3.9 billion).

In this work, we focus on information contained directly in the knowledge bases but sketch how derived (incomplete) rankings of knowledge bases can be enriched/complemented with information obtained through (open) data bases. Next, we describe how the categories of entities computed in the previous section can be ranked according to ranking at-tributes contained in the knowledge base. Figure 2: Number of ranking skeletons for different minimum support thresholds.
To derive performance measures from YAGO, we leverage the fact that each predicate in YAGO has a well defined range. For example, the range of the predicate hasWon-Prize is  X  X ordnet award X , meaning that all objects in the facts (Subject, hasWonPrize, Object) are entities of TYPE award. Thus, we identify all ranges that restrict the objects of the facts to numerical values and extract the correspond-ing predicates.

These predicates form a general set of ranking dimensions, not specific to one of the ranking skeletons created with the algorithm presented in Section 3. To derive ranking dimen-sions specific to a ranking skeleton, we use a procedure sim-ilar to the previously described algorithm. We iterate over all pairs of skeletons and the ranking dimensions (the pred-icates in the set of ranking dimensions) and compute the intersection of the corresponding sets of subjects. If the in-tersect is of size larger than  X  we add the ranking dimension to the set of ranking dimensions specific for this skeleton.
The number of rankings generated with our approach de-pends on the minimum support threshold  X  which specifies the minimum number of qualifying entities for a ranking. It is not clear how to set this value as it needs to be high enough to eliminate the  X  X on interesting X  rankings, but at the same time, it needs to be low enough not to lose too many in-teresting ones. To get a better handle on this problem, we measured the number of rankings generated for different val-ues of  X  , shown in Figure 2. As expected, rising the value of  X  decreases the number of generated rankings. At the same time, increasing the number of constraints decreases the number of rankings that satisfy the support threshold. Figure 2 reveals a resemblance with the power law distribu-tion where the most of the rankings have a small number of qualifying entities, and thus, initial increase of the minimum support threshold results in a large drop of the number of created rankings.
There are three aspects of a knowledge base that impairs the usefulness for phrasing and generating rankings. First, the quality of the stored facts, like (Albert Einstein, bornIn, Ulm). The authors of YAGO have proven through user stud-ies that YAGO provides an accuracy of 95%, i.e., 95% of all facts stated are true. In practice, this should be good enough. The second aspect is the completeness/coverage of the infor-mation stored in the knowledge base: for a given (Predicate, Object)-pair, there might be only a subset of all known en-tities that have a corresponding ranking criterion fact. For example the knowledge base might contain the number of awards for only a subset of USA-born scientists. Third, the knowledge base is incomplete with respect to the open world, which is certainly true as knowledge bases capture only a limited view of all entities and their relations/properties. We are mainly concerned with the second aspect.
 Ranking Title Entities property ratio
Boroughs located in USA ranked by area
US Police depart-ments ranked by budget
Museums located in USA, ranked by area Table 1: Samples of Rankings and the coverage of ranking attributes given by YAGO
Table 1 shows a sample of generated rankings. It reports on the ratio between entities that qualify for a ranking (given in the left column) and the number of them for which we know the ranking criterion fact (and, thus, are able to rank them). We see that for some rankings the fraction  X  of known properties is quite large. This shows, assuming the known values to be spread uniformly over all entities, that the top-k ranking has a relative high recall/precision of k  X   X  . For others, the coverage is quite low and, hence, the resulting ranking would be of little insight. With the above unifor-mity assumption (which appears to be conservative as we assume high-valued entities are more completed in wikipedia as people are more interested), we still can provide the com-pleteness/quality information to users.

The ultimate goal is to enrich the rankings with infor-mation obtained from other sources, like open databases or linked open data datasets. If, by whatever database, there is information for any of the rankings, the computation of subsequent (more constraint) rankings can benefit from this. This calls for solving data integration problem where rank-ings in one database need to be matched as ancestor (more general) or successor (more constrained) of a ranking pro-duced by our approach.
Research in the area of database mining is mainly con-cerned with finding high support patterns (rules) in a given database [1, 12, 8]. These patterns are often represented as frequent itemsets, association rules, causal rules, or mutual dependencies. In similar fashion, the work in [7] is concerned with association rules mining in the domain of knowledge bases. Work by Miah et al. [10] aims at finding properties of database tuples that render them particularly outstanding.
There is a multitude of publicly available data: mined on-tologies, published census data, stock market data, product reviews etc. At the same time, there is an effort in creating knowledge bases from the massive amounts of knowledge present on the Web [9]. Many of these datasets are intercon-nected through the Linked Open Data (LOD) initiative [4] which increases the value of the information compared to its value in the isolation. Similarly, [5] describes the OPEN framework to render multiple  X  X pen X  data sources easily ac-cessible. Such work can act as a the data access substrate for our task to build entity rankings.

Portals like rankopedia.com and ranker.com make use of crowd sourcing to obtain entity rankings. That is, people can (i) propose their favorite rankings, (ii) re-arrange existing rankings, or (iii) vote for specific entries of existing rankings using  X  X humbs up/down X  actions. Such rankings are purely subjective, like the rankings of the best movies of all time. Instead, we work on deriving rankings based on measurable performances but capture also a wide variety of such gen-eral portals X  X hich are, on the other hand, heavily centered around popular topics such as movies, celebrities, or soccer clubs.
In this work we addressed the problem of phrasing and computing entity rankings over the knowledge facts con-tained in common knowledge bases. The key idea is that the derived constraints (theme of a ranking) are semantically meaningful as it harnesses well defined entity types/classes and entity-specific properties. We believe that this work can act as an essential ingredient for forthcoming efforts to au-tomate the process of computing entity rankings over large and diverse datasets or to guide crowdsourcing efforts. [1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining [2] F. Alvanaki, E. Ilieva, S. Michel, and A. Stupar. [3] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, [4] C. Bizer, T. Heath, and T. Berners-Lee. Linked data -[5] K. Braunschweig, J. Eberius, M. Thiele, and [6] S. Chaudhuri and U. Dayal. An overview of data [7] L. A. Gal  X arraga, C. Teflioudi, K. Hose, and F. M. [8] M. A. Hasan, V. Chaoji, S. Salem, J. Besson, and [9] R. Kumar, P. Raghavan, S. Rajagopalan, and [10] M. Miah, G. Das, V. Hristidis, and H. Mannila. [11] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a [12] M. J. Zaki. Efficiently mining frequent trees in a
