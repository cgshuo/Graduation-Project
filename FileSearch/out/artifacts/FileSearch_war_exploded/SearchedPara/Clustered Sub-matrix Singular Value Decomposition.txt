 We present, in this paper, an alternative algorithm called Clustered Sub-matrix Singular Value Decom-position(CSSVD) algorithm, which applied cluster-ing techniques before basis vector calculation in SVD (Golub and Loan, 1996). The work was motivated by an application aimed to provide vec-tor representation for terms and text segments in a document collection. These vector representations were then used for further preprocessing in a multi-document summarization system.

The SVD is an orthogonal decomposition tech-nique closely related to eigenvector decomposition and factor analysis. It is commonly used in infor-mation retrieval as well as language analysis appli-cations. In SVD, a real m-by-n matrix A is decom-posed into three matrices, A = U m-by-n matrix such that the singular value  X  i = the square root of the i th largest eigenvalue of AA T , and trices U and V define the orthonormal eigenvectors associated with eigenvalues of AA T and A T A , re-spectively. Zeroing out all but the k, k &lt; rank ( A ) , largest singular values yields A k = which is the closest rank-k matrix to A. Let A be a term-document matrix. Applications such as latent semantic indexing (Deerwester et al., 1990) apply the rank-k approximation A k to the original matrix A, which corresponds to projecting A onto the k-dimension subspace spanned by u 1 , u 2 , ..., u k . Be-cause k  X  m , in this k-dimension space, minor terms are ignored, so that terms are not indepen-dent as they are in the traditional vector space model. This allows semantically related documents to be re-lated to each other even though they may not share terms.

However, SVD tends to wipe out outlier (minority-class) documents as well as minor terms (Ando, 2000). Consequently, topics underlying out-lier documents tend to be lost. In applications such as multi-document summarization, a set of related documents are used as the information source. Typ-ically, the documents describe one broad topic from several different view points or sub-topics. It is im-portant for each of the sub-topics underlying the document collection to be represented well.
 Based on the above consideration, we propose the CSSVD algorithm with the intention of compensat-ing for SVD X  X  tendency to wipe out minor topics. The basic idea is to group the documents into a set of clusters using clustering algorithms. The SVD is then applied on each of the document clusters. The algorithm thus selects basis vectors by treat-ing equally each of the topics. Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the SVD. The input to the CSSVD algorithm is an m  X  n term-document matrix A. Documents in matrix A are grouped into a set of document clusters. Here, we adopt single-link algorithm to develop the ini-tial clusters, then use K-means method to refine the clusters. After clustering, columns in matrix A are partitioned and regrouped into a set of sub-matrices A 1 , A 2 ,..., A q . Each of these matrices represents a document cluster. Assume A i , 1  X  i  X  q , is an m  X  n i matrix, these sub-matrices are ranked in de-creasing order of their sizes, i.e., n 1  X  n 2  X  ...  X  n , then n 1 + n 2 + ... + n q = n .

The algorithm computes basis vectors as follows: the first basis vector u 1 is computed from A 1 , i.e., the first left singular vector of A 1 is selected. In or-der to ensure that the basis vectors are orthogonal, singular vectors are actually computed on residual matrices. R ij , the residual matrix of A i after the se-lection of basis vectors u 1 , u 2 ,..., u j , is defined as where, proj ( A ij ) is the orthogonal projection of the document vectors in A i onto the span of u 1 , u 2 ,..., u i.e., the residual matrix of A i describes how much the document vectors in A i are excluded from the pro-posed basis vectors u 1 , u 2 ,..., u j . For the first ba-sis vector computation, residual matrices are initial-ized as original sub-matrices. The computation of the residual matrix makes the remaining vectors per-pendicular to the previous basis vectors, thus ensures that the basis vectors are orthogonal, as the eigen-vector computed next is a linear combination of the remaining vectors.

After calculating a basis vector, the algorithm judges whether the sub-matrices have been well rep-resented by the derived basis vectors. The residual ratio was defined as a criterion for this judgement, where R ij is the residual matrix of A i after j basis vectors have been selected 1 ; n i is the number of the documents in matrix A i ; k i is the number of singular vectors that have been selected from matrix A i . Residual ratios of each sub-matrix are calculated. The sub-matrix with the largest residual ratio is assumed to be the one that contains the most information that has not been represented by the previous chosen basis vectors. The first left singular vector of this sub-matrix is computed and selected as the next basis vector. As described above, the computation of a basis vector uses the corresponding residual matrix. Once a basis vector is selected, its influence from each sub-matrix is subtracted. The procedure is repeated until an expected number of basis vectors have been chosen. The pseudo-code of the algorithm for semantic space construction is shown as follows: 1. Partition A into matrices A 1 ,..., A q corresponding to document clusters, where A i , 1  X  i  X  q , is an m  X  n i ( n 1  X  n 2  X  ...  X  n q ) matrix. 2. For i=1,2,...,q { R i = A i ; k[i]=0; } 3. j=1; r=1; 4. u r = the first unit eigenvector of R j R T j ; 5. For i=1,2,...,q R i = R i -u r u T r R i ; 6. k[r]=k[r]+1; r=r+1; 8. j=t if rr t &gt; rr p for p=1,2,...,q and p 6 = t ; 9. If rr j  X  threshold then stop else goto step 4.
For the single-link algorithm used in the CSSVD, we use a threshold 0.2 and cosine measure to cal-culate the similarity between two clusters in our ex-periments. The performance of the CSSVD is also relative to the number of dimensions of the created subspace. As described above, the algorithm uses the residual ratio as a stopping criterion for the basis vector computation. In each iteration, after a basis vector is created, the residual ratio is compared to a threshold. Once the residual ratio of each sub-matrix fell below a certain threshold, the process of basis-vector selection is finished. In our experiments, the threshold was trained on corpus.

After all the k basis vectors are chosen, a term-document vector d i can be converted to d k i , a vector in the k-dimensional space, by multiply-ing the matrix of basis vectors following the stan-dard method of orthogonal transformation,i.e., d k i = [ u 1 , u 2 , ..., u k ] T d i . 3.1 Experimental Setup For the evaluation of the algorithm, 38 topics from the Text REtrieval Conference (TREC) collections were used in our experiments. These topics include foreign minorities, behavioral genetics, steel pro-duction, etc. We deleted documents relevant to more than one topic so that each document is related only to one topic. The total number of documents used was 2962. These documents were split into two dis-joint groups, called  X  X ool 1 X  and  X  X ool 2 X . The num-ber of documents in  X  X ool 1 X  and  X  X ool 2 X  were 1453 and 1509, respectively. Each of the two groups used 19 topics.

We generated training and testing data by simu-lating the result obtained by a query search. This simulation is further simplified by selecting docu-ments containing same keywords from each docu-ment group. Thirty document sets were generated from each of the two document groups, i.e. 60 doc-ument sets in total. The number of documents for each set ranges from 51 to 582 with an average of 128; the number of topics ranges from 5 to 19 with an average of 12. Due to the limited number of the document sets we created, these sets were used both for training and evaluation. For the evaluation of the documents sets from  X  X ool 1 X ,  X  X ool 2 X  was used for training, and vice versa.

To construct the original term-document matrix, the following operations were performed on each of the documents: 1) filtering out all non-text tags in the documents; 2) converting all the characters into lower case; 3) removing stop words -a stoplist con-taining 319 words was used; and 4) term indexing -the tf.idf scheme was used to calculate a term X  X  weight in a document. Finally, a document set is represented as a matrix A = [ a ij ] , where a ij de-notes the normalized weight assigned to term i in document j. 3.2 Evaluation Measures Our algorithm was motivated by a multi-document summarization application which is mainly based on measuring the similarities and differences among text segments. Therefore, the basic requisite is to ac-curately measure similarities among texts. Based on this consideration, we used the CSSVD algorithm to create the document vectors in a reduced space for each of the document sets; cosine similarities among these document vectors were computed; and the re-sults were then compared with the TREC relevance judgments. As each of the TREC documents we used has one specific topic. Assume that similarity should be higher for any document pair relevant to the same topic than for any pair relevant to different topics. The algorithm X  X  accuracy for measuring the similarities among documents was evaluated using average precision taken at various recall levels (Har-man, 1995). Let p i denote the document pair that has the i th largest similarity value among all pairs of documents in the document set. The precision for an intra-topic pair p k is calculated by precision ( p k ) = where p j is an intra-topic pair. The average of the precision values over all intra-topic pairs is com-puted as the average precision. 3.3 Results The algorithms are evaluated by the average preci-sion over 60 document sets. In order to make a com-parison, two baseline algorithms besides CSSVD are evaluated. One is the vector space model (VSM) without dimension reduction. The other is SVD tak-ing the left singular vectors as the basis vectors.
To treat the selection of dimensions as a separate issue, we first evaluate the algorithms in terms of the best average precision. The  X  X est average preci-sion X  means the best over all the possible numbers of dimensions. The second row of Table 1 shows the best average precision of our algorithm, VSM, and SVD. The best average precision on average over 60 document sets of CSSVD is 69.6%, which is 11.5% higher than VSM and 6.1% higher than SVD. average precision (%) 58.1 59.5 66.8 In the experiments, we observed that the CSSVD al-gorithm obtained its best performance with the num-ber of dimensions lower than that of SVD. The Di-mensional Ratio (DR) is defined as the number of dimensions of the derived sub-space compared with the dimension number of the original space, i.e.,
DR = The average dimensional ratio is calculated over all the 60 document sets. As the algorithms X  computa-tional efficiency is dependent on the number of di-mensions computed, our interest is in getting good performance with an average dimensional ratio as low as possible. The third row of Table 1 shows the average dimensional ratio that yielded the best av-erage precision. The average dimensional ratio that CSSVD yielded the best average precision is 32.1%, which is 22.3% lower than that of SVD. Thus, our algorithm has the advantage of being computation-ally inexpensive, assuming that we can find the op-timal number of dimensions.

The bottom row of Table 1 shows the average precision of the algorithms. The threshold used in CSSVD algorithm was trained on corpus. Let p be the threshold on residual ratio that yielded the best average precision on the training data. The value of p is then used as the threshold on the evaluation data. For the SVD algorithm, the average dimen-sional ratio that yielded the best average precision on training data was used as the dimensional ratio to determine the subspace dimensionality in evalua-tion. The performance shown here are the average of average precision over 60 document sets. Again, the CSSVD achieves the best performance, which is 7.3% higher than the performance of SVD and 8.7% higher than VSM. We have presented an alternative algorithm, the CSSVD, that creates vector representation for lin-guistic units with reduced dimensionality. The al-gorithm aims to compensate for SVD X  X  bias towards dominant-topic documents by grouping documents into clusters and selecting basis vectors from each of the clusters. It introduces a threshold on the resid-ual ratio of clusters as a stopping criterion of basis vector selection. It thus treats each topic underly-ing the document collection equally while focuses on the dominant documents in each topic. The pre-liminary experiments on measuring document simi-larities have shown that the CSSVD achieves higher average precision with lower number of dimensions than the baseline algorithms.

Motivated by a multi-document summarization application, the CSSVD algorithm X  X  emphasis on topics and dominant information within each topic meets the general demand of summarization. We ex-pect that the algorithm fits the task of summarization better than SVD. Our future work will focus on more thorough evaluation of the algorithm and integrating it into a summarization system. We would like to thank Mark Sanderson, Horacio Saggion, and Robert Gaizauskas for helpful com-ments at the beginning of this research.

