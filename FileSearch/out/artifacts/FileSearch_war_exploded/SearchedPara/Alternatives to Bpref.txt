 Recently, a number of TREC tracks have adopted a retrieval effectiveness metric called bpref which has been designed for evaluation environments with incomplete relevance data. A graded-relevance version of this metric called rpref has also been proposed. However, we show that the application of Q-measure, normalised Discounted Cumulative Gain (nDCG) or Average Precision (AveP) to condensed lists , obtained by filtering out all unjudged documents from the original ranked lists, is actually a better solution to the incomplete-ness problem than bpref. Furthermore, we show that the use of graded relevance boosts the robustness of IR evaluation to incompleteness and therefore that Q-measure and nDCG based on condensed lists are the best choices. To this end, we use four graded-relevance test collections from NTCIR to compare ten different IR metrics in terms of system ranking stability and pairwise discriminative power.

Information Retrieval (IR) evaluation using incomplete relevance data is beginning to receive attention. Large-scale test collections constructed through pooling [4, 5, 16, 17], such as the TREC, CLEF and NTCIR collections, are all incomplete to some degree, in that only a small sample of the document collection has been judged for relevance for each topic. While the collection sizes tend to grow mono-tonically in order to mimic real-world data such as the Web, the available manpower for relevance assessments often re-main more or less constant, and therefore IR researchers are expected to live with the incompleteness issue as long they adhere to the Cranfield paradigm [3, 13].

At SIGIR  X 04, Buckley and Voorhees [3] proposed an IR evaluation metric called bpref (binary preference) which is highly correlated with AveP (Average Precision) when full relevance assessments are available and is yet more robust when the relevance assessments are reduced. Recent TREC tracks have started using this metric along with AveP. Bpref penalises a system if it ranks a judged nonrelevant docu-ment above a judged relevant one, and is indepedendent of how the unjudged documents are retrieved. At the SIGIR  X 06 poster session, De Beer and Moens [6] proposed rpref , which is a graded-relevance extension of bpref. More re-cently at CIKM  X 06, Yilmaz and Aslam [16] proposed three new methods that may replace bpref, which we shall discuss in Section 2.

This paper shows that the application of Q-measure [9, 10], normalised Discounted Cumulative Gain (nDCG) [7] or AveP to condensed lists , obtained by filtering out all un-judged documents from the original ranked lists, is actu-ally a better solution to the incompleteness problem than bpref. Furthermore, we show that the use of graded rele-vance boosts the robustness of IR evaluation to incomplete-ness and therefore that Q-measure and nDCG based on con-densed lists are the best choices. To this end, we use four graded-relevance test collections from NTCIR to compare ten different IR metrics in terms of system ranking stability and pairwise discriminative power [9].

Section 2 discusses some work that are related to this study. Section 3 provides the original definitions of bpref and rpref. Section 4 redefines these metrics based on a con-densed list of documents, which leads to some simple al-ternatives to bpref. Section 5 describes our NTCIR data. Section 6 compares the metrics in terms of the entire sys-tem ranking using Kendall X  X  rank correlation [3, 10, 16], and Section 7 compares them in terms of discriminative power using Sakai X  X  bootstrap sensitivity method [9]. Finally, Sec-tion 8 concludes this paper.
Several researchers have tackled the problem of reducing human effort required for relevance assessments: SIGIR  X 98 saw new methods for creating judgment pools efficiently [5, 17]; At SIGIR  X 01, Soboroff [12] proposed a method for ranking systems without any relevance assessments, but the method tends to rank them by  X  X opularity X  rather than per-formance [1]. More recently at SIGIR  X 06, Carterette, Allan and Sitaraman [4] analyzed the distribution of AveP over all possible assignments of relevance to all unjudged documents and proposed a method to construct a test collection with minimal relevance assessments; Coincidently, Aslam, Pavlu and Yilmaz [2] proposed a method for obtaining unbiased estimates of standard metrics such as AveP based on ran-dom sampling, while Soboroff [13] used bpref to examine how  X  X ecay X  of Web documents affect system evaluation.
Among existing studies, the CIKM  X 06 paper by Yilmaz and Aslam [16] is probably the most relevant to the present work, since they also proposed their  X  X lternatives to bpref X , namely, Induced , Subcol lection and Inferred versions of AveP. Induced AveP is exactly what we call  X  X veP  X , which we de-rived independently as shown in Section 4. The other two metrics are more complex and less widely applicable in that they require knowledge of all pooled documents, including pooled but unjudged ones. (Subcollection AveP requires even more knowledge [16].) While the aim of Yilmaz and Aslam was to estimate the  X  X ctual X  AveP values accurately, the present study considers a wider range of widely applicable IR metrics, including those based on graded relevance, for the purpose of ranking systems reliably in an incomplete rel-evance environment. Our experiments include both AveP (i.e., Induced AveP) and the actual AveP. Another contri-bution of this paper is that we evaluate the metrics in terms of discriminative power [9] given incomplete relevance data.
We begin with the original definitions of bpref and rpref, although we have replaced some of the symbols in order to maintain notational consistency throughout the paper.
Bpref is an IR metric based on binary relevance, designed to evaluate systems using judged documents only. Let R denote the number of judged relevant documents, and let N denote that of judged nonrelevant documents. Let D denote a relevant retrieved document, and let  X  D R denote a member of the first R judged nonrelevant documents as retrieved by the system. Then, bpref in the literature [13] is known as: where |  X  D R ranked higher than D | represents a penalty based on binary relevance. In words, the penalty is the number of judged nonrelevant documents ranked above a retrieved relevant document. To ensure that the penalty does not exceed one, it is normalised by min( R, N ): Note that, by definition, both |  X  D R | X  R and |  X  D R | X  N hold. An early version of bpref, known as old bpref in Chris Buckley X  X  trec eval version 8.1, is  X  X uggy X . We shall come back to this in Section 4 when we redefine bpref.
To handle graded relevance, De Beer and Moens [6] have defined two versions of rpref, which we shall refer to as rpref N and rpref relative, respectively. As we shall see later, rpref N is a generalisation of one of bpref variants called bpref allnonrel in trec eval version 8.1.

Let C denote a collection containing documents D k (1  X  k  X | C | ). Let r k denote the rank of document D k in a system output for a topic. (For documents not retrieved, assume that r k =  X  .) Let J  X  C denote the set of judged documents, so that | J | = R + N according to the notations we used earlier. For each D k  X  J ,let  X  k  X  [0 , 1] denote its judged relevance with respect to the topic in question, where 0 represents judged nonrelevance and 1 represents the highest possible relevance. Then, rpref N is defined as: where and
We now interpret the above equations in words. R is the sum of all judged relevance values, which reduces to R in a binary relevance environment (i.e., if  X  k  X  X  0 , 1 } ). From Eqs. 3 and 4, it is clear that and that N reduces to N in a binary relevance environment. Meanwhile, instead of the binary-relevance penalty of bpref, rpref uses penalty k (Eq. 5), an analogous graded-relevance penalty: For a given retrieved relevant document D k ,we examine all judged documents ranked above it ( D l  X  J , r l &lt;r k ), and find ones that are less relevant than D (  X  l &lt; X  k ). For each of such illegitimately highly ranked documents D l , we accumulate the difference between the two relevance values (  X  k  X   X  l ) normalised by  X  k , the highest possible relevance difference between D k and any document that is less relevant. The penalty is then normalised by N : It is easy to show that penalty k  X  N . Finally, rpref Ntakes the weighted average of 1  X  penalty k /N ,usingtherelevance values  X  k as weights. That is, misplaced documents above a highly relevant document are emphasised compared to those above a partial ly relevant one. Note that, in a binary rele-vance environment, the weight  X  k in Eq. 2 reduces to a flag indicating either relevance or nonrelevance.

De Beer and Moens [6] note that N is not a tight upper-bound of penalty k , especially for highly ranked documents, since the number of documents ranked higher than D k must be smaller than the rank of D k , i.e., r k .Thus,using N for normalisation implies relatively small penalties for mis-placements above highly ranked relevant documents. To put more emphasis on the higher ranks, De Beer and Moens [6] prefer  X  X elative normalisation X : rpref relative = 1 where That is, N k is the number of judged documents ranked above D k .Notethat N k =0if r k =1,hencethe X  r k =1 X  condition is necessary.

Section 3 showed that bpref, rpref N and rpref relative rely on judged documents only, and that they do not explic-itly depend on the absolute document ranks. This section begins by redefining these preference-based metrics using a condensed list , obtained by removing all unjudged docu-ments from the original system output. This suggests that  X  X tandard X  metrics based explicitly on ranks may in fact be applied successfully to IR evaluation given incomplete rele-vance data.
Suppose we need to evaluate a ranked list that contains (say) 1000 documents. The document at Rank r may or may not be a judged one. Since bpref relies on judged doc-uments only, we can safely remove all unjudged documents from the list before computing bpref. As a result, we obtain a condensed list that contains judged documents only. Let r (  X  r ) denote the new rank of a document in the condensed list. Then, based on these ranks, bpref can be rewritten as: bpref = 1 where isrel ( r ) is one if the document at Rank r is relevant and zero otherwise; and count ( r ) is the number of relevant documents in top r of the ranked list. Hence r  X  count ( r )is the number of judged nonrelevant documents ranked above thedocumentatRank r . It is easy to see that the two defi-nitions (Eqs. 1 and 9) are equivalent. (The  X  X uggy X  bref, or old bpref ,usesmin( R, N ret ) instead of min( R, N )inEq.9, where N ret is the number of retrieved judged nonorelevant documents. See the file bpref bug in trec eval version 8.1.) Eq. 9 implies that bpref is in fact two different metrics : For any topic such that R  X  N , bpref reduces to: bpref R = 1 Whereas, for any topic such that R  X  N , bpref reduces to: The latter is also known as bref allnonrel .Notethat R  X  N  X  r  X  count ( r ) holds in the latter case and therefore the minimum operator is not necessary.

Similarly, let us now rewrite rpref N(Eq.2),whichis in fact a graded-relevance version of bpref Nshownabove. We begin by adopting the concept of cumulative gain as proposed by J  X  arvelin and Kek  X  al  X  ainen [7]. Let L denote a relevance level ,andlet gain ( L )denotethe gain value for retrieving an L -relevant document. Without loss of gen-erality, this paper assumes that we have S-relevant (highly relevant), A-relevant (relevant) and B-relevant (partially rel-evant) documents as in NTCIR [8] in addition to judged nonrelevant documents. Also, let R ( L ) denote the number of L -relevant documents. Let cg ( r )= the cumulative gain at Rank r of the system output, where g ( i )= gain ( L ) if the document at Rank i is L -relevant and g ( i ) = 0 otherwise (i.e., if the document at Rank i is ei-ther judged nonrelevant or unjudged). It is easy to see that rpref X  X  judged relevance values  X  k  X  [0 , 1] (See Section 3.2) can be obtained by dividing the raw gain values by gain ( where H is the highest relevance level across all topics of the test collection, e.g., H = S .

Although bpref and rpref do not require the notion of ideal ranked output [7] for their definitions, we introduce it here in order to clarify how the preference-based metrics are related to cumulative-gain-based ones: An ideal ranked output for a given topic with R = such that g ( r ) &gt; 0for1  X  r  X  R and g ( r )  X  g ( r  X  for r&gt; 1. Thus, for NTCIR, listing up all S-relevant doc-uments, followed by all A-relevant documents, followed by all B-relevant documents produces an ideal ranked output. Note that an ideal ranked output is unaffected by the re-moval of unjudged documents. Let g I ( r )and cg I ( r )denote the (cumulative) gain of an ideal ranked output. Then: That is, the ideal cumulative gain at Rank R is the sum of all available gain values for the topic. Hence, From Eqs. 3 and 12, Moreover, from Eqs. 6 and 13,
We can also rewrite rpref X  X  p enalty (Eq. 5) based on the notion of gain for a condensed list. The penalty based on a relevant document at Rank r can be expressed as: since the gain ( H ) X  X  for relevance value transformation cancel out. Therefore, rpref N can be rewritten as: rpref N = gain (
Similarly, since the number of judged documents ranked above a document at Rank r in a condensed list is exactly r  X  1, rpref relative (Eq. 7) can be rewritten as: rpref relative = 1 De Beer and Moens [6] also mention its binary-relevance variant, which we call bpref relative and formalise as: bpref relative = 1 Note that, in a binary relevance environment, Eq. 15 reduces to
P of judged nonrelevant documents above Rank r , i.e., r  X  count ( r ).

Thus we have successfully redefined bpref and rpref vari-ants based on the ranks of a condensed list, which contains judged documents only.

From our rank-based notations of Eqs. 17 and 18, it is now clear that both rpref relative and bpref relative have flaws. Firstly, these relative metrics ignore a relevant document at Rank 1 [6], so a system that returns only one relevant document at Rank 1 receives zero as its score. Moreover, a system that returns only one relevant document at Rank 2 also receives zero, since the penalty and the normalisation factor r  X  1canceloutwhen r =2. Thusthesemetricsare clearly counterintuitive. Another problem is that the two metrics do not average well across topics, as the score for an ideal ranked output varies depending on the value of R :Itis easy to show that rpref relative for an ideal ranked output is that bpref relative for an ideal ranked output equals ( R 1) /R : An ideal ranked list for a topic with R =2relevant documents would receive 0.5 as its score, while one for a topic with R = 5 relevant documents would receive 0.8. Clearly, more well-designed metrics are in order.
A very simple solution to all of the above problems is to use r instead of r  X  1 for relative normalisation. Thus, rpref relative 2= 1 Note that now we do examine a relevant document at Rank 1. It is also easy to show that rpref relative2 equals one for an ideal ranked output for any topic.

Now, let us modify bpref relative similarly: But this is exactly the well-known noninterpolated Average Precision (AveP) based on a condensed list in place of a raw list containing both judged and unjudged documents .
We can summarise our discussions so far as follows: In short, the only essential difference between bpref and AveP is that while the former uses absolute normalisation, the lat-ter uses relative normalisation , as noted also in [16].
The above fact poses this question: For evaluating IR systems with incomplete relevance information, is it really necessary to introduce bpref or its variants? Why can X  X  we just use existing metrics based explicitly on ranks instead, after filtering out all unjudged documents from the original ranked list? Thus, let us consider, in addition to AveP, Q-measure [9, 10] and nDCG [7], both of which explicitly rely on document ranks, and can handle graded relevance. Q-measure is defined as: where  X  is a parameter for controlling the penalty on late arrival of relevant documents. It is very highly correlated with AveP (Note that  X  = 0 reduces Q-measure to AveP) and its discriminative power (See Section 7) is known to be at least as high as that of AveP. Since Sakai [11] has shown Q-measure X  X  robustness to the choice of  X  ,welet  X  =1.
Instead of the raw gains g ( r ), nDCG uses discounted gains dg ( r )= g ( r ) / log a ( r )for r&gt;a and dg ( r )= g ( r )for r Let dg I ( r ) denote a discounted gain for an ideal ranked list. Then, nDCG at document cut-off l is defined as: Throughout this paper, we let l = 1000 as it is known that small document cut-offs hurt nDCG s stability [10]. More-over, we let a = 2 because it is known that using a large logarithm base makes nDCG counterintuitive and insensi-tive [11]. nDCG is more forgiving for low-recall topics than AveP and Q-measure, as it does not depend on R directly.
Just like AveP (i.e., bpref relative2), we can apply Q-measure and nDCG to condensed lists, i.e., after removal of all unjudged documents. We shall refer to these metrics as Q and nDCG . The remainder of this paper studies how these metrics compare to bpref and rpref variants. Since Q-measure and nDCG are robust to the choice of gain val-ues [10], we let gain ( S )=3 ,gain ( A )=2 ,gain ( B )=1 throughout this paper.
We use four data sets (i.e., test collections and submitted runs) from the NTCIR CLIR task [8]: Table 1 shows some statistics. We use the top 30 runs from each data set as measured by (actual) Mean AveP for our analyses: Under this condition, Kendall X  X  rank correlation between two sys-tem rankings, representing two IR metrics, is statistically significant at  X  =0 . 01 if it exceeds 0.34 (two-sided normal test) [9]. The table also shows that N is always larger than R for the NTCIR collections: For example, for NTCIR-3C, the smallest N across all topics is 911, while the largest R across all topics is only 264. Thus, since R&lt;N always holds, bpref is equivalent to bpref R for our data .Hencewe shall refer to bpref as bpref Rinourexperiments.

To examine the effect of incompleteness on the entire sys-tem ranking and discriminative power, we created reduced relevance data from the full relevance data, following the original Buckley/Voorhees methodology [3]: First, for each topic, we created a randomised list of judged relevant doc-uments of size R , and a separate randomised list of judged nonrelevant documents of size N . Then, for each reduction rate j  X  X  90 , 70 , 50 , 30 , 10 } , we created a reduced set of rel-evance data by taking the first R j and N j documents from the two lists, respectively, where R j =max(1 ,truncate ( R j/ 100)) and N j =max(10 ,truncate ( N  X  j/ 100)). The con-tants 1 and 10 have been copied from [3], representing the minimum number of judged (non)relevant documents re-quired for a topic. (In practice, the constant 10 was seldom used since N was generally very large.) This stratified sam-pling is essentially equivalent to random sampling from the entire set of judged documents [16].

Figure 1 shows the effect of relevance data reduction on the absolute overall performances (e.g., Mean AveP) aver-aged across all 30 runs for each data set. The horizontal axis represents the reduction rate j . It is clear that the  X  X ctual X  AveP, Q-measure, nDCG values quickly diminish as the relevance data becomes more and more incomplete (as represented by the dotted lines), while the bpref R (i.e., bpref) curve is relatively flat. This much supports a find-ing in [3]. However, it is also clear that the other metrics (Q ,nDCG ,rpref relative2 and AveP , ) do just as well as bpref in terms of the absolute value stability . This further encouraged us to test whether applying a standard metric to condensed lists works at least as wel l as bpref .
This section uses Kendall X  X  rank correlation [3, 10, 16] to study the effect of incompleteness on the entire system ranking. Table 2 shows the correlation values between sys-tem rankings by two different metrics for all four data sets, given full relevance data. Among the ten metrics we con-sider, recall that AveP, Q-measure and nDCG are based on the original ranked lists, while AveP ,Q and nDCG are based on judged documents only just like the bpref variants. While all of the correlation values are statistically highly sig-nificant (See Section 5), we indicate values higher than 0.8 in bold just for convenience. Let ( M 1 , M 2 ) denote a correla-tion value between two rankings by metrics M 1 and M 2 .We can observe that bpref N and rpref N are not highly corre-lated with  X  X tandard X  metrics such as AveP: For example, (bpref N, AveP)= . 480 for NTCIR-5J. (They are not highly correlated with bpref R either.) This reflects the fact that bpref N and rpref N give small penalties for misplacements above highly ranked relevant documents: Since N is gener-ally very large for the NTCIR data, absolute normalisation virtually ignores the penalties for small values of r .Aswe shall see in Section 7, bpref N and rpref N are also poor in terms of discriminative power. Hence we shall omit these two metrics in our reduced relevance data experiments.
As also noted in [16], the correlation between bpref R (i.e., bpref) and the actual AveP is consistently lower than that between AveP and the actual AveP. For example, for NTCIR-5J, (bpref R, AveP)= . 885, while (AveP ,AveP)= . 963. That is, given the full relevance data, AveP is a better approximation of the actual AveP than bpref is . Similarly, the correlations (Q ,Q-measure)and(nDCG ,nDCG)are also very high.

Figure 2 shows the effect of relevance data reduction on the system ranking for each metric. For example, the AveP curve represents the values of (AveP, AveP( j )), where AveP( j ) denotes AveP computed based on the j % relevance data ( j  X  X  90 , 70 , 50 , 30 , 10 } ). It can be observed that only the dotted line of the actual AveP stands apart from the others. More specifically, our observations are as follows: (i) The rankings by Q ,nDCG ,rpref relative2 and AveP (ii) The rankings by the actual Q-measure and nDCG are For example, for NTCIR-5J, (Q ,Q (10))=0.66, (bpref R, bpref R(10))=0.42 and (Q-measure, Q-measure(10))=0.41. Whereas, (AveP,AveP(10))=0.21 which is not even statisti-cally significant.

Observation (i) supports our hypothesis that applying a standard metric to condensed lists works at least as well as bpref . Observation (ii) suggests that the best graded-relevance metrics are more robust to incompleteness than the binary AveP. Ourexplanationforthisisasfollows: Due to the distribution of R ( X ) shown in Table 1, randomised relevance data reduction is expected to remove more par-tially relevant documents than highly relevant documents. Whereas, reflecting the same original distribution, most runs actually contain many more partially relevant documents than highly relevant ones. Hence, highly relevant docu-ments contained within the runs are relatively unaffected by the reduction, while partially relevant documents contained within the runs may become  X  X onrelevant X  due to the re-duction. Therefore, graded-relevance metrics, which depend more on highly relevant documents than on partially rele-vant ones, demonstrate a higher robustness to incomplete-ness.
This section compares the IR metrics in terms of dis-criminative power using the paired test version of Sakai X  X  bootstrap sensitivity method [9], which is known to agree very well with the more ad hoc Voorhees/Buckley swap method [15]. The input to this method are a test collec-tion, a set of runs, an IR metric, and the significance level  X  for bootstrap hypothesis tests. Using resampled topic sets, the method conducts a bootstrap hypothesis test for every system pair, and computes the discriminative power, i.e., for how many system pairs the IR metric was able to detect a significant difference, and the overall performance difference required to achieve that significance.

Table 3 ranks the IR metrics according to discriminative power at  X  =0 . 05 given full relevance data. For example, for NTCIR-5C, the actual Q-measure detects a significant difference at  X  =0 . 05 for 174 out of 435 (30*29/2) system pairs (40%), and the estimated overall performance differ-ence required to detect one given 50 topics is 0.11 [9]. Met-rics based on judged documents only are shown in bold. It is clear that, given full relevance data, bpref R is substan-tially more discriminative than bpref N and rpref N, but all other metrics are more discriminative than bpref. In par-ticular, note that Q ,nDCG ,rpref relative2 and AveP all outperform bpref consistently, using judged documents only . In addition, the superiority of Q over bpref R, AveP and rpref relative2 is consistent across all data sets.
Figure 3 shows the effect of relevance data reduction ( j { 70 , 50 , 30 , 10 } ) on discriminative power at  X  =0 . 05. For example, for NTCIR-5C with only 10% relevance data, the actual AveP detects a significant difference for only about 2.5% of all run pairs. (Note that its discriminative power is 36.6% with 100% relevance data, as shown in Table 3.) Whereas, under the same condition, the actual Q-measure and nDCG are comparable to bpref (a little over 10% in dis-criminative power); Q ,nDCG ,rpref relative2 and AveP are the most discriminative (over 20%). Although the ac-tual Q-measure does not do well with NTCIR-3J at j = 10, and nDCG( ) appears to do worse at j =50thanat j =30 with NTCIR-5C and 5J, the trends consistent across all four data sets are as follows: (i )Q ,nDCG ,rpref relative2 and AveP are more dis-(ii ) Given incomplete relevance data, bpref Rismoredis-Thus, the benefit of introducing bpref is not clear in terms of discriminative power either.
This paper showed that the application of Q-measure, nDCG or AveP to condensed lists, obtained by filtering out all unjudged documents from the original ranked lists, is actually a better solution to the incompleteness problem than bpref. Furthermore, we showed that the use of graded relevance boosts the robustness of IR evaluation to incom-pleteness and therefore that Q-measure and nDCG based on condensed lists are the best choices. In terms of the en-tire system ranking, Q-measure, nDCG and AveP based on condensed lists (Q ,nDCG and AveP ) are more robust to incompleteness than bpref R (i.e., bpref); even the actual Q-measure and nDCG do as well as bpref R. In terms of pairwise discriminative power, Q ,nDCG and AveP are more discriminative than bpref R whether full or reduced relevance data is used; Q is slightly but consistently more discriminative than AveP ; even the actual Q-measure and nDCG can often be as discriminative as bpref Rinavery incomplete relevance environment. A variant of bpref/rpref (rpref relative2) does well also, but the benefit of introduc-ing this relatively complex metric is not clear either.
Finally, it should be noted that we have not examined IR metrics from the viewpoint of user satisfaction: It remains to be seen, for example, how recent criticisms of AveP [14] extend to different experimental environments and differ-ent metrics. Properties such a s rank correlation with an es-tablished metric and discriminative power provide necessary conditions for a good IR metric, not sufficient conditions. [1] Aslam, J. A. and Savell, R.: On the Effectiveness of [2] Aslam, J. A., Pavlu, V. and Yilmaz, E.: A Statistical [3] Buckley, C. and Voorhees, E. M.: Retrieval Evaluation [4] Carterette, B., Allan, J. and Sitaraman, R.: Minimal [5] Cormack,G.V.,Palmer,C.R.andClarke,C.L.A.
 [6] De Beer, J. and Moens, M.-F.: Rpref -A [7] J  X  arvelin, K. and Kek  X  al  X  ainen, J.: Cumulated [8] Kando, N.: Overview of the Fifth NTCIR Workshop, [9] Sakai, T.: Evaluating Evaluation Metrics based on the [10] Sakai, T.: On the Reliability of Information Retrieval [11] Sakai, T.: On Penalising Late Arrival of Relevant [12] Soboroff, I., Nicholas, C. and Cahan, P.: Ranking [13] Soboroff, I.: Dynamic Test Collections: Measuring [14] Turpin, A. and Scholer, F.: User Performance versus [15] Voorhees, E. M. and Buckley, C.: The Effect of Topic [16] Yilmaz, E. and Aslam, J. A.: Estimating Average [17] Zobel, J.: How Reliable are the Results of Large-Scale
