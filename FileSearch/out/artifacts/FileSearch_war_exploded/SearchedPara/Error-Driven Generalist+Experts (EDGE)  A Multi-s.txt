 We introduce a multi-stage ensemble framework, Error-Driven Generalist+Expert or Edge , for improved classifica-tion on large-scale text categorization problems. Edge first trains a generalist , capable of classifying under all classes, to deliver a reasonably accurate initial category ranking given an instance. Edge then computes a confusion graph for the generalist and allocates the learning resources to train experts on relatively small groups of classes that tend to be systematically confused with one another by the generalist. The experts X  votes, when invoked on a given instance, yield a reranking of the classes, thereby correcting the errors of the generalist. Our evaluations showcase the improved classification and ranking performance on several large-scale text categorization datasets. Edge is in particular efficient when the underlying learners are efficient. Our study of confusion graphs is also of independent interest. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Experimentation, Theory Ensemble learning, text categorization, many class classifi-cation
Automated text categorization has found a variety of applications, in personalization, document routing, recom- X  This work was primarily done when the author interned at Yahoo! Research.
 mendation, and ad placement [24]. Often, the number of classes needed for informative and useful categorization of text is large, easily exceeding 1000s. A number of data sets, such as Wikipedia, Yahoo! Directory, and OHSUMED already contain many thousands of classes and 100s of thousands of instances [15]. Well-designed linear classifiers have been shown to be among the most accurate [7, 24, 5, 4], and recently, highly scalable discriminative linear methods have been developed [17, 16, 18]. These methods treat classes as  X  X lat X , and hence improving their accuracy is a good possibility: identifying the one correct class from thousands of candidates, under linearity constraints, can be error-prone. In this work, we take a data driven approach to improve accuracy, while exploiting the efficiency benefits provided by recent advances in linear methods. We note at the outset that these ideas are in principle applicable to other learning algorithms and data types as well. The overall approach, which we refer to as Error-Driven Generalist+Expert or Edge , has roughly two parts. First a reasonably accurate (and efficient) generalist classifier is trained on the entire data. The confusion pattern of the generalist is characterized by a confusion graph, wherein nodes are concepts and the presence of an edge indicates significant confusion between the concept pair connected (as we describe in more detail). Highly connected concepts are then identified as  X  X ompound concepts X . In the second part, an expert classifier is trained for each such group (only on the instances belonging to some concept in the group). Classification then consists of first utilizing the generalist, then possibly invoking some of the experts and aggregating their votes, for improved classification.

The first part of the paper reports on our studies of confusion graphs obtained from training supervised learning algorithms on several data sets, and is of independent interest. We explore the question of whether a significant portion of errors tend to be systematic, and if so in what patterns. We find that there are significant amount of systematic errors and groupings, and in particular the confusion patterns are often in star shapes. We also report a number of different statistics, and provide examples of confused concepts.

The second part of the paper explores whether this systematic confusion can be substantially corrected, without incurring too many extra offsetting errors. We train experts on each of the compound concepts, possibly using a learning algorithm different from the generalist. Experts tend to classify more accurately within their own classes than the generalist (as we show), the intuition being that the number of classes is substantially smaller, thus the discrimination task can be easier. Another question is how to combine the votes of the different experts (when invoked) and the generalist together. We explore aggregation schemes and study different combinations for the generalist and the experts. Our experiments on a number of data sets show that simple aggregation schemes can significantly enhance overall accuracy. The overall classification and training time degrades but only moderately due to the second stage. Thus we show that the data driven Edge approach is promising.
We assume basic familiarity with machine learning for text classification. A good reference is [24].
We first define the concept of confusion and then analyze its patterns on real world data, the findings of which motivate the design of algorithms in the following section.
The confusion matrix is a well known technique for depicting the error made by the classifier, especially in the multiclass single label learning setting [11]. Specifically, a confusion matrix CM is a K  X  K matrix ( K is the number of classes), where an element n i,j represents the number of times the classifier classifies an instance of class i as class j . Consequently, the off-diagonal elements correspond to the errors made by the classifier on the data. In our setting, classifiers may recall multiple true categories for each instance. We extend the idea to characterize ranking error with the notion of confusion as follows. Seeing an instance x t , the learned model M retrieves i n concepts and assigns a score s c i for each of the concept c i . Hence the model produces a multi-label  X y t = ( c t i descending order according to the scores.

Definition 2.1 (Confusion). For a labeled instance ( x t , y t ) , the confusion function CONF is defined as where I is the Iverson bracket and y t are the true categories. In other words, concept c i is confused with concept c j iff c is ranked higher than c i , where c i is one of the true labels but c j is not.

Confusion corresponds to an arc pointing from a true class to a false one that has been ranked higher by the system. Confusion captures error in categorization and is referred to as a reversed pair in machine learning literature [5].
Similar to the confusion matrix, we are mostly interested in systematic error made by the model in the dataset and thus this leads to the definition of cumulative confusion between concept c i and c j as follows: Note that in the single label case, N CONF [ c i  X  c j ] exactly corresponds to the element nc i,j in the confusion matrix.
As an analogy to confusion matrix, we formally define confusion graph as a means to analyze classification errors. Definition 2.2 (Confusion Graph). Confusion graph G ( V, A ;  X  ) is a directed graph. In this graph, an arc ( v belongs to the arc set A if and only if N CONF [ v i  X  v j where  X  is a positive threshold. Each arc ( v i , v j ) is associated with weight w i,j = N CONF [ v i  X  v j ] . The vertex set V is constituted with all the categories minus the singleton categories (i.e. having zero degree).
 Figure 1 demonstrates the confusion graph of the Feature Focus algorithm (FF) [16] on the Newsgroup 20 dataset [20]. Setting  X  = 5 (Figure 1 right), we observe that confusion is usually incurred on highly related categories. For instance, the categories sci.electronics , comp.os.ms-windows.misc , comp.graphics and misc.forsale are confused with comp.sys.ibm.pc.hardware . In particular, we note that confusion may occur between categories that belong to different branches of the taxonomy, e.g. sci.electronics and comp.sys.ibm.pc.hardware . On the other hand, categories that are close, in terms of  X  X emantic X  distance in the taxonomy, may not necessarily be confused with each other, using logarithmic binning. e.g. comp.sys.mac.hardware and comp.sys.ibm.pc.hardware . In this real example, traditional hierarchical classification approaches may redundantly train multiclass classifiers, dictated by the underlying taxonomy. Also, they may risk prematurely splitting the classification tasks, which makes the discrimination more challenging due to the large number of categories in high levels of the hierarchy. The remainder of this section seeks for natural groupings of classification tasks transcending the boundaries of a taxonomy. We explore the confusion graph G RCV1 ( V, A ;  X  ) 1 of the Reuters dataset (see Section 4) by using statistical graph analysis techniques. The findings motivate the choice of sub-learning problems for experts. Intuitively, we expect the confusion graph to possess some structural characteristics, e.g., clustering based on topics as relevant topics are more difficult to differentiate.
Figure 2(a) illustrates the degree distribution of the confusion graph of RCV1. We note that both in-degree and out-degree follow the power-law distribution, as one can visually fit a line in the log-log plot (the fitted in-and out-degree distributions are P ( in ) k  X  k  X  0 . 683 and P k  X  0 . 549 respectively). Similar scale-free degree distribution is also observed in the OHSUMED (Figure 2(b)) and Y! Web data. We note that the in-degree distribution has a wider spread in both diagrams. In Figure 2(a), there are 30 nodes with in-degree greater than 100, compared to only 4 with out-degree greater than 100. Such a high in-degree node (called sink), combined with its satellite categories, constitutes a star pattern in the confusion graph. An example of the star pattern is found in Figure 1, comprised by ibm.pc.hardware and its satellites ms-windows , electronics , graphics and forsale .
We consider the mixing patterns in the confusion graph in terms of node degrees. For a directed graph, the
In this confusion graph, | V | = 306 , | A | = 7268 and  X  = 5 . assortativity coefficient r [19] is defined as where j i and k i are the excess in-and out-degree of a node i respectively, and M is the total number of arcs. Essentially, this quantity is the Pearson correlation between the normalized out-and in-degree. We found the confusion graph of RCV1 to be disassortative ( r =  X  0 . 274 ). This type of degree anti-correlation implies that high in-degree nodes (sinks) are preferentially attached to by low out-degree nodes, and vice versa. Similar disassortative pattern is also found in technological (e.g. the assortativity of Internet is -0.189) and biological networks [19].
We are interested in the two categories incident on a confusion arc. In G RCV1 , nearly 90% arcs originate from a less frequent category to a equally or more frequent category in the training data. Also, 58% arcs link from a category to another one that is at least five times more frequent. Hence, rarer concepts are more prone to be confused with more frequent concepts, but not vice versa. This is not surprising as many text categorization algorithms favor common categories to attain high (micro) accuracy.
Clustering coefficient of node i is defined as the proportion of the number of (closed) triangles connected to i to the number of triples centered on i . Formally, the clustering coefficient of a node i is [27]: where d i is the total degree of node i and Neighborhood ( v { v j | ( v i , v j )
In G RCV1 , the mean clustering coefficient is 0.203, manifesting significant local clustering effect. This finding is in concord with scale-free networks, as a similar size random network has much lower clustering coefficient value 0.0777. For confusion graphs, this indicates that two concepts confused with a common concept are also likely to be confused with each other.
To sum up, the confusion graph is a scale-free network dominated by star-patterned subgraphs. More precisely, each subgraph has many satellite concepts with confusion edges pointing to the sink. Due to disassortative degree mixing, overlaps between subgraphs are most likely low degree nodes. According to directional connection, these satellite categories typically correspond to rarer categories. Moreover, the high local clustering effect of confusion graphs suggest that concepts are likely to connect to each other in these subgraphs as well.

These findings suggest a natural way to divide the complex text categorization problem into sub-problems and we may obtain a better solution if we can better solve these smaller learning problems. The systematic patterns of error, rather than random occurrences, permit the development of a two-stage generalist and expert learning framework, which is described in detail in the following section.
In this section, we first formally define a few important concepts, before discussing the details of the Edge frame-work. We then describe the methods of aggregation and finally make remarks on the proposed algorithm.
The learning problem is naturally divided into sub-problems in an error-driven manner.

Definition 3.1 (Compound Concept). A compound concept CC s is a set of concepts containing a sink v s and its immediate neighbors: where the sink v s belongs to the set of top p % nodes 2 according to the in-degree distribution of the confusion graph G ( V, A ;  X  ) .

Definition 3.2 (Expert). An expert E s is a model learned from the truncated problem Tr ( CC s ) , derived from the compound concept CC s and its corresponding instances,
By contrast, a generalist is a model learned from the entire learning problem { ( x t , y t ) } . The concepts of generalist and expert play a central role in the Edge framework. Briefly, the generalist provides an initial ranking of concepts efficiently, while experts fine tune the ranking among the retrieved concepts. It is also worthwhile to note the connection between the two concepts. An expert can be regarded as a special type of  X  X eneralist X , which simply acknowledges the predictions of the generalist for concepts outside its realm of expertise. For concepts within the compound concept, the expert reweighs them and thus the expert can also implicitly rank all the categories given the generalist X  X  advice.
The parameter p % in the definition is motivated by the previous findings, as nodes in the tail of the in-degree distribution correspond to the sinks in the star patterns.
The Edge framework is a two stage ensemble learning method consists of generalist and expert learning, as shown in Figure 3. An aggregator is responsible for combining the generalist and experts X  decisions.

Algorithm 1 describes the training phase of Edge . The framework utilizes multiclass learning algorithm(s) (denoted by Comp Learn Algo) for training a generalist and experts. After learning a generalist, we compute cumulative con-fusion and construct the confusion graph G . A set of compound concepts are then found for the top p % sinks according to the in-degree distribution of G . In line 6, the original training data are truncated so that instances not containing any relevant concepts (in the compound concept) are removed. Also, features not pertinent to the compound concept are not presented to the learner, and thus the experts may consume much less model space. For each of these compound concepts, an expert is learned in line 7. Here the learner may differ from that in line 2, as a more accurate model may better serve as an expert. If the expert is better suited for classification than the generalist (line 8), it is added to the set of experts Exp (line 9).
 Algorithm 1 Edge training phase.
 Input: Training dataset { ( x t , y t ) } N t =1 ;  X  Output: Gen and Exp 1: Train generalist Gen =Comp Learn Algo( { ( x t , y t ) } 2: Compute the confusion graph G ( V, A ;  X  ) of Gen in the 3: Obtain compound concepts { CC 1 , ..., CC M } by using 4: Exp  X   X  5: for m = 1 to M do 6: Obtain a truncated learning problem Tr ( CC m ) for 7: Train an expert E m = Comp Learn Algo 0 ( Tr ( CC m )) 8: if E m  X  X  training error is lower than that of Gen in 9: Exp  X  Exp  X  X  E m } 10: end if 11: end for 12: return Generalist model Gen and experts Exp
The classification phase of Edge is shown in Algorithm 2, where the subscripts 1 and 2 differentiate the probabilities predicted by first level classifier (the generalist) and second level classifiers (experts) respectively. First, the generalist produces an initial ranking of categories and computes the prior probability of compound concepts. For each compound concept having prior probability higher than  X  3 the corresponding expert is then activated to re-score the categories within the compound concept. Line 6 and 7 realize the view of a specialist learning framework [10]. An expert reweighs the concepts in its compound concept proportionate to its own probabilistic prediction (line 7). On the other hand, it leaves the sum of weights for the concepts outside its expertise unchanged by simply passing on the prediction of the generalist. In line 10, the scores of  X  is an insensitive parameter and is empirically set to 0.1 in our experiments. Algorithm 2 Edge classification phase.
 Input: Testing instance x ; activation threshold  X  . Output: Predicted categories  X c . 1: Gen predicts probabilistic score s c i ( Gen ) = P 1 ( c 2: for all E m  X  Exp do 3: Compute the probability of compound concept CC m : 4: if P 1 ( CC m | x ) &gt;  X  then 5: Activate expert E m to predict probabilistic score 6: Normalize the probabilistic scores of concepts 7: s c j ( E m ) = P 2 ( c j | CC m , x )  X  P 1 ( CC m | x ) 8: end if 9: end for 10: Aggregate scores assigned by Gen and the subset awake 11: return Sorted categories  X c using the aggregate scores the generalist and experts are aggregated to make the final prediction, the details of which are discussed next.
We now turn our attention to exploring several aggrega-tion methods for combining the ranking decisions given by the generalist and experts. The first approach applies fixed mathematical rules for aggregation, and the latter learns such aggregation rules from data.
Before proceeding to the aggregation methods, we first recall the setting of our work. First, the framework is intended to handle large scale datasets and thus it prohibits the use of computationally expensive combination methods. Second, experts in the Edge framework specialize on different part of the feature space and only have access to the features and categories in their respective compound concepts. This is further complicated by insufficient training data in rare categories. As such, experts X  performance may not be directly comparable with each other. Various simple combination methods have been proposed in the literature, including averaging [25], voting [29] belief integration in Bayesian formalism and Dempster-Schafer formalism [29], etc. However, many of these methods implicitly assume the comparability of classifiers for pooling or require expensive computation, which is not applicable in our setting.
The averaging method, which simply computes the arith-metic average of the classifiers X  output, is an exception to these drawbacks. Simple averaging has been observed to provide comparable, sometimes superior, accuracy, with minimum computation cost [25]. Following the nota-tions in Algorithm 2, consider the aggregation of the generalist output s c i ( Gen ) and the scores s c i ( E m E m  X  A Exp ( i ) (the set of activated experts for class i ). If A Exp ( i ) is empty, the aggregated score is trivially s ( Gen ) . Otherwise, the simple averaging score is 4 ,
Since the performance of experts learned in Edge may be uneven, simple averaging can be susceptible to poorly performed expert. Robust order statistics combiners have been suggested as alternative simple combination methods with theoretical underpinnings [26]. Assume that we have N scores from the generalist and experts for class i (by replacement of labels), ordered ascendingly, Accordingly, the k th order statistics is s ( k ) c i and the max and min combiners are s max c
Depending on the underlying component learning algo-rithm, we assume the probabilistic scores assigned by the generalist and experts to be an estimation of the posterior class probability. Hence the interpretation of the max combiner is to choose the classifier that is most confident about its prediction as the combiner X  X  output. The downside is that it can be dominated by the classifier that generally outputs high values, and deteriorates if such classifier performs poorly. The min combiner performs a minimax operation, and may suffer less from a single error [26]. The spread combiner was proposed in [26] to avoid the impact of a single classifier X  X  output on the final output,
From an alternative point of view, the aggregation of decisions in general can be regarded as obtaining the optimal
Recall that an expert can be seen as a special  X  X eneralist X  that reranks categories in its compound concept and passes on the generalist X  decisions for other categories. The output of the generalist and experts are thus treated similarly. configuration of the prediction of experts and generalist. Therefore, we can create a new learning problem where the features are the outputs of the generalist (for all categories) and the experts (for their respective compound concepts), and the output space corresponds to all the categories. A fully fledged classifier can be used for learning the aggregation. From the perspective of this additional learner, the generalist and experts act as agents for feature reduction and selection, as generally there are many more (orders of magnitude) features than categories. Also, the learning task may be much less difficult as feature values are strongly indicative of the categories. Therefore, a simple (perhaps linear) learner would suffice for this task to minimize computation burden and avoid overfitting.
As a conclusion to this part, we establish the links of aggregation to other ensemble learning framework. By using different types of classifiers in the generalist and experts, or by training classifiers using different features, the biases and error of the classifiers will be less correlated. In this manner, the ensemble framework promotes diversity among classifiers which closely follows the same rationale as many other ensemble methods. Also, the fixed combination method has also been shown to reduce error due to variance reduction.
We first comment on several aspects of efficiency of the proposed Edge framework. The computation of the compound concepts is done by testing the generalist on the training data. In online learning setting, this can usually be done along with the update step.

Second, compared to boosting methods [9] where weak hypotheses are of the same size, each expert is usually much smaller than the generalist and is learned much faster. Also, unlike weak hypotheses learned in a sequential manner, experts learn and predict independently and thus the computation can be performed in parallel.

Third, Edge is flexible in combining different types of multi-class learners (to strike the balance of space and time efficiency) to achieve superior results. Boosting strong classifiers, on the other hand, may lead to overfitting.
Also, although we use linear methods for efficient cate-gorization, the two-stage classification and the combination methods render the Edge method nonlinear. Nonlinearity (and the capability of experts to better discriminate between difficult categories) yields performance gains over a mono-lithic linear learning method, as we show in the next section.
In this section we first introduce two sets of evaluation metrics, followed by the description of the experiment datasets and component learners. A series of experiments are conducted to investigate the performance of the aggre-gation methods, the experts and the Edge algorithm.
We use two sets of metrics to evaluate the efficacy of the proposed approach. The first set of metrics measure the position of the highest ranked true category, denoted by k ( k x is an integer and k x =  X  if true categories are not retrieved). We define recall at k , denoted by R k , to be the proportion of instances for which at least one of the retrieved true categories are among the top k categories: In the single label (multiclass) setting, R 1 corresponds to the traditional accuracy. Since the learners in this paper are capable of providing a list of ranked categories, we also report R 5 to measure the portion of test instances having at least one true category among the top five categories.
In the multi-label or ranked retrieval setting, we are inter-ested in the algorithm X  X  performance in ranking categories. max F 1 is derived from the F 1 measure which has been commonly used in evaluating Information Retrieval systems [5]. For an instance x , the F 1( r ) value (at position r ) is the harmonic average of precision P ( r ) and recall R ( r ) , i.e.
P ( r ) = TP ( r ) where TP ( r ) is the number of true positives in the top r positions and NTP is the total number of true categories of instance x . maxF 1 is thus defined as maxF 1 = max r F 1( r ) .
Precision Recall Break Even Point (PRBEP) [2, 30] is commonly used for measuring the performance of text categorization. A text categorization algorithm typically exhibits some trade-offs between precision and recall. For instance, an algorithm that retrieves all categories can achieve perfect recall (1) while scoring poor precision, and vice versa. For a document (or an instance) x , PRBEP is the precision or recall at position r where they are equal. Equivalently, PRBEP corresponds to the point where the classifier is tuned to have the same false positive and false negative. Note that at this point, F 1( r ) is equal to P ( r ) or R ( r ) , and thus PRBEP is no greater than maxF 1 .
Another popular metric for the text categorization com-munity is 11-point Average Precision (11-pts AvgP), which computes the average of interpolated precision values of the retrieved categories in a document in 11 recall levels (0%, 10% ... , 100%) [30].

The global average values of maxF 1 , PRBEP and 11-pts AvgP, giving equal weight to each document (micro-averaging), are reported in the sequel. We remark that in tasks with thousands of classes, such as categorizing web pages or news articles, we seek to label a given text document with one or a few classes that the system is confident about. In these tasks, performance measures based on ranking classes per instance, as we have presented above, are more appropriate than category based metrics (i.e. those measures that evaluate the quality of the ranking of instances for each category). The ranking metrics maxF 1 and PRBEP can be regarded as summary statistics of the precision-recall curve. Moreover, they are independent of the cutoff threshold usually required by text categorization algorithms. In addition, 11-pts AvgP accounts for the precision of the algorithm across all recall levels. To sum up, R 1 and PRBEP are strict measures of relevance, while R 5 and maxF 1 are relatively more optimistic metrics. Table 1 summarizes the datasets used in the evaluation. Two benchmark datasets, commonly used for text catego-rization [31], are used for evaluating the performance of the proposed algorithm. RCV1 [14] is the training split of the Reuters Corpus Volume 1 data. OHSUMED [12] is a sample of collection from the US National Library of Medicine X  X  bibliographical database PubMed. OHSUMED covers abstracts from 270 bio-medical journals in a five year span from 1987 to 1991. The labels are the human-assigned Medical Subject Heading (MeSH) terms. Y! Web is a subset of the Yahoo! web directory for classifying web pages.
These three datasets are large-scale collections (hundreds of thousands of instances and features as well as tens of thousands of categories) used in operational settings. In our experiments, features are standard unigrams and instance vectors are l 2 normalized. Also, these datasets have been randomly split into two subsets in each trial, 90% for training and the remaining 10% for testing. More details of these datasets can be found in [16].
 Table 1: Dataset overview. N , D and K stand for the total number of instances, features and categories. The last two columns represent the average number of features and categories per instance.
 Dataset N D K Feats Cats. /Inst. /Inst.
 RCV1 23,149 47k 414 76 2.1 Y! Web 69,591 685k 14k 210 1.0
OHSUMED 233,445 233k 14.3k 87 12.3
We briefly describe the two types of component learners used in our experiments. These algorithms learn linear clas-sifiers, and are capable of handling many training instances and very high input (feature) dimensionality. Since the goal of the paper is to develop a meta-learning framework, we refer the readers to the original papers for more details of the respective algorithms.
 The first type of multi-class learners are feature-based . The Feature-Focus (FF) [17, 16] algorithm efficiently learns an index from features to categories. The index is kept sparse by controlling out-degrees of features and using weights adding and dropping policy during online learning [16]. FF drastically reduces training and prediction time compared to other one-against-rest and top-down hierarchi-cal classification approaches. Meanwhile, FF enjoys similar or higher levels of accuracy when compared to the state of the art (e.g. one-versus-rest SVMs), and it is substantially better than simpler methods such as Naive Bayes [16].
The second type of multi-class classifiers are prototype-based , where a prototype vector w is learned for each class. In prediction, the similarity between a class prototype and a test instance is computed by their inner product and categories are retrieved according to the similarity scores. Passive Aggressive (PA) [4] formalizes the learning problem as an optimization problem of minimizing the sum of the prototype changes and the hinge loss (or its variants). Compared to feature-based methods, prototype-based methods require the storage of a K by D prototype matrix (where K is the number of classes and D the feature dimensions), which may be infeasible when K and D are large. Also, the time for evaluating the similarity of class prototypes is proportional to the number of classes and thus they are inefficient when there are thousands of classes.
The first set of experiments evaluate the performance of different aggregation rules as discussed in Section 3.3. We picked RCV1 as the test set which has relatively fewer categories. Hence the overlap between compound concepts is more significant and allows us to better distinguish the relative performance of different combiners. We arbitrarily set the number of experts to 10 and each compound concept covers 30% to 60% of the categories.

The upper half of Table 2 shows the performance of baseline classifiers FF and PA. PA outperforms FF in this dataset, at the cost of more space consumption and much longer (over 10 times) training time. Hence we used FF as the generalist and PA as experts in Edge . The lower half of the table demonstrates the performance of Edge using different combiners. We observe that in general, Edge outperforms the baseline component learners. The performance of the order statistics combiners min and max is similar, outperformed by the learning aggregation method (using PA). The simple averaging combiner performs slightly better than the learning aggregation method in the ranking metrics, and similar in the binary recall metrics. Consider-ing the efficiency in learning and testing, we choose to use the averaging combiner in the following.

On a side note, one is interested in the performance of combining the stronger baseline in Edge . As is shown, Edge using PA as both the generalist and experts performs better than the PA baseline but inferior to that with the FF and PA combination. One intuitive explanation is that using different classifiers promotes diversity in the ensemble and reduces the correlation of errors (biases) in the generalist and experts. Experts may thus be more capable of rectifying the confusion of the generalist.
The premise for Edge to outperform the generalist is that experts should be more capable of ranking categories in their compound concepts, either because the discrimination task is easier with fewer classes or because a more sophisticated learning method is used. We verify this by using FF as the learner for both the generalist and experts in Y! Web data. Figure 4 compares the R1 value of the generalist and experts (before deactivation in line 8 in Algorithm 1) in the truncated learning problems. Since all the points locate above the y = x line, we conclude that experts perform better than the generalist within their domains of expertise. Similar scatter plots were also observed in other datasets (not shown due to space constraints).
 Table 2: Performance comparison of component learners and EDGE using different combiners in the RCV1 dataset. The generalist and experts (and learning aggregation) are separated by a plus sign. Highest values in a column are shown in bold font. Learner R 1 R 5 maxF 1 PRBEP FF baseline 0.781 0.943 0.767 0.651 PA baseline 0.856 0.974 0.833 0.750 Edge [FF+PA] max 0.849 0.981 0.847 0.758 Edge [FF+PA] min 0.866 0.969 0.834 0.755 Edge [FF+PA] spr 0.865 0.981 0.855 0.774 Edge [FF+PA] ave 0.874 0.983 0.860 0.778 Edge [PA+PA] ave 0.865 0.981 0.851 0.767
Edge [FF+PA+PA] 0.874 0.978 0.851 0.772 Dataset Learner R 1 R 5 maxF 1 PRBEP 11-pts AvgP RCV1 Y! Web OHSUMED Figure 4: Scatter plot of R1 values of the experts and the generalist in Y! Web dataset using FF.
We include the comparison results with several popular linear text categorization methods in the RCV1 and Y! Web datasets [16]. Support Vector Machines (SVMs) was originally designed for two class classification problems. One-versus-rest is the most popular mechanism for reducing multi-class problems to two-class problems, where a set of K classifiers are trained to discriminate between one class and the other K  X  1 classes. Though the comparison of these independently trained classifiers (on different problems) may not be meaningful, properly regularized binary classifiers are capable of achieving competitively empirical results [21]. We use a state-of-the-art efficient variant of the SVMs algorithm [13] in our comparison. As the number of classes exceeds the range of 10s or 100s of categories (many class setting), one-against-rest methods are incapable of handling problems of such scale. In the realm of K -ary classification, We empirically compare our methods with the Perceptron algorithm [22], which uses additive updates for linear online learning. Additionally, we compare with a committee of 10 Perceptrons [3], which has shown performance comparable to linear SVM while being more efficient. Note that these learning algorithms are different from our component learners as well as the Edge ensemble mechanism.
Table 3 summarizes the average results of Edge over five trials of each dataset 5 . The generalists used in these datasets are competent in the text categorization task (e.g., FF achieves similar R1 and R5 as SVMs, significantly outperforming Perceptron and Committee). On the other hand, Edge overall yields substantial performance gains in both classification and ranking metrics in all three datasets.
As we X  X e shown earlier, Edge achieves over 10% gain in the R 1 and maxF 1 metrics compared to the generalist in the RCV1 dataset. Edge also outperforms linear one-versus-rest SVMs in the R1 and R5 metrics. In the single-label Y! Web dataset, the improvement of Edge in the R 1 and R 5 metrics are 5% and 3% respectively. On the other hand, the OHSUMED dataset has as many as 12 categories per instance and we are more interested in the ranking results. Using FF and PA as the experts, Edge outperforms the generalist by 5%, 7% and 7% in the maxF 1 , PRBEP and 11-pts AvgP metrics. We also provide the results of committee of 10 perceptrons [16] for comparison. Edge outperforms committee by 13% in the RCV1 dataset and as much as 91% in the Y! Web dataset. In all, Edge improves the accuracies of the component learners without showing signs of overfitting in these large-scale datasets.

It is also worthwhile to note the scalability of the methods using the largest dataset OHSUMED. FF finished learning in 2 minutes, whereas PA was not able to run as a generalist algorithm due to the huge number of categories (and consequently the prohibitive space and time consumption). Edge learned 60 FF experts (with tens to hundreds of concepts each) in less than 25 minutes. By decomposing the categories into compound concepts, Edge also allowed PA to learn as experts, yielding slightly better results. We note that the performance is better than that of KNN (micro-F1 = 0.51, substantially slower and with significant feature reduction) [30], the only text categorization method used on the full domains of MeSH terms in OHSUMED [31].
The ultimate performance metric for classification algo-rithms is classification error, which consists of model error and Bayes error. Bayes error is the irreducible error of the Empirically, 20% was used for p % and  X  =5, 5, and 200 in RCV1, Y! Web and OHSUMED respectively. The choice of these parameters is however not sensitive for Edge . optimal classifier respectively.
 problem, with respect to the true data distribution. The underlying distribution, however, is in general unknown and thus one can only measure such error indirectly. This section reveals the approximate reduction of model error to help us better interpret the results in Table 3.

We first define two events corresponding to the fine-tuning of the experts. We call an instance fixed , if the generalist fails to rank its true category in the first rank but Edge successfully ranks it first (via experts X  reranking and decision aggregation). Similarly, an instance is reversed , if Edge fails to rank the true category first while the generalist ranks it correctly. Clearly, model error is reduced if the number of net fixed (#fixed -#reversed) is greater than 0. We also call an instance fixable if there is an edge from the true category to the top-ranked false category in the confusion graph. In other words, we ignore those insignificant confusions below the  X  threshold which are prone to be noise. A perfect error-driven framework (denoted by max fixed) may optimally remove all edges from the confusion graph, though this is generally not achievable because the Bayes error is non-zero with overlapping class distributions.

Table 4 illustrates the gain of Edge in the R 1 metric in the RCV1 dataset. Edge is able to fix 62% of the fixable instances in the training data, and approximately half in the test data. This is also reflected in its R1 value, which is approximately the mid-point of that of Gen and max fixed.
Automatically categorizing text documents is a challeng-ing task with the availability of massive datasets [7, 31, 15]. The Edge method proposed in this paper leverages recent advances in linear classification methods [5, 4, 16] and provides a scalable solution for this problem.
Edge belongs to the family of ensemble learning, which is an important subject of machine learning research. We only discuss a few popular ensemble methods, highlight-ing similarity and distinction with the proposed method. Boosting is a particularly popular error-driven ensemble method. AdaBoost [9] regards each learner as a weak hypothesis and combines multiple learners into a committee for classification. Unfortunately, boosting methods are not practical given the scale of the text categorization problems: it is infeasible to run a many-class learner on the reweighed data for hundreds or thousands of iterations. Also, they risk overfitting when the underlying learner is strong. TreeBoost.MH [8] is an extension to the popular multiclass AdaBoost.MH algorithm [23] by accounting for the hierarchical structure. It reduces the time complexity by an exponential factor compared to AdaBoost.MH. However, to achieve similar accuracy, TreeBoost.MH is still two orders of magnitude slower than the linear learning methods used in this paper, and thus is not scalable to massive data. Stacking [28] is another meta-learning approach for pooling classifiers. Stacking works by first learning multiple classifiers and then a generalizer which combines their outputs. This is not unlike learning aggregation in Section 3.3.2. However, the first level classifiers still learn from the entire dataset. In contrast to these ensemble methods, samples from the entire feature space during testing, we propose to Edge only learns experts that specialize only on part of the feature space (features and categories).

Taxonomies offer potential efficiency and efficacy advan-tages in text categorization [6, 1, 15]. However, premature mistakes in high-level classifiers can hamper accuracy in lower level categories. Also, a taxonomy may be unavailable or may not be a tree (e.g. MeSH terms in OHSUMED form a DAG), besides its complex implementation. Instead of learning classifiers for sibling categories, Edge discov-ers compound concepts in a data-driven manner. These compound concepts are not confined to the hierarchical structure, as sibling categories are not necessarily confused and easily confused categories may be non-siblings (e.g. alt.atheism vs. soc.religion.christian in Figure 1). Hence error-driven decomposition of the text categorization prob-lem may overcome the aforementioned drawbacks in hierar-chical text categorization and yield improved performance.
Our work shares similarities with the work of [11], who used the confusion matrix of a Naive Bayes classifier to scale up one-versus-rest SVM learning. In their work, the first level classifier discriminates among all categories. For the highest predicted class, the two-class SVMs (trained on that class and its confused categories in one-versus-rest setting) are invoked to confirm or overturn that prediction. However, the first level classifier may prematurely categorize an instance into a class that the SVMs experts are unable to recover. Also, since the set of SVMs need to be learned for each category, this method is not appropriate for the many-class and multi-label learning setting, and the experiments were run on relatively small data sets (news groups and Reuters, with 20 and 60 classes). In our work, Edge learns an expert per compound concept with respect to the confusion graph (a generalized notion of the confusion matrix in the multi-label setting). Since the number of compound concepts tends to be significantly lower than the total number of concepts, and each generated problem is smaller, training time is significantly reduced in the second stage. Furthermore, rather than relying on the decision of a single expert, Edge aggregates the decision of the generalist and all activated experts, which possesses the merits of ensemble learning mentioned earlier.
In the setting of text categorization with tens of thousands of categories, building a highly accurate and efficient many-class classifier is essentially a very difficult task. Rather than attempting to build a monolithic classifier, Edge decom-poses the learning problem and builds a number of smaller and more accurate classifiers, which we call experts in this paper. Experts utilize fewer classes and lower dimensionality and may be complementary to each other. The discovery of these experts is data driven: learning resources are placed on distinguishing concepts that tend to be confused by the generalist. By combining the decisions of the experts and the generalist, Edge demonstrates substantial improvement in classification and ranking accuracies in large-scale text categorization datasets. There are several opportunities for improvement. First, experts can be combined with consideration of their ratings (i.e. their relative prediction performance on data). Also, learning a generalist in conjunction with the experts, possibly in an online manner, may prove useful. We also want to explore the use of statistical tests of significance in constructing the confusion graphs and in computing compound concepts. The authors would like to thank Rosie Jones, Preston McAfee and Ron Brachman from Yahoo! Research for valuable comments and reviewers for helpful feedback. [1] L. Cai and T. Hofmann. Hierarchical document [2] R. Caruana and A. Niculescu-Mizil. Data mining in [3] V. R. Carvalho and W. W. Cohen. Single-pass online [4] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, [5] K. Crammer and Y. Singer. A family of additive [6] S. Dumais and H. Chen. Hierarchical classification of [7] S. Dumais, J. Platt, D. Heckerman, and M. Sahami. [8] A. Esuli, T. Fagni, and F. Sebastiani. TreeBoost.MH: [9] Y. Freund and R. Schapire. A decision-theoretic [10] Y. Freund, R. Schapire, Y. Singer, and M. Warmuth. [11] S. Godbole, S. Sarawagi, and S. Chakrabarti. Scaling [12] W. Hersh, C. Buckley, T. Leone, and D. Hickam. [13] S. Keerthi and D. DeCoste. A modified finite newton [14] D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A new [15] T. Liu, Y. Yang, H. Wan, H. Zeng, Z. Chen, and [16] O. Madani and M. Connor. Large-scale many-class [17] O. Madani, W. Greiner, D. Kempe, and M. R.
 [18] O. Madani and J. Huang. On updates that constrain [19] M. E. J. Newman. Mixing patterns in networks. [20] J. Rennie, L. Shih, J. Teevan, and D. Karger. Tackling [21] R. Rifkin and A. Klautau. In defense of one-vs-all [22] F. Rosenblatt. The perceptron: A probabilistic model [23] R. Schapire and Y. Singer. Improved boosting [24] F. Sebastiani. Machine learning in automated text [25] K. Tumer and J. Ghosh. Analysis of decision [26] K. Tumer and J. Ghosh. Robust combining of [27] D. J. Watts and S. Strogatz. Collective dynamics of [28] D. H. Wolpert. Stacked generalization. Neural [29] L. Xu, A. Krzyzak, and C. Y. Suen. Methods of [30] Y. Yang. An evaluation of statistical approaches to [31] Y. Yang, J. Zhang, and B. Kisiel. A scalability
