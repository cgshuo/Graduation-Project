 PageRank [1], HITS [2] and SALSA [3] are three of the most popular link-structure based algorithms for web mining. No matter which method is used, a graph consisting of web pages and their hyperlinks will be constructed for fur-ther processing. Consequently, different graph analysis algorithms or traversing strategies are applied in order to measure the importance of each web page. Borodin and colleagues [4] further differentiated between these algorithms ac-cording to the scope of their derived target graph. In their opinion, PageRank is query independent because it operates on the whole web. On the other hand, both HITS and SALSA are query dependent since a base set consisting of web pages relevant to a given topic must be constructed from a query on the topic. Using filtering strategies is needed to construct such a base set since the amounts of irrelevant pages examined during construction are enormous. Whether the fil-tering strategies are good enough for excluding topic irrelevant pages as far as possible will significantly influence the recommendation results. However, elim-inating navigational links is the only effort to filter irrelevant pages for most of the HITS based algorithms. In this paper, we will address the quality issue of the base set. The definition of quality used here is how relevance of the rec-ommendation results to the given topic. We will measure the relevance by two different ways. The first is to calculate the precision and recall rates suggested by Vaughan [5]. The second is to investigate the quality of recommendation results via a user satisfaction survey.
 approach called potential hubs and authorities first (PHA-first) approach to con-struct a base set for further hub and authority analysis. Our PHA-first approach examines the outgoing and incoming links of the root set as usual. Instead of including all pages pointing to or pointed by the root set, we apply several inclu-sion rules, including co-citation, recommendation and relay to both the outgoing and incoming link sets.
 PHA-first approach and the derived algorithm. Section 3 is our experimental results and discussion. And then, we draw a conclusion and describe the future work in the last section. The hypertext-induced topic selection (HITS) algorithm proposed by Klein-berg is a very popular and effective algorithm to rank documents. However, the method still suffers from the problem of topic drift [6]. To reduce the impact of topic drift, various schemes have been proposed [6][7][8]. Although, these meth-ods improved the performance of the original HITS, the underlying methods of base set construction were nearly the same.
 an elementary requirement for any HITS-base algorithms. That motivated us to explore the possibility to improve the recommendation results by control-ling the  X  X uality X  of the base set. The concept is actually not unusual. Chau et.al., [9] have proposed a priority first approach to collect topic relevant web pages, although the method was not HITS-based. Inspired by Chau X  X  research, we try to develop a priority first approach based on the framework of HITS algorithm.
 recommending many good quality pages and a good authority is a web page cited by many good quality pages. If we have constructed a root set of good quality web pages, then we can iteratively apply some inclusion rules which choose the pages in accordance with recommendation or co-citation to construct the base set. Based on the motivation, we come up with a novel base set construction method. Instead of filtering the candidate pages randomly[2][3] or by examining anchor text [7][9], our method applies three inclusion rules further utilizing the definition of hub and authority. Provided that a root set with good quality, such as a set consisting of the top dozens query results of a content-based search engine on a keyword, our inclusion rules will select potential hubs or authorities into the base set as far as possible. Therefore, our method is named as potential hubs and authorities first (PHA-first) approach.
 to p i and F ( p i ) as the set pages pointed by p i . The three rules in our PHA-first approach are: 1. The co-citation rule. As shown in left side of Figure 1, every page p a not 2. The recommendation rule. As shown in right side of Figure 1, every page 3. The relay rule. Two preceding rules aim at including pages with the stronger ure 2. In Figure 2, p h ,p a ,p relay are potential hubs, authorities and relays relative to the current base set S q respectively. After adding these pages and deleting all irrelevant pages, the current iteration ends. Depending on how many links away from the root set we want to explore, we can decide the number of iter-ations. The default value for iterations is 3, which is larger than the setting of the CLEVER [7] algorithm by 1.
 1. Choice of the root set. Because of iterative nature, our PHA-first approach 2. Deletion of irrelevant pages. The main purpose of deleting irrelevant pages can be calculated. The calculation is identical to the original HIT algorithm. We have implemented two versions of PHA-first based algorithms. In the first version, we used 10 pages generated from three search engines (Yahoo!, Lycos and Teoma) as the root set. We call this version as PHA sr ( X  X r X  stands for a small root set). In the other version, the root set consisted of 200 pages found by searching from Yahoo!. We call this version as PHA 200 . In addition, we have implemented the HITS algorithm for comparison.
 ble 1. Instead of comparing the performance by using the traditional recall and precision rates, we use a web-version recall and precision rates proposed in [5]. Table 1 shows the recall and precision rates of three different algorithms. 96. Comparing with the number of pages collected by HITS, it is about only 4% on average. Even so, both the recall as well as precision rates of PHA sr are significantly superior to the ones of HITS. The higher recall rates mean that PHA sr do not miss any important pages, which are capable of being found by HITS, even the initial root set is very small. More importantly, the higher precision rates mean that the linkage structure inside the base set still accurately reflects the relative importance of topic relevant pages. PHA 200 is about 40%. However, PHA 200 got unusual high precision and terrible low recall. That is, the recommendation ranks match the expectation of respon-dents very well. But the recommendation pages are not good enough, at least not matching the best results collected from the commercial search engines. If we refer the moderate satisfaction (to be proposed later) of PHA 200 , it is clear that our PHA-first approach is more suitable for a small root set. Since the satisfaction scores of PHA sr is better in most cases.
 That implies the ranks recommended by HITS algorithm are very opposite to the expectation of respondents. In other words, it is possible to improve the results generated by HITS algorithm.
 of respondents to grade the top 5 hubs and authorities recommended by these three algorithms on a score from 0 to 4 (the highest score). Figure 3 shows the satisfaction scores of these three algorithms on those 12 keywords.
 1.49 respectively. PHA sr obtains better scores than the other two, and hence is more able to satisfy the requirement of respondents on average. Actually, in Figure 3, 8 of 12 scores are the highest for PHA sr .Moreover,3to4ofthese8 cases have statistically significant better scores. In this paper, we propose a novel approach to construct a good quality base set for HITS based link structure analysis. We were motivated by the fact that the quality of base set will influence the final results of hub and authority analysis. Three inclusion rules to ensure the quality of base set have been presented. The rules are co-citation, recommendation, and relay. According to the experimen-tal results, our method performs well on the measure of precision. In addition, the satisfaction investigation is also promising. However, comparing with com-mercial search engines, the diversity of pages recommended by our method is insufficient. We will try to increase the iterations performed by our method and to see whether the recall rate will be improved.

