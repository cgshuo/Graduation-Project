 Pu Wang, Carlotta Domeniconi, Huzefa Rangwala, and Kathryn B. Laskey Co-clustering [11] has emerged as an important approach for mining relational data. Often, data can be organized in a matrix, where rows and columns present a symmetrical relation. Co-clustering simultaneously groups the different kinds of objects involved in a relation; for example, proteins and molecules indexing a contingency matrix that holds information about their interaction. Molecules are grouped based on their binding patterns to proteins; similarly, proteins are clustered based on the molecules they interact with. The two clustering processes are inter-dependent. Understanding these interactions provides insight into the underlying biological processes and is useful for designing therapeutic drugs.
Existing co-clustering techniques typically only leverage the entries of the given contingency matrix to perform the two-way clustering. As a consequence, they cannot predict the interaction valu es for new objects. This greatly limits the applicability of current co-clustering approaches.

In many applications additional features associated to the objects of interest are available, e.g., sequence information for proteins. Such features can be lever-aged to perform predictions on new data. The Infinite Hidden Relational Model (IHRM) [36] has been proposed to leverage features associated to the rows and columns of the contingency matrix to forecast relationships among previously un-seen data. Although IHRM was originally introduced from a relational learning point of view, it is essentially a co-clustering model that overcomes the afore-mentioned limitations of existing co-clustering techniques. In particular, IHRM is a nonparametric Bayesian model, which learns the number of row and column clusters from the given samples. This is achieved by assuming Dirichlet Process priors to the rows and columns of the contingency matrix. As such, IHRM does not require the apriori specification of the numbers o f row and column clusters in the data.

Existing Bayesian co-clustering models [30,35,19] are related to IHRM, but none makes use of features associated to the rows and columns of the contin-gency matrix. As a consequence, these met hods can handle missing entries only for already observed rows and columns (e.g., for a protein and a molecule used during training, although not necessarily in combination). In particular, IHRM can be viewed as an extension to the nonparametric Bayesian co-clustering (NBCC) model [19]. IHRM adds to NBCC the ability to exploit features as-sociated to rows and columns, thus enabling IHRM to predict entries for unseen rows and/or columns. The authors in [36] have applied IHRM to collaborative filtering [27]. Co-clustering techniques have also been applied to collaborative filtering [33,15,10], but again none of these involve features associated to rows or columns of the data matrix.

The work on IHRM [36] lacks an evaluation of the improvement that can be achieved when leveraging features to ma ke predictions for unseen objects. In this work, we fill this gap and re-interpret IHRM from a co-clustering point of view. We call the resulting method Feature Enriched Dirichlet Process Co-clustering (FE-DPCC). We focus on the empirical evaluation of forecasting relationships between previously unseen objects b y leveraging object features. Researchers have proposed several discri minative and generative co-clustering models, e.g. [7,29]. Bayesian Co-clustering (BCC) [30] maintains separate Dirich-let priors for row-and column-cluster probabilities. To generate an entry in the data matrix, the model first generates the row and column clusters for the en-try from their respective Dirichlet-multinomial distributions. The entry is then generated from a distribution specific to the row-and column-cluster. Like the original Latent Dirichlet Allocation (LDA) [5] model, BCC assumes symmetric Dirichlet priors for the data distributions given the row-and column-clusters. Shan and Banerjee [30] proposed a variational Bayesian algorithm to perform inference with the BCC model. In [35], the authors proposed a variation of BCC, and developed a collapsed Gibbs sampling and a collapsed variational algorithm to perform inference. All aforementione d co-clustering models are parametric, i.e., they need to have specified the number of row-and column-clusters.
A nonparametric Bayesian co-clustering (NBCC) approach has been proposed in [19]. NBCC assumes two independent Bayesian priors on rows and columns. As such, NBCC does not require apriori the number of row-and column-clusters. NBCC assumes a Pitman-Yor Process [24] prior, which generalizes the Dirich-let Process. The feature-enriched met hodweintroducehereisanextensionof NBCC, where features associated to rows and columns are used. Such features enable our technique to predict entries for unseen rows/columns.

A related work is Bayesian matrix factorization. In [17], the authors allevi-ated overfitting in singular value decomposition (SVD) by specifying a prior distribution over parameters, and performing variational inference. In [26], the authors proposed a Bayesian probabilistic matrix factorization method, that as-signs a prior distribution to the Gaussian parameters involved in the model. These Bayesian approaches to matrix factorization are parametric. Nonparamet-ric Bayesian matrix factorization models include [8,32,25].

Our work is also related to collaborative filtering (CF) [27]. CF learns the relationships between users and items u sing only user preferences to items, and then recommends items to users based on the learned relationships. Various ap-proaches have been proposed to discover underlying patterns in user consump-tion behaviors [6,16,1,18,17,26,31,12,14]. Co-clustering techniques have already been applied to CF [33,15,10]. None of these techniques involve features asso-ciated to rows or columns of the data ma trix. On the contrary, content-based (CB) recommendation systems [3] predic t user preferences to items using user and item features. In practice, CB methods are usually combined with CF. The approach we introduce in this paper is a Bayesian combination of CF and CB. The Dirichlet process (DP) [9] is an infinite-dimensional generalization of the Dirichlet distribution. Formally, let S be a set, G 0 ameasureon S ,and  X  0 a positive real number. The random probability distribution G on S is dis-tributed as a DP with concentration parameter  X  0 (also called the pseudo-count) and base measure G 0 if, for any finite partition { B k } 1  X  k  X  K of S :
Let G be a sample drawn from a DP. Then with probability 1, G is a discrete distribution [9]. Further, if the first N  X  1drawsfrom G yield K distinct values  X  1: K with multiplicities n 1: K , then the probability of the N on the previous N  X  1drawsisgivenbytheP  X  olya urn scheme [4]:
The DP is often used as a nonparametric prior in Bayesian mixture mod-els [2]. Assume the data are generated from the following generative procedure: G  X  Dir(  X  ity distributions known as mixture components. Typically, there are duplicates among the  X  1: N ; thus, multiple data points are generated from the same mix-ture component. It is natural to define a c luster as those obser vations generated from a given mixture component. This model is known as the Dirichlet process mixture (DPM) model. Although any finite sample contains only finitely many clusters, there is no bound on the number of clusters and any new data point has non-zero probability of being drawn from a new cluster [20]. Therefore, DPM is known as an  X  X nfinite X  mixture model.

The DP can be generated via the stick-breaking construction [28]. Stick-breaking draws two infinite sequences of independent random variables, v k  X  Beta(1 , X  0 )and  X   X  k  X  G 0 for k = { 1 , 2 ,  X  X  X } .Let G be defined as: that samples the value  X  with probability 1. Then G  X  Dir(  X  0 ,G 0 ). It is helpful to use an indicator variable z n to denote which mixture component is associated with x n . The generative process for the DPM model using stick-breaking is as follows (additional details on the DPM model can be found in [20,23]): The observed data X of FE-DPCC are composed of three parts: the observed row features X R , the observed column features X C , and the observed relational features X E between rows and columns. If there are R rows and C columns, then X { not be observed.
FE-DPCC is a generative model that assumes two independent DPM priors on rows and columns.
 We follow a stick-breaking representation to de-scribe the FE-DPCC model. Specifically, assum-ing row and column DP priors Dir(  X  R 0 ,G R 0 )and Dir(  X  C 0 ,G C 0 ), FE-DPCC draws row-cluster pa-rameters  X   X  R k from G R 0 ,for k = { 1 ,  X  X  X  ,  X  X  , column-cluster parameters  X   X  C l from G C 0 ,for l = { 1 ,  X  X  X  ,  X  X  , and co-cluster parameters  X   X  E kl from G 0 , for each combination of k and l and column mixture proportion  X  C as defined in Eq. 1. For each row r and each column c , FE-DPCC draws the row-cluster indicator z R r and column-cluster in-dicator z C c according to  X  R and  X  C , respectively. Further, FE-DPCC assumes the observed features of each row r and each column c are drawn from two para-where z R r = k and z C c = l .

The generative process for FE-DPCC is as follows and the FE-DPCC model is illustrated in Figure 1. 4.1 Inference The likelihood of the observed data is given by: {  X 
The marginal likelihood obtained by integrating out the model parameters  X  G 0 , G expression for the marginal likelihood (2). The conditional distribution for sam-k th row-cluster excluding the r th row,  X   X  R  X  r to the k th row-cluster excluding the r th row, and  X   X  E  X  r kz C becomes:
The conditional distribution for sampling the column-cluster indicator variable z c for the c l  X  X  Z C where  X  c means excluding the c th column, N  X  c l is the number of columns as-parameter of the posterior distribution of the l th column-cluster parameter  X   X  C l given all columns assigned to the l th column-cluster excluding the c th column, ( z r ,l ) given all entries assigned to it excluding the entries in the c z column-cluster, the conditional distribution becomes: We conducted experiments on two rating datasets and two protein-molecule interaction datasets. MovieLens 2 is a movie recommendation dataset containing 100,000 ratings in a sparse data matrix for 1682 movies rated by 943 users. Jester 3 is a joke rating dataset. The original dataset contains 4.1 million continuous ratings of 140 jokes from 73,421 users. We chose a subset containing 100,000 ratings. Following [30], we uniformly discretized the ratings into 10 bins.
We also used two protein-molecule interaction datasets. The first dataset (MP1 4 ) consists of G-protein coupled recep tor (GPCR) proteins and their in-teraction with small molecules [13]. These interactions are the product of an experiment that determines whether a particular protein target is modulated by a molecule. MP1 had 4051 interactions between 166 proteins and 2687 molecules. The second dataset (MP2 5 ) [21] differs from MP1 in that the protein targets be-long to a more general class and are not restricted to GPCRs. The use of targets restricted to a specific group of p roteins (GPCRs) is similar to a chemogenomics approach where the assumption is that proteins in the same family have a similar activity/interaction profile. MP2 had 154 proteins, 2876 molecules and a total of 7146 positive interactions. Table 1 summarizes the dataset characteristics. 5.1 Experimental Methodology and Feature Information We first compared FE-DPCC with a variant of NBCC, called Dirichlet Process Co-clustering (DPCC). DPCC restricts the Pitman-Yor pri-ors of NBCC to the special case of independent Dirichlet Processpriorsonrowsand columns, so as to compare with FE-DPCC fairly. So, the difference between FE-DPCC and DPCC is that FE-DPCC augments DPCC to exploit row and column features. We ran 1000 iterations of Gibbs sampling for both FE-DPCC and DPCC. We used perplexity as an evaluation metric to compare FE-DPCC with DPCC on all the test data. The perplexity of a dataset D is defined as perplexity ( D )=exp(  X  X  ( D ) /N ), where L ( D ) is the log-likelihood of D ,and N is the number of data points in D . The higher the log-likelihood, the lower the perplexity, and the better a model fits the data.

The relational features in our data are discrete. We assume f (  X |  X   X  E kl )isa ical distribution with support { 1 ,  X  X  X  ,D } , and we denote the Dirichlet hyperpa-rameter as  X  E =  X  =  X  d | d = { 1 ,  X  X  X  ,D } . The predictive distribution of the co-cluster ( k, l ) to observe a new entry x E r c = d , d  X  X  1 ,  X  X  X  ,D } ,is: where  X   X  kl is the posterior hyperparameter of the Dirichlet distribution of the ( k, l ) and is equal to d .

In MovieLens, users (rows) are represented with age, gender, and occupa-tion, whereas the movies (columns) are associated with a 19-dimensional genre-representing binary vector. We assumed independence among the row features and the column features conditional on row-and column-clusters. We modeled age as drawn from a Poisson distribution, Poi(  X |  X  ), with a conjugate Gamma prior, Gamma(  X  | ",  X  ). We modeled gender as drawn from a Bernoulli distribu-perparameter is  X  R = ",  X ,  X ,  X  . We denote the feature vector of a new user as x r = a r ,g r ,o r ,where a r , g r ,and o r represent the age, gender and occupa-tion, respectively. The predictive distribution of the k th row-cluster observing a new user, x R r ,is: sociated with movies are generated fr om a Multinomial distribution, Mul(  X |  X  ), dictive distribution of the l th column-cluster observing a new movie, x C c ,is: p ( x C c |  X   X  l ,z C c = l )= rior hyperparameter of the Dirichlet distribution ( l indexes the column-clusters).
In Jester, there are no features associated with the users (rows), thus row-clusters cannot predict an unseen user. We used a bag-of-word representation for joke features, and assumed each joke feature vector is generated from a dictive distribution of the l th column-cluster observing a new joke, x C c ,is: p ( x C c |  X   X  l ,z C c = l )= For MP1 and MP2, rows represent molecu les and columns represent proteins. We extracted k -mer features from protein seq uences. For MP1, we also used hierarchical features for proteins obtained from annotation databases. We used a graph-fragment-based feature represen tation that computes the frequency of different length cycles and paths for each molecule. These graph-fragment-based features were derived using AFGEN [34] (default parameters were used), known to capture structural aspects of molecu les effectively. We assumed each pro-tein was generated from a Multinomial distribution, Mul(  X |  X  p ), with a Dirich-let prior, Dir(  X  p |  X  p ). We also assumed each molecule was generated from a predictive distribution of the k th row-cluster observing a new molecule, x R r ,is:
The predictive distribution of the l th column-cluster observing a new protein, x c ,is: p ( x 5.2 Results We performed a series of experiments to evaluate the performance of FE-DPCC across the four datasets. All experiments were repeated five times, and we re-port the average (and standard deviation) perplexity across the five runs. The experiments were performed on an Intel four core, Linux server with 4GB mem-ory. The average running time for FE-DPCC was 1, 3, 3.5 and 2.5 hours on the MovieLens, Jester, MP1 and M P2 datasets, respectively.
 Feature Enrichment Evaluation. Table 2 shows the average perplexity (and standard deviations) across five runs on the test data. To analyze the effect of new rows and columns on the prediction capabilities of the algorithms, we split each test set into subsets based on whether the subset contains new rows or columns w.r.t. the corresponding training data. Table 2 shows that the overall perplexity of FE-DPCC is lower than that of DPCC on all data, with an improvement of 12%, 1.5%, 84% and 81% for MovieLens, Jester, MP1 and MP2, respectively.
FE-DPCC is significantly better than DPCC on the portion of the test data that contains unseen rows and/or column s. These test sets consist of entries for rows and columns that are not included in the training set. The DPCC algorithm does not use features; as such it can predict entries for the new rows and columns using prior probabilities only. In contrast, the FE-DPCC algorithm leverages features along with prior probabilities; this enables our approach to predict values for the independent test entries more accurately. This ability is a major strength of our FE-DPCC algorithm. For the portion of the test data whose rows and columns are observed in the training as well, the perplexity values of FE-DPCC and DPCC are comparable. The standard deviations indicate that the algorithms are stable, yielding consistent results across different runs.
To accurately assess the performance of FE-DPCC, we performed a set of experiments that involved a perturbation of the protein and molecule features on MP1. Results are in Table 3. For these experiments, we used k -mer sequence features. First, we took the protein seq uences (i.e., columns) and shuffled the ordering of the amino acids. This alters the ordering of the protein sequence but maintains the same composition (i.e., the shuffled sequences have the same number of characters or amino acids). We refer to this scheme as  X  X huffle X . It achieves an average perplexity of 3 . 034, versus the average perplexity of 1 . 450 achieved by FE-DPCC (with no shuffling of features). We also devised a scheme in which the row and/or column features are exchanged, e.g., the features of a particular molecule are exchanged with the features of another molecule. Such an exchange causes the inclusion of incorrect information within the FE-DPCC algorithm. Our aim was to assess the strength of FE-DPCC when enriched with meaningful and correct features. We refer to this scheme as  X  X xchange. X  Table 3 shows the results of exchanging molecule features only (Exchange M), protein features only (Exchange P), and both (Exchange M and P). We noticed an average perplexity of 2.9 in each case. We also evaluated the FE-DPCC algorithm when only molecule or only protein fea tures are used ( X  X se Only M X  and  X  X se only P X  in Table 3). The use of only one set of features prevents the co-clustering algorithm from making inferences on the unseen rows or columns in the test set.
For MP1 we performed additional experiments to evaluate the sequence fea-tures. The features are overlapping subs equences of a fixed len gth extracted from the protein sequences. We used k -merlengthsof2,3,4and5,andobserved that the average perplexity (Table 4) remained similar. As such, we used 5-mer features in all the experiments. We also compared the sequence features for the proteins to an alternate feature derived from a hierarchical biological annotation of the proteins. For MP1 the hierarchical features were extracted as done in the previous study [13,22]. From Table 4 we observe that the hierarchical features (HF) achieved a slightly lower perplexity as compared to the 5-mer features. This is encouraging, as it suggests that sequence features perform similarly to manual annotation (hierarchy), that may not be easily available for all the proteins. Comparative Performance. We compared FE-DPCC with a well known col-laborative filtering model, Slope One [16]. We used a Slope One implementation from the Apache Mahout machine learning library 6 . We used the root mean square error (RMSE) [6] to compare FE-DPCC and Slope One on MovieLens and Jester. Table 5 shows the RMSE values (and standard deviations) of FE-DPCC and Slope One across five runs on the test sets 7 . These results show that incorporating row and column features is beneficial for the prediction of relationships.
 Visualization of Co-clusters. In Figure 2 we illustrate the co-cluster struc-tures learned by FE-DPCC on MovieLens and Jester. We calculate the mean entry value for each co-cluster, and plot the resulting mean values. Data Density. We varied the density of MovieLens and Jester to see how it affects the perplexity of FE-DPCC and DPCC. We varied the matrix density by randomly sampling 25%, 50% and 75% of the entries in the training data. The sampled matrices were then given as input to DPCC and FE-DPCC to train a model and infer unknown entries on the test data. Figure 3 illustrates the results averaged across five iterations. As the sparsity of the relational matrix increases the test perplexity increases for both FE-DPCC and DPCC. But DPCC has far higher perplexity for a sparser matrix. As the matrix sparsity increases, the information within the relational matrix is lost and the FE-DPCC algorithm relies on the row and column features. T hus, for sparser matrices FE-DPCC shows far better results than DPCC. These experiments suggest the reason why we see a more dramatic difference between the two algorithms for MP1 and MP2, which are very sparse (see Table 1). In this work, we focus on the empirical evaluation of FE-DPCC to predict rela-tionships between previously unseen obj ects by using object features. We con-ducted experiments on a variety of relational data, including protein-molecule interaction data. The evaluation demon strates the effectiveness of the feature-enriched approach and demonstrates that features are most useful when data are sparse.
 Acknowledgements. This work was in part supported by NSF III-0905117.
