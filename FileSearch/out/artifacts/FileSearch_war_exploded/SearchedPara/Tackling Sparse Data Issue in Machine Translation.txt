 Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correla-tion with human judgments while various tech-niques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008) 1 , WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009) 2 .

The contribution of this paper is twofold. Sec-tion 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of lan-guages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal repre-sentation leading to low coverage of e.g. a transla-tion dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hy-pothesis and the reference. Figure 1: BLEU and human ranks of systems par-ticipating in the English-to-Czech WMT09 shared task.

Section 3 introduces and evaluates some new variations of SemPOS (Kos and Bojar, 2009), a metric based on the deep syntactic representation of the sentence performing very well for Czech as the target language. Aside from including depen-dency and n -gram relations in the scoring, we also apply and evaluate SemPOS for English. BLEU (Papineni et al., 2002) is an established language-independent MT metric. Its correlation to human judgments was originally deemed high (for English) but better correlating metrics (esp. for other languages) were found later, usually em-ploying language-specific tools, see e.g. Przy-bocki et al. (2008) or Callison-Burch et al. (2009). The unbeaten advantage of BLEU is its simplicity.
Figure 1 illustrates a very low correlation to hu-man judgments when translating to Czech. We plot the official BLEU score against the rank es-tablished as the percentage of sentences where a system ranked no worse than all its competitors (Callison-Burch et al., 2009). The systems devel-oped at Charles University (cu-) are described in Bojar et al. (2009), uedin is a vanilla configuration of Moses (Koehn et al., 2007) and the remaining ones are commercial MT systems.

In a manual analysis, we identified the reasons for the low correlation: BLEU is overly sensitive to sequences and forms in the hypothesis matching Table 1: n -grams confirmed by the reference and containing error flags. the reference translation. This focus goes directly against the properties of Czech: relatively free word order allows many permutations of words and rich morphology renders many valid word forms not confirmed by the reference. 3 These problems are to some extent mitigated if several reference translations are available, but this is of-ten not the case.

Figure 2 illustrates the problem of  X  X parse data X  in the reference. Due to the lexical and morpho-logical variance of Czech, only a single word in each hypothesis matches a word in the reference. In the case of pctrans, the match is even a false positive,  X  X o X  (to) is a preposition that should be used for the  X  X inus X  phrase and not for the  X  X nd of the day X  phrase. In terms of BLEU, both hy-potheses are equally poor but 90% of their tokens were not evaluated.

Table 1 estimates the overall magnitude of this issue: For 1-grams to 4-grams in 1640 instances (different MT outputs and different annotators) of 200 sentences with manually flagged errors 4 , we count how often the n -gram is confirmed by the reference and how often it contains an error flag. The suspicious cases are n -grams confirmed by the reference but still containing a flag (false posi-tives) and n -grams not confirmed despite contain-ing no error flag (false negatives).

Fortunately, there are relatively few false posi-tives in n -gram based metrics: 6.3% of unigrams and far fewer higher n -grams.

The issue of false negatives is more serious and confirms the problem of sparse data if only one reference is available. 30 to 40% of n -grams do not contain any error and yet they are not con-firmed by the reference. This amounts to 34% of running unigrams, giving enough space to differ in human judgments and still remain unscored.

Figure 3 documents the issue across languages: the lower the BLEU score itself (i.e. fewer con-firmed n -grams), the lower the correlation to hu-man judgments regardless of the target language (WMT09 shared task, 2025 sentences per lan-guage).

Figure 4 illustrates the overestimation of scores caused by too much attention to sequences of to-kens. A phrase-based system like Moses (cu-bojar) can sometimes produce a long sequence of tokens exactly as required by the reference, lead-ing to a high BLEU score. The framed words in the illustration are not confirmed by the refer-ence, but the actual error in these words is very severe for comprehension: nouns were used twice instead of finite verbs, and a misleading transla-tion of a preposition was chosen. The output by pctrans preserves the meaning much better despite not scoring in either of the finite verbs and produc-ing far shorter confirmed sequences. SemPOS (Kos and Bojar, 2009) is inspired by met-rics based on overlapping of linguistic features in the reference and in the translation (Gim  X  enez and M  X  arquez, 2007). It operates on so-called  X  X ec-togrammatical X  (deep syntactic) representation of the sentence (Sgall et al., 1986; Haji  X  c et al., 2006), formally a dependency tree that includes only au-tosemantic (content-bearing) words. 5 SemPOS as defined in Kos and Bojar (2009) disregards the syntactic structure and uses the semantic part of speech of the words (noun, verb, etc.). There are 19 fine-grained parts of speech. For each semantic part of speech t , the overlapping O( t ) is set to zero if the part of speech does not occur in the reference or the candidate set and otherwise it is computed as given in Equation 1 below. single unigram in each hypothesis is confirmed in the reference. little about translation quality.
 O( t ) =
The semantic part of speech is denoted t ; c i and r i are the candidate and reference translations of sentence i , and cnt( w,t,rc ) is the number of words w with type t in rc (the reference or the can-didate). The matching is performed on the level of lemmas, i.e. no morphological information is pre-served in w s. See Figure 5 for an example; the sentence is the same as in Figure 4.

The final SemPOS score is obtained by macro-averaging over all parts of speech: where T is the set of all possible semantic parts of speech types. (The degenerate case of blank candidate and reference has SemPOS zero.) 3.1 Variations of SemPOS This section describes our modifications of Sem-POS. All methods are evaluated in Section 3.2. Different Classification of Autosemantic Words. SemPOS uses semantic parts of speech to classify autosemantic words. The tectogram-matical layer offers also a feature called Functor describing the relation of a word to its governor similarly as semantic roles do. There are 67 functor types in total.

Using Functor instead of SemPOS increases the number of word classes that independently require a high overlap. For a contrast we also completely remove the classification and use only one global class ( Void ).
 Deep Syntactic Relations in SemPOS. In SemPOS, an autosemantic word of a class is con-firmed if its lemma matches the reference. We uti-lize the dependency relations at the tectogrammat-ical layer to validate valence by refining the over-lap and requiring also the lemma of 1) the parent (denoted  X  X ar X ), or 2) all the children regardless of their order (denoted  X  X ons X ) to match.

Combining BLEU and SemPOS. One of the major drawbacks of SemPOS is that it completely ignores word order. This is too coarse even for languages with relatively free word order like Czech. Another issue is that it operates on lemmas and it completely disregards correct word forms. Thus, a weighted linear combination of SemPOS and BLEU (computed on the surface representa-tion of the sentence) should compensate for this. For the purposes of the combination, we compute BLEU only on unigrams up to fourgrams (denoted BLEU 1 , . . . , BLEU 4 ) but including the brevity penalty as usual. Here we try only a few weight settings in the linear combination but given a held-out dataset, one could optimize the weights for the best performance. important errors (not considered by BLEU) are framed.
SemPOS for English. The tectogrammatical layer is being adapted for English (Cinkov  X  a et al., 2004; Haji  X  c et al., 2009) and we are able to use the available tools to obtain all SemPOS features for English sentences as well. 3.2 Evaluation of SemPOS and Friends We measured the metric performance on data used in MetricsMATR08, WMT09 and WMT08. For the evaluation of metric correlation with human judgments at the system level, we used the Pearson correlation coefficient  X  applied to ranks. In case of a tie, the systems were assigned the average po-sition. For example if three systems achieved the same highest score (thus occupying the positions 1, 2 and 3 when sorted by score), each of them would obtain the average rank of 2 = 1+2+3 3 . When correlating ranks (instead of exact scores) and with this handling of ties, the Pearson coeffi-cient is equivalent to Spearman X  X  rank correlation coefficient.

The MetricsMATR08 human judgments include preferences for pairs of MT systems saying which one of the two systems is better, while the WMT08 and WMT09 data contain system scores (for up to 5 systems) on the scale 1 to 5 for a given sentence. We assigned a human ranking to the systems based on the percent of time that their translations were judged to be better than or equal to the translations of any other system in the manual evaluation. We converted automatic metric scores to ranks.

Metrics X  performance for translation to English and Czech was measured on the following test-sets (the number of human judgments for a given source language in brackets): To English: MetricsMATR08 (cn+ar: 1652), To Czech: WMT08 News Articles (en: 267),
The MetricsMATR08 testset contained 4 refer-ence translations for each sentence whereas the re-maining testsets only one reference.

Correlation coefficients for English are shown in Table 2. The best metric is Void par closely fol-lowed by Void sons . The explanation is that Void compared to SemPOS or Functor does not lose points by an erroneous assignment of the POS or the functor, and that Void par profits from check-ing the dependency relations between autoseman-tic words. The combination of BLEU and Sem-POS 6 outperforms both individual metrics, but in case of SemPOS only by a minimal difference. Additionally, we confirm that 4-grams alone have little discriminative power both when used as a metric of their own (BLEU 4 ) as well as in a lin-ear combination with SemPOS.

The best metric for Czech (see Table 3) is a lin-ear combination of SemPOS and 4-gram BLEU closely followed by other SemPOS and BLEU n combinations. We assume this is because BLEU 4 can capture correctly translated fixed phrases, which is positively reflected in human judgments. Including BLEU 1 in the combination favors trans-lations with word forms as expected by the refer-Table 2: Average, best and worst system-level cor-relation coefficients for translation to English from various source languages evaluated on 10 different testsets. ence, thus allowing to spot bad word forms. In all cases, the linear combination puts more weight on SemPOS. Given the negligible difference be-tween SemPOS alone and the linear combinations, we see that word forms are not the major issue for humans interpreting the translation X  X ost likely because the systems so far often make more im-portant errors. This is also confirmed by the obser-vation that using BLEU alone is rather unreliable for Czech and BLEU-1 (which judges unigrams only) is even worse. Surprisingly BLEU-2 per-formed better than any other n -grams for reasons that have yet to be examined. The error metrics PER and TER showed the lowest correlation with human judgments for translation to Czech. This paper documented problems of single-reference BLEU when applied to morphologically rich languages such as Czech. BLEU suffers from a sparse data problem, unable to judge the quality of tokens not confirmed by the reference. This is confirmed for other languages as well: the lower the BLEU score the lower the correlation to hu-man judgments.

We introduced a refinement of SemPOS, an automatic metric of MT quality based on deep-syntactic representation of the sentence tackling Table 3: System-level correlation coefficients for English-to-Czech translation evaluated on 3 differ-ent testsets. the sparse data issue. SemPOS was evaluated on translation to Czech and to English, scoring better than or comparable to many established metrics.
