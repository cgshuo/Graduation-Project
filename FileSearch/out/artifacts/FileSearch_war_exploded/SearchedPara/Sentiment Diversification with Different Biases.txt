 Prior search result diversification work focuses on achiev-ing topical variety in a ranked list, typically equally across all aspects. In this paper, we diversify with sentiments ac-cording to an explicit bias. We want to allow users to switch the result perspective to better grasp the polarity of opinion-ated content, such as during a literature review. For this, we first infer the prior sentiment bias inherent in a controversial topic  X  the  X  X opic Sentiment X . Then, we utilize this infor-mation in 3 different ways to diversify results according to various sentiment biases: (1) Equal diversification to achieve a balanced and unbiased representation of all sentiments on the topic; (2) Diversification towards the Topic Sentiment, in which the actual sentiment bias in the topic is mirrored to emphasize the general perception of the topic; (3) Diver-sification against the Topic Sentiment, in which documents about the  X  X inority X  or outlying sentiment(s) are boosted and those with the popular sentiment are demoted.

Since sentiment classification is an essential tool for this task, we experiment by gradually degrading the accuracy of a perfect classifier down to 40%, and show which diversifi-cation approaches prove most stable in this setting. The re-sults reveal that the proportionality-based methods and our SCSF model, considering sentiment strength and frequency in the diversified list, yield the highest gains. Further, in case the Topic Sentiment cannot be reliably estimated, we show how performance is affected by equal diversification when actually an emphasis either towards or against the Topic Sentiment is desired: in the former case, an average of 6.48% is lost across all evaluation measures, whereas in the latter case this is 16.23%, confirming that bias-specific sentiment diversification is crucial.
 Categories and Subject Descriptors: H.3.3 [ Information Search and Retrieval ]: Retrieval Models General Terms: Experimentation, Algorithms, Measure-ment Keywords: Diversity, Opinions, Sentiment, Proportional-ity
In previous work diversification has mainly been applied for better topical variety in search results [9, 25, 26, 27]. Equal preference is typically given to all aspects. How can opinionated content exhibiting sentiments be diversified? Ini-tial approaches have been presented [10, 17, 18]; however these only consider equal diversification. In this paper, we view the problem from a high-level perspective to allow for sentiment diversification according to different biases , which will be vital for situations like a literature review on a con-troversial topic.

Consider the topic  X  X lobal warming. X  In a typical use case, a user engages in a comprehensive literature review with the aim of understanding the positions on this topic. This in-volves  X  besides searching and finding relevant opinionated documents [16]  X  understanding and mentally categorizing opinionated content. This can be done by organizing the discussed arguments by topical content; or, they can also be grouped by sentiment, such as positive, negative, neutral, and mixed [18]. We focus on facilitating the latter approach for the user. For some topics that can clearly be general-ized into  X  X ro X  versus  X  X on X  arguments, this sentiment cate-gorization is more natural, whereas it can be less obvious for topics like global warming that are associated with various arguments. Focusing on the sentiment dimension of these arguments, we can see that negative sentiments for global warming typically express criticism and concern about it and its effects on the environment. Those with positive sen-timent often claim that worries about global climate change are unjustified ( X  X here is no such issue X ), playing down the concerns in a  X  X alming X  (i.e., positive) way. Mixed or neutral statements either express no sentiments or contain an equal amount of positive and negative arguments. Those could be  X  X  don X  X  care X , or  X  X t X  X  a serious problem but we X  X e handling it X  kind of stances towards global warming.

Getting back to our use case: while a balanced and un-biased presentation of the results helps the user understand various viewpoints on a topic, discerning the topic X  X  polarity is harder if minority opinions are  X  X uried X  in the results [18]. Therefore, the user should be able to switch the result per-spective as needed. This way, she can either obtain a bal-anced or a biased view on majority or minority opinions, make her own comparisons across the representations, and perform this task in a more informed manner. Note that this is different from showing all positive or all negative or all neutral/mixed documents at a time: with such a rep-resentation the user would still need to draw her own con-clusions about which sentiments form majority or minority Figure 1: Topic Sentiment: Dots represent relevant docu-ments for this topic, which are grouped according to their sentiments. The obtained sentiment distribution is used for sentiment diversification. opinions. Our aim is to analyze this information for the user and to match the inferred trend as closely as possible in the results. For this, we need to have a good grasp of the topic a priori: we consider a large pool of relevant docu-ments about the topic, which are grouped by sentiments as visualized in Figure 1. Then, we can infer the topic X  X  senti-ment distribution or inherent bias  X  Topic Sentiment  X  from this analysis. We categorize the aspects  X  X ixed X  and  X  X eu-tral X  together to represent the  X  X alanced X  aspect, whereas  X  X ositive X  and  X  X egative X  refer to arguments that are clearly biased towards one side only. If Figure 1 represented the Topic Sentiment for global warming, this could be inter-preted as the issue being perceived with great concern since negative sentiments constitute the majority, and while there are some  X  X alanced X  positions on it, the positive sentiments form a clear minority. By utilizing this information during diversification, three biases are emphasized in search results: (1) Equal diversification by preferring all sentiments equally. This allows for a balanced representation of all sentiments on the topic; (2) Diversification towards the Topic Senti-ment, in which the resulting reranked list mirrors the actual sentiment bias in a topic. This approach highlights the gen-eral perception of a topic; (3) Diversification against the Topic Sentiment, in which documents about the minority sentiment(s) are boosted whereas those with the majority sentiment are demoted. Such a list highlights unusual and outlying opinions on the topic.

In this paper we propose different diversification models for sentiment diversity with these 3 biases, and perform ex-periments using the TREC Blog Track data [23]. Since sen-timent classification is an essential tool for this task, we experiment by gradually reducing the accuracy of a perfect classifier down to 40%, and show which diversification ap-proaches prove most stable in this setting. Further, in case the Topic Sentiment cannot be reliably estimated, we show how performance is affected by equal diversification when actually an emphasis either towards or against the Topic Sentiment is desired. The results reveal that particularly when highlighting minority sentiments, diversifying with the corresponding bias yields significant improvements.
There is a large amount of work in the area of topical diversity: the main aim is to eliminate topical redundancy in results while maximizing the number of documents con-taining novel information [1, 3, 6, 9, 25, 26, 27]. One of the earliest works is the maximal marginal relevance (MMR) approach [3], which employs content-based similarity mea-sures to balance the tradeoff between novelty and relevance. More recently, researchers have shown explicit diversification approaches to be superior over implicit diversification tech-niques: well-known algorithms are xQuAD [25, 26, 27], IA-select [1], and more recently PM-1 and PM-2 [9]. Other ap-proaches to topical diversity are language modeling based [31], probabilistic [4] and correlation-based [30]. Very recent re-search addresses personalized diversification [29], blog feed diversity [19, 28], and combined implicit and explicit aspect diversification [15].

Among these approaches, it is common to equally or uni-formly diversify across all query aspects or subtopics due to the lack of data [9, 25, 26, 27]. Although the TREC Web Track diversity task provides topical query aspects [8], dis-tributions over these aspects are not included. Agrawal et al. [1] use their own classifiers and judgments for obtaining query intent aspect distributions. Further, the NTCIR-9 Intent task provides non-uniform aspect probabilities [24]. One of our contributions in this paper is to present alter-natives to the equal distribution approach (Section 3.4): a query X  X  topic X  X  sentiment distribution can be employed in various ways to yield an emphasis for a certain bias. Topi-cal diversity could also benefit from these ideas.

The proportionality-based approaches PM-1 and PM-2 [9] distinguish themselves from prior research by explicitly match-ing the aspect distribution in the diversified list to the over-all popularity of these aspects, thus yielding a proportionally diversified list. We adapt this approach to sentiment diver-sity, and propose a minor variation for dealing with retrieval limitations (Section 3.3).

Extensive work on opinion detection and retrieval has dealt with techniques to boost opinionated documents in retrieval [14, 28, 32, 33]. Prior work focusing on opinion diversity is very recent: Demartini and Siersdorfer describe a study about opinions in search results as given by popular search engines for controversial queries [11]. In later work, Demartini then tackles opinion diversification [10]: his ap-proach is based on the xQuAD framework [25]. Retrieved search results are classified into the sentiment categories pos-itive, negative, or objective. Diversification ensures maxi-mum variety among these aspects with uniform preference. We implement this approach as the SCS model and combine it with the 3 biases (Section 3.2.1). The SCSF model is a further extension, presented in Section 3.2.2.

Kacimi and Gamper propose a different opinion diversifi-cation framework for controversial queries [17, 18]: three cri-teria are considered for diversification: topical relevance, se-mantic diversification, and sentiment diversification. Their model favors documents most different in sentiment direc-tion and in the arguments they discuss. The sentiments are again one of positive, negative, and neutral. In the model the components are linearly combined; however, in order to find the documents maximizing the distances for all criteria the authors consider all subsets of documents. Our work differs from this work in several points: (1) We perform sen-timent diversification only and not opinion diversification. Opinions refer to topical content, whereas sentiments are a non-topical aspect that we focus on in this paper. (2) This choice allows us to study sentiment diversification perfor-mance with different biases, which has not been researched in prior work.

In this context, unlike topical diversity we make a sim-plifying assumption that each query belongs to one topic and therefore represents one topical aspect. We avoid deal-ing with ambiguity by using long and specific queries in our experiments, as explained in Section 4. That is, the topi-cal dimension is kept static so we can focus on the varied sentiment dimension. We leave it to future work to explore the interplay of topical and sentiment aspects together for diversification.
In order to diversify a retrieved list with respect to the distribution of sentiments in a query X  X  topic, we need to introduce a few concepts first. Let Q be a query, and let T be the query X  X  topic T ( Q ), abbreviated as T for simplicity. As visualized in Figure 1, we define T to include all the relevant documents that can be retrieved for Q , i.e., T = rel ( Q ). Further, let each document D in T have a sentiment, i.e., each document is positive, negative, neutral or mixed. These can be generalized to countable sentiment criteria  X   X  sent ( T ). We will use this sentiment information from T in our models to diversify search results according to the distribution of sentiments in the topic.

Sentiment criteria of the form positive, negative, and neu-tral/mixed can take different shapes when converted into a sentiment score. In the literature [10, 17, 23] we identified a document to either have a single discrete sentiment from { X  1 , 0 , 1 } , or the sentiment is broken down into three scores positivity , negativity , and neutrality such that they sum to 1.0 for a single document. We refer to these latter ones as finer grained  X  X ractional scores X  in the rest of the paper. Our models are designed for these kinds of scores, but discrete scores can also be handled by simple conversion as we will show later.

Below we consider two different diversification frameworks and present several modifications to them. Algorithm 1 Retrieval Interpolated Diversification Frame-work.
Algorithm 1 shows the Retrieval-Interpolated Diversifica-tion Framework, which is similar to xQuAD, first introduced by Santos et al. [25] for topical diversity. In this diversifi-cation framework, documents retrieved in R are iteratively added to the new ranked list S . The  X  documents are chosen according to the maximization objective function in line 4:
D  X  = argmax D  X  R  X   X  RetC( Q ) + (1  X   X  )  X  SentC( T ) (1) where RetC( Q ) is the retrieval contribution , which is always estimated with P ( D | Q )  X  how likely D is to be relevant to Q by content, and SentC( T ) is the sentiment contribution , which we will define in two different ways below. The scores from these two components are interpolated for diversity estimation.
In this version of the model we estimate the sentiment contribution in the maximization objective function (Equa-tion 1) as follows: Here P ( D,  X  S | T ) measures how much D can contribute to the sentiment diversity of S . Structurally, this resembles xQuAD [25] with the difference that the estimation is con-ditioned on the query X  X  topic T .

In order to make the model more flexible towards senti-ment scores, we define each document to have a fractional score for each sentiment criterion  X   X  sent ( T ). For exam-ple, a document may be classified as positive with 75% con-fidence. Then, this can be converted into a trinary score P ( D |  X  = positive) = 0 . 75, P ( D |  X  = neutral) = 0 . 25, and P ( D |  X  = negative) = 0. Fractional classification scores di-rectly obtained from a classifier (such as logistic regression) fit in nicely into this framework. If documents are manually judged, they are often associated with only one  X  X ominant X  sentiment score from { X  1 , 0 , 1 } such as -1, which can be con-verted into a 100% negative score. Given this information, we can further decompose P ( D,  X  S | T ) as follows: where P (  X  S |  X  ) denotes the likelihood of  X  not being satis-fied by the documents already chosen into S (see below for further derivation) and P (  X  | T ) stands for the importance of sentiment  X  to topic T . This is discussed in detail in Sec-tion 3.4. From Equation 3 to Equation 4 we make the same independence assumption as Santos et al. [25]: the diversity estimation of D with respect to the sentiments  X  can be made independently of the documents already selected into S . We continue with Equation 4: Here we make another independence assumption for P ( D j |  X  ) as Santos et al. [25]: the likelihood of not sampling D j timent from T is independent of the sentiments of the other documents in S . Since each D j was independently chosen into S , this is a reasonable assumption.

To summarize, Equation 5 estimates the diversity of a document D by considering how well D represents each sen-timent criterion, which is weighted by how important that Algorithm 2 Diversity by Proportionality (PM-2). sentiment criterion is to T . This whole part is demoted ac-cording to how many documents of the same sentiment S already contains.
We consider an alternative formulation of the sentiment contribution component above in Equation 1 in which the punish/reward factor is estimated slightly differently: where P ( D | T ) stands for how important D  X  X  sentiment is for T , and 1  X  P ( S | T ) describes how well the sentiments from T are already represented in S . We further derive: Here we apply the Bayes X  Rule to P ( S |  X  ): which is rank-equivalent since P ( S ) is a constant across all documents in an iteration, and P (  X  ), the prior probability of a particular sentiment, is equal across all sentiments. Hence we obtain from Equation 7: Now we can see that the first part of Equation 9 is identical to Equation 5. We can estimate the components P ( D |  X  )  X  P (  X  | T ) the same way as described in Sections 3.2 and 3.4. However, P (  X   X  | S ), the likelihood of S not having sentiment  X  , is new. We define its complement as follows: which is the number of documents in S having dominant sentiment  X  . Each document in S can be mapped into its dominant or most confident sentiment class  X   X  sent ( T ), typically positive, negative, or neutral/mixed. Given this, we count the number of times a particular sentiment  X  oc-curs in S as sent (  X ,S ). We set P (  X  | S ) = 0 if S =  X  to avoid zero division in the first iteration.

To summarize, this formulation calculates the punish/ re-ward factor directly from the frequency of documents present in the whole set S with certain sentiments. Contrarily, in the SCS model the strength of sentiments of each document in S is considered individually, whereas the frequency of such documents is implicit in the multiplication over all docu-ments in S . In the experiments we empirically verify the effectiveness of the two models in sentiment diversification to draw conclusions about their usefulness.
As a second diversification framework we consider Algo-rithm 2, the best-performing approach in Dang and Croft X  X  work [9]. This framework is based on the Sante-Lagu  X  e method for seat allocation and is adapted here to sentiment diver-sification. In each iteration documents are chosen based on the proportionality of the diversified list. We only de-scribe modified components due to space limitations. In-stead of applying the algorithm to topical aspects, here it is employed together with sentiments  X   X  sent ( T ). Further, P ( D |  X  ) is estimated by means of fractional sentiment scores as defined in Section 3.2 instead of estimating the relevance of the document with respect to a (sub)topical aspect. Note that under this modification, a document is purely evaluated on the basis of its sentiments and not according to topical relevance.
 The variables v  X  and s  X  in the quotient are important. The former is the number of relevant documents the senti-ment  X  should have, whereas the latter represents the es-timated number of documents actually present in the list for  X  . v  X  at a particular rank i can easily be inferred from P (  X  | T ) as follows: According to PM-2, s  X  is updated with the fractional sen-timent scores of the chosen document, since each sentiment takes up a  X  X ortion X  of the seats in S . This denotes how well the chosen document represents each sentiment. For the relationship between document sentiments and the topic sentiment distribution, please refer to Section 3.4.
Unlike the seat allocation problem in a voting system, in a retrieved list of documents there is an additional con-straining factor. The top K documents retrieved from a search system constitute the source for diversification, so it is possible that a particular sentiment is underrepresented in this list. Unless the system requests more documents, the desired proportionality in the diversified list may not be optimally achieved with the current set of documents. In this situation, with respect to PM-2 the given votes v overestimate l  X  , the actual number of documents with senti-ment  X  in the retrieved top K set. For a large enough rank K , this may result in a suboptimally diversified list where documents with an over-emphasized sentiment are exploited early in the ranks. Therefore, we propose a small change to the quotient defined in Algorithm 2: which ensures that the quotient does not over-emphasize the importance of a sentiment if data is missing in the retrieved list. This technique has a remote resemblance to dispro-portionate stratified sampling in that documents are cho-sen slightly differently than dictated by the topic sentiment distribution in favor of improved overall diversity. We re-fer to this modified diversification approach as PM-2M and compare its effectiveness to PM-2, SCS and SCSF in the experimental section 4. In the presentation of the diversification models above P (  X  | T ) plays a central role in defining which sentiment bias is favored in search results. Intuitively, this component stands for the importance of sentiment  X  to topic T . Below we present three different possible biases in search results that the estimation of P (  X  | T ) impacts.
This is our baseline approach, which does not give pref-erence to any sentiment, but weights them equally or uni-formly. Therefore, this approach does not utilize informa-tion from the query X  X  topic about its prior sentiment distri-bution. We set which results in each sentiment criterion  X   X  sent ( T ) to be considered equally important. We refer to this bias method as  X  X alance X  (BAL) in Section 4.

We assume that with this balanced estimation the SCS model is equivalent to Demartini X  X  [10] approach. Since this detail is not explicitly described in their work, it is most reasonable to assume an equal bias as in prior research.
In this approach we choose to diversify the retrieved list towards the distribution of sentiments in the query X  X  topic. Such results strongly represent the crowd X  X  opinion(s). For this, we need information about the sentiments in T . Re-call from Section 3.1 that T is defined as a topic space to include all the relevant documents that can be retrieved for the query Q , i.e., T = rel ( Q ). Then, we can map each relevant document into its dominant or most confident sen-timent class  X   X  sent ( T ). Given this, we count the number of times a particular sentiment  X  occurs in T as sent (  X ,T ). This allows us to interpret P (  X  | T ) as the likelihood of sen-timent  X  being drawn from T : which represents the fraction of documents in T with dom-inant sentiment  X  ; for instance the fraction of positive doc-uments in T . We name this bias as  X  X rowd X  (short: CRD).
What if a user is interested in viewing minority sentiments on the topic? For favoring outlying sentiments, we need to diversify the search results against the Topic Sentiment. For this, we introduce one minor modification to CRD above: Let the n sentiment estimations for  X   X  sent ( T ) be sorted in increasing order of P (  X  | T ). Then, for each  X  at rank i we swap its estimation P (  X  | T ) with the one at rank n  X  i . This  X  X everses X  the values in the topic distribution without changing the properties of the distribution. Consequently, if originally in T positive documents are strongly favored and negative documents are least favored, this trend is reversed through the value swap in T so that outlying sentiments (negative documents) will be strongly preferred during di-versification. We refer to this bias as  X  X utlier X  (OTL) in the experiments (Section 4).

Irrespective of the preferred bias, we apply Add-1 Smooth-ing [5] to P (  X  | T ) estimates to account for zero probabilities. In order to correct such unrealistic estimations, an unob-served sentiment class is assigned a very small probability, and the estimations for the other sentiment classes are ad-justed accordingly. Retrieval Corpus As retrieval corpus we use the TREC Blog Track data from 2006 and 2008 [23] for all our experi-ments. For preparation, the DiffPost algorithm is applied to the corpus for better retrieval as shown in prior work [20, 22]. Further, we perform stop word removal and Porter stem-ming.
 Queries and Retrieval Model We split the 150 TREC Blog Track 2008 queries into 3 non-overlapping randomly chosen sets of size 50 each in order not to bias training or testing towards a specific year: split 1 is used for train-ing and tuning parameters; the results in this paper are reported on split 2, and split 3 is reserved for sentiment classifier training. For our diversification experiments, we use a strong retrieval baseline: the queries X  stopped title and description texts are combined for use with the Sequen-tial Dependence Model in Lemur/Indri [21], smoothed using Dirichlet (  X  = 10 , 000). All diversification models are ap-plied to the top K = 50 retrieved documents as determined during training. The retrieval scores are normalized to yield document likelihood scores.
 Sentiment Classification The sentiment classifier is trained as a logistic regression model using Liblinear [13] with de-fault settings. For this, we utilize the judged documents from the 50 split 3 TREC Blog Track queries. Training is done for three classes  X  positive, negative, and neutral to obtain probability estimates that are employed as frac-tional scores for sentiment estimation (Section 3.2.1). As features we extract Sentiwordnet 3.0 terms with their length-normalized term frequencies in the documents [12].
 Topic Sentiment Estimation Given a query, the topic sentiment distribution can be estimated in various ways: (1) in the form of opinion relevance judgments for a pool of doc-uments where all judged relevant documents are included in the distribution; (2) by retrieving the top M documents from a separate corpus or web search engine and tagging them with sentiment judgments. We experimented with both ap-proaches but only present the results for (1) here due to space limitations: we use the relevance judgments from the TREC 2008 Blog Track [23], which are divided into the same sentiment aspects as required in the models.
The sentiment diversification approaches are evaluated us-ing standard evaluation measures that were designed for top-ical diversity: Precision-IA [1], s-recall [31],  X  -NDCG [6], ERR-IA [2], and NRBP [7]. The former two measures are set-based, whereas the remaining ones are cascade measures as described by Ashkan and Clarke [2], punishing redun-dancy through parameters  X  (  X  -NDCG, ERR-IA, NRBP) and additionally  X  (NRBP), which represents user patience. In order to measure sentiment diversity with a chosen bias, we implement all the measures in their intent-aware (or for us,  X  X entiment-aware X ) version [1, 2]. Hence, the weighted average over the sentiment-dependent scores of a measure is computed as given by measure-IA for a query Q and topic T : measure-IA( Q,T ) = X where P (  X  | T ) defines the weight for the sentiment-specific result yielded by measure( Q |  X  ).
 Intent-aware measures can be rank-specific such as Precision-IA@k or  X  -NDCG@k for example, or rank-independent as NRBP. We utilize another rank-specific measure defined by Dang and Croft [9], Cumulative Proportionality (CPR) at rank K : in which PR @ i is computed as the inverse normalized dis-proportionality at rank i (see [9] for details). Here, we define the disproportionality at rank i as follows: where v  X  is the number of relevant documents the sentiment  X  should have, s  X  is the number of relevant documents ac-tually found for  X  , n NR is the number of documents that are non-relevant (to any sentiment), and c  X  = 1 if v  X   X  s 0 otherwise. This measure allows us to assess how propor-tional the diversified list is with respect to the desired topic distribution. v  X  can be inferred from the true topic sen-timent distribution P (  X  | T ) in the same way as detailed in Equation 11. As noted by Dang and Croft [9], CPR penal-izes the under-representation of aspects (here: sentiments) and the over-representation of non-relevant documents.
In this section we discuss the results of the retrieval base-line SDM and all the proposed diversification models in Sec-tion 3, SCS, SCSF, PM-2 and PM-2M, with the three biases, Crowd (CRD), Balance (BAL) and Outlier (OTL). The in-terpolation parameter  X   X  X  0 . 0 ,..., 1 . 0 } is tuned in 0.1 steps separately for each model and bias on our training split. The results are presented with fixed parameters K and  X  on test split 2, and the evaluation is performed with the TREC 2008 Blog Track judgments at rank 20.  X  -NDCG, ERR-IA, and NRBP require parameters, which are set to  X  = 0 . 5 and  X  = 0 . 5.
Our primary aim in the experiments is to evaluate sen-timent diversification performance. Sentiment classification is an important part of the system since both the to-be-diversified documents need to be tagged with sentiments, as well as those for the topic sentiment distribution esti-mation. Since a  X  X ull evaluation X  of sentiment diversifica-tion techniques on a publicly available dataset has not been done yet in prior work, it is important to understand how sentiment classification quality affects diversification per-formance. Therefore, we start with a  X  X erfect system X  in which classification accuracy is 100% for judged documents. For unjudged documents the trained sentiment classifier de-scribed in Section 4.1 is applied. We then gradually reduce the overall classification performance in 10% steps until 40% as follows: given the top K = 50 retrieved documents for a query, before diversification we randomly sample the ranks at which the true classification label is switched to another label randomly to achieve the desired classification error for each query.

Figure 2 shows the results for the straight-bias experi-ment, in which the topic sentiment distribution employed in experiment and evaluation underlies the same favored bias . For instance, the left-most column in Figure 2 shows the re-sults for diversifying towards Crowd in the experiments, and measuring performance for Crowd in the evaluation (short-CRD-CRD). The middle column shows the same for Bal-ance (short: BAL-BAL), and the right-most column is for the Outlier bias (short: OTL-OTL).

At the top-most row in the Precision-IA@20 graphs we ob-serve a big gap between the SDM baseline and SCS model versus the rest of the models. For Crowd, the SCSF model only dominates when classification accuracy is at least 60% while it achieves the best (however not statistically signif-icant) numbers in the Outlier graph. PM-2M and PM-2 also perform well and dominate some of the lower accuracy ranges. Statistical significance with the paired two-sided t-test (p-value &lt; 0.05) is indicated in the graphs with circles: the lighter blue circles refer to the result being significant over the SCS and SDM models, whereas the darker dotted circles indicate significance over the SDM model only. In the Precision-IA@20 graphs the results for SCSF and the proportionality-based methods are significant over SCS and SDM even for lower accuracies. We conclude that if preci-sion is important, the SCSF diversification model should be used.

Among the s-recall@20 graphs the one for Crowd is the most arbitrary one. Performance drops well below the base-line for the SCSF and proportionality-based methods with medium quality classification: this indicates that the major-ity sentiment(s) are being emphasized too strongly, whereas minority sentiments appear much later in the ranked list for the first time, which is when the subtopic-recall measure is affected. This is expected, since we explicitly diversify in favor of majority sentiments. In the Balance and Outlier graphs for s-recall@20 there is no such trend, however pre-cision is not as high for those biases as it is for Crowd. This is a typical precision versus recall tradeoff observation. The next row shows results for  X  -NDCG@20, followed by ERR-IA@20 and NRBP: we note that the trends in these graphs look very similar, although the ranges of the val-ues differ greatly. It is interesting to observe that the peak performance for the proportionality-based methods for the Crowd bias is not at 100% classification accuracy, but at 90%. What these three measures have in common is pun-ishing redundancy based on the rank and sentiment crite-rion in addition to non-relevance. Since usually there are many documents with the majority sentiment in the re-trieved list to start with, a strong emphasis on a single sentiment criterion results in more redundancy. With the 10% error in classification documents with other sentiments are slightly boosted, yielding better overall varied ranking. In the Balance and Outlier graphs this trend cannot be ob-served, since the Balance bias does not strongly emphasize a single sentiment criterion to begin with. Concerning the Outlier bias, there are fewer documents with minority sen-timents in the retrieved list to cause the same  X  X lustered X  ranking effect as for Crowd. Summarizing the trends across the  X  -NDCG@20, ERR-IA@20, and NRBP graphs we make the following conclusion: if ranking is important, the PM-2 and PM-2M methods should be chosen.

Finally, we look at the last row of graphs with the CPR@20 results: this measure evaluates how proportional the overall list is with respect to the chosen bias. PM-2 and PM-2M achieve the best results, which is closely followed by SCSF. PM-2 and SCSF are more appropriate for lower classifica-tion accuracies (  X  70%), whereas PM-2M performs slightly better with better classification quality.

Looking at the fixed values of the interpolation parameter  X  during training for this experiment, the following insights can be drawn: for the SCS model, across all classifier accu-racies and biases generally  X   X  0 . 6 values are preferred. So this model performs best with a weaker emphasis on diver-sity, which pulls it closer to the SDM baseline as observed in the graphs of Figure 2. SCSF on the other hand has a good mixture of higher and lower  X  values across classifier accuracies and biases, with many of them being &lt; 0 . 5, par-ticularly when the classifier is more accurate. So a heavier emphasis on the diversification part helps this model. The distinguishing feature between SCS and SCSF is the con-sideration of sentiment frequencies in addition to sentiment strength contributions. When the classifier is noisy however ( &lt; 60%) and thus sentiment frequency counts are not accu-rate, SCSF also benefits from higher  X  values. In the PM-2 and PM-2M models the role of  X  is different: it balances the emphasis on the chosen aspect  X   X  versus all the other aspects  X   X  sent ( T ) , X  6 =  X   X  . Here, consistently higher  X  values are preferred for both models, i.e., a high emphasis on the chosen aspect and a minimal weight on the other ones seems most beneficial. The effectiveness of these two models solely relies on sentiment estimations: given our adaption of PM-2 from its original definition ([9]) to sentiment diversity, the retrieval scores are not used for building the diversified list.
Consider the following real-world setting: for certain top-ics, it may not be feasible to collect data for calculating Topic Sentiment estimations, or suitable corpora may cur-rently not be available. This could happen if the topic is very new and the data is not substantial enough for draw-ing general conclusions. If judgments shall be obtained, the data tagging effort may also be a burden. In such a situation we can fall back to the Balance bias or equal diversification approach [9, 25, 26, 27]. Naturally, the next question to answer is how much performance is lost when diversifying with Balance instead of the desired bias such as Outlier. The cross-bias experiments in this section investigate this case, and enable us to draw conclusions about the value of collecting and using information about topic sentiment dis-tributions for controversial topics.

We analyze two cases. The first, presented in Table 1 shows the results for equally diversifying for Balance, but performance is measured for the Crowd bias (BAL-CRD). This is contrasted with diversifying for the Crowd bias, and evaluating for the same (CRD-CRD). Bold entries in CRD-CRD indicate statistical significance over BAL-CRD with a p-value of &lt; 0 . 004 (t-test, as before). The SDM baseline is included for comparison. We omit s-recall@20 due to space limitations. All CRD-CRD results for the proportionality-based methods are significant over BAL-CRD results, whereas for the SCSF and SCS models there are a few exceptions. document, o refers to mixed/neutral, and + to positive. We observe a maximum loss of 16.92% for Precision-IA@20 with SCSF, and an average loss of 6.48% across all measures and diversification approaches.

The second case is presented in Table 2: we observe the results for equally diversifying for Balance, but performance is measured for the Outlier bias (BAL-OTL). This is con-trasted with diversifying for the Outlier bias, and evaluat-ing for the same (OTL-OTL). Similar to Table 1 the results are statistically significant for OTL-OTL over BAL-OTL, but the losses with equal diversification are more heavily pronounced here: there is a maximum loss of 48.79% for NRBP with PM-2M, and an average loss of 16.23% across all measures and diversification approaches. So for highlighting minority sentiments through diversification it is even more important to collect biased data about topic sentiment dis-tributions than it is for emphasizing majority sentiments as observed in Table 1. This way diversification can be per-formed with the intended bias rather than with equal diver-sification, which yields significantly worse results.
We presented the cross-bias experiments with perfect sen-timent classification to reveal the maximum performance loss. As classification accuracy degrades, the losses become smaller but remain noticeable.
To see the models in action, we look at the output for one query in Table 3, number 1007 from the TREC Blog Track:  X  X omen in Saudi Arabia X , asking for opinions about the treatment of women in Saudi Arabia. We show titles or characteristic excerpts from the documents together with their overall sentiment. The topic of this query has the fol-lowing Topic Sentiment: 67% negative, 17% mixed/neutral, and 16% positive. Here we diversify for the Crowd Bias, so the aim is to mirror this distribution in the results. The top 10 retrieved results with the SDM baseline are presented at the top left: this result list does not include any positive doc-uments, and an equal amount of negative and mixed/neutral documents, which is clearly unsatisfactory for a Crowd bias representation of the results. The SCS model includes one positive document at rank 3, since lower ranked documents through the SDM baseline can be pulled up by the diversifi-cation models. Although the documents are nicely shuffled around across ranks, the ratio of the sentiments is still not close to the Topic Sentiment. The SCSF model is able to cor-rect this, explicitly considering the frequency of documents with their dominant sentiments: we have 6 negative docu-ments, 3 mixed/neutral, and 1 positive. But 4 negative doc-uments are clustered right after each other, which slightly af-fects measures such as  X  -NDCG@10. The PM-2 results (bot-tom right) use the overall proportionality of the sentiments in the list as a guidance for choosing further documents: here, a second positive document is pulled up from lower ranks, yielding the best CPR@10 score among the 4 mod-els for this query at a cost of slightly lower Precision-IA@10 than SCSF. With 5 negative documents, 3 mixed/neutral ones, and 2 positive documents we are very close to the de-sired distribution of sentiments.
In this paper we demonstrate how to diversify search re-sults according to sentiments by considering a pre-defined bias. This allows us to emphasize either majority or minor-ity sentiments during diversification, or to give an unbiased representation across all sentiment aspects. For this, we in-troduce several diversification models that use sentiments and topic sentiment distributions. Diversifying the output of a strong retrieval baseline, the results on the TREC Blog Track data reveal that the proportionality-based methods and the SCSF model perform best according to most mea-sures, but an individual choice should be made based on the quality of the sentiment classifier at hand. Finally, we demonstrate the value of using biases and collecting topic sentiment distribution estimations by means of cross-bias experiments in which equal diversification is performed in-stead of the desired bias.

The ideas presented in this paper are not only valuable for sentiment diversity, but can also be applied to topical diversity with modifications. To what extent does it make sense to consider biases for topical diversity? For instance, with an Outlier bias-like approach underrepresented query aspects could be highlighted in search results. Further, we have proposed different extensions to existing diversification models such as xQuAD and PM-2 with the SCSF and PM-2M models, which may be effective for topical diversity as well.

There are many directions for future work: (1) Exploring other biases applicable for sentiment diversity; (2) We found that our trained 3-class sentiment classifier and ready-to-use classifiers on the web perform rather poorly at document-level sentiment classification. State-of-the-art sentiment clas-sification works better on sentences or short text, but inter-preting the overall sentiment of a document is more diffi-cult, particularly on the web. Therefore, advances in this area would greatly benefit sentiment diversification so that it can be applied to the web beyond the TREC Blog Track; (3) In case this is difficult to realize, how can the diversifi-cation models be adapted to yield higher gains with noisy classification input; (3) Analyzing opinion or topical argu-ments and sentiments together with biases. One question to solve is what kind of biases could be defined to capture both, and whether more fine-grained topic-specific biases would be required.
This work was supported in part by the Center for Intelli-gent Information Retrieval, in part under subcontract #19-000208 from SRI International, prime contractor to DARPA contract #HR0011-12-C-0016, and in part by NSF grant #IIS-11217281. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
