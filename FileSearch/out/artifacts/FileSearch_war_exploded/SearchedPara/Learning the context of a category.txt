 Human knowledge and expertise is often tied to particular co ntexts. The superior memory that chess masters have for chessboard configurations is limited to pla usible games, and does not generalize to arbitrary groupings of pieces [1]. Expert firefighters mak e different predictions about the same this context specificity reflects the tendency for people to o rganize knowledge into independent  X  X undles X  which may contain contradictory information, an d which may be deemed appropriate to different contexts. This phenomenon is called knowledge partitioning [2 X 6], and is observed in artificial category learning experiments as well as real w orld situations. When people learn to classify stimuli in an environment where there are systemat ic changes in the  X  X ontext X  in which observations are made, they often construct category repre sentations that are tightly linked to the context, and only generalize their knowledge when the conte xt is deemed appropriate [3, 4, 6]. Context induced knowledge partitioning poses a challenge t o models of human learning. As noted in [4] many models cannot accommodate the effect, or, as discus sed later in this paper, are somewhat unsatisfying in the manner that they do so. This paper explor es the possibility that Bayesian models of human category learning can provide the missing explanat ion. The structure of the paper is as follows: first, a context-sensitive Bayesian category lear ning model is described. This model is then shown to provide a parsimonious and psychologically ap pealing account of the knowledge partitioning effect. Following this, a hierarchical exten sion is introduced to the model, which allows it to acquire abstract knowledge about the context specifici ty of the categories, in a manner that is consistent with the data on human learning. This section outlines a Bayesian model that is sensitive to t he learning context. It extends Anderson X  X  [7] rational model of categorization (RMC) by allowing the m odel to track the context in which observations are made, and draw inferences about the role th at context plays. 2.1 The statistical model The central assumption in the RMC is that the learner seeks to organize his or her observations into clusters. If z distribution over z Each cluster of observations is mapped onto a distribution o ver features. Feature values are denoted by the vector x feature values vary continuously, the RMC associates the k th cluster with a multivariate Gaussian that has mean vector This is a minor generalization of the original model, as it al lows any covariance matrix (i.e., symmet-ric positive definite  X  ) and does not require the restrictive assumption that the st imulus dimensions are independent (which would force  X  to be diagonal). While independence is reasonable when stimulus dimensions are separable [9], knowledge partitio ning can occur regardless of whether di-mensions are separable or integral (see [6] for details), so the more general formulation is useful. In the RMC, labels are treated in the same way as discrete-val ued features. Each cluster is associated with a distribution over category labels. If  X  The  X  parameter describes the extent to which items in the same clu ster are allowed to have different labels. If there are more than two labels, this generalizes t o a Dirichlet-multinomial model. Equations 1 X 3 define the standard RMC. The extension to handl e context dependence is straight-forward: contextual information is treated as an auxiliary feature, and so each cluster is linked to a distribution over contexts. In the experiments considere d later, each observation is assigned to a context individually, which allows us to apply the exact sa me model for contextual features as regular ones. Thus a very simple context model is sufficient: The context specificity parameter  X  is analogous to  X  and controls the extent to which clusters can include observations made in different contexts. In more ge neral contexts, a richer model would be required to capture the manner in which context can vary.
 Applying the model requires values to be chosen for  X  ,  X  ,  X  , ,  X  be fixed in a sensible way. Firstly, since the categories do no t overlap in the experiments discussed here it makes sense to set  X  = 0 , which has the effect of forcing each cluster to be associate d only with one category. Secondly, human learners rarely have str ong prior knowledge about the features used in artificial category learning experiments, expresse d by setting  X  to ensure that the priors over features always has a well defin ed covariance structure). Thirdly, to approximate the fact that the experiments quickly reveal th e full range of stimuli to participants, it makes sense to set Having made these choices, we may restrict our attention to  X  (the bias to introduce new clusters) and  X  (the bias to treat clusters as context general). 2.2 Inference in the model Inference is performed via a collapsed Gibbs sampler, integ rating out  X  ,  X  , and  X  and defining a sampler only over the cluster assignments z . To do so, note that where the dependence on the parameters that describe the pri or (i.e.,  X  ,  X  ,  X  ,  X  pressed for the sake of readability. In this expression z except the i th, and the normalizing term is calculated by summing Equati on 6 over all possible clus-ter assignments k , including the possibility that the i th item is assigned to an entirely new cluster. The conditional prior probability P ( z where n cluster. Since the context is modelled using a beta-Bernoul li model: the same context as the i th item. A similar result applies to the labelling scheme: same label as observation i . Finally, integrating out the mean vector for the feature values yields a d -dimensional multivariate t distribution (e.g., [10], ch. 3): P ( x i | x  X  i , z  X  i , z i = k ) = In this expression the posterior degrees of freedom for clus ter k is  X   X  posterior mean is  X  values for items in the cluster. Finally, the posterior scal e matrix is where S  X  x , and the sum in question is taken over all observations assig ned to cluster k .
 Taken together, Equations 6, 8, 9 and 11 suggest a simple a Gib bs sampler over the cluster assign-ments z . Cluster assignments z the conditional posterior distribution in Equation 6. For t he applications in this paper, the sampler typically converges within only a few iterations, but a much longer burn in (usually 1000 iterations, never less than 100) was used in order to be safe. Successive s amples are drawn at a lag of 10 iterations, and multiple runs (between 5 and 10) are used in a ll cases. To illustrate the behavior of the model, consider the most ty pical example of a knowledge partition-ing experiment [3, 4, 6]. Stimuli vary along two continuous d imensions (e.g., height of a rectangle, location of a radial line), and are organized into categorie s using the scheme shown in Figure 1a. There are two categories organized into an  X  X nside-outside  X  structure, with one category (black cir-cles/squares) occupying a region along either side of the ot her one (white circles/squares). The critical characteristic of the experiment is that each stim ulus is presented in a particular  X  X ontext X , usually operationalized as an auxiliary feature not tied to the stimulus itself, such as the background color. In Figure 1a, squares correspond to items presented i n one context, and circles to items pre-sented in the other context. Participants are trained on the se items in a standard supervised catego-rization experiment: stimuli are presented one at a time (wi th the context variable), and participants are asked to predict the category label. After making a predi ction, the true label is revealed to them. Figure 1: Stimuli used in the typical knowledge partitionin g design (left) and the different general-ization patterns that are displayed by human learners (righ t). Percentages refer to the probability of selecting category label A.
 This procedure is repeated until participants can correctl y label all items. At this point, participants are shown transfer items (the crosses in Figure 1a), and aske d what category label these items should be given. No feedback is given during this phase. Critically , each transfer item is presented in both contexts, to determine whether people generalize in a conte xt specific way.
 The basic effect, replicated across several different expe riments, is that there are strong individual differences in how people solve the problem. This leads to th e two characteristic patterns of general-ization shown in Figure 1b (these data are from Experiments 1 and 2A in [6]). Some participants are context insensitive (lower two panels) and their predictio ns about the transfer items do not change as a function of context. However, other participants are co ntext sensitive (upper panels) and adopt a very different strategy depending on which context the tra nsfer item is presented in. This is taken to imply [3, 4, 6] that the context sensitive participants ha ve learned a conceptual representation in which knowledge is  X  X artitioned X  into different bundles, e ach associated with a different context. 3.1 Learning the knowledge partition The initial investigation focused on what category represe ntations the model learns, as a function of  X  and  X  . After varying both parameters over a broad range, it was cle ar that there are two quite different solutions that the model can produce, illustrate d in Figure 2. In the four cluster solution contrast, the three cluster solution (panel a, larger  X  ) is more context general, and collapses category B into a single cluster. However, there is an interaction wit h  X  , since large  X  values drive the model to introduce more clusters. As a result, for  X  &gt; 1 the model tends not to produce the three cluster solution. Given that the main interest is in  X  , we can fix  X  such that the prior expected number of clusters is 3.5, so as to be neutral with respect to the two sol utions. Since the expected number of clusters is given by  X  P n  X  1 The next aim was to quantify the extent to which  X  influences the relative prevalence of the four cluster solution versus the three cluster solution. For any given partition produced by the model, the adjusted Rand index [12] can be used to assess its similarity to the two idealized solutions (Figure 2a and 2b). Since the adjusted Rand index measures the extent to which any given pair of items are clas-sified in the same way by the two solutions, it is a natural meas ure of how close a model-generated solution is to one of the two idealized solutions. Then, adop ting an approach loosely inspired by PAC-learning [13], two partitions were deemed to be approxi mately the same if the adjusted Rand Figure 2: The two different clustering schemes produced by t he context sensitive RMC, and the values of  X  that produce them (for  X  fixed at 0.72). See main text for details. index between the two exceeded 0.9. The estimated posterior probability that the model solutions approximate either of the the two idealized partitions is pl otted in Figure 2c as a function of  X  . At smaller values of  X  (below about 3.7) the four cluster solution is extremely dom inant whereas at larger values the three cluster solution is preferred. Si nce there are approximately 1 . 6  X  10 35 possible partitions of 40 objects, the extent of this domina nce is clearly very strong. The fact that the model concentrates on two different but ent irely sensible solutions as a function of  X  is very appealing from a psychological perspective. One of t he most desirable characteristics is the fact that the partitioning of the learners knowledge is made explicit. That is, the model learns a much more differentiated and context bound representation when  X  is small, and a more context general and less differentiated representation when  X  is large. By way of comparison, the only other model that has been shown to produce the effect is ATRIUM [14], whic h in its standard form consists of a linked  X  X ule learning X  module and an  X  X xemplar learning X  m odule. In order to fit the data, the model was modified [4] so that it starts with two rule modules a nd an exemplar model. During training, the model learns to weight each of the rule modules differently depending on context, thereby producing context specific generalizations. This p rovides a partial explanation of the effect, but it is rather unsatisfying in some ways. In ATRIUM, the kno wledge partition is represented via the learned division of responsibilities between two hard c oded rule modules [4]. In a very real sense, the partition is actually hard coded into the archite cture of the model. As such, ATRIUM learns the context dependence, but not the knowledge partit ion itself. 3.2 Generalizing in context-specific and context-general w ays The discussion to this point shows how the value of  X  shapes the conceptual knowledge that the model acquires, but has not looked at what generalizations t he model makes. However, it is straight-forward to show that varying  X  does allow the context sensitive RMC to capture the two gener aliza-tion patterns in Figure 1. With this in mind, Figure 3 plots th e generalizations made by the model for two different levels of context specificity (  X  = 0 and  X  = 10 ) and for the two different clustering solutions. Obviously, in view of the results in Figure 2c the most interesting cases are panels (a) and (d), since those correspond to the solutions most likely to b e learned by the model, but it is useful to consider all four cases. As is clear from inspection  X  and v erified by the squared correlations listed in the Figure caption  X  when  X  is small the model generalizes in a context specific manner, but when  X  is large the generalizations are the same in all contexts. Th is happens for both clustering specificity of both the learned knowledge partition and the generalizations to new observations. One thing missing from both ATRIUM and the RMC is an explanati on for how the leaner decides whether context specific or context general representation s are appropriate. In both cases, the model has free parameters that govern the switch between the two ca ses, and these parameters must be Figure 3: Generalizations made by the model. In panel (a) the model accounts for 82.1% of the variance in the context sensitive data, but only 35.2% of the variance in the context insensitive data. For panel (b) these numbers are 77.9% and 3.6% respectively. When  X  is large the pattern reverses: in panel (c) only 23.6% of the variance in the context sensiti ve data is explained, whereas 67.1% of the context insensitive data can be accounted for. In panel ( d), the numbers are 17.5% and 73.9%. estimated from data. In the RMC,  X  is a free parameter that does all the work; for ATRIUM, four separate parameters are varied [4]. This poses the ques tion: how do people acquire abstract knowledge about which way to generalize? In RMC terms, how do we infer the value of  X  ? To answer this, note that if the context varies in a systemati c fashion, an intelligent learner might come to suspect that the context matters, and would be more li kely to decide to generalize in a context specific way. On the other hand, if there are no system atic patterns to the way that observa-tions are distributed across contexts, then the learner sho uld deem the context to be irrelevant and hence decide to generalize broadly across contexts. Indeed , this is exactly what happens with human learners. For instance, consider the data from Experiment 1 in [4]. One condition of this experiment was a standard knowledge partitioning experiment, identic al in every meaningful respect to the data described earlier in this paper. As is typical for such exper iments, knowledge partitioning was ob-served for at least some of the participants. In the other con dition, however, the context variable was randomized: each of the training items was assigned to a rand omly chosen context. In this condition, no knowledge partitioning was observed.
 What this implies is that human learners use the systematici ty of the context as a cue to determine how broadly to generalize. As such, the model should learn that  X  is small when the context varies systematically; and similarly should learn that  X  is large if the context is random. To that end, this section develops a hierarchical extension to the model that is able to do exactly this, and shows that it is able to capture both conditions of the data in [4] withou t varying any parameter values. 4.1 A hierarchical context-sensitive RMC Extending the statistical model is straightforward: we pla ce priors over  X  , and allow the model to closely related to other hierarchical Bayesian models of ca tegory learning [15 X 19]. A simple choice of prior for this situation is the exponential distribution , Following the approach taken with  X  ,  X  was fixed so as to ensure that the model has no a priori bias to prefer either of the two solutions. When  X  = 3 . 7 the two solutions are equally likely (Figure 2); a value of  X  = . 19 ensures that this value of  X  is the prior median. Figure 4: Learned distributions over  X  in the systematic (dark rectangles) and randomized (light rectangles) conditions, plotted on a logarithmic scale. Th e dashed line shows the location of the prior median (i.e.,  X  = 3 . 7 ).
 Inference in the hierarchical model proceeds as before, wit h a Metropolis step added to resample  X  . The acceptance probabilities for the Metropolis sampler ma y be calculated by observing that where B ( a, b ) =  X ( a ) X ( b ) /  X ( a + b ) denotes the beta function, and n ( c = j ) items in cluster k that appeared in context j . 4.2 Application of the extended model To explore the performance of the hierarchical extension of the context sensitive RMC, the model was trained on both the original, systematic version of the k nowledge partitioning experiments, and on a version with the context variables randomly permuted. T he posterior distributions over  X  that this produces are shown in Figure 4. As expected, in the syste matic condition the model notices the fact that the context varies systematically as a function of the feature values x , and learns to form context specific clusters. Indeed, 97% of the posterior dist ribution over z is absorbed by the four process, the model infers that  X  is small and generalizes in a context specific way (as per Figu re 3). Nevertheless, without changing any parameter values, the s ame model in the randomized condition infers that there is no pattern to the context variable, whic h ends up being randomly scattered across the clusters. For this condition 57% of the posterior mass is approximately equivalent to the three cluster solution. As a result, the model infers that  X  is large, and generalizes in the context general fashion. In short, the model captures human performance qui te effectively.
 When considering the implications of Figure 4, it is clear th at the model captures the critical fea-ture of the experiment: the ability to learn when to make context specific generalizations and when not to. The distributions over  X  are very different as a function of condition, indicating th at the model learns appropriately. What is less clear is the extent to which the model would be expected to produce the correct pattern of individual differences. I nspection of Figure 4 reveals that in the randomized context condition the posterior distribution o ver  X  does not move all that far above the prior median of 3.7 (dashed line) which by construction is in tended to be a fairly neutral value, whereas in the systematic condition nearly the entire distr ibution lies below this value. In other words, the systematic condition produces more learning abo ut  X  . If one were to suppose that people had no inherent prior biases to prefer to generalize one way o r the other, it should follow that the less informative condition (i.e., random context) should r eveal more individual differences. Empir-ically, the reverse is true: in the less informative conditi on, all participants generalize in a context general fashion; whereas in the more informative condition (i.e., systematic context) some but not all participants learn to generalize more narrowly. This do es not pose any inherent difficulty for the model, but it does suggest that the  X  X nbiased X  prior chosen f or this demonstration is not quite right: people do appear to have strong prior biases to prefer contex t general representations. Fortunately, a cursory investigation revealed that altering the prior ove r  X  moves the posteriors in a sensible fashion while still keeping the two distributions distinct. The hierarchical Bayesian model outlined in this paper expl ains how human conceptual learning can be context general in some situations, and context sensi tive in others. It captures the critical  X  X nowledge partitioning X  effect [2 X 4, 6] and does so withou t altering the core components of the RMC [7] and its extensions [15, 16, 18, 20]. This success lead s to an interesting question: why does ALCOVE [21] not account for knowledge partitioning (see [4])? Arguably, AL COVE has been the dominant theory for learned selective attention for alm ost 20 years, and its attentional learning mechanisms bear a striking similarity to the hierarchical B ayesian learning idea used in this paper and elsewhere [15 X 19], as well as to statistical methods for automatic relevance determination in Bayesian neural networks [22]. On the basis of these similar ities, one might expect similar behavior from ALCOVE and the context sensitive RMC. Yet this is not the case. The answer to this lies in the details of why one learns dimensional biases. In ALCOVE, as in many connect ionist models, the dimensional biases are chosen to optimize the ability to pre dict the category label. Since the context variable is not correlated with the label in these experimen ts (by construction), ALCOVE learns to ignore the context variable in all cases. The approach taken by the RMC is qualitatively different: it looks for clusters of items where the label, the context an d the feature values are all similar to one another. Knowledge partitioning experiments more or le ss require that such clusters exist, so the RMC can learn that the context variable is not distribute d randomly. In short, ALCOVE treats context as important only if it can predict the label; the RMC treats the context as important if it helps the learner infer the structure of the world.
 Looking beyond artificial learning tasks, learning the situ ations in which knowledge should be ap-plied is an important task for an intelligent agent operatin g in a complex world. Moreover, hierar-chical Bayesian models provide a natural formalism for desc ribing how human learners are able to do so. Viewed in this light, the fact that it is possible for pe ople to hold contradictory knowledge in different  X  X arcels X  should be viewed as a special case of t he general problem of learning the set of relevant contexts. Consider, for instance, the example i n which fire fighters make different judg-ments about the same fire depending on whether it is called a ba ck-burn or a to-be-controlled fire [2]. If fire fighters observe a very different distribution of fires in the context of back-burns than in the context of to-be-controlled fires, then it should be no surprise that they acquire two distinct theories of  X  X ires X , each bound to a different context. Altho ugh this particular example is a case in which the learned context specificity is incorrect, it takes only a minor shift to make the behavior correct. While the behavior of fires does not depend on the rea son why they were lit, it does depend on what combustibles they are fed. If the distinction were be tween fires observed in a forest con-text and fires observed in a tyre yard, context specific catego ry representations suddenly seem very sensible. Similarly, social categories such as  X  X olite beh avior X  are necessarily highly context depen-dent, so it makes sense that the learner would construct diff erent rules for different contexts. If the world presents the learner with observations that vary syst ematically across contexts, partitioning knowledge by context would seem to be a rational learning str ategy.
 Acknowledgements This research was supported by an Australian Research Fello wship (ARC grant DP-0773794). [1] W. G. Chase and H. A. Simon. Perception in chess. Cognitive Psychology , 4:55 X 81, 1973. [2] S. Lewandowsky and K. Kirsner. Knowledge partitioning: Context-dependent use of expertise. [3] L.-X. Yang and S. Lewandowsky. Context-gated knowledge partitioning in categorization. [4] L.-X. Yang and S. Lewandowsky. Knowledge partitioning i n categorization: Constraints on [5] M. L. Kalish, S. Lewandowsky, and J. K. Kruschke. Populat ion of linear experts: Knowledge [6] S. Lewandowsky, L. Roberts, and L.-X. Yang. Knowledge pa rtitioning in category learning: [7] J. R. Anderson. The adaptive nature of human categorizat ion. Psychological Review , 98: [9] R. N. Shepard. Integrality versus separability of stimu lus dimensions: From an early conver-[10] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis . Chapman and [11] C. E. Antoniak. Mixtures of Dirichlet processes with ap plications to Bayesian nonparametric [12] L. Hubert and P. Arabie. Comparing partitions. Journal of Classification , 2:193 X 218, 1985. [13] L. Valiant. A theory of the learnable. Communications of the ACM , 27:1134 X 1142, 1984. [14] M. A. Erickson and J. K. Kruschke. Rules and exemplars in category learning. Journal of [15] C. Kemp, A. Perfors, and J. B. Tenenbaum. Learning overh ypotheses with hierarchical [16] A. Perfors and J. B. Tenenbaum. Learning to learn catego ries. In N. Taatgen, H. van Rijn, [17] D. J. Navarro. From natural kinds to complex categories . In R. Sun and N. Miyake, editors, [18] T. L. Griffiths, K. R. Canini, A. N. Sanborn, and D. J. Nava rro. Unifying rational models of [19] K. Heller, A. N. Sanborn, and N. Chater. Hierarchical le arning of dimensional biases in hu-[20] A. N. Sanborn, T. L. Griffiths, and D. J. Navarro. Rationa l approximations to rational models: [21] J. K. Kruschke. ALCOVE: An exemplar-based connectioni st model of category learning. Psy-[22] R. Neal. Bayesian learning for neural networks . Springer-Verlag, New York, 1996.
