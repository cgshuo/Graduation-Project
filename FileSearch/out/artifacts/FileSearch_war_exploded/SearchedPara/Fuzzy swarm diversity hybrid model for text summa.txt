 1. Introduction
Automatic text summarization is creation of a summary by machine. It has become a very important factor required in 2001 ). High quality summary is the target and challenge of any automatic text summarization. control the redundancy in summarized text and produce a more appropriate summary. Many approaches have been pro-already selected sentences. Sweeney et al. (2008) studied two different approaches to determine whether the focus on approach focuses on generating a summary by extracting the most dissimilar sentences and the other approach adds some researchers concluded that the existence of additional information with the most diverse content was not so important and minimizes the relevance. That approach treats a sentence with high maximal importance as one that has high impor-tance in the document and less relevance to already selected sentences.
 Zhang, &amp; Zhang, 2007; Ziegler &amp; Skubacz, 2007 ).
 fuzzy logic and swarm intelligence could give better performance in terms of summarization.
Improvement of summary quality remains a key research problem and needs much work. The good performance of represent large texts for storage and retrieval purposes.
 experimental results. Section 11 presents discussion of the results. Section 12 draws the conclusion and future work. 2. Related work important and unimportant based on the outputs of the four modules. Alemany and Fort (2003) presented a summarizer good summaries. A different hybrid model was introduced by Cunha et al. (2007) , which combines mainly three systems, scores from the three extracts after scoring of those extracted sentences.
 cuss each method separately and then we combine them together in some hybrid models. 3. Maximal marginal importance (MMI) diversity-based text summarization
Maximal marginal importance (MMI) ( Binwahlan et al., 2009a ) is a diversity-based text summarization method for sum-this method are combined in a linear combination to show the importance of the sentence. The reason for including the novelty. The features used are as follow: 3.1. Sentence centrality
The sentence centrality consists of summation of three features: the similarity between the sentence in hand s document sentence s j , shared friends (the group of sentences which are similar to both sentences s grams (the group of n -grams which are contained in both sentences s of sentences in the document similarity threshold is either 0.03 or 0.16. sim is the similarity between the sentence in hand s s which are contained in both sentences s i and s j (calculated using Eq. (3) )). where s i (friends) is the group of sentences similar to s group of terms contained in sentence s i and s j (n-grams) is the group of terms contained in sentence s 3.2. Title feature relevance sentence (THSRS).
 tence (calculated using Eq. (6) ).
 where s i (n-grams) are terms of any sentence s i in the document and T(n-grams) are terms of the document title. where s i (n-grams) are terms of any sentence s j in the document and THS(s previously marked as title-help sentence. 3.3. Word sentence score (WSS) It is calculated as the following: where: 0.1 = Minimum score the sentence gets in case it X  X  terms are not important
W ij =Asin (8) , is the term weight (TF X  X SF) of the term t isf = 1-[ (log(sf(t ij ) + 1)/log(n + 1) ]. Then TF X  X SF = tf sf is number of sentences containing the term ( t ij ) and n is total number of sentences in the document. LS = Summary length and HTFS is highest term weights (TF X  X SF) summation of a sentence in the document. 3.4. Key word feature
The top 10 words whose high TF X  X SF scores are chosen as key words. 3.5. Similarity to first sentence
Algorithm 1. Maximal Marginal Importance (MMI) diversity-based method 1. Input document D : take the document D as input, D ={ T, s 2. Preprocessing : segment document D into separated sentences D ={ T, s stem the words. 3. Features Extraction : extract the features, for each s nfriends, ngrams, sim }. 4. Sentence clustering and binary tree building : 5. Sentence order in the binary tree : calculate the sentence score in the binary tree using Eq. (9) where 6. Summary generation : where: 6.2 Order the summary sentences in the same order of the original document.
 For step 1, For example, the input document D is as shown in Fig. 1a .
 centrality, SS_NG : average of THS and THSRS features, sim_fsd : the similarity of the sentence s friends (the group of sentences which are similar to both sentences s friends using Eq. (9) . Where Score BT (s i ) is the score of the sentence s tance of the sentence s i calculated using normal features (Eq. (10) ) and friendsNo(s &amp; Radev, 2004 ) and kwrd(s i ) is the key word feature.
 top level contains one sentence which is a sentence with highest score.
 the generated summary of the input document shown in Fig. 1a . In Eq. (11) , rel(s petitive sentences calculated using Eq. (12) , s i is the unselected sentence in the current binary tree, s level which is 0.01 times the level number. In Eq. (12) , n-friends(s similar to both sentences s i and s j ), n-grams(s i, s j ( Erkan &amp; Radev, 2004 ).
 4. Swarm-based summarization
The swarm-based text summarization method ( Binwahlan et al., 2009c ) generates a summary of the original document know whether each feature got a suitable weight or not. For example, D is an input document, D ={ T, s
For each s i , there is a set of features F , F ={ f 1 ,f w w f w collection. We have formed the data set by selecting 100 documents from the first document sets in DUC 2002 which are (D061j, D062j, D063j, D064j, D065j, D066j, D067f, D068f, D069f, D070f, D071f, D072f and D073b) to be used as training and testing data. The swarm method is defined as combination of adjusted features scores as in (13) where swarm_impr(s i ) is The score of the sentence s i , w number of features and score_f j (s i ) is the score of the feature j given to sentence s The steps of summarization process using this method can be concluded in Algorithm 2 .
Algorithm 2. Swarm-based summarization method 1. Input document D : take the document D as input, D ={ T, s 2. Preprocessing : same as step 2 in Algorithm 1 . 3. Features extraction : same as step 3 in Algorithm 1 . 4. Feature scores modification: 4.1 Use the optimized weights W ={ w 1 ,w 2 ,w 3 ,w 4 ,w 5 4.2 Get the set of modified feature scores WF ={ w 1 WSS, w 5. Sentence score calculation : use Eq. (13) to calculate the score of each sentence s 6. Summary generation : For step 6, top n sentences equal summary length. (three is the summary length of document D in the same example, see Section 3 ).
 ficial Neural Network (ANN) or to any other gradient methods, ANN is a local searching method depending on gradient can, 2008; Kiran et al., 2006; Xu, He, Zhu, Liu, &amp; Li Y., 2008 ). 5. Swarm diversity-based text summarization methods presented in the previous sections (MMI Diversity-based Text Summarization and Swarm-based Text Summariza-
We replaced the sentence importance in the second position by the sentence importance ( swarm_impr(s lated using Eq. (13) . The new formula of scoring of the sentence in the binary tree is as shown: where: impr(s i ) is the importance of the sentence s i calculated using normal features (Eq. (10) ), swarm_impr(s tance of the sentence s i calculated using Eq. (13) and friendsNo(s The steps of summarization process using this method can be concluded in Algorithm 3 .
Algorithm 3. Swarm diversity-based summarization method 1. Input document D : take the document D as input, D ={ T, s 2. Preprocessing : same as step 2 in Algorithm 1 . 3. Features extraction : same as step 3 in Algorithm 1 . 4. Sentence clustering and binary tree building : same as step 4 in Algorithm 1 . 5. Sentence order in the binary tree : where: and 5.2. Order the sentences in the binary tree based on their scores. 6. Summary generation : same as step 6 in Algorithm 1 with a different generated summary. level contains one sentence which is a sentence with highest score.
The reason of making the features the central point for integrating the two methods is because the features are the cornerstone in the generating text summary. The summary quality is sensitive to how the sentences are scored based sentences. 6. Fuzzy swarm-based text summarization
Summaries from a number of humans are used for such purpose. The agreement among those humans on selection of spe-the end of the training process, the optimized weights of the text features are obtained W ={ w the text features scores which were adjusted by those imprecise values of weights WF ={ w quality summaries. For such problem, we model human knowledge in the form of IF-THEN rules. We believe that the inte-rization process using this method can be concluded in Algorithm 4 .

Algorithm 4. Fuzzy swarm-based text summarization method 1. Input document D : take the document D as input, D ={ T, s 2. Preprocessing : same as step 2 in Algorithm 1 . 3. Features Extraction : same as step 3 in Algorithm 1 . 4. Feature scores modification : 4.1 Use the optimized weights W ={ w 1 ,w 2 ,w 3 ,w 4 ,w 5 4.2 Get the set of modified feature scores WF ={ w 1 WSS, w 5. Sentence score calculation : calculate the score of each sentence s 5.2. Inference: 5.3. Defuzzification: 6. Summary generation: fuzzy sets, we use the trapezoidal membership function due to its simplicity and wide use. Three fuzzy sets are used: of the trapezium which determine the shape of the function. Moreover, the membership function is described by the two indices i and j . For example, the membership function A ij
B the membership functions of fuzzification of the input value of the sentence centrality feature (SC).
For step 5.2, for inference process, around 200 IF-THEN rules were defined by human experts. Three human experts were following is an example for those rules: If (WSS is H) and (SC is H) and (S_FD is M) and (SS_NG is H) and (KWRD is H) then (output is important) in class c at value z j . The class c represents an output fuzzy set and z sentence scores and their membership degrees, the final score of the sentence Z will be obtained. 7. Fuzzy swarm diversity hybrid model for automatic text summarization
Our hybrid model consists of three components: diversity-based method (presented in Section 3 ), fuzzy swarm-based method (introduced in Section 6 ) and third component which has two different forms: the first form as integration of swarm-based method with diversity-based method (swarm diversity was discussed in Section 5 ) and second form is only hybrid model; Fig. 6a shows the proposed model in which diversity, swarm diversity and fuzzy swarm-based methods were two methods. The fuzzy swarm-based method has higher sentence score than the other two methods because we need to tence selected by diversity-based method and non diversity method is more important than the sentence selected by two posed model Fig. 6b , the fuzzy swarm-based method replaces the swarm diversity-based method and the swarm-based on the model behavior. That means the diversity-based method works same as fuzzy swarm-based method. Based on this, the sentence which was selected by fuzzy swarm-based method alone or by fuzzy swarm-based method plus swarm-based method will not be included in the final summary. The new scores of the sentence are 1, 1.5 and 2 for diversity-based posed model in the first form.

Algorithm 5. Fuzzy swarm diversity hybrid model for automatic text summarization (first form). 1. Input document D : For example, the input document D is as that in Fig. 1a . 2. Preprocessing : same as step 2 in Algorithm 1 . 3. Features Extraction : same as step 3 in Algorithm 1 . 4. Summary generation using MMI diversity-based method : 5. Summary generation using swarm diversity-based method : 6. Summary generation using fuzzy swarm-based method : 7. Final summary generation :
For step 7.3, the sentences of summary 3 are used only for helping the sentences of other two summaries to get higher scores. 8. Generalizing the proposed model results via confidence limits measure the performance of the proposed model, we need to check each evaluation value separately. Doing so is a tough ping (resampling) method. 9. Experimental design The proposed model will be evaluated using 100 documents selected from the first document sets (D061j, D062j, D063j, D064j, D065j, D066j, D067f, D068f, D069f, D070f, D071f, D072f and D073b) of the DUC 2002. The Document Understanding human written summaries for each document. The whole set of human written summaries were written by 10 human ex-perts. The test set is comprised of 30 document sets.
 tion methods, where ROUGE compares a system generated summary against a human generated summary to measure the factor a = 1.2, ROUGE-S and ROUGE-SU (maximum skip distance dskip = 1, 4, and 9).
ROUGE-N calculates the shared n -grams between system generated summary and one or a set of human generated sum-maries producing recall score: where n is the length of the n -gram (gram n ) and count match generated summary and a set of reference summaries produced by human.

ROUGE-L calculates the longest common subsequence (LCS). Suppose we have two sentences X and Y , and LCS is a com-mon subsequence with maximum length. In our experiment, we use ROUGE-N ( N = 1 and 2) and ROUGE-L . The reason for selecting these measures is because they work well for single document summarization ( Lin, 2004 ). In DUC 2002 document sets, each document set contains two model or human generated summaries for each document.
We gave the names H1 and H2 for those two model summaries. The human summary H2 is used as benchmark to measure the quality of our proposed model summary, while the human summary H1 is used as reference summary. Beside the human with human benchmark (H2 X  X 1) (H2 against H1); we also use another benchmark which is MS word summarizer (Msword).
In addition to the two benchmarks (MS word summarizer and human summarizer) we also compare the performance of our have participated in DUC 2002 ( Nenkova, 2005 ). 10. Experimental results
Tables 1 X 3 show comparison between the proposed model and the other six methods (swarm model (M1), fuzzy swarm show how much the performance of the two forms of the proposed model, swarm model, fuzzy swarm, Msword, sys19 and M4.
 model (M4: composed of fuzzy swarm-based method, swarm based model and diversity-based method) got better perfor-mance than the first form of the proposed model (M3: composed of fuzzy swarm-based method, swarm diversity-based other methods (except H1 X  X 2) including the proposed model. In general, the proposed model provides good enhancement ior of the model keeping the final summary empty from redundant information.
 11. Discussion the generalization of the results, the second form of the proposed model (M4: composed of fuzzy swarm-based method, swarm based model and diversity-based method) got better performance than the first form of the proposed model (M3: the swarm model (M1), fuzzy swarm (M2), Msword, sys19 and sys30 summarizers. The experimental results supported the tences in the original document. A few of the 20% sentences are added to the final summary due to the limited summary ber of sentences in the human summary to be greater than the number of sentences in the system summary. Based on this cause in the recall evaluation, the total shared n -grams between the human summary and the system summary is divided by the human summary length but in the precision evaluation the total shared n -grams between the human summary and of redundant information. 12. Conclusion and future work weights was to emphasize on dealing with the text features fairly based on their importance. The weights suggested by where n is equal to the compression rate.

We presented the proposed model in two forms based on the structure. The difference between the two forms is: in the work, we are planning to improve our model performance by adding an annotator module in order to provide semantic
The aim of such action is to avoid the inclusion of unimportant semantic concepts in the summary, which will consume the proposed model for multi document summarization problem to find out if they can give better. Acknowledgment SF0502, Malaysia.
 References
