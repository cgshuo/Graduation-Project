 A geometrically motivated classi er is presented and ap-plied, with b oth training and testing stages, to 3 real datasets. Our results compared to those from 23 other classi ers have the least error. The algorithm is based on parallel co ordi-nates and : ~ has worst-case computational complexity O ( N 2 j P j 2 ) ~ pro vides comprehensible and explicit rules, ~~ do es dimensional ity selection { where the minimal set ~ orders these variables so as to optimize the clarit yof I.2.4 [ Arti cial Intelligence ]: Knowledge Representation Formalisms and Metho ds(F.4.1)I.2.6Learning (K.3.2); I.3.m [ Computer Graphics ]: Miscellaneous| Visualization Classi cation, Multidimensi ona l Visualizatio n T hough it is fun to do Visual Data Mining (as describ ed in [2]) using Parallel Co ordinates the level of skill and patience required tends to limit the n um b er of e ective users. It
Senior Fellow San Diego Sup erComputing Center &amp; Mul-tidimension al Graphs Ltd, Raanana, Israel is not surprising then that the most p ersistent requests and admonitions to us have b een for additions to our visual to ols which, at least partially, automate the knowledge discovery pro cess.
 Classi cation is a basic task in data analysis and pattern recognition and an algorithm accomplishing it is called a Classi er [6], [9], [5]. The input is a dataset P and a des-ignated subset S . The output is a characterization, that is a set of conditions or rules, to distinguish elemen ts of S from all other mem bers of P the \global" dataset. The out-put ma y also b e that there is insu X cient information in the dataset to provide the desired distinction.
 With parallel co ordinates (see [3] for a recen t review) a dataset P with N variables is transformed into a set of p oints in N-dimensional space. In this setting, the designated sub-set S can b e describ ed b ymeansofah yp ersurface whic hen-closes just the p oints of S . In practical situations the strict enclosure requirement is dropp ed and some p oints of S may b e omitted (\false negatives"), and some p oints of P S are allowed (\false p ositiv es") in the h yp ersurface. The de-scription of suc hah yp ersurface is equivalen t to the rule for iden tifying, within some acceptable error, the elemen ts of S . It turns out that using Parallel Co ordinates (abbr. k -co ords) not only helps the visualiza tion of the pro cess, but also the metho dology's representational results enable the e X cient description and construction of the hyp ersurfaces. Also the rules obtained can b e visualized . T he algorithm accomplishin g this entails: } use of an e X cient \wrapping" algorithm to enclose the } the p oints in ( P S ) \ S 1 are isolated and the wrapping 1 T oa void unnecessary verbiage by a statement S j S k we also mean that the set of p oints enclosed in the hyp er-surface S j is contained in the set of p oints enclosed by the hyp ersurface
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 } } the p oin ts in S not included in S 1 S 2 are next mark ed } the pro cess is rep eated alternativ ely pro ducing upp er Basically , the \wrapping" algorithm is a fast w a y of pro-ducing a h yp ersurface enclosing tigh tly a giv en p oin tset. The kind of surface pro duced is a con v ex-h ull appro xima-tion. The e X ciency of the v ersion implemen ted here is due to the use of the k -co ords represen tations of N-dimensional ob jects applied in the description of the resulting h yp er-surface. T o summarize, initiall y the wrapping S 1 encloses all the p oin ts of S = S 0 . Then in the attempt to remo v e all extraneous p oin ts a c avity is created b y the subsequen t wrapping. Suc hca vities are generically denoted b y S 2 n n =1 ; 2 ;::: . Usually some of the p oin ts of S are enclosed in 2 n , so a correction follo ws with a S 2 n +1 ,the h yp ersurfaces with o dd subscript, whic h enclose and add these p oin ts to the previous appro ximation for the enclosure of S .Suc ha correction ma y also add some p oin ts of P S whic hneed to b e subsequen tly remo v ed, or b etter reduced, to pro vide an increasingly tigh ter b ound. So the pro cess en tails b ound-ing the designated set S alternately from ab o v e and b elo w pro viding, in case of con v ergence, an increasingly b etter ap-pro ximation for S . It can and do es happ en that the pro cess do es not con v erge when P do es not con tain su X cien tin-formation to c haracterize S .Itma y also happ en that S is so \p orous" (i.e. sp onge-lik e) that an inordinate n um ber of iterations are required.
 A tstep r the output is the description of the set S r whic h consists of: ? a list of the minim um n um ber of v ariables needed to ? ? the curren t appro ximation of the rule stated in terms of So on con v ergence, sa yatstep 2 n , the description of S is pro vided as : this b eing the terminating expression resulting from the al-gorithm.
 The implemen tation allo ws the user to select a subset of the a v ailable v ariables and restrict the rule generation to these v ariables. In certain applications, as in pro cess con trol, not all v ariables can b e con trolled and hence it w ould b e useful to ha v e a rule in v olvin g suc hv ariables that are \accessible" in a meaningful w a y .Bythew a y , there is nothing to prev en t using PCA after the dimensionali t y selection.
 W e fondly call this algorithm Nested Ca vities (abbr. NC ). Rather than wrap from ab ove ,the un w an ted p oin ts can b e remo v ed from the interior in a metho dical w a y creating ca v-ities. This is not the righ t forum to analyze the computa-tional complexit y and other in tricacies of the algorithms. It is w orth p oin ting out that ac hieving an \optim um", in the sense of minimizing the n um ber of ca vities, turns out to b e an NP-complete problem. Still the next b est thing is done here in terms of disco v ering and remo ving the ca vities in or-der of decreasing size. W e called this v ariation initially the Swiss Cheese but, when it lo ok ed that it ma y foster misun-derstandings b et w een c heese pro ducing nations, w ec hanged the name to Enclosed Ca vities and abbreviate it b y EC . There are also t w o other options a v ailable : In the rst case, when the rst h yp ersurface S 1 is found, the v ariables o ccurring in its description are the minim um n um ber of v ariables needed to describ e S .F rom this p oin t on either v ariation of the algorithm can b e restricted to use only these. If con v ergence is ac hiev ed a rule in v olving this minimal set of v ariables is obtained. By con trast, when the algorithm is allo w ed to op erate on all the initiall y selected v ariables at eac hstep , the n um b er of op erations in the ter-minating expression is reduced. In practice, the reduction in the n um b er of steps turns out to b e substan tial. F or com-pletely esoteric and eccen tric reasons EC w as programmed to op erate on the minimal n um ber of v ariables. In the 3 cases presen ted next the dimensional i t yw as lo w ered signi -can tly not only b y EC but also b y NC to less than half and in one case to ab out 1/4 of the original v ariables. One of the di X cult problems in using parallel co ordinates for viewing a sp eci c dataset is to someho w nd an axes perm utation whic h is \go o d" (i.e. pro vides ric h visual cues on what ma ybe trueornot)aboutthe sp eci c dataset.
 There is an inheren t ordering emerging from dimensional -it y selection whic h, as w e see b elo w, answ ers this need w ell. This ordering is completely dataset sp eci c. F urther, since the algorithm is display indep enden t there is no inheren tlim-itation as to the size and n um ber of v ariables in the dataset. The most signi can t limitation then in visual data mining is nally o v ercome. The visual asp ects can no w b e used for displa yin g the result as w ell as exploring the salien t features of the distribution of data brough toutb y the classi er. In summary: T hree datasets, b enc hmarks in classi cation, are used to test the classi er. The results are then compared to those obtained with other w ell-kno wn classi ers.
 During 1990-1993, Mic hie, Spiegelhal ter and T a ylor [4], on b ehalf of the ESPRIT program of the Europ ean Union, made extensiv e studies of sev eral classi ers applied to div erse datasets. Ab out 23 di eren t classi ers w ere applied to ab out 20 datasets for comparativ e trials in the StatLog pro ject. This w as de-signed to test classi cation pro cedures on large-scale com-mercially imp ortan t problems in order to determine suitabil-it y of the v arious tec hniques to industry .W ec hose t w oof these datasets. 3.1 Satellite Image data This dataset used in Statlo g is from a region in Australia. It consists of m ulti-sp ectral v alues and asso ciated classi cation according to ground t yp e and can b e found in the Statlo g ftp site. Eac h frame consists of four digital images of the same scene in di eren t sp ectral bands, t w o in the visible and t w o in the near infra-red region. There are 36 v ariables (the attributes) and the class attribute for six (6) soil t yp es i.e. the six classes to b e c haracterized b y the classi er(s). T able 1: Summary of the StatL o g results and com-parison with the Nested Ca vities (NC) classi er for the satellite image data. The error is a v eraged o v er the six classes.
 The data has 4435 samples(data items) for the training set and 2000 samples in the test set. By w a y of explanation, for v alidation the dataset is partitioned in to tr aining and testing subsets, the \p opular" prop ortions b eing ab out 2/3 to 1/3. The rule is deriv ed, b y the classi er, on the training set and tested on the remainder of the data; the error p ertains to the false \p ositiv es" and \negativ es". The comparativ e results are sho wn in T able 1. An imp ortan t observ ation, is that in a great man y cases, S 2 turned out to b e the h yp ersurface requiring the largest n um ber of v ariables for its de nition. W e conjecture that this is an indication of the existence of man y \b orderline" cases (i.e. close elemen ts in the class S and it's complemen t), or it ma y suggest that the class de nition ma y b e \fuzzy". 3.2 V o w el Recognition data The data collection pro cess in v olv es digital sampling of sp eec h with acoustic signal pro cessing, follo w ed b y recognition of the phonemes, groups of phonemes and w ords. The goal here is a sp eak er-indep endent rule based on 10 v ariables of 11 v o w els o ccurring in v arious w ords sp ok en (recorded and pro cessed) b y 15 British male and female sp eak ers. Deter-ding [1] collected this dataset of v o w els and whic h can b e found in the CMU b enc hmark rep ository on the WWW. There are 528 en tries for training and 462 for testing. Three other t yp es of classi ers w ere also applied to this dataset: neural net w orks, k-NN b y Robinson &amp; F allside [7], and Deci-selection. sion trees b y Shang and Breiman [8]. F or the sak eofv ariet y b oth v ersions of our classi er w ere used and a somewhat dif-feren t error test pro cedure w as used. The results are sho wn in T able 2. 3.3 Monk ey Neural data W eha v e decided to include the result on this dataset due to its in teresting and un usual features. Here there are t w o classes to b e distinguish ed consisting of pulses measured on t w ot yp es of neurons in a monk ey's brain (p o or thing!). The exp erimen tw as conducted at the Y ale Medical Sc ho ol and w e receiv ed the data from Prof. R. Coi man's 2 group w ork-ing on this classi cation problem. There are 600 samples with 32 v ariables. Remark ably , con v ergence w as obtained and required only 9 of the 32 parameters. The resulting ordering sho ws a striking separation. In the attac hed g-ure the rst pair of v ariables x 1 ;x 2 originally giv en is plot-ted sho wing no separation. In the adjoining plot the b est pair x 11 ;x 14 ,as c hosen b y the classi er's ordering, sho ws re-mark able separation. The disco v ery of this man ually w ould require constructing and insp ecting a scatterplot with 496 pairs ...! The result sho ws that the data consists of t w o \banana-lik e" 3 clusters in 8-D one (the complemen tinthis case) enclosing the other (class for whic h the rule w as found). Note that the classi er can actually describ e highly complex regions. It can build and \carv e" the ca vit ysho wn. It is no w onder that separation attempts in terms of h yp erplanes or nearest-neigh b or tec hniques can fail badly on suc h datasets. The rule ga v e an error of 3.92 % using train-and-test with 66 % of the data for training) and impressed the Y ale group { not an easy feat! The geometric form ulation com bined with the results on the represen tation of m ultidimensio nal ob jects in k -co ords ga v e a classi er with remark ably lo w computational complexit y . This mak es feasible the classi cation of truly large in size and n um ber of v ariable datasets something w e hop e to test with suitable partners in the near future. The lo w com-plexit y , enables the deriv ation of the rule in near real-time, and then apply it to incoming data, rendering the classi er adaptiv e to c hanging conditions. In fact, w e are to ying with the idea of using it as a MET A-classi er , that is an algorithm for nding the rule(s) of c hange b y applying it on the data of the c hanges found. The rules pro vided are explicit, \visuali zabl e" and yield dimensional it yselec-tion whic hc hoses and orders the minimal set of v ariables needed to state the rule without loss of information .T o k eep us out of misc hief, at least for a while, w ein tend to con template some of the questions and ideas whic h arose on termination criteria, approac hes to o v er ting, in terpretation of the \geometry" of the dataset and others. [1] D. H. Deterding. Sp e aker Normalization for A utomatic 2 He is the recipien t of the National Medal of Science in Mathematics for the y ear 2000. 3 P erhaps the monk ey w as dreaming ab out bananas during this fateful exp erimen t ... [2] A. Inselb erg. Visual data mining with parallel [3] A. Inselb erg. Don't panic ... do it in parallel! Comp. [4] D. Mic hie, D. Spiegelhal ter and C. T a ylor. Machine [5] T. M. Mitc hell. Machine L e arning . McGra w Hill, New [6] J. R. Quinlan. C4.5 : Pr o gr ams for Machine L e arning . [7] A. J. Robinson and F. F allside. A dynamic [8] N. Shang and L. Breiman. Distribution based trees are [9] P .Sm yth, G. Piatesky-Shapiro, U. F a y ad and
