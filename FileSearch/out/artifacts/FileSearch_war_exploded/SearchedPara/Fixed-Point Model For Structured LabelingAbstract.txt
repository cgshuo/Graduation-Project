 Quannan Li 1 quannan.li@gmail.com Jingdong Wang 2 jingdw@microsoft.com David Wipf 2 davidwipf@gmail.com Zhuowen Tu 1 , 2 zhuowen.tu@gmail.com
Lab of Neuro Imaging and Department of Computer Science, UCL A Microsoft Research Asia Here we study the problem of predicting a labeling for a structured input, which is denoted as a graph, G = ( V , E ). Each node v i  X  V corresponds to a data entry with its features denoted as x i ; the ob-jective of the structured labeling task is to jointly as-sign labels y = ( y i : i = 1 .. |V| ) ( y i  X  L , L is the label space) to all nodes V = ( v i : i = 1 .. |V| ) as a joint output. This problem is fundamental since structured inputs and outputs are common in a wide range of applications. For example, in computer vi-sion/image processing, a structured input is an image of all pixels, and the structured outputs are the cor-responding labels of these pixels. There are correla-tions among the structured outputs, denoted by the edges between the nodes, and the correlation may oc-cur between neighboring nodes, or the nodes relatively distant apart. These correlations make the structured prediction problem a difficult task.
 A simple scheme is to treat the structured outputs as independent entries and apply the standard classifica-tion/regression algorithms. Such a scheme is straight-forward, but it loses the important interdependency information, which is crucial in modeling and under-standing the structured data. The other extreme of the solution is to treat each instance of y = ( y 1 ,..,y |V| a single label and transform the problem into a multi-class classification problem. This implementation is infeasible because the space of the output labels is ex-ponentially large at the size of |L| |V| .
 Markov random fields (MRF) ( Geman &amp; Geman , 1984 ) and conditional random fields (CRF) ( Lafferty et al. , 2001 ) have been widely used to model the correlations of the structured labels. However, due to the heavy computational burdens in their training and testing (inference) stages, MRF and CRF are often limited to capturing a few neighborhood interactions, and thus, limiting their modeling capabilities. Structural SVM methods ( Tsochantaridis et al. , 2005 ) and maximum margin Markov networks (M 3 N) ( Taskar et al. , 2003 ) model the correlation in a similar way as the CRF, but they try to specifically maximize the prediction margin. These approaches are also limited in the range of contexts due to the high computational demand. When long range contexts are used, approximations should be used to trade-off the accuracy and the running time ( Finley &amp; Joachims , 2008 ). Recently, layered models ( Tu &amp; Bai , 2010 ; Heitz et al. , 2008 ; Daum  X e et al. , 2009 ), in the spirit of stacking ( Wolpert , 1992 ), are proposed to take the outputs of classifiers of the current layer as added features to classifiers of the next layer. Since these approaches perform direct label prediction as functions instead of performing inferences as in MRF or CRF, the layered models ( Tu &amp; Bai , 2010 ; Heitz et al. , 2008 ) are able to model complex and long range contexts.
 In this paper, we look into the structured labeling problem from a different angle and develop a simple yet effective approach, a fixed-point model. We introduce a contextual prediction function f : ( x , L |V| X  1 )  X  L with the output being the labeling of an individual node and the input being both its features and the la-beling of the rest of the nodes (or its neighbors). The overall fixed-point function f : ( x 1 , , x |V| , L |V| )  X  L |V| is a vector form of the contextual prediction func-tion of the nodes, and is trained with the property of a contraction mapping so that an iterative solution is applicable in the prediction process. We also ana-lyze conditions for ensuring that our training strategy leads to a contraction mapping, provably so in certain cases. Not only does the learned fixed-point function preserve the modeling capability of the layered mod-els ( Tu &amp; Bai , 2010 ; Heitz et al. , 2008 ), but also it is simper and much easier to scale since it only consists of a single layer function. The conditional random fields (CRF) model ( Lafferty et al. , 2001 ) is a state-of-the-art work for solving the structured prediction problem. In the max-margin Markov networks ( Taskar et al. , 2003 ; Tsochantaridis et al. , 2005 ), the authors propose to maximize the margin for structured output, in a spirit similar to the multi-class SVM method ( Weston &amp; Watkins , 1998 ). Due to the computational demand in both the training and the testing stages, usually only a small number of interactions among the neighboring outputs are included in both the CRF and the M 3 N. The hidden Markov models ( Rabiner , 1989 ) share a similar property in modeling the graph connections.
 The layered contextual models ( Tu &amp; Bai , 2010 ; Heitz et al. , 2008 ) train a sequences of classifiers us-ing the output of the previous layers as additional fea-tures to the next layer; on tasks where the long range contexts play a significant role, e.g., the OCR task, they greatly outperform the CRF and M 3 N (as shown in the experiments). The proposed fixed-point model has a similar modeling capability to model long range contexts as the layered models. During the training process, while the layered models train a series of clas-sifiers, the fixed-point model trains a single classifica-tion/regression function which assumes a stable status for the ground-truth labeling. Layered models have to compute the classification scores for each training sam-ple as the input to the next layer, thus limiting their capability to scale up. On the contrary, the fixed-point model is much faster to train than the layered models, and thus is much more scalable. In addition, the con-vergence behavior of the layered models has not been clearly stated so far, whereas the proposed fixed-point model provides a contraction mapping interpretation to the convergence.
 The pseudo-likelihood algorithm in ( Besag , 1975 ) models the conditional probability of an entry based on its neighborhoods; in ( Sontag et al. , 2010 ), a pseudo-max framework is introduced to approximate the expo-nential number of constraints by a polynomial number of constraints; in structured output-associative regres-sion (SOAR) ( Bo &amp; Sminchisescu , 2009 ), the output components, other than the one being considered, are used as auxiliary features to train a vector of regres-sion functions for the task of image reconstructions and human pose estimation. Compared with SOAR, the main purpose of the proposed fixed-point model is to train a fixed-point function that assumes the stable status of the structured labels; in SOAR, a generalized linear regression function is trained for reconstruction, whereas we study the structured labeling problem by exploring rich contextual correlations; the lack of anal-ysis in the learned function also leaves the convergence in SOAR untouched; on the contrary, our algorithm is not limited to generalized linear regressions and, ex-isting methods such as logistic regression and random forest ( Breiman , 2001 ) can be used too.
 There are also other related algorithms. In ( Collins , 2002 ), an averaged perceptron algorithm is proposed: the training samples are processed iteratively; once a mistake occurs, the weight vector is updated according to the prediction error and the final weight vector is a weighted version of all the vectors that have appeared. In ( Nguyen &amp; Guo , 2007 ), an ensemble method is pro-posed to transform the predictions of different models into a chain with state transition matrices and then dy-namic programming is used to get the voted prediction result. The underlying mechanism of the fixed-point model is different from that of these algorithms, and we will compare the performance in Section 4 . It is worth mentioning that the proposed fixed-point model is not a method merely designed to balance the performance and learning-time; it provides a new way of thinking about the structured learning problem by investigating a shallow model (instead of cascaded ap-proaches with deep layers) and also having the capa-bility to incorporate rich structural/contextual infor-mation with effective and efficient inference. 3.1. Model Description In this paper, we are interested in the structured la-beling task for a graph G = ( V , E ). The edges E decides the graph topology, and hence the neighbor-hoods of each node. For instance, in sequence la-beling, where the nodes { v 1 ,v 2 , ,v n } in V form a chain, we can specify the neighborhood N i of v i to be the m nodes preceding and after it, i.e., N We use m to denote the number of neighbors a node can have in the neighborhood specification.
 We assume that our problem is a binary-classification problem where L = { X  1 , +1 } . To this end, we train a contextual prediction function which outputs the la-beling of the node. Note that, as a lexical category label, y i cannot be used in the equation/function di-rectly. Instead, we can represent y with a labeling con-fidence q . For the binary class case, if y i = 1, q i = 1 and if y i =  X  1, q i = 0. At the prediction process, the label y i is unknown, and thus q can be relaxed to a real value ranging in [0 , 1]. We use q N labeling of the neighborhood of v i and use q to de-note the labeling of all the nodes in G . This can easily be extended to multiclass problems by encoding the labeling with a matrix.
 For each node v i , the contextual prediction function f takes in both v i  X  X  feature x i and the labeling q N its neighborhood. The contextual prediction function f can be formulated as where f is a regression function within range [0 , 1], and  X  is the parameter of the function. From Equation 1 , the labeling q of all the nodes can be written in a vector form, where q = [ q 1 ,q 2 , ,q n ] T , f ( ) = [ f ( x 1 , q N As from Equation 2 , the labeling q appears as both the output as well as part of the input. Given the label-ing q and the features { x 1 , x 2 , , x n } of the training data, we learn the parameter  X  .
 To get the labeling of a structured input G , one can solve for the non-linear equation set q = f ( x 1 , x 2 , , x n , q ;  X  ), which is generally a difficult task. In this paper, we focus on a type of func-tions f that assumes the property of a contraction mapping, i.e., having a stable status (an attractive fixed-point) for each structured input. When using the ground-truth labeling in the training process, the ground-truth labeling is assumed to be the stable sta-tus and the existence of the stable status leads to the fixed-point iteration in the prediction process: q t = f ( x 1 , x 2 , , x n , q t  X  1 ;  X  ) and q t  X  q as t  X   X  . We name the functions f with such a property the fixed-point functions (models) . In the following subsections, we provide a sufficient condition for the fixed-point model, propose the learning strategy, and describe the training and testing processes. 3.2. Contraction Condition for the In this section, we give a sufficient condition for f ( x 1 , x 2 , , x n , q ;  X  ) to be the fixed-point model and illustrate it using a logistic regression model first. Our derivation is based on the Banach Fixed-Point theorem ( Banach , 1922 ): for a complete metric space ( X, dist) and a mapping F : X  X  X , if there exists a non-negative real number  X  &lt; 1 such that, then F is a contraction mapping and it has a unique fixed-point.
 Since for a node v i , its feature x i is given and fixed, we thus for notational simplicity we can simply write q = f i ( q N Assuming f i is a continuously differentiable real-valued function, then according to the mean value the-orem for a scalar function of several variables,  X  q , q f ( q N where  X  f i (  X  q N c ) q N where J f is a matrix of coordinate-wise derivatives, and its i -th row corresponds to  X  f i (  X  q N ( i,k )-th element of J f , if the node v k is a neighbor of node v i , J f not in the neighborhood of v i , J f where k J f k denotes the matrix norm on J f induced from some vector norm kk . For example, k J f k 1 = We then have the following: Lemma 1 If k J f k &lt; 1  X  q , q , then f is a contraction mapping. 3.2.1. Contraction for logistic regression Now we assume a linear logistic regression model.  X  can be decomposed into  X  (  X   X  X  d  X  1 , d is the dimen-sion of x i ) and  X  (  X   X  R m  X  1 ), corresponding to the appearance feature x i and the contextual feature q N respectively. q i = f i ( q N
In the following, we use I ( k,j ) to denote the in-dex of the node that has v k as its j -th neigh-bor and define an auxiliary function h i (  X  ,  X  ) = we give a sufficient condition for k J f k 1 &lt; 1 for logistic regression.
 Lemma 2 For the logistic regression model, if the fixed-point function f is a contraction mapping. Proof If v k is the j -th neighbor of v i , then the par-tial derivative  X  X  i  X  X  is in the range [0 , 1], for the term h  X  , q N ative entries of  X  , and its maximum is ( P m j =1  X  j + P j =1 |  X  j | ) / 2, the sum of the positive entries of  X  . Ignoring the boundary effect of the structured input, the k -th absolute column sum of J f sums over the m nodes that have v k as their neighbor giving
X traction mapping.
 The value of x plays a role in the constraint require-ment in Equation 8 . So long as the value of x satisfies the constraint requirement in Equation 8 , the fixed-points can be guaranteed. Another possible sufficient condition is k  X  k 1 &lt; 1 as J f This condition is much simpler but more difficult to satisfy because it ignores dependency on x entirely. 3.2.2. Contraction in general cases The condition described above can be used as con-straints for the fixed-point function in the training process when f is restricted to a logistic regression function. We now briefly discuss a scheme for learn-ing a contraction function in a more general setting, which can be used to implicitly enforce the contraction condition for other functions.
 It is well-known that adding some amount of input noise (also called input jitter) during the training pro-cess can improve the robustness of neural network clas-sifiers ( Reed et al. , 1992 ). Moreover, this is gener-ally true with recursive models such as the algorithm being proposed herein, in part by favoring the con-traction condition. For example, when training the function q = f ( x 1 , x 2 , , x n , q ;  X  ), we may produce some replica Q = { q +  X  r ,r = 1 ..R } by introducing small random perturbations  X  r . This small amount of randomness is added to the input q of the function f while keeping the targeted output the same as the ground-truth q , leading to an augmented training set. Given modest assumptions on the space of classifiers and training algorithms, it is possible to show that when a sufficient number of such replica are included with suitable distribution, then a contraction mapping will be obtained as part of the learning process with high probability. Intuitively, this occurs because these replica will effectively reduce the relative importance of q as an input feature to f ( x 1 , x 2 , , x n , q ;  X  ). In the case of logistic regression, this is tantamount to reducing the magnitude of the coefficients  X  ; however, with other classifiers the effect may be less transparent. While it is difficult to know the optimal distribution of such replica a priori, we have found empirically that a contraction mapping is consistently obtained without sensitivity to this distribution. For evaluation pur-poses, the gradient of any classifier can be computed in principle, numerically or analytically, to examine if the contraction condition is satisfied in a particular re-gion. Additionally, if a classifier is ever observed to vi-olate the contraction condition, we can always retrain after increasing the number and/or magnitude of the replica. Given some assumptions about the classifier and the replica, we next briefly discuss efficient meth-ods for checking (at least locally around the training data) whether or not a contracting function has been obtained.
 From Lemma 1 , we want to guarantee that k J f k &lt; 1, for some norm kk . If we choose the induced  X   X  norm, this corresponds to the requirement that all rows of J f have  X  1 norm less than one. This can be guaranteed if the gradient of each f i with respect to q is less than one for all q . Let  X  i ( q ) denote this gradient. Using a Taylor series expansion we can approximate each func-tion f i as f where for simplicity we use f i ( q ) to denote the i -th element of f ( x 1 , x 2 , , x n , q ;  X  ). It is not uncommon to assume a relatively smooth function f i with small higher-order derivatives O (  X  n f i ) ,n  X  2. Moreover, we may also assume in some situations that the  X  r are small, in which case higher-order terms can be largely Given the first-order Taylor-series approximation, we then have
The above constraints represent a linear system that can be viewed as random samples of the unknown  X  ( q ). These samples, which can be efficiently col-lected and monitored during the training process, can then be used to help determine whether or not the con-traction condition is satisfied (at least in the locality of the training data). Depending on the neighborhood structure of the graph, we know that  X  i ( q ) will typi-cally be sparse, with nonzero-valued locations inferred from the edges. In cases where this degree of sparsity is sufficiently high relative to the number of replica, we can simply solve for  X  i ( q ) directly via the above linear system. However, when this is not possible, we may still potentially estimate whether k  X  i ( q ) k 1 &lt; 1. For example, assume for simplicity that the replica  X  r are iid Gaussian distributed with zero mean and known covariance  X  2 I (other distributions can be accommo-dated as well). It then follows that each e r represents an iid sample from a zero-mean Gaussian with variance  X  2 k  X  i ( q ) k 2 2 . Given R such samples, it is a simple mat-ter to design any number of standard statistical tests stant C . So we need only determine some C sufficiently small such that we ensure the contraction condition holds, namely k  X  i ( q ) k 1 &lt; 1 with high probability. Now assume that the number of significant elements in  X  ( q ) is less than or equal to some value  X  (in addition to zero-valued elements enforced by the graph, there are typically many other elements with marginal in-fluence, although these locations may not be known). Using the well-known relationships among p -norms, if  X  k  X  i ( q ) k 2 2 &lt; 1, then k  X  i ( q ) k 1  X  C = 1 / X  is an appropriate choice.
 This methodology can be loosely used to show that our learned function satisfies the contraction condition of Algorithm 1 The training process of the fixed-point model
Input: Training structures {G 1 , G 2 , , G N } and their labelings { q 1 , q 2 , , q N } ;
Output: The trained contextual prediction func-tion f ;  X  r } by adding random perturbations; feature q N f : q i = f ( x i , q N Algorithm 2 The testing process of the fixed-point model
Input: The testing structure G = ( V , E ); the trained contextual prediction function f ; the num-ber of iterations T ; a threshold  X  ; Output: The labeling q of G ;
Initialize: t = 1; for each v i  X  X  , q 0 i = 0; repeat until t  X T or k q t  X  q t  X  1 k X   X  . q = [ q t 1 ,q t 2 , ,q t n ] T . k  X  i ( q ) k 1 &lt; 1 at the q from the training data and in neighboring regions such that an affine approximation to the true f is sufficient. We could also potentially incorporate an additional penalty term to encourage each e r and therefore  X  i ( q ) to be small. In practice, as shown in the experiments, our method can learn a good contraction mapping with few or even no per-turbations during training. Moreover, it can quickly converge to good solutions even though we always ini-tialize from q = 0 in testing, demonstrating a nice convergence property of the fixed-point model. 3.3. The Training and Prediction Processes The training and prediction processes are depicted in Algorithm 1 and Algorithm 2 . The training process is to learn a contextual prediction function f by fa-voring fixed-point solutions (or nearly so) using some perturbations. Once learned, the contraction mapping is applied iteratively to the new structured inputs. For a novel structured input G = ( V , E ), the labeling q i of a node v i  X  X  is initialized with a value. Note that q i is not sensitive to the choice of initialization, and it is simply initialized with 0 in our experiments. We now apply the proposed fixed-point model to the tasks of Optical Character Recognition (OCR), Part-of-Speech tagging (POS) and Hypertext (web pages) classification. The data in OCR and POS have chain structure and the average error per sequence in ( Nguyen &amp; Guo , 2007 ) is used for performance evalu-ation. In hypertext classification, the linking structure of the hypertext is highly non-regular and we use the average labeling error over all the testing web pages. 4.1. Optical Character Recognition (OCR) Optical character recognition involves the identifica-tion of letters in scanned texts. In this paper, the benchmark dataset ( Taskar et al. , 2003 ) is used. In OCR, a word corresponds to a structured input V and the i -th character corresponds to v i . We use the m/ 2 characters preceding and the m/ 2 characters af-ter v i as its neighbors and thus m indicates the com-plexity of the interdependence. For each character, its pixel values are concatenated to form x i . In train-ing, the lexical label y i is encoded to a |L| -dimensional contextual feature vector, which has value 1 only at the entry corresponding to the value of y i . In all, a |L| X  m -dimensional contextual feature is created. In testing, the entry corresponding to the label with the maximum score is assigned 1 at each iteration. No perturbed replicas are used in OCR and we use kernel logistic regression (KLR) ( Zhu &amp; Hastie , 2001 ) as the contextual prediction function; at the testing process, 5 iterations are used, i.e., T = 5. In Fig. 1 (a), we compare the fixed-point model with the auto-context model. Both of the two methods use KLR with RBF kernel as the classifier. We note that the original result of the auto-context model reported in ( Tu &amp; Bai , 2010 ) is only 19.5% because the Harr-like features used in that paper are not so effective on the OCR dataset. We compare the two models by varying m from 0 to 14. As from Fig. 1 (a), the er-
Auto-context Fixed-Point rors decrease monotonously as m increases. The fixed-point model performs slightly worse than the auto-context model, but it is simpler and more efficient: it takes about 7 . 55 minutes to train the model, while the auto-context model takes around 50 minutes because the auto-context model needs to train T classifiers se-quentially and apply the classifiers to each of the train-ing sequences.
 In Table 1 , we compare the fixed-point model with several state-of-the-art algorithms: SVM ( Crammer &amp; Singer , 2001 ), SVM struct ( Tsochantaridis et al. , 2005 ), M 3 N ( Taskar et al. , 2003 ), Perceptron ( Collins , 2002 ), KLR ( Zhu &amp; Hastie , 2001 ), SEARN ( Daum  X e et al. , 2009 ), CRF ( Lafferty et al. , 2001 ), HMM ( Rabiner , 1989 ), structured learning ensemble (SLE) ( Nguyen &amp; Guo , 2007 ), kernel conditional graphic model (KCGM) ( Cruz et al. , 2007 ) and the auto-context model ( Tu &amp; Bai , 2010 ). For SVM struct , M 3 N and CRF, the results are from ( Nguyen &amp; Guo , 2007 ) with linear kernel. As from Table 1 , with the exception of the auto-context model, the fixed-point model outperforms the state-of-the-art methods.
 In ( Keerthi &amp; Sundararajan , 2007 ), CRF and struc-tural SVM are implemented with a different set of features and the performance is better than that in ( Nguyen &amp; Guo , 2007 ), see CRF 2 and SVM struct 2 in Table 1 . Still, the errors are much higher than that of the fixed-point model. In ( Taskar et al. , 2003 ), M 3 N reports the average error per character 12 . 8% with cu-bic kernel while the average error per character of the fixed-point model is 2 . 13%. One may argue that if CRF, SVM struct and M 3 N were to use the contexts like those in the fixed-point model, they would generate similar results. However, it is exactly their large com-putational burden in taking into account long range interactions that limits their modeling ability. In Fig. 1 (b), we illustrate the convergence rate of the fixed-point model, revealing that both the train-ing and testing errors are very small after the second iteration. This suggests that we are able to train a fixed-point function that satisfies the conditions for convergence. In addition, the fixed-point model con-verges very quickly at the testing stage with only 2  X  3 iterations. 4.2. Part-of-Speech Tagging (POS) For the Part-of-Speech Tagging task, we use the POS dataset ( Treebank , 2002 ) and comply with the training/validation/testing splits in ( Nguyen &amp; Guo , 2007 ). For each word, 446 , 054 lexical features are used. We use the L1 regularized support vector ma-chine (SVM-L1) provided in the LIBLINEAR software package ( Fan et al. , 2008 ) as the classifier. In our experiment, m = 6 is used as it gives the best results on the validation datasets. For each sequence, one perturbed replica is produced using Gaussian noise with  X  = 0 . 25 and the contextual prediction function is trained with the perturbed replica and the original sequences.
 In Table 2 and Table 3 , we compare the average er-rors and the times to train the classifiers respectively. HMM is the most efficient in training, but its per-formance is poor. The fixed-point model is nearly as efficient as SVM-L1 since it needs only more feature dimensions and more training samples than SVM-L1; it is much simpler and more efficient than algorithms such as the auto-context model: on the data split of 8 , 000 training sentences, it takes 2 . 214 hours for the auto-context model to train, while it takes only 0 . 192 hours for the fixed-point model to train. The average error of the fixed-point model is slightly worse than the auto-context model but the difference is rather small. With the exception of the auto-context model and SLE, the fixed-point model outperforms the other methods. SLE performs the best but is the most com-plex since it is an ensemble method using about 200 different models and its training time is not listed in ( Nguyen &amp; Guo , 2007 ). 4.3. Hypertext Classification Hypertext classification aims to classify the web pages based on their contents and the linking structures. We use the WebKB dataset in ( Craven et al. , 1998 ) which contains web pages from 4 universities: Cornell, Texas, Washington and Wisconsin. Each page belongs to one of the 5 categories: course, faculty, student, project or other. The Bag of Words representation is used, and a codebook with 40 , 195 codes is built using Rain-bow ( McCallum , 1996 ). We compare the fixed-point model with SVM, CRF, and the auto-context model. The statistics (a normalized histogram) of the labels of the in-linking and out-linking pages are used as the context feature. The fixed-point model and the auto-context algorithm both achieve small average errors when the third-order in-linking and out-linking statis-tics are used. For CRF, we adopt the UGM toolbox ( Schmidt , 2011 ) and use loopy belief propagation for the inference. The first order, token-independent first order, and token-independent second order feature functions are used as these feature functions give the best performance in ( Keerthi &amp; Sundararajan , 2007 ). The models are trained on three universities and tested on the remaining one. The average errors of the 4 universities are reported in Table 4 . With one layer of fixed-point function, the proposed method achieves comparable result but is more efficient than the auto-context model. CRF performs the best but is much more computationally demanding.
 In this paper, we have proposed a fixed-point model for the structured labeling problem. The fixed-point model takes the labeling of the structure as both the input and the output with the assumption that the ground-truth labeling being the stable status for the function. The fixed-point model preserves the ability to capture long range contexts as in more complex lay-ered models. A simple learning strategy is adopted and contraction conditions are analyzed. On three structured labeling problems, the fixed-point model has achieved encouraging performance and efficiency.
