 Content-Based Multimedia Information Retrieval (CBMIR) systems which leverage multiple retrieval experts ( E n ) of-ten employ a weighting scheme when combining expert re-sults through data fusion. Typically however a query will comprise multiple query images ( I m ) leading to potentially N  X  M weights to be assigned. Because of the large number of potential weights, existing approaches impose a hierarchy for data fusion, such as uniformly combining query image results from a single retrieval expert into a single list and then weighting the results of each expert. In this paper we will demonstrate that this approach is sub-optimal and leads to the poor state of CBMIR performance in benchmarking evaluations. We utilize an optimization method known as Coordinate Ascent to discover the optimal set of weights ( |
E n | X | I m | ) which demonstrates a dramatic difference be-tween known results and the theoretical maximum. We find that imposing common combinatorial hierarchies for data fu-sion will half the optimal performance that can be achieved. By examining the optimal weight sets at the topic level, we observe that approximately 15% of the weights (from set |
E n | X | I m | ) for any given query, are assigned 70%-82% of the total weight mass for that topic. Furthermore we discover that the ideal distribution of weights follows a log-normal distribution. We find that we can achieve up to 88% of the performance of fully optimized query using just these 15% of the weights. Our investigation was conducted on TRECVID evaluations 2003 to 2007 inclusive and ImageCLEFPhoto 2007, totalling 181 search topics optimized over a combined collection size of 661,213 images and 1,594 topic images. H.3.3 [ Information Search and Retrieval ]: Retrieval Models Measurement, Experimentation uate two different fusion systems, and after executing both having first trained on the training collection we can make the observation that system  X  X  X  outperforms system  X  X  X  by 15%. On the surface this seems fine, system  X  X  X  has achieved a good performance improvement over system  X  X  X . However, this 15% is a relative increase, it is only meaningful when comparing the two systems under observation, and far less important when we do not know when given a fixed set of inputs, what the maximum achievable performance is. For instance, if system  X  X  X  scored a MAP of 0.2, but the theoreti-cal maximum attainable given the same inputs was 0.8, then the relative importance of system  X  X  X  X  15% improvement is diminished as there is clearly room for greater improvement. However, if the maximum was determined to be 0.21, then that 15% improvement is very significant.

A better use when determining what the maximum per-formance is for a fixed set of inputs, is to study the proper-ties of this maximally performing model. Rather than being primarily concerned with the maximum performance MAP value, to flip this around such that given we have a model which achieves excellent performance, what are the proper-ties of this model that led to this performance. In order to achieve this it necessitates optimisation directly on the test data. We are in-fact not proposing any particular model for data fusion in this paper, but rather we have created and observed the optimal model for this retrieval problem, such that we can report on the properties of this model which future systems should seek to leverage.

The objective of this paper therefore is to study what are the variables which generate a maximally performing CB-MIR data fusion system, and if there are any commonalities to these that can be discovered so as to inform the devel-opment of new data fusion algorithms and systems. The performance of these models is in themselves immaterial, it is these factors which we wish to identify and examine. In particular in this paper, we will be examining the impact of combinational hierarchies upon performance, and the distri-bution that an ideal form of weighting takes.

No doubt at this point after mention of optimizing directly on the test set, alarm bells are ringing in several readers minds, however we believe we have good justification for doing this. Whilst we have laboured the point as to why we are examining models optimized on the test data, we believe this is necessary as the approach is unorthodox, and inadequately justified can lead to the the results presented in this paper being dismissed out of hand.

The paper is organized as follows. Section 2 presents re-lated work in data fusion and evaluation, with reference to CBMIR applications. Section 3 details our experimental en-vironment including retrieval experts, data sets, and a brief discussion of the optimization technique. Section 4 presents the results of our optimization on TRECVID 2003-2007 and ImageCLEFPhoto 2007 and discusses observations on this data set. Section 5 tests these observations to determine if they can be exploited by existing data fusion approaches. We finish with our conclusions in Section 6.
Early data fusion research began with experimentation into the combination of different retrieval models, document representations and query representations [13, 6, 16]. Re-search by Belkin et al. [2, 3] noted that varying these differ-ent factors produced different sets of relevant documents, yet ters are held constant, that the combination of vector, prob-abilistic and Boolean retrieval models did not improve per-formance of retrieval, contrary to previous accepted wisdom. This was further demonstrated by a lack of performance im-provement when combining results from TREC-6, 7 and 8 queries which produced high overlaps in both R overlap and N overlap , meaning that each of the approaches were return-ing very similar content. Nevertheless this work found that the overlap coefficients were a good predictor of the potential for performance improvement with data fusion, particularly when systems were combined with weights, such that a poor performing system could be discounted. The combination of a poor system with a good system, using weights where the good system was weighted highly, produced performance in-creases, lending support to the application of weights for expert combination [11].

Beitzel et al. [1] like McCabe also conducted experiments where system parameters are held constant to measure the impact of combination of different aspects of retrieval sys-tems. The work of Beitzel et al. specifically examined the combination of  X  X ighly effective retrieval strategies X . As-suming this, Beitzel et al. hypothesize that combination of highly effective systems through voting mechanisms like CombMNZ are more likely to harm performance, as the highly effective systems have already been optimized and will rank relevant documents highly, therefore the candi-dates for promotion up a ranked list are lower ranked com-mon nonrelevant documents as the relevant documents are already highly ranked. They further hypothesize that as constants such as the query and stemming for each retrieval model are held constant, that different models will produce approximately the same set of documents for a query, only the relative ranks of these sets are likely to be different. For highly effective systems Beitzel et al. found that the combi-nation of retrieval models (e.g. vector space and probabilis-tic) hurts performance, rather than helps, whilst the overlap coefficients defined by Lee [9] provide a poor indicator of po-tential for improvement through data fusion [1].

These two results however give credence to the application of weighted data fusion to the task of CBMIR. Given that CBMIR is characterized by the combination of multiple poor retrieval experts [17], we are unlikely to be combining mul-tiple experts that actually perform consistently well for any set of queries. Furthermore as the work of McCabe shows, weighted combination of poor retrieval experts can lead to significant performance improvements.

Within the multimedia research community, several fu-sion approaches have been investigated. Yan et al. propose the use of  X  X uery-Class X  dependent weights [22], where a set of predefined query classes are assigned feature weights learned from the training data. This approach is extended by Kennedy et al. [8] to automatically discover query classes from training data. These approaches, however, typically weight entire features (retrieval experts).

Two previous investigations into the role of feature com-bination for multimedia retrieval have been completed, by McDonald and Smeaton [12] and by Yan and Hauptmann [21]. McDonald and Smeaton empirically compare combina-tion approaches for score, rank and probability techniques. Their work evaluated these approaches by optimizing Mean Average Precision (MAP) on a training collection with mul-tiple topics, then applying these generalized optimized pa-rameters to a test set. Our work differs as our optimizations
The optimization method we use in this work is known as Coordinate Ascent (also known as Alternating Variables Method ). It is a method which is able to optimize directly on Average Precision (AP), by randomly initializing the lin-ear weights, then finding a local maxima based on AP. The method is then repeated multiple times so that a global max-ima can be found. This approach has been used to good effect by Metzler and Croft, and a complete explanation of this method can be found in their work [14]. There are three basic levels of combination available to CBMIR designers whose systems utilize multiple retrieval experts and multiple query components: combination at the  X  X uery X  level [12], combination at the  X  X xpert X  level [22] and direct combination. Figure 1 illustrates these variations.
The elements for weighting can be formally defined as fol-lows. A CBMIR search topic will contain multiple example visual images, Images I = { queryimage i ... queryimage n } where 1  X  i  X  n . A CBMIR system will have at its disposal multiple retrieval experts, E = { expert j , ... expert m } where 1  X  j  X  m . Therefore, we can define the pair I i ,E j ,which is a unique coupling of every example query image to every visual retrieval expert. This will generate n  X  m pairs.
We can now further define the set of weights to be tuned at each of the three different levels of combination. For  X  X uery X  level combination, the results of every retrieval ex-pert for a specific query image ( I i ) are linearly combined with uniform weight into a single ranked list which repre-sents a given image. The merged results lists for every image are then weighted and combined, meaning for this level we need to optimize n weights (i.e. | I | ), giving Image Weights IW = { w i } where w is the weight and
At an  X  X xpert X  level of combination we execute the oppo-site. For a specific expert ( E j ) we query against it all query images for the topic, merging the results to produce for each expert a single ranked list. We then weight the result list of each expert, and combine to form our final ranked list. In this level, we are required to optimize m weights (i.e. | E | ), formally Expert Weights EW = { w j } and
Finally we have the  X  X irect X  level of combination which specifies weights for every coupling of an example image and retrieval expert. That is, for every pair I i ,E j we are re-quired to set a weight. This will produce a weight set of size n  X  m (i.e. | I | X | E | )where IEW = { w ij } ,where i refers to the query image, j refers to the retrieval expert and
We performed the optimization of CBMIR on TRECVID 2003 to 2007 and on ImageCLEFPhoto 2007. The results are presented in Table 2 (where TV is TRECVID and IC is ImageCLEFPhoto).
 Eval. TV03 TV04 TV05 TV06 TV07 IC07 MAP 0.122 0.108 0.141 0.056 0.130 0.216 Uniform 0.059 0.029 0.065 0.016 0.042 0.128 Table 2: Optimized Results compared to  X  X est Re-ported X  (BR).  X  X niform X  represents using all pairs
I ,E j with no weighting. *IC07 BR is visual only Figure 2: Standard Scores for assigned weights across all corpora This optimization generated automatic retrieval runs which achieved excellent performance with the use of no semantic information or text. For comparison, the row  X  X R X  shows the best reported automatic system MAP from that year X  X  activity. These figures are actually a bit startling, as the very high levels of performance achieved run contrary to expectations from previous experiments [17]. This is par-ticularly apparent if we compare the optimized MAP to the  X  X niform X  MAP. The  X  X niform X  map demonstrates a retrieval run where all pairs I i ,E j are equally weighted, i.e. there is no weighting at all. We can see that the optimal weights ap-plied to pairs I i ,E j can produce up to a 300% increase in performance. The impact of these figures is such that peo-ple question if such performance is achievable with low-level visual MPEG-7 features and no text, as it runs contrary to previous experimental knowledge.

We show the comparison to the best reported runs in that year X  X  evaluation, as it demonstrates the effectiveness of our optimization, producing retrieval runs which achieve excel-lent performance. The comparison highlights the maximum of what can be achieved with data fusion and global low-level visual features, particularly when compared against the top performing runs which made use of multiple evidence modal-ities including text and semantic information. We note that this comparison to published retrieval runs ( X  X R X ) is not a fair comparison as we optimized on the test data, however the intention of this work is to demonstrate the gains achiev-able with optimized weights, even when compared against retrieval runs that used high quality signals such as text.
We analyzed the optimal weight topic sets IEW generated for each topic and calculated the standard score (also known as Z-Score). The standard score allows us to express for any given pair I i ,E j weight w ij how far from the topic mean weight it is in terms of standard deviations. This provides us with a measure which can be used across topics reliably. Figure 2 is a histogram of the distribution of standard scores across all topics and corpora.

We can infer multiple insights from the presented distri-bution and measures of central tendency. Firstly, that whilst the distribution of weights has some properties of that of a normal distribution, such as a majority of the data points clustered around the mean and within the range  X  3  X  ,there does exist a very definitive positive skew. Secondly, as part of this positive skew approximately 10%-11% of the weights first graph in Figure 4 represents topics from TRECVID 2005. In topic  X 0149 X  we can observe a blue bar at 70%, and a yellow bar at 6%. This means that for topic  X 0149 X , 6% of the pairs I i ,E j used for that topic were allocated 70% of the weight and the remaining 94% of I i ,E j had only 30% of the topic weight.
 We can see from this graph that for all topics across TRECVID 2005, achieving the maximum average precision possible is dependant upon specific pairs I i ,E j being al-located the bulk of the weight for that topic, rather than specific experts EW being correctly weighted. Whilst we only show TRECVID 2005 here, the patterns expressed in this graph are replicated for all evaluations examined.
The possibility exists that we are inadvertently seeing one retrieval expert for a topic performing strongly, and thus all pairs I i ,E j which utilize that expert are up-weighted. We examined the distribution of each of the six experts within the set of highly weighted I i ,E j to determine if there was a bias towards any particular expert, shown in Table 3. Table 3: Distribution of Retrieval Experts in I i ,E j with w ij &gt; 1  X  There is a slight bias towards EH and to a lesser extent the HT experts, however as there are only two texture but four color experts, this bias can be accounted for. The data presented in Figure 4 and Table 3 shows that highly weighted
I ,E j are distributed across different experts.

We observe that the key to maximizing AP is to correctly identify salient pairs I i ,E j and ensure that these are highly weighted, rather than weighting the overall performance of any given retrieval expert. To test this observation, we de-vise a series of experiments that utilize only highly weighted pairs I i ,E j to see if we still achieve good performance. A highly-weighed pair I i ,E j is a pair whose weight w ij is greater than +1  X  of the mean weight for that topic.
To test our observations we devised three experiments in order to (1) determine to what extent the highly-weighted pairs I i ,E j impact upon performance; (2) to determine if the weighting of these pairs needs to be exact or if merely the performance achieved if no weighting scheme at all is em-ployed. The maximum is the fully optimized result as shown in Section 4, demonstrating the best performance that can be achieved. These two figures provide a lower and upper bound for data fusion performance comparisons, allowing us to make decisions using absolute observations with regard to the bounds, rather than relative observations by comparing only to existing data fusion approaches.

Our results are presented in Figure 5. Each table presents the minimum (Uniform All), maximum (All Optimized) and results of the 5 experiments using MAP, recall and P10. For every experiment X  X  MAP, we show in brackets how close that approach came to achieving the optimal performance. The MAP of each of the experiments, along with the maximum MAP, is graphed in Figure 6. For each of our 5 runs we ran significance tests (partial randomization) with  X  0.05. For the TRECVID benchmarks we found no significant differ-ence between the  X  X uery X  and  X  X xpert X  levels of hierarchical combination, indicating that if hierarchical combination is employed and optimally weighted, there is no difference in between them. However for ImageCLEF  X  X xpert X  was sig-nificantly different. For benchmarks TRECVID 2003-2006, all runs using highly weighted (1  X  ) pairs performed signifi-cantly better than the hierarchical combination approaches. For TRECVID 2007, only run 1  X  was significantly different.
The graph presents a clear stratification of the results, particularly for benchmarks TRECVID 2003 -2006. We can clearly see the very large discrepancy in performance be-tween the hierarchical fusion approaches (at the bottom of the graph) versus the targeted weighting approaches in the middle. This separation illustrates the performance gains achievable by moving away from hierarchical combinations. Of exception is TRECVID 2007 and ImageCLEF 2007, where there is less of a difference in performance. These two bench-marks exhibit the greatest ratio of topic images to collection images  X  in the case of ImageCLEF one topic image for ev-ery 112 collection images. This indicates that recall plays a more prominent role in these evaluations, and that the selec-tion of highly-weighted pairs may have been too restrictive to provide adequate topic coverage. This is reinforced by the run 1  X  U-T, which included all pairs I i ,E j : it performed the best even though it used non-specific weights.
The run 1  X  highlights that, using a subset of pairs I i ,E j from IEW , very good performance can be achieved despite a reduction in potential recall by not using all pairs. Far more encouraging is the performance of runs 1  X  Uand1  X  U-T. Whilst run 1  X  had value as an illustrative run, it is hard to conceptualize a data fusion algorithm that would create the exact optimal weights for these pairs. However, as runs 1  X  U and 1  X  U-T did not use the optimal weights, but rather only identified what the high-performing pairs 1  X  Uand1  X  U-T were (essentially a binary weighting), and still achieved ex-cellent performance, it provides a clear direction for devel-opment of data fusion algorithms. These runs demonstrate that if methods can be developed to identify pairs I i ,E j that are likely to be highly weighted, then exact weighting is not required to obtain performance superior to that of methods which employ hierarchies.
In this paper we have demonstrated that the application of a data fusion hierarchy severely limits the performance that a CBMIR retrieval run can possibly achieve. We propose Legend Run Uniform All Expert Level
Query Level 1  X  Uniform 1  X  Uniform &amp; Tail 1  X  All Optimized Legend Run Uniform All Expert Level
Query Level 1  X  Uniform 1  X  Uniform &amp; Tail 1  X  All Optimized [5] W. B. Croft. Combining approaches to information [6] P. Das-Gupta and J. Katzer. A study of the overlap [7] E. A. Fox and J. A. Shaw. Combination of Multiple [8] L. S. Kennedy, A. P. Natsev, and S.-F. Chang. [9] J. H. Lee. Analyses of multiple evidence combination. [10] B. Manjunath, P. Salembier, and T. Sikora, editors. [11] M. McCabe, A. Chowdhury, D. Grossman, and [12] K. McDonald and A. F. Smeaton. A comparison of [13] M. McGill, M. Koll, and T. Noreault. An evaluation of
