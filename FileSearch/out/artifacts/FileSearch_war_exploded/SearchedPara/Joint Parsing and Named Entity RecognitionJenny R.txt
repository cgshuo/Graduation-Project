 In order to build high quality systems for complex NLP tasks, such as question answering and textual entailment, it is essential to first have high quality systems for lower level tasks. A good (deep analy-sis) question answering system requires the data to first be annotated with several types of information: parse trees, named entities, word sense disambigua-tion, etc. However, having high performing, low-level systems is not enough; the assertions of the various levels of annotation must be consistent with one another. When a named entity span has crossing brackets with the spans in the parse tree it is usually impossible to effectively combine these pieces of in-formation, and system performance suffers. But, un-fortunately, it is still common practice to cobble to-gether independent systems for the various types of annotation, and there is no guarantee that their out-puts will be consistent.

This paper begins to address this problem by building a joint model of both parsing and named entity recognition. Vapnik has observed (Vapnik, 1998; Ng and Jordan, 2002) that  X  X ne should solve the problem directly and never solve a more gen-eral problem as an intermediate step, X  implying that building a joint model of two phenomena is more likely to harm performance on the individual tasks than to help it. Indeed, it has proven very diffi-cult to build a joint model of parsing and seman-tic role labeling, either with PCFG trees (Sutton and McCallum, 2005) or with dependency trees. The CoNLL 2008 shared task (Surdeanu et al., 2008) was intended to be about joint dependency parsing and semantic role labeling, but the top performing systems decoupled the tasks and outperformed the systems which attempted to learn them jointly. De-spite these earlier results, we found that combining parsing and named entity recognition modestly im-proved performance on both tasks. Our joint model produces an output which has consistent parse struc-ture and named entity spans, and does a better job at both tasks than separate models with the same fea-tures.

We first present the joint, discriminative model that we use, which is a feature-based CRF-CFG parser operating over tree structures augmented with NER information. We then discuss in detail how we make use of the recently developed OntoNotes corpus both for training and testing the model, and then finally present the performance of the model and some discussion of what causes its superior per-formance, and how the model relates to prior work. When constructing a joint model of parsing and named entity recognition, it makes sense to think about how the two distinct levels of annotation may help one another. Ideally, a named entity should cor-respond to a phrase in the constituency tree. How-ever, parse trees will occasionally lack some explicit structure, such as with right branching NPs. In these cases, a named entity may correspond to a contigu-ous set of children within a subtree of the entire parse. The one thing that should never happen is for a named entity span to have crossing brackets with any spans in the parse tree.

For named entities, the joint model should help with boundaries. The internal structure of the named entity, and the structural context in which it ap-pears, can also help with determining the type of entity. Finding the best parse for a sentence can be helped by the named entity information in similar ways. Because named entities should correspond to phrases, information about them should lead to better bracketing. Also, knowing that a phrase is a named entity, and the type of entity, may help in get-ting the structural context, and internal structure, of that entity correct. 2.1 Joint Representation After modifying the OntoNotes dataset to ensure consistency, which we will discuss in Section 4, we augment the parse tree with named entity informa-tion, for input to our learning algorithm. In the cases where a named entity corresponds to multiple con-tiguous children of a subtree, we add a new Name-dEntity node, which is the new parent to those chil-dren. Now, all named entities correspond to a single phrasal node in the entire tree. We then augment the labels of the phrasal node and its descendents with the type of named entity. We also distinguish be-tween the root node of an entity, and the descendent nodes. See Figure 1 for an illustration. This repre-sentation has several benefits, outlined below. 2.1.1 Nested Entities The OntoNotes data does not contain any nested en-tities. Consider the named entity portions of the rules seen in the training data. These will look, for instance, like none  X  none person , and organization  X  organization organization . Because we only al-low named entity derivations which we have seen in the data, nested entities are impossible. However, there is clear benefit in a representation allowing nested entities. For example, it would be beneficial to recognize that the United States Supreme Court is a an organization , but that it also contains a nested GPE . 1 Fortunately, if we encounter data which has been annotated with nested entities, this representa-tion will be able to handle them in a natural way. In the given example, we would have a derivation which includes organization  X  GPE organization . This information will be helpful for correctly la-beling nested entities such as New Jersey Supreme Court , because the model will learn how nested en-tities tend to decompose. 2.1.2 Feature Representation for Named Currently, named entity recognizers are usually con-structed using sequence models, with linear chain conditional random fields (CRFs) being the most common. While it is possible for CRFs to have links that are longer distance than just between adjacent words, most of the benefit is from local features, over the words and labels themselves, and from fea-tures over adjacent pairs of words and labels. Our joint representation allows us to port both types of features from such a named entity recognizer. The local features can be computed at the same time the features over parts of speech are computed. These are the leaves of the tree, when only the named en-features, over adjacent labels, are computed at the same time as features over binary rules. Binariza-tion of the tree is necessary for efficient computa-tion, so the trees consist solely of unary and bi-nary productions. Because of this, for all pairs of adjacent words within an entity, there will be a bi-nary rule applied where one word will be under the left child and the other word will be under the right child. Therefore, we compute features over adjacent words/labels when computing the features for the bi-nary rule which joins them. 2.2 Learning the Joint Model We construct our joint model as an extension to the discriminatively trained, feature-rich, conditional random field-based, CRF-CFG parser of (Finkel and Manning, 2008). Their parser is similar to a chart-based PCFG parser, except that instead of putting probabilities over rules, it puts clique potentials over local subtrees. These unnormalized potentials know what span (and split) the rule is over, and arbitrary features can be defined over the local subtree, the span/split and the words of the sentence. The inside-outside algorithm is run over the clique potentials to produce the partial derivatives and normalizing con-stant which are necessary for optimizing the log like-lihood. 2.3 Grammar Smoothing Because of the addition of named entity annota-tions to grammar rules, if we use the grammar as read off the treebank, we will encounter prob-lems with sparseness which severely degrade per-formance. This degradation occurs because of CFG rules which only occur in the training data aug-mented with named entity information, and because of rules which only occur without the named entity information. To combat this problem, we added ex-tra rules, unseen in the training data. 2.3.1 Augmenting the Grammar For every rule encountered in the training data which has been augmented with named entity information, we add extra copies of that rule to the grammar. We add one copy with all of the named entity informa-tion stripped away, and another copy for each other entity type, where the named entity augmentation has been changed to the other entity type.
 These additions help, but they are not sufficient. Most entities correspond to noun phrases, so we took all rules which had an NP as a child, and made copies of that rule where the NP was augmented with each possible entity type. These grammar ad-ditions sufficed to improve overall performance. 2.3.2 Augmenting the Lexicon The lexicon is augmented in a similar manner to the rules. For every part of speech tag seen with a named entity annotation, we also add that tag with no named entity information, and a version which has been augmented with each type of named entity.
It would be computationally infeasible to allow any word to have any part of speech tag. We there-fore limit the allowed part of speech tags for com-mon words based on the tags they have been ob-served with in the training data. We also augment each word with a distributional similarity tag, which we discuss in greater depth in Section 3, and al-low tags seen with other words which belong to the same distributional similarity cluster. When decid-ing what tags are allowed for each word, we initially ignore named entity information. Once we deter-mine what base tags are allowed for a word, we also allow that tag, augmented with any type of named entity, if the augmented tag is present in the lexicon. We defined features over both the parse rules and the named entities. Most of our features are over one or the other aspects of the structure, but not both.
Both the named entity and parsing features utilize the words of the sentence, as well as orthographic and distributional similarity information. For each word we computed a word shape which encoded information about capitalization, length, and inclu-sion of numbers and other non-alphabetic charac-ters. For the distributional similarity information, we had to first train a distributional similarity model. We trained the model described in (Clark, 2000), with code downloaded from his website, on several hundred million words from the British national cor-pus, and the English Gigaword corpus. The model we trained had 200 clusters, and we used it to assign each word in the training and test data to one of the clusters.

For the named entity features, we used a fairly standard feature set, similar to those described in (Finkel et al., 2005). For parse features, we used the exact same features as described in (Finkel and Man-ning, 2008). When computing those features, we re-moved all of the named entity information from the rules, so that these features were just over the parse information and not at all over the named entity in-formation.

Lastly, we have the joint features. We included as features each augmented rule and each augmented label. This allowed the model to learn that certain types of phrasal nodes, such as NP s are more likely to be named entities, and that certain entities were more likely to occur in certain contexts and have par-ticular types of internal structure. For our experiments we used the LDC2008T04 OntoNotes Release 2.0 corpus (Hovy et al., 2006). The OntoNotes project leaders describe it as  X  X  large, multilingual richly-annotated corpus con-structed at 90% internanotator agreement. X  The cor-pus has been annotated with multiple levels of anno-tation, including constituency trees, predicate struc-ture, word senses, coreference, and named entities. For this work, we focus on the parse trees and named entities. The corpus has English and Chinese por-tions, and we used only the English portion, which itself has been split into seven sections: ABC, CNN, MNB, NBC, PRI, VOA, and WSJ. These sections represent a mix of speech and newswire data. 4.1 Data Inconsistencies While other work has utilized the OntoNotes corpus (Pradhan et al., 2007; Yu et al., 2008), this is the first work to our knowledge to simultaneously model the multiple levels of annotation available. Because this is a new corpus, still under development, it is not surprising that we found places where the data was inconsistently annotated, namely with crossing brackets between named entity and tree annotations.
In the places where we found inconsistent anno-tation it was rarely the case that the different lev-els of annotation were inherently inconsistent, but rather inconsistency results from somewhat arbitrary choices made by the annotators. For example, when the last word in a sentence ends with a period, such as Corp. , one period functions both to mark the ab-breviation and the end of the sentence. The conven-tion of the Penn Treebank is to separate the final pe-riod and treat it as the end of sentence marker, but when the final word is also part of an entity, that final period was frequently included in the named entity annotation, resulting in the sentence terminat-ing period being part of the entity, and the entity not corresponding to a single phrase. See Figure 2 for an illustration from the data. In this case, we removed the terminating period from the entity, to produce a consistent annotation.

Overall, we found that 656 entities, out of 55 , 665 total, could not be aligned to a phrase, or multiple contiguous children of a node. We identified and corrected the following sources of inconsistencies: Periods and abbreviations. This is the problem Determiners and PPs. Noun phrases composed of Adjectives and PPs. This problem is similar to the
These three modifications to the data solved most, but not all, of the inconsistencies. Another source of problems was conjunctions, such as North and South Korea , where North and South are a phrase, but South Korea is an entity. The rest of the er-rors seemed to be due to annotation errors and other random weirdnesses. We ended up unable to make 0 . 4% of the entities consistent with the parses, so we omitted those entities from the training and test data.
One more change we made to the data was with respect to possessive NPs. When we encountered noun phrases which ended with (POS  X  X ) or (POS  X ) , we modified the internal structure of the NP. Origi-nally, these NPs were flat, but we introduced a new nested NP which contained the entire contents of the original NP except for the POS. The original NP la-bel was then changed to PossNP. This change is mo-tivated by the status of  X  X  as a phrasal affix or clitic: It is the NP preceding  X  X  that is structurally equiva-lent to other NPs, not the larger unit that includes  X  X  . This change has the additional benefit in this context that more named entities will correspond to a single phrase in the parse tree, rather than a contiguous set of phrases. 4.2 Named Entity Types The data has been annotated with eighteen types of entities. Many of these entity types do not occur very often, and coupled with the relatively small amount of data, make it difficult to learn accurate entity models. Examples are work of art , product , and law . Early experiments showed that it was dif-ficult for even our baseline named entity recognizer, based on a state-of-the-art CRF, to learn these types but the three most dominant entity types into into one general entity type called misc . The result was four distinct entity types: person , organization , GPE (geo-political entity, such as a city or a country), and misc . We ran our model on six of the OntoNotes datasets 40 and under (approximately 200 , 000 annotated En-glish words, considerably smaller than the Penn Treebank (Marcus et al., 1993)). For each dataset, we aimed for roughly a 75% train / 25% test split. See Table 1 for the the files used to train and test, along with the number of sentences in each.

For comparison, we also trained the parser with-out the named entity information (and omitted the NamedEntity nodes), and a linear chain CRF using just the named entity information. Both the base-line parser and CRF were trained using the exact same features as the joint model, and all were op-timized using stochastic gradient descent. The full results can be found in Table 2. Parse trees were scored using evalB (the extra NamedEntity nodes were ignored when computing evalB for the joint model), and named entities were scored using entity F-measure (as in the CoNLL 2003 conlleval ). 5
While the main benefit of our joint model is the ability to get a consistent output over both types of annotations, we also found that modeling the parse and named entities jointly resulted in improved per-formance on both. When looking at these numbers, it is important to keep in mind that the sizes of the training and test sets are significantly smaller than the Penn Treebank. The largest of the six datasets, CNN, has about one seventh the amount of training data as the Penn Treebank, and the smallest, MNB, has around 500 sentences from which to train. Parse performance was improved by the joint model for five of the six datasets, by up to 1 . 36%. Looking at the parsing improvements on a per-label basis, the largest gains came from improved identication of NML consituents, from an F-score of 45 . 9% to 57 . 0% (on all the data combined, for a total of 420 NML constituents). This label was added in the new treebank annotation conventions, so as to identify in-ternal left-branching structure inside previously flat NPs. To our surprise, performance on NPs only in-creased by 1%, though over 12 , 949 constituents, for the largest improvement in absolute terms. The sec-ond largest gain was on PPs, where we improved by 1 . 7% over 3 , 775 constituents. We tested the signif-icance of our results (on all the data combined) us-ing Dan Bikel X  X  randomized parsing evaluation com-gains were significant at p  X  0 . 01.

Much greater improvements in performance were seen on named entity recognition, where most of the domains saw improvements in the range of 3 X  4%, with performance on the VOA data improving by nearly 9%, which is a 45% reduction in error. There was no clear trend in terms of precision ver-sus recall, or the different entity types. The first place to look for improvements is with the bound-aries for named entities. Once again looking at all of the data combined, in the baseline model there were 203 entities where part of the entity was found, but one or both boundaries were incorrectly identified. The joint model corrected 72 of those entities, while incorrectly identifying the boundaries of 37 entities which had previously been correctly identified. In the baseline NER model, there were 243 entities for which the boundaries were correctly identified, but the type of entity was incorrect. The joint model cor-rected 80 of them, while changing the labels of 39 entities which had previously been correctly identi-fied. Additionally, 190 entities were found which the baseline model had missed entirely, and 68 enti-ties were lost. We tested the statistical significance of the gains (of all the data combined) using the same sentence-level, stratified shuffling technique as Bikel X  X  parse comparator and found that both preci-sion and recall gains were significant at p &lt; 10  X  4 .
An example from the data where the joint model helped improve both parse structure and named en-tity recognition is shown in Figure 4. The output from the individual models is shown in part (a), with the output from the named entity recognizer shown in brackets on the words at leaves of the parse. The output from the joint model is shown in part (b), with the named entity information encoded within the parse. In this example, the named entity Egyp-tian Islamic Jihad helped the parser to get its sur-rounding context correct, because it is improbable to attach a PP headed by with to an organization . At the same time, the surrounding context helped the joint model correctly identify Egyptian Islamic Jihad as an organization and not a person . The baseline parser also incorrectly added an extra level of structure to the person name Osama Bin Laden , while the joint model found the correct structure. A pioneering antecedent for our work is (Miller et al., 2000), who trained a Collins-style generative parser (Collins, 1997) over a syntactic structure aug-mented with the template entity and template rela-tions annotations for the MUC-7 shared task. Their sentence augmentations were similar to ours, but they did not make use of features due to the gen-erative nature of their model. This approach was not followed up on in other work, presumably because around this time nearly all the activity in named entity and relation extraction moved to the use of discriminative sequence models, which allowed the flexible specification of feature templates that are very useful for these tasks. The present model is able to bring together both these lines of work, by integrating the strengths of both approaches.
There have been other attempts in NLP to jointly model multiple levels of structure, with varying de-grees of success. Most work on joint parsing and se-mantic role labeling (SRL) has been disappointing, despite obvious connections between the two tasks. Sutton and McCallum (2005) attempted to jointly model PCFG parsing and SRL for the CoNLL 2005 shared task, but were unable to improve perfor-mance on either task. The CoNLL 2008 shared task (Surdeanu et al., 2008) was joint dependency pars-ing and SRL, but the top performing systems de-coupled the tasks, rather than building joint models. Zhang and Clark (2008) successfully built a joint model of Chinese word segmentation and parts of speech using a single perceptron.

An alternative approach to joint modeling is to take a pipelined approach. Previous work on linguis-tic annotation pipelines (Finkel et al., 2006; Holling-shead and Roark, 2007) has enforced consistency from one stage to the next. However, these models are only used at test time; training of the compo-nents is still independent. These models also have the potential to suffer from search errors and are not guaranteed to find the optimal output. We presented a discriminatively trained joint model of parsing and named entity recognition, which im-proved performance on both tasks. Our model is based on a discriminative constituency parser, with the data, grammar, and features carefully con-structed for the joint task. In the future, we would like to add other levels of annotation available in the OntoNotes corpus to our model, including word sense disambiguation and semantic role labeling. The first author is supported by a Stanford Gradu-ate Fellowship. This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM. The content does not neces-sarily reflect the views of the U.S. Government, and no official endorsement should be inferred. We also wish to thank the creators of OntoNotes, without which this project would not have been possible.
