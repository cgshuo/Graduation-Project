 University ofEdinburgh
Thetitleofthiscolumn,LastWords,remindsmeofanoccasionin2005,whenIhadthe privilegeofattendingtheawardceremonyfortheprestigiousBenjaminFranklinMedal, givenannuallytoafewscientistswhohavemadeoutstandinglifetimecontributionsto science.Thistime,acomputationallinguist,AravindJoshi,wasamongthem,soseveral past, present, and future presidents and officers of the ACL joined the Great and the Goodat the ceremony at the Franklin Institute in Philadelphia.
 whichmostly consisted ofvoice-over byanarrator, interspersed withsound-bites from therecipientsabouttheirlifeandwork,inthelastofwhichtheyhadclearlybeenasked todeliver as their last words abrief take-home message.
 distinguished pioneer of string theory. I was initially puzzled by the enthusiasm on the part of a mostly lay audience for such theoretical work, which for all its elegance and beauty, could not (as far as I could see) be expected to have nearly as much impact on their everyday lives as that of some of the other recipients, who that year included not only Aravind, but another computer scientist whose impact on information processing will be obvious tothe members of ACL, Andrew Viterbi.
 with string theory. Thisadmirable man X  X  last words tous hadbeen the following: 1. The Public Image of a Science
I realized then that what we were applauding was not a physicist, or the beauty of string theory, but physics itself . I was reminded that physicists are looked on as public intellectuals who can be relied on to deliver Truth, and who are generally assumed to be doing A Good Job, even when they say that the universe must really be ten-dimensional,exceptthatexactlysixofthedimensionsarecurledupsotightlythatthere is no conceivable way of detecting them, nor any prospect of bringing to bear the huge energies that would be required to straighten them out a bit so we could take a look at them. (Asmany of you will know, I AmNot Making ThisUp.) artificial intelligence, and even core computer science. To the extent that the public thinks at all about what we do, they thinkof us as producing gadgets, such as amusing ne wsorts of telephone. Only the other day, a colleague was called up by someone in a neighboring department asking if we could mend his PC for him. ( X  X on X  X  you have a Little Man? You used tohave aLittleMan. X ) seeking assurance that we know what we are doing, and are doing enough for the economy. Many of these reviews draw very negative conclusions X  X he 1966 report of the Automatic Language Processing Advisory Committee (ALPAC) of the US National
Academy of Sciences effectively shut down research in machine translation for over a decade, and was the main reason for ACL changing its name in 1968 from the As-sociation for Machine Translation and Computational Linguistics (AMTCL). The 1973 reportofSirJamesLighthill(afluiddynamicistknownforhisfoundationalworkinthe field of aeroacoustics) to the UK Science Research Council (SRC) closed down artificial intelligence and NLP research for a decade, until the Alvey report decided that British industry had fallen behind in this area, and opened it up again. There have been many similar examples since then, though fe was catastrophic.
 sionschairedbycompleteoutsiders(roboticistsorcomputationallinguists,perhaps?)to decidewhetherphysicistsareearningtheirkeep.Thephysiciststellthegovermentwhat they think it is right to do, and the government either funds it or it doesn X  X . Even when it doesn X  X , as in the case of the superconducting supercollider, it X  X  because they can X  X  afford it, or lack the political po wer, not because of lo westeem. Ho wdo the physicists do it? through the Beatles X  era, you have to have a gimmick. The physicists gave us atomic energy and the bomb, so no one can ever suggest again that they do not deliver Bang fortheBuck, even when they actually don X  X  ,asmaywellturnouttobethecaseforthepast twenty years or soof research in string theory andsupersymmetry. proven laws that all scientists recognize, from the laws of thermodynamics to the special and general theories of relativity and quantum theory. This body of knowledge lends both authority, and a breadth of vision that transcends any individual physicist X  X  work and any individual theory, even if parts of it can be temporarily ignored when convenient.
 lives at least as profoundly as atomic energy. The statistical machine translation tools that Google launched around May 2006 with Arabic for all the world to freely beta-test, and which have since been extended to Chinese, Russian, Japanese, and Korean, imperfectastheyare,maywellhaveanevenbiggerimpact.OurcolleaguesinAIrejoice in beating international chess Grand Masters with Deep Blue, and boast of robots on
Mars and autonomous vehicles charging around the Mojave desert. Computer science has theInternet itself tosho woff.
 statistically approximate language models, the only-just-trans-context-free automata-theoretic level of natural languages, the surface-compositionality of natural language 138 semantics, and have dreams of a grand unified head dependency-driven Theory of
Everything that that will one day make probabilistic and deterministic components work together toyield Meaning. How come we don X  X  get respect? cipline, within which those important truths are respected and held beyond reasonable doubt,inwhosenamewestandunitedintheeyesoftheworld.Instead,ourhistoryhas been sectarian, with dominant factions seeking to suppress proposals that diverge too much from the current orthodoxy, until theytoo are overthrown.
 funding agencies, which are notably harsher than in other sciences, and which have the effect of making the agencies assume a shortage of good science in the field, so that they reduce funding accordingly. Worse still, when we have theoretical disagreements, weplaythemoutinpublic.(ThesplitbetweenconnectionistsandtherestofAI/NLPis a case in point. It is sad to note that similar internal dissension seems to have played a role in the adverse conclusions reached in both the ALPAC and Lighthill reports.) lent at around twice the rate in CS, even if it means not getting funded themselves in the current round. The funding agencies conclude that there is a surplus of good science there, and seek or allocate further funding for the next round. When there are disagreements X  X s there seem to be currently about string theory X  X hey are argued out behind closed doors, until a consensus can be reached and be presented to the generalpublic.Thebiologistsbehavedsimilarlyoverarecentdisagreementconcerning methods for sequencing the genome.
 1980s, the information theoreticians and statistical modelers among us used to make commoncausewiththelinguists,wehavesubsequentlydriftedapart.Wehavecometo believe that the linguists have forgotten Zipf X  X  law, which says that most of the variance in language behavior can be captured byasmall part of the system.
 Long Tail.
 onareactuallyverybadatinducingsystemsforwhichthecrucialinformationisinrare events X  X ike physics, for example.
 becauseofthedemiseofMoore X  X law,orsimplybecausewehavedonealltheeasystuff, theLongTailwillcomebacktohauntus.Forexample,considerthecurrentstateofour former nemesis MT. 2. Machine Translation Then and Now
Machine translation (MT) was one of the earliest applications envisaged for computing machinery. Weaver (1949) identified the extreme ambiguity of natural language as the central problem of MT, and outlined as possible solutions not only the  X  X oisy channel X  modelofSMTanditsbasisin n -gramlanguagemodels,investigatedwithClaudeShan-non(ShannonandWeaver1949),butalsothenotionofparsingaslogicaldeduction,and the interlingua-based syntax-driven approach to MT, which he based on the linguists X  notion ofUniversal Grammar. (Nosectarianism there ,atleast.) easier cognitive tasks to mimic by machine. There seemed to be a real possibility that there might be ways to bypass understanding altogether using simple purely syntactic and probabilistic devices. Over the next few years, there was a gradual disillusion with thisexpectation, for at least three reasons.
 not only fell outside the class that could be completely recognized using finite-state machines of the class implicit in Shannon X  X  models, but also fell outside the class that couldbeadequatelyrepresentedbycontext-freegrammars.Itwasn X  X immediatelyclear ho wto translate such grammars into computational terms, and they appeared to break the unity of grammar and probabilistic model that had been one of the attractions of theinformation-theoreticapproach(althoughChomskywascarefultoacknowledgethe possibilityofstatisticallyapproximatingsuchgrammarswithMarkovprocessesforthe purpose of reducing ambiguity).
 within narrow genres like newspaper text or scientific papers, were very large indeed.
The lack of computational transparency of grammar formalisms tended to give the hand-built grammars of this period the character of unstructured programs, with no apparent prospect of inducing themautomatically.
 anyone had expected.
 problems, requiring deep understanding of semantics and knowledge of the world, as well asfull syntactic processing.
 ofamyth)concerningademonstrationofanearlyRussiandictionary-basedMTsystem using back-translation from English to Russian and back again. According to legend, the demonstrators were disconcerted to find that the sentence Time flies like an arrow would have been appropriate for the sentence Fruit flies like a banana. of language models, of a kind that is no wcommonplace, thanks to Moore X  X  la wand
HMMs, together with a method for learning such models and integrating them with structural rules.Somerealprogresshasbeenmade,andMTisonceagainthoughtofas an  X  X asy X  problem, that can be at least partly solved with relatively low-level methods. 3. An Experiment
So, ho wmuch progress have we made? We can repeat the back-translation experi-ment with Google Language Tools Beta n -gram-and-finite-state-transducer-based Ara-lineisaglossoftheArabicwords,andthelastlineistheresultoftranslatingtheArabic back again bySMT.) 140 a bit of morphology, but of course end-to-end back-translation is a very weak test, where you can just get lucky. Readers of Arabic will notice that the translation of like is indeed a comparative, not a verb meaning enjoy, as in the legend. However, they foretold.
 still comparative, rather than a verb. So the two sentences are analyzed the same way, asin the story.
 domain,sothelanguagemodeldoesn X  X helpusatall.Solet X  X tryanin-domainexample ofnewswire text.
 the Arabic for  X  X oogle Machine Translation, X  simply because I had already read the
Englishreferencedocument, 3 andIwasprettysureitwouldbeouttheresomewhere.It isahuman-authoredArabictranslationofarecentReutersstoryaboutthelaunchingof Google Language Tools, taken fromAl Jazeera: 4
Here is the SMT translation, delivered in about the time it would take a native speaker toread theoriginal:
Thefirstthingtonotice isthatthisisreallyverygood.Itisquiteclear whatthestoryis, and you can even guess that what Franz Och actually said in the English reference text was:  X  X he more data we feed into the system, the better it gets. X  It even seems to know that  X  X oogle X  can be a verb.

Both pronouns  X  X e X  in the last paragraph will be understood as referring to Franz, whereas in the reference text it is Miles Osborne who does the commending and the pointing out. Moreover, the alarming rumor of the latter X  X  death has been greatly exag-gerated by the English language-model: The reference text says he  X  X pent a sabbatical last year working on the Google project. X  The human Arabic translation says much the same,buttheArabicwordsfor spent and died arehomographs,andthenewswire-based model favors the latter.
 thatthesystemmakesahashoftheunboundeddependencyinFranz X  X useofwhatthey call the  X  MORE -MORE  X  X onstruction.
 ed dependencies, using back-translation on artificially generated (but in-domain) examples: 142
Comparing the examples that are translated correctly and those (labeled *) that are not, the generalization is already clear: even a 5-gram model can only handle root subject relativeclauses.Objectrelativesarebeyondthehorizon.(Theseeffectsarerobustunder variation of the content words.) Here are some more challenging embedded examples that confirm the diagnosis: 4. Who Cares?
What does this tell us? Nothing that we shouldn X  X  have already known. We knew that n -gram models and FSTs weren X  X  going to handle long-range dependencies, because Chomsky toldus so. That X  X one of the Big Truths of computational linguistics. talking about phenomena on a large scale, just as they have the General Theory of Relativity, and another theory for talking about the very small scale, just as they have
QuantumTheory.Likethephysicists,wehavedifficultyinreconcilingthosetheoretical levels. Like them, some of us think it X  X  fine to have two theories, whereas others of us thinkit X  X intolerable.
 dencies are sparse. (There are around 20K *T* empty categories in around 16K of the roughly 40K sentences in the Penn Treebank, of which around 6K seem to be non-subject, non-sentential wh -traces of some kind.) Worrying about them isn X  X  going to significantly impact overall parser dependency recovery rates, much less n -gram-precision-based BLEU scores. By the time we have fed enough data into the system to make it kno wthat spending a sabbatical at Google is more likely than dying there, and Moore X  X  la whas made the machines exponentially bigger and faster, and fancier algorithms allo wus todeal with bigger n -grams, maybe thisproblem will go away. working at the large scale, giving the world these amazing search engines and transla-tion aids that give human beings vastly increased access to other cultures. This is our discipline X  X equivalentinstreet-credibilitytermsofdeliveringatomicenergyandrobots on Mars.
 like getting better and better at recalling what is already well-known, and understand-ingwhat has often been said before.

Accuracy in most areas (WER in ASR, BLEU score in SMT, Eval-b for parsers) is at best linear in the logarithm of the amount of training data. Even optimistic extrapolation of current learning curves suggests truly incredible amounts of data will be needed (Lamel, Gauvain, and Adda 2002; Moore 2003; Knight and Koehn 2004; Brants et al. 2007).
 thesoftware,andhencethemorenoticeablelongrangedependencieswillbecome,and the more upset people will get if theyare deceived bya wrong analysis.
 dencies of the kind investigated above are semantically crucial. Ignoring them disrupts all the other dependencies in those examples. (They are also more frequent in genres likequestions.)Soweneedtoremember X  X ndaboveall,teachourstudents X  X hatour discipline tells us the problem is, even when it X  X  not doing much for our BLEU score.
Inthisconnection, itisencouraging toseethatmanyoftheMTpapersinthe2007ACL explicitly invoked syntax-level representations.

AI/NLP Winter, we will need to pull ourselves together as a discipline, lift our eyes above our rivalries, and take a longer view of where we are going (and where we have come from) than seems to be the current style. This will probably require a gradual move to a more considered and authoritative style of publication, with journal articles taking the place of hastily written and reviewed conference papers, as another author of thiscolumn recently suggested.
 supporting a diversity of views that transcends fashion and funding, wherever the science is good. It also means telling the public in honest terms ho wto think about whatwedo,whatcanbereliedonandwhatthereallyhardproblemsare,ingoodtimes and bad. This should not be too difficult if we keep reminding them and ourselves of the following: References
