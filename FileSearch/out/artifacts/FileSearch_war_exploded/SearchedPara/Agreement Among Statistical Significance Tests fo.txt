 Research has shown that little practical difference exists be-tween the randomization, Student X  X  paired t, and bootstrap tests of statistical significance for TREC ad-hoc retrieval ex-periments with 50 topics. We compared these three tests on runs with topic sizes down to 10 topics. We found that these tests show increasing disagreement as the number of topics decreases. At smaller numbers of topics, the randomization test tended to produce smaller p-values than the t-test for p-values less than 0.1. The bootstrap exhibited a system-atic bias towards p-values strictly less than the t-test with this bias increasing as the number of topics decreased. We recommend the use of the randomization test although the t-test appears to be suitable even when the number of topics is small.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Experimentation Keywords: Statistical significance
Information retrieval (IR) researchers rely on statistical significance tests to allow them to accurately detect and re-port significant improvements in performance. In an earlier work, we compared the randomization, bootstrap, Wilcoxon signed rank, sign, and Student X  X  paired t tests of statistical significance as applied to IR evaluation [2]. By comparing the p-values produced by various statistical tests, one can determine if a practical difference exists between tests. For example, if two tests are in close agreement across different experiments, there is no practical difference in the tests to an IR researcher.

We found that the randomization, bootstrap, and t tests all largely agreed with each other while the Wilcoxon and sign tests disagreed with each other and the three other tests. Based on these results and the fundamental properties of the tests, we recommended the use of the randomization test but noted that if the test statistic of concern was the mean (as opposed to the median, e.g.) then the t-test appeared to be rand. vs. t-test 0.007 0.009 0.011 0.018 0.037 boot. vs. t-test 0.007 0.009 0.011 0.017 0.035 boot. vs. rand. 0.011 0.014 0.017 0.026 0.051 rand. vs. t-test 0.005 0.006 0.008 0.012 0.027 boot. vs. t-test 0.008 0.010 0.013 0.020 0.041 boot. vs. rand. 0.010 0.013 0.016 0.024 0.047 Table 1: The root mean square error among the ran-domization (rand.), t-test, and the bootstrap (boot.) test X  X  p-values for pairs of TREC runs such that all three tests agree that the p-value p is  X  0.0001 (top) and 0 . 0001 &lt;p&lt; 0 . 5 (bottom). where E i is the estimated p-value given by one test and O i is the other test X  X  p-value.
Table 1 (top) shows the root mean square error (RMSE) between the three tests for different numbers of topics. These results show that all three tests largely agree with each other but as the sample size (number of topics) decreases, the agreement decreases. In line with the results found for 50 topics, the randomization and bootstrap tests agree more with the t-test than with each other.

We looked at pairwise scatterplots of the three tests at the different topic sizes. While there is some disagreement among the tests at large p-values, i.e. those greater than 0.5, none of the tests would predict such a run pair to have a significant difference. More interesting to us is the behavior of the tests for run pairs with lower p-values.

Table 1 (bottom) shows the RMSE among the three tests for run pairs that all three tests agreed had a p-value greater than 0.0001 and less than 0.5. In contrast to all pairs with p-values  X  0.0001 (Table 1 top), these run pairs are of more importance to the IR researcher since they are the runs that require a statistical test to judge the significance of the per-formance difference. For these run pairs, the randomization and t tests are much more in agreement with each other than the bootstrap is with either of the other two tests.
Looking at scatterplots, we found that the bootstrap tracks the t-test very well but shows a systematic bias to produce p-values smaller than the t-test. As the number of topics de-creases, this bias becomes more pronounced. Figure 1 shows a pairwise scatterplot of the three tests when the number of topics is 10. The randomization test also tends to produce smaller p-values than the t-test for run pairs where the t-test estimated a p-value smaller than 0.1, but at the same time, produces some p-values greater than the t-test X  X . As Figure 1 shows, the bootstrap consistently gives smaller p-values than the t-test for these smaller p-values.
While the bootstrap and the randomization test disagree with each other more than with the t-test, Figure 1 shows that for a low number of topics, the randomization test
