 one trained using data from the original feature space, the other trained with new features that are derived by clustering both the labelled and unlabelled data. Hence, unlike standard co-training ing algorithms that complement each other. 
We evaluated our method with two classifiers and three text bench-evaluation shows that our co-training technique improves text clas-sification accuracy especially when the number of labelled exam-ples are very few. 
Text classification is the problem of automatically assigning elec-tronic text documents to pre-specified categories. Typically, text belled examples are difficult and expensive to obtain, whilst unla-belled documents are plentiful and easy to obtain. 
A common method for utilising this unlabelled data for classi-create two predictors that are complementary to each other. Each train new corresponding complementary predictors. Typically, the complementarity is achieved either through two redundant views rithrns [3]. Thus, these co-training methods are applicable only to a limited class of problems. permission and/or a fee. SIGKDD 02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 -.$5.00. 
In this paper, we present a new co-training strategy for using 
The paper is organised as follows. In the next section, we de-
Various researchers have shown that unlabelled data is useful for 
Zelikovitz and Hirsh [13] follow the first approach, and use the 
The second approach of labelling part of the unlabelled data is 
Our study is based on yet another technique for labelling part of strategy where the two predictors consist of two supervised learn-ing algorithms that complement each other, i.e., they use different representations for their hypotheses and use the provided labelled data in different ways. there are a priori two redundant views of the data. Nor is there a requirement as in [3] for two different supervised learning algo-rithms that complement each other. Instead, we create new features ternate view of the labelled data. The original view and the new ing a single learning algorithm. Thus, our approach can be applied have natural redundancies. ~ym := ((xl,Yl),....,(xm,Ym)) of patterns zi E X CR n and target an alternate view of the input space by clustering the m labelled samples and the k unlabelled samples in order to create a set of new features. This view is then used for co-training. clusters [10]. The computational complexity of the algorithm on the number of samples to be clustered. Hence, if the total number set. This reduces the computational effort required when generat-ing clusters. set consisting of a large number of clusters (around ~), and many representative of the major concepts within the labelled and unla-features to the patterns in the labelled and test set: ters. where the unlabelled centroid is the average of the unlabelled pat-terns that belong to the cluster. from the full class membership information contained in the data set, and hence, has implications for co-training. partitions of the data set is SN(q + 3). 11], with a separate machine trained for each class. The output for a class is a ranked list of documents with scores allocated accord-ing to the following formula: scoresvM(d) = w.x+b, where x is the vector of features for document d, "." denotes the dot-product in features space. The vector w and the bias b are determined by minimising the functional: the feature vector for document a~ and corresponding label, where 1 indicates membership of the class and -I indicates non-membership and m is the number of training instances. The constant C controls the trade-off between the complexity of the solution and the error penalty. 
We have used our method with two different SVMs: (1) a quad-penalty machine, i.e., p = 1 (SVM t) [12]. 
In our co-training approach we distinguish predictors based on the data view that they use. Predictors trained using the word presence features are referred to as word presence (WP) classi-known as cluster feature (CF) classifiers. Co-training proceeds as follows: (1) Form a training set for each data view from the labelled ex-amples, and train a WP classifier and a CF classifier. (2) Use the CF classifier to assign labels to the unlabelled set. newly labelled cases together with the original labelled examples. (3) Similarly, use the WP classifier to form another co-training data set for the cluster features. (4) Train a co-trained word presence (WPco) classifier using the classifier using the co-training set from step 3. predicted labels for the unlabelled data set become stable. In our text classification experiments we use three distinct cor-chosen corpus and create the word presence matrix for the whole collection. The presence matrix provides the features for the word presence view. Next we create 5 partitions (S = 5) of the labelled and unlabelled computational effort during clustering. From each partition, we features used in the cluster view. Finally, for each corpus, we evaluate each model on an indepen-dent test set. Our first corpus is the WebKB data set containing 8145 web pages gathered from university computer science departments [8]. We use the four most populous categories (excluding the category 'other'): 
Course, Student, Faculty and Project, and remove all those pages that specify browser relocation only. This creates a data set containing 4108 pages. For our experiments, the web pages for the four uni-is performed using only the training and unlabelled data set. 
The second data corpus is the modApte split of the Reuters-21578 The third data set is the 20 newsgroups data set collected by Ken 
Standard text processing consistent with previous research [4, 8, 
The standard evaluation criterion for the Reuters benchmark is 
We evaluate the impact of our co-training strategy on the micro-2 a (SVM), and (2) SVM with linear penalty (SVM). For both ma-"g o.f  X  0 ~ -OA ._~ Figure 3: Relative gain in/zBP of WebKB data set as a function of the number of initial labelled examples. provement is larger when when it is most needed, i.e., when the number of labelled samples is small (e.g. over 10% improvements on avarage for SVM 2 and &lt; 50 of the labelled samples). 
The results on/JBP for 20 NewsGroups and Reuters data sets are labelled training examples consisting of 0.5% and 2% of data, re-spectively. For Reuters, the improvement in the/zBP for the 10 most the SVM 2, and from 0.63 to 0.69 for SVM 1, which translates to a gain of 12.5% and 13.7% respectively. The improvement for the a gain of 11.1% and 7.6% respectively. 
Micro-averaging the breakeven point gives equal weight to each category-document assignment decision. This averaging step can suits are shown for the 10 most populous categories (in decreasing just 48 initial labelled examples (Table l(a)). For 20 NewsGroups, the number of initial labelled examples is 200 (Table l(b)). The average number of labelled examples in the class is shown in the second column entitled nl. Results are presented for both SVM 2 and SVM 1 classifiers. 
As seen from the results in Table l(a), there is an increase in improvement with WPco and a decrease in accuracy with CFco. 
The results for the 20 NewsGroups show a similar story. When-there is always an accompanying increase in the performance of the WPco classifier. However notice that unlike the Reuters case, the 1 15.8 93 I 95 94 95 92 95 95 95 2 8.7 72 84 79 83 7 83 81 85 3 2.3 36 41 42 38 33 41 42 39 4 2.6 34 39 42 44 38 43 43 43 5 1.8 24 32 47 28 25 34 41 29 6 1.5 12 12 13 07 15 11 11 08 7 1.5 20 17 16 16 23 19 19 17 8 1.5 15 18 19 19 21 16 19 15 9 1.2 21 16 15 12 20 18 13 13 10 0.9 16 14 16 12 16 19 16 13 1 9.2 47 54 41 41 38 45 39 ] 41 2 11.0 28 28 23 23 29 31 25 26 I 3 11.8 50 60 50 57 44 54 49 52 4 19 27 32 27 30 23 32 291 31 5 11.6 40 37 27 27 39 34 27 i 27 6 12 39 54 41 50 34 50 42 46 7 8.9 62 72 60 70 60 67 60 64 8 8.6 43 28 20 24 37 29 22 19 9 12.8 65 78 59 68 61 70 56 61 10 8.1 43 64 61 64 38 63 60 64 11 13 69 78 72 80 68 79 72 78 12 14.0 73 82 70 75 69 74 70 73 13 8.8 24 19 13 13 22 17 15 14 14 9.8 35 37 26 33 32 28 29 28 15 9.6 47 59 44 52 44 50 40 46 16 9.9 59 67 59 61 53 46 50 40 17 9.4 50 55 42 $0 45 48 43 44 18 8.9 64 75 66 68 61 71 64 66 19 11 37 38 32 32 32 27 26 25 20 6.1 25 26 20 20 21 19 22 16 
As shown in Figure 4, co-training usually increases the break-(a) Class:Course (Size=24% of data (c) Class:Faculty (Size=12% ot data Number of Labelled ,Examples 0.12 0.1 .~0.~  X 0.02 ,..0.14 ~0.12 o 0.1 (a) Class:Course (Size=24% of dat~ (c) Class:Faculty (Size=12% of dat~ 
Figure 5: Classification Error for WebKB with SVM 2. 
Classifier WebKB S~IM e-WPco 13.4% SVM 1 " WPco 6.6% 
SVM e -CFco " SVM l -CFco able from the word-presence matrix. The clusters can be thought of rived from these clusters indicate the similarity of each document quality given the performance of the CF classifier which uses only the WebKB case. not available in the word-presence features. This suggests that the investigation indicate that such augmentation indeed improves the classification performance [10]. 
Creating the Final Classifier: Our experiments have shown that major improvements are obtained when the WP classifier is co-trained with the CF classifier. Thus, our ultimate co-trained clas-sifier is the WPco classifier which obtains a micro-averaged break-even point gain of over 6.6% for all data sets (Table 2). 
The marginal gain in CFco classifier is due to the fact that our labelled set. Full class membership information generated from the scores of the binary classifiers may improve the accuracy of CFco of the CFco and WPco classifiers. data with co-training. In COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan 
Kaufmann Publishers, 1998. and other Kernel Based Methods. Cambridge University 
Press, 2000. with unlabeled data. In Proceedings of the Seventeenth International Conf. on Machine Learning, pages 327-334. 
Morgan Kaufmann, San Francisco, CA, 2000. using support vector machines. In Proceedings of the 
Sixteenth International Conf. on Machine Learning, pages 200-209. Morgan Kaufmann, San Francisco, CA, 1999. P.L. Bartlett, B. Scholkopf, and D. Schuurmans (Editors). Advances in Large Margin Classifiers. MIT Press, 
Cambridge, MA, 2000., 2000. case study of SVM for information retrieval. In Proceedings of the Fourteenth Australian Joint Conference on Artificial 
Intelligence, 2001. 
International Conference on Machine Learning, pages 331-339, 1995. classification from labeled and unlabeled documents using 
EM. Machine Learning, 39(2/3): 103-134, 2000. Features for Maxirnising Text Classification Performance. In Proceedings of the Twelfth European Conference on 
Machine Learning ECMLO1, 2001. Data for Text Classification through Addition of Cluster Parameters. In Proceedings of the Nineteenth International 
Conference on Machine Learning ICML02, 2002. Support Vector Machines, Regularization, Optimization and 
Beyond. MIT Press, 2002. in the presence of background text. In Proceedings of the Tenth International Conf. on Information and Knowledge 
Management, 2001. 
