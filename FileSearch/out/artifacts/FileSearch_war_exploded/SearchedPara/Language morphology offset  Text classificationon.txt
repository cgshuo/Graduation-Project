 1. Introduction
Text classification is an information retrieval task in which textual documents are assigned labels from pre-defined categories (classes). When designing a text classification system, decisions must be made as to how to represent the text, and what feature selection method and classification algorithm to use.
The role of text representation is crucial in a text classification system because it must capture as much information as possible in a computer-comprehensible form. Text is most commonly represented as a bag-of-words in which each document is represented by a feature vector comprising term frequencies usually modified by one of the term-weighting schemes ( Salton &amp; Buckley, 1988 ). Approaches often differ on what constitutes a term in a document; most obvious choices are the surface form (as seen in text), the word X  X  lemma (its canonical form), and the word X  X  stem (the substring obtained by removal of the affixes) ( Krovetz, 1993; Nakov, 2003 ). Alternative approaches use n -grams ( Cavnar &amp; Trenkle, 1994 ) and multi-word forms ( Lewis, 1992; Turney, 1999; Vechtomova, 2005; Vechtomova, Karamuftuoglu, &amp; Robertson, 2006 ) as terms.
Although richer representations than the bag-of-words representation exist, it seems that the results do not justify the effort ( Joachims, 2002 ).

Feature selection methods are used to remove the non-informative terms (i.e., features of low discriminative power) in order to optimize classifier performance. Extensive studies that compare different feature selection although predominantly on English language data sets.
 The final decision in design of the text classification system is the choice of the classification algorithm.
Among many different classification algorithms (such as the naive Bayes classifier, k -NN, and Neural Net-works), the support vector machines (SVM) proved to be the most efficient and robust in text classification tasks ( Joachims, 2002 ).
 The information content of the bag-of-words representation is in distributions of the terms over classes.
The classes are described with their term-class distributions on the training set, which are estimated from the term frequencies. The reliability of the term-class distribution estimation is, in principle, proportional to the term frequency. The terms with the lowest frequencies of occurrence are inherently of lower discrimi-native power. For the morphologically complex languages, such as Croatian (or other Slavic languages), in which a single word may appear in over 30 different grammatical forms, the term frequencies are dispersed across different word forms thereby lowering the reliability of the estimation. Morphological normalisation is a preprocessing step that produces more compact and more reliable feature space by replacing all forms of the same word with a single representative form (the word X  X  norm). Since the frequency of a norm is equiv-alent to the sum of the frequencies of the associated word forms, more reliable distribution estimations are cution and the lower are the memory requirements.

As stated in ( Savoy, 2006 ), empirical evidence clearly shows that morphological normalisation through stemming improves information retrieval effectiveness when applied to European languages belonging to either Latin (French, Portuguese), Germanic (German), or Finno-Ugrian (Hungarian) families. Similar is known for some other languages: Finnish, Dutch, Swedish, Italian, Russian ( Tomlinson, 2003 ), and Slovenian been extensively studied; in particular, no attempt was made to quantify the performance differences with respect to the various parameters involved in the text classification process.

A literature survey shows that text classification is an extensively studied problem with majority of research based on English language. To fill this gap, in this work we compare text classification for two lan-guages of radically different morphological complexity  X  Croatian and English  X  using Croatian X  X nglish par-allel corpus. We investigate how, and to what extent, morphological complexity of the language influences
SVM classifier performance, and how different levels of morphological normalisation facilitates the feature space reduction. We performed experiments for different values of SVM parameter over a large scale of dif-normalisation. The paper is organized as follows. Section 2 describes the methods used in this work.
Experimental setting is given in Section 3 , whereas in Section 4 we present and discuss the results. Section 5 concludes the paper. 2. Methods
The majority of methods and terms used in the context of the text classification in this work are widely dis-cussed in the literature. Thus, they are only briefly described here. 2.1. Support vector machines
Support vector machines (SVM) is a machine-learning algorithm introduced by Vladimir Vapnik and his principle ( Vapnik &amp; Chervonenkis, 1974 ). The SVM has shown to be a competitive learning method on very diverse problems, including text mining ( Joachims, 1998 ).

The basic idea of SVM is to find a binary classification function f : X R set of data points (( x 1 , y 1 ), ... ,( x n , y n )) 2 ( X  X  { 1,1}) training data, where n is the number of examples, x i 2 X is the input vector, and y
Linear classification functions of the form: are the easiest to understand and the simplest to apply. However, linear separating hyperplane may not exist due to the overlapping of the classes. The solution to this problem can be formalised in a statement of the following convex optimization problem:
This is a soft margin formulation of the problem ( Cortes &amp; Vapnik, 1995 ), where n is proportional to i w i 2 , and the tolerated amount of error.

In other words, the linear function f ( x ), as a solution to the optimization problem, defines a separating hyperplane between two classes by maximizing the margin between the two opposite classes. The margin is defined as a projection of the vector between two closest points from opposite classes (e.g., between vector x , for which target value is y 1 = 1, and vector x 2 , for which target value is y the hyperplane (i.e., w / i w i ).

For high values of C , errors are strongly penalized and the algorithm attempts to find a hyperplane that per-fectly separates classes, thereby making the solution (hyperplane) more complex and the margin smaller. Lower value of C allows for some misclassifications, and at the same time larger margin and less complex hyperplane.
In general, the solution of the soft margin optimization problem from (2) involves constructing a dual prob-lem ( Cristianini &amp; Shawe-Taylor, 2000 ), where a Lagrange multiplier, a the primary problem:
The solution is of the form
Each non-zero multiplier, a i , indicates that the corresponding x tion relies on an inner product between the point x that is to be classified and the support vectors x product / ( x i ) T / ( x j ) can be expressed as K ( x i some expanded feature space. This  X  X  X ernel trick X  X  largely expands the capabilities of the SVM: mapping to a higher dimensional feature space can turn an inseparable training set into a separable one. Common non-linear kernels include polynomial, sigmoid, and radial basis function (RBF). For text classification problems, linear kernel has proven to be the most effective approach ( Joachims, 2002 ). 2.2. Feature selection
Given a training data set, the scoring measure for each unique term is computed to obtain a ranked term list. The actual feature selection is performed by selecting a predefined number of top scoring features.
A variety of feature selection methods are used in the context of text classification. In order to introduce some of them, basic variables A , B , C , and D are defined by a two-way contingency table of a term t and a class c ( Table 1 ).

Document Frequency is the number of documents in which a term occurs. It is calculated as ( Yang &amp; Peder-sen, 1997 ) tion, it is commonly defined as ( Yang &amp; Pedersen, 1997 ) where N is the total number of documents.

The information gain measures how much can be learned about the class c from the presence or absence of the term t in a document. It is calculated using the following formula ( Yang &amp; Pedersen, 1997 ): the document, and P ( a j b ) is the probability of document belonging to class a if it contains term b . the width of the margin of the resulting hyperplane. First, linear SVM is trained on a complete training set, and then only the features that correspond to highly weighted components of the normal to the resulting hyperplane are retained, whereas the others are discarded. This makes the NB feature selection method fun-damentally different from other feature selection methods that are not based on the properties of the obtained classification space. 2.3. Morphological normalisation
A single word may appear in various morphologically related word forms. There are two main branches of morphology: inflectional and derivational . The inflectional morphology describes how a basic word form (the lemma ) is transformed as a result of syntax (e.g., number and case for nouns; number, case, gender and degree the transformation of one lemma into another, possibly changing its part-of-speech. For example, suffixing cially true of suffixal derivation.

Morphological normalisation replaces the various morphologically related word forms by a single represen-tative form  X  the word X  X  norm . The process of replacing various inflectional forms of a word by its lemma is group of derivationally related lemmas by a single common lemma serving as a group representative. For not imply semantic relationship, the latter always being a matter of degree and often context-dependent.
Hence, semantic differences among derivationally related words, or among different uses of a polysemious
For morphological normalisation of Croatian language, we will consider two levels of normalisation: a malisation is followed by derivational normalisation. Consequently, the norm of a word corresponds to the lemma in the former, and to the common derivative in the latter case. In our work, the morphological nor-malisation is based on queries made to a morphological lexicon associating norms to word forms. For this pur-pose, a morphological lexicon was constructed by a rule-based automatic acquisition ( S subset of Croatian National Corpus ( Tadic  X  , 2002 ) totalling 10 tional norms and over 25,000 derivational norms. Since in Croatian language prefixal derivation changes the meaning of a word rather radically, the derivational norms are restricted to those obtainable by suffixal der-ivation. It should be noted that in this work we restrict ourselves to ambiguous normalisation in that we do not take into account the context of a word. Hence, if the word is lexically ambiguous (a homograph), it will be associated with several distinct norms.

For morphological normalisation of English language, we use the standard Porter stemming algorithm to distinguish between inflectional and derivational normalisation, the level of normalisation achieved by the
Porter algorithm may be considered roughly comparable to that of a combined inflectional and derivational normalisation of Croatian language. 2.4. Performance measures
In this work, we use the F 1 measure ( Van Rijsbergen, 1979 ) to compare performances of classifiers obtained using different feature subsets. The F 1 measure is based on two other frequently used classifier performance measures: precision and recall . The basic variables used to define these measures are introduced in Table 2 by a two-way contingency table.
 by and is estimated by
Depending on the specific goal of the classification, one of these measures are maximised. When the cost of the F w measure
The F w measure is the weighted harmonic mean of the precision and recall. For w = 0 it becomes precision, and for w = 1 it reduces to recall. The most commonly used value is w = 1, which gives an equal weight to the precision and recall. Expressed in terms from Table 2 , F
The F 1 measure is one of the standard measures by which the performance of text classification algorithms is evaluated.

In multi-class setting, the average performance of the learning algorithm over different classes can be esti-mated in two different ways: (i) by computing the average of performance measures over all classes (the macro-averaged F 1 ) from the contingency table first, and then computing the F
It should be noted that the macro-average value tends to be dominated by the classifier performance on rare classes, whereas the micro-average value is dominated by the performance on more common classes. 3. Experimental setting 3.1. Parallel corpus The parallel corpus used in this work is a part of the Croatian X  X nglish Parallel Corpus ( Tadic  X  , 2000 ).
The source is Croatia Weekly , the newspaper which was published in English and translated to Croatian from 1998 to 2000 by the Croatian Institute for Information and Culture. The published articles are in Table 3 .

The data set numbers 3580 articles from 113 issues of Croatia Weekly . Distributions of classes and publi-lion words. With stop words removed, both sets contain roughly 1 million words. 3.2. Data sets
We build two English and three Croatian language data sets ( Table 4 ) from the parallel corpus using the bag-of-words representation with TFIDF term weighted feature vectors normalised to unit length ( Salton &amp;
Buckley, 1988 ). The cro is the non-normalised Croatian data set, from which the cro-i data set is obtained by is the non-normalised English data set, from which the eng-p data set is obtained by morphological normal-isation with the Porter stemming algorithm.

Table 4 shows the dimensionality of the term space (number of features) for each data set. Terms occurring less than three times in a data set are considered unreliable features and were not used in the experiments. 3.3. Feature subsets
A feature subset is obtained from a data set by selecting a predefined number of features using one of the four feature selection methods: document frequency (DF), v lowing ranges: small feature subsets (from 10 1 to 10 2 features), medium feature subsets (from 10 features), and large feature subsets (from 10 3 to 10 4 features). 3.4. Statistical significance of classifiers performance differences
Comparison of classifiers performance with respect to F micro signed-ranks test ( Wilcoxon, 1954 ) for comparing two related samples. Hypothesis of no difference in perfor-mance between two classifiers trained on two different data sets are tested. Samples comprising 13 paired F compared) are performed for both IG and NB feature selection methods. 3.5. Implementation details
The experiments were performed using a classification algorithm implementation based on the LIBSVM next section are averaged over five cross-validation runs. 4. Results and discussion parametrization, the choice of feature selection method, the complexity of language morphology, and the effect of morphological normalisation on SVM classifier performance. 4.1. The effects of SVM parameter selection
The complexity and the performance of classifiers produced by the SVM algorithm using linear kernel ing the learning phase. We performed experiments for different values of parameter C from the set {2 2 1 , ... ,2 12 } with NB and DF feature selection methods on English and Croatian data sets. Qualitatively very similar performance was obtained on all data sets, suggesting that, in the case of Croatian and English lan-guage, the optimization of parameter C is independent of the language as well as the level of morphological normalisation.
 Fig. 1 depicts the SVM X  X  unstable range of performances for a few illustrative positive values of parameter size, hence these results are omitted). For subsets of more than 100 features, the classifiers become strongly dependent on the choice of the parameter C . Such sensitivity coincides with the minimum number of support on the complete set of features.

To our knowledge, the sensitivity of the classifier performance to the value of parameter C , as the one we have shown in Fig. 1 , has not been previously reported in the literature.

Since the classifier performs best and most stable on the whole range of feature subsets with C = 1, we use this setting in the experiments that follow. 4.2. The effects of the feature selection method improvement in classifier performance requires enlargement of the feature subset according to the ranks obtained by a feature selection method. For example, to improve F 93%, approx. 4200 features must be added. As classifier performance improves, more and more features of decreasing discriminative power must be added to further improve classifier performance. For example, to improve F micro 1 classifier performance from 93% to 93.5%, approx. 9000 features must be added. Qualitatively
Croatian and English, the choice of optimal feature selection method is independent of the language as well as the level of morphological normalisation.
 The pairwise comparison of F micro 1 classifier performance for three feature subset size ranges is given in
Table 5 . The IG and CHI feature selection methods exhibit very similar performance over the whole range of feature subsets, and perform better (up to 12.44%) than the NB or the DF for small feature subsets. On medium feature subsets, the IG feature selection method performs best (up to 2.61%). The NB feature selec-tion method slightly outperforms (up to 1.23%) all other considered methods on large feature subsets. It per-complete set of features used for the actual ranking of the NB features. The worst overall performance is obtained using the DF feature selection method; this result was expected since it has been previously reported in the literature ( Forman, 2003; Yang &amp; Pedersen, 1997 ).

Since IG and NB feature selection methods outperform other considered methods, in our further experi-ments we focus on these methods only. 4.3. The effects of morphology and normalisation
Morphological normalisation reduces the feature space by reducing morphologically caused feature redun-dancy in text classification. In principle, the more morphologically complex the language, the higher the mor-phologically caused feature redundancy would be. Subsequently, the greater the would be feature space reduction achieved by morphological normalisation.

The morphological complexity of Croatian language is the main cause for the large number of features in the from 11,500 to 9000 ( eng / eng -p = 1.28), reflecting the morphological simplicity of the English language. subsets obtained using the IG and the NB feature selection methods, respectively. General observation is that the differences in classification performance among the various data sets  X  although we are dealing with two the number of features increases.

Table 6 presents statistically tested differences in classification performance for the IG feature selection method. On non-normalised data sets, the classifier performs better on eng data set (up to 2.17%), however, the difference decreases as the number of features increases. Inflectional morphological normalisation of cro data set improves classifier performance (up to 2.18%). Additional derivational normalisation results in small improvements (up to 0.76%). Altogether, morphological normalisation on Croatian language improves clas-sifier performance (up to 2.94%) on small feature subsets, whereas for medium and large feature subsets there are only small improvements. Morphological normalisation on eng data set results in rather small improve-ments in classifier performance (up to 0.65%), which is expected due to simplicity of English language mor-phology. Consequently, benefits of morphological normalisation are greater for Croatian than for the
English language on small and medium feature subsets. On large feature subsets, notwithstanding the difference in levels of morphological complexity, the improvements due to normalisation  X  although statisti-cally significant  X  are small, and may well be negligible in practice.

Table 7 presents statistically tested differences in classification performance for the NB feature selection method. Comparison with Table 6 reveals that the NB feature selection method is more sensitive to morpho-eng and cro data sets are greater (up to 4.37%), and so are the improvements in classifier performance due to morphological normalisation (up to 5.25% for Croatian and 1.04% for English). The earlier conclusions regarding the IG feature selection method pertain to the NB feature selection method as well, the most impor-tant being that on large feature subsets the improvements due to morphological normalisation is small for both languages.

The overall effect of morphological normalisation used in conjunction with the IG and NB feature selection ratio (reduction ratio) between Croatian and English normalised and non-normalised data sets on which the
F 1 classifier performance is the same. For example, in case of IG feature selection method, twice as many time, one and a half time more features are needed for the eng than the for the eng-p data set to achieve per-formance of 93%. The improvement in classifier performance is achieved by adding increasingly more features, ing that the discriminative power of highly ranked normalised features is on average greater than that of equally ranked non-normalised features. As classifier performance increases, the difference between the dis-criminative power of the newly added normalised and non-normalised features increases in favor of the for-mer, but then starts to decrease gradually. When the newly added normalised and non-normalised features are of equal discriminative power, the reduction ratio reaches the maximum. After that point, the remaining nor-malised features are of less or equal discriminative power than the remaining non-normalised features, hence the reduction ratio drops as the classifier performance increases further.
 As the eng/eng-p curve in Fig. 5 demonstrates, in case of IG feature selection method the reduction ratio for
English is approx. 1.2, up to the point where classifier performance is 92%. This means that for each norma-lised feature on average 1.28 non-normalised features must be added to the feature subset to achieve the same performance. This ratio is approximately the same as the reduction that is achieved by morphological normal-increases and reaches the maximum of 1.55 at F micro 1  X  93 : 25 % .

The likely reason for this is that word forms of a single English word tend to be of comparable discriminative power, whereas in Croatian a single word may have a large number of word forms, only some of which are of significant discriminative power. The maximal reduction ratio on Croatian data set is around 2.0, achieved at
F 1  X  91 % with inflectional normalisation and at F micro 1
Fig. 6 shows the reduction ratio achieved by morphological normalisation used in conjunction with the NB feature selection method. As noted earlier, the NB feature selection method is more sensitive to the morpho-logical complexity of the language than the IG feature selection method. Because of this, in case of the Cro-atian data set, the non-normalised features ranked high by the NB feature selection method are of less discriminative power than the non-normalised features ranked high by the IG feature selection method. Con-sequently, the reduction ratio on the Croatian data set is higher at the beginning (approx. 1.9), and then lower at the beginning than for the IG feature selection method (approx. 1.2), but increases more rapidly as classifier performance improves. The maximum reduction ratio on Croatian data set is 2.25 at
F 1  X  92 : 75 % , whereas on English data set it is 1.65 at F 5. Conclusion In this paper we quantify the effects of morphological complexity of a language in text classification using
SVM. The issue is examined through a number of classification experiments scaled over SVM parameter C , of morphological normalisation. The Croatian X  X nglish parallel corpus provided a basis for direct comparison of classification performances for languages of different levels of morphological complexity. The main results of the paper can be summarized as follows:
Classifier performance is highly sensitive to C value on medium and large feature subsets (more than 100 features). In this range, classifiers with C = 1 perform best for both Croatian and English language, regard-less of the level of morphological normalisation.
 For both Croatian and English languages, and regardless of the level of morphological normalisation, the
IG feature selection method performs best on small and medium feature subsets, whereas the NB feature selection method performs best on large feature subsets. In contrast to the IG feature selection method, the NB method is more sensitive to the morphological complexity of the language.

The improvements in classifier performance due to morphological normalisation is greater for Croatian than for English language on small and medium feature subsets (less than 1000 features) for both IG and NB feature selection methods, whereas on large feature subsets the improvements are rather small and may be negligible in practice. In Croatian language, the improvements due to additional derivational normalisation over only inflectional normalisation may also be negligible in practice.

Morphological normalisation used in conjunction with the IG or NB feature selection methods facilitates the feature space reduction. This is because the discriminative power of highly ranked normalised features is on average greater than that of equally ranked non-normalised features, so less number of normalised fea-tures are needed than non-normalised features in order to achieve the same classifier performance. The maximum feature space reduction achieved for Croatian language is around 55%, whereas for English lan-guage it is around 40%.

Classifier performance is generally better on English than on Croatian data sets despite morphological normalisation.
 Our experiments have shown that for large feature subsets, morphological normalisation of Croatian and
English results in rather small improvements in SVM classifier performance, which may well be negligible in practice. The most important benefit of morphological normalisation is the significant dimensionality reduc-tion for small and medium sized feature sets, more emphasized for Croatian language due to its higher mor-phological complexity.
 Acknowledgements The authors would like to thank Marko Tadic  X  for his support and for making the Croatian X  X nglish Parallel
Corpus available to them. The authors would also like to thank two anonymous reviewers for helpful comments cation and Sports, Republic of Croatia, under the grants No. 036-1300646-1986 and 098-0982560-2563. References
