 Feature construction is one of the major research topics for supporting classifica-tion tasks. Based on a set of original features, the idea is to compute new features that may better describe labeled sample s such that the predictive accuracy of classifiers can be improved. When consid ering the case of 0/1 data (i.e., in most of the cases, collections of attribute-value pairs that are true or not within a sample), several authors have proposed to look at feature construction based on patterns that satisfy closedness-related constraints [1,2,3,4,5,6]. Using pat-terns that hold in 0/1 data as features (e.g., itemsets or association rules) is not new. Indeed, pioneering work on classific ation based on association rules [7] or emerging pattern discovery [8,9] have gi ven rise to many proposals. Descriptive pattern discovery from unlabeled 0/1 data has been studied extensively during the last decade: many algorithms have been designed to compute every set pat-tern that satisfies a given constraint (e.g., a conjunction of constraints whose one conjunct is a minimal frequency constraint). One breakthrough into the compu-tational complexity of such mining tasks has been obtained thanks to condensed representations for frequent itemsets, i.e., rather small collections of patterns from which one can infer the frequency of many sets instead of counting for it (see [10] for a survey). In this paper, we consider closure equivalence classes, i.e., frequent closed sets and their generators [11]. Furthermore, when considering the  X  -free itemsets with  X &gt; 0 [12,13], we can consider a  X  X ear equivalence X  per-spective and thus, roughly speaking, the concept of almost-closed itemsets. We want to contribute to difficult classific ation tasks by using a method based on: (1) the efficient extraction o f set patterns that satisfy given constraints, (2) the encoding of the original data into a new data set by using extracted patterns as new features. Clearly, one of the technica l difficulties is to discuss the impact of the intrinsic properties of these patterns (i.e., closedness-related properties) on a classification process.

Our work is related to pattern-based classification. Since [7], various authors have considered the use of association rules. These proposals are based on a pruned set of extracted rules built w.r.t. support and confidence ranking. Differ-ences between these methods mainly come from the way they use the selected set of rules when an unseen example x is coming. For example, CBA [7] ranks the rules and it uses the best one to label x . Other algorithms choose the class that maximizes a defined score ( CMAR [14] uses combined effect of subsets of rules when CPAR [15] uses average expected accuracy of the best k rules). Also, starting from ideas for class characterization [16], [17] is an in-depth formalization of all these approaches. Another related resear ch stream concerns emerging patterns [18]. These patterns are frequent in samples of a given class and infrequent for samples from the other classes. Several algorithms have exploited this for fea-ture construction. Some of them select essential ones ( CAEP classifier [8]) or the most expressive ones ( JEPs classifier [9]). Then, an incoming example is labeled with the class c which maximizes scores based on these sets. Moreover, a few researchers have considered condensed re presentations of frequent sets for fea-ture construction. Garriga et al. [3] have proposed to characterize a target class with a collection of relevant closed itemsets. Li et al. [1] invoke MDL principle and suggest that free itemsets might be better than closed ones. However, classifica-tion experimental results to support such a claim are still lacking. It turns out that the rules studied in [17] are based on 0-free sets such that a minimal body property holds. The relevancy of such a minimality property is also discussed in terms of  X  X ear equivalence X  in [19]. In [2], we have considered preliminary results on feature construction based on  X  -freeness [12,13]. Feature construction approaches based on closedness properties differ in two main aspects: (i) mining can be performed on the whole database or per class, and (ii) we can mine with or without the class labels. The pros and cons of these alternatives are discussed in this paper.

In Section 2, we provide more details on state-of-the-art approaches before introducing our feature construction method. Section 3 reports on our experi-mental results for UCI data sets [20] and a real-world medical database. Section 4 concludes. A binary database r is defined as a binary relation ( T , I ,R )where T is a set of objects (or transactions), I is a set of attributes (or items) and R  X  X   X I . The frequency of an itemset I  X  X  in r is freq ( I,r )= | Objects ( I,r ) | where Objects ( I,r )= { t  X  X  | X  i  X  I, ( t, i )  X  R } .Let  X  be an integer, an itemset I is said to be  X  -frequent if freq ( I,r )  X   X  .

Considering that  X  X hat is frequent may be interesting X  is intuitive, Cheng et al. [4] brought some evidence to support such a claim and they have linked frequency with other interestingness measures such as Information Gain and Fis-cher score. Since the number of frequent itemsets can be huge in dense databases, it is now common to use condensed represen tations (e.g., free itemsets, closed ones, non derivable itemsets [10]) to save space and time during the frequent itemset mining task and to avoid some redundancy.
 Definition 1 (Closed itemset). An itemset I is a closed itemset in r iff there is no superset of I with the same frequency than I in r , i.e., I  X  I s.t. freq ( I ,r )= freq ( I,r ) . Another definition exploits the closure operation cl : P ( I )  X  X  ( I ) . Assume that Items is the dual operator for Objects : given T  X  X  , Items ( T,r )= { i  X  X | X  t  X  T, ( t, i )  X  R } , and assume cl ( I,r )  X  Items ( Objects ( I,r ) ,r ) :theitemset I is a closed itemset in r iff I = cl ( I,r ) . Since [11], it is common to formalize the fact that many itemsets have the same closure by means of closure equivalence relation .
 Definition 2 (Closure equivalence). Two itemsets I and J are said to be equivalent in r (denoted I  X  cl J )iff cl ( I,r )= cl ( J, r ) .Thus,a closure equiva-lence class (CEC) is made of itemsets that have the same closure, i.e., they are all supported by the same set of objects ( Objects ( I,r )= Objects ( J, r ) ). Each CEC contains exactly one maximal itemset (w.r.t. set inclusion) which is a closed itemset. It may contain several minimal itemsets which are 0-free itemsets according to the terminology in [12] (also called key patterns in [11]). Example 1. Considering Tab. 1, we have r =( T , I ,R ), T = { t 1 ,...,t 6 } ,and I = {
A, B, C, D, c 1 ,c 2 } , c 1 and c 2 being the class labels. For a frequency threshold  X  =2,itemsets AB and AC are  X  -frequent. ABCc 1 is a  X  -frequent closed itemset. Considering the equivalence class C = { AB, AC, ABC, ABc 1 ,ACc 1 ,ABCc 1 } , AB and AC are its minimal elements (i.e., they are 0-free itemsets) and ABCc 1 is the maximal element, i.e., one of the closed itemsets in this toy database. 2.1 Freeness or Closedness? Two different approaches for feature const ruction based on condensed represen-tations have been considered so far. In, e.g ., [1,5], the authors mine free itemsets and closed itemsets (i.e., CECs) once the class attribute has been removed from the entire database. Other proposals, e.g., [3,4], consider (closed) itemset mining from samples of each class separately.

Looking at the first direction of research, we may consider that closed sets, because of their maximality, are good candidates for characterizing labeled data, but not necessarily suitable to predict classes for unseen samples. Moreover, thanks to their minimality, free itemsets might be better for predictive tasks. Due to closedness properties, every itemse t of a given closure equivalence class C in r covers exactly the same set of objects. Th us, free itemsets and their associated closed are equivalent w.r.t. interesting ness measures based on frequencies. As a result, it is unclear whether choosing a fr ee itemset or its clos ure to characterize a class is important or not. Let us now consider an incoming sample x (test phase) that is exactly described by the itemset Y (i.e., all its properties that are true are in Y ). Furthermore, assume that we have F  X  Y  X  cl ( F, r )where F is a free itemset from the closure equivalence class C F . Using free itemsets to label x will not lead to the same decision th an using closed itemsets. Indeed, x  X  F and it satisfies rule F  X  c while x cl ( Y, r ) and it does not satisfy rule cl ( F, r )  X  c . Following that direction of work, Baralis et al. have proposed classification rules basedonfreeitemsets[17].

On the other hand, for the  X  X er-class X  approach, let us consider w.l.o.g a two-class classification problem. In suc h a context, the equivalence between free itemsets and their associated closed ones is lost. The intuition is that, for a given free itemset Y in r c 1  X  X atabase restricted to samples of class c 1  X  X nd its closure X = cl ( Y, r c 1 ), X is more relevant than Y since Objects ( X, r c 1 )= Objects ( Y, r c 1 )and Objects ( X, r c 2 )  X  Objects ( Y, r c 2 ). The closed itemsets (say X = cl ( X, r c 1 )) such that there is no other closed itemset (say X = cl ( X ,r )) for which cl ( X, r c 2 )= cl ( X ,r c 2 ) are chosen as relevant itemsets to characterize c 1 . In some cases, a free itemset Y could be equivalent to its closure X = cl ( Y, r c 1 ), i.e., Objects ( X, r c 2 )= Objects ( Y, r c 2 ). Here, for the same reason as above, a free itemset may be chosen instead of its clos ed counterpart. Note that relevancy of closed itemsets does not avoid conflict ing rules, i.e., we can have two closed itemsets X relevant for c 1 and Y relevant for c 2 with X  X  Y .

Moreover, these approaches need for a po st-processing of th e extracted pat-terns. Indeed, we not only look for closedness-related properties but we have also to exploit interesting measures to keep only the ones that are discriminating. To avoid such a post-processing, we propose to use syntactic constraint (i.e., keeping the class attribute during the mining phase) to mine class-discriminant closure equivalence classes. 2.2 What Is Interesting in Closure Equivalence Classes? In Fig. 1, we report the different kinds of CECs that can be obtained when considering class attributes during the mining phase.

These CECs have nice properties that are useful to our purpose: since associ-ation rules with a maximal confidence (no exception, also called hereafter exact rules) stand between a free itemset and its closure, we are interested in CECs whose closure contains a class attribute to characterize classes. Thus, we may neglect Case 1 in Fig. 1.
 Definition 3 (Association rule). Given r = {T , I ,R } ,an association rule  X  on r is an expression I  X  J ,where I  X  X  and J  X  X \ I .The frequency of the rule It provides a ratio about the numbers of exceptions for  X  in r .When J turns to be a single class attribute,  X  is called a classification rule .
 From Case 3 (resp. Case 4), we can ext ract the exact classification rule  X  3 : L are interested in exact rules only, we also neglect Case 2: L 1 C is a free itemset and it implies there is no exact rule I  X  J such that I  X  J  X  L 1 C .Thus,weare interested in CECs whose closed itemset contains a class attribute and whose free itemsets (at least one) do not contain a class attribute. This also leads to a closedness-related condensed representation of Jumping Emerging Patterns [21]. Unfortunately, in pattern-based classification (a fortiori in associative classifica-tion), for a given frequency threshold  X  , mining exact rules is restrictive since they can be rare and the training datab ase may not be covered by the rule set. In a relaxed setting, we consider associ ation rules that en able exceptions. Definition 4 (  X  -strong rule,  X  -free itemset). Let  X  be an integer. A  X  -strong rule is an association rule of the form I  X   X  J which is violated in at most  X  objects, and where I  X  X  and J  X  X \ I .Anitemset I  X  X  is a  X  -free itemset iff there is no  X  -strong rule which holds between its proper subsets. When  X  =0 ,  X  is omitted, and we talk about strong rules ,and free itemsets .
 When the right-hand side is a single item i ,sayingthat I  X   X  i is a  X  -strong rule in r means that freq ( I,r )  X  freq ( I  X  X  i } )  X   X  . When this item is a class attribute, a  X  -strong rule is called a  X  -strong classification rule [16]. The set of  X  -strong rules can be built from  X  -free itemsets and their  X  -closures. Definition 5 (  X  -closure). Let  X  be an integer. The  X  -closure of an itemset I on Once again, when  X  =0 , cl 0 ( I,r )= { i  X  X | freq ( I,r )= freq ( I  X  X  i } ) } and it corresponds to the closure operator that we already defined. We can also group itemsets by  X  -closure equivalence classes :two  X  -free itemsets I and J are  X  -equivalent ( I  X  cl  X  J )if cl  X  ( I,r )= cl  X  ( J, r ) .
 The intuition is that the  X  -closure of a set I is the superset X of I such that every added attribute is almost always true for the objects which satisfy the properties from I :atmost  X  false values (or exceptions) are enabled. The computation of every frequent  X  -free set (i.e., sets which are both frequent and  X  -free) can be performed efficiently [13]. Given threshold values for  X  (frequency) and  X  (free-ness), the used AC like 1 implementation outputs each  X  -free frequent itemset and its associated  X  -closure. Considering Table 1, a frequency threshold  X  =3 and a number of exceptions  X  =1,itemset C is a 3-frequent 1-free itemset ; items B and c 1 belong to its  X  -closure and  X  : C  X   X  c 1 is a 1-strong classification rule. 2.3 Information and Equivalence Classes We get more information from  X  -closure equivalence classes than with other approaches. Indeed, when considering contingency tables (See Tab. 2), for all the studied approaches, f  X  1 and f  X  0 are known (class distribution). However, if we consider the proposals from [3,4] bas ed on frequent closed itemsets mined per class, we get directly the value f 11 (i.e., freq ( X  X  c, r )) and the value for f 01 can be inferred. Closure equivalence classes in [5] only inform us on f 1  X  (i.e., freq ( X, r )) and f 0  X  . In our approach, when mining  X  -frequent  X  -free itemsets whose closure contains a class attribute, f 1  X   X   X  and we have a lower bound f other bounds for f 01 and f 00 2 .

Moreover,  X  -frequent  X  -free itemsets, bodies of  X  -strong classification rules are known to have a minimal body property. Some constraints on  X  and  X  can help to avoid some of the classification conflicts announced at the end of Section 2.1. Indeed, [16] has shown that setting  X   X  [0;  X / 2 [ ensures that we can not have two classification rules  X  1 : I  X   X  c i and  X  2 : I  X   X  c j with i = j s.t. I  X  J .This constraint also enforces confidence to be greater than 1 2 .Furthermore,weknow that we can produce  X  -strong classification rules that exhibit the discriminant power of emerging patterns if  X   X  [0;  X   X  (1  X  | r c i | | r | )[, r c i being the database restricted to objects of the majority class c i [6]. One may say that the concept of  X  -frequent  X  -free itemsets (  X  = 0) can be considered as an interestingness measures (function of  X  and  X  ) for feature selection. 2.4 Towards a New Space of Descriptors Once  X  -frequent (  X  )-free itemsets have been mine d, we can build a new represen-tation of the original database using these new features. Each selected itemset I will generate a new attribute NewAtt I in the new database. One may encode NewAtt I to a binary attribute, i.e., for a given object t , NewAtt I equals 1 if I  X  Items ( t, r ) else 0. In a relaxed setting and noise-tolerant way, we propose to compute Newatt I as follows: This way, I is a multivalued ordinal attribute. It is obvious that for an object the proportion of items i  X  I that describe t . We think that multivalued encoding  X  X ollowed by an entropy-based su pervised discretization step 3  X  should hold more information than binary encoding. Indeed, in the worst case, the split will take place between p  X  1 p and 1, that is equivalent to binary case; in other better cases, split may take place between j  X  1 p and j p ,1  X  j  X  p  X  1 and this split leads to a better separation of data. The frequency threshold  X  and the accepted number of exceptions  X  are impor-tant parameters for our Feature Construction (FC) proposal. Let us discuss how to set up sensible values for them. Extreme values for  X  bring either (for low-est values) a huge amount of features  X  X ome of which are obviously irrelevant X  or (for highest values) not enough features to correctly cover the training set. Furthermore, in both cases, these solutions are of limited interest in terms of Information Gain (see [4]). Then,  X  varies from 0 to  X   X  (1  X  | r c i | r )tocapture discriminating power of emerging patterns. Once again, lowest values of  X  lead to strong emerging patterns but a potentially low coverage proportion of data and features with high values of  X  lacks of discriminating power.
Intuitively, a high coverage proportion implies a relatively good representation of data. In Fig. 2, we plotted proportion of the database coverage w.r.t.  X  for a given frequency threshold. Results for breast , cleve , heart and hepatic data (from UCI repository) are reported. We easily observe that coverage proportion grows as  X  grows. Then, it reaches a saturation point for  X  0 which is interesting: higher values of  X &gt; X  0 are less discriminant and lower values  X &lt; X  0 cover less objects. In our following experiments, we report (1) maximal accuracies over all  X  and  X  values (denoted Max ), and (2) average accuracies of all  X  values with  X  =  X  0 (denoted Av ).
To validate our feature construction ( FC ) process, we used it on several data sets from UCI repository [20] and a real-world data set meningitis 4 .Wehave been using popular classification algorithms such as NB and C4.5 on both the original data and the new representation based on extracted features. As a result, our main objective criterion is the accuracy of the obtained classifiers.
Notice that before performing feature construction, we translated all attributes into binary ones. While the translation of nominal attributes is straightforward, we decided to discretize continuous attr ibutes with the entropy-based method by Fayyad et al. [22]. Discretizations and classifier constructions have been per-formed with WEKA [23] (10-folds stratified cross validation).
We report in Tab. 3 the accuracy results obtained on both the original data and its new representation. NB , C4.5 classifiers built on the new representation often perform better (i.e., it lead to hi gher accuracies) than respective NB and C4.5 classifiers built from the original data. One can see that we have often (12 times among 15) a combination of  X  and  X  for which NB accuracies are improved by feature construction (column Max ). And this is experimentally always the case for C 4 . 5. Now considering average accuracies (column Av ), improvement is still there w.r.t. C4.5 but it appears less obvious when using NB .

Then, we also compared our results with state-of-the-art classification techniques: FC &amp; NB is compared with other bayesian approaches, LB [24] and BCEP [25]. When accessible, accuracies were repo rted from original papers within Tab. 4. Then,wehavecompared FC&amp;C4.5 with other associative classification approaches, namely CBA [7], CMAR [14], CPAR [15], and an EPs-based classifier SJEP-classifier [26]. Accuracy results for associative cla ssifiers are taken from [14]. Others results are taken from the published papers. FC allows to often achieve better accuracies than the state-of-the-art classifiers, e.g., FC &amp; C4.5 wins 9 times over 15 against CPAR , 8 times over 13 against CMAR , 10 times over 15 against CBA when considering average accuracies (column Av ). Considering optimal  X  and  X  values (column Max ), it wins 10 times over 15 (see bold faced results). We study the use of closedness-related condensed representations for feature construction. We pointed out that diff erences about  X  X reeness or closedness X  within existing approaches come from the way that condensed representations are mined : with or without class label, per class or in the whole database. We proposed a systematic framework to construct features. Our new features are built from mined (  X  )-closure equivalence classes  X  more precisely from  X  -frequent  X  -free itemsets whose  X  -closures involve a class attribute. Mining these types of itemsets differs from other approaches since (1) mined itemsets hold more in-formation (such as emergence) and (2) th ere is no need for post-processing the set of features to select interesting fea tures. We also proposed a new numeric encoding that is more suitable than binary encoding. Our FC process has been validated by means of an empirical evaluation. Using C4.5 and NB on new rep-resentations of various datasets, we demonstrated improvement compared with original data features. We have also shown comparable accuracy results w.r.t. efficient state-of-the-art classification techniques. We have now a better under-standing of critical issues w.r.t. feature construction when considering closedness related properties. One perspective of this work is to consider our FC process in terms of constraints over sets of patte rns and its recent formalization in [27]. Acknowledgments. The authors wish to thank B. Cr  X  emilleux for exciting dis-cussions and the data set meningitis . They also thank J. Besson for technical support during this study. Finally, this work is partly funded by EU contract IST-FET IQ FP6-516169.

