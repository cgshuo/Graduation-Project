 Result diversification is an effective method to reduce the risk that none of the returned results satisfies a user X  X  query intention. It has been shown to decrease query abandonment substantially. On the other hand, computing an optimally diverse set is NP-hard for the usual objectives. Existing greedy diversification algorithms require random access to the input set, rendering them impractical in the context of large result sets or continuous data.

To solve this issue, we present a novel diversification ap-proach which treats the input as a stream and processes each element in an incremental fashion, maintaining a near-optimal diverse set at any point in the stream. Our approach exhibits a linear computation and constant memory com-plexity with respect to input size, without significant loss of diversification quality. In an extensive evaluation on several real-world data sets, we show the applicability and efficiency of our algorithm for large result sets as well as for continuous query scenarios such as news stream subscriptions. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval ALGORITHMS,PERFORMANCE,EXPERIMENTATION Approximation, Diversification, Large sets, Streams
A given user query often has a variety of intended mean-ings or associated information needs. To address this phe-nomenon, result diversification has been introduced as a technique to reduce the risk that none of the returned items matches the user X  X  information need [25]. A good result set balances relevance and diversity to optimize the probabil-ity of user satisfaction. While diversification is an effective means to cater for diverse query interpretations, it is also computationally expensive. To compute an optimally di-verse result subset from an input set of relevant items is NP-hard [1, 12]. Therefore, existing approaches use approx-imation techniques and heuristics to increase the efficiency of diversification [9, 29, 24, 26, 12]. While the computational complexity of state-of-the-art approximation algorithms is usually linear to the input size, they rely on random access to items of the input set. To allow for high performance, the entire input set is hold in main memory, imposing sub-stantial memory requirements on systems processing many user queries in parallel. Further, these algorithms cannot be applied for diversification of result streams, such as results for continuous queries in databases [6, 2], publish/subscribe systems [15, 14] or on Semantic Web data [3]. Diversification of results is impossible for systems that potentially process thousands of queries in parallel when they use algorithms with super-linear runtime and linear memory requirements.
To solve this issue, we present our streaming-based ap-proach which processes items incrementally, maintaining a near-optimal diverse set at any point in time. This approach has linear computation and constant memory complexity with respect to input set size, and naturally fits for stream-ing applications. It can be used to streamline existing diver-sification approaches, such as MaxMin and MaxSum [12, 9, 10]. Though we reduce memory consumption without compensating with more computation, our evaluation shows that diversification quality does not suffer, while efficiency is increased by orders of magnitude for large sets and streams.
In particular, this work makes the following contributions: 1. We formally define the task of stream diversification , 2. We propose an incremental diversification framework 3. Incremental algorithms for the MaxSum and MaxMin 4. While substantially improving diversification efficiency,
The remainder of this paper is organized as follows. In t he next section, we discuss existing work on set and stream diversification. In Section 3, we provide the formal founda-tion of diversification. Section 4 presents our main contri-bution, the incremental diversification framework and two incremental algorithms based on this framework, including a complexity analysis. Their quality and performance is evaluated in Section 5 and compared to state-of-the-art di-versification algorithms. Section 6 concludes our work.
Diversification is studied in many different fields such as biology, ecology, psychology, linguistics, and economics [17]. In media, it manifests itself in a number of dimensions like sub-topic structure, style of writing, sentiment, and ideo-logical perspectives [18]. In the context of Information Re-trieval, these aspects should not be regarded as noise or con-tradictions, but rather as a rich source of information [11].
In [28], Zhai and Lafferty model the Information Retrieval task as a statistical decision problem with the aim to mini-mize the risk of loss. Here, loss describes how much a result fails to satisfy a user X  X  information need . Since this loss is al-ways connected to the relevance of the results, the choice of a specific loss function allows to model different approaches in result set optimization. Traditional Information Retrieval approaches are modeled by a function in which the loss is independently estimated for each document. Diversification can be introduced by choosing a function where loss of one document depends on other documents in the result set.
Several such diversification objectives have been proposed and successfully validated. The use of Maximum Marginal Relevance (MMR) was proposed by Carbonell and Gold-stein [4], and also used in Zhai et al. [27]. Chen and Karger [5] introduced the k -call at n metric: it is 1 if at least k of the top n documents in a result set are deemed relevant, oth-erwise 0. The aim is to achieve a high average k -call over multiple queries. The choice of a small k leads to higher diversification, a higher k favors relevance. Gollapudi and Sharma [12] express their diversification objectives in terms of facility dispersion optimization problems. In this work, we use their MaxSumDispersion and MaxMinDispersion al-gorithms as baseline for evaluation. In contrast to [12], only the diversified set is required to be in main memory and the input items are processed incrementally. This allows us to increase the efficiency for large item sets without significant loss of quality, as we show in section 5. Our streaming-based approach is not limited to these two diversification objectives, though, and is independent of any particular rel-evance and distance measures.

Document similarity is not the only possible source for a diversification objective. Another established approach is to view each document as covering a subset of all possible aspects of the query input [27, 7, 21]. The topical similarity based on the distance in taxonomies (categorical distance) is also used for diversifying product search [12] as well as for book, bookmark, and movie recommendations [29, 26]. Also click rates [23] and query reformulation [21] are valuable sources for effective diversification of Web search results.
As search engine query logs show, there is a fair number of news-related queries [19], suggesting that blog search users have an interest in the blogosphere response to news stories as these develop. To the best of our knowledge, in the con-text of diversification, this need has only been addressed by Drosou and Pitoura [9]. They propose algorithms that diver-sify a stream of results using a jumping window approach. The authors argue that an incremental computation of a di-verse set of a stream is not possible. While this is true for the optimal diverse set, we show that an incremental heuris-tic approximation is very well possible. Their SGC algo-rithm  X  a heuristic variant based on a sliding window  X  is the most similar one in the literature compared to our incre-mental approach. However, our  X  X indow X  always starts at the beginning of the stream, providing a diverse set of the entire history, rather than a fixed number of recent items. This allows our algorithm to actively withdraw items from the diverse set to improve its diversity, whereas SGC simply drops diverse items as they leave the window.

Diversification of search results is further considered in the domain of database search. A clustering and weighted sampling approach is used in [24] to obtain a diverse sub-set of results. Greedy algorithms are used in [13] in a similar way as presented here. However, the author does not focus design and evaluation on the streamline character of diver-sification, and does not consider input streams that are not ordered by relevance (e.g., from continuous queries).
In this section, we formally define the problem of diversi-fication and provide the notation for the rest of this paper, mostly following [12]. We first define the classical Set Di-versification problem, and then extend this notion to the problem of Stream Diversification to cover diversification of continuous data.

For a given set of results U : | U | = N (e.g., search results, news or blog articles matching a user query) we aim to find the set S k  X  U : | S k | = k that is both relevant and diverse. Relevance of a result measures the probability that the user is actually interested in that result ( w : U  X  R + ) 1 , whereas diversity reflects dissimilarity ((1 -similarity), also denoted distance) of one result to another ( d : U  X  U  X  R + ). To simplify the presentation, we assume in the following that d is a metric. Usually, relevance and diversity are conflict-ing objectives: when the top-k most relevant results do not constitute the maximal diverse set, diversity can only be in-creased by replacing a top-k result with a less relevant but more dissimilar one. This obviously reduces the overall rel-evance of the set. The tradeoff between the two measures is captured in a diversification objective , a function over a set of items that consolidates the relevance of results w ( ) and the diversity between pairs of results d ( , ) into a single measure: where 2 U denotes the power set of U .

Having the diversification objective at hand, we can de-fine the diversification problem as finding the k -size random subset S k  X  2 U that maximizes the diversification objective:
Wh ile result set U and relevance function w depend on the user query, the explicit introduction of the query itself into this diversification model complicates the notation (cf. [12]). This optimization is known to be NP-hard [12, 1]. As the n umber of candidate sets |{ S k  X  2 U : | S k | = k }| is an exhaustive exploration of all sets to find the optimal solu-tion is impractical even for small k  X  10 as soon as N &gt; 20. Therefore, all proposed diversification approaches use greedy algorithms and/or heuristics to approximate the optimal so-lution S  X  k by a near-optimal S + k .

Throughout the literature of diversification, there are two popular diversification objectives: Sum 2 and Min [12, 9]. As the names imply, they are based on the sum or minimum of the two measures w and d . Formally, Sum is defined as The first term in Equation 3 sums the relevance scores for each result, the second term sums the distances of all pairs ( i, j )  X  S k  X  S k : i 6 = j . Since the first sum runs over k values and the second over k ( k  X  1), this imbalance is compensated by multiplying the first term with k  X  1. In case of d ( , ) being a metric [12, Eq. 1], it suffices to sum over all k ( k  X  1) u nique unordered pairs and multiply the second term by 2.
The parameter  X  allows to balance the influence of rele-vance and diversity on the objective value. A  X  &lt; 1 . 0 in-creases the contribution of relevance, whereas  X  &gt; 1 . 0 favors item dissimilarity. In order to consider both cases, we use  X  = 0 . 5 , 1 . 0 , 2 . 0 in our evaluation.

The Min objective only considers minimum relevance and minimum pairwise distance: The diversification problems of maximizing the Sum and Min objectives are referred to as MaxSum and MaxMin . These problems can be cast as facility dispersion problems as shown in [12], for which approximation algorithms exist. We briefly summarize these algorithms in the following.
MaxSumDispersion composes the diverse set by select-ing k 2 p airs of most distant items, with distance defined as d ( i, j )  X  = w ( i ) + w ( j ) + 2  X d ( i, j ). If k is odd, the k is randomly added to the set. Finding the pair of most dis-tant items is of time complexity O ( N 2 ), thus this algorithm belongs to the complexity class O ( N 2 k ).

Similarly, MaxMinDispersion initializes the diverse set with the pair of most distant items, where distance is defined i that maximizes min j  X  S d  X  ( i, j ). After k  X  2 iterations, the diverse set is constructed. Again, finding the pair of most distant items requires O ( N 2 ) distance computations, while the k  X  2 iterations demand O ( N k ) operations, summing up to O ( N 2 + N k ). As described in [9], the MaxMinDisper-sion algorithm can be improved: instead of selecting the initial two most distant items from the entire input set, it can selected from a uniformly random sample set of size h ( h  X  N ). This has only a small impact on the diversification quality [8], but reduces the complexity to O ( N k + h 2 ).
T he Avg objective bases on average relevance and average pairwise distance. For a fixed k , it is proportional to the Sum objective and is not separately considered in this work.
Since both algorithms require random access to the in-put set, it is usually held in memory, yielding to memory complexity of O ( N ). This makes the application of these al-gorithms practically infeasible for large sets. In case of data streams, the algorithms need to be executed repeatedly as new items arrive, causing an even higher computational ef-fort. In the following Section 4, we present our incremental approach that solves these issues.

As an extension to the set diversification problem , we consider the items of the set to be a sequence or stream S = ( s 1 , . . . ) in the order as retrieved by the respective IR system. For a given query, a traditional document retrieval system provides documents in descending order of relevance. A news or blogs publish/subscribe system provides results matching a continuous query as they are recognized by the system, i.e., ordered by time. We define the diversification problem over a stream S at each position i in the stream as where S i = { s j : j  X  i } is the set of the items at positions [1 , i ]. In other words, for each position i in S , a diversified set S  X  k,i is defined over the subsequence S i = ( s 1 , s It can be seen that the set diversification problem is a special case of stream diversification, by viewing the input set U as stream S = ( s 1 , . . . , s N ). Then, In contrast to our stream diversification problem, [9, 10] consider a fixed window of recent items for the diversification task. This, for instance, poses a problem for items like news articles or blogs that are not published at a fixed rate. Then, at two different points in time the window covers different periods of time. Further, picking the window size represents an additional optimization problem.

Diversification along a time-ordered stream of results may lead to diversified k -size sets that primarily contain  X  X ld X  but relevant and highly diverse results while new, similar results are omitted. Such a behavior can be compensated by introducing a decay factor to the relevance function which lowers the item score as its age increases [9]. Function w is the relevance of item s j at time i (i.e., when item s observed): sults X  relevances, while t ( s j , s i ) = max( t ( s i )  X  t ( s vides the time difference between result s i and s j if the point in time t ( s i ) &gt; t ( s j ), or 0 otherwise.
The following section describes our incremental diversifi-cation framework . It is composed of a generic incremental diversification algorithm that employs a general incremental objective model to optimize computation. This algorithm ef-ficiently and effectively produces diverse k -size sets in linear time (for fixed k ) and over streams in constant time (at each position) with respect to input size. Based on this frame-work, we develop incremental algorithms for the MaxSum and MaxMin diversification objectives.
The core idea of our incremental diversification algorithm is to process the input as a stream of items and to con-tinuously maintain a diverse subset at each position of the stream. Let S + k,i  X  1 be a near-optimal diverse k -element sub-set of ( s 1 , . . . , s i  X  1 ). Then, its successor S + as the set with the highest objective among all candidate sets S k,i that can be created by replacing at most one element Formally, this can be defined as To put it differently, two consecutive sets S + k,i  X  1 and S with i  X  [ k + 1 , N ] relate to each other as if a modification took place, otherwise S + k,i = S + k,i  X  1 first k items, S + k,i is trivally defined as S + k,i = S i  X  k with S k, 0 =  X  .

Algorithm 1 specifies this approach in pseudo-code. At the end of each iteration i over [1 , N ], the algorithm provides the diverse set S + k,i . In case of streams, this algorithm can continue computation as soon as a new item arrives. Algorithm 1 : Incremental Diversification Algorithm Input : Set of items { s 1 , . . . , s N } , size of diverse set k
Output : diverse set of items S + k,N for i  X  1 to N do
Additionally, this algorithm can be modified in line 10 t o minimize the objective value, rather than maximizing it. This allows to also use the algorithm along with all those diversification objectives that are to be minimized, for in-stance the MaxCov objective [22].

An important characteristic of this algorithm is that the objective value is computed in the inner loop for k candidate sets S  X  k,i that, compared to the outcome set of the previous iteration, mutually differ in only one item. In the follow-ing, we show how the objective computation can further be optimized by exploiting this fact.

The objective value of a set S + k,i is computed based on the known objective value of S + k,i  X  1 . Since both sets relate to each other as defined by Equation 9, only the relevance scores of item s j (removal) and s i (addition), as well as the distances d ( s j , s l ) and d ( s i , s l ) : s l  X  S + considered for computation. The incremental diversification algorithm computes the objective of different S + k,i by sub-sequently varying s j (denoted in line 8 as S  X  k,i ). Therefore, each distance d ( s i , s l ) is reused k  X  1 times in the inner loop and does not need to be recomputed.
The core idea of the incremental Sum objective function exploits the fact that a sum s = P n i =1 a i can  X  instead of recomputing it  X  be updated when one value a j : j  X  [1 , n ] gets replaced by a  X  as s  X  = s + a  X   X  a j . Equivalently, f in Equation 3 with symmetric distance can be incrementally computed as: f where f Sum ( ) is a short hand notation for f Sum ( , w ( ) , d ( , )).
As the incremental algorithm computes the objective of k different candidate sets S  X  k,i with only s j varying, the algo-rithm keeps all distances between the elements of S + k,i  X  1 a matrix D = ( d m,n ) with d m,n = d ( s ( m ) , s ( n )) :  X  n, m = 1 , . . . , k . The function s ( ) provides the element of the di-verse set that corresponds to the index [1 , k ] of this matrix and the following vectors. The symmetry of d ( ) leads to a symmetric matrix D . During the construction of the initial set (line 3 of the algorithm), this matrix is built up for the first k items. From then on, it is updated after the inner loop, as we show later.

The distance between the new item s i and the items in k  X  1 times for each s ( l ). Therefore we define the vector vector is computed once before the inner loop. We define the sum over all these distances as  X  k,i = P k n =1 d n . sums of matrix D :  X  m,i  X  1 = P k n =1 d m,n : m = 1 , . . . , k . Then, Equation 10 can be rewritten as
In case S + k,i differs from S + k,i  X  1 (after the inner loop), the following updates are required: Vector  X  k,i is derived from  X  k,i  X  1 for all l = 1 , . . . , k as where j refers to the index of s j as s ( j ) = s j . Then, matrix D is updated as d l,j = d j,l = d l : l = 1 , . . . , k , and function s ( ) is modified so that s ( j ) = s i . With this strategy, our in-cremental Sum objective achieves a computation complexity of O (1), while requiring O ( k 2 ) memory. distances basic items basic
The Min objective defined in Equation 4 bases on the minimum relevance and pairwise distance. Therefore, the objective of each S  X  k,i can be computed with the new mini-mum relevance and distance values of S + k,i  X  1 and item s
Similarly to the incremental Sum objective, the pairwise distances between s i and s j  X  S + k,i  X  1 are computed once before the outer loop of the incremental algorithm and are reused to compute the objective of the candidate sets:
With an ordered vector of all relevances of the diverse set, Equation 13 can be computed in O (1) by comparing the first element (if the respective relevance does not belong to s j ) or the second element (otherwise) of the vector with w ( s i ). Equation 14 can be evaluated by finding the smallest distance among d n,m and d l with n, m, l  X  [1 , k ] that does not belong to s j . This can be done by a two-way merge [16, Sec. 5.2.4] over the ordered vector of all d n,m , the ordered vector of all d l , and the ordered vector of d j,m . The result distance is the minimum of the former two vectors that does not appear in the latter one (thus it does not belong to s ). This operation has a worst-case complexity of O ( k ), occurring when the k smallest pairwise distances of S + k,i  X  1 belong to s j and all distances to s i are larger than these. In the best case, when the smallest pairwise distance of S + does not belong to s j and is larger than the smallest d l has complexity O (1). The complexities of our incremental objectives for Max-Sum and MaxMin were briefly outlined before. We now analyze the total complexity of our algorithm in detail. The complexities of all algorithms are summarized in Table 1.
The computation and memory complexities are each con-sidered to be compound. The computation contains distance computations and basic arithmetic operations. The former may involve several of the latter, potentially rendering dis-tance computations to be of an order of magnitude slower. Memory consumption is considered a conjunction of mem-ory for items (to be diversified) and memory for basic data types such as double precision floating point values. Again, the former can be orders of magnitude larger than the latter.
As one can see from the pseudo-code, the incremental di-versification algorithm has a complexity of O ( N k ) set mod-ifications and objective computations. In case of MaxSum-Incremental , the structures D , d ,  X  j,i  X  1 , and  X  k,i constructed and updated incrementally outside of the in-ner loop, requiring O ( k ) operations for each item. As we have shown, the computation of the objective itself is O (1). Thus, the total computational complexity of MaxSumIn-cremental is still O ( N k ).

The objective computation of MaxMinIncremental has complexity of O ( k ) in worst-case and O (1) in best-case. The removal and addition of an element from and to the ordered relevance and distances vectors each requires O (log k ) op-erations. These operations are performed before and after the inner loop, leading to a total complexity of O ( N k 2 O ( N k log k ), respectively. For both objectives, the memory requirements for maintaining the distance matrix dominates the overall memory complexity, leading to O ( k 2 ). Note that k is usually rather small, according to the  X  X ess is more X  [5] motto. Thus, our algorithm is essentially linear in the num-ber of items, with a small constant factor determined by k .
We presented a generic incremental diversification frame-work, as well as two specific incremental diversification al-gorithms: MaxMinIncremental and MaxSumIncremen-tal . As shown, computation and memory complexity is re-duced compared to their baseline algorithms MaxMinDis-persion and MaxSumDispersion , at the cost of giving up approximation guarantees. Therefore, the main goal of the experimental evaluation is to show that for real-world data, this does not lead to a decrease in diversification quality for sets and streams. In addition, we measure the performance characteristics of our algorithms and the baseline to confirm analytical results presented in Section 4.4.
To show the wide applicability of our algorithms, we evalu-ated them on three well-known data sets. We opted for data sets that provide us with large result sets and streams  X  the challenge our incremental diversification algorithms tackle. We used these data sets and queries for evaluation: MaxSum MaxMin F igure 1: Diversification quality compared to baseline M NYTimes The New York Times Annotated Corpus 3 con-Blogs08 The TREC Blogs 2008 data set 4 provides over 28 DBpedia The DBpedia data set v3.5.1 7 is a semantic data
T he New York Times Annotated Corpus: http://corpus.nytimes.com/ .
TREC Web Corpus  X  BLOGS08: http://ir.dcs.gla.ac.uk/test_collections/blogs08info.html .
TextCat Library: http://textcat.sourceforge.net/ .
Topics for opinion search and polarity task (topics 851 X 950, 1000 X 1050): http://trec.nist.gov/data/blog08.html . DBpedia: http://dbpedia.org/ .

These three data sets are used to generate realistic result sets and streams to put our diversification algorithms under real workload. With a result set ordered by relevance we face the set diversification task, ordered by time we tackle the stream diversification task.

The diversification tasks we are focusing on require rel-evance and distance functions, which are to be defined for each data set individually.

NYTimes/Blogs08 The two document-centric data sets were indexed using the Java Lucene 8 library along with the Snowball stemmer 9 . We use Lucene X  X  normalized relevance score as relevance function. Based on the cosine similarity CosineSim ( , ) of the term frequency vectors, the distance between items is defined as d ( s i , s j ) = 1  X  CosineSim ( s
DBpedia The semantic data set is indexed and queried with Java Sesame 10 . After querying, results referring to the same things but with varying values for the attributes were consolidated into a single result with multi-value attributes.
A pache Lucene: http://lucene.apache.org/java/ .
Snowball Stemmer: http://snowball.tartarus.org/ .
OpenRDF Sesame: http://www.openrdf.org/ . MaxSum MaxMin M
Following [20], we define the distance between two results s and s j as a multi-type-multi-value distance measure: where the query contains the attributes a = ( a 1 , . . . , a s ( a l ) refers to the multi-value for attribute a l of result s Further, the distance d l ( v i , v j ) between multi-value v v of result s i and s j for attribute a l is defined as d l ( v i = s i ( a l ) , v j = s j ( a l )) = 1 | v which corresponds to the average pairwise distance between the values of result s i and s j for attribute a l . The dis-tance between empty multi-values is defined as 1. Obviously, this definition considers two sparse results with disjoint sets of non-empty attributes to be maximally distant (diverse). Consequently, the diversification would favor sets of sparse results, which is unlikely to satisfy users. Therefore, we define relevance as the fraction that reflects the number of attributes of the query that are covered by the result (non-empty). A result that provides values for all attributes ob-tains relevance 1, whereas results with only a few attribute values get assigned a relevance close to 0. The diversification objective that balances diversity and relevance also balances between sparse and non-sparse results.

The result distance measure can employ different distance measures for different attributes. We use cosine similarity over term frequency vectors for textual values and the abso-lute difference between numerical values, normalized by the maximum range of the respective attribute.

Our evaluation targets at effectiveness and efficiency of our incremental algorithms compared to baseline algorithms.
Effectiveness Usually, when evaluating diversification ap-proaches, the challenge is to measure result set diversity. Clearly, this measurement must be independent from the diversification objective, which is maximized. A number of established diversity-aware evaluation measures for search approaches exist, e.g.,  X  -NDCG [7], NDCG-IA [1], and S-recall [27].

These measures require sub-topic annotations or relevance judgments which are not available for result set sizes in the order of 10 4 that we evaluate. Fortunately, in our case it is not necessary to directly evaluate the quality of the diversification, because our work approximates already es-tablished diversification approaches with validated objective functions [12], and focuses on gains in efficiency, not in di-versification quality. Therefore, we only need to assess the approximation quality to validate the effectiveness of our algorithms. For that purpose, a direct comparison of the achieved objective scores instead of using one of the above mentioned quality measures is appropriate.

For comparison, we run each algorithm for all queries, and measure the objective score of the diverse sets. These scores are then normalized to the interval [0 , 1] by division by the maximal possible objective value. The difference of an algorithm X  X  and its baseline X  X  normalized objectives renders the absolute objective difference (AOD) measure. These are averaged over all queries and used to quantify the loss of quality introduced by streamlining. An AOD value of  X  0 . 1 expresses an improvement / impairment of 10% within the normalized interval of [0 , 1].

Efficiency The algorithm efficiency was measured as fol-lows. The queries were evaluated against the data sets and all results were fetched into memory. Then, the algorithms MaxSum MaxMin F igure 4: Absolute objective improvements of M AX S UM (top) and M AX M IN (bottom) for the Blogs08 data the choice of  X  . On the other hand, the respective differences for Min to the baseline algorithm are significantly affected.
For  X  = 0 . 5, the relevance-only reference Top achieves Sum objective scores comparable to the baseline, and for Min its score is even higher. As expected, for higher  X  values the Top scores deteriorate soon. We also observe that our incremental algorithms are able to even diversify the top-k results for  X  &lt; 1 . 0 and always outperform the other variants (except for one case with Min at k = 50 and  X  = 2 . 0). The runtime of our experiments is depicted in Figure 3. Each query is represented by a corresponding dot. In addi-tion, we show fitted regression curves according to Table 1 for these results. Note that results are shown with log-log scales. Here, polynomial complexity classes are displayed as straight lines, with varying slopes according to the degree of complexity, i.e., O ( N 2 ) has a steeper slope than O ( N ).
All experimental results correspond to the analytical pre-dictions presented in Section 4.4. We see that the extracted queries yield result set sizes from 100 up to 10,000 docu-ments. As expected, processing times of the non-incremental algorithms grow quadratic with this set size.

Computing the distance values for our document collec-tions NYTimes and Blogs08 is equally fast for all documents. In contrast, we observe a high variance for the DBPedia col-lection (Figure 3, right column). This can be explained by characteristics of the distance function used for the struc-tured data. The performance of this function depends on the number of attributes which varies by an order of magni-tude in our query set. Regardless of this variance, the fitting curve matches the analytical curves for this data set, too.
Already for N = 1,000 and k = 10, MSDisp requires one minute to compute the diversified set. In contrast, our in-cremental variant needs only  X  100 milliseconds for this set size. MMInc has slightly higher response times than MSInc , due to the additional k factor in objective computation.
We now evaluate our algorithms against the stream diver-sification task. The experimental setting mimics a stream of results as retrieved from a news or blogs publish/subscribe-system, for example an RSS stream. In contrast to the set-based experiments, items are ordered by timestamp.
With Equation 5, we define the task of stream diversifica-tion as finding at each position of the stream a diverse subset of all items that were observed until that position. As the baseline algorithm, we take all these observed results as a set and obtain the diverse set with the set-based baseline algo-rithms. This computation turned out to be very expensive. We were unable to process the baseline for streams larger than 10,000 documents while collecting a sufficient amount of measurement points. For that amount, the diversification with k = 10 required nearly one hour.

Additional to this baseline, we evaluated a window-variant following the work of Drosou and Pitoura [9] on stream di-versification, denoted as MSDispWin and MMDispWin , respec-tively. We show the results for a jumping window of 100.
As before, we include two reference algorithms for com-parison purposes. The Last reference algorithm proposes the k most recent items from a stream as the diverse set, whereas TopRel picks the k most relevant items from the stream, ignoring diversity completely.

Optimally, we would compute the diverse set after each new item. However, this is prohibitively expensive for the baseline. Therefore, we distributed ten measuring positions logarithmically over the stream, and computed the results for all algorithms at these positions.

Results. Figure 4 shows the diversification quality for stream data, computed on Blogs08, our largest data set. To evaluate the influence of item relevance decay, the ex-periments were performed without decay (left column) and with a half-life over 100 k (middle) and 1 k (right) items. As expected, Last yi elds very low objective scores, and TopRel is only better for low  X  values. MSInc / MMInc and MSDispWin / MMDispWin achieve diversification comparable to the baseline, with a slight advantage for our variants.
The baseline algorithm is unusable in practice to perform stream diversification, as mentioned above. MSDispWin and MMDispWin require on average 250 ms and 80 ms per step. MSInc and MMInc update the diverse set for each incoming item in virtually no time ( &lt; 1 ms time measure accuracy).
We conclude that the windows-based and the incremental variants efficiently allow for stream diversification. MSInc and MMInc have the advantage that diversification is imme-diately available for each new incoming item; this would be substantially more expensive for MSDispWin and MMDispWin .
Diversification is an important strategy to improve user satisfaction in the presence of ambiguous or broad queries. Due to their linear memory complexity, current approxi-mation algorithms cannot be used to diversify large sets of items, or a large number of sets concurrently. Our new incre-mental approach is linear in computational complexity and constant in memory complexity w.r.t the number of items, and therefore also very well suited for very large sets. As a framework for incremental diversification, our approach is applicable to a wide range of diversification objectives. A thorough evaluation based on three real-world data sets shows that such an incremental computation exhibits the same quality as the baseline algorithm. On the other hand, computation and thus query response time is sped up by several orders of magnitude for sets of a few thousand items and more. In addition, our streaming-based computation enables very efficient diversification of queries over contin-uous data sources as well, such as news streams or tweets, opening up further applications of diversification. This work was supported by the EU FP7 Integrated Project LivingKnowledge (Contract No. 231126). Experiments were conducted on the high performance computing cluster of the Regional Computing Centre for Lower Saxony (RRZN). [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [2] S. Babu and J. Widom. Continuous queries over data [3] D. F. Barbieri, D. Braga, S. Ceri, E. D. Valle, and [4] J. G. Carbonell and J. Goldstein. The use of MMR, [5] H. Chen and D. R. Karger. Less is more: probabilistic [6] J. Chen, D. J. DeWitt, F. Tian, and Y. Wang.
 [7] C. L. Clarke, M. Kolla, G. V. Cormack, [8] M. Drosou and E. Pitoura. Comparing diversity [9] M. Drosou and E. Pitoura. Diversity over continuous [10] M. Drosou, K. Stefanidis, and E. Pitoura.
 [11] F. Giunchiglia. Managing Diversity in Knowledge. In [12] S. Gollapudi and A. Sharma. An axiomatic approach [13] J. R. Haritsa. The KNDN problem: A quest for unity [14] V. Hristidis, O. Valdivia, M. Vlachos, and P. S. Yu. A [15] U. Irmak, S. Mihaylov, T. Suel, S. Ganguly, and [16] D. E. Knuth. Sorting and Searching , volume 3 of The [17] D. G. McDonald and J. Dimmick. The [18] E. Minack, G. Demartini, and W. Nejdl. Current [19] G. Mishne and M. de Rijke. A study of blog search. In [20] T.-W. Ryu, , T. wan Ryu, and C. F. Eick. A unified [21] R. L. T. Santos, C. Macdonald, and I. Ounis.
 [22] D. Skoutas, E. Minack, and W. Nejdl. Dealing with [23] A. Slivkins, F. Radlinski, and S. Gollapudi. Learning [24] E. Vee, U. Srivastava, J. Shanmugasundaram, P. Bhat, [25] J. Wang and J. Zhu. Portfolio theory of information [26] C. Yu, L. Lakshmanan, and S. Amer-Yahia. It takes [27] C. Zhai, W. W. Cohen, and J. D. Lafferty. Beyond [28] C. Zhai and J. D. Lafferty. A risk minimization [29] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
