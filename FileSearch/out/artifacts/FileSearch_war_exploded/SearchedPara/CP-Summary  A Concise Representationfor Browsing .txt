 This paper tackles the problem of summarizing frequent itemsets. We observe that previous notions of summaries cannot be directly used for analyzing frequent itemsets. In order to be used for analysis, one requirement is that the analysts should be able to browse all frequent itemsets by only having the summary.

For this purpose, we propose to build the summary based upon a novel formulation, conditional profile (or c-profile ). Several features of our proposed summary are: (1) each pro-file in the summary can be analyzed independently, (2) it provides error guarantee (  X  -adequate), and (3) it produces no false positives or false negatives.

Having the formulation, the next challenge is to produce the most concise summary which satisfies the requirement. In this paper, we also designed an algorithm which is both effective and efficient for this task. The quality of our ap-proach is justified by extensive experiments.

The implementations for the algorithms are available from www.cais.ntu.edu.sg/~vivek/pubs/cprofile09 .
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms frequent itemset, summarization, concise representations, conditional profile
The frequent itemset mining (FIM) problem aims to enu-merate all frequent itemsets, which are defined as sets of items that appear frequently in the database. Frequent itemsets have been found useful in many data mining ap-plications, such as association rule mining [3], classification [12], clustering [2], indexing [18], and feature selection [5]. However, despite their prowess, there is one subtle limitation of frequent itemsets regarding their abundant size. Due to their combinatoric nature, the number of frequent itemsets can easily reach thousands or even millions, and hence don X  X  lend themselves to human analysis.

This limitation has motivated the popular research area of concisely representing frequent itemsets. However, many previous works merely perform compression (archiving) or indexing instead of summarization . While archives and in-dexes are concise and contain similar information as the original data, they are not human interpretable. In order to analyze frequent itemsets, the analyst should be able to somehow see or browse the frequent itemsets [10]. This re-quirement constitutes our objective, i.e., to build a concise representation (summary) of the frequent itemsets, which allows analysts to browse all frequent itemsets and their sup-port.

Based on this objective, we study the criteria of good sum-maries for analysis, and postulate some requirements: 1. The summary should provide ease of analyzing the 2. The summary must be adequate [13]. In other words, 3. In order to be interpretable, the size of the summary
In this paper, we propose a summarization model called cp-summary , based on conditional-profiles ,or c-profile in short. In brief, c-profile is an itemset profile [17], extended with a base specifying the condition where the respective summary is applicable. The cp-summary consists of a list of c-profiles , each of which encodes several frequent itemsets in a natural manner. This concept is illustrated in Figure 1, and formalized in Section 3.

In order to produce the intended summary, we also design a systematic algorithm, which achieves following results: Figure 1: Illustration of cp-summary .Each c-profile encodes several frequent itemsets along with the support.
We also conducted extensive experimental evaluations which justify the effectiveness and the efficiency of our approach using both real and synthetic datasets.
The rest of this paper is organized as follows. Section 2 discusses the related work. Section 3 presents the problem definition and notations. CP-Summary , our novel proposed form of summary, is explained in Section 4, followed by the algorithm to construct it in Section 5. The goodness of the algorithm is then analyzed using empirical evaluations in Section 6. Finally, section 7 concludes this paper with future research directions.
Several approaches have been proposed to concisely rep-resent frequent itemsets. First, we name Maximal Frequent Itemset (MFI) [7], which is the set of frequent itemsets with no frequent supersets, and Spanning Set [1] which is the set of K itemsets, that can cover as many frequent itemsets as possible. These approaches do not satisfy our requirement since they do not encode the support of itemsets. A similar case exists with Representative patterns (RP) [16], which is a set of representative frequent itemsets, such that all fre-quent itemsets are similar to at least one representation. The problem is, since we do not know which pattern repre-sents each frequent itemset, the support of frequent itemsets cannot be derived as well.

Example 1. Consider that we have two representative pat-terns, a (with absolute support 200) and abc (100). Observe that it is impossible to derive the support of itemset ab ,since it can be either similar to 100 (represented by abc ) or 200 (represented by a ).

There are also several techniques that relate the summa-rization problem with information theory. In other words, as long as the original information can be accurately de-rived, then the summary is perfect. Examples of representa-tions within this class are NDI [4], generalized disjunction-free pattern [11], and Markov Random Field (MRF) [15]. These variants require complicated logical (and probabilis-tic in case of MRF) inference, to derive the support of item-set, by means of inclusion-exclusion principle and solving a system of linear equations.

Example 2. Consider following summary (NDI), and note the difficulty to see the support of itemset abc :
With regard to the ease of interpreting the summary, we have Closed Frequent Itemsets [14] and  X  -CFI [6]. A fre-quent itemset is considered a CFI (or  X  -CFI)ifnoneofits supersets has the same (or respectively, similar )supportas it. In most cases, CFI is not concise enough, and  X  -CFI does not provide the desired error guarantee. Moreover, the num-ber of  X  -CFI is still lower bounded by that of MFI, which itself may be in the order of thousands.

Eliminating these candidates, we left with the profile-based approaches [17, 8]. In these approaches, the sum-mary contains a list of profiles , which is formed as Bernoulli distribution vector of items, which assumes independence between items. This representation should be sufficiently easy to observe, since the support of an itemset is simply the product of each items X  probability, and the weight of the vector. Being easily interpretable, profile-based summariza-tion also allows accurate support estimation by using small number of profiles. To build the profile, Yan et al. [17] do clustering with respect to KL-divergence, and estimate the profile using MLE principle. This algorithm is then en-hanced in [8] by realizing that the problem to find the best profile can be transformed into a regression problem.
However, the profile-based approach has several limita-tions: Issue 1: Dependence on the whole profiles. Using this approach, an itemset may appear in many profiles. Yan et al. [17] propose to estimate the support as the maximum es-timation from all profiles, Jin et al. [8] propose to calculate the average of them. Note that an itemset can be badly es-timated by some profiles even though the overall estimation is good. (Violate Requirement 1).
 Issue 2: Lack of error guarantee. Having the number of profile, K , as input, both approaches attempt to minimize the total estimation errors of the support of frequent item-sets. The consequence is that, some itemsets can be very badly estimated, even though the average errors are low. (Violate Requirement 2).
 Issue 3: False Positives. Both techniques focus only on frequent itemsets. Neither do they consider the possibil-ity of infrequent itemsets to be marked as frequent (false positives), nor do they differentiate between frequent and infrequent itemsets. (Violate Requirement 2).

These claims are empirically validated in Section 6.2. A transactional database consists of a set of transactions T over a set of items I . For clarity, each item is denoted as lower-case characters a, b, c, ... , and an itemset (set of items) is denoted by a sequence of characters, e.g., abc .The abso-lute support of an itemset X is the number of transactions containing X . The ratio of absolute support to the num-ber of transactions | T | is called relative support ,whichis denoted as sup ( X ). Unless explicitly mentioned, the term support relates to relative support. An itemset X is consid-ered frequent if sup ( X )  X   X  , for a user-specified threshold  X  .

As mentioned in the introduction, our proposed form of summary consists of a set of c-profile , which is the modi-fication of itemset profile [17]. For easier presentation, we first define the problem with respect to itemset profile ,or just profile .An itemset profile is formed as Bernoulli distri-bution vector, which consists of a weight and a set of items along with the probability distribution.
 Definition 1. [Itemset Profile] [17] .Anitemsetprofile P =( w : { &lt; i, p &gt;, ... } ), consists of a weight w and a set of item-probability pairs ( i, p ). The set of all items included in aprofile P is denoted as I P , and the probability distribution of item a  X  I P in P is denoted as p P ( a ). This profile covers itemsets { X | X  X  I P } .

The support of itemset X observed from a profile P ,de-noted as  X  sup P ( X ), is calculated as: ers itemset a, b, ab , and estimates their supports as  X  sup 0 . 5  X  0 . 9=0 . 45,  X  sup P ( b )=0 . 5  X  0 . 8=0 . 4, and  X  sup 0 . 5  X  0 . 9  X  0 . 8=0 . 36 respectively.

From Requirement (2), we need to accurately estimate the support of all frequent itemsets. Here, we formulate the support restoration error as:
This formula represents the re lative error, ranging from 0 to 1. Relative error is desirable since it is independent of the absolute value, and the usual relative errors, 1  X  X   X  sup ( X ) can go beyond 100%. Moreover, this error function is more lenient compared to the conventional one.

An itemset X is considered to be accurately estimated if it is not mis-represented (neither false negative nor false posi-tive), and in case it is frequent, its support restoration error is within a specified tolerance parameter  X  . Mathematically, X is accurately estimated if: j
Definition 2. [Valid Profile]. Aprofile P is valid if all itemsets X covered by P are accurately estimated . Next, we define the notion of valid summary as:
Definition 3. [Valid Summary]. A valid summary of a database D is a set of valid profiles { P 1 ,P 2 , ... } , such that each frequent itemset in the database is covered by at least one valid profile.

Based on the definition, a valid summary produces no false negatives or false positives. Finally, we define itemset summarization problem as:
Definition 4. [Itemset Summarization Problem] .Given a transactional database, the minimum support, and the er-ror tolerance, return a valid summary with as few profiles as possible.
One requirement of a valid summary is that every profile in it has to be valid. However, it is very difficult, and even impossible in most cases, to build a valid summary using itemset profile .Foraprofile P to be valid, all itemsets cov-ered by P have to be accurately estimated. From Definition 2, while summarizing itemset X , all subsets of X must be accurately estimated, meaning all items in X must be inde-pendent of the others. Such a case is very rare in practice, yielding an impossibility to build a valid summary.
To resolve this issue, we extend the definition of profile, by appending an itemset base B .Aprofile B | P covers item-sets containing B and a subset of B  X  I P , or mathematically: {
X | B  X  X  X  B  X  I P } . We observe that some items in I P could be closures 1 of B , of which the probability distribution is useless to mention, i.e., adding such an item to itemset X does not change the support. Therefore, we add another term C (closure), representing a set of items which are clo-sures of B . We denote this kind of profile as conditional profile ,or c-profile , which is formally defined as follows.
Definition 5. [Conditional Profile] . A conditional pro-file CP is of the form B [ C ] s | P ,where B is the itemset base, C is the set of closures of B , s is the support of B ,and P is the itemset profile. CP covers all itemsets {
X | B  X  X  X  B  X  C  X  I P } . The support of the base, i.e., the  X  s  X  X art of CP , is denoted as s CP .

The support of itemset X observed from a c-profile CP , denoted as  X  sup CP ( X ), is calculated as:  X  sup CP ( X )= item a  X  X is a closure of itemset X if sup ( X )= sup ( Xa ) Figure 2: Tree-structure of a sample cp-summary .
 The summary contains 5 c-profiles ,wherethe base s are  X  , a , b , c ,and ab .

Example 4. Let the conditional profile CP = B [ C ] s | P = all itemsets { X | pq  X  X  X  pqabxy } , for example pqa , pqxab , and pqxy . The estimated support of itemset pqa , X  sup CP 0 . 6  X   X  sup P ( a )=0 . 6  X  0 . 45 = 0 . 27. Similarly,  X  sup 0 . 6  X   X  sup P ( ab )=0 . 216, and  X  sup CP ( pqxy )=0 . 6.
As opposed to pattern profile that requires all items in the profile to be independent, here we require all items in P to be conditionally independent with respect to B , while items in B do not need to be independent. Therefore, our proposed model of summary, cp-summary is a set of c-profile sinstead of itemset profiles.

Our objective is to summarize itemsets into c-profile .For the sake of presentation, we define another term called X -compressible itemset, as follows.
 Definition 6. [ X -Compressible Itemset]. An itemset Y is X -compressible if Y can be compressed into one valid profile with base X . Formally, Y is X -compressible , X  X  , if there is a valid c-profile CP = B [ C ] s | P where B = X and I P  X  C = Y .

For this purpose, all items in Y must be conditionally independent with respect to X .
To simplify the algorithm design procedure hence allow-ing efficient solution, and to improve the interpretability, we require every c-profile to have a distinct base ,andthe bases should form a prefix-tree. This constrained problem will produce a summary with a tree-structure, which is il-lustrated in Figure 2. Having these two restrictions, we can use an algorithm similar to Tree-Regression [8] to build the summary.
 This algorithm is presented in Algorithm 1. Function Build ( B, Tail ) (line 8) produces a summary (list of c-profiles) for all frequent itemsets { X | B  X  X  X  B  X  Tail } . Function Split ( B, Tail ) (line 1) finds the best splitting item a ,and then recursively splits the itemsets to be summarized, de-pending on whether or not they contain item a . Function BuildCProfile ( B, Tail ) (line 12) returns a valid c-profile with base B and itemset Tail . This is always possible since item-set Tail is B -compressible (Definition 6) (line 11). Starting with the entire set of items and an empty base (correspond-ing to all frequent itemsets), this algorithm repeatedly splits the frequent itemsets until they are compressible. Note that each split is independent of the others, hence this algorithm can be easily parallelized. Regardless of the choice of the splitting item a , this algorithm always produces a valid sum-mary .
 Algorithm 1 : Framework for Itemset Summarization
C-Profiles Split (itemset B ,itemset Tail ) 1
C-Profiles Build (itemset B ,itemset Tail ) 8
C-Profiles Summarize 17 (a) Seeking the best split-ting item a
Lemma 1. Algorithm 1 produces a valid summary, re-gardless of the choice of the splitting item a .

Proof. By definition, if a profile is valid, then all fre-quent subsets of the profiles are correctly generated. Since the first recursion call uses all the items as the parameter, then all frequent itemsets are covered. By induction, P 1 ers all itemsets containing item a ,and P 2 covers all itemsets not containing item a .Hence, P 1 and P 2 are disjoint and complete, implying that the algorithm produces a valid sum-mary.

This algorithm consists of two sub-problems, which are: (1) to find the best splitting item a (lines 1 X 7), and (2) to verify whether Tail is B -compressible and if so, to build the profiles (find the probability distributions) (lines 11 and 12).
One subproblem is to find the best splitting items, which can be illustrated by Figure 3 (a). The na  X   X ve (brute force) solution is to try all possible items in each recursion call, and pick the one that produces the least number of profiles. This algorithm, though optimal, is not feasible, as shown by Lemma 2.

Lemma 2. Brute force algorithm has complexity  X ( | FI | + 2
F | ) ,where I F is the set of all frequent items. Algorithm 2 : Another perspective for splitting function
C-Profiles Split (itemset B ,itemset Tail ) 1
Proof. Observe that any frequent itemset can be a base of a profile. With respect to one base, let I B = { x | B is frequent } ,and Tail be any subset of I B . Therefore, the overall state space is hence the state space for that particular base is 2 | I F
Jin et al. [8] proposed to use a greedy strategy, by choos-ing an item that results in the l east restoration errors, if we summarize each partition with one profile. However, this approach is not promising for our problem. Consider a case where more than two profiles are required to accurately sum-marize the frequent itemsets. This greedy strategy will have no clue to split the items, since all splits produce very bad restoration errors. While this algorithm is not promising, there is nothing else to optimize since we have limited infor-mation.

We then try to view this problem from another perspec-tive. Analyzing the procedure, we can see that the first recursion call (Line 4 of Algorithm 1) projects the database with respect to item a , and the second (Line 5) simply re-moves item a . Focusing on the second recursive call, the procedure of splitting items is equivalent to removing (and subsequently projecting) items, until the remaining itemset is B -compressible . Hence, instead of finding the splitting items, alternatively, we transform the problem into that of seeking the best compressible itemset.

This alternative perspective is presented in Algorithm 2, and illustrated in Figure 3 (b). First we find the best B -compressible itemset P (line3). Theprofileformedby B and P is equivalent to that in line 12 of Algorithm 1. Having obtained this itemset, we throw each item not in P one by one as the splitting item of this branch, which is equivalent to item a in line 3 of Algorithm 1. Next, we formally prove that the Split function in Algorithm 2 is logically equivalent with that in Algorithm 1.

Lemma 3. A summary can be produced by the splitting function in Algorithm 1 if and only if it can be produced by the splitting function in Algorithm 2.

Proof. Let Tail = Q  X  P ,where Q =( q 1 ,q 2 , ... )isan ordered set, and P is B -compressible. We can simulate this procedure with the conventional perspective, by taking items q 1 , q 2 , ..., in order.

From Lemma 3, we see that Algorithm 2 simply trans-lates the problem of choosing the best item a into two sub-problems, viz., to choose the best compressible itemset P , and to find the best item ordering in Tail \ P .
 Algorithm 3 : Greedy algorithm for finding the best B -compressible itemset
C-Profiles Greedy (itemset B ,itemset Tail ) 1
Larger P covers more frequent itemsets, hence resulting in lesser frequent itemsets to be summarized. So intuitively, we should choose P to be as large as possible. This is achieved by enumerating all compressible itemsets exclusively, which can be made efficient since compressibility satisfies the anti-monotone property.

Lemma 4. If itemset X is B -compressible, then all item-sets { Y | Y  X  X } are also B -compressible.

Proof. For an itemset X to be compressible, there must be a probability distribution such that all subsets of X can be accurately estimated. Hence, if itemset X is compress-ible, we can use the same probability distribution for build-ing the profiles of X  X  X  subsets.

However, the number of compressible itemsets can be very large. In one of our experiments, the length of a compressible itemset P reaches 100, implying that number of compressible itemsets is at least 2 100 . This large number forbids an exact algorithm that finds the longest itemset.

Therefore, instead of focusing on the length of the item-set, we focus on individual items: whether we want P to contain more frequent items or less frequent items. If we keep high support items in P , then all branches will contain these frequent items. Otherwise, such an item will produce a large conditional database, or equivalently a large branch. Both extremes are good in this case. If an item is closed with respect to the base (the most frequent), then it will also be closed in subsequent projected database, and hence can always be part of the subsequent P . On the other hand, if an item is barely frequent, it is very likely that its combi-nation with other items becomes infrequent, implying that it will not appear in any subsequent Tail . Since we have no clear winner, we tried both approaches and compared them empirically.

Finding either lexicographically smallest or largest item-sets with support order can be done efficiently with a greedy algorithm as in Algorithm 3. Starting with P =  X  , this algo-rithm iteratively checks each item a with the specified order, and puts a inside P if P  X  X  a } is B -compressible. Moreover, because of the anti-monotone property, this algorithm will always produce maximal itemsets, hence the resulting item-set should be reasonably large. Analyzing the complexity, we find that this algorithm has complexity linear with re-spect to the size of Tail , multiplied with the time taken to do compressible verification (details in Section 5.2). In each iteration (Line 6 X 9 in Algorithm 2), the size of Tail is reduced by one, as also illustrated in Figure 3. We note two possible orderings, i.e., ascending and descending order of frequency. If we order the items in decreasing order of frequency, we will have many moderate sized branches. Otherwise, we will end up with one large branch and a lot of very small branches.
For an itemset Q  X  B to be B -compressible, there must be a conditional profile with I P = Q , such that all itemsets { X | B  X  X  X  B  X  Q } are accurately estimated . Based on Equation (3), the estimated support of an itemset X must satisfy several following conditions. First, if X is frequent, then we require max(  X , (1  X   X  )  X  sup ( X ))  X   X  sup CP ( X ) when X is not frequent, we require  X  sup CP ( X ) &lt; X  .Note that all terms in the formula invoke only multiplication. Hence, by applying a logarithmic transformation, we obtain linear inequalities. This problem has become identical to the problem of finding a basic feasible solution of linear pro-gram, which can be solved in an expected polynomial time, with respect to the number of variables and the number of constraints [9]. Optionally, we can minimize the restoration error per profile, by treating (1  X   X  ) as another variable, and setting the objective function to maximize that value.
If we build the inequalities for all subsets of I P ,thenum-ber of constraints is O (2 | I P | ), which is infeasible even for moderately large I P . We can reduce the number of variables by only using frequent itemsets and the negative borders of frequent itemsets (minimal itemsets which are infrequent) with respect to I P . The correctness of this reduction is pre-sented as follows.

Lemma 5. If a negative border itemset X is accurately estimated with respect to P , then all its supersets are also accurately estimated.
 Proof.  X  sup CP ( X  X  X  a } )=  X  sup CP  X  p P ( a )  X   X  sup any item a ,since p P ( a )  X  1. If X is not frequent and it is accurately estimated, then  X  sup CP &lt; X  . Similarly, X will also be infrequent, and  X  sup CP ( X  X  X  a } )  X   X  sup  X  . Therefore, X  X  X  a } is also accurately estimated.
Using this observation, the number of constraints are bounded by O ( | FI ( X ) | + | NB ( X ) | ), where FI ( X )isthesetofall frequent subsets of X ,and NB ( X ) is the set of all negative borders subsets of X . | NB ( X ) | itself is upper bounded by | FI ( X ) | X | X | since each frequent itemset has at most | neighboring negative borders.
For each recursive call (each node in the prefix-tree), our technique produces one profile. In each state, ignoring the verification time, the complexity is O ( n log n )forsortingthe items, where n is the number of items. Therefore, the total worst case complexity is given as O ( K  X  ( n  X  log n + V )), where V is the verification time.

Analyzing the verification time is difficult since we need to solve a linear programming problem. For solving LP, the complexity is only known as polynomial to the number of variables and constraints . Here, the worst verification time is when we consider all the frequent itemsets to be covered by the itemset. Let the number of frequent itemsets be m ,then the number of variables are n , and the number of constraints are bounded by n  X  m . Therefore, the worst case complexity complexity can be given as O ( K  X  ( n log n + Poly ( n  X 
In practice, our algorithm is usually find a large first pro-file (with empty base), hence the time is dominated by the first recursive call. Therefore, we can assume that the com-plexity is independent to K , i.e. O ( Poly ( n  X  FI )). Moreover, more items in the profile implies more constraints for LP, hence the verification times becomes longer. This practical analysis contradicts with previous worst case analysis, as the algorithm with less results usually takes longer time.
Since Requirements (1) and (2) are provably met by the construction algorithm (Lemma 1 ), we now focus on Re-quirement (3), i.e., the size of the summary. We first analyze the tree-structure of our cp-summary (Figure 2).

Lemma 6. Algorithm 2 has an approximation ratio of n/ 4 , where n is the number of items.

Proof. Consider that the optimal solution consists of k c-profiles { P 1 ,P 2 , ..., P k } , and the distribution vector of P consists of n i items. This condition can only happen when I
P is disjoint for all P , and no itemset formed by items in different profiles is frequent. Consider that we choose one profile P x as the c-profile . In this case, the current node (base) will have n  X  n x children, all of which are compress-ible. For example, if the child has item a as the base, it will contain only items which are originally in the same profile as a , so it is summarizable. Hence, the total number of pro-files equal n  X  n x + 1. Therefore, to minimize the number of profiles, we have to take the largest profile, and the worst case appears when all profiles are of the same size n i = n/k . Since the optimal solution contains k profiles, and our solu-tion produces n  X  n/k + 1 profiles, by simple algebra, we can derive that the worst approximation ratio is achieved when k = 2, yielding n/ 4 approximation ratio.

Analyzing the worst possible choice of P is difficult. It is possible to construct a test case where the optimal algo-rithm produces 2 profiles, while the poor algorithm produces O ( | FI | ) profiles. Therefore, we analyze the quality using empirical evaluation.
All algorithms are implemented in C++, and all experi-ments are run on an Intel Core 2, 2.22 GHz machine, with 3GB main memory. As we need to solve the linear program-ming problem (Section 5.2), we use the LPSolve package that is freely downloadable from SourceForge 2 .
We use three real datasets, viz., BMS-Webview-1, Mush-room, and Chess, which can be obtained from the FIMI website 3 , and a bunch of synthetic datasets. Some charac-teristics of the real data are presented in Figure 1.
Our results are presented in Figure 4. For each dataset, we measure the number of profiles and the time taken, by varying the error tolerance and the minimum support. As a http://lpsolve.sourceforge.net/5.5/ http://fimi.cs.helsinki.fi/data/ BMS-Webview-1 59,602 497 267 0.51% benchmark, we compare our summary size with the number of MFIs. We chose MFI because it is generally treated as the most compact representation [7], and constitutes the lower bound of several summarization techniques [14, 6]. In the figures, our techniques are denoted as a pair asc/des , corresponding to the heuristic used for each sub-problem. To recall, we have two heuristics and two subproblems. The first heuristic is to order the items not contained in P (Line 5 of Algorithm 2), and the second heuristic is to order the item in the Greedy algorithm, for choosing the best P (Algorithm 3). In each heuristic, we consider to sort the items in both ascending and descending order of frequency.
The summary size using our best heuristic is always smaller than the number of MFIs, except in the Mushroom dataset with 2% tolerance. Regarding scalability, the size of our pro-duced summary grows when minimum support decreases. This is expected since we have more inputs, while the con-straint is retained. Interestingly, except in the Mushroom dataset, the size of our summary is not sensitive to  X  .This implies that, even with very low error tolerance, our tech-nique can produce a very concise summary.

Regarding efficiency, overall, our summarization algorithm is quite efficient, especially in the Mushroom dataset. How-ever, in some cases, viz., asc-des and des-des in the Chess dataset, our algorithm is not efficient. By deeper analysis, we see that those heuristics (RHS-des) produce a very large profile for that particular case with empty base. Hence, the verification time is very long. The time inconsistency is shown in the BMS-Webview-1 dataset when the time taken is unreasonably long when  X  =0 . 2% and  X  = 10% (Figures 4(b) and (d)). As we see nothing unusual in the result (note the summary size in Figures 4 (a) and (c)), we suspect that this anomaly appears due to the inconsistent behavior of the LP-solver. In general, the LP solver is efficient when the constraints are very tight and very loose.
Next, we conducted experiments with synthetic data, which are generated using the IBM-Market-Basket database gen-erator 4 . Here, we ran three experiments where we fix the average length of transaction, the number of items, and the density. The minimum support is chosen in such a way that the number of frequent itemsets is roughly 10,000. The re-sults are depicted in Figure 5.
 First, we observe that our technique is always better than MFI. We can also see a trend  X  the denser datasets are eas-ier to summarize. However, the impact of each heuristic is again not clear. In these datasets, RHS-asc is always better than RHS-des ,with asc-asc being significantly supe-rior. This result is inconsistent with the previous study on Mushroom and Chess, where asc-des performs better. More-over, all approaches perform reasonably well (compared to http://www.almaden.ibm.com/software/quest/ Resources/index.shtml MFI), whereas des-asc had performed badly in Mushroom and Chess. We suspect that asc-asc suits the IBM-generated dataset, while asc-des suits the machine learning database. We leave deeper analysis of the heuristics for future work.
Next, we compare our approach with previous profile-based approaches [17, 8], and present the results in Figure 6. We use the Hierarchical technique from [17] (denoted as H in the figure), and the K-Regression technique from [8] (de-noted as KR ), which are the best techniques in their respec-tive problems. For our technique, we use asc-des variant, that performs reasonably well in all datasets. We compare the summary quality using the following method. First, we set  X  for our algorithm, which produces a summary of size k . We then produce summaries of size k using the compet-ing algorithms, and extract the FIs from them. We consider an itemset to be frequent if its estimated support is greater than  X  . Such itemsets can be simply enumerated using the depth-first enumeration of each profile. Having obtained the itemsets, we then measure the precision, recall, and F1. We consider an itemset as true positive if its support restora-tion error is no more than  X  . We use the same real dataset as in previous experiments, with  X  =0 . 2%, 25%, and 80% for BMS-Webview-1, Mushroom, and Chess dataset respec-tively.

Our algorithm is not plotted since it always achieves 100% precision and recall. From the figures, in general, we see that Hierarchical produces better precision, and K-Regression produces better recall when  X  increases. Considering the F 1 measure, the best technique in the best case achieves around 75% performance.

A detailed snapshot, ignoring the support restoration er-ror, is presented in Table 2. The summary size ( K ) is deter-mined by the one produced by our algorithm with  X  = 10%. From the table, we can see that previous techniques are not comprehensive even for restoring the frequent itemsets.
Since we always achieve 100% quality in the results, we conclude that our technique is better for recovering the orig-inal frequent itemsets. To add, our technique also offers the feature to independently analyze each profile, which are not provided in those two previous works.
This paper tackles the classical problem of summarizing frequent itemsets. We observe that there are several require-ments for a summary to be directly useful for analysis, which are not fulfilled by previous techniques. We propose a no-tion of c-profile , which then forms the summary. We design an algorithm to build the summary, and show that our al-gorithm is both effective and efficient. More importantly, the summary produced by our algorithm is able to fulfill the requirements.
 Future Work:
While our algorithm produces the desired result, the tech-nique in each submodule is elementary. There are still many possible variations to explore, which may produce better re-sult. For example, in Section 5.1.1, we show that taking extremely frequent itemsets and extremely infrequent item-sets are beneficial. Hence, it might be possible to perform a hybrid algorithm which combine the benefits of both. Some kind of backtracking algorithm might also be useful to boost (c) BMS1,  X  = 10% (g) Mushroom,  X  = 10% (k) Chess,  X  = 10% 100% precision, recall, and F1. the result quality, instead of a pure greedy algorithm as pro-posed in this paper. We would also like to seek an alterna-tive for the tree-structure approach, since as presented in Section 5.4, the approximation ratio of this structure is only n/ 4. For verification (Section 5.2), we reduce the problem to Linear Programming problem. Though optimal, this al-gorithm is not very efficient. Hence, it might be better to consider an approximate but efficient algorithm.

As the purpose of having a summary is for analysis, the next step is to visualize this summary. While the problem of visualizing frequent itemsets have been studied, we are not aware of works that visualize summaries. Currently, our summarization model cannot show relations between item-sets covered in different profiles, or project the database with respect to a given itemset. We would also like to research on these problems in the future.
We would like to thank Ruoming Jin &amp; Muad Abu-Ata, and Hong Cheng &amp; Xifeng Yan for their kind assistance and implementations of their approaches compared in this paper. [1] F. N. Afrati, A. Gionis, and H. Mannila.
 [2] R. Agrawal, J. Gehrke, D. Gunopulos, and [3] R. Agrawal, T. Imielinski, and A. N. Swami. Mining [4] T. Calders and B. Goethals. Non-derivable itemset [5] H. Cheng, X. Yan, J. Han, and P. S. Yu. Direct [6] J.Cheng,Y.Ke,andW.Ng.  X  -tolerance closed [7] D. Gunopulos, R. Khardon, H. Mannila, and [8] R. Jin, M. Abu-Ata, Y. Xiang, and N. Ruan. Effective [9] J. A. Kelner and D. A. Spielman. A randomized [10] M. Klemettinen, H. Mannila, P. Ronkainen, [11] M. Kryszkiewicz and M. Gajek. Concise [12] B. Liu, W. Hsu, and Y. Ma. Integrating classification [13] H. Mannila and H. Toivonen. Multiple uses of frequent [14] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. [15] C. Wang and S. Parthasarathy. Summarizing itemset [16] D. Xin, J. Han, X. Yan, and H. Cheng. Mining [17] X. Yan, H. Cheng, J. Han, and D. Xin. Summarizing [18] X. Yan, P. S. Yu, and J. Han. Graph indexing: A
